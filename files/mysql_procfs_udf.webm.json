{"text": " Thanks guys for holding until the last presentation, we've been through three days for those who were in pre-fossil days, I know it's hard, a lot of information, my brain is half melted, but yes, I hope this won't be the 20 longest minutes of your life. LeFredi he was talking about components, I didn't know, these I will talk about plugins, but I think the good thing of Fossil is that you get out with some insights on what to do next, so let's see. And the objective here of this talk is to talk about the PROCFS plugin that Percona developed in order so you can monitor Linux metrics without an agent on the server. I know I saw some of you already in pre-fossil, but I'm Vinicius, I've worked for Percona for six years almost, working with Database for a plent of time, and also the co-author of the book Learning MySQL, my colleague is here, the other author, Sergei. And this is our agenda for the day, so what to monitor in Database, let's get some insights and why monitoring, what is the difference between agent versus agentless monitoring, what are the pros and cons of each option, we will go in details of them, then we will go through the PROCFS plugin, you see it's very easy to install, how to use it, we can use it natively or with any Prometheus, and an example how to integrate with PMM from Percona, for those who doesn't know, PMM it's monitoring tool, a bundle of like several open source projects, components, glue it together to provide monitoring and observability. But if you want, you can ask me anytime. So what to monitor in a database, usually we like to monitor, for example, customer response time, so if we are booking, I'm trying to make a search for hotels, I want my search to return, let's say in less than 10 milliseconds, also KPS, so I want my search to return in less than 10 milliseconds, but I also want the search for 10,000 people at the same time return under 10 milliseconds. Understand workload behavior, what is the busiest day, what is the busiest hour of my workload, is there something different, some anomaly, this can indicate some security breach, also the most basic infrastructure components, we need to verify if our equipment is working correct, so if network that have a lot of retransmissions, files, storage, if my power supply is failing, all this kind of stuff that needs to work, otherwise we won't have the service. And as I said, like resource utilization, not only to predict what is going on now, but also for the future, security breach and so on. Maybe each company has more relevant metrics than the ones that I'm showing here, but this is just an overall example of what is most monitored around database. So why monitoring? It helps diagnose issues that happen, for example, in the weekend, something happened and I don't know what, you need a monitoring tool, because if you open the database now, you won't see the metrics from the time that problem was going on. We can understand issues that are actually happening, and also proactively, we can see, look, my load is raising, is raising, raising, I think I'm going to have an issue, maybe I need to increase the number of servers, I need to optimize some queries and so on. For those who use cloud, it is very important, because each CPU cycle, each byte costs money, so if you optimize a query, you optimize a table, you're saving CPU performance, disk storage, backup and so on. And my favorite one, it helps you sleep at night, because you don't need to keep working with disasters, you can predict them or work in a better way. Continue, so this is some metrics from Grafana, so we can get memory utilization, disk space, also we can see an estimation of, it will take me 1.86 years to run out of space, so I can plan ahead my budget, buy disk and everything, because when the database gets out of disk, it crashes, and then we need to decide who we are going to sacrifice. And as I said, we can understand what is going on inside the database, what kind of queries here you can see, but it's insert, select, predates, commands, so it's the call handlers from a SQL, and you can understand the fluctuation of them. In this case, it's a database that is doing this green line, the bigger one are inserts, so it's a heavy right database. And as I said, to understand behavior, so during normal days, business days, I have peaks of almost 40k EPS, but on weekends, I don't get more than 10, so if I'm on a weekend and then I have 30k, probably I'm under attack, or someone is doing a promotion, something that I don't know, some important person died, whatever. And there are two ways to monitor, one is using agent, the other one is letting the monitoring system monitor the database. And when you have the agent, you have it installed locally, so it's gathering information, sending to the server, you can get a lot of details, like Linux metrics, you can get all of them with node exporters from Prometheus. Sometimes you can run custom scripts, so maybe you can embed, for example, backups or restores, routines, something like that on the agent, but the cons are more related, for example, enterprise companies may suffer more, because you need authorization to run the, to install the agent from the machine, so maybe you have multiple teams, so you need to ask authorization for security, then the CISA demeaning needs to install, some other team needs to configure, and so this is one of the bad things, and it happens a lot with enterprise companies. There's an example from PMM, so we have the agents running in the database server, sending information to the central server, where data is analyzed. This is an example from Datadog, same way, agent running the same server, sending to the back end. And now agentless, the pros, it is basically the opposite of what we saw, we can reduce overhead of administrative tasks, because we only need the monitoring server to reach the database, and data will be fetched from there, like, for example, from performance schema, or the global status variables. Again, easier to approve, we are talking more about bureaucracy here. The cons, you still have some job to do, for example, I can't say I'm going to monitor LeFred's database, if he doesn't give me a user and password, of course I won't have access, so you still have some job to do. And the problem that we will try to solve with Prof KFS is you have limited scope to analyze, because you are not on the server, for example, you may need some special Linux permission to gather certain metrics, and you won't have it. And one last, if you are monitoring, let's say, 1,000 hosts remote, so you are putting a lot of stress on the central server, like it's natural, someone needs to do the job, if it's not the database server, it will be the monitoring server. This is an example of another company, the EGI, so they work by sending the information, the monitoring appliances collecting data from these hosts. And another example from PMM, in this case without agent, we can connect to anything, the advantage is, for example, RDS or OCI, that you don't have access to system metrics, unless you are using, for example, the CloudWatch metrics or the metrics from Oracle, but you can get MySQL information by simply connecting to them. So the Prof KFS plugin, it provides access to the Linux performance data, basically, it's information from slash proc and slash sys, so when you are running VMSTAT, it is basically collecting information from slash proc, and this information is gathered and populated in a view that is created with the plugin, the Prof KFS view. For those who also, security is a concern, there is a parameter, Prof KFS files spec, where you can say exactly what you want to gather. Now, for the problems, the sad face is, currently, it works only for Percona server, maybe if we go to components, we can make it work for MySQL. If you try to copy the Libby, I did it, of course, in Keras, you will crash the database with signal 11, so don't try this. The other caveat, the other cons, it only works with the same version, so even if we are talking about MySQL, Percona server, if you copy the Libby from 8010 and put on 8030, it will fail. So it always needs to be on the same version. To install the plugin, it has to install any plugin in MySQL, just a command line. When you are installing Percona server, the Libby is already in the plugin folder directory, as the thread showed in the, not only the components, but also the plugins will be there. You just install, and it works. You need to use this particular privilege that is created for this specific plugin, which is AccessProcFS. If you have a super, which I thought was a bit weird, it doesn't work. I thought that, okay, super should override, but it's just a matter of providing the privilege. And as I said, this is the variable that controls what you can collect, I highlighted three, which is our S version, I will load the average, but this is a string, so you just remove the ones that you don't want, and it is required that we start. This is a static variable, so you can change it at runtime. And using the plugin, so this is raw data, you can get from MySQL, you run the command, and exactly what you want to collect. In this case, I was running on Ubuntu 22, which is, I don't remember, oh, it's on AWS. It's a bit weird, because there is no binary, I was using generic packages, because there is no official package for MySQL. And now here we have the raw data, so we need to keep running selects all the time. And for example, when you get the CPU counts, you see a bunch of numbers, which is hard to figure it out what's going on by that way. So we can use Prometheus. Prometheus can work with its open source, it works with exporters. You have an old exporter, I think they have here, that showed about ProcSQL, there is a ProcSQL exporter, so you have plenty of options. In this case, we have the MySQL ProcFS collector running using the Prometheus exporter. So you can integrate with any tool. It doesn't need to be anything like, I know that AWS Oracle Google, they have features that you can use with Prometheus. This is an example that I took from Pierre Grafana, so if you want, you just populate here and start using it. In this case, I will show my example, we'll be with PMM, because I think it's easier. This, I'm going to leave more for the records, for those who want to try this at home, because these are basically codes. In this case, I have a container running the node exporter. As you can see, my username, password, and by the way, this host is still running, if you want to connect, please don't drop the database. And if you want to test if the exporter is running, you can do a simple curve, it will work and you'll see the metrics. These slides about integrating with PMM, basically what you have to do. First, you need to add the database to the monitoring server. So we add as a MySQL, here we will ask for you for user and password. And later on, we have this Docker exporter running, this container running, we will add as an external service. This is how you can visualize the services that are running there. So you can see, I have the database first, and this one is my container that is gathering the metrics from the OS. Those are the commands, how you list the servers, how you add the external service. Don't worry, as I said, it's here more for later on if you want to try at home from where the ideas are coming from, so you can understand what's going on. So adding the agent, and the last slide here is on Grafana, as here I was using a container, and then I can see that I'm having some issues, because I have my user 30, 30, and my IOH is at 30%. So for Docker instance, for those who are using, for example, Kubernetes, that it's hard to collect OS metrics, if you have the plugin installed, it can give you a very nice idea of what's going on. On my experience, lots of time people come in, like, my SQL is not working, and then you are going to see on a Kubernetes node, the worker node where the services host that are like 3,000 containers running, the worker is completely saturated and dying, and we are blaming my SQL, which is a small piece using the whole thing. I think I went too fast, but I had to skip some of the codes. Do you guys have any questions or any curiosity about this? Yes, Muka? That is a good question. I'm not sure what is the frequency of the collection, because like, this is more experimental too, probably don't have the frequency. It should be hard coded, for example, five seconds or something. It was Nicolai who developed it from Percona, and I tested when we restart, the database information is lost, so it's only a view that it's populated along the time. It's not started, like, you don't have 30 days of monitoring or anything like that. So, you don't know how it went, where is it? No, no, I don't. It's when you ask for it. No, it says it's lost after restart. It's when you query the table that it checks the value. This is the case, it should be data. No, it's lost today. He said it's lost after restart. So, when you query that table, that view, it goes to the clock to get the information and return it. Yes, my name is in the search box, because you know it is lost because you have to query the physicality in some signals or something like that. No, that's fine, because I think it's not there when I change the code. Do you think when you restart, there is nothing? I didn't see, you know. Don't say something, Sergei. Yeah, I want to ask you that the implication of no cache and implication of this plugin is let it go, actually, go ahead, take data from proper pass load it into memory, parse it up, use CPUs, there are any controls from the amount of memory being used, the hard limit of the number of nodes in the proper pass, like, it can be, there are situations where it has furious proper pass expansion. It is documented, there is a limitation. I think there is a number of lines that the proper pass will try to collect, and if it's kind of similar to PT's talk when there are more than 1,000, 10,000 tables, he will ignore because of the overhead. Yeah, is it the same work which was done in 2018 by Nikolai, or is it another? Probably it's a continuation, because I saw that the project in GitHub is old. What it is? It is a new plugin written by somebody else. No, no, it's by Nikolai, by Nikolai, yes. Okay, can that plugin be compiled for Oracle? So, you can, but we saw that on the source code because there is a particular privilege, Marcelo helped me, helped me, you need to change parts of my SQL code. Yes, yes, and talking to our engineering team, they have component to this. Yeah, they were talking and probably they will make something, because this is more like an adventure, a college project. It's written in the documentation, it's a experimental feature, but the engineering team, because it was crashing during my presentation, then I said, guys, I'm opening a Gira ticket, I want this next week. They didn't agree, but yeah, I think hopefully this can become a real part of my SQL. As Lefred said, if it helps the community, why not? I want to say thanks to Lefred and the whole organization of Fosden for you guys that survived until the end. It was a pleasure to be here again, and I hope to see you next year. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.68, "text": " Thanks guys for holding until the last presentation, we've been through three days for those who", "tokens": [50364, 2561, 1074, 337, 5061, 1826, 264, 1036, 5860, 11, 321, 600, 668, 807, 1045, 1708, 337, 729, 567, 50898], "temperature": 0.0, "avg_logprob": -0.25561191724694293, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.15162932872772217}, {"id": 1, "seek": 0, "start": 10.68, "end": 18.36, "text": " were in pre-fossil days, I know it's hard, a lot of information, my brain is half melted,", "tokens": [50898, 645, 294, 659, 12, 69, 772, 388, 1708, 11, 286, 458, 309, 311, 1152, 11, 257, 688, 295, 1589, 11, 452, 3567, 307, 1922, 19057, 11, 51282], "temperature": 0.0, "avg_logprob": -0.25561191724694293, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.15162932872772217}, {"id": 2, "seek": 0, "start": 18.36, "end": 24.560000000000002, "text": " but yes, I hope this won't be the 20 longest minutes of your life.", "tokens": [51282, 457, 2086, 11, 286, 1454, 341, 1582, 380, 312, 264, 945, 15438, 2077, 295, 428, 993, 13, 51592], "temperature": 0.0, "avg_logprob": -0.25561191724694293, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.15162932872772217}, {"id": 3, "seek": 2456, "start": 24.56, "end": 30.759999999999998, "text": " LeFredi he was talking about components, I didn't know, these I will talk about plugins,", "tokens": [50364, 1456, 37, 986, 72, 415, 390, 1417, 466, 6677, 11, 286, 994, 380, 458, 11, 613, 286, 486, 751, 466, 33759, 11, 50674], "temperature": 0.0, "avg_logprob": -0.281908266472094, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.05633192136883736}, {"id": 4, "seek": 2456, "start": 30.759999999999998, "end": 34.879999999999995, "text": " but I think the good thing of Fossil is that you get out with some insights on what to", "tokens": [50674, 457, 286, 519, 264, 665, 551, 295, 479, 772, 388, 307, 300, 291, 483, 484, 365, 512, 14310, 322, 437, 281, 50880], "temperature": 0.0, "avg_logprob": -0.281908266472094, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.05633192136883736}, {"id": 5, "seek": 2456, "start": 34.879999999999995, "end": 38.16, "text": " do next, so let's see.", "tokens": [50880, 360, 958, 11, 370, 718, 311, 536, 13, 51044], "temperature": 0.0, "avg_logprob": -0.281908266472094, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.05633192136883736}, {"id": 6, "seek": 2456, "start": 38.16, "end": 45.0, "text": " And the objective here of this talk is to talk about the PROCFS plugin that Percona", "tokens": [51044, 400, 264, 10024, 510, 295, 341, 751, 307, 281, 751, 466, 264, 15008, 34, 29318, 23407, 300, 3026, 1671, 64, 51386], "temperature": 0.0, "avg_logprob": -0.281908266472094, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.05633192136883736}, {"id": 7, "seek": 2456, "start": 45.0, "end": 53.32, "text": " developed in order so you can monitor Linux metrics without an agent on the server.", "tokens": [51386, 4743, 294, 1668, 370, 291, 393, 6002, 18734, 16367, 1553, 364, 9461, 322, 264, 7154, 13, 51802], "temperature": 0.0, "avg_logprob": -0.281908266472094, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.05633192136883736}, {"id": 8, "seek": 5332, "start": 53.32, "end": 59.68, "text": " I know I saw some of you already in pre-fossil, but I'm Vinicius, I've worked for Percona", "tokens": [50364, 286, 458, 286, 1866, 512, 295, 291, 1217, 294, 659, 12, 69, 772, 388, 11, 457, 286, 478, 15011, 299, 4872, 11, 286, 600, 2732, 337, 3026, 1671, 64, 50682], "temperature": 0.0, "avg_logprob": -0.24387210023169423, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.020708994939923286}, {"id": 9, "seek": 5332, "start": 59.68, "end": 68.44, "text": " for six years almost, working with Database for a plent of time, and also the co-author", "tokens": [50682, 337, 2309, 924, 1920, 11, 1364, 365, 40461, 651, 337, 257, 499, 317, 295, 565, 11, 293, 611, 264, 598, 12, 34224, 51120], "temperature": 0.0, "avg_logprob": -0.24387210023169423, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.020708994939923286}, {"id": 10, "seek": 5332, "start": 68.44, "end": 74.96000000000001, "text": " of the book Learning MySQL, my colleague is here, the other author, Sergei.", "tokens": [51120, 295, 264, 1446, 15205, 1222, 39934, 11, 452, 13532, 307, 510, 11, 264, 661, 3793, 11, 18885, 72, 13, 51446], "temperature": 0.0, "avg_logprob": -0.24387210023169423, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.020708994939923286}, {"id": 11, "seek": 5332, "start": 74.96000000000001, "end": 83.12, "text": " And this is our agenda for the day, so what to monitor in Database, let's get some insights", "tokens": [51446, 400, 341, 307, 527, 9829, 337, 264, 786, 11, 370, 437, 281, 6002, 294, 40461, 651, 11, 718, 311, 483, 512, 14310, 51854], "temperature": 0.0, "avg_logprob": -0.24387210023169423, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.020708994939923286}, {"id": 12, "seek": 8312, "start": 84.04, "end": 91.48, "text": " and why monitoring, what is the difference between agent versus agentless monitoring,", "tokens": [50410, 293, 983, 11028, 11, 437, 307, 264, 2649, 1296, 9461, 5717, 9461, 1832, 11028, 11, 50782], "temperature": 0.0, "avg_logprob": -0.21335397047155044, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.039097778499126434}, {"id": 13, "seek": 8312, "start": 91.48, "end": 98.56, "text": " what are the pros and cons of each option, we will go in details of them, then we will", "tokens": [50782, 437, 366, 264, 6267, 293, 1014, 295, 1184, 3614, 11, 321, 486, 352, 294, 4365, 295, 552, 11, 550, 321, 486, 51136], "temperature": 0.0, "avg_logprob": -0.21335397047155044, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.039097778499126434}, {"id": 14, "seek": 8312, "start": 98.56, "end": 107.60000000000001, "text": " go through the PROCFS plugin, you see it's very easy to install, how to use it, we can", "tokens": [51136, 352, 807, 264, 15008, 34, 29318, 23407, 11, 291, 536, 309, 311, 588, 1858, 281, 3625, 11, 577, 281, 764, 309, 11, 321, 393, 51588], "temperature": 0.0, "avg_logprob": -0.21335397047155044, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.039097778499126434}, {"id": 15, "seek": 10760, "start": 107.6, "end": 119.88, "text": " use it natively or with any Prometheus, and an example how to integrate with PMM from", "tokens": [50364, 764, 309, 8470, 356, 420, 365, 604, 2114, 649, 42209, 11, 293, 364, 1365, 577, 281, 13365, 365, 12499, 44, 490, 50978], "temperature": 0.0, "avg_logprob": -0.2738355187808766, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.02698386088013649}, {"id": 16, "seek": 10760, "start": 119.88, "end": 127.16, "text": " Percona, for those who doesn't know, PMM it's monitoring tool, a bundle of like several", "tokens": [50978, 3026, 1671, 64, 11, 337, 729, 567, 1177, 380, 458, 11, 12499, 44, 309, 311, 11028, 2290, 11, 257, 24438, 295, 411, 2940, 51342], "temperature": 0.0, "avg_logprob": -0.2738355187808766, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.02698386088013649}, {"id": 17, "seek": 10760, "start": 127.16, "end": 136.24, "text": " open source projects, components, glue it together to provide monitoring and observability.", "tokens": [51342, 1269, 4009, 4455, 11, 6677, 11, 8998, 309, 1214, 281, 2893, 11028, 293, 9951, 2310, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2738355187808766, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.02698386088013649}, {"id": 18, "seek": 13624, "start": 136.48000000000002, "end": 140.44, "text": " But if you want, you can ask me anytime.", "tokens": [50376, 583, 498, 291, 528, 11, 291, 393, 1029, 385, 13038, 13, 50574], "temperature": 0.0, "avg_logprob": -0.22739663234976834, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.013405387289822102}, {"id": 19, "seek": 13624, "start": 140.44, "end": 145.16, "text": " So what to monitor in a database, usually we like to monitor, for example, customer", "tokens": [50574, 407, 437, 281, 6002, 294, 257, 8149, 11, 2673, 321, 411, 281, 6002, 11, 337, 1365, 11, 5474, 50810], "temperature": 0.0, "avg_logprob": -0.22739663234976834, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.013405387289822102}, {"id": 20, "seek": 13624, "start": 145.16, "end": 154.36, "text": " response time, so if we are booking, I'm trying to make a search for hotels, I want my search", "tokens": [50810, 4134, 565, 11, 370, 498, 321, 366, 34424, 11, 286, 478, 1382, 281, 652, 257, 3164, 337, 22718, 11, 286, 528, 452, 3164, 51270], "temperature": 0.0, "avg_logprob": -0.22739663234976834, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.013405387289822102}, {"id": 21, "seek": 13624, "start": 154.36, "end": 164.08, "text": " to return, let's say in less than 10 milliseconds, also KPS, so I want my search to return in", "tokens": [51270, 281, 2736, 11, 718, 311, 584, 294, 1570, 813, 1266, 34184, 11, 611, 591, 6273, 11, 370, 286, 528, 452, 3164, 281, 2736, 294, 51756], "temperature": 0.0, "avg_logprob": -0.22739663234976834, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.013405387289822102}, {"id": 22, "seek": 16408, "start": 164.16000000000003, "end": 171.28, "text": " less than 10 milliseconds, but I also want the search for 10,000 people at the same time", "tokens": [50368, 1570, 813, 1266, 34184, 11, 457, 286, 611, 528, 264, 3164, 337, 1266, 11, 1360, 561, 412, 264, 912, 565, 50724], "temperature": 0.0, "avg_logprob": -0.2347236756355532, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.005566971842199564}, {"id": 23, "seek": 16408, "start": 171.28, "end": 179.12, "text": " return under 10 milliseconds. Understand workload behavior, what is the busiest", "tokens": [50724, 2736, 833, 1266, 34184, 13, 26093, 20139, 5223, 11, 437, 307, 264, 1255, 6495, 51116], "temperature": 0.0, "avg_logprob": -0.2347236756355532, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.005566971842199564}, {"id": 24, "seek": 16408, "start": 179.12, "end": 186.4, "text": " day, what is the busiest hour of my workload, is there something different, some anomaly,", "tokens": [51116, 786, 11, 437, 307, 264, 1255, 6495, 1773, 295, 452, 20139, 11, 307, 456, 746, 819, 11, 512, 42737, 11, 51480], "temperature": 0.0, "avg_logprob": -0.2347236756355532, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.005566971842199564}, {"id": 25, "seek": 18640, "start": 187.36, "end": 194.16, "text": " this can indicate some security breach, also the most basic infrastructure components,", "tokens": [50412, 341, 393, 13330, 512, 3825, 31086, 11, 611, 264, 881, 3875, 6896, 6677, 11, 50752], "temperature": 0.0, "avg_logprob": -0.23888915584933373, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.027274204418063164}, {"id": 26, "seek": 18640, "start": 196.16, "end": 202.72, "text": " we need to verify if our equipment is working correct, so if network that have a lot of", "tokens": [50852, 321, 643, 281, 16888, 498, 527, 5927, 307, 1364, 3006, 11, 370, 498, 3209, 300, 362, 257, 688, 295, 51180], "temperature": 0.0, "avg_logprob": -0.23888915584933373, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.027274204418063164}, {"id": 27, "seek": 18640, "start": 202.72, "end": 210.72, "text": " retransmissions, files, storage, if my power supply is failing, all this kind of stuff", "tokens": [51180, 23106, 599, 76, 7922, 11, 7098, 11, 6725, 11, 498, 452, 1347, 5847, 307, 18223, 11, 439, 341, 733, 295, 1507, 51580], "temperature": 0.0, "avg_logprob": -0.23888915584933373, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.027274204418063164}, {"id": 28, "seek": 21072, "start": 211.44, "end": 214.96, "text": " that needs to work, otherwise we won't have the service.", "tokens": [50400, 300, 2203, 281, 589, 11, 5911, 321, 1582, 380, 362, 264, 2643, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10872724321153429, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.00832912977784872}, {"id": 29, "seek": 21072, "start": 216.48, "end": 222.48, "text": " And as I said, like resource utilization, not only to predict what is going on now,", "tokens": [50652, 400, 382, 286, 848, 11, 411, 7684, 37074, 11, 406, 787, 281, 6069, 437, 307, 516, 322, 586, 11, 50952], "temperature": 0.0, "avg_logprob": -0.10872724321153429, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.00832912977784872}, {"id": 30, "seek": 21072, "start": 222.48, "end": 231.2, "text": " but also for the future, security breach and so on. Maybe each company has more relevant", "tokens": [50952, 457, 611, 337, 264, 2027, 11, 3825, 31086, 293, 370, 322, 13, 2704, 1184, 2237, 575, 544, 7340, 51388], "temperature": 0.0, "avg_logprob": -0.10872724321153429, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.00832912977784872}, {"id": 31, "seek": 21072, "start": 231.2, "end": 238.88, "text": " metrics than the ones that I'm showing here, but this is just an overall example of what is most", "tokens": [51388, 16367, 813, 264, 2306, 300, 286, 478, 4099, 510, 11, 457, 341, 307, 445, 364, 4787, 1365, 295, 437, 307, 881, 51772], "temperature": 0.0, "avg_logprob": -0.10872724321153429, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.00832912977784872}, {"id": 32, "seek": 23888, "start": 238.96, "end": 249.28, "text": " monitored around database. So why monitoring? It helps diagnose issues that happen, for example,", "tokens": [50368, 36255, 926, 8149, 13, 407, 983, 11028, 30, 467, 3665, 36238, 2663, 300, 1051, 11, 337, 1365, 11, 50884], "temperature": 0.0, "avg_logprob": -0.11078154889843132, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00491878017783165}, {"id": 33, "seek": 23888, "start": 249.28, "end": 253.6, "text": " in the weekend, something happened and I don't know what, you need a monitoring tool,", "tokens": [50884, 294, 264, 6711, 11, 746, 2011, 293, 286, 500, 380, 458, 437, 11, 291, 643, 257, 11028, 2290, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11078154889843132, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00491878017783165}, {"id": 34, "seek": 23888, "start": 254.4, "end": 259.36, "text": " because if you open the database now, you won't see the metrics from the time that", "tokens": [51140, 570, 498, 291, 1269, 264, 8149, 586, 11, 291, 1582, 380, 536, 264, 16367, 490, 264, 565, 300, 51388], "temperature": 0.0, "avg_logprob": -0.11078154889843132, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00491878017783165}, {"id": 35, "seek": 23888, "start": 259.36, "end": 264.88, "text": " problem was going on. We can understand issues that are actually happening,", "tokens": [51388, 1154, 390, 516, 322, 13, 492, 393, 1223, 2663, 300, 366, 767, 2737, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11078154889843132, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00491878017783165}, {"id": 36, "seek": 26488, "start": 265.84, "end": 271.12, "text": " and also proactively, we can see, look, my load is raising, is raising, raising,", "tokens": [50412, 293, 611, 447, 45679, 11, 321, 393, 536, 11, 574, 11, 452, 3677, 307, 11225, 11, 307, 11225, 11, 11225, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1829498699733189, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0022446352522820234}, {"id": 37, "seek": 26488, "start": 271.12, "end": 275.6, "text": " I think I'm going to have an issue, maybe I need to increase the number of servers,", "tokens": [50676, 286, 519, 286, 478, 516, 281, 362, 364, 2734, 11, 1310, 286, 643, 281, 3488, 264, 1230, 295, 15909, 11, 50900], "temperature": 0.0, "avg_logprob": -0.1829498699733189, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0022446352522820234}, {"id": 38, "seek": 26488, "start": 275.6, "end": 286.4, "text": " I need to optimize some queries and so on. For those who use cloud, it is very important,", "tokens": [50900, 286, 643, 281, 19719, 512, 24109, 293, 370, 322, 13, 1171, 729, 567, 764, 4588, 11, 309, 307, 588, 1021, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1829498699733189, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0022446352522820234}, {"id": 39, "seek": 28640, "start": 286.47999999999996, "end": 295.03999999999996, "text": " because each CPU cycle, each byte costs money, so if you optimize a query, you optimize a table,", "tokens": [50368, 570, 1184, 13199, 6586, 11, 1184, 40846, 5497, 1460, 11, 370, 498, 291, 19719, 257, 14581, 11, 291, 19719, 257, 3199, 11, 50796], "temperature": 0.0, "avg_logprob": -0.11793287035445092, "compression_ratio": 1.5, "no_speech_prob": 0.086174376308918}, {"id": 40, "seek": 28640, "start": 295.03999999999996, "end": 303.2, "text": " you're saving CPU performance, disk storage, backup and so on. And my favorite one, it helps", "tokens": [50796, 291, 434, 6816, 13199, 3389, 11, 12355, 6725, 11, 14807, 293, 370, 322, 13, 400, 452, 2954, 472, 11, 309, 3665, 51204], "temperature": 0.0, "avg_logprob": -0.11793287035445092, "compression_ratio": 1.5, "no_speech_prob": 0.086174376308918}, {"id": 41, "seek": 28640, "start": 303.2, "end": 309.67999999999995, "text": " you sleep at night, because you don't need to keep working with disasters, you can predict them", "tokens": [51204, 291, 2817, 412, 1818, 11, 570, 291, 500, 380, 643, 281, 1066, 1364, 365, 27966, 11, 291, 393, 6069, 552, 51528], "temperature": 0.0, "avg_logprob": -0.11793287035445092, "compression_ratio": 1.5, "no_speech_prob": 0.086174376308918}, {"id": 42, "seek": 30968, "start": 309.68, "end": 319.76, "text": " or work in a better way. Continue, so this is some metrics from Grafana, so we can get memory", "tokens": [50364, 420, 589, 294, 257, 1101, 636, 13, 24472, 11, 370, 341, 307, 512, 16367, 490, 8985, 69, 2095, 11, 370, 321, 393, 483, 4675, 50868], "temperature": 0.0, "avg_logprob": -0.14629182275736108, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.006356904748827219}, {"id": 43, "seek": 30968, "start": 319.76, "end": 332.0, "text": " utilization, disk space, also we can see an estimation of, it will take me 1.86 years to run", "tokens": [50868, 37074, 11, 12355, 1901, 11, 611, 321, 393, 536, 364, 35701, 295, 11, 309, 486, 747, 385, 502, 13, 22193, 924, 281, 1190, 51480], "temperature": 0.0, "avg_logprob": -0.14629182275736108, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.006356904748827219}, {"id": 44, "seek": 33200, "start": 332.08, "end": 340.32, "text": " out of space, so I can plan ahead my budget, buy disk and everything, because when the database", "tokens": [50368, 484, 295, 1901, 11, 370, 286, 393, 1393, 2286, 452, 4706, 11, 2256, 12355, 293, 1203, 11, 570, 562, 264, 8149, 50780], "temperature": 0.0, "avg_logprob": -0.12954086576189314, "compression_ratio": 1.5625, "no_speech_prob": 0.020509781315922737}, {"id": 45, "seek": 33200, "start": 340.32, "end": 346.8, "text": " gets out of disk, it crashes, and then we need to decide who we are going to sacrifice.", "tokens": [50780, 2170, 484, 295, 12355, 11, 309, 28642, 11, 293, 550, 321, 643, 281, 4536, 567, 321, 366, 516, 281, 11521, 13, 51104], "temperature": 0.0, "avg_logprob": -0.12954086576189314, "compression_ratio": 1.5625, "no_speech_prob": 0.020509781315922737}, {"id": 46, "seek": 33200, "start": 349.84, "end": 356.24, "text": " And as I said, we can understand what is going on inside the database, what kind of queries", "tokens": [51256, 400, 382, 286, 848, 11, 321, 393, 1223, 437, 307, 516, 322, 1854, 264, 8149, 11, 437, 733, 295, 24109, 51576], "temperature": 0.0, "avg_logprob": -0.12954086576189314, "compression_ratio": 1.5625, "no_speech_prob": 0.020509781315922737}, {"id": 47, "seek": 35624, "start": 356.24, "end": 364.24, "text": " here you can see, but it's insert, select, predates, commands, so it's the call handlers", "tokens": [50364, 510, 291, 393, 536, 11, 457, 309, 311, 8969, 11, 3048, 11, 3852, 1024, 11, 16901, 11, 370, 309, 311, 264, 818, 1011, 11977, 50764], "temperature": 0.0, "avg_logprob": -0.2031624984741211, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.008813681080937386}, {"id": 48, "seek": 35624, "start": 364.24, "end": 372.0, "text": " from a SQL, and you can understand the fluctuation of them. In this case, it's a database that is", "tokens": [50764, 490, 257, 19200, 11, 293, 291, 393, 1223, 264, 23448, 16073, 295, 552, 13, 682, 341, 1389, 11, 309, 311, 257, 8149, 300, 307, 51152], "temperature": 0.0, "avg_logprob": -0.2031624984741211, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.008813681080937386}, {"id": 49, "seek": 35624, "start": 372.0, "end": 377.76, "text": " doing this green line, the bigger one are inserts, so it's a heavy right database.", "tokens": [51152, 884, 341, 3092, 1622, 11, 264, 3801, 472, 366, 49163, 11, 370, 309, 311, 257, 4676, 558, 8149, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2031624984741211, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.008813681080937386}, {"id": 50, "seek": 37776, "start": 377.76, "end": 390.15999999999997, "text": " And as I said, to understand behavior, so during normal days, business days, I have peaks of almost", "tokens": [50364, 400, 382, 286, 848, 11, 281, 1223, 5223, 11, 370, 1830, 2710, 1708, 11, 1606, 1708, 11, 286, 362, 26897, 295, 1920, 50984], "temperature": 0.0, "avg_logprob": -0.15702941093915773, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.017608443275094032}, {"id": 51, "seek": 37776, "start": 390.15999999999997, "end": 398.08, "text": " 40k EPS, but on weekends, I don't get more than 10, so if I'm on a weekend and then I have 30k,", "tokens": [50984, 3356, 74, 462, 6273, 11, 457, 322, 23595, 11, 286, 500, 380, 483, 544, 813, 1266, 11, 370, 498, 286, 478, 322, 257, 6711, 293, 550, 286, 362, 2217, 74, 11, 51380], "temperature": 0.0, "avg_logprob": -0.15702941093915773, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.017608443275094032}, {"id": 52, "seek": 37776, "start": 398.71999999999997, "end": 404.15999999999997, "text": " probably I'm under attack, or someone is doing a promotion, something that I don't know,", "tokens": [51412, 1391, 286, 478, 833, 2690, 11, 420, 1580, 307, 884, 257, 15783, 11, 746, 300, 286, 500, 380, 458, 11, 51684], "temperature": 0.0, "avg_logprob": -0.15702941093915773, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.017608443275094032}, {"id": 53, "seek": 40416, "start": 404.88000000000005, "end": 415.84000000000003, "text": " some important person died, whatever. And there are two ways to monitor, one is using agent,", "tokens": [50400, 512, 1021, 954, 4539, 11, 2035, 13, 400, 456, 366, 732, 2098, 281, 6002, 11, 472, 307, 1228, 9461, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1100219170252482, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0017911636969074607}, {"id": 54, "seek": 40416, "start": 415.84000000000003, "end": 420.64000000000004, "text": " the other one is letting the monitoring system monitor the database.", "tokens": [50948, 264, 661, 472, 307, 8295, 264, 11028, 1185, 6002, 264, 8149, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1100219170252482, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0017911636969074607}, {"id": 55, "seek": 40416, "start": 423.04, "end": 429.04, "text": " And when you have the agent, you have it installed locally, so it's gathering information, sending", "tokens": [51308, 400, 562, 291, 362, 264, 9461, 11, 291, 362, 309, 8899, 16143, 11, 370, 309, 311, 13519, 1589, 11, 7750, 51608], "temperature": 0.0, "avg_logprob": -0.1100219170252482, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0017911636969074607}, {"id": 56, "seek": 42904, "start": 429.04, "end": 437.36, "text": " to the server, you can get a lot of details, like Linux metrics, you can get all of them", "tokens": [50364, 281, 264, 7154, 11, 291, 393, 483, 257, 688, 295, 4365, 11, 411, 18734, 16367, 11, 291, 393, 483, 439, 295, 552, 50780], "temperature": 0.0, "avg_logprob": -0.13494230641259086, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.01048591360449791}, {"id": 57, "seek": 42904, "start": 437.36, "end": 445.84000000000003, "text": " with node exporters from Prometheus. Sometimes you can run custom scripts, so maybe you can embed,", "tokens": [50780, 365, 9984, 1278, 12168, 490, 2114, 649, 42209, 13, 4803, 291, 393, 1190, 2375, 23294, 11, 370, 1310, 291, 393, 12240, 11, 51204], "temperature": 0.0, "avg_logprob": -0.13494230641259086, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.01048591360449791}, {"id": 58, "seek": 42904, "start": 445.84000000000003, "end": 453.44, "text": " for example, backups or restores, routines, something like that on the agent, but the cons", "tokens": [51204, 337, 1365, 11, 50160, 420, 1472, 2706, 11, 33827, 11, 746, 411, 300, 322, 264, 9461, 11, 457, 264, 1014, 51584], "temperature": 0.0, "avg_logprob": -0.13494230641259086, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.01048591360449791}, {"id": 59, "seek": 45344, "start": 453.44, "end": 459.84, "text": " are more related, for example, enterprise companies may suffer more, because you need", "tokens": [50364, 366, 544, 4077, 11, 337, 1365, 11, 14132, 3431, 815, 9753, 544, 11, 570, 291, 643, 50684], "temperature": 0.0, "avg_logprob": -0.19436941475703798, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.008810446597635746}, {"id": 60, "seek": 45344, "start": 459.84, "end": 465.52, "text": " authorization to run the, to install the agent from the machine, so maybe you have multiple teams,", "tokens": [50684, 33697, 281, 1190, 264, 11, 281, 3625, 264, 9461, 490, 264, 3479, 11, 370, 1310, 291, 362, 3866, 5491, 11, 50968], "temperature": 0.0, "avg_logprob": -0.19436941475703798, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.008810446597635746}, {"id": 61, "seek": 45344, "start": 465.52, "end": 470.08, "text": " so you need to ask authorization for security, then the CISA demeaning needs to install,", "tokens": [50968, 370, 291, 643, 281, 1029, 33697, 337, 3825, 11, 550, 264, 383, 26183, 35465, 8415, 2203, 281, 3625, 11, 51196], "temperature": 0.0, "avg_logprob": -0.19436941475703798, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.008810446597635746}, {"id": 62, "seek": 45344, "start": 471.44, "end": 478.15999999999997, "text": " some other team needs to configure, and so this is one of the bad things, and it happens a lot", "tokens": [51264, 512, 661, 1469, 2203, 281, 22162, 11, 293, 370, 341, 307, 472, 295, 264, 1578, 721, 11, 293, 309, 2314, 257, 688, 51600], "temperature": 0.0, "avg_logprob": -0.19436941475703798, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.008810446597635746}, {"id": 63, "seek": 47816, "start": 478.16, "end": 485.52000000000004, "text": " with enterprise companies. There's an example from PMM, so we have the agents running in the", "tokens": [50364, 365, 14132, 3431, 13, 821, 311, 364, 1365, 490, 12499, 44, 11, 370, 321, 362, 264, 12554, 2614, 294, 264, 50732], "temperature": 0.0, "avg_logprob": -0.10639427669012724, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.004461126867681742}, {"id": 64, "seek": 47816, "start": 485.52000000000004, "end": 491.92, "text": " database server, sending information to the central server, where data is analyzed.", "tokens": [50732, 8149, 7154, 11, 7750, 1589, 281, 264, 5777, 7154, 11, 689, 1412, 307, 28181, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10639427669012724, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.004461126867681742}, {"id": 65, "seek": 47816, "start": 494.16, "end": 503.12, "text": " This is an example from Datadog, same way, agent running the same server, sending to the back end.", "tokens": [51164, 639, 307, 364, 1365, 490, 9315, 345, 664, 11, 912, 636, 11, 9461, 2614, 264, 912, 7154, 11, 7750, 281, 264, 646, 917, 13, 51612], "temperature": 0.0, "avg_logprob": -0.10639427669012724, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.004461126867681742}, {"id": 66, "seek": 50312, "start": 503.92, "end": 513.44, "text": " And now agentless, the pros, it is basically the opposite of what we saw, we can reduce overhead", "tokens": [50404, 400, 586, 9461, 1832, 11, 264, 6267, 11, 309, 307, 1936, 264, 6182, 295, 437, 321, 1866, 11, 321, 393, 5407, 19922, 50880], "temperature": 0.0, "avg_logprob": -0.27386710047721863, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0009804217843338847}, {"id": 67, "seek": 50312, "start": 513.44, "end": 521.12, "text": " of administrative tasks, because we only need the monitoring server to reach the database,", "tokens": [50880, 295, 17900, 9608, 11, 570, 321, 787, 643, 264, 11028, 7154, 281, 2524, 264, 8149, 11, 51264], "temperature": 0.0, "avg_logprob": -0.27386710047721863, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0009804217843338847}, {"id": 68, "seek": 50312, "start": 521.12, "end": 526.24, "text": " and data will be fetched from there, like, for example, from performance schema,", "tokens": [51264, 293, 1412, 486, 312, 23673, 292, 490, 456, 11, 411, 11, 337, 1365, 11, 490, 3389, 34078, 11, 51520], "temperature": 0.0, "avg_logprob": -0.27386710047721863, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0009804217843338847}, {"id": 69, "seek": 52624, "start": 527.2, "end": 538.0, "text": " or the global status variables. Again, easier to approve, we are talking more about bureaucracy here.", "tokens": [50412, 420, 264, 4338, 6558, 9102, 13, 3764, 11, 3571, 281, 18827, 11, 321, 366, 1417, 544, 466, 44671, 510, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2588123773273669, "compression_ratio": 1.4120603015075377, "no_speech_prob": 0.007986064068973064}, {"id": 70, "seek": 52624, "start": 539.36, "end": 545.52, "text": " The cons, you still have some job to do, for example, I can't say I'm going to monitor", "tokens": [51020, 440, 1014, 11, 291, 920, 362, 512, 1691, 281, 360, 11, 337, 1365, 11, 286, 393, 380, 584, 286, 478, 516, 281, 6002, 51328], "temperature": 0.0, "avg_logprob": -0.2588123773273669, "compression_ratio": 1.4120603015075377, "no_speech_prob": 0.007986064068973064}, {"id": 71, "seek": 52624, "start": 545.52, "end": 550.5600000000001, "text": " LeFred's database, if he doesn't give me a user and password, of course I won't have access,", "tokens": [51328, 1456, 37, 986, 311, 8149, 11, 498, 415, 1177, 380, 976, 385, 257, 4195, 293, 11524, 11, 295, 1164, 286, 1582, 380, 362, 2105, 11, 51580], "temperature": 0.0, "avg_logprob": -0.2588123773273669, "compression_ratio": 1.4120603015075377, "no_speech_prob": 0.007986064068973064}, {"id": 72, "seek": 55056, "start": 551.1199999999999, "end": 561.52, "text": " so you still have some job to do. And the problem that we will try to solve with Prof KFS is you", "tokens": [50392, 370, 291, 920, 362, 512, 1691, 281, 360, 13, 400, 264, 1154, 300, 321, 486, 853, 281, 5039, 365, 6039, 591, 29318, 307, 291, 50912], "temperature": 0.0, "avg_logprob": -0.19564418231739716, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.031687695533037186}, {"id": 73, "seek": 55056, "start": 561.52, "end": 568.9599999999999, "text": " have limited scope to analyze, because you are not on the server, for example, you may need some", "tokens": [50912, 362, 5567, 11923, 281, 12477, 11, 570, 291, 366, 406, 322, 264, 7154, 11, 337, 1365, 11, 291, 815, 643, 512, 51284], "temperature": 0.0, "avg_logprob": -0.19564418231739716, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.031687695533037186}, {"id": 74, "seek": 55056, "start": 568.9599999999999, "end": 572.8, "text": " special Linux permission to gather certain metrics, and you won't have it.", "tokens": [51284, 2121, 18734, 11226, 281, 5448, 1629, 16367, 11, 293, 291, 1582, 380, 362, 309, 13, 51476], "temperature": 0.0, "avg_logprob": -0.19564418231739716, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.031687695533037186}, {"id": 75, "seek": 57280, "start": 573.12, "end": 579.04, "text": " And one last, if you are monitoring, let's say, 1,000 hosts remote, so you are putting a lot of stress", "tokens": [50380, 400, 472, 1036, 11, 498, 291, 366, 11028, 11, 718, 311, 584, 11, 502, 11, 1360, 21573, 8607, 11, 370, 291, 366, 3372, 257, 688, 295, 4244, 50676], "temperature": 0.0, "avg_logprob": -0.25177425808376735, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.005849197506904602}, {"id": 76, "seek": 57280, "start": 579.04, "end": 583.76, "text": " on the central server, like it's natural, someone needs to do the job, if it's not the database", "tokens": [50676, 322, 264, 5777, 7154, 11, 411, 309, 311, 3303, 11, 1580, 2203, 281, 360, 264, 1691, 11, 498, 309, 311, 406, 264, 8149, 50912], "temperature": 0.0, "avg_logprob": -0.25177425808376735, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.005849197506904602}, {"id": 77, "seek": 57280, "start": 583.76, "end": 592.7199999999999, "text": " server, it will be the monitoring server. This is an example of another company, the EGI, so they", "tokens": [50912, 7154, 11, 309, 486, 312, 264, 11028, 7154, 13, 639, 307, 364, 1365, 295, 1071, 2237, 11, 264, 462, 26252, 11, 370, 436, 51360], "temperature": 0.0, "avg_logprob": -0.25177425808376735, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.005849197506904602}, {"id": 78, "seek": 59272, "start": 593.12, "end": 606.0, "text": " work by sending the information, the monitoring appliances collecting data from these hosts.", "tokens": [50384, 589, 538, 7750, 264, 1589, 11, 264, 11028, 35480, 12510, 1412, 490, 613, 21573, 13, 51028], "temperature": 0.0, "avg_logprob": -0.13998988366896106, "compression_ratio": 1.4261363636363635, "no_speech_prob": 0.007941210642457008}, {"id": 79, "seek": 59272, "start": 608.08, "end": 613.76, "text": " And another example from PMM, in this case without agent, we can connect to anything,", "tokens": [51132, 400, 1071, 1365, 490, 12499, 44, 11, 294, 341, 1389, 1553, 9461, 11, 321, 393, 1745, 281, 1340, 11, 51416], "temperature": 0.0, "avg_logprob": -0.13998988366896106, "compression_ratio": 1.4261363636363635, "no_speech_prob": 0.007941210642457008}, {"id": 80, "seek": 59272, "start": 613.76, "end": 618.88, "text": " the advantage is, for example, RDS or OCI, that you don't have access to", "tokens": [51416, 264, 5002, 307, 11, 337, 1365, 11, 497, 11844, 420, 422, 25240, 11, 300, 291, 500, 380, 362, 2105, 281, 51672], "temperature": 0.0, "avg_logprob": -0.13998988366896106, "compression_ratio": 1.4261363636363635, "no_speech_prob": 0.007941210642457008}, {"id": 81, "seek": 61888, "start": 619.84, "end": 626.4, "text": " system metrics, unless you are using, for example, the CloudWatch metrics or the metrics from Oracle,", "tokens": [50412, 1185, 16367, 11, 5969, 291, 366, 1228, 11, 337, 1365, 11, 264, 8061, 36204, 16367, 420, 264, 16367, 490, 25654, 11, 50740], "temperature": 0.0, "avg_logprob": -0.1556203524271647, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.001836635754443705}, {"id": 82, "seek": 61888, "start": 626.4, "end": 630.48, "text": " but you can get MySQL information by simply connecting to them.", "tokens": [50740, 457, 291, 393, 483, 1222, 39934, 1589, 538, 2935, 11015, 281, 552, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1556203524271647, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.001836635754443705}, {"id": 83, "seek": 61888, "start": 632.96, "end": 640.24, "text": " So the Prof KFS plugin, it provides access to the Linux performance data, basically,", "tokens": [51068, 407, 264, 6039, 591, 29318, 23407, 11, 309, 6417, 2105, 281, 264, 18734, 3389, 1412, 11, 1936, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1556203524271647, "compression_ratio": 1.404494382022472, "no_speech_prob": 0.001836635754443705}, {"id": 84, "seek": 64024, "start": 641.2, "end": 648.4, "text": " it's information from slash proc and slash sys, so when you are running VMSTAT,", "tokens": [50412, 309, 311, 1589, 490, 17330, 9510, 293, 17330, 262, 749, 11, 370, 562, 291, 366, 2614, 18038, 6840, 2218, 11, 50772], "temperature": 0.0, "avg_logprob": -0.20533375740051268, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.014122307300567627}, {"id": 85, "seek": 64024, "start": 649.2, "end": 655.52, "text": " it is basically collecting information from slash proc, and this information is gathered and", "tokens": [50812, 309, 307, 1936, 12510, 1589, 490, 17330, 9510, 11, 293, 341, 1589, 307, 13032, 293, 51128], "temperature": 0.0, "avg_logprob": -0.20533375740051268, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.014122307300567627}, {"id": 86, "seek": 64024, "start": 655.52, "end": 660.72, "text": " populated in a view that is created with the plugin, the Prof KFS view.", "tokens": [51128, 32998, 294, 257, 1910, 300, 307, 2942, 365, 264, 23407, 11, 264, 6039, 591, 29318, 1910, 13, 51388], "temperature": 0.0, "avg_logprob": -0.20533375740051268, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.014122307300567627}, {"id": 87, "seek": 66072, "start": 660.88, "end": 670.88, "text": " For those who also, security is a concern, there is a parameter, Prof KFS files spec,", "tokens": [50372, 1171, 729, 567, 611, 11, 3825, 307, 257, 3136, 11, 456, 307, 257, 13075, 11, 6039, 591, 29318, 7098, 1608, 11, 50872], "temperature": 0.0, "avg_logprob": -0.20844260506007983, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0070877401158213615}, {"id": 88, "seek": 66072, "start": 671.52, "end": 678.64, "text": " where you can say exactly what you want to gather. Now, for the problems,", "tokens": [50904, 689, 291, 393, 584, 2293, 437, 291, 528, 281, 5448, 13, 823, 11, 337, 264, 2740, 11, 51260], "temperature": 0.0, "avg_logprob": -0.20844260506007983, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0070877401158213615}, {"id": 89, "seek": 66072, "start": 679.6800000000001, "end": 686.0, "text": " the sad face is, currently, it works only for Percona server, maybe if we go to components,", "tokens": [51312, 264, 4227, 1851, 307, 11, 4362, 11, 309, 1985, 787, 337, 3026, 1671, 64, 7154, 11, 1310, 498, 321, 352, 281, 6677, 11, 51628], "temperature": 0.0, "avg_logprob": -0.20844260506007983, "compression_ratio": 1.4425287356321839, "no_speech_prob": 0.0070877401158213615}, {"id": 90, "seek": 68600, "start": 686.0, "end": 692.72, "text": " we can make it work for MySQL. If you try to copy the Libby, I did it, of course, in Keras,", "tokens": [50364, 321, 393, 652, 309, 589, 337, 1222, 39934, 13, 759, 291, 853, 281, 5055, 264, 15834, 2322, 11, 286, 630, 309, 11, 295, 1164, 11, 294, 591, 6985, 11, 50700], "temperature": 0.0, "avg_logprob": -0.14925121344052827, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.005659486632794142}, {"id": 91, "seek": 68600, "start": 692.72, "end": 696.8, "text": " you will crash the database with signal 11, so don't try this.", "tokens": [50700, 291, 486, 8252, 264, 8149, 365, 6358, 2975, 11, 370, 500, 380, 853, 341, 13, 50904], "temperature": 0.0, "avg_logprob": -0.14925121344052827, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.005659486632794142}, {"id": 92, "seek": 68600, "start": 700.72, "end": 707.52, "text": " The other caveat, the other cons, it only works with the same version, so even if we are talking", "tokens": [51100, 440, 661, 43012, 11, 264, 661, 1014, 11, 309, 787, 1985, 365, 264, 912, 3037, 11, 370, 754, 498, 321, 366, 1417, 51440], "temperature": 0.0, "avg_logprob": -0.14925121344052827, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.005659486632794142}, {"id": 93, "seek": 68600, "start": 707.52, "end": 714.8, "text": " about MySQL, Percona server, if you copy the Libby from 8010 and put on 8030, it will fail.", "tokens": [51440, 466, 1222, 39934, 11, 3026, 1671, 64, 7154, 11, 498, 291, 5055, 264, 15834, 2322, 490, 4688, 3279, 293, 829, 322, 4688, 3446, 11, 309, 486, 3061, 13, 51804], "temperature": 0.0, "avg_logprob": -0.14925121344052827, "compression_ratio": 1.5244444444444445, "no_speech_prob": 0.005659486632794142}, {"id": 94, "seek": 71480, "start": 715.68, "end": 717.8399999999999, "text": " So it always needs to be on the same version.", "tokens": [50408, 407, 309, 1009, 2203, 281, 312, 322, 264, 912, 3037, 13, 50516], "temperature": 0.0, "avg_logprob": -0.17408879597981772, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.0013599359663203359}, {"id": 95, "seek": 71480, "start": 723.92, "end": 729.12, "text": " To install the plugin, it has to install any plugin in MySQL, just a command line.", "tokens": [50820, 1407, 3625, 264, 23407, 11, 309, 575, 281, 3625, 604, 23407, 294, 1222, 39934, 11, 445, 257, 5622, 1622, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17408879597981772, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.0013599359663203359}, {"id": 96, "seek": 71480, "start": 730.24, "end": 736.24, "text": " When you are installing Percona server, the Libby is already in the plugin folder directory,", "tokens": [51136, 1133, 291, 366, 20762, 3026, 1671, 64, 7154, 11, 264, 15834, 2322, 307, 1217, 294, 264, 23407, 10820, 21120, 11, 51436], "temperature": 0.0, "avg_logprob": -0.17408879597981772, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.0013599359663203359}, {"id": 97, "seek": 71480, "start": 736.24, "end": 741.8399999999999, "text": " as the thread showed in the, not only the components, but also the plugins will be there.", "tokens": [51436, 382, 264, 7207, 4712, 294, 264, 11, 406, 787, 264, 6677, 11, 457, 611, 264, 33759, 486, 312, 456, 13, 51716], "temperature": 0.0, "avg_logprob": -0.17408879597981772, "compression_ratio": 1.6030927835051547, "no_speech_prob": 0.0013599359663203359}, {"id": 98, "seek": 74184, "start": 741.84, "end": 748.64, "text": " You just install, and it works. You need to use this particular privilege that is created for", "tokens": [50364, 509, 445, 3625, 11, 293, 309, 1985, 13, 509, 643, 281, 764, 341, 1729, 12122, 300, 307, 2942, 337, 50704], "temperature": 0.0, "avg_logprob": -0.1464255784687243, "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.006617193575948477}, {"id": 99, "seek": 74184, "start": 748.64, "end": 755.9200000000001, "text": " this specific plugin, which is AccessProcFS. If you have a super, which I thought was a bit weird,", "tokens": [50704, 341, 2685, 23407, 11, 597, 307, 17166, 12681, 66, 29318, 13, 759, 291, 362, 257, 1687, 11, 597, 286, 1194, 390, 257, 857, 3657, 11, 51068], "temperature": 0.0, "avg_logprob": -0.1464255784687243, "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.006617193575948477}, {"id": 100, "seek": 74184, "start": 755.9200000000001, "end": 763.0400000000001, "text": " it doesn't work. I thought that, okay, super should override, but it's just a matter of providing", "tokens": [51068, 309, 1177, 380, 589, 13, 286, 1194, 300, 11, 1392, 11, 1687, 820, 42321, 11, 457, 309, 311, 445, 257, 1871, 295, 6530, 51424], "temperature": 0.0, "avg_logprob": -0.1464255784687243, "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.006617193575948477}, {"id": 101, "seek": 74184, "start": 763.76, "end": 771.52, "text": " the privilege. And as I said, this is the variable that controls what you can collect,", "tokens": [51460, 264, 12122, 13, 400, 382, 286, 848, 11, 341, 307, 264, 7006, 300, 9003, 437, 291, 393, 2500, 11, 51848], "temperature": 0.0, "avg_logprob": -0.1464255784687243, "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.006617193575948477}, {"id": 102, "seek": 77184, "start": 772.8000000000001, "end": 779.9200000000001, "text": " I highlighted three, which is our S version, I will load the average, but this is a string,", "tokens": [50412, 286, 17173, 1045, 11, 597, 307, 527, 318, 3037, 11, 286, 486, 3677, 264, 4274, 11, 457, 341, 307, 257, 6798, 11, 50768], "temperature": 0.0, "avg_logprob": -0.1704219182332357, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.0007229481125250459}, {"id": 103, "seek": 77184, "start": 779.9200000000001, "end": 786.5600000000001, "text": " so you just remove the ones that you don't want, and it is required that we start. This is a static", "tokens": [50768, 370, 291, 445, 4159, 264, 2306, 300, 291, 500, 380, 528, 11, 293, 309, 307, 4739, 300, 321, 722, 13, 639, 307, 257, 13437, 51100], "temperature": 0.0, "avg_logprob": -0.1704219182332357, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.0007229481125250459}, {"id": 104, "seek": 77184, "start": 786.5600000000001, "end": 796.96, "text": " variable, so you can change it at runtime. And using the plugin, so this is raw data,", "tokens": [51100, 7006, 11, 370, 291, 393, 1319, 309, 412, 34474, 13, 400, 1228, 264, 23407, 11, 370, 341, 307, 8936, 1412, 11, 51620], "temperature": 0.0, "avg_logprob": -0.1704219182332357, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.0007229481125250459}, {"id": 105, "seek": 79696, "start": 796.96, "end": 803.2, "text": " you can get from MySQL, you run the command, and exactly what you want to collect. In this case,", "tokens": [50364, 291, 393, 483, 490, 1222, 39934, 11, 291, 1190, 264, 5622, 11, 293, 2293, 437, 291, 528, 281, 2500, 13, 682, 341, 1389, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1760726464100373, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.01303759403526783}, {"id": 106, "seek": 79696, "start": 803.2, "end": 812.32, "text": " I was running on Ubuntu 22, which is, I don't remember, oh, it's on AWS. It's a bit weird,", "tokens": [50676, 286, 390, 2614, 322, 30230, 45605, 5853, 11, 597, 307, 11, 286, 500, 380, 1604, 11, 1954, 11, 309, 311, 322, 17650, 13, 467, 311, 257, 857, 3657, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1760726464100373, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.01303759403526783}, {"id": 107, "seek": 79696, "start": 812.32, "end": 817.9200000000001, "text": " because there is no binary, I was using generic packages, because there is no official", "tokens": [51132, 570, 456, 307, 572, 17434, 11, 286, 390, 1228, 19577, 17401, 11, 570, 456, 307, 572, 4783, 51412], "temperature": 0.0, "avg_logprob": -0.1760726464100373, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.01303759403526783}, {"id": 108, "seek": 81792, "start": 818.64, "end": 829.12, "text": " package for MySQL. And now here we have the raw data, so we need to keep running selects all the", "tokens": [50400, 7372, 337, 1222, 39934, 13, 400, 586, 510, 321, 362, 264, 8936, 1412, 11, 370, 321, 643, 281, 1066, 2614, 3048, 82, 439, 264, 50924], "temperature": 0.0, "avg_logprob": -0.12073096522578487, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.02457537315785885}, {"id": 109, "seek": 81792, "start": 829.12, "end": 835.68, "text": " time. And for example, when you get the CPU counts, you see a bunch of numbers, which is hard to", "tokens": [50924, 565, 13, 400, 337, 1365, 11, 562, 291, 483, 264, 13199, 14893, 11, 291, 536, 257, 3840, 295, 3547, 11, 597, 307, 1152, 281, 51252], "temperature": 0.0, "avg_logprob": -0.12073096522578487, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.02457537315785885}, {"id": 110, "seek": 81792, "start": 835.68, "end": 842.7199999999999, "text": " figure it out what's going on by that way. So we can use Prometheus. Prometheus can work with", "tokens": [51252, 2573, 309, 484, 437, 311, 516, 322, 538, 300, 636, 13, 407, 321, 393, 764, 2114, 649, 42209, 13, 2114, 649, 42209, 393, 589, 365, 51604], "temperature": 0.0, "avg_logprob": -0.12073096522578487, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.02457537315785885}, {"id": 111, "seek": 84272, "start": 842.8000000000001, "end": 850.32, "text": " its open source, it works with exporters. You have an old exporter, I think they have here,", "tokens": [50368, 1080, 1269, 4009, 11, 309, 1985, 365, 1278, 12168, 13, 509, 362, 364, 1331, 1278, 6122, 11, 286, 519, 436, 362, 510, 11, 50744], "temperature": 0.0, "avg_logprob": -0.15304166078567505, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.029680171981453896}, {"id": 112, "seek": 84272, "start": 850.32, "end": 857.2, "text": " that showed about ProcSQL, there is a ProcSQL exporter, so you have plenty of options. In this", "tokens": [50744, 300, 4712, 466, 1705, 66, 39934, 11, 456, 307, 257, 1705, 66, 39934, 1278, 6122, 11, 370, 291, 362, 7140, 295, 3956, 13, 682, 341, 51088], "temperature": 0.0, "avg_logprob": -0.15304166078567505, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.029680171981453896}, {"id": 113, "seek": 84272, "start": 857.2, "end": 866.64, "text": " case, we have the MySQL ProcFS collector running using the Prometheus exporter. So you can integrate", "tokens": [51088, 1389, 11, 321, 362, 264, 1222, 39934, 1705, 66, 29318, 23960, 2614, 1228, 264, 2114, 649, 42209, 1278, 6122, 13, 407, 291, 393, 13365, 51560], "temperature": 0.0, "avg_logprob": -0.15304166078567505, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.029680171981453896}, {"id": 114, "seek": 86664, "start": 866.64, "end": 873.04, "text": " with any tool. It doesn't need to be anything like, I know that AWS Oracle Google, they have", "tokens": [50364, 365, 604, 2290, 13, 467, 1177, 380, 643, 281, 312, 1340, 411, 11, 286, 458, 300, 17650, 25654, 3329, 11, 436, 362, 50684], "temperature": 0.0, "avg_logprob": -0.1747293472290039, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.028951717540621758}, {"id": 115, "seek": 86664, "start": 874.88, "end": 882.72, "text": " features that you can use with Prometheus. This is an example that I took from Pierre Grafana,", "tokens": [50776, 4122, 300, 291, 393, 764, 365, 2114, 649, 42209, 13, 639, 307, 364, 1365, 300, 286, 1890, 490, 28461, 8985, 69, 2095, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1747293472290039, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.028951717540621758}, {"id": 116, "seek": 86664, "start": 882.72, "end": 890.0, "text": " so if you want, you just populate here and start using it. In this case, I will show my example,", "tokens": [51168, 370, 498, 291, 528, 11, 291, 445, 1665, 5256, 510, 293, 722, 1228, 309, 13, 682, 341, 1389, 11, 286, 486, 855, 452, 1365, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1747293472290039, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.028951717540621758}, {"id": 117, "seek": 89000, "start": 890.0, "end": 898.4, "text": " we'll be with PMM, because I think it's easier. This, I'm going to leave more for the records,", "tokens": [50364, 321, 603, 312, 365, 12499, 44, 11, 570, 286, 519, 309, 311, 3571, 13, 639, 11, 286, 478, 516, 281, 1856, 544, 337, 264, 7724, 11, 50784], "temperature": 0.0, "avg_logprob": -0.13795336328371607, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.009179329499602318}, {"id": 118, "seek": 89000, "start": 898.4, "end": 905.12, "text": " for those who want to try this at home, because these are basically codes. In this case, I have a", "tokens": [50784, 337, 729, 567, 528, 281, 853, 341, 412, 1280, 11, 570, 613, 366, 1936, 14211, 13, 682, 341, 1389, 11, 286, 362, 257, 51120], "temperature": 0.0, "avg_logprob": -0.13795336328371607, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.009179329499602318}, {"id": 119, "seek": 89000, "start": 905.12, "end": 913.28, "text": " container running the node exporter. As you can see, my username, password, and by the way,", "tokens": [51120, 10129, 2614, 264, 9984, 1278, 6122, 13, 1018, 291, 393, 536, 11, 452, 30351, 11, 11524, 11, 293, 538, 264, 636, 11, 51528], "temperature": 0.0, "avg_logprob": -0.13795336328371607, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.009179329499602318}, {"id": 120, "seek": 89000, "start": 913.28, "end": 917.76, "text": " this host is still running, if you want to connect, please don't drop the database.", "tokens": [51528, 341, 3975, 307, 920, 2614, 11, 498, 291, 528, 281, 1745, 11, 1767, 500, 380, 3270, 264, 8149, 13, 51752], "temperature": 0.0, "avg_logprob": -0.13795336328371607, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.009179329499602318}, {"id": 121, "seek": 92000, "start": 920.4, "end": 924.88, "text": " And if you want to test if the exporter is running, you can do a simple curve,", "tokens": [50384, 400, 498, 291, 528, 281, 1500, 498, 264, 1278, 6122, 307, 2614, 11, 291, 393, 360, 257, 2199, 7605, 11, 50608], "temperature": 0.0, "avg_logprob": -0.14321885790143693, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0010553428437560797}, {"id": 122, "seek": 92000, "start": 924.88, "end": 931.12, "text": " it will work and you'll see the metrics. These slides about integrating with", "tokens": [50608, 309, 486, 589, 293, 291, 603, 536, 264, 16367, 13, 1981, 9788, 466, 26889, 365, 50920], "temperature": 0.0, "avg_logprob": -0.14321885790143693, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0010553428437560797}, {"id": 123, "seek": 92000, "start": 931.84, "end": 939.6, "text": " PMM, basically what you have to do. First, you need to add the database to the monitoring server.", "tokens": [50956, 12499, 44, 11, 1936, 437, 291, 362, 281, 360, 13, 2386, 11, 291, 643, 281, 909, 264, 8149, 281, 264, 11028, 7154, 13, 51344], "temperature": 0.0, "avg_logprob": -0.14321885790143693, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0010553428437560797}, {"id": 124, "seek": 92000, "start": 939.6, "end": 946.88, "text": " So we add as a MySQL, here we will ask for you for user and password. And later on,", "tokens": [51344, 407, 321, 909, 382, 257, 1222, 39934, 11, 510, 321, 486, 1029, 337, 291, 337, 4195, 293, 11524, 13, 400, 1780, 322, 11, 51708], "temperature": 0.0, "avg_logprob": -0.14321885790143693, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0010553428437560797}, {"id": 125, "seek": 94688, "start": 946.88, "end": 953.4399999999999, "text": " we have this Docker exporter running, this container running, we will add as an external service.", "tokens": [50364, 321, 362, 341, 33772, 1278, 6122, 2614, 11, 341, 10129, 2614, 11, 321, 486, 909, 382, 364, 8320, 2643, 13, 50692], "temperature": 0.0, "avg_logprob": -0.10410958070021409, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0013994112377986312}, {"id": 126, "seek": 94688, "start": 955.12, "end": 962.08, "text": " This is how you can visualize the services that are running there. So you can see,", "tokens": [50776, 639, 307, 577, 291, 393, 23273, 264, 3328, 300, 366, 2614, 456, 13, 407, 291, 393, 536, 11, 51124], "temperature": 0.0, "avg_logprob": -0.10410958070021409, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0013994112377986312}, {"id": 127, "seek": 94688, "start": 962.08, "end": 968.64, "text": " I have the database first, and this one is my container that is gathering the metrics from the", "tokens": [51124, 286, 362, 264, 8149, 700, 11, 293, 341, 472, 307, 452, 10129, 300, 307, 13519, 264, 16367, 490, 264, 51452], "temperature": 0.0, "avg_logprob": -0.10410958070021409, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0013994112377986312}, {"id": 128, "seek": 96864, "start": 968.64, "end": 978.3199999999999, "text": " OS. Those are the commands, how you list the servers, how you add the external service.", "tokens": [50364, 12731, 13, 3950, 366, 264, 16901, 11, 577, 291, 1329, 264, 15909, 11, 577, 291, 909, 264, 8320, 2643, 13, 50848], "temperature": 0.0, "avg_logprob": -0.16751961209880772, "compression_ratio": 1.4817073170731707, "no_speech_prob": 0.021837230771780014}, {"id": 129, "seek": 96864, "start": 979.12, "end": 983.68, "text": " Don't worry, as I said, it's here more for later on if you want to try at home", "tokens": [50888, 1468, 380, 3292, 11, 382, 286, 848, 11, 309, 311, 510, 544, 337, 1780, 322, 498, 291, 528, 281, 853, 412, 1280, 51116], "temperature": 0.0, "avg_logprob": -0.16751961209880772, "compression_ratio": 1.4817073170731707, "no_speech_prob": 0.021837230771780014}, {"id": 130, "seek": 96864, "start": 984.48, "end": 990.56, "text": " from where the ideas are coming from, so you can understand what's going on.", "tokens": [51156, 490, 689, 264, 3487, 366, 1348, 490, 11, 370, 291, 393, 1223, 437, 311, 516, 322, 13, 51460], "temperature": 0.0, "avg_logprob": -0.16751961209880772, "compression_ratio": 1.4817073170731707, "no_speech_prob": 0.021837230771780014}, {"id": 131, "seek": 99056, "start": 990.88, "end": 1002.4799999999999, "text": " So adding the agent, and the last slide here is on Grafana, as here I was using", "tokens": [50380, 407, 5127, 264, 9461, 11, 293, 264, 1036, 4137, 510, 307, 322, 8985, 69, 2095, 11, 382, 510, 286, 390, 1228, 50960], "temperature": 0.0, "avg_logprob": -0.2750280978632908, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.011231673881411552}, {"id": 132, "seek": 99056, "start": 1004.64, "end": 1012.9599999999999, "text": " a container, and then I can see that I'm having some issues, because I have my user 30, 30,", "tokens": [51068, 257, 10129, 11, 293, 550, 286, 393, 536, 300, 286, 478, 1419, 512, 2663, 11, 570, 286, 362, 452, 4195, 2217, 11, 2217, 11, 51484], "temperature": 0.0, "avg_logprob": -0.2750280978632908, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.011231673881411552}, {"id": 133, "seek": 101296, "start": 1012.96, "end": 1021.36, "text": " and my IOH is at 30%. So for Docker instance, for those who are using, for example, Kubernetes,", "tokens": [50364, 293, 452, 39839, 39, 307, 412, 2217, 6856, 407, 337, 33772, 5197, 11, 337, 729, 567, 366, 1228, 11, 337, 1365, 11, 23145, 11, 50784], "temperature": 0.0, "avg_logprob": -0.16076288725200452, "compression_ratio": 1.5, "no_speech_prob": 0.059826403856277466}, {"id": 134, "seek": 101296, "start": 1021.36, "end": 1027.2, "text": " that it's hard to collect OS metrics, if you have the plugin installed, it can give you", "tokens": [50784, 300, 309, 311, 1152, 281, 2500, 12731, 16367, 11, 498, 291, 362, 264, 23407, 8899, 11, 309, 393, 976, 291, 51076], "temperature": 0.0, "avg_logprob": -0.16076288725200452, "compression_ratio": 1.5, "no_speech_prob": 0.059826403856277466}, {"id": 135, "seek": 101296, "start": 1028.24, "end": 1033.76, "text": " a very nice idea of what's going on. On my experience, lots of time people come in,", "tokens": [51128, 257, 588, 1481, 1558, 295, 437, 311, 516, 322, 13, 1282, 452, 1752, 11, 3195, 295, 565, 561, 808, 294, 11, 51404], "temperature": 0.0, "avg_logprob": -0.16076288725200452, "compression_ratio": 1.5, "no_speech_prob": 0.059826403856277466}, {"id": 136, "seek": 101296, "start": 1033.76, "end": 1038.96, "text": " like, my SQL is not working, and then you are going to see on a Kubernetes node,", "tokens": [51404, 411, 11, 452, 19200, 307, 406, 1364, 11, 293, 550, 291, 366, 516, 281, 536, 322, 257, 23145, 9984, 11, 51664], "temperature": 0.0, "avg_logprob": -0.16076288725200452, "compression_ratio": 1.5, "no_speech_prob": 0.059826403856277466}, {"id": 137, "seek": 103896, "start": 1038.96, "end": 1045.52, "text": " the worker node where the services host that are like 3,000 containers running, the worker is", "tokens": [50364, 264, 11346, 9984, 689, 264, 3328, 3975, 300, 366, 411, 805, 11, 1360, 17089, 2614, 11, 264, 11346, 307, 50692], "temperature": 0.0, "avg_logprob": -0.15763427151574028, "compression_ratio": 1.455, "no_speech_prob": 0.011773170903325081}, {"id": 138, "seek": 103896, "start": 1045.52, "end": 1051.52, "text": " completely saturated and dying, and we are blaming my SQL, which is a small piece using the whole", "tokens": [50692, 2584, 25408, 293, 8639, 11, 293, 321, 366, 32364, 452, 19200, 11, 597, 307, 257, 1359, 2522, 1228, 264, 1379, 50992], "temperature": 0.0, "avg_logprob": -0.15763427151574028, "compression_ratio": 1.455, "no_speech_prob": 0.011773170903325081}, {"id": 139, "seek": 103896, "start": 1051.52, "end": 1063.1200000000001, "text": " thing. I think I went too fast, but I had to skip some of the codes. Do you guys have any questions", "tokens": [50992, 551, 13, 286, 519, 286, 1437, 886, 2370, 11, 457, 286, 632, 281, 10023, 512, 295, 264, 14211, 13, 1144, 291, 1074, 362, 604, 1651, 51572], "temperature": 0.0, "avg_logprob": -0.15763427151574028, "compression_ratio": 1.455, "no_speech_prob": 0.011773170903325081}, {"id": 140, "seek": 106312, "start": 1063.76, "end": 1068.08, "text": " or any curiosity about this? Yes, Muka?", "tokens": [50396, 420, 604, 18769, 466, 341, 30, 1079, 11, 376, 13599, 30, 50612], "temperature": 0.0, "avg_logprob": -0.28281154130634506, "compression_ratio": 1.1785714285714286, "no_speech_prob": 0.05230511724948883}, {"id": 141, "seek": 106312, "start": 1085.1999999999998, "end": 1091.12, "text": " That is a good question. I'm not sure what is the frequency of the collection, because like,", "tokens": [51468, 663, 307, 257, 665, 1168, 13, 286, 478, 406, 988, 437, 307, 264, 7893, 295, 264, 5765, 11, 570, 411, 11, 51764], "temperature": 0.0, "avg_logprob": -0.28281154130634506, "compression_ratio": 1.1785714285714286, "no_speech_prob": 0.05230511724948883}, {"id": 142, "seek": 109112, "start": 1091.12, "end": 1097.12, "text": " this is more experimental too, probably don't have the frequency. It should be hard coded,", "tokens": [50364, 341, 307, 544, 17069, 886, 11, 1391, 500, 380, 362, 264, 7893, 13, 467, 820, 312, 1152, 34874, 11, 50664], "temperature": 0.0, "avg_logprob": -0.20184465249379477, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.02128434367477894}, {"id": 143, "seek": 109112, "start": 1097.12, "end": 1104.0, "text": " for example, five seconds or something. It was Nicolai who developed it from Percona, and", "tokens": [50664, 337, 1365, 11, 1732, 3949, 420, 746, 13, 467, 390, 14776, 401, 1301, 567, 4743, 309, 490, 3026, 1671, 64, 11, 293, 51008], "temperature": 0.0, "avg_logprob": -0.20184465249379477, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.02128434367477894}, {"id": 144, "seek": 109112, "start": 1107.36, "end": 1113.84, "text": " I tested when we restart, the database information is lost, so it's only a view that it's populated", "tokens": [51176, 286, 8246, 562, 321, 21022, 11, 264, 8149, 1589, 307, 2731, 11, 370, 309, 311, 787, 257, 1910, 300, 309, 311, 32998, 51500], "temperature": 0.0, "avg_logprob": -0.20184465249379477, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.02128434367477894}, {"id": 145, "seek": 109112, "start": 1113.84, "end": 1119.84, "text": " along the time. It's not started, like, you don't have 30 days of monitoring or anything like that.", "tokens": [51500, 2051, 264, 565, 13, 467, 311, 406, 1409, 11, 411, 11, 291, 500, 380, 362, 2217, 1708, 295, 11028, 420, 1340, 411, 300, 13, 51800], "temperature": 0.0, "avg_logprob": -0.20184465249379477, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.02128434367477894}, {"id": 146, "seek": 112112, "start": 1122.08, "end": 1127.84, "text": " So, you don't know how it went, where is it? No, no, I don't. It's when you ask for it.", "tokens": [50412, 407, 11, 291, 500, 380, 458, 577, 309, 1437, 11, 689, 307, 309, 30, 883, 11, 572, 11, 286, 500, 380, 13, 467, 311, 562, 291, 1029, 337, 309, 13, 50700], "temperature": 0.0, "avg_logprob": -0.43876390678938043, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.18820218741893768}, {"id": 147, "seek": 112112, "start": 1127.84, "end": 1137.84, "text": " No, it says it's lost after restart. It's when you query the table that it checks the value.", "tokens": [50700, 883, 11, 309, 1619, 309, 311, 2731, 934, 21022, 13, 467, 311, 562, 291, 14581, 264, 3199, 300, 309, 13834, 264, 2158, 13, 51200], "temperature": 0.0, "avg_logprob": -0.43876390678938043, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.18820218741893768}, {"id": 148, "seek": 112112, "start": 1137.84, "end": 1147.84, "text": " This is the case, it should be data. No, it's lost today. He said it's lost after restart.", "tokens": [51200, 639, 307, 264, 1389, 11, 309, 820, 312, 1412, 13, 883, 11, 309, 311, 2731, 965, 13, 634, 848, 309, 311, 2731, 934, 21022, 13, 51700], "temperature": 0.0, "avg_logprob": -0.43876390678938043, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.18820218741893768}, {"id": 149, "seek": 114784, "start": 1148.1599999999999, "end": 1156.1599999999999, "text": " So, when you query that table, that view, it goes to the clock to get the information and return it.", "tokens": [50380, 407, 11, 562, 291, 14581, 300, 3199, 11, 300, 1910, 11, 309, 1709, 281, 264, 7830, 281, 483, 264, 1589, 293, 2736, 309, 13, 50780], "temperature": 0.0, "avg_logprob": -0.47589297043649775, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.07019156962633133}, {"id": 150, "seek": 114784, "start": 1156.1599999999999, "end": 1164.1599999999999, "text": " Yes, my name is in the search box, because you know it is lost because you have to query the physicality", "tokens": [50780, 1079, 11, 452, 1315, 307, 294, 264, 3164, 2424, 11, 570, 291, 458, 309, 307, 2731, 570, 291, 362, 281, 14581, 264, 4001, 507, 51180], "temperature": 0.0, "avg_logprob": -0.47589297043649775, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.07019156962633133}, {"id": 151, "seek": 114784, "start": 1164.1599999999999, "end": 1171.84, "text": " in some signals or something like that. No, that's fine, because I think it's not", "tokens": [51180, 294, 512, 12354, 420, 746, 411, 300, 13, 883, 11, 300, 311, 2489, 11, 570, 286, 519, 309, 311, 406, 51564], "temperature": 0.0, "avg_logprob": -0.47589297043649775, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.07019156962633133}, {"id": 152, "seek": 117184, "start": 1172.08, "end": 1175.6, "text": " there when I change the code. Do you think when you restart, there is nothing?", "tokens": [50376, 456, 562, 286, 1319, 264, 3089, 13, 1144, 291, 519, 562, 291, 21022, 11, 456, 307, 1825, 30, 50552], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 153, "seek": 117184, "start": 1175.6, "end": 1179.1999999999998, "text": " I didn't see, you know. Don't say something, Sergei.", "tokens": [50552, 286, 994, 380, 536, 11, 291, 458, 13, 1468, 380, 584, 746, 11, 18885, 72, 13, 50732], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 154, "seek": 117184, "start": 1179.1999999999998, "end": 1183.04, "text": " Yeah, I want to ask you that the implication of no cache and implication of this plugin is", "tokens": [50732, 865, 11, 286, 528, 281, 1029, 291, 300, 264, 37814, 295, 572, 19459, 293, 37814, 295, 341, 23407, 307, 50924], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 155, "seek": 117184, "start": 1183.04, "end": 1187.84, "text": " let it go, actually, go ahead, take data from proper pass load it into memory, parse it up,", "tokens": [50924, 718, 309, 352, 11, 767, 11, 352, 2286, 11, 747, 1412, 490, 2296, 1320, 3677, 309, 666, 4675, 11, 48377, 309, 493, 11, 51164], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 156, "seek": 117184, "start": 1187.84, "end": 1192.72, "text": " use CPUs, there are any controls from the amount of memory being used, the hard limit of the number", "tokens": [51164, 764, 13199, 82, 11, 456, 366, 604, 9003, 490, 264, 2372, 295, 4675, 885, 1143, 11, 264, 1152, 4948, 295, 264, 1230, 51408], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 157, "seek": 117184, "start": 1192.72, "end": 1198.08, "text": " of nodes in the proper pass, like, it can be, there are situations where it has furious", "tokens": [51408, 295, 13891, 294, 264, 2296, 1320, 11, 411, 11, 309, 393, 312, 11, 456, 366, 6851, 689, 309, 575, 33470, 51676], "temperature": 0.0, "avg_logprob": -0.32287577426794806, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.07886841893196106}, {"id": 158, "seek": 119808, "start": 1198.08, "end": 1205.9199999999998, "text": " proper pass expansion. It is documented, there is a limitation. I think there is a number of lines", "tokens": [50364, 2296, 1320, 11260, 13, 467, 307, 23007, 11, 456, 307, 257, 27432, 13, 286, 519, 456, 307, 257, 1230, 295, 3876, 50756], "temperature": 0.0, "avg_logprob": -0.3000902525136169, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.017184805124998093}, {"id": 159, "seek": 119808, "start": 1207.6799999999998, "end": 1212.72, "text": " that the proper pass will try to collect, and if it's kind of similar to PT's talk when there", "tokens": [50844, 300, 264, 2296, 1320, 486, 853, 281, 2500, 11, 293, 498, 309, 311, 733, 295, 2531, 281, 35460, 311, 751, 562, 456, 51096], "temperature": 0.0, "avg_logprob": -0.3000902525136169, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.017184805124998093}, {"id": 160, "seek": 119808, "start": 1212.72, "end": 1217.36, "text": " are more than 1,000, 10,000 tables, he will ignore because of the overhead.", "tokens": [51096, 366, 544, 813, 502, 11, 1360, 11, 1266, 11, 1360, 8020, 11, 415, 486, 11200, 570, 295, 264, 19922, 13, 51328], "temperature": 0.0, "avg_logprob": -0.3000902525136169, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.017184805124998093}, {"id": 161, "seek": 121736, "start": 1218.0, "end": 1228.24, "text": " Yeah, is it the same work which was done in 2018 by Nikolai, or is it another?", "tokens": [50396, 865, 11, 307, 309, 264, 912, 589, 597, 390, 1096, 294, 6096, 538, 13969, 401, 1301, 11, 420, 307, 309, 1071, 30, 50908], "temperature": 0.0, "avg_logprob": -0.3667091619772989, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.15388314425945282}, {"id": 162, "seek": 121736, "start": 1228.24, "end": 1232.8, "text": " Probably it's a continuation, because I saw that the project in GitHub is old.", "tokens": [50908, 9210, 309, 311, 257, 29357, 11, 570, 286, 1866, 300, 264, 1716, 294, 23331, 307, 1331, 13, 51136], "temperature": 0.0, "avg_logprob": -0.3667091619772989, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.15388314425945282}, {"id": 163, "seek": 121736, "start": 1237.12, "end": 1241.36, "text": " What it is? It is a new plugin written by somebody else.", "tokens": [51352, 708, 309, 307, 30, 467, 307, 257, 777, 23407, 3720, 538, 2618, 1646, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3667091619772989, "compression_ratio": 1.3048780487804879, "no_speech_prob": 0.15388314425945282}, {"id": 164, "seek": 124136, "start": 1241.36, "end": 1244.0, "text": " No, no, it's by Nikolai, by Nikolai, yes.", "tokens": [50364, 883, 11, 572, 11, 309, 311, 538, 13969, 401, 1301, 11, 538, 13969, 401, 1301, 11, 2086, 13, 50496], "temperature": 0.0, "avg_logprob": -0.2126090081183465, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.027294382452964783}, {"id": 165, "seek": 124136, "start": 1245.1999999999998, "end": 1249.84, "text": " Okay, can that plugin be compiled for Oracle?", "tokens": [50556, 1033, 11, 393, 300, 23407, 312, 36548, 337, 25654, 30, 50788], "temperature": 0.0, "avg_logprob": -0.2126090081183465, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.027294382452964783}, {"id": 166, "seek": 124136, "start": 1250.4799999999998, "end": 1258.4799999999998, "text": " So, you can, but we saw that on the source code because there is a particular privilege,", "tokens": [50820, 407, 11, 291, 393, 11, 457, 321, 1866, 300, 322, 264, 4009, 3089, 570, 456, 307, 257, 1729, 12122, 11, 51220], "temperature": 0.0, "avg_logprob": -0.2126090081183465, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.027294382452964783}, {"id": 167, "seek": 124136, "start": 1258.4799999999998, "end": 1263.52, "text": " Marcelo helped me, helped me, you need to change parts of my SQL code.", "tokens": [51220, 34738, 78, 4254, 385, 11, 4254, 385, 11, 291, 643, 281, 1319, 3166, 295, 452, 19200, 3089, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2126090081183465, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.027294382452964783}, {"id": 168, "seek": 124136, "start": 1265.6799999999998, "end": 1269.6, "text": " Yes, yes, and talking to our engineering team, they have", "tokens": [51580, 1079, 11, 2086, 11, 293, 1417, 281, 527, 7043, 1469, 11, 436, 362, 51776], "temperature": 0.0, "avg_logprob": -0.2126090081183465, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.027294382452964783}, {"id": 169, "seek": 126960, "start": 1269.9199999999998, "end": 1271.1999999999998, "text": " component to this.", "tokens": [50380, 6542, 281, 341, 13, 50444], "temperature": 0.0, "avg_logprob": -0.2750665380599651, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.011054678820073605}, {"id": 170, "seek": 126960, "start": 1271.1999999999998, "end": 1279.04, "text": " Yeah, they were talking and probably they will make something, because this is more like", "tokens": [50444, 865, 11, 436, 645, 1417, 293, 1391, 436, 486, 652, 746, 11, 570, 341, 307, 544, 411, 50836], "temperature": 0.0, "avg_logprob": -0.2750665380599651, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.011054678820073605}, {"id": 171, "seek": 126960, "start": 1280.8, "end": 1286.7199999999998, "text": " an adventure, a college project. It's written in the documentation, it's a", "tokens": [50924, 364, 9868, 11, 257, 3859, 1716, 13, 467, 311, 3720, 294, 264, 14333, 11, 309, 311, 257, 51220], "temperature": 0.0, "avg_logprob": -0.2750665380599651, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.011054678820073605}, {"id": 172, "seek": 126960, "start": 1286.7199999999998, "end": 1293.4399999999998, "text": " experimental feature, but the engineering team, because it was crashing during my presentation,", "tokens": [51220, 17069, 4111, 11, 457, 264, 7043, 1469, 11, 570, 309, 390, 26900, 1830, 452, 5860, 11, 51556], "temperature": 0.0, "avg_logprob": -0.2750665380599651, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.011054678820073605}, {"id": 173, "seek": 126960, "start": 1293.4399999999998, "end": 1299.12, "text": " then I said, guys, I'm opening a Gira ticket, I want this next week. They didn't agree, but yeah,", "tokens": [51556, 550, 286, 848, 11, 1074, 11, 286, 478, 5193, 257, 460, 4271, 10550, 11, 286, 528, 341, 958, 1243, 13, 814, 994, 380, 3986, 11, 457, 1338, 11, 51840], "temperature": 0.0, "avg_logprob": -0.2750665380599651, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.011054678820073605}, {"id": 174, "seek": 129912, "start": 1299.12, "end": 1306.32, "text": " I think hopefully this can become a real part of my SQL. As Lefred said,", "tokens": [50364, 286, 519, 4696, 341, 393, 1813, 257, 957, 644, 295, 452, 19200, 13, 1018, 1456, 69, 986, 848, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1804292027543231, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0018861002754420042}, {"id": 175, "seek": 129912, "start": 1306.32, "end": 1308.8, "text": " if it helps the community, why not?", "tokens": [50724, 498, 309, 3665, 264, 1768, 11, 983, 406, 30, 50848], "temperature": 0.0, "avg_logprob": -0.1804292027543231, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0018861002754420042}, {"id": 176, "seek": 129912, "start": 1313.6, "end": 1317.6, "text": " I want to say thanks to Lefred and the whole organization of Fosden for you guys that", "tokens": [51088, 286, 528, 281, 584, 3231, 281, 1456, 69, 986, 293, 264, 1379, 4475, 295, 479, 329, 1556, 337, 291, 1074, 300, 51288], "temperature": 0.0, "avg_logprob": -0.1804292027543231, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0018861002754420042}, {"id": 177, "seek": 129912, "start": 1317.6, "end": 1323.76, "text": " survived until the end. It was a pleasure to be here again, and I hope to see you next year.", "tokens": [51288, 14433, 1826, 264, 917, 13, 467, 390, 257, 6834, 281, 312, 510, 797, 11, 293, 286, 1454, 281, 536, 291, 958, 1064, 13, 51596], "temperature": 0.0, "avg_logprob": -0.1804292027543231, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0018861002754420042}, {"id": 178, "seek": 132376, "start": 1323.76, "end": 1326.4, "text": " Thanks.", "tokens": [50396, 2561, 13, 50496], "temperature": 0.0, "avg_logprob": -0.621145486831665, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.05660629644989967}], "language": "en"}