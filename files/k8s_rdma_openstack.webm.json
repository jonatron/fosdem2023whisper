{"text": " Next speaker is John Garbert from StackHPC who's going to talk about self-service Kubernetes with RDMA on OpenStack. Thank you. Hello, everyone. Yeah, I pressed the button. Excellent. I'm Green. Hello, everyone. I'm John Garbert. I'm here to talk to you about OpenStack, RDMA, Kubernetes, and are they oil and water mixing or are they bread, oil, and vinegar? Hopefully I'll come into you at something nice. So start with some thank yous from my sponsors. So I work at StackHPC. We're about 20-something people now. We've got people across the UK and across Europe. So I'm based out of Cambridge, but the head office is a lot of people around Bristol, people in Poland, and people in France as well. And we work on helping people create OpenStack clouds, train them up on how to look after them, and support them through that journey and everything that's happening there. For this particular topic today, I want to say a big thank you to all of these organizations. These are all in the UK. Lastly, Jasmine. So I'm going to talk today about how do we package up these solutions and stamp them out for people as reusable pieces, and this is a project that's come out of the Jasmine Institution. And that got taken on by Iris, which is an STFC community cloud project. So they're trying to get ways in which more STFC funded activities in the UK can share the same sets of infrastructure. How do we get one pool of infrastructure and share that between all of these different research use cases? And in particular, there's lots of organizations we've been working on getting feedback from. So we've been working a lot with the SKA community in the UK, particularly the SLC community at the moment. And they've been giving us great feedback on some early versions of all of this and how to improve things. And that's actually been funded partly by also the Dirac project, which is the HPC center, a group of HPC systems. Also note the small I, not the capital I, Dirac, just to confuse everything. If you look for the small I, Dirac, that's the group of the HPC centers as opposed to the job submission system. And we've been working very closely with the research computing services at the University of Cambridge and tying this together. One of the iris sites and one of the Dirac sites, and we're starting to reuse the things coming out of Jasmine. Anyway, big thank you to all those folks. So I want to start with, why on earth would you use OpenStack and Kubernetes and not just have one big batch schedule? And really it's about getting the most value out of the infrastructure investment you've made. And today also it's worth saying that the, getting, what I really mean by that partly is the, that investment in your infrastructure is also investment in carbon cost. How do you get the best out of that investment in carbon to manufacture these machines and run these machines? And what do I mean by value? Well, that's different things to different people. I mean, had we reduced time to science, how do we get more science out of that particular investment that a community has made? So firstly it's a bit about sharing diverse infrastructure. Hopefully people aren't hungry, apologies. I've spent far too much time on unsplash, so thank you to unsplash. So there's increasing diversity, as in different flavours on the pizza here, in lots of the user requirements. So in terms of the iris community, they're currently working actually a lot more with large international collaborations, and often those users come with a system that they want to run on your infrastructure, regardless of everything else that's happening. And so one of the problems that's been happening is you sort of silo your infrastructure into well, this was bought for purpose A, this was bought for purpose B, but actually those infrastructures are getting more diverse. There's only so many GPUs anyone person can afford in a particular institution, and everyone wants to use them. How do we share that out? How do we share out the accelerators and all the special bits of kit between these different use cases that day to day might be different people wanting to use those bits of infrastructure? That's kind of how do we slice it up? And also one physical server, particularly when you're doing test and development, is getting bigger and bigger in terms of consuming it, so giving people one whole server can be a problem. The other thing, and I'm speaking as a developer here before I bash developers, we love breaking things. So if you give people access to the kernel and they're going crazy and they crash the kernel, if it's just your little kernel, then it's only you you've just crashed. That's a bit of an extreme example to be fair. I don't really mean crashing the kernel, I more mean crashing the thing that you put in the kernel more likely to be particular. Anyway, how do we separate this up? And actually probably a better analogy rather than pizza is sort of a reconfigurable conference room. So if you plan ahead, you can make this kind of change. So sometimes you want to use all of the room for a really big meeting, like this one. Sometimes you want to divide it up, and when you divide it up, you kind of want a certain amount of isolation, and not accidentally, you can get the noisy neighbor problem in these setups. So you have to be careful about actually how you're doing that dividing. And so one of the things that's also changed most recently is how do we get these reusable bits of infrastructure. So I said we've got a well reusable platforms on top of the infrastructure. So one of the things I said about the IRIS project is it's working a lot with international communities coming with a thing to run. Very often these days that thing to run is packages and Kubernetes. Sometimes people are developing on Kubernetes on their laptops, and they need a bigger Kubernetes, but this is certainly becoming a thing now. People just say, you know, whereas this is how I'm wanting to deploy, how do I carve out the Kubernetes infrastructure and have Kubernetes on top of it to do what I need to do? And actually it's been very helpful in terms of giving us a higher level of abstraction that we're working with to kind of, you know, to package up web applications and interactive applications and a whole manner of things. Okay, so the next piece in the topic was why RDMA networking or why random access, remote direct memory access. I can remember that, so I put it in there. I thought to try and prove my point, I'd show a pretty graph. This is open foam. At the bottom here, there's a link to the tool that we use to actually run these benchmarks, and to make it nice and repeatable. Essentially you can describe in a Kubernetes CRD the kind of benchmark you want to run, and then it basically submits a job to Volcano, monitors the output and just tells you what the output of that was. It's just a way of just making it nice and quickly reproducible. So if you look at this graph, it's showing basically wall clock time for the simulation. And on these lines, we've got lots of different networking technologies that were being tested out, and not unsurprisingly the ones that were performing the best have all got the lowest wall clock time, so the best result in this particular benchmark. As you can see, this was probably an interesting configuration in the sense that as you were scaling out the compute, there was actually no benefit at all in terms of the simulation time. Actually, interestingly, because of this slightly wackadoodle configuration, or the job was too small essentially, you can actually see in the TCP ones above, they gradually actually get worse as they've got more cross communication, as you would expect with MPI underneath here. So if we dive down into MPI, on the left-hand side we've got the latencies, and these bottom two latencies for people at the back of the room, there's two at about five microseconds and one that's about half of that. These are interesting, these are the RDMA ones. Actually, I'm saying RDMA here, these are actually all rocky using Ethernet, as you probably guessed, because I just said what the latencies were, if you're interested in that kind of thing. So there's no such thing as a free coffee unless you're at Fosden, I guess, but let's just compare very briefly those three technologies. If we have a look at the bandwidth, there's something interesting happening here. It would be slightly more interesting if we'd actually had the hardware for long enough and run the rest of the points, but you can see that the one with the lowest latency actually caps out about 100 gigabits a second, and the ones with a slightly higher latency, or double if you're being mean, actually go all the way up to the 200 gigabits a second, and actually there's a difference in the way in which that's been wired up, which I'll go into in a bit more detail later, but essentially one of them can use the whole bond, and one of them can only use one side of the bond. So these were on service with bonded 100 gig ethernet. If you pay a latency penalty, you can use both sides of the bond in an interesting way. If you want the ultimate lowest latency, you kind of have to dedicate and just use one side of the bond. Anyway, so why do you make a big difference to these kind of workloads? I'm referencing a talk here that was at KubeCon, five ways with a CNI. If you look at the FOSDEM session information for this talk, one of the links on there is to a blog that we wrote about this kind of thing, and there's a video from KubeCon you can watch to have more detail, and this particular set of bang for bang, how's all these different ways of wiring the networks. So that all sounded a bit complicated, right? How do we actually stamp this out in a kind of useful way for users and get this all tied together? So how do we manage that operational complexity? So the first side of this is in terms of deploying at the OpenStack layer and configuring all of that, we've got tools from the OpenStack community, from the Collar community in particular, Kube and Collar Ansible, and we use those with Ansible playbooks to sort of repeatedly, once you've got a working configuration, make sure you do that every time. It involves ensuring you can re-image the machines easily and make sure that you apply the Ansible on there and get the same thing each time, so sort of package that up, and that is all open for people to reuse. And then the next stage is the users need to actually consume this infrastructure. So if we give people OpenStack directly, they can get very confused, because the people that are trying to just create a platform are typically not experts in using cloud infrastructure. So how do we make that easier? So I want to talk about azimuth. This is the project that I mentioned at the beginning coming from the Jasmine team, and the idea here is for the people creating platforms, so for the platform creators, people who want to create a Jupyter Hub or a Dask Hub or a Slurm cluster that's isolated and dedicated for their own needs. This might be for a development use case or otherwise, or create a Kubernetes cluster. How do we just package up those good practices and make that really easy to deploy, so calling this platform as a service? If you've seen me talk about this before, one of the changes here is that you get all of the platforms in one view now, so you can log in using your OpenStack credentials. So there's the cloud operator, and then there's the platform operator logs into azimuth, creates the platform, then on top of the platform, you can choose which users can log into that, just to make all of that much easier to do. So I'll quickly go through the types of things that are going on here and the different types of platforms. So firstly, there's Ansible-based platforms. So things like, give me a bigger laptop, which is a particular case, and so give me a Linux workstation that I can just guacamole into, or then give me a Slurm cluster. What we do for that is they're not Kubernetes-based. We use Terraform to stamp out virtual machines, and then there's Ansible, basically, Ansible's running Terraform to stamp out machines and do any final configuration that might be required. So when you click the button, all of that happens in the background, and it sets up the infrastructure and you can get straight in. The other type is, give me a Kubernetes cluster, I go into this in a bit more detail in a sec, but you choose your Kubernetes cluster, set that up, and it stamps that out. And the third type, which is relatively new now, is, well, I just want a Jupyter Hub or a Dask Hub, and so for those kind of situations, we're deploying those on the Kubernetes cluster, so you can go through that. So let's go into a bit more detail. This is more just a bit of an eye chart, particularly because it's not rendering at all. The idea is you just ask some basic questions about creating a Kubernetes cluster, what size nodes you want, what name it is. If you're creating your Kubernetes application, and you've pressed go into the Kubernetes application, you give it a name and the basic constraints for the notebooks, and it's sort of pre-configured and you tell it which Kubernetes cluster to put it on, or create one if you haven't got one yet. And then finally, when you've stamped out all of these bits of infrastructure, you can see there's a nice single sign-on to go and dig in. So if you've got Dask Hub, you can click on the link to sort of open your notebook, and it gets you straight in. One of the issues we've got at the moment is that there's a cost of IPv4 addresses or the shortage of IPv4 addresses is a big deal. So we're actually using a Zenith proxy here, a tunneling proxy called Zenith. So essentially when we create the infrastructure, there's an SSH session poking out, doing a port forward essentially, out into the proxy, and the proxy secures that it does all the authentication and authorization, and then punches that through. So essentially it means that these are inside, you've got a VM inside your private network, and then it goes out through the NAT, not consuming floating IPs for each of these bits of infrastructure that you're stamping out. And there's lots of, I'm not going to go into too much detail on all these things. If you create a Kubernetes cluster, it's easy to get the kubectl out. It's got monitoring included, and SLIM, similarly, it comes with monitoring open-on-demand dashboards. So in this case, you can get in and out through open-on-demand, although this one does require a public IP so that you can do SSH. I said about bigger desktop, so if you just want a VM, you can get into without worrying about SSH, without having to configure all that. You can go in through Guacamole, get a web terminal and otherwise. Again, you can stamp out all of these without consuming a floating IP. Another mode, which is a bit like Binderhub, but just inside a single VM, is just you specify your repo to Docker. Same kind of idea. It spins up the Jupyter Notebook, punches it out with Zenith, so it's all nice and simple to just get that up and running. Okay, so let's do a little bit of a shortish technical dive into actually how do you get RDMA in Loki, what the heck is Loki, you may have said. If you've been in some of the open-infotalks, Thierry described this quite well. This is the idea of Linux OpenStack and Kubernetes, giving you dynamic infrastructure. How do we get RDMA into this stack? There's three main steps. First of all, you do need RDMA in the OpenStack servers that you're creating. Second step is, if you want Kubernetes, you need the Kubernetes clusters on those OpenStack servers. The third step is you need RDMA inside the Kubernetes pods, executing within the Kubernetes clusters. So let's just drill down into each of those. So how do we do RDMA inside the OpenStack servers? Well, there's two main routes here. The first route is if it's a bare metal server, you've got the nick there, RDMA is generally available in the way it's normally available. This is not a lot special to do there. I should stop there for a moment. What I've said is you're using the standard OpenStack APIs and all the Terraform tooling and you're stamping out bare metal machines. That's totally possible. When you select the flavor dropdown, it might be give me a box with an 8A100 on it. I want the whole thing. That's perfectly possible. So I referenced Cambridge as helping us out with this. Cambridge's HPC clusters are actually deployed on OpenStack using the bare metal orchestration. So it doesn't get in the way of anything in terms of RDMA or InfiniBand or whatever. You get the bare metal machine. On the VM side, it's a little bit more complicated. Essentially, the easiest way to get RDMA working in there is that we pass in an actual nick using PCI Passu, the SROV. So the VM itself has to have drivers appropriate for the nick that you've passed through. Now there's a whole bunch of different strategies for doing that, but I wanted to quickly go through this one, which is using specifically in some MeloLux cards, and there are other ways of doing this. Essentially you do OVS offload onto your virtual function. So if you do SROV into the VM, that virtual function can actually get attached into OVS. Now that sounds insane because that's a really slow path and you just put a nice fast thing into a slow path. What happens is OVS gets told that actually you look for hardware offloaded flows. So when you actually start getting connections going into your different machines, it notices the MAC and IP address pairs and those flows in OVS get put into the hardware and then it goes onto a fast path. The other part of this is that you connect the OVS directly to your bond on the host and the VFs are actually getting connected to the bond. So in that earlier graph where I was showing 200 gigabits a second and basically getting line rate, that's using this setup where essentially your VM with its virtual function is going through the bond rather than through one of the individual interfaces. And this is actually quite a nice setup in terms of wiring. So if you've got a server that's got dual 100 gig ethernet going in or dual 25 gig ethernet, you don't have to dedicate one of those ports to SROV. You have the host bond on there and you can connect the virtual functions into the host bond. Okay, so the next bit, create Kubernetes. I'm not going to go into that too much detail. Essentially we're using Cluster API. I really like its logo because it uses basically you create a management cluster. In CRDs you describe what you want your HA cluster to be or your other cluster to be and it stamps that out for using an operator. This has proved to be really quite a stable way and reliable way of creating Kubernetes. One part of this is that we're actually hoping to try and, well, while I'm in the room, I'm trying to fix the unit test on it, but we're developing a Magnum driver for OpenStack Magnum to actually consume Cluster API and just stamp them out. To make this repeatable, it's all been packaged up in Helm charts, which are here. So now we've got OpenStack machines that have got RDMA in, we can do that. We've set up a Kubernetes cluster that's using those OpenStack machines that have the virtual function in that's doing RDMA at line rate. Now how on earth do we get the Kubernetes pods to actually make use of RDMA? Now if this was a bare metal machine, there's actually quite a lot of standard patterns. It seemed to be quite well documented in terms of actually using virtual functions into the pod. If we're inside a VM, we've already done the PF to VF translation, so you can't go again. You can't have a VVF yet, although VDPA and other things might change this. So what we're actually doing is we're using Maltis and something called the Mac VLAN CNI. So essentially when you create your Kubernetes pod, you give it two interfaces, your regular CNI interface, so that has all the usual smarts, and you give it an additional Mac IP address pair on your virtual function for the VM. Now at the moment, you have to turn off port security to ensure that those extra Macs that are auto-generated inside Kubernetes are punching out correctly and not restricted by the virtual function. There's a plan to try and orchestrate that, so you can use allowed address pairs to explicitly decide which ones. But that's basically, so you use Maltis to say, give me two network connections, and you use Mac VLAN to get that connection to your RDMA. And there's also some permission stuff, which is actually quite a simple decorator on the pod. But essentially, extra pod YAML to opt in to actually how to get this all wired together. Okay, so it'd be really great if people have these problems, and this is interesting, to get involved. There's a whole load of links, but yeah, thank you very much. And before you've got time for half a question. Yeah, you mentioned, I thought you mentioned that we are doing the bond on the network interface. You're getting the full bandwidth of the bond. So whenever I do LICP bonding, any particular connection, I only get half the interface. So I'm just wondering how you're doing that. It depends on your bonding mode. But yeah, so with LICP bonding, I only say make it half. Well, no, so there's a hashing mode on your bond. So what you need to make sure is that you do something like L3 plus L4 hashing, so that from a single client, it depends, it's basically, each of your traffic flows gets hashed onto a different bit of the bond. So you need drivers that are respecting that hashing function. But yeah, if you get enough different flows, then it will actually hash across the bond, okay. It's all about the hashing modes. Not all switches support all hashing modes, which is the gotcha in that. Yeah, the other question I have is, I don't understand the connection between MAC VLAN and RDNA. Sorry, what's that? The connection between MAC VLAN and RDNA. The connection between the MAC VLAN and RDNA. Yeah, why do you need MAC VLAN to do the RDNA into your VMs? So you could just do host networking. So if you did host networking on the pod, you would just have access to all of those host interfaces. But if you want to have multiple different RDNA flows with different MAC and IP address pairs, then the MAC VLAN allows you to have those multiple pods, each with their own identity on your VLAN that's doing RDNA. Anyway, emails for the next questions, I think. So I should let the next person set up. Any other questions for Joan? Can it? Oh. Yeah, last one. Actually, I had two, but okay, I'll re-work it. Okay. So I saw that you were also creating slurm clusters. Yes. So how does Kubernetes and slurm play together for the network topology and placement of your networks? Well, I have lots of ideas for that after your talk. At the moment, not really. So they just, the pods get placed wherever and then... Yeah, at the moment, they're totally isolated environments. So you stamp out a slurm cluster and it's your own to do what you need. And then super briefly, the pink line that was legacy RDNA, SR, IO virtualization. Yes. Is that bare metal or is that also virtualized? That was... Is it running on... Because that was... We can catch up later. Okay. So that specific scenario, I definitely recommend watching Stigt Alpha's talk, they're five ways on CNI. I think that particular setup was actually bare metal with a virtual function. So it was actually Kubernetes on bare metal with the virtual function passed into the container. Right. I believe we got similar results without doing that legacy path into the VM as well. The extra cost, I believe, is on the VF lag piece because there's an extra bit in routing inside the silicon, I believe, but I'm not certain on that, so I'd have to check. Thank you. Pleasure. Thank you very much, John. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.540000000000001, "text": " Next speaker is John Garbert from StackHPC who's going to talk about self-service Kubernetes", "tokens": [50364, 3087, 8145, 307, 2619, 7995, 4290, 490, 37649, 39, 12986, 567, 311, 516, 281, 751, 466, 2698, 12, 39279, 23145, 50891], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 1, "seek": 0, "start": 10.540000000000001, "end": 13.26, "text": " with RDMA on OpenStack.", "tokens": [50891, 365, 49488, 9998, 322, 7238, 4520, 501, 13, 51027], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 2, "seek": 0, "start": 13.26, "end": 14.26, "text": " Thank you.", "tokens": [51027, 1044, 291, 13, 51077], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 3, "seek": 0, "start": 14.26, "end": 15.26, "text": " Hello, everyone.", "tokens": [51077, 2425, 11, 1518, 13, 51127], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 4, "seek": 0, "start": 15.26, "end": 18.14, "text": " Yeah, I pressed the button.", "tokens": [51127, 865, 11, 286, 17355, 264, 2960, 13, 51271], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 5, "seek": 0, "start": 18.14, "end": 19.14, "text": " Excellent.", "tokens": [51271, 16723, 13, 51321], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 6, "seek": 0, "start": 19.14, "end": 20.14, "text": " I'm Green.", "tokens": [51321, 286, 478, 6969, 13, 51371], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 7, "seek": 0, "start": 20.14, "end": 21.14, "text": " Hello, everyone.", "tokens": [51371, 2425, 11, 1518, 13, 51421], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 8, "seek": 0, "start": 21.14, "end": 22.14, "text": " I'm John Garbert.", "tokens": [51421, 286, 478, 2619, 7995, 4290, 13, 51471], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 9, "seek": 0, "start": 22.14, "end": 29.86, "text": " I'm here to talk to you about OpenStack, RDMA, Kubernetes, and are they oil and water mixing", "tokens": [51471, 286, 478, 510, 281, 751, 281, 291, 466, 7238, 4520, 501, 11, 49488, 9998, 11, 23145, 11, 293, 366, 436, 3184, 293, 1281, 11983, 51857], "temperature": 0.0, "avg_logprob": -0.25985090549175555, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.5706444978713989}, {"id": 10, "seek": 2986, "start": 29.86, "end": 32.74, "text": " or are they bread, oil, and vinegar?", "tokens": [50364, 420, 366, 436, 5961, 11, 3184, 11, 293, 18030, 30, 50508], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 11, "seek": 2986, "start": 32.74, "end": 37.42, "text": " Hopefully I'll come into you at something nice.", "tokens": [50508, 10429, 286, 603, 808, 666, 291, 412, 746, 1481, 13, 50742], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 12, "seek": 2986, "start": 37.42, "end": 41.18, "text": " So start with some thank yous from my sponsors.", "tokens": [50742, 407, 722, 365, 512, 1309, 291, 82, 490, 452, 22593, 13, 50930], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 13, "seek": 2986, "start": 41.18, "end": 43.18, "text": " So I work at StackHPC.", "tokens": [50930, 407, 286, 589, 412, 37649, 39, 12986, 13, 51030], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 14, "seek": 2986, "start": 43.18, "end": 47.78, "text": " We're about 20-something people now.", "tokens": [51030, 492, 434, 466, 945, 12, 31681, 561, 586, 13, 51260], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 15, "seek": 2986, "start": 47.78, "end": 52.9, "text": " We've got people across the UK and across Europe.", "tokens": [51260, 492, 600, 658, 561, 2108, 264, 7051, 293, 2108, 3315, 13, 51516], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 16, "seek": 2986, "start": 52.9, "end": 56.22, "text": " So I'm based out of Cambridge, but the head office is a lot of people around Bristol, people", "tokens": [51516, 407, 286, 478, 2361, 484, 295, 24876, 11, 457, 264, 1378, 3398, 307, 257, 688, 295, 561, 926, 41208, 11, 561, 51682], "temperature": 0.0, "avg_logprob": -0.29343682527542114, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.01683906465768814}, {"id": 17, "seek": 5622, "start": 56.22, "end": 59.98, "text": " in Poland, and people in France as well.", "tokens": [50364, 294, 15950, 11, 293, 561, 294, 6190, 382, 731, 13, 50552], "temperature": 0.0, "avg_logprob": -0.18337010201953707, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.02994341403245926}, {"id": 18, "seek": 5622, "start": 59.98, "end": 67.12, "text": " And we work on helping people create OpenStack clouds, train them up on how to look after", "tokens": [50552, 400, 321, 589, 322, 4315, 561, 1884, 7238, 4520, 501, 12193, 11, 3847, 552, 493, 322, 577, 281, 574, 934, 50909], "temperature": 0.0, "avg_logprob": -0.18337010201953707, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.02994341403245926}, {"id": 19, "seek": 5622, "start": 67.12, "end": 72.82, "text": " them, and support them through that journey and everything that's happening there.", "tokens": [50909, 552, 11, 293, 1406, 552, 807, 300, 4671, 293, 1203, 300, 311, 2737, 456, 13, 51194], "temperature": 0.0, "avg_logprob": -0.18337010201953707, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.02994341403245926}, {"id": 20, "seek": 5622, "start": 72.82, "end": 78.22, "text": " For this particular topic today, I want to say a big thank you to all of these organizations.", "tokens": [51194, 1171, 341, 1729, 4829, 965, 11, 286, 528, 281, 584, 257, 955, 1309, 291, 281, 439, 295, 613, 6150, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18337010201953707, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.02994341403245926}, {"id": 21, "seek": 5622, "start": 78.22, "end": 80.58, "text": " These are all in the UK.", "tokens": [51464, 1981, 366, 439, 294, 264, 7051, 13, 51582], "temperature": 0.0, "avg_logprob": -0.18337010201953707, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.02994341403245926}, {"id": 22, "seek": 8058, "start": 80.58, "end": 81.58, "text": " Lastly, Jasmine.", "tokens": [50364, 18072, 11, 36224, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 23, "seek": 8058, "start": 81.58, "end": 88.58, "text": " So I'm going to talk today about how do we package up these solutions and stamp them", "tokens": [50414, 407, 286, 478, 516, 281, 751, 965, 466, 577, 360, 321, 7372, 493, 613, 6547, 293, 9921, 552, 50764], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 24, "seek": 8058, "start": 88.58, "end": 93.06, "text": " out for people as reusable pieces, and this is a project that's come out of the Jasmine", "tokens": [50764, 484, 337, 561, 382, 41807, 3755, 11, 293, 341, 307, 257, 1716, 300, 311, 808, 484, 295, 264, 36224, 50988], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 25, "seek": 8058, "start": 93.06, "end": 94.06, "text": " Institution.", "tokens": [50988, 2730, 6518, 13, 51038], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 26, "seek": 8058, "start": 94.06, "end": 99.86, "text": " And that got taken on by Iris, which is an STFC community cloud project.", "tokens": [51038, 400, 300, 658, 2726, 322, 538, 40789, 11, 597, 307, 364, 4904, 18671, 1768, 4588, 1716, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 27, "seek": 8058, "start": 99.86, "end": 105.62, "text": " So they're trying to get ways in which more STFC funded activities in the UK can share", "tokens": [51328, 407, 436, 434, 1382, 281, 483, 2098, 294, 597, 544, 4904, 18671, 14385, 5354, 294, 264, 7051, 393, 2073, 51616], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 28, "seek": 8058, "start": 105.62, "end": 107.06, "text": " the same sets of infrastructure.", "tokens": [51616, 264, 912, 6352, 295, 6896, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1859581704233207, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.48787960410118103}, {"id": 29, "seek": 10706, "start": 107.06, "end": 111.58, "text": " How do we get one pool of infrastructure and share that between all of these different", "tokens": [50364, 1012, 360, 321, 483, 472, 7005, 295, 6896, 293, 2073, 300, 1296, 439, 295, 613, 819, 50590], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 30, "seek": 10706, "start": 111.58, "end": 113.58, "text": " research use cases?", "tokens": [50590, 2132, 764, 3331, 30, 50690], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 31, "seek": 10706, "start": 113.58, "end": 119.5, "text": " And in particular, there's lots of organizations we've been working on getting feedback from.", "tokens": [50690, 400, 294, 1729, 11, 456, 311, 3195, 295, 6150, 321, 600, 668, 1364, 322, 1242, 5824, 490, 13, 50986], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 32, "seek": 10706, "start": 119.5, "end": 124.62, "text": " So we've been working a lot with the SKA community in the UK, particularly the SLC community", "tokens": [50986, 407, 321, 600, 668, 1364, 257, 688, 365, 264, 318, 16135, 1768, 294, 264, 7051, 11, 4098, 264, 318, 14766, 1768, 51242], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 33, "seek": 10706, "start": 124.62, "end": 125.62, "text": " at the moment.", "tokens": [51242, 412, 264, 1623, 13, 51292], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 34, "seek": 10706, "start": 125.62, "end": 130.98000000000002, "text": " And they've been giving us great feedback on some early versions of all of this and", "tokens": [51292, 400, 436, 600, 668, 2902, 505, 869, 5824, 322, 512, 2440, 9606, 295, 439, 295, 341, 293, 51560], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 35, "seek": 10706, "start": 130.98000000000002, "end": 131.98000000000002, "text": " how to improve things.", "tokens": [51560, 577, 281, 3470, 721, 13, 51610], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 36, "seek": 10706, "start": 131.98000000000002, "end": 135.94, "text": " And that's actually been funded partly by also the Dirac project, which is the HPC", "tokens": [51610, 400, 300, 311, 767, 668, 14385, 17031, 538, 611, 264, 34422, 326, 1716, 11, 597, 307, 264, 12557, 34, 51808], "temperature": 0.0, "avg_logprob": -0.21279686787089364, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.05225612595677376}, {"id": 37, "seek": 13594, "start": 135.94, "end": 143.38, "text": " center, a group of HPC systems.", "tokens": [50364, 3056, 11, 257, 1594, 295, 12557, 34, 3652, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 38, "seek": 13594, "start": 143.38, "end": 149.06, "text": " Also note the small I, not the capital I, Dirac, just to confuse everything.", "tokens": [50736, 2743, 3637, 264, 1359, 286, 11, 406, 264, 4238, 286, 11, 34422, 326, 11, 445, 281, 28584, 1203, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 39, "seek": 13594, "start": 149.06, "end": 152.82, "text": " If you look for the small I, Dirac, that's the group of the HPC centers as opposed to", "tokens": [51020, 759, 291, 574, 337, 264, 1359, 286, 11, 34422, 326, 11, 300, 311, 264, 1594, 295, 264, 12557, 34, 10898, 382, 8851, 281, 51208], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 40, "seek": 13594, "start": 152.82, "end": 156.7, "text": " the job submission system.", "tokens": [51208, 264, 1691, 23689, 1185, 13, 51402], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 41, "seek": 13594, "start": 156.7, "end": 160.74, "text": " And we've been working very closely with the research computing services at the University", "tokens": [51402, 400, 321, 600, 668, 1364, 588, 8185, 365, 264, 2132, 15866, 3328, 412, 264, 3535, 51604], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 42, "seek": 13594, "start": 160.74, "end": 163.38, "text": " of Cambridge and tying this together.", "tokens": [51604, 295, 24876, 293, 32405, 341, 1214, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1976426373357358, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.016354480758309364}, {"id": 43, "seek": 16338, "start": 163.38, "end": 168.1, "text": " One of the iris sites and one of the Dirac sites, and we're starting to reuse the things", "tokens": [50364, 1485, 295, 264, 3418, 271, 7533, 293, 472, 295, 264, 34422, 326, 7533, 11, 293, 321, 434, 2891, 281, 26225, 264, 721, 50600], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 44, "seek": 16338, "start": 168.1, "end": 170.1, "text": " coming out of Jasmine.", "tokens": [50600, 1348, 484, 295, 36224, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 45, "seek": 16338, "start": 170.1, "end": 175.1, "text": " Anyway, big thank you to all those folks.", "tokens": [50700, 5684, 11, 955, 1309, 291, 281, 439, 729, 4024, 13, 50950], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 46, "seek": 16338, "start": 175.1, "end": 181.7, "text": " So I want to start with, why on earth would you use OpenStack and Kubernetes and not just", "tokens": [50950, 407, 286, 528, 281, 722, 365, 11, 983, 322, 4120, 576, 291, 764, 7238, 4520, 501, 293, 23145, 293, 406, 445, 51280], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 47, "seek": 16338, "start": 181.7, "end": 186.46, "text": " have one big batch schedule?", "tokens": [51280, 362, 472, 955, 15245, 7567, 30, 51518], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 48, "seek": 16338, "start": 186.46, "end": 190.46, "text": " And really it's about getting the most value out of the infrastructure investment you've", "tokens": [51518, 400, 534, 309, 311, 466, 1242, 264, 881, 2158, 484, 295, 264, 6896, 6078, 291, 600, 51718], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 49, "seek": 16338, "start": 190.46, "end": 191.46, "text": " made.", "tokens": [51718, 1027, 13, 51768], "temperature": 0.0, "avg_logprob": -0.2599690106450295, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.02400997467339039}, {"id": 50, "seek": 19146, "start": 191.46, "end": 195.86, "text": " And today also it's worth saying that the, getting, what I really mean by that partly", "tokens": [50364, 400, 965, 611, 309, 311, 3163, 1566, 300, 264, 11, 1242, 11, 437, 286, 534, 914, 538, 300, 17031, 50584], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 51, "seek": 19146, "start": 195.86, "end": 200.9, "text": " is the, that investment in your infrastructure is also investment in carbon cost.", "tokens": [50584, 307, 264, 11, 300, 6078, 294, 428, 6896, 307, 611, 6078, 294, 5954, 2063, 13, 50836], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 52, "seek": 19146, "start": 200.9, "end": 205.18, "text": " How do you get the best out of that investment in carbon to manufacture these machines and", "tokens": [50836, 1012, 360, 291, 483, 264, 1151, 484, 295, 300, 6078, 294, 5954, 281, 27400, 613, 8379, 293, 51050], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 53, "seek": 19146, "start": 205.18, "end": 207.46, "text": " run these machines?", "tokens": [51050, 1190, 613, 8379, 30, 51164], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 54, "seek": 19146, "start": 207.46, "end": 208.66, "text": " And what do I mean by value?", "tokens": [51164, 400, 437, 360, 286, 914, 538, 2158, 30, 51224], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 55, "seek": 19146, "start": 208.66, "end": 210.82, "text": " Well, that's different things to different people.", "tokens": [51224, 1042, 11, 300, 311, 819, 721, 281, 819, 561, 13, 51332], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 56, "seek": 19146, "start": 210.82, "end": 215.9, "text": " I mean, had we reduced time to science, how do we get more science out of that particular", "tokens": [51332, 286, 914, 11, 632, 321, 9212, 565, 281, 3497, 11, 577, 360, 321, 483, 544, 3497, 484, 295, 300, 1729, 51586], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 57, "seek": 19146, "start": 215.9, "end": 220.58, "text": " investment that a community has made?", "tokens": [51586, 6078, 300, 257, 1768, 575, 1027, 30, 51820], "temperature": 0.0, "avg_logprob": -0.19898952872066175, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.005988472141325474}, {"id": 58, "seek": 22058, "start": 220.58, "end": 226.46, "text": " So firstly it's a bit about sharing diverse infrastructure.", "tokens": [50364, 407, 27376, 309, 311, 257, 857, 466, 5414, 9521, 6896, 13, 50658], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 59, "seek": 22058, "start": 226.46, "end": 228.78, "text": " Hopefully people aren't hungry, apologies.", "tokens": [50658, 10429, 561, 3212, 380, 8067, 11, 34929, 13, 50774], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 60, "seek": 22058, "start": 228.78, "end": 236.26000000000002, "text": " I've spent far too much time on unsplash, so thank you to unsplash.", "tokens": [50774, 286, 600, 4418, 1400, 886, 709, 565, 322, 2693, 564, 1299, 11, 370, 1309, 291, 281, 2693, 564, 1299, 13, 51148], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 61, "seek": 22058, "start": 236.26000000000002, "end": 241.54000000000002, "text": " So there's increasing diversity, as in different flavours on the pizza here, in lots of the", "tokens": [51148, 407, 456, 311, 5662, 8811, 11, 382, 294, 819, 49450, 322, 264, 8298, 510, 11, 294, 3195, 295, 264, 51412], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 62, "seek": 22058, "start": 241.54000000000002, "end": 243.18, "text": " user requirements.", "tokens": [51412, 4195, 7728, 13, 51494], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 63, "seek": 22058, "start": 243.18, "end": 247.86, "text": " So in terms of the iris community, they're currently working actually a lot more with", "tokens": [51494, 407, 294, 2115, 295, 264, 3418, 271, 1768, 11, 436, 434, 4362, 1364, 767, 257, 688, 544, 365, 51728], "temperature": 0.0, "avg_logprob": -0.23485602101972025, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.0025325785391032696}, {"id": 64, "seek": 24786, "start": 247.86, "end": 253.06, "text": " large international collaborations, and often those users come with a system that they want", "tokens": [50364, 2416, 5058, 36908, 11, 293, 2049, 729, 5022, 808, 365, 257, 1185, 300, 436, 528, 50624], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 65, "seek": 24786, "start": 253.06, "end": 256.46000000000004, "text": " to run on your infrastructure, regardless of everything else that's happening.", "tokens": [50624, 281, 1190, 322, 428, 6896, 11, 10060, 295, 1203, 1646, 300, 311, 2737, 13, 50794], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 66, "seek": 24786, "start": 256.46000000000004, "end": 260.34000000000003, "text": " And so one of the problems that's been happening is you sort of silo your infrastructure into", "tokens": [50794, 400, 370, 472, 295, 264, 2740, 300, 311, 668, 2737, 307, 291, 1333, 295, 3425, 78, 428, 6896, 666, 50988], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 67, "seek": 24786, "start": 260.34000000000003, "end": 265.62, "text": " well, this was bought for purpose A, this was bought for purpose B, but actually those", "tokens": [50988, 731, 11, 341, 390, 4243, 337, 4334, 316, 11, 341, 390, 4243, 337, 4334, 363, 11, 457, 767, 729, 51252], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 68, "seek": 24786, "start": 265.62, "end": 267.90000000000003, "text": " infrastructures are getting more diverse.", "tokens": [51252, 6534, 44513, 366, 1242, 544, 9521, 13, 51366], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 69, "seek": 24786, "start": 267.90000000000003, "end": 272.90000000000003, "text": " There's only so many GPUs anyone person can afford in a particular institution, and everyone", "tokens": [51366, 821, 311, 787, 370, 867, 18407, 82, 2878, 954, 393, 6157, 294, 257, 1729, 7818, 11, 293, 1518, 51616], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 70, "seek": 24786, "start": 272.90000000000003, "end": 274.02000000000004, "text": " wants to use them.", "tokens": [51616, 2738, 281, 764, 552, 13, 51672], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 71, "seek": 24786, "start": 274.02000000000004, "end": 275.06, "text": " How do we share that out?", "tokens": [51672, 1012, 360, 321, 2073, 300, 484, 30, 51724], "temperature": 0.0, "avg_logprob": -0.17078473138027503, "compression_ratio": 1.77, "no_speech_prob": 0.054965946823358536}, {"id": 72, "seek": 27506, "start": 275.1, "end": 279.78000000000003, "text": " How do we share out the accelerators and all the special bits of kit between these different", "tokens": [50366, 1012, 360, 321, 2073, 484, 264, 10172, 3391, 293, 439, 264, 2121, 9239, 295, 8260, 1296, 613, 819, 50600], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 73, "seek": 27506, "start": 279.78000000000003, "end": 285.54, "text": " use cases that day to day might be different people wanting to use those bits of infrastructure?", "tokens": [50600, 764, 3331, 300, 786, 281, 786, 1062, 312, 819, 561, 7935, 281, 764, 729, 9239, 295, 6896, 30, 50888], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 74, "seek": 27506, "start": 285.54, "end": 289.06, "text": " That's kind of how do we slice it up?", "tokens": [50888, 663, 311, 733, 295, 577, 360, 321, 13153, 309, 493, 30, 51064], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 75, "seek": 27506, "start": 289.06, "end": 295.02, "text": " And also one physical server, particularly when you're doing test and development, is", "tokens": [51064, 400, 611, 472, 4001, 7154, 11, 4098, 562, 291, 434, 884, 1500, 293, 3250, 11, 307, 51362], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 76, "seek": 27506, "start": 295.02, "end": 300.22, "text": " getting bigger and bigger in terms of consuming it, so giving people one whole server can", "tokens": [51362, 1242, 3801, 293, 3801, 294, 2115, 295, 19867, 309, 11, 370, 2902, 561, 472, 1379, 7154, 393, 51622], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 77, "seek": 27506, "start": 300.22, "end": 301.94, "text": " be a problem.", "tokens": [51622, 312, 257, 1154, 13, 51708], "temperature": 0.0, "avg_logprob": -0.16195475325292472, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.0009399856207892299}, {"id": 78, "seek": 30194, "start": 301.94, "end": 307.14, "text": " The other thing, and I'm speaking as a developer here before I bash developers, we love breaking", "tokens": [50364, 440, 661, 551, 11, 293, 286, 478, 4124, 382, 257, 10754, 510, 949, 286, 46183, 8849, 11, 321, 959, 7697, 50624], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 79, "seek": 30194, "start": 307.14, "end": 308.14, "text": " things.", "tokens": [50624, 721, 13, 50674], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 80, "seek": 30194, "start": 308.14, "end": 312.18, "text": " So if you give people access to the kernel and they're going crazy and they crash the", "tokens": [50674, 407, 498, 291, 976, 561, 2105, 281, 264, 28256, 293, 436, 434, 516, 3219, 293, 436, 8252, 264, 50876], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 81, "seek": 30194, "start": 312.18, "end": 318.26, "text": " kernel, if it's just your little kernel, then it's only you you've just crashed.", "tokens": [50876, 28256, 11, 498, 309, 311, 445, 428, 707, 28256, 11, 550, 309, 311, 787, 291, 291, 600, 445, 24190, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 82, "seek": 30194, "start": 318.26, "end": 320.62, "text": " That's a bit of an extreme example to be fair.", "tokens": [51180, 663, 311, 257, 857, 295, 364, 8084, 1365, 281, 312, 3143, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 83, "seek": 30194, "start": 320.62, "end": 323.9, "text": " I don't really mean crashing the kernel, I more mean crashing the thing that you put", "tokens": [51298, 286, 500, 380, 534, 914, 26900, 264, 28256, 11, 286, 544, 914, 26900, 264, 551, 300, 291, 829, 51462], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 84, "seek": 30194, "start": 323.9, "end": 327.9, "text": " in the kernel more likely to be particular.", "tokens": [51462, 294, 264, 28256, 544, 3700, 281, 312, 1729, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2006671905517578, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.0035157580859959126}, {"id": 85, "seek": 32790, "start": 327.9, "end": 330.46, "text": " Anyway, how do we separate this up?", "tokens": [50364, 5684, 11, 577, 360, 321, 4994, 341, 493, 30, 50492], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 86, "seek": 32790, "start": 330.46, "end": 335.06, "text": " And actually probably a better analogy rather than pizza is sort of a reconfigurable conference", "tokens": [50492, 400, 767, 1391, 257, 1101, 21663, 2831, 813, 8298, 307, 1333, 295, 257, 9993, 20646, 25863, 7586, 50722], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 87, "seek": 32790, "start": 335.06, "end": 336.21999999999997, "text": " room.", "tokens": [50722, 1808, 13, 50780], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 88, "seek": 32790, "start": 336.21999999999997, "end": 339.82, "text": " So if you plan ahead, you can make this kind of change.", "tokens": [50780, 407, 498, 291, 1393, 2286, 11, 291, 393, 652, 341, 733, 295, 1319, 13, 50960], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 89, "seek": 32790, "start": 339.82, "end": 346.21999999999997, "text": " So sometimes you want to use all of the room for a really big meeting, like this one.", "tokens": [50960, 407, 2171, 291, 528, 281, 764, 439, 295, 264, 1808, 337, 257, 534, 955, 3440, 11, 411, 341, 472, 13, 51280], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 90, "seek": 32790, "start": 346.21999999999997, "end": 349.5, "text": " Sometimes you want to divide it up, and when you divide it up, you kind of want a certain", "tokens": [51280, 4803, 291, 528, 281, 9845, 309, 493, 11, 293, 562, 291, 9845, 309, 493, 11, 291, 733, 295, 528, 257, 1629, 51444], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 91, "seek": 32790, "start": 349.5, "end": 355.21999999999997, "text": " amount of isolation, and not accidentally, you can get the noisy neighbor problem in", "tokens": [51444, 2372, 295, 16001, 11, 293, 406, 15715, 11, 291, 393, 483, 264, 24518, 5987, 1154, 294, 51730], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 92, "seek": 32790, "start": 355.21999999999997, "end": 357.21999999999997, "text": " these setups.", "tokens": [51730, 613, 46832, 13, 51830], "temperature": 0.0, "avg_logprob": -0.17401612599690755, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.27459025382995605}, {"id": 93, "seek": 35722, "start": 357.22, "end": 365.02000000000004, "text": " So you have to be careful about actually how you're doing that dividing.", "tokens": [50364, 407, 291, 362, 281, 312, 5026, 466, 767, 577, 291, 434, 884, 300, 26764, 13, 50754], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 94, "seek": 35722, "start": 365.02000000000004, "end": 370.26000000000005, "text": " And so one of the things that's also changed most recently is how do we get these reusable", "tokens": [50754, 400, 370, 472, 295, 264, 721, 300, 311, 611, 3105, 881, 3938, 307, 577, 360, 321, 483, 613, 41807, 51016], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 95, "seek": 35722, "start": 370.26000000000005, "end": 371.82000000000005, "text": " bits of infrastructure.", "tokens": [51016, 9239, 295, 6896, 13, 51094], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 96, "seek": 35722, "start": 371.82000000000005, "end": 376.42, "text": " So I said we've got a well reusable platforms on top of the infrastructure.", "tokens": [51094, 407, 286, 848, 321, 600, 658, 257, 731, 41807, 9473, 322, 1192, 295, 264, 6896, 13, 51324], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 97, "seek": 35722, "start": 376.42, "end": 379.66, "text": " So one of the things I said about the IRIS project is it's working a lot with international", "tokens": [51324, 407, 472, 295, 264, 721, 286, 848, 466, 264, 286, 9698, 1716, 307, 309, 311, 1364, 257, 688, 365, 5058, 51486], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 98, "seek": 35722, "start": 379.66, "end": 382.78000000000003, "text": " communities coming with a thing to run.", "tokens": [51486, 4456, 1348, 365, 257, 551, 281, 1190, 13, 51642], "temperature": 0.0, "avg_logprob": -0.15225523710250854, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.008071251213550568}, {"id": 99, "seek": 38278, "start": 382.78, "end": 387.97999999999996, "text": " Very often these days that thing to run is packages and Kubernetes.", "tokens": [50364, 4372, 2049, 613, 1708, 300, 551, 281, 1190, 307, 17401, 293, 23145, 13, 50624], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 100, "seek": 38278, "start": 387.97999999999996, "end": 392.94, "text": " Sometimes people are developing on Kubernetes on their laptops, and they need a bigger Kubernetes,", "tokens": [50624, 4803, 561, 366, 6416, 322, 23145, 322, 641, 27642, 11, 293, 436, 643, 257, 3801, 23145, 11, 50872], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 101, "seek": 38278, "start": 392.94, "end": 395.7, "text": " but this is certainly becoming a thing now.", "tokens": [50872, 457, 341, 307, 3297, 5617, 257, 551, 586, 13, 51010], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 102, "seek": 38278, "start": 395.7, "end": 400.29999999999995, "text": " People just say, you know, whereas this is how I'm wanting to deploy, how do I carve", "tokens": [51010, 3432, 445, 584, 11, 291, 458, 11, 9735, 341, 307, 577, 286, 478, 7935, 281, 7274, 11, 577, 360, 286, 33832, 51240], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 103, "seek": 38278, "start": 400.29999999999995, "end": 405.34, "text": " out the Kubernetes infrastructure and have Kubernetes on top of it to do what I need", "tokens": [51240, 484, 264, 23145, 6896, 293, 362, 23145, 322, 1192, 295, 309, 281, 360, 437, 286, 643, 51492], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 104, "seek": 38278, "start": 405.34, "end": 406.34, "text": " to do?", "tokens": [51492, 281, 360, 30, 51542], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 105, "seek": 38278, "start": 406.34, "end": 412.61999999999995, "text": " And actually it's been very helpful in terms of giving us a higher level of abstraction", "tokens": [51542, 400, 767, 309, 311, 668, 588, 4961, 294, 2115, 295, 2902, 505, 257, 2946, 1496, 295, 37765, 51856], "temperature": 0.0, "avg_logprob": -0.20334347656794957, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.08502262830734253}, {"id": 106, "seek": 41262, "start": 412.62, "end": 418.06, "text": " that we're working with to kind of, you know, to package up web applications and interactive", "tokens": [50364, 300, 321, 434, 1364, 365, 281, 733, 295, 11, 291, 458, 11, 281, 7372, 493, 3670, 5821, 293, 15141, 50636], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 107, "seek": 41262, "start": 418.06, "end": 422.94, "text": " applications and a whole manner of things.", "tokens": [50636, 5821, 293, 257, 1379, 9060, 295, 721, 13, 50880], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 108, "seek": 41262, "start": 422.94, "end": 432.74, "text": " Okay, so the next piece in the topic was why RDMA networking or why random access, remote", "tokens": [50880, 1033, 11, 370, 264, 958, 2522, 294, 264, 4829, 390, 983, 49488, 9998, 17985, 420, 983, 4974, 2105, 11, 8607, 51370], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 109, "seek": 41262, "start": 432.74, "end": 433.74, "text": " direct memory access.", "tokens": [51370, 2047, 4675, 2105, 13, 51420], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 110, "seek": 41262, "start": 433.74, "end": 437.22, "text": " I can remember that, so I put it in there.", "tokens": [51420, 286, 393, 1604, 300, 11, 370, 286, 829, 309, 294, 456, 13, 51594], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 111, "seek": 41262, "start": 437.22, "end": 441.5, "text": " I thought to try and prove my point, I'd show a pretty graph.", "tokens": [51594, 286, 1194, 281, 853, 293, 7081, 452, 935, 11, 286, 1116, 855, 257, 1238, 4295, 13, 51808], "temperature": 0.0, "avg_logprob": -0.25996109747117563, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.006325138732790947}, {"id": 112, "seek": 44150, "start": 441.5, "end": 443.02, "text": " This is open foam.", "tokens": [50364, 639, 307, 1269, 12958, 13, 50440], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 113, "seek": 44150, "start": 443.02, "end": 448.86, "text": " At the bottom here, there's a link to the tool that we use to actually run these benchmarks,", "tokens": [50440, 1711, 264, 2767, 510, 11, 456, 311, 257, 2113, 281, 264, 2290, 300, 321, 764, 281, 767, 1190, 613, 43751, 11, 50732], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 114, "seek": 44150, "start": 448.86, "end": 452.74, "text": " and to make it nice and repeatable.", "tokens": [50732, 293, 281, 652, 309, 1481, 293, 7149, 712, 13, 50926], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 115, "seek": 44150, "start": 452.74, "end": 457.82, "text": " Essentially you can describe in a Kubernetes CRD the kind of benchmark you want to run,", "tokens": [50926, 23596, 291, 393, 6786, 294, 257, 23145, 14123, 35, 264, 733, 295, 18927, 291, 528, 281, 1190, 11, 51180], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 116, "seek": 44150, "start": 457.82, "end": 464.3, "text": " and then it basically submits a job to Volcano, monitors the output and just tells you what", "tokens": [51180, 293, 550, 309, 1936, 8286, 1208, 257, 1691, 281, 8911, 18651, 11, 26518, 264, 5598, 293, 445, 5112, 291, 437, 51504], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 117, "seek": 44150, "start": 464.3, "end": 465.3, "text": " the output of that was.", "tokens": [51504, 264, 5598, 295, 300, 390, 13, 51554], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 118, "seek": 44150, "start": 465.3, "end": 470.9, "text": " It's just a way of just making it nice and quickly reproducible.", "tokens": [51554, 467, 311, 445, 257, 636, 295, 445, 1455, 309, 1481, 293, 2661, 11408, 32128, 13, 51834], "temperature": 0.0, "avg_logprob": -0.23228508342396129, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.08580153435468674}, {"id": 119, "seek": 47090, "start": 470.9, "end": 476.85999999999996, "text": " So if you look at this graph, it's showing basically wall clock time for the simulation.", "tokens": [50364, 407, 498, 291, 574, 412, 341, 4295, 11, 309, 311, 4099, 1936, 2929, 7830, 565, 337, 264, 16575, 13, 50662], "temperature": 0.0, "avg_logprob": -0.15139830842310067, "compression_ratio": 1.69140625, "no_speech_prob": 0.00953682791441679}, {"id": 120, "seek": 47090, "start": 476.85999999999996, "end": 482.17999999999995, "text": " And on these lines, we've got lots of different networking technologies that were being tested", "tokens": [50662, 400, 322, 613, 3876, 11, 321, 600, 658, 3195, 295, 819, 17985, 7943, 300, 645, 885, 8246, 50928], "temperature": 0.0, "avg_logprob": -0.15139830842310067, "compression_ratio": 1.69140625, "no_speech_prob": 0.00953682791441679}, {"id": 121, "seek": 47090, "start": 482.17999999999995, "end": 488.14, "text": " out, and not unsurprisingly the ones that were performing the best have all got the", "tokens": [50928, 484, 11, 293, 406, 2693, 374, 34408, 264, 2306, 300, 645, 10205, 264, 1151, 362, 439, 658, 264, 51226], "temperature": 0.0, "avg_logprob": -0.15139830842310067, "compression_ratio": 1.69140625, "no_speech_prob": 0.00953682791441679}, {"id": 122, "seek": 47090, "start": 488.14, "end": 493.82, "text": " lowest wall clock time, so the best result in this particular benchmark.", "tokens": [51226, 12437, 2929, 7830, 565, 11, 370, 264, 1151, 1874, 294, 341, 1729, 18927, 13, 51510], "temperature": 0.0, "avg_logprob": -0.15139830842310067, "compression_ratio": 1.69140625, "no_speech_prob": 0.00953682791441679}, {"id": 123, "seek": 47090, "start": 493.82, "end": 500.65999999999997, "text": " As you can see, this was probably an interesting configuration in the sense that as you were", "tokens": [51510, 1018, 291, 393, 536, 11, 341, 390, 1391, 364, 1880, 11694, 294, 264, 2020, 300, 382, 291, 645, 51852], "temperature": 0.0, "avg_logprob": -0.15139830842310067, "compression_ratio": 1.69140625, "no_speech_prob": 0.00953682791441679}, {"id": 124, "seek": 50066, "start": 500.66, "end": 504.54, "text": " scaling out the compute, there was actually no benefit at all in terms of the simulation", "tokens": [50364, 21589, 484, 264, 14722, 11, 456, 390, 767, 572, 5121, 412, 439, 294, 2115, 295, 264, 16575, 50558], "temperature": 0.0, "avg_logprob": -0.200767360014074, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0028911379631608725}, {"id": 125, "seek": 50066, "start": 504.54, "end": 505.54, "text": " time.", "tokens": [50558, 565, 13, 50608], "temperature": 0.0, "avg_logprob": -0.200767360014074, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0028911379631608725}, {"id": 126, "seek": 50066, "start": 505.54, "end": 514.0600000000001, "text": " Actually, interestingly, because of this slightly wackadoodle configuration, or the job was", "tokens": [50608, 5135, 11, 25873, 11, 570, 295, 341, 4748, 42138, 1573, 30013, 11694, 11, 420, 264, 1691, 390, 51034], "temperature": 0.0, "avg_logprob": -0.200767360014074, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0028911379631608725}, {"id": 127, "seek": 50066, "start": 514.0600000000001, "end": 519.46, "text": " too small essentially, you can actually see in the TCP ones above, they gradually actually", "tokens": [51034, 886, 1359, 4476, 11, 291, 393, 767, 536, 294, 264, 48965, 2306, 3673, 11, 436, 13145, 767, 51304], "temperature": 0.0, "avg_logprob": -0.200767360014074, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0028911379631608725}, {"id": 128, "seek": 50066, "start": 519.46, "end": 526.02, "text": " get worse as they've got more cross communication, as you would expect with MPI underneath here.", "tokens": [51304, 483, 5324, 382, 436, 600, 658, 544, 3278, 6101, 11, 382, 291, 576, 2066, 365, 14146, 40, 7223, 510, 13, 51632], "temperature": 0.0, "avg_logprob": -0.200767360014074, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0028911379631608725}, {"id": 129, "seek": 52602, "start": 526.02, "end": 533.14, "text": " So if we dive down into MPI, on the left-hand side we've got the latencies, and these bottom", "tokens": [50364, 407, 498, 321, 9192, 760, 666, 14146, 40, 11, 322, 264, 1411, 12, 5543, 1252, 321, 600, 658, 264, 4465, 6464, 11, 293, 613, 2767, 50720], "temperature": 0.0, "avg_logprob": -0.18125140910245935, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.03742926940321922}, {"id": 130, "seek": 52602, "start": 533.14, "end": 539.66, "text": " two latencies for people at the back of the room, there's two at about five microseconds", "tokens": [50720, 732, 4465, 6464, 337, 561, 412, 264, 646, 295, 264, 1808, 11, 456, 311, 732, 412, 466, 1732, 3123, 37841, 28750, 51046], "temperature": 0.0, "avg_logprob": -0.18125140910245935, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.03742926940321922}, {"id": 131, "seek": 52602, "start": 539.66, "end": 542.46, "text": " and one that's about half of that.", "tokens": [51046, 293, 472, 300, 311, 466, 1922, 295, 300, 13, 51186], "temperature": 0.0, "avg_logprob": -0.18125140910245935, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.03742926940321922}, {"id": 132, "seek": 52602, "start": 542.46, "end": 546.02, "text": " These are interesting, these are the RDMA ones.", "tokens": [51186, 1981, 366, 1880, 11, 613, 366, 264, 49488, 9998, 2306, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18125140910245935, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.03742926940321922}, {"id": 133, "seek": 52602, "start": 546.02, "end": 553.26, "text": " Actually, I'm saying RDMA here, these are actually all rocky using Ethernet, as you", "tokens": [51364, 5135, 11, 286, 478, 1566, 49488, 9998, 510, 11, 613, 366, 767, 439, 33301, 1228, 38636, 7129, 11, 382, 291, 51726], "temperature": 0.0, "avg_logprob": -0.18125140910245935, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.03742926940321922}, {"id": 134, "seek": 55326, "start": 553.26, "end": 557.74, "text": " probably guessed, because I just said what the latencies were, if you're interested in", "tokens": [50364, 1391, 21852, 11, 570, 286, 445, 848, 437, 264, 4465, 6464, 645, 11, 498, 291, 434, 3102, 294, 50588], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 135, "seek": 55326, "start": 557.74, "end": 559.74, "text": " that kind of thing.", "tokens": [50588, 300, 733, 295, 551, 13, 50688], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 136, "seek": 55326, "start": 559.74, "end": 570.54, "text": " So there's no such thing as a free coffee unless you're at Fosden, I guess, but let's", "tokens": [50688, 407, 456, 311, 572, 1270, 551, 382, 257, 1737, 4982, 5969, 291, 434, 412, 479, 329, 1556, 11, 286, 2041, 11, 457, 718, 311, 51228], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 137, "seek": 55326, "start": 570.54, "end": 575.1, "text": " just compare very briefly those three technologies.", "tokens": [51228, 445, 6794, 588, 10515, 729, 1045, 7943, 13, 51456], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 138, "seek": 55326, "start": 575.1, "end": 578.9, "text": " If we have a look at the bandwidth, there's something interesting happening here.", "tokens": [51456, 759, 321, 362, 257, 574, 412, 264, 23647, 11, 456, 311, 746, 1880, 2737, 510, 13, 51646], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 139, "seek": 55326, "start": 578.9, "end": 582.38, "text": " It would be slightly more interesting if we'd actually had the hardware for long enough", "tokens": [51646, 467, 576, 312, 4748, 544, 1880, 498, 321, 1116, 767, 632, 264, 8837, 337, 938, 1547, 51820], "temperature": 0.0, "avg_logprob": -0.18916962878538832, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.037206750363111496}, {"id": 140, "seek": 58238, "start": 582.5, "end": 586.82, "text": " and run the rest of the points, but you can see that the one with the lowest latency actually", "tokens": [50370, 293, 1190, 264, 1472, 295, 264, 2793, 11, 457, 291, 393, 536, 300, 264, 472, 365, 264, 12437, 27043, 767, 50586], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 141, "seek": 58238, "start": 586.82, "end": 592.74, "text": " caps out about 100 gigabits a second, and the ones with a slightly higher latency, or", "tokens": [50586, 13855, 484, 466, 2319, 8741, 455, 1208, 257, 1150, 11, 293, 264, 2306, 365, 257, 4748, 2946, 27043, 11, 420, 50882], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 142, "seek": 58238, "start": 592.74, "end": 600.18, "text": " double if you're being mean, actually go all the way up to the 200 gigabits a second, and", "tokens": [50882, 3834, 498, 291, 434, 885, 914, 11, 767, 352, 439, 264, 636, 493, 281, 264, 2331, 8741, 455, 1208, 257, 1150, 11, 293, 51254], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 143, "seek": 58238, "start": 600.18, "end": 604.46, "text": " actually there's a difference in the way in which that's been wired up, which I'll go", "tokens": [51254, 767, 456, 311, 257, 2649, 294, 264, 636, 294, 597, 300, 311, 668, 27415, 493, 11, 597, 286, 603, 352, 51468], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 144, "seek": 58238, "start": 604.46, "end": 609.06, "text": " into in a bit more detail later, but essentially one of them can use the whole bond, and one", "tokens": [51468, 666, 294, 257, 857, 544, 2607, 1780, 11, 457, 4476, 472, 295, 552, 393, 764, 264, 1379, 6086, 11, 293, 472, 51698], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 145, "seek": 58238, "start": 609.06, "end": 611.42, "text": " of them can only use one side of the bond.", "tokens": [51698, 295, 552, 393, 787, 764, 472, 1252, 295, 264, 6086, 13, 51816], "temperature": 0.0, "avg_logprob": -0.15686693117600078, "compression_ratio": 1.9031007751937985, "no_speech_prob": 0.18411464989185333}, {"id": 146, "seek": 61142, "start": 611.4599999999999, "end": 615.5799999999999, "text": " So these were on service with bonded 100 gig ethernet.", "tokens": [50366, 407, 613, 645, 322, 2643, 365, 41194, 2319, 8741, 37096, 7129, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 147, "seek": 61142, "start": 615.5799999999999, "end": 620.4599999999999, "text": " If you pay a latency penalty, you can use both sides of the bond in an interesting way.", "tokens": [50572, 759, 291, 1689, 257, 27043, 16263, 11, 291, 393, 764, 1293, 4881, 295, 264, 6086, 294, 364, 1880, 636, 13, 50816], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 148, "seek": 61142, "start": 620.4599999999999, "end": 626.4599999999999, "text": " If you want the ultimate lowest latency, you kind of have to dedicate and just use one", "tokens": [50816, 759, 291, 528, 264, 9705, 12437, 27043, 11, 291, 733, 295, 362, 281, 30718, 293, 445, 764, 472, 51116], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 149, "seek": 61142, "start": 626.4599999999999, "end": 629.4599999999999, "text": " side of the bond.", "tokens": [51116, 1252, 295, 264, 6086, 13, 51266], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 150, "seek": 61142, "start": 629.4599999999999, "end": 635.18, "text": " Anyway, so why do you make a big difference to these kind of workloads?", "tokens": [51266, 5684, 11, 370, 983, 360, 291, 652, 257, 955, 2649, 281, 613, 733, 295, 32452, 30, 51552], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 151, "seek": 61142, "start": 635.18, "end": 640.38, "text": " I'm referencing a talk here that was at KubeCon, five ways with a CNI.", "tokens": [51552, 286, 478, 40582, 257, 751, 510, 300, 390, 412, 591, 1977, 9838, 11, 1732, 2098, 365, 257, 14589, 40, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1998699006580171, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0010033159051090479}, {"id": 152, "seek": 64038, "start": 640.38, "end": 647.1, "text": " If you look at the FOSDEM session information for this talk, one of the links on there is", "tokens": [50364, 759, 291, 574, 412, 264, 479, 4367, 35, 6683, 5481, 1589, 337, 341, 751, 11, 472, 295, 264, 6123, 322, 456, 307, 50700], "temperature": 0.0, "avg_logprob": -0.1602837347215222, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0009873624658212066}, {"id": 153, "seek": 64038, "start": 647.1, "end": 650.98, "text": " to a blog that we wrote about this kind of thing, and there's a video from KubeCon you", "tokens": [50700, 281, 257, 6968, 300, 321, 4114, 466, 341, 733, 295, 551, 11, 293, 456, 311, 257, 960, 490, 591, 1977, 9838, 291, 50894], "temperature": 0.0, "avg_logprob": -0.1602837347215222, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0009873624658212066}, {"id": 154, "seek": 64038, "start": 650.98, "end": 657.74, "text": " can watch to have more detail, and this particular set of bang for bang, how's all these different", "tokens": [50894, 393, 1159, 281, 362, 544, 2607, 11, 293, 341, 1729, 992, 295, 8550, 337, 8550, 11, 577, 311, 439, 613, 819, 51232], "temperature": 0.0, "avg_logprob": -0.1602837347215222, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0009873624658212066}, {"id": 155, "seek": 64038, "start": 657.74, "end": 663.14, "text": " ways of wiring the networks.", "tokens": [51232, 2098, 295, 27520, 264, 9590, 13, 51502], "temperature": 0.0, "avg_logprob": -0.1602837347215222, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0009873624658212066}, {"id": 156, "seek": 64038, "start": 663.14, "end": 666.78, "text": " So that all sounded a bit complicated, right?", "tokens": [51502, 407, 300, 439, 17714, 257, 857, 6179, 11, 558, 30, 51684], "temperature": 0.0, "avg_logprob": -0.1602837347215222, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0009873624658212066}, {"id": 157, "seek": 66678, "start": 666.78, "end": 672.26, "text": " How do we actually stamp this out in a kind of useful way for users and get this all tied", "tokens": [50364, 1012, 360, 321, 767, 9921, 341, 484, 294, 257, 733, 295, 4420, 636, 337, 5022, 293, 483, 341, 439, 9601, 50638], "temperature": 0.0, "avg_logprob": -0.1759284817895224, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.014263286255300045}, {"id": 158, "seek": 66678, "start": 672.26, "end": 675.26, "text": " together?", "tokens": [50638, 1214, 30, 50788], "temperature": 0.0, "avg_logprob": -0.1759284817895224, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.014263286255300045}, {"id": 159, "seek": 66678, "start": 675.26, "end": 680.14, "text": " So how do we manage that operational complexity?", "tokens": [50788, 407, 577, 360, 321, 3067, 300, 16607, 14024, 30, 51032], "temperature": 0.0, "avg_logprob": -0.1759284817895224, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.014263286255300045}, {"id": 160, "seek": 66678, "start": 680.14, "end": 687.38, "text": " So the first side of this is in terms of deploying at the OpenStack layer and configuring all", "tokens": [51032, 407, 264, 700, 1252, 295, 341, 307, 294, 2115, 295, 34198, 412, 264, 7238, 4520, 501, 4583, 293, 6662, 1345, 439, 51394], "temperature": 0.0, "avg_logprob": -0.1759284817895224, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.014263286255300045}, {"id": 161, "seek": 66678, "start": 687.38, "end": 693.8199999999999, "text": " of that, we've got tools from the OpenStack community, from the Collar community in particular,", "tokens": [51394, 295, 300, 11, 321, 600, 658, 3873, 490, 264, 7238, 4520, 501, 1768, 11, 490, 264, 4586, 289, 1768, 294, 1729, 11, 51716], "temperature": 0.0, "avg_logprob": -0.1759284817895224, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.014263286255300045}, {"id": 162, "seek": 69382, "start": 693.86, "end": 699.0200000000001, "text": " Kube and Collar Ansible, and we use those with Ansible playbooks to sort of repeatedly,", "tokens": [50366, 591, 1977, 293, 4586, 289, 14590, 964, 11, 293, 321, 764, 729, 365, 14590, 964, 862, 15170, 281, 1333, 295, 18227, 11, 50624], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 163, "seek": 69382, "start": 699.0200000000001, "end": 702.4200000000001, "text": " once you've got a working configuration, make sure you do that every time.", "tokens": [50624, 1564, 291, 600, 658, 257, 1364, 11694, 11, 652, 988, 291, 360, 300, 633, 565, 13, 50794], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 164, "seek": 69382, "start": 702.4200000000001, "end": 708.5, "text": " It involves ensuring you can re-image the machines easily and make sure that you apply", "tokens": [50794, 467, 11626, 16882, 291, 393, 319, 12, 26624, 264, 8379, 3612, 293, 652, 988, 300, 291, 3079, 51098], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 165, "seek": 69382, "start": 708.5, "end": 713.82, "text": " the Ansible on there and get the same thing each time, so sort of package that up, and", "tokens": [51098, 264, 14590, 964, 322, 456, 293, 483, 264, 912, 551, 1184, 565, 11, 370, 1333, 295, 7372, 300, 493, 11, 293, 51364], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 166, "seek": 69382, "start": 713.82, "end": 719.2600000000001, "text": " that is all open for people to reuse.", "tokens": [51364, 300, 307, 439, 1269, 337, 561, 281, 26225, 13, 51636], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 167, "seek": 69382, "start": 719.2600000000001, "end": 722.86, "text": " And then the next stage is the users need to actually consume this infrastructure.", "tokens": [51636, 400, 550, 264, 958, 3233, 307, 264, 5022, 643, 281, 767, 14732, 341, 6896, 13, 51816], "temperature": 0.0, "avg_logprob": -0.24059913032933286, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.01018623635172844}, {"id": 168, "seek": 72286, "start": 722.9, "end": 729.86, "text": " So if we give people OpenStack directly, they can get very confused, because the people", "tokens": [50366, 407, 498, 321, 976, 561, 7238, 4520, 501, 3838, 11, 436, 393, 483, 588, 9019, 11, 570, 264, 561, 50714], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 169, "seek": 72286, "start": 729.86, "end": 736.42, "text": " that are trying to just create a platform are typically not experts in using cloud infrastructure.", "tokens": [50714, 300, 366, 1382, 281, 445, 1884, 257, 3663, 366, 5850, 406, 8572, 294, 1228, 4588, 6896, 13, 51042], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 170, "seek": 72286, "start": 736.42, "end": 738.5, "text": " So how do we make that easier?", "tokens": [51042, 407, 577, 360, 321, 652, 300, 3571, 30, 51146], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 171, "seek": 72286, "start": 738.5, "end": 742.0600000000001, "text": " So I want to talk about azimuth.", "tokens": [51146, 407, 286, 528, 281, 751, 466, 7883, 332, 2910, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 172, "seek": 72286, "start": 742.0600000000001, "end": 746.9, "text": " This is the project that I mentioned at the beginning coming from the Jasmine team, and", "tokens": [51324, 639, 307, 264, 1716, 300, 286, 2835, 412, 264, 2863, 1348, 490, 264, 36224, 1469, 11, 293, 51566], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 173, "seek": 72286, "start": 746.9, "end": 752.4200000000001, "text": " the idea here is for the people creating platforms, so for the platform creators, people who want", "tokens": [51566, 264, 1558, 510, 307, 337, 264, 561, 4084, 9473, 11, 370, 337, 264, 3663, 16039, 11, 561, 567, 528, 51842], "temperature": 0.0, "avg_logprob": -0.1567229491013747, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0022521011997014284}, {"id": 174, "seek": 75242, "start": 752.4599999999999, "end": 757.2199999999999, "text": " to create a Jupyter Hub or a Dask Hub or a Slurm cluster that's isolated and dedicated", "tokens": [50366, 281, 1884, 257, 22125, 88, 391, 18986, 420, 257, 2846, 74, 18986, 420, 257, 6187, 26717, 13630, 300, 311, 14621, 293, 8374, 50604], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 175, "seek": 75242, "start": 757.2199999999999, "end": 758.2199999999999, "text": " for their own needs.", "tokens": [50604, 337, 641, 1065, 2203, 13, 50654], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 176, "seek": 75242, "start": 758.2199999999999, "end": 764.06, "text": " This might be for a development use case or otherwise, or create a Kubernetes cluster.", "tokens": [50654, 639, 1062, 312, 337, 257, 3250, 764, 1389, 420, 5911, 11, 420, 1884, 257, 23145, 13630, 13, 50946], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 177, "seek": 75242, "start": 764.06, "end": 770.5, "text": " How do we just package up those good practices and make that really easy to deploy, so calling", "tokens": [50946, 1012, 360, 321, 445, 7372, 493, 729, 665, 7525, 293, 652, 300, 534, 1858, 281, 7274, 11, 370, 5141, 51268], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 178, "seek": 75242, "start": 770.5, "end": 773.5799999999999, "text": " this platform as a service?", "tokens": [51268, 341, 3663, 382, 257, 2643, 30, 51422], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 179, "seek": 75242, "start": 773.5799999999999, "end": 778.42, "text": " If you've seen me talk about this before, one of the changes here is that you get all", "tokens": [51422, 759, 291, 600, 1612, 385, 751, 466, 341, 949, 11, 472, 295, 264, 2962, 510, 307, 300, 291, 483, 439, 51664], "temperature": 0.0, "avg_logprob": -0.16783414303677754, "compression_ratio": 1.5680933852140078, "no_speech_prob": 0.003855368122458458}, {"id": 180, "seek": 77842, "start": 778.42, "end": 785.26, "text": " of the platforms in one view now, so you can log in using your OpenStack credentials.", "tokens": [50364, 295, 264, 9473, 294, 472, 1910, 586, 11, 370, 291, 393, 3565, 294, 1228, 428, 7238, 4520, 501, 27404, 13, 50706], "temperature": 0.0, "avg_logprob": -0.18460481143692165, "compression_ratio": 1.7782608695652173, "no_speech_prob": 0.014144577085971832}, {"id": 181, "seek": 77842, "start": 785.26, "end": 789.42, "text": " So there's the cloud operator, and then there's the platform operator logs into azimuth, creates", "tokens": [50706, 407, 456, 311, 264, 4588, 12973, 11, 293, 550, 456, 311, 264, 3663, 12973, 20820, 666, 7883, 332, 2910, 11, 7829, 50914], "temperature": 0.0, "avg_logprob": -0.18460481143692165, "compression_ratio": 1.7782608695652173, "no_speech_prob": 0.014144577085971832}, {"id": 182, "seek": 77842, "start": 789.42, "end": 794.9, "text": " the platform, then on top of the platform, you can choose which users can log into that,", "tokens": [50914, 264, 3663, 11, 550, 322, 1192, 295, 264, 3663, 11, 291, 393, 2826, 597, 5022, 393, 3565, 666, 300, 11, 51188], "temperature": 0.0, "avg_logprob": -0.18460481143692165, "compression_ratio": 1.7782608695652173, "no_speech_prob": 0.014144577085971832}, {"id": 183, "seek": 77842, "start": 794.9, "end": 803.26, "text": " just to make all of that much easier to do.", "tokens": [51188, 445, 281, 652, 439, 295, 300, 709, 3571, 281, 360, 13, 51606], "temperature": 0.0, "avg_logprob": -0.18460481143692165, "compression_ratio": 1.7782608695652173, "no_speech_prob": 0.014144577085971832}, {"id": 184, "seek": 77842, "start": 803.26, "end": 808.02, "text": " So I'll quickly go through the types of things that are going on here and the different types", "tokens": [51606, 407, 286, 603, 2661, 352, 807, 264, 3467, 295, 721, 300, 366, 516, 322, 510, 293, 264, 819, 3467, 51844], "temperature": 0.0, "avg_logprob": -0.18460481143692165, "compression_ratio": 1.7782608695652173, "no_speech_prob": 0.014144577085971832}, {"id": 185, "seek": 80802, "start": 808.02, "end": 810.78, "text": " of platforms.", "tokens": [50364, 295, 9473, 13, 50502], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 186, "seek": 80802, "start": 810.78, "end": 814.74, "text": " So firstly, there's Ansible-based platforms.", "tokens": [50502, 407, 27376, 11, 456, 311, 14590, 964, 12, 6032, 9473, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 187, "seek": 80802, "start": 814.74, "end": 820.5, "text": " So things like, give me a bigger laptop, which is a particular case, and so give me a Linux", "tokens": [50700, 407, 721, 411, 11, 976, 385, 257, 3801, 10732, 11, 597, 307, 257, 1729, 1389, 11, 293, 370, 976, 385, 257, 18734, 50988], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 188, "seek": 80802, "start": 820.5, "end": 826.38, "text": " workstation that I can just guacamole into, or then give me a Slurm cluster.", "tokens": [50988, 589, 19159, 300, 286, 393, 445, 695, 47190, 4812, 666, 11, 420, 550, 976, 385, 257, 6187, 26717, 13630, 13, 51282], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 189, "seek": 80802, "start": 826.38, "end": 829.18, "text": " What we do for that is they're not Kubernetes-based.", "tokens": [51282, 708, 321, 360, 337, 300, 307, 436, 434, 406, 23145, 12, 6032, 13, 51422], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 190, "seek": 80802, "start": 829.18, "end": 836.38, "text": " We use Terraform to stamp out virtual machines, and then there's Ansible, basically, Ansible's", "tokens": [51422, 492, 764, 25366, 837, 281, 9921, 484, 6374, 8379, 11, 293, 550, 456, 311, 14590, 964, 11, 1936, 11, 14590, 964, 311, 51782], "temperature": 0.0, "avg_logprob": -0.17719713846842447, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.010890109464526176}, {"id": 191, "seek": 83638, "start": 836.38, "end": 843.26, "text": " running Terraform to stamp out machines and do any final configuration that might be required.", "tokens": [50364, 2614, 25366, 837, 281, 9921, 484, 8379, 293, 360, 604, 2572, 11694, 300, 1062, 312, 4739, 13, 50708], "temperature": 0.0, "avg_logprob": -0.19080401440056002, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.028943805024027824}, {"id": 192, "seek": 83638, "start": 843.26, "end": 846.46, "text": " So when you click the button, all of that happens in the background, and it sets up", "tokens": [50708, 407, 562, 291, 2052, 264, 2960, 11, 439, 295, 300, 2314, 294, 264, 3678, 11, 293, 309, 6352, 493, 50868], "temperature": 0.0, "avg_logprob": -0.19080401440056002, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.028943805024027824}, {"id": 193, "seek": 83638, "start": 846.46, "end": 850.5, "text": " the infrastructure and you can get straight in.", "tokens": [50868, 264, 6896, 293, 291, 393, 483, 2997, 294, 13, 51070], "temperature": 0.0, "avg_logprob": -0.19080401440056002, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.028943805024027824}, {"id": 194, "seek": 83638, "start": 850.5, "end": 855.74, "text": " The other type is, give me a Kubernetes cluster, I go into this in a bit more detail in a sec,", "tokens": [51070, 440, 661, 2010, 307, 11, 976, 385, 257, 23145, 13630, 11, 286, 352, 666, 341, 294, 257, 857, 544, 2607, 294, 257, 907, 11, 51332], "temperature": 0.0, "avg_logprob": -0.19080401440056002, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.028943805024027824}, {"id": 195, "seek": 83638, "start": 855.74, "end": 861.3, "text": " but you choose your Kubernetes cluster, set that up, and it stamps that out.", "tokens": [51332, 457, 291, 2826, 428, 23145, 13630, 11, 992, 300, 493, 11, 293, 309, 30800, 300, 484, 13, 51610], "temperature": 0.0, "avg_logprob": -0.19080401440056002, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.028943805024027824}, {"id": 196, "seek": 86130, "start": 861.3, "end": 867.62, "text": " And the third type, which is relatively new now, is, well, I just want a Jupyter Hub or", "tokens": [50364, 400, 264, 2636, 2010, 11, 597, 307, 7226, 777, 586, 11, 307, 11, 731, 11, 286, 445, 528, 257, 22125, 88, 391, 18986, 420, 50680], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 197, "seek": 86130, "start": 867.62, "end": 874.42, "text": " a Dask Hub, and so for those kind of situations, we're deploying those on the Kubernetes cluster,", "tokens": [50680, 257, 2846, 74, 18986, 11, 293, 370, 337, 729, 733, 295, 6851, 11, 321, 434, 34198, 729, 322, 264, 23145, 13630, 11, 51020], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 198, "seek": 86130, "start": 874.42, "end": 876.74, "text": " so you can go through that.", "tokens": [51020, 370, 291, 393, 352, 807, 300, 13, 51136], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 199, "seek": 86130, "start": 876.74, "end": 878.62, "text": " So let's go into a bit more detail.", "tokens": [51136, 407, 718, 311, 352, 666, 257, 857, 544, 2607, 13, 51230], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 200, "seek": 86130, "start": 878.62, "end": 883.8599999999999, "text": " This is more just a bit of an eye chart, particularly because it's not rendering at all.", "tokens": [51230, 639, 307, 544, 445, 257, 857, 295, 364, 3313, 6927, 11, 4098, 570, 309, 311, 406, 22407, 412, 439, 13, 51492], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 201, "seek": 86130, "start": 883.8599999999999, "end": 887.14, "text": " The idea is you just ask some basic questions about creating a Kubernetes cluster, what", "tokens": [51492, 440, 1558, 307, 291, 445, 1029, 512, 3875, 1651, 466, 4084, 257, 23145, 13630, 11, 437, 51656], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 202, "seek": 86130, "start": 887.14, "end": 891.26, "text": " size nodes you want, what name it is.", "tokens": [51656, 2744, 13891, 291, 528, 11, 437, 1315, 309, 307, 13, 51862], "temperature": 0.0, "avg_logprob": -0.19987628173828126, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.022546950727701187}, {"id": 203, "seek": 89126, "start": 891.26, "end": 897.02, "text": " If you're creating your Kubernetes application, and you've pressed go into the Kubernetes application,", "tokens": [50364, 759, 291, 434, 4084, 428, 23145, 3861, 11, 293, 291, 600, 17355, 352, 666, 264, 23145, 3861, 11, 50652], "temperature": 0.0, "avg_logprob": -0.16214579724250955, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.002030176343396306}, {"id": 204, "seek": 89126, "start": 897.02, "end": 902.54, "text": " you give it a name and the basic constraints for the notebooks, and it's sort of pre-configured", "tokens": [50652, 291, 976, 309, 257, 1315, 293, 264, 3875, 18491, 337, 264, 43782, 11, 293, 309, 311, 1333, 295, 659, 12, 1671, 20646, 3831, 50928], "temperature": 0.0, "avg_logprob": -0.16214579724250955, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.002030176343396306}, {"id": 205, "seek": 89126, "start": 902.54, "end": 906.7, "text": " and you tell it which Kubernetes cluster to put it on, or create one if you haven't got", "tokens": [50928, 293, 291, 980, 309, 597, 23145, 13630, 281, 829, 309, 322, 11, 420, 1884, 472, 498, 291, 2378, 380, 658, 51136], "temperature": 0.0, "avg_logprob": -0.16214579724250955, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.002030176343396306}, {"id": 206, "seek": 89126, "start": 906.7, "end": 911.02, "text": " one yet.", "tokens": [51136, 472, 1939, 13, 51352], "temperature": 0.0, "avg_logprob": -0.16214579724250955, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.002030176343396306}, {"id": 207, "seek": 89126, "start": 911.02, "end": 916.02, "text": " And then finally, when you've stamped out all of these bits of infrastructure, you can", "tokens": [51352, 400, 550, 2721, 11, 562, 291, 600, 39111, 484, 439, 295, 613, 9239, 295, 6896, 11, 291, 393, 51602], "temperature": 0.0, "avg_logprob": -0.16214579724250955, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.002030176343396306}, {"id": 208, "seek": 91602, "start": 916.02, "end": 922.54, "text": " see there's a nice single sign-on to go and dig in.", "tokens": [50364, 536, 456, 311, 257, 1481, 2167, 1465, 12, 266, 281, 352, 293, 2528, 294, 13, 50690], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 209, "seek": 91602, "start": 922.54, "end": 928.06, "text": " So if you've got Dask Hub, you can click on the link to sort of open your notebook,", "tokens": [50690, 407, 498, 291, 600, 658, 2846, 74, 18986, 11, 291, 393, 2052, 322, 264, 2113, 281, 1333, 295, 1269, 428, 21060, 11, 50966], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 210, "seek": 91602, "start": 928.06, "end": 931.38, "text": " and it gets you straight in.", "tokens": [50966, 293, 309, 2170, 291, 2997, 294, 13, 51132], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 211, "seek": 91602, "start": 931.38, "end": 935.46, "text": " One of the issues we've got at the moment is that there's a cost of IPv4 addresses or", "tokens": [51132, 1485, 295, 264, 2663, 321, 600, 658, 412, 264, 1623, 307, 300, 456, 311, 257, 2063, 295, 8671, 85, 19, 16862, 420, 51336], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 212, "seek": 91602, "start": 935.46, "end": 938.74, "text": " the shortage of IPv4 addresses is a big deal.", "tokens": [51336, 264, 24708, 295, 8671, 85, 19, 16862, 307, 257, 955, 2028, 13, 51500], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 213, "seek": 91602, "start": 938.74, "end": 943.46, "text": " So we're actually using a Zenith proxy here, a tunneling proxy called Zenith.", "tokens": [51500, 407, 321, 434, 767, 1228, 257, 22387, 355, 29690, 510, 11, 257, 13186, 278, 29690, 1219, 22387, 355, 13, 51736], "temperature": 0.0, "avg_logprob": -0.21061163550024634, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.051739368587732315}, {"id": 214, "seek": 94346, "start": 944.02, "end": 950.14, "text": " So essentially when we create the infrastructure, there's an SSH session poking out, doing a", "tokens": [50392, 407, 4476, 562, 321, 1884, 264, 6896, 11, 456, 311, 364, 12238, 39, 5481, 42684, 484, 11, 884, 257, 50698], "temperature": 0.0, "avg_logprob": -0.21466047113591974, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.006159849464893341}, {"id": 215, "seek": 94346, "start": 950.14, "end": 957.0600000000001, "text": " port forward essentially, out into the proxy, and the proxy secures that it does all the", "tokens": [50698, 2436, 2128, 4476, 11, 484, 666, 264, 29690, 11, 293, 264, 29690, 907, 1303, 300, 309, 775, 439, 264, 51044], "temperature": 0.0, "avg_logprob": -0.21466047113591974, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.006159849464893341}, {"id": 216, "seek": 94346, "start": 957.0600000000001, "end": 961.6600000000001, "text": " authentication and authorization, and then punches that through.", "tokens": [51044, 26643, 293, 33697, 11, 293, 550, 34103, 300, 807, 13, 51274], "temperature": 0.0, "avg_logprob": -0.21466047113591974, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.006159849464893341}, {"id": 217, "seek": 94346, "start": 961.6600000000001, "end": 968.4200000000001, "text": " So essentially it means that these are inside, you've got a VM inside your private network,", "tokens": [51274, 407, 4476, 309, 1355, 300, 613, 366, 1854, 11, 291, 600, 658, 257, 18038, 1854, 428, 4551, 3209, 11, 51612], "temperature": 0.0, "avg_logprob": -0.21466047113591974, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.006159849464893341}, {"id": 218, "seek": 96842, "start": 968.42, "end": 973.42, "text": " and then it goes out through the NAT, not consuming floating IPs for each of these bits", "tokens": [50364, 293, 550, 309, 1709, 484, 807, 264, 14500, 11, 406, 19867, 12607, 8671, 82, 337, 1184, 295, 613, 9239, 50614], "temperature": 0.0, "avg_logprob": -0.2700994564936711, "compression_ratio": 1.548, "no_speech_prob": 0.0038346450310200453}, {"id": 219, "seek": 96842, "start": 973.42, "end": 976.14, "text": " of infrastructure that you're stamping out.", "tokens": [50614, 295, 6896, 300, 291, 434, 41792, 484, 13, 50750], "temperature": 0.0, "avg_logprob": -0.2700994564936711, "compression_ratio": 1.548, "no_speech_prob": 0.0038346450310200453}, {"id": 220, "seek": 96842, "start": 976.14, "end": 982.8199999999999, "text": " And there's lots of, I'm not going to go into too much detail on all these things.", "tokens": [50750, 400, 456, 311, 3195, 295, 11, 286, 478, 406, 516, 281, 352, 666, 886, 709, 2607, 322, 439, 613, 721, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2700994564936711, "compression_ratio": 1.548, "no_speech_prob": 0.0038346450310200453}, {"id": 221, "seek": 96842, "start": 982.8199999999999, "end": 987.2199999999999, "text": " If you create a Kubernetes cluster, it's easy to get the kubectl out.", "tokens": [51084, 759, 291, 1884, 257, 23145, 13630, 11, 309, 311, 1858, 281, 483, 264, 350, 836, 557, 75, 484, 13, 51304], "temperature": 0.0, "avg_logprob": -0.2700994564936711, "compression_ratio": 1.548, "no_speech_prob": 0.0038346450310200453}, {"id": 222, "seek": 96842, "start": 987.2199999999999, "end": 994.5, "text": " It's got monitoring included, and SLIM, similarly, it comes with monitoring open-on-demand dashboards.", "tokens": [51304, 467, 311, 658, 11028, 5556, 11, 293, 22999, 6324, 11, 14138, 11, 309, 1487, 365, 11028, 1269, 12, 266, 12, 10730, 474, 8240, 17228, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2700994564936711, "compression_ratio": 1.548, "no_speech_prob": 0.0038346450310200453}, {"id": 223, "seek": 99450, "start": 994.5, "end": 999.22, "text": " So in this case, you can get in and out through open-on-demand, although this one does require", "tokens": [50364, 407, 294, 341, 1389, 11, 291, 393, 483, 294, 293, 484, 807, 1269, 12, 266, 12, 10730, 474, 11, 4878, 341, 472, 775, 3651, 50600], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 224, "seek": 99450, "start": 999.22, "end": 1003.06, "text": " a public IP so that you can do SSH.", "tokens": [50600, 257, 1908, 8671, 370, 300, 291, 393, 360, 12238, 39, 13, 50792], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 225, "seek": 99450, "start": 1003.06, "end": 1008.74, "text": " I said about bigger desktop, so if you just want a VM, you can get into without worrying", "tokens": [50792, 286, 848, 466, 3801, 14502, 11, 370, 498, 291, 445, 528, 257, 18038, 11, 291, 393, 483, 666, 1553, 18788, 51076], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 226, "seek": 99450, "start": 1008.74, "end": 1011.7, "text": " about SSH, without having to configure all that.", "tokens": [51076, 466, 12238, 39, 11, 1553, 1419, 281, 22162, 439, 300, 13, 51224], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 227, "seek": 99450, "start": 1011.7, "end": 1016.74, "text": " You can go in through Guacamole, get a web terminal and otherwise.", "tokens": [51224, 509, 393, 352, 294, 807, 2694, 47190, 4812, 11, 483, 257, 3670, 14709, 293, 5911, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 228, "seek": 99450, "start": 1016.74, "end": 1023.42, "text": " Again, you can stamp out all of these without consuming a floating IP.", "tokens": [51476, 3764, 11, 291, 393, 9921, 484, 439, 295, 613, 1553, 19867, 257, 12607, 8671, 13, 51810], "temperature": 0.0, "avg_logprob": -0.18951304324038393, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0030383632984012365}, {"id": 229, "seek": 102342, "start": 1023.42, "end": 1027.58, "text": " Another mode, which is a bit like Binderhub, but just inside a single VM, is just you specify", "tokens": [50364, 3996, 4391, 11, 597, 307, 257, 857, 411, 363, 5669, 71, 836, 11, 457, 445, 1854, 257, 2167, 18038, 11, 307, 445, 291, 16500, 50572], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 230, "seek": 102342, "start": 1027.58, "end": 1029.54, "text": " your repo to Docker.", "tokens": [50572, 428, 49040, 281, 33772, 13, 50670], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 231, "seek": 102342, "start": 1029.54, "end": 1030.54, "text": " Same kind of idea.", "tokens": [50670, 10635, 733, 295, 1558, 13, 50720], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 232, "seek": 102342, "start": 1030.54, "end": 1034.54, "text": " It spins up the Jupyter Notebook, punches it out with Zenith, so it's all nice and simple", "tokens": [50720, 467, 31587, 493, 264, 22125, 88, 391, 11633, 2939, 11, 34103, 309, 484, 365, 22387, 355, 11, 370, 309, 311, 439, 1481, 293, 2199, 50920], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 233, "seek": 102342, "start": 1034.54, "end": 1036.34, "text": " to just get that up and running.", "tokens": [50920, 281, 445, 483, 300, 493, 293, 2614, 13, 51010], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 234, "seek": 102342, "start": 1036.34, "end": 1043.7, "text": " Okay, so let's do a little bit of a shortish technical dive into actually how do you get", "tokens": [51010, 1033, 11, 370, 718, 311, 360, 257, 707, 857, 295, 257, 2099, 742, 6191, 9192, 666, 767, 577, 360, 291, 483, 51378], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 235, "seek": 102342, "start": 1043.7, "end": 1049.06, "text": " RDMA in Loki, what the heck is Loki, you may have said.", "tokens": [51378, 49488, 9998, 294, 37940, 11, 437, 264, 12872, 307, 37940, 11, 291, 815, 362, 848, 13, 51646], "temperature": 0.0, "avg_logprob": -0.23874337332589285, "compression_ratio": 1.4962686567164178, "no_speech_prob": 0.050748080015182495}, {"id": 236, "seek": 104906, "start": 1049.74, "end": 1054.86, "text": " If you've been in some of the open-infotalks, Thierry described this quite well.", "tokens": [50398, 759, 291, 600, 668, 294, 512, 295, 264, 1269, 12, 19920, 310, 667, 82, 11, 334, 811, 627, 7619, 341, 1596, 731, 13, 50654], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 237, "seek": 104906, "start": 1054.86, "end": 1061.98, "text": " This is the idea of Linux OpenStack and Kubernetes, giving you dynamic infrastructure.", "tokens": [50654, 639, 307, 264, 1558, 295, 18734, 7238, 4520, 501, 293, 23145, 11, 2902, 291, 8546, 6896, 13, 51010], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 238, "seek": 104906, "start": 1061.98, "end": 1063.6599999999999, "text": " How do we get RDMA into this stack?", "tokens": [51010, 1012, 360, 321, 483, 49488, 9998, 666, 341, 8630, 30, 51094], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 239, "seek": 104906, "start": 1063.6599999999999, "end": 1066.1399999999999, "text": " There's three main steps.", "tokens": [51094, 821, 311, 1045, 2135, 4439, 13, 51218], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 240, "seek": 104906, "start": 1066.1399999999999, "end": 1071.22, "text": " First of all, you do need RDMA in the OpenStack servers that you're creating.", "tokens": [51218, 2386, 295, 439, 11, 291, 360, 643, 49488, 9998, 294, 264, 7238, 4520, 501, 15909, 300, 291, 434, 4084, 13, 51472], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 241, "seek": 104906, "start": 1071.22, "end": 1075.98, "text": " Second step is, if you want Kubernetes, you need the Kubernetes clusters on those OpenStack", "tokens": [51472, 5736, 1823, 307, 11, 498, 291, 528, 23145, 11, 291, 643, 264, 23145, 23313, 322, 729, 7238, 4520, 501, 51710], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 242, "seek": 104906, "start": 1075.98, "end": 1076.98, "text": " servers.", "tokens": [51710, 15909, 13, 51760], "temperature": 0.0, "avg_logprob": -0.19106247150792485, "compression_ratio": 1.6254980079681276, "no_speech_prob": 0.20640882849693298}, {"id": 243, "seek": 107698, "start": 1077.3, "end": 1083.22, "text": " The third step is you need RDMA inside the Kubernetes pods, executing within the Kubernetes", "tokens": [50380, 440, 2636, 1823, 307, 291, 643, 49488, 9998, 1854, 264, 23145, 31925, 11, 32368, 1951, 264, 23145, 50676], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 244, "seek": 107698, "start": 1083.22, "end": 1084.22, "text": " clusters.", "tokens": [50676, 23313, 13, 50726], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 245, "seek": 107698, "start": 1084.22, "end": 1086.78, "text": " So let's just drill down into each of those.", "tokens": [50726, 407, 718, 311, 445, 11392, 760, 666, 1184, 295, 729, 13, 50854], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 246, "seek": 107698, "start": 1086.78, "end": 1089.6200000000001, "text": " So how do we do RDMA inside the OpenStack servers?", "tokens": [50854, 407, 577, 360, 321, 360, 49488, 9998, 1854, 264, 7238, 4520, 501, 15909, 30, 50996], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 247, "seek": 107698, "start": 1089.6200000000001, "end": 1092.6200000000001, "text": " Well, there's two main routes here.", "tokens": [50996, 1042, 11, 456, 311, 732, 2135, 18242, 510, 13, 51146], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 248, "seek": 107698, "start": 1092.6200000000001, "end": 1102.02, "text": " The first route is if it's a bare metal server, you've got the nick there, RDMA is generally", "tokens": [51146, 440, 700, 7955, 307, 498, 309, 311, 257, 6949, 5760, 7154, 11, 291, 600, 658, 264, 15416, 456, 11, 49488, 9998, 307, 5101, 51616], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 249, "seek": 107698, "start": 1102.02, "end": 1104.78, "text": " available in the way it's normally available.", "tokens": [51616, 2435, 294, 264, 636, 309, 311, 5646, 2435, 13, 51754], "temperature": 0.0, "avg_logprob": -0.23156617419554457, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00073617062298581}, {"id": 250, "seek": 110478, "start": 1105.58, "end": 1107.78, "text": " This is not a lot special to do there.", "tokens": [50404, 639, 307, 406, 257, 688, 2121, 281, 360, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 251, "seek": 110478, "start": 1107.78, "end": 1108.94, "text": " I should stop there for a moment.", "tokens": [50514, 286, 820, 1590, 456, 337, 257, 1623, 13, 50572], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 252, "seek": 110478, "start": 1108.94, "end": 1112.8999999999999, "text": " What I've said is you're using the standard OpenStack APIs and all the Terraform tooling", "tokens": [50572, 708, 286, 600, 848, 307, 291, 434, 1228, 264, 3832, 7238, 4520, 501, 21445, 293, 439, 264, 25366, 837, 46593, 50770], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 253, "seek": 110478, "start": 1112.8999999999999, "end": 1114.74, "text": " and you're stamping out bare metal machines.", "tokens": [50770, 293, 291, 434, 41792, 484, 6949, 5760, 8379, 13, 50862], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 254, "seek": 110478, "start": 1114.74, "end": 1116.06, "text": " That's totally possible.", "tokens": [50862, 663, 311, 3879, 1944, 13, 50928], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 255, "seek": 110478, "start": 1116.06, "end": 1123.06, "text": " When you select the flavor dropdown, it might be give me a box with an 8A100 on it.", "tokens": [50928, 1133, 291, 3048, 264, 6813, 47599, 11, 309, 1062, 312, 976, 385, 257, 2424, 365, 364, 1649, 32, 6879, 322, 309, 13, 51278], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 256, "seek": 110478, "start": 1123.06, "end": 1124.3799999999999, "text": " I want the whole thing.", "tokens": [51278, 286, 528, 264, 1379, 551, 13, 51344], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 257, "seek": 110478, "start": 1124.3799999999999, "end": 1126.3799999999999, "text": " That's perfectly possible.", "tokens": [51344, 663, 311, 6239, 1944, 13, 51444], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 258, "seek": 110478, "start": 1126.3799999999999, "end": 1130.8999999999999, "text": " So I referenced Cambridge as helping us out with this.", "tokens": [51444, 407, 286, 32734, 24876, 382, 4315, 505, 484, 365, 341, 13, 51670], "temperature": 0.0, "avg_logprob": -0.19052770220000168, "compression_ratio": 1.5535055350553506, "no_speech_prob": 0.011983294039964676}, {"id": 259, "seek": 113090, "start": 1131.02, "end": 1136.6200000000001, "text": " Cambridge's HPC clusters are actually deployed on OpenStack using the bare metal orchestration.", "tokens": [50370, 24876, 311, 12557, 34, 23313, 366, 767, 17826, 322, 7238, 4520, 501, 1228, 264, 6949, 5760, 14161, 2405, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 260, "seek": 113090, "start": 1136.6200000000001, "end": 1143.0600000000002, "text": " So it doesn't get in the way of anything in terms of RDMA or InfiniBand or whatever.", "tokens": [50650, 407, 309, 1177, 380, 483, 294, 264, 636, 295, 1340, 294, 2115, 295, 49488, 9998, 420, 11537, 3812, 33, 474, 420, 2035, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 261, "seek": 113090, "start": 1143.0600000000002, "end": 1145.98, "text": " You get the bare metal machine.", "tokens": [50972, 509, 483, 264, 6949, 5760, 3479, 13, 51118], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 262, "seek": 113090, "start": 1145.98, "end": 1149.3000000000002, "text": " On the VM side, it's a little bit more complicated.", "tokens": [51118, 1282, 264, 18038, 1252, 11, 309, 311, 257, 707, 857, 544, 6179, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 263, "seek": 113090, "start": 1149.3000000000002, "end": 1153.98, "text": " Essentially, the easiest way to get RDMA working in there is that we pass in an actual nick", "tokens": [51284, 23596, 11, 264, 12889, 636, 281, 483, 49488, 9998, 1364, 294, 456, 307, 300, 321, 1320, 294, 364, 3539, 15416, 51518], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 264, "seek": 113090, "start": 1153.98, "end": 1157.3000000000002, "text": " using PCI Passu, the SROV.", "tokens": [51518, 1228, 6465, 40, 10319, 84, 11, 264, 318, 7142, 53, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2409946584255896, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.010030467994511127}, {"id": 265, "seek": 115730, "start": 1157.3, "end": 1163.34, "text": " So the VM itself has to have drivers appropriate for the nick that you've passed through.", "tokens": [50364, 407, 264, 18038, 2564, 575, 281, 362, 11590, 6854, 337, 264, 15416, 300, 291, 600, 4678, 807, 13, 50666], "temperature": 0.0, "avg_logprob": -0.21478470988657283, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.007197713013738394}, {"id": 266, "seek": 115730, "start": 1163.34, "end": 1167.74, "text": " Now there's a whole bunch of different strategies for doing that, but I wanted to quickly go", "tokens": [50666, 823, 456, 311, 257, 1379, 3840, 295, 819, 9029, 337, 884, 300, 11, 457, 286, 1415, 281, 2661, 352, 50886], "temperature": 0.0, "avg_logprob": -0.21478470988657283, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.007197713013738394}, {"id": 267, "seek": 115730, "start": 1167.74, "end": 1175.18, "text": " through this one, which is using specifically in some MeloLux cards, and there are other", "tokens": [50886, 807, 341, 472, 11, 597, 307, 1228, 4682, 294, 512, 7375, 78, 43, 2449, 5632, 11, 293, 456, 366, 661, 51258], "temperature": 0.0, "avg_logprob": -0.21478470988657283, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.007197713013738394}, {"id": 268, "seek": 115730, "start": 1175.18, "end": 1176.7, "text": " ways of doing this.", "tokens": [51258, 2098, 295, 884, 341, 13, 51334], "temperature": 0.0, "avg_logprob": -0.21478470988657283, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.007197713013738394}, {"id": 269, "seek": 115730, "start": 1176.7, "end": 1181.1, "text": " Essentially you do OVS offload onto your virtual function.", "tokens": [51334, 23596, 291, 360, 422, 53, 50, 766, 2907, 3911, 428, 6374, 2445, 13, 51554], "temperature": 0.0, "avg_logprob": -0.21478470988657283, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.007197713013738394}, {"id": 270, "seek": 118110, "start": 1181.1, "end": 1189.6599999999999, "text": " So if you do SROV into the VM, that virtual function can actually get attached into OVS.", "tokens": [50364, 407, 498, 291, 360, 318, 7142, 53, 666, 264, 18038, 11, 300, 6374, 2445, 393, 767, 483, 8570, 666, 422, 53, 50, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14107327820152366, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0024437003303319216}, {"id": 271, "seek": 118110, "start": 1189.6599999999999, "end": 1193.26, "text": " Now that sounds insane because that's a really slow path and you just put a nice fast thing", "tokens": [50792, 823, 300, 3263, 10838, 570, 300, 311, 257, 534, 2964, 3100, 293, 291, 445, 829, 257, 1481, 2370, 551, 50972], "temperature": 0.0, "avg_logprob": -0.14107327820152366, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0024437003303319216}, {"id": 272, "seek": 118110, "start": 1193.26, "end": 1194.8999999999999, "text": " into a slow path.", "tokens": [50972, 666, 257, 2964, 3100, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14107327820152366, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0024437003303319216}, {"id": 273, "seek": 118110, "start": 1194.8999999999999, "end": 1202.1399999999999, "text": " What happens is OVS gets told that actually you look for hardware offloaded flows.", "tokens": [51054, 708, 2314, 307, 422, 53, 50, 2170, 1907, 300, 767, 291, 574, 337, 8837, 766, 2907, 292, 12867, 13, 51416], "temperature": 0.0, "avg_logprob": -0.14107327820152366, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0024437003303319216}, {"id": 274, "seek": 118110, "start": 1202.1399999999999, "end": 1206.4199999999998, "text": " So when you actually start getting connections going into your different machines, it notices", "tokens": [51416, 407, 562, 291, 767, 722, 1242, 9271, 516, 666, 428, 819, 8379, 11, 309, 32978, 51630], "temperature": 0.0, "avg_logprob": -0.14107327820152366, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0024437003303319216}, {"id": 275, "seek": 120642, "start": 1206.42, "end": 1215.54, "text": " the MAC and IP address pairs and those flows in OVS get put into the hardware and then", "tokens": [50364, 264, 27716, 293, 8671, 2985, 15494, 293, 729, 12867, 294, 422, 53, 50, 483, 829, 666, 264, 8837, 293, 550, 50820], "temperature": 0.0, "avg_logprob": -0.17237731085883246, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.03938823938369751}, {"id": 276, "seek": 120642, "start": 1215.54, "end": 1217.6200000000001, "text": " it goes onto a fast path.", "tokens": [50820, 309, 1709, 3911, 257, 2370, 3100, 13, 50924], "temperature": 0.0, "avg_logprob": -0.17237731085883246, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.03938823938369751}, {"id": 277, "seek": 120642, "start": 1217.6200000000001, "end": 1224.94, "text": " The other part of this is that you connect the OVS directly to your bond on the host", "tokens": [50924, 440, 661, 644, 295, 341, 307, 300, 291, 1745, 264, 422, 53, 50, 3838, 281, 428, 6086, 322, 264, 3975, 51290], "temperature": 0.0, "avg_logprob": -0.17237731085883246, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.03938823938369751}, {"id": 278, "seek": 120642, "start": 1224.94, "end": 1229.02, "text": " and the VFs are actually getting connected to the bond.", "tokens": [51290, 293, 264, 691, 37, 82, 366, 767, 1242, 4582, 281, 264, 6086, 13, 51494], "temperature": 0.0, "avg_logprob": -0.17237731085883246, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.03938823938369751}, {"id": 279, "seek": 120642, "start": 1229.02, "end": 1232.26, "text": " So in that earlier graph where I was showing 200 gigabits a second and basically getting", "tokens": [51494, 407, 294, 300, 3071, 4295, 689, 286, 390, 4099, 2331, 8741, 455, 1208, 257, 1150, 293, 1936, 1242, 51656], "temperature": 0.0, "avg_logprob": -0.17237731085883246, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.03938823938369751}, {"id": 280, "seek": 123226, "start": 1232.26, "end": 1238.1, "text": " line rate, that's using this setup where essentially your VM with its virtual function", "tokens": [50364, 1622, 3314, 11, 300, 311, 1228, 341, 8657, 689, 4476, 428, 18038, 365, 1080, 6374, 2445, 50656], "temperature": 0.0, "avg_logprob": -0.15418356022936233, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.25117841362953186}, {"id": 281, "seek": 123226, "start": 1238.1, "end": 1242.58, "text": " is going through the bond rather than through one of the individual interfaces.", "tokens": [50656, 307, 516, 807, 264, 6086, 2831, 813, 807, 472, 295, 264, 2609, 28416, 13, 50880], "temperature": 0.0, "avg_logprob": -0.15418356022936233, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.25117841362953186}, {"id": 282, "seek": 123226, "start": 1242.58, "end": 1248.3799999999999, "text": " And this is actually quite a nice setup in terms of wiring.", "tokens": [50880, 400, 341, 307, 767, 1596, 257, 1481, 8657, 294, 2115, 295, 27520, 13, 51170], "temperature": 0.0, "avg_logprob": -0.15418356022936233, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.25117841362953186}, {"id": 283, "seek": 123226, "start": 1248.3799999999999, "end": 1253.18, "text": " So if you've got a server that's got dual 100 gig ethernet going in or dual 25 gig", "tokens": [51170, 407, 498, 291, 600, 658, 257, 7154, 300, 311, 658, 11848, 2319, 8741, 37096, 7129, 516, 294, 420, 11848, 3552, 8741, 51410], "temperature": 0.0, "avg_logprob": -0.15418356022936233, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.25117841362953186}, {"id": 284, "seek": 123226, "start": 1253.18, "end": 1257.9, "text": " ethernet, you don't have to dedicate one of those ports to SROV.", "tokens": [51410, 37096, 7129, 11, 291, 500, 380, 362, 281, 30718, 472, 295, 729, 18160, 281, 318, 7142, 53, 13, 51646], "temperature": 0.0, "avg_logprob": -0.15418356022936233, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.25117841362953186}, {"id": 285, "seek": 125790, "start": 1257.9, "end": 1261.6200000000001, "text": " You have the host bond on there and you can connect the virtual functions into the host", "tokens": [50364, 509, 362, 264, 3975, 6086, 322, 456, 293, 291, 393, 1745, 264, 6374, 6828, 666, 264, 3975, 50550], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 286, "seek": 125790, "start": 1261.6200000000001, "end": 1265.02, "text": " bond.", "tokens": [50550, 6086, 13, 50720], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 287, "seek": 125790, "start": 1265.02, "end": 1270.8200000000002, "text": " Okay, so the next bit, create Kubernetes.", "tokens": [50720, 1033, 11, 370, 264, 958, 857, 11, 1884, 23145, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 288, "seek": 125790, "start": 1270.8200000000002, "end": 1274.22, "text": " I'm not going to go into that too much detail.", "tokens": [51010, 286, 478, 406, 516, 281, 352, 666, 300, 886, 709, 2607, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 289, "seek": 125790, "start": 1274.22, "end": 1277.6200000000001, "text": " Essentially we're using Cluster API.", "tokens": [51180, 23596, 321, 434, 1228, 2033, 8393, 9362, 13, 51350], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 290, "seek": 125790, "start": 1277.6200000000001, "end": 1284.3000000000002, "text": " I really like its logo because it uses basically you create a management cluster.", "tokens": [51350, 286, 534, 411, 1080, 9699, 570, 309, 4960, 1936, 291, 1884, 257, 4592, 13630, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2667348568256085, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.1315149962902069}, {"id": 291, "seek": 128430, "start": 1284.3, "end": 1290.5, "text": " In CRDs you describe what you want your HA cluster to be or your other cluster to be", "tokens": [50364, 682, 14123, 35, 82, 291, 6786, 437, 291, 528, 428, 11979, 13630, 281, 312, 420, 428, 661, 13630, 281, 312, 50674], "temperature": 0.0, "avg_logprob": -0.17456043992087106, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.01543539296835661}, {"id": 292, "seek": 128430, "start": 1290.5, "end": 1293.58, "text": " and it stamps that out for using an operator.", "tokens": [50674, 293, 309, 30800, 300, 484, 337, 1228, 364, 12973, 13, 50828], "temperature": 0.0, "avg_logprob": -0.17456043992087106, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.01543539296835661}, {"id": 293, "seek": 128430, "start": 1293.58, "end": 1299.3, "text": " This has proved to be really quite a stable way and reliable way of creating Kubernetes.", "tokens": [50828, 639, 575, 14617, 281, 312, 534, 1596, 257, 8351, 636, 293, 12924, 636, 295, 4084, 23145, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17456043992087106, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.01543539296835661}, {"id": 294, "seek": 128430, "start": 1299.3, "end": 1306.78, "text": " One part of this is that we're actually hoping to try and, well, while I'm in the room, I'm", "tokens": [51114, 1485, 644, 295, 341, 307, 300, 321, 434, 767, 7159, 281, 853, 293, 11, 731, 11, 1339, 286, 478, 294, 264, 1808, 11, 286, 478, 51488], "temperature": 0.0, "avg_logprob": -0.17456043992087106, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.01543539296835661}, {"id": 295, "seek": 128430, "start": 1306.78, "end": 1312.3, "text": " trying to fix the unit test on it, but we're developing a Magnum driver for OpenStack Magnum", "tokens": [51488, 1382, 281, 3191, 264, 4985, 1500, 322, 309, 11, 457, 321, 434, 6416, 257, 19664, 449, 6787, 337, 7238, 4520, 501, 19664, 449, 51764], "temperature": 0.0, "avg_logprob": -0.17456043992087106, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.01543539296835661}, {"id": 296, "seek": 131230, "start": 1312.3, "end": 1316.5, "text": " to actually consume Cluster API and just stamp them out.", "tokens": [50364, 281, 767, 14732, 2033, 8393, 9362, 293, 445, 9921, 552, 484, 13, 50574], "temperature": 0.0, "avg_logprob": -0.16555344065030417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.013256442733108997}, {"id": 297, "seek": 131230, "start": 1316.5, "end": 1323.98, "text": " To make this repeatable, it's all been packaged up in Helm charts, which are here.", "tokens": [50574, 1407, 652, 341, 7149, 712, 11, 309, 311, 439, 668, 38162, 493, 294, 6128, 76, 17767, 11, 597, 366, 510, 13, 50948], "temperature": 0.0, "avg_logprob": -0.16555344065030417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.013256442733108997}, {"id": 298, "seek": 131230, "start": 1323.98, "end": 1330.6599999999999, "text": " So now we've got OpenStack machines that have got RDMA in, we can do that.", "tokens": [50948, 407, 586, 321, 600, 658, 7238, 4520, 501, 8379, 300, 362, 658, 49488, 9998, 294, 11, 321, 393, 360, 300, 13, 51282], "temperature": 0.0, "avg_logprob": -0.16555344065030417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.013256442733108997}, {"id": 299, "seek": 131230, "start": 1330.6599999999999, "end": 1335.1, "text": " We've set up a Kubernetes cluster that's using those OpenStack machines that have the virtual", "tokens": [51282, 492, 600, 992, 493, 257, 23145, 13630, 300, 311, 1228, 729, 7238, 4520, 501, 8379, 300, 362, 264, 6374, 51504], "temperature": 0.0, "avg_logprob": -0.16555344065030417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.013256442733108997}, {"id": 300, "seek": 131230, "start": 1335.1, "end": 1338.1, "text": " function in that's doing RDMA at line rate.", "tokens": [51504, 2445, 294, 300, 311, 884, 49488, 9998, 412, 1622, 3314, 13, 51654], "temperature": 0.0, "avg_logprob": -0.16555344065030417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.013256442733108997}, {"id": 301, "seek": 133810, "start": 1338.1, "end": 1344.98, "text": " Now how on earth do we get the Kubernetes pods to actually make use of RDMA?", "tokens": [50364, 823, 577, 322, 4120, 360, 321, 483, 264, 23145, 31925, 281, 767, 652, 764, 295, 49488, 9998, 30, 50708], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 302, "seek": 133810, "start": 1344.98, "end": 1350.86, "text": " Now if this was a bare metal machine, there's actually quite a lot of standard patterns.", "tokens": [50708, 823, 498, 341, 390, 257, 6949, 5760, 3479, 11, 456, 311, 767, 1596, 257, 688, 295, 3832, 8294, 13, 51002], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 303, "seek": 133810, "start": 1350.86, "end": 1354.06, "text": " It seemed to be quite well documented in terms of actually using virtual functions into the", "tokens": [51002, 467, 6576, 281, 312, 1596, 731, 23007, 294, 2115, 295, 767, 1228, 6374, 6828, 666, 264, 51162], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 304, "seek": 133810, "start": 1354.06, "end": 1355.06, "text": " pod.", "tokens": [51162, 2497, 13, 51212], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 305, "seek": 133810, "start": 1355.06, "end": 1360.3, "text": " If we're inside a VM, we've already done the PF to VF translation, so you can't go again.", "tokens": [51212, 759, 321, 434, 1854, 257, 18038, 11, 321, 600, 1217, 1096, 264, 43402, 281, 691, 37, 12853, 11, 370, 291, 393, 380, 352, 797, 13, 51474], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 306, "seek": 133810, "start": 1360.3, "end": 1366.4599999999998, "text": " You can't have a VVF yet, although VDPA and other things might change this.", "tokens": [51474, 509, 393, 380, 362, 257, 691, 53, 37, 1939, 11, 4878, 691, 11373, 32, 293, 661, 721, 1062, 1319, 341, 13, 51782], "temperature": 0.0, "avg_logprob": -0.20137234563412873, "compression_ratio": 1.539568345323741, "no_speech_prob": 0.0049104043282568455}, {"id": 307, "seek": 136646, "start": 1366.46, "end": 1372.02, "text": " So what we're actually doing is we're using Maltis and something called the Mac VLAN CNI.", "tokens": [50364, 407, 437, 321, 434, 767, 884, 307, 321, 434, 1228, 376, 3198, 271, 293, 746, 1219, 264, 5707, 691, 36527, 14589, 40, 13, 50642], "temperature": 0.0, "avg_logprob": -0.19781094152950546, "compression_ratio": 1.603921568627451, "no_speech_prob": 0.024192960932850838}, {"id": 308, "seek": 136646, "start": 1372.02, "end": 1376.78, "text": " So essentially when you create your Kubernetes pod, you give it two interfaces, your regular", "tokens": [50642, 407, 4476, 562, 291, 1884, 428, 23145, 2497, 11, 291, 976, 309, 732, 28416, 11, 428, 3890, 50880], "temperature": 0.0, "avg_logprob": -0.19781094152950546, "compression_ratio": 1.603921568627451, "no_speech_prob": 0.024192960932850838}, {"id": 309, "seek": 136646, "start": 1376.78, "end": 1384.3400000000001, "text": " CNI interface, so that has all the usual smarts, and you give it an additional Mac IP address", "tokens": [50880, 14589, 40, 9226, 11, 370, 300, 575, 439, 264, 7713, 4069, 82, 11, 293, 291, 976, 309, 364, 4497, 5707, 8671, 2985, 51258], "temperature": 0.0, "avg_logprob": -0.19781094152950546, "compression_ratio": 1.603921568627451, "no_speech_prob": 0.024192960932850838}, {"id": 310, "seek": 136646, "start": 1384.3400000000001, "end": 1388.02, "text": " pair on your virtual function for the VM.", "tokens": [51258, 6119, 322, 428, 6374, 2445, 337, 264, 18038, 13, 51442], "temperature": 0.0, "avg_logprob": -0.19781094152950546, "compression_ratio": 1.603921568627451, "no_speech_prob": 0.024192960932850838}, {"id": 311, "seek": 136646, "start": 1388.02, "end": 1392.8600000000001, "text": " Now at the moment, you have to turn off port security to ensure that those extra Macs that", "tokens": [51442, 823, 412, 264, 1623, 11, 291, 362, 281, 1261, 766, 2436, 3825, 281, 5586, 300, 729, 2857, 5707, 82, 300, 51684], "temperature": 0.0, "avg_logprob": -0.19781094152950546, "compression_ratio": 1.603921568627451, "no_speech_prob": 0.024192960932850838}, {"id": 312, "seek": 139286, "start": 1392.86, "end": 1398.74, "text": " are auto-generated inside Kubernetes are punching out correctly and not restricted by the virtual", "tokens": [50364, 366, 8399, 12, 21848, 770, 1854, 23145, 366, 34866, 484, 8944, 293, 406, 20608, 538, 264, 6374, 50658], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 313, "seek": 139286, "start": 1398.74, "end": 1399.74, "text": " function.", "tokens": [50658, 2445, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 314, "seek": 139286, "start": 1399.74, "end": 1403.78, "text": " There's a plan to try and orchestrate that, so you can use allowed address pairs to explicitly", "tokens": [50708, 821, 311, 257, 1393, 281, 853, 293, 14161, 4404, 300, 11, 370, 291, 393, 764, 4350, 2985, 15494, 281, 20803, 50910], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 315, "seek": 139286, "start": 1403.78, "end": 1405.78, "text": " decide which ones.", "tokens": [50910, 4536, 597, 2306, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 316, "seek": 139286, "start": 1405.78, "end": 1411.1, "text": " But that's basically, so you use Maltis to say, give me two network connections, and", "tokens": [51010, 583, 300, 311, 1936, 11, 370, 291, 764, 376, 3198, 271, 281, 584, 11, 976, 385, 732, 3209, 9271, 11, 293, 51276], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 317, "seek": 139286, "start": 1411.1, "end": 1417.6599999999999, "text": " you use Mac VLAN to get that connection to your RDMA.", "tokens": [51276, 291, 764, 5707, 691, 36527, 281, 483, 300, 4984, 281, 428, 49488, 9998, 13, 51604], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 318, "seek": 139286, "start": 1417.6599999999999, "end": 1422.26, "text": " And there's also some permission stuff, which is actually quite a simple decorator on the", "tokens": [51604, 400, 456, 311, 611, 512, 11226, 1507, 11, 597, 307, 767, 1596, 257, 2199, 7919, 1639, 322, 264, 51834], "temperature": 0.0, "avg_logprob": -0.2117290667125157, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.04703957214951515}, {"id": 319, "seek": 142226, "start": 1422.26, "end": 1423.26, "text": " pod.", "tokens": [50364, 2497, 13, 50414], "temperature": 0.0, "avg_logprob": -0.29259047637114655, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.018837347626686096}, {"id": 320, "seek": 142226, "start": 1423.26, "end": 1431.22, "text": " But essentially, extra pod YAML to opt in to actually how to get this all wired together.", "tokens": [50414, 583, 4476, 11, 2857, 2497, 398, 2865, 43, 281, 2427, 294, 281, 767, 577, 281, 483, 341, 439, 27415, 1214, 13, 50812], "temperature": 0.0, "avg_logprob": -0.29259047637114655, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.018837347626686096}, {"id": 321, "seek": 142226, "start": 1431.22, "end": 1438.54, "text": " Okay, so it'd be really great if people have these problems, and this is interesting, to", "tokens": [50812, 1033, 11, 370, 309, 1116, 312, 534, 869, 498, 561, 362, 613, 2740, 11, 293, 341, 307, 1880, 11, 281, 51178], "temperature": 0.0, "avg_logprob": -0.29259047637114655, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.018837347626686096}, {"id": 322, "seek": 142226, "start": 1438.54, "end": 1439.54, "text": " get involved.", "tokens": [51178, 483, 3288, 13, 51228], "temperature": 0.0, "avg_logprob": -0.29259047637114655, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.018837347626686096}, {"id": 323, "seek": 142226, "start": 1439.54, "end": 1446.22, "text": " There's a whole load of links, but yeah, thank you very much.", "tokens": [51228, 821, 311, 257, 1379, 3677, 295, 6123, 11, 457, 1338, 11, 1309, 291, 588, 709, 13, 51562], "temperature": 0.0, "avg_logprob": -0.29259047637114655, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.018837347626686096}, {"id": 324, "seek": 144622, "start": 1446.22, "end": 1461.98, "text": " And before you've got time for half a question.", "tokens": [50364, 400, 949, 291, 600, 658, 565, 337, 1922, 257, 1168, 13, 51152], "temperature": 0.0, "avg_logprob": -0.41688788170908014, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.3252609074115753}, {"id": 325, "seek": 144622, "start": 1461.98, "end": 1471.98, "text": " Yeah, you mentioned, I thought you mentioned that we are doing the bond on the network", "tokens": [51152, 865, 11, 291, 2835, 11, 286, 1194, 291, 2835, 300, 321, 366, 884, 264, 6086, 322, 264, 3209, 51652], "temperature": 0.0, "avg_logprob": -0.41688788170908014, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.3252609074115753}, {"id": 326, "seek": 144622, "start": 1471.98, "end": 1472.98, "text": " interface.", "tokens": [51652, 9226, 13, 51702], "temperature": 0.0, "avg_logprob": -0.41688788170908014, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.3252609074115753}, {"id": 327, "seek": 144622, "start": 1472.98, "end": 1475.54, "text": " You're getting the full bandwidth of the bond.", "tokens": [51702, 509, 434, 1242, 264, 1577, 23647, 295, 264, 6086, 13, 51830], "temperature": 0.0, "avg_logprob": -0.41688788170908014, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.3252609074115753}, {"id": 328, "seek": 147554, "start": 1475.54, "end": 1480.34, "text": " So whenever I do LICP bonding, any particular connection, I only get half the interface.", "tokens": [50364, 407, 5699, 286, 360, 441, 2532, 47, 28824, 11, 604, 1729, 4984, 11, 286, 787, 483, 1922, 264, 9226, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 329, "seek": 147554, "start": 1480.34, "end": 1482.58, "text": " So I'm just wondering how you're doing that.", "tokens": [50604, 407, 286, 478, 445, 6359, 577, 291, 434, 884, 300, 13, 50716], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 330, "seek": 147554, "start": 1482.58, "end": 1484.06, "text": " It depends on your bonding mode.", "tokens": [50716, 467, 5946, 322, 428, 28824, 4391, 13, 50790], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 331, "seek": 147554, "start": 1484.06, "end": 1487.3, "text": " But yeah, so with LICP bonding, I only say make it half.", "tokens": [50790, 583, 1338, 11, 370, 365, 441, 2532, 47, 28824, 11, 286, 787, 584, 652, 309, 1922, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 332, "seek": 147554, "start": 1487.3, "end": 1491.58, "text": " Well, no, so there's a hashing mode on your bond.", "tokens": [50952, 1042, 11, 572, 11, 370, 456, 311, 257, 575, 571, 4391, 322, 428, 6086, 13, 51166], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 333, "seek": 147554, "start": 1491.58, "end": 1497.22, "text": " So what you need to make sure is that you do something like L3 plus L4 hashing, so that", "tokens": [51166, 407, 437, 291, 643, 281, 652, 988, 307, 300, 291, 360, 746, 411, 441, 18, 1804, 441, 19, 575, 571, 11, 370, 300, 51448], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 334, "seek": 147554, "start": 1497.22, "end": 1502.46, "text": " from a single client, it depends, it's basically, each of your traffic flows gets hashed onto", "tokens": [51448, 490, 257, 2167, 6423, 11, 309, 5946, 11, 309, 311, 1936, 11, 1184, 295, 428, 6419, 12867, 2170, 22019, 292, 3911, 51710], "temperature": 0.0, "avg_logprob": -0.2641668466421274, "compression_ratio": 1.6727941176470589, "no_speech_prob": 0.4640885293483734}, {"id": 335, "seek": 150246, "start": 1502.46, "end": 1506.14, "text": " a different bit of the bond.", "tokens": [50364, 257, 819, 857, 295, 264, 6086, 13, 50548], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 336, "seek": 150246, "start": 1506.14, "end": 1510.54, "text": " So you need drivers that are respecting that hashing function.", "tokens": [50548, 407, 291, 643, 11590, 300, 366, 41968, 300, 575, 571, 2445, 13, 50768], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 337, "seek": 150246, "start": 1510.54, "end": 1516.54, "text": " But yeah, if you get enough different flows, then it will actually hash across the bond,", "tokens": [50768, 583, 1338, 11, 498, 291, 483, 1547, 819, 12867, 11, 550, 309, 486, 767, 22019, 2108, 264, 6086, 11, 51068], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 338, "seek": 150246, "start": 1516.54, "end": 1517.54, "text": " okay.", "tokens": [51068, 1392, 13, 51118], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 339, "seek": 150246, "start": 1517.54, "end": 1519.42, "text": " It's all about the hashing modes.", "tokens": [51118, 467, 311, 439, 466, 264, 575, 571, 14068, 13, 51212], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 340, "seek": 150246, "start": 1519.42, "end": 1523.6200000000001, "text": " Not all switches support all hashing modes, which is the gotcha in that.", "tokens": [51212, 1726, 439, 19458, 1406, 439, 575, 571, 14068, 11, 597, 307, 264, 658, 4413, 294, 300, 13, 51422], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 341, "seek": 150246, "start": 1523.6200000000001, "end": 1527.6200000000001, "text": " Yeah, the other question I have is, I don't understand the connection between MAC VLAN", "tokens": [51422, 865, 11, 264, 661, 1168, 286, 362, 307, 11, 286, 500, 380, 1223, 264, 4984, 1296, 27716, 691, 36527, 51622], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 342, "seek": 150246, "start": 1527.6200000000001, "end": 1528.6200000000001, "text": " and RDNA.", "tokens": [51622, 293, 49488, 5321, 13, 51672], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 343, "seek": 150246, "start": 1528.6200000000001, "end": 1529.6200000000001, "text": " Sorry, what's that?", "tokens": [51672, 4919, 11, 437, 311, 300, 30, 51722], "temperature": 0.0, "avg_logprob": -0.27385180929432745, "compression_ratio": 1.595330739299611, "no_speech_prob": 0.4815662205219269}, {"id": 344, "seek": 152962, "start": 1529.62, "end": 1532.3, "text": " The connection between MAC VLAN and RDNA.", "tokens": [50364, 440, 4984, 1296, 27716, 691, 36527, 293, 49488, 5321, 13, 50498], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 345, "seek": 152962, "start": 1532.3, "end": 1534.3, "text": " The connection between the MAC VLAN and RDNA.", "tokens": [50498, 440, 4984, 1296, 264, 27716, 691, 36527, 293, 49488, 5321, 13, 50598], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 346, "seek": 152962, "start": 1534.3, "end": 1539.3799999999999, "text": " Yeah, why do you need MAC VLAN to do the RDNA into your VMs?", "tokens": [50598, 865, 11, 983, 360, 291, 643, 27716, 691, 36527, 281, 360, 264, 49488, 5321, 666, 428, 18038, 82, 30, 50852], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 347, "seek": 152962, "start": 1539.3799999999999, "end": 1541.8999999999999, "text": " So you could just do host networking.", "tokens": [50852, 407, 291, 727, 445, 360, 3975, 17985, 13, 50978], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 348, "seek": 152962, "start": 1541.8999999999999, "end": 1545.9799999999998, "text": " So if you did host networking on the pod, you would just have access to all of those host", "tokens": [50978, 407, 498, 291, 630, 3975, 17985, 322, 264, 2497, 11, 291, 576, 445, 362, 2105, 281, 439, 295, 729, 3975, 51182], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 349, "seek": 152962, "start": 1545.9799999999998, "end": 1546.9799999999998, "text": " interfaces.", "tokens": [51182, 28416, 13, 51232], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 350, "seek": 152962, "start": 1546.9799999999998, "end": 1551.1799999999998, "text": " But if you want to have multiple different RDNA flows with different MAC and IP address", "tokens": [51232, 583, 498, 291, 528, 281, 362, 3866, 819, 49488, 5321, 12867, 365, 819, 27716, 293, 8671, 2985, 51442], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 351, "seek": 152962, "start": 1551.1799999999998, "end": 1556.58, "text": " pairs, then the MAC VLAN allows you to have those multiple pods, each with their own identity", "tokens": [51442, 15494, 11, 550, 264, 27716, 691, 36527, 4045, 291, 281, 362, 729, 3866, 31925, 11, 1184, 365, 641, 1065, 6575, 51712], "temperature": 0.0, "avg_logprob": -0.19468194580078124, "compression_ratio": 1.88, "no_speech_prob": 0.08031351864337921}, {"id": 352, "seek": 155658, "start": 1556.58, "end": 1560.54, "text": " on your VLAN that's doing RDNA.", "tokens": [50364, 322, 428, 691, 36527, 300, 311, 884, 49488, 5321, 13, 50562], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 353, "seek": 155658, "start": 1560.54, "end": 1565.1, "text": " Anyway, emails for the next questions, I think.", "tokens": [50562, 5684, 11, 12524, 337, 264, 958, 1651, 11, 286, 519, 13, 50790], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 354, "seek": 155658, "start": 1565.1, "end": 1569.6599999999999, "text": " So I should let the next person set up.", "tokens": [50790, 407, 286, 820, 718, 264, 958, 954, 992, 493, 13, 51018], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 355, "seek": 155658, "start": 1569.6599999999999, "end": 1571.06, "text": " Any other questions for Joan?", "tokens": [51018, 2639, 661, 1651, 337, 25748, 30, 51088], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 356, "seek": 155658, "start": 1571.06, "end": 1572.06, "text": " Can it?", "tokens": [51088, 1664, 309, 30, 51138], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 357, "seek": 155658, "start": 1572.06, "end": 1573.06, "text": " Oh.", "tokens": [51138, 876, 13, 51188], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 358, "seek": 155658, "start": 1573.06, "end": 1574.06, "text": " Yeah, last one.", "tokens": [51188, 865, 11, 1036, 472, 13, 51238], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 359, "seek": 155658, "start": 1574.06, "end": 1580.06, "text": " Actually, I had two, but okay, I'll re-work it.", "tokens": [51238, 5135, 11, 286, 632, 732, 11, 457, 1392, 11, 286, 603, 319, 12, 1902, 309, 13, 51538], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 360, "seek": 155658, "start": 1580.06, "end": 1581.06, "text": " Okay.", "tokens": [51538, 1033, 13, 51588], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 361, "seek": 155658, "start": 1581.06, "end": 1584.06, "text": " So I saw that you were also creating slurm clusters.", "tokens": [51588, 407, 286, 1866, 300, 291, 645, 611, 4084, 1061, 26717, 23313, 13, 51738], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 362, "seek": 155658, "start": 1584.06, "end": 1585.06, "text": " Yes.", "tokens": [51738, 1079, 13, 51788], "temperature": 0.0, "avg_logprob": -0.39975645935651166, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.4884856045246124}, {"id": 363, "seek": 158506, "start": 1585.06, "end": 1591.78, "text": " So how does Kubernetes and slurm play together for the network topology and placement of", "tokens": [50364, 407, 577, 775, 23145, 293, 1061, 26717, 862, 1214, 337, 264, 3209, 1192, 1793, 293, 17257, 295, 50700], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 364, "seek": 158506, "start": 1591.78, "end": 1592.78, "text": " your networks?", "tokens": [50700, 428, 9590, 30, 50750], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 365, "seek": 158506, "start": 1592.78, "end": 1595.78, "text": " Well, I have lots of ideas for that after your talk.", "tokens": [50750, 1042, 11, 286, 362, 3195, 295, 3487, 337, 300, 934, 428, 751, 13, 50900], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 366, "seek": 158506, "start": 1595.78, "end": 1597.22, "text": " At the moment, not really.", "tokens": [50900, 1711, 264, 1623, 11, 406, 534, 13, 50972], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 367, "seek": 158506, "start": 1597.22, "end": 1600.46, "text": " So they just, the pods get placed wherever and then...", "tokens": [50972, 407, 436, 445, 11, 264, 31925, 483, 7074, 8660, 293, 550, 485, 51134], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 368, "seek": 158506, "start": 1600.46, "end": 1602.78, "text": " Yeah, at the moment, they're totally isolated environments.", "tokens": [51134, 865, 11, 412, 264, 1623, 11, 436, 434, 3879, 14621, 12388, 13, 51250], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 369, "seek": 158506, "start": 1602.78, "end": 1606.78, "text": " So you stamp out a slurm cluster and it's your own to do what you need.", "tokens": [51250, 407, 291, 9921, 484, 257, 1061, 26717, 13630, 293, 309, 311, 428, 1065, 281, 360, 437, 291, 643, 13, 51450], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 370, "seek": 158506, "start": 1606.78, "end": 1613.78, "text": " And then super briefly, the pink line that was legacy RDNA, SR, IO virtualization.", "tokens": [51450, 400, 550, 1687, 10515, 11, 264, 7022, 1622, 300, 390, 11711, 49488, 5321, 11, 20840, 11, 39839, 6374, 2144, 13, 51800], "temperature": 0.0, "avg_logprob": -0.36717034962551653, "compression_ratio": 1.6178571428571429, "no_speech_prob": 0.11146692931652069}, {"id": 371, "seek": 161378, "start": 1614.78, "end": 1615.78, "text": " Yes.", "tokens": [50414, 1079, 13, 50464], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 372, "seek": 161378, "start": 1615.78, "end": 1619.5, "text": " Is that bare metal or is that also virtualized?", "tokens": [50464, 1119, 300, 6949, 5760, 420, 307, 300, 611, 6374, 1602, 30, 50650], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 373, "seek": 161378, "start": 1619.5, "end": 1621.0, "text": " That was...", "tokens": [50650, 663, 390, 485, 50725], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 374, "seek": 161378, "start": 1621.0, "end": 1624.0, "text": " Is it running on...", "tokens": [50725, 1119, 309, 2614, 322, 485, 50875], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 375, "seek": 161378, "start": 1624.0, "end": 1625.0, "text": " Because that was...", "tokens": [50875, 1436, 300, 390, 485, 50925], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 376, "seek": 161378, "start": 1625.0, "end": 1628.0, "text": " We can catch up later.", "tokens": [50925, 492, 393, 3745, 493, 1780, 13, 51075], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 377, "seek": 161378, "start": 1628.0, "end": 1629.0, "text": " Okay.", "tokens": [51075, 1033, 13, 51125], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 378, "seek": 161378, "start": 1629.0, "end": 1635.02, "text": " So that specific scenario, I definitely recommend watching Stigt Alpha's talk, they're five", "tokens": [51125, 407, 300, 2685, 9005, 11, 286, 2138, 2748, 1976, 745, 5828, 20588, 311, 751, 11, 436, 434, 1732, 51426], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 379, "seek": 161378, "start": 1635.02, "end": 1636.78, "text": " ways on CNI.", "tokens": [51426, 2098, 322, 14589, 40, 13, 51514], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 380, "seek": 161378, "start": 1636.78, "end": 1642.22, "text": " I think that particular setup was actually bare metal with a virtual function.", "tokens": [51514, 286, 519, 300, 1729, 8657, 390, 767, 6949, 5760, 365, 257, 6374, 2445, 13, 51786], "temperature": 0.0, "avg_logprob": -0.36498065616773523, "compression_ratio": 1.4744186046511627, "no_speech_prob": 0.055011797696352005}, {"id": 381, "seek": 164222, "start": 1642.22, "end": 1646.38, "text": " So it was actually Kubernetes on bare metal with the virtual function passed into the", "tokens": [50364, 407, 309, 390, 767, 23145, 322, 6949, 5760, 365, 264, 6374, 2445, 4678, 666, 264, 50572], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 382, "seek": 164222, "start": 1646.38, "end": 1647.38, "text": " container.", "tokens": [50572, 10129, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 383, "seek": 164222, "start": 1647.38, "end": 1648.38, "text": " Right.", "tokens": [50622, 1779, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 384, "seek": 164222, "start": 1648.38, "end": 1655.38, "text": " I believe we got similar results without doing that legacy path into the VM as well.", "tokens": [50672, 286, 1697, 321, 658, 2531, 3542, 1553, 884, 300, 11711, 3100, 666, 264, 18038, 382, 731, 13, 51022], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 385, "seek": 164222, "start": 1656.18, "end": 1661.58, "text": " The extra cost, I believe, is on the VF lag piece because there's an extra bit in routing", "tokens": [51062, 440, 2857, 2063, 11, 286, 1697, 11, 307, 322, 264, 691, 37, 8953, 2522, 570, 456, 311, 364, 2857, 857, 294, 32722, 51332], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 386, "seek": 164222, "start": 1661.58, "end": 1668.18, "text": " inside the silicon, I believe, but I'm not certain on that, so I'd have to check.", "tokens": [51332, 1854, 264, 22848, 11, 286, 1697, 11, 457, 286, 478, 406, 1629, 322, 300, 11, 370, 286, 1116, 362, 281, 1520, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 387, "seek": 164222, "start": 1668.18, "end": 1669.18, "text": " Thank you.", "tokens": [51662, 1044, 291, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 388, "seek": 164222, "start": 1669.18, "end": 1670.18, "text": " Pleasure.", "tokens": [51712, 25658, 2508, 13, 51762], "temperature": 0.0, "avg_logprob": -0.2516141671400804, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.08276455849409103}, {"id": 389, "seek": 167018, "start": 1670.98, "end": 1671.98, "text": " Thank you very much, John.", "tokens": [50404, 1044, 291, 588, 709, 11, 2619, 13, 50454], "temperature": 0.0, "avg_logprob": -0.47494646708170574, "compression_ratio": 0.9487179487179487, "no_speech_prob": 0.05562755465507507}, {"id": 390, "seek": 167018, "start": 1671.98, "end": 1672.98, "text": " Thank you.", "tokens": [50454, 1044, 291, 13, 50504], "temperature": 0.0, "avg_logprob": -0.47494646708170574, "compression_ratio": 0.9487179487179487, "no_speech_prob": 0.05562755465507507}], "language": "en"}