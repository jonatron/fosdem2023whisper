{"text": " Hello everyone, I'm Martin Gerlach, I'm a senior research scientist in the research team at the Wikimedia Foundation. First of all, I want to thank the organizers for the opportunity to present here today. I'm very excited to share some of our recent work around building open tools to support research around Wikimedia projects. Before going into the details, I want to provide some background around what is Wikimedia and its research team. I want to start with something that most of you are probably familiar with, Wikipedia, which is by now the largest encyclopedia in the history of humankind. Wikipedia, together with its sister projects like Wikimedia Commons or Wiktionary, are operated by the Wikimedia Foundation. The Wikimedia Foundation is a nonprofit organization and has a staff of around 600 employees. It provides support to the communities and the projects in different ways, but it's important to know that it does not create or modify the content and it does not define or enforce policies on the projects. One of the teams at the Wikimedia Foundation is the research team, and we are a small team of eight scientists, engineers, and community officers, and we work with collaborators from different universities to do research around Wikimedia projects. These activities can be grouped in roughly three main areas. The first one is to address knowledge gap, so what content is missing or underrepresented? One example of this is the gender gap. Second is to improve knowledge integrity, that is making sure the content on the projects is accurate, can think of vandalism or misinformation or disinformation. The third aspect is growing the research community, that is empowering others to do research around the projects. Today I want to focus on the activities in this last area, specifically I want to present four facets in which we have been contributing towards this goal, that is around data sets, tools for data processing and building machine learning APIs. Finally, I want to conclude with how developers or interested researchers can contribute to these three areas. So let's go. Wikimedia Foundation provides already many, many different data sets, most notably Wikimedia dumps around the content, but also containing information about edits and page views of articles. This is public and openly available, and it's used by many researchers as well as developers to build dashboards or tools for editors. However, when working with this data, this might prove still very challenging for people who might not identify as Wikimedia researchers or for someone lacking the expertise about database schemas or which data is where or how to filter is. Therefore, we try to release clean and pre-process data set to facilitate that. And one such example is to Wikipedia image caption data set. This is a clean and processed data set of millions of examples of images from Wikimedia comments with their captions extracted from more than 100 language versions of Wikipedia. The background is that many articles on Wikipedia are still lacking visual content, which we know are crucial for learning. Creating text to these images increases the accessibility and enables better search. So with the release of this data, we hope to enable other researchers to build better machine learning models to assist editors in writing image captions. In this case, we did not just release the data, but provided it in a more structured form as part of a competition with a very specific task. And the idea was to also attract new contributors through this structure so that researchers could find examples of the types of tools that could be useful for the community, experienced researchers outside of Wikimedia could easily contribute their expertise. And for new researchers is an easy way to become familiar with Wikimedia data. The outcome of this was a Kaggle competition with more than 100 participants and many, many open source solutions in how to approach this problem. This was just one example of data sets that release, and I just want to highlight there's other cleaned process data sets we are releasing around quality score of Wikipedia articles around readability of Wikipedia articles, and also their upcoming releases around using differential privacy around geography of readers. In the next part, I want to blend how to work with all this data. We always aim to make data as much as the data publicly available. However, that doesn't necessarily mean it is accessible because it might still require a lot of technical expertise to effectively work with this data. Therefore, we try to build tools to lower the technical barriers. And here I want to present one such example related to the HTML dump data set. What is this? This is a new dump data set available since October 2021, and it's now published and updated in regular intervals. And it contains the HTML version of all articles of Wikipedia. Why is this so exciting? Traditional dumps, when we are using the traditional dumps, the content of the articles is only available in the WikiText markup. This is what you see when you edit the source of an article. However, what you see as a reader when browsing is not the WikiText markup, but the WikiText gets parsed into an HTML. The problem is the WikiText does not explicitly contain all the elements that are visible in the HTML. This comes mainly from parsing of templates or info boxes. This becomes an issue for researchers studying the content of articles because they will miss many of the elements when only when looking at the WikiText. One example for this is when looking for hyperlinks in articles. One study by Mitrovsky looked at counting the number of links in articles and found that WikiText contains less than half of the links that are visible in the HTML version of the reader. So we can conclude that researchers should use the HTML dumps because they capture more accurately the content of the article. However, the challenge is how to parse the HTML dumps or the articles in the HTML dumps version. This is not just about knowing HTML, but it's also about very specific knowledge about how the media Wiki software translates different Wiki elements and how they will appear in the HTML version. Existing packages exist for WikiText, but not for HTML. Therefore, this is a very high barrier for practitioners to switch their existing pipelines to use this new dataset. Our solution was to build a Python library to make working with these dumps very easily. We called it MWParser from HTML, and it parses HTML and extracts elements of an article such as links, references, templates, or the plain text without the user having to know anything about HTML and the way Wiki elements appear in it. We recently released the first version of this. This is work in progress. There's tons of open issues. So if you're interested, contributions from anyone are very, very welcome to improve this in the future. Check out the repo on GitLab for more information. As a third step, I want to mention, present how we use these datasets in practice. I want to show one example in the context of knowledge integrity in order to ensure quality of articles in Wikipedia. There are many, many editors who try to review the edits that are made to articles in Wikipedia and try to check whether these edits are okay or whether they're not okay and what should be reverted. The problem is there are a lot of edits happening. So just in English Wikipedia, there's around 100,000 edits per day to work through. And the aim is, can we build a tool to support editors in dealing with the large volume of edits? Can we help them identify the very bad edits more easily? And this is what we do with a so-called risk revert model. What is this? So we look at an edit by comparing the old version of an article with its new version. And we would like to make a prediction whether the change is good or whether it is a very bad edit and it should be reverted. How we do this is we extract different features from this article. So which text was changed, where their links that were removed, where their images that were removed, and so on. And then we built a model by looking into the history of all Wikipedia edits and extract those edits which have been reverted by editors and use that as a ground truth of bad edits for our model. And the resulting output is that we can, for each of these edits, we can calculate a so-called revert risk. This is a very bad edit, will have a very high probability, a very high risk for being reverted. And this is what our model will output. And our model performs fairly well. It has an accuracy between 70 and 80%. And I want to mention that we consider this OK. It does not need to be perfect. Our model, the way our model is used is there's editors that will surface these scores to help editors identify at which edits they should take a closer look. Similar models for annotating content of articles exist. We have been developing these types of models. In addition to knowledge integrity, what I presented, we have been trying to build models for finding easily similar articles, for identifying automatically the topic of an article to assess its readability or geography, or identifying related images, et cetera. I only want to briefly highlight that the development of these models is rooted in some core principles to which we are committed to. And this can create additional challenges in developing this model, specifically this context I want to highlight a multilingual aspect so that we always try to prefer language agnostic approaches in order to support as many as possible of the 300 different language versions in Wikipedia. I want to conclude with potential ways in which to contribute in any of these three areas that I mentioned previously. Generally, one can contribute as a developer to media wiki or other aspects of the wiki media ecosystem. And there, the place to get started is the so-called developer portal, which is a centralized entry point for finding technical documentation and community resources. Not going into more detail here, I want to give a shout out and refer to the talk by my colleague Slavina Stefanova from the developer acquisition advocacy team. But specifically in the area of research, I want to highlight a few entry points depending on your interest. In case you would like to build a specific tool, there is wiki media foundations toolforge infrastructure and that is a hosting environment that allows you to run bots or different APIs in case you would like to provide that tool to the public. If you want to work with us on improving tools or algorithms, you can check out the different packages that we have been releasing in the past months. These are all work in progress. There's many open issues and we're happy about any contributions about improving, fixing existing issues or even finding new bugs. So please check out our repository too. If you are interested in getting funding, there are different opportunities. There is an existing program to fund research around wiki media projects. This covers many different disciplines, humanities, social science, computer science, education, law, et cetera, and is around work that has potential for direct positive impact on the local communities. In addition, I want to mention that coming in the future, there are plans for a similar program to improve wiki media's technology and tools. If you want to learn about the projects we are working on, I want to mention that we publish a research report, a summary of our ongoing research projects every six months and you can find more details about some of the projects that I have mentioned. Finally, if you would like to engage with the research community, you can join us at wiki workshop. This is the primary meeting venue of the wiki media research community. This year will be the 10th edition of wiki workshop and it is expected to be held in May. You can submit your works there. I invite you for the submissions. We highly encourage ongoing or preliminary works by submitting extended abstracts. In this edition, there will also be a novel track for wiki media developers. If you are a developer of a tool or a system or an algorithm that could be of interest to research on wiki media, please check it out and make a submission. Even if you do not plan to make a submission, you are welcome to participate. As done in the last three editions, wiki workshop will be fully virtual and attendance will be free. With this, I want to conclude. I want to thank you very much for your attention. I am looking forward to your questions in the Q&A. If you want to stay in touch, feel free to reach out to me personally on my email or any of the other channels that I am listing here through office hours or mailing lists on IRC, et cetera, and with this, thank you very much. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.0, "text": " Hello everyone, I'm Martin Gerlach, I'm a senior research scientist in the research", "tokens": [50364, 2425, 1518, 11, 286, 478, 9184, 9409, 75, 608, 11, 286, 478, 257, 7965, 2132, 12662, 294, 264, 2132, 50914], "temperature": 0.0, "avg_logprob": -0.16056545869803723, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.06080718711018562}, {"id": 1, "seek": 0, "start": 11.0, "end": 14.120000000000001, "text": " team at the Wikimedia Foundation.", "tokens": [50914, 1469, 412, 264, 23377, 332, 14212, 10335, 13, 51070], "temperature": 0.0, "avg_logprob": -0.16056545869803723, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.06080718711018562}, {"id": 2, "seek": 0, "start": 14.120000000000001, "end": 19.52, "text": " First of all, I want to thank the organizers for the opportunity to present here today.", "tokens": [51070, 2386, 295, 439, 11, 286, 528, 281, 1309, 264, 35071, 337, 264, 2650, 281, 1974, 510, 965, 13, 51340], "temperature": 0.0, "avg_logprob": -0.16056545869803723, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.06080718711018562}, {"id": 3, "seek": 0, "start": 19.52, "end": 25.84, "text": " I'm very excited to share some of our recent work around building open tools to support", "tokens": [51340, 286, 478, 588, 2919, 281, 2073, 512, 295, 527, 5162, 589, 926, 2390, 1269, 3873, 281, 1406, 51656], "temperature": 0.0, "avg_logprob": -0.16056545869803723, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.06080718711018562}, {"id": 4, "seek": 0, "start": 25.84, "end": 29.0, "text": " research around Wikimedia projects.", "tokens": [51656, 2132, 926, 23377, 332, 14212, 4455, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16056545869803723, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.06080718711018562}, {"id": 5, "seek": 2900, "start": 29.0, "end": 35.4, "text": " Before going into the details, I want to provide some background around what is Wikimedia", "tokens": [50364, 4546, 516, 666, 264, 4365, 11, 286, 528, 281, 2893, 512, 3678, 926, 437, 307, 23377, 332, 14212, 50684], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 6, "seek": 2900, "start": 35.4, "end": 37.4, "text": " and its research team.", "tokens": [50684, 293, 1080, 2132, 1469, 13, 50784], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 7, "seek": 2900, "start": 37.4, "end": 42.92, "text": " I want to start with something that most of you are probably familiar with, Wikipedia,", "tokens": [50784, 286, 528, 281, 722, 365, 746, 300, 881, 295, 291, 366, 1391, 4963, 365, 11, 28999, 11, 51060], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 8, "seek": 2900, "start": 42.92, "end": 49.2, "text": " which is by now the largest encyclopedia in the history of humankind.", "tokens": [51060, 597, 307, 538, 586, 264, 6443, 465, 34080, 47795, 294, 264, 2503, 295, 1484, 40588, 13, 51374], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 9, "seek": 2900, "start": 49.2, "end": 54.8, "text": " Wikipedia, together with its sister projects like Wikimedia Commons or Wiktionary, are", "tokens": [51374, 28999, 11, 1214, 365, 1080, 4892, 4455, 411, 23377, 332, 14212, 34894, 420, 343, 9874, 313, 822, 11, 366, 51654], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 10, "seek": 2900, "start": 54.8, "end": 58.480000000000004, "text": " operated by the Wikimedia Foundation.", "tokens": [51654, 20826, 538, 264, 23377, 332, 14212, 10335, 13, 51838], "temperature": 0.0, "avg_logprob": -0.18528362156189593, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.003358496120199561}, {"id": 11, "seek": 5848, "start": 58.48, "end": 67.28, "text": " The Wikimedia Foundation is a nonprofit organization and has a staff of around 600 employees.", "tokens": [50364, 440, 23377, 332, 14212, 10335, 307, 257, 23348, 4475, 293, 575, 257, 3525, 295, 926, 11849, 6619, 13, 50804], "temperature": 0.0, "avg_logprob": -0.11617091122795553, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0007626371225342155}, {"id": 12, "seek": 5848, "start": 67.28, "end": 73.32, "text": " It provides support to the communities and the projects in different ways, but it's", "tokens": [50804, 467, 6417, 1406, 281, 264, 4456, 293, 264, 4455, 294, 819, 2098, 11, 457, 309, 311, 51106], "temperature": 0.0, "avg_logprob": -0.11617091122795553, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0007626371225342155}, {"id": 13, "seek": 5848, "start": 73.32, "end": 79.88, "text": " important to know that it does not create or modify the content and it does not define", "tokens": [51106, 1021, 281, 458, 300, 309, 775, 406, 1884, 420, 16927, 264, 2701, 293, 309, 775, 406, 6964, 51434], "temperature": 0.0, "avg_logprob": -0.11617091122795553, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0007626371225342155}, {"id": 14, "seek": 5848, "start": 79.88, "end": 83.96, "text": " or enforce policies on the projects.", "tokens": [51434, 420, 24825, 7657, 322, 264, 4455, 13, 51638], "temperature": 0.0, "avg_logprob": -0.11617091122795553, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0007626371225342155}, {"id": 15, "seek": 8396, "start": 83.96, "end": 90.96, "text": " One of the teams at the Wikimedia Foundation is the research team, and we are a small team", "tokens": [50364, 1485, 295, 264, 5491, 412, 264, 23377, 332, 14212, 10335, 307, 264, 2132, 1469, 11, 293, 321, 366, 257, 1359, 1469, 50714], "temperature": 0.0, "avg_logprob": -0.15984269847040591, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.011629891581833363}, {"id": 16, "seek": 8396, "start": 90.96, "end": 97.47999999999999, "text": " of eight scientists, engineers, and community officers, and we work with collaborators from", "tokens": [50714, 295, 3180, 7708, 11, 11955, 11, 293, 1768, 9199, 11, 293, 321, 589, 365, 39789, 490, 51040], "temperature": 0.0, "avg_logprob": -0.15984269847040591, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.011629891581833363}, {"id": 17, "seek": 8396, "start": 97.47999999999999, "end": 103.36, "text": " different universities to do research around Wikimedia projects.", "tokens": [51040, 819, 11779, 281, 360, 2132, 926, 23377, 332, 14212, 4455, 13, 51334], "temperature": 0.0, "avg_logprob": -0.15984269847040591, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.011629891581833363}, {"id": 18, "seek": 8396, "start": 103.36, "end": 108.83999999999999, "text": " These activities can be grouped in roughly three main areas.", "tokens": [51334, 1981, 5354, 393, 312, 41877, 294, 9810, 1045, 2135, 3179, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15984269847040591, "compression_ratio": 1.5634517766497462, "no_speech_prob": 0.011629891581833363}, {"id": 19, "seek": 10884, "start": 108.84, "end": 114.92, "text": " The first one is to address knowledge gap, so what content is missing or underrepresented?", "tokens": [50364, 440, 700, 472, 307, 281, 2985, 3601, 7417, 11, 370, 437, 2701, 307, 5361, 420, 833, 38293, 30, 50668], "temperature": 0.0, "avg_logprob": -0.15825466548695283, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.05230666324496269}, {"id": 20, "seek": 10884, "start": 114.92, "end": 119.12, "text": " One example of this is the gender gap.", "tokens": [50668, 1485, 1365, 295, 341, 307, 264, 7898, 7417, 13, 50878], "temperature": 0.0, "avg_logprob": -0.15825466548695283, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.05230666324496269}, {"id": 21, "seek": 10884, "start": 119.12, "end": 125.84, "text": " Second is to improve knowledge integrity, that is making sure the content on the projects", "tokens": [50878, 5736, 307, 281, 3470, 3601, 16000, 11, 300, 307, 1455, 988, 264, 2701, 322, 264, 4455, 51214], "temperature": 0.0, "avg_logprob": -0.15825466548695283, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.05230666324496269}, {"id": 22, "seek": 10884, "start": 125.84, "end": 132.12, "text": " is accurate, can think of vandalism or misinformation or disinformation.", "tokens": [51214, 307, 8559, 11, 393, 519, 295, 371, 18905, 1434, 420, 34238, 420, 717, 20941, 13, 51528], "temperature": 0.0, "avg_logprob": -0.15825466548695283, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.05230666324496269}, {"id": 23, "seek": 13212, "start": 132.12, "end": 139.72, "text": " The third aspect is growing the research community, that is empowering others to do research around", "tokens": [50364, 440, 2636, 4171, 307, 4194, 264, 2132, 1768, 11, 300, 307, 28261, 2357, 281, 360, 2132, 926, 50744], "temperature": 0.0, "avg_logprob": -0.17813864621249112, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.005052413325756788}, {"id": 24, "seek": 13212, "start": 139.72, "end": 142.48000000000002, "text": " the projects.", "tokens": [50744, 264, 4455, 13, 50882], "temperature": 0.0, "avg_logprob": -0.17813864621249112, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.005052413325756788}, {"id": 25, "seek": 13212, "start": 142.48000000000002, "end": 150.08, "text": " Today I want to focus on the activities in this last area, specifically I want to present", "tokens": [50882, 2692, 286, 528, 281, 1879, 322, 264, 5354, 294, 341, 1036, 1859, 11, 4682, 286, 528, 281, 1974, 51262], "temperature": 0.0, "avg_logprob": -0.17813864621249112, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.005052413325756788}, {"id": 26, "seek": 13212, "start": 150.08, "end": 157.88, "text": " four facets in which we have been contributing towards this goal, that is around data sets,", "tokens": [51262, 1451, 49752, 294, 597, 321, 362, 668, 19270, 3030, 341, 3387, 11, 300, 307, 926, 1412, 6352, 11, 51652], "temperature": 0.0, "avg_logprob": -0.17813864621249112, "compression_ratio": 1.6120218579234973, "no_speech_prob": 0.005052413325756788}, {"id": 27, "seek": 15788, "start": 157.88, "end": 162.28, "text": " tools for data processing and building machine learning APIs.", "tokens": [50364, 3873, 337, 1412, 9007, 293, 2390, 3479, 2539, 21445, 13, 50584], "temperature": 0.0, "avg_logprob": -0.24756574630737305, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.0033723642118275166}, {"id": 28, "seek": 15788, "start": 162.28, "end": 170.12, "text": " Finally, I want to conclude with how developers or interested researchers can contribute to", "tokens": [50584, 6288, 11, 286, 528, 281, 16886, 365, 577, 8849, 420, 3102, 10309, 393, 10586, 281, 50976], "temperature": 0.0, "avg_logprob": -0.24756574630737305, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.0033723642118275166}, {"id": 29, "seek": 15788, "start": 170.12, "end": 172.24, "text": " these three areas.", "tokens": [50976, 613, 1045, 3179, 13, 51082], "temperature": 0.0, "avg_logprob": -0.24756574630737305, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.0033723642118275166}, {"id": 30, "seek": 15788, "start": 172.24, "end": 175.32, "text": " So let's go.", "tokens": [51082, 407, 718, 311, 352, 13, 51236], "temperature": 0.0, "avg_logprob": -0.24756574630737305, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.0033723642118275166}, {"id": 31, "seek": 15788, "start": 175.32, "end": 181.2, "text": " Wikimedia Foundation provides already many, many different data sets, most notably Wikimedia", "tokens": [51236, 23377, 332, 14212, 10335, 6417, 1217, 867, 11, 867, 819, 1412, 6352, 11, 881, 31357, 23377, 332, 14212, 51530], "temperature": 0.0, "avg_logprob": -0.24756574630737305, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.0033723642118275166}, {"id": 32, "seek": 18120, "start": 181.2, "end": 189.2, "text": " dumps around the content, but also containing information about edits and page views of articles.", "tokens": [50364, 11430, 82, 926, 264, 2701, 11, 457, 611, 19273, 1589, 466, 41752, 293, 3028, 6809, 295, 11290, 13, 50764], "temperature": 0.0, "avg_logprob": -0.21078617472044178, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.012587176635861397}, {"id": 33, "seek": 18120, "start": 189.2, "end": 196.04, "text": " This is public and openly available, and it's used by many researchers as well as developers", "tokens": [50764, 639, 307, 1908, 293, 23109, 2435, 11, 293, 309, 311, 1143, 538, 867, 10309, 382, 731, 382, 8849, 51106], "temperature": 0.0, "avg_logprob": -0.21078617472044178, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.012587176635861397}, {"id": 34, "seek": 18120, "start": 196.04, "end": 199.2, "text": " to build dashboards or tools for editors.", "tokens": [51106, 281, 1322, 8240, 17228, 420, 3873, 337, 31446, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21078617472044178, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.012587176635861397}, {"id": 35, "seek": 18120, "start": 199.2, "end": 207.32, "text": " However, when working with this data, this might prove still very challenging for people", "tokens": [51264, 2908, 11, 562, 1364, 365, 341, 1412, 11, 341, 1062, 7081, 920, 588, 7595, 337, 561, 51670], "temperature": 0.0, "avg_logprob": -0.21078617472044178, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.012587176635861397}, {"id": 36, "seek": 20732, "start": 207.32, "end": 214.95999999999998, "text": " who might not identify as Wikimedia researchers or for someone lacking the expertise about", "tokens": [50364, 567, 1062, 406, 5876, 382, 23377, 332, 14212, 10309, 420, 337, 1580, 20889, 264, 11769, 466, 50746], "temperature": 0.0, "avg_logprob": -0.202504088913185, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.024386819452047348}, {"id": 37, "seek": 20732, "start": 214.95999999999998, "end": 220.51999999999998, "text": " database schemas or which data is where or how to filter is.", "tokens": [50746, 8149, 22627, 296, 420, 597, 1412, 307, 689, 420, 577, 281, 6608, 307, 13, 51024], "temperature": 0.0, "avg_logprob": -0.202504088913185, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.024386819452047348}, {"id": 38, "seek": 20732, "start": 220.51999999999998, "end": 229.12, "text": " Therefore, we try to release clean and pre-process data set to facilitate that.", "tokens": [51024, 7504, 11, 321, 853, 281, 4374, 2541, 293, 659, 12, 41075, 1412, 992, 281, 20207, 300, 13, 51454], "temperature": 0.0, "avg_logprob": -0.202504088913185, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.024386819452047348}, {"id": 39, "seek": 20732, "start": 229.12, "end": 233.92, "text": " And one such example is to Wikipedia image caption data set.", "tokens": [51454, 400, 472, 1270, 1365, 307, 281, 28999, 3256, 31974, 1412, 992, 13, 51694], "temperature": 0.0, "avg_logprob": -0.202504088913185, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.024386819452047348}, {"id": 40, "seek": 23392, "start": 233.92, "end": 239.6, "text": " This is a clean and processed data set of millions of examples of images from Wikimedia", "tokens": [50364, 639, 307, 257, 2541, 293, 18846, 1412, 992, 295, 6803, 295, 5110, 295, 5267, 490, 23377, 332, 14212, 50648], "temperature": 0.0, "avg_logprob": -0.12111910432577133, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.0012055947445333004}, {"id": 41, "seek": 23392, "start": 239.6, "end": 248.2, "text": " comments with their captions extracted from more than 100 language versions of Wikipedia.", "tokens": [50648, 3053, 365, 641, 44832, 34086, 490, 544, 813, 2319, 2856, 9606, 295, 28999, 13, 51078], "temperature": 0.0, "avg_logprob": -0.12111910432577133, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.0012055947445333004}, {"id": 42, "seek": 23392, "start": 248.2, "end": 254.67999999999998, "text": " The background is that many articles on Wikipedia are still lacking visual content, which we", "tokens": [51078, 440, 3678, 307, 300, 867, 11290, 322, 28999, 366, 920, 20889, 5056, 2701, 11, 597, 321, 51402], "temperature": 0.0, "avg_logprob": -0.12111910432577133, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.0012055947445333004}, {"id": 43, "seek": 23392, "start": 254.67999999999998, "end": 258.52, "text": " know are crucial for learning.", "tokens": [51402, 458, 366, 11462, 337, 2539, 13, 51594], "temperature": 0.0, "avg_logprob": -0.12111910432577133, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.0012055947445333004}, {"id": 44, "seek": 25852, "start": 258.52, "end": 266.59999999999997, "text": " Creating text to these images increases the accessibility and enables better search.", "tokens": [50364, 40002, 2487, 281, 613, 5267, 8637, 264, 15002, 293, 17077, 1101, 3164, 13, 50768], "temperature": 0.0, "avg_logprob": -0.15860766002110072, "compression_ratio": 1.62, "no_speech_prob": 0.009837673977017403}, {"id": 45, "seek": 25852, "start": 266.59999999999997, "end": 271.79999999999995, "text": " So with the release of this data, we hope to enable other researchers to build better", "tokens": [50768, 407, 365, 264, 4374, 295, 341, 1412, 11, 321, 1454, 281, 9528, 661, 10309, 281, 1322, 1101, 51028], "temperature": 0.0, "avg_logprob": -0.15860766002110072, "compression_ratio": 1.62, "no_speech_prob": 0.009837673977017403}, {"id": 46, "seek": 25852, "start": 271.79999999999995, "end": 278.24, "text": " machine learning models to assist editors in writing image captions.", "tokens": [51028, 3479, 2539, 5245, 281, 4255, 31446, 294, 3579, 3256, 44832, 13, 51350], "temperature": 0.0, "avg_logprob": -0.15860766002110072, "compression_ratio": 1.62, "no_speech_prob": 0.009837673977017403}, {"id": 47, "seek": 25852, "start": 278.24, "end": 284.08, "text": " In this case, we did not just release the data, but provided it in a more structured", "tokens": [51350, 682, 341, 1389, 11, 321, 630, 406, 445, 4374, 264, 1412, 11, 457, 5649, 309, 294, 257, 544, 18519, 51642], "temperature": 0.0, "avg_logprob": -0.15860766002110072, "compression_ratio": 1.62, "no_speech_prob": 0.009837673977017403}, {"id": 48, "seek": 28408, "start": 284.08, "end": 289.68, "text": " form as part of a competition with a very specific task.", "tokens": [50364, 1254, 382, 644, 295, 257, 6211, 365, 257, 588, 2685, 5633, 13, 50644], "temperature": 0.0, "avg_logprob": -0.17835385982806867, "compression_ratio": 1.463855421686747, "no_speech_prob": 0.014247313141822815}, {"id": 49, "seek": 28408, "start": 289.68, "end": 300.79999999999995, "text": " And the idea was to also attract new contributors through this structure so that researchers", "tokens": [50644, 400, 264, 1558, 390, 281, 611, 5049, 777, 45627, 807, 341, 3877, 370, 300, 10309, 51200], "temperature": 0.0, "avg_logprob": -0.17835385982806867, "compression_ratio": 1.463855421686747, "no_speech_prob": 0.014247313141822815}, {"id": 50, "seek": 28408, "start": 300.79999999999995, "end": 307.76, "text": " could find examples of the types of tools that could be useful for the community, experienced", "tokens": [51200, 727, 915, 5110, 295, 264, 3467, 295, 3873, 300, 727, 312, 4420, 337, 264, 1768, 11, 6751, 51548], "temperature": 0.0, "avg_logprob": -0.17835385982806867, "compression_ratio": 1.463855421686747, "no_speech_prob": 0.014247313141822815}, {"id": 51, "seek": 30776, "start": 307.76, "end": 313.71999999999997, "text": " researchers outside of Wikimedia could easily contribute their expertise.", "tokens": [50364, 10309, 2380, 295, 23377, 332, 14212, 727, 3612, 10586, 641, 11769, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1909835445347117, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.021910063922405243}, {"id": 52, "seek": 30776, "start": 313.71999999999997, "end": 322.28, "text": " And for new researchers is an easy way to become familiar with Wikimedia data.", "tokens": [50662, 400, 337, 777, 10309, 307, 364, 1858, 636, 281, 1813, 4963, 365, 23377, 332, 14212, 1412, 13, 51090], "temperature": 0.0, "avg_logprob": -0.1909835445347117, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.021910063922405243}, {"id": 53, "seek": 30776, "start": 322.28, "end": 328.8, "text": " The outcome of this was a Kaggle competition with more than 100 participants and many,", "tokens": [51090, 440, 9700, 295, 341, 390, 257, 48751, 22631, 6211, 365, 544, 813, 2319, 10503, 293, 867, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1909835445347117, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.021910063922405243}, {"id": 54, "seek": 30776, "start": 328.8, "end": 332.96, "text": " many open source solutions in how to approach this problem.", "tokens": [51416, 867, 1269, 4009, 6547, 294, 577, 281, 3109, 341, 1154, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1909835445347117, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.021910063922405243}, {"id": 55, "seek": 33296, "start": 332.96, "end": 339.47999999999996, "text": " This was just one example of data sets that release, and I just want to highlight there's", "tokens": [50364, 639, 390, 445, 472, 1365, 295, 1412, 6352, 300, 4374, 11, 293, 286, 445, 528, 281, 5078, 456, 311, 50690], "temperature": 0.0, "avg_logprob": -0.2078990638256073, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.0014543554279953241}, {"id": 56, "seek": 33296, "start": 339.47999999999996, "end": 345.64, "text": " other cleaned process data sets we are releasing around quality score of Wikipedia articles", "tokens": [50690, 661, 16146, 1399, 1412, 6352, 321, 366, 16327, 926, 3125, 6175, 295, 28999, 11290, 50998], "temperature": 0.0, "avg_logprob": -0.2078990638256073, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.0014543554279953241}, {"id": 57, "seek": 33296, "start": 345.64, "end": 353.76, "text": " around readability of Wikipedia articles, and also their upcoming releases around using", "tokens": [50998, 926, 1401, 2310, 295, 28999, 11290, 11, 293, 611, 641, 11500, 16952, 926, 1228, 51404], "temperature": 0.0, "avg_logprob": -0.2078990638256073, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.0014543554279953241}, {"id": 58, "seek": 33296, "start": 353.76, "end": 360.88, "text": " differential privacy around geography of readers.", "tokens": [51404, 15756, 11427, 926, 26695, 295, 17147, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2078990638256073, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.0014543554279953241}, {"id": 59, "seek": 36088, "start": 360.88, "end": 366.2, "text": " In the next part, I want to blend how to work with all this data.", "tokens": [50364, 682, 264, 958, 644, 11, 286, 528, 281, 10628, 577, 281, 589, 365, 439, 341, 1412, 13, 50630], "temperature": 0.0, "avg_logprob": -0.21111649203013225, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.07464714348316193}, {"id": 60, "seek": 36088, "start": 366.2, "end": 371.71999999999997, "text": " We always aim to make data as much as the data publicly available.", "tokens": [50630, 492, 1009, 5939, 281, 652, 1412, 382, 709, 382, 264, 1412, 14843, 2435, 13, 50906], "temperature": 0.0, "avg_logprob": -0.21111649203013225, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.07464714348316193}, {"id": 61, "seek": 36088, "start": 371.71999999999997, "end": 379.28, "text": " However, that doesn't necessarily mean it is accessible because it might still require", "tokens": [50906, 2908, 11, 300, 1177, 380, 4725, 914, 309, 307, 9515, 570, 309, 1062, 920, 3651, 51284], "temperature": 0.0, "avg_logprob": -0.21111649203013225, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.07464714348316193}, {"id": 62, "seek": 36088, "start": 379.28, "end": 384.0, "text": " a lot of technical expertise to effectively work with this data.", "tokens": [51284, 257, 688, 295, 6191, 11769, 281, 8659, 589, 365, 341, 1412, 13, 51520], "temperature": 0.0, "avg_logprob": -0.21111649203013225, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.07464714348316193}, {"id": 63, "seek": 36088, "start": 384.0, "end": 390.4, "text": " Therefore, we try to build tools to lower the technical barriers.", "tokens": [51520, 7504, 11, 321, 853, 281, 1322, 3873, 281, 3126, 264, 6191, 13565, 13, 51840], "temperature": 0.0, "avg_logprob": -0.21111649203013225, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.07464714348316193}, {"id": 64, "seek": 39040, "start": 390.4, "end": 397.91999999999996, "text": " And here I want to present one such example related to the HTML dump data set.", "tokens": [50364, 400, 510, 286, 528, 281, 1974, 472, 1270, 1365, 4077, 281, 264, 17995, 11430, 1412, 992, 13, 50740], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 65, "seek": 39040, "start": 397.91999999999996, "end": 398.91999999999996, "text": " What is this?", "tokens": [50740, 708, 307, 341, 30, 50790], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 66, "seek": 39040, "start": 398.91999999999996, "end": 406.03999999999996, "text": " This is a new dump data set available since October 2021, and it's now published and", "tokens": [50790, 639, 307, 257, 777, 11430, 1412, 992, 2435, 1670, 7617, 7201, 11, 293, 309, 311, 586, 6572, 293, 51146], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 67, "seek": 39040, "start": 406.03999999999996, "end": 408.76, "text": " updated in regular intervals.", "tokens": [51146, 10588, 294, 3890, 26651, 13, 51282], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 68, "seek": 39040, "start": 408.76, "end": 415.0, "text": " And it contains the HTML version of all articles of Wikipedia.", "tokens": [51282, 400, 309, 8306, 264, 17995, 3037, 295, 439, 11290, 295, 28999, 13, 51594], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 69, "seek": 39040, "start": 415.0, "end": 418.23999999999995, "text": " Why is this so exciting?", "tokens": [51594, 1545, 307, 341, 370, 4670, 30, 51756], "temperature": 0.0, "avg_logprob": -0.18434347604450427, "compression_ratio": 1.4532019704433496, "no_speech_prob": 0.0013235441874712706}, {"id": 70, "seek": 41824, "start": 418.24, "end": 424.48, "text": " Traditional dumps, when we are using the traditional dumps, the content of the articles is only", "tokens": [50364, 46738, 11430, 82, 11, 562, 321, 366, 1228, 264, 5164, 11430, 82, 11, 264, 2701, 295, 264, 11290, 307, 787, 50676], "temperature": 0.0, "avg_logprob": -0.1834500668996788, "compression_ratio": 1.7262569832402235, "no_speech_prob": 0.0060919225215911865}, {"id": 71, "seek": 41824, "start": 424.48, "end": 428.48, "text": " available in the WikiText markup.", "tokens": [50676, 2435, 294, 264, 35892, 50198, 1491, 1010, 13, 50876], "temperature": 0.0, "avg_logprob": -0.1834500668996788, "compression_ratio": 1.7262569832402235, "no_speech_prob": 0.0060919225215911865}, {"id": 72, "seek": 41824, "start": 428.48, "end": 432.48, "text": " This is what you see when you edit the source of an article.", "tokens": [50876, 639, 307, 437, 291, 536, 562, 291, 8129, 264, 4009, 295, 364, 7222, 13, 51076], "temperature": 0.0, "avg_logprob": -0.1834500668996788, "compression_ratio": 1.7262569832402235, "no_speech_prob": 0.0060919225215911865}, {"id": 73, "seek": 41824, "start": 432.48, "end": 439.32, "text": " However, what you see as a reader when browsing is not the WikiText markup, but the WikiText", "tokens": [51076, 2908, 11, 437, 291, 536, 382, 257, 15149, 562, 38602, 307, 406, 264, 35892, 50198, 1491, 1010, 11, 457, 264, 35892, 50198, 51418], "temperature": 0.0, "avg_logprob": -0.1834500668996788, "compression_ratio": 1.7262569832402235, "no_speech_prob": 0.0060919225215911865}, {"id": 74, "seek": 41824, "start": 439.32, "end": 442.36, "text": " gets parsed into an HTML.", "tokens": [51418, 2170, 21156, 292, 666, 364, 17995, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1834500668996788, "compression_ratio": 1.7262569832402235, "no_speech_prob": 0.0060919225215911865}, {"id": 75, "seek": 44236, "start": 442.36, "end": 450.04, "text": " The problem is the WikiText does not explicitly contain all the elements that are visible", "tokens": [50364, 440, 1154, 307, 264, 35892, 50198, 775, 406, 20803, 5304, 439, 264, 4959, 300, 366, 8974, 50748], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 76, "seek": 44236, "start": 450.04, "end": 451.6, "text": " in the HTML.", "tokens": [50748, 294, 264, 17995, 13, 50826], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 77, "seek": 44236, "start": 451.6, "end": 456.04, "text": " This comes mainly from parsing of templates or info boxes.", "tokens": [50826, 639, 1487, 8704, 490, 21156, 278, 295, 21165, 420, 13614, 9002, 13, 51048], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 78, "seek": 44236, "start": 456.04, "end": 461.2, "text": " This becomes an issue for researchers studying the content of articles because they will", "tokens": [51048, 639, 3643, 364, 2734, 337, 10309, 7601, 264, 2701, 295, 11290, 570, 436, 486, 51306], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 79, "seek": 44236, "start": 461.2, "end": 466.12, "text": " miss many of the elements when only when looking at the WikiText.", "tokens": [51306, 1713, 867, 295, 264, 4959, 562, 787, 562, 1237, 412, 264, 35892, 50198, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 80, "seek": 44236, "start": 466.12, "end": 470.8, "text": " One example for this is when looking for hyperlinks in articles.", "tokens": [51552, 1485, 1365, 337, 341, 307, 562, 1237, 337, 9848, 75, 16431, 294, 11290, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10275012796575372, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.004397537559270859}, {"id": 81, "seek": 47080, "start": 470.8, "end": 478.6, "text": " One study by Mitrovsky looked at counting the number of links in articles and found", "tokens": [50364, 1485, 2979, 538, 10821, 24088, 25810, 2956, 412, 13251, 264, 1230, 295, 6123, 294, 11290, 293, 1352, 50754], "temperature": 0.0, "avg_logprob": -0.1820855206006194, "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.0036444985307753086}, {"id": 82, "seek": 47080, "start": 478.6, "end": 485.64, "text": " that WikiText contains less than half of the links that are visible in the HTML version", "tokens": [50754, 300, 35892, 50198, 8306, 1570, 813, 1922, 295, 264, 6123, 300, 366, 8974, 294, 264, 17995, 3037, 51106], "temperature": 0.0, "avg_logprob": -0.1820855206006194, "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.0036444985307753086}, {"id": 83, "seek": 47080, "start": 485.64, "end": 487.28000000000003, "text": " of the reader.", "tokens": [51106, 295, 264, 15149, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1820855206006194, "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.0036444985307753086}, {"id": 84, "seek": 47080, "start": 487.28000000000003, "end": 493.44, "text": " So we can conclude that researchers should use the HTML dumps because they capture more", "tokens": [51188, 407, 321, 393, 16886, 300, 10309, 820, 764, 264, 17995, 11430, 82, 570, 436, 7983, 544, 51496], "temperature": 0.0, "avg_logprob": -0.1820855206006194, "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.0036444985307753086}, {"id": 85, "seek": 47080, "start": 493.44, "end": 496.44, "text": " accurately the content of the article.", "tokens": [51496, 20095, 264, 2701, 295, 264, 7222, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1820855206006194, "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.0036444985307753086}, {"id": 86, "seek": 49644, "start": 496.71999999999997, "end": 503.71999999999997, "text": " However, the challenge is how to parse the HTML dumps or the articles in the HTML dumps", "tokens": [50378, 2908, 11, 264, 3430, 307, 577, 281, 48377, 264, 17995, 11430, 82, 420, 264, 11290, 294, 264, 17995, 11430, 82, 50728], "temperature": 0.0, "avg_logprob": -0.2141759327479771, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.004672410897910595}, {"id": 87, "seek": 49644, "start": 503.71999999999997, "end": 504.71999999999997, "text": " version.", "tokens": [50728, 3037, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2141759327479771, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.004672410897910595}, {"id": 88, "seek": 49644, "start": 504.71999999999997, "end": 511.16, "text": " This is not just about knowing HTML, but it's also about very specific knowledge about", "tokens": [50778, 639, 307, 406, 445, 466, 5276, 17995, 11, 457, 309, 311, 611, 466, 588, 2685, 3601, 466, 51100], "temperature": 0.0, "avg_logprob": -0.2141759327479771, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.004672410897910595}, {"id": 89, "seek": 49644, "start": 511.16, "end": 518.4, "text": " how the media Wiki software translates different Wiki elements and how they will appear in", "tokens": [51100, 577, 264, 3021, 35892, 4722, 28468, 819, 35892, 4959, 293, 577, 436, 486, 4204, 294, 51462], "temperature": 0.0, "avg_logprob": -0.2141759327479771, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.004672410897910595}, {"id": 90, "seek": 49644, "start": 518.4, "end": 520.84, "text": " the HTML version.", "tokens": [51462, 264, 17995, 3037, 13, 51584], "temperature": 0.0, "avg_logprob": -0.2141759327479771, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.004672410897910595}, {"id": 91, "seek": 52084, "start": 520.84, "end": 526.88, "text": " Existing packages exist for WikiText, but not for HTML.", "tokens": [50364, 2111, 468, 278, 17401, 2514, 337, 35892, 50198, 11, 457, 406, 337, 17995, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2047344692169674, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.028407761827111244}, {"id": 92, "seek": 52084, "start": 526.88, "end": 534.8000000000001, "text": " Therefore, this is a very high barrier for practitioners to switch their existing pipelines", "tokens": [50666, 7504, 11, 341, 307, 257, 588, 1090, 13357, 337, 25742, 281, 3679, 641, 6741, 40168, 51062], "temperature": 0.0, "avg_logprob": -0.2047344692169674, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.028407761827111244}, {"id": 93, "seek": 52084, "start": 534.8000000000001, "end": 537.4, "text": " to use this new dataset.", "tokens": [51062, 281, 764, 341, 777, 28872, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2047344692169674, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.028407761827111244}, {"id": 94, "seek": 52084, "start": 537.4, "end": 544.84, "text": " Our solution was to build a Python library to make working with these dumps very easily.", "tokens": [51192, 2621, 3827, 390, 281, 1322, 257, 15329, 6405, 281, 652, 1364, 365, 613, 11430, 82, 588, 3612, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2047344692169674, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.028407761827111244}, {"id": 95, "seek": 54484, "start": 544.84, "end": 551.9200000000001, "text": " We called it MWParser from HTML, and it parses HTML and extracts elements of an article", "tokens": [50364, 492, 1219, 309, 376, 54, 47, 685, 260, 490, 17995, 11, 293, 309, 21156, 279, 17995, 293, 8947, 82, 4959, 295, 364, 7222, 50718], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 96, "seek": 54484, "start": 551.9200000000001, "end": 559.64, "text": " such as links, references, templates, or the plain text without the user having to know", "tokens": [50718, 1270, 382, 6123, 11, 15400, 11, 21165, 11, 420, 264, 11121, 2487, 1553, 264, 4195, 1419, 281, 458, 51104], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 97, "seek": 54484, "start": 559.64, "end": 565.48, "text": " anything about HTML and the way Wiki elements appear in it.", "tokens": [51104, 1340, 466, 17995, 293, 264, 636, 35892, 4959, 4204, 294, 309, 13, 51396], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 98, "seek": 54484, "start": 565.48, "end": 568.76, "text": " We recently released the first version of this.", "tokens": [51396, 492, 3938, 4736, 264, 700, 3037, 295, 341, 13, 51560], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 99, "seek": 54484, "start": 568.76, "end": 570.32, "text": " This is work in progress.", "tokens": [51560, 639, 307, 589, 294, 4205, 13, 51638], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 100, "seek": 54484, "start": 570.32, "end": 572.0400000000001, "text": " There's tons of open issues.", "tokens": [51638, 821, 311, 9131, 295, 1269, 2663, 13, 51724], "temperature": 0.0, "avg_logprob": -0.15324697601661252, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.03669445961713791}, {"id": 101, "seek": 57204, "start": 572.04, "end": 579.28, "text": " So if you're interested, contributions from anyone are very, very welcome to improve this", "tokens": [50364, 407, 498, 291, 434, 3102, 11, 15725, 490, 2878, 366, 588, 11, 588, 2928, 281, 3470, 341, 50726], "temperature": 0.0, "avg_logprob": -0.15402168035507202, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.0004105877014808357}, {"id": 102, "seek": 57204, "start": 579.28, "end": 580.5999999999999, "text": " in the future.", "tokens": [50726, 294, 264, 2027, 13, 50792], "temperature": 0.0, "avg_logprob": -0.15402168035507202, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.0004105877014808357}, {"id": 103, "seek": 57204, "start": 580.5999999999999, "end": 584.7199999999999, "text": " Check out the repo on GitLab for more information.", "tokens": [50792, 6881, 484, 264, 49040, 322, 16939, 37880, 337, 544, 1589, 13, 50998], "temperature": 0.0, "avg_logprob": -0.15402168035507202, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.0004105877014808357}, {"id": 104, "seek": 57204, "start": 584.7199999999999, "end": 592.64, "text": " As a third step, I want to mention, present how we use these datasets in practice.", "tokens": [50998, 1018, 257, 2636, 1823, 11, 286, 528, 281, 2152, 11, 1974, 577, 321, 764, 613, 42856, 294, 3124, 13, 51394], "temperature": 0.0, "avg_logprob": -0.15402168035507202, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.0004105877014808357}, {"id": 105, "seek": 57204, "start": 592.64, "end": 601.1999999999999, "text": " I want to show one example in the context of knowledge integrity in order to ensure quality", "tokens": [51394, 286, 528, 281, 855, 472, 1365, 294, 264, 4319, 295, 3601, 16000, 294, 1668, 281, 5586, 3125, 51822], "temperature": 0.0, "avg_logprob": -0.15402168035507202, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.0004105877014808357}, {"id": 106, "seek": 60120, "start": 601.2, "end": 602.88, "text": " of articles in Wikipedia.", "tokens": [50364, 295, 11290, 294, 28999, 13, 50448], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 107, "seek": 60120, "start": 602.88, "end": 609.12, "text": " There are many, many editors who try to review the edits that are made to articles in Wikipedia", "tokens": [50448, 821, 366, 867, 11, 867, 31446, 567, 853, 281, 3131, 264, 41752, 300, 366, 1027, 281, 11290, 294, 28999, 50760], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 108, "seek": 60120, "start": 609.12, "end": 614.36, "text": " and try to check whether these edits are okay or whether they're not okay and what should", "tokens": [50760, 293, 853, 281, 1520, 1968, 613, 41752, 366, 1392, 420, 1968, 436, 434, 406, 1392, 293, 437, 820, 51022], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 109, "seek": 60120, "start": 614.36, "end": 615.5600000000001, "text": " be reverted.", "tokens": [51022, 312, 319, 18537, 13, 51082], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 110, "seek": 60120, "start": 615.5600000000001, "end": 619.4000000000001, "text": " The problem is there are a lot of edits happening.", "tokens": [51082, 440, 1154, 307, 456, 366, 257, 688, 295, 41752, 2737, 13, 51274], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 111, "seek": 60120, "start": 619.4000000000001, "end": 626.6800000000001, "text": " So just in English Wikipedia, there's around 100,000 edits per day to work through.", "tokens": [51274, 407, 445, 294, 3669, 28999, 11, 456, 311, 926, 2319, 11, 1360, 41752, 680, 786, 281, 589, 807, 13, 51638], "temperature": 0.0, "avg_logprob": -0.21709198421902126, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.05096076428890228}, {"id": 112, "seek": 62668, "start": 626.68, "end": 633.28, "text": " And the aim is, can we build a tool to support editors in dealing with the large volume of", "tokens": [50364, 400, 264, 5939, 307, 11, 393, 321, 1322, 257, 2290, 281, 1406, 31446, 294, 6260, 365, 264, 2416, 5523, 295, 50694], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 113, "seek": 62668, "start": 633.28, "end": 634.28, "text": " edits?", "tokens": [50694, 41752, 30, 50744], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 114, "seek": 62668, "start": 634.28, "end": 640.7199999999999, "text": " Can we help them identify the very bad edits more easily?", "tokens": [50744, 1664, 321, 854, 552, 5876, 264, 588, 1578, 41752, 544, 3612, 30, 51066], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 115, "seek": 62668, "start": 640.7199999999999, "end": 646.3199999999999, "text": " And this is what we do with a so-called risk revert model.", "tokens": [51066, 400, 341, 307, 437, 321, 360, 365, 257, 370, 12, 11880, 3148, 319, 3281, 2316, 13, 51346], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 116, "seek": 62668, "start": 646.3199999999999, "end": 647.3199999999999, "text": " What is this?", "tokens": [51346, 708, 307, 341, 30, 51396], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 117, "seek": 62668, "start": 647.3199999999999, "end": 653.3199999999999, "text": " So we look at an edit by comparing the old version of an article with its new version.", "tokens": [51396, 407, 321, 574, 412, 364, 8129, 538, 15763, 264, 1331, 3037, 295, 364, 7222, 365, 1080, 777, 3037, 13, 51696], "temperature": 0.0, "avg_logprob": -0.12620041288178543, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.01470929104834795}, {"id": 118, "seek": 65332, "start": 653.32, "end": 659.44, "text": " And we would like to make a prediction whether the change is good or whether it is a very", "tokens": [50364, 400, 321, 576, 411, 281, 652, 257, 17630, 1968, 264, 1319, 307, 665, 420, 1968, 309, 307, 257, 588, 50670], "temperature": 0.0, "avg_logprob": -0.1586013397613129, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.0038199846167117357}, {"id": 119, "seek": 65332, "start": 659.44, "end": 663.6800000000001, "text": " bad edit and it should be reverted.", "tokens": [50670, 1578, 8129, 293, 309, 820, 312, 319, 18537, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1586013397613129, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.0038199846167117357}, {"id": 120, "seek": 65332, "start": 663.6800000000001, "end": 667.5200000000001, "text": " How we do this is we extract different features from this article.", "tokens": [50882, 1012, 321, 360, 341, 307, 321, 8947, 819, 4122, 490, 341, 7222, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1586013397613129, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.0038199846167117357}, {"id": 121, "seek": 65332, "start": 667.5200000000001, "end": 672.5200000000001, "text": " So which text was changed, where their links that were removed, where their images that", "tokens": [51074, 407, 597, 2487, 390, 3105, 11, 689, 641, 6123, 300, 645, 7261, 11, 689, 641, 5267, 300, 51324], "temperature": 0.0, "avg_logprob": -0.1586013397613129, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.0038199846167117357}, {"id": 122, "seek": 65332, "start": 672.5200000000001, "end": 675.88, "text": " were removed, and so on.", "tokens": [51324, 645, 7261, 11, 293, 370, 322, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1586013397613129, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.0038199846167117357}, {"id": 123, "seek": 67588, "start": 675.88, "end": 685.48, "text": " And then we built a model by looking into the history of all Wikipedia edits and extract", "tokens": [50364, 400, 550, 321, 3094, 257, 2316, 538, 1237, 666, 264, 2503, 295, 439, 28999, 41752, 293, 8947, 50844], "temperature": 0.0, "avg_logprob": -0.12913118264614007, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.03406943380832672}, {"id": 124, "seek": 67588, "start": 685.48, "end": 693.4, "text": " those edits which have been reverted by editors and use that as a ground truth of bad edits", "tokens": [50844, 729, 41752, 597, 362, 668, 319, 18537, 538, 31446, 293, 764, 300, 382, 257, 2727, 3494, 295, 1578, 41752, 51240], "temperature": 0.0, "avg_logprob": -0.12913118264614007, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.03406943380832672}, {"id": 125, "seek": 67588, "start": 693.4, "end": 695.2, "text": " for our model.", "tokens": [51240, 337, 527, 2316, 13, 51330], "temperature": 0.0, "avg_logprob": -0.12913118264614007, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.03406943380832672}, {"id": 126, "seek": 67588, "start": 695.2, "end": 703.92, "text": " And the resulting output is that we can, for each of these edits, we can calculate a so-called", "tokens": [51330, 400, 264, 16505, 5598, 307, 300, 321, 393, 11, 337, 1184, 295, 613, 41752, 11, 321, 393, 8873, 257, 370, 12, 11880, 51766], "temperature": 0.0, "avg_logprob": -0.12913118264614007, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.03406943380832672}, {"id": 127, "seek": 67588, "start": 703.92, "end": 705.24, "text": " revert risk.", "tokens": [51766, 319, 3281, 3148, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12913118264614007, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.03406943380832672}, {"id": 128, "seek": 70524, "start": 705.24, "end": 713.0, "text": " This is a very bad edit, will have a very high probability, a very high risk for being", "tokens": [50364, 639, 307, 257, 588, 1578, 8129, 11, 486, 362, 257, 588, 1090, 8482, 11, 257, 588, 1090, 3148, 337, 885, 50752], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 129, "seek": 70524, "start": 713.0, "end": 714.0, "text": " reverted.", "tokens": [50752, 319, 18537, 13, 50802], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 130, "seek": 70524, "start": 714.0, "end": 716.72, "text": " And this is what our model will output.", "tokens": [50802, 400, 341, 307, 437, 527, 2316, 486, 5598, 13, 50938], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 131, "seek": 70524, "start": 716.72, "end": 721.12, "text": " And our model performs fairly well.", "tokens": [50938, 400, 527, 2316, 26213, 6457, 731, 13, 51158], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 132, "seek": 70524, "start": 721.12, "end": 725.6800000000001, "text": " It has an accuracy between 70 and 80%.", "tokens": [51158, 467, 575, 364, 14170, 1296, 5285, 293, 4688, 6856, 51386], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 133, "seek": 70524, "start": 725.6800000000001, "end": 729.44, "text": " And I want to mention that we consider this OK.", "tokens": [51386, 400, 286, 528, 281, 2152, 300, 321, 1949, 341, 2264, 13, 51574], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 134, "seek": 70524, "start": 729.44, "end": 731.72, "text": " It does not need to be perfect.", "tokens": [51574, 467, 775, 406, 643, 281, 312, 2176, 13, 51688], "temperature": 0.0, "avg_logprob": -0.22210663485239787, "compression_ratio": 1.515625, "no_speech_prob": 0.0005101453862152994}, {"id": 135, "seek": 73172, "start": 731.72, "end": 738.48, "text": " Our model, the way our model is used is there's editors that will surface these scores to help", "tokens": [50364, 2621, 2316, 11, 264, 636, 527, 2316, 307, 1143, 307, 456, 311, 31446, 300, 486, 3753, 613, 13444, 281, 854, 50702], "temperature": 0.0, "avg_logprob": -0.17308444664126538, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.000895811477676034}, {"id": 136, "seek": 73172, "start": 738.48, "end": 747.2, "text": " editors identify at which edits they should take a closer look.", "tokens": [50702, 31446, 5876, 412, 597, 41752, 436, 820, 747, 257, 4966, 574, 13, 51138], "temperature": 0.0, "avg_logprob": -0.17308444664126538, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.000895811477676034}, {"id": 137, "seek": 73172, "start": 747.2, "end": 754.08, "text": " Similar models for annotating content of articles exist.", "tokens": [51138, 10905, 5245, 337, 25339, 990, 2701, 295, 11290, 2514, 13, 51482], "temperature": 0.0, "avg_logprob": -0.17308444664126538, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.000895811477676034}, {"id": 138, "seek": 73172, "start": 754.08, "end": 757.08, "text": " We have been developing these types of models.", "tokens": [51482, 492, 362, 668, 6416, 613, 3467, 295, 5245, 13, 51632], "temperature": 0.0, "avg_logprob": -0.17308444664126538, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.000895811477676034}, {"id": 139, "seek": 75708, "start": 757.08, "end": 763.2, "text": " In addition to knowledge integrity, what I presented, we have been trying to build models", "tokens": [50364, 682, 4500, 281, 3601, 16000, 11, 437, 286, 8212, 11, 321, 362, 668, 1382, 281, 1322, 5245, 50670], "temperature": 0.0, "avg_logprob": -0.18222089009742215, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.002930138725787401}, {"id": 140, "seek": 75708, "start": 763.2, "end": 771.0400000000001, "text": " for finding easily similar articles, for identifying automatically the topic of an article to assess", "tokens": [50670, 337, 5006, 3612, 2531, 11290, 11, 337, 16696, 6772, 264, 4829, 295, 364, 7222, 281, 5877, 51062], "temperature": 0.0, "avg_logprob": -0.18222089009742215, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.002930138725787401}, {"id": 141, "seek": 75708, "start": 771.0400000000001, "end": 779.96, "text": " its readability or geography, or identifying related images, et cetera.", "tokens": [51062, 1080, 1401, 2310, 420, 26695, 11, 420, 16696, 4077, 5267, 11, 1030, 11458, 13, 51508], "temperature": 0.0, "avg_logprob": -0.18222089009742215, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.002930138725787401}, {"id": 142, "seek": 75708, "start": 779.96, "end": 786.4000000000001, "text": " I only want to briefly highlight that the development of these models is rooted in some", "tokens": [51508, 286, 787, 528, 281, 10515, 5078, 300, 264, 3250, 295, 613, 5245, 307, 25277, 294, 512, 51830], "temperature": 0.0, "avg_logprob": -0.18222089009742215, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.002930138725787401}, {"id": 143, "seek": 78640, "start": 786.4, "end": 790.0799999999999, "text": " core principles to which we are committed to.", "tokens": [50364, 4965, 9156, 281, 597, 321, 366, 7784, 281, 13, 50548], "temperature": 0.0, "avg_logprob": -0.2189774380789863, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.0048194569535553455}, {"id": 144, "seek": 78640, "start": 790.0799999999999, "end": 795.0799999999999, "text": " And this can create additional challenges in developing this model, specifically this", "tokens": [50548, 400, 341, 393, 1884, 4497, 4759, 294, 6416, 341, 2316, 11, 4682, 341, 50798], "temperature": 0.0, "avg_logprob": -0.2189774380789863, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.0048194569535553455}, {"id": 145, "seek": 78640, "start": 795.0799999999999, "end": 802.76, "text": " context I want to highlight a multilingual aspect so that we always try to prefer language", "tokens": [50798, 4319, 286, 528, 281, 5078, 257, 2120, 38219, 4171, 370, 300, 321, 1009, 853, 281, 4382, 2856, 51182], "temperature": 0.0, "avg_logprob": -0.2189774380789863, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.0048194569535553455}, {"id": 146, "seek": 78640, "start": 802.76, "end": 811.0799999999999, "text": " agnostic approaches in order to support as many as possible of the 300 different language", "tokens": [51182, 623, 77, 19634, 11587, 294, 1668, 281, 1406, 382, 867, 382, 1944, 295, 264, 6641, 819, 2856, 51598], "temperature": 0.0, "avg_logprob": -0.2189774380789863, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.0048194569535553455}, {"id": 147, "seek": 78640, "start": 811.0799999999999, "end": 815.04, "text": " versions in Wikipedia.", "tokens": [51598, 9606, 294, 28999, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2189774380789863, "compression_ratio": 1.5296803652968036, "no_speech_prob": 0.0048194569535553455}, {"id": 148, "seek": 81504, "start": 815.24, "end": 827.0799999999999, "text": " I want to conclude with potential ways in which to contribute in any of these three areas that I mentioned previously.", "tokens": [50374, 286, 528, 281, 16886, 365, 3995, 2098, 294, 597, 281, 10586, 294, 604, 295, 613, 1045, 3179, 300, 286, 2835, 8046, 13, 50966], "temperature": 0.0, "avg_logprob": -0.23359208636813694, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.001568756764754653}, {"id": 149, "seek": 81504, "start": 827.0799999999999, "end": 837.04, "text": " Generally, one can contribute as a developer to media wiki or other aspects of the wiki media ecosystem.", "tokens": [50966, 21082, 11, 472, 393, 10586, 382, 257, 10754, 281, 3021, 261, 9850, 420, 661, 7270, 295, 264, 261, 9850, 3021, 11311, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23359208636813694, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.001568756764754653}, {"id": 150, "seek": 81504, "start": 837.04, "end": 842.0799999999999, "text": " And there, the place to get started is the so-called developer portal, which is a centralized", "tokens": [51464, 400, 456, 11, 264, 1081, 281, 483, 1409, 307, 264, 370, 12, 11880, 10754, 14982, 11, 597, 307, 257, 32395, 51716], "temperature": 0.0, "avg_logprob": -0.23359208636813694, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.001568756764754653}, {"id": 151, "seek": 84208, "start": 842.08, "end": 849.2, "text": " entry point for finding technical documentation and community resources.", "tokens": [50364, 8729, 935, 337, 5006, 6191, 14333, 293, 1768, 3593, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16548528426732773, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.002276618266478181}, {"id": 152, "seek": 84208, "start": 849.2, "end": 854.0400000000001, "text": " Not going into more detail here, I want to give a shout out and refer to the talk by", "tokens": [50720, 1726, 516, 666, 544, 2607, 510, 11, 286, 528, 281, 976, 257, 8043, 484, 293, 2864, 281, 264, 751, 538, 50962], "temperature": 0.0, "avg_logprob": -0.16548528426732773, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.002276618266478181}, {"id": 153, "seek": 84208, "start": 854.0400000000001, "end": 861.24, "text": " my colleague Slavina Stefanova from the developer acquisition advocacy team.", "tokens": [50962, 452, 13532, 6187, 706, 1426, 43421, 3730, 2757, 490, 264, 10754, 21668, 22011, 1469, 13, 51322], "temperature": 0.0, "avg_logprob": -0.16548528426732773, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.002276618266478181}, {"id": 154, "seek": 84208, "start": 861.24, "end": 867.0, "text": " But specifically in the area of research, I want to highlight a few entry points depending", "tokens": [51322, 583, 4682, 294, 264, 1859, 295, 2132, 11, 286, 528, 281, 5078, 257, 1326, 8729, 2793, 5413, 51610], "temperature": 0.0, "avg_logprob": -0.16548528426732773, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.002276618266478181}, {"id": 155, "seek": 84208, "start": 867.0, "end": 869.08, "text": " on your interest.", "tokens": [51610, 322, 428, 1179, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16548528426732773, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.002276618266478181}, {"id": 156, "seek": 86908, "start": 869.08, "end": 875.84, "text": " In case you would like to build a specific tool, there is wiki media foundations toolforge", "tokens": [50364, 682, 1389, 291, 576, 411, 281, 1322, 257, 2685, 2290, 11, 456, 307, 261, 9850, 3021, 22467, 2290, 2994, 432, 50702], "temperature": 0.0, "avg_logprob": -0.1603085199991862, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0011864911066368222}, {"id": 157, "seek": 86908, "start": 875.84, "end": 883.9200000000001, "text": " infrastructure and that is a hosting environment that allows you to run bots or different APIs", "tokens": [50702, 6896, 293, 300, 307, 257, 16058, 2823, 300, 4045, 291, 281, 1190, 35410, 420, 819, 21445, 51106], "temperature": 0.0, "avg_logprob": -0.1603085199991862, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0011864911066368222}, {"id": 158, "seek": 86908, "start": 883.9200000000001, "end": 890.88, "text": " in case you would like to provide that tool to the public.", "tokens": [51106, 294, 1389, 291, 576, 411, 281, 2893, 300, 2290, 281, 264, 1908, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1603085199991862, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0011864911066368222}, {"id": 159, "seek": 86908, "start": 890.88, "end": 898.08, "text": " If you want to work with us on improving tools or algorithms, you can check out the different", "tokens": [51454, 759, 291, 528, 281, 589, 365, 505, 322, 11470, 3873, 420, 14642, 11, 291, 393, 1520, 484, 264, 819, 51814], "temperature": 0.0, "avg_logprob": -0.1603085199991862, "compression_ratio": 1.648780487804878, "no_speech_prob": 0.0011864911066368222}, {"id": 160, "seek": 89808, "start": 898.08, "end": 903.2800000000001, "text": " packages that we have been releasing in the past months.", "tokens": [50364, 17401, 300, 321, 362, 668, 16327, 294, 264, 1791, 2493, 13, 50624], "temperature": 0.0, "avg_logprob": -0.19223256264963456, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0036445511505007744}, {"id": 161, "seek": 89808, "start": 903.2800000000001, "end": 906.12, "text": " These are all work in progress.", "tokens": [50624, 1981, 366, 439, 589, 294, 4205, 13, 50766], "temperature": 0.0, "avg_logprob": -0.19223256264963456, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0036445511505007744}, {"id": 162, "seek": 89808, "start": 906.12, "end": 915.2, "text": " There's many open issues and we're happy about any contributions about improving, fixing", "tokens": [50766, 821, 311, 867, 1269, 2663, 293, 321, 434, 2055, 466, 604, 15725, 466, 11470, 11, 19442, 51220], "temperature": 0.0, "avg_logprob": -0.19223256264963456, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0036445511505007744}, {"id": 163, "seek": 89808, "start": 915.2, "end": 919.5600000000001, "text": " existing issues or even finding new bugs.", "tokens": [51220, 6741, 2663, 420, 754, 5006, 777, 15120, 13, 51438], "temperature": 0.0, "avg_logprob": -0.19223256264963456, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0036445511505007744}, {"id": 164, "seek": 89808, "start": 919.5600000000001, "end": 924.6800000000001, "text": " So please check out our repository too.", "tokens": [51438, 407, 1767, 1520, 484, 527, 25841, 886, 13, 51694], "temperature": 0.0, "avg_logprob": -0.19223256264963456, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0036445511505007744}, {"id": 165, "seek": 92468, "start": 924.68, "end": 930.3199999999999, "text": " If you are interested in getting funding, there are different opportunities.", "tokens": [50364, 759, 291, 366, 3102, 294, 1242, 6137, 11, 456, 366, 819, 4786, 13, 50646], "temperature": 0.0, "avg_logprob": -0.19981020375301964, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007045310456305742}, {"id": 166, "seek": 92468, "start": 930.3199999999999, "end": 936.7199999999999, "text": " There is an existing program to fund research around wiki media projects.", "tokens": [50646, 821, 307, 364, 6741, 1461, 281, 2374, 2132, 926, 261, 9850, 3021, 4455, 13, 50966], "temperature": 0.0, "avg_logprob": -0.19981020375301964, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007045310456305742}, {"id": 167, "seek": 92468, "start": 936.7199999999999, "end": 943.0, "text": " This covers many different disciplines, humanities, social science, computer science, education,", "tokens": [50966, 639, 10538, 867, 819, 21919, 11, 36140, 11, 2093, 3497, 11, 3820, 3497, 11, 3309, 11, 51280], "temperature": 0.0, "avg_logprob": -0.19981020375301964, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007045310456305742}, {"id": 168, "seek": 92468, "start": 943.0, "end": 951.3599999999999, "text": " law, et cetera, and is around work that has potential for direct positive impact on the", "tokens": [51280, 2101, 11, 1030, 11458, 11, 293, 307, 926, 589, 300, 575, 3995, 337, 2047, 3353, 2712, 322, 264, 51698], "temperature": 0.0, "avg_logprob": -0.19981020375301964, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007045310456305742}, {"id": 169, "seek": 92468, "start": 951.3599999999999, "end": 953.0799999999999, "text": " local communities.", "tokens": [51698, 2654, 4456, 13, 51784], "temperature": 0.0, "avg_logprob": -0.19981020375301964, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007045310456305742}, {"id": 170, "seek": 95308, "start": 953.08, "end": 959.5200000000001, "text": " In addition, I want to mention that coming in the future, there are plans for a similar", "tokens": [50364, 682, 4500, 11, 286, 528, 281, 2152, 300, 1348, 294, 264, 2027, 11, 456, 366, 5482, 337, 257, 2531, 50686], "temperature": 0.0, "avg_logprob": -0.12932990048382734, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.007326871156692505}, {"id": 171, "seek": 95308, "start": 959.5200000000001, "end": 965.0400000000001, "text": " program to improve wiki media's technology and tools.", "tokens": [50686, 1461, 281, 3470, 261, 9850, 3021, 311, 2899, 293, 3873, 13, 50962], "temperature": 0.0, "avg_logprob": -0.12932990048382734, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.007326871156692505}, {"id": 172, "seek": 95308, "start": 965.0400000000001, "end": 973.12, "text": " If you want to learn about the projects we are working on, I want to mention that we", "tokens": [50962, 759, 291, 528, 281, 1466, 466, 264, 4455, 321, 366, 1364, 322, 11, 286, 528, 281, 2152, 300, 321, 51366], "temperature": 0.0, "avg_logprob": -0.12932990048382734, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.007326871156692505}, {"id": 173, "seek": 95308, "start": 973.12, "end": 979.44, "text": " publish a research report, a summary of our ongoing research projects every six months", "tokens": [51366, 11374, 257, 2132, 2275, 11, 257, 12691, 295, 527, 10452, 2132, 4455, 633, 2309, 2493, 51682], "temperature": 0.0, "avg_logprob": -0.12932990048382734, "compression_ratio": 1.656084656084656, "no_speech_prob": 0.007326871156692505}, {"id": 174, "seek": 97944, "start": 979.44, "end": 985.6800000000001, "text": " and you can find more details about some of the projects that I have mentioned.", "tokens": [50364, 293, 291, 393, 915, 544, 4365, 466, 512, 295, 264, 4455, 300, 286, 362, 2835, 13, 50676], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 175, "seek": 97944, "start": 985.6800000000001, "end": 993.36, "text": " Finally, if you would like to engage with the research community, you can join us at", "tokens": [50676, 6288, 11, 498, 291, 576, 411, 281, 4683, 365, 264, 2132, 1768, 11, 291, 393, 3917, 505, 412, 51060], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 176, "seek": 97944, "start": 993.36, "end": 994.6400000000001, "text": " wiki workshop.", "tokens": [51060, 261, 9850, 13541, 13, 51124], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 177, "seek": 97944, "start": 994.6400000000001, "end": 999.0400000000001, "text": " This is the primary meeting venue of the wiki media research community.", "tokens": [51124, 639, 307, 264, 6194, 3440, 21645, 295, 264, 261, 9850, 3021, 2132, 1768, 13, 51344], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 178, "seek": 97944, "start": 999.0400000000001, "end": 1004.6, "text": " This year will be the 10th edition of wiki workshop and it is expected to be held in", "tokens": [51344, 639, 1064, 486, 312, 264, 1266, 392, 11377, 295, 261, 9850, 13541, 293, 309, 307, 5176, 281, 312, 5167, 294, 51622], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 179, "seek": 97944, "start": 1004.6, "end": 1005.7600000000001, "text": " May.", "tokens": [51622, 1891, 13, 51680], "temperature": 0.0, "avg_logprob": -0.16446344418959183, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.038263916969299316}, {"id": 180, "seek": 100576, "start": 1005.76, "end": 1008.12, "text": " You can submit your works there.", "tokens": [50364, 509, 393, 10315, 428, 1985, 456, 13, 50482], "temperature": 0.0, "avg_logprob": -0.14622904459635416, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.005544714629650116}, {"id": 181, "seek": 100576, "start": 1008.12, "end": 1010.8, "text": " I invite you for the submissions.", "tokens": [50482, 286, 7980, 291, 337, 264, 40429, 13, 50616], "temperature": 0.0, "avg_logprob": -0.14622904459635416, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.005544714629650116}, {"id": 182, "seek": 100576, "start": 1010.8, "end": 1020.08, "text": " We highly encourage ongoing or preliminary works by submitting extended abstracts.", "tokens": [50616, 492, 5405, 5373, 10452, 420, 28817, 1985, 538, 31836, 10913, 12649, 82, 13, 51080], "temperature": 0.0, "avg_logprob": -0.14622904459635416, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.005544714629650116}, {"id": 183, "seek": 100576, "start": 1020.08, "end": 1024.96, "text": " In this edition, there will also be a novel track for wiki media developers.", "tokens": [51080, 682, 341, 11377, 11, 456, 486, 611, 312, 257, 7613, 2837, 337, 261, 9850, 3021, 8849, 13, 51324], "temperature": 0.0, "avg_logprob": -0.14622904459635416, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.005544714629650116}, {"id": 184, "seek": 100576, "start": 1024.96, "end": 1031.12, "text": " If you are a developer of a tool or a system or an algorithm that could be of interest", "tokens": [51324, 759, 291, 366, 257, 10754, 295, 257, 2290, 420, 257, 1185, 420, 364, 9284, 300, 727, 312, 295, 1179, 51632], "temperature": 0.0, "avg_logprob": -0.14622904459635416, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.005544714629650116}, {"id": 185, "seek": 103112, "start": 1031.12, "end": 1036.9599999999998, "text": " to research on wiki media, please check it out and make a submission.", "tokens": [50364, 281, 2132, 322, 261, 9850, 3021, 11, 1767, 1520, 309, 484, 293, 652, 257, 23689, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 186, "seek": 103112, "start": 1036.9599999999998, "end": 1043.76, "text": " Even if you do not plan to make a submission, you are welcome to participate.", "tokens": [50656, 2754, 498, 291, 360, 406, 1393, 281, 652, 257, 23689, 11, 291, 366, 2928, 281, 8197, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 187, "seek": 103112, "start": 1043.76, "end": 1050.4399999999998, "text": " As done in the last three editions, wiki workshop will be fully virtual and attendance", "tokens": [50996, 1018, 1096, 294, 264, 1036, 1045, 44840, 11, 261, 9850, 13541, 486, 312, 4498, 6374, 293, 24337, 51330], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 188, "seek": 103112, "start": 1050.4399999999998, "end": 1053.1599999999999, "text": " will be free.", "tokens": [51330, 486, 312, 1737, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 189, "seek": 103112, "start": 1053.1599999999999, "end": 1055.9199999999998, "text": " With this, I want to conclude.", "tokens": [51466, 2022, 341, 11, 286, 528, 281, 16886, 13, 51604], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 190, "seek": 103112, "start": 1055.9199999999998, "end": 1058.9599999999998, "text": " I want to thank you very much for your attention.", "tokens": [51604, 286, 528, 281, 1309, 291, 588, 709, 337, 428, 3202, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1379652352168642, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.03056792914867401}, {"id": 191, "seek": 105896, "start": 1058.96, "end": 1063.48, "text": " I am looking forward to your questions in the Q&A.", "tokens": [50364, 286, 669, 1237, 2128, 281, 428, 1651, 294, 264, 1249, 5, 32, 13, 50590], "temperature": 0.0, "avg_logprob": -0.20328551369744377, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03970377519726753}, {"id": 192, "seek": 105896, "start": 1063.48, "end": 1069.28, "text": " If you want to stay in touch, feel free to reach out to me personally on my email or", "tokens": [50590, 759, 291, 528, 281, 1754, 294, 2557, 11, 841, 1737, 281, 2524, 484, 281, 385, 5665, 322, 452, 3796, 420, 50880], "temperature": 0.0, "avg_logprob": -0.20328551369744377, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03970377519726753}, {"id": 193, "seek": 105896, "start": 1069.28, "end": 1074.2, "text": " any of the other channels that I am listing here through office hours or mailing lists", "tokens": [50880, 604, 295, 264, 661, 9235, 300, 286, 669, 22161, 510, 807, 3398, 2496, 420, 41612, 14511, 51126], "temperature": 0.0, "avg_logprob": -0.20328551369744377, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03970377519726753}, {"id": 194, "seek": 105896, "start": 1074.2, "end": 1081.56, "text": " on IRC, et cetera, and with this, thank you very much.", "tokens": [51126, 322, 16486, 34, 11, 1030, 11458, 11, 293, 365, 341, 11, 1309, 291, 588, 709, 13, 51494], "temperature": 0.0, "avg_logprob": -0.20328551369744377, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03970377519726753}, {"id": 195, "seek": 108896, "start": 1088.96, "end": 1089.96, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.9574706077575683, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.9990795850753784}, {"id": 196, "seek": 108896, "start": 1089.96, "end": 1089.96, "text": "", "tokens": [], "temperature": 0.0, "avg_logprob": -0.9574706077575683, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.9990795850753784, "words": []}], "language": "en"}