{"text": " about migrating from proprietary to open source knowledge management tools. I'll talk a bit in general about the migration process and then demonstrate two migrations, one from conference and one from SharePoint with different technologies just to see what's possible on our side and in general. So first, why migrate from proprietary to open source? I'm sure everyone has a lot of good reasons for that in mind but some that we have identified as well would be first for privacy and security concerns. Of course, with open source software, you have more flexibility in where you host your data, how you host it, the environment and access that you set up. Two other concerns that are related and we have also seen some important examples in the recent years are vendor and data lock-in. So when you're using a proprietary software, you're a bit more vulnerable to the vendor than when you're using an open source software in the sense that if they make any significant strategy or pricing changes, you may find yourself in a situation where you need to migrate quickly or you need to quickly adjust to that change. A recent example is Confluence. Maybe some of you have stumbled across it. They had a cloud version and a server version and a data center but that's usually for larger teams. The server version was used by smaller teams that wanted to host the data on their premise and in late 2020, Confluence decided to stop the server version. So because of that, a lot of small companies had to either move to cloud, so host their data in the states, which wasn't possible for a lot of people, or move to data center, which was much more expensive and difficult to handle. And that was an example of how using proprietary software can make you quite vulnerable to their changes. One other thing that is important is the data lock-in. So using open source software usually allows you to migrate easier and integrate with certain tools due to open standards and protocols and formats. So if you're using an open source tool at some point and then you want to migrate to another, it may be easier than migrating from proprietary tools to other proprietary tools or open source ones. Then of course, accessibility, having access to the code and all the features may allow us to further extend it or contribute to it and implement other features. And finally, from a values or ethical point of view, when we are using open source, we kind of facilitate integration and technological growth for everyone, rather than focusing on products and standards that just have a small benefit or a benefit for one company. Okay, so with all these good reasons I hope in mind, I'm going to talk about how to approach migration. So in general, as a plan, I hope that's okay. So in general, as a plan, when we migrate from one knowledge management tool to another, but this can be kind of extended to other software as well, we first need to think about data format and dependencies. What type of data do we have? Do we have flat HTML pages? Do we have structured pages? Do we have any metadata tags? All that needs to be considered. Then we also need to look at the other extensions that we're using. What type of authentication do we have? Are we integrating with anything else that needs to be kept? Then once we have that listed, based on that, we can identify and set up the new software. For example, let's say that we have identified that on the confidence side, we can export to XML. Xfiki supports that and we want to do this migration. We need to set up the new software in the environment that works for us. Then we need to make the migration plan and clean up. This is of course quite specific to the software that you that you're migrating to and from. But in general, it's also interesting to know that it can be a good moment to clean up your data. For example, if you have a system that you have been using for 10 years, you may have a lot of obsolete data that can be cleaned up at this moment. Aside from the plan of migrating, you can also eliminate some pages or even reorganize the content if possible. Then of course, we need to execute the migration. Again, it depends on each organization or company if they executed with a team, if they executed with other services. But this is an important step that also needs to be planned and kind of realistically planned in time as well to say so. When we migrate itself, we also need to try to map the data and its structure. As I said previously, if you have some type of structured data, you will want to map that. Also, you will want to create the new integrations or dependencies or install them. Then finally, once that is done, we need to do a thorough testing of course before releasing the new system. Finally, delivering in production and a step that is quite commonly skipped would be the user training. If you have any sort of organization, you may have been in the situation where you had to change the software and people may have been resistant to using the new software or became a bit less efficient or didn't really know how to handle certain aspects. A bit of user training can be very helpful as well if you're changing your knowledge management tools. Now we know the general plan and we can see two examples in action. For the conference to xWiki migration, I'm going to demonstrate the XML export way. For the share point, we're going to see an example of CSV export. On the migration from conference to xWiki, we already had a lot of tools available to migrate. However, in recent years, we dedicated a bit more efforts to trying to make them as easy to use as possible. We had before the filter streams converter that also supported the conference format. Nowadays, we have an integrated migrator that has a couple of steps. We'll see them right away. For the macros, in conference, we have some macros like we have in other wiki systems or knowledge management tools. One concern while migrating were the macros that had to be supported or migrated properly. Because of that, we also developed a package of bridge macros, how we call them, or macros that were identical to what exists in conference and that would support migration. Of course, we don't have all the macros that exist in conference because both xWiki and conference are products that have been around for a really long time and they have their own specificities. That is a concern to keep in mind not only when migrating from conference to xWiki, but from any software. Again, this fits into the dependencies or applications that you're using part. Let's see how a migration works. Again, I'm going to use the integrated one, the integrated migrator, which reuses the original filter streams. What we did for this migrator itself, to make it, again, a bit nicer to use, we created this concept of a profile in which you can basically import each space separately and you'll have a bit of an analysis on what pages were imported, if you had any pages that caused issues and you're able to follow the process a bit better. You can name your profile however you want. If you want, you can also connect to the conference instance. This is not mandatory, but if you connect to the conference instance, it also gives you a pre-analysis of the pages that were already imported into xWiki, so that can be useful. If you're having conference cloud, your username and token would be username and the token that you get, the API token. If you're running conference server, the username would be your administrator username and the token will be your password. We have here, of course, a demo purpose conference instance. It's not what we use internally. Then we need to put the URL as well in order to map the URL schema. Let's take that as well. We don't have a custom URL path. This is important again for the URL schema and for their adaptation. If you have wiki in the path, that will be a standard URL. If you don't, if you have things like display or something else, that would be a custom URL. In this case, it's not applied. Then we need to mention the name, the key of the space that we're migrating. In this case, I made this dummy text demo space called FOS. This is the space that I want to import. Now I have my profile. Let's see if I'm connected. Yes, I can start the migration. The way in which the migration works is that you have a couple of steps. The first one is just the prerequisites. It would tell you what would be the requirements for migration. They usually apply for larger imports. In our case, we're just going to do a 7-megabyte zip. It's not that large. We don't need to adapt everything. Of course, in general, when you're running a migration, you need to have enough resources on the machine, enough memory, disk space and all that. Specifically, on the Xfiki side, you can also disable notifications in the Xfiki properties file. You can also disable some listeners if you know that you will be importing very large packages. The second step that I told you about previously with that analysis, if you are connected to the conference instance, you can see if you have on Xfiki any pages that already exist, so that if you have in the package that you're trying to import from conference pages that exist on Xfiki. We have some logs here. We can see that it looked at all the pages. We don't have on Xfiki pages that we're trying to import right now. In the third step, it's just to tell you to go to conference and export. It depends on what server version or conference or cloud version you have. In this case, it's a cloud version with XML, full or custom export. You can choose again between those two. I already have it downloaded, so I'm not going to download it again. At this step, you just have to import your export file. Let me show you the example to import. If you have it on the same server, you can also specify the source in the server. If you have Xfiki running on the same server that you have the files in, you can also specify it directly. All of this configuration is the filter streams configuration that you can adapt. It has some fields that are prefilled, but there's also a lot of power in other things that you can configure. For example, you can also import users. You can do user ID mapping. For example, if you have an LDAP that has generated on the conference site some random number IDs, and you want to map those to the new users that you have created on Xfiki, that's something you can do. Also, you can choose if you want to keep the author and history metadata and so on. You have some nice configuration that is quite granular. Once the configuration is done, you would import. This is the point where our documents are getting created. Because they configured it, we also have the history. For example, here you see this was created and then updated because on the conference side, I had multiple changes on those pages. Now, we see that we have the pages imported with no error. With no error, it's of course a great thing, but you can also have errors, of course. In our experience, the most common errors are caused by unsupported characters or corrupted pages on the conference side. If you are trying this out and you have some errors, the logs should tell you what is the page that is causing the issue. You can then fix it on the conference side and then re-import or fix it manually in Xfiki, whatever suits you best. Now, we have the pages imported. This is a post-import fixes check that we can also perform in case we have pages that were imported that don't have a parent or pages that have corrupted parents. Both in Confluence and Xfiki, we have the hierarchy system. In Xfiki, we have nest pages. In Confluence, you may have situations where the parent pages are corrupted. If you would have had that, you would see it in these reports. It's not the case here. Finally, we would need to recreate the hierarchy that we had in Confluence. You can see now that the pages that I have imported are flat. We have just one level hierarchy here. Now, I'm going to execute the nested pages migration tool that we also have at Xfiki. The pages will be moved into their parents according to the hierarchy that they had in Confluence. As you see, it's converting all the pages and they will be moved in the right place. Okay, cool. Now, we have a migration done. You can look at the pages to see all of your content. You also have, again, a lot of the macros that are also installed and can be reused. For the macros, the pro macros that I told you about, the bridge macros on our side are packaged. They are open source. They are public here, if you want to check them out or repackage them. On our side, they are packaged as under a license to be able to further support the development of the product. If you want to check them out or contribute to them, you can see them on our Git. We have here a Confluence migration done very quickly and without much hassle. We saw the Confluence migration. Now, let's see the SharePoint one. The way in which we migrate from Confluence is based on the XML export. From SharePoint, it's very different. In SharePoint, you have the option to export to CSV. If you're using SharePoint as a knowledge management tools and you have your documents with a bit of metadata, so like we have in this case, department could be considered a metadata or a structure data of field that you can check or uncheck and change. The pages have a form structure. If you have this type of data, one thing that you can do is to export to CSV, then create the same data structure on Xwiki. On Xwiki, we have an application that is called App Within Minutes that allows you to create structured data systems. Here, I already have an example made, but we can look at the structure. Basically, I just created the same structure that I had in the SharePoint example, so title, department, reviewed, and finally the content of my documents. Then, once I have that structure done, I can use the batch import application. Sorry, not here. Okay. With the batch import application, I would import the CSV that I have just got from SharePoint. I'm able to map the columns from the CSV to fields in Xwiki that I have just created. Here is the mapping that I just did before. You can choose whatever you want, even exclude some columns if it's the case. Then, we preview the mapping, and this is what they would look like on the Xwiki side. You can see that all the content is getting migrated. Let's just see a page. Here, you can say what you want to happen if you have duplicates. Then, we do the import, and the final result is something like this. All the pages were imported, and if you go to a page, you can see that you have this structured form type, and you can further edit it. Okay. That's all for the two examples. Sorry, I had to go through them very quickly. There are a lot of things that you can do to migrate, and of course, we're very happy to facilitate any migration from any other preparatory tool to get more users to open source. Thank you if you have any question. No questions? That clear? Yes. Yes, please. How would you deal with migration from basically just the directory with all its office documents? So, how do we deal with migration from the directory of all the office documents? So, two things that we can do. So, when you import office documents into Xwiki, we do have abundant integration with LibreOffice that allows you to convert office pages into Xwiki pages, but that's page by page. Or, if you have any sort of directory of office files, what you can do is to actually create manually this type of a CSV where you put in a row the content, and in this way, you can also add some metadata, for example, if you want to organize them in some departments or responsible person, so on. You can do that and then still use the batch import. At the moment, we don't have an existing tool for just feeding some files. We have something in progress also with batch import, but yeah, the one option is to either convert them one by one or use the batch import, but you would still need to organize them in a sort of a list. Yeah, that answers it. Yes, please. Thank you for the question. Just to repeat it, if it's the case, the question is if we can, if we facilitate in any way the addition of metadata or the cleanup, I would assume. So, on the metadata, as just mentioned now, for the office part, if you have office documents in any way, you can adapt that CSV file before migrating. So, for example, if you have office files or if you have an export from SharePoint, but it's not all documents have metadata, you can add them manually in the CSV that you do. On the conference side, not that much. You can, of course, so the labels and everything are imported, but to be straight here, it depends more on what you have on conference, because basically, with the migration from conference, we just take everything that you have and put it into Xfiki. We don't really facilitate any cleanup, but we allow you to migrate labels and macros that also do reports and all that. But for conference, specifically, it's a bit difficult to add metadata. Do you also migrate pages and lists? Sorry? For SharePoint, do you also migrate pages and lists so it will be not only documents? From SharePoint, at the moment, we migrate documents, so Word documents. There are other tools that we're working on with office integrations and Microsoft integration, but yeah, at the moment, we only import documents. Thank you. Maybe you told it, what about the user's permission right to view part of the document? Thank you. That's a really good question. The question is for user rights or permissions. That's in the part of the dependencies or integrations that we need to mind. At Xfiki, if you migrate from conference, for example, and you have native conference users, yes, we have the option to import them. You just need to configure that in the filter streams, and you can import the users, but not the permissions. The issue with the permissions is that the systems are very different. In conference, you have a very different system of access permissions compared to Xfiki. You can do that custom, like if you do a script that maps the rights and tries to set up some rights, we can imagine that, but at the moment, it's not possible. It's very difficult to do it generically. The alternative or the best case scenario is if you have something like an LDAP or even an SSO system that you have connected to your current tool, and when you migrate, you connect that same user directory to the new tool, such as Xfiki, and you just have the users created at the first login. That's, of course, the best case scenario. It's also actually possible to migrate users with the batch import, so you can do a bit of a hack there and import users as well, but for permissions, it's generally very complicated, and it's a case-by-case situation. You can import permissions, you can import groups from LDAP. We're also working on importing groups from as your SSO, but permissions, it's not yet generic enough done in our extensions. Yes? So thank you. Also a great question. If the history of additions is kept for the conference migration, yes, or for the XML migrations in general, yes. We do have that, and you can also see in our example here, I'm not sure if this one has enough history, but yeah, okay, so just a quick example, the history is retained, again, if you configure the filter to do so, and if you have this history retained, you can also see the changes between the versions, so that's something very nice. For SharePoint, we don't have that at the moment because we're not taking gold metadata from the documents, and also on other tools that support this type of filter streams migration, you may also get the history. Okay, thank you very much. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.32, "text": " about migrating from proprietary to open source knowledge management tools. I'll talk a bit", "tokens": [466, 6186, 8754, 490, 38992, 281, 1269, 4009, 3601, 4592, 3873, 13, 286, 603, 751, 257, 857], "temperature": 0.0, "avg_logprob": -0.21565725864508214, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.12125711888074875}, {"id": 1, "seek": 0, "start": 10.32, "end": 15.8, "text": " in general about the migration process and then demonstrate two migrations, one from", "tokens": [294, 2674, 466, 264, 17011, 1399, 293, 550, 11698, 732, 6186, 12154, 11, 472, 490], "temperature": 0.0, "avg_logprob": -0.21565725864508214, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.12125711888074875}, {"id": 2, "seek": 0, "start": 15.8, "end": 22.64, "text": " conference and one from SharePoint with different technologies just to see what's possible on", "tokens": [7586, 293, 472, 490, 14945, 18705, 365, 819, 7943, 445, 281, 536, 437, 311, 1944, 322], "temperature": 0.0, "avg_logprob": -0.21565725864508214, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.12125711888074875}, {"id": 3, "seek": 0, "start": 22.64, "end": 28.64, "text": " our side and in general. So first, why migrate from proprietary to open source? I'm sure", "tokens": [527, 1252, 293, 294, 2674, 13, 407, 700, 11, 983, 31821, 490, 38992, 281, 1269, 4009, 30, 286, 478, 988], "temperature": 0.0, "avg_logprob": -0.21565725864508214, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.12125711888074875}, {"id": 4, "seek": 2864, "start": 28.64, "end": 34.4, "text": " everyone has a lot of good reasons for that in mind but some that we have identified as well", "tokens": [1518, 575, 257, 688, 295, 665, 4112, 337, 300, 294, 1575, 457, 512, 300, 321, 362, 9234, 382, 731], "temperature": 0.0, "avg_logprob": -0.12367936067802962, "compression_ratio": 1.6351931330472103, "no_speech_prob": 9.1729307314381e-05}, {"id": 5, "seek": 2864, "start": 34.4, "end": 39.92, "text": " would be first for privacy and security concerns. Of course, with open source software, you have", "tokens": [576, 312, 700, 337, 11427, 293, 3825, 7389, 13, 2720, 1164, 11, 365, 1269, 4009, 4722, 11, 291, 362], "temperature": 0.0, "avg_logprob": -0.12367936067802962, "compression_ratio": 1.6351931330472103, "no_speech_prob": 9.1729307314381e-05}, {"id": 6, "seek": 2864, "start": 39.92, "end": 46.400000000000006, "text": " more flexibility in where you host your data, how you host it, the environment and access that", "tokens": [544, 12635, 294, 689, 291, 3975, 428, 1412, 11, 577, 291, 3975, 309, 11, 264, 2823, 293, 2105, 300], "temperature": 0.0, "avg_logprob": -0.12367936067802962, "compression_ratio": 1.6351931330472103, "no_speech_prob": 9.1729307314381e-05}, {"id": 7, "seek": 2864, "start": 46.400000000000006, "end": 53.6, "text": " you set up. Two other concerns that are related and we have also seen some important examples in", "tokens": [291, 992, 493, 13, 4453, 661, 7389, 300, 366, 4077, 293, 321, 362, 611, 1612, 512, 1021, 5110, 294], "temperature": 0.0, "avg_logprob": -0.12367936067802962, "compression_ratio": 1.6351931330472103, "no_speech_prob": 9.1729307314381e-05}, {"id": 8, "seek": 5360, "start": 53.6, "end": 60.480000000000004, "text": " the recent years are vendor and data lock-in. So when you're using a proprietary software, you're", "tokens": [264, 5162, 924, 366, 24321, 293, 1412, 4017, 12, 259, 13, 407, 562, 291, 434, 1228, 257, 38992, 4722, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.10792673465817473, "compression_ratio": 1.7201834862385321, "no_speech_prob": 8.548049663659185e-05}, {"id": 9, "seek": 5360, "start": 60.480000000000004, "end": 66.48, "text": " a bit more vulnerable to the vendor than when you're using an open source software in the sense", "tokens": [257, 857, 544, 10955, 281, 264, 24321, 813, 562, 291, 434, 1228, 364, 1269, 4009, 4722, 294, 264, 2020], "temperature": 0.0, "avg_logprob": -0.10792673465817473, "compression_ratio": 1.7201834862385321, "no_speech_prob": 8.548049663659185e-05}, {"id": 10, "seek": 5360, "start": 66.48, "end": 71.84, "text": " that if they make any significant strategy or pricing changes, you may find yourself in a", "tokens": [300, 498, 436, 652, 604, 4776, 5206, 420, 17621, 2962, 11, 291, 815, 915, 1803, 294, 257], "temperature": 0.0, "avg_logprob": -0.10792673465817473, "compression_ratio": 1.7201834862385321, "no_speech_prob": 8.548049663659185e-05}, {"id": 11, "seek": 5360, "start": 71.84, "end": 78.64, "text": " situation where you need to migrate quickly or you need to quickly adjust to that change. A", "tokens": [2590, 689, 291, 643, 281, 31821, 2661, 420, 291, 643, 281, 2661, 4369, 281, 300, 1319, 13, 316], "temperature": 0.0, "avg_logprob": -0.10792673465817473, "compression_ratio": 1.7201834862385321, "no_speech_prob": 8.548049663659185e-05}, {"id": 12, "seek": 7864, "start": 78.64, "end": 86.56, "text": " recent example is Confluence. Maybe some of you have stumbled across it. They had a cloud version", "tokens": [5162, 1365, 307, 11701, 40432, 13, 2704, 512, 295, 291, 362, 36668, 2108, 309, 13, 814, 632, 257, 4588, 3037], "temperature": 0.0, "avg_logprob": -0.0701077832116021, "compression_ratio": 1.7092511013215859, "no_speech_prob": 7.87727640272351e-06}, {"id": 13, "seek": 7864, "start": 86.56, "end": 93.84, "text": " and a server version and a data center but that's usually for larger teams. The server version was", "tokens": [293, 257, 7154, 3037, 293, 257, 1412, 3056, 457, 300, 311, 2673, 337, 4833, 5491, 13, 440, 7154, 3037, 390], "temperature": 0.0, "avg_logprob": -0.0701077832116021, "compression_ratio": 1.7092511013215859, "no_speech_prob": 7.87727640272351e-06}, {"id": 14, "seek": 7864, "start": 93.84, "end": 100.32, "text": " used by smaller teams that wanted to host the data on their premise and in late 2020, Confluence", "tokens": [1143, 538, 4356, 5491, 300, 1415, 281, 3975, 264, 1412, 322, 641, 22045, 293, 294, 3469, 4808, 11, 11701, 40432], "temperature": 0.0, "avg_logprob": -0.0701077832116021, "compression_ratio": 1.7092511013215859, "no_speech_prob": 7.87727640272351e-06}, {"id": 15, "seek": 7864, "start": 100.32, "end": 106.56, "text": " decided to stop the server version. So because of that, a lot of small companies had to either", "tokens": [3047, 281, 1590, 264, 7154, 3037, 13, 407, 570, 295, 300, 11, 257, 688, 295, 1359, 3431, 632, 281, 2139], "temperature": 0.0, "avg_logprob": -0.0701077832116021, "compression_ratio": 1.7092511013215859, "no_speech_prob": 7.87727640272351e-06}, {"id": 16, "seek": 10656, "start": 106.56, "end": 112.08, "text": " move to cloud, so host their data in the states, which wasn't possible for a lot of people,", "tokens": [1286, 281, 4588, 11, 370, 3975, 641, 1412, 294, 264, 4368, 11, 597, 2067, 380, 1944, 337, 257, 688, 295, 561, 11], "temperature": 0.0, "avg_logprob": -0.10124037986577944, "compression_ratio": 1.6043478260869566, "no_speech_prob": 4.152613655605819e-06}, {"id": 17, "seek": 10656, "start": 112.08, "end": 117.92, "text": " or move to data center, which was much more expensive and difficult to handle. And that was", "tokens": [420, 1286, 281, 1412, 3056, 11, 597, 390, 709, 544, 5124, 293, 2252, 281, 4813, 13, 400, 300, 390], "temperature": 0.0, "avg_logprob": -0.10124037986577944, "compression_ratio": 1.6043478260869566, "no_speech_prob": 4.152613655605819e-06}, {"id": 18, "seek": 10656, "start": 117.92, "end": 124.16, "text": " an example of how using proprietary software can make you quite vulnerable to their changes.", "tokens": [364, 1365, 295, 577, 1228, 38992, 4722, 393, 652, 291, 1596, 10955, 281, 641, 2962, 13], "temperature": 0.0, "avg_logprob": -0.10124037986577944, "compression_ratio": 1.6043478260869566, "no_speech_prob": 4.152613655605819e-06}, {"id": 19, "seek": 10656, "start": 125.12, "end": 130.16, "text": " One other thing that is important is the data lock-in. So using open source software usually", "tokens": [1485, 661, 551, 300, 307, 1021, 307, 264, 1412, 4017, 12, 259, 13, 407, 1228, 1269, 4009, 4722, 2673], "temperature": 0.0, "avg_logprob": -0.10124037986577944, "compression_ratio": 1.6043478260869566, "no_speech_prob": 4.152613655605819e-06}, {"id": 20, "seek": 13016, "start": 130.16, "end": 137.04, "text": " allows you to migrate easier and integrate with certain tools due to open standards and protocols", "tokens": [4045, 291, 281, 31821, 3571, 293, 13365, 365, 1629, 3873, 3462, 281, 1269, 7787, 293, 20618], "temperature": 0.0, "avg_logprob": -0.07000893638247535, "compression_ratio": 1.7990654205607477, "no_speech_prob": 2.384376603004057e-05}, {"id": 21, "seek": 13016, "start": 137.04, "end": 141.44, "text": " and formats. So if you're using an open source tool at some point and then you want to migrate to", "tokens": [293, 25879, 13, 407, 498, 291, 434, 1228, 364, 1269, 4009, 2290, 412, 512, 935, 293, 550, 291, 528, 281, 31821, 281], "temperature": 0.0, "avg_logprob": -0.07000893638247535, "compression_ratio": 1.7990654205607477, "no_speech_prob": 2.384376603004057e-05}, {"id": 22, "seek": 13016, "start": 141.44, "end": 148.48, "text": " another, it may be easier than migrating from proprietary tools to other proprietary tools", "tokens": [1071, 11, 309, 815, 312, 3571, 813, 6186, 8754, 490, 38992, 3873, 281, 661, 38992, 3873], "temperature": 0.0, "avg_logprob": -0.07000893638247535, "compression_ratio": 1.7990654205607477, "no_speech_prob": 2.384376603004057e-05}, {"id": 23, "seek": 13016, "start": 148.48, "end": 154.96, "text": " or open source ones. Then of course, accessibility, having access to the code and all the features", "tokens": [420, 1269, 4009, 2306, 13, 1396, 295, 1164, 11, 15002, 11, 1419, 2105, 281, 264, 3089, 293, 439, 264, 4122], "temperature": 0.0, "avg_logprob": -0.07000893638247535, "compression_ratio": 1.7990654205607477, "no_speech_prob": 2.384376603004057e-05}, {"id": 24, "seek": 15496, "start": 154.96, "end": 161.12, "text": " may allow us to further extend it or contribute to it and implement other features. And finally,", "tokens": [815, 2089, 505, 281, 3052, 10101, 309, 420, 10586, 281, 309, 293, 4445, 661, 4122, 13, 400, 2721, 11], "temperature": 0.0, "avg_logprob": -0.08476268677484422, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.461613960098475e-05}, {"id": 25, "seek": 15496, "start": 161.12, "end": 168.96, "text": " from a values or ethical point of view, when we are using open source, we kind of facilitate", "tokens": [490, 257, 4190, 420, 18890, 935, 295, 1910, 11, 562, 321, 366, 1228, 1269, 4009, 11, 321, 733, 295, 20207], "temperature": 0.0, "avg_logprob": -0.08476268677484422, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.461613960098475e-05}, {"id": 26, "seek": 15496, "start": 168.96, "end": 176.64000000000001, "text": " integration and technological growth for everyone, rather than focusing on products and standards", "tokens": [10980, 293, 18439, 4599, 337, 1518, 11, 2831, 813, 8416, 322, 3383, 293, 7787], "temperature": 0.0, "avg_logprob": -0.08476268677484422, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.461613960098475e-05}, {"id": 27, "seek": 15496, "start": 176.64000000000001, "end": 184.8, "text": " that just have a small benefit or a benefit for one company. Okay, so with all these good reasons", "tokens": [300, 445, 362, 257, 1359, 5121, 420, 257, 5121, 337, 472, 2237, 13, 1033, 11, 370, 365, 439, 613, 665, 4112], "temperature": 0.0, "avg_logprob": -0.08476268677484422, "compression_ratio": 1.5909090909090908, "no_speech_prob": 3.461613960098475e-05}, {"id": 28, "seek": 18480, "start": 184.8, "end": 192.24, "text": " I hope in mind, I'm going to talk about how to approach migration. So in general, as a plan,", "tokens": [286, 1454, 294, 1575, 11, 286, 478, 516, 281, 751, 466, 577, 281, 3109, 17011, 13, 407, 294, 2674, 11, 382, 257, 1393, 11], "temperature": 0.0, "avg_logprob": -0.08937037467956543, "compression_ratio": 1.648068669527897, "no_speech_prob": 4.6741442929487675e-05}, {"id": 29, "seek": 18480, "start": 192.24, "end": 199.28, "text": " I hope that's okay. So in general, as a plan, when we migrate from one knowledge management tool to", "tokens": [286, 1454, 300, 311, 1392, 13, 407, 294, 2674, 11, 382, 257, 1393, 11, 562, 321, 31821, 490, 472, 3601, 4592, 2290, 281], "temperature": 0.0, "avg_logprob": -0.08937037467956543, "compression_ratio": 1.648068669527897, "no_speech_prob": 4.6741442929487675e-05}, {"id": 30, "seek": 18480, "start": 199.28, "end": 204.96, "text": " another, but this can be kind of extended to other software as well, we first need to think", "tokens": [1071, 11, 457, 341, 393, 312, 733, 295, 10913, 281, 661, 4722, 382, 731, 11, 321, 700, 643, 281, 519], "temperature": 0.0, "avg_logprob": -0.08937037467956543, "compression_ratio": 1.648068669527897, "no_speech_prob": 4.6741442929487675e-05}, {"id": 31, "seek": 18480, "start": 204.96, "end": 209.92000000000002, "text": " about data format and dependencies. What type of data do we have? Do we have flat HTML pages? Do we", "tokens": [466, 1412, 7877, 293, 36606, 13, 708, 2010, 295, 1412, 360, 321, 362, 30, 1144, 321, 362, 4962, 17995, 7183, 30, 1144, 321], "temperature": 0.0, "avg_logprob": -0.08937037467956543, "compression_ratio": 1.648068669527897, "no_speech_prob": 4.6741442929487675e-05}, {"id": 32, "seek": 20992, "start": 209.92, "end": 215.35999999999999, "text": " have structured pages? Do we have any metadata tags? All that needs to be considered. Then we", "tokens": [362, 18519, 7183, 30, 1144, 321, 362, 604, 26603, 18632, 30, 1057, 300, 2203, 281, 312, 4888, 13, 1396, 321], "temperature": 0.0, "avg_logprob": -0.06458865835311565, "compression_ratio": 1.6540084388185654, "no_speech_prob": 1.590462306921836e-05}, {"id": 33, "seek": 20992, "start": 215.35999999999999, "end": 220.39999999999998, "text": " also need to look at the other extensions that we're using. What type of authentication do we have?", "tokens": [611, 643, 281, 574, 412, 264, 661, 25129, 300, 321, 434, 1228, 13, 708, 2010, 295, 26643, 360, 321, 362, 30], "temperature": 0.0, "avg_logprob": -0.06458865835311565, "compression_ratio": 1.6540084388185654, "no_speech_prob": 1.590462306921836e-05}, {"id": 34, "seek": 20992, "start": 220.39999999999998, "end": 227.6, "text": " Are we integrating with anything else that needs to be kept? Then once we have that listed, based", "tokens": [2014, 321, 26889, 365, 1340, 1646, 300, 2203, 281, 312, 4305, 30, 1396, 1564, 321, 362, 300, 10052, 11, 2361], "temperature": 0.0, "avg_logprob": -0.06458865835311565, "compression_ratio": 1.6540084388185654, "no_speech_prob": 1.590462306921836e-05}, {"id": 35, "seek": 20992, "start": 227.6, "end": 234.23999999999998, "text": " on that, we can identify and set up the new software. For example, let's say that we have identified", "tokens": [322, 300, 11, 321, 393, 5876, 293, 992, 493, 264, 777, 4722, 13, 1171, 1365, 11, 718, 311, 584, 300, 321, 362, 9234], "temperature": 0.0, "avg_logprob": -0.06458865835311565, "compression_ratio": 1.6540084388185654, "no_speech_prob": 1.590462306921836e-05}, {"id": 36, "seek": 23424, "start": 234.24, "end": 242.8, "text": " that on the confidence side, we can export to XML. Xfiki supports that and we want to do this", "tokens": [300, 322, 264, 6687, 1252, 11, 321, 393, 10725, 281, 43484, 13, 1783, 69, 9850, 9346, 300, 293, 321, 528, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.14181110810260383, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.253191829979187e-06}, {"id": 37, "seek": 23424, "start": 242.8, "end": 249.20000000000002, "text": " migration. We need to set up the new software in the environment that works for us. Then we need", "tokens": [17011, 13, 492, 643, 281, 992, 493, 264, 777, 4722, 294, 264, 2823, 300, 1985, 337, 505, 13, 1396, 321, 643], "temperature": 0.0, "avg_logprob": -0.14181110810260383, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.253191829979187e-06}, {"id": 38, "seek": 23424, "start": 249.20000000000002, "end": 254.48000000000002, "text": " to make the migration plan and clean up. This is of course quite specific to the software that you", "tokens": [281, 652, 264, 17011, 1393, 293, 2541, 493, 13, 639, 307, 295, 1164, 1596, 2685, 281, 264, 4722, 300, 291], "temperature": 0.0, "avg_logprob": -0.14181110810260383, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.253191829979187e-06}, {"id": 39, "seek": 23424, "start": 254.48000000000002, "end": 260.56, "text": " that you're migrating to and from. But in general, it's also interesting to know that it can be a", "tokens": [300, 291, 434, 6186, 8754, 281, 293, 490, 13, 583, 294, 2674, 11, 309, 311, 611, 1880, 281, 458, 300, 309, 393, 312, 257], "temperature": 0.0, "avg_logprob": -0.14181110810260383, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.253191829979187e-06}, {"id": 40, "seek": 26056, "start": 260.56, "end": 264.88, "text": " good moment to clean up your data. For example, if you have a system that you have been using for", "tokens": [665, 1623, 281, 2541, 493, 428, 1412, 13, 1171, 1365, 11, 498, 291, 362, 257, 1185, 300, 291, 362, 668, 1228, 337], "temperature": 0.0, "avg_logprob": -0.07144046582673726, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.922741805377882e-05}, {"id": 41, "seek": 26056, "start": 264.88, "end": 271.12, "text": " 10 years, you may have a lot of obsolete data that can be cleaned up at this moment. Aside from the", "tokens": [1266, 924, 11, 291, 815, 362, 257, 688, 295, 46333, 1412, 300, 393, 312, 16146, 493, 412, 341, 1623, 13, 33726, 490, 264], "temperature": 0.0, "avg_logprob": -0.07144046582673726, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.922741805377882e-05}, {"id": 42, "seek": 26056, "start": 271.68, "end": 279.12, "text": " plan of migrating, you can also eliminate some pages or even reorganize the content if possible.", "tokens": [1393, 295, 6186, 8754, 11, 291, 393, 611, 13819, 512, 7183, 420, 754, 41203, 1125, 264, 2701, 498, 1944, 13], "temperature": 0.0, "avg_logprob": -0.07144046582673726, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.922741805377882e-05}, {"id": 43, "seek": 26056, "start": 280.0, "end": 285.44, "text": " Then of course, we need to execute the migration. Again, it depends on each organization or company", "tokens": [1396, 295, 1164, 11, 321, 643, 281, 14483, 264, 17011, 13, 3764, 11, 309, 5946, 322, 1184, 4475, 420, 2237], "temperature": 0.0, "avg_logprob": -0.07144046582673726, "compression_ratio": 1.6416666666666666, "no_speech_prob": 2.922741805377882e-05}, {"id": 44, "seek": 28544, "start": 285.44, "end": 293.6, "text": " if they executed with a team, if they executed with other services. But this is an important step", "tokens": [498, 436, 17577, 365, 257, 1469, 11, 498, 436, 17577, 365, 661, 3328, 13, 583, 341, 307, 364, 1021, 1823], "temperature": 0.0, "avg_logprob": -0.08118229845295781, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.3821402717439923e-05}, {"id": 45, "seek": 28544, "start": 293.6, "end": 300.88, "text": " that also needs to be planned and kind of realistically planned in time as well to say so.", "tokens": [300, 611, 2203, 281, 312, 8589, 293, 733, 295, 40734, 8589, 294, 565, 382, 731, 281, 584, 370, 13], "temperature": 0.0, "avg_logprob": -0.08118229845295781, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.3821402717439923e-05}, {"id": 46, "seek": 28544, "start": 302.8, "end": 308.32, "text": " When we migrate itself, we also need to try to map the data and its structure. As I said", "tokens": [1133, 321, 31821, 2564, 11, 321, 611, 643, 281, 853, 281, 4471, 264, 1412, 293, 1080, 3877, 13, 1018, 286, 848], "temperature": 0.0, "avg_logprob": -0.08118229845295781, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.3821402717439923e-05}, {"id": 47, "seek": 28544, "start": 308.32, "end": 313.6, "text": " previously, if you have some type of structured data, you will want to map that. Also, you will", "tokens": [8046, 11, 498, 291, 362, 512, 2010, 295, 18519, 1412, 11, 291, 486, 528, 281, 4471, 300, 13, 2743, 11, 291, 486], "temperature": 0.0, "avg_logprob": -0.08118229845295781, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.3821402717439923e-05}, {"id": 48, "seek": 31360, "start": 313.6, "end": 323.84000000000003, "text": " want to create the new integrations or dependencies or install them. Then finally, once that is done,", "tokens": [528, 281, 1884, 264, 777, 3572, 763, 420, 36606, 420, 3625, 552, 13, 1396, 2721, 11, 1564, 300, 307, 1096, 11], "temperature": 0.0, "avg_logprob": -0.1024971243775921, "compression_ratio": 1.6, "no_speech_prob": 3.209719943697564e-05}, {"id": 49, "seek": 31360, "start": 323.84000000000003, "end": 329.68, "text": " we need to do a thorough testing of course before releasing the new system. Finally,", "tokens": [321, 643, 281, 360, 257, 12934, 4997, 295, 1164, 949, 16327, 264, 777, 1185, 13, 6288, 11], "temperature": 0.0, "avg_logprob": -0.1024971243775921, "compression_ratio": 1.6, "no_speech_prob": 3.209719943697564e-05}, {"id": 50, "seek": 31360, "start": 329.68, "end": 334.8, "text": " delivering in production and a step that is quite commonly skipped would be the user training.", "tokens": [14666, 294, 4265, 293, 257, 1823, 300, 307, 1596, 12719, 30193, 576, 312, 264, 4195, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1024971243775921, "compression_ratio": 1.6, "no_speech_prob": 3.209719943697564e-05}, {"id": 51, "seek": 31360, "start": 335.68, "end": 339.84000000000003, "text": " If you have any sort of organization, you may have been in the situation where", "tokens": [759, 291, 362, 604, 1333, 295, 4475, 11, 291, 815, 362, 668, 294, 264, 2590, 689], "temperature": 0.0, "avg_logprob": -0.1024971243775921, "compression_ratio": 1.6, "no_speech_prob": 3.209719943697564e-05}, {"id": 52, "seek": 33984, "start": 339.84, "end": 345.03999999999996, "text": " you had to change the software and people may have been resistant to using the new software or", "tokens": [291, 632, 281, 1319, 264, 4722, 293, 561, 815, 362, 668, 20383, 281, 1228, 264, 777, 4722, 420], "temperature": 0.0, "avg_logprob": -0.14583566712170112, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.00010331736120861024}, {"id": 53, "seek": 33984, "start": 346.47999999999996, "end": 352.88, "text": " became a bit less efficient or didn't really know how to handle certain aspects. A bit of user", "tokens": [3062, 257, 857, 1570, 7148, 420, 994, 380, 534, 458, 577, 281, 4813, 1629, 7270, 13, 316, 857, 295, 4195], "temperature": 0.0, "avg_logprob": -0.14583566712170112, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.00010331736120861024}, {"id": 54, "seek": 33984, "start": 352.88, "end": 359.28, "text": " training can be very helpful as well if you're changing your knowledge management tools.", "tokens": [3097, 393, 312, 588, 4961, 382, 731, 498, 291, 434, 4473, 428, 3601, 4592, 3873, 13], "temperature": 0.0, "avg_logprob": -0.14583566712170112, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.00010331736120861024}, {"id": 55, "seek": 33984, "start": 360.79999999999995, "end": 367.12, "text": " Now we know the general plan and we can see two examples in action. For the conference", "tokens": [823, 321, 458, 264, 2674, 1393, 293, 321, 393, 536, 732, 5110, 294, 3069, 13, 1171, 264, 7586], "temperature": 0.0, "avg_logprob": -0.14583566712170112, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.00010331736120861024}, {"id": 56, "seek": 36712, "start": 367.12, "end": 374.72, "text": " to xWiki migration, I'm going to demonstrate the XML export way. For the share point,", "tokens": [281, 2031, 54, 9850, 17011, 11, 286, 478, 516, 281, 11698, 264, 43484, 10725, 636, 13, 1171, 264, 2073, 935, 11], "temperature": 0.0, "avg_logprob": -0.22155441840489706, "compression_ratio": 1.3206106870229009, "no_speech_prob": 0.00012115729623474181}, {"id": 57, "seek": 36712, "start": 374.72, "end": 392.64, "text": " we're going to see an example of CSV export. On the migration from conference to xWiki,", "tokens": [321, 434, 516, 281, 536, 364, 1365, 295, 48814, 10725, 13, 1282, 264, 17011, 490, 7586, 281, 2031, 54, 9850, 11], "temperature": 0.0, "avg_logprob": -0.22155441840489706, "compression_ratio": 1.3206106870229009, "no_speech_prob": 0.00012115729623474181}, {"id": 58, "seek": 39264, "start": 392.64, "end": 401.52, "text": " we already had a lot of tools available to migrate. However, in recent years, we", "tokens": [321, 1217, 632, 257, 688, 295, 3873, 2435, 281, 31821, 13, 2908, 11, 294, 5162, 924, 11, 321], "temperature": 0.0, "avg_logprob": -0.13582114250429214, "compression_ratio": 1.4751381215469612, "no_speech_prob": 1.1104022632935084e-05}, {"id": 59, "seek": 39264, "start": 401.52, "end": 408.24, "text": " dedicated a bit more efforts to trying to make them as easy to use as possible. We had before", "tokens": [8374, 257, 857, 544, 6484, 281, 1382, 281, 652, 552, 382, 1858, 281, 764, 382, 1944, 13, 492, 632, 949], "temperature": 0.0, "avg_logprob": -0.13582114250429214, "compression_ratio": 1.4751381215469612, "no_speech_prob": 1.1104022632935084e-05}, {"id": 60, "seek": 39264, "start": 408.24, "end": 414.96, "text": " the filter streams converter that also supported the conference format. Nowadays, we have an", "tokens": [264, 6608, 15842, 33905, 300, 611, 8104, 264, 7586, 7877, 13, 28908, 11, 321, 362, 364], "temperature": 0.0, "avg_logprob": -0.13582114250429214, "compression_ratio": 1.4751381215469612, "no_speech_prob": 1.1104022632935084e-05}, {"id": 61, "seek": 41496, "start": 414.96, "end": 423.68, "text": " integrated migrator that has a couple of steps. We'll see them right away. For the macros,", "tokens": [10919, 6186, 19802, 300, 575, 257, 1916, 295, 4439, 13, 492, 603, 536, 552, 558, 1314, 13, 1171, 264, 7912, 2635, 11], "temperature": 0.0, "avg_logprob": -0.10892563435568739, "compression_ratio": 1.536723163841808, "no_speech_prob": 5.2511027206492145e-06}, {"id": 62, "seek": 41496, "start": 425.03999999999996, "end": 433.67999999999995, "text": " in conference, we have some macros like we have in other wiki systems or knowledge management", "tokens": [294, 7586, 11, 321, 362, 512, 7912, 2635, 411, 321, 362, 294, 661, 261, 9850, 3652, 420, 3601, 4592], "temperature": 0.0, "avg_logprob": -0.10892563435568739, "compression_ratio": 1.536723163841808, "no_speech_prob": 5.2511027206492145e-06}, {"id": 63, "seek": 41496, "start": 433.67999999999995, "end": 442.88, "text": " tools. One concern while migrating were the macros that had to be supported or migrated", "tokens": [3873, 13, 1485, 3136, 1339, 6186, 8754, 645, 264, 7912, 2635, 300, 632, 281, 312, 8104, 420, 48329], "temperature": 0.0, "avg_logprob": -0.10892563435568739, "compression_ratio": 1.536723163841808, "no_speech_prob": 5.2511027206492145e-06}, {"id": 64, "seek": 44288, "start": 442.88, "end": 449.44, "text": " properly. Because of that, we also developed a package of bridge macros, how we call them,", "tokens": [6108, 13, 1436, 295, 300, 11, 321, 611, 4743, 257, 7372, 295, 7283, 7912, 2635, 11, 577, 321, 818, 552, 11], "temperature": 0.0, "avg_logprob": -0.08398319954095884, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.964655818184838e-05}, {"id": 65, "seek": 44288, "start": 449.44, "end": 456.71999999999997, "text": " or macros that were identical to what exists in conference and that would support migration.", "tokens": [420, 7912, 2635, 300, 645, 14800, 281, 437, 8198, 294, 7586, 293, 300, 576, 1406, 17011, 13], "temperature": 0.0, "avg_logprob": -0.08398319954095884, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.964655818184838e-05}, {"id": 66, "seek": 44288, "start": 456.71999999999997, "end": 462.4, "text": " Of course, we don't have all the macros that exist in conference because both xWiki and", "tokens": [2720, 1164, 11, 321, 500, 380, 362, 439, 264, 7912, 2635, 300, 2514, 294, 7586, 570, 1293, 2031, 54, 9850, 293], "temperature": 0.0, "avg_logprob": -0.08398319954095884, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.964655818184838e-05}, {"id": 67, "seek": 44288, "start": 462.4, "end": 467.04, "text": " conference are products that have been around for a really long time and they have their own", "tokens": [7586, 366, 3383, 300, 362, 668, 926, 337, 257, 534, 938, 565, 293, 436, 362, 641, 1065], "temperature": 0.0, "avg_logprob": -0.08398319954095884, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.964655818184838e-05}, {"id": 68, "seek": 46704, "start": 467.04, "end": 475.28000000000003, "text": " specificities. That is a concern to keep in mind not only when migrating from conference to xWiki,", "tokens": [2685, 1088, 13, 663, 307, 257, 3136, 281, 1066, 294, 1575, 406, 787, 562, 6186, 8754, 490, 7586, 281, 2031, 54, 9850, 11], "temperature": 0.0, "avg_logprob": -0.093046638403046, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.1758893126389012e-05}, {"id": 69, "seek": 46704, "start": 475.28000000000003, "end": 480.88, "text": " but from any software. Again, this fits into the dependencies or applications that you're using", "tokens": [457, 490, 604, 4722, 13, 3764, 11, 341, 9001, 666, 264, 36606, 420, 5821, 300, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.093046638403046, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.1758893126389012e-05}, {"id": 70, "seek": 46704, "start": 481.44, "end": 488.48, "text": " part. Let's see how a migration works. Again, I'm going to use the integrated one,", "tokens": [644, 13, 961, 311, 536, 577, 257, 17011, 1985, 13, 3764, 11, 286, 478, 516, 281, 764, 264, 10919, 472, 11], "temperature": 0.0, "avg_logprob": -0.093046638403046, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.1758893126389012e-05}, {"id": 71, "seek": 46704, "start": 489.6, "end": 496.40000000000003, "text": " the integrated migrator, which reuses the original filter streams. What we did for", "tokens": [264, 10919, 6186, 19802, 11, 597, 319, 8355, 264, 3380, 6608, 15842, 13, 708, 321, 630, 337], "temperature": 0.0, "avg_logprob": -0.093046638403046, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.1758893126389012e-05}, {"id": 72, "seek": 49640, "start": 496.4, "end": 503.12, "text": " this migrator itself, to make it, again, a bit nicer to use, we created this concept of a profile", "tokens": [341, 6186, 19802, 2564, 11, 281, 652, 309, 11, 797, 11, 257, 857, 22842, 281, 764, 11, 321, 2942, 341, 3410, 295, 257, 7964], "temperature": 0.0, "avg_logprob": -0.09504613374408923, "compression_ratio": 1.683982683982684, "no_speech_prob": 5.0569076847750694e-05}, {"id": 73, "seek": 49640, "start": 503.91999999999996, "end": 510.32, "text": " in which you can basically import each space separately and you'll have a bit of an analysis", "tokens": [294, 597, 291, 393, 1936, 974, 1184, 1901, 14759, 293, 291, 603, 362, 257, 857, 295, 364, 5215], "temperature": 0.0, "avg_logprob": -0.09504613374408923, "compression_ratio": 1.683982683982684, "no_speech_prob": 5.0569076847750694e-05}, {"id": 74, "seek": 49640, "start": 510.32, "end": 517.04, "text": " on what pages were imported, if you had any pages that caused issues and you're able to follow the", "tokens": [322, 437, 7183, 645, 25524, 11, 498, 291, 632, 604, 7183, 300, 7008, 2663, 293, 291, 434, 1075, 281, 1524, 264], "temperature": 0.0, "avg_logprob": -0.09504613374408923, "compression_ratio": 1.683982683982684, "no_speech_prob": 5.0569076847750694e-05}, {"id": 75, "seek": 49640, "start": 517.04, "end": 524.0, "text": " process a bit better. You can name your profile however you want. If you want, you can also connect", "tokens": [1399, 257, 857, 1101, 13, 509, 393, 1315, 428, 7964, 4461, 291, 528, 13, 759, 291, 528, 11, 291, 393, 611, 1745], "temperature": 0.0, "avg_logprob": -0.09504613374408923, "compression_ratio": 1.683982683982684, "no_speech_prob": 5.0569076847750694e-05}, {"id": 76, "seek": 52400, "start": 524.0, "end": 530.32, "text": " to the conference instance. This is not mandatory, but if you connect to the conference instance,", "tokens": [281, 264, 7586, 5197, 13, 639, 307, 406, 22173, 11, 457, 498, 291, 1745, 281, 264, 7586, 5197, 11], "temperature": 0.0, "avg_logprob": -0.11638210036537865, "compression_ratio": 1.7403846153846154, "no_speech_prob": 3.4212822356494144e-05}, {"id": 77, "seek": 52400, "start": 530.32, "end": 536.4, "text": " it also gives you a pre-analysis of the pages that were already imported into xWiki, so that", "tokens": [309, 611, 2709, 291, 257, 659, 12, 29702, 4642, 295, 264, 7183, 300, 645, 1217, 25524, 666, 2031, 54, 9850, 11, 370, 300], "temperature": 0.0, "avg_logprob": -0.11638210036537865, "compression_ratio": 1.7403846153846154, "no_speech_prob": 3.4212822356494144e-05}, {"id": 78, "seek": 52400, "start": 537.12, "end": 543.44, "text": " can be useful. If you're having conference cloud, your username and token would be", "tokens": [393, 312, 4420, 13, 759, 291, 434, 1419, 7586, 4588, 11, 428, 30351, 293, 14862, 576, 312], "temperature": 0.0, "avg_logprob": -0.11638210036537865, "compression_ratio": 1.7403846153846154, "no_speech_prob": 3.4212822356494144e-05}, {"id": 79, "seek": 52400, "start": 544.08, "end": 550.16, "text": " username and the token that you get, the API token. If you're running conference server,", "tokens": [30351, 293, 264, 14862, 300, 291, 483, 11, 264, 9362, 14862, 13, 759, 291, 434, 2614, 7586, 7154, 11], "temperature": 0.0, "avg_logprob": -0.11638210036537865, "compression_ratio": 1.7403846153846154, "no_speech_prob": 3.4212822356494144e-05}, {"id": 80, "seek": 55016, "start": 550.16, "end": 556.0799999999999, "text": " the username would be your administrator username and the token will be your password.", "tokens": [264, 30351, 576, 312, 428, 25529, 30351, 293, 264, 14862, 486, 312, 428, 11524, 13], "temperature": 0.0, "avg_logprob": -0.17251153425736862, "compression_ratio": 1.5371428571428571, "no_speech_prob": 1.4489390196104068e-05}, {"id": 81, "seek": 55016, "start": 557.12, "end": 564.24, "text": " We have here, of course, a demo purpose conference instance. It's not what we use internally.", "tokens": [492, 362, 510, 11, 295, 1164, 11, 257, 10723, 4334, 7586, 5197, 13, 467, 311, 406, 437, 321, 764, 19501, 13], "temperature": 0.0, "avg_logprob": -0.17251153425736862, "compression_ratio": 1.5371428571428571, "no_speech_prob": 1.4489390196104068e-05}, {"id": 82, "seek": 56424, "start": 564.24, "end": 584.08, "text": " Then we need to put the URL as well in order to map the URL schema. Let's take that as well.", "tokens": [1396, 321, 643, 281, 829, 264, 12905, 382, 731, 294, 1668, 281, 4471, 264, 12905, 34078, 13, 961, 311, 747, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19074191365923202, "compression_ratio": 1.1645569620253164, "no_speech_prob": 5.469576717587188e-05}, {"id": 83, "seek": 58408, "start": 584.08, "end": 594.24, "text": " We don't have a custom URL path. This is important again for the URL schema and for", "tokens": [492, 500, 380, 362, 257, 2375, 12905, 3100, 13, 639, 307, 1021, 797, 337, 264, 12905, 34078, 293, 337], "temperature": 0.0, "avg_logprob": -0.09562195563802914, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.3817403669236228e-05}, {"id": 84, "seek": 58408, "start": 594.24, "end": 600.8000000000001, "text": " their adaptation. If you have wiki in the path, that will be a standard URL. If you don't,", "tokens": [641, 21549, 13, 759, 291, 362, 261, 9850, 294, 264, 3100, 11, 300, 486, 312, 257, 3832, 12905, 13, 759, 291, 500, 380, 11], "temperature": 0.0, "avg_logprob": -0.09562195563802914, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.3817403669236228e-05}, {"id": 85, "seek": 58408, "start": 600.8000000000001, "end": 604.5600000000001, "text": " if you have things like display or something else, that would be a custom URL. In this case,", "tokens": [498, 291, 362, 721, 411, 4674, 420, 746, 1646, 11, 300, 576, 312, 257, 2375, 12905, 13, 682, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.09562195563802914, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.3817403669236228e-05}, {"id": 86, "seek": 58408, "start": 604.5600000000001, "end": 610.32, "text": " it's not applied. Then we need to mention the name, the key of the space that we're migrating.", "tokens": [309, 311, 406, 6456, 13, 1396, 321, 643, 281, 2152, 264, 1315, 11, 264, 2141, 295, 264, 1901, 300, 321, 434, 6186, 8754, 13], "temperature": 0.0, "avg_logprob": -0.09562195563802914, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.3817403669236228e-05}, {"id": 87, "seek": 61032, "start": 610.32, "end": 618.6400000000001, "text": " In this case, I made this dummy text demo space called FOS. This is the space that I want to", "tokens": [682, 341, 1389, 11, 286, 1027, 341, 35064, 2487, 10723, 1901, 1219, 479, 4367, 13, 639, 307, 264, 1901, 300, 286, 528, 281], "temperature": 0.0, "avg_logprob": -0.07332857658988551, "compression_ratio": 1.4866310160427807, "no_speech_prob": 2.108976332237944e-05}, {"id": 88, "seek": 61032, "start": 618.6400000000001, "end": 626.88, "text": " import. Now I have my profile. Let's see if I'm connected. Yes, I can start the migration.", "tokens": [974, 13, 823, 286, 362, 452, 7964, 13, 961, 311, 536, 498, 286, 478, 4582, 13, 1079, 11, 286, 393, 722, 264, 17011, 13], "temperature": 0.0, "avg_logprob": -0.07332857658988551, "compression_ratio": 1.4866310160427807, "no_speech_prob": 2.108976332237944e-05}, {"id": 89, "seek": 61032, "start": 629.0400000000001, "end": 633.7600000000001, "text": " The way in which the migration works is that you have a couple of steps. The first one is just", "tokens": [440, 636, 294, 597, 264, 17011, 1985, 307, 300, 291, 362, 257, 1916, 295, 4439, 13, 440, 700, 472, 307, 445], "temperature": 0.0, "avg_logprob": -0.07332857658988551, "compression_ratio": 1.4866310160427807, "no_speech_prob": 2.108976332237944e-05}, {"id": 90, "seek": 63376, "start": 633.76, "end": 641.36, "text": " the prerequisites. It would tell you what would be the requirements for migration. They usually", "tokens": [264, 38333, 15398, 3324, 13, 467, 576, 980, 291, 437, 576, 312, 264, 7728, 337, 17011, 13, 814, 2673], "temperature": 0.0, "avg_logprob": -0.11948582560745712, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.0443490939214826e-05}, {"id": 91, "seek": 63376, "start": 641.36, "end": 647.68, "text": " apply for larger imports. In our case, we're just going to do a 7-megabyte zip. It's not", "tokens": [3079, 337, 4833, 41596, 13, 682, 527, 1389, 11, 321, 434, 445, 516, 281, 360, 257, 1614, 12, 42800, 34529, 20730, 13, 467, 311, 406], "temperature": 0.0, "avg_logprob": -0.11948582560745712, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.0443490939214826e-05}, {"id": 92, "seek": 63376, "start": 647.68, "end": 652.16, "text": " that large. We don't need to adapt everything. Of course, in general, when you're running a", "tokens": [300, 2416, 13, 492, 500, 380, 643, 281, 6231, 1203, 13, 2720, 1164, 11, 294, 2674, 11, 562, 291, 434, 2614, 257], "temperature": 0.0, "avg_logprob": -0.11948582560745712, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.0443490939214826e-05}, {"id": 93, "seek": 63376, "start": 652.16, "end": 659.12, "text": " migration, you need to have enough resources on the machine, enough memory, disk space and all that.", "tokens": [17011, 11, 291, 643, 281, 362, 1547, 3593, 322, 264, 3479, 11, 1547, 4675, 11, 12355, 1901, 293, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.11948582560745712, "compression_ratio": 1.5578512396694215, "no_speech_prob": 2.0443490939214826e-05}, {"id": 94, "seek": 65912, "start": 659.12, "end": 666.08, "text": " Specifically, on the Xfiki side, you can also disable notifications in the Xfiki properties", "tokens": [26058, 11, 322, 264, 1783, 69, 9850, 1252, 11, 291, 393, 611, 28362, 13426, 294, 264, 1783, 69, 9850, 7221], "temperature": 0.0, "avg_logprob": -0.1626680005680431, "compression_ratio": 1.6771300448430493, "no_speech_prob": 1.2023286217299756e-05}, {"id": 95, "seek": 65912, "start": 666.08, "end": 672.16, "text": " file. You can also disable some listeners if you know that you will be importing very large", "tokens": [3991, 13, 509, 393, 611, 28362, 512, 23274, 498, 291, 458, 300, 291, 486, 312, 43866, 588, 2416], "temperature": 0.0, "avg_logprob": -0.1626680005680431, "compression_ratio": 1.6771300448430493, "no_speech_prob": 1.2023286217299756e-05}, {"id": 96, "seek": 65912, "start": 673.2, "end": 680.88, "text": " packages. The second step that I told you about previously with that analysis, if you are connected", "tokens": [17401, 13, 440, 1150, 1823, 300, 286, 1907, 291, 466, 8046, 365, 300, 5215, 11, 498, 291, 366, 4582], "temperature": 0.0, "avg_logprob": -0.1626680005680431, "compression_ratio": 1.6771300448430493, "no_speech_prob": 1.2023286217299756e-05}, {"id": 97, "seek": 65912, "start": 680.88, "end": 688.72, "text": " to the conference instance, you can see if you have on Xfiki any pages that already exist,", "tokens": [281, 264, 7586, 5197, 11, 291, 393, 536, 498, 291, 362, 322, 1783, 69, 9850, 604, 7183, 300, 1217, 2514, 11], "temperature": 0.0, "avg_logprob": -0.1626680005680431, "compression_ratio": 1.6771300448430493, "no_speech_prob": 1.2023286217299756e-05}, {"id": 98, "seek": 68872, "start": 688.72, "end": 693.36, "text": " so that if you have in the package that you're trying to import from conference pages that exist", "tokens": [370, 300, 498, 291, 362, 294, 264, 7372, 300, 291, 434, 1382, 281, 974, 490, 7586, 7183, 300, 2514], "temperature": 0.0, "avg_logprob": -0.07953555458470395, "compression_ratio": 1.7707317073170732, "no_speech_prob": 1.5196987078525126e-05}, {"id": 99, "seek": 68872, "start": 693.36, "end": 701.0400000000001, "text": " on Xfiki. We have some logs here. We can see that it looked at all the pages. We don't have", "tokens": [322, 1783, 69, 9850, 13, 492, 362, 512, 20820, 510, 13, 492, 393, 536, 300, 309, 2956, 412, 439, 264, 7183, 13, 492, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.07953555458470395, "compression_ratio": 1.7707317073170732, "no_speech_prob": 1.5196987078525126e-05}, {"id": 100, "seek": 68872, "start": 701.0400000000001, "end": 708.48, "text": " on Xfiki pages that we're trying to import right now. In the third step, it's just to", "tokens": [322, 1783, 69, 9850, 7183, 300, 321, 434, 1382, 281, 974, 558, 586, 13, 682, 264, 2636, 1823, 11, 309, 311, 445, 281], "temperature": 0.0, "avg_logprob": -0.07953555458470395, "compression_ratio": 1.7707317073170732, "no_speech_prob": 1.5196987078525126e-05}, {"id": 101, "seek": 68872, "start": 708.48, "end": 714.72, "text": " tell you to go to conference and export. It depends on what server version or conference", "tokens": [980, 291, 281, 352, 281, 7586, 293, 10725, 13, 467, 5946, 322, 437, 7154, 3037, 420, 7586], "temperature": 0.0, "avg_logprob": -0.07953555458470395, "compression_ratio": 1.7707317073170732, "no_speech_prob": 1.5196987078525126e-05}, {"id": 102, "seek": 71472, "start": 714.72, "end": 723.52, "text": " or cloud version you have. In this case, it's a cloud version with XML, full or custom export.", "tokens": [420, 4588, 3037, 291, 362, 13, 682, 341, 1389, 11, 309, 311, 257, 4588, 3037, 365, 43484, 11, 1577, 420, 2375, 10725, 13], "temperature": 0.0, "avg_logprob": -0.0795364761352539, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.3837610822520219e-05}, {"id": 103, "seek": 71472, "start": 724.64, "end": 729.12, "text": " You can choose again between those two. I already have it downloaded, so I'm not going to download", "tokens": [509, 393, 2826, 797, 1296, 729, 732, 13, 286, 1217, 362, 309, 21748, 11, 370, 286, 478, 406, 516, 281, 5484], "temperature": 0.0, "avg_logprob": -0.0795364761352539, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.3837610822520219e-05}, {"id": 104, "seek": 71472, "start": 729.12, "end": 741.2, "text": " it again. At this step, you just have to import your export file. Let me show you the example to", "tokens": [309, 797, 13, 1711, 341, 1823, 11, 291, 445, 362, 281, 974, 428, 10725, 3991, 13, 961, 385, 855, 291, 264, 1365, 281], "temperature": 0.0, "avg_logprob": -0.0795364761352539, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.3837610822520219e-05}, {"id": 105, "seek": 74120, "start": 741.2, "end": 758.0, "text": " import. If you have it on the same server, you can also specify the source in the server.", "tokens": [974, 13, 759, 291, 362, 309, 322, 264, 912, 7154, 11, 291, 393, 611, 16500, 264, 4009, 294, 264, 7154, 13], "temperature": 0.0, "avg_logprob": -0.09626882917740766, "compression_ratio": 1.8533333333333333, "no_speech_prob": 3.4462726034689695e-05}, {"id": 106, "seek": 74120, "start": 758.0, "end": 763.0400000000001, "text": " If you have Xfiki running on the same server that you have the files in, you can also specify it", "tokens": [759, 291, 362, 1783, 69, 9850, 2614, 322, 264, 912, 7154, 300, 291, 362, 264, 7098, 294, 11, 291, 393, 611, 16500, 309], "temperature": 0.0, "avg_logprob": -0.09626882917740766, "compression_ratio": 1.8533333333333333, "no_speech_prob": 3.4462726034689695e-05}, {"id": 107, "seek": 74120, "start": 763.0400000000001, "end": 768.96, "text": " directly. All of this configuration is the filter streams configuration that you can adapt.", "tokens": [3838, 13, 1057, 295, 341, 11694, 307, 264, 6608, 15842, 11694, 300, 291, 393, 6231, 13], "temperature": 0.0, "avg_logprob": -0.09626882917740766, "compression_ratio": 1.8533333333333333, "no_speech_prob": 3.4462726034689695e-05}, {"id": 108, "seek": 76896, "start": 768.96, "end": 777.76, "text": " It has some fields that are prefilled, but there's also a lot of power in other things that you can", "tokens": [467, 575, 512, 7909, 300, 366, 18417, 6261, 11, 457, 456, 311, 611, 257, 688, 295, 1347, 294, 661, 721, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.10245495846396999, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.8385937184793875e-05}, {"id": 109, "seek": 76896, "start": 777.76, "end": 784.24, "text": " configure. For example, you can also import users. You can do user ID mapping. For example,", "tokens": [22162, 13, 1171, 1365, 11, 291, 393, 611, 974, 5022, 13, 509, 393, 360, 4195, 7348, 18350, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10245495846396999, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.8385937184793875e-05}, {"id": 110, "seek": 76896, "start": 784.24, "end": 789.52, "text": " if you have an LDAP that has generated on the conference site some random number IDs,", "tokens": [498, 291, 362, 364, 33936, 4715, 300, 575, 10833, 322, 264, 7586, 3621, 512, 4974, 1230, 48212, 11], "temperature": 0.0, "avg_logprob": -0.10245495846396999, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.8385937184793875e-05}, {"id": 111, "seek": 76896, "start": 790.4000000000001, "end": 796.4000000000001, "text": " and you want to map those to the new users that you have created on Xfiki, that's something you", "tokens": [293, 291, 528, 281, 4471, 729, 281, 264, 777, 5022, 300, 291, 362, 2942, 322, 1783, 69, 9850, 11, 300, 311, 746, 291], "temperature": 0.0, "avg_logprob": -0.10245495846396999, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.8385937184793875e-05}, {"id": 112, "seek": 79640, "start": 796.4, "end": 803.28, "text": " can do. Also, you can choose if you want to keep the author and history metadata and so on.", "tokens": [393, 360, 13, 2743, 11, 291, 393, 2826, 498, 291, 528, 281, 1066, 264, 3793, 293, 2503, 26603, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13256310253608516, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.112482590950094e-05}, {"id": 113, "seek": 79640, "start": 804.56, "end": 809.92, "text": " You have some nice configuration that is quite granular. Once the configuration is done,", "tokens": [509, 362, 512, 1481, 11694, 300, 307, 1596, 39962, 13, 3443, 264, 11694, 307, 1096, 11], "temperature": 0.0, "avg_logprob": -0.13256310253608516, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.112482590950094e-05}, {"id": 114, "seek": 79640, "start": 810.56, "end": 816.64, "text": " you would import. This is the point where our documents are getting created.", "tokens": [291, 576, 974, 13, 639, 307, 264, 935, 689, 527, 8512, 366, 1242, 2942, 13], "temperature": 0.0, "avg_logprob": -0.13256310253608516, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.112482590950094e-05}, {"id": 115, "seek": 79640, "start": 818.56, "end": 823.36, "text": " Because they configured it, we also have the history. For example, here you see this was", "tokens": [1436, 436, 30538, 309, 11, 321, 611, 362, 264, 2503, 13, 1171, 1365, 11, 510, 291, 536, 341, 390], "temperature": 0.0, "avg_logprob": -0.13256310253608516, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.112482590950094e-05}, {"id": 116, "seek": 82336, "start": 823.36, "end": 830.4, "text": " created and then updated because on the conference side, I had multiple changes on those pages.", "tokens": [2942, 293, 550, 10588, 570, 322, 264, 7586, 1252, 11, 286, 632, 3866, 2962, 322, 729, 7183, 13], "temperature": 0.0, "avg_logprob": -0.13389274809095594, "compression_ratio": 1.5837837837837838, "no_speech_prob": 2.1401488993433304e-05}, {"id": 117, "seek": 82336, "start": 831.36, "end": 843.2, "text": " Now, we see that we have the pages imported with no error. With no error, it's of course a great", "tokens": [823, 11, 321, 536, 300, 321, 362, 264, 7183, 25524, 365, 572, 6713, 13, 2022, 572, 6713, 11, 309, 311, 295, 1164, 257, 869], "temperature": 0.0, "avg_logprob": -0.13389274809095594, "compression_ratio": 1.5837837837837838, "no_speech_prob": 2.1401488993433304e-05}, {"id": 118, "seek": 82336, "start": 843.2, "end": 850.32, "text": " thing, but you can also have errors, of course. In our experience, the most common errors are caused", "tokens": [551, 11, 457, 291, 393, 611, 362, 13603, 11, 295, 1164, 13, 682, 527, 1752, 11, 264, 881, 2689, 13603, 366, 7008], "temperature": 0.0, "avg_logprob": -0.13389274809095594, "compression_ratio": 1.5837837837837838, "no_speech_prob": 2.1401488993433304e-05}, {"id": 119, "seek": 85032, "start": 850.32, "end": 857.9200000000001, "text": " by unsupported characters or corrupted pages on the conference side. If you are trying this out", "tokens": [538, 2693, 10504, 14813, 4342, 420, 39480, 7183, 322, 264, 7586, 1252, 13, 759, 291, 366, 1382, 341, 484], "temperature": 0.0, "avg_logprob": -0.09143829345703125, "compression_ratio": 1.6790697674418604, "no_speech_prob": 3.2128133170772344e-05}, {"id": 120, "seek": 85032, "start": 857.9200000000001, "end": 863.44, "text": " and you have some errors, the logs should tell you what is the page that is causing the issue.", "tokens": [293, 291, 362, 512, 13603, 11, 264, 20820, 820, 980, 291, 437, 307, 264, 3028, 300, 307, 9853, 264, 2734, 13], "temperature": 0.0, "avg_logprob": -0.09143829345703125, "compression_ratio": 1.6790697674418604, "no_speech_prob": 3.2128133170772344e-05}, {"id": 121, "seek": 85032, "start": 865.12, "end": 870.0, "text": " You can then fix it on the conference side and then re-import or fix it manually in Xfiki,", "tokens": [509, 393, 550, 3191, 309, 322, 264, 7586, 1252, 293, 550, 319, 12, 20737, 420, 3191, 309, 16945, 294, 1783, 69, 9850, 11], "temperature": 0.0, "avg_logprob": -0.09143829345703125, "compression_ratio": 1.6790697674418604, "no_speech_prob": 3.2128133170772344e-05}, {"id": 122, "seek": 85032, "start": 870.88, "end": 878.0, "text": " whatever suits you best. Now, we have the pages imported. This is a post-import", "tokens": [2035, 15278, 291, 1151, 13, 823, 11, 321, 362, 264, 7183, 25524, 13, 639, 307, 257, 2183, 12, 20737], "temperature": 0.0, "avg_logprob": -0.09143829345703125, "compression_ratio": 1.6790697674418604, "no_speech_prob": 3.2128133170772344e-05}, {"id": 123, "seek": 87800, "start": 878.0, "end": 883.68, "text": " fixes check that we can also perform in case we have pages that were imported that don't have", "tokens": [32539, 1520, 300, 321, 393, 611, 2042, 294, 1389, 321, 362, 7183, 300, 645, 25524, 300, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.12130253573498094, "compression_ratio": 1.75, "no_speech_prob": 4.317933417041786e-05}, {"id": 124, "seek": 87800, "start": 884.88, "end": 890.48, "text": " a parent or pages that have corrupted parents. Both in Confluence and Xfiki,", "tokens": [257, 2596, 420, 7183, 300, 362, 39480, 3152, 13, 6767, 294, 11701, 40432, 293, 1783, 69, 9850, 11], "temperature": 0.0, "avg_logprob": -0.12130253573498094, "compression_ratio": 1.75, "no_speech_prob": 4.317933417041786e-05}, {"id": 125, "seek": 87800, "start": 890.48, "end": 893.92, "text": " we have the hierarchy system. In Xfiki, we have nest pages.", "tokens": [321, 362, 264, 22333, 1185, 13, 682, 1783, 69, 9850, 11, 321, 362, 15646, 7183, 13], "temperature": 0.0, "avg_logprob": -0.12130253573498094, "compression_ratio": 1.75, "no_speech_prob": 4.317933417041786e-05}, {"id": 126, "seek": 87800, "start": 895.84, "end": 902.72, "text": " In Confluence, you may have situations where the parent pages are corrupted. If you would have had", "tokens": [682, 11701, 40432, 11, 291, 815, 362, 6851, 689, 264, 2596, 7183, 366, 39480, 13, 759, 291, 576, 362, 632], "temperature": 0.0, "avg_logprob": -0.12130253573498094, "compression_ratio": 1.75, "no_speech_prob": 4.317933417041786e-05}, {"id": 127, "seek": 90272, "start": 902.72, "end": 909.6800000000001, "text": " that, you would see it in these reports. It's not the case here. Finally, we would need to", "tokens": [300, 11, 291, 576, 536, 309, 294, 613, 7122, 13, 467, 311, 406, 264, 1389, 510, 13, 6288, 11, 321, 576, 643, 281], "temperature": 0.0, "avg_logprob": -0.06996092925200591, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.8027454643743113e-05}, {"id": 128, "seek": 90272, "start": 909.6800000000001, "end": 914.72, "text": " recreate the hierarchy that we had in Confluence. You can see now that the pages that I have", "tokens": [25833, 264, 22333, 300, 321, 632, 294, 11701, 40432, 13, 509, 393, 536, 586, 300, 264, 7183, 300, 286, 362], "temperature": 0.0, "avg_logprob": -0.06996092925200591, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.8027454643743113e-05}, {"id": 129, "seek": 90272, "start": 914.72, "end": 924.4, "text": " imported are flat. We have just one level hierarchy here. Now, I'm going to execute the nested pages", "tokens": [25524, 366, 4962, 13, 492, 362, 445, 472, 1496, 22333, 510, 13, 823, 11, 286, 478, 516, 281, 14483, 264, 15646, 292, 7183], "temperature": 0.0, "avg_logprob": -0.06996092925200591, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.8027454643743113e-05}, {"id": 130, "seek": 92440, "start": 924.4, "end": 933.12, "text": " migration tool that we also have at Xfiki. The pages will be moved into their parents", "tokens": [17011, 2290, 300, 321, 611, 362, 412, 1783, 69, 9850, 13, 440, 7183, 486, 312, 4259, 666, 641, 3152], "temperature": 0.0, "avg_logprob": -0.06592552426835181, "compression_ratio": 1.5082872928176796, "no_speech_prob": 6.0334255067573395e-06}, {"id": 131, "seek": 92440, "start": 933.76, "end": 937.52, "text": " according to the hierarchy that they had in Confluence. As you see, it's converting all the", "tokens": [4650, 281, 264, 22333, 300, 436, 632, 294, 11701, 40432, 13, 1018, 291, 536, 11, 309, 311, 29942, 439, 264], "temperature": 0.0, "avg_logprob": -0.06592552426835181, "compression_ratio": 1.5082872928176796, "no_speech_prob": 6.0334255067573395e-06}, {"id": 132, "seek": 92440, "start": 937.52, "end": 946.48, "text": " pages and they will be moved in the right place. Okay, cool. Now, we have a migration done. You", "tokens": [7183, 293, 436, 486, 312, 4259, 294, 264, 558, 1081, 13, 1033, 11, 1627, 13, 823, 11, 321, 362, 257, 17011, 1096, 13, 509], "temperature": 0.0, "avg_logprob": -0.06592552426835181, "compression_ratio": 1.5082872928176796, "no_speech_prob": 6.0334255067573395e-06}, {"id": 133, "seek": 94648, "start": 946.48, "end": 954.5600000000001, "text": " can look at the pages to see all of your content. You also have, again, a lot of the macros that", "tokens": [393, 574, 412, 264, 7183, 281, 536, 439, 295, 428, 2701, 13, 509, 611, 362, 11, 797, 11, 257, 688, 295, 264, 7912, 2635, 300], "temperature": 0.0, "avg_logprob": -0.11943409659645775, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.633943823748268e-06}, {"id": 134, "seek": 94648, "start": 955.9200000000001, "end": 964.0, "text": " are also installed and can be reused. For the macros, the pro macros that I told you about,", "tokens": [366, 611, 8899, 293, 393, 312, 319, 4717, 13, 1171, 264, 7912, 2635, 11, 264, 447, 7912, 2635, 300, 286, 1907, 291, 466, 11], "temperature": 0.0, "avg_logprob": -0.11943409659645775, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.633943823748268e-06}, {"id": 135, "seek": 94648, "start": 964.0, "end": 971.9200000000001, "text": " the bridge macros on our side are packaged. They are open source. They are public here,", "tokens": [264, 7283, 7912, 2635, 322, 527, 1252, 366, 38162, 13, 814, 366, 1269, 4009, 13, 814, 366, 1908, 510, 11], "temperature": 0.0, "avg_logprob": -0.11943409659645775, "compression_ratio": 1.6046511627906976, "no_speech_prob": 7.633943823748268e-06}, {"id": 136, "seek": 97192, "start": 971.92, "end": 980.24, "text": " if you want to check them out or repackage them. On our side, they are packaged as under a license", "tokens": [498, 291, 528, 281, 1520, 552, 484, 420, 1085, 501, 609, 552, 13, 1282, 527, 1252, 11, 436, 366, 38162, 382, 833, 257, 10476], "temperature": 0.0, "avg_logprob": -0.10846907562679714, "compression_ratio": 1.6264367816091954, "no_speech_prob": 2.4259363271994516e-05}, {"id": 137, "seek": 97192, "start": 980.24, "end": 988.0799999999999, "text": " to be able to further support the development of the product. If you want to check them out", "tokens": [281, 312, 1075, 281, 3052, 1406, 264, 3250, 295, 264, 1674, 13, 759, 291, 528, 281, 1520, 552, 484], "temperature": 0.0, "avg_logprob": -0.10846907562679714, "compression_ratio": 1.6264367816091954, "no_speech_prob": 2.4259363271994516e-05}, {"id": 138, "seek": 97192, "start": 988.0799999999999, "end": 997.04, "text": " or contribute to them, you can see them on our Git. We have here a Confluence migration done", "tokens": [420, 10586, 281, 552, 11, 291, 393, 536, 552, 322, 527, 16939, 13, 492, 362, 510, 257, 11701, 40432, 17011, 1096], "temperature": 0.0, "avg_logprob": -0.10846907562679714, "compression_ratio": 1.6264367816091954, "no_speech_prob": 2.4259363271994516e-05}, {"id": 139, "seek": 99704, "start": 997.04, "end": 1005.4399999999999, "text": " very quickly and without much hassle. We saw the Confluence migration. Now, let's see the SharePoint", "tokens": [588, 2661, 293, 1553, 709, 39526, 13, 492, 1866, 264, 11701, 40432, 17011, 13, 823, 11, 718, 311, 536, 264, 14945, 18705], "temperature": 0.0, "avg_logprob": -0.08235703287897883, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.0948973795166239e-05}, {"id": 140, "seek": 99704, "start": 1005.4399999999999, "end": 1013.36, "text": " one. The way in which we migrate from Confluence is based on the XML export. From SharePoint,", "tokens": [472, 13, 440, 636, 294, 597, 321, 31821, 490, 11701, 40432, 307, 2361, 322, 264, 43484, 10725, 13, 3358, 14945, 18705, 11], "temperature": 0.0, "avg_logprob": -0.08235703287897883, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.0948973795166239e-05}, {"id": 141, "seek": 99704, "start": 1013.36, "end": 1022.3199999999999, "text": " it's very different. In SharePoint, you have the option to export to CSV. If you're using", "tokens": [309, 311, 588, 819, 13, 682, 14945, 18705, 11, 291, 362, 264, 3614, 281, 10725, 281, 48814, 13, 759, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.08235703287897883, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.0948973795166239e-05}, {"id": 142, "seek": 102232, "start": 1022.32, "end": 1027.2, "text": " SharePoint as a knowledge management tools and you have your documents with a bit of metadata,", "tokens": [14945, 18705, 382, 257, 3601, 4592, 3873, 293, 291, 362, 428, 8512, 365, 257, 857, 295, 26603, 11], "temperature": 0.0, "avg_logprob": -0.12480543960224498, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.3357615545101e-06}, {"id": 143, "seek": 102232, "start": 1027.2, "end": 1033.28, "text": " so like we have in this case, department could be considered a metadata or a structure data", "tokens": [370, 411, 321, 362, 294, 341, 1389, 11, 5882, 727, 312, 4888, 257, 26603, 420, 257, 3877, 1412], "temperature": 0.0, "avg_logprob": -0.12480543960224498, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.3357615545101e-06}, {"id": 144, "seek": 102232, "start": 1033.28, "end": 1041.68, "text": " of field that you can check or uncheck and change. The pages have a form structure.", "tokens": [295, 2519, 300, 291, 393, 1520, 420, 46672, 293, 1319, 13, 440, 7183, 362, 257, 1254, 3877, 13], "temperature": 0.0, "avg_logprob": -0.12480543960224498, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.3357615545101e-06}, {"id": 145, "seek": 102232, "start": 1042.88, "end": 1049.68, "text": " If you have this type of data, one thing that you can do is to export to CSV, then create the same", "tokens": [759, 291, 362, 341, 2010, 295, 1412, 11, 472, 551, 300, 291, 393, 360, 307, 281, 10725, 281, 48814, 11, 550, 1884, 264, 912], "temperature": 0.0, "avg_logprob": -0.12480543960224498, "compression_ratio": 1.669683257918552, "no_speech_prob": 6.3357615545101e-06}, {"id": 146, "seek": 104968, "start": 1049.68, "end": 1056.4, "text": " data structure on Xwiki. On Xwiki, we have an application that is called App Within Minutes", "tokens": [1412, 3877, 322, 1783, 86, 9850, 13, 1282, 1783, 86, 9850, 11, 321, 362, 364, 3861, 300, 307, 1219, 3132, 15996, 2829, 1819], "temperature": 0.0, "avg_logprob": -0.12525734371609157, "compression_ratio": 1.6444444444444444, "no_speech_prob": 8.656142199470196e-06}, {"id": 147, "seek": 104968, "start": 1056.4, "end": 1063.52, "text": " that allows you to create structured data systems. Here, I already have an example made,", "tokens": [300, 4045, 291, 281, 1884, 18519, 1412, 3652, 13, 1692, 11, 286, 1217, 362, 364, 1365, 1027, 11], "temperature": 0.0, "avg_logprob": -0.12525734371609157, "compression_ratio": 1.6444444444444444, "no_speech_prob": 8.656142199470196e-06}, {"id": 148, "seek": 104968, "start": 1063.52, "end": 1068.8, "text": " but we can look at the structure. Basically, I just created the same structure that I had", "tokens": [457, 321, 393, 574, 412, 264, 3877, 13, 8537, 11, 286, 445, 2942, 264, 912, 3877, 300, 286, 632], "temperature": 0.0, "avg_logprob": -0.12525734371609157, "compression_ratio": 1.6444444444444444, "no_speech_prob": 8.656142199470196e-06}, {"id": 149, "seek": 106880, "start": 1068.8, "end": 1080.0, "text": " in the SharePoint example, so title, department, reviewed, and finally the content of my documents.", "tokens": [294, 264, 14945, 18705, 1365, 11, 370, 4876, 11, 5882, 11, 18429, 11, 293, 2721, 264, 2701, 295, 452, 8512, 13], "temperature": 0.0, "avg_logprob": -0.20913310926787707, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2403837899910286e-05}, {"id": 150, "seek": 106880, "start": 1080.0, "end": 1088.3999999999999, "text": " Then, once I have that structure done, I can use the batch import application. Sorry, not here.", "tokens": [1396, 11, 1564, 286, 362, 300, 3877, 1096, 11, 286, 393, 764, 264, 15245, 974, 3861, 13, 4919, 11, 406, 510, 13], "temperature": 0.0, "avg_logprob": -0.20913310926787707, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2403837899910286e-05}, {"id": 151, "seek": 108840, "start": 1088.4, "end": 1101.76, "text": " Okay. With the batch import application, I would import the CSV that I have just", "tokens": [1033, 13, 2022, 264, 15245, 974, 3861, 11, 286, 576, 974, 264, 48814, 300, 286, 362, 445], "temperature": 0.0, "avg_logprob": -0.10464430567043931, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.1870397378952475e-06}, {"id": 152, "seek": 108840, "start": 1102.64, "end": 1109.92, "text": " got from SharePoint. I'm able to map the columns from the CSV to fields in Xwiki", "tokens": [658, 490, 14945, 18705, 13, 286, 478, 1075, 281, 4471, 264, 13766, 490, 264, 48814, 281, 7909, 294, 1783, 86, 9850], "temperature": 0.0, "avg_logprob": -0.10464430567043931, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.1870397378952475e-06}, {"id": 153, "seek": 108840, "start": 1109.92, "end": 1116.0800000000002, "text": " that I have just created. Here is the mapping that I just did before. You can choose whatever you", "tokens": [300, 286, 362, 445, 2942, 13, 1692, 307, 264, 18350, 300, 286, 445, 630, 949, 13, 509, 393, 2826, 2035, 291], "temperature": 0.0, "avg_logprob": -0.10464430567043931, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.1870397378952475e-06}, {"id": 154, "seek": 111608, "start": 1116.08, "end": 1122.08, "text": " want, even exclude some columns if it's the case. Then, we preview the mapping,", "tokens": [528, 11, 754, 33536, 512, 13766, 498, 309, 311, 264, 1389, 13, 1396, 11, 321, 14281, 264, 18350, 11], "temperature": 0.0, "avg_logprob": -0.11024747292200725, "compression_ratio": 1.4555555555555555, "no_speech_prob": 9.075179150386248e-06}, {"id": 155, "seek": 111608, "start": 1123.28, "end": 1129.12, "text": " and this is what they would look like on the Xwiki side. You can see that all the content", "tokens": [293, 341, 307, 437, 436, 576, 574, 411, 322, 264, 1783, 86, 9850, 1252, 13, 509, 393, 536, 300, 439, 264, 2701], "temperature": 0.0, "avg_logprob": -0.11024747292200725, "compression_ratio": 1.4555555555555555, "no_speech_prob": 9.075179150386248e-06}, {"id": 156, "seek": 111608, "start": 1130.96, "end": 1139.1999999999998, "text": " is getting migrated. Let's just see a page. Here, you can say what you want to happen if you", "tokens": [307, 1242, 48329, 13, 961, 311, 445, 536, 257, 3028, 13, 1692, 11, 291, 393, 584, 437, 291, 528, 281, 1051, 498, 291], "temperature": 0.0, "avg_logprob": -0.11024747292200725, "compression_ratio": 1.4555555555555555, "no_speech_prob": 9.075179150386248e-06}, {"id": 157, "seek": 113920, "start": 1139.2, "end": 1148.0, "text": " have duplicates. Then, we do the import, and the final result is something like this. All", "tokens": [362, 17154, 1024, 13, 1396, 11, 321, 360, 264, 974, 11, 293, 264, 2572, 1874, 307, 746, 411, 341, 13, 1057], "temperature": 0.0, "avg_logprob": -0.11956343712744774, "compression_ratio": 1.5185185185185186, "no_speech_prob": 8.262175470008515e-06}, {"id": 158, "seek": 113920, "start": 1148.0, "end": 1155.1200000000001, "text": " the pages were imported, and if you go to a page, you can see that you have this structured form", "tokens": [264, 7183, 645, 25524, 11, 293, 498, 291, 352, 281, 257, 3028, 11, 291, 393, 536, 300, 291, 362, 341, 18519, 1254], "temperature": 0.0, "avg_logprob": -0.11956343712744774, "compression_ratio": 1.5185185185185186, "no_speech_prob": 8.262175470008515e-06}, {"id": 159, "seek": 113920, "start": 1155.1200000000001, "end": 1163.3600000000001, "text": " type, and you can further edit it. Okay. That's all for the two examples. Sorry, I had to go through", "tokens": [2010, 11, 293, 291, 393, 3052, 8129, 309, 13, 1033, 13, 663, 311, 439, 337, 264, 732, 5110, 13, 4919, 11, 286, 632, 281, 352, 807], "temperature": 0.0, "avg_logprob": -0.11956343712744774, "compression_ratio": 1.5185185185185186, "no_speech_prob": 8.262175470008515e-06}, {"id": 160, "seek": 116336, "start": 1163.36, "end": 1169.84, "text": " them very quickly. There are a lot of things that you can do to migrate, and of course,", "tokens": [552, 588, 2661, 13, 821, 366, 257, 688, 295, 721, 300, 291, 393, 360, 281, 31821, 11, 293, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.1276460619115118, "compression_ratio": 1.5056818181818181, "no_speech_prob": 4.307112249080092e-05}, {"id": 161, "seek": 116336, "start": 1169.84, "end": 1177.04, "text": " we're very happy to facilitate any migration from any other preparatory tool to get more", "tokens": [321, 434, 588, 2055, 281, 20207, 604, 17011, 490, 604, 661, 8231, 4745, 2290, 281, 483, 544], "temperature": 0.0, "avg_logprob": -0.1276460619115118, "compression_ratio": 1.5056818181818181, "no_speech_prob": 4.307112249080092e-05}, {"id": 162, "seek": 116336, "start": 1177.04, "end": 1190.7199999999998, "text": " users to open source. Thank you if you have any question. No questions? That clear? Yes.", "tokens": [5022, 281, 1269, 4009, 13, 1044, 291, 498, 291, 362, 604, 1168, 13, 883, 1651, 30, 663, 1850, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.1276460619115118, "compression_ratio": 1.5056818181818181, "no_speech_prob": 4.307112249080092e-05}, {"id": 163, "seek": 119072, "start": 1190.72, "end": 1192.72, "text": " Yes, please.", "tokens": [1079, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.2542169819707456, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.00040328307659365237}, {"id": 164, "seek": 119072, "start": 1192.72, "end": 1199.2, "text": " How would you deal with migration from basically just the directory with all its office documents?", "tokens": [1012, 576, 291, 2028, 365, 17011, 490, 1936, 445, 264, 21120, 365, 439, 1080, 3398, 8512, 30], "temperature": 0.0, "avg_logprob": -0.2542169819707456, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.00040328307659365237}, {"id": 165, "seek": 119072, "start": 1200.64, "end": 1204.56, "text": " So, how do we deal with migration from the directory of all the office documents?", "tokens": [407, 11, 577, 360, 321, 2028, 365, 17011, 490, 264, 21120, 295, 439, 264, 3398, 8512, 30], "temperature": 0.0, "avg_logprob": -0.2542169819707456, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.00040328307659365237}, {"id": 166, "seek": 119072, "start": 1210.08, "end": 1216.32, "text": " So, two things that we can do. So, when you import office documents into Xwiki,", "tokens": [407, 11, 732, 721, 300, 321, 393, 360, 13, 407, 11, 562, 291, 974, 3398, 8512, 666, 1783, 86, 9850, 11], "temperature": 0.0, "avg_logprob": -0.2542169819707456, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.00040328307659365237}, {"id": 167, "seek": 121632, "start": 1216.32, "end": 1222.8, "text": " we do have abundant integration with LibreOffice that allows you to convert office pages into", "tokens": [321, 360, 362, 30657, 10980, 365, 15834, 265, 29745, 573, 300, 4045, 291, 281, 7620, 3398, 7183, 666], "temperature": 0.0, "avg_logprob": -0.09916731323858705, "compression_ratio": 1.6260504201680672, "no_speech_prob": 5.719566252082586e-05}, {"id": 168, "seek": 121632, "start": 1222.8, "end": 1229.2, "text": " Xwiki pages, but that's page by page. Or, if you have any sort of directory of office files, what", "tokens": [1783, 86, 9850, 7183, 11, 457, 300, 311, 3028, 538, 3028, 13, 1610, 11, 498, 291, 362, 604, 1333, 295, 21120, 295, 3398, 7098, 11, 437], "temperature": 0.0, "avg_logprob": -0.09916731323858705, "compression_ratio": 1.6260504201680672, "no_speech_prob": 5.719566252082586e-05}, {"id": 169, "seek": 121632, "start": 1229.2, "end": 1235.52, "text": " you can do is to actually create manually this type of a CSV where you put in a row the content,", "tokens": [291, 393, 360, 307, 281, 767, 1884, 16945, 341, 2010, 295, 257, 48814, 689, 291, 829, 294, 257, 5386, 264, 2701, 11], "temperature": 0.0, "avg_logprob": -0.09916731323858705, "compression_ratio": 1.6260504201680672, "no_speech_prob": 5.719566252082586e-05}, {"id": 170, "seek": 121632, "start": 1235.52, "end": 1240.48, "text": " and in this way, you can also add some metadata, for example, if you want to organize them in some", "tokens": [293, 294, 341, 636, 11, 291, 393, 611, 909, 512, 26603, 11, 337, 1365, 11, 498, 291, 528, 281, 13859, 552, 294, 512], "temperature": 0.0, "avg_logprob": -0.09916731323858705, "compression_ratio": 1.6260504201680672, "no_speech_prob": 5.719566252082586e-05}, {"id": 171, "seek": 124048, "start": 1240.48, "end": 1246.56, "text": " departments or responsible person, so on. You can do that and then still use the batch import.", "tokens": [15326, 420, 6250, 954, 11, 370, 322, 13, 509, 393, 360, 300, 293, 550, 920, 764, 264, 15245, 974, 13], "temperature": 0.0, "avg_logprob": -0.10224838878797449, "compression_ratio": 1.6651785714285714, "no_speech_prob": 3.5247339837951586e-05}, {"id": 172, "seek": 124048, "start": 1246.56, "end": 1252.48, "text": " At the moment, we don't have an existing tool for just feeding some files. We have something in", "tokens": [1711, 264, 1623, 11, 321, 500, 380, 362, 364, 6741, 2290, 337, 445, 12919, 512, 7098, 13, 492, 362, 746, 294], "temperature": 0.0, "avg_logprob": -0.10224838878797449, "compression_ratio": 1.6651785714285714, "no_speech_prob": 3.5247339837951586e-05}, {"id": 173, "seek": 124048, "start": 1252.48, "end": 1259.76, "text": " progress also with batch import, but yeah, the one option is to either convert them one by one", "tokens": [4205, 611, 365, 15245, 974, 11, 457, 1338, 11, 264, 472, 3614, 307, 281, 2139, 7620, 552, 472, 538, 472], "temperature": 0.0, "avg_logprob": -0.10224838878797449, "compression_ratio": 1.6651785714285714, "no_speech_prob": 3.5247339837951586e-05}, {"id": 174, "seek": 124048, "start": 1259.76, "end": 1265.84, "text": " or use the batch import, but you would still need to organize them in a sort of a list.", "tokens": [420, 764, 264, 15245, 974, 11, 457, 291, 576, 920, 643, 281, 13859, 552, 294, 257, 1333, 295, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.10224838878797449, "compression_ratio": 1.6651785714285714, "no_speech_prob": 3.5247339837951586e-05}, {"id": 175, "seek": 126584, "start": 1265.84, "end": 1270.48, "text": " Yeah, that answers it. Yes, please.", "tokens": [50364, 865, 11, 300, 6338, 309, 13, 1079, 11, 1767, 13, 50596], "temperature": 0.0, "avg_logprob": -0.319456724020151, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.0002079856931231916}, {"id": 176, "seek": 129584, "start": 1296.1599999999999, "end": 1318.48, "text": " Thank you for the question. Just to repeat it, if it's the case, the question is if we can,", "tokens": [1044, 291, 337, 264, 1168, 13, 1449, 281, 7149, 309, 11, 498, 309, 311, 264, 1389, 11, 264, 1168, 307, 498, 321, 393, 11], "temperature": 0.0, "avg_logprob": -0.2069244384765625, "compression_ratio": 1.408, "no_speech_prob": 0.03039177507162094}, {"id": 177, "seek": 129584, "start": 1320.0, "end": 1324.6399999999999, "text": " if we facilitate in any way the addition of metadata or the cleanup, I would assume.", "tokens": [498, 321, 20207, 294, 604, 636, 264, 4500, 295, 26603, 420, 264, 40991, 11, 286, 576, 6552, 13], "temperature": 0.0, "avg_logprob": -0.2069244384765625, "compression_ratio": 1.408, "no_speech_prob": 0.03039177507162094}, {"id": 178, "seek": 132464, "start": 1324.64, "end": 1333.8400000000001, "text": " So, on the metadata, as just mentioned now, for the office part, if you have office documents", "tokens": [407, 11, 322, 264, 26603, 11, 382, 445, 2835, 586, 11, 337, 264, 3398, 644, 11, 498, 291, 362, 3398, 8512], "temperature": 0.0, "avg_logprob": -0.11926827165815565, "compression_ratio": 1.6488095238095237, "no_speech_prob": 5.632321699522436e-05}, {"id": 179, "seek": 132464, "start": 1333.8400000000001, "end": 1340.96, "text": " in any way, you can adapt that CSV file before migrating. So, for example, if you have office", "tokens": [294, 604, 636, 11, 291, 393, 6231, 300, 48814, 3991, 949, 6186, 8754, 13, 407, 11, 337, 1365, 11, 498, 291, 362, 3398], "temperature": 0.0, "avg_logprob": -0.11926827165815565, "compression_ratio": 1.6488095238095237, "no_speech_prob": 5.632321699522436e-05}, {"id": 180, "seek": 132464, "start": 1340.96, "end": 1349.76, "text": " files or if you have an export from SharePoint, but it's not all documents have metadata,", "tokens": [7098, 420, 498, 291, 362, 364, 10725, 490, 14945, 18705, 11, 457, 309, 311, 406, 439, 8512, 362, 26603, 11], "temperature": 0.0, "avg_logprob": -0.11926827165815565, "compression_ratio": 1.6488095238095237, "no_speech_prob": 5.632321699522436e-05}, {"id": 181, "seek": 134976, "start": 1349.76, "end": 1354.64, "text": " you can add them manually in the CSV that you do. On the conference side,", "tokens": [291, 393, 909, 552, 16945, 294, 264, 48814, 300, 291, 360, 13, 1282, 264, 7586, 1252, 11], "temperature": 0.0, "avg_logprob": -0.15169474657844095, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.755531674367376e-05}, {"id": 182, "seek": 134976, "start": 1355.68, "end": 1362.48, "text": " not that much. You can, of course, so the labels and everything are imported,", "tokens": [406, 300, 709, 13, 509, 393, 11, 295, 1164, 11, 370, 264, 16949, 293, 1203, 366, 25524, 11], "temperature": 0.0, "avg_logprob": -0.15169474657844095, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.755531674367376e-05}, {"id": 183, "seek": 134976, "start": 1362.48, "end": 1368.24, "text": " but to be straight here, it depends more on what you have on conference, because basically,", "tokens": [457, 281, 312, 2997, 510, 11, 309, 5946, 544, 322, 437, 291, 362, 322, 7586, 11, 570, 1936, 11], "temperature": 0.0, "avg_logprob": -0.15169474657844095, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.755531674367376e-05}, {"id": 184, "seek": 134976, "start": 1368.24, "end": 1372.96, "text": " with the migration from conference, we just take everything that you have and put it into Xfiki.", "tokens": [365, 264, 17011, 490, 7586, 11, 321, 445, 747, 1203, 300, 291, 362, 293, 829, 309, 666, 1783, 69, 9850, 13], "temperature": 0.0, "avg_logprob": -0.15169474657844095, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.755531674367376e-05}, {"id": 185, "seek": 137296, "start": 1372.96, "end": 1380.48, "text": " We don't really facilitate any cleanup, but we allow you to migrate labels and macros that also", "tokens": [492, 500, 380, 534, 20207, 604, 40991, 11, 457, 321, 2089, 291, 281, 31821, 16949, 293, 7912, 2635, 300, 611], "temperature": 0.0, "avg_logprob": -0.15790532497649498, "compression_ratio": 1.3714285714285714, "no_speech_prob": 7.436122541548684e-05}, {"id": 186, "seek": 137296, "start": 1380.48, "end": 1387.8400000000001, "text": " do reports and all that. But for conference, specifically, it's a bit difficult to add metadata.", "tokens": [360, 7122, 293, 439, 300, 13, 583, 337, 7586, 11, 4682, 11, 309, 311, 257, 857, 2252, 281, 909, 26603, 13], "temperature": 0.0, "avg_logprob": -0.15790532497649498, "compression_ratio": 1.3714285714285714, "no_speech_prob": 7.436122541548684e-05}, {"id": 187, "seek": 138784, "start": 1387.84, "end": 1395.12, "text": " Do you also migrate pages and lists? Sorry? For SharePoint, do you also migrate pages and lists", "tokens": [1144, 291, 611, 31821, 7183, 293, 14511, 30, 4919, 30, 1171, 14945, 18705, 11, 360, 291, 611, 31821, 7183, 293, 14511], "temperature": 0.0, "avg_logprob": -0.17394474731094536, "compression_ratio": 1.8181818181818181, "no_speech_prob": 5.2012514061061665e-05}, {"id": 188, "seek": 138784, "start": 1395.12, "end": 1400.56, "text": " so it will be not only documents? From SharePoint, at the moment, we migrate documents,", "tokens": [370, 309, 486, 312, 406, 787, 8512, 30, 3358, 14945, 18705, 11, 412, 264, 1623, 11, 321, 31821, 8512, 11], "temperature": 0.0, "avg_logprob": -0.17394474731094536, "compression_ratio": 1.8181818181818181, "no_speech_prob": 5.2012514061061665e-05}, {"id": 189, "seek": 138784, "start": 1400.56, "end": 1406.8799999999999, "text": " so Word documents. There are other tools that we're working on with office integrations and", "tokens": [370, 8725, 8512, 13, 821, 366, 661, 3873, 300, 321, 434, 1364, 322, 365, 3398, 3572, 763, 293], "temperature": 0.0, "avg_logprob": -0.17394474731094536, "compression_ratio": 1.8181818181818181, "no_speech_prob": 5.2012514061061665e-05}, {"id": 190, "seek": 138784, "start": 1407.52, "end": 1415.36, "text": " Microsoft integration, but yeah, at the moment, we only import documents. Thank you.", "tokens": [8116, 10980, 11, 457, 1338, 11, 412, 264, 1623, 11, 321, 787, 974, 8512, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.17394474731094536, "compression_ratio": 1.8181818181818181, "no_speech_prob": 5.2012514061061665e-05}, {"id": 191, "seek": 141536, "start": 1415.36, "end": 1422.9599999999998, "text": " Maybe you told it, what about the user's permission right to view part of the document?", "tokens": [2704, 291, 1907, 309, 11, 437, 466, 264, 4195, 311, 11226, 558, 281, 1910, 644, 295, 264, 4166, 30], "temperature": 0.0, "avg_logprob": -0.16788588013759878, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.0001597890368429944}, {"id": 192, "seek": 141536, "start": 1424.6399999999999, "end": 1431.12, "text": " Thank you. That's a really good question. The question is for user rights or permissions.", "tokens": [1044, 291, 13, 663, 311, 257, 534, 665, 1168, 13, 440, 1168, 307, 337, 4195, 4601, 420, 32723, 13], "temperature": 0.0, "avg_logprob": -0.16788588013759878, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.0001597890368429944}, {"id": 193, "seek": 141536, "start": 1431.9199999999998, "end": 1438.24, "text": " That's in the part of the dependencies or integrations that we need to mind.", "tokens": [663, 311, 294, 264, 644, 295, 264, 36606, 420, 3572, 763, 300, 321, 643, 281, 1575, 13], "temperature": 0.0, "avg_logprob": -0.16788588013759878, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.0001597890368429944}, {"id": 194, "seek": 141536, "start": 1439.28, "end": 1444.9599999999998, "text": " At Xfiki, if you migrate from conference, for example, and you have native conference users,", "tokens": [1711, 1783, 69, 9850, 11, 498, 291, 31821, 490, 7586, 11, 337, 1365, 11, 293, 291, 362, 8470, 7586, 5022, 11], "temperature": 0.0, "avg_logprob": -0.16788588013759878, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.0001597890368429944}, {"id": 195, "seek": 144496, "start": 1444.96, "end": 1449.52, "text": " yes, we have the option to import them. You just need to configure that in the", "tokens": [2086, 11, 321, 362, 264, 3614, 281, 974, 552, 13, 509, 445, 643, 281, 22162, 300, 294, 264], "temperature": 0.0, "avg_logprob": -0.10076124017888849, "compression_ratio": 1.848360655737705, "no_speech_prob": 3.11533258354757e-05}, {"id": 196, "seek": 144496, "start": 1449.52, "end": 1455.2, "text": " filter streams, and you can import the users, but not the permissions. The issue with the", "tokens": [6608, 15842, 11, 293, 291, 393, 974, 264, 5022, 11, 457, 406, 264, 32723, 13, 440, 2734, 365, 264], "temperature": 0.0, "avg_logprob": -0.10076124017888849, "compression_ratio": 1.848360655737705, "no_speech_prob": 3.11533258354757e-05}, {"id": 197, "seek": 144496, "start": 1455.2, "end": 1459.8400000000001, "text": " permissions is that the systems are very different. In conference, you have a very different", "tokens": [32723, 307, 300, 264, 3652, 366, 588, 819, 13, 682, 7586, 11, 291, 362, 257, 588, 819], "temperature": 0.0, "avg_logprob": -0.10076124017888849, "compression_ratio": 1.848360655737705, "no_speech_prob": 3.11533258354757e-05}, {"id": 198, "seek": 144496, "start": 1460.48, "end": 1466.0, "text": " system of access permissions compared to Xfiki. You can do that custom, like if you do a script", "tokens": [1185, 295, 2105, 32723, 5347, 281, 1783, 69, 9850, 13, 509, 393, 360, 300, 2375, 11, 411, 498, 291, 360, 257, 5755], "temperature": 0.0, "avg_logprob": -0.10076124017888849, "compression_ratio": 1.848360655737705, "no_speech_prob": 3.11533258354757e-05}, {"id": 199, "seek": 144496, "start": 1466.0, "end": 1471.1200000000001, "text": " that maps the rights and tries to set up some rights, we can imagine that, but at the moment,", "tokens": [300, 11317, 264, 4601, 293, 9898, 281, 992, 493, 512, 4601, 11, 321, 393, 3811, 300, 11, 457, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.10076124017888849, "compression_ratio": 1.848360655737705, "no_speech_prob": 3.11533258354757e-05}, {"id": 200, "seek": 147112, "start": 1471.12, "end": 1476.6399999999999, "text": " it's not possible. It's very difficult to do it generically. The alternative or the best case", "tokens": [309, 311, 406, 1944, 13, 467, 311, 588, 2252, 281, 360, 309, 1337, 984, 13, 440, 8535, 420, 264, 1151, 1389], "temperature": 0.0, "avg_logprob": -0.07359250386555989, "compression_ratio": 1.6844106463878328, "no_speech_prob": 1.2773355592798907e-05}, {"id": 201, "seek": 147112, "start": 1476.6399999999999, "end": 1482.0, "text": " scenario is if you have something like an LDAP or even an SSO system that you have connected to", "tokens": [9005, 307, 498, 291, 362, 746, 411, 364, 33936, 4715, 420, 754, 364, 12238, 46, 1185, 300, 291, 362, 4582, 281], "temperature": 0.0, "avg_logprob": -0.07359250386555989, "compression_ratio": 1.6844106463878328, "no_speech_prob": 1.2773355592798907e-05}, {"id": 202, "seek": 147112, "start": 1482.0, "end": 1488.08, "text": " your current tool, and when you migrate, you connect that same user directory to the new tool,", "tokens": [428, 2190, 2290, 11, 293, 562, 291, 31821, 11, 291, 1745, 300, 912, 4195, 21120, 281, 264, 777, 2290, 11], "temperature": 0.0, "avg_logprob": -0.07359250386555989, "compression_ratio": 1.6844106463878328, "no_speech_prob": 1.2773355592798907e-05}, {"id": 203, "seek": 147112, "start": 1488.08, "end": 1492.4799999999998, "text": " such as Xfiki, and you just have the users created at the first login.", "tokens": [1270, 382, 1783, 69, 9850, 11, 293, 291, 445, 362, 264, 5022, 2942, 412, 264, 700, 24276, 13], "temperature": 0.0, "avg_logprob": -0.07359250386555989, "compression_ratio": 1.6844106463878328, "no_speech_prob": 1.2773355592798907e-05}, {"id": 204, "seek": 147112, "start": 1493.52, "end": 1498.56, "text": " That's, of course, the best case scenario. It's also actually possible to migrate users", "tokens": [663, 311, 11, 295, 1164, 11, 264, 1151, 1389, 9005, 13, 467, 311, 611, 767, 1944, 281, 31821, 5022], "temperature": 0.0, "avg_logprob": -0.07359250386555989, "compression_ratio": 1.6844106463878328, "no_speech_prob": 1.2773355592798907e-05}, {"id": 205, "seek": 149856, "start": 1498.56, "end": 1503.44, "text": " with the batch import, so you can do a bit of a hack there and import users as well,", "tokens": [365, 264, 15245, 974, 11, 370, 291, 393, 360, 257, 857, 295, 257, 10339, 456, 293, 974, 5022, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.11632979434469472, "compression_ratio": 1.6589861751152073, "no_speech_prob": 4.815601641894318e-05}, {"id": 206, "seek": 149856, "start": 1503.44, "end": 1508.48, "text": " but for permissions, it's generally very complicated, and it's a case-by-case situation.", "tokens": [457, 337, 32723, 11, 309, 311, 5101, 588, 6179, 11, 293, 309, 311, 257, 1389, 12, 2322, 12, 9765, 2590, 13], "temperature": 0.0, "avg_logprob": -0.11632979434469472, "compression_ratio": 1.6589861751152073, "no_speech_prob": 4.815601641894318e-05}, {"id": 207, "seek": 149856, "start": 1508.48, "end": 1514.3999999999999, "text": " You can import permissions, you can import groups from LDAP. We're also working on importing groups", "tokens": [509, 393, 974, 32723, 11, 291, 393, 974, 3935, 490, 33936, 4715, 13, 492, 434, 611, 1364, 322, 43866, 3935], "temperature": 0.0, "avg_logprob": -0.11632979434469472, "compression_ratio": 1.6589861751152073, "no_speech_prob": 4.815601641894318e-05}, {"id": 208, "seek": 149856, "start": 1514.3999999999999, "end": 1522.72, "text": " from as your SSO, but permissions, it's not yet generic enough done in our extensions.", "tokens": [490, 382, 428, 12238, 46, 11, 457, 32723, 11, 309, 311, 406, 1939, 19577, 1547, 1096, 294, 527, 25129, 13], "temperature": 0.0, "avg_logprob": -0.11632979434469472, "compression_ratio": 1.6589861751152073, "no_speech_prob": 4.815601641894318e-05}, {"id": 209, "seek": 152272, "start": 1522.72, "end": 1528.72, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.2536833821510782, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.0001549757580505684}, {"id": 210, "seek": 152272, "start": 1535.92, "end": 1542.16, "text": " So thank you. Also a great question. If the history of additions is kept for the", "tokens": [407, 1309, 291, 13, 2743, 257, 869, 1168, 13, 759, 264, 2503, 295, 35113, 307, 4305, 337, 264], "temperature": 0.0, "avg_logprob": -0.2536833821510782, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.0001549757580505684}, {"id": 211, "seek": 152272, "start": 1542.16, "end": 1548.56, "text": " conference migration, yes, or for the XML migrations in general, yes. We do have that,", "tokens": [7586, 17011, 11, 2086, 11, 420, 337, 264, 43484, 6186, 12154, 294, 2674, 11, 2086, 13, 492, 360, 362, 300, 11], "temperature": 0.0, "avg_logprob": -0.2536833821510782, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.0001549757580505684}, {"id": 212, "seek": 154856, "start": 1548.56, "end": 1556.24, "text": " and you can also see in our example here, I'm not sure if this one has enough history, but", "tokens": [293, 291, 393, 611, 536, 294, 527, 1365, 510, 11, 286, 478, 406, 988, 498, 341, 472, 575, 1547, 2503, 11, 457], "temperature": 0.0, "avg_logprob": -0.1330473254425357, "compression_ratio": 1.673913043478261, "no_speech_prob": 5.994279126753099e-05}, {"id": 213, "seek": 154856, "start": 1556.24, "end": 1562.8799999999999, "text": " yeah, okay, so just a quick example, the history is retained, again, if you configure the filter to", "tokens": [1338, 11, 1392, 11, 370, 445, 257, 1702, 1365, 11, 264, 2503, 307, 33438, 11, 797, 11, 498, 291, 22162, 264, 6608, 281], "temperature": 0.0, "avg_logprob": -0.1330473254425357, "compression_ratio": 1.673913043478261, "no_speech_prob": 5.994279126753099e-05}, {"id": 214, "seek": 154856, "start": 1562.8799999999999, "end": 1567.84, "text": " do so, and if you have this history retained, you can also see the changes between the versions,", "tokens": [360, 370, 11, 293, 498, 291, 362, 341, 2503, 33438, 11, 291, 393, 611, 536, 264, 2962, 1296, 264, 9606, 11], "temperature": 0.0, "avg_logprob": -0.1330473254425357, "compression_ratio": 1.673913043478261, "no_speech_prob": 5.994279126753099e-05}, {"id": 215, "seek": 154856, "start": 1568.56, "end": 1575.2, "text": " so that's something very nice. For SharePoint, we don't have that at the moment because we're not", "tokens": [370, 300, 311, 746, 588, 1481, 13, 1171, 14945, 18705, 11, 321, 500, 380, 362, 300, 412, 264, 1623, 570, 321, 434, 406], "temperature": 0.0, "avg_logprob": -0.1330473254425357, "compression_ratio": 1.673913043478261, "no_speech_prob": 5.994279126753099e-05}, {"id": 216, "seek": 157520, "start": 1575.2, "end": 1583.28, "text": " taking gold metadata from the documents, and also on other tools that support this type of", "tokens": [1940, 3821, 26603, 490, 264, 8512, 11, 293, 611, 322, 661, 3873, 300, 1406, 341, 2010, 295], "temperature": 0.0, "avg_logprob": -0.2110139528910319, "compression_ratio": 1.3609022556390977, "no_speech_prob": 0.0001801811158657074}, {"id": 217, "seek": 158328, "start": 1583.28, "end": 1605.92, "text": " filter streams migration, you may also get the history. Okay, thank you very much. Thanks.", "tokens": [50364, 6608, 15842, 17011, 11, 291, 815, 611, 483, 264, 2503, 13, 1033, 11, 1309, 291, 588, 709, 13, 2561, 13, 51496], "temperature": 0.0, "avg_logprob": -0.24021698080975076, "compression_ratio": 1.0843373493975903, "no_speech_prob": 0.0001331912208115682}], "language": "en"}