{"text": " Hello, everyone. Good afternoon. I am Thanos from the University of Manchester, and today I have the pleasure to present you Tornado VM, what is the state of Tornado VM at this moment. And in fact, I want to focus also on the slogan that's very known to everyone, the right ones run anywhere for Java. So, I will start with that. So, this is a known slogan derived since 90s from some micro systems in a way to advertise that Java language and the JVM in particular, it is a platform that can ensure portability across different CPU structures and architectures. So, the idea is that programmers can run their code, they can compile it once, and it can run transparently on different hardware architectures. However, hardware has changed in the last years. It is evolving, and perhaps this is not sufficient for the new types of hardware resources that are coming. So, lately we have GPUs and FPGAs which are coming to complement the power of the CPUs in a way to maximize performance and reduce the energy consumption. These are good, but there is some challenges that are deriving, and these are mainly posed in programmability. So, how programmers can harness this power from these resources. I don't know if you have experience with OpenCL and CUDA, but mainly these programming models that have been designed for these hardware types to get access to these hardware types, they are mainly focused on the C and C++ world. So, there are different programming models from different companies like SQL, one API, NVIDIA CUDA, OpenCL, which is a standard that can run on all the devices. And if you have FPGA expertise, then perhaps you can write RTL and Verilog, which is a hardware description language, but this is very low level. And here we are talking about Java, so we want to go high level. So, if you are a Java developer, then you use the JVM and you go to the CPU. If you want to have access to these devices, then you need to write your own native interfaces in the JNI and then tap into the C and C++ world. But still, you need to be aware of how these programming models are written. So, you need to be familiar with this. And this is exactly the problem that Tornado VM has been designed to solve. So, Tornado VM, it is a plug-in to existing OpenJDK distributions, like Amazon Goreto, Red Hat Mandrel, Azul Zulu, and others. And the way that it is built, it is to enable hardware acceleration in an easy manner. So, it offers a Java API and it has inside a JIT compiler for the hardware devices that are showing this figure. Our compiler inside, it can automatically translate the Java bytecodes to run on CPUs, multi-code CPUs, GPUs, integrated or discrete GPUs and FPGAs. And the compiler in the backend, it has three different backend types. It can emit OpenCLC, PDX, which is the assembly for the CUDA, for the NVIDIA GPUs. And it has recently also the SPIRV backend, which enables to utilize the level zero dispatcher from the one API. So, Tornado VM, it is a technology that can be used as a JVM plug-in to enable hardware acceleration for JVMs. And some of the key features is that it has a lightweight Java API. It is coded in a platform agnostic manner, so one command can be the same no matter which device it will be executed to program. And it can transparently, at the compile time, specialize the code. Because the code that is generated for the GPU, it is completely different from a code that is generated for an FPGA. So, regarding the compiler, we have different phases that will be enabled for GPUs and different phases that will be enabled to specialize the code for an FPGA. Our code is available in GitHub, so we encourage everyone who wants to have a look to fork it, download, play with examples, or even create their own examples. And also to come back to us. I mean, feel free to use the discussions to trigger the discussion if you have questions or to open issues if something is broken in order to fix it. And we have also available docker images for NVIDIA GPUs and Intel integrated GPUs. Now, the next part that I want to talk, it is regarding the API. Two weeks ago, we released a new version of Tornado VM, the version 0.15. And this comes with many new changes in the API level. So, our goal was to make the API easier for Java programmers in order to use it in a comprehensive manner. So, to know how to use and how to express parallelism from Java. But first, I have to make you familiar with the programming model of Tornado VM. And this programming model comes, it is inspired from the heterogeneous programming models like OpenCL and CUDA, the way that these programming models are operating. And in this sense, a Java program can be composed of two parts. The first part is the host code, where it is the actual core of the Java application. And the second part, it is the accelerated code, which actually it is the method or the set of methods that will be offloaded for execution on a GPU. Once we have made this clear, then we can move with the execution model, which it requires first because the processing will take place on a device. It will have first to move the data from the host code, from the CPU to the actual device. Then perform the processing. And once the processing is finished, then the data, the result, will have to be transferred back to the host code. Now, in Tornado VM, in the API of Tornado VM, we have exposed the set of objects and annotations for each of these two parts, the host code and accelerated code. In the host code, we have the task graph object and the Tornado execution plan. The task graph corresponds to what to run on the GPU. And the Tornado execution plan, it is how to run on the GPU. And for the accelerated code, we have a set of annotations and objects that I will show you later. So let's start with the task graph, what to run. Assuming that you are a Java programmer, then you want to offload the execution of a method. In this example, method A to the GPU. This method, it has some input and some output. Now, this method corresponds to what in the Tornado VM terminology call a task. So each method that will be offloaded for execution on hardware acceleration, it is a task. And it has the input data and the output data. And then we have a group of tasks, which is the task graph. Now, task graph can be a group of tasks that may have dependency or may not have dependency. And the programmers, they want to offload them all for hardware acceleration. In this particular example, I have put one task in this task graph. Once we have defined what to run, one question that comes, it is how often to transfer the data between the host, CPU and the device. And this can have a tremendous impact because it can affect the data transfer time. So it can have a long execution time. So it can affect performance, but can also affect energy, the power consumption. So how to transfer data? It matters. It depends on the pattern of the application. So one application may need to copy only the first execution if the data are read only, then always or only in the last execution, for example, for the output for the result. And here is a code snippet of how the task graph can be used to define this functionality in the Tornado API. So we create a new object, the task graph. We assign a name, which is a string. In this particular example, it is TG. And then we utilize the exposed methods of the API in order to fulfill the execution model. At first, we use the transfer to device, which has two inputs. The first argument that we put, it is the data transfer mode, which will be used to trigger how often the data will be moved. In this particular example, it is the first execution. So only in the first execution, the data will be moved. And then we have the parameter, which is the input array. The second method, it is the task, and it defines which method will be used for hardware acceleration. The first parameter, it is a name, a string, actually, of the method. It could be any name. And this is associated for the dynamic configuration, which I will show you later. The second parameter, it is the method reference. So the reference to the method that will be offloaded to the GPU for acceleration. And then it is the list of parameters of this method that corresponds to the method signature. And the last method, it is the transfer to host. And this, again, this method, it is configured the first argument to be the data transfer mode. And this example, we will copy the data, the output, in every execution. Okay. And once we have defined the task through the task graph, this task can be appended, can be updated. We can add a new task, a second task. We can change the way that the data transfers will be triggered in every execution only in the first execution. Then the next step, it is to define the immutable state of the task graph. So how to preserve the shape of a task graph. And this is done by taking a snapshot of the task graph, by using the snapshot method in the task graph object. Then we retrieve back an immutable task graph. And this means that this can be used for jit compilation and execution on the hardware. And this ensures that the Java programmers, they can create different versions of their task graph. They can update it. And then the code cache that we have in Tornado VM, it can store all these versions. It doesn't need to recompile and override the generated code. And this is the final step before we move to the actual execution plan. We have the immutable state of the task graph that can be modified and the immutable task graph that it cannot be modified anymore. So if the users they want to do a change, they can still change the task graph and get a new snapshot for a second version of their code. And now we move to how to run, how to execute the task graph. And this is done through the execution plan. Here is a snippet of Tornado execution plan. We create a new object that accepts as input the immutable task graph that doesn't change anymore. And then we can either directly execute it in the default execution mode by invoking the execution plan.execute method or we can configure it with some various optimizations. In this particular example, I have enabled the configuration to run with dynamic reconfiguration, which is a feature in Tornado VM that will launch a Java thread to GIT compile and execute the application per device that is available on the system. So we can have a CPU, a GPU and an FPGA. Java thread will run for all the devices and then it is triggered with a policy of performance, which means that the first device that will finish the execution, it will be the best and the rest Java threads will be killed. Now I have concluded the part of the host code. We can briefly go to the accelerated code, which is the way to express parallelism within the kernel, within the method or the Tornado VM task, as we call it. We have two ways, two APIs. The first one is called loop parallel API. And in a sense, we expose the parallel annotations that can be used by programmers as a hint to the Tornado VM GIT compiler that these loops can run in parallel. And the second one is the kernel API, which is an API exposed to the users through the kernel context object. And in a sense, the meaning of this API, it is meant for OpenCL and CUDA programmers, or Java programmers who know OpenCL, in a way to get more freedom on how to code things so they can get access to local memory, which is the equivalent to the cache memory of the CPU for GPUs. So they have more freedom on what to express. And in fact, I have used this API to port existing kernels written in OpenCL and CUDA to Java. For more information, you can use this link, which is the actual documentation of Tornado VM and describes some examples. I will briefly go to one example of a matrix multiplication, which I presented last year in FOSDEM. So in this example, we have the accelerated code and the host code. The matrix multiplication method, it implements matrix multiplication over a flattened arrays in two dimensions. And the way to annotate and express parallelism using the add parallel annotation, it would be to add the add parallel annotation inside the four loops. That means that we indicate that these loops could be executed in parallel. And now regarding the second API, the kernel API, we would use the kernel context object. And in particular, we would use the global ID X and Y, which correspond to the two dimensions that we have. So in a sense, it is like having the thread ID that will execute on the GPU. Here are some use cases that we used on Tornado VM. And concluding this talk, I would like to focus on a feature that we implemented in a research project that we are working. It is called elegant. And the idea is to create a software stack that unifies development for big data and IoT deployment. And there Tornado VM is used as a technology to enable acceleration as a service. So we have implemented the REST API. It is still a prototype. But the programmers, they can write a method. They can specify a method. They can specify the characteristics of the targeted device. And then the service will return back OpenCL code that it is meant to run parallel dysfunction. The interesting part is that this code, the OpenCL code, it is generated to be portable across different programming languages. So it doesn't only bind to Java. It can run also through C++, Python because it is OpenCL. And this means that in this particular example, we have Java. We use OpenZDK. We take the byte code and we pass the byte code to Tornado VM. And Tornado VM is running on an experimental feature which is called code interoperability mode. And in this mode, it converts this byte code to OpenCL that can run from any programming language and run time. Therefore, it is like prototyping in Java for parallel programming. Wrapping up, we would like to receive feedback. And we are looking also for collaborations if we can help to port use cases or for any other issues. And summarizing this talk, I briefly went through the right ones, run anywhere in the context of heterogeneous hardware acceleration. I have familiarized you with Tornado VM which is an open source project and the code base is available in GitHub. And I familiarize you with the programming model of Tornado VM and the new API, how to use it. And more are about to come in the FUJ blog with a new blog. So finally, just to acknowledge the projects that they have supported our research in the University of Manchester. And I'm ready for questions.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.96, "text": " Hello, everyone. Good afternoon. I am Thanos from the University of Manchester, and today", "tokens": [2425, 11, 1518, 13, 2205, 6499, 13, 286, 669, 35993, 490, 264, 3535, 295, 27180, 11, 293, 965], "temperature": 0.0, "avg_logprob": -0.2938666479928153, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.17879287898540497}, {"id": 1, "seek": 0, "start": 11.96, "end": 16.96, "text": " I have the pleasure to present you Tornado VM, what is the state of Tornado VM at this", "tokens": [286, 362, 264, 6834, 281, 1974, 291, 314, 1865, 1573, 18038, 11, 437, 307, 264, 1785, 295, 314, 1865, 1573, 18038, 412, 341], "temperature": 0.0, "avg_logprob": -0.2938666479928153, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.17879287898540497}, {"id": 2, "seek": 0, "start": 16.96, "end": 24.44, "text": " moment. And in fact, I want to focus also on the slogan that's very known to everyone,", "tokens": [1623, 13, 400, 294, 1186, 11, 286, 528, 281, 1879, 611, 322, 264, 33052, 300, 311, 588, 2570, 281, 1518, 11], "temperature": 0.0, "avg_logprob": -0.2938666479928153, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.17879287898540497}, {"id": 3, "seek": 2444, "start": 24.44, "end": 31.8, "text": " the right ones run anywhere for Java. So, I will start with that. So, this is a known", "tokens": [264, 558, 2306, 1190, 4992, 337, 10745, 13, 407, 11, 286, 486, 722, 365, 300, 13, 407, 11, 341, 307, 257, 2570], "temperature": 0.0, "avg_logprob": -0.2654414959807894, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0015790300676599145}, {"id": 4, "seek": 2444, "start": 31.8, "end": 40.72, "text": " slogan derived since 90s from some micro systems in a way to advertise that Java language and", "tokens": [33052, 18949, 1670, 4289, 82, 490, 512, 4532, 3652, 294, 257, 636, 281, 35379, 300, 10745, 2856, 293], "temperature": 0.0, "avg_logprob": -0.2654414959807894, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0015790300676599145}, {"id": 5, "seek": 2444, "start": 40.72, "end": 47.0, "text": " the JVM in particular, it is a platform that can ensure portability across different CPU", "tokens": [264, 508, 53, 44, 294, 1729, 11, 309, 307, 257, 3663, 300, 393, 5586, 2436, 2310, 2108, 819, 13199], "temperature": 0.0, "avg_logprob": -0.2654414959807894, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0015790300676599145}, {"id": 6, "seek": 4700, "start": 47.0, "end": 54.08, "text": " structures and architectures. So, the idea is that programmers can run their code, they", "tokens": [9227, 293, 6331, 1303, 13, 407, 11, 264, 1558, 307, 300, 41504, 393, 1190, 641, 3089, 11, 436], "temperature": 0.0, "avg_logprob": -0.16240584661090185, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.0004901241627521813}, {"id": 7, "seek": 4700, "start": 54.08, "end": 62.68, "text": " can compile it once, and it can run transparently on different hardware architectures. However,", "tokens": [393, 31413, 309, 1564, 11, 293, 309, 393, 1190, 7132, 6420, 322, 819, 8837, 6331, 1303, 13, 2908, 11], "temperature": 0.0, "avg_logprob": -0.16240584661090185, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.0004901241627521813}, {"id": 8, "seek": 4700, "start": 62.68, "end": 69.28, "text": " hardware has changed in the last years. It is evolving, and perhaps this is not sufficient", "tokens": [8837, 575, 3105, 294, 264, 1036, 924, 13, 467, 307, 21085, 11, 293, 4317, 341, 307, 406, 11563], "temperature": 0.0, "avg_logprob": -0.16240584661090185, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.0004901241627521813}, {"id": 9, "seek": 6928, "start": 69.28, "end": 81.48, "text": " for the new types of hardware resources that are coming. So, lately we have GPUs and FPGAs", "tokens": [337, 264, 777, 3467, 295, 8837, 3593, 300, 366, 1348, 13, 407, 11, 12881, 321, 362, 18407, 82, 293, 36655, 38, 10884], "temperature": 0.0, "avg_logprob": -0.16116714477539062, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008187792263925076}, {"id": 10, "seek": 6928, "start": 81.48, "end": 87.88, "text": " which are coming to complement the power of the CPUs in a way to maximize performance", "tokens": [597, 366, 1348, 281, 17103, 264, 1347, 295, 264, 13199, 82, 294, 257, 636, 281, 19874, 3389], "temperature": 0.0, "avg_logprob": -0.16116714477539062, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008187792263925076}, {"id": 11, "seek": 6928, "start": 87.88, "end": 96.08, "text": " and reduce the energy consumption. These are good, but there is some challenges that are", "tokens": [293, 5407, 264, 2281, 12126, 13, 1981, 366, 665, 11, 457, 456, 307, 512, 4759, 300, 366], "temperature": 0.0, "avg_logprob": -0.16116714477539062, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008187792263925076}, {"id": 12, "seek": 9608, "start": 96.08, "end": 103.6, "text": " deriving, and these are mainly posed in programmability. So, how programmers can harness this power", "tokens": [1163, 2123, 11, 293, 613, 366, 8704, 31399, 294, 37648, 2310, 13, 407, 11, 577, 41504, 393, 19700, 341, 1347], "temperature": 0.0, "avg_logprob": -0.1390566485268729, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001213913201354444}, {"id": 13, "seek": 9608, "start": 103.6, "end": 110.6, "text": " from these resources. I don't know if you have experience with OpenCL and CUDA, but", "tokens": [490, 613, 3593, 13, 286, 500, 380, 458, 498, 291, 362, 1752, 365, 7238, 22458, 293, 29777, 7509, 11, 457], "temperature": 0.0, "avg_logprob": -0.1390566485268729, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001213913201354444}, {"id": 14, "seek": 9608, "start": 110.6, "end": 115.12, "text": " mainly these programming models that have been designed for these hardware types to", "tokens": [8704, 613, 9410, 5245, 300, 362, 668, 4761, 337, 613, 8837, 3467, 281], "temperature": 0.0, "avg_logprob": -0.1390566485268729, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001213913201354444}, {"id": 15, "seek": 9608, "start": 115.12, "end": 121.52, "text": " get access to these hardware types, they are mainly focused on the C and C++ world. So,", "tokens": [483, 2105, 281, 613, 8837, 3467, 11, 436, 366, 8704, 5178, 322, 264, 383, 293, 383, 25472, 1002, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.1390566485268729, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001213913201354444}, {"id": 16, "seek": 12152, "start": 121.52, "end": 127.28, "text": " there are different programming models from different companies like SQL, one API, NVIDIA", "tokens": [456, 366, 819, 9410, 5245, 490, 819, 3431, 411, 19200, 11, 472, 9362, 11, 426, 3958, 6914], "temperature": 0.0, "avg_logprob": -0.22844607035319012, "compression_ratio": 1.4602510460251046, "no_speech_prob": 0.0011768806725740433}, {"id": 17, "seek": 12152, "start": 127.28, "end": 136.96, "text": " CUDA, OpenCL, which is a standard that can run on all the devices. And if you have FPGA", "tokens": [29777, 7509, 11, 7238, 22458, 11, 597, 307, 257, 3832, 300, 393, 1190, 322, 439, 264, 5759, 13, 400, 498, 291, 362, 36655, 12570], "temperature": 0.0, "avg_logprob": -0.22844607035319012, "compression_ratio": 1.4602510460251046, "no_speech_prob": 0.0011768806725740433}, {"id": 18, "seek": 12152, "start": 136.96, "end": 143.56, "text": " expertise, then perhaps you can write RTL and Verilog, which is a hardware description", "tokens": [11769, 11, 550, 4317, 291, 393, 2464, 21797, 43, 293, 4281, 388, 664, 11, 597, 307, 257, 8837, 3855], "temperature": 0.0, "avg_logprob": -0.22844607035319012, "compression_ratio": 1.4602510460251046, "no_speech_prob": 0.0011768806725740433}, {"id": 19, "seek": 12152, "start": 143.56, "end": 147.51999999999998, "text": " language, but this is very low level. And here we are talking about Java, so we want", "tokens": [2856, 11, 457, 341, 307, 588, 2295, 1496, 13, 400, 510, 321, 366, 1417, 466, 10745, 11, 370, 321, 528], "temperature": 0.0, "avg_logprob": -0.22844607035319012, "compression_ratio": 1.4602510460251046, "no_speech_prob": 0.0011768806725740433}, {"id": 20, "seek": 14752, "start": 147.52, "end": 157.60000000000002, "text": " to go high level. So, if you are a Java developer, then you use the JVM and you go to the CPU.", "tokens": [281, 352, 1090, 1496, 13, 407, 11, 498, 291, 366, 257, 10745, 10754, 11, 550, 291, 764, 264, 508, 53, 44, 293, 291, 352, 281, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.08308236206634136, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.00018000479030888528}, {"id": 21, "seek": 14752, "start": 157.60000000000002, "end": 164.92000000000002, "text": " If you want to have access to these devices, then you need to write your own native interfaces", "tokens": [759, 291, 528, 281, 362, 2105, 281, 613, 5759, 11, 550, 291, 643, 281, 2464, 428, 1065, 8470, 28416], "temperature": 0.0, "avg_logprob": -0.08308236206634136, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.00018000479030888528}, {"id": 22, "seek": 14752, "start": 164.92000000000002, "end": 171.72, "text": " in the JNI and then tap into the C and C++ world. But still, you need to be aware of", "tokens": [294, 264, 508, 42496, 293, 550, 5119, 666, 264, 383, 293, 383, 25472, 1002, 13, 583, 920, 11, 291, 643, 281, 312, 3650, 295], "temperature": 0.0, "avg_logprob": -0.08308236206634136, "compression_ratio": 1.5747126436781609, "no_speech_prob": 0.00018000479030888528}, {"id": 23, "seek": 17172, "start": 171.72, "end": 178.16, "text": " how these programming models are written. So, you need to be familiar with this. And", "tokens": [577, 613, 9410, 5245, 366, 3720, 13, 407, 11, 291, 643, 281, 312, 4963, 365, 341, 13, 400], "temperature": 0.0, "avg_logprob": -0.19442017873128256, "compression_ratio": 1.4824561403508771, "no_speech_prob": 0.0002998409327119589}, {"id": 24, "seek": 17172, "start": 178.16, "end": 183.96, "text": " this is exactly the problem that Tornado VM has been designed to solve. So, Tornado", "tokens": [341, 307, 2293, 264, 1154, 300, 314, 1865, 1573, 18038, 575, 668, 4761, 281, 5039, 13, 407, 11, 314, 1865, 1573], "temperature": 0.0, "avg_logprob": -0.19442017873128256, "compression_ratio": 1.4824561403508771, "no_speech_prob": 0.0002998409327119589}, {"id": 25, "seek": 17172, "start": 183.96, "end": 190.56, "text": " VM, it is a plug-in to existing OpenJDK distributions, like Amazon Goreto, Red Hat", "tokens": [18038, 11, 309, 307, 257, 5452, 12, 259, 281, 6741, 7238, 41, 35, 42, 37870, 11, 411, 6795, 45450, 1353, 11, 4477, 15867], "temperature": 0.0, "avg_logprob": -0.19442017873128256, "compression_ratio": 1.4824561403508771, "no_speech_prob": 0.0002998409327119589}, {"id": 26, "seek": 17172, "start": 190.56, "end": 197.24, "text": " Mandrel, Azul Zulu, and others. And the way that it is built, it is to enable hardware", "tokens": [15458, 4419, 11, 7607, 425, 1176, 12845, 11, 293, 2357, 13, 400, 264, 636, 300, 309, 307, 3094, 11, 309, 307, 281, 9528, 8837], "temperature": 0.0, "avg_logprob": -0.19442017873128256, "compression_ratio": 1.4824561403508771, "no_speech_prob": 0.0002998409327119589}, {"id": 27, "seek": 19724, "start": 197.24, "end": 205.96, "text": " acceleration in an easy manner. So, it offers a Java API and it has inside a JIT compiler", "tokens": [17162, 294, 364, 1858, 9060, 13, 407, 11, 309, 7736, 257, 10745, 9362, 293, 309, 575, 1854, 257, 508, 3927, 31958], "temperature": 0.0, "avg_logprob": -0.20195932388305665, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.000767448334954679}, {"id": 28, "seek": 19724, "start": 205.96, "end": 213.24, "text": " for the hardware devices that are showing this figure. Our compiler inside, it can automatically", "tokens": [337, 264, 8837, 5759, 300, 366, 4099, 341, 2573, 13, 2621, 31958, 1854, 11, 309, 393, 6772], "temperature": 0.0, "avg_logprob": -0.20195932388305665, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.000767448334954679}, {"id": 29, "seek": 19724, "start": 213.24, "end": 222.52, "text": " translate the Java bytecodes to run on CPUs, multi-code CPUs, GPUs, integrated or discrete", "tokens": [13799, 264, 10745, 40846, 66, 4789, 281, 1190, 322, 13199, 82, 11, 4825, 12, 22332, 13199, 82, 11, 18407, 82, 11, 10919, 420, 27706], "temperature": 0.0, "avg_logprob": -0.20195932388305665, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.000767448334954679}, {"id": 30, "seek": 22252, "start": 222.52, "end": 229.72, "text": " GPUs and FPGAs. And the compiler in the backend, it has three different backend types. It can", "tokens": [18407, 82, 293, 36655, 38, 10884, 13, 400, 264, 31958, 294, 264, 38087, 11, 309, 575, 1045, 819, 38087, 3467, 13, 467, 393], "temperature": 0.0, "avg_logprob": -0.20636712587796724, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0001395155122736469}, {"id": 31, "seek": 22252, "start": 229.72, "end": 237.4, "text": " emit OpenCLC, PDX, which is the assembly for the CUDA, for the NVIDIA GPUs. And it has", "tokens": [32084, 7238, 22458, 34, 11, 10464, 55, 11, 597, 307, 264, 12103, 337, 264, 29777, 7509, 11, 337, 264, 426, 3958, 6914, 18407, 82, 13, 400, 309, 575], "temperature": 0.0, "avg_logprob": -0.20636712587796724, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0001395155122736469}, {"id": 32, "seek": 22252, "start": 237.4, "end": 243.96, "text": " recently also the SPIRV backend, which enables to utilize the level zero dispatcher from the", "tokens": [3938, 611, 264, 8420, 7740, 53, 38087, 11, 597, 17077, 281, 16117, 264, 1496, 4018, 36729, 260, 490, 264], "temperature": 0.0, "avg_logprob": -0.20636712587796724, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0001395155122736469}, {"id": 33, "seek": 24396, "start": 243.96, "end": 253.68, "text": " one API. So, Tornado VM, it is a technology that can be used as a JVM plug-in to enable", "tokens": [472, 9362, 13, 407, 11, 314, 1865, 1573, 18038, 11, 309, 307, 257, 2899, 300, 393, 312, 1143, 382, 257, 508, 53, 44, 5452, 12, 259, 281, 9528], "temperature": 0.0, "avg_logprob": -0.08921423206081638, "compression_ratio": 1.4193548387096775, "no_speech_prob": 5.754186713602394e-05}, {"id": 34, "seek": 24396, "start": 253.68, "end": 260.08, "text": " hardware acceleration for JVMs. And some of the key features is that it has a lightweight", "tokens": [8837, 17162, 337, 508, 53, 26386, 13, 400, 512, 295, 264, 2141, 4122, 307, 300, 309, 575, 257, 22052], "temperature": 0.0, "avg_logprob": -0.08921423206081638, "compression_ratio": 1.4193548387096775, "no_speech_prob": 5.754186713602394e-05}, {"id": 35, "seek": 24396, "start": 260.08, "end": 267.36, "text": " Java API. It is coded in a platform agnostic manner, so one command can be the same no", "tokens": [10745, 9362, 13, 467, 307, 34874, 294, 257, 3663, 623, 77, 19634, 9060, 11, 370, 472, 5622, 393, 312, 264, 912, 572], "temperature": 0.0, "avg_logprob": -0.08921423206081638, "compression_ratio": 1.4193548387096775, "no_speech_prob": 5.754186713602394e-05}, {"id": 36, "seek": 26736, "start": 267.36, "end": 274.8, "text": " matter which device it will be executed to program. And it can transparently, at the", "tokens": [1871, 597, 4302, 309, 486, 312, 17577, 281, 1461, 13, 400, 309, 393, 7132, 6420, 11, 412, 264], "temperature": 0.0, "avg_logprob": -0.1733426473226892, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004377693112473935}, {"id": 37, "seek": 26736, "start": 274.8, "end": 280.44, "text": " compile time, specialize the code. Because the code that is generated for the GPU, it", "tokens": [31413, 565, 11, 37938, 264, 3089, 13, 1436, 264, 3089, 300, 307, 10833, 337, 264, 18407, 11, 309], "temperature": 0.0, "avg_logprob": -0.1733426473226892, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004377693112473935}, {"id": 38, "seek": 26736, "start": 280.44, "end": 288.44, "text": " is completely different from a code that is generated for an FPGA. So, regarding the compiler,", "tokens": [307, 2584, 819, 490, 257, 3089, 300, 307, 10833, 337, 364, 36655, 12570, 13, 407, 11, 8595, 264, 31958, 11], "temperature": 0.0, "avg_logprob": -0.1733426473226892, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004377693112473935}, {"id": 39, "seek": 26736, "start": 288.44, "end": 295.0, "text": " we have different phases that will be enabled for GPUs and different phases that will be", "tokens": [321, 362, 819, 18764, 300, 486, 312, 15172, 337, 18407, 82, 293, 819, 18764, 300, 486, 312], "temperature": 0.0, "avg_logprob": -0.1733426473226892, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0004377693112473935}, {"id": 40, "seek": 29500, "start": 295.0, "end": 306.52, "text": " enabled to specialize the code for an FPGA. Our code is available in GitHub, so we encourage", "tokens": [15172, 281, 37938, 264, 3089, 337, 364, 36655, 12570, 13, 2621, 3089, 307, 2435, 294, 23331, 11, 370, 321, 5373], "temperature": 0.0, "avg_logprob": -0.1255001123400702, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.00025658265803940594}, {"id": 41, "seek": 29500, "start": 306.52, "end": 312.08, "text": " everyone who wants to have a look to fork it, download, play with examples, or even create", "tokens": [1518, 567, 2738, 281, 362, 257, 574, 281, 17716, 309, 11, 5484, 11, 862, 365, 5110, 11, 420, 754, 1884], "temperature": 0.0, "avg_logprob": -0.1255001123400702, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.00025658265803940594}, {"id": 42, "seek": 29500, "start": 312.08, "end": 320.92, "text": " their own examples. And also to come back to us. I mean, feel free to use the discussions", "tokens": [641, 1065, 5110, 13, 400, 611, 281, 808, 646, 281, 505, 13, 286, 914, 11, 841, 1737, 281, 764, 264, 11088], "temperature": 0.0, "avg_logprob": -0.1255001123400702, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.00025658265803940594}, {"id": 43, "seek": 32092, "start": 320.92, "end": 326.36, "text": " to trigger the discussion if you have questions or to open issues if something is broken in", "tokens": [281, 7875, 264, 5017, 498, 291, 362, 1651, 420, 281, 1269, 2663, 498, 746, 307, 5463, 294], "temperature": 0.0, "avg_logprob": -0.1624917189280192, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.0004559812950901687}, {"id": 44, "seek": 32092, "start": 326.36, "end": 333.52000000000004, "text": " order to fix it. And we have also available docker images for NVIDIA GPUs and Intel integrated", "tokens": [1668, 281, 3191, 309, 13, 400, 321, 362, 611, 2435, 360, 9178, 5267, 337, 426, 3958, 6914, 18407, 82, 293, 19762, 10919], "temperature": 0.0, "avg_logprob": -0.1624917189280192, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.0004559812950901687}, {"id": 45, "seek": 32092, "start": 333.52000000000004, "end": 343.92, "text": " GPUs. Now, the next part that I want to talk, it is regarding the API. Two weeks ago, we", "tokens": [18407, 82, 13, 823, 11, 264, 958, 644, 300, 286, 528, 281, 751, 11, 309, 307, 8595, 264, 9362, 13, 4453, 3259, 2057, 11, 321], "temperature": 0.0, "avg_logprob": -0.1624917189280192, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.0004559812950901687}, {"id": 46, "seek": 34392, "start": 343.92, "end": 351.12, "text": " released a new version of Tornado VM, the version 0.15. And this comes with many new", "tokens": [4736, 257, 777, 3037, 295, 314, 1865, 1573, 18038, 11, 264, 3037, 1958, 13, 5211, 13, 400, 341, 1487, 365, 867, 777], "temperature": 0.0, "avg_logprob": -0.13054935667249892, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0003682300739455968}, {"id": 47, "seek": 34392, "start": 351.12, "end": 358.76, "text": " changes in the API level. So, our goal was to make the API easier for Java programmers", "tokens": [2962, 294, 264, 9362, 1496, 13, 407, 11, 527, 3387, 390, 281, 652, 264, 9362, 3571, 337, 10745, 41504], "temperature": 0.0, "avg_logprob": -0.13054935667249892, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0003682300739455968}, {"id": 48, "seek": 34392, "start": 358.76, "end": 364.0, "text": " in order to use it in a comprehensive manner. So, to know how to use and how to express", "tokens": [294, 1668, 281, 764, 309, 294, 257, 13914, 9060, 13, 407, 11, 281, 458, 577, 281, 764, 293, 577, 281, 5109], "temperature": 0.0, "avg_logprob": -0.13054935667249892, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0003682300739455968}, {"id": 49, "seek": 34392, "start": 364.0, "end": 371.76, "text": " parallelism from Java. But first, I have to make you familiar with the programming model", "tokens": [8952, 1434, 490, 10745, 13, 583, 700, 11, 286, 362, 281, 652, 291, 4963, 365, 264, 9410, 2316], "temperature": 0.0, "avg_logprob": -0.13054935667249892, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0003682300739455968}, {"id": 50, "seek": 37176, "start": 371.76, "end": 378.68, "text": " of Tornado VM. And this programming model comes, it is inspired from the heterogeneous", "tokens": [295, 314, 1865, 1573, 18038, 13, 400, 341, 9410, 2316, 1487, 11, 309, 307, 7547, 490, 264, 20789, 31112], "temperature": 0.0, "avg_logprob": -0.14260540008544922, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0005837807548232377}, {"id": 51, "seek": 37176, "start": 378.68, "end": 385.68, "text": " programming models like OpenCL and CUDA, the way that these programming models are operating.", "tokens": [9410, 5245, 411, 7238, 22458, 293, 29777, 7509, 11, 264, 636, 300, 613, 9410, 5245, 366, 7447, 13], "temperature": 0.0, "avg_logprob": -0.14260540008544922, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0005837807548232377}, {"id": 52, "seek": 37176, "start": 385.68, "end": 392.24, "text": " And in this sense, a Java program can be composed of two parts. The first part is the host code,", "tokens": [400, 294, 341, 2020, 11, 257, 10745, 1461, 393, 312, 18204, 295, 732, 3166, 13, 440, 700, 644, 307, 264, 3975, 3089, 11], "temperature": 0.0, "avg_logprob": -0.14260540008544922, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0005837807548232377}, {"id": 53, "seek": 37176, "start": 392.24, "end": 398.28, "text": " where it is the actual core of the Java application. And the second part, it is the accelerated", "tokens": [689, 309, 307, 264, 3539, 4965, 295, 264, 10745, 3861, 13, 400, 264, 1150, 644, 11, 309, 307, 264, 29763], "temperature": 0.0, "avg_logprob": -0.14260540008544922, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0005837807548232377}, {"id": 54, "seek": 39828, "start": 398.28, "end": 404.41999999999996, "text": " code, which actually it is the method or the set of methods that will be offloaded for", "tokens": [3089, 11, 597, 767, 309, 307, 264, 3170, 420, 264, 992, 295, 7150, 300, 486, 312, 766, 2907, 292, 337], "temperature": 0.0, "avg_logprob": -0.1341275170791981, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.00022716181410942227}, {"id": 55, "seek": 39828, "start": 404.41999999999996, "end": 411.28, "text": " execution on a GPU. Once we have made this clear, then we can move with the execution", "tokens": [15058, 322, 257, 18407, 13, 3443, 321, 362, 1027, 341, 1850, 11, 550, 321, 393, 1286, 365, 264, 15058], "temperature": 0.0, "avg_logprob": -0.1341275170791981, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.00022716181410942227}, {"id": 56, "seek": 39828, "start": 411.28, "end": 418.76, "text": " model, which it requires first because the processing will take place on a device. It", "tokens": [2316, 11, 597, 309, 7029, 700, 570, 264, 9007, 486, 747, 1081, 322, 257, 4302, 13, 467], "temperature": 0.0, "avg_logprob": -0.1341275170791981, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.00022716181410942227}, {"id": 57, "seek": 39828, "start": 418.76, "end": 426.35999999999996, "text": " will have first to move the data from the host code, from the CPU to the actual device.", "tokens": [486, 362, 700, 281, 1286, 264, 1412, 490, 264, 3975, 3089, 11, 490, 264, 13199, 281, 264, 3539, 4302, 13], "temperature": 0.0, "avg_logprob": -0.1341275170791981, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.00022716181410942227}, {"id": 58, "seek": 42636, "start": 426.36, "end": 431.40000000000003, "text": " Then perform the processing. And once the processing is finished, then the data, the", "tokens": [1396, 2042, 264, 9007, 13, 400, 1564, 264, 9007, 307, 4335, 11, 550, 264, 1412, 11, 264], "temperature": 0.0, "avg_logprob": -0.14084258286849313, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.00046307299635373056}, {"id": 59, "seek": 42636, "start": 431.40000000000003, "end": 440.32, "text": " result, will have to be transferred back to the host code. Now, in Tornado VM, in the", "tokens": [1874, 11, 486, 362, 281, 312, 15809, 646, 281, 264, 3975, 3089, 13, 823, 11, 294, 314, 1865, 1573, 18038, 11, 294, 264], "temperature": 0.0, "avg_logprob": -0.14084258286849313, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.00046307299635373056}, {"id": 60, "seek": 42636, "start": 440.32, "end": 446.48, "text": " API of Tornado VM, we have exposed the set of objects and annotations for each of these", "tokens": [9362, 295, 314, 1865, 1573, 18038, 11, 321, 362, 9495, 264, 992, 295, 6565, 293, 25339, 763, 337, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.14084258286849313, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.00046307299635373056}, {"id": 61, "seek": 42636, "start": 446.48, "end": 452.88, "text": " two parts, the host code and accelerated code. In the host code, we have the task graph object", "tokens": [732, 3166, 11, 264, 3975, 3089, 293, 29763, 3089, 13, 682, 264, 3975, 3089, 11, 321, 362, 264, 5633, 4295, 2657], "temperature": 0.0, "avg_logprob": -0.14084258286849313, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.00046307299635373056}, {"id": 62, "seek": 45288, "start": 452.88, "end": 459.84, "text": " and the Tornado execution plan. The task graph corresponds to what to run on the GPU. And", "tokens": [293, 264, 314, 1865, 1573, 15058, 1393, 13, 440, 5633, 4295, 23249, 281, 437, 281, 1190, 322, 264, 18407, 13, 400], "temperature": 0.0, "avg_logprob": -0.11913128903037623, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.00026987845194526017}, {"id": 63, "seek": 45288, "start": 459.84, "end": 466.04, "text": " the Tornado execution plan, it is how to run on the GPU. And for the accelerated code,", "tokens": [264, 314, 1865, 1573, 15058, 1393, 11, 309, 307, 577, 281, 1190, 322, 264, 18407, 13, 400, 337, 264, 29763, 3089, 11], "temperature": 0.0, "avg_logprob": -0.11913128903037623, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.00026987845194526017}, {"id": 64, "seek": 45288, "start": 466.04, "end": 475.2, "text": " we have a set of annotations and objects that I will show you later. So let's start with", "tokens": [321, 362, 257, 992, 295, 25339, 763, 293, 6565, 300, 286, 486, 855, 291, 1780, 13, 407, 718, 311, 722, 365], "temperature": 0.0, "avg_logprob": -0.11913128903037623, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.00026987845194526017}, {"id": 65, "seek": 45288, "start": 475.2, "end": 481.2, "text": " the task graph, what to run. Assuming that you are a Java programmer, then you want to", "tokens": [264, 5633, 4295, 11, 437, 281, 1190, 13, 6281, 24919, 300, 291, 366, 257, 10745, 32116, 11, 550, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.11913128903037623, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.00026987845194526017}, {"id": 66, "seek": 48120, "start": 481.2, "end": 488.03999999999996, "text": " offload the execution of a method. In this example, method A to the GPU. This method,", "tokens": [766, 2907, 264, 15058, 295, 257, 3170, 13, 682, 341, 1365, 11, 3170, 316, 281, 264, 18407, 13, 639, 3170, 11], "temperature": 0.0, "avg_logprob": -0.16754195454356435, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.000876028323546052}, {"id": 67, "seek": 48120, "start": 488.03999999999996, "end": 494.96, "text": " it has some input and some output. Now, this method corresponds to what in the Tornado", "tokens": [309, 575, 512, 4846, 293, 512, 5598, 13, 823, 11, 341, 3170, 23249, 281, 437, 294, 264, 314, 1865, 1573], "temperature": 0.0, "avg_logprob": -0.16754195454356435, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.000876028323546052}, {"id": 68, "seek": 48120, "start": 494.96, "end": 502.08, "text": " VM terminology call a task. So each method that will be offloaded for execution on hardware", "tokens": [18038, 27575, 818, 257, 5633, 13, 407, 1184, 3170, 300, 486, 312, 766, 2907, 292, 337, 15058, 322, 8837], "temperature": 0.0, "avg_logprob": -0.16754195454356435, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.000876028323546052}, {"id": 69, "seek": 48120, "start": 502.08, "end": 510.12, "text": " acceleration, it is a task. And it has the input data and the output data. And then we", "tokens": [17162, 11, 309, 307, 257, 5633, 13, 400, 309, 575, 264, 4846, 1412, 293, 264, 5598, 1412, 13, 400, 550, 321], "temperature": 0.0, "avg_logprob": -0.16754195454356435, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.000876028323546052}, {"id": 70, "seek": 51012, "start": 510.12, "end": 516.52, "text": " have a group of tasks, which is the task graph. Now, task graph can be a group of tasks that", "tokens": [362, 257, 1594, 295, 9608, 11, 597, 307, 264, 5633, 4295, 13, 823, 11, 5633, 4295, 393, 312, 257, 1594, 295, 9608, 300], "temperature": 0.0, "avg_logprob": -0.12750094587152655, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0005429491866379976}, {"id": 71, "seek": 51012, "start": 516.52, "end": 521.92, "text": " may have dependency or may not have dependency. And the programmers, they want to offload", "tokens": [815, 362, 33621, 420, 815, 406, 362, 33621, 13, 400, 264, 41504, 11, 436, 528, 281, 766, 2907], "temperature": 0.0, "avg_logprob": -0.12750094587152655, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0005429491866379976}, {"id": 72, "seek": 51012, "start": 521.92, "end": 527.96, "text": " them all for hardware acceleration. In this particular example, I have put one task in", "tokens": [552, 439, 337, 8837, 17162, 13, 682, 341, 1729, 1365, 11, 286, 362, 829, 472, 5633, 294], "temperature": 0.0, "avg_logprob": -0.12750094587152655, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0005429491866379976}, {"id": 73, "seek": 51012, "start": 527.96, "end": 535.6, "text": " this task graph. Once we have defined what to run, one question that comes, it is how", "tokens": [341, 5633, 4295, 13, 3443, 321, 362, 7642, 437, 281, 1190, 11, 472, 1168, 300, 1487, 11, 309, 307, 577], "temperature": 0.0, "avg_logprob": -0.12750094587152655, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0005429491866379976}, {"id": 74, "seek": 53560, "start": 535.6, "end": 543.4, "text": " often to transfer the data between the host, CPU and the device. And this can have a tremendous", "tokens": [2049, 281, 5003, 264, 1412, 1296, 264, 3975, 11, 13199, 293, 264, 4302, 13, 400, 341, 393, 362, 257, 10048], "temperature": 0.0, "avg_logprob": -0.15199421153349035, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0003898466529790312}, {"id": 75, "seek": 53560, "start": 543.4, "end": 551.32, "text": " impact because it can affect the data transfer time. So it can have a long execution time.", "tokens": [2712, 570, 309, 393, 3345, 264, 1412, 5003, 565, 13, 407, 309, 393, 362, 257, 938, 15058, 565, 13], "temperature": 0.0, "avg_logprob": -0.15199421153349035, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0003898466529790312}, {"id": 76, "seek": 53560, "start": 551.32, "end": 557.48, "text": " So it can affect performance, but can also affect energy, the power consumption. So", "tokens": [407, 309, 393, 3345, 3389, 11, 457, 393, 611, 3345, 2281, 11, 264, 1347, 12126, 13, 407], "temperature": 0.0, "avg_logprob": -0.15199421153349035, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0003898466529790312}, {"id": 77, "seek": 53560, "start": 557.48, "end": 564.48, "text": " how to transfer data? It matters. It depends on the pattern of the application. So one", "tokens": [577, 281, 5003, 1412, 30, 467, 7001, 13, 467, 5946, 322, 264, 5102, 295, 264, 3861, 13, 407, 472], "temperature": 0.0, "avg_logprob": -0.15199421153349035, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0003898466529790312}, {"id": 78, "seek": 56448, "start": 564.48, "end": 570.28, "text": " application may need to copy only the first execution if the data are read only, then", "tokens": [3861, 815, 643, 281, 5055, 787, 264, 700, 15058, 498, 264, 1412, 366, 1401, 787, 11, 550], "temperature": 0.0, "avg_logprob": -0.1503133238031623, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.0003081326431129128}, {"id": 79, "seek": 56448, "start": 570.28, "end": 576.96, "text": " always or only in the last execution, for example, for the output for the result. And", "tokens": [1009, 420, 787, 294, 264, 1036, 15058, 11, 337, 1365, 11, 337, 264, 5598, 337, 264, 1874, 13, 400], "temperature": 0.0, "avg_logprob": -0.1503133238031623, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.0003081326431129128}, {"id": 80, "seek": 56448, "start": 576.96, "end": 583.28, "text": " here is a code snippet of how the task graph can be used to define this functionality in", "tokens": [510, 307, 257, 3089, 35623, 302, 295, 577, 264, 5633, 4295, 393, 312, 1143, 281, 6964, 341, 14980, 294], "temperature": 0.0, "avg_logprob": -0.1503133238031623, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.0003081326431129128}, {"id": 81, "seek": 56448, "start": 583.28, "end": 589.72, "text": " the Tornado API. So we create a new object, the task graph. We assign a name, which is", "tokens": [264, 314, 1865, 1573, 9362, 13, 407, 321, 1884, 257, 777, 2657, 11, 264, 5633, 4295, 13, 492, 6269, 257, 1315, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1503133238031623, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.0003081326431129128}, {"id": 82, "seek": 58972, "start": 589.72, "end": 597.0400000000001, "text": " a string. In this particular example, it is TG. And then we utilize the exposed methods", "tokens": [257, 6798, 13, 682, 341, 1729, 1365, 11, 309, 307, 314, 38, 13, 400, 550, 321, 16117, 264, 9495, 7150], "temperature": 0.0, "avg_logprob": -0.1396768655669823, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.001004014047794044}, {"id": 83, "seek": 58972, "start": 597.0400000000001, "end": 604.76, "text": " of the API in order to fulfill the execution model. At first, we use the transfer to device,", "tokens": [295, 264, 9362, 294, 1668, 281, 13875, 264, 15058, 2316, 13, 1711, 700, 11, 321, 764, 264, 5003, 281, 4302, 11], "temperature": 0.0, "avg_logprob": -0.1396768655669823, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.001004014047794044}, {"id": 84, "seek": 58972, "start": 604.76, "end": 610.96, "text": " which has two inputs. The first argument that we put, it is the data transfer mode, which", "tokens": [597, 575, 732, 15743, 13, 440, 700, 6770, 300, 321, 829, 11, 309, 307, 264, 1412, 5003, 4391, 11, 597], "temperature": 0.0, "avg_logprob": -0.1396768655669823, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.001004014047794044}, {"id": 85, "seek": 58972, "start": 610.96, "end": 618.4, "text": " will be used to trigger how often the data will be moved. In this particular example,", "tokens": [486, 312, 1143, 281, 7875, 577, 2049, 264, 1412, 486, 312, 4259, 13, 682, 341, 1729, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1396768655669823, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.001004014047794044}, {"id": 86, "seek": 61840, "start": 618.4, "end": 623.56, "text": " it is the first execution. So only in the first execution, the data will be moved. And", "tokens": [309, 307, 264, 700, 15058, 13, 407, 787, 294, 264, 700, 15058, 11, 264, 1412, 486, 312, 4259, 13, 400], "temperature": 0.0, "avg_logprob": -0.14611329089154254, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.0003173108270857483}, {"id": 87, "seek": 61840, "start": 623.56, "end": 631.3199999999999, "text": " then we have the parameter, which is the input array. The second method, it is the task,", "tokens": [550, 321, 362, 264, 13075, 11, 597, 307, 264, 4846, 10225, 13, 440, 1150, 3170, 11, 309, 307, 264, 5633, 11], "temperature": 0.0, "avg_logprob": -0.14611329089154254, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.0003173108270857483}, {"id": 88, "seek": 61840, "start": 631.3199999999999, "end": 637.56, "text": " and it defines which method will be used for hardware acceleration. The first parameter,", "tokens": [293, 309, 23122, 597, 3170, 486, 312, 1143, 337, 8837, 17162, 13, 440, 700, 13075, 11], "temperature": 0.0, "avg_logprob": -0.14611329089154254, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.0003173108270857483}, {"id": 89, "seek": 61840, "start": 637.56, "end": 644.04, "text": " it is a name, a string, actually, of the method. It could be any name. And this is associated", "tokens": [309, 307, 257, 1315, 11, 257, 6798, 11, 767, 11, 295, 264, 3170, 13, 467, 727, 312, 604, 1315, 13, 400, 341, 307, 6615], "temperature": 0.0, "avg_logprob": -0.14611329089154254, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.0003173108270857483}, {"id": 90, "seek": 64404, "start": 644.04, "end": 649.9599999999999, "text": " for the dynamic configuration, which I will show you later. The second parameter, it is", "tokens": [337, 264, 8546, 11694, 11, 597, 286, 486, 855, 291, 1780, 13, 440, 1150, 13075, 11, 309, 307], "temperature": 0.0, "avg_logprob": -0.1231353198780733, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.00035152724012732506}, {"id": 91, "seek": 64404, "start": 649.9599999999999, "end": 655.8399999999999, "text": " the method reference. So the reference to the method that will be offloaded to the GPU", "tokens": [264, 3170, 6408, 13, 407, 264, 6408, 281, 264, 3170, 300, 486, 312, 766, 2907, 292, 281, 264, 18407], "temperature": 0.0, "avg_logprob": -0.1231353198780733, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.00035152724012732506}, {"id": 92, "seek": 64404, "start": 655.8399999999999, "end": 661.0799999999999, "text": " for acceleration. And then it is the list of parameters of this method that corresponds", "tokens": [337, 17162, 13, 400, 550, 309, 307, 264, 1329, 295, 9834, 295, 341, 3170, 300, 23249], "temperature": 0.0, "avg_logprob": -0.1231353198780733, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.00035152724012732506}, {"id": 93, "seek": 64404, "start": 661.0799999999999, "end": 669.8, "text": " to the method signature. And the last method, it is the transfer to host. And this, again,", "tokens": [281, 264, 3170, 13397, 13, 400, 264, 1036, 3170, 11, 309, 307, 264, 5003, 281, 3975, 13, 400, 341, 11, 797, 11], "temperature": 0.0, "avg_logprob": -0.1231353198780733, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.00035152724012732506}, {"id": 94, "seek": 66980, "start": 669.8, "end": 676.16, "text": " this method, it is configured the first argument to be the data transfer mode. And this example,", "tokens": [341, 3170, 11, 309, 307, 30538, 264, 700, 6770, 281, 312, 264, 1412, 5003, 4391, 13, 400, 341, 1365, 11], "temperature": 0.0, "avg_logprob": -0.15549966563349185, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.00102213304489851}, {"id": 95, "seek": 66980, "start": 676.16, "end": 685.8, "text": " we will copy the data, the output, in every execution. Okay. And once we have defined", "tokens": [321, 486, 5055, 264, 1412, 11, 264, 5598, 11, 294, 633, 15058, 13, 1033, 13, 400, 1564, 321, 362, 7642], "temperature": 0.0, "avg_logprob": -0.15549966563349185, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.00102213304489851}, {"id": 96, "seek": 66980, "start": 685.8, "end": 692.88, "text": " the task through the task graph, this task can be appended, can be updated. We can add", "tokens": [264, 5633, 807, 264, 5633, 4295, 11, 341, 5633, 393, 312, 724, 3502, 11, 393, 312, 10588, 13, 492, 393, 909], "temperature": 0.0, "avg_logprob": -0.15549966563349185, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.00102213304489851}, {"id": 97, "seek": 69288, "start": 692.88, "end": 700.72, "text": " a new task, a second task. We can change the way that the data transfers will be triggered", "tokens": [257, 777, 5633, 11, 257, 1150, 5633, 13, 492, 393, 1319, 264, 636, 300, 264, 1412, 29137, 486, 312, 21710], "temperature": 0.0, "avg_logprob": -0.08044681549072266, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.00026507567963562906}, {"id": 98, "seek": 69288, "start": 700.72, "end": 707.84, "text": " in every execution only in the first execution. Then the next step, it is to define the immutable", "tokens": [294, 633, 15058, 787, 294, 264, 700, 15058, 13, 1396, 264, 958, 1823, 11, 309, 307, 281, 6964, 264, 3397, 32148], "temperature": 0.0, "avg_logprob": -0.08044681549072266, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.00026507567963562906}, {"id": 99, "seek": 69288, "start": 707.84, "end": 716.36, "text": " state of the task graph. So how to preserve the shape of a task graph. And this is done", "tokens": [1785, 295, 264, 5633, 4295, 13, 407, 577, 281, 15665, 264, 3909, 295, 257, 5633, 4295, 13, 400, 341, 307, 1096], "temperature": 0.0, "avg_logprob": -0.08044681549072266, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.00026507567963562906}, {"id": 100, "seek": 71636, "start": 716.36, "end": 723.32, "text": " by taking a snapshot of the task graph, by using the snapshot method in the task graph", "tokens": [538, 1940, 257, 30163, 295, 264, 5633, 4295, 11, 538, 1228, 264, 30163, 3170, 294, 264, 5633, 4295], "temperature": 0.0, "avg_logprob": -0.15864981674566503, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.0005023858393542469}, {"id": 101, "seek": 71636, "start": 723.32, "end": 730.0, "text": " object. Then we retrieve back an immutable task graph. And this means that this can be", "tokens": [2657, 13, 1396, 321, 30254, 646, 364, 3397, 32148, 5633, 4295, 13, 400, 341, 1355, 300, 341, 393, 312], "temperature": 0.0, "avg_logprob": -0.15864981674566503, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.0005023858393542469}, {"id": 102, "seek": 71636, "start": 730.0, "end": 736.76, "text": " used for jit compilation and execution on the hardware. And this ensures that the Java", "tokens": [1143, 337, 361, 270, 40261, 293, 15058, 322, 264, 8837, 13, 400, 341, 28111, 300, 264, 10745], "temperature": 0.0, "avg_logprob": -0.15864981674566503, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.0005023858393542469}, {"id": 103, "seek": 71636, "start": 736.76, "end": 742.8000000000001, "text": " programmers, they can create different versions of their task graph. They can update it. And", "tokens": [41504, 11, 436, 393, 1884, 819, 9606, 295, 641, 5633, 4295, 13, 814, 393, 5623, 309, 13, 400], "temperature": 0.0, "avg_logprob": -0.15864981674566503, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.0005023858393542469}, {"id": 104, "seek": 74280, "start": 742.8, "end": 748.5999999999999, "text": " then the code cache that we have in Tornado VM, it can store all these versions. It doesn't", "tokens": [550, 264, 3089, 19459, 300, 321, 362, 294, 314, 1865, 1573, 18038, 11, 309, 393, 3531, 439, 613, 9606, 13, 467, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15921503922034955, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0006150513654574752}, {"id": 105, "seek": 74280, "start": 748.5999999999999, "end": 757.5799999999999, "text": " need to recompile and override the generated code. And this is the final step before we", "tokens": [643, 281, 48000, 794, 293, 42321, 264, 10833, 3089, 13, 400, 341, 307, 264, 2572, 1823, 949, 321], "temperature": 0.0, "avg_logprob": -0.15921503922034955, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0006150513654574752}, {"id": 106, "seek": 74280, "start": 757.5799999999999, "end": 764.1999999999999, "text": " move to the actual execution plan. We have the immutable state of the task graph that", "tokens": [1286, 281, 264, 3539, 15058, 1393, 13, 492, 362, 264, 3397, 32148, 1785, 295, 264, 5633, 4295, 300], "temperature": 0.0, "avg_logprob": -0.15921503922034955, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0006150513654574752}, {"id": 107, "seek": 74280, "start": 764.1999999999999, "end": 769.68, "text": " can be modified and the immutable task graph that it cannot be modified anymore. So if", "tokens": [393, 312, 15873, 293, 264, 3397, 32148, 5633, 4295, 300, 309, 2644, 312, 15873, 3602, 13, 407, 498], "temperature": 0.0, "avg_logprob": -0.15921503922034955, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0006150513654574752}, {"id": 108, "seek": 76968, "start": 769.68, "end": 775.92, "text": " the users they want to do a change, they can still change the task graph and get a new", "tokens": [264, 5022, 436, 528, 281, 360, 257, 1319, 11, 436, 393, 920, 1319, 264, 5633, 4295, 293, 483, 257, 777], "temperature": 0.0, "avg_logprob": -0.11920350034471969, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0007100784569047391}, {"id": 109, "seek": 76968, "start": 775.92, "end": 786.0, "text": " snapshot for a second version of their code. And now we move to how to run, how to execute", "tokens": [30163, 337, 257, 1150, 3037, 295, 641, 3089, 13, 400, 586, 321, 1286, 281, 577, 281, 1190, 11, 577, 281, 14483], "temperature": 0.0, "avg_logprob": -0.11920350034471969, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0007100784569047391}, {"id": 110, "seek": 76968, "start": 786.0, "end": 794.4, "text": " the task graph. And this is done through the execution plan. Here is a snippet of Tornado", "tokens": [264, 5633, 4295, 13, 400, 341, 307, 1096, 807, 264, 15058, 1393, 13, 1692, 307, 257, 35623, 302, 295, 314, 1865, 1573], "temperature": 0.0, "avg_logprob": -0.11920350034471969, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0007100784569047391}, {"id": 111, "seek": 79440, "start": 794.4, "end": 800.72, "text": " execution plan. We create a new object that accepts as input the immutable task graph", "tokens": [15058, 1393, 13, 492, 1884, 257, 777, 2657, 300, 33538, 382, 4846, 264, 3397, 32148, 5633, 4295], "temperature": 0.0, "avg_logprob": -0.13167312817695814, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.00034394796239212155}, {"id": 112, "seek": 79440, "start": 800.72, "end": 805.88, "text": " that doesn't change anymore. And then we can either directly execute it in the default", "tokens": [300, 1177, 380, 1319, 3602, 13, 400, 550, 321, 393, 2139, 3838, 14483, 309, 294, 264, 7576], "temperature": 0.0, "avg_logprob": -0.13167312817695814, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.00034394796239212155}, {"id": 113, "seek": 79440, "start": 805.88, "end": 813.16, "text": " execution mode by invoking the execution plan.execute method or we can configure it with some", "tokens": [15058, 4391, 538, 1048, 5953, 264, 15058, 1393, 13, 3121, 3045, 1169, 3170, 420, 321, 393, 22162, 309, 365, 512], "temperature": 0.0, "avg_logprob": -0.13167312817695814, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.00034394796239212155}, {"id": 114, "seek": 79440, "start": 813.16, "end": 820.12, "text": " various optimizations. In this particular example, I have enabled the configuration", "tokens": [3683, 5028, 14455, 13, 682, 341, 1729, 1365, 11, 286, 362, 15172, 264, 11694], "temperature": 0.0, "avg_logprob": -0.13167312817695814, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.00034394796239212155}, {"id": 115, "seek": 82012, "start": 820.12, "end": 826.0, "text": " to run with dynamic reconfiguration, which is a feature in Tornado VM that will launch", "tokens": [281, 1190, 365, 8546, 9993, 20646, 8167, 11, 597, 307, 257, 4111, 294, 314, 1865, 1573, 18038, 300, 486, 4025], "temperature": 0.0, "avg_logprob": -0.18082900359251788, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.00043724747956730425}, {"id": 116, "seek": 82012, "start": 826.0, "end": 831.5600000000001, "text": " a Java thread to GIT compile and execute the application per device that is available", "tokens": [257, 10745, 7207, 281, 460, 3927, 31413, 293, 14483, 264, 3861, 680, 4302, 300, 307, 2435], "temperature": 0.0, "avg_logprob": -0.18082900359251788, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.00043724747956730425}, {"id": 117, "seek": 82012, "start": 831.5600000000001, "end": 837.72, "text": " on the system. So we can have a CPU, a GPU and an FPGA. Java thread will run for all", "tokens": [322, 264, 1185, 13, 407, 321, 393, 362, 257, 13199, 11, 257, 18407, 293, 364, 36655, 12570, 13, 10745, 7207, 486, 1190, 337, 439], "temperature": 0.0, "avg_logprob": -0.18082900359251788, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.00043724747956730425}, {"id": 118, "seek": 82012, "start": 837.72, "end": 842.88, "text": " the devices and then it is triggered with a policy of performance, which means that the", "tokens": [264, 5759, 293, 550, 309, 307, 21710, 365, 257, 3897, 295, 3389, 11, 597, 1355, 300, 264], "temperature": 0.0, "avg_logprob": -0.18082900359251788, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.00043724747956730425}, {"id": 119, "seek": 82012, "start": 842.88, "end": 848.6800000000001, "text": " first device that will finish the execution, it will be the best and the rest Java threads", "tokens": [700, 4302, 300, 486, 2413, 264, 15058, 11, 309, 486, 312, 264, 1151, 293, 264, 1472, 10745, 19314], "temperature": 0.0, "avg_logprob": -0.18082900359251788, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.00043724747956730425}, {"id": 120, "seek": 84868, "start": 848.68, "end": 857.56, "text": " will be killed. Now I have concluded the part of the host", "tokens": [486, 312, 4652, 13, 823, 286, 362, 22960, 264, 644, 295, 264, 3975], "temperature": 0.0, "avg_logprob": -0.14728446233840214, "compression_ratio": 1.4625, "no_speech_prob": 0.0007551173912361264}, {"id": 121, "seek": 84868, "start": 857.56, "end": 864.0, "text": " code. We can briefly go to the accelerated code, which is the way to express parallelism", "tokens": [3089, 13, 492, 393, 10515, 352, 281, 264, 29763, 3089, 11, 597, 307, 264, 636, 281, 5109, 8952, 1434], "temperature": 0.0, "avg_logprob": -0.14728446233840214, "compression_ratio": 1.4625, "no_speech_prob": 0.0007551173912361264}, {"id": 122, "seek": 84868, "start": 864.0, "end": 870.4, "text": " within the kernel, within the method or the Tornado VM task, as we call it. We have two", "tokens": [1951, 264, 28256, 11, 1951, 264, 3170, 420, 264, 314, 1865, 1573, 18038, 5633, 11, 382, 321, 818, 309, 13, 492, 362, 732], "temperature": 0.0, "avg_logprob": -0.14728446233840214, "compression_ratio": 1.4625, "no_speech_prob": 0.0007551173912361264}, {"id": 123, "seek": 87040, "start": 870.4, "end": 878.68, "text": " ways, two APIs. The first one is called loop parallel API. And in a sense, we expose the", "tokens": [2098, 11, 732, 21445, 13, 440, 700, 472, 307, 1219, 6367, 8952, 9362, 13, 400, 294, 257, 2020, 11, 321, 19219, 264], "temperature": 0.0, "avg_logprob": -0.22898825553998556, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0005876513896510005}, {"id": 124, "seek": 87040, "start": 878.68, "end": 885.56, "text": " parallel annotations that can be used by programmers as a hint to the Tornado VM GIT compiler that", "tokens": [8952, 25339, 763, 300, 393, 312, 1143, 538, 41504, 382, 257, 12075, 281, 264, 314, 1865, 1573, 18038, 460, 3927, 31958, 300], "temperature": 0.0, "avg_logprob": -0.22898825553998556, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0005876513896510005}, {"id": 125, "seek": 87040, "start": 885.56, "end": 894.92, "text": " these loops can run in parallel. And the second one is the kernel API, which is an API exposed", "tokens": [613, 16121, 393, 1190, 294, 8952, 13, 400, 264, 1150, 472, 307, 264, 28256, 9362, 11, 597, 307, 364, 9362, 9495], "temperature": 0.0, "avg_logprob": -0.22898825553998556, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0005876513896510005}, {"id": 126, "seek": 89492, "start": 894.92, "end": 901.64, "text": " to the users through the kernel context object. And in a sense, the meaning of this API, it", "tokens": [281, 264, 5022, 807, 264, 28256, 4319, 2657, 13, 400, 294, 257, 2020, 11, 264, 3620, 295, 341, 9362, 11, 309], "temperature": 0.0, "avg_logprob": -0.1771080310528095, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0003237372438888997}, {"id": 127, "seek": 89492, "start": 901.64, "end": 908.4399999999999, "text": " is meant for OpenCL and CUDA programmers, or Java programmers who know OpenCL, in a way", "tokens": [307, 4140, 337, 7238, 22458, 293, 29777, 7509, 41504, 11, 420, 10745, 41504, 567, 458, 7238, 22458, 11, 294, 257, 636], "temperature": 0.0, "avg_logprob": -0.1771080310528095, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0003237372438888997}, {"id": 128, "seek": 89492, "start": 908.4399999999999, "end": 915.1999999999999, "text": " to get more freedom on how to code things so they can get access to local memory, which", "tokens": [281, 483, 544, 5645, 322, 577, 281, 3089, 721, 370, 436, 393, 483, 2105, 281, 2654, 4675, 11, 597], "temperature": 0.0, "avg_logprob": -0.1771080310528095, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0003237372438888997}, {"id": 129, "seek": 89492, "start": 915.1999999999999, "end": 921.8399999999999, "text": " is the equivalent to the cache memory of the CPU for GPUs. So they have more freedom on", "tokens": [307, 264, 10344, 281, 264, 19459, 4675, 295, 264, 13199, 337, 18407, 82, 13, 407, 436, 362, 544, 5645, 322], "temperature": 0.0, "avg_logprob": -0.1771080310528095, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0003237372438888997}, {"id": 130, "seek": 92184, "start": 921.84, "end": 928.08, "text": " what to express. And in fact, I have used this API to port existing kernels written", "tokens": [437, 281, 5109, 13, 400, 294, 1186, 11, 286, 362, 1143, 341, 9362, 281, 2436, 6741, 23434, 1625, 3720], "temperature": 0.0, "avg_logprob": -0.1604121560635774, "compression_ratio": 1.4771784232365146, "no_speech_prob": 0.0028863598126918077}, {"id": 131, "seek": 92184, "start": 928.08, "end": 937.5600000000001, "text": " in OpenCL and CUDA to Java. For more information, you can use this link, which is the actual", "tokens": [294, 7238, 22458, 293, 29777, 7509, 281, 10745, 13, 1171, 544, 1589, 11, 291, 393, 764, 341, 2113, 11, 597, 307, 264, 3539], "temperature": 0.0, "avg_logprob": -0.1604121560635774, "compression_ratio": 1.4771784232365146, "no_speech_prob": 0.0028863598126918077}, {"id": 132, "seek": 92184, "start": 937.5600000000001, "end": 945.36, "text": " documentation of Tornado VM and describes some examples. I will briefly go to one example", "tokens": [14333, 295, 314, 1865, 1573, 18038, 293, 15626, 512, 5110, 13, 286, 486, 10515, 352, 281, 472, 1365], "temperature": 0.0, "avg_logprob": -0.1604121560635774, "compression_ratio": 1.4771784232365146, "no_speech_prob": 0.0028863598126918077}, {"id": 133, "seek": 92184, "start": 945.36, "end": 951.1600000000001, "text": " of a matrix multiplication, which I presented last year in FOSDEM. So in this example, we", "tokens": [295, 257, 8141, 27290, 11, 597, 286, 8212, 1036, 1064, 294, 479, 4367, 35, 6683, 13, 407, 294, 341, 1365, 11, 321], "temperature": 0.0, "avg_logprob": -0.1604121560635774, "compression_ratio": 1.4771784232365146, "no_speech_prob": 0.0028863598126918077}, {"id": 134, "seek": 95116, "start": 951.16, "end": 961.0, "text": " have the accelerated code and the host code. The matrix multiplication method, it implements", "tokens": [362, 264, 29763, 3089, 293, 264, 3975, 3089, 13, 440, 8141, 27290, 3170, 11, 309, 704, 17988], "temperature": 0.0, "avg_logprob": -0.19264886318108973, "compression_ratio": 1.82, "no_speech_prob": 0.0001493161980761215}, {"id": 135, "seek": 95116, "start": 961.0, "end": 968.68, "text": " matrix multiplication over a flattened arrays in two dimensions. And the way to annotate", "tokens": [8141, 27290, 670, 257, 24183, 292, 41011, 294, 732, 12819, 13, 400, 264, 636, 281, 25339, 473], "temperature": 0.0, "avg_logprob": -0.19264886318108973, "compression_ratio": 1.82, "no_speech_prob": 0.0001493161980761215}, {"id": 136, "seek": 95116, "start": 968.68, "end": 974.3199999999999, "text": " and express parallelism using the add parallel annotation, it would be to add the add parallel", "tokens": [293, 5109, 8952, 1434, 1228, 264, 909, 8952, 48654, 11, 309, 576, 312, 281, 909, 264, 909, 8952], "temperature": 0.0, "avg_logprob": -0.19264886318108973, "compression_ratio": 1.82, "no_speech_prob": 0.0001493161980761215}, {"id": 137, "seek": 95116, "start": 974.3199999999999, "end": 979.8, "text": " annotation inside the four loops. That means that we indicate that these loops could be", "tokens": [48654, 1854, 264, 1451, 16121, 13, 663, 1355, 300, 321, 13330, 300, 613, 16121, 727, 312], "temperature": 0.0, "avg_logprob": -0.19264886318108973, "compression_ratio": 1.82, "no_speech_prob": 0.0001493161980761215}, {"id": 138, "seek": 97980, "start": 979.8, "end": 989.04, "text": " executed in parallel. And now regarding the second API, the kernel API, we would use the", "tokens": [17577, 294, 8952, 13, 400, 586, 8595, 264, 1150, 9362, 11, 264, 28256, 9362, 11, 321, 576, 764, 264], "temperature": 0.0, "avg_logprob": -0.15737194173476277, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.00015550789248663932}, {"id": 139, "seek": 97980, "start": 989.04, "end": 999.12, "text": " kernel context object. And in particular, we would use the global ID X and Y, which correspond", "tokens": [28256, 4319, 2657, 13, 400, 294, 1729, 11, 321, 576, 764, 264, 4338, 7348, 1783, 293, 398, 11, 597, 6805], "temperature": 0.0, "avg_logprob": -0.15737194173476277, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.00015550789248663932}, {"id": 140, "seek": 97980, "start": 999.12, "end": 1003.8, "text": " to the two dimensions that we have. So in a sense, it is like having the thread ID that", "tokens": [281, 264, 732, 12819, 300, 321, 362, 13, 407, 294, 257, 2020, 11, 309, 307, 411, 1419, 264, 7207, 7348, 300], "temperature": 0.0, "avg_logprob": -0.15737194173476277, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.00015550789248663932}, {"id": 141, "seek": 100380, "start": 1003.8, "end": 1020.7199999999999, "text": " will execute on the GPU. Here are some use cases that we used on Tornado VM. And concluding", "tokens": [486, 14483, 322, 264, 18407, 13, 1692, 366, 512, 764, 3331, 300, 321, 1143, 322, 314, 1865, 1573, 18038, 13, 400, 9312, 278], "temperature": 0.0, "avg_logprob": -0.1505288236281451, "compression_ratio": 1.4480874316939891, "no_speech_prob": 0.0003307107836008072}, {"id": 142, "seek": 100380, "start": 1020.7199999999999, "end": 1025.48, "text": " this talk, I would like to focus on a feature that we implemented in a research project", "tokens": [341, 751, 11, 286, 576, 411, 281, 1879, 322, 257, 4111, 300, 321, 12270, 294, 257, 2132, 1716], "temperature": 0.0, "avg_logprob": -0.1505288236281451, "compression_ratio": 1.4480874316939891, "no_speech_prob": 0.0003307107836008072}, {"id": 143, "seek": 100380, "start": 1025.48, "end": 1031.6399999999999, "text": " that we are working. It is called elegant. And the idea is to create a software stack", "tokens": [300, 321, 366, 1364, 13, 467, 307, 1219, 21117, 13, 400, 264, 1558, 307, 281, 1884, 257, 4722, 8630], "temperature": 0.0, "avg_logprob": -0.1505288236281451, "compression_ratio": 1.4480874316939891, "no_speech_prob": 0.0003307107836008072}, {"id": 144, "seek": 103164, "start": 1031.64, "end": 1039.3200000000002, "text": " that unifies development for big data and IoT deployment. And there Tornado VM is used", "tokens": [300, 517, 11221, 3250, 337, 955, 1412, 293, 30112, 19317, 13, 400, 456, 314, 1865, 1573, 18038, 307, 1143], "temperature": 0.0, "avg_logprob": -0.15375363259088426, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00024907352053560317}, {"id": 145, "seek": 103164, "start": 1039.3200000000002, "end": 1044.24, "text": " as a technology to enable acceleration as a service. So we have implemented the REST", "tokens": [382, 257, 2899, 281, 9528, 17162, 382, 257, 2643, 13, 407, 321, 362, 12270, 264, 497, 14497], "temperature": 0.0, "avg_logprob": -0.15375363259088426, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00024907352053560317}, {"id": 146, "seek": 103164, "start": 1044.24, "end": 1051.0800000000002, "text": " API. It is still a prototype. But the programmers, they can write a method. They can specify", "tokens": [9362, 13, 467, 307, 920, 257, 19475, 13, 583, 264, 41504, 11, 436, 393, 2464, 257, 3170, 13, 814, 393, 16500], "temperature": 0.0, "avg_logprob": -0.15375363259088426, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00024907352053560317}, {"id": 147, "seek": 103164, "start": 1051.0800000000002, "end": 1056.4, "text": " a method. They can specify the characteristics of the targeted device. And then the service", "tokens": [257, 3170, 13, 814, 393, 16500, 264, 10891, 295, 264, 15045, 4302, 13, 400, 550, 264, 2643], "temperature": 0.0, "avg_logprob": -0.15375363259088426, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00024907352053560317}, {"id": 148, "seek": 105640, "start": 1056.4, "end": 1064.3200000000002, "text": " will return back OpenCL code that it is meant to run parallel dysfunction. The interesting", "tokens": [486, 2736, 646, 7238, 22458, 3089, 300, 309, 307, 4140, 281, 1190, 8952, 32002, 13, 440, 1880], "temperature": 0.0, "avg_logprob": -0.1756158173084259, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.0018365525174885988}, {"id": 149, "seek": 105640, "start": 1064.3200000000002, "end": 1071.0, "text": " part is that this code, the OpenCL code, it is generated to be portable across different", "tokens": [644, 307, 300, 341, 3089, 11, 264, 7238, 22458, 3089, 11, 309, 307, 10833, 281, 312, 21800, 2108, 819], "temperature": 0.0, "avg_logprob": -0.1756158173084259, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.0018365525174885988}, {"id": 150, "seek": 105640, "start": 1071.0, "end": 1079.68, "text": " programming languages. So it doesn't only bind to Java. It can run also through C++,", "tokens": [9410, 8650, 13, 407, 309, 1177, 380, 787, 14786, 281, 10745, 13, 467, 393, 1190, 611, 807, 383, 25472, 11], "temperature": 0.0, "avg_logprob": -0.1756158173084259, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.0018365525174885988}, {"id": 151, "seek": 107968, "start": 1079.68, "end": 1088.6000000000001, "text": " Python because it is OpenCL. And this means that in this particular example, we have Java.", "tokens": [15329, 570, 309, 307, 7238, 22458, 13, 400, 341, 1355, 300, 294, 341, 1729, 1365, 11, 321, 362, 10745, 13], "temperature": 0.0, "avg_logprob": -0.17244643353401345, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005084728472866118}, {"id": 152, "seek": 107968, "start": 1088.6000000000001, "end": 1094.8400000000001, "text": " We use OpenZDK. We take the byte code and we pass the byte code to Tornado VM. And Tornado", "tokens": [492, 764, 7238, 57, 35, 42, 13, 492, 747, 264, 40846, 3089, 293, 321, 1320, 264, 40846, 3089, 281, 314, 1865, 1573, 18038, 13, 400, 314, 1865, 1573], "temperature": 0.0, "avg_logprob": -0.17244643353401345, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005084728472866118}, {"id": 153, "seek": 107968, "start": 1094.8400000000001, "end": 1100.8400000000001, "text": " VM is running on an experimental feature which is called code interoperability mode. And", "tokens": [18038, 307, 2614, 322, 364, 17069, 4111, 597, 307, 1219, 3089, 728, 7192, 2310, 4391, 13, 400], "temperature": 0.0, "avg_logprob": -0.17244643353401345, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005084728472866118}, {"id": 154, "seek": 107968, "start": 1100.8400000000001, "end": 1107.5600000000002, "text": " in this mode, it converts this byte code to OpenCL that can run from any programming language", "tokens": [294, 341, 4391, 11, 309, 38874, 341, 40846, 3089, 281, 7238, 22458, 300, 393, 1190, 490, 604, 9410, 2856], "temperature": 0.0, "avg_logprob": -0.17244643353401345, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005084728472866118}, {"id": 155, "seek": 110756, "start": 1107.56, "end": 1116.3999999999999, "text": " and run time. Therefore, it is like prototyping in Java for parallel programming.", "tokens": [293, 1190, 565, 13, 7504, 11, 309, 307, 411, 46219, 3381, 294, 10745, 337, 8952, 9410, 13], "temperature": 0.0, "avg_logprob": -0.17065089299128605, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00047699332935735583}, {"id": 156, "seek": 110756, "start": 1116.3999999999999, "end": 1121.9199999999998, "text": " Wrapping up, we would like to receive feedback. And we are looking also for collaborations", "tokens": [343, 424, 3759, 493, 11, 321, 576, 411, 281, 4774, 5824, 13, 400, 321, 366, 1237, 611, 337, 36908], "temperature": 0.0, "avg_logprob": -0.17065089299128605, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00047699332935735583}, {"id": 157, "seek": 110756, "start": 1121.9199999999998, "end": 1132.08, "text": " if we can help to port use cases or for any other issues. And summarizing this talk, I", "tokens": [498, 321, 393, 854, 281, 2436, 764, 3331, 420, 337, 604, 661, 2663, 13, 400, 14611, 3319, 341, 751, 11, 286], "temperature": 0.0, "avg_logprob": -0.17065089299128605, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.00047699332935735583}, {"id": 158, "seek": 113208, "start": 1132.08, "end": 1137.8, "text": " briefly went through the right ones, run anywhere in the context of heterogeneous hardware", "tokens": [10515, 1437, 807, 264, 558, 2306, 11, 1190, 4992, 294, 264, 4319, 295, 20789, 31112, 8837], "temperature": 0.0, "avg_logprob": -0.1719000311458812, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.000872116768732667}, {"id": 159, "seek": 113208, "start": 1137.8, "end": 1143.8, "text": " acceleration. I have familiarized you with Tornado VM which is an open source project", "tokens": [17162, 13, 286, 362, 4963, 1602, 291, 365, 314, 1865, 1573, 18038, 597, 307, 364, 1269, 4009, 1716], "temperature": 0.0, "avg_logprob": -0.1719000311458812, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.000872116768732667}, {"id": 160, "seek": 113208, "start": 1143.8, "end": 1150.08, "text": " and the code base is available in GitHub. And I familiarize you with the programming", "tokens": [293, 264, 3089, 3096, 307, 2435, 294, 23331, 13, 400, 286, 4963, 1125, 291, 365, 264, 9410], "temperature": 0.0, "avg_logprob": -0.1719000311458812, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.000872116768732667}, {"id": 161, "seek": 113208, "start": 1150.08, "end": 1158.32, "text": " model of Tornado VM and the new API, how to use it. And more are about to come in the", "tokens": [2316, 295, 314, 1865, 1573, 18038, 293, 264, 777, 9362, 11, 577, 281, 764, 309, 13, 400, 544, 366, 466, 281, 808, 294, 264], "temperature": 0.0, "avg_logprob": -0.1719000311458812, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.000872116768732667}, {"id": 162, "seek": 115832, "start": 1158.32, "end": 1164.36, "text": " FUJ blog with a new blog. So finally, just to acknowledge the projects that they have", "tokens": [479, 52, 41, 6968, 365, 257, 777, 6968, 13, 407, 2721, 11, 445, 281, 10692, 264, 4455, 300, 436, 362], "temperature": 0.0, "avg_logprob": -0.27982344278475135, "compression_ratio": 1.2408759124087592, "no_speech_prob": 0.0007637754315510392}, {"id": 163, "seek": 116436, "start": 1164.36, "end": 1191.84, "text": " supported our research in the University of Manchester. And I'm ready for questions.", "tokens": [8104, 527, 2132, 294, 264, 3535, 295, 27180, 13, 400, 286, 478, 1919, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.278997015953064, "compression_ratio": 1.0120481927710843, "no_speech_prob": 0.001135700149461627}], "language": "en"}