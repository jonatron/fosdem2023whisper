{"text": " Hello, everyone. I'm Marco. Thank you for being here to listen to my talk. I'm an engineer manager at Mozilla. I've been at Mozilla for almost 10 years now. I started as a contributor, then an intern, then I was hired, and I've been here for almost 10 years. I started working on some funny projects, like writing a Java VM in JavaScript, and then more recently I started focusing on using machine learning and data mining techniques to improve developer efficiency, which has also been the subject of my PhD. During this talk, I will show you how we will all be out of a job in a few years, joking. I will just thank you through our journey of how we incrementally built features based on machine learning for improving software engineering, one on top of each other. I'm the father of two, Luna and Nika. Before we start with the presentation, I wanted to explain a little why we need to do all these complex machine learning things on top of bugs, CI, patches, et cetera, et cetera. Firefox is a very complex software. It's a browser. We have hundreds of bug reports and feature requests open per day. We have 108 million bug reports at this time, which is almost the price of one bedroom apartment in London. We release every four weeks with thousands of changes, and during 2022 we had 13 major releases and 45 million minor releases. As you can see, we even sometimes party when we reach a certain number of bugs. As I said, Firefox is one of the biggest software in the world. We have a lot of legacy. Netscape was open sourced 25 years ago. A few days ago we celebrated the 25 birthday. Over time we had 800,000 commits made by 9,000 unique contributors representing 25 million lines of code. We had 37,000 commits only last year by 1,000 unique contributors. Not all of them are paid. Many of them are volunteers. And this is a list of the languages that we use. As you can see, we use many of them. We have C++ and Rust for low-level things. Rust is gaining ground and is probably going to overcome C soon. We use JavaScript for front-end and for tests. And we use Python for CI and build system. But we have many more. So if anybody is interested in contributing, you have many options to choose from. But let's see. So as I said, the complexity is really large. We have thousands and thousands of bugs. And we need some way to control the quality, to increase the visibility into the quality of the software. And we cannot do that if the bugs are left uncontrolled. One of the first problems that we had was that there is no way to differentiate between defects and feature requests. We call them bugs on bugzilla. But they are actually just reports. Many of them are defects. Many of them are actually just feature requests. And so at the time, we had no way to measure quality. We had no way to tell in this release we have 100 bugs. In this other release, we had 50. So this release is better than the previous. And so we need a way to make this differentiation in order to measure the quality. And it was also hard to improve workflows if we had no way to differentiate between them. So we thought of introducing a new type field. This might seem simple. It's just choice between defect, enhancement and task. But in practice, when you have 9,000 unique contributors, some of them not paid. It's not easy to enforce a change like this. And we also had another problem. We have 100 million bugs. If we just introduce this type, it's not going to help us at all until we reach a mass of bugs that we change. So if we just introduce it at this time, it will only start to be useful six months from now. So we thought, how do we set the field for existing bugs so that this actually becomes useful from day one? And we thought of using machine learning. So we collected a dataset. I'm not sure it can be considered large nowadays. With 2,000 manually labelled bugs, few of us labelled independently. And then we shared the labelling so that we were consistent. And we had 9,000 labelled with some heuristics based on fields that were already present in bugzilla. Then we, using the fields from bugzilla and the title and comment through an NLP pipeline, we trained an XGB boost model. And we achieved accuracy that we deemed good enough to be used on production. And this is how the bug, bug project started. It was just a way to differentiate between defects and non-defects on bugzilla. We saw it worked and then we decided, we thought, what if we extend this to something else? What is the next big problem that we have on bugzilla? And it was assigning components. Again, we have lots of bugs, millions of, hundreds of thousands of bugs. We need a way to split them in groups so that the right team sees them, so that the right people see them. And the faster we do it, the faster we can fix them. At the time, it was manually done by volunteers and developers. So you can see a screenshot here, product and component, PDF viewer. In this case, we didn't need to manually create a data set because all of the 1 million bugs were already manually split into groups by volunteers and developers in the past. So we had, in this case, a very large data set, two decades worth of bugs. The problem here was that we had to roll back the bug to the initial state because otherwise, by training the model on the final state of the bug, we would have used future data to predict the past. And it was not possible, of course. So we rolled back the history of the bug to the beginning. We also reduced the number of components because, again, with the Firefox scale, we have hundreds of thousands of components. Many of them are no longer actually maintained and no longer relevant. So we reduced them to a smaller subset. And again, we had the same kind of architecture to train the model. With a small tweak, we didn't have perfect accuracy. And so we needed a way to choose confidence and recall. So pay the price of lower quality but catching more bugs or catching fewer bugs but be precise more time. So we can control this easily with a confidence level that is output by the model, which allows us to sometimes be more aggressive, sometimes be less aggressive. But at least we can have a minimum level of quality that we enforce. The average time to assign a bug then went from one week to a few seconds. Over time, we auto-classified 20,000 bugs. And since it worked, we also extended it to webcompad.com, which is yet another bug reporting system that we have at Mozilla, which if you find web compatibility bugs, please go there and file them because it's pretty important. And you can see here an action of the bot moving the bug to, again, the Firefox PDF viewer component. Maybe I should have used another example just for fun. Now we had something working. And it was starting to become promising. But we needed to make it better. We needed to have a better architecture for the machine learning side of things. We needed to retrain the models. We needed to collect new data. We needed to make sure that whenever a new component comes in, we retrain the model with the new components. If a component stops being used, we need to remove it from the dataset and things like that. So we built, over time, a very complex architecture. I won't go into too many details because it will take too long. But maybe if somebody has questions later, we can go into that. And then with the architecture now, it was easier to build new models. So we even had contributors building models just all by themselves. In particular, there was a contributor, Ayush, which helped us build a model to root out spam from bugzilla. So it seems weird, but yes, we do have spam on bugzilla as well. People are trying to get links to their websites into bugzilla because they think the search engine will index them. It's not actually the case. We tell them all the time, but they keep doing it anyway. We have university students. Bugzilla is probably the most studied bug tracking system in research. And we have many university students from many countries that use bugzilla as a playing field. Many times we even contact the universities and professors asking them if we can help them give more relevant topics to students, et cetera, but they keep filing bugs. And this contributor maybe was from one of these schools, was tired of it and helped us build a model. And the results were pretty good. I'll show you a few examples of bugs that were caught by the model. So this one was, if you look just at the first comment of the bug, it looks like a legit bug. But then the person created a second comment with a link to the website. And it was pretty clear that it was spam. This one is another example. This is actually a legit bug. It's not spam. Maybe it's not so usable as a bug report, but it was not spam. And then somebody else, a spammer, took exactly the same contents, created a new bug injecting the link to their website in the bug report. And somehow, I don't know how the model was able to detect that it was spam. It's funny because you can see that, so when you file a bug on bugzilla, bugzilla will automatically insert a user agent so that we have more information as possible to fix bugs. But in this case, he was filing the bug, copying the contents of the other bug, so we have two user agents. And they're even on different platforms, one on Mac and one on Chrome, actually. Okay. So we were done with bugs. We are not done with the bugs. We will have plenty of things to do in the future forever. But we were happy enough with bugs and we thought, what can we improve next? One of the topics that we were focusing on at the time was testing and cost associated to testing. We were experimenting with code coverage, trying to collect coverage to select relevant tests to run on a given patch. But it was pretty complex for various reasons. So we thought maybe we can apply machine learning here as well. But before we go into that, let me explain a bit about RCI because it's a little complex. So we have three branches, three repositories, which all kind of share the same code, Firefox. We have Try, which is on demand CI. We have AutoLand, which is the repository where patches land after they've been reviewed and approved. And we have Mozilla Central, which is the actual repository where Firefox source code lives and from which we build Firefox nightly. On Try, we run whatever the user wants. On AutoLand, we run a subset of tests. At the time, it was kind of random, what we decided to run. And on Mozilla Central, we run everything. To give you an idea on Try, we will have hundreds of pushes per day. On AutoLand, the same. And on Mozilla Central, we have only three or four. And it's restricted only to certain people that have the necessary permissions since you can build Firefox nightly from there. And it's going to be shipped to everyone. The scale here is similar to the bug case. We have 100,000 unique test files. We have around 150 unique test configurations. So combinations of operating systems, high level Firefox configurations. So old style engine versus new style engine, certain graphics engine versus another graphics engine, et cetera, et cetera. We have debug builds versus optimized builds. We have asan, code coverage, et cetera, et cetera. Of course, the matrix is huge and you get to 150 configurations. We have more than 300 pushes per day by developers. And the average push takes 1,500 hours if you were to run it all one after the other. It takes 300 machine years per month and we run around 100 million machines per month to run these tests. If you were to run all of the tests, you would need to run all of the tests in all of the configurations. You would need to run around 2.3 billion test files per day. Which is, of course, unfeasible. And this is a view of our tree herder, which is the user interface for Mozilla test results. You can see that it is almost unreadable. The green stuff is good. The orange stuff is probably not good. You can see that we have lots of tests and we spend a lot of money to run these tests. So what we wanted to do, we wanted to reduce the machine time, spend to run the tests. We wanted to reduce the end-to-end time so that developers, when they push, they get a result, yes or no, your patch is good or not, quickly. And we also wanted to reduce the cognitive overload for developers. Looking at a page like this, what is it? It's impossible to understand. Also, to give you an obvious example, if you're changing the Linux version of Firefox, I don't know, you're touching GTK, you don't need to run Windows tests. At the time, we were doing that. At the time, if you touched GTK code, we were running Android, Windows, Mac, that was totally useless. And the traditional way of running tests on browsers doesn't really work. You cannot run everything on all of the pushes. Otherwise, you will have a huge bill from the cloud provider. So we couldn't use coverage because of some technical reasons. We thought, what if we use machine learning? What if we extend bug, bug to also learn patches and tests? So the first part was to use machines to try to parse this information and try to understand what exactly failed. It might seem like an easy task if you have 100 tests or 10 tests, but when you have two billion tests, you have lots of intermittently failing tests. These tests fail randomly. They are not always the same. Every week, we see 150 new intermittent tests coming in. It's impossible to, it's not easy to automatically say if a failure is actually a failure or if it is an intermittent. Not even developers are able to do that sometimes. So not all of the tests are run on all of the pushes. So if I push my patch and a test doesn't run, but runs later on another patch and it fails, I don't know if it was my fault or somebody else's fault. And so we have sheriffs, people that are only focused, whose only focus, whose main focus is watching the CI, and they are pretty experienced at doing that, probably better than most developers. But human errors still exist. Even if we have their annotations, it's pretty hard to be sure about the results. You can see a meme that some sheriff created. Lucky tests are the infamous intermittently failing tests. So the first step, the second step after we implemented some heuristics to try to understand the failures due to a given patch was to analyze patches. We didn't have readily available tools, at least not fast enough for the amount of data that we are talking about. We just used Mercurial for authorship info. So who's the author of the push? Who's the reviewer? When was it pushed? Et cetera, et cetera. And we created a couple of projects written in Rust to parse patches efficiently and to analyze source code. The second one was actually a research partnership with the Politecnico di Torino. And the machine learning model itself, it's not a multi-label model as one might think, where each test is a label. It would be too large with the number of tests that we have. The model is simplified. The input is the table, test, and patch. And the label is just fail, not fail. So the features actually come from both the test, the patch, and the link between the test and the patch. So, for example, the past failures, when the same files were touched, the distance from the source files to the test files in the tree. How often source files were modified together with test files? Of course, if they're modified together, probably they are somehow linked. Maybe you need to fix the test. And so when you push your patch, you also fix the test. This is a clear link. But even then, we have lots of test redundancies. So we used frequent item set mining to try to understand which tests are redundant and remove them from the set of tests that are selected to run. And this was pretty successful as well. So now we had architecture to train models on bugs, to train models on patches and tests. The next step was to reuse what we built for patches to also try to predict defects. This is actually still an experimental phase. It's kind of a research project. So if anybody is interested in collaborating with us on this topic, we will be happy to do so. I will just show you a few things that we have done in the space for now. So the goals are to reduce the regressions by detecting the patches that reviewers should focus on more than others, to reduce the time spent by reviewers on less risky patches, and to when we detect that the patch is risky, trigger some risk control operations. For example, I don't know, running phasing tests more comprehensively in these patches and things like this. Of course, the model is just an evaluation of the risk. It's not actually going to tell us if there is a bug or not. And it will never replace a real reviewer who can actually review the patch more precisely. The first step was, again, build a data set. It is not easy to know which patches cause regressions. It's actually impossible at this time. There are some algorithms that are used in research. The most famous one is SZZ. But we had some answers that it was not so good. So we started here, again, introducing a change in the process that we have. We introduced a new field, which is called regressed by, so that developers, QA users, can specify what caused a given regression. So when they file a bug, if they know what caused it, they can specify it here. If they don't know what caused it, we have a few tools that we built over time to automatically download builds from RCI that we showed earlier. Automatically download builds from the past and run a bisection to try to find what the cause is for the given bug. With this, we managed to build a pretty large data set, 5,000 links between bug introducing and bug fixing commits. Actually, commit sets. And then this amounts to 24,000 commits. And then we were able, with this data set, to evaluate the current algorithms that are presented in the literature. And as we thought, they are not working well at all. So this is one of the areas of improvement for research. One of the improvements that we tried to apply and to SZZ was to improve the blame algorithm. If you're more familiar with Mercurial annotate algorithm, to try to, instead of looking at lines, splitting changes by words and tokens, so you can see changes, past changes by token instead of by line. This is a visualization from the Linux kernel. This is going to give you a much more precise view of what changed in the past. For example, it will skip over tab only changes, white space only changes and things like that. If you add an if, your code will be intended more, but you're not actually changing everything inside. You're changing only the if. This actually improved the results, but it was not enough to get to an acceptable level of accuracy. But it's nice and we can actually use it in the IDE. We're not doing it yet, but we will to give more information to users because developers use annotate and get blame a lot. And this is a UI that is work in progress for analyzing the risk of a patch. This is a screenshot from our code review tool. So we are showing the result of the algorithm with the confidence. So in this case, it was a risky patch with 79% confidence. And we give a few explanations to the developers. This is one of the most important things. Developers do not always trust developers like any other user. Do not always trust results from machine learning. And so you need to give them an explanation. And this is another part of the output of our tool. This is again on our code review tool. We're showing on the functions that are being changed by the patch if the function is risky or not. And which bugs in the past were involved in this function. So developers can try to see if the patch is reintroducing a previously fixed bug. And they can also know what kind of side effects there are when you make changes to a given area of the code. Now we did a lot of stuff for developers. We trained models for bugs. We trained models for patches. We trained models for tests. We trained models to predict the facts. Now I'm going to go to a slightly different topic even though it's connected. Privacy-friendly translations. So we're working on introducing translations in Firefox. The subtitle was actually translated automatically using Firefox translate, which you can use nowadays. The idea is that translation models improved a lot in recent times. Current cloud-based services do not offer the privacy guarantees that we like to offer in Firefox. They are closed source. They are not privacy-preserving. So we started a project. It was funded by the European Union to investigate client-side private translation capabilities in Firefox itself. It is currently available as an add-on that you can install in Firefox. We support many European languages and we're working on supporting even more. We're going to also work on supporting non-European languages like Chinese, Korean, Japanese, etc. And in this case, we use machine learning on the client side to perform the translation. So your data never leaves your Firefox. The models are downloaded from our servers, but they run locally on your machine. So the contents of the web page that you're looking at will never go to Google Bing or whatever. They will be translated locally on your machine. We use a few open data sets. Luckily, we have lots of them from past research. Not all of them have good quality, but many of them have. But we are looking for more. So if you have suggestions for data sets that we can use, please let us know. On the data sets, we perform some basic data cleaning. And we use machine learning-based techniques to clean the data, to remove bad sentence pairs that we believe are bad. Of course, the data set that I showed before are open, but sometimes they are just crawled from the web, so they contain all sorts of bad sentences. Also, HTML tags and stuff like that, we need to clean them up. Otherwise, the translations will learn to translate HTML tags. And we use some techniques to increase the size of the data set automatically, like back translations, translating sentences from one language to the other, and back translating it in order to increase the size of the data sets. So we trained a large model on cloud machines, which is pretty large. You can see it's around 800 megabytes, so every language pair, you would need to download 800 megabytes, and it is super slow, so we can only use that on cloud. So we use some techniques in order to reduce the size of these models and to make them faster. We use knowledge distillation, basically using the model, the large model that we trained as a trainer for a student model, which is much smaller, so you can see that from 800 megabytes we got to 16, I think now we're around 5, 6, something like that, so it's much smaller and you can actually download it on demand from our servers. And we use quantization for further compression and perf improvements, so moving the data from the model from float 32 to int 8. Then we compiled the machine translation engine to WebAssembly in order to be able to use it inside Firefox. We introduced some SIMD extensions into WebAssembly and into Firefox in order to be able to be even faster when translating, even though we translate a bit at a time, so it's pretty fast. And the engines are downloaded and updated on demand. Let me show you a demo. So, you can see my Firefox is in Italian, but you can see that it automatically detected that the page is in French and it is suggesting me to translate it to Italian. I will change it to English. Oh, fuck. So it is downloading the model. Now it's translating. So while it was translating, you already saw the contents of the first part of the page was already translated, so it's super quick in the end. And the translation seems to be pretty good. I don't speak French, but I think it makes sense. You can also use it from the toolbar, so you can choose a language and translate it to another. Let's do Italian to French. It works. All right. So if you know any data set that we can use, in addition to the ones that we already use, or if you're interested in building a new great feature in Firefox, or if you want to add support for your language or improving support for your language, come and talk to us at our booth. We would be really happy if you could help us. And before we come to an end, let me show you how far we've come. The dogs have grown, and we have learned that it is possible to tame the complexity of a large-scale software. It is possible to use the past history of development to support the future development, and it is possible to use machine learning in a privacy-friendly way and in the open. What else could we do with the data and the tools that we have at our disposal? I don't know. I'm looking forward to know. I'm looking forward to see what other wild ideas you and us at Mozilla can come up with. Thank you. Thank you very much, Marco, for the amazing talk. Now we're open for questions. If anyone would like to make a question, please raise your hand so I can take the microphone. Questions, questions, hands up. There. Okay, okay. I'm sorry, I'm learning. I'm new to this. I'm coming up. Hello. I have actually two questions. First question is, have you actually think about the idea to use this mechanism to automatically translate interface of Mozilla products? Sorry? This thing? Yes. Yeah. So the question is, have you think about mechanism of automatically translating the interface of Mozilla Firefox products, or maybe documentation you already have like MDN, because it's still a demand to translate this stuff? I'm sorry. I'm not hearing well. Can you maybe come closer? From here? Okay. Now it's better? Yes. Okay. So my question is, have you trying to use this mechanism of automatic translation to use this translation for existing interface you have in the products, and especially also documentation part? Because it's kind of vital part when you need to translate new functionality, or you have to translate something new in the interface, you need the help of translator. But if you already know how to translate in doing this stuff, so that means like you already have a data set, you can actually automatically translate new parts of interface without translator? Yes. So it is definitely something that could be used to help translators do their job. We could translate parts of the interface automatically. And of course, there will always be some review from actual translator to make sure that the translation makes sense in the context, especially because Firefox UI sometimes you have very short text and it needs to make sense. But yeah, it's definitely something that we have considered. And actually one of the data sets that we use from the list, it's not possible to see from the slide, but it's called Mozilla L10N and they are sentence pairs from our browser UI. People are actually using it in research for automating translations. Does anyone have any other question? Please raise your hands. If you have any other questions, Marco? Okay. If not, thank you very much again, Marco. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.120000000000001, "text": " Hello, everyone. I'm Marco. Thank you for being here to listen to my talk. I'm an engineer", "tokens": [2425, 11, 1518, 13, 286, 478, 26535, 13, 1044, 291, 337, 885, 510, 281, 2140, 281, 452, 751, 13, 286, 478, 364, 11403], "temperature": 0.0, "avg_logprob": -0.2178703609265779, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.3614824414253235}, {"id": 1, "seek": 0, "start": 13.120000000000001, "end": 21.72, "text": " manager at Mozilla. I've been at Mozilla for almost 10 years now. I started as a contributor,", "tokens": [6598, 412, 3335, 26403, 13, 286, 600, 668, 412, 3335, 26403, 337, 1920, 1266, 924, 586, 13, 286, 1409, 382, 257, 42859, 11], "temperature": 0.0, "avg_logprob": -0.2178703609265779, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.3614824414253235}, {"id": 2, "seek": 0, "start": 21.72, "end": 27.92, "text": " then an intern, then I was hired, and I've been here for almost 10 years. I started working", "tokens": [550, 364, 2154, 11, 550, 286, 390, 13144, 11, 293, 286, 600, 668, 510, 337, 1920, 1266, 924, 13, 286, 1409, 1364], "temperature": 0.0, "avg_logprob": -0.2178703609265779, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.3614824414253235}, {"id": 3, "seek": 2792, "start": 27.92, "end": 34.84, "text": " on some funny projects, like writing a Java VM in JavaScript, and then more recently I", "tokens": [322, 512, 4074, 4455, 11, 411, 3579, 257, 10745, 18038, 294, 15778, 11, 293, 550, 544, 3938, 286], "temperature": 0.0, "avg_logprob": -0.15736259848384535, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.0009325526771135628}, {"id": 4, "seek": 2792, "start": 34.84, "end": 41.24, "text": " started focusing on using machine learning and data mining techniques to improve developer", "tokens": [1409, 8416, 322, 1228, 3479, 2539, 293, 1412, 15512, 7512, 281, 3470, 10754], "temperature": 0.0, "avg_logprob": -0.15736259848384535, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.0009325526771135628}, {"id": 5, "seek": 2792, "start": 41.24, "end": 53.400000000000006, "text": " efficiency, which has also been the subject of my PhD. During this talk, I will show you", "tokens": [10493, 11, 597, 575, 611, 668, 264, 3983, 295, 452, 14476, 13, 6842, 341, 751, 11, 286, 486, 855, 291], "temperature": 0.0, "avg_logprob": -0.15736259848384535, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.0009325526771135628}, {"id": 6, "seek": 5340, "start": 53.4, "end": 61.12, "text": " how we will all be out of a job in a few years, joking. I will just thank you through our journey", "tokens": [577, 321, 486, 439, 312, 484, 295, 257, 1691, 294, 257, 1326, 924, 11, 17396, 13, 286, 486, 445, 1309, 291, 807, 527, 4671], "temperature": 0.0, "avg_logprob": -0.12970732558857312, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.000658665201626718}, {"id": 7, "seek": 5340, "start": 61.12, "end": 69.72, "text": " of how we incrementally built features based on machine learning for improving software", "tokens": [295, 577, 321, 26200, 379, 3094, 4122, 2361, 322, 3479, 2539, 337, 11470, 4722], "temperature": 0.0, "avg_logprob": -0.12970732558857312, "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.000658665201626718}, {"id": 8, "seek": 6972, "start": 69.72, "end": 85.08, "text": " engineering, one on top of each other. I'm the father of two, Luna and Nika. Before we start", "tokens": [7043, 11, 472, 322, 1192, 295, 1184, 661, 13, 286, 478, 264, 3086, 295, 732, 11, 27355, 293, 426, 5439, 13, 4546, 321, 722], "temperature": 0.0, "avg_logprob": -0.12441631158192952, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0011961412383243442}, {"id": 9, "seek": 6972, "start": 85.08, "end": 91.64, "text": " with the presentation, I wanted to explain a little why we need to do all these complex", "tokens": [365, 264, 5860, 11, 286, 1415, 281, 2903, 257, 707, 983, 321, 643, 281, 360, 439, 613, 3997], "temperature": 0.0, "avg_logprob": -0.12441631158192952, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0011961412383243442}, {"id": 10, "seek": 9164, "start": 91.64, "end": 101.76, "text": " machine learning things on top of bugs, CI, patches, et cetera, et cetera. Firefox is a very", "tokens": [3479, 2539, 721, 322, 1192, 295, 15120, 11, 37777, 11, 26531, 11, 1030, 11458, 11, 1030, 11458, 13, 46613, 307, 257, 588], "temperature": 0.0, "avg_logprob": -0.12849682480541627, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0005189169896766543}, {"id": 11, "seek": 9164, "start": 101.76, "end": 107.84, "text": " complex software. It's a browser. We have hundreds of bug reports and feature requests", "tokens": [3997, 4722, 13, 467, 311, 257, 11185, 13, 492, 362, 6779, 295, 7426, 7122, 293, 4111, 12475], "temperature": 0.0, "avg_logprob": -0.12849682480541627, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0005189169896766543}, {"id": 12, "seek": 9164, "start": 107.84, "end": 116.24000000000001, "text": " open per day. We have 108 million bug reports at this time, which is almost the price of", "tokens": [1269, 680, 786, 13, 492, 362, 41342, 2459, 7426, 7122, 412, 341, 565, 11, 597, 307, 1920, 264, 3218, 295], "temperature": 0.0, "avg_logprob": -0.12849682480541627, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0005189169896766543}, {"id": 13, "seek": 11624, "start": 116.24, "end": 124.24, "text": " one bedroom apartment in London. We release every four weeks with thousands of changes,", "tokens": [472, 11211, 9587, 294, 7042, 13, 492, 4374, 633, 1451, 3259, 365, 5383, 295, 2962, 11], "temperature": 0.0, "avg_logprob": -0.1357677247789171, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0005105164018459618}, {"id": 14, "seek": 11624, "start": 124.24, "end": 132.32, "text": " and during 2022 we had 13 major releases and 45 million minor releases. As you can see,", "tokens": [293, 1830, 20229, 321, 632, 3705, 2563, 16952, 293, 6905, 2459, 6696, 16952, 13, 1018, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.1357677247789171, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0005105164018459618}, {"id": 15, "seek": 11624, "start": 132.32, "end": 144.64, "text": " we even sometimes party when we reach a certain number of bugs. As I said, Firefox is one", "tokens": [321, 754, 2171, 3595, 562, 321, 2524, 257, 1629, 1230, 295, 15120, 13, 1018, 286, 848, 11, 46613, 307, 472], "temperature": 0.0, "avg_logprob": -0.1357677247789171, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0005105164018459618}, {"id": 16, "seek": 14464, "start": 144.64, "end": 151.32, "text": " of the biggest software in the world. We have a lot of legacy. Netscape was open sourced", "tokens": [295, 264, 3880, 4722, 294, 264, 1002, 13, 492, 362, 257, 688, 295, 11711, 13, 426, 1385, 4747, 390, 1269, 11006, 1232], "temperature": 0.0, "avg_logprob": -0.16162767675187853, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.0008613176760263741}, {"id": 17, "seek": 14464, "start": 151.32, "end": 161.16, "text": " 25 years ago. A few days ago we celebrated the 25 birthday. Over time we had 800,000", "tokens": [3552, 924, 2057, 13, 316, 1326, 1708, 2057, 321, 19366, 264, 3552, 6154, 13, 4886, 565, 321, 632, 13083, 11, 1360], "temperature": 0.0, "avg_logprob": -0.16162767675187853, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.0008613176760263741}, {"id": 18, "seek": 14464, "start": 161.16, "end": 169.95999999999998, "text": " commits made by 9,000 unique contributors representing 25 million lines of code. We had 37,000 commits", "tokens": [48311, 1027, 538, 1722, 11, 1360, 3845, 45627, 13460, 3552, 2459, 3876, 295, 3089, 13, 492, 632, 13435, 11, 1360, 48311], "temperature": 0.0, "avg_logprob": -0.16162767675187853, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.0008613176760263741}, {"id": 19, "seek": 16996, "start": 169.96, "end": 177.24, "text": " only last year by 1,000 unique contributors. Not all of them are paid. Many of them are", "tokens": [787, 1036, 1064, 538, 502, 11, 1360, 3845, 45627, 13, 1726, 439, 295, 552, 366, 4835, 13, 5126, 295, 552, 366], "temperature": 0.0, "avg_logprob": -0.16909239743206952, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0018306473502889276}, {"id": 20, "seek": 16996, "start": 177.24, "end": 184.04000000000002, "text": " volunteers. And this is a list of the languages that we use. As you can see, we use many of", "tokens": [14352, 13, 400, 341, 307, 257, 1329, 295, 264, 8650, 300, 321, 764, 13, 1018, 291, 393, 536, 11, 321, 764, 867, 295], "temperature": 0.0, "avg_logprob": -0.16909239743206952, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0018306473502889276}, {"id": 21, "seek": 16996, "start": 184.04000000000002, "end": 191.20000000000002, "text": " them. We have C++ and Rust for low-level things. Rust is gaining ground and is probably going", "tokens": [552, 13, 492, 362, 383, 25472, 293, 34952, 337, 2295, 12, 12418, 721, 13, 34952, 307, 19752, 2727, 293, 307, 1391, 516], "temperature": 0.0, "avg_logprob": -0.16909239743206952, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0018306473502889276}, {"id": 22, "seek": 19120, "start": 191.2, "end": 201.79999999999998, "text": " to overcome C soon. We use JavaScript for front-end and for tests. And we use Python for CI and", "tokens": [281, 10473, 383, 2321, 13, 492, 764, 15778, 337, 1868, 12, 521, 293, 337, 6921, 13, 400, 321, 764, 15329, 337, 37777, 293], "temperature": 0.0, "avg_logprob": -0.20028730119977678, "compression_ratio": 1.4308510638297873, "no_speech_prob": 0.00041307660285383463}, {"id": 23, "seek": 19120, "start": 201.79999999999998, "end": 208.0, "text": " build system. But we have many more. So if anybody is interested in contributing, you", "tokens": [1322, 1185, 13, 583, 321, 362, 867, 544, 13, 407, 498, 4472, 307, 3102, 294, 19270, 11, 291], "temperature": 0.0, "avg_logprob": -0.20028730119977678, "compression_ratio": 1.4308510638297873, "no_speech_prob": 0.00041307660285383463}, {"id": 24, "seek": 19120, "start": 208.0, "end": 219.92, "text": " have many options to choose from. But let's see. So as I said, the complexity is really", "tokens": [362, 867, 3956, 281, 2826, 490, 13, 583, 718, 311, 536, 13, 407, 382, 286, 848, 11, 264, 14024, 307, 534], "temperature": 0.0, "avg_logprob": -0.20028730119977678, "compression_ratio": 1.4308510638297873, "no_speech_prob": 0.00041307660285383463}, {"id": 25, "seek": 21992, "start": 219.92, "end": 227.11999999999998, "text": " large. We have thousands and thousands of bugs. And we need some way to control the", "tokens": [2416, 13, 492, 362, 5383, 293, 5383, 295, 15120, 13, 400, 321, 643, 512, 636, 281, 1969, 264], "temperature": 0.0, "avg_logprob": -0.11670822567409939, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.0002485696168150753}, {"id": 26, "seek": 21992, "start": 227.11999999999998, "end": 233.79999999999998, "text": " quality, to increase the visibility into the quality of the software. And we cannot do", "tokens": [3125, 11, 281, 3488, 264, 19883, 666, 264, 3125, 295, 264, 4722, 13, 400, 321, 2644, 360], "temperature": 0.0, "avg_logprob": -0.11670822567409939, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.0002485696168150753}, {"id": 27, "seek": 21992, "start": 233.79999999999998, "end": 240.44, "text": " that if the bugs are left uncontrolled. One of the first problems that we had was that", "tokens": [300, 498, 264, 15120, 366, 1411, 36019, 28850, 13, 1485, 295, 264, 700, 2740, 300, 321, 632, 390, 300], "temperature": 0.0, "avg_logprob": -0.11670822567409939, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.0002485696168150753}, {"id": 28, "seek": 21992, "start": 240.44, "end": 246.64, "text": " there is no way to differentiate between defects and feature requests. We call them bugs on", "tokens": [456, 307, 572, 636, 281, 23203, 1296, 32655, 293, 4111, 12475, 13, 492, 818, 552, 15120, 322], "temperature": 0.0, "avg_logprob": -0.11670822567409939, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.0002485696168150753}, {"id": 29, "seek": 24664, "start": 246.64, "end": 252.51999999999998, "text": " bugzilla. But they are actually just reports. Many of them are defects. Many of them are", "tokens": [7426, 26403, 13, 583, 436, 366, 767, 445, 7122, 13, 5126, 295, 552, 366, 32655, 13, 5126, 295, 552, 366], "temperature": 0.0, "avg_logprob": -0.15366694691416982, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.0003043704782612622}, {"id": 30, "seek": 24664, "start": 252.51999999999998, "end": 260.0, "text": " actually just feature requests. And so at the time, we had no way to measure quality.", "tokens": [767, 445, 4111, 12475, 13, 400, 370, 412, 264, 565, 11, 321, 632, 572, 636, 281, 3481, 3125, 13], "temperature": 0.0, "avg_logprob": -0.15366694691416982, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.0003043704782612622}, {"id": 31, "seek": 24664, "start": 260.0, "end": 266.36, "text": " We had no way to tell in this release we have 100 bugs. In this other release, we had 50.", "tokens": [492, 632, 572, 636, 281, 980, 294, 341, 4374, 321, 362, 2319, 15120, 13, 682, 341, 661, 4374, 11, 321, 632, 2625, 13], "temperature": 0.0, "avg_logprob": -0.15366694691416982, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.0003043704782612622}, {"id": 32, "seek": 24664, "start": 266.36, "end": 272.64, "text": " So this release is better than the previous. And so we need a way to make this differentiation", "tokens": [407, 341, 4374, 307, 1101, 813, 264, 3894, 13, 400, 370, 321, 643, 257, 636, 281, 652, 341, 38902], "temperature": 0.0, "avg_logprob": -0.15366694691416982, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.0003043704782612622}, {"id": 33, "seek": 27264, "start": 272.64, "end": 276.96, "text": " in order to measure the quality. And it was also hard to improve workflows if we had no", "tokens": [294, 1668, 281, 3481, 264, 3125, 13, 400, 309, 390, 611, 1152, 281, 3470, 43461, 498, 321, 632, 572], "temperature": 0.0, "avg_logprob": -0.13013958108836207, "compression_ratio": 1.4895397489539748, "no_speech_prob": 0.00047045917017385364}, {"id": 34, "seek": 27264, "start": 276.96, "end": 282.96, "text": " way to differentiate between them. So we thought of introducing a new type field. This might", "tokens": [636, 281, 23203, 1296, 552, 13, 407, 321, 1194, 295, 15424, 257, 777, 2010, 2519, 13, 639, 1062], "temperature": 0.0, "avg_logprob": -0.13013958108836207, "compression_ratio": 1.4895397489539748, "no_speech_prob": 0.00047045917017385364}, {"id": 35, "seek": 27264, "start": 282.96, "end": 290.15999999999997, "text": " seem simple. It's just choice between defect, enhancement and task. But in practice, when", "tokens": [1643, 2199, 13, 467, 311, 445, 3922, 1296, 16445, 11, 40776, 293, 5633, 13, 583, 294, 3124, 11, 562], "temperature": 0.0, "avg_logprob": -0.13013958108836207, "compression_ratio": 1.4895397489539748, "no_speech_prob": 0.00047045917017385364}, {"id": 36, "seek": 27264, "start": 290.15999999999997, "end": 297.32, "text": " you have 9,000 unique contributors, some of them not paid. It's not easy to enforce a", "tokens": [291, 362, 1722, 11, 1360, 3845, 45627, 11, 512, 295, 552, 406, 4835, 13, 467, 311, 406, 1858, 281, 24825, 257], "temperature": 0.0, "avg_logprob": -0.13013958108836207, "compression_ratio": 1.4895397489539748, "no_speech_prob": 0.00047045917017385364}, {"id": 37, "seek": 29732, "start": 297.32, "end": 305.59999999999997, "text": " change like this. And we also had another problem. We have 100 million bugs. If we just", "tokens": [1319, 411, 341, 13, 400, 321, 611, 632, 1071, 1154, 13, 492, 362, 2319, 2459, 15120, 13, 759, 321, 445], "temperature": 0.0, "avg_logprob": -0.11634105184803838, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0019482615171000361}, {"id": 38, "seek": 29732, "start": 305.59999999999997, "end": 313.28, "text": " introduce this type, it's not going to help us at all until we reach a mass of bugs that", "tokens": [5366, 341, 2010, 11, 309, 311, 406, 516, 281, 854, 505, 412, 439, 1826, 321, 2524, 257, 2758, 295, 15120, 300], "temperature": 0.0, "avg_logprob": -0.11634105184803838, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0019482615171000361}, {"id": 39, "seek": 29732, "start": 313.28, "end": 320.32, "text": " we change. So if we just introduce it at this time, it will only start to be useful six", "tokens": [321, 1319, 13, 407, 498, 321, 445, 5366, 309, 412, 341, 565, 11, 309, 486, 787, 722, 281, 312, 4420, 2309], "temperature": 0.0, "avg_logprob": -0.11634105184803838, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0019482615171000361}, {"id": 40, "seek": 29732, "start": 320.32, "end": 326.24, "text": " months from now. So we thought, how do we set the field for existing bugs so that this", "tokens": [2493, 490, 586, 13, 407, 321, 1194, 11, 577, 360, 321, 992, 264, 2519, 337, 6741, 15120, 370, 300, 341], "temperature": 0.0, "avg_logprob": -0.11634105184803838, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0019482615171000361}, {"id": 41, "seek": 32624, "start": 326.24, "end": 333.16, "text": " actually becomes useful from day one? And we thought of using machine learning. So we", "tokens": [767, 3643, 4420, 490, 786, 472, 30, 400, 321, 1194, 295, 1228, 3479, 2539, 13, 407, 321], "temperature": 0.0, "avg_logprob": -0.20681787259650952, "compression_ratio": 1.4385026737967914, "no_speech_prob": 0.0009832476498559117}, {"id": 42, "seek": 32624, "start": 333.16, "end": 342.56, "text": " collected a dataset. I'm not sure it can be considered large nowadays. With 2,000 manually", "tokens": [11087, 257, 28872, 13, 286, 478, 406, 988, 309, 393, 312, 4888, 2416, 13434, 13, 2022, 568, 11, 1360, 16945], "temperature": 0.0, "avg_logprob": -0.20681787259650952, "compression_ratio": 1.4385026737967914, "no_speech_prob": 0.0009832476498559117}, {"id": 43, "seek": 32624, "start": 342.56, "end": 353.08, "text": " labelled bugs, few of us labelled independently. And then we shared the labelling so that we", "tokens": [2715, 41307, 15120, 11, 1326, 295, 505, 2715, 41307, 21761, 13, 400, 550, 321, 5507, 264, 2715, 11073, 370, 300, 321], "temperature": 0.0, "avg_logprob": -0.20681787259650952, "compression_ratio": 1.4385026737967914, "no_speech_prob": 0.0009832476498559117}, {"id": 44, "seek": 35308, "start": 353.08, "end": 358.96, "text": " were consistent. And we had 9,000 labelled with some heuristics based on fields that", "tokens": [645, 8398, 13, 400, 321, 632, 1722, 11, 1360, 2715, 41307, 365, 512, 415, 374, 6006, 2361, 322, 7909, 300], "temperature": 0.0, "avg_logprob": -0.18151550862326551, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0009252371964976192}, {"id": 45, "seek": 35308, "start": 358.96, "end": 365.91999999999996, "text": " were already present in bugzilla. Then we, using the fields from bugzilla and the title", "tokens": [645, 1217, 1974, 294, 7426, 26403, 13, 1396, 321, 11, 1228, 264, 7909, 490, 7426, 26403, 293, 264, 4876], "temperature": 0.0, "avg_logprob": -0.18151550862326551, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0009252371964976192}, {"id": 46, "seek": 35308, "start": 365.91999999999996, "end": 373.84, "text": " and comment through an NLP pipeline, we trained an XGB boost model. And we achieved accuracy", "tokens": [293, 2871, 807, 364, 426, 45196, 15517, 11, 321, 8895, 364, 1783, 8769, 9194, 2316, 13, 400, 321, 11042, 14170], "temperature": 0.0, "avg_logprob": -0.18151550862326551, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.0009252371964976192}, {"id": 47, "seek": 37384, "start": 373.84, "end": 385.44, "text": " that we deemed good enough to be used on production. And this is how the bug, bug project started.", "tokens": [300, 321, 27637, 665, 1547, 281, 312, 1143, 322, 4265, 13, 400, 341, 307, 577, 264, 7426, 11, 7426, 1716, 1409, 13], "temperature": 0.0, "avg_logprob": -0.12779731750488282, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0004601686086971313}, {"id": 48, "seek": 37384, "start": 385.44, "end": 392.0, "text": " It was just a way to differentiate between defects and non-defects on bugzilla. We saw", "tokens": [467, 390, 445, 257, 636, 281, 23203, 1296, 32655, 293, 2107, 12, 1479, 1836, 82, 322, 7426, 26403, 13, 492, 1866], "temperature": 0.0, "avg_logprob": -0.12779731750488282, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0004601686086971313}, {"id": 49, "seek": 37384, "start": 392.0, "end": 399.91999999999996, "text": " it worked and then we decided, we thought, what if we extend this to something else?", "tokens": [309, 2732, 293, 550, 321, 3047, 11, 321, 1194, 11, 437, 498, 321, 10101, 341, 281, 746, 1646, 30], "temperature": 0.0, "avg_logprob": -0.12779731750488282, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0004601686086971313}, {"id": 50, "seek": 39992, "start": 399.92, "end": 407.6, "text": " What is the next big problem that we have on bugzilla? And it was assigning components.", "tokens": [708, 307, 264, 958, 955, 1154, 300, 321, 362, 322, 7426, 26403, 30, 400, 309, 390, 49602, 6677, 13], "temperature": 0.0, "avg_logprob": -0.11332496520011656, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0007896352326497436}, {"id": 51, "seek": 39992, "start": 407.6, "end": 414.96000000000004, "text": " Again, we have lots of bugs, millions of, hundreds of thousands of bugs. We need a way", "tokens": [3764, 11, 321, 362, 3195, 295, 15120, 11, 6803, 295, 11, 6779, 295, 5383, 295, 15120, 13, 492, 643, 257, 636], "temperature": 0.0, "avg_logprob": -0.11332496520011656, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0007896352326497436}, {"id": 52, "seek": 39992, "start": 414.96000000000004, "end": 421.28000000000003, "text": " to split them in groups so that the right team sees them, so that the right people see them.", "tokens": [281, 7472, 552, 294, 3935, 370, 300, 264, 558, 1469, 8194, 552, 11, 370, 300, 264, 558, 561, 536, 552, 13], "temperature": 0.0, "avg_logprob": -0.11332496520011656, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0007896352326497436}, {"id": 53, "seek": 39992, "start": 421.28000000000003, "end": 425.88, "text": " And the faster we do it, the faster we can fix them. At the time, it was manually done", "tokens": [400, 264, 4663, 321, 360, 309, 11, 264, 4663, 321, 393, 3191, 552, 13, 1711, 264, 565, 11, 309, 390, 16945, 1096], "temperature": 0.0, "avg_logprob": -0.11332496520011656, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0007896352326497436}, {"id": 54, "seek": 42588, "start": 425.88, "end": 431.08, "text": " by volunteers and developers. So you can see a screenshot here, product and component,", "tokens": [538, 14352, 293, 8849, 13, 407, 291, 393, 536, 257, 27712, 510, 11, 1674, 293, 6542, 11], "temperature": 0.0, "avg_logprob": -0.19475561885510462, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.0004609567404258996}, {"id": 55, "seek": 42588, "start": 431.08, "end": 442.4, "text": " PDF viewer. In this case, we didn't need to manually create a data set because all of", "tokens": [17752, 16767, 13, 682, 341, 1389, 11, 321, 994, 380, 643, 281, 16945, 1884, 257, 1412, 992, 570, 439, 295], "temperature": 0.0, "avg_logprob": -0.19475561885510462, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.0004609567404258996}, {"id": 56, "seek": 42588, "start": 442.4, "end": 450.76, "text": " the 1 million bugs were already manually split into groups by volunteers and developers", "tokens": [264, 502, 2459, 15120, 645, 1217, 16945, 7472, 666, 3935, 538, 14352, 293, 8849], "temperature": 0.0, "avg_logprob": -0.19475561885510462, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.0004609567404258996}, {"id": 57, "seek": 45076, "start": 450.76, "end": 459.76, "text": " in the past. So we had, in this case, a very large data set, two decades worth of bugs.", "tokens": [294, 264, 1791, 13, 407, 321, 632, 11, 294, 341, 1389, 11, 257, 588, 2416, 1412, 992, 11, 732, 7878, 3163, 295, 15120, 13], "temperature": 0.0, "avg_logprob": -0.11491822159808615, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0005775804165750742}, {"id": 58, "seek": 45076, "start": 459.76, "end": 468.24, "text": " The problem here was that we had to roll back the bug to the initial state because otherwise,", "tokens": [440, 1154, 510, 390, 300, 321, 632, 281, 3373, 646, 264, 7426, 281, 264, 5883, 1785, 570, 5911, 11], "temperature": 0.0, "avg_logprob": -0.11491822159808615, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0005775804165750742}, {"id": 59, "seek": 45076, "start": 468.24, "end": 473.88, "text": " by training the model on the final state of the bug, we would have used future data to", "tokens": [538, 3097, 264, 2316, 322, 264, 2572, 1785, 295, 264, 7426, 11, 321, 576, 362, 1143, 2027, 1412, 281], "temperature": 0.0, "avg_logprob": -0.11491822159808615, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0005775804165750742}, {"id": 60, "seek": 45076, "start": 473.88, "end": 478.71999999999997, "text": " predict the past. And it was not possible, of course. So we rolled back the history of", "tokens": [6069, 264, 1791, 13, 400, 309, 390, 406, 1944, 11, 295, 1164, 13, 407, 321, 14306, 646, 264, 2503, 295], "temperature": 0.0, "avg_logprob": -0.11491822159808615, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0005775804165750742}, {"id": 61, "seek": 47872, "start": 478.72, "end": 484.24, "text": " the bug to the beginning. We also reduced the number of components because, again, with", "tokens": [264, 7426, 281, 264, 2863, 13, 492, 611, 9212, 264, 1230, 295, 6677, 570, 11, 797, 11, 365], "temperature": 0.0, "avg_logprob": -0.1327543373567512, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0008045941940508783}, {"id": 62, "seek": 47872, "start": 484.24, "end": 489.44000000000005, "text": " the Firefox scale, we have hundreds of thousands of components. Many of them are no longer", "tokens": [264, 46613, 4373, 11, 321, 362, 6779, 295, 5383, 295, 6677, 13, 5126, 295, 552, 366, 572, 2854], "temperature": 0.0, "avg_logprob": -0.1327543373567512, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0008045941940508783}, {"id": 63, "seek": 47872, "start": 489.44000000000005, "end": 495.24, "text": " actually maintained and no longer relevant. So we reduced them to a smaller subset. And", "tokens": [767, 17578, 293, 572, 2854, 7340, 13, 407, 321, 9212, 552, 281, 257, 4356, 25993, 13, 400], "temperature": 0.0, "avg_logprob": -0.1327543373567512, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0008045941940508783}, {"id": 64, "seek": 47872, "start": 495.24, "end": 503.72, "text": " again, we had the same kind of architecture to train the model. With a small tweak, we", "tokens": [797, 11, 321, 632, 264, 912, 733, 295, 9482, 281, 3847, 264, 2316, 13, 2022, 257, 1359, 29879, 11, 321], "temperature": 0.0, "avg_logprob": -0.1327543373567512, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0008045941940508783}, {"id": 65, "seek": 50372, "start": 503.72, "end": 513.6, "text": " didn't have perfect accuracy. And so we needed a way to choose confidence and recall. So", "tokens": [994, 380, 362, 2176, 14170, 13, 400, 370, 321, 2978, 257, 636, 281, 2826, 6687, 293, 9901, 13, 407], "temperature": 0.0, "avg_logprob": -0.1689291230167251, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.000414772832300514}, {"id": 66, "seek": 50372, "start": 513.6, "end": 521.36, "text": " pay the price of lower quality but catching more bugs or catching fewer bugs but be precise", "tokens": [1689, 264, 3218, 295, 3126, 3125, 457, 16124, 544, 15120, 420, 16124, 13366, 15120, 457, 312, 13600], "temperature": 0.0, "avg_logprob": -0.1689291230167251, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.000414772832300514}, {"id": 67, "seek": 50372, "start": 521.36, "end": 527.88, "text": " more time. So we can control this easily with a confidence level that is output by the model,", "tokens": [544, 565, 13, 407, 321, 393, 1969, 341, 3612, 365, 257, 6687, 1496, 300, 307, 5598, 538, 264, 2316, 11], "temperature": 0.0, "avg_logprob": -0.1689291230167251, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.000414772832300514}, {"id": 68, "seek": 50372, "start": 527.88, "end": 533.28, "text": " which allows us to sometimes be more aggressive, sometimes be less aggressive. But at least", "tokens": [597, 4045, 505, 281, 2171, 312, 544, 10762, 11, 2171, 312, 1570, 10762, 13, 583, 412, 1935], "temperature": 0.0, "avg_logprob": -0.1689291230167251, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.000414772832300514}, {"id": 69, "seek": 53328, "start": 533.28, "end": 540.16, "text": " we can have a minimum level of quality that we enforce. The average time to assign a bug", "tokens": [321, 393, 362, 257, 7285, 1496, 295, 3125, 300, 321, 24825, 13, 440, 4274, 565, 281, 6269, 257, 7426], "temperature": 0.0, "avg_logprob": -0.12243321121379894, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0013895636657252908}, {"id": 70, "seek": 53328, "start": 540.16, "end": 548.6, "text": " then went from one week to a few seconds. Over time, we auto-classified 20,000 bugs.", "tokens": [550, 1437, 490, 472, 1243, 281, 257, 1326, 3949, 13, 4886, 565, 11, 321, 8399, 12, 11665, 2587, 945, 11, 1360, 15120, 13], "temperature": 0.0, "avg_logprob": -0.12243321121379894, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0013895636657252908}, {"id": 71, "seek": 53328, "start": 548.6, "end": 555.9599999999999, "text": " And since it worked, we also extended it to webcompad.com, which is yet another bug reporting", "tokens": [400, 1670, 309, 2732, 11, 321, 611, 10913, 309, 281, 3670, 1112, 13647, 13, 1112, 11, 597, 307, 1939, 1071, 7426, 10031], "temperature": 0.0, "avg_logprob": -0.12243321121379894, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0013895636657252908}, {"id": 72, "seek": 53328, "start": 555.9599999999999, "end": 561.48, "text": " system that we have at Mozilla, which if you find web compatibility bugs, please go there", "tokens": [1185, 300, 321, 362, 412, 3335, 26403, 11, 597, 498, 291, 915, 3670, 34237, 15120, 11, 1767, 352, 456], "temperature": 0.0, "avg_logprob": -0.12243321121379894, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0013895636657252908}, {"id": 73, "seek": 56148, "start": 561.48, "end": 565.64, "text": " and file them because it's pretty important. And you can see here an action of the bot", "tokens": [293, 3991, 552, 570, 309, 311, 1238, 1021, 13, 400, 291, 393, 536, 510, 364, 3069, 295, 264, 10592], "temperature": 0.0, "avg_logprob": -0.10524892246021944, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0011299251345917583}, {"id": 74, "seek": 56148, "start": 565.64, "end": 571.8000000000001, "text": " moving the bug to, again, the Firefox PDF viewer component. Maybe I should have used", "tokens": [2684, 264, 7426, 281, 11, 797, 11, 264, 46613, 17752, 16767, 6542, 13, 2704, 286, 820, 362, 1143], "temperature": 0.0, "avg_logprob": -0.10524892246021944, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0011299251345917583}, {"id": 75, "seek": 56148, "start": 571.8000000000001, "end": 581.24, "text": " another example just for fun. Now we had something working. And it was starting to become promising.", "tokens": [1071, 1365, 445, 337, 1019, 13, 823, 321, 632, 746, 1364, 13, 400, 309, 390, 2891, 281, 1813, 20257, 13], "temperature": 0.0, "avg_logprob": -0.10524892246021944, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0011299251345917583}, {"id": 76, "seek": 56148, "start": 581.24, "end": 586.4, "text": " But we needed to make it better. We needed to have a better architecture for the machine", "tokens": [583, 321, 2978, 281, 652, 309, 1101, 13, 492, 2978, 281, 362, 257, 1101, 9482, 337, 264, 3479], "temperature": 0.0, "avg_logprob": -0.10524892246021944, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0011299251345917583}, {"id": 77, "seek": 58640, "start": 586.4, "end": 591.56, "text": " learning side of things. We needed to retrain the models. We needed to collect new data.", "tokens": [2539, 1252, 295, 721, 13, 492, 2978, 281, 1533, 7146, 264, 5245, 13, 492, 2978, 281, 2500, 777, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13000030517578126, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0010248348116874695}, {"id": 78, "seek": 58640, "start": 591.56, "end": 596.88, "text": " We needed to make sure that whenever a new component comes in, we retrain the model with", "tokens": [492, 2978, 281, 652, 988, 300, 5699, 257, 777, 6542, 1487, 294, 11, 321, 1533, 7146, 264, 2316, 365], "temperature": 0.0, "avg_logprob": -0.13000030517578126, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0010248348116874695}, {"id": 79, "seek": 58640, "start": 596.88, "end": 603.76, "text": " the new components. If a component stops being used, we need to remove it from the dataset", "tokens": [264, 777, 6677, 13, 759, 257, 6542, 10094, 885, 1143, 11, 321, 643, 281, 4159, 309, 490, 264, 28872], "temperature": 0.0, "avg_logprob": -0.13000030517578126, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0010248348116874695}, {"id": 80, "seek": 58640, "start": 603.76, "end": 610.88, "text": " and things like that. So we built, over time, a very complex architecture. I won't go into", "tokens": [293, 721, 411, 300, 13, 407, 321, 3094, 11, 670, 565, 11, 257, 588, 3997, 9482, 13, 286, 1582, 380, 352, 666], "temperature": 0.0, "avg_logprob": -0.13000030517578126, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0010248348116874695}, {"id": 81, "seek": 61088, "start": 610.88, "end": 617.08, "text": " too many details because it will take too long. But maybe if somebody has questions later,", "tokens": [886, 867, 4365, 570, 309, 486, 747, 886, 938, 13, 583, 1310, 498, 2618, 575, 1651, 1780, 11], "temperature": 0.0, "avg_logprob": -0.13909277708634085, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0003997995227109641}, {"id": 82, "seek": 61088, "start": 617.08, "end": 630.8, "text": " we can go into that. And then with the architecture now, it was easier to build new models. So", "tokens": [321, 393, 352, 666, 300, 13, 400, 550, 365, 264, 9482, 586, 11, 309, 390, 3571, 281, 1322, 777, 5245, 13, 407], "temperature": 0.0, "avg_logprob": -0.13909277708634085, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0003997995227109641}, {"id": 83, "seek": 63080, "start": 630.8, "end": 641.0799999999999, "text": " we even had contributors building models just all by themselves. In particular, there was", "tokens": [321, 754, 632, 45627, 2390, 5245, 445, 439, 538, 2969, 13, 682, 1729, 11, 456, 390], "temperature": 0.0, "avg_logprob": -0.14972624138220034, "compression_ratio": 1.4775280898876404, "no_speech_prob": 0.0007481839857064188}, {"id": 84, "seek": 63080, "start": 641.0799999999999, "end": 649.8399999999999, "text": " a contributor, Ayush, which helped us build a model to root out spam from bugzilla. So", "tokens": [257, 42859, 11, 9081, 1498, 11, 597, 4254, 505, 1322, 257, 2316, 281, 5593, 484, 24028, 490, 7426, 26403, 13, 407], "temperature": 0.0, "avg_logprob": -0.14972624138220034, "compression_ratio": 1.4775280898876404, "no_speech_prob": 0.0007481839857064188}, {"id": 85, "seek": 63080, "start": 649.8399999999999, "end": 655.4799999999999, "text": " it seems weird, but yes, we do have spam on bugzilla as well. People are trying to get", "tokens": [309, 2544, 3657, 11, 457, 2086, 11, 321, 360, 362, 24028, 322, 7426, 26403, 382, 731, 13, 3432, 366, 1382, 281, 483], "temperature": 0.0, "avg_logprob": -0.14972624138220034, "compression_ratio": 1.4775280898876404, "no_speech_prob": 0.0007481839857064188}, {"id": 86, "seek": 65548, "start": 655.48, "end": 662.12, "text": " links to their websites into bugzilla because they think the search engine will index them.", "tokens": [6123, 281, 641, 12891, 666, 7426, 26403, 570, 436, 519, 264, 3164, 2848, 486, 8186, 552, 13], "temperature": 0.0, "avg_logprob": -0.11175437271595001, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.00137867103330791}, {"id": 87, "seek": 65548, "start": 662.12, "end": 668.08, "text": " It's not actually the case. We tell them all the time, but they keep doing it anyway. We", "tokens": [467, 311, 406, 767, 264, 1389, 13, 492, 980, 552, 439, 264, 565, 11, 457, 436, 1066, 884, 309, 4033, 13, 492], "temperature": 0.0, "avg_logprob": -0.11175437271595001, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.00137867103330791}, {"id": 88, "seek": 65548, "start": 668.08, "end": 678.32, "text": " have university students. Bugzilla is probably the most studied bug tracking system in research.", "tokens": [362, 5454, 1731, 13, 23821, 26403, 307, 1391, 264, 881, 9454, 7426, 11603, 1185, 294, 2132, 13], "temperature": 0.0, "avg_logprob": -0.11175437271595001, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.00137867103330791}, {"id": 89, "seek": 67832, "start": 678.32, "end": 689.44, "text": " And we have many university students from many countries that use bugzilla as a playing field.", "tokens": [400, 321, 362, 867, 5454, 1731, 490, 867, 3517, 300, 764, 7426, 26403, 382, 257, 2433, 2519, 13], "temperature": 0.0, "avg_logprob": -0.13577482255838685, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0017386223189532757}, {"id": 90, "seek": 67832, "start": 689.44, "end": 695.6400000000001, "text": " Many times we even contact the universities and professors asking them if we can help", "tokens": [5126, 1413, 321, 754, 3385, 264, 11779, 293, 15924, 3365, 552, 498, 321, 393, 854], "temperature": 0.0, "avg_logprob": -0.13577482255838685, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0017386223189532757}, {"id": 91, "seek": 67832, "start": 695.6400000000001, "end": 705.48, "text": " them give more relevant topics to students, et cetera, but they keep filing bugs. And", "tokens": [552, 976, 544, 7340, 8378, 281, 1731, 11, 1030, 11458, 11, 457, 436, 1066, 26854, 15120, 13, 400], "temperature": 0.0, "avg_logprob": -0.13577482255838685, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0017386223189532757}, {"id": 92, "seek": 70548, "start": 705.48, "end": 710.12, "text": " this contributor maybe was from one of these schools, was tired of it and helped us build", "tokens": [341, 42859, 1310, 390, 490, 472, 295, 613, 4656, 11, 390, 5868, 295, 309, 293, 4254, 505, 1322], "temperature": 0.0, "avg_logprob": -0.11557390448752414, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0016778066055849195}, {"id": 93, "seek": 70548, "start": 710.12, "end": 717.84, "text": " a model. And the results were pretty good. I'll show you a few examples of bugs that", "tokens": [257, 2316, 13, 400, 264, 3542, 645, 1238, 665, 13, 286, 603, 855, 291, 257, 1326, 5110, 295, 15120, 300], "temperature": 0.0, "avg_logprob": -0.11557390448752414, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0016778066055849195}, {"id": 94, "seek": 70548, "start": 717.84, "end": 726.76, "text": " were caught by the model. So this one was, if you look just at the first comment of the", "tokens": [645, 5415, 538, 264, 2316, 13, 407, 341, 472, 390, 11, 498, 291, 574, 445, 412, 264, 700, 2871, 295, 264], "temperature": 0.0, "avg_logprob": -0.11557390448752414, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0016778066055849195}, {"id": 95, "seek": 70548, "start": 726.76, "end": 732.64, "text": " bug, it looks like a legit bug. But then the person created a second comment with a link", "tokens": [7426, 11, 309, 1542, 411, 257, 10275, 7426, 13, 583, 550, 264, 954, 2942, 257, 1150, 2871, 365, 257, 2113], "temperature": 0.0, "avg_logprob": -0.11557390448752414, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0016778066055849195}, {"id": 96, "seek": 73264, "start": 732.64, "end": 743.0, "text": " to the website. And it was pretty clear that it was spam. This one is another example. This", "tokens": [281, 264, 3144, 13, 400, 309, 390, 1238, 1850, 300, 309, 390, 24028, 13, 639, 472, 307, 1071, 1365, 13, 639], "temperature": 0.0, "avg_logprob": -0.12418101284955, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0006560030742548406}, {"id": 97, "seek": 73264, "start": 743.0, "end": 751.16, "text": " is actually a legit bug. It's not spam. Maybe it's not so usable as a bug report, but it", "tokens": [307, 767, 257, 10275, 7426, 13, 467, 311, 406, 24028, 13, 2704, 309, 311, 406, 370, 29975, 382, 257, 7426, 2275, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.12418101284955, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0006560030742548406}, {"id": 98, "seek": 73264, "start": 751.16, "end": 759.3199999999999, "text": " was not spam. And then somebody else, a spammer, took exactly the same contents, created a", "tokens": [390, 406, 24028, 13, 400, 550, 2618, 1646, 11, 257, 24028, 936, 11, 1890, 2293, 264, 912, 15768, 11, 2942, 257], "temperature": 0.0, "avg_logprob": -0.12418101284955, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0006560030742548406}, {"id": 99, "seek": 75932, "start": 759.32, "end": 766.36, "text": " new bug injecting the link to their website in the bug report. And somehow, I don't know", "tokens": [777, 7426, 10711, 278, 264, 2113, 281, 641, 3144, 294, 264, 7426, 2275, 13, 400, 6063, 11, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.16126632690429688, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00043108497629873455}, {"id": 100, "seek": 75932, "start": 766.36, "end": 773.12, "text": " how the model was able to detect that it was spam. It's funny because you can see that,", "tokens": [577, 264, 2316, 390, 1075, 281, 5531, 300, 309, 390, 24028, 13, 467, 311, 4074, 570, 291, 393, 536, 300, 11], "temperature": 0.0, "avg_logprob": -0.16126632690429688, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00043108497629873455}, {"id": 101, "seek": 75932, "start": 773.12, "end": 779.5600000000001, "text": " so when you file a bug on bugzilla, bugzilla will automatically insert a user agent so", "tokens": [370, 562, 291, 3991, 257, 7426, 322, 7426, 26403, 11, 7426, 26403, 486, 6772, 8969, 257, 4195, 9461, 370], "temperature": 0.0, "avg_logprob": -0.16126632690429688, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00043108497629873455}, {"id": 102, "seek": 75932, "start": 779.5600000000001, "end": 785.8800000000001, "text": " that we have more information as possible to fix bugs. But in this case, he was filing", "tokens": [300, 321, 362, 544, 1589, 382, 1944, 281, 3191, 15120, 13, 583, 294, 341, 1389, 11, 415, 390, 26854], "temperature": 0.0, "avg_logprob": -0.16126632690429688, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00043108497629873455}, {"id": 103, "seek": 78588, "start": 785.88, "end": 791.68, "text": " the bug, copying the contents of the other bug, so we have two user agents. And they're", "tokens": [264, 7426, 11, 27976, 264, 15768, 295, 264, 661, 7426, 11, 370, 321, 362, 732, 4195, 12554, 13, 400, 436, 434], "temperature": 0.0, "avg_logprob": -0.18851423935151437, "compression_ratio": 1.5290697674418605, "no_speech_prob": 0.0009368248865939677}, {"id": 104, "seek": 78588, "start": 791.68, "end": 804.24, "text": " even on different platforms, one on Mac and one on Chrome, actually. Okay. So we were", "tokens": [754, 322, 819, 9473, 11, 472, 322, 5707, 293, 472, 322, 15327, 11, 767, 13, 1033, 13, 407, 321, 645], "temperature": 0.0, "avg_logprob": -0.18851423935151437, "compression_ratio": 1.5290697674418605, "no_speech_prob": 0.0009368248865939677}, {"id": 105, "seek": 78588, "start": 804.24, "end": 810.28, "text": " done with bugs. We are not done with the bugs. We will have plenty of things to do in the", "tokens": [1096, 365, 15120, 13, 492, 366, 406, 1096, 365, 264, 15120, 13, 492, 486, 362, 7140, 295, 721, 281, 360, 294, 264], "temperature": 0.0, "avg_logprob": -0.18851423935151437, "compression_ratio": 1.5290697674418605, "no_speech_prob": 0.0009368248865939677}, {"id": 106, "seek": 81028, "start": 810.28, "end": 820.4, "text": " future forever. But we were happy enough with bugs and we thought, what can we improve next?", "tokens": [2027, 5680, 13, 583, 321, 645, 2055, 1547, 365, 15120, 293, 321, 1194, 11, 437, 393, 321, 3470, 958, 30], "temperature": 0.0, "avg_logprob": -0.12391742331082703, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.0011259770253673196}, {"id": 107, "seek": 81028, "start": 820.4, "end": 828.6, "text": " One of the topics that we were focusing on at the time was testing and cost associated", "tokens": [1485, 295, 264, 8378, 300, 321, 645, 8416, 322, 412, 264, 565, 390, 4997, 293, 2063, 6615], "temperature": 0.0, "avg_logprob": -0.12391742331082703, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.0011259770253673196}, {"id": 108, "seek": 81028, "start": 828.6, "end": 835.48, "text": " to testing. We were experimenting with code coverage, trying to collect coverage to select", "tokens": [281, 4997, 13, 492, 645, 29070, 365, 3089, 9645, 11, 1382, 281, 2500, 9645, 281, 3048], "temperature": 0.0, "avg_logprob": -0.12391742331082703, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.0011259770253673196}, {"id": 109, "seek": 83548, "start": 835.48, "end": 847.0, "text": " relevant tests to run on a given patch. But it was pretty complex for various reasons.", "tokens": [7340, 6921, 281, 1190, 322, 257, 2212, 9972, 13, 583, 309, 390, 1238, 3997, 337, 3683, 4112, 13], "temperature": 0.0, "avg_logprob": -0.11545046763633614, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00037884427001699805}, {"id": 110, "seek": 83548, "start": 847.0, "end": 853.6800000000001, "text": " So we thought maybe we can apply machine learning here as well. But before we go into that,", "tokens": [407, 321, 1194, 1310, 321, 393, 3079, 3479, 2539, 510, 382, 731, 13, 583, 949, 321, 352, 666, 300, 11], "temperature": 0.0, "avg_logprob": -0.11545046763633614, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00037884427001699805}, {"id": 111, "seek": 83548, "start": 853.6800000000001, "end": 859.6800000000001, "text": " let me explain a bit about RCI because it's a little complex. So we have three branches,", "tokens": [718, 385, 2903, 257, 857, 466, 497, 25240, 570, 309, 311, 257, 707, 3997, 13, 407, 321, 362, 1045, 14770, 11], "temperature": 0.0, "avg_logprob": -0.11545046763633614, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00037884427001699805}, {"id": 112, "seek": 85968, "start": 859.68, "end": 865.76, "text": " three repositories, which all kind of share the same code, Firefox. We have Try, which", "tokens": [1045, 22283, 2083, 11, 597, 439, 733, 295, 2073, 264, 912, 3089, 11, 46613, 13, 492, 362, 6526, 11, 597], "temperature": 0.0, "avg_logprob": -0.1965511663636165, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0007583289407193661}, {"id": 113, "seek": 85968, "start": 865.76, "end": 875.4799999999999, "text": " is on demand CI. We have AutoLand, which is the repository where patches land after they've", "tokens": [307, 322, 4733, 37777, 13, 492, 362, 13738, 43, 474, 11, 597, 307, 264, 25841, 689, 26531, 2117, 934, 436, 600], "temperature": 0.0, "avg_logprob": -0.1965511663636165, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0007583289407193661}, {"id": 114, "seek": 85968, "start": 875.4799999999999, "end": 882.0799999999999, "text": " been reviewed and approved. And we have Mozilla Central, which is the actual repository where", "tokens": [668, 18429, 293, 10826, 13, 400, 321, 362, 3335, 26403, 9701, 11, 597, 307, 264, 3539, 25841, 689], "temperature": 0.0, "avg_logprob": -0.1965511663636165, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0007583289407193661}, {"id": 115, "seek": 88208, "start": 882.08, "end": 891.44, "text": " Firefox source code lives and from which we build Firefox nightly. On Try, we run whatever", "tokens": [46613, 4009, 3089, 2909, 293, 490, 597, 321, 1322, 46613, 1818, 356, 13, 1282, 6526, 11, 321, 1190, 2035], "temperature": 0.0, "avg_logprob": -0.13222439148846796, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.001459319144487381}, {"id": 116, "seek": 88208, "start": 891.44, "end": 898.08, "text": " the user wants. On AutoLand, we run a subset of tests. At the time, it was kind of random,", "tokens": [264, 4195, 2738, 13, 1282, 13738, 43, 474, 11, 321, 1190, 257, 25993, 295, 6921, 13, 1711, 264, 565, 11, 309, 390, 733, 295, 4974, 11], "temperature": 0.0, "avg_logprob": -0.13222439148846796, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.001459319144487381}, {"id": 117, "seek": 88208, "start": 898.08, "end": 905.2, "text": " what we decided to run. And on Mozilla Central, we run everything. To give you an idea on", "tokens": [437, 321, 3047, 281, 1190, 13, 400, 322, 3335, 26403, 9701, 11, 321, 1190, 1203, 13, 1407, 976, 291, 364, 1558, 322], "temperature": 0.0, "avg_logprob": -0.13222439148846796, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.001459319144487381}, {"id": 118, "seek": 88208, "start": 905.2, "end": 910.48, "text": " Try, we will have hundreds of pushes per day. On AutoLand, the same. And on Mozilla Central,", "tokens": [6526, 11, 321, 486, 362, 6779, 295, 21020, 680, 786, 13, 1282, 13738, 43, 474, 11, 264, 912, 13, 400, 322, 3335, 26403, 9701, 11], "temperature": 0.0, "avg_logprob": -0.13222439148846796, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.001459319144487381}, {"id": 119, "seek": 91048, "start": 910.48, "end": 921.0, "text": " we have only three or four. And it's restricted only to certain people that have the necessary", "tokens": [321, 362, 787, 1045, 420, 1451, 13, 400, 309, 311, 20608, 787, 281, 1629, 561, 300, 362, 264, 4818], "temperature": 0.0, "avg_logprob": -0.1423310664162707, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0018134824931621552}, {"id": 120, "seek": 91048, "start": 921.0, "end": 927.0, "text": " permissions since you can build Firefox nightly from there. And it's going to be shipped to", "tokens": [32723, 1670, 291, 393, 1322, 46613, 1818, 356, 490, 456, 13, 400, 309, 311, 516, 281, 312, 25312, 281], "temperature": 0.0, "avg_logprob": -0.1423310664162707, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0018134824931621552}, {"id": 121, "seek": 91048, "start": 927.0, "end": 938.52, "text": " everyone. The scale here is similar to the bug case. We have 100,000 unique test files.", "tokens": [1518, 13, 440, 4373, 510, 307, 2531, 281, 264, 7426, 1389, 13, 492, 362, 2319, 11, 1360, 3845, 1500, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1423310664162707, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0018134824931621552}, {"id": 122, "seek": 93852, "start": 938.52, "end": 947.28, "text": " We have around 150 unique test configurations. So combinations of operating systems, high", "tokens": [492, 362, 926, 8451, 3845, 1500, 31493, 13, 407, 21267, 295, 7447, 3652, 11, 1090], "temperature": 0.0, "avg_logprob": -0.17491119248526438, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.001081356662325561}, {"id": 123, "seek": 93852, "start": 947.28, "end": 953.96, "text": " level Firefox configurations. So old style engine versus new style engine, certain graphics", "tokens": [1496, 46613, 31493, 13, 407, 1331, 3758, 2848, 5717, 777, 3758, 2848, 11, 1629, 11837], "temperature": 0.0, "avg_logprob": -0.17491119248526438, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.001081356662325561}, {"id": 124, "seek": 93852, "start": 953.96, "end": 961.76, "text": " engine versus another graphics engine, et cetera, et cetera. We have debug builds versus optimized", "tokens": [2848, 5717, 1071, 11837, 2848, 11, 1030, 11458, 11, 1030, 11458, 13, 492, 362, 24083, 15182, 5717, 26941], "temperature": 0.0, "avg_logprob": -0.17491119248526438, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.001081356662325561}, {"id": 125, "seek": 96176, "start": 961.76, "end": 969.16, "text": " builds. We have asan, code coverage, et cetera, et cetera. Of course, the matrix is huge and", "tokens": [15182, 13, 492, 362, 382, 282, 11, 3089, 9645, 11, 1030, 11458, 11, 1030, 11458, 13, 2720, 1164, 11, 264, 8141, 307, 2603, 293], "temperature": 0.0, "avg_logprob": -0.20799396407436316, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0012880901340395212}, {"id": 126, "seek": 96176, "start": 969.16, "end": 975.48, "text": " you get to 150 configurations. We have more than 300 pushes per day by developers. And", "tokens": [291, 483, 281, 8451, 31493, 13, 492, 362, 544, 813, 6641, 21020, 680, 786, 538, 8849, 13, 400], "temperature": 0.0, "avg_logprob": -0.20799396407436316, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0012880901340395212}, {"id": 127, "seek": 96176, "start": 975.48, "end": 983.16, "text": " the average push takes 1,500 hours if you were to run it all one after the other. It", "tokens": [264, 4274, 2944, 2516, 502, 11, 7526, 2496, 498, 291, 645, 281, 1190, 309, 439, 472, 934, 264, 661, 13, 467], "temperature": 0.0, "avg_logprob": -0.20799396407436316, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0012880901340395212}, {"id": 128, "seek": 98316, "start": 983.16, "end": 993.16, "text": " takes 300 machine years per month and we run around 100 million machines per month to run", "tokens": [2516, 6641, 3479, 924, 680, 1618, 293, 321, 1190, 926, 2319, 2459, 8379, 680, 1618, 281, 1190], "temperature": 0.0, "avg_logprob": -0.15601410585291245, "compression_ratio": 1.8299319727891157, "no_speech_prob": 0.0014984379522502422}, {"id": 129, "seek": 98316, "start": 993.16, "end": 1000.64, "text": " these tests. If you were to run all of the tests, you would need to run all of the tests", "tokens": [613, 6921, 13, 759, 291, 645, 281, 1190, 439, 295, 264, 6921, 11, 291, 576, 643, 281, 1190, 439, 295, 264, 6921], "temperature": 0.0, "avg_logprob": -0.15601410585291245, "compression_ratio": 1.8299319727891157, "no_speech_prob": 0.0014984379522502422}, {"id": 130, "seek": 98316, "start": 1000.64, "end": 1006.48, "text": " in all of the configurations. You would need to run around 2.3 billion test files per day.", "tokens": [294, 439, 295, 264, 31493, 13, 509, 576, 643, 281, 1190, 926, 568, 13, 18, 5218, 1500, 7098, 680, 786, 13], "temperature": 0.0, "avg_logprob": -0.15601410585291245, "compression_ratio": 1.8299319727891157, "no_speech_prob": 0.0014984379522502422}, {"id": 131, "seek": 100648, "start": 1006.48, "end": 1016.8000000000001, "text": " Which is, of course, unfeasible. And this is a view of our tree herder, which is the user", "tokens": [3013, 307, 11, 295, 1164, 11, 517, 2106, 296, 964, 13, 400, 341, 307, 257, 1910, 295, 527, 4230, 720, 1068, 11, 597, 307, 264, 4195], "temperature": 0.0, "avg_logprob": -0.15646469270860827, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.00027938743005506694}, {"id": 132, "seek": 100648, "start": 1016.8000000000001, "end": 1024.2, "text": " interface for Mozilla test results. You can see that it is almost unreadable. The green", "tokens": [9226, 337, 3335, 26403, 1500, 3542, 13, 509, 393, 536, 300, 309, 307, 1920, 517, 2538, 712, 13, 440, 3092], "temperature": 0.0, "avg_logprob": -0.15646469270860827, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.00027938743005506694}, {"id": 133, "seek": 100648, "start": 1024.2, "end": 1032.1200000000001, "text": " stuff is good. The orange stuff is probably not good. You can see that we have lots of", "tokens": [1507, 307, 665, 13, 440, 7671, 1507, 307, 1391, 406, 665, 13, 509, 393, 536, 300, 321, 362, 3195, 295], "temperature": 0.0, "avg_logprob": -0.15646469270860827, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.00027938743005506694}, {"id": 134, "seek": 103212, "start": 1032.12, "end": 1040.56, "text": " tests and we spend a lot of money to run these tests. So what we wanted to do, we wanted", "tokens": [6921, 293, 321, 3496, 257, 688, 295, 1460, 281, 1190, 613, 6921, 13, 407, 437, 321, 1415, 281, 360, 11, 321, 1415], "temperature": 0.0, "avg_logprob": -0.1226208970901814, "compression_ratio": 1.79, "no_speech_prob": 0.0008552050567232072}, {"id": 135, "seek": 103212, "start": 1040.56, "end": 1045.32, "text": " to reduce the machine time, spend to run the tests. We wanted to reduce the end-to-end", "tokens": [281, 5407, 264, 3479, 565, 11, 3496, 281, 1190, 264, 6921, 13, 492, 1415, 281, 5407, 264, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.1226208970901814, "compression_ratio": 1.79, "no_speech_prob": 0.0008552050567232072}, {"id": 136, "seek": 103212, "start": 1045.32, "end": 1051.6399999999999, "text": " time so that developers, when they push, they get a result, yes or no, your patch is good", "tokens": [565, 370, 300, 8849, 11, 562, 436, 2944, 11, 436, 483, 257, 1874, 11, 2086, 420, 572, 11, 428, 9972, 307, 665], "temperature": 0.0, "avg_logprob": -0.1226208970901814, "compression_ratio": 1.79, "no_speech_prob": 0.0008552050567232072}, {"id": 137, "seek": 103212, "start": 1051.6399999999999, "end": 1057.8799999999999, "text": " or not, quickly. And we also wanted to reduce the cognitive overload for developers. Looking", "tokens": [420, 406, 11, 2661, 13, 400, 321, 611, 1415, 281, 5407, 264, 15605, 28777, 337, 8849, 13, 11053], "temperature": 0.0, "avg_logprob": -0.1226208970901814, "compression_ratio": 1.79, "no_speech_prob": 0.0008552050567232072}, {"id": 138, "seek": 105788, "start": 1057.88, "end": 1068.2800000000002, "text": " at a page like this, what is it? It's impossible to understand. Also, to give you an obvious", "tokens": [412, 257, 3028, 411, 341, 11, 437, 307, 309, 30, 467, 311, 6243, 281, 1223, 13, 2743, 11, 281, 976, 291, 364, 6322], "temperature": 0.0, "avg_logprob": -0.14405355955425062, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.0022592321038246155}, {"id": 139, "seek": 105788, "start": 1068.2800000000002, "end": 1078.16, "text": " example, if you're changing the Linux version of Firefox, I don't know, you're touching", "tokens": [1365, 11, 498, 291, 434, 4473, 264, 18734, 3037, 295, 46613, 11, 286, 500, 380, 458, 11, 291, 434, 11175], "temperature": 0.0, "avg_logprob": -0.14405355955425062, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.0022592321038246155}, {"id": 140, "seek": 105788, "start": 1078.16, "end": 1085.0400000000002, "text": " GTK, you don't need to run Windows tests. At the time, we were doing that. At the time,", "tokens": [17530, 42, 11, 291, 500, 380, 643, 281, 1190, 8591, 6921, 13, 1711, 264, 565, 11, 321, 645, 884, 300, 13, 1711, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.14405355955425062, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.0022592321038246155}, {"id": 141, "seek": 108504, "start": 1085.04, "end": 1093.84, "text": " if you touched GTK code, we were running Android, Windows, Mac, that was totally useless. And", "tokens": [498, 291, 9828, 17530, 42, 3089, 11, 321, 645, 2614, 8853, 11, 8591, 11, 5707, 11, 300, 390, 3879, 14115, 13, 400], "temperature": 0.0, "avg_logprob": -0.098528828731803, "compression_ratio": 1.4896265560165975, "no_speech_prob": 0.0002710698754526675}, {"id": 142, "seek": 108504, "start": 1093.84, "end": 1100.3999999999999, "text": " the traditional way of running tests on browsers doesn't really work. You cannot run everything", "tokens": [264, 5164, 636, 295, 2614, 6921, 322, 36069, 1177, 380, 534, 589, 13, 509, 2644, 1190, 1203], "temperature": 0.0, "avg_logprob": -0.098528828731803, "compression_ratio": 1.4896265560165975, "no_speech_prob": 0.0002710698754526675}, {"id": 143, "seek": 108504, "start": 1100.3999999999999, "end": 1108.08, "text": " on all of the pushes. Otherwise, you will have a huge bill from the cloud provider.", "tokens": [322, 439, 295, 264, 21020, 13, 10328, 11, 291, 486, 362, 257, 2603, 2961, 490, 264, 4588, 12398, 13], "temperature": 0.0, "avg_logprob": -0.098528828731803, "compression_ratio": 1.4896265560165975, "no_speech_prob": 0.0002710698754526675}, {"id": 144, "seek": 108504, "start": 1108.08, "end": 1113.3999999999999, "text": " So we couldn't use coverage because of some technical reasons. We thought, what if we", "tokens": [407, 321, 2809, 380, 764, 9645, 570, 295, 512, 6191, 4112, 13, 492, 1194, 11, 437, 498, 321], "temperature": 0.0, "avg_logprob": -0.098528828731803, "compression_ratio": 1.4896265560165975, "no_speech_prob": 0.0002710698754526675}, {"id": 145, "seek": 111340, "start": 1113.4, "end": 1124.44, "text": " use machine learning? What if we extend bug, bug to also learn patches and tests? So the", "tokens": [764, 3479, 2539, 30, 708, 498, 321, 10101, 7426, 11, 7426, 281, 611, 1466, 26531, 293, 6921, 30, 407, 264], "temperature": 0.0, "avg_logprob": -0.16422774975116436, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0006070872768759727}, {"id": 146, "seek": 111340, "start": 1124.44, "end": 1134.5600000000002, "text": " first part was to use machines to try to parse this information and try to understand what", "tokens": [700, 644, 390, 281, 764, 8379, 281, 853, 281, 48377, 341, 1589, 293, 853, 281, 1223, 437], "temperature": 0.0, "avg_logprob": -0.16422774975116436, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0006070872768759727}, {"id": 147, "seek": 111340, "start": 1134.5600000000002, "end": 1141.0400000000002, "text": " exactly failed. It might seem like an easy task if you have 100 tests or 10 tests, but", "tokens": [2293, 7612, 13, 467, 1062, 1643, 411, 364, 1858, 5633, 498, 291, 362, 2319, 6921, 420, 1266, 6921, 11, 457], "temperature": 0.0, "avg_logprob": -0.16422774975116436, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0006070872768759727}, {"id": 148, "seek": 114104, "start": 1141.04, "end": 1147.8, "text": " when you have two billion tests, you have lots of intermittently failing tests. These", "tokens": [562, 291, 362, 732, 5218, 6921, 11, 291, 362, 3195, 295, 38548, 2276, 18223, 6921, 13, 1981], "temperature": 0.0, "avg_logprob": -0.13322495305260948, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0006327982991933823}, {"id": 149, "seek": 114104, "start": 1147.8, "end": 1155.84, "text": " tests fail randomly. They are not always the same. Every week, we see 150 new intermittent", "tokens": [6921, 3061, 16979, 13, 814, 366, 406, 1009, 264, 912, 13, 2048, 1243, 11, 321, 536, 8451, 777, 44084], "temperature": 0.0, "avg_logprob": -0.13322495305260948, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0006327982991933823}, {"id": 150, "seek": 114104, "start": 1155.84, "end": 1163.36, "text": " tests coming in. It's impossible to, it's not easy to automatically say if a failure", "tokens": [6921, 1348, 294, 13, 467, 311, 6243, 281, 11, 309, 311, 406, 1858, 281, 6772, 584, 498, 257, 7763], "temperature": 0.0, "avg_logprob": -0.13322495305260948, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0006327982991933823}, {"id": 151, "seek": 114104, "start": 1163.36, "end": 1170.92, "text": " is actually a failure or if it is an intermittent. Not even developers are able to do that sometimes.", "tokens": [307, 767, 257, 7763, 420, 498, 309, 307, 364, 44084, 13, 1726, 754, 8849, 366, 1075, 281, 360, 300, 2171, 13], "temperature": 0.0, "avg_logprob": -0.13322495305260948, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0006327982991933823}, {"id": 152, "seek": 117092, "start": 1170.92, "end": 1177.5600000000002, "text": " So not all of the tests are run on all of the pushes. So if I push my patch and a test", "tokens": [407, 406, 439, 295, 264, 6921, 366, 1190, 322, 439, 295, 264, 21020, 13, 407, 498, 286, 2944, 452, 9972, 293, 257, 1500], "temperature": 0.0, "avg_logprob": -0.13807606387448002, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0022141621448099613}, {"id": 153, "seek": 117092, "start": 1177.5600000000002, "end": 1186.2, "text": " doesn't run, but runs later on another patch and it fails, I don't know if it was my fault", "tokens": [1177, 380, 1190, 11, 457, 6676, 1780, 322, 1071, 9972, 293, 309, 18199, 11, 286, 500, 380, 458, 498, 309, 390, 452, 7441], "temperature": 0.0, "avg_logprob": -0.13807606387448002, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0022141621448099613}, {"id": 154, "seek": 117092, "start": 1186.2, "end": 1197.0800000000002, "text": " or somebody else's fault. And so we have sheriffs, people that are only focused, whose only focus,", "tokens": [420, 2618, 1646, 311, 7441, 13, 400, 370, 321, 362, 29855, 17643, 11, 561, 300, 366, 787, 5178, 11, 6104, 787, 1879, 11], "temperature": 0.0, "avg_logprob": -0.13807606387448002, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0022141621448099613}, {"id": 155, "seek": 119708, "start": 1197.08, "end": 1203.6, "text": " whose main focus is watching the CI, and they are pretty experienced at doing that, probably", "tokens": [6104, 2135, 1879, 307, 1976, 264, 37777, 11, 293, 436, 366, 1238, 6751, 412, 884, 300, 11, 1391], "temperature": 0.0, "avg_logprob": -0.14749258214777167, "compression_ratio": 1.453125, "no_speech_prob": 0.0005029141320846975}, {"id": 156, "seek": 119708, "start": 1203.6, "end": 1211.76, "text": " better than most developers. But human errors still exist. Even if we have their annotations,", "tokens": [1101, 813, 881, 8849, 13, 583, 1952, 13603, 920, 2514, 13, 2754, 498, 321, 362, 641, 25339, 763, 11], "temperature": 0.0, "avg_logprob": -0.14749258214777167, "compression_ratio": 1.453125, "no_speech_prob": 0.0005029141320846975}, {"id": 157, "seek": 119708, "start": 1211.76, "end": 1220.72, "text": " it's pretty hard to be sure about the results. You can see a meme that some sheriff created.", "tokens": [309, 311, 1238, 1152, 281, 312, 988, 466, 264, 3542, 13, 509, 393, 536, 257, 21701, 300, 512, 37103, 2942, 13], "temperature": 0.0, "avg_logprob": -0.14749258214777167, "compression_ratio": 1.453125, "no_speech_prob": 0.0005029141320846975}, {"id": 158, "seek": 122072, "start": 1220.72, "end": 1229.2, "text": " Lucky tests are the infamous intermittently failing tests. So the first step, the second", "tokens": [26639, 6921, 366, 264, 30769, 38548, 2276, 18223, 6921, 13, 407, 264, 700, 1823, 11, 264, 1150], "temperature": 0.0, "avg_logprob": -0.18156081835428875, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0006734981434419751}, {"id": 159, "seek": 122072, "start": 1229.2, "end": 1236.3600000000001, "text": " step after we implemented some heuristics to try to understand the failures due to a", "tokens": [1823, 934, 321, 12270, 512, 415, 374, 6006, 281, 853, 281, 1223, 264, 20774, 3462, 281, 257], "temperature": 0.0, "avg_logprob": -0.18156081835428875, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0006734981434419751}, {"id": 160, "seek": 122072, "start": 1236.3600000000001, "end": 1246.08, "text": " given patch was to analyze patches. We didn't have readily available tools, at least not", "tokens": [2212, 9972, 390, 281, 12477, 26531, 13, 492, 994, 380, 362, 26336, 2435, 3873, 11, 412, 1935, 406], "temperature": 0.0, "avg_logprob": -0.18156081835428875, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0006734981434419751}, {"id": 161, "seek": 124608, "start": 1246.08, "end": 1253.28, "text": " fast enough for the amount of data that we are talking about. We just used Mercurial", "tokens": [2370, 1547, 337, 264, 2372, 295, 1412, 300, 321, 366, 1417, 466, 13, 492, 445, 1143, 18897, 374, 831], "temperature": 0.0, "avg_logprob": -0.14867406541650946, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0013514163438230753}, {"id": 162, "seek": 124608, "start": 1253.28, "end": 1261.8, "text": " for authorship info. So who's the author of the push? Who's the reviewer? When was it", "tokens": [337, 6979, 14752, 13614, 13, 407, 567, 311, 264, 3793, 295, 264, 2944, 30, 2102, 311, 264, 3131, 260, 30, 1133, 390, 309], "temperature": 0.0, "avg_logprob": -0.14867406541650946, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0013514163438230753}, {"id": 163, "seek": 124608, "start": 1261.8, "end": 1268.6799999999998, "text": " pushed? Et cetera, et cetera. And we created a couple of projects written in Rust to parse", "tokens": [9152, 30, 3790, 11458, 11, 1030, 11458, 13, 400, 321, 2942, 257, 1916, 295, 4455, 3720, 294, 34952, 281, 48377], "temperature": 0.0, "avg_logprob": -0.14867406541650946, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0013514163438230753}, {"id": 164, "seek": 124608, "start": 1268.6799999999998, "end": 1274.1999999999998, "text": " patches efficiently and to analyze source code. The second one was actually a research partnership", "tokens": [26531, 19621, 293, 281, 12477, 4009, 3089, 13, 440, 1150, 472, 390, 767, 257, 2132, 9982], "temperature": 0.0, "avg_logprob": -0.14867406541650946, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0013514163438230753}, {"id": 165, "seek": 127420, "start": 1274.2, "end": 1287.52, "text": " with the Politecnico di Torino. And the machine learning model itself, it's not a multi-label", "tokens": [365, 264, 3635, 642, 16279, 2789, 1026, 7160, 2982, 13, 400, 264, 3479, 2539, 2316, 2564, 11, 309, 311, 406, 257, 4825, 12, 75, 18657], "temperature": 0.0, "avg_logprob": -0.15278996673284792, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.00072810600977391}, {"id": 166, "seek": 127420, "start": 1287.52, "end": 1297.0800000000002, "text": " model as one might think, where each test is a label. It would be too large with the", "tokens": [2316, 382, 472, 1062, 519, 11, 689, 1184, 1500, 307, 257, 7645, 13, 467, 576, 312, 886, 2416, 365, 264], "temperature": 0.0, "avg_logprob": -0.15278996673284792, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.00072810600977391}, {"id": 167, "seek": 129708, "start": 1297.08, "end": 1304.04, "text": " number of tests that we have. The model is simplified. The input is the table, test,", "tokens": [1230, 295, 6921, 300, 321, 362, 13, 440, 2316, 307, 26335, 13, 440, 4846, 307, 264, 3199, 11, 1500, 11], "temperature": 0.0, "avg_logprob": -0.16700003941853842, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.00033298388007096946}, {"id": 168, "seek": 129708, "start": 1304.04, "end": 1312.1599999999999, "text": " and patch. And the label is just fail, not fail. So the features actually come from both", "tokens": [293, 9972, 13, 400, 264, 7645, 307, 445, 3061, 11, 406, 3061, 13, 407, 264, 4122, 767, 808, 490, 1293], "temperature": 0.0, "avg_logprob": -0.16700003941853842, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.00033298388007096946}, {"id": 169, "seek": 129708, "start": 1312.1599999999999, "end": 1317.6799999999998, "text": " the test, the patch, and the link between the test and the patch. So, for example, the", "tokens": [264, 1500, 11, 264, 9972, 11, 293, 264, 2113, 1296, 264, 1500, 293, 264, 9972, 13, 407, 11, 337, 1365, 11, 264], "temperature": 0.0, "avg_logprob": -0.16700003941853842, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.00033298388007096946}, {"id": 170, "seek": 129708, "start": 1317.6799999999998, "end": 1324.1999999999998, "text": " past failures, when the same files were touched, the distance from the source files to the", "tokens": [1791, 20774, 11, 562, 264, 912, 7098, 645, 9828, 11, 264, 4560, 490, 264, 4009, 7098, 281, 264], "temperature": 0.0, "avg_logprob": -0.16700003941853842, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.00033298388007096946}, {"id": 171, "seek": 132420, "start": 1324.2, "end": 1331.04, "text": " test files in the tree. How often source files were modified together with test files? Of", "tokens": [1500, 7098, 294, 264, 4230, 13, 1012, 2049, 4009, 7098, 645, 15873, 1214, 365, 1500, 7098, 30, 2720], "temperature": 0.0, "avg_logprob": -0.13711804610032302, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005282398196868598}, {"id": 172, "seek": 132420, "start": 1331.04, "end": 1337.04, "text": " course, if they're modified together, probably they are somehow linked. Maybe you need to", "tokens": [1164, 11, 498, 436, 434, 15873, 1214, 11, 1391, 436, 366, 6063, 9408, 13, 2704, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.13711804610032302, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005282398196868598}, {"id": 173, "seek": 132420, "start": 1337.04, "end": 1343.44, "text": " fix the test. And so when you push your patch, you also fix the test. This is a clear link.", "tokens": [3191, 264, 1500, 13, 400, 370, 562, 291, 2944, 428, 9972, 11, 291, 611, 3191, 264, 1500, 13, 639, 307, 257, 1850, 2113, 13], "temperature": 0.0, "avg_logprob": -0.13711804610032302, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005282398196868598}, {"id": 174, "seek": 132420, "start": 1343.44, "end": 1351.76, "text": " But even then, we have lots of test redundancies. So we used frequent item set mining to try", "tokens": [583, 754, 550, 11, 321, 362, 3195, 295, 1500, 27830, 32286, 13, 407, 321, 1143, 18004, 3174, 992, 15512, 281, 853], "temperature": 0.0, "avg_logprob": -0.13711804610032302, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0005282398196868598}, {"id": 175, "seek": 135176, "start": 1351.76, "end": 1359.8799999999999, "text": " to understand which tests are redundant and remove them from the set of tests that are", "tokens": [281, 1223, 597, 6921, 366, 40997, 293, 4159, 552, 490, 264, 992, 295, 6921, 300, 366], "temperature": 0.0, "avg_logprob": -0.09034537499950777, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0005597111885435879}, {"id": 176, "seek": 135176, "start": 1359.8799999999999, "end": 1372.36, "text": " selected to run. And this was pretty successful as well. So now we had architecture to train", "tokens": [8209, 281, 1190, 13, 400, 341, 390, 1238, 4406, 382, 731, 13, 407, 586, 321, 632, 9482, 281, 3847], "temperature": 0.0, "avg_logprob": -0.09034537499950777, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0005597111885435879}, {"id": 177, "seek": 135176, "start": 1372.36, "end": 1381.0, "text": " models on bugs, to train models on patches and tests. The next step was to reuse what", "tokens": [5245, 322, 15120, 11, 281, 3847, 5245, 322, 26531, 293, 6921, 13, 440, 958, 1823, 390, 281, 26225, 437], "temperature": 0.0, "avg_logprob": -0.09034537499950777, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0005597111885435879}, {"id": 178, "seek": 138100, "start": 1381.0, "end": 1390.52, "text": " we built for patches to also try to predict defects. This is actually still an experimental", "tokens": [321, 3094, 337, 26531, 281, 611, 853, 281, 6069, 32655, 13, 639, 307, 767, 920, 364, 17069], "temperature": 0.0, "avg_logprob": -0.11941626622126653, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.0010159447556361556}, {"id": 179, "seek": 138100, "start": 1390.52, "end": 1396.32, "text": " phase. It's kind of a research project. So if anybody is interested in collaborating", "tokens": [5574, 13, 467, 311, 733, 295, 257, 2132, 1716, 13, 407, 498, 4472, 307, 3102, 294, 30188], "temperature": 0.0, "avg_logprob": -0.11941626622126653, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.0010159447556361556}, {"id": 180, "seek": 138100, "start": 1396.32, "end": 1405.24, "text": " with us on this topic, we will be happy to do so. I will just show you a few things that", "tokens": [365, 505, 322, 341, 4829, 11, 321, 486, 312, 2055, 281, 360, 370, 13, 286, 486, 445, 855, 291, 257, 1326, 721, 300], "temperature": 0.0, "avg_logprob": -0.11941626622126653, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.0010159447556361556}, {"id": 181, "seek": 140524, "start": 1405.24, "end": 1412.92, "text": " we have done in the space for now. So the goals are to reduce the regressions by detecting", "tokens": [321, 362, 1096, 294, 264, 1901, 337, 586, 13, 407, 264, 5493, 366, 281, 5407, 264, 1121, 735, 626, 538, 40237], "temperature": 0.0, "avg_logprob": -0.134267951166907, "compression_ratio": 1.6531531531531531, "no_speech_prob": 0.0010476824827492237}, {"id": 182, "seek": 140524, "start": 1412.92, "end": 1419.1200000000001, "text": " the patches that reviewers should focus on more than others, to reduce the time spent", "tokens": [264, 26531, 300, 45837, 820, 1879, 322, 544, 813, 2357, 11, 281, 5407, 264, 565, 4418], "temperature": 0.0, "avg_logprob": -0.134267951166907, "compression_ratio": 1.6531531531531531, "no_speech_prob": 0.0010476824827492237}, {"id": 183, "seek": 140524, "start": 1419.1200000000001, "end": 1426.68, "text": " by reviewers on less risky patches, and to when we detect that the patch is risky, trigger", "tokens": [538, 45837, 322, 1570, 21137, 26531, 11, 293, 281, 562, 321, 5531, 300, 264, 9972, 307, 21137, 11, 7875], "temperature": 0.0, "avg_logprob": -0.134267951166907, "compression_ratio": 1.6531531531531531, "no_speech_prob": 0.0010476824827492237}, {"id": 184, "seek": 140524, "start": 1426.68, "end": 1434.0, "text": " some risk control operations. For example, I don't know, running phasing tests more comprehensively", "tokens": [512, 3148, 1969, 7705, 13, 1171, 1365, 11, 286, 500, 380, 458, 11, 2614, 903, 3349, 6921, 544, 13914, 356], "temperature": 0.0, "avg_logprob": -0.134267951166907, "compression_ratio": 1.6531531531531531, "no_speech_prob": 0.0010476824827492237}, {"id": 185, "seek": 143400, "start": 1434.0, "end": 1439.52, "text": " in these patches and things like this. Of course, the model is just an evaluation of", "tokens": [294, 613, 26531, 293, 721, 411, 341, 13, 2720, 1164, 11, 264, 2316, 307, 445, 364, 13344, 295], "temperature": 0.0, "avg_logprob": -0.09376561452472021, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.0002228201919933781}, {"id": 186, "seek": 143400, "start": 1439.52, "end": 1446.6, "text": " the risk. It's not actually going to tell us if there is a bug or not. And it will never", "tokens": [264, 3148, 13, 467, 311, 406, 767, 516, 281, 980, 505, 498, 456, 307, 257, 7426, 420, 406, 13, 400, 309, 486, 1128], "temperature": 0.0, "avg_logprob": -0.09376561452472021, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.0002228201919933781}, {"id": 187, "seek": 143400, "start": 1446.6, "end": 1457.08, "text": " replace a real reviewer who can actually review the patch more precisely.", "tokens": [7406, 257, 957, 3131, 260, 567, 393, 767, 3131, 264, 9972, 544, 13402, 13], "temperature": 0.0, "avg_logprob": -0.09376561452472021, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.0002228201919933781}, {"id": 188, "seek": 145708, "start": 1457.08, "end": 1464.52, "text": " The first step was, again, build a data set. It is not easy to know which patches cause", "tokens": [440, 700, 1823, 390, 11, 797, 11, 1322, 257, 1412, 992, 13, 467, 307, 406, 1858, 281, 458, 597, 26531, 3082], "temperature": 0.0, "avg_logprob": -0.14110020668275894, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000331841321894899}, {"id": 189, "seek": 145708, "start": 1464.52, "end": 1469.9199999999998, "text": " regressions. It's actually impossible at this time. There are some algorithms that are used", "tokens": [1121, 735, 626, 13, 467, 311, 767, 6243, 412, 341, 565, 13, 821, 366, 512, 14642, 300, 366, 1143], "temperature": 0.0, "avg_logprob": -0.14110020668275894, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000331841321894899}, {"id": 190, "seek": 145708, "start": 1469.9199999999998, "end": 1478.32, "text": " in research. The most famous one is SZZ. But we had some answers that it was not so good.", "tokens": [294, 2132, 13, 440, 881, 4618, 472, 307, 318, 57, 57, 13, 583, 321, 632, 512, 6338, 300, 309, 390, 406, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.14110020668275894, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000331841321894899}, {"id": 191, "seek": 145708, "start": 1478.32, "end": 1484.6399999999999, "text": " So we started here, again, introducing a change in the process that we have. We introduced", "tokens": [407, 321, 1409, 510, 11, 797, 11, 15424, 257, 1319, 294, 264, 1399, 300, 321, 362, 13, 492, 7268], "temperature": 0.0, "avg_logprob": -0.14110020668275894, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000331841321894899}, {"id": 192, "seek": 148464, "start": 1484.64, "end": 1494.5200000000002, "text": " a new field, which is called regressed by, so that developers, QA users, can specify", "tokens": [257, 777, 2519, 11, 597, 307, 1219, 1121, 3805, 538, 11, 370, 300, 8849, 11, 1249, 32, 5022, 11, 393, 16500], "temperature": 0.0, "avg_logprob": -0.1250613530476888, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0015211172867566347}, {"id": 193, "seek": 148464, "start": 1494.5200000000002, "end": 1500.76, "text": " what caused a given regression. So when they file a bug, if they know what caused it, they", "tokens": [437, 7008, 257, 2212, 24590, 13, 407, 562, 436, 3991, 257, 7426, 11, 498, 436, 458, 437, 7008, 309, 11, 436], "temperature": 0.0, "avg_logprob": -0.1250613530476888, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0015211172867566347}, {"id": 194, "seek": 148464, "start": 1500.76, "end": 1506.8400000000001, "text": " can specify it here. If they don't know what caused it, we have a few tools that we built", "tokens": [393, 16500, 309, 510, 13, 759, 436, 500, 380, 458, 437, 7008, 309, 11, 321, 362, 257, 1326, 3873, 300, 321, 3094], "temperature": 0.0, "avg_logprob": -0.1250613530476888, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0015211172867566347}, {"id": 195, "seek": 150684, "start": 1506.84, "end": 1516.3999999999999, "text": " over time to automatically download builds from RCI that we showed earlier. Automatically", "tokens": [670, 565, 281, 6772, 5484, 15182, 490, 497, 25240, 300, 321, 4712, 3071, 13, 24619, 5030], "temperature": 0.0, "avg_logprob": -0.15016821960904705, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.000616748642642051}, {"id": 196, "seek": 150684, "start": 1516.3999999999999, "end": 1523.32, "text": " download builds from the past and run a bisection to try to find what the cause is for the given", "tokens": [5484, 15182, 490, 264, 1791, 293, 1190, 257, 7393, 10183, 281, 853, 281, 915, 437, 264, 3082, 307, 337, 264, 2212], "temperature": 0.0, "avg_logprob": -0.15016821960904705, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.000616748642642051}, {"id": 197, "seek": 150684, "start": 1523.32, "end": 1531.36, "text": " bug. With this, we managed to build a pretty large data set, 5,000 links between bug introducing", "tokens": [7426, 13, 2022, 341, 11, 321, 6453, 281, 1322, 257, 1238, 2416, 1412, 992, 11, 1025, 11, 1360, 6123, 1296, 7426, 15424], "temperature": 0.0, "avg_logprob": -0.15016821960904705, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.000616748642642051}, {"id": 198, "seek": 153136, "start": 1531.36, "end": 1543.3999999999999, "text": " and bug fixing commits. Actually, commit sets. And then this amounts to 24,000 commits. And", "tokens": [293, 7426, 19442, 48311, 13, 5135, 11, 5599, 6352, 13, 400, 550, 341, 11663, 281, 4022, 11, 1360, 48311, 13, 400], "temperature": 0.0, "avg_logprob": -0.1419216604793773, "compression_ratio": 1.5397727272727273, "no_speech_prob": 0.0002836111525539309}, {"id": 199, "seek": 153136, "start": 1543.3999999999999, "end": 1548.6, "text": " then we were able, with this data set, to evaluate the current algorithms that are presented", "tokens": [550, 321, 645, 1075, 11, 365, 341, 1412, 992, 11, 281, 13059, 264, 2190, 14642, 300, 366, 8212], "temperature": 0.0, "avg_logprob": -0.1419216604793773, "compression_ratio": 1.5397727272727273, "no_speech_prob": 0.0002836111525539309}, {"id": 200, "seek": 153136, "start": 1548.6, "end": 1554.8799999999999, "text": " in the literature. And as we thought, they are not working well at all. So this is one", "tokens": [294, 264, 10394, 13, 400, 382, 321, 1194, 11, 436, 366, 406, 1364, 731, 412, 439, 13, 407, 341, 307, 472], "temperature": 0.0, "avg_logprob": -0.1419216604793773, "compression_ratio": 1.5397727272727273, "no_speech_prob": 0.0002836111525539309}, {"id": 201, "seek": 155488, "start": 1554.88, "end": 1566.24, "text": " of the areas of improvement for research. One of the improvements that we tried to apply", "tokens": [295, 264, 3179, 295, 10444, 337, 2132, 13, 1485, 295, 264, 13797, 300, 321, 3031, 281, 3079], "temperature": 0.0, "avg_logprob": -0.18261311848958334, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.0010938560590147972}, {"id": 202, "seek": 155488, "start": 1566.24, "end": 1575.92, "text": " and to SZZ was to improve the blame algorithm. If you're more familiar with Mercurial annotate", "tokens": [293, 281, 318, 30108, 390, 281, 3470, 264, 10127, 9284, 13, 759, 291, 434, 544, 4963, 365, 18897, 374, 831, 25339, 473], "temperature": 0.0, "avg_logprob": -0.18261311848958334, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.0010938560590147972}, {"id": 203, "seek": 157592, "start": 1575.92, "end": 1586.3600000000001, "text": " algorithm, to try to, instead of looking at lines, splitting changes by words and tokens,", "tokens": [9284, 11, 281, 853, 281, 11, 2602, 295, 1237, 412, 3876, 11, 30348, 2962, 538, 2283, 293, 22667, 11], "temperature": 0.0, "avg_logprob": -0.10155947025005634, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.0006250307196751237}, {"id": 204, "seek": 157592, "start": 1586.3600000000001, "end": 1593.0800000000002, "text": " so you can see changes, past changes by token instead of by line. This is a visualization", "tokens": [370, 291, 393, 536, 2962, 11, 1791, 2962, 538, 14862, 2602, 295, 538, 1622, 13, 639, 307, 257, 25801], "temperature": 0.0, "avg_logprob": -0.10155947025005634, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.0006250307196751237}, {"id": 205, "seek": 157592, "start": 1593.0800000000002, "end": 1599.64, "text": " from the Linux kernel. This is going to give you a much more precise view of what changed", "tokens": [490, 264, 18734, 28256, 13, 639, 307, 516, 281, 976, 291, 257, 709, 544, 13600, 1910, 295, 437, 3105], "temperature": 0.0, "avg_logprob": -0.10155947025005634, "compression_ratio": 1.5823529411764705, "no_speech_prob": 0.0006250307196751237}, {"id": 206, "seek": 159964, "start": 1599.64, "end": 1607.96, "text": " in the past. For example, it will skip over tab only changes, white space only changes", "tokens": [294, 264, 1791, 13, 1171, 1365, 11, 309, 486, 10023, 670, 4421, 787, 2962, 11, 2418, 1901, 787, 2962], "temperature": 0.0, "avg_logprob": -0.14251296464786972, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0012812375789508224}, {"id": 207, "seek": 159964, "start": 1607.96, "end": 1615.3200000000002, "text": " and things like that. If you add an if, your code will be intended more, but you're not", "tokens": [293, 721, 411, 300, 13, 759, 291, 909, 364, 498, 11, 428, 3089, 486, 312, 10226, 544, 11, 457, 291, 434, 406], "temperature": 0.0, "avg_logprob": -0.14251296464786972, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0012812375789508224}, {"id": 208, "seek": 159964, "start": 1615.3200000000002, "end": 1621.64, "text": " actually changing everything inside. You're changing only the if. This actually improved", "tokens": [767, 4473, 1203, 1854, 13, 509, 434, 4473, 787, 264, 498, 13, 639, 767, 9689], "temperature": 0.0, "avg_logprob": -0.14251296464786972, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0012812375789508224}, {"id": 209, "seek": 159964, "start": 1621.64, "end": 1628.64, "text": " the results, but it was not enough to get to an acceptable level of accuracy. But it's", "tokens": [264, 3542, 11, 457, 309, 390, 406, 1547, 281, 483, 281, 364, 15513, 1496, 295, 14170, 13, 583, 309, 311], "temperature": 0.0, "avg_logprob": -0.14251296464786972, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0012812375789508224}, {"id": 210, "seek": 162864, "start": 1628.64, "end": 1635.2800000000002, "text": " nice and we can actually use it in the IDE. We're not doing it yet, but we will to give", "tokens": [1481, 293, 321, 393, 767, 764, 309, 294, 264, 40930, 13, 492, 434, 406, 884, 309, 1939, 11, 457, 321, 486, 281, 976], "temperature": 0.0, "avg_logprob": -0.17801656615868044, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.0003300073731224984}, {"id": 211, "seek": 162864, "start": 1635.2800000000002, "end": 1644.16, "text": " more information to users because developers use annotate and get blame a lot. And this", "tokens": [544, 1589, 281, 5022, 570, 8849, 764, 25339, 473, 293, 483, 10127, 257, 688, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.17801656615868044, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.0003300073731224984}, {"id": 212, "seek": 162864, "start": 1644.16, "end": 1651.0800000000002, "text": " is a UI that is work in progress for analyzing the risk of a patch. This is a screenshot", "tokens": [307, 257, 15682, 300, 307, 589, 294, 4205, 337, 23663, 264, 3148, 295, 257, 9972, 13, 639, 307, 257, 27712], "temperature": 0.0, "avg_logprob": -0.17801656615868044, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.0003300073731224984}, {"id": 213, "seek": 162864, "start": 1651.0800000000002, "end": 1658.0, "text": " from our code review tool. So we are showing the result of the algorithm with the confidence.", "tokens": [490, 527, 3089, 3131, 2290, 13, 407, 321, 366, 4099, 264, 1874, 295, 264, 9284, 365, 264, 6687, 13], "temperature": 0.0, "avg_logprob": -0.17801656615868044, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.0003300073731224984}, {"id": 214, "seek": 165800, "start": 1658.0, "end": 1664.92, "text": " So in this case, it was a risky patch with 79% confidence. And we give a few explanations", "tokens": [407, 294, 341, 1389, 11, 309, 390, 257, 21137, 9972, 365, 32803, 4, 6687, 13, 400, 321, 976, 257, 1326, 28708], "temperature": 0.0, "avg_logprob": -0.13598921942332434, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.000732103013433516}, {"id": 215, "seek": 165800, "start": 1664.92, "end": 1670.64, "text": " to the developers. This is one of the most important things. Developers do not always", "tokens": [281, 264, 8849, 13, 639, 307, 472, 295, 264, 881, 1021, 721, 13, 11442, 433, 360, 406, 1009], "temperature": 0.0, "avg_logprob": -0.13598921942332434, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.000732103013433516}, {"id": 216, "seek": 165800, "start": 1670.64, "end": 1677.56, "text": " trust developers like any other user. Do not always trust results from machine learning.", "tokens": [3361, 8849, 411, 604, 661, 4195, 13, 1144, 406, 1009, 3361, 3542, 490, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.13598921942332434, "compression_ratio": 1.5085714285714287, "no_speech_prob": 0.000732103013433516}, {"id": 217, "seek": 167756, "start": 1677.56, "end": 1688.1599999999999, "text": " And so you need to give them an explanation. And this is another part of the output of", "tokens": [400, 370, 291, 643, 281, 976, 552, 364, 10835, 13, 400, 341, 307, 1071, 644, 295, 264, 5598, 295], "temperature": 0.0, "avg_logprob": -0.13032046231356534, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00021671390277333558}, {"id": 218, "seek": 167756, "start": 1688.1599999999999, "end": 1695.1599999999999, "text": " our tool. This is again on our code review tool. We're showing on the functions that", "tokens": [527, 2290, 13, 639, 307, 797, 322, 527, 3089, 3131, 2290, 13, 492, 434, 4099, 322, 264, 6828, 300], "temperature": 0.0, "avg_logprob": -0.13032046231356534, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00021671390277333558}, {"id": 219, "seek": 167756, "start": 1695.1599999999999, "end": 1703.12, "text": " are being changed by the patch if the function is risky or not. And which bugs in the past", "tokens": [366, 885, 3105, 538, 264, 9972, 498, 264, 2445, 307, 21137, 420, 406, 13, 400, 597, 15120, 294, 264, 1791], "temperature": 0.0, "avg_logprob": -0.13032046231356534, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00021671390277333558}, {"id": 220, "seek": 170312, "start": 1703.12, "end": 1710.9199999999998, "text": " were involved in this function. So developers can try to see if the patch is reintroducing", "tokens": [645, 3288, 294, 341, 2445, 13, 407, 8849, 393, 853, 281, 536, 498, 264, 9972, 307, 319, 38132, 2175], "temperature": 0.0, "avg_logprob": -0.13345787354878016, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0004851014818996191}, {"id": 221, "seek": 170312, "start": 1710.9199999999998, "end": 1718.9199999999998, "text": " a previously fixed bug. And they can also know what kind of side effects there are when", "tokens": [257, 8046, 6806, 7426, 13, 400, 436, 393, 611, 458, 437, 733, 295, 1252, 5065, 456, 366, 562], "temperature": 0.0, "avg_logprob": -0.13345787354878016, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0004851014818996191}, {"id": 222, "seek": 170312, "start": 1718.9199999999998, "end": 1725.84, "text": " you make changes to a given area of the code.", "tokens": [291, 652, 2962, 281, 257, 2212, 1859, 295, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13345787354878016, "compression_ratio": 1.4088050314465408, "no_speech_prob": 0.0004851014818996191}, {"id": 223, "seek": 172584, "start": 1725.84, "end": 1736.1999999999998, "text": " Now we did a lot of stuff for developers. We trained models for bugs. We trained models", "tokens": [823, 321, 630, 257, 688, 295, 1507, 337, 8849, 13, 492, 8895, 5245, 337, 15120, 13, 492, 8895, 5245], "temperature": 0.0, "avg_logprob": -0.1329866409301758, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007610537577420473}, {"id": 224, "seek": 172584, "start": 1736.1999999999998, "end": 1742.72, "text": " for patches. We trained models for tests. We trained models to predict the facts. Now", "tokens": [337, 26531, 13, 492, 8895, 5245, 337, 6921, 13, 492, 8895, 5245, 281, 6069, 264, 9130, 13, 823], "temperature": 0.0, "avg_logprob": -0.1329866409301758, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007610537577420473}, {"id": 225, "seek": 172584, "start": 1742.72, "end": 1750.6399999999999, "text": " I'm going to go to a slightly different topic even though it's connected. Privacy-friendly", "tokens": [286, 478, 516, 281, 352, 281, 257, 4748, 819, 4829, 754, 1673, 309, 311, 4582, 13, 39691, 2551, 12, 22864], "temperature": 0.0, "avg_logprob": -0.1329866409301758, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.0007610537577420473}, {"id": 226, "seek": 175064, "start": 1750.64, "end": 1760.5600000000002, "text": " translations. So we're working on introducing translations in Firefox. The subtitle was", "tokens": [37578, 13, 407, 321, 434, 1364, 322, 15424, 37578, 294, 46613, 13, 440, 30706, 306, 390], "temperature": 0.0, "avg_logprob": -0.17316998375786674, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00022361848095897585}, {"id": 227, "seek": 175064, "start": 1760.5600000000002, "end": 1770.2800000000002, "text": " actually translated automatically using Firefox translate, which you can use nowadays. The", "tokens": [767, 16805, 6772, 1228, 46613, 13799, 11, 597, 291, 393, 764, 13434, 13, 440], "temperature": 0.0, "avg_logprob": -0.17316998375786674, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00022361848095897585}, {"id": 228, "seek": 175064, "start": 1770.2800000000002, "end": 1776.6000000000001, "text": " idea is that translation models improved a lot in recent times. Current cloud-based", "tokens": [1558, 307, 300, 12853, 5245, 9689, 257, 688, 294, 5162, 1413, 13, 15629, 4588, 12, 6032], "temperature": 0.0, "avg_logprob": -0.17316998375786674, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.00022361848095897585}, {"id": 229, "seek": 177660, "start": 1776.6, "end": 1782.48, "text": " services do not offer the privacy guarantees that we like to offer in Firefox. They are", "tokens": [3328, 360, 406, 2626, 264, 11427, 32567, 300, 321, 411, 281, 2626, 294, 46613, 13, 814, 366], "temperature": 0.0, "avg_logprob": -0.1424101134877146, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0010892617283388972}, {"id": 230, "seek": 177660, "start": 1782.48, "end": 1789.8, "text": " closed source. They are not privacy-preserving. So we started a project. It was funded by", "tokens": [5395, 4009, 13, 814, 366, 406, 11427, 12, 14508, 20186, 13, 407, 321, 1409, 257, 1716, 13, 467, 390, 14385, 538], "temperature": 0.0, "avg_logprob": -0.1424101134877146, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0010892617283388972}, {"id": 231, "seek": 177660, "start": 1789.8, "end": 1796.9599999999998, "text": " the European Union to investigate client-side private translation capabilities in Firefox", "tokens": [264, 6473, 8133, 281, 15013, 6423, 12, 1812, 4551, 12853, 10862, 294, 46613], "temperature": 0.0, "avg_logprob": -0.1424101134877146, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0010892617283388972}, {"id": 232, "seek": 177660, "start": 1796.9599999999998, "end": 1803.9599999999998, "text": " itself. It is currently available as an add-on that you can install in Firefox. We support", "tokens": [2564, 13, 467, 307, 4362, 2435, 382, 364, 909, 12, 266, 300, 291, 393, 3625, 294, 46613, 13, 492, 1406], "temperature": 0.0, "avg_logprob": -0.1424101134877146, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0010892617283388972}, {"id": 233, "seek": 180396, "start": 1803.96, "end": 1809.8400000000001, "text": " many European languages and we're working on supporting even more. We're going to also", "tokens": [867, 6473, 8650, 293, 321, 434, 1364, 322, 7231, 754, 544, 13, 492, 434, 516, 281, 611], "temperature": 0.0, "avg_logprob": -0.1409295399983724, "compression_ratio": 1.5582822085889572, "no_speech_prob": 0.0009054499678313732}, {"id": 234, "seek": 180396, "start": 1809.8400000000001, "end": 1820.68, "text": " work on supporting non-European languages like Chinese, Korean, Japanese, etc.", "tokens": [589, 322, 7231, 2107, 12, 32293, 282, 8650, 411, 4649, 11, 6933, 11, 5433, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.1409295399983724, "compression_ratio": 1.5582822085889572, "no_speech_prob": 0.0009054499678313732}, {"id": 235, "seek": 180396, "start": 1820.68, "end": 1826.56, "text": " And in this case, we use machine learning on the client side to perform the translation.", "tokens": [400, 294, 341, 1389, 11, 321, 764, 3479, 2539, 322, 264, 6423, 1252, 281, 2042, 264, 12853, 13], "temperature": 0.0, "avg_logprob": -0.1409295399983724, "compression_ratio": 1.5582822085889572, "no_speech_prob": 0.0009054499678313732}, {"id": 236, "seek": 182656, "start": 1826.56, "end": 1834.1599999999999, "text": " So your data never leaves your Firefox. The models are downloaded from our servers, but", "tokens": [407, 428, 1412, 1128, 5510, 428, 46613, 13, 440, 5245, 366, 21748, 490, 527, 15909, 11, 457], "temperature": 0.0, "avg_logprob": -0.1128441566644713, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007051703287288547}, {"id": 237, "seek": 182656, "start": 1834.1599999999999, "end": 1841.28, "text": " they run locally on your machine. So the contents of the web page that you're looking at will", "tokens": [436, 1190, 16143, 322, 428, 3479, 13, 407, 264, 15768, 295, 264, 3670, 3028, 300, 291, 434, 1237, 412, 486], "temperature": 0.0, "avg_logprob": -0.1128441566644713, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007051703287288547}, {"id": 238, "seek": 182656, "start": 1841.28, "end": 1847.3999999999999, "text": " never go to Google Bing or whatever. They will be translated locally on your machine.", "tokens": [1128, 352, 281, 3329, 30755, 420, 2035, 13, 814, 486, 312, 16805, 16143, 322, 428, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1128441566644713, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007051703287288547}, {"id": 239, "seek": 182656, "start": 1847.3999999999999, "end": 1854.8799999999999, "text": " We use a few open data sets. Luckily, we have lots of them from past research. Not all of", "tokens": [492, 764, 257, 1326, 1269, 1412, 6352, 13, 19726, 11, 321, 362, 3195, 295, 552, 490, 1791, 2132, 13, 1726, 439, 295], "temperature": 0.0, "avg_logprob": -0.1128441566644713, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007051703287288547}, {"id": 240, "seek": 185488, "start": 1854.88, "end": 1861.0, "text": " them have good quality, but many of them have. But we are looking for more. So if you have", "tokens": [552, 362, 665, 3125, 11, 457, 867, 295, 552, 362, 13, 583, 321, 366, 1237, 337, 544, 13, 407, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.1357916866440371, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.00024046387989073992}, {"id": 241, "seek": 185488, "start": 1861.0, "end": 1867.6000000000001, "text": " suggestions for data sets that we can use, please let us know.", "tokens": [13396, 337, 1412, 6352, 300, 321, 393, 764, 11, 1767, 718, 505, 458, 13], "temperature": 0.0, "avg_logprob": -0.1357916866440371, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.00024046387989073992}, {"id": 242, "seek": 185488, "start": 1867.6000000000001, "end": 1874.5600000000002, "text": " On the data sets, we perform some basic data cleaning. And we use machine learning-based", "tokens": [1282, 264, 1412, 6352, 11, 321, 2042, 512, 3875, 1412, 8924, 13, 400, 321, 764, 3479, 2539, 12, 6032], "temperature": 0.0, "avg_logprob": -0.1357916866440371, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.00024046387989073992}, {"id": 243, "seek": 185488, "start": 1874.5600000000002, "end": 1883.24, "text": " techniques to clean the data, to remove bad sentence pairs that we believe are bad. Of", "tokens": [7512, 281, 2541, 264, 1412, 11, 281, 4159, 1578, 8174, 15494, 300, 321, 1697, 366, 1578, 13, 2720], "temperature": 0.0, "avg_logprob": -0.1357916866440371, "compression_ratio": 1.6127450980392157, "no_speech_prob": 0.00024046387989073992}, {"id": 244, "seek": 188324, "start": 1883.24, "end": 1887.92, "text": " course, the data set that I showed before are open, but sometimes they are just crawled", "tokens": [1164, 11, 264, 1412, 992, 300, 286, 4712, 949, 366, 1269, 11, 457, 2171, 436, 366, 445, 13999, 1493], "temperature": 0.0, "avg_logprob": -0.15905050389906938, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0007113816100172698}, {"id": 245, "seek": 188324, "start": 1887.92, "end": 1898.68, "text": " from the web, so they contain all sorts of bad sentences. Also, HTML tags and stuff", "tokens": [490, 264, 3670, 11, 370, 436, 5304, 439, 7527, 295, 1578, 16579, 13, 2743, 11, 17995, 18632, 293, 1507], "temperature": 0.0, "avg_logprob": -0.15905050389906938, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0007113816100172698}, {"id": 246, "seek": 188324, "start": 1898.68, "end": 1904.04, "text": " like that, we need to clean them up. Otherwise, the translations will learn to translate HTML", "tokens": [411, 300, 11, 321, 643, 281, 2541, 552, 493, 13, 10328, 11, 264, 37578, 486, 1466, 281, 13799, 17995], "temperature": 0.0, "avg_logprob": -0.15905050389906938, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0007113816100172698}, {"id": 247, "seek": 188324, "start": 1904.04, "end": 1910.1200000000001, "text": " tags. And we use some techniques to increase the size of the data set automatically, like", "tokens": [18632, 13, 400, 321, 764, 512, 7512, 281, 3488, 264, 2744, 295, 264, 1412, 992, 6772, 11, 411], "temperature": 0.0, "avg_logprob": -0.15905050389906938, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0007113816100172698}, {"id": 248, "seek": 191012, "start": 1910.12, "end": 1916.12, "text": " back translations, translating sentences from one language to the other, and back translating", "tokens": [646, 37578, 11, 35030, 16579, 490, 472, 2856, 281, 264, 661, 11, 293, 646, 35030], "temperature": 0.0, "avg_logprob": -0.1258977907044547, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.0002862734254449606}, {"id": 249, "seek": 191012, "start": 1916.12, "end": 1924.28, "text": " it in order to increase the size of the data sets.", "tokens": [309, 294, 1668, 281, 3488, 264, 2744, 295, 264, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.1258977907044547, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.0002862734254449606}, {"id": 250, "seek": 191012, "start": 1924.28, "end": 1936.4799999999998, "text": " So we trained a large model on cloud machines, which is pretty large. You can see it's around", "tokens": [407, 321, 8895, 257, 2416, 2316, 322, 4588, 8379, 11, 597, 307, 1238, 2416, 13, 509, 393, 536, 309, 311, 926], "temperature": 0.0, "avg_logprob": -0.1258977907044547, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.0002862734254449606}, {"id": 251, "seek": 193648, "start": 1936.48, "end": 1942.4, "text": " 800 megabytes, so every language pair, you would need to download 800 megabytes, and", "tokens": [13083, 10816, 24538, 11, 370, 633, 2856, 6119, 11, 291, 576, 643, 281, 5484, 13083, 10816, 24538, 11, 293], "temperature": 0.0, "avg_logprob": -0.144552743434906, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.0003480480518192053}, {"id": 252, "seek": 193648, "start": 1942.4, "end": 1950.64, "text": " it is super slow, so we can only use that on cloud.", "tokens": [309, 307, 1687, 2964, 11, 370, 321, 393, 787, 764, 300, 322, 4588, 13], "temperature": 0.0, "avg_logprob": -0.144552743434906, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.0003480480518192053}, {"id": 253, "seek": 193648, "start": 1950.64, "end": 1955.48, "text": " So we use some techniques in order to reduce the size of these models and to make them", "tokens": [407, 321, 764, 512, 7512, 294, 1668, 281, 5407, 264, 2744, 295, 613, 5245, 293, 281, 652, 552], "temperature": 0.0, "avg_logprob": -0.144552743434906, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.0003480480518192053}, {"id": 254, "seek": 193648, "start": 1955.48, "end": 1965.3600000000001, "text": " faster. We use knowledge distillation, basically using the model, the large model that we trained", "tokens": [4663, 13, 492, 764, 3601, 42923, 399, 11, 1936, 1228, 264, 2316, 11, 264, 2416, 2316, 300, 321, 8895], "temperature": 0.0, "avg_logprob": -0.144552743434906, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.0003480480518192053}, {"id": 255, "seek": 196536, "start": 1965.36, "end": 1971.8, "text": " as a trainer for a student model, which is much smaller, so you can see that from 800", "tokens": [382, 257, 21110, 337, 257, 3107, 2316, 11, 597, 307, 709, 4356, 11, 370, 291, 393, 536, 300, 490, 13083], "temperature": 0.0, "avg_logprob": -0.19660550355911255, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00040681526297703385}, {"id": 256, "seek": 196536, "start": 1971.8, "end": 1976.8799999999999, "text": " megabytes we got to 16, I think now we're around 5, 6, something like that, so it's", "tokens": [10816, 24538, 321, 658, 281, 3165, 11, 286, 519, 586, 321, 434, 926, 1025, 11, 1386, 11, 746, 411, 300, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.19660550355911255, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00040681526297703385}, {"id": 257, "seek": 196536, "start": 1976.8799999999999, "end": 1981.9199999999998, "text": " much smaller and you can actually download it on demand from our servers. And we use", "tokens": [709, 4356, 293, 291, 393, 767, 5484, 309, 322, 4733, 490, 527, 15909, 13, 400, 321, 764], "temperature": 0.0, "avg_logprob": -0.19660550355911255, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00040681526297703385}, {"id": 258, "seek": 196536, "start": 1981.9199999999998, "end": 1988.08, "text": " quantization for further compression and perf improvements, so moving the data from the", "tokens": [4426, 2144, 337, 3052, 19355, 293, 13826, 13797, 11, 370, 2684, 264, 1412, 490, 264], "temperature": 0.0, "avg_logprob": -0.19660550355911255, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00040681526297703385}, {"id": 259, "seek": 196536, "start": 1988.08, "end": 1994.12, "text": " model from float 32 to int 8.", "tokens": [2316, 490, 15706, 8858, 281, 560, 1649, 13], "temperature": 0.0, "avg_logprob": -0.19660550355911255, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00040681526297703385}, {"id": 260, "seek": 199412, "start": 1994.12, "end": 2001.84, "text": " Then we compiled the machine translation engine to WebAssembly in order to be able to use it", "tokens": [1396, 321, 36548, 264, 3479, 12853, 2848, 281, 9573, 10884, 19160, 294, 1668, 281, 312, 1075, 281, 764, 309], "temperature": 0.0, "avg_logprob": -0.11566137900719275, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0011023940751329064}, {"id": 261, "seek": 199412, "start": 2001.84, "end": 2009.36, "text": " inside Firefox. We introduced some SIMD extensions into WebAssembly and into Firefox in order", "tokens": [1854, 46613, 13, 492, 7268, 512, 24738, 35, 25129, 666, 9573, 10884, 19160, 293, 666, 46613, 294, 1668], "temperature": 0.0, "avg_logprob": -0.11566137900719275, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0011023940751329064}, {"id": 262, "seek": 199412, "start": 2009.36, "end": 2016.9199999999998, "text": " to be able to be even faster when translating, even though we translate a bit at a time,", "tokens": [281, 312, 1075, 281, 312, 754, 4663, 562, 35030, 11, 754, 1673, 321, 13799, 257, 857, 412, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.11566137900719275, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0011023940751329064}, {"id": 263, "seek": 201692, "start": 2016.92, "end": 2027.64, "text": " so it's pretty fast. And the engines are downloaded and updated on demand.", "tokens": [370, 309, 311, 1238, 2370, 13, 400, 264, 12982, 366, 21748, 293, 10588, 322, 4733, 13], "temperature": 0.0, "avg_logprob": -0.20694639682769775, "compression_ratio": 1.0136986301369864, "no_speech_prob": 0.0008765389793552458}, {"id": 264, "seek": 202764, "start": 2027.64, "end": 2055.32, "text": " Let me show you a demo.", "tokens": [961, 385, 855, 291, 257, 10723, 13], "temperature": 0.0, "avg_logprob": -0.5351164124228738, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.007983616553246975}, {"id": 265, "seek": 205532, "start": 2055.32, "end": 2065.92, "text": " So, you can see my Firefox is in Italian, but you can see that it automatically detected", "tokens": [407, 11, 291, 393, 536, 452, 46613, 307, 294, 10003, 11, 457, 291, 393, 536, 300, 309, 6772, 21896], "temperature": 0.0, "avg_logprob": -0.15180089738633898, "compression_ratio": 1.4031007751937985, "no_speech_prob": 0.00669720908626914}, {"id": 266, "seek": 205532, "start": 2065.92, "end": 2073.56, "text": " that the page is in French and it is suggesting me to translate it to Italian. I will change", "tokens": [300, 264, 3028, 307, 294, 5522, 293, 309, 307, 18094, 385, 281, 13799, 309, 281, 10003, 13, 286, 486, 1319], "temperature": 0.0, "avg_logprob": -0.15180089738633898, "compression_ratio": 1.4031007751937985, "no_speech_prob": 0.00669720908626914}, {"id": 267, "seek": 207356, "start": 2073.56, "end": 2100.92, "text": " it to English. Oh, fuck.", "tokens": [309, 281, 3669, 13, 876, 11, 3275, 13], "temperature": 0.0, "avg_logprob": -0.2402555545171102, "compression_ratio": 0.75, "no_speech_prob": 0.008367008529603481}, {"id": 268, "seek": 210092, "start": 2100.92, "end": 2109.2400000000002, "text": " So it is downloading the model. Now it's translating. So while it was translating, you already", "tokens": [407, 309, 307, 32529, 264, 2316, 13, 823, 309, 311, 35030, 13, 407, 1339, 309, 390, 35030, 11, 291, 1217], "temperature": 0.0, "avg_logprob": -0.13758655092609462, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0011966298334300518}, {"id": 269, "seek": 210092, "start": 2109.2400000000002, "end": 2113.36, "text": " saw the contents of the first part of the page was already translated, so it's super", "tokens": [1866, 264, 15768, 295, 264, 700, 644, 295, 264, 3028, 390, 1217, 16805, 11, 370, 309, 311, 1687], "temperature": 0.0, "avg_logprob": -0.13758655092609462, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0011966298334300518}, {"id": 270, "seek": 210092, "start": 2113.36, "end": 2120.2400000000002, "text": " quick in the end. And the translation seems to be pretty good. I don't speak French, but", "tokens": [1702, 294, 264, 917, 13, 400, 264, 12853, 2544, 281, 312, 1238, 665, 13, 286, 500, 380, 1710, 5522, 11, 457], "temperature": 0.0, "avg_logprob": -0.13758655092609462, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0011966298334300518}, {"id": 271, "seek": 212024, "start": 2120.24, "end": 2134.7999999999997, "text": " I think it makes sense. You can also use it from the toolbar, so you can choose a language", "tokens": [286, 519, 309, 1669, 2020, 13, 509, 393, 611, 764, 309, 490, 264, 47715, 11, 370, 291, 393, 2826, 257, 2856], "temperature": 0.0, "avg_logprob": -0.15004331588745118, "compression_ratio": 1.1111111111111112, "no_speech_prob": 0.002917056903243065}, {"id": 272, "seek": 213480, "start": 2134.8, "end": 2159.8, "text": " and translate it to another. Let's do Italian to French. It works.", "tokens": [293, 13799, 309, 281, 1071, 13, 961, 311, 360, 10003, 281, 5522, 13, 467, 1985, 13], "temperature": 0.0, "avg_logprob": -0.3077094078063965, "compression_ratio": 0.9565217391304348, "no_speech_prob": 0.00485181761905551}, {"id": 273, "seek": 215980, "start": 2159.8, "end": 2183.04, "text": " All right. So if you know any data set that we can use, in addition to the ones that we", "tokens": [1057, 558, 13, 407, 498, 291, 458, 604, 1412, 992, 300, 321, 393, 764, 11, 294, 4500, 281, 264, 2306, 300, 321], "temperature": 0.0, "avg_logprob": -0.14347602923711142, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.002522771479561925}, {"id": 274, "seek": 215980, "start": 2183.04, "end": 2188.6400000000003, "text": " already use, or if you're interested in building a new great feature in Firefox, or if you", "tokens": [1217, 764, 11, 420, 498, 291, 434, 3102, 294, 2390, 257, 777, 869, 4111, 294, 46613, 11, 420, 498, 291], "temperature": 0.0, "avg_logprob": -0.14347602923711142, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.002522771479561925}, {"id": 275, "seek": 218864, "start": 2188.64, "end": 2193.3599999999997, "text": " want to add support for your language or improving support for your language, come and talk to", "tokens": [528, 281, 909, 1406, 337, 428, 2856, 420, 11470, 1406, 337, 428, 2856, 11, 808, 293, 751, 281], "temperature": 0.0, "avg_logprob": -0.09696540203723279, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004701356403529644}, {"id": 276, "seek": 218864, "start": 2193.3599999999997, "end": 2199.8399999999997, "text": " us at our booth. We would be really happy if you could help us. And before we come to", "tokens": [505, 412, 527, 20912, 13, 492, 576, 312, 534, 2055, 498, 291, 727, 854, 505, 13, 400, 949, 321, 808, 281], "temperature": 0.0, "avg_logprob": -0.09696540203723279, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004701356403529644}, {"id": 277, "seek": 218864, "start": 2199.8399999999997, "end": 2206.92, "text": " an end, let me show you how far we've come. The dogs have grown, and we have learned that", "tokens": [364, 917, 11, 718, 385, 855, 291, 577, 1400, 321, 600, 808, 13, 440, 7197, 362, 7709, 11, 293, 321, 362, 3264, 300], "temperature": 0.0, "avg_logprob": -0.09696540203723279, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004701356403529644}, {"id": 278, "seek": 218864, "start": 2206.92, "end": 2214.04, "text": " it is possible to tame the complexity of a large-scale software. It is possible to use", "tokens": [309, 307, 1944, 281, 45774, 264, 14024, 295, 257, 2416, 12, 20033, 4722, 13, 467, 307, 1944, 281, 764], "temperature": 0.0, "avg_logprob": -0.09696540203723279, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.004701356403529644}, {"id": 279, "seek": 221404, "start": 2214.04, "end": 2221.4, "text": " the past history of development to support the future development, and it is possible", "tokens": [264, 1791, 2503, 295, 3250, 281, 1406, 264, 2027, 3250, 11, 293, 309, 307, 1944], "temperature": 0.0, "avg_logprob": -0.12781592932614413, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00192769814748317}, {"id": 280, "seek": 221404, "start": 2221.4, "end": 2228.7599999999998, "text": " to use machine learning in a privacy-friendly way and in the open. What else could we do", "tokens": [281, 764, 3479, 2539, 294, 257, 11427, 12, 22864, 636, 293, 294, 264, 1269, 13, 708, 1646, 727, 321, 360], "temperature": 0.0, "avg_logprob": -0.12781592932614413, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00192769814748317}, {"id": 281, "seek": 221404, "start": 2228.7599999999998, "end": 2234.2799999999997, "text": " with the data and the tools that we have at our disposal? I don't know. I'm looking forward", "tokens": [365, 264, 1412, 293, 264, 3873, 300, 321, 362, 412, 527, 26400, 30, 286, 500, 380, 458, 13, 286, 478, 1237, 2128], "temperature": 0.0, "avg_logprob": -0.12781592932614413, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00192769814748317}, {"id": 282, "seek": 221404, "start": 2234.2799999999997, "end": 2241.72, "text": " to know. I'm looking forward to see what other wild ideas you and us at Mozilla can come", "tokens": [281, 458, 13, 286, 478, 1237, 2128, 281, 536, 437, 661, 4868, 3487, 291, 293, 505, 412, 3335, 26403, 393, 808], "temperature": 0.0, "avg_logprob": -0.12781592932614413, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00192769814748317}, {"id": 283, "seek": 224172, "start": 2241.72, "end": 2244.9199999999996, "text": " up with. Thank you.", "tokens": [493, 365, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.21762969277121805, "compression_ratio": 1.3783783783783783, "no_speech_prob": 0.0036261274944990873}, {"id": 284, "seek": 224172, "start": 2244.9199999999996, "end": 2256.68, "text": " Thank you very much, Marco, for the amazing talk. Now we're open for questions. If anyone", "tokens": [1044, 291, 588, 709, 11, 26535, 11, 337, 264, 2243, 751, 13, 823, 321, 434, 1269, 337, 1651, 13, 759, 2878], "temperature": 0.0, "avg_logprob": -0.21762969277121805, "compression_ratio": 1.3783783783783783, "no_speech_prob": 0.0036261274944990873}, {"id": 285, "seek": 224172, "start": 2256.68, "end": 2263.24, "text": " would like to make a question, please raise your hand so I can take the microphone. Questions,", "tokens": [576, 411, 281, 652, 257, 1168, 11, 1767, 5300, 428, 1011, 370, 286, 393, 747, 264, 10952, 13, 27738, 11], "temperature": 0.0, "avg_logprob": -0.21762969277121805, "compression_ratio": 1.3783783783783783, "no_speech_prob": 0.0036261274944990873}, {"id": 286, "seek": 226324, "start": 2263.24, "end": 2275.4799999999996, "text": " questions, hands up. There. Okay, okay. I'm sorry, I'm learning. I'm new to this. I'm", "tokens": [1651, 11, 2377, 493, 13, 821, 13, 1033, 11, 1392, 13, 286, 478, 2597, 11, 286, 478, 2539, 13, 286, 478, 777, 281, 341, 13, 286, 478], "temperature": 0.0, "avg_logprob": -0.2097266412550403, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.0002858484804164618}, {"id": 287, "seek": 227548, "start": 2275.48, "end": 2301.4, "text": " coming up. Hello. I have actually two questions. First question is, have you actually think", "tokens": [1348, 493, 13, 2425, 13, 286, 362, 767, 732, 1651, 13, 2386, 1168, 307, 11, 362, 291, 767, 519], "temperature": 0.0, "avg_logprob": -0.14248864547066067, "compression_ratio": 1.1518987341772151, "no_speech_prob": 0.0036769621074199677}, {"id": 288, "seek": 230140, "start": 2301.4, "end": 2308.6, "text": " about the idea to use this mechanism to automatically translate interface of Mozilla products?", "tokens": [466, 264, 1558, 281, 764, 341, 7513, 281, 6772, 13799, 9226, 295, 3335, 26403, 3383, 30], "temperature": 0.0, "avg_logprob": -0.2656531891265473, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00527114188298583}, {"id": 289, "seek": 230140, "start": 2308.6, "end": 2318.76, "text": " Sorry? This thing? Yes. Yeah. So the question is, have you think about mechanism of automatically", "tokens": [4919, 30, 639, 551, 30, 1079, 13, 865, 13, 407, 264, 1168, 307, 11, 362, 291, 519, 466, 7513, 295, 6772], "temperature": 0.0, "avg_logprob": -0.2656531891265473, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00527114188298583}, {"id": 290, "seek": 230140, "start": 2318.76, "end": 2324.92, "text": " translating the interface of Mozilla Firefox products, or maybe documentation you already", "tokens": [35030, 264, 9226, 295, 3335, 26403, 46613, 3383, 11, 420, 1310, 14333, 291, 1217], "temperature": 0.0, "avg_logprob": -0.2656531891265473, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00527114188298583}, {"id": 291, "seek": 230140, "start": 2324.92, "end": 2329.0, "text": " have like MDN, because it's still a demand to translate this stuff?", "tokens": [362, 411, 22521, 45, 11, 570, 309, 311, 920, 257, 4733, 281, 13799, 341, 1507, 30], "temperature": 0.0, "avg_logprob": -0.2656531891265473, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00527114188298583}, {"id": 292, "seek": 232900, "start": 2329.0, "end": 2350.36, "text": " I'm sorry. I'm not hearing well. Can you maybe come closer?", "tokens": [286, 478, 2597, 13, 286, 478, 406, 4763, 731, 13, 1664, 291, 1310, 808, 4966, 30], "temperature": 0.0, "avg_logprob": -0.24382694907810376, "compression_ratio": 1.1774193548387097, "no_speech_prob": 0.008680766448378563}, {"id": 293, "seek": 232900, "start": 2350.36, "end": 2356.72, "text": " From here? Okay. Now it's better? Yes. Okay. So my question is, have you trying to use", "tokens": [3358, 510, 30, 1033, 13, 823, 309, 311, 1101, 30, 1079, 13, 1033, 13, 407, 452, 1168, 307, 11, 362, 291, 1382, 281, 764], "temperature": 0.0, "avg_logprob": -0.24382694907810376, "compression_ratio": 1.1774193548387097, "no_speech_prob": 0.008680766448378563}, {"id": 294, "seek": 235672, "start": 2356.72, "end": 2363.8399999999997, "text": " this mechanism of automatic translation to use this translation for existing interface", "tokens": [341, 7513, 295, 12509, 12853, 281, 764, 341, 12853, 337, 6741, 9226], "temperature": 0.0, "avg_logprob": -0.17420131644022832, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.006308093201369047}, {"id": 295, "seek": 235672, "start": 2363.8399999999997, "end": 2368.72, "text": " you have in the products, and especially also documentation part? Because it's kind of vital", "tokens": [291, 362, 294, 264, 3383, 11, 293, 2318, 611, 14333, 644, 30, 1436, 309, 311, 733, 295, 11707], "temperature": 0.0, "avg_logprob": -0.17420131644022832, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.006308093201369047}, {"id": 296, "seek": 235672, "start": 2368.72, "end": 2373.3599999999997, "text": " part when you need to translate new functionality, or you have to translate something new in", "tokens": [644, 562, 291, 643, 281, 13799, 777, 14980, 11, 420, 291, 362, 281, 13799, 746, 777, 294], "temperature": 0.0, "avg_logprob": -0.17420131644022832, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.006308093201369047}, {"id": 297, "seek": 235672, "start": 2373.3599999999997, "end": 2377.68, "text": " the interface, you need the help of translator. But if you already know how to translate in", "tokens": [264, 9226, 11, 291, 643, 264, 854, 295, 35223, 13, 583, 498, 291, 1217, 458, 577, 281, 13799, 294], "temperature": 0.0, "avg_logprob": -0.17420131644022832, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.006308093201369047}, {"id": 298, "seek": 235672, "start": 2377.68, "end": 2381.52, "text": " doing this stuff, so that means like you already have a data set, you can actually automatically", "tokens": [884, 341, 1507, 11, 370, 300, 1355, 411, 291, 1217, 362, 257, 1412, 992, 11, 291, 393, 767, 6772], "temperature": 0.0, "avg_logprob": -0.17420131644022832, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.006308093201369047}, {"id": 299, "seek": 238152, "start": 2381.52, "end": 2387.56, "text": " translate new parts of interface without translator? Yes. So it is definitely something", "tokens": [13799, 777, 3166, 295, 9226, 1553, 35223, 30, 1079, 13, 407, 309, 307, 2138, 746], "temperature": 0.0, "avg_logprob": -0.12043346961339314, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0024078148417174816}, {"id": 300, "seek": 238152, "start": 2387.56, "end": 2394.16, "text": " that could be used to help translators do their job. We could translate parts of the", "tokens": [300, 727, 312, 1143, 281, 854, 5105, 3391, 360, 641, 1691, 13, 492, 727, 13799, 3166, 295, 264], "temperature": 0.0, "avg_logprob": -0.12043346961339314, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0024078148417174816}, {"id": 301, "seek": 238152, "start": 2394.16, "end": 2400.0, "text": " interface automatically. And of course, there will always be some review from actual translator", "tokens": [9226, 6772, 13, 400, 295, 1164, 11, 456, 486, 1009, 312, 512, 3131, 490, 3539, 35223], "temperature": 0.0, "avg_logprob": -0.12043346961339314, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0024078148417174816}, {"id": 302, "seek": 238152, "start": 2400.0, "end": 2404.4, "text": " to make sure that the translation makes sense in the context, especially because Firefox", "tokens": [281, 652, 988, 300, 264, 12853, 1669, 2020, 294, 264, 4319, 11, 2318, 570, 46613], "temperature": 0.0, "avg_logprob": -0.12043346961339314, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0024078148417174816}, {"id": 303, "seek": 238152, "start": 2404.4, "end": 2410.16, "text": " UI sometimes you have very short text and it needs to make sense. But yeah, it's definitely", "tokens": [15682, 2171, 291, 362, 588, 2099, 2487, 293, 309, 2203, 281, 652, 2020, 13, 583, 1338, 11, 309, 311, 2138], "temperature": 0.0, "avg_logprob": -0.12043346961339314, "compression_ratio": 1.7203065134099618, "no_speech_prob": 0.0024078148417174816}, {"id": 304, "seek": 241016, "start": 2410.16, "end": 2415.04, "text": " something that we have considered. And actually one of the data sets that we use from the", "tokens": [746, 300, 321, 362, 4888, 13, 400, 767, 472, 295, 264, 1412, 6352, 300, 321, 764, 490, 264], "temperature": 0.0, "avg_logprob": -0.18123458993845973, "compression_ratio": 1.553648068669528, "no_speech_prob": 0.0008859346271492541}, {"id": 305, "seek": 241016, "start": 2415.04, "end": 2422.08, "text": " list, it's not possible to see from the slide, but it's called Mozilla L10N and they are", "tokens": [1329, 11, 309, 311, 406, 1944, 281, 536, 490, 264, 4137, 11, 457, 309, 311, 1219, 3335, 26403, 441, 3279, 45, 293, 436, 366], "temperature": 0.0, "avg_logprob": -0.18123458993845973, "compression_ratio": 1.553648068669528, "no_speech_prob": 0.0008859346271492541}, {"id": 306, "seek": 241016, "start": 2422.08, "end": 2431.6, "text": " sentence pairs from our browser UI. People are actually using it in research for automating", "tokens": [8174, 15494, 490, 527, 11185, 15682, 13, 3432, 366, 767, 1228, 309, 294, 2132, 337, 3553, 990], "temperature": 0.0, "avg_logprob": -0.18123458993845973, "compression_ratio": 1.553648068669528, "no_speech_prob": 0.0008859346271492541}, {"id": 307, "seek": 241016, "start": 2431.6, "end": 2439.7599999999998, "text": " translations. Does anyone have any other question? Please raise your hands. If you have any", "tokens": [37578, 13, 4402, 2878, 362, 604, 661, 1168, 30, 2555, 5300, 428, 2377, 13, 759, 291, 362, 604], "temperature": 0.0, "avg_logprob": -0.18123458993845973, "compression_ratio": 1.553648068669528, "no_speech_prob": 0.0008859346271492541}, {"id": 308, "seek": 243976, "start": 2439.76, "end": 2450.96, "text": " other questions, Marco? Okay. If not, thank you very much again, Marco.", "tokens": [661, 1651, 11, 26535, 30, 1033, 13, 759, 406, 11, 1309, 291, 588, 709, 797, 11, 26535, 13], "temperature": 0.0, "avg_logprob": -0.35994716791006237, "compression_ratio": 1.0379746835443038, "no_speech_prob": 0.0013336114352568984}, {"id": 309, "seek": 245096, "start": 2450.96, "end": 2470.2400000000002, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51328], "temperature": 0.0, "avg_logprob": -0.79336945215861, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0009748789016157389}], "language": "en"}