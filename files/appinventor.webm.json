{"text": " Okay, I think we can start already. Hi everybody, I'm Diego Barreiro. I'm one of the open source contributors to the MIT App Inventor project and today I'm going to be talking about App Inventor and how we can introduce artificial intelligence to kids using this platform. But before getting into it, I would like to introduce myself a little bit more. I first started coding with App Inventor when I was 14 years old. It was 2013 at that time and I basically wanted to build an app. I didn't know anything about coding and a high school teacher showed me this amazing platform. So I just spent the next couple of years building up with App Inventor and eventually I switched it to Java coding and I was able to contribute to the project later on as I'm doing right now. So what is MIT App Inventor? MIT App Inventor is an online platform that enables anybody to build any kind of application without having to learn any programming language like Java or coding that is nowadays the most popular ones. This is the interface and it has the mock font on the center and on the left side we can have the components, the elements that will make the app like buttons, labels, text boxes, text areas, like any kind of interaction that the user will have with the app. And then we can customize the properties like colors, text fonts, text sizes, whatever on this panel so we can make the app look as we wish for the final user. And how does the logic work? Well, most of you may know about Scratch. App Inventor works somehow like Scratch using this block language. So let's say that we want to play a sound when the app opens. We will be using a block that says when the screen one has opened, we want to play a specific sound later on. And that's how we can just make any kind of application. MIT App Inventor allows existing Android developers and Android developers to introduce new components using extensions. And we will be using today one of those extensions that was developed by a research project at MIT that enables to classify images on different groups using artificial intelligence. And to give some numbers of App Inventor, it was tested in 2008 as a Google project. And then a few years later it eventually was transferred to MIT. Right now it has gathered over 18 million users since it was created, since it was transferred to MIT with nearly 85 million apps that have been developed. And on a monthly basis we get roughly around one million users. And in terms of open source contributions, we have seen 164 different contributors to the project on GitHub. So today I'm not going to be giving the classic talk. I'm going to be showing a tutorial and people in the audience and at home can just follow this tutorial by visiting the following link on how to build an app. And what we will be doing today is building the Pickaboo app. Pickaboo is a game that is usually played with babies that when you see the baby, the baby loves and when you hide yourself, it just cries. So to show the final result, this will be the final result. Let me just switch to my phone. I'm going to be mirroring this phone. So I open the final app and I'm going to start using the camera. I can see, here I am. I'm looking at the baby that is happy. If I hide myself, it just cries. So let's get into it. So how we can use MIT App Inventor 2. The standard instance of App Inventor is hosted at aiu.appinventor.mit.eru, but that requires an account. So MIT has created this specific instance called.appinventor.mit that allows you to create these projects without any existing account. We will be using an anonymous account so that can be cleaned up later after finishing. And we will receive a code like the following one. Now this is blurred, but you will be able to see a code. And on the previous screen, if you want to recover the project later on, you can just paste the code that you previously get right here. And once that is done, you will be able to access an anonymous account as you can see over here and you can just start creating the app. So let's get into it. So we can visit fosdm23.appinventor.mit.eru and we can click on this link. This link is basically the code App Inventor instance and we are loading a template project for the pickable project. So I'm going to click on this one. So I click on continue without an account. And just wait a few seconds for the project to download from the repository and here it is. That was faster than last night. I can see the code here so I can just copy paste the code to access this instance later on. And as this is a tutorial, we can see on the left side that we are seeing a description of what we are trying to build with a detailed step by step guide. And this is the instance of the project. I can see the happy baby and then the sad babies hidden here. Okay, so let me just continue the presentation. The next step is turning the classifier. I'm not going to get too deep into the machine learning and how it works. I'm just going to be providing a very high level overview of how this works. So we will be using an image classification system that consists in creating two groups of images. We will be creating one group of images that is the face of myself looking at the baby and another group of images that is me hiding from the baby so we can show the sad face. So how does this work? We can visit this website, classifier.appinventor.mit.edu to train this model. Just as a side note, this instance only needs internet to load once. To train the model, that all happens in your browser so no servers are involved, no images are transferred outside your desktop. And this website is also open source so you can just check. There is the link on the FOSDEM23 website. So we visit the website and we start first creating the images group. So in this case we will be creating one image group for me so I'm looking at the baby and not me I'm hiding from the baby. If for example we are in a biology class and we want to classify trees, we will be creating one group of images for each kind of tree so we can recognize them later on. Then we will turn on the camera to take a photo of myself setting the group of images that I'm going to be saving this. So if I'm looking at the camera, it's going to be the not me group. If I'm looking at the camera, it's going to be the me group. This is the same for the not me. And once that is done, when we have a reasonable amount of images for each group, which should be around 5 or 10 images for each group, we can train the model. As again, this training happens in your computer so no images transferred outside of your computer. We can then test the model to with new images to make sure that we have properly trained the model and can identify ourselves. And once that is done, we can export the model and load it into App Inventor. So I'm going to be doing that very quickly. Go to foster203appinventor.mit.edu. I open the classifier instance. It's going to ask for permission to use the webcam. There it is, I just accepted it before. So the first step is creating the labels. First the me label, enter, and next the not me label. And well, the light is quite hard. I think it was... Take a few more images of me like looking to different places so I can train better the model. One more, one more. So seven images should be fine for this demo to not take too much time. And now for the not me, I'm going to take like one photo of me not being there, basically. I'm going to be using the right hand to hide myself so I can put this one in front of my eyes, turn it upside down, diagonal, like more images, the other hand as well, like that. So that should be enough for now. And once that is done, we can just hit the train button to train this model. It will be built on your local machine without sending anywhere. Okay, this was faster last night. Seems like the time that we saved from the loading the project, we lost it here. So now it's training. Yeah, it's my laptop. So this is a React app that has been open sourced and the only internet required is to just get the initial webpage later on. We can just disconnect and it will work perfectly. It's just offline training. If you really want to build it fully offline, you can just launch the React app locally and it will work. So now the model is built and to test it, I'm going to be looking at the camera as I did before. I captured the image and I can see that there is a 99.42 confidence that I'm looking at the camera. If I take a photo of myself hiding from it, there is a 99.33 confidence that I'm not looking at the baby. So once we have validated the model properly, we can export it to the app. And we will get this model.mdl file for AppInventor. So let's go to the presentation. And once we have the model, it's time to code the app using blocks. I'm not going to go through the slides anymore for people at home. If you have internet problems or the streaming is down, feel free to follow the slides. It's a step-by-step guide. But for here, I'm going to be showing the tutorial live. So let's go back to the project that we just loaded. And right here, we can see a quick description of the project of what we are going to do. A set up of the computer of how to connect to the MIT instance to the app. I'm going to show that at the end of the presentation. And we have here the pickable example. This is the final result. This is one of the MIT curriculum developers that made the original tutorial. And yeah, basically it says that we will be using the personally much classified extension that was developed by that research of MIT. And the first step is loading, turning the model. And then we have to upload the model. To upload the model, we go to this section over here on the media. We select the just downloaded model file. It should be here. And now it's uploaded. It's in the asset file of the app. And we can just change the model of the image property, personal image classifier to the now loaded new model. And we have just loaded the model properly. To give an overview of how the app is going to look like, there is going to be this status label that will tell the user when the app is ready to work. Now it says loading, because it's the initial state. It will let it go to the ready state and it will just identify the faces. We have these two bars that will be showing which percentage of confidence we are that we are looking at the baby or not. This is going to be the live image from the camera that I just showed before. And these are the interaction buttons to start the classification, to toggle the camera from the front to the back camera. And we have here the happy baby in this case. So uploading the turning model. This is the sequence of events that I was just talking about. First we start the app. The app will show to ready as soon as the classifier is ready to start working with the app. The user will press the start button and then the personal image classifier extension will keep classifying the live stream video from the camera continuously. And once they have identified the result, if there is a higher confidence that the me model is working, we will show the smiley face, otherwise the baby will just start crying. So in this template, there is already a set of blogs that are available to speed up the process. And we can go here that we see the one personal image classifier error. So that means that if for any reason the personal image classifier shows an error, maybe because there are some missing things on the phone or whatever, we will set the status label text to the actual error that we return from the image classifier. Once the image classifier is ready, we will enable the start button as well as the toggle camera button. We will set the status label text to be ready so the user knows that they can start using the app. And we will set the text boxes of each classification group to the previously defined labels me and not me in this case. If the user presses the toggle camera button, we will be changing from the front to the back camera just every time that they press so we can use the front selfie camera or the back normal camera. And once the user presses the start button, if the personal image classifier is already classifying an image, we will just stop it and we will show the start button with the start text. Otherwise, we have to start the classification. So to do so, we just invoke the start continuous classification method and we change the text to stop because we will be changing the the button interactions. And that's the quick overview of the code that is already available in the in the app. So how does the image classification work in MIT Preventor? Well, we have this big block that is the when personal image classifier has received a classification succeeded. We will receive this result variable. This result variable is a dictionary that to just give a little high level overview of what is a dictionary. It's a key value list of elements. So if we have two different groups, me and not me, we will receive me equals a specific value, not me equals a specific value. If we have three groups, we have one, two, and three that they each equal to a specific values. This is a little example of how it looks like. So we have key father equals this value, key model equals this value, then equals this value, etc. For the image classifier, a specific example, we will have something like this. We have me with this value and not me with this other specific value. So how we can retrieve a value in the dictionaries area, in the dictionaries block area, we have get value for a specific key. And we will be doing something similar to this. So we have the original dictionary here. We are building it in this area, make a dictionary me and not me. And we will be getting the value of the group that we want to use right now. In this case, it's the me example. If we want to take the not me, we just have to change this label to not me. And if we are using the wrong model because the groups are not the same, we just return a zero because we cannot classify that group. So let's get into it. By default, the tutorial will provide this block that is some variable, some me confidence level, and we have to complete them using this block. So to do so, we will take the get for key in dictionary block. We join it to the me confidence block. We remove, nice, get value for the key. And we will take from the text blocks an empty text block to touch it right here. And we can type me. So we can get the me group into the me confidence variable. The dictionary is the result. We can just attach it here. And if not found, we will just returning an empty zero value. And for the not me confidence, it's basically the same. So we can copy paste the blocks. We attach them to the not me confidence area. And we just have to prepend a note in front of the me. And now we have just defined that me confidence variable that can be accessed like that. We'll have the percentage of confidence that we are looking at the baby. And the not me confidence, it's the opposite. It's how confidence we are that we are not looking at the baby. The next step, the interesting variables. And now we just have to recap what do we have to do in the app. So in the app, we have to first update these labels here with the percentage. And we have to update these color bars with the correct confidence levels. We can do that by going to these components, to these two horizontal arrangements. And we have percentage one, bar graph one, percentage two and bar graph two. Percentage one, we can update the text to the percentage that we are showing. One second. There it is. So the value that we return from the dictionary goes from zero to one, but we want to return a percentage which goes from zero to 100. So we will take this me confidence value and we will multiply it by 100. So we can get the zero to 100 range. We just join it right here with the math number and we multiply it by 100, 100. But we will be missing the percentage sign. To get the percentage sign, we can use the text blocks with the join block. So we can join two text together and we can just create a new percentage symbol like this using the percentage symbol. And this is for the percentage labels. For the bar graph labels, we will be pledging with the length of the actual graph, bar graph. To do so, we have the width percent block that can modify the width according to a percentage. And we already have defined the percentage right here, so we can just copy paste these blocks and attach them to the width percent. And this is for the me group. For the not me group, we can copy paste the percentage one, which changes to percentage two, and we change the me confidence value to the not me confidence value. And for the bar graph, it's going to be bar graph two right here. And me confidence changes to not me confidence. And with that, we already have all the sequence of events for the labels updates. We can just go to the next step and confirm that we have defined it correctly, which is the same result. The next step is the fancy image change that if we think that we are looking at the baby, we will show the happy face, otherwise we just show the crying face. We will be using the if then logic. So go to the control blocks and we just take the if then otherwise block. We append it here. And what will we do is we will be comparing the me confidence value to the not me confidence value to know if we are looking at the baby or not. We go to the math blocks, we pick this comparator block, attach them to the if statement, and we are going to be changing the comparison to higher or equal because we will not, we don't worry about the equal in this case, we just want the higher or equal. We take again the me confidence variable, we compare it here, and we take the not me confidence value and we compare it right here. Then we will be updating the background of the of the app, which is available in the screen one. We take the background color block, we attach it here, and the tutorial already provides the example colors. So I'm just going to be dragging this right below so I can have them more easily accessible right here. And I can just join it here. And for the baby images, we have two images available here, happy baby and sad baby. So if we think that we are looking at the baby, we show the happy baby. So we use the visible block. And if not, we just hide, sorry, if we think that we're looking at the baby, we hide the we hide the side baby face. We go to the logic blocks, we take the true so we can set to true to visible, we can set visible to true, and we set visible to false for the sad baby like that. And we just join it. For the case of me confidence being higher than not me confidence, for the opposite case, when we are not looking at the baby, we just change the background color to this pink color. We hide the happy baby face like that. And we show the sad baby face just like this. And now the app is finished. So here we can just check the final code, which is exactly the same as we have right here. There are other possibilities like we can just implement a classifier using a different person. But to show how this works, we can use the MIT company map that is available on the Play Store. Let me just show my phone again. Here it is. So you can just go to the Play Store and go to MIT App Inventor, search MIT App Inventor and you have right here the company. You can open it. Yeah, you can just ignore this warning. It works without Wi-Fi. Continue without Wi-Fi. And over here you can connect the AI companion. And now I can scan the QR code like this. I'm sorry, it just disappeared. It takes a few seconds to connect. Let's see if it was faster than tonight. Now it's loading the extension, the personal classifier extension into my phone. Like this works with Wi-Fi, mobile network. It doesn't have to be connected. It's just connected because I'm just mirroring the screen through that cable. And I see here the layout of the app. I can see that it shows ready. So I can just toggle the camera to be the from one. And I can look at the camera and I'm just going to be start. And there is a higher confidence that I'm looking at the Wi-Fi. Just put a hand in front of it. It's just crying. And yeah, that's it. Later, if you want to build any other apps, you can export it to APK files. So you can start it on your phone or 200 app bundles if you want to distribute it through Play Store. But yeah, this is just a very high-level introduction to artificial intelligence in Inventor. You can just build any kind of classifier, for example, to classify trees, flowers, to classify even people. For example, for a faculty, if you want to build an app that recognizes people in your class as a game, you can just use Inventor and build any kind of app. Thank you so much and hope that it was useful for everybody. Thank you. Any questions? Do you mentor a technobation team? No. Sorry, I'm a software engineer. I just contributed to Inventor. I started as building apps and then I transitioned to open source. I participated in Google Summer of Code, like this option to export a 100 app bundles was my project in 2020 for Google Summer of Code. But yeah, I'm more like, more technical than actually teaching to kids. Any other questions? What's your experience with the relation between the number of pictures you have to submit to your classifier and your accuracy? That's a very good question. So for the linear example, he was asking like, what's the experience with the amount of images that we are going to be using for the classifier? So I haven't really tested it right now. But we have seen that if we go higher than 10 images for each class, for each group of images, we'll have really good results. In this case, because I was just turning very fast and using just a few number of images, you can see that the confidence levels were a little bit like 80, 20. But if we provide more than 10 images for each class, we should be able to get around over 90, 95% of confidence for each number. I'm not sure if there are any questions from the chat. Let me just check. What do you capture? Is that just for your face or did the learning, what was learned? It recognizes, it depends on what you are training, because in this case, we are just providing a very specific gestures. It's training my face like any face looking at the camera, or a hand in front of the face. By default, the model that is available, that is here, this model is turned by Salim. It's the example guy that is at the beginning of the tutorial. And I just tested it last night, and it worked with me because it recognizes the gestures, not the faces. If instead we train recognizing people, we will all be looking in the same way at the camera. So it will just go for specific facial, how do you say, facial features. In this case, in this case, it will work. You can just try if you want. We can try. Yeah, it should work. Can you, it's going to be a little bit tough, but Mark, can you, can you just try with Mark, for example? Toggle camera. Toggle camera. Yeah, and just try with, and start. Press start. It's a happy face, so if you put a hand in front, it's a sad face. It's recognizing the gestures. So can you also train it to recognize specific people? Yeah, it can be trained, but in this case, because the higher difference was the hand, it's just looking for the hand in model. But if you don't show the hand, it will look for faces. Yeah, it's a, it can be a fit because it's just, you can just use this website and fill any kind of models. The only restriction is that it has to be an MLD file, but yeah, it can classify any model basically. No problem. Any other questions? Well, I think we can leave it here. Thank you so much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.700000000000001, "text": " Okay, I think we can start already. Hi everybody, I'm Diego Barreiro. I'm one of the open source", "tokens": [1033, 11, 286, 519, 321, 393, 722, 1217, 13, 2421, 2201, 11, 286, 478, 16377, 4156, 265, 5182, 13, 286, 478, 472, 295, 264, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.2255326377020942, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.3486030101776123}, {"id": 1, "seek": 0, "start": 12.700000000000001, "end": 16.3, "text": " contributors to the MIT App Inventor project and today I'm going to be talking about App", "tokens": [45627, 281, 264, 13100, 3132, 682, 2475, 284, 1716, 293, 965, 286, 478, 516, 281, 312, 1417, 466, 3132], "temperature": 0.0, "avg_logprob": -0.2255326377020942, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.3486030101776123}, {"id": 2, "seek": 0, "start": 16.3, "end": 21.94, "text": " Inventor and how we can introduce artificial intelligence to kids using this platform.", "tokens": [682, 2475, 284, 293, 577, 321, 393, 5366, 11677, 7599, 281, 2301, 1228, 341, 3663, 13], "temperature": 0.0, "avg_logprob": -0.2255326377020942, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.3486030101776123}, {"id": 3, "seek": 0, "start": 21.94, "end": 25.76, "text": " But before getting into it, I would like to introduce myself a little bit more. I first", "tokens": [583, 949, 1242, 666, 309, 11, 286, 576, 411, 281, 5366, 2059, 257, 707, 857, 544, 13, 286, 700], "temperature": 0.0, "avg_logprob": -0.2255326377020942, "compression_ratio": 1.5450643776824033, "no_speech_prob": 0.3486030101776123}, {"id": 4, "seek": 2576, "start": 25.76, "end": 31.840000000000003, "text": " started coding with App Inventor when I was 14 years old. It was 2013 at that time and", "tokens": [1409, 17720, 365, 3132, 682, 2475, 284, 562, 286, 390, 3499, 924, 1331, 13, 467, 390, 9012, 412, 300, 565, 293], "temperature": 0.0, "avg_logprob": -0.11536961793899536, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0002223101764684543}, {"id": 5, "seek": 2576, "start": 31.840000000000003, "end": 36.08, "text": " I basically wanted to build an app. I didn't know anything about coding and a high school", "tokens": [286, 1936, 1415, 281, 1322, 364, 724, 13, 286, 994, 380, 458, 1340, 466, 17720, 293, 257, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.11536961793899536, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0002223101764684543}, {"id": 6, "seek": 2576, "start": 36.08, "end": 40.160000000000004, "text": " teacher showed me this amazing platform. So I just spent the next couple of years building", "tokens": [5027, 4712, 385, 341, 2243, 3663, 13, 407, 286, 445, 4418, 264, 958, 1916, 295, 924, 2390], "temperature": 0.0, "avg_logprob": -0.11536961793899536, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0002223101764684543}, {"id": 7, "seek": 2576, "start": 40.160000000000004, "end": 45.36, "text": " up with App Inventor and eventually I switched it to Java coding and I was able to contribute", "tokens": [493, 365, 3132, 682, 2475, 284, 293, 4728, 286, 16858, 309, 281, 10745, 17720, 293, 286, 390, 1075, 281, 10586], "temperature": 0.0, "avg_logprob": -0.11536961793899536, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0002223101764684543}, {"id": 8, "seek": 2576, "start": 45.36, "end": 51.92, "text": " to the project later on as I'm doing right now. So what is MIT App Inventor? MIT App", "tokens": [281, 264, 1716, 1780, 322, 382, 286, 478, 884, 558, 586, 13, 407, 437, 307, 13100, 3132, 682, 2475, 284, 30, 13100, 3132], "temperature": 0.0, "avg_logprob": -0.11536961793899536, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.0002223101764684543}, {"id": 9, "seek": 5192, "start": 51.92, "end": 56.92, "text": " Inventor is an online platform that enables anybody to build any kind of application without", "tokens": [682, 2475, 284, 307, 364, 2950, 3663, 300, 17077, 4472, 281, 1322, 604, 733, 295, 3861, 1553], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 10, "seek": 5192, "start": 56.92, "end": 61.0, "text": " having to learn any programming language like Java or coding that is nowadays the most popular", "tokens": [1419, 281, 1466, 604, 9410, 2856, 411, 10745, 420, 17720, 300, 307, 13434, 264, 881, 3743], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 11, "seek": 5192, "start": 61.0, "end": 67.12, "text": " ones. This is the interface and it has the mock font on the center and on the left side", "tokens": [2306, 13, 639, 307, 264, 9226, 293, 309, 575, 264, 17362, 10703, 322, 264, 3056, 293, 322, 264, 1411, 1252], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 12, "seek": 5192, "start": 67.12, "end": 70.92, "text": " we can have the components, the elements that will make the app like buttons, labels, text", "tokens": [321, 393, 362, 264, 6677, 11, 264, 4959, 300, 486, 652, 264, 724, 411, 9905, 11, 16949, 11, 2487], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 13, "seek": 5192, "start": 70.92, "end": 76.08, "text": " boxes, text areas, like any kind of interaction that the user will have with the app. And", "tokens": [9002, 11, 2487, 3179, 11, 411, 604, 733, 295, 9285, 300, 264, 4195, 486, 362, 365, 264, 724, 13, 400], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 14, "seek": 5192, "start": 76.08, "end": 81.6, "text": " then we can customize the properties like colors, text fonts, text sizes, whatever on", "tokens": [550, 321, 393, 19734, 264, 7221, 411, 4577, 11, 2487, 35316, 11, 2487, 11602, 11, 2035, 322], "temperature": 0.0, "avg_logprob": -0.1504403556265482, "compression_ratio": 1.8249158249158248, "no_speech_prob": 0.00019107629486825317}, {"id": 15, "seek": 8160, "start": 81.6, "end": 87.32, "text": " this panel so we can make the app look as we wish for the final user. And how does the", "tokens": [341, 4831, 370, 321, 393, 652, 264, 724, 574, 382, 321, 3172, 337, 264, 2572, 4195, 13, 400, 577, 775, 264], "temperature": 0.0, "avg_logprob": -0.13048962752024332, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0007601763354614377}, {"id": 16, "seek": 8160, "start": 87.32, "end": 93.44, "text": " logic work? Well, most of you may know about Scratch. App Inventor works somehow like Scratch", "tokens": [9952, 589, 30, 1042, 11, 881, 295, 291, 815, 458, 466, 34944, 852, 13, 3132, 682, 2475, 284, 1985, 6063, 411, 34944, 852], "temperature": 0.0, "avg_logprob": -0.13048962752024332, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0007601763354614377}, {"id": 17, "seek": 8160, "start": 93.44, "end": 98.11999999999999, "text": " using this block language. So let's say that we want to play a sound when the app opens.", "tokens": [1228, 341, 3461, 2856, 13, 407, 718, 311, 584, 300, 321, 528, 281, 862, 257, 1626, 562, 264, 724, 9870, 13], "temperature": 0.0, "avg_logprob": -0.13048962752024332, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0007601763354614377}, {"id": 18, "seek": 8160, "start": 98.11999999999999, "end": 103.63999999999999, "text": " We will be using a block that says when the screen one has opened, we want to play a specific", "tokens": [492, 486, 312, 1228, 257, 3461, 300, 1619, 562, 264, 2568, 472, 575, 5625, 11, 321, 528, 281, 862, 257, 2685], "temperature": 0.0, "avg_logprob": -0.13048962752024332, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0007601763354614377}, {"id": 19, "seek": 8160, "start": 103.63999999999999, "end": 110.52, "text": " sound later on. And that's how we can just make any kind of application. MIT App Inventor", "tokens": [1626, 1780, 322, 13, 400, 300, 311, 577, 321, 393, 445, 652, 604, 733, 295, 3861, 13, 13100, 3132, 682, 2475, 284], "temperature": 0.0, "avg_logprob": -0.13048962752024332, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0007601763354614377}, {"id": 20, "seek": 11052, "start": 110.52, "end": 116.16, "text": " allows existing Android developers and Android developers to introduce new components using", "tokens": [4045, 6741, 8853, 8849, 293, 8853, 8849, 281, 5366, 777, 6677, 1228], "temperature": 0.0, "avg_logprob": -0.17109535137812296, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.000548920885194093}, {"id": 21, "seek": 11052, "start": 116.16, "end": 120.67999999999999, "text": " extensions. And we will be using today one of those extensions that was developed by a", "tokens": [25129, 13, 400, 321, 486, 312, 1228, 965, 472, 295, 729, 25129, 300, 390, 4743, 538, 257], "temperature": 0.0, "avg_logprob": -0.17109535137812296, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.000548920885194093}, {"id": 22, "seek": 11052, "start": 120.67999999999999, "end": 126.8, "text": " research project at MIT that enables to classify images on different groups using artificial", "tokens": [2132, 1716, 412, 13100, 300, 17077, 281, 33872, 5267, 322, 819, 3935, 1228, 11677], "temperature": 0.0, "avg_logprob": -0.17109535137812296, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.000548920885194093}, {"id": 23, "seek": 11052, "start": 126.8, "end": 133.56, "text": " intelligence. And to give some numbers of App Inventor, it was tested in 2008 as a Google", "tokens": [7599, 13, 400, 281, 976, 512, 3547, 295, 3132, 682, 2475, 284, 11, 309, 390, 8246, 294, 10389, 382, 257, 3329], "temperature": 0.0, "avg_logprob": -0.17109535137812296, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.000548920885194093}, {"id": 24, "seek": 11052, "start": 133.56, "end": 140.35999999999999, "text": " project. And then a few years later it eventually was transferred to MIT. Right now it has gathered", "tokens": [1716, 13, 400, 550, 257, 1326, 924, 1780, 309, 4728, 390, 15809, 281, 13100, 13, 1779, 586, 309, 575, 13032], "temperature": 0.0, "avg_logprob": -0.17109535137812296, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.000548920885194093}, {"id": 25, "seek": 14036, "start": 140.36, "end": 145.28, "text": " over 18 million users since it was created, since it was transferred to MIT with nearly", "tokens": [670, 2443, 2459, 5022, 1670, 309, 390, 2942, 11, 1670, 309, 390, 15809, 281, 13100, 365, 6217], "temperature": 0.0, "avg_logprob": -0.165189219456093, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00020849241991527379}, {"id": 26, "seek": 14036, "start": 145.28, "end": 150.76000000000002, "text": " 85 million apps that have been developed. And on a monthly basis we get roughly around", "tokens": [14695, 2459, 7733, 300, 362, 668, 4743, 13, 400, 322, 257, 12878, 5143, 321, 483, 9810, 926], "temperature": 0.0, "avg_logprob": -0.165189219456093, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00020849241991527379}, {"id": 27, "seek": 14036, "start": 150.76000000000002, "end": 158.12, "text": " one million users. And in terms of open source contributions, we have seen 164 different contributors", "tokens": [472, 2459, 5022, 13, 400, 294, 2115, 295, 1269, 4009, 15725, 11, 321, 362, 1612, 3165, 19, 819, 45627], "temperature": 0.0, "avg_logprob": -0.165189219456093, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00020849241991527379}, {"id": 28, "seek": 14036, "start": 158.12, "end": 165.96, "text": " to the project on GitHub. So today I'm not going to be giving the classic", "tokens": [281, 264, 1716, 322, 23331, 13, 407, 965, 286, 478, 406, 516, 281, 312, 2902, 264, 7230], "temperature": 0.0, "avg_logprob": -0.165189219456093, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00020849241991527379}, {"id": 29, "seek": 14036, "start": 165.96, "end": 169.32000000000002, "text": " talk. I'm going to be showing a tutorial and people in the audience and at home can just", "tokens": [751, 13, 286, 478, 516, 281, 312, 4099, 257, 7073, 293, 561, 294, 264, 4034, 293, 412, 1280, 393, 445], "temperature": 0.0, "avg_logprob": -0.165189219456093, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00020849241991527379}, {"id": 30, "seek": 16932, "start": 169.32, "end": 174.56, "text": " follow this tutorial by visiting the following link on how to build an app. And what we will", "tokens": [1524, 341, 7073, 538, 11700, 264, 3480, 2113, 322, 577, 281, 1322, 364, 724, 13, 400, 437, 321, 486], "temperature": 0.0, "avg_logprob": -0.16623490432213092, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.001019358285702765}, {"id": 31, "seek": 16932, "start": 174.56, "end": 179.16, "text": " be doing today is building the Pickaboo app. Pickaboo is a game that is usually played", "tokens": [312, 884, 965, 307, 2390, 264, 14129, 455, 1986, 724, 13, 14129, 455, 1986, 307, 257, 1216, 300, 307, 2673, 3737], "temperature": 0.0, "avg_logprob": -0.16623490432213092, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.001019358285702765}, {"id": 32, "seek": 16932, "start": 179.16, "end": 183.72, "text": " with babies that when you see the baby, the baby loves and when you hide yourself, it", "tokens": [365, 10917, 300, 562, 291, 536, 264, 3186, 11, 264, 3186, 6752, 293, 562, 291, 6479, 1803, 11, 309], "temperature": 0.0, "avg_logprob": -0.16623490432213092, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.001019358285702765}, {"id": 33, "seek": 16932, "start": 183.72, "end": 191.35999999999999, "text": " just cries. So to show the final result, this will be the final result. Let me just switch", "tokens": [445, 29206, 13, 407, 281, 855, 264, 2572, 1874, 11, 341, 486, 312, 264, 2572, 1874, 13, 961, 385, 445, 3679], "temperature": 0.0, "avg_logprob": -0.16623490432213092, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.001019358285702765}, {"id": 34, "seek": 16932, "start": 191.35999999999999, "end": 197.16, "text": " to my phone. I'm going to be mirroring this phone. So I open the final app and I'm going", "tokens": [281, 452, 2593, 13, 286, 478, 516, 281, 312, 8013, 278, 341, 2593, 13, 407, 286, 1269, 264, 2572, 724, 293, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.16623490432213092, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.001019358285702765}, {"id": 35, "seek": 19716, "start": 197.16, "end": 203.6, "text": " to start using the camera. I can see, here I am. I'm looking at the baby that is happy.", "tokens": [281, 722, 1228, 264, 2799, 13, 286, 393, 536, 11, 510, 286, 669, 13, 286, 478, 1237, 412, 264, 3186, 300, 307, 2055, 13], "temperature": 0.0, "avg_logprob": -0.2384400407807166, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005954966181889176}, {"id": 36, "seek": 19716, "start": 203.6, "end": 210.72, "text": " If I hide myself, it just cries. So let's get into it.", "tokens": [759, 286, 6479, 2059, 11, 309, 445, 29206, 13, 407, 718, 311, 483, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.2384400407807166, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005954966181889176}, {"id": 37, "seek": 19716, "start": 210.72, "end": 215.28, "text": " So how we can use MIT App Inventor 2. The standard instance of App Inventor is hosted", "tokens": [407, 577, 321, 393, 764, 13100, 3132, 682, 2475, 284, 568, 13, 440, 3832, 5197, 295, 3132, 682, 2475, 284, 307, 19204], "temperature": 0.0, "avg_logprob": -0.2384400407807166, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005954966181889176}, {"id": 38, "seek": 19716, "start": 215.28, "end": 220.88, "text": " at aiu.appinventor.mit.eru, but that requires an account. So MIT has created this specific", "tokens": [412, 257, 5951, 13, 1746, 259, 2475, 284, 13, 3508, 13, 260, 84, 11, 457, 300, 7029, 364, 2696, 13, 407, 13100, 575, 2942, 341, 2685], "temperature": 0.0, "avg_logprob": -0.2384400407807166, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005954966181889176}, {"id": 39, "seek": 19716, "start": 220.88, "end": 226.68, "text": " instance called.appinventor.mit that allows you to create these projects without any existing", "tokens": [5197, 1219, 2411, 1746, 259, 2475, 284, 13, 3508, 300, 4045, 291, 281, 1884, 613, 4455, 1553, 604, 6741], "temperature": 0.0, "avg_logprob": -0.2384400407807166, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005954966181889176}, {"id": 40, "seek": 22668, "start": 226.68, "end": 234.24, "text": " account. We will be using an anonymous account so that can be cleaned up later after finishing.", "tokens": [2696, 13, 492, 486, 312, 1228, 364, 24932, 2696, 370, 300, 393, 312, 16146, 493, 1780, 934, 12693, 13], "temperature": 0.0, "avg_logprob": -0.1377191028079471, "compression_ratio": 1.895397489539749, "no_speech_prob": 0.0002991690125782043}, {"id": 41, "seek": 22668, "start": 234.24, "end": 238.96, "text": " And we will receive a code like the following one. Now this is blurred, but you will be able", "tokens": [400, 321, 486, 4774, 257, 3089, 411, 264, 3480, 472, 13, 823, 341, 307, 43525, 11, 457, 291, 486, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1377191028079471, "compression_ratio": 1.895397489539749, "no_speech_prob": 0.0002991690125782043}, {"id": 42, "seek": 22668, "start": 238.96, "end": 242.92000000000002, "text": " to see a code. And on the previous screen, if you want to recover the project later on,", "tokens": [281, 536, 257, 3089, 13, 400, 322, 264, 3894, 2568, 11, 498, 291, 528, 281, 8114, 264, 1716, 1780, 322, 11], "temperature": 0.0, "avg_logprob": -0.1377191028079471, "compression_ratio": 1.895397489539749, "no_speech_prob": 0.0002991690125782043}, {"id": 43, "seek": 22668, "start": 242.92000000000002, "end": 249.36, "text": " you can just paste the code that you previously get right here. And once that is done, you", "tokens": [291, 393, 445, 9163, 264, 3089, 300, 291, 8046, 483, 558, 510, 13, 400, 1564, 300, 307, 1096, 11, 291], "temperature": 0.0, "avg_logprob": -0.1377191028079471, "compression_ratio": 1.895397489539749, "no_speech_prob": 0.0002991690125782043}, {"id": 44, "seek": 22668, "start": 249.36, "end": 253.04000000000002, "text": " will be able to access an anonymous account as you can see over here and you can just", "tokens": [486, 312, 1075, 281, 2105, 364, 24932, 2696, 382, 291, 393, 536, 670, 510, 293, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.1377191028079471, "compression_ratio": 1.895397489539749, "no_speech_prob": 0.0002991690125782043}, {"id": 45, "seek": 25304, "start": 253.04, "end": 259.32, "text": " start creating the app. So let's get into it.", "tokens": [722, 4084, 264, 724, 13, 407, 718, 311, 483, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.14923635761389573, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0004058846679981798}, {"id": 46, "seek": 25304, "start": 259.32, "end": 264.92, "text": " So we can visit fosdm23.appinventor.mit.eru and we can click on this link. This link", "tokens": [407, 321, 393, 3441, 283, 329, 67, 76, 9356, 13, 1746, 259, 2475, 284, 13, 3508, 13, 260, 84, 293, 321, 393, 2052, 322, 341, 2113, 13, 639, 2113], "temperature": 0.0, "avg_logprob": -0.14923635761389573, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0004058846679981798}, {"id": 47, "seek": 25304, "start": 264.92, "end": 269.59999999999997, "text": " is basically the code App Inventor instance and we are loading a template project for", "tokens": [307, 1936, 264, 3089, 3132, 682, 2475, 284, 5197, 293, 321, 366, 15114, 257, 12379, 1716, 337], "temperature": 0.0, "avg_logprob": -0.14923635761389573, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0004058846679981798}, {"id": 48, "seek": 25304, "start": 269.59999999999997, "end": 276.4, "text": " the pickable project. So I'm going to click on this one. So I click on continue without", "tokens": [264, 1888, 712, 1716, 13, 407, 286, 478, 516, 281, 2052, 322, 341, 472, 13, 407, 286, 2052, 322, 2354, 1553], "temperature": 0.0, "avg_logprob": -0.14923635761389573, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0004058846679981798}, {"id": 49, "seek": 27640, "start": 276.4, "end": 283.15999999999997, "text": " an account. And just wait a few seconds for the project to download from the repository", "tokens": [364, 2696, 13, 400, 445, 1699, 257, 1326, 3949, 337, 264, 1716, 281, 5484, 490, 264, 25841], "temperature": 0.0, "avg_logprob": -0.13004315227543542, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00023915848578326404}, {"id": 50, "seek": 27640, "start": 283.15999999999997, "end": 289.56, "text": " and here it is. That was faster than last night. I can see the code here so I can just", "tokens": [293, 510, 309, 307, 13, 663, 390, 4663, 813, 1036, 1818, 13, 286, 393, 536, 264, 3089, 510, 370, 286, 393, 445], "temperature": 0.0, "avg_logprob": -0.13004315227543542, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00023915848578326404}, {"id": 51, "seek": 27640, "start": 289.56, "end": 295.52, "text": " copy paste the code to access this instance later on. And as this is a tutorial, we can", "tokens": [5055, 9163, 264, 3089, 281, 2105, 341, 5197, 1780, 322, 13, 400, 382, 341, 307, 257, 7073, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.13004315227543542, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00023915848578326404}, {"id": 52, "seek": 27640, "start": 295.52, "end": 300.28, "text": " see on the left side that we are seeing a description of what we are trying to build", "tokens": [536, 322, 264, 1411, 1252, 300, 321, 366, 2577, 257, 3855, 295, 437, 321, 366, 1382, 281, 1322], "temperature": 0.0, "avg_logprob": -0.13004315227543542, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00023915848578326404}, {"id": 53, "seek": 27640, "start": 300.28, "end": 306.08, "text": " with a detailed step by step guide. And this is the instance of the project. I can see", "tokens": [365, 257, 9942, 1823, 538, 1823, 5934, 13, 400, 341, 307, 264, 5197, 295, 264, 1716, 13, 286, 393, 536], "temperature": 0.0, "avg_logprob": -0.13004315227543542, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00023915848578326404}, {"id": 54, "seek": 30608, "start": 306.08, "end": 315.0, "text": " the happy baby and then the sad babies hidden here. Okay, so let me just continue the presentation.", "tokens": [264, 2055, 3186, 293, 550, 264, 4227, 10917, 7633, 510, 13, 1033, 11, 370, 718, 385, 445, 2354, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.10764912578547112, "compression_ratio": 1.7578125, "no_speech_prob": 0.00027368165319785476}, {"id": 55, "seek": 30608, "start": 315.0, "end": 319.28, "text": " The next step is turning the classifier. I'm not going to get too deep into the machine", "tokens": [440, 958, 1823, 307, 6246, 264, 1508, 9902, 13, 286, 478, 406, 516, 281, 483, 886, 2452, 666, 264, 3479], "temperature": 0.0, "avg_logprob": -0.10764912578547112, "compression_ratio": 1.7578125, "no_speech_prob": 0.00027368165319785476}, {"id": 56, "seek": 30608, "start": 319.28, "end": 324.24, "text": " learning and how it works. I'm just going to be providing a very high level overview", "tokens": [2539, 293, 577, 309, 1985, 13, 286, 478, 445, 516, 281, 312, 6530, 257, 588, 1090, 1496, 12492], "temperature": 0.0, "avg_logprob": -0.10764912578547112, "compression_ratio": 1.7578125, "no_speech_prob": 0.00027368165319785476}, {"id": 57, "seek": 30608, "start": 324.24, "end": 329.84, "text": " of how this works. So we will be using an image classification system that consists in creating", "tokens": [295, 577, 341, 1985, 13, 407, 321, 486, 312, 1228, 364, 3256, 21538, 1185, 300, 14689, 294, 4084], "temperature": 0.0, "avg_logprob": -0.10764912578547112, "compression_ratio": 1.7578125, "no_speech_prob": 0.00027368165319785476}, {"id": 58, "seek": 30608, "start": 329.84, "end": 335.91999999999996, "text": " two groups of images. We will be creating one group of images that is the face of", "tokens": [732, 3935, 295, 5267, 13, 492, 486, 312, 4084, 472, 1594, 295, 5267, 300, 307, 264, 1851, 295], "temperature": 0.0, "avg_logprob": -0.10764912578547112, "compression_ratio": 1.7578125, "no_speech_prob": 0.00027368165319785476}, {"id": 59, "seek": 33592, "start": 335.92, "end": 340.40000000000003, "text": " myself looking at the baby and another group of images that is me hiding from the baby", "tokens": [2059, 1237, 412, 264, 3186, 293, 1071, 1594, 295, 5267, 300, 307, 385, 10596, 490, 264, 3186], "temperature": 0.0, "avg_logprob": -0.18622940465023644, "compression_ratio": 1.6875, "no_speech_prob": 0.00031433795811608434}, {"id": 60, "seek": 33592, "start": 340.40000000000003, "end": 349.40000000000003, "text": " so we can show the sad face. So how does this work? We can visit this website, classifier.appinventor.mit.edu", "tokens": [370, 321, 393, 855, 264, 4227, 1851, 13, 407, 577, 775, 341, 589, 30, 492, 393, 3441, 341, 3144, 11, 1508, 9902, 13, 1746, 259, 2475, 284, 13, 3508, 13, 22938], "temperature": 0.0, "avg_logprob": -0.18622940465023644, "compression_ratio": 1.6875, "no_speech_prob": 0.00031433795811608434}, {"id": 61, "seek": 33592, "start": 349.40000000000003, "end": 355.04, "text": " to train this model. Just as a side note, this instance only needs internet to load", "tokens": [281, 3847, 341, 2316, 13, 1449, 382, 257, 1252, 3637, 11, 341, 5197, 787, 2203, 4705, 281, 3677], "temperature": 0.0, "avg_logprob": -0.18622940465023644, "compression_ratio": 1.6875, "no_speech_prob": 0.00031433795811608434}, {"id": 62, "seek": 33592, "start": 355.04, "end": 359.96000000000004, "text": " once. To train the model, that all happens in your browser so no servers are involved,", "tokens": [1564, 13, 1407, 3847, 264, 2316, 11, 300, 439, 2314, 294, 428, 11185, 370, 572, 15909, 366, 3288, 11], "temperature": 0.0, "avg_logprob": -0.18622940465023644, "compression_ratio": 1.6875, "no_speech_prob": 0.00031433795811608434}, {"id": 63, "seek": 33592, "start": 359.96000000000004, "end": 365.04, "text": " no images are transferred outside your desktop. And this website is also open source so you", "tokens": [572, 5267, 366, 15809, 2380, 428, 14502, 13, 400, 341, 3144, 307, 611, 1269, 4009, 370, 291], "temperature": 0.0, "avg_logprob": -0.18622940465023644, "compression_ratio": 1.6875, "no_speech_prob": 0.00031433795811608434}, {"id": 64, "seek": 36504, "start": 365.04, "end": 372.32, "text": " can just check. There is the link on the FOSDEM23 website. So we visit the website and we start", "tokens": [393, 445, 1520, 13, 821, 307, 264, 2113, 322, 264, 479, 4367, 35, 6683, 9356, 3144, 13, 407, 321, 3441, 264, 3144, 293, 321, 722], "temperature": 0.0, "avg_logprob": -0.12222698427015735, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0001880841882666573}, {"id": 65, "seek": 36504, "start": 372.32, "end": 377.04, "text": " first creating the images group. So in this case we will be creating one image group for me so I'm", "tokens": [700, 4084, 264, 5267, 1594, 13, 407, 294, 341, 1389, 321, 486, 312, 4084, 472, 3256, 1594, 337, 385, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.12222698427015735, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0001880841882666573}, {"id": 66, "seek": 36504, "start": 377.04, "end": 382.24, "text": " looking at the baby and not me I'm hiding from the baby. If for example we are in a biology class", "tokens": [1237, 412, 264, 3186, 293, 406, 385, 286, 478, 10596, 490, 264, 3186, 13, 759, 337, 1365, 321, 366, 294, 257, 14956, 1508], "temperature": 0.0, "avg_logprob": -0.12222698427015735, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0001880841882666573}, {"id": 67, "seek": 36504, "start": 382.24, "end": 387.36, "text": " and we want to classify trees, we will be creating one group of images for each kind of tree so we", "tokens": [293, 321, 528, 281, 33872, 5852, 11, 321, 486, 312, 4084, 472, 1594, 295, 5267, 337, 1184, 733, 295, 4230, 370, 321], "temperature": 0.0, "avg_logprob": -0.12222698427015735, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0001880841882666573}, {"id": 68, "seek": 36504, "start": 387.36, "end": 394.24, "text": " can recognize them later on. Then we will turn on the camera to take a photo of myself setting", "tokens": [393, 5521, 552, 1780, 322, 13, 1396, 321, 486, 1261, 322, 264, 2799, 281, 747, 257, 5052, 295, 2059, 3287], "temperature": 0.0, "avg_logprob": -0.12222698427015735, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0001880841882666573}, {"id": 69, "seek": 39424, "start": 394.24, "end": 399.44, "text": " the group of images that I'm going to be saving this. So if I'm looking at the camera, it's going", "tokens": [264, 1594, 295, 5267, 300, 286, 478, 516, 281, 312, 6816, 341, 13, 407, 498, 286, 478, 1237, 412, 264, 2799, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.13346098339746867, "compression_ratio": 1.9835390946502058, "no_speech_prob": 0.00017959285469260067}, {"id": 70, "seek": 39424, "start": 399.44, "end": 405.04, "text": " to be the not me group. If I'm looking at the camera, it's going to be the me group. This is the same", "tokens": [281, 312, 264, 406, 385, 1594, 13, 759, 286, 478, 1237, 412, 264, 2799, 11, 309, 311, 516, 281, 312, 264, 385, 1594, 13, 639, 307, 264, 912], "temperature": 0.0, "avg_logprob": -0.13346098339746867, "compression_ratio": 1.9835390946502058, "no_speech_prob": 0.00017959285469260067}, {"id": 71, "seek": 39424, "start": 405.04, "end": 409.84000000000003, "text": " for the not me. And once that is done, when we have a reasonable amount of images for each group,", "tokens": [337, 264, 406, 385, 13, 400, 1564, 300, 307, 1096, 11, 562, 321, 362, 257, 10585, 2372, 295, 5267, 337, 1184, 1594, 11], "temperature": 0.0, "avg_logprob": -0.13346098339746867, "compression_ratio": 1.9835390946502058, "no_speech_prob": 0.00017959285469260067}, {"id": 72, "seek": 39424, "start": 409.84000000000003, "end": 414.56, "text": " which should be around 5 or 10 images for each group, we can train the model. As again, this", "tokens": [597, 820, 312, 926, 1025, 420, 1266, 5267, 337, 1184, 1594, 11, 321, 393, 3847, 264, 2316, 13, 1018, 797, 11, 341], "temperature": 0.0, "avg_logprob": -0.13346098339746867, "compression_ratio": 1.9835390946502058, "no_speech_prob": 0.00017959285469260067}, {"id": 73, "seek": 39424, "start": 414.56, "end": 421.04, "text": " training happens in your computer so no images transferred outside of your computer. We can", "tokens": [3097, 2314, 294, 428, 3820, 370, 572, 5267, 15809, 2380, 295, 428, 3820, 13, 492, 393], "temperature": 0.0, "avg_logprob": -0.13346098339746867, "compression_ratio": 1.9835390946502058, "no_speech_prob": 0.00017959285469260067}, {"id": 74, "seek": 42104, "start": 421.04, "end": 425.52000000000004, "text": " then test the model to with new images to make sure that we have properly trained the model and", "tokens": [550, 1500, 264, 2316, 281, 365, 777, 5267, 281, 652, 988, 300, 321, 362, 6108, 8895, 264, 2316, 293], "temperature": 0.0, "avg_logprob": -0.1867805329879912, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0004262056609150022}, {"id": 75, "seek": 42104, "start": 425.52000000000004, "end": 431.68, "text": " can identify ourselves. And once that is done, we can export the model and load it into App Inventor.", "tokens": [393, 5876, 4175, 13, 400, 1564, 300, 307, 1096, 11, 321, 393, 10725, 264, 2316, 293, 3677, 309, 666, 3132, 682, 2475, 284, 13], "temperature": 0.0, "avg_logprob": -0.1867805329879912, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0004262056609150022}, {"id": 76, "seek": 42104, "start": 431.68, "end": 439.12, "text": " So I'm going to be doing that very quickly. Go to foster203appinventor.mit.edu. I open the", "tokens": [407, 286, 478, 516, 281, 312, 884, 300, 588, 2661, 13, 1037, 281, 17114, 2009, 18, 1746, 259, 2475, 284, 13, 3508, 13, 22938, 13, 286, 1269, 264], "temperature": 0.0, "avg_logprob": -0.1867805329879912, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0004262056609150022}, {"id": 77, "seek": 42104, "start": 439.12, "end": 445.76, "text": " classifier instance. It's going to ask for permission to use the webcam. There it is,", "tokens": [1508, 9902, 5197, 13, 467, 311, 516, 281, 1029, 337, 11226, 281, 764, 264, 39490, 13, 821, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.1867805329879912, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0004262056609150022}, {"id": 78, "seek": 44576, "start": 445.76, "end": 452.32, "text": " I just accepted it before. So the first step is creating the labels. First the me label, enter,", "tokens": [286, 445, 9035, 309, 949, 13, 407, 264, 700, 1823, 307, 4084, 264, 16949, 13, 2386, 264, 385, 7645, 11, 3242, 11], "temperature": 0.0, "avg_logprob": -0.21466269700423532, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00025688321329653263}, {"id": 79, "seek": 45232, "start": 452.32, "end": 478.71999999999997, "text": " and next the not me label. And well, the light is quite hard. I think it was...", "tokens": [50364, 293, 958, 264, 406, 385, 7645, 13, 400, 731, 11, 264, 1442, 307, 1596, 1152, 13, 286, 519, 309, 390, 485, 51684], "temperature": 0.2, "avg_logprob": -0.38532034556070965, "compression_ratio": 1.0533333333333332, "no_speech_prob": 0.0012912291567772627}, {"id": 80, "seek": 48232, "start": 483.04, "end": 487.68, "text": " Take a few more images of me like looking to different places so I can train better the model.", "tokens": [3664, 257, 1326, 544, 5267, 295, 385, 411, 1237, 281, 819, 3190, 370, 286, 393, 3847, 1101, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11198679606119792, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.03600437939167023}, {"id": 81, "seek": 48232, "start": 491.44, "end": 497.44, "text": " One more, one more. So seven images should be fine for this demo to not take too much time.", "tokens": [1485, 544, 11, 472, 544, 13, 407, 3407, 5267, 820, 312, 2489, 337, 341, 10723, 281, 406, 747, 886, 709, 565, 13], "temperature": 0.0, "avg_logprob": -0.11198679606119792, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.03600437939167023}, {"id": 82, "seek": 48232, "start": 498.15999999999997, "end": 504.88, "text": " And now for the not me, I'm going to take like one photo of me not being there, basically.", "tokens": [400, 586, 337, 264, 406, 385, 11, 286, 478, 516, 281, 747, 411, 472, 5052, 295, 385, 406, 885, 456, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.11198679606119792, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.03600437939167023}, {"id": 83, "seek": 48232, "start": 504.88, "end": 510.64, "text": " I'm going to be using the right hand to hide myself so I can put this one in front of my eyes,", "tokens": [286, 478, 516, 281, 312, 1228, 264, 558, 1011, 281, 6479, 2059, 370, 286, 393, 829, 341, 472, 294, 1868, 295, 452, 2575, 11], "temperature": 0.0, "avg_logprob": -0.11198679606119792, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.03600437939167023}, {"id": 84, "seek": 51064, "start": 510.64, "end": 520.16, "text": " turn it upside down, diagonal, like more images, the other hand as well, like that.", "tokens": [1261, 309, 14119, 760, 11, 21539, 11, 411, 544, 5267, 11, 264, 661, 1011, 382, 731, 11, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14306036631266275, "compression_ratio": 1.463276836158192, "no_speech_prob": 9.569557732902467e-05}, {"id": 85, "seek": 51064, "start": 522.0, "end": 526.64, "text": " So that should be enough for now. And once that is done, we can just hit the train button", "tokens": [407, 300, 820, 312, 1547, 337, 586, 13, 400, 1564, 300, 307, 1096, 11, 321, 393, 445, 2045, 264, 3847, 2960], "temperature": 0.0, "avg_logprob": -0.14306036631266275, "compression_ratio": 1.463276836158192, "no_speech_prob": 9.569557732902467e-05}, {"id": 86, "seek": 51064, "start": 526.64, "end": 532.16, "text": " to train this model. It will be built on your local machine without sending anywhere.", "tokens": [281, 3847, 341, 2316, 13, 467, 486, 312, 3094, 322, 428, 2654, 3479, 1553, 7750, 4992, 13], "temperature": 0.0, "avg_logprob": -0.14306036631266275, "compression_ratio": 1.463276836158192, "no_speech_prob": 9.569557732902467e-05}, {"id": 87, "seek": 53216, "start": 532.16, "end": 539.6, "text": " Okay, this was faster last night.", "tokens": [1033, 11, 341, 390, 4663, 1036, 1818, 13], "temperature": 0.0, "avg_logprob": -0.15861853634018497, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.0001394005084875971}, {"id": 88, "seek": 53216, "start": 543.04, "end": 547.68, "text": " Seems like the time that we saved from the loading the project, we lost it here. So now it's training.", "tokens": [22524, 411, 264, 565, 300, 321, 6624, 490, 264, 15114, 264, 1716, 11, 321, 2731, 309, 510, 13, 407, 586, 309, 311, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15861853634018497, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.0001394005084875971}, {"id": 89, "seek": 53216, "start": 552.0, "end": 556.0799999999999, "text": " Yeah, it's my laptop. So this is a React app that has been open sourced and the only internet", "tokens": [865, 11, 309, 311, 452, 10732, 13, 407, 341, 307, 257, 30644, 724, 300, 575, 668, 1269, 11006, 1232, 293, 264, 787, 4705], "temperature": 0.0, "avg_logprob": -0.15861853634018497, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.0001394005084875971}, {"id": 90, "seek": 53216, "start": 556.0799999999999, "end": 560.3199999999999, "text": " required is to just get the initial webpage later on. We can just disconnect and it will", "tokens": [4739, 307, 281, 445, 483, 264, 5883, 37852, 1780, 322, 13, 492, 393, 445, 14299, 293, 309, 486], "temperature": 0.0, "avg_logprob": -0.15861853634018497, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.0001394005084875971}, {"id": 91, "seek": 56032, "start": 560.32, "end": 565.5200000000001, "text": " work perfectly. It's just offline training. If you really want to build it fully offline,", "tokens": [589, 6239, 13, 467, 311, 445, 21857, 3097, 13, 759, 291, 534, 528, 281, 1322, 309, 4498, 21857, 11], "temperature": 0.0, "avg_logprob": -0.10015396156696359, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0002283417124999687}, {"id": 92, "seek": 56032, "start": 565.5200000000001, "end": 571.36, "text": " you can just launch the React app locally and it will work. So now the model is built and to", "tokens": [291, 393, 445, 4025, 264, 30644, 724, 16143, 293, 309, 486, 589, 13, 407, 586, 264, 2316, 307, 3094, 293, 281], "temperature": 0.0, "avg_logprob": -0.10015396156696359, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0002283417124999687}, {"id": 93, "seek": 56032, "start": 571.36, "end": 578.08, "text": " test it, I'm going to be looking at the camera as I did before. I captured the image and I can see", "tokens": [1500, 309, 11, 286, 478, 516, 281, 312, 1237, 412, 264, 2799, 382, 286, 630, 949, 13, 286, 11828, 264, 3256, 293, 286, 393, 536], "temperature": 0.0, "avg_logprob": -0.10015396156696359, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0002283417124999687}, {"id": 94, "seek": 56032, "start": 578.08, "end": 584.8000000000001, "text": " that there is a 99.42 confidence that I'm looking at the camera. If I take a photo of myself hiding", "tokens": [300, 456, 307, 257, 11803, 13, 15628, 6687, 300, 286, 478, 1237, 412, 264, 2799, 13, 759, 286, 747, 257, 5052, 295, 2059, 10596], "temperature": 0.0, "avg_logprob": -0.10015396156696359, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0002283417124999687}, {"id": 95, "seek": 58480, "start": 584.8, "end": 593.5999999999999, "text": " from it, there is a 99.33 confidence that I'm not looking at the baby. So once we have validated", "tokens": [490, 309, 11, 456, 307, 257, 11803, 13, 10191, 6687, 300, 286, 478, 406, 1237, 412, 264, 3186, 13, 407, 1564, 321, 362, 40693], "temperature": 0.0, "avg_logprob": -0.09281344640822638, "compression_ratio": 1.5396825396825398, "no_speech_prob": 9.225675603374839e-05}, {"id": 96, "seek": 58480, "start": 593.5999999999999, "end": 601.92, "text": " the model properly, we can export it to the app. And we will get this model.mdl file for AppInventor.", "tokens": [264, 2316, 6108, 11, 321, 393, 10725, 309, 281, 264, 724, 13, 400, 321, 486, 483, 341, 2316, 13, 76, 67, 75, 3991, 337, 3132, 4575, 2475, 284, 13], "temperature": 0.0, "avg_logprob": -0.09281344640822638, "compression_ratio": 1.5396825396825398, "no_speech_prob": 9.225675603374839e-05}, {"id": 97, "seek": 58480, "start": 603.8399999999999, "end": 612.3199999999999, "text": " So let's go to the presentation. And once we have the model, it's time to code the app using", "tokens": [407, 718, 311, 352, 281, 264, 5860, 13, 400, 1564, 321, 362, 264, 2316, 11, 309, 311, 565, 281, 3089, 264, 724, 1228], "temperature": 0.0, "avg_logprob": -0.09281344640822638, "compression_ratio": 1.5396825396825398, "no_speech_prob": 9.225675603374839e-05}, {"id": 98, "seek": 61232, "start": 612.32, "end": 617.12, "text": " blocks. I'm not going to go through the slides anymore for people at home. If you have internet", "tokens": [8474, 13, 286, 478, 406, 516, 281, 352, 807, 264, 9788, 3602, 337, 561, 412, 1280, 13, 759, 291, 362, 4705], "temperature": 0.0, "avg_logprob": -0.12473625090064072, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.0003751364420168102}, {"id": 99, "seek": 61232, "start": 617.12, "end": 621.2800000000001, "text": " problems or the streaming is down, feel free to follow the slides. It's a step-by-step guide.", "tokens": [2740, 420, 264, 11791, 307, 760, 11, 841, 1737, 281, 1524, 264, 9788, 13, 467, 311, 257, 1823, 12, 2322, 12, 16792, 5934, 13], "temperature": 0.0, "avg_logprob": -0.12473625090064072, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.0003751364420168102}, {"id": 100, "seek": 61232, "start": 621.84, "end": 628.1600000000001, "text": " But for here, I'm going to be showing the tutorial live. So let's go back to the", "tokens": [583, 337, 510, 11, 286, 478, 516, 281, 312, 4099, 264, 7073, 1621, 13, 407, 718, 311, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.12473625090064072, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.0003751364420168102}, {"id": 101, "seek": 61232, "start": 628.1600000000001, "end": 633.0400000000001, "text": " project that we just loaded. And right here, we can see a quick description of the project of what", "tokens": [1716, 300, 321, 445, 13210, 13, 400, 558, 510, 11, 321, 393, 536, 257, 1702, 3855, 295, 264, 1716, 295, 437], "temperature": 0.0, "avg_logprob": -0.12473625090064072, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.0003751364420168102}, {"id": 102, "seek": 61232, "start": 633.0400000000001, "end": 639.5200000000001, "text": " we are going to do. A set up of the computer of how to connect to the MIT instance to the app.", "tokens": [321, 366, 516, 281, 360, 13, 316, 992, 493, 295, 264, 3820, 295, 577, 281, 1745, 281, 264, 13100, 5197, 281, 264, 724, 13], "temperature": 0.0, "avg_logprob": -0.12473625090064072, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.0003751364420168102}, {"id": 103, "seek": 63952, "start": 639.52, "end": 646.88, "text": " I'm going to show that at the end of the presentation. And we have here the pickable example.", "tokens": [286, 478, 516, 281, 855, 300, 412, 264, 917, 295, 264, 5860, 13, 400, 321, 362, 510, 264, 1888, 712, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11717022296994231, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00021410557383205742}, {"id": 104, "seek": 63952, "start": 647.52, "end": 654.3199999999999, "text": " This is the final result. This is one of the MIT curriculum developers that made the original", "tokens": [639, 307, 264, 2572, 1874, 13, 639, 307, 472, 295, 264, 13100, 14302, 8849, 300, 1027, 264, 3380], "temperature": 0.0, "avg_logprob": -0.11717022296994231, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00021410557383205742}, {"id": 105, "seek": 63952, "start": 654.3199999999999, "end": 660.56, "text": " tutorial. And yeah, basically it says that we will be using the personally much classified", "tokens": [7073, 13, 400, 1338, 11, 1936, 309, 1619, 300, 321, 486, 312, 1228, 264, 5665, 709, 20627], "temperature": 0.0, "avg_logprob": -0.11717022296994231, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00021410557383205742}, {"id": 106, "seek": 63952, "start": 660.56, "end": 668.0799999999999, "text": " extension that was developed by that research of MIT. And the first step is loading, turning the", "tokens": [10320, 300, 390, 4743, 538, 300, 2132, 295, 13100, 13, 400, 264, 700, 1823, 307, 15114, 11, 6246, 264], "temperature": 0.0, "avg_logprob": -0.11717022296994231, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00021410557383205742}, {"id": 107, "seek": 66808, "start": 668.08, "end": 673.36, "text": " model. And then we have to upload the model. To upload the model, we go to this section over here", "tokens": [2316, 13, 400, 550, 321, 362, 281, 6580, 264, 2316, 13, 1407, 6580, 264, 2316, 11, 321, 352, 281, 341, 3541, 670, 510], "temperature": 0.0, "avg_logprob": -0.1030222541407535, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00023170094937086105}, {"id": 108, "seek": 66808, "start": 673.36, "end": 681.9200000000001, "text": " on the media. We select the just downloaded model file. It should be here. And now it's", "tokens": [322, 264, 3021, 13, 492, 3048, 264, 445, 21748, 2316, 3991, 13, 467, 820, 312, 510, 13, 400, 586, 309, 311], "temperature": 0.0, "avg_logprob": -0.1030222541407535, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00023170094937086105}, {"id": 109, "seek": 66808, "start": 681.9200000000001, "end": 688.88, "text": " uploaded. It's in the asset file of the app. And we can just change the model of the image", "tokens": [17135, 13, 467, 311, 294, 264, 11999, 3991, 295, 264, 724, 13, 400, 321, 393, 445, 1319, 264, 2316, 295, 264, 3256], "temperature": 0.0, "avg_logprob": -0.1030222541407535, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00023170094937086105}, {"id": 110, "seek": 66808, "start": 688.88, "end": 696.88, "text": " property, personal image classifier to the now loaded new model. And we have just loaded the", "tokens": [4707, 11, 2973, 3256, 1508, 9902, 281, 264, 586, 13210, 777, 2316, 13, 400, 321, 362, 445, 13210, 264], "temperature": 0.0, "avg_logprob": -0.1030222541407535, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00023170094937086105}, {"id": 111, "seek": 69688, "start": 696.88, "end": 702.08, "text": " model properly. To give an overview of how the app is going to look like, there is going to be", "tokens": [2316, 6108, 13, 1407, 976, 364, 12492, 295, 577, 264, 724, 307, 516, 281, 574, 411, 11, 456, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.09984785112841375, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.00013452178973238915}, {"id": 112, "seek": 69688, "start": 702.08, "end": 706.8, "text": " this status label that will tell the user when the app is ready to work. Now it says loading,", "tokens": [341, 6558, 7645, 300, 486, 980, 264, 4195, 562, 264, 724, 307, 1919, 281, 589, 13, 823, 309, 1619, 15114, 11], "temperature": 0.0, "avg_logprob": -0.09984785112841375, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.00013452178973238915}, {"id": 113, "seek": 69688, "start": 706.8, "end": 711.36, "text": " because it's the initial state. It will let it go to the ready state and it will just identify", "tokens": [570, 309, 311, 264, 5883, 1785, 13, 467, 486, 718, 309, 352, 281, 264, 1919, 1785, 293, 309, 486, 445, 5876], "temperature": 0.0, "avg_logprob": -0.09984785112841375, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.00013452178973238915}, {"id": 114, "seek": 69688, "start": 711.36, "end": 718.72, "text": " the faces. We have these two bars that will be showing which percentage of confidence we are", "tokens": [264, 8475, 13, 492, 362, 613, 732, 10228, 300, 486, 312, 4099, 597, 9668, 295, 6687, 321, 366], "temperature": 0.0, "avg_logprob": -0.09984785112841375, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.00013452178973238915}, {"id": 115, "seek": 69688, "start": 718.72, "end": 724.64, "text": " that we are looking at the baby or not. This is going to be the live image from the camera that", "tokens": [300, 321, 366, 1237, 412, 264, 3186, 420, 406, 13, 639, 307, 516, 281, 312, 264, 1621, 3256, 490, 264, 2799, 300], "temperature": 0.0, "avg_logprob": -0.09984785112841375, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.00013452178973238915}, {"id": 116, "seek": 72464, "start": 724.64, "end": 729.68, "text": " I just showed before. And these are the interaction buttons to start the classification,", "tokens": [286, 445, 4712, 949, 13, 400, 613, 366, 264, 9285, 9905, 281, 722, 264, 21538, 11], "temperature": 0.0, "avg_logprob": -0.11234710413381593, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0002401606907369569}, {"id": 117, "seek": 72464, "start": 729.68, "end": 734.0, "text": " to toggle the camera from the front to the back camera. And we have here the happy baby in this", "tokens": [281, 31225, 264, 2799, 490, 264, 1868, 281, 264, 646, 2799, 13, 400, 321, 362, 510, 264, 2055, 3186, 294, 341], "temperature": 0.0, "avg_logprob": -0.11234710413381593, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0002401606907369569}, {"id": 118, "seek": 72464, "start": 734.0, "end": 743.12, "text": " case. So uploading the turning model. This is the sequence of events that I was just talking about.", "tokens": [1389, 13, 407, 27301, 264, 6246, 2316, 13, 639, 307, 264, 8310, 295, 3931, 300, 286, 390, 445, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.11234710413381593, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0002401606907369569}, {"id": 119, "seek": 72464, "start": 744.3199999999999, "end": 748.3199999999999, "text": " First we start the app. The app will show to ready as soon as the classifier is ready to", "tokens": [2386, 321, 722, 264, 724, 13, 440, 724, 486, 855, 281, 1919, 382, 2321, 382, 264, 1508, 9902, 307, 1919, 281], "temperature": 0.0, "avg_logprob": -0.11234710413381593, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0002401606907369569}, {"id": 120, "seek": 72464, "start": 748.3199999999999, "end": 753.4399999999999, "text": " start working with the app. The user will press the start button and then the personal image", "tokens": [722, 1364, 365, 264, 724, 13, 440, 4195, 486, 1886, 264, 722, 2960, 293, 550, 264, 2973, 3256], "temperature": 0.0, "avg_logprob": -0.11234710413381593, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0002401606907369569}, {"id": 121, "seek": 75344, "start": 753.44, "end": 759.12, "text": " classifier extension will keep classifying the live stream video from the camera continuously.", "tokens": [1508, 9902, 10320, 486, 1066, 1508, 5489, 264, 1621, 4309, 960, 490, 264, 2799, 15684, 13], "temperature": 0.0, "avg_logprob": -0.10608895619710286, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0001227075990755111}, {"id": 122, "seek": 75344, "start": 759.7600000000001, "end": 764.5600000000001, "text": " And once they have identified the result, if there is a higher confidence that the me", "tokens": [400, 1564, 436, 362, 9234, 264, 1874, 11, 498, 456, 307, 257, 2946, 6687, 300, 264, 385], "temperature": 0.0, "avg_logprob": -0.10608895619710286, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0001227075990755111}, {"id": 123, "seek": 75344, "start": 765.9200000000001, "end": 771.2, "text": " model is working, we will show the smiley face, otherwise the baby will just start crying.", "tokens": [2316, 307, 1364, 11, 321, 486, 855, 264, 7563, 88, 1851, 11, 5911, 264, 3186, 486, 445, 722, 8554, 13], "temperature": 0.0, "avg_logprob": -0.10608895619710286, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0001227075990755111}, {"id": 124, "seek": 75344, "start": 774.1600000000001, "end": 780.08, "text": " So in this template, there is already a set of blogs that are available to speed up the process.", "tokens": [407, 294, 341, 12379, 11, 456, 307, 1217, 257, 992, 295, 31038, 300, 366, 2435, 281, 3073, 493, 264, 1399, 13], "temperature": 0.0, "avg_logprob": -0.10608895619710286, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0001227075990755111}, {"id": 125, "seek": 78008, "start": 780.08, "end": 785.6, "text": " And we can go here that we see the one personal image classifier error. So that means that if for", "tokens": [400, 321, 393, 352, 510, 300, 321, 536, 264, 472, 2973, 3256, 1508, 9902, 6713, 13, 407, 300, 1355, 300, 498, 337], "temperature": 0.0, "avg_logprob": -0.1548084020614624, "compression_ratio": 2.008403361344538, "no_speech_prob": 0.00010970039875246584}, {"id": 126, "seek": 78008, "start": 785.6, "end": 790.4000000000001, "text": " any reason the personal image classifier shows an error, maybe because there are some missing", "tokens": [604, 1778, 264, 2973, 3256, 1508, 9902, 3110, 364, 6713, 11, 1310, 570, 456, 366, 512, 5361], "temperature": 0.0, "avg_logprob": -0.1548084020614624, "compression_ratio": 2.008403361344538, "no_speech_prob": 0.00010970039875246584}, {"id": 127, "seek": 78008, "start": 790.4000000000001, "end": 795.44, "text": " things on the phone or whatever, we will set the status label text to the actual error that we", "tokens": [721, 322, 264, 2593, 420, 2035, 11, 321, 486, 992, 264, 6558, 7645, 2487, 281, 264, 3539, 6713, 300, 321], "temperature": 0.0, "avg_logprob": -0.1548084020614624, "compression_ratio": 2.008403361344538, "no_speech_prob": 0.00010970039875246584}, {"id": 128, "seek": 78008, "start": 795.44, "end": 801.44, "text": " return from the image classifier. Once the image classifier is ready, we will enable the start", "tokens": [2736, 490, 264, 3256, 1508, 9902, 13, 3443, 264, 3256, 1508, 9902, 307, 1919, 11, 321, 486, 9528, 264, 722], "temperature": 0.0, "avg_logprob": -0.1548084020614624, "compression_ratio": 2.008403361344538, "no_speech_prob": 0.00010970039875246584}, {"id": 129, "seek": 78008, "start": 801.44, "end": 807.6800000000001, "text": " button as well as the toggle camera button. We will set the status label text to be ready so the", "tokens": [2960, 382, 731, 382, 264, 31225, 2799, 2960, 13, 492, 486, 992, 264, 6558, 7645, 2487, 281, 312, 1919, 370, 264], "temperature": 0.0, "avg_logprob": -0.1548084020614624, "compression_ratio": 2.008403361344538, "no_speech_prob": 0.00010970039875246584}, {"id": 130, "seek": 80768, "start": 807.68, "end": 813.1999999999999, "text": " user knows that they can start using the app. And we will set the text boxes of each classification", "tokens": [4195, 3255, 300, 436, 393, 722, 1228, 264, 724, 13, 400, 321, 486, 992, 264, 2487, 9002, 295, 1184, 21538], "temperature": 0.0, "avg_logprob": -0.1433489481608073, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0001686672621872276}, {"id": 131, "seek": 80768, "start": 813.1999999999999, "end": 819.92, "text": " group to the previously defined labels me and not me in this case. If the user presses the toggle", "tokens": [1594, 281, 264, 8046, 7642, 16949, 385, 293, 406, 385, 294, 341, 1389, 13, 759, 264, 4195, 40892, 264, 31225], "temperature": 0.0, "avg_logprob": -0.1433489481608073, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0001686672621872276}, {"id": 132, "seek": 80768, "start": 819.92, "end": 825.3599999999999, "text": " camera button, we will be changing from the front to the back camera just every time that they press", "tokens": [2799, 2960, 11, 321, 486, 312, 4473, 490, 264, 1868, 281, 264, 646, 2799, 445, 633, 565, 300, 436, 1886], "temperature": 0.0, "avg_logprob": -0.1433489481608073, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0001686672621872276}, {"id": 133, "seek": 80768, "start": 825.3599999999999, "end": 833.04, "text": " so we can use the front selfie camera or the back normal camera. And once the user presses the", "tokens": [370, 321, 393, 764, 264, 1868, 22147, 2799, 420, 264, 646, 2710, 2799, 13, 400, 1564, 264, 4195, 40892, 264], "temperature": 0.0, "avg_logprob": -0.1433489481608073, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0001686672621872276}, {"id": 134, "seek": 83304, "start": 833.04, "end": 839.8399999999999, "text": " start button, if the personal image classifier is already classifying an image, we will just", "tokens": [722, 2960, 11, 498, 264, 2973, 3256, 1508, 9902, 307, 1217, 1508, 5489, 364, 3256, 11, 321, 486, 445], "temperature": 0.0, "avg_logprob": -0.11634125106636135, "compression_ratio": 1.8413461538461537, "no_speech_prob": 3.599312549340539e-05}, {"id": 135, "seek": 83304, "start": 839.8399999999999, "end": 844.9599999999999, "text": " stop it and we will show the start button with the start text. Otherwise, we have to start the", "tokens": [1590, 309, 293, 321, 486, 855, 264, 722, 2960, 365, 264, 722, 2487, 13, 10328, 11, 321, 362, 281, 722, 264], "temperature": 0.0, "avg_logprob": -0.11634125106636135, "compression_ratio": 1.8413461538461537, "no_speech_prob": 3.599312549340539e-05}, {"id": 136, "seek": 83304, "start": 844.9599999999999, "end": 850.64, "text": " classification. So to do so, we just invoke the start continuous classification method and we", "tokens": [21538, 13, 407, 281, 360, 370, 11, 321, 445, 41117, 264, 722, 10957, 21538, 3170, 293, 321], "temperature": 0.0, "avg_logprob": -0.11634125106636135, "compression_ratio": 1.8413461538461537, "no_speech_prob": 3.599312549340539e-05}, {"id": 137, "seek": 83304, "start": 850.64, "end": 856.7199999999999, "text": " change the text to stop because we will be changing the the button interactions. And that's the quick", "tokens": [1319, 264, 2487, 281, 1590, 570, 321, 486, 312, 4473, 264, 264, 2960, 13280, 13, 400, 300, 311, 264, 1702], "temperature": 0.0, "avg_logprob": -0.11634125106636135, "compression_ratio": 1.8413461538461537, "no_speech_prob": 3.599312549340539e-05}, {"id": 138, "seek": 85672, "start": 856.72, "end": 864.1600000000001, "text": " overview of the code that is already available in the in the app. So how does the image classification", "tokens": [12492, 295, 264, 3089, 300, 307, 1217, 2435, 294, 264, 294, 264, 724, 13, 407, 577, 775, 264, 3256, 21538], "temperature": 0.0, "avg_logprob": -0.15859310278731786, "compression_ratio": 1.75, "no_speech_prob": 0.00012534244160633534}, {"id": 139, "seek": 85672, "start": 864.1600000000001, "end": 869.6, "text": " work in MIT Preventor? Well, we have this big block that is the when personal image classifier", "tokens": [589, 294, 13100, 6001, 2475, 284, 30, 1042, 11, 321, 362, 341, 955, 3461, 300, 307, 264, 562, 2973, 3256, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.15859310278731786, "compression_ratio": 1.75, "no_speech_prob": 0.00012534244160633534}, {"id": 140, "seek": 85672, "start": 870.32, "end": 877.12, "text": " has received a classification succeeded. We will receive this result variable. This result variable", "tokens": [575, 4613, 257, 21538, 20263, 13, 492, 486, 4774, 341, 1874, 7006, 13, 639, 1874, 7006], "temperature": 0.0, "avg_logprob": -0.15859310278731786, "compression_ratio": 1.75, "no_speech_prob": 0.00012534244160633534}, {"id": 141, "seek": 85672, "start": 877.12, "end": 883.12, "text": " is a dictionary that to just give a little high level overview of what is a dictionary. It's a", "tokens": [307, 257, 25890, 300, 281, 445, 976, 257, 707, 1090, 1496, 12492, 295, 437, 307, 257, 25890, 13, 467, 311, 257], "temperature": 0.0, "avg_logprob": -0.15859310278731786, "compression_ratio": 1.75, "no_speech_prob": 0.00012534244160633534}, {"id": 142, "seek": 88312, "start": 883.12, "end": 889.92, "text": " key value list of elements. So if we have two different groups, me and not me, we will receive", "tokens": [2141, 2158, 1329, 295, 4959, 13, 407, 498, 321, 362, 732, 819, 3935, 11, 385, 293, 406, 385, 11, 321, 486, 4774], "temperature": 0.0, "avg_logprob": -0.09731637825400143, "compression_ratio": 2.0085470085470085, "no_speech_prob": 0.0002490264014340937}, {"id": 143, "seek": 88312, "start": 889.92, "end": 894.8, "text": " me equals a specific value, not me equals a specific value. If we have three groups, we have", "tokens": [385, 6915, 257, 2685, 2158, 11, 406, 385, 6915, 257, 2685, 2158, 13, 759, 321, 362, 1045, 3935, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.09731637825400143, "compression_ratio": 2.0085470085470085, "no_speech_prob": 0.0002490264014340937}, {"id": 144, "seek": 88312, "start": 894.8, "end": 900.88, "text": " one, two, and three that they each equal to a specific values. This is a little example of how", "tokens": [472, 11, 732, 11, 293, 1045, 300, 436, 1184, 2681, 281, 257, 2685, 4190, 13, 639, 307, 257, 707, 1365, 295, 577], "temperature": 0.0, "avg_logprob": -0.09731637825400143, "compression_ratio": 2.0085470085470085, "no_speech_prob": 0.0002490264014340937}, {"id": 145, "seek": 88312, "start": 900.88, "end": 905.84, "text": " it looks like. So we have key father equals this value, key model equals this value, then equals", "tokens": [309, 1542, 411, 13, 407, 321, 362, 2141, 3086, 6915, 341, 2158, 11, 2141, 2316, 6915, 341, 2158, 11, 550, 6915], "temperature": 0.0, "avg_logprob": -0.09731637825400143, "compression_ratio": 2.0085470085470085, "no_speech_prob": 0.0002490264014340937}, {"id": 146, "seek": 88312, "start": 905.84, "end": 911.92, "text": " this value, etc. For the image classifier, a specific example, we will have something like", "tokens": [341, 2158, 11, 5183, 13, 1171, 264, 3256, 1508, 9902, 11, 257, 2685, 1365, 11, 321, 486, 362, 746, 411], "temperature": 0.0, "avg_logprob": -0.09731637825400143, "compression_ratio": 2.0085470085470085, "no_speech_prob": 0.0002490264014340937}, {"id": 147, "seek": 91192, "start": 911.92, "end": 916.9599999999999, "text": " this. We have me with this value and not me with this other specific value.", "tokens": [341, 13, 492, 362, 385, 365, 341, 2158, 293, 406, 385, 365, 341, 661, 2685, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09706723559033739, "compression_ratio": 1.8608247422680413, "no_speech_prob": 7.093090243870392e-05}, {"id": 148, "seek": 91192, "start": 920.4, "end": 926.3199999999999, "text": " So how we can retrieve a value in the dictionaries area, in the dictionaries block area, we have", "tokens": [407, 577, 321, 393, 30254, 257, 2158, 294, 264, 22352, 4889, 1859, 11, 294, 264, 22352, 4889, 3461, 1859, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.09706723559033739, "compression_ratio": 1.8608247422680413, "no_speech_prob": 7.093090243870392e-05}, {"id": 149, "seek": 91192, "start": 926.3199999999999, "end": 931.52, "text": " get value for a specific key. And we will be doing something similar to this. So we have the", "tokens": [483, 2158, 337, 257, 2685, 2141, 13, 400, 321, 486, 312, 884, 746, 2531, 281, 341, 13, 407, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.09706723559033739, "compression_ratio": 1.8608247422680413, "no_speech_prob": 7.093090243870392e-05}, {"id": 150, "seek": 91192, "start": 931.52, "end": 936.9599999999999, "text": " original dictionary here. We are building it in this area, make a dictionary me and not me. And", "tokens": [3380, 25890, 510, 13, 492, 366, 2390, 309, 294, 341, 1859, 11, 652, 257, 25890, 385, 293, 406, 385, 13, 400], "temperature": 0.0, "avg_logprob": -0.09706723559033739, "compression_ratio": 1.8608247422680413, "no_speech_prob": 7.093090243870392e-05}, {"id": 151, "seek": 93696, "start": 936.96, "end": 942.24, "text": " we will be getting the value of the group that we want to use right now. In this case, it's the", "tokens": [321, 486, 312, 1242, 264, 2158, 295, 264, 1594, 300, 321, 528, 281, 764, 558, 586, 13, 682, 341, 1389, 11, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.06125705242156983, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.00013350536755751818}, {"id": 152, "seek": 93696, "start": 942.24, "end": 947.52, "text": " me example. If we want to take the not me, we just have to change this label to not me. And if", "tokens": [385, 1365, 13, 759, 321, 528, 281, 747, 264, 406, 385, 11, 321, 445, 362, 281, 1319, 341, 7645, 281, 406, 385, 13, 400, 498], "temperature": 0.0, "avg_logprob": -0.06125705242156983, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.00013350536755751818}, {"id": 153, "seek": 93696, "start": 947.52, "end": 953.6800000000001, "text": " we are using the wrong model because the groups are not the same, we just return a zero because", "tokens": [321, 366, 1228, 264, 2085, 2316, 570, 264, 3935, 366, 406, 264, 912, 11, 321, 445, 2736, 257, 4018, 570], "temperature": 0.0, "avg_logprob": -0.06125705242156983, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.00013350536755751818}, {"id": 154, "seek": 93696, "start": 953.6800000000001, "end": 962.24, "text": " we cannot classify that group. So let's get into it. By default, the tutorial will provide this", "tokens": [321, 2644, 33872, 300, 1594, 13, 407, 718, 311, 483, 666, 309, 13, 3146, 7576, 11, 264, 7073, 486, 2893, 341], "temperature": 0.0, "avg_logprob": -0.06125705242156983, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.00013350536755751818}, {"id": 155, "seek": 96224, "start": 962.24, "end": 967.2, "text": " block that is some variable, some me confidence level, and we have to complete them using this", "tokens": [3461, 300, 307, 512, 7006, 11, 512, 385, 6687, 1496, 11, 293, 321, 362, 281, 3566, 552, 1228, 341], "temperature": 0.0, "avg_logprob": -0.11601359503609794, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0002936151868198067}, {"id": 156, "seek": 96224, "start": 967.2, "end": 973.6800000000001, "text": " block. So to do so, we will take the get for key in dictionary block. We join it to the me", "tokens": [3461, 13, 407, 281, 360, 370, 11, 321, 486, 747, 264, 483, 337, 2141, 294, 25890, 3461, 13, 492, 3917, 309, 281, 264, 385], "temperature": 0.0, "avg_logprob": -0.11601359503609794, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0002936151868198067}, {"id": 157, "seek": 96224, "start": 973.6800000000001, "end": 982.72, "text": " confidence block. We remove, nice, get value for the key. And we will take from the text blocks", "tokens": [6687, 3461, 13, 492, 4159, 11, 1481, 11, 483, 2158, 337, 264, 2141, 13, 400, 321, 486, 747, 490, 264, 2487, 8474], "temperature": 0.0, "avg_logprob": -0.11601359503609794, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0002936151868198067}, {"id": 158, "seek": 96224, "start": 983.28, "end": 989.92, "text": " an empty text block to touch it right here. And we can type me. So we can get the me group", "tokens": [364, 6707, 2487, 3461, 281, 2557, 309, 558, 510, 13, 400, 321, 393, 2010, 385, 13, 407, 321, 393, 483, 264, 385, 1594], "temperature": 0.0, "avg_logprob": -0.11601359503609794, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0002936151868198067}, {"id": 159, "seek": 98992, "start": 989.92, "end": 995.5999999999999, "text": " into the me confidence variable. The dictionary is the result. We can just attach it here.", "tokens": [666, 264, 385, 6687, 7006, 13, 440, 25890, 307, 264, 1874, 13, 492, 393, 445, 5085, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.09236150038869757, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.5853436505422e-05}, {"id": 160, "seek": 98992, "start": 996.4799999999999, "end": 1003.36, "text": " And if not found, we will just returning an empty zero value. And for the not me confidence,", "tokens": [400, 498, 406, 1352, 11, 321, 486, 445, 12678, 364, 6707, 4018, 2158, 13, 400, 337, 264, 406, 385, 6687, 11], "temperature": 0.0, "avg_logprob": -0.09236150038869757, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.5853436505422e-05}, {"id": 161, "seek": 98992, "start": 1003.36, "end": 1008.9599999999999, "text": " it's basically the same. So we can copy paste the blocks. We attach them to the not me confidence", "tokens": [309, 311, 1936, 264, 912, 13, 407, 321, 393, 5055, 9163, 264, 8474, 13, 492, 5085, 552, 281, 264, 406, 385, 6687], "temperature": 0.0, "avg_logprob": -0.09236150038869757, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.5853436505422e-05}, {"id": 162, "seek": 98992, "start": 1008.9599999999999, "end": 1016.48, "text": " area. And we just have to prepend a note in front of the me. And now we have just defined", "tokens": [1859, 13, 400, 321, 445, 362, 281, 2666, 521, 257, 3637, 294, 1868, 295, 264, 385, 13, 400, 586, 321, 362, 445, 7642], "temperature": 0.0, "avg_logprob": -0.09236150038869757, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.5853436505422e-05}, {"id": 163, "seek": 101648, "start": 1016.48, "end": 1021.84, "text": " that me confidence variable that can be accessed like that. We'll have the percentage of confidence", "tokens": [300, 385, 6687, 7006, 300, 393, 312, 34211, 411, 300, 13, 492, 603, 362, 264, 9668, 295, 6687], "temperature": 0.0, "avg_logprob": -0.163357675075531, "compression_ratio": 1.9095477386934674, "no_speech_prob": 8.454016642645001e-05}, {"id": 164, "seek": 101648, "start": 1022.48, "end": 1027.6, "text": " that we are looking at the baby. And the not me confidence, it's the opposite. It's how", "tokens": [300, 321, 366, 1237, 412, 264, 3186, 13, 400, 264, 406, 385, 6687, 11, 309, 311, 264, 6182, 13, 467, 311, 577], "temperature": 0.0, "avg_logprob": -0.163357675075531, "compression_ratio": 1.9095477386934674, "no_speech_prob": 8.454016642645001e-05}, {"id": 165, "seek": 101648, "start": 1027.6, "end": 1034.16, "text": " confidence we are that we are not looking at the baby. The next step, the interesting variables.", "tokens": [6687, 321, 366, 300, 321, 366, 406, 1237, 412, 264, 3186, 13, 440, 958, 1823, 11, 264, 1880, 9102, 13], "temperature": 0.0, "avg_logprob": -0.163357675075531, "compression_ratio": 1.9095477386934674, "no_speech_prob": 8.454016642645001e-05}, {"id": 166, "seek": 101648, "start": 1035.1200000000001, "end": 1041.44, "text": " And now we just have to recap what do we have to do in the app. So in the app, we have to first", "tokens": [400, 586, 321, 445, 362, 281, 20928, 437, 360, 321, 362, 281, 360, 294, 264, 724, 13, 407, 294, 264, 724, 11, 321, 362, 281, 700], "temperature": 0.0, "avg_logprob": -0.163357675075531, "compression_ratio": 1.9095477386934674, "no_speech_prob": 8.454016642645001e-05}, {"id": 167, "seek": 104144, "start": 1041.44, "end": 1048.24, "text": " update these labels here with the percentage. And we have to update these color bars with the", "tokens": [5623, 613, 16949, 510, 365, 264, 9668, 13, 400, 321, 362, 281, 5623, 613, 2017, 10228, 365, 264], "temperature": 0.0, "avg_logprob": -0.10497449392295745, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.00021134276175871491}, {"id": 168, "seek": 104144, "start": 1048.24, "end": 1056.16, "text": " correct confidence levels. We can do that by going to these components, to these two horizontal", "tokens": [3006, 6687, 4358, 13, 492, 393, 360, 300, 538, 516, 281, 613, 6677, 11, 281, 613, 732, 12750], "temperature": 0.0, "avg_logprob": -0.10497449392295745, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.00021134276175871491}, {"id": 169, "seek": 104144, "start": 1056.16, "end": 1061.1200000000001, "text": " arrangements. And we have percentage one, bar graph one, percentage two and bar graph two.", "tokens": [22435, 13, 400, 321, 362, 9668, 472, 11, 2159, 4295, 472, 11, 9668, 732, 293, 2159, 4295, 732, 13], "temperature": 0.0, "avg_logprob": -0.10497449392295745, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.00021134276175871491}, {"id": 170, "seek": 104144, "start": 1062.0, "end": 1065.92, "text": " Percentage one, we can update the text to the percentage that we are showing.", "tokens": [3026, 2207, 609, 472, 11, 321, 393, 5623, 264, 2487, 281, 264, 9668, 300, 321, 366, 4099, 13], "temperature": 0.0, "avg_logprob": -0.10497449392295745, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.00021134276175871491}, {"id": 171, "seek": 106592, "start": 1065.92, "end": 1074.8000000000002, "text": " One second. There it is. So the value that we return from the dictionary goes from zero to one,", "tokens": [1485, 1150, 13, 821, 309, 307, 13, 407, 264, 2158, 300, 321, 2736, 490, 264, 25890, 1709, 490, 4018, 281, 472, 11], "temperature": 0.0, "avg_logprob": -0.13972471872965495, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00022146955598145723}, {"id": 172, "seek": 106592, "start": 1074.8000000000002, "end": 1080.4, "text": " but we want to return a percentage which goes from zero to 100. So we will take this me confidence", "tokens": [457, 321, 528, 281, 2736, 257, 9668, 597, 1709, 490, 4018, 281, 2319, 13, 407, 321, 486, 747, 341, 385, 6687], "temperature": 0.0, "avg_logprob": -0.13972471872965495, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00022146955598145723}, {"id": 173, "seek": 106592, "start": 1080.4, "end": 1088.64, "text": " value and we will multiply it by 100. So we can get the zero to 100 range. We just join it right", "tokens": [2158, 293, 321, 486, 12972, 309, 538, 2319, 13, 407, 321, 393, 483, 264, 4018, 281, 2319, 3613, 13, 492, 445, 3917, 309, 558], "temperature": 0.0, "avg_logprob": -0.13972471872965495, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00022146955598145723}, {"id": 174, "seek": 108864, "start": 1088.64, "end": 1099.3600000000001, "text": " here with the math number and we multiply it by 100, 100. But we will be missing the percentage", "tokens": [510, 365, 264, 5221, 1230, 293, 321, 12972, 309, 538, 2319, 11, 2319, 13, 583, 321, 486, 312, 5361, 264, 9668], "temperature": 0.0, "avg_logprob": -0.12228566321773805, "compression_ratio": 1.7454545454545454, "no_speech_prob": 5.780007268185727e-05}, {"id": 175, "seek": 108864, "start": 1099.3600000000001, "end": 1105.2800000000002, "text": " sign. To get the percentage sign, we can use the text blocks with the join block. So we can join two", "tokens": [1465, 13, 1407, 483, 264, 9668, 1465, 11, 321, 393, 764, 264, 2487, 8474, 365, 264, 3917, 3461, 13, 407, 321, 393, 3917, 732], "temperature": 0.0, "avg_logprob": -0.12228566321773805, "compression_ratio": 1.7454545454545454, "no_speech_prob": 5.780007268185727e-05}, {"id": 176, "seek": 108864, "start": 1106.5600000000002, "end": 1115.2, "text": " text together and we can just create a new percentage symbol like this using the percentage", "tokens": [2487, 1214, 293, 321, 393, 445, 1884, 257, 777, 9668, 5986, 411, 341, 1228, 264, 9668], "temperature": 0.0, "avg_logprob": -0.12228566321773805, "compression_ratio": 1.7454545454545454, "no_speech_prob": 5.780007268185727e-05}, {"id": 177, "seek": 111520, "start": 1115.2, "end": 1122.16, "text": " symbol. And this is for the percentage labels. For the bar graph labels, we will be pledging with", "tokens": [5986, 13, 400, 341, 307, 337, 264, 9668, 16949, 13, 1171, 264, 2159, 4295, 16949, 11, 321, 486, 312, 34263, 3249, 365], "temperature": 0.0, "avg_logprob": -0.11590782288582094, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.563240797026083e-05}, {"id": 178, "seek": 111520, "start": 1122.16, "end": 1129.04, "text": " the length of the actual graph, bar graph. To do so, we have the width percent block that can", "tokens": [264, 4641, 295, 264, 3539, 4295, 11, 2159, 4295, 13, 1407, 360, 370, 11, 321, 362, 264, 11402, 3043, 3461, 300, 393], "temperature": 0.0, "avg_logprob": -0.11590782288582094, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.563240797026083e-05}, {"id": 179, "seek": 111520, "start": 1129.04, "end": 1134.16, "text": " modify the width according to a percentage. And we already have defined the percentage right here,", "tokens": [16927, 264, 11402, 4650, 281, 257, 9668, 13, 400, 321, 1217, 362, 7642, 264, 9668, 558, 510, 11], "temperature": 0.0, "avg_logprob": -0.11590782288582094, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.563240797026083e-05}, {"id": 180, "seek": 111520, "start": 1134.16, "end": 1140.0800000000002, "text": " so we can just copy paste these blocks and attach them to the width percent. And this is for the", "tokens": [370, 321, 393, 445, 5055, 9163, 613, 8474, 293, 5085, 552, 281, 264, 11402, 3043, 13, 400, 341, 307, 337, 264], "temperature": 0.0, "avg_logprob": -0.11590782288582094, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.563240797026083e-05}, {"id": 181, "seek": 114008, "start": 1140.08, "end": 1147.36, "text": " me group. For the not me group, we can copy paste the percentage one, which changes to percentage", "tokens": [385, 1594, 13, 1171, 264, 406, 385, 1594, 11, 321, 393, 5055, 9163, 264, 9668, 472, 11, 597, 2962, 281, 9668], "temperature": 0.0, "avg_logprob": -0.09785225126478407, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.0002578851708676666}, {"id": 182, "seek": 114008, "start": 1147.36, "end": 1153.9199999999998, "text": " two, and we change the me confidence value to the not me confidence value. And for the bar graph,", "tokens": [732, 11, 293, 321, 1319, 264, 385, 6687, 2158, 281, 264, 406, 385, 6687, 2158, 13, 400, 337, 264, 2159, 4295, 11], "temperature": 0.0, "avg_logprob": -0.09785225126478407, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.0002578851708676666}, {"id": 183, "seek": 114008, "start": 1153.9199999999998, "end": 1162.3999999999999, "text": " it's going to be bar graph two right here. And me confidence changes to not me confidence.", "tokens": [309, 311, 516, 281, 312, 2159, 4295, 732, 558, 510, 13, 400, 385, 6687, 2962, 281, 406, 385, 6687, 13], "temperature": 0.0, "avg_logprob": -0.09785225126478407, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.0002578851708676666}, {"id": 184, "seek": 114008, "start": 1163.52, "end": 1169.28, "text": " And with that, we already have all the sequence of events for the labels updates.", "tokens": [400, 365, 300, 11, 321, 1217, 362, 439, 264, 8310, 295, 3931, 337, 264, 16949, 9205, 13], "temperature": 0.0, "avg_logprob": -0.09785225126478407, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.0002578851708676666}, {"id": 185, "seek": 116928, "start": 1169.28, "end": 1177.44, "text": " We can just go to the next step and confirm that we have defined it correctly, which is the same", "tokens": [492, 393, 445, 352, 281, 264, 958, 1823, 293, 9064, 300, 321, 362, 7642, 309, 8944, 11, 597, 307, 264, 912], "temperature": 0.0, "avg_logprob": -0.08023588524924384, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.00016460831102449447}, {"id": 186, "seek": 116928, "start": 1177.44, "end": 1187.2, "text": " result. The next step is the fancy image change that if we think that we are looking at the baby,", "tokens": [1874, 13, 440, 958, 1823, 307, 264, 10247, 3256, 1319, 300, 498, 321, 519, 300, 321, 366, 1237, 412, 264, 3186, 11], "temperature": 0.0, "avg_logprob": -0.08023588524924384, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.00016460831102449447}, {"id": 187, "seek": 116928, "start": 1187.2, "end": 1192.56, "text": " we will show the happy face, otherwise we just show the crying face. We will be using the if", "tokens": [321, 486, 855, 264, 2055, 1851, 11, 5911, 321, 445, 855, 264, 8554, 1851, 13, 492, 486, 312, 1228, 264, 498], "temperature": 0.0, "avg_logprob": -0.08023588524924384, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.00016460831102449447}, {"id": 188, "seek": 119256, "start": 1192.56, "end": 1200.32, "text": " then logic. So go to the control blocks and we just take the if then otherwise block. We append", "tokens": [550, 9952, 13, 407, 352, 281, 264, 1969, 8474, 293, 321, 445, 747, 264, 498, 550, 5911, 3461, 13, 492, 34116], "temperature": 0.0, "avg_logprob": -0.0928224764372173, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.145381096052006e-05}, {"id": 189, "seek": 119256, "start": 1200.32, "end": 1205.36, "text": " it here. And what will we do is we will be comparing the me confidence value to the not", "tokens": [309, 510, 13, 400, 437, 486, 321, 360, 307, 321, 486, 312, 15763, 264, 385, 6687, 2158, 281, 264, 406], "temperature": 0.0, "avg_logprob": -0.0928224764372173, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.145381096052006e-05}, {"id": 190, "seek": 119256, "start": 1205.36, "end": 1211.36, "text": " me confidence value to know if we are looking at the baby or not. We go to the math blocks,", "tokens": [385, 6687, 2158, 281, 458, 498, 321, 366, 1237, 412, 264, 3186, 420, 406, 13, 492, 352, 281, 264, 5221, 8474, 11], "temperature": 0.0, "avg_logprob": -0.0928224764372173, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.145381096052006e-05}, {"id": 191, "seek": 119256, "start": 1211.36, "end": 1218.32, "text": " we pick this comparator block, attach them to the if statement, and we are going to be changing the", "tokens": [321, 1888, 341, 6311, 1639, 3461, 11, 5085, 552, 281, 264, 498, 5629, 11, 293, 321, 366, 516, 281, 312, 4473, 264], "temperature": 0.0, "avg_logprob": -0.0928224764372173, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.145381096052006e-05}, {"id": 192, "seek": 121832, "start": 1218.32, "end": 1223.6, "text": " comparison to higher or equal because we will not, we don't worry about the equal in this case,", "tokens": [9660, 281, 2946, 420, 2681, 570, 321, 486, 406, 11, 321, 500, 380, 3292, 466, 264, 2681, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.09147559955555906, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.00017098846728913486}, {"id": 193, "seek": 121832, "start": 1223.6, "end": 1230.56, "text": " we just want the higher or equal. We take again the me confidence variable, we compare it here,", "tokens": [321, 445, 528, 264, 2946, 420, 2681, 13, 492, 747, 797, 264, 385, 6687, 7006, 11, 321, 6794, 309, 510, 11], "temperature": 0.0, "avg_logprob": -0.09147559955555906, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.00017098846728913486}, {"id": 194, "seek": 121832, "start": 1231.12, "end": 1238.1599999999999, "text": " and we take the not me confidence value and we compare it right here. Then we will be updating", "tokens": [293, 321, 747, 264, 406, 385, 6687, 2158, 293, 321, 6794, 309, 558, 510, 13, 1396, 321, 486, 312, 25113], "temperature": 0.0, "avg_logprob": -0.09147559955555906, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.00017098846728913486}, {"id": 195, "seek": 121832, "start": 1238.1599999999999, "end": 1244.72, "text": " the background of the of the app, which is available in the screen one. We take the background color", "tokens": [264, 3678, 295, 264, 295, 264, 724, 11, 597, 307, 2435, 294, 264, 2568, 472, 13, 492, 747, 264, 3678, 2017], "temperature": 0.0, "avg_logprob": -0.09147559955555906, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.00017098846728913486}, {"id": 196, "seek": 124472, "start": 1244.72, "end": 1252.08, "text": " block, we attach it here, and the tutorial already provides the example colors. So I'm just going", "tokens": [3461, 11, 321, 5085, 309, 510, 11, 293, 264, 7073, 1217, 6417, 264, 1365, 4577, 13, 407, 286, 478, 445, 516], "temperature": 0.0, "avg_logprob": -0.09471097926503604, "compression_ratio": 1.7022222222222223, "no_speech_prob": 4.2351821321062744e-05}, {"id": 197, "seek": 124472, "start": 1252.08, "end": 1259.92, "text": " to be dragging this right below so I can have them more easily accessible right here. And I can", "tokens": [281, 312, 24385, 341, 558, 2507, 370, 286, 393, 362, 552, 544, 3612, 9515, 558, 510, 13, 400, 286, 393], "temperature": 0.0, "avg_logprob": -0.09471097926503604, "compression_ratio": 1.7022222222222223, "no_speech_prob": 4.2351821321062744e-05}, {"id": 198, "seek": 124472, "start": 1259.92, "end": 1265.2, "text": " just join it here. And for the baby images, we have two images available here, happy baby and", "tokens": [445, 3917, 309, 510, 13, 400, 337, 264, 3186, 5267, 11, 321, 362, 732, 5267, 2435, 510, 11, 2055, 3186, 293], "temperature": 0.0, "avg_logprob": -0.09471097926503604, "compression_ratio": 1.7022222222222223, "no_speech_prob": 4.2351821321062744e-05}, {"id": 199, "seek": 124472, "start": 1265.2, "end": 1270.24, "text": " sad baby. So if we think that we are looking at the baby, we show the happy baby. So we use the", "tokens": [4227, 3186, 13, 407, 498, 321, 519, 300, 321, 366, 1237, 412, 264, 3186, 11, 321, 855, 264, 2055, 3186, 13, 407, 321, 764, 264], "temperature": 0.0, "avg_logprob": -0.09471097926503604, "compression_ratio": 1.7022222222222223, "no_speech_prob": 4.2351821321062744e-05}, {"id": 200, "seek": 127024, "start": 1270.24, "end": 1276.64, "text": " visible block. And if not, we just hide, sorry, if we think that we're looking at the baby,", "tokens": [8974, 3461, 13, 400, 498, 406, 11, 321, 445, 6479, 11, 2597, 11, 498, 321, 519, 300, 321, 434, 1237, 412, 264, 3186, 11], "temperature": 0.0, "avg_logprob": -0.12315594571308025, "compression_ratio": 1.8495145631067962, "no_speech_prob": 6.706379645038396e-05}, {"id": 201, "seek": 127024, "start": 1276.64, "end": 1283.52, "text": " we hide the we hide the side baby face. We go to the logic blocks, we take the true so we can set", "tokens": [321, 6479, 264, 321, 6479, 264, 1252, 3186, 1851, 13, 492, 352, 281, 264, 9952, 8474, 11, 321, 747, 264, 2074, 370, 321, 393, 992], "temperature": 0.0, "avg_logprob": -0.12315594571308025, "compression_ratio": 1.8495145631067962, "no_speech_prob": 6.706379645038396e-05}, {"id": 202, "seek": 127024, "start": 1283.52, "end": 1289.44, "text": " to true to visible, we can set visible to true, and we set visible to false for the sad baby", "tokens": [281, 2074, 281, 8974, 11, 321, 393, 992, 8974, 281, 2074, 11, 293, 321, 992, 8974, 281, 7908, 337, 264, 4227, 3186], "temperature": 0.0, "avg_logprob": -0.12315594571308025, "compression_ratio": 1.8495145631067962, "no_speech_prob": 6.706379645038396e-05}, {"id": 203, "seek": 127024, "start": 1289.44, "end": 1296.08, "text": " like that. And we just join it. For the case of me confidence being higher than not me confidence,", "tokens": [411, 300, 13, 400, 321, 445, 3917, 309, 13, 1171, 264, 1389, 295, 385, 6687, 885, 2946, 813, 406, 385, 6687, 11], "temperature": 0.0, "avg_logprob": -0.12315594571308025, "compression_ratio": 1.8495145631067962, "no_speech_prob": 6.706379645038396e-05}, {"id": 204, "seek": 129608, "start": 1296.08, "end": 1300.56, "text": " for the opposite case, when we are not looking at the baby, we just change the background color to", "tokens": [337, 264, 6182, 1389, 11, 562, 321, 366, 406, 1237, 412, 264, 3186, 11, 321, 445, 1319, 264, 3678, 2017, 281], "temperature": 0.0, "avg_logprob": -0.0613896301814488, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.7160400602733716e-05}, {"id": 205, "seek": 129608, "start": 1302.1599999999999, "end": 1312.8799999999999, "text": " this pink color. We hide the happy baby face like that. And we show the sad baby face", "tokens": [341, 7022, 2017, 13, 492, 6479, 264, 2055, 3186, 1851, 411, 300, 13, 400, 321, 855, 264, 4227, 3186, 1851], "temperature": 0.0, "avg_logprob": -0.0613896301814488, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.7160400602733716e-05}, {"id": 206, "seek": 129608, "start": 1313.9199999999998, "end": 1323.9199999999998, "text": " just like this. And now the app is finished. So here we can just check the final code,", "tokens": [445, 411, 341, 13, 400, 586, 264, 724, 307, 4335, 13, 407, 510, 321, 393, 445, 1520, 264, 2572, 3089, 11], "temperature": 0.0, "avg_logprob": -0.0613896301814488, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.7160400602733716e-05}, {"id": 207, "seek": 132392, "start": 1323.92, "end": 1328.4, "text": " which is exactly the same as we have right here. There are other possibilities like we can just", "tokens": [597, 307, 2293, 264, 912, 382, 321, 362, 558, 510, 13, 821, 366, 661, 12178, 411, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.14165754240702808, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00010598780500004068}, {"id": 208, "seek": 132392, "start": 1328.4, "end": 1334.64, "text": " implement a classifier using a different person. But to show how this works, we can use the MIT", "tokens": [4445, 257, 1508, 9902, 1228, 257, 819, 954, 13, 583, 281, 855, 577, 341, 1985, 11, 321, 393, 764, 264, 13100], "temperature": 0.0, "avg_logprob": -0.14165754240702808, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00010598780500004068}, {"id": 209, "seek": 132392, "start": 1334.64, "end": 1341.3600000000001, "text": " company map that is available on the Play Store. Let me just show my phone again. Here it is. So", "tokens": [2237, 4471, 300, 307, 2435, 322, 264, 5506, 17242, 13, 961, 385, 445, 855, 452, 2593, 797, 13, 1692, 309, 307, 13, 407], "temperature": 0.0, "avg_logprob": -0.14165754240702808, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00010598780500004068}, {"id": 210, "seek": 132392, "start": 1341.3600000000001, "end": 1347.1200000000001, "text": " you can just go to the Play Store and go to MIT App Inventor, search MIT App Inventor and you have", "tokens": [291, 393, 445, 352, 281, 264, 5506, 17242, 293, 352, 281, 13100, 3132, 682, 2475, 284, 11, 3164, 13100, 3132, 682, 2475, 284, 293, 291, 362], "temperature": 0.0, "avg_logprob": -0.14165754240702808, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00010598780500004068}, {"id": 211, "seek": 132392, "start": 1347.1200000000001, "end": 1353.8400000000001, "text": " right here the company. You can open it. Yeah, you can just ignore this warning. It works without", "tokens": [558, 510, 264, 2237, 13, 509, 393, 1269, 309, 13, 865, 11, 291, 393, 445, 11200, 341, 9164, 13, 467, 1985, 1553], "temperature": 0.0, "avg_logprob": -0.14165754240702808, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00010598780500004068}, {"id": 212, "seek": 135384, "start": 1353.84, "end": 1363.6, "text": " Wi-Fi. Continue without Wi-Fi. And over here you can connect the AI companion. And now I can scan", "tokens": [14035, 12, 13229, 13, 24472, 1553, 14035, 12, 13229, 13, 400, 670, 510, 291, 393, 1745, 264, 7318, 22363, 13, 400, 586, 286, 393, 11049], "temperature": 0.0, "avg_logprob": -0.15932171161358172, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.00017426871636416763}, {"id": 213, "seek": 135384, "start": 1363.6, "end": 1374.72, "text": " the QR code like this. I'm sorry, it just disappeared. It takes a few seconds to connect.", "tokens": [264, 32784, 3089, 411, 341, 13, 286, 478, 2597, 11, 309, 445, 13954, 13, 467, 2516, 257, 1326, 3949, 281, 1745, 13], "temperature": 0.0, "avg_logprob": -0.15932171161358172, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.00017426871636416763}, {"id": 214, "seek": 135384, "start": 1374.72, "end": 1376.32, "text": " Let's see if it was faster than tonight.", "tokens": [961, 311, 536, 498, 309, 390, 4663, 813, 4440, 13], "temperature": 0.0, "avg_logprob": -0.15932171161358172, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.00017426871636416763}, {"id": 215, "seek": 137632, "start": 1376.32, "end": 1383.6799999999998, "text": " Now it's loading the extension, the personal classifier extension into my phone.", "tokens": [823, 309, 311, 15114, 264, 10320, 11, 264, 2973, 1508, 9902, 10320, 666, 452, 2593, 13], "temperature": 0.0, "avg_logprob": -0.12237426330303323, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0003506196662783623}, {"id": 216, "seek": 137632, "start": 1384.8, "end": 1388.56, "text": " Like this works with Wi-Fi, mobile network. It doesn't have to be connected. It's just connected", "tokens": [1743, 341, 1985, 365, 14035, 12, 13229, 11, 6013, 3209, 13, 467, 1177, 380, 362, 281, 312, 4582, 13, 467, 311, 445, 4582], "temperature": 0.0, "avg_logprob": -0.12237426330303323, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0003506196662783623}, {"id": 217, "seek": 137632, "start": 1388.56, "end": 1394.08, "text": " because I'm just mirroring the screen through that cable. And I see here the layout of the app.", "tokens": [570, 286, 478, 445, 8013, 278, 264, 2568, 807, 300, 8220, 13, 400, 286, 536, 510, 264, 13333, 295, 264, 724, 13], "temperature": 0.0, "avg_logprob": -0.12237426330303323, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0003506196662783623}, {"id": 218, "seek": 137632, "start": 1394.08, "end": 1399.12, "text": " I can see that it shows ready. So I can just toggle the camera to be the from one.", "tokens": [286, 393, 536, 300, 309, 3110, 1919, 13, 407, 286, 393, 445, 31225, 264, 2799, 281, 312, 264, 490, 472, 13], "temperature": 0.0, "avg_logprob": -0.12237426330303323, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0003506196662783623}, {"id": 219, "seek": 137632, "start": 1400.0, "end": 1405.52, "text": " And I can look at the camera and I'm just going to be start. And there is a higher confidence", "tokens": [400, 286, 393, 574, 412, 264, 2799, 293, 286, 478, 445, 516, 281, 312, 722, 13, 400, 456, 307, 257, 2946, 6687], "temperature": 0.0, "avg_logprob": -0.12237426330303323, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0003506196662783623}, {"id": 220, "seek": 140552, "start": 1405.52, "end": 1409.6, "text": " that I'm looking at the Wi-Fi. Just put a hand in front of it. It's just crying.", "tokens": [300, 286, 478, 1237, 412, 264, 14035, 12, 13229, 13, 1449, 829, 257, 1011, 294, 1868, 295, 309, 13, 467, 311, 445, 8554, 13], "temperature": 0.0, "avg_logprob": -0.15951418682811705, "compression_ratio": 1.5886524822695036, "no_speech_prob": 0.0003477729915175587}, {"id": 221, "seek": 140552, "start": 1411.2, "end": 1417.92, "text": " And yeah, that's it. Later, if you want to build any other apps, you can export it", "tokens": [400, 1338, 11, 300, 311, 309, 13, 11965, 11, 498, 291, 528, 281, 1322, 604, 661, 7733, 11, 291, 393, 10725, 309], "temperature": 0.0, "avg_logprob": -0.15951418682811705, "compression_ratio": 1.5886524822695036, "no_speech_prob": 0.0003477729915175587}, {"id": 222, "seek": 140552, "start": 1418.72, "end": 1424.32, "text": " to APK files. So you can start it on your phone or 200 app bundles if you want to distribute it", "tokens": [281, 5372, 42, 7098, 13, 407, 291, 393, 722, 309, 322, 428, 2593, 420, 2331, 724, 13882, 904, 498, 291, 528, 281, 20594, 309], "temperature": 0.0, "avg_logprob": -0.15951418682811705, "compression_ratio": 1.5886524822695036, "no_speech_prob": 0.0003477729915175587}, {"id": 223, "seek": 140552, "start": 1424.32, "end": 1429.04, "text": " through Play Store. But yeah, this is just a very high-level introduction to artificial", "tokens": [807, 5506, 17242, 13, 583, 1338, 11, 341, 307, 445, 257, 588, 1090, 12, 12418, 9339, 281, 11677], "temperature": 0.0, "avg_logprob": -0.15951418682811705, "compression_ratio": 1.5886524822695036, "no_speech_prob": 0.0003477729915175587}, {"id": 224, "seek": 140552, "start": 1429.04, "end": 1434.32, "text": " intelligence in Inventor. You can just build any kind of classifier, for example, to classify trees,", "tokens": [7599, 294, 682, 2475, 284, 13, 509, 393, 445, 1322, 604, 733, 295, 1508, 9902, 11, 337, 1365, 11, 281, 33872, 5852, 11], "temperature": 0.0, "avg_logprob": -0.15951418682811705, "compression_ratio": 1.5886524822695036, "no_speech_prob": 0.0003477729915175587}, {"id": 225, "seek": 143432, "start": 1434.32, "end": 1439.36, "text": " flowers, to classify even people. For example, for a faculty, if you want to build an app that", "tokens": [8085, 11, 281, 33872, 754, 561, 13, 1171, 1365, 11, 337, 257, 6389, 11, 498, 291, 528, 281, 1322, 364, 724, 300], "temperature": 0.0, "avg_logprob": -0.11184405599321638, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.00015882958541624248}, {"id": 226, "seek": 143432, "start": 1439.36, "end": 1446.72, "text": " recognizes people in your class as a game, you can just use Inventor and build any kind of app.", "tokens": [26564, 561, 294, 428, 1508, 382, 257, 1216, 11, 291, 393, 445, 764, 682, 2475, 284, 293, 1322, 604, 733, 295, 724, 13], "temperature": 0.0, "avg_logprob": -0.11184405599321638, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.00015882958541624248}, {"id": 227, "seek": 143432, "start": 1447.36, "end": 1450.8799999999999, "text": " Thank you so much and hope that it was useful for everybody.", "tokens": [1044, 291, 370, 709, 293, 1454, 300, 309, 390, 4420, 337, 2201, 13], "temperature": 0.0, "avg_logprob": -0.11184405599321638, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.00015882958541624248}, {"id": 228, "seek": 145088, "start": 1450.88, "end": 1461.68, "text": " Thank you. Any questions?", "tokens": [1044, 291, 13, 2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2723186879918195, "compression_ratio": 1.3915343915343916, "no_speech_prob": 0.0001935073669301346}, {"id": 229, "seek": 145088, "start": 1464.3200000000002, "end": 1466.96, "text": " Do you mentor a technobation team? No.", "tokens": [1144, 291, 14478, 257, 1537, 996, 399, 1469, 30, 883, 13], "temperature": 0.0, "avg_logprob": -0.2723186879918195, "compression_ratio": 1.3915343915343916, "no_speech_prob": 0.0001935073669301346}, {"id": 230, "seek": 145088, "start": 1468.96, "end": 1474.4, "text": " Sorry, I'm a software engineer. I just contributed to Inventor. I started as building apps and then", "tokens": [4919, 11, 286, 478, 257, 4722, 11403, 13, 286, 445, 18434, 281, 682, 2475, 284, 13, 286, 1409, 382, 2390, 7733, 293, 550], "temperature": 0.0, "avg_logprob": -0.2723186879918195, "compression_ratio": 1.3915343915343916, "no_speech_prob": 0.0001935073669301346}, {"id": 231, "seek": 145088, "start": 1474.4, "end": 1480.72, "text": " I transitioned to open source. I participated in Google Summer of Code, like this option to export", "tokens": [286, 47346, 281, 1269, 4009, 13, 286, 17978, 294, 3329, 16161, 295, 15549, 11, 411, 341, 3614, 281, 10725], "temperature": 0.0, "avg_logprob": -0.2723186879918195, "compression_ratio": 1.3915343915343916, "no_speech_prob": 0.0001935073669301346}, {"id": 232, "seek": 148072, "start": 1480.72, "end": 1486.08, "text": " a 100 app bundles was my project in 2020 for Google Summer of Code. But yeah, I'm more like,", "tokens": [257, 2319, 724, 13882, 904, 390, 452, 1716, 294, 4808, 337, 3329, 16161, 295, 15549, 13, 583, 1338, 11, 286, 478, 544, 411, 11], "temperature": 0.0, "avg_logprob": -0.20660534611454717, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0008483018027618527}, {"id": 233, "seek": 148072, "start": 1486.08, "end": 1489.68, "text": " more technical than actually teaching to kids.", "tokens": [544, 6191, 813, 767, 4571, 281, 2301, 13], "temperature": 0.0, "avg_logprob": -0.20660534611454717, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0008483018027618527}, {"id": 234, "seek": 148072, "start": 1492.4, "end": 1500.08, "text": " Any other questions? What's your experience with the relation between the number of pictures you", "tokens": [2639, 661, 1651, 30, 708, 311, 428, 1752, 365, 264, 9721, 1296, 264, 1230, 295, 5242, 291], "temperature": 0.0, "avg_logprob": -0.20660534611454717, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0008483018027618527}, {"id": 235, "seek": 148072, "start": 1500.08, "end": 1506.48, "text": " have to submit to your classifier and your accuracy? That's a very good question. So for the linear", "tokens": [362, 281, 10315, 281, 428, 1508, 9902, 293, 428, 14170, 30, 663, 311, 257, 588, 665, 1168, 13, 407, 337, 264, 8213], "temperature": 0.0, "avg_logprob": -0.20660534611454717, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0008483018027618527}, {"id": 236, "seek": 150648, "start": 1506.48, "end": 1514.08, "text": " example, he was asking like, what's the experience with the amount of images that we are going to", "tokens": [1365, 11, 415, 390, 3365, 411, 11, 437, 311, 264, 1752, 365, 264, 2372, 295, 5267, 300, 321, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.10277911822001139, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.00026822995278052986}, {"id": 237, "seek": 150648, "start": 1514.08, "end": 1519.28, "text": " be using for the classifier? So I haven't really tested it right now. But we have seen that if we", "tokens": [312, 1228, 337, 264, 1508, 9902, 30, 407, 286, 2378, 380, 534, 8246, 309, 558, 586, 13, 583, 321, 362, 1612, 300, 498, 321], "temperature": 0.0, "avg_logprob": -0.10277911822001139, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.00026822995278052986}, {"id": 238, "seek": 150648, "start": 1519.28, "end": 1524.0, "text": " go higher than 10 images for each class, for each group of images, we'll have really good results.", "tokens": [352, 2946, 813, 1266, 5267, 337, 1184, 1508, 11, 337, 1184, 1594, 295, 5267, 11, 321, 603, 362, 534, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.10277911822001139, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.00026822995278052986}, {"id": 239, "seek": 150648, "start": 1524.0, "end": 1528.32, "text": " In this case, because I was just turning very fast and using just a few number of images,", "tokens": [682, 341, 1389, 11, 570, 286, 390, 445, 6246, 588, 2370, 293, 1228, 445, 257, 1326, 1230, 295, 5267, 11], "temperature": 0.0, "avg_logprob": -0.10277911822001139, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.00026822995278052986}, {"id": 240, "seek": 150648, "start": 1528.32, "end": 1534.08, "text": " you can see that the confidence levels were a little bit like 80, 20. But if we provide more", "tokens": [291, 393, 536, 300, 264, 6687, 4358, 645, 257, 707, 857, 411, 4688, 11, 945, 13, 583, 498, 321, 2893, 544], "temperature": 0.0, "avg_logprob": -0.10277911822001139, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.00026822995278052986}, {"id": 241, "seek": 153408, "start": 1534.08, "end": 1539.6799999999998, "text": " than 10 images for each class, we should be able to get around over 90, 95% of confidence for each", "tokens": [813, 1266, 5267, 337, 1184, 1508, 11, 321, 820, 312, 1075, 281, 483, 926, 670, 4289, 11, 13420, 4, 295, 6687, 337, 1184], "temperature": 0.0, "avg_logprob": -0.12376474847598952, "compression_ratio": 1.2587412587412588, "no_speech_prob": 0.00018999268650077283}, {"id": 242, "seek": 153408, "start": 1539.6799999999998, "end": 1546.24, "text": " number. I'm not sure if there are any questions from the chat. Let me just check.", "tokens": [1230, 13, 286, 478, 406, 988, 498, 456, 366, 604, 1651, 490, 264, 5081, 13, 961, 385, 445, 1520, 13], "temperature": 0.0, "avg_logprob": -0.12376474847598952, "compression_ratio": 1.2587412587412588, "no_speech_prob": 0.00018999268650077283}, {"id": 243, "seek": 154624, "start": 1546.24, "end": 1560.88, "text": " What do you capture? Is that just for your face or did the learning, what was learned?", "tokens": [708, 360, 291, 7983, 30, 1119, 300, 445, 337, 428, 1851, 420, 630, 264, 2539, 11, 437, 390, 3264, 30], "temperature": 0.0, "avg_logprob": -0.23342183141997366, "compression_ratio": 1.5344827586206897, "no_speech_prob": 9.742415568325669e-05}, {"id": 244, "seek": 154624, "start": 1565.92, "end": 1569.44, "text": " It recognizes, it depends on what you are training, because in this case, we are just", "tokens": [467, 26564, 11, 309, 5946, 322, 437, 291, 366, 3097, 11, 570, 294, 341, 1389, 11, 321, 366, 445], "temperature": 0.0, "avg_logprob": -0.23342183141997366, "compression_ratio": 1.5344827586206897, "no_speech_prob": 9.742415568325669e-05}, {"id": 245, "seek": 154624, "start": 1569.44, "end": 1575.52, "text": " providing a very specific gestures. It's training my face like any face looking at the camera,", "tokens": [6530, 257, 588, 2685, 28475, 13, 467, 311, 3097, 452, 1851, 411, 604, 1851, 1237, 412, 264, 2799, 11], "temperature": 0.0, "avg_logprob": -0.23342183141997366, "compression_ratio": 1.5344827586206897, "no_speech_prob": 9.742415568325669e-05}, {"id": 246, "seek": 157552, "start": 1575.52, "end": 1583.04, "text": " or a hand in front of the face. By default, the model that is available, that is here,", "tokens": [420, 257, 1011, 294, 1868, 295, 264, 1851, 13, 3146, 7576, 11, 264, 2316, 300, 307, 2435, 11, 300, 307, 510, 11], "temperature": 0.0, "avg_logprob": -0.15288347839027322, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00026697490829974413}, {"id": 247, "seek": 157552, "start": 1583.76, "end": 1589.92, "text": " this model is turned by Salim. It's the example guy that is at the beginning of the tutorial.", "tokens": [341, 2316, 307, 3574, 538, 5996, 332, 13, 467, 311, 264, 1365, 2146, 300, 307, 412, 264, 2863, 295, 264, 7073, 13], "temperature": 0.0, "avg_logprob": -0.15288347839027322, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00026697490829974413}, {"id": 248, "seek": 157552, "start": 1589.92, "end": 1594.48, "text": " And I just tested it last night, and it worked with me because it recognizes the gestures,", "tokens": [400, 286, 445, 8246, 309, 1036, 1818, 11, 293, 309, 2732, 365, 385, 570, 309, 26564, 264, 28475, 11], "temperature": 0.0, "avg_logprob": -0.15288347839027322, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00026697490829974413}, {"id": 249, "seek": 157552, "start": 1594.48, "end": 1601.52, "text": " not the faces. If instead we train recognizing people, we will all be looking in the same way", "tokens": [406, 264, 8475, 13, 759, 2602, 321, 3847, 18538, 561, 11, 321, 486, 439, 312, 1237, 294, 264, 912, 636], "temperature": 0.0, "avg_logprob": -0.15288347839027322, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00026697490829974413}, {"id": 250, "seek": 160152, "start": 1601.52, "end": 1609.92, "text": " at the camera. So it will just go for specific facial, how do you say, facial features.", "tokens": [412, 264, 2799, 13, 407, 309, 486, 445, 352, 337, 2685, 15642, 11, 577, 360, 291, 584, 11, 15642, 4122, 13], "temperature": 0.0, "avg_logprob": -0.23575519112979665, "compression_ratio": 1.401639344262295, "no_speech_prob": 0.0016569977160543203}, {"id": 251, "seek": 160152, "start": 1621.36, "end": 1627.76, "text": " In this case, in this case, it will work. You can just try if you want. We can try.", "tokens": [682, 341, 1389, 11, 294, 341, 1389, 11, 309, 486, 589, 13, 509, 393, 445, 853, 498, 291, 528, 13, 492, 393, 853, 13], "temperature": 0.0, "avg_logprob": -0.23575519112979665, "compression_ratio": 1.401639344262295, "no_speech_prob": 0.0016569977160543203}, {"id": 252, "seek": 162776, "start": 1627.76, "end": 1631.44, "text": " Yeah, it should work.", "tokens": [865, 11, 309, 820, 589, 13], "temperature": 0.0, "avg_logprob": -0.29964745762836503, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0007825715583749115}, {"id": 253, "seek": 162776, "start": 1635.2, "end": 1639.12, "text": " Can you, it's going to be a little bit tough, but Mark, can you,", "tokens": [1664, 291, 11, 309, 311, 516, 281, 312, 257, 707, 857, 4930, 11, 457, 3934, 11, 393, 291, 11], "temperature": 0.0, "avg_logprob": -0.29964745762836503, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0007825715583749115}, {"id": 254, "seek": 162776, "start": 1641.52, "end": 1647.6, "text": " can you just try with Mark, for example? Toggle camera. Toggle camera.", "tokens": [393, 291, 445, 853, 365, 3934, 11, 337, 1365, 30, 314, 664, 22631, 2799, 13, 314, 664, 22631, 2799, 13], "temperature": 0.0, "avg_logprob": -0.29964745762836503, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0007825715583749115}, {"id": 255, "seek": 162776, "start": 1649.04, "end": 1656.0, "text": " Yeah, and just try with, and start. Press start. It's a happy face, so if you put a hand in front,", "tokens": [865, 11, 293, 445, 853, 365, 11, 293, 722, 13, 6776, 722, 13, 467, 311, 257, 2055, 1851, 11, 370, 498, 291, 829, 257, 1011, 294, 1868, 11], "temperature": 0.0, "avg_logprob": -0.29964745762836503, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0007825715583749115}, {"id": 256, "seek": 165600, "start": 1656.0, "end": 1658.4, "text": " it's a sad face. It's recognizing the gestures.", "tokens": [309, 311, 257, 4227, 1851, 13, 467, 311, 18538, 264, 28475, 13], "temperature": 0.0, "avg_logprob": -0.133485292777037, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.0003281192039139569}, {"id": 257, "seek": 165600, "start": 1661.76, "end": 1665.28, "text": " So can you also train it to recognize specific people?", "tokens": [407, 393, 291, 611, 3847, 309, 281, 5521, 2685, 561, 30], "temperature": 0.0, "avg_logprob": -0.133485292777037, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.0003281192039139569}, {"id": 258, "seek": 165600, "start": 1665.28, "end": 1669.36, "text": " Yeah, it can be trained, but in this case, because the higher difference was the hand,", "tokens": [865, 11, 309, 393, 312, 8895, 11, 457, 294, 341, 1389, 11, 570, 264, 2946, 2649, 390, 264, 1011, 11], "temperature": 0.0, "avg_logprob": -0.133485292777037, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.0003281192039139569}, {"id": 259, "seek": 165600, "start": 1669.36, "end": 1673.52, "text": " it's just looking for the hand in model. But if you don't show the hand, it will look for faces.", "tokens": [309, 311, 445, 1237, 337, 264, 1011, 294, 2316, 13, 583, 498, 291, 500, 380, 855, 264, 1011, 11, 309, 486, 574, 337, 8475, 13], "temperature": 0.0, "avg_logprob": -0.133485292777037, "compression_ratio": 1.545945945945946, "no_speech_prob": 0.0003281192039139569}, {"id": 260, "seek": 167352, "start": 1673.52, "end": 1686.0, "text": " Yeah, it's a, it can be a fit because it's just, you can just use this website and", "tokens": [865, 11, 309, 311, 257, 11, 309, 393, 312, 257, 3318, 570, 309, 311, 445, 11, 291, 393, 445, 764, 341, 3144, 293], "temperature": 0.0, "avg_logprob": -0.1811848948983585, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.0004705360624939203}, {"id": 261, "seek": 167352, "start": 1686.0, "end": 1690.0, "text": " fill any kind of models. The only restriction is that it has to be an MLD file,", "tokens": [2836, 604, 733, 295, 5245, 13, 440, 787, 29529, 307, 300, 309, 575, 281, 312, 364, 21601, 35, 3991, 11], "temperature": 0.0, "avg_logprob": -0.1811848948983585, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.0004705360624939203}, {"id": 262, "seek": 167352, "start": 1690.0, "end": 1695.76, "text": " but yeah, it can classify any model basically. No problem. Any other questions?", "tokens": [457, 1338, 11, 309, 393, 33872, 604, 2316, 1936, 13, 883, 1154, 13, 2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.1811848948983585, "compression_ratio": 1.423529411764706, "no_speech_prob": 0.0004705360624939203}, {"id": 263, "seek": 169576, "start": 1695.76, "end": 1701.76, "text": " Well, I think we can leave it here. Thank you so much.", "tokens": [50364, 1042, 11, 286, 519, 321, 393, 1856, 309, 510, 13, 1044, 291, 370, 709, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2408183150821262, "compression_ratio": 0.8852459016393442, "no_speech_prob": 0.0003635287575889379}], "language": "en"}