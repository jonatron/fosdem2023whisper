{"text": " Let's write in, we don't have much time. My name is Paul Mazurel. I go by the name of Filmi Coton on Mastonon on Twitter. I've been a rest developer since 2016. I spent most of my career as a search developer, so it was only natural for me as my first pet project to develop a library to build search engines. So if you're familiar with Lucine, that's kind of like Lucine, but for the rest of the world. That was my first pet project. It grew. And two years ago, I co-founded a startup called Quit Quit, that is about building a search engine that is specialized for logs. Just a word about Quit Quit. I'm not here to do an advertisement. It's not a commercial, but it's related to the tool, so I need to explain to you what's the problem, which is the problem that we are trying to solve. Quit Quit is a Rust project. It's under the AGPL license. It's open source. You can find the source code on GitHub. And so we specialize on log search. So what's specific about log search compared to, let's say, an e-commerce search engine is that the data that we get is more or less immutable. So we assume that people will want to ingest documents into our search engine and won't modify it too much. So after ingestion, the document stays there until it goes out of its retention period, at which point we will delete it. Or maybe you might want to delete it if you have a request to comply to GDPR. We handle that kind of stuff, but you cannot modify it like you would do for any e-commerce website. And so one of the big differences in terms of efficiency is that when you deal with log search, the volume that you have to deal with has no limits. The largest amount that we've seen so far is people indexing 100 terabytes a day. So that's the volume of data. Imagine if it was actually generated not by machine but by humans. You would have to have a lot of people typing grief as to deal with that kind of volume. So that's something that you will only get if you're doing log search. And compared to any e-commerce website, most of the CPU is actually spent indexing and not searching because you have comparatively way less search and way more documents. So indexing is actually crucial to our problems, and that's very different from usual search engines. Indexing, what does it look like? That's the problem that we are trying to solve. Super oversimplified. We get, as an input, a stream of documents. It's interesting to have another idea for one pipeline of indexing. We have to deal with around 40 megabytes per second. And as an output, every 30 seconds, we write a file. We put it somewhere, and usually we register it on some metadata backends. And at this point, the file is searchable. And the rules of the game here is we want to have the highest possible throughput. And we want to keep what we call time to search as low as possible. So time to search is at the moment when JSON file is entering the system, we start the clock, and we measure how long it takes for it to go out of the system in the form of one of those files, at which point it is searchable. We need that to be as low as possible, and we need, it's very important to keep it very stable. We don't want to have, like, a period of time where it goes through the roof. So that's the whole game. And in that black box, we do a lot of stuff. I was voluntarily very simple. I won't go through all of the stuff that we do, but the important part here is every single of those steps is using different resources. The time is spent on different stuff. So, for instance, when we index things, when we build or in memory index, we are spending CPU. When we are writing the file, we use IO. When we upload with network, and sometimes we are waiting for something that is outside of the system. We spend no resources at all except we wait. So you might think that the implementation is obvious. We have one function for all of those steps, and we call them sequentially. But if you do that, you are wasting the resources of our system, of course. For instance, when you are uploading, you are spending your network resource, but your CPU is not doing anything, so you're wasting money. So the solution to this problem is relatively simple, but it's not that simple to implement. You want to streamline your pipeline. What I mean by streamlining in a very concrete way is you might have two steps, like indexing, spending CPU, upload, spending network. They go sequentially, but what you want is you want indexing to work on building the first file, and when it has finished, you start uploading, of course. But as you upload, you want to start working on the second file, so that you are spending CPU while you are doing your network stuff. That's what the kind of behavior that we want. And of course, this example is a little bit too simple because the second step here is shorter than the first one. It's a nice case, but if it was the other way around, we would have to have some kind of mechanism to deal with, to have back pressure. And in my experience, a lot of very good engineers are not familiar with the concept of back pressure, so let me explain what it is about. If you already know what it is, bear with me and enjoy the fine artworks that we have here. So the idea of back pressure is, imagine you are cleaning dishes with a friend. One of you is cleaning the plates, and the other one is wiping them dry. And the person wiping the dishes dry is a little bit too slow compared to the person cleaning the dishes. What's going to happen is that your plates will accumulate like forever. And in the computer system, it's a very common problem, and that's how you get out of memory errors. So the solution is rather simple. You need back pressure, which means you need some way to signal the person who is cleaning too fast that they should slow down. And the simplest mechanism to do this is you need to have some kind of limit on your stack of plates or your work queue or whatever you are using in your system. And then they stop once they reach the limit. So it's the simplest way you could have back pressure. So with all this said, the game here is how would you implement that? What would be your go-to implementation if you had to have such a system in one hour? And being Rust developers, I think that most of the people in this room will come with the following solution. So the upload part, it's not CPUV, it's just dealing with networks. So it's very natural to think, OK, I'm going to do that in a Tokyo task. And back pressure, that's easy. I already know what I'm going to use. I feel that the good solution is I'm going to have a channel with some capacity. And once it reaches capacity, of course, people sending work to my task, they will have to wait. It's going to be very nice and natural. And then on the indexing part, we will have the same mechanism. And only right now, we will have to actually do a lot of CPU heavy work. So maybe we won't run that in a Tokyo task and we will spawn our own thread or maybe we will use a thread pool to do that work. It would be better, right? And we use the same mechanism. We will have a channel to receive the work. The capacity here is much larger because the type of stuff that we put in the channel is very different. For the uploader, we are getting files, possibly they can be large, like 100 megabytes. So it makes sense to have it as small as possible. Here, it's documents. So it's for many documents, you will emit one file. You won't probably have capacity that this is larger than three. And yeah, everything is fine and handy. It's quite natural. So we just reinvented Actors. That's basically what Actors are. So Actors is a programming paradigm that has been invented in the 70s by a researcher called Karlewit. It has been popularized more recently with Erlong. And the actual formal definition is here. It's from Karlewit himself. And I'm going to read it even if it's a little bit weird to read slides out loud. This one is important. So an actor is a computational entity that, in response to a message it receives, can concurrently. A, send a finite number of messages to other Actors. We've done that. Two, create finite numbers of new Actors. We haven't been spawning any Actors in our example yet, but we do that in quick reads as something that we do especially for supervision or spawning a pipeline or stuff like that. So we do that. C, designates behavior to be used for the next message it receives. That one is a little bit fuzzy. Do we do this? No, we definitely don't. But if you water it down and you squint a little, the fact that the Actors have actually a state, and the whole point of having this Actors running on a specific thread is that it will be possible to handle a message and mutate our state. And mutating our state is a bit like designating the behavior that it will be able to use for the next message. So we ended up building our own Actors framework. So to be honest, I'm not trying to advertise for this framework. It's under the AGPL license, so you can use it. You're free to use it. If you want to take over it for kids and make it better, I'd be happy for us to use it. If you want to use it as is and you would like to have it as a MIT license, I'm perfectly happy to put it under MIT license as well, actually. But right now, it's not redesigned to be reused by other people. But I think I might be able to tell you about our journey and maybe it could inspire other people to write their own framework. There are actually a lot of Actors frameworks in Rust. This one is Actix, there are many others. So under our Actors framework, what does an Actor look like compared to our original snippets? Yeah, it looks like this. So I implemented the uploader there. So you have to implement first a trait called Actor where you will define a bunch of small properties about your Actor, especially the capacity that we have seen before. So it will be like the capacity of the channel that we described before. And then you will have to implement a handler trait for each type of message that you deal with. So, contrary to our example, now we can deal with several types of messages. Same Actor can receive different kind of requests, if you will. Another difference is most of the time you want to communicate with an Actor in an asynchronous way. That's how the Actor pattern is working usually. But sometimes it's handy to actually have some kind of reply when you do a request. It's a bummer because if you really want a reply, that means that you will be waiting for the Actor to process the entire queue and execute your command and then return the result. But you don't have to use it. Most of our messages don't use it, but when we need it, it's handy. And our indexer, it's about the same. The one thing that I want to point out is this thing on the Actor trait where we specialized what should be returned on the method RuntimeHandle. So, you remember that in our example, we said that an indexer spends a lot of CPU. We wanted to run it on a dedicated thread. We don't do that here. What we do is we target a specific Tokyo runtime, which is weird. So, that's an implementation shortcut that we used instead of running stuff on a thread pool. What we do is that we have several Tokyo runtime and we have a Tokyo runtime that is dedicated to act as a thread pool. The benefit of this is this implementation shortcut gives us the possibility to write exactly the same code for an Async Actor or Async Actor, which is neat. And by the way, we're not the only one to use that trick. InflexDB actually wrote a blog post about this a little bit of time ago. Now that we have a framework, you might have noticed that the code was not even shorter. We have seen a couple of features that we got from the framework, but what can we get like more? Within QuickWit, we have 25 different actors. What's cool when you have a framework is that you code stuff once and the benefit is multiplied by 25. What could be the benefit? So, hopefully, we get better structure and code that is a little bit more readable. People open up files and they know what to expect. I want to know what is the capacity associated to the cure of this actor. Where should I look? That's something that you get from having a framework. So, we will talk a little bit about that, but we get supervision from the framework. We get a neat solution to deal with time, which is probably the main reason why we don't use Actix today. And then we have a bunch of recipes to write unique tests, which is very important. We also have some stuff to be able to see what our actors are doing and we have a solution for discoverability also, but we won't talk about this in this talk. So, supervision. Of course, I would like to tell you our code is perfect and perfect code is, especially for telling the rest, it never fails. And in a sense, we don't experience panics or stuff like that, but we have to run our code, third-party code, user-defined code, like user, they can write a VRI script to transform their documents in the pipeline and that's running on our indexing pipeline. We have to do IO. We have to interact with different systems. For instance, we get our documents from a source. We're running the pipeline. We send them to a storage. We have different storage that are implemented. That's a lot of components and any one of them can fail and we want a very large amount of time. So, one solution to this, it's not discoverability, it's not like it works all of the time, but just try to turn it off and on again. It feels a little bit stupid, but you just restart everything from a blank state or blank slate and sometimes things work fine that way. So, the supervision works as follows. We have an actor that is in charge of supervising all of the actors that are in our pipeline. What he's doing is that it's pulling actors and when it detects that they failed, it will kill everyone and restart everyone. The definition of failure is a little bit sophisticated in our case, so it could be an actor that has returned an error from a handler or it panicked or we have some system to detect if an actor has not been progressing for three seconds. So, we have an option of progression in our framework. That's an original way to do stuff. And we use one for all supervision, which means that if one actor failed, we restart everyone. Okay, that was for supervision. Now, about handling time, which is probably the most interesting part of our framework. So, we need to be able to deal with the idea that, for instance, in our indexer, we want to emit a file after 30 seconds has passed. So, we have a condition like this. 30 seconds after the first document was added in that batch. And we cannot do time.sleep in the handler because it will block the entire processing of documents. So, the solution for this is rather simple. We have a method so that actors can ask the framework to send back a message to themselves after a given amount of time, 30 seconds here. And it seems like a very simple solution, but it has a problem. So, here I showed how it worked. The actor is sending a message to the scheduler. This is what is happening under the hood. It's sending a message to actually an actor that is run by the framework called the scheduler actor. And 30 seconds later, the scheduler will stack a message into the queue of the actor. The trouble there is, imagine that you already had a lot of messages in the queue of the actor. Then, where are your 30 seconds? Maybe we have one minute worth of messages in that queue. And then, our entire contract of, I want to emit a file every 30 seconds, it's broken. We cannot do that. So, the solution we went for is actually mailbox are a tiny bit more complicated. They have two queues. One is a low priority queue, the usual one. And we have a high priority queue. And the scheduler will put that in the high priority queue. So, as soon as the actor has finished dealing with the message that it was processing at the time, it will jump on this scheduled message. And we will get our nice 30 seconds call back. Testability, let me check the time to know. If I'm good or not, 20 minutes, perfect. Testability, we have a bunch of solutions to write tests. Let's go through code to see, like, actual RISD code to see how we can implement complicated real-life stuff and unit test it. So, the code that we will look at is a batch builder that is mimicking pretty much what we do in indexing. So, we have two possible conditions. We emit a file either because we have enough data and it's enough to cut a file. So, let's say if you have 100 messages, it's not 100 in relative, but if you have 100 messages, then you emit a file. Or, if 30 seconds has elapsed in the reception of the first document of the batch. So, let's start slow and easy. So, we have our actor here. So, this is the state of the actor, but this is the actor as well. So, something that is obvious is it will have to have some mailbox to push the speed that it produced to. It will have a document batch which will be a vex of string. So, document will be just string. It will append document to that. When it's big enough, it's now flash it and send it to the mailbox of the consumer. And one thing that is new here is we added some counters and we will be able in the unit test to do some assert on this internal state. I didn't talk about it, but the actor trait actually has an associated type which is called observable state. Of course, the whole idea of actors is to encapsulate your state into your thread or your token task and you're not supposed to be able to mutate it or even read it from the external world. But we have some thing that makes it possible to ask from outside the actor what is your observable state and it will send a message to the actor and the actor will send back the result of the observable state method here which is nifty for observability and for unit test. And then there is our handler. So, we will have two messages. One message will be receiving a document here. It was just a string as I told you. I wanted to keep stuff simple. And we will do several things. The first thing that we do is if this was the first document in the batch, we need to register our callback message using the schedule self message. We will append this document to our batch and then we check for the conditions. We have enough documents in the batch to actually emit a batch using our second batch emission condition. And in that case, we call emit batch. I didn't put the code of emit batch because it was too easy. Not very interesting. And then I didn't put the handler of the time out message, but you can guess it basically it's emitting the batch. And then when we want to unit that stuff, we write things like this. So, we have a universe in our unit test. It's a very important thing. We want to isolate our unit test one from each other. And the universe is in charge of this. So, all of the actors of your program have to belong to the same universe. Otherwise, they're not supposed to communicate together. And we will see that this isolation will make it possible to do something really cool in the next slide. And so, this universe makes it possible to make a fake mailbox that we create like the consumer side of things. We can create our batch builder and it's alone and send message to it. That's what we do there. So, yeah, I usually like to jump and point at the screen, but I've been told that I cannot cross the wait line. So, yeah, and then what we do when we want to create an assert is that we call this function called process pending and observe, which just means that we wait for all of the messages that are currently in the queue of the actor. We have all of those process and then we call observe and we get a snapshot of what is the observable state of the actor. And here, the observable state was a counter, so we check that it's equal to the number of documents that we wanted. And then we check that the consumer mailbox does contain our two batches, because 250 is 100 and 102 batches. And we also want to be able to check the timeout, because the timeout counter is working well. So, here, what is interesting is we created our universe, but with the method with accelerated time, and we would be marking time at this point. So, you won't have to wait 30 seconds to have your unit test to run for 30 seconds. It will do magic and the result will be exactly the same as if you were not accelerating time, but it will just be faster. And I will explain a little bit how it works. And so, for the unit test, obviously, we have to call some way to wait, and we call universe.sleep to do that. And it's important to use the universe.sleep and time.sleep because we are marking stuff, obviously. We cannot use the marking facilities that we have in Tokyo because we use several runtimes. And also, what we do is similar in semantics as pose and auto-advance if you're familiar with it, except we never freeze time. We keep time flowing, but what we do is tiny bit different. So, you can imagine that if you were not accelerating time, your actor execution would look like this. So, actor are processing stuff, and sometimes you don't have any message in any queue, or actor are either, and the only thing that will resume the processing is some time out to happen and the scheduler saying, okay, I have a message for you, you asked for a self-scheduled message. It's happening now. So, our framework detects that we are in a phase where no one is working and waiting for the scheduler, and in that case, and only in that case, we accelerate time. And that's why we get a result that is exactly the same as if we didn't accelerate time, but just faster. So, we compress our execution before, after. That's how it works. I wanted to show you the actual indexing pipeline in its full complexity. I said 25 actors, it's not 25 actors here, but we have other actors in other parts of the code because the pattern got quite popular. It's quite complex, but it makes me extremely good. It makes me feel good to be able to show that when we have to explain a new developer what the indexing is doing. We can point at things. Every single one of these box is doing one very simple thing. It has its own file, it has its own unit test. It makes me happy to have this very simple figure that we can discuss around. One thing that I need to talk to you about is one problem with Actors is if you have cycles in your Actors network, you might experience deadlock. And it's a pretty terrible thing that kind of deadlock because it can happen at any time in production, like it can work for one week and then you experience a deadlock and it's a scary thing. So there's a sufficient condition to have deadlocks. If you don't have any cycles, right? And usually that's the case when you are writing a pipeline. In the graph before, there was actually a cycle. We will have a look at it in a second. There is another, there is a nicer condition to have deadlocks. It's if the graph of your Actors where you removed all of the queue where you had an infinite capacity, if that one is a DAG, then you won't have deadlocks. And that's what we are doing here. So the loop, the cycle that we had was due to the fact that we have like an auxiliary pipeline that is merging the file together and there is an arrow over there. Sorry, I'm going to cross the line. I did it, I apologize. If you remove this arrow, then it's a DAG and that's sufficient condition to never experience deadlock. It helps me sleep at night. And yeah, we have a bunch of other features. The most important one I think I want to tell you about is that we measure back pressure. So the framework is automatically measuring back pressure and expose it as a promoter counter. That's really neat. Very useful for us. And let's use the rest of the time for questions. So... APPLAUSE So are there any questions in the room? Yes. The last slide of the previous slide was you didn't need parallel actors. What did you need like Fanny and Fanna out for having any parallel work? Oh, yes. So there is something that I didn't read but Sylvain was very fast and noticed that we don't have anything to be able to have several actor work on the same queue or work concurrently to process stuff faster. So yeah, strongly, we haven't needed that strongly enough to actually implement it. I wrote an implementation and never managed it because we didn't really need it. So indexing, we just spawn several pipelines on the same machine, not too much. So that part, it's unparalleled. But yeah, we just never need... We haven't needed it yet. I can't really tell where. Yes, exactly. So the parallel behavior. Sorry, you want me to repeat. So Sylvain was saying, we use more than one core just because within the pipeline we do the streamlining thing. So we may have different actors that are consuming the CPU at the same time but we don't have one actor going, oh, there's actually five instance of the actor and we are doing the work five times faster. So we didn't need that. Any more questions? Yes? Do you have the fairness system so that one actor doesn't keep on the processing restriction other orders in the process? So no, we don't have that. So one thing that we have, actually we don't want... We have the opposite problem. We don't want fairness. So if you look at our pipeline, the stuff that is taking a lot of CPU would be the indexer. Sylvain would take a lot of IO and we want to give it priority because it's the thing that we want it to use as much as CPU as it can. So we want to give it one core and we want it to use as much IO as it needs. And we would like to give it priority. So the way we do that is that we run it on a specific runtime and over there it has its dedicated core. For IO, the framework is actually not really helping. So what we do is that we have some IO throttling that makes it so that the other actors are not able to actually more write on disk faster and you can configure that and there's some corner of the table computation to compute what you should do. But yeah, other actors will not be able to write on disk faster than, let's say, 80 megabytes per second. And the merge that you have below, it's okay if it lags a little bit. That's the part that we want to be low priority and the part on the top we want to be high priority. So we don't have any fairness and we don't want any fairness. Yes? So I guess you're supervising that because otherwise the timeouts may also be delayed. So the supervisor is running on, it's very fast, it doesn't do much. So it's running on a Tokyo runtime that has a dedicated core and runs one bazillion actors, but they're all very light. So it doesn't matter at all. Okay, yeah. Yeah. Because, I mean, if your actors are very heavy now, of course, the supervisor will catch you. Yeah. At some point, because otherwise your timeouts will still be delayed. Yeah, absolutely. You're absolutely right, but the thing is it's running on its specific runtime and it's not CPU-HV, so there's plenty of core to work with. Yes? When you accelerate time in the testing universe, do you have to specify the steps in time you take? You mean the number of times? I assume that when you accelerate time, basically when nothing is happening, you take a time step and then see if something would have happened at that time point. Does that mean that you have to specify, we take steps of 100 milliseconds and then test every time if something would be happening now? No, it's not. We don't need to say how many steps we take. We don't need to say what is the resolution of the steps that we take. So the only thing that we do is that the scheduler, when it detects, it needs to accelerate time. It has some kind of heap that says, OK, the next event is actually in 70 milliseconds. So let's jump 70 milliseconds in the future and it triggers that event. And then the execution of the actor that was supposed to receive this message will go and if no actor is working anymore, then we re-accidate them again. So it's no steps, no resolution or nothing. Yes? How about reliability if you want to be sure that the bot line will make it to the index, you count them or how do you know they made it through? So, yeah, it should be the subject of another talk. Because now that's a super interesting question. So the pipeline, you want to know, to have an idea of what kind of semantics, delivery semantics that you want to have. We actually offer exactly one semantics and the way we deal with that is, so we didn't talk about that, we have the documents that are coming from a source actor and the source actors, when we spawn it, we tell it, OK, you need to stream messages from these specific checkpoints. And when we publish stuff like downstream, we publish stuff by running a transaction on our metadata store backend and that transaction updates the checkpoints of the stuff that we have published and it publishes the speed as well. So that when we restart everything, we can check in the metadata store what is the last checkpoint and it starts from there. And if there is an error, the metadata store will just yell at us and return an error. It will say, OK, no, something weird has been happening, maybe we all had two pipelines working at the same time and your checkpoints are overlapping and you have a problem. That's the way we work with this problem. Yes? At the universe, is that a special crate or is it in the standard category? No, the universe thing is something within our framework and that's what we use to be able to isolate typically different programs or different unit tests or different systems. Yeah, it's within our active framework. It is. Do we still have time? I think we have one more question or one more minute. I think there was a... Yes. I understand on this graph, the numbers on the rows indicate the capacity of the queue, right? The capacity of the channel between the actors, right? The numbers, yes. Yes. So we have a lot of tuning points in this system, right? Yes. With relation to the back pressure. So the question is, from your experience, how sensitive the performance of the system is to the tuning of back pressure on the channels? And maybe you have some kind of advice or a rule of thumb on what to choose for the best performance. Yes. So the question was, on this slide, all of the little numbers that we have on the arrow is the capacity of the different queues between actors that's a lot of parameters to tune. They probably have an impact on performance. Is there a cool recipe to... So the first question was, how much do they impact performance? And the second one is, do we have a nice recipe to be able to tune them maybe automatically? I'll go first because there is no more time. So they don't impact performance all that much as long as you got them a little bit correct. So you usually need to identify the stuff that should be at one, and then you put it at one and where you want a little bit of capacity. It should be quite obvious if you know your system. And I'm sure that there is a nice recipe to auto-detect that. I haven't found it. So if you have ideas, I'd love to... Usually that kind of question is someone who is thinking about something. So please come to me after the talk. And I'd love to hear your thoughts. Thank you, everyone. Time is up. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " Let's write in, we don't have much time.", "tokens": [961, 311, 2464, 294, 11, 321, 500, 380, 362, 709, 565, 13], "temperature": 0.0, "avg_logprob": -0.34238543510437014, "compression_ratio": 1.3641304347826086, "no_speech_prob": 0.29786479473114014}, {"id": 1, "seek": 0, "start": 13.0, "end": 14.0, "text": " My name is Paul Mazurel.", "tokens": [1222, 1315, 307, 4552, 28568, 540, 75, 13], "temperature": 0.0, "avg_logprob": -0.34238543510437014, "compression_ratio": 1.3641304347826086, "no_speech_prob": 0.29786479473114014}, {"id": 2, "seek": 0, "start": 14.0, "end": 18.2, "text": " I go by the name of Filmi Coton on Mastonon on Twitter.", "tokens": [286, 352, 538, 264, 1315, 295, 7905, 3057, 383, 27794, 322, 376, 525, 266, 266, 322, 5794, 13], "temperature": 0.0, "avg_logprob": -0.34238543510437014, "compression_ratio": 1.3641304347826086, "no_speech_prob": 0.29786479473114014}, {"id": 3, "seek": 0, "start": 18.2, "end": 22.400000000000002, "text": " I've been a rest developer since 2016.", "tokens": [286, 600, 668, 257, 1472, 10754, 1670, 6549, 13], "temperature": 0.0, "avg_logprob": -0.34238543510437014, "compression_ratio": 1.3641304347826086, "no_speech_prob": 0.29786479473114014}, {"id": 4, "seek": 0, "start": 22.400000000000002, "end": 27.88, "text": " I spent most of my career as a search developer, so it was only natural for me as my first", "tokens": [286, 4418, 881, 295, 452, 3988, 382, 257, 3164, 10754, 11, 370, 309, 390, 787, 3303, 337, 385, 382, 452, 700], "temperature": 0.0, "avg_logprob": -0.34238543510437014, "compression_ratio": 1.3641304347826086, "no_speech_prob": 0.29786479473114014}, {"id": 5, "seek": 2788, "start": 27.88, "end": 31.88, "text": " pet project to develop a library to build search engines.", "tokens": [3817, 1716, 281, 1499, 257, 6405, 281, 1322, 3164, 12982, 13], "temperature": 0.0, "avg_logprob": -0.21553549655648166, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.0005351753206923604}, {"id": 6, "seek": 2788, "start": 31.88, "end": 35.879999999999995, "text": " So if you're familiar with Lucine, that's kind of like Lucine, but for the rest of the world.", "tokens": [407, 498, 291, 434, 4963, 365, 9593, 533, 11, 300, 311, 733, 295, 411, 9593, 533, 11, 457, 337, 264, 1472, 295, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.21553549655648166, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.0005351753206923604}, {"id": 7, "seek": 2788, "start": 35.879999999999995, "end": 37.879999999999995, "text": " That was my first pet project.", "tokens": [663, 390, 452, 700, 3817, 1716, 13], "temperature": 0.0, "avg_logprob": -0.21553549655648166, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.0005351753206923604}, {"id": 8, "seek": 2788, "start": 37.879999999999995, "end": 38.879999999999995, "text": " It grew.", "tokens": [467, 6109, 13], "temperature": 0.0, "avg_logprob": -0.21553549655648166, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.0005351753206923604}, {"id": 9, "seek": 2788, "start": 38.879999999999995, "end": 53.879999999999995, "text": " And two years ago, I co-founded a startup called Quit Quit, that is about building a search engine that is specialized for logs.", "tokens": [400, 732, 924, 2057, 11, 286, 598, 12, 49547, 257, 18578, 1219, 50139, 50139, 11, 300, 307, 466, 2390, 257, 3164, 2848, 300, 307, 19813, 337, 20820, 13], "temperature": 0.0, "avg_logprob": -0.21553549655648166, "compression_ratio": 1.5165876777251184, "no_speech_prob": 0.0005351753206923604}, {"id": 10, "seek": 5388, "start": 53.88, "end": 55.88, "text": " Just a word about Quit Quit.", "tokens": [1449, 257, 1349, 466, 50139, 50139, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 11, "seek": 5388, "start": 55.88, "end": 57.88, "text": " I'm not here to do an advertisement.", "tokens": [286, 478, 406, 510, 281, 360, 364, 31370, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 12, "seek": 5388, "start": 57.88, "end": 64.88, "text": " It's not a commercial, but it's related to the tool, so I need to explain to you what's the problem, which is the problem that we are trying to solve.", "tokens": [467, 311, 406, 257, 6841, 11, 457, 309, 311, 4077, 281, 264, 2290, 11, 370, 286, 643, 281, 2903, 281, 291, 437, 311, 264, 1154, 11, 597, 307, 264, 1154, 300, 321, 366, 1382, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 13, "seek": 5388, "start": 64.88, "end": 67.88, "text": " Quit Quit is a Rust project.", "tokens": [50139, 50139, 307, 257, 34952, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 14, "seek": 5388, "start": 67.88, "end": 70.88, "text": " It's under the AGPL license.", "tokens": [467, 311, 833, 264, 28406, 21593, 10476, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 15, "seek": 5388, "start": 70.88, "end": 71.88, "text": " It's open source.", "tokens": [467, 311, 1269, 4009, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 16, "seek": 5388, "start": 71.88, "end": 75.88, "text": " You can find the source code on GitHub.", "tokens": [509, 393, 915, 264, 4009, 3089, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 17, "seek": 5388, "start": 75.88, "end": 78.88, "text": " And so we specialize on log search.", "tokens": [400, 370, 321, 37938, 322, 3565, 3164, 13], "temperature": 0.0, "avg_logprob": -0.1511826338591399, "compression_ratio": 1.6, "no_speech_prob": 0.00026075594360008836}, {"id": 18, "seek": 7888, "start": 78.88, "end": 88.88, "text": " So what's specific about log search compared to, let's say, an e-commerce search engine is that the data that we get is more or less immutable.", "tokens": [407, 437, 311, 2685, 466, 3565, 3164, 5347, 281, 11, 718, 311, 584, 11, 364, 308, 12, 26926, 3164, 2848, 307, 300, 264, 1412, 300, 321, 483, 307, 544, 420, 1570, 3397, 32148, 13], "temperature": 0.0, "avg_logprob": -0.06527266659579434, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.0003458352875895798}, {"id": 19, "seek": 7888, "start": 88.88, "end": 95.88, "text": " So we assume that people will want to ingest documents into our search engine and won't modify it too much.", "tokens": [407, 321, 6552, 300, 561, 486, 528, 281, 3957, 377, 8512, 666, 527, 3164, 2848, 293, 1582, 380, 16927, 309, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.06527266659579434, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.0003458352875895798}, {"id": 20, "seek": 7888, "start": 95.88, "end": 104.88, "text": " So after ingestion, the document stays there until it goes out of its retention period, at which point we will delete it.", "tokens": [407, 934, 3957, 31342, 11, 264, 4166, 10834, 456, 1826, 309, 1709, 484, 295, 1080, 22871, 2896, 11, 412, 597, 935, 321, 486, 12097, 309, 13], "temperature": 0.0, "avg_logprob": -0.06527266659579434, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.0003458352875895798}, {"id": 21, "seek": 10488, "start": 104.88, "end": 110.88, "text": " Or maybe you might want to delete it if you have a request to comply to GDPR.", "tokens": [1610, 1310, 291, 1062, 528, 281, 12097, 309, 498, 291, 362, 257, 5308, 281, 27956, 281, 19599, 49, 13], "temperature": 0.0, "avg_logprob": -0.0804634326841773, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.00020954286446794868}, {"id": 22, "seek": 10488, "start": 110.88, "end": 117.88, "text": " We handle that kind of stuff, but you cannot modify it like you would do for any e-commerce website.", "tokens": [492, 4813, 300, 733, 295, 1507, 11, 457, 291, 2644, 16927, 309, 411, 291, 576, 360, 337, 604, 308, 12, 26926, 3144, 13], "temperature": 0.0, "avg_logprob": -0.0804634326841773, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.00020954286446794868}, {"id": 23, "seek": 10488, "start": 117.88, "end": 129.88, "text": " And so one of the big differences in terms of efficiency is that when you deal with log search, the volume that you have to deal with has no limits.", "tokens": [400, 370, 472, 295, 264, 955, 7300, 294, 2115, 295, 10493, 307, 300, 562, 291, 2028, 365, 3565, 3164, 11, 264, 5523, 300, 291, 362, 281, 2028, 365, 575, 572, 10406, 13], "temperature": 0.0, "avg_logprob": -0.0804634326841773, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.00020954286446794868}, {"id": 24, "seek": 12988, "start": 129.88, "end": 137.88, "text": " The largest amount that we've seen so far is people indexing 100 terabytes a day.", "tokens": [440, 6443, 2372, 300, 321, 600, 1612, 370, 1400, 307, 561, 8186, 278, 2319, 1796, 24538, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.12352858890186656, "compression_ratio": 1.5648148148148149, "no_speech_prob": 8.020409586606547e-05}, {"id": 25, "seek": 12988, "start": 137.88, "end": 139.88, "text": " So that's the volume of data.", "tokens": [407, 300, 311, 264, 5523, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12352858890186656, "compression_ratio": 1.5648148148148149, "no_speech_prob": 8.020409586606547e-05}, {"id": 26, "seek": 12988, "start": 139.88, "end": 143.88, "text": " Imagine if it was actually generated not by machine but by humans.", "tokens": [11739, 498, 309, 390, 767, 10833, 406, 538, 3479, 457, 538, 6255, 13], "temperature": 0.0, "avg_logprob": -0.12352858890186656, "compression_ratio": 1.5648148148148149, "no_speech_prob": 8.020409586606547e-05}, {"id": 27, "seek": 12988, "start": 143.88, "end": 148.88, "text": " You would have to have a lot of people typing grief as to deal with that kind of volume.", "tokens": [509, 576, 362, 281, 362, 257, 688, 295, 561, 18444, 18998, 382, 281, 2028, 365, 300, 733, 295, 5523, 13], "temperature": 0.0, "avg_logprob": -0.12352858890186656, "compression_ratio": 1.5648148148148149, "no_speech_prob": 8.020409586606547e-05}, {"id": 28, "seek": 12988, "start": 148.88, "end": 152.88, "text": " So that's something that you will only get if you're doing log search.", "tokens": [407, 300, 311, 746, 300, 291, 486, 787, 483, 498, 291, 434, 884, 3565, 3164, 13], "temperature": 0.0, "avg_logprob": -0.12352858890186656, "compression_ratio": 1.5648148148148149, "no_speech_prob": 8.020409586606547e-05}, {"id": 29, "seek": 15288, "start": 152.88, "end": 163.88, "text": " And compared to any e-commerce website, most of the CPU is actually spent indexing and not searching because you have comparatively way less search and way more documents.", "tokens": [400, 5347, 281, 604, 308, 12, 26926, 3144, 11, 881, 295, 264, 13199, 307, 767, 4418, 8186, 278, 293, 406, 10808, 570, 291, 362, 6311, 19020, 636, 1570, 3164, 293, 636, 544, 8512, 13], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 30, "seek": 15288, "start": 163.88, "end": 171.88, "text": " So indexing is actually crucial to our problems, and that's very different from usual search engines.", "tokens": [407, 8186, 278, 307, 767, 11462, 281, 527, 2740, 11, 293, 300, 311, 588, 819, 490, 7713, 3164, 12982, 13], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 31, "seek": 15288, "start": 171.88, "end": 174.88, "text": " Indexing, what does it look like?", "tokens": [33552, 278, 11, 437, 775, 309, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 32, "seek": 15288, "start": 174.88, "end": 176.88, "text": " That's the problem that we are trying to solve.", "tokens": [663, 311, 264, 1154, 300, 321, 366, 1382, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 33, "seek": 15288, "start": 176.88, "end": 178.88, "text": " Super oversimplified.", "tokens": [4548, 15488, 332, 564, 2587, 13], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 34, "seek": 15288, "start": 178.88, "end": 181.88, "text": " We get, as an input, a stream of documents.", "tokens": [492, 483, 11, 382, 364, 4846, 11, 257, 4309, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.10670418109533922, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.88130094204098e-05}, {"id": 35, "seek": 18188, "start": 181.88, "end": 185.88, "text": " It's interesting to have another idea for one pipeline of indexing.", "tokens": [467, 311, 1880, 281, 362, 1071, 1558, 337, 472, 15517, 295, 8186, 278, 13], "temperature": 0.0, "avg_logprob": -0.1255768140157064, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.00012914401304442436}, {"id": 36, "seek": 18188, "start": 185.88, "end": 190.88, "text": " We have to deal with around 40 megabytes per second.", "tokens": [492, 362, 281, 2028, 365, 926, 3356, 10816, 24538, 680, 1150, 13], "temperature": 0.0, "avg_logprob": -0.1255768140157064, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.00012914401304442436}, {"id": 37, "seek": 18188, "start": 190.88, "end": 197.88, "text": " And as an output, every 30 seconds, we write a file.", "tokens": [400, 382, 364, 5598, 11, 633, 2217, 3949, 11, 321, 2464, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1255768140157064, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.00012914401304442436}, {"id": 38, "seek": 18188, "start": 197.88, "end": 209.88, "text": " We put it somewhere, and usually we register it on some metadata backends.", "tokens": [492, 829, 309, 4079, 11, 293, 2673, 321, 7280, 309, 322, 512, 26603, 646, 2581, 13], "temperature": 0.0, "avg_logprob": -0.1255768140157064, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.00012914401304442436}, {"id": 39, "seek": 20988, "start": 209.88, "end": 212.88, "text": " And at this point, the file is searchable.", "tokens": [400, 412, 341, 935, 11, 264, 3991, 307, 3164, 712, 13], "temperature": 0.0, "avg_logprob": -0.09482409222291248, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00032675216789357364}, {"id": 40, "seek": 20988, "start": 212.88, "end": 217.88, "text": " And the rules of the game here is we want to have the highest possible throughput.", "tokens": [400, 264, 4474, 295, 264, 1216, 510, 307, 321, 528, 281, 362, 264, 6343, 1944, 44629, 13], "temperature": 0.0, "avg_logprob": -0.09482409222291248, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00032675216789357364}, {"id": 41, "seek": 20988, "start": 217.88, "end": 222.88, "text": " And we want to keep what we call time to search as low as possible.", "tokens": [400, 321, 528, 281, 1066, 437, 321, 818, 565, 281, 3164, 382, 2295, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.09482409222291248, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00032675216789357364}, {"id": 42, "seek": 20988, "start": 222.88, "end": 230.88, "text": " So time to search is at the moment when JSON file is entering the system, we start the clock,", "tokens": [407, 565, 281, 3164, 307, 412, 264, 1623, 562, 31828, 3991, 307, 11104, 264, 1185, 11, 321, 722, 264, 7830, 11], "temperature": 0.0, "avg_logprob": -0.09482409222291248, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00032675216789357364}, {"id": 43, "seek": 20988, "start": 230.88, "end": 235.88, "text": " and we measure how long it takes for it to go out of the system in the form of one of those files,", "tokens": [293, 321, 3481, 577, 938, 309, 2516, 337, 309, 281, 352, 484, 295, 264, 1185, 294, 264, 1254, 295, 472, 295, 729, 7098, 11], "temperature": 0.0, "avg_logprob": -0.09482409222291248, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00032675216789357364}, {"id": 44, "seek": 23588, "start": 235.88, "end": 239.88, "text": " at which point it is searchable. We need that to be as low as possible,", "tokens": [412, 597, 935, 309, 307, 3164, 712, 13, 492, 643, 300, 281, 312, 382, 2295, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 45, "seek": 23588, "start": 239.88, "end": 243.88, "text": " and we need, it's very important to keep it very stable.", "tokens": [293, 321, 643, 11, 309, 311, 588, 1021, 281, 1066, 309, 588, 8351, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 46, "seek": 23588, "start": 243.88, "end": 248.88, "text": " We don't want to have, like, a period of time where it goes through the roof.", "tokens": [492, 500, 380, 528, 281, 362, 11, 411, 11, 257, 2896, 295, 565, 689, 309, 1709, 807, 264, 8418, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 47, "seek": 23588, "start": 248.88, "end": 251.88, "text": " So that's the whole game.", "tokens": [407, 300, 311, 264, 1379, 1216, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 48, "seek": 23588, "start": 251.88, "end": 253.88, "text": " And in that black box, we do a lot of stuff.", "tokens": [400, 294, 300, 2211, 2424, 11, 321, 360, 257, 688, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 49, "seek": 23588, "start": 253.88, "end": 256.88, "text": " I was voluntarily very simple.", "tokens": [286, 390, 41782, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 50, "seek": 23588, "start": 256.88, "end": 258.88, "text": " I won't go through all of the stuff that we do,", "tokens": [286, 1582, 380, 352, 807, 439, 295, 264, 1507, 300, 321, 360, 11], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 51, "seek": 23588, "start": 258.88, "end": 264.88, "text": " but the important part here is every single of those steps is using different resources.", "tokens": [457, 264, 1021, 644, 510, 307, 633, 2167, 295, 729, 4439, 307, 1228, 819, 3593, 13], "temperature": 0.0, "avg_logprob": -0.10715226745605469, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00016912379942368716}, {"id": 52, "seek": 26488, "start": 264.88, "end": 266.88, "text": " The time is spent on different stuff.", "tokens": [440, 565, 307, 4418, 322, 819, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 53, "seek": 26488, "start": 266.88, "end": 270.88, "text": " So, for instance, when we index things, when we build or in memory index,", "tokens": [407, 11, 337, 5197, 11, 562, 321, 8186, 721, 11, 562, 321, 1322, 420, 294, 4675, 8186, 11], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 54, "seek": 26488, "start": 270.88, "end": 271.88, "text": " we are spending CPU.", "tokens": [321, 366, 6434, 13199, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 55, "seek": 26488, "start": 271.88, "end": 273.88, "text": " When we are writing the file, we use IO.", "tokens": [1133, 321, 366, 3579, 264, 3991, 11, 321, 764, 39839, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 56, "seek": 26488, "start": 273.88, "end": 279.88, "text": " When we upload with network, and sometimes we are waiting for something that is outside of the system.", "tokens": [1133, 321, 6580, 365, 3209, 11, 293, 2171, 321, 366, 3806, 337, 746, 300, 307, 2380, 295, 264, 1185, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 57, "seek": 26488, "start": 279.88, "end": 283.88, "text": " We spend no resources at all except we wait.", "tokens": [492, 3496, 572, 3593, 412, 439, 3993, 321, 1699, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 58, "seek": 26488, "start": 283.88, "end": 287.88, "text": " So you might think that the implementation is obvious.", "tokens": [407, 291, 1062, 519, 300, 264, 11420, 307, 6322, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 59, "seek": 26488, "start": 287.88, "end": 291.88, "text": " We have one function for all of those steps, and we call them sequentially.", "tokens": [492, 362, 472, 2445, 337, 439, 295, 729, 4439, 11, 293, 321, 818, 552, 5123, 3137, 13], "temperature": 0.0, "avg_logprob": -0.14145419128939637, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.000186825986020267}, {"id": 60, "seek": 29188, "start": 291.88, "end": 295.88, "text": " But if you do that, you are wasting the resources of our system, of course.", "tokens": [583, 498, 291, 360, 300, 11, 291, 366, 20457, 264, 3593, 295, 527, 1185, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 61, "seek": 29188, "start": 295.88, "end": 300.88, "text": " For instance, when you are uploading, you are spending your network resource,", "tokens": [1171, 5197, 11, 562, 291, 366, 27301, 11, 291, 366, 6434, 428, 3209, 7684, 11], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 62, "seek": 29188, "start": 300.88, "end": 305.88, "text": " but your CPU is not doing anything, so you're wasting money.", "tokens": [457, 428, 13199, 307, 406, 884, 1340, 11, 370, 291, 434, 20457, 1460, 13], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 63, "seek": 29188, "start": 305.88, "end": 308.88, "text": " So the solution to this problem is relatively simple,", "tokens": [407, 264, 3827, 281, 341, 1154, 307, 7226, 2199, 11], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 64, "seek": 29188, "start": 308.88, "end": 310.88, "text": " but it's not that simple to implement.", "tokens": [457, 309, 311, 406, 300, 2199, 281, 4445, 13], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 65, "seek": 29188, "start": 310.88, "end": 313.88, "text": " You want to streamline your pipeline.", "tokens": [509, 528, 281, 47141, 428, 15517, 13], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 66, "seek": 29188, "start": 313.88, "end": 319.88, "text": " What I mean by streamlining in a very concrete way is you might have two steps,", "tokens": [708, 286, 914, 538, 4309, 31079, 294, 257, 588, 9859, 636, 307, 291, 1062, 362, 732, 4439, 11], "temperature": 0.0, "avg_logprob": -0.07303088179258542, "compression_ratio": 1.6932270916334662, "no_speech_prob": 5.001797035220079e-05}, {"id": 67, "seek": 31988, "start": 319.88, "end": 324.88, "text": " like indexing, spending CPU, upload, spending network.", "tokens": [411, 8186, 278, 11, 6434, 13199, 11, 6580, 11, 6434, 3209, 13], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 68, "seek": 31988, "start": 324.88, "end": 330.88, "text": " They go sequentially, but what you want is you want indexing to work on building the first file,", "tokens": [814, 352, 5123, 3137, 11, 457, 437, 291, 528, 307, 291, 528, 8186, 278, 281, 589, 322, 2390, 264, 700, 3991, 11], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 69, "seek": 31988, "start": 330.88, "end": 334.88, "text": " and when it has finished, you start uploading, of course.", "tokens": [293, 562, 309, 575, 4335, 11, 291, 722, 27301, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 70, "seek": 31988, "start": 334.88, "end": 337.88, "text": " But as you upload, you want to start working on the second file,", "tokens": [583, 382, 291, 6580, 11, 291, 528, 281, 722, 1364, 322, 264, 1150, 3991, 11], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 71, "seek": 31988, "start": 337.88, "end": 342.88, "text": " so that you are spending CPU while you are doing your network stuff.", "tokens": [370, 300, 291, 366, 6434, 13199, 1339, 291, 366, 884, 428, 3209, 1507, 13], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 72, "seek": 31988, "start": 342.88, "end": 346.88, "text": " That's what the kind of behavior that we want.", "tokens": [663, 311, 437, 264, 733, 295, 5223, 300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.08100767418889716, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.00013979907089378685}, {"id": 73, "seek": 34688, "start": 346.88, "end": 349.88, "text": " And of course, this example is a little bit too simple", "tokens": [400, 295, 1164, 11, 341, 1365, 307, 257, 707, 857, 886, 2199], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 74, "seek": 34688, "start": 349.88, "end": 352.88, "text": " because the second step here is shorter than the first one.", "tokens": [570, 264, 1150, 1823, 510, 307, 11639, 813, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 75, "seek": 34688, "start": 352.88, "end": 355.88, "text": " It's a nice case, but if it was the other way around,", "tokens": [467, 311, 257, 1481, 1389, 11, 457, 498, 309, 390, 264, 661, 636, 926, 11], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 76, "seek": 34688, "start": 355.88, "end": 363.88, "text": " we would have to have some kind of mechanism to deal with, to have back pressure.", "tokens": [321, 576, 362, 281, 362, 512, 733, 295, 7513, 281, 2028, 365, 11, 281, 362, 646, 3321, 13], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 77, "seek": 34688, "start": 363.88, "end": 368.88, "text": " And in my experience, a lot of very good engineers are not familiar with the concept of back pressure,", "tokens": [400, 294, 452, 1752, 11, 257, 688, 295, 588, 665, 11955, 366, 406, 4963, 365, 264, 3410, 295, 646, 3321, 11], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 78, "seek": 34688, "start": 368.88, "end": 371.88, "text": " so let me explain what it is about.", "tokens": [370, 718, 385, 2903, 437, 309, 307, 466, 13], "temperature": 0.0, "avg_logprob": -0.08601824127801574, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.00015297501522582024}, {"id": 79, "seek": 37188, "start": 371.88, "end": 380.88, "text": " If you already know what it is, bear with me and enjoy the fine artworks that we have here.", "tokens": [759, 291, 1217, 458, 437, 309, 307, 11, 6155, 365, 385, 293, 2103, 264, 2489, 15829, 82, 300, 321, 362, 510, 13], "temperature": 0.0, "avg_logprob": -0.10502417385578156, "compression_ratio": 1.4969325153374233, "no_speech_prob": 7.01529934303835e-05}, {"id": 80, "seek": 37188, "start": 380.88, "end": 387.88, "text": " So the idea of back pressure is, imagine you are cleaning dishes with a friend.", "tokens": [407, 264, 1558, 295, 646, 3321, 307, 11, 3811, 291, 366, 8924, 10814, 365, 257, 1277, 13], "temperature": 0.0, "avg_logprob": -0.10502417385578156, "compression_ratio": 1.4969325153374233, "no_speech_prob": 7.01529934303835e-05}, {"id": 81, "seek": 37188, "start": 387.88, "end": 393.88, "text": " One of you is cleaning the plates, and the other one is wiping them dry.", "tokens": [1485, 295, 291, 307, 8924, 264, 14231, 11, 293, 264, 661, 472, 307, 40611, 552, 4016, 13], "temperature": 0.0, "avg_logprob": -0.10502417385578156, "compression_ratio": 1.4969325153374233, "no_speech_prob": 7.01529934303835e-05}, {"id": 82, "seek": 39388, "start": 393.88, "end": 401.88, "text": " And the person wiping the dishes dry is a little bit too slow compared to the person cleaning the dishes.", "tokens": [400, 264, 954, 40611, 264, 10814, 4016, 307, 257, 707, 857, 886, 2964, 5347, 281, 264, 954, 8924, 264, 10814, 13], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 83, "seek": 39388, "start": 401.88, "end": 405.88, "text": " What's going to happen is that your plates will accumulate like forever.", "tokens": [708, 311, 516, 281, 1051, 307, 300, 428, 14231, 486, 33384, 411, 5680, 13], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 84, "seek": 39388, "start": 405.88, "end": 408.88, "text": " And in the computer system, it's a very common problem,", "tokens": [400, 294, 264, 3820, 1185, 11, 309, 311, 257, 588, 2689, 1154, 11], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 85, "seek": 39388, "start": 408.88, "end": 411.88, "text": " and that's how you get out of memory errors.", "tokens": [293, 300, 311, 577, 291, 483, 484, 295, 4675, 13603, 13], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 86, "seek": 39388, "start": 411.88, "end": 413.88, "text": " So the solution is rather simple.", "tokens": [407, 264, 3827, 307, 2831, 2199, 13], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 87, "seek": 39388, "start": 413.88, "end": 418.88, "text": " You need back pressure, which means you need some way to signal the person who is cleaning too fast", "tokens": [509, 643, 646, 3321, 11, 597, 1355, 291, 643, 512, 636, 281, 6358, 264, 954, 567, 307, 8924, 886, 2370], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 88, "seek": 39388, "start": 418.88, "end": 420.88, "text": " that they should slow down.", "tokens": [300, 436, 820, 2964, 760, 13], "temperature": 0.0, "avg_logprob": -0.0828404426574707, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.3668278522090986e-05}, {"id": 89, "seek": 42088, "start": 420.88, "end": 427.88, "text": " And the simplest mechanism to do this is you need to have some kind of limit on your stack of plates", "tokens": [400, 264, 22811, 7513, 281, 360, 341, 307, 291, 643, 281, 362, 512, 733, 295, 4948, 322, 428, 8630, 295, 14231], "temperature": 0.0, "avg_logprob": -0.10515295891534715, "compression_ratio": 1.6354679802955665, "no_speech_prob": 8.248044468928128e-05}, {"id": 90, "seek": 42088, "start": 427.88, "end": 432.88, "text": " or your work queue or whatever you are using in your system.", "tokens": [420, 428, 589, 18639, 420, 2035, 291, 366, 1228, 294, 428, 1185, 13], "temperature": 0.0, "avg_logprob": -0.10515295891534715, "compression_ratio": 1.6354679802955665, "no_speech_prob": 8.248044468928128e-05}, {"id": 91, "seek": 42088, "start": 432.88, "end": 435.88, "text": " And then they stop once they reach the limit.", "tokens": [400, 550, 436, 1590, 1564, 436, 2524, 264, 4948, 13], "temperature": 0.0, "avg_logprob": -0.10515295891534715, "compression_ratio": 1.6354679802955665, "no_speech_prob": 8.248044468928128e-05}, {"id": 92, "seek": 42088, "start": 435.88, "end": 438.88, "text": " So it's the simplest way you could have back pressure.", "tokens": [407, 309, 311, 264, 22811, 636, 291, 727, 362, 646, 3321, 13], "temperature": 0.0, "avg_logprob": -0.10515295891534715, "compression_ratio": 1.6354679802955665, "no_speech_prob": 8.248044468928128e-05}, {"id": 93, "seek": 42088, "start": 438.88, "end": 446.88, "text": " So with all this said, the game here is how would you implement that?", "tokens": [407, 365, 439, 341, 848, 11, 264, 1216, 510, 307, 577, 576, 291, 4445, 300, 30], "temperature": 0.0, "avg_logprob": -0.10515295891534715, "compression_ratio": 1.6354679802955665, "no_speech_prob": 8.248044468928128e-05}, {"id": 94, "seek": 44688, "start": 446.88, "end": 453.88, "text": " What would be your go-to implementation if you had to have such a system in one hour?", "tokens": [708, 576, 312, 428, 352, 12, 1353, 11420, 498, 291, 632, 281, 362, 1270, 257, 1185, 294, 472, 1773, 30], "temperature": 0.0, "avg_logprob": -0.12449310638092376, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.0003053783148061484}, {"id": 95, "seek": 44688, "start": 453.88, "end": 461.88, "text": " And being Rust developers, I think that most of the people in this room will come with the following solution.", "tokens": [400, 885, 34952, 8849, 11, 286, 519, 300, 881, 295, 264, 561, 294, 341, 1808, 486, 808, 365, 264, 3480, 3827, 13], "temperature": 0.0, "avg_logprob": -0.12449310638092376, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.0003053783148061484}, {"id": 96, "seek": 44688, "start": 461.88, "end": 468.88, "text": " So the upload part, it's not CPUV, it's just dealing with networks.", "tokens": [407, 264, 6580, 644, 11, 309, 311, 406, 13199, 53, 11, 309, 311, 445, 6260, 365, 9590, 13], "temperature": 0.0, "avg_logprob": -0.12449310638092376, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.0003053783148061484}, {"id": 97, "seek": 44688, "start": 468.88, "end": 474.88, "text": " So it's very natural to think, OK, I'm going to do that in a Tokyo task.", "tokens": [407, 309, 311, 588, 3303, 281, 519, 11, 2264, 11, 286, 478, 516, 281, 360, 300, 294, 257, 15147, 5633, 13], "temperature": 0.0, "avg_logprob": -0.12449310638092376, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.0003053783148061484}, {"id": 98, "seek": 47488, "start": 474.88, "end": 476.88, "text": " And back pressure, that's easy.", "tokens": [400, 646, 3321, 11, 300, 311, 1858, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 99, "seek": 47488, "start": 476.88, "end": 479.88, "text": " I already know what I'm going to use.", "tokens": [286, 1217, 458, 437, 286, 478, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 100, "seek": 47488, "start": 479.88, "end": 484.88, "text": " I feel that the good solution is I'm going to have a channel with some capacity.", "tokens": [286, 841, 300, 264, 665, 3827, 307, 286, 478, 516, 281, 362, 257, 2269, 365, 512, 6042, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 101, "seek": 47488, "start": 484.88, "end": 490.88, "text": " And once it reaches capacity, of course, people sending work to my task, they will have to wait.", "tokens": [400, 1564, 309, 14235, 6042, 11, 295, 1164, 11, 561, 7750, 589, 281, 452, 5633, 11, 436, 486, 362, 281, 1699, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 102, "seek": 47488, "start": 490.88, "end": 495.88, "text": " It's going to be very nice and natural.", "tokens": [467, 311, 516, 281, 312, 588, 1481, 293, 3303, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 103, "seek": 47488, "start": 495.88, "end": 500.88, "text": " And then on the indexing part, we will have the same mechanism.", "tokens": [400, 550, 322, 264, 8186, 278, 644, 11, 321, 486, 362, 264, 912, 7513, 13], "temperature": 0.0, "avg_logprob": -0.07656102819541066, "compression_ratio": 1.6325581395348838, "no_speech_prob": 4.6902583562768996e-05}, {"id": 104, "seek": 50088, "start": 500.88, "end": 506.88, "text": " And only right now, we will have to actually do a lot of CPU heavy work.", "tokens": [400, 787, 558, 586, 11, 321, 486, 362, 281, 767, 360, 257, 688, 295, 13199, 4676, 589, 13], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 105, "seek": 50088, "start": 506.88, "end": 511.88, "text": " So maybe we won't run that in a Tokyo task and we will spawn our own thread", "tokens": [407, 1310, 321, 1582, 380, 1190, 300, 294, 257, 15147, 5633, 293, 321, 486, 17088, 527, 1065, 7207], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 106, "seek": 50088, "start": 511.88, "end": 514.88, "text": " or maybe we will use a thread pool to do that work.", "tokens": [420, 1310, 321, 486, 764, 257, 7207, 7005, 281, 360, 300, 589, 13], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 107, "seek": 50088, "start": 514.88, "end": 516.88, "text": " It would be better, right?", "tokens": [467, 576, 312, 1101, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 108, "seek": 50088, "start": 516.88, "end": 517.88, "text": " And we use the same mechanism.", "tokens": [400, 321, 764, 264, 912, 7513, 13], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 109, "seek": 50088, "start": 517.88, "end": 519.88, "text": " We will have a channel to receive the work.", "tokens": [492, 486, 362, 257, 2269, 281, 4774, 264, 589, 13], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 110, "seek": 50088, "start": 519.88, "end": 524.88, "text": " The capacity here is much larger because the type of stuff that we put in the channel is very different.", "tokens": [440, 6042, 510, 307, 709, 4833, 570, 264, 2010, 295, 1507, 300, 321, 829, 294, 264, 2269, 307, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.10690455003218217, "compression_ratio": 1.6612244897959183, "no_speech_prob": 6.706719432258978e-05}, {"id": 111, "seek": 52488, "start": 524.88, "end": 530.88, "text": " For the uploader, we are getting files, possibly they can be large, like 100 megabytes.", "tokens": [1171, 264, 6580, 260, 11, 321, 366, 1242, 7098, 11, 6264, 436, 393, 312, 2416, 11, 411, 2319, 10816, 24538, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 112, "seek": 52488, "start": 530.88, "end": 533.88, "text": " So it makes sense to have it as small as possible.", "tokens": [407, 309, 1669, 2020, 281, 362, 309, 382, 1359, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 113, "seek": 52488, "start": 533.88, "end": 534.88, "text": " Here, it's documents.", "tokens": [1692, 11, 309, 311, 8512, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 114, "seek": 52488, "start": 534.88, "end": 539.88, "text": " So it's for many documents, you will emit one file.", "tokens": [407, 309, 311, 337, 867, 8512, 11, 291, 486, 32084, 472, 3991, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 115, "seek": 52488, "start": 539.88, "end": 544.88, "text": " You won't probably have capacity that this is larger than three.", "tokens": [509, 1582, 380, 1391, 362, 6042, 300, 341, 307, 4833, 813, 1045, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 116, "seek": 52488, "start": 544.88, "end": 549.88, "text": " And yeah, everything is fine and handy.", "tokens": [400, 1338, 11, 1203, 307, 2489, 293, 13239, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 117, "seek": 52488, "start": 549.88, "end": 553.88, "text": " It's quite natural.", "tokens": [467, 311, 1596, 3303, 13], "temperature": 0.0, "avg_logprob": -0.20310819525467722, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.00011150877980981022}, {"id": 118, "seek": 55388, "start": 553.88, "end": 556.88, "text": " So we just reinvented Actors.", "tokens": [407, 321, 445, 33477, 292, 3251, 830, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 119, "seek": 55388, "start": 556.88, "end": 559.88, "text": " That's basically what Actors are.", "tokens": [663, 311, 1936, 437, 3251, 830, 366, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 120, "seek": 55388, "start": 559.88, "end": 571.88, "text": " So Actors is a programming paradigm that has been invented in the 70s by a researcher called Karlewit.", "tokens": [407, 3251, 830, 307, 257, 9410, 24709, 300, 575, 668, 14479, 294, 264, 5285, 82, 538, 257, 21751, 1219, 8009, 306, 86, 270, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 121, "seek": 55388, "start": 571.88, "end": 575.88, "text": " It has been popularized more recently with Erlong.", "tokens": [467, 575, 668, 3743, 1602, 544, 3938, 365, 3300, 13025, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 122, "seek": 55388, "start": 575.88, "end": 579.88, "text": " And the actual formal definition is here.", "tokens": [400, 264, 3539, 9860, 7123, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 123, "seek": 55388, "start": 579.88, "end": 582.88, "text": " It's from Karlewit himself.", "tokens": [467, 311, 490, 8009, 306, 86, 270, 3647, 13], "temperature": 0.0, "avg_logprob": -0.1681602524548042, "compression_ratio": 1.4947916666666667, "no_speech_prob": 0.0001809745008358732}, {"id": 124, "seek": 58288, "start": 582.88, "end": 587.88, "text": " And I'm going to read it even if it's a little bit weird to read slides out loud.", "tokens": [400, 286, 478, 516, 281, 1401, 309, 754, 498, 309, 311, 257, 707, 857, 3657, 281, 1401, 9788, 484, 6588, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 125, "seek": 58288, "start": 587.88, "end": 589.88, "text": " This one is important.", "tokens": [639, 472, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 126, "seek": 58288, "start": 589.88, "end": 595.88, "text": " So an actor is a computational entity that, in response to a message it receives, can concurrently.", "tokens": [407, 364, 8747, 307, 257, 28270, 13977, 300, 11, 294, 4134, 281, 257, 3636, 309, 20717, 11, 393, 37702, 356, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 127, "seek": 58288, "start": 595.88, "end": 599.88, "text": " A, send a finite number of messages to other Actors.", "tokens": [316, 11, 2845, 257, 19362, 1230, 295, 7897, 281, 661, 3251, 830, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 128, "seek": 58288, "start": 599.88, "end": 601.88, "text": " We've done that.", "tokens": [492, 600, 1096, 300, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 129, "seek": 58288, "start": 601.88, "end": 606.88, "text": " Two, create finite numbers of new Actors.", "tokens": [4453, 11, 1884, 19362, 3547, 295, 777, 3251, 830, 13], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 130, "seek": 58288, "start": 606.88, "end": 609.88, "text": " We haven't been spawning any Actors in our example yet,", "tokens": [492, 2378, 380, 668, 637, 35880, 604, 3251, 830, 294, 527, 1365, 1939, 11], "temperature": 0.0, "avg_logprob": -0.13075132824125743, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00019409623928368092}, {"id": 131, "seek": 60988, "start": 609.88, "end": 615.88, "text": " but we do that in quick reads as something that we do especially for supervision or spawning a pipeline or stuff like that.", "tokens": [457, 321, 360, 300, 294, 1702, 15700, 382, 746, 300, 321, 360, 2318, 337, 32675, 420, 637, 35880, 257, 15517, 420, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 132, "seek": 60988, "start": 615.88, "end": 618.88, "text": " So we do that.", "tokens": [407, 321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 133, "seek": 60988, "start": 618.88, "end": 623.88, "text": " C, designates behavior to be used for the next message it receives.", "tokens": [383, 11, 1715, 1024, 5223, 281, 312, 1143, 337, 264, 958, 3636, 309, 20717, 13], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 134, "seek": 60988, "start": 623.88, "end": 626.88, "text": " That one is a little bit fuzzy.", "tokens": [663, 472, 307, 257, 707, 857, 34710, 13], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 135, "seek": 60988, "start": 626.88, "end": 627.88, "text": " Do we do this?", "tokens": [1144, 321, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 136, "seek": 60988, "start": 627.88, "end": 629.88, "text": " No, we definitely don't.", "tokens": [883, 11, 321, 2138, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 137, "seek": 60988, "start": 629.88, "end": 633.88, "text": " But if you water it down and you squint a little,", "tokens": [583, 498, 291, 1281, 309, 760, 293, 291, 2339, 686, 257, 707, 11], "temperature": 0.0, "avg_logprob": -0.16118203832748088, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00011525578156579286}, {"id": 138, "seek": 63388, "start": 633.88, "end": 639.88, "text": " the fact that the Actors have actually a state,", "tokens": [264, 1186, 300, 264, 3251, 830, 362, 767, 257, 1785, 11], "temperature": 0.0, "avg_logprob": -0.1283305946149324, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.00011405577970435843}, {"id": 139, "seek": 63388, "start": 639.88, "end": 645.88, "text": " and the whole point of having this Actors running on a specific thread is that", "tokens": [293, 264, 1379, 935, 295, 1419, 341, 3251, 830, 2614, 322, 257, 2685, 7207, 307, 300], "temperature": 0.0, "avg_logprob": -0.1283305946149324, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.00011405577970435843}, {"id": 140, "seek": 63388, "start": 645.88, "end": 650.88, "text": " it will be possible to handle a message and mutate our state.", "tokens": [309, 486, 312, 1944, 281, 4813, 257, 3636, 293, 5839, 473, 527, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1283305946149324, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.00011405577970435843}, {"id": 141, "seek": 63388, "start": 650.88, "end": 660.88, "text": " And mutating our state is a bit like designating the behavior that it will be able to use for the next message.", "tokens": [400, 5839, 990, 527, 1785, 307, 257, 857, 411, 1715, 990, 264, 5223, 300, 309, 486, 312, 1075, 281, 764, 337, 264, 958, 3636, 13], "temperature": 0.0, "avg_logprob": -0.1283305946149324, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.00011405577970435843}, {"id": 142, "seek": 66088, "start": 660.88, "end": 666.88, "text": " So we ended up building our own Actors framework.", "tokens": [407, 321, 4590, 493, 2390, 527, 1065, 3251, 830, 8388, 13], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 143, "seek": 66088, "start": 666.88, "end": 672.88, "text": " So to be honest, I'm not trying to advertise for this framework.", "tokens": [407, 281, 312, 3245, 11, 286, 478, 406, 1382, 281, 35379, 337, 341, 8388, 13], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 144, "seek": 66088, "start": 672.88, "end": 674.88, "text": " It's under the AGPL license, so you can use it.", "tokens": [467, 311, 833, 264, 28406, 21593, 10476, 11, 370, 291, 393, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 145, "seek": 66088, "start": 674.88, "end": 675.88, "text": " You're free to use it.", "tokens": [509, 434, 1737, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 146, "seek": 66088, "start": 675.88, "end": 679.88, "text": " If you want to take over it for kids and make it better,", "tokens": [759, 291, 528, 281, 747, 670, 309, 337, 2301, 293, 652, 309, 1101, 11], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 147, "seek": 66088, "start": 679.88, "end": 681.88, "text": " I'd be happy for us to use it.", "tokens": [286, 1116, 312, 2055, 337, 505, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 148, "seek": 66088, "start": 681.88, "end": 685.88, "text": " If you want to use it as is and you would like to have it as a MIT license,", "tokens": [759, 291, 528, 281, 764, 309, 382, 307, 293, 291, 576, 411, 281, 362, 309, 382, 257, 13100, 10476, 11], "temperature": 0.0, "avg_logprob": -0.10862218553774824, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00016233348287642002}, {"id": 149, "seek": 68588, "start": 685.88, "end": 691.88, "text": " I'm perfectly happy to put it under MIT license as well, actually.", "tokens": [286, 478, 6239, 2055, 281, 829, 309, 833, 13100, 10476, 382, 731, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.15001751736896793, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0001609453174751252}, {"id": 150, "seek": 68588, "start": 691.88, "end": 695.88, "text": " But right now, it's not redesigned to be reused by other people.", "tokens": [583, 558, 586, 11, 309, 311, 406, 16762, 16690, 281, 312, 319, 4717, 538, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.15001751736896793, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0001609453174751252}, {"id": 151, "seek": 68588, "start": 695.88, "end": 699.88, "text": " But I think I might be able to tell you about our journey", "tokens": [583, 286, 519, 286, 1062, 312, 1075, 281, 980, 291, 466, 527, 4671], "temperature": 0.0, "avg_logprob": -0.15001751736896793, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0001609453174751252}, {"id": 152, "seek": 68588, "start": 699.88, "end": 705.88, "text": " and maybe it could inspire other people to write their own framework.", "tokens": [293, 1310, 309, 727, 15638, 661, 561, 281, 2464, 641, 1065, 8388, 13], "temperature": 0.0, "avg_logprob": -0.15001751736896793, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0001609453174751252}, {"id": 153, "seek": 68588, "start": 705.88, "end": 709.88, "text": " There are actually a lot of Actors frameworks in Rust.", "tokens": [821, 366, 767, 257, 688, 295, 3251, 830, 29834, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.15001751736896793, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.0001609453174751252}, {"id": 154, "seek": 70988, "start": 709.88, "end": 716.88, "text": " This one is Actix, there are many others.", "tokens": [639, 472, 307, 3251, 970, 11, 456, 366, 867, 2357, 13], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 155, "seek": 70988, "start": 716.88, "end": 720.88, "text": " So under our Actors framework, what does an Actor look like", "tokens": [407, 833, 527, 3251, 830, 8388, 11, 437, 775, 364, 45457, 574, 411], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 156, "seek": 70988, "start": 720.88, "end": 723.88, "text": " compared to our original snippets?", "tokens": [5347, 281, 527, 3380, 35623, 1385, 30], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 157, "seek": 70988, "start": 723.88, "end": 725.88, "text": " Yeah, it looks like this.", "tokens": [865, 11, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 158, "seek": 70988, "start": 725.88, "end": 731.88, "text": " So I implemented the uploader there.", "tokens": [407, 286, 12270, 264, 6580, 260, 456, 13], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 159, "seek": 70988, "start": 731.88, "end": 734.88, "text": " So you have to implement first a trait called Actor", "tokens": [407, 291, 362, 281, 4445, 700, 257, 22538, 1219, 45457], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 160, "seek": 70988, "start": 734.88, "end": 738.88, "text": " where you will define a bunch of small properties about your Actor,", "tokens": [689, 291, 486, 6964, 257, 3840, 295, 1359, 7221, 466, 428, 45457, 11], "temperature": 0.0, "avg_logprob": -0.1334426431094899, "compression_ratio": 1.5485436893203883, "no_speech_prob": 9.693765605334193e-05}, {"id": 161, "seek": 73888, "start": 738.88, "end": 740.88, "text": " especially the capacity that we have seen before.", "tokens": [2318, 264, 6042, 300, 321, 362, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 162, "seek": 73888, "start": 740.88, "end": 744.88, "text": " So it will be like the capacity of the channel that we described before.", "tokens": [407, 309, 486, 312, 411, 264, 6042, 295, 264, 2269, 300, 321, 7619, 949, 13], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 163, "seek": 73888, "start": 744.88, "end": 750.88, "text": " And then you will have to implement a handler trait", "tokens": [400, 550, 291, 486, 362, 281, 4445, 257, 41967, 22538], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 164, "seek": 73888, "start": 750.88, "end": 753.88, "text": " for each type of message that you deal with.", "tokens": [337, 1184, 2010, 295, 3636, 300, 291, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 165, "seek": 73888, "start": 753.88, "end": 758.88, "text": " So, contrary to our example, now we can deal with several types of messages.", "tokens": [407, 11, 19506, 281, 527, 1365, 11, 586, 321, 393, 2028, 365, 2940, 3467, 295, 7897, 13], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 166, "seek": 73888, "start": 758.88, "end": 762.88, "text": " Same Actor can receive different kind of requests, if you will.", "tokens": [10635, 45457, 393, 4774, 819, 733, 295, 12475, 11, 498, 291, 486, 13], "temperature": 0.0, "avg_logprob": -0.13544601743871515, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.0001242689322680235}, {"id": 167, "seek": 76288, "start": 762.88, "end": 768.88, "text": " Another difference is most of the time you want to communicate with", "tokens": [3996, 2649, 307, 881, 295, 264, 565, 291, 528, 281, 7890, 365], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 168, "seek": 76288, "start": 768.88, "end": 770.88, "text": " an Actor in an asynchronous way.", "tokens": [364, 45457, 294, 364, 49174, 636, 13], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 169, "seek": 76288, "start": 770.88, "end": 774.88, "text": " That's how the Actor pattern is working usually.", "tokens": [663, 311, 577, 264, 45457, 5102, 307, 1364, 2673, 13], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 170, "seek": 76288, "start": 774.88, "end": 778.88, "text": " But sometimes it's handy to actually have some kind of reply", "tokens": [583, 2171, 309, 311, 13239, 281, 767, 362, 512, 733, 295, 16972], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 171, "seek": 76288, "start": 778.88, "end": 780.88, "text": " when you do a request.", "tokens": [562, 291, 360, 257, 5308, 13], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 172, "seek": 76288, "start": 780.88, "end": 783.88, "text": " It's a bummer because if you really want a reply,", "tokens": [467, 311, 257, 13309, 936, 570, 498, 291, 534, 528, 257, 16972, 11], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 173, "seek": 76288, "start": 783.88, "end": 787.88, "text": " that means that you will be waiting for the Actor to process", "tokens": [300, 1355, 300, 291, 486, 312, 3806, 337, 264, 45457, 281, 1399], "temperature": 0.0, "avg_logprob": -0.0895035050132058, "compression_ratio": 1.6074766355140186, "no_speech_prob": 5.630357554764487e-05}, {"id": 174, "seek": 78788, "start": 787.88, "end": 792.88, "text": " the entire queue and execute your command and then return the result.", "tokens": [264, 2302, 18639, 293, 14483, 428, 5622, 293, 550, 2736, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1013983130455017, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00015279778745025396}, {"id": 175, "seek": 78788, "start": 792.88, "end": 795.88, "text": " But you don't have to use it.", "tokens": [583, 291, 500, 380, 362, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1013983130455017, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00015279778745025396}, {"id": 176, "seek": 78788, "start": 795.88, "end": 803.88, "text": " Most of our messages don't use it, but when we need it, it's handy.", "tokens": [4534, 295, 527, 7897, 500, 380, 764, 309, 11, 457, 562, 321, 643, 309, 11, 309, 311, 13239, 13], "temperature": 0.0, "avg_logprob": -0.1013983130455017, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00015279778745025396}, {"id": 177, "seek": 78788, "start": 803.88, "end": 806.88, "text": " And our indexer, it's about the same.", "tokens": [400, 527, 8186, 260, 11, 309, 311, 466, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1013983130455017, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00015279778745025396}, {"id": 178, "seek": 78788, "start": 806.88, "end": 812.88, "text": " The one thing that I want to point out is this thing on the Actor trait", "tokens": [440, 472, 551, 300, 286, 528, 281, 935, 484, 307, 341, 551, 322, 264, 45457, 22538], "temperature": 0.0, "avg_logprob": -0.1013983130455017, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.00015279778745025396}, {"id": 179, "seek": 81288, "start": 812.88, "end": 819.88, "text": " where we specialized what should be returned on the method RuntimeHandle.", "tokens": [689, 321, 19813, 437, 820, 312, 8752, 322, 264, 3170, 497, 2760, 1312, 39, 474, 306, 13], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 180, "seek": 81288, "start": 819.88, "end": 823.88, "text": " So, you remember that in our example, we said that an indexer", "tokens": [407, 11, 291, 1604, 300, 294, 527, 1365, 11, 321, 848, 300, 364, 8186, 260], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 181, "seek": 81288, "start": 823.88, "end": 824.88, "text": " spends a lot of CPU.", "tokens": [25620, 257, 688, 295, 13199, 13], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 182, "seek": 81288, "start": 824.88, "end": 827.88, "text": " We wanted to run it on a dedicated thread.", "tokens": [492, 1415, 281, 1190, 309, 322, 257, 8374, 7207, 13], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 183, "seek": 81288, "start": 827.88, "end": 829.88, "text": " We don't do that here.", "tokens": [492, 500, 380, 360, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 184, "seek": 81288, "start": 829.88, "end": 834.88, "text": " What we do is we target a specific Tokyo runtime, which is weird.", "tokens": [708, 321, 360, 307, 321, 3779, 257, 2685, 15147, 34474, 11, 597, 307, 3657, 13], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 185, "seek": 81288, "start": 834.88, "end": 837.88, "text": " So, that's an implementation shortcut that we used", "tokens": [407, 11, 300, 311, 364, 11420, 24822, 300, 321, 1143], "temperature": 0.0, "avg_logprob": -0.13715357581774393, "compression_ratio": 1.5622119815668203, "no_speech_prob": 7.92488208389841e-05}, {"id": 186, "seek": 83788, "start": 837.88, "end": 842.88, "text": " instead of running stuff on a thread pool.", "tokens": [2602, 295, 2614, 1507, 322, 257, 7207, 7005, 13], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 187, "seek": 83788, "start": 842.88, "end": 845.88, "text": " What we do is that we have several Tokyo runtime", "tokens": [708, 321, 360, 307, 300, 321, 362, 2940, 15147, 34474], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 188, "seek": 83788, "start": 845.88, "end": 853.88, "text": " and we have a Tokyo runtime that is dedicated to act as a thread pool.", "tokens": [293, 321, 362, 257, 15147, 34474, 300, 307, 8374, 281, 605, 382, 257, 7207, 7005, 13], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 189, "seek": 83788, "start": 853.88, "end": 857.88, "text": " The benefit of this is this implementation shortcut", "tokens": [440, 5121, 295, 341, 307, 341, 11420, 24822], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 190, "seek": 83788, "start": 857.88, "end": 860.88, "text": " gives us the possibility to write exactly the same code", "tokens": [2709, 505, 264, 7959, 281, 2464, 2293, 264, 912, 3089], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 191, "seek": 83788, "start": 860.88, "end": 864.88, "text": " for an Async Actor or Async Actor, which is neat.", "tokens": [337, 364, 1018, 34015, 45457, 420, 1018, 34015, 45457, 11, 597, 307, 10654, 13], "temperature": 0.0, "avg_logprob": -0.09458700227148739, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00012598090688697994}, {"id": 192, "seek": 86488, "start": 864.88, "end": 867.88, "text": " And by the way, we're not the only one to use that trick.", "tokens": [400, 538, 264, 636, 11, 321, 434, 406, 264, 787, 472, 281, 764, 300, 4282, 13], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 193, "seek": 86488, "start": 867.88, "end": 876.88, "text": " InflexDB actually wrote a blog post about this a little bit of time ago.", "tokens": [11537, 2021, 27735, 767, 4114, 257, 6968, 2183, 466, 341, 257, 707, 857, 295, 565, 2057, 13], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 194, "seek": 86488, "start": 876.88, "end": 879.88, "text": " Now that we have a framework, you might have noticed", "tokens": [823, 300, 321, 362, 257, 8388, 11, 291, 1062, 362, 5694], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 195, "seek": 86488, "start": 879.88, "end": 881.88, "text": " that the code was not even shorter.", "tokens": [300, 264, 3089, 390, 406, 754, 11639, 13], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 196, "seek": 86488, "start": 881.88, "end": 885.88, "text": " We have seen a couple of features that we got from the framework,", "tokens": [492, 362, 1612, 257, 1916, 295, 4122, 300, 321, 658, 490, 264, 8388, 11], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 197, "seek": 86488, "start": 885.88, "end": 888.88, "text": " but what can we get like more?", "tokens": [457, 437, 393, 321, 483, 411, 544, 30], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 198, "seek": 86488, "start": 888.88, "end": 891.88, "text": " Within QuickWit, we have 25 different actors.", "tokens": [15996, 12101, 54, 270, 11, 321, 362, 3552, 819, 10037, 13], "temperature": 0.0, "avg_logprob": -0.14588524091361774, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.00012316643551457673}, {"id": 199, "seek": 89188, "start": 891.88, "end": 895.88, "text": " What's cool when you have a framework is that you code stuff once", "tokens": [708, 311, 1627, 562, 291, 362, 257, 8388, 307, 300, 291, 3089, 1507, 1564], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 200, "seek": 89188, "start": 895.88, "end": 898.88, "text": " and the benefit is multiplied by 25.", "tokens": [293, 264, 5121, 307, 17207, 538, 3552, 13], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 201, "seek": 89188, "start": 898.88, "end": 900.88, "text": " What could be the benefit?", "tokens": [708, 727, 312, 264, 5121, 30], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 202, "seek": 89188, "start": 900.88, "end": 904.88, "text": " So, hopefully, we get better structure and code", "tokens": [407, 11, 4696, 11, 321, 483, 1101, 3877, 293, 3089], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 203, "seek": 89188, "start": 904.88, "end": 906.88, "text": " that is a little bit more readable.", "tokens": [300, 307, 257, 707, 857, 544, 49857, 13], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 204, "seek": 89188, "start": 906.88, "end": 909.88, "text": " People open up files and they know what to expect.", "tokens": [3432, 1269, 493, 7098, 293, 436, 458, 437, 281, 2066, 13], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 205, "seek": 89188, "start": 909.88, "end": 912.88, "text": " I want to know what is the capacity associated to the cure of this actor.", "tokens": [286, 528, 281, 458, 437, 307, 264, 6042, 6615, 281, 264, 13698, 295, 341, 8747, 13], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 206, "seek": 89188, "start": 912.88, "end": 914.88, "text": " Where should I look?", "tokens": [2305, 820, 286, 574, 30], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 207, "seek": 89188, "start": 914.88, "end": 917.88, "text": " That's something that you get from having a framework.", "tokens": [663, 311, 746, 300, 291, 483, 490, 1419, 257, 8388, 13], "temperature": 0.0, "avg_logprob": -0.13577427995314292, "compression_ratio": 1.656, "no_speech_prob": 0.00015121079923119396}, {"id": 208, "seek": 91788, "start": 917.88, "end": 921.88, "text": " So, we will talk a little bit about that,", "tokens": [407, 11, 321, 486, 751, 257, 707, 857, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 209, "seek": 91788, "start": 921.88, "end": 924.88, "text": " but we get supervision from the framework.", "tokens": [457, 321, 483, 32675, 490, 264, 8388, 13], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 210, "seek": 91788, "start": 924.88, "end": 928.88, "text": " We get a neat solution to deal with time,", "tokens": [492, 483, 257, 10654, 3827, 281, 2028, 365, 565, 11], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 211, "seek": 91788, "start": 928.88, "end": 933.88, "text": " which is probably the main reason why we don't use Actix today.", "tokens": [597, 307, 1391, 264, 2135, 1778, 983, 321, 500, 380, 764, 3251, 970, 965, 13], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 212, "seek": 91788, "start": 933.88, "end": 937.88, "text": " And then we have a bunch of recipes to write unique tests,", "tokens": [400, 550, 321, 362, 257, 3840, 295, 13035, 281, 2464, 3845, 6921, 11], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 213, "seek": 91788, "start": 937.88, "end": 939.88, "text": " which is very important.", "tokens": [597, 307, 588, 1021, 13], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 214, "seek": 91788, "start": 939.88, "end": 946.88, "text": " We also have some stuff to be able to see what our actors are doing", "tokens": [492, 611, 362, 512, 1507, 281, 312, 1075, 281, 536, 437, 527, 10037, 366, 884], "temperature": 0.0, "avg_logprob": -0.0839413212191674, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.0001253731461474672}, {"id": 215, "seek": 94688, "start": 946.88, "end": 950.88, "text": " and we have a solution for discoverability also,", "tokens": [293, 321, 362, 257, 3827, 337, 4411, 2310, 611, 11], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 216, "seek": 94688, "start": 950.88, "end": 954.88, "text": " but we won't talk about this in this talk.", "tokens": [457, 321, 1582, 380, 751, 466, 341, 294, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 217, "seek": 94688, "start": 954.88, "end": 957.88, "text": " So, supervision.", "tokens": [407, 11, 32675, 13], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 218, "seek": 94688, "start": 957.88, "end": 960.88, "text": " Of course, I would like to tell you our code is perfect", "tokens": [2720, 1164, 11, 286, 576, 411, 281, 980, 291, 527, 3089, 307, 2176], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 219, "seek": 94688, "start": 960.88, "end": 965.88, "text": " and perfect code is, especially for telling the rest, it never fails.", "tokens": [293, 2176, 3089, 307, 11, 2318, 337, 3585, 264, 1472, 11, 309, 1128, 18199, 13], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 220, "seek": 94688, "start": 965.88, "end": 971.88, "text": " And in a sense, we don't experience panics or stuff like that,", "tokens": [400, 294, 257, 2020, 11, 321, 500, 380, 1752, 2462, 1167, 420, 1507, 411, 300, 11], "temperature": 0.0, "avg_logprob": -0.22131855516548615, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.00025230360915884376}, {"id": 221, "seek": 97188, "start": 971.88, "end": 977.88, "text": " but we have to run our code, third-party code,", "tokens": [457, 321, 362, 281, 1190, 527, 3089, 11, 2636, 12, 23409, 3089, 11], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 222, "seek": 97188, "start": 977.88, "end": 983.88, "text": " user-defined code, like user, they can write a VRI script", "tokens": [4195, 12, 37716, 3089, 11, 411, 4195, 11, 436, 393, 2464, 257, 691, 5577, 5755], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 223, "seek": 97188, "start": 983.88, "end": 987.88, "text": " to transform their documents in the pipeline", "tokens": [281, 4088, 641, 8512, 294, 264, 15517], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 224, "seek": 97188, "start": 987.88, "end": 990.88, "text": " and that's running on our indexing pipeline.", "tokens": [293, 300, 311, 2614, 322, 527, 8186, 278, 15517, 13], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 225, "seek": 97188, "start": 990.88, "end": 992.88, "text": " We have to do IO.", "tokens": [492, 362, 281, 360, 39839, 13], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 226, "seek": 97188, "start": 992.88, "end": 995.88, "text": " We have to interact with different systems.", "tokens": [492, 362, 281, 4648, 365, 819, 3652, 13], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 227, "seek": 97188, "start": 995.88, "end": 998.88, "text": " For instance, we get our documents from a source.", "tokens": [1171, 5197, 11, 321, 483, 527, 8512, 490, 257, 4009, 13], "temperature": 0.0, "avg_logprob": -0.16066280631131905, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.00018599284521769732}, {"id": 228, "seek": 99888, "start": 998.88, "end": 1002.88, "text": " We're running the pipeline. We send them to a storage.", "tokens": [492, 434, 2614, 264, 15517, 13, 492, 2845, 552, 281, 257, 6725, 13], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 229, "seek": 99888, "start": 1002.88, "end": 1004.88, "text": " We have different storage that are implemented.", "tokens": [492, 362, 819, 6725, 300, 366, 12270, 13], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 230, "seek": 99888, "start": 1004.88, "end": 1008.88, "text": " That's a lot of components and any one of them can fail", "tokens": [663, 311, 257, 688, 295, 6677, 293, 604, 472, 295, 552, 393, 3061], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 231, "seek": 99888, "start": 1008.88, "end": 1010.88, "text": " and we want a very large amount of time.", "tokens": [293, 321, 528, 257, 588, 2416, 2372, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 232, "seek": 99888, "start": 1010.88, "end": 1014.88, "text": " So, one solution to this, it's not discoverability,", "tokens": [407, 11, 472, 3827, 281, 341, 11, 309, 311, 406, 4411, 2310, 11], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 233, "seek": 99888, "start": 1014.88, "end": 1016.88, "text": " it's not like it works all of the time,", "tokens": [309, 311, 406, 411, 309, 1985, 439, 295, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 234, "seek": 99888, "start": 1016.88, "end": 1020.88, "text": " but just try to turn it off and on again.", "tokens": [457, 445, 853, 281, 1261, 309, 766, 293, 322, 797, 13], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 235, "seek": 99888, "start": 1020.88, "end": 1023.88, "text": " It feels a little bit stupid,", "tokens": [467, 3417, 257, 707, 857, 6631, 11], "temperature": 0.0, "avg_logprob": -0.17821112045874962, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.00038604260771535337}, {"id": 236, "seek": 102388, "start": 1023.88, "end": 1028.88, "text": " but you just restart everything from a blank state", "tokens": [457, 291, 445, 21022, 1203, 490, 257, 8247, 1785], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 237, "seek": 102388, "start": 1028.88, "end": 1035.88, "text": " or blank slate and sometimes things work fine that way.", "tokens": [420, 8247, 39118, 293, 2171, 721, 589, 2489, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 238, "seek": 102388, "start": 1035.88, "end": 1038.88, "text": " So, the supervision works as follows.", "tokens": [407, 11, 264, 32675, 1985, 382, 10002, 13], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 239, "seek": 102388, "start": 1038.88, "end": 1041.88, "text": " We have an actor that is in charge of supervising", "tokens": [492, 362, 364, 8747, 300, 307, 294, 4602, 295, 37971, 3436], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 240, "seek": 102388, "start": 1041.88, "end": 1044.88, "text": " all of the actors that are in our pipeline.", "tokens": [439, 295, 264, 10037, 300, 366, 294, 527, 15517, 13], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 241, "seek": 102388, "start": 1044.88, "end": 1046.88, "text": " What he's doing is that it's pulling actors", "tokens": [708, 415, 311, 884, 307, 300, 309, 311, 8407, 10037], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 242, "seek": 102388, "start": 1046.88, "end": 1048.88, "text": " and when it detects that they failed,", "tokens": [293, 562, 309, 5531, 82, 300, 436, 7612, 11], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 243, "seek": 102388, "start": 1048.88, "end": 1052.88, "text": " it will kill everyone and restart everyone.", "tokens": [309, 486, 1961, 1518, 293, 21022, 1518, 13], "temperature": 0.0, "avg_logprob": -0.11727656709386948, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00017316134471911937}, {"id": 244, "seek": 105288, "start": 1052.88, "end": 1056.88, "text": " The definition of failure is a little bit sophisticated", "tokens": [440, 7123, 295, 7763, 307, 257, 707, 857, 16950], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 245, "seek": 105288, "start": 1056.88, "end": 1060.88, "text": " in our case, so it could be an actor that has returned", "tokens": [294, 527, 1389, 11, 370, 309, 727, 312, 364, 8747, 300, 575, 8752], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 246, "seek": 105288, "start": 1060.88, "end": 1063.88, "text": " an error from a handler or it panicked", "tokens": [364, 6713, 490, 257, 41967, 420, 309, 2462, 12598], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 247, "seek": 105288, "start": 1063.88, "end": 1068.88, "text": " or we have some system to detect if an actor", "tokens": [420, 321, 362, 512, 1185, 281, 5531, 498, 364, 8747], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 248, "seek": 105288, "start": 1068.88, "end": 1071.88, "text": " has not been progressing for three seconds.", "tokens": [575, 406, 668, 36305, 337, 1045, 3949, 13], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 249, "seek": 105288, "start": 1071.88, "end": 1074.88, "text": " So, we have an option of progression in our framework.", "tokens": [407, 11, 321, 362, 364, 3614, 295, 18733, 294, 527, 8388, 13], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 250, "seek": 105288, "start": 1074.88, "end": 1077.88, "text": " That's an original way to do stuff.", "tokens": [663, 311, 364, 3380, 636, 281, 360, 1507, 13], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 251, "seek": 105288, "start": 1077.88, "end": 1080.88, "text": " And we use one for all supervision,", "tokens": [400, 321, 764, 472, 337, 439, 32675, 11], "temperature": 0.0, "avg_logprob": -0.12686182061831155, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.661510219099e-05}, {"id": 252, "seek": 108088, "start": 1080.88, "end": 1084.88, "text": " which means that if one actor failed,", "tokens": [597, 1355, 300, 498, 472, 8747, 7612, 11], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 253, "seek": 108088, "start": 1084.88, "end": 1090.88, "text": " we restart everyone.", "tokens": [321, 21022, 1518, 13], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 254, "seek": 108088, "start": 1090.88, "end": 1092.88, "text": " Okay, that was for supervision.", "tokens": [1033, 11, 300, 390, 337, 32675, 13], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 255, "seek": 108088, "start": 1092.88, "end": 1094.88, "text": " Now, about handling time,", "tokens": [823, 11, 466, 13175, 565, 11], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 256, "seek": 108088, "start": 1094.88, "end": 1098.88, "text": " which is probably the most interesting part of our framework.", "tokens": [597, 307, 1391, 264, 881, 1880, 644, 295, 527, 8388, 13], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 257, "seek": 108088, "start": 1098.88, "end": 1104.88, "text": " So, we need to be able to deal with the idea", "tokens": [407, 11, 321, 643, 281, 312, 1075, 281, 2028, 365, 264, 1558], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 258, "seek": 108088, "start": 1104.88, "end": 1107.88, "text": " that, for instance, in our indexer,", "tokens": [300, 11, 337, 5197, 11, 294, 527, 8186, 260, 11], "temperature": 0.0, "avg_logprob": -0.1140828648129025, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.0922390073537827e-05}, {"id": 259, "seek": 110788, "start": 1107.88, "end": 1110.88, "text": " we want to emit a file after 30 seconds has passed.", "tokens": [321, 528, 281, 32084, 257, 3991, 934, 2217, 3949, 575, 4678, 13], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 260, "seek": 110788, "start": 1110.88, "end": 1112.88, "text": " So, we have a condition like this.", "tokens": [407, 11, 321, 362, 257, 4188, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 261, "seek": 110788, "start": 1112.88, "end": 1118.88, "text": " 30 seconds after the first document was added in that batch.", "tokens": [2217, 3949, 934, 264, 700, 4166, 390, 3869, 294, 300, 15245, 13], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 262, "seek": 110788, "start": 1118.88, "end": 1121.88, "text": " And we cannot do time.sleep in the handler", "tokens": [400, 321, 2644, 360, 565, 13, 82, 7927, 294, 264, 41967], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 263, "seek": 110788, "start": 1121.88, "end": 1124.88, "text": " because it will block the entire processing of documents.", "tokens": [570, 309, 486, 3461, 264, 2302, 9007, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 264, "seek": 110788, "start": 1124.88, "end": 1126.88, "text": " So, the solution for this is rather simple.", "tokens": [407, 11, 264, 3827, 337, 341, 307, 2831, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 265, "seek": 110788, "start": 1126.88, "end": 1131.88, "text": " We have a method so that actors can ask the framework", "tokens": [492, 362, 257, 3170, 370, 300, 10037, 393, 1029, 264, 8388], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 266, "seek": 110788, "start": 1131.88, "end": 1133.88, "text": " to send back a message to themselves", "tokens": [281, 2845, 646, 257, 3636, 281, 2969], "temperature": 0.0, "avg_logprob": -0.10114002227783203, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00011750684643629938}, {"id": 267, "seek": 113388, "start": 1133.88, "end": 1137.88, "text": " after a given amount of time, 30 seconds here.", "tokens": [934, 257, 2212, 2372, 295, 565, 11, 2217, 3949, 510, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 268, "seek": 113388, "start": 1137.88, "end": 1141.88, "text": " And it seems like a very simple solution,", "tokens": [400, 309, 2544, 411, 257, 588, 2199, 3827, 11], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 269, "seek": 113388, "start": 1141.88, "end": 1142.88, "text": " but it has a problem.", "tokens": [457, 309, 575, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 270, "seek": 113388, "start": 1142.88, "end": 1145.88, "text": " So, here I showed how it worked.", "tokens": [407, 11, 510, 286, 4712, 577, 309, 2732, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 271, "seek": 113388, "start": 1145.88, "end": 1147.88, "text": " The actor is sending a message to the scheduler.", "tokens": [440, 8747, 307, 7750, 257, 3636, 281, 264, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 272, "seek": 113388, "start": 1147.88, "end": 1149.88, "text": " This is what is happening under the hood.", "tokens": [639, 307, 437, 307, 2737, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 273, "seek": 113388, "start": 1149.88, "end": 1151.88, "text": " It's sending a message to actually an actor", "tokens": [467, 311, 7750, 257, 3636, 281, 767, 364, 8747], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 274, "seek": 113388, "start": 1151.88, "end": 1155.88, "text": " that is run by the framework called the scheduler actor.", "tokens": [300, 307, 1190, 538, 264, 8388, 1219, 264, 12000, 260, 8747, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 275, "seek": 113388, "start": 1155.88, "end": 1159.88, "text": " And 30 seconds later, the scheduler will stack a message", "tokens": [400, 2217, 3949, 1780, 11, 264, 12000, 260, 486, 8630, 257, 3636], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 276, "seek": 113388, "start": 1159.88, "end": 1162.88, "text": " into the queue of the actor.", "tokens": [666, 264, 18639, 295, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.1047567628387712, "compression_ratio": 1.7325102880658436, "no_speech_prob": 0.00010296438995283097}, {"id": 277, "seek": 116288, "start": 1162.88, "end": 1165.88, "text": " The trouble there is, imagine that you already had a lot", "tokens": [440, 5253, 456, 307, 11, 3811, 300, 291, 1217, 632, 257, 688], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 278, "seek": 116288, "start": 1165.88, "end": 1167.88, "text": " of messages in the queue of the actor.", "tokens": [295, 7897, 294, 264, 18639, 295, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 279, "seek": 116288, "start": 1167.88, "end": 1169.88, "text": " Then, where are your 30 seconds?", "tokens": [1396, 11, 689, 366, 428, 2217, 3949, 30], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 280, "seek": 116288, "start": 1169.88, "end": 1173.88, "text": " Maybe we have one minute worth of messages in that queue.", "tokens": [2704, 321, 362, 472, 3456, 3163, 295, 7897, 294, 300, 18639, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 281, "seek": 116288, "start": 1173.88, "end": 1177.88, "text": " And then, our entire contract of,", "tokens": [400, 550, 11, 527, 2302, 4364, 295, 11], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 282, "seek": 116288, "start": 1177.88, "end": 1181.88, "text": " I want to emit a file every 30 seconds, it's broken.", "tokens": [286, 528, 281, 32084, 257, 3991, 633, 2217, 3949, 11, 309, 311, 5463, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 283, "seek": 116288, "start": 1181.88, "end": 1183.88, "text": " We cannot do that.", "tokens": [492, 2644, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 284, "seek": 116288, "start": 1183.88, "end": 1187.88, "text": " So, the solution we went for is actually mailbox", "tokens": [407, 11, 264, 3827, 321, 1437, 337, 307, 767, 43602], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 285, "seek": 116288, "start": 1187.88, "end": 1189.88, "text": " are a tiny bit more complicated.", "tokens": [366, 257, 5870, 857, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 286, "seek": 116288, "start": 1189.88, "end": 1190.88, "text": " They have two queues.", "tokens": [814, 362, 732, 631, 1247, 13], "temperature": 0.0, "avg_logprob": -0.12478611532565767, "compression_ratio": 1.584, "no_speech_prob": 9.268965368391946e-05}, {"id": 287, "seek": 119088, "start": 1190.88, "end": 1192.88, "text": " One is a low priority queue, the usual one.", "tokens": [1485, 307, 257, 2295, 9365, 18639, 11, 264, 7713, 472, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 288, "seek": 119088, "start": 1192.88, "end": 1194.88, "text": " And we have a high priority queue.", "tokens": [400, 321, 362, 257, 1090, 9365, 18639, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 289, "seek": 119088, "start": 1194.88, "end": 1197.88, "text": " And the scheduler will put that in the high priority queue.", "tokens": [400, 264, 12000, 260, 486, 829, 300, 294, 264, 1090, 9365, 18639, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 290, "seek": 119088, "start": 1197.88, "end": 1201.88, "text": " So, as soon as the actor has finished dealing with the message", "tokens": [407, 11, 382, 2321, 382, 264, 8747, 575, 4335, 6260, 365, 264, 3636], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 291, "seek": 119088, "start": 1201.88, "end": 1203.88, "text": " that it was processing at the time,", "tokens": [300, 309, 390, 9007, 412, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 292, "seek": 119088, "start": 1203.88, "end": 1206.88, "text": " it will jump on this scheduled message.", "tokens": [309, 486, 3012, 322, 341, 15678, 3636, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 293, "seek": 119088, "start": 1206.88, "end": 1214.88, "text": " And we will get our nice 30 seconds call back.", "tokens": [400, 321, 486, 483, 527, 1481, 2217, 3949, 818, 646, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 294, "seek": 119088, "start": 1214.88, "end": 1219.88, "text": " Testability, let me check the time to know.", "tokens": [9279, 2310, 11, 718, 385, 1520, 264, 565, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.12956631537711266, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0001585629943292588}, {"id": 295, "seek": 121988, "start": 1219.88, "end": 1223.88, "text": " If I'm good or not, 20 minutes, perfect.", "tokens": [759, 286, 478, 665, 420, 406, 11, 945, 2077, 11, 2176, 13], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 296, "seek": 121988, "start": 1223.88, "end": 1229.88, "text": " Testability, we have a bunch of solutions to write tests.", "tokens": [9279, 2310, 11, 321, 362, 257, 3840, 295, 6547, 281, 2464, 6921, 13], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 297, "seek": 121988, "start": 1229.88, "end": 1232.88, "text": " Let's go through code to see, like,", "tokens": [961, 311, 352, 807, 3089, 281, 536, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 298, "seek": 121988, "start": 1232.88, "end": 1235.88, "text": " actual RISD code to see how we can implement", "tokens": [3539, 497, 2343, 35, 3089, 281, 536, 577, 321, 393, 4445], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 299, "seek": 121988, "start": 1235.88, "end": 1240.88, "text": " complicated real-life stuff and unit test it.", "tokens": [6179, 957, 12, 9073, 1507, 293, 4985, 1500, 309, 13], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 300, "seek": 121988, "start": 1240.88, "end": 1245.88, "text": " So, the code that we will look at is a batch builder", "tokens": [407, 11, 264, 3089, 300, 321, 486, 574, 412, 307, 257, 15245, 27377], "temperature": 0.0, "avg_logprob": -0.1874828568424087, "compression_ratio": 1.4111675126903553, "no_speech_prob": 0.00016678322572261095}, {"id": 301, "seek": 124588, "start": 1245.88, "end": 1249.88, "text": " that is mimicking pretty much what we do in indexing.", "tokens": [300, 307, 12247, 10401, 1238, 709, 437, 321, 360, 294, 8186, 278, 13], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 302, "seek": 124588, "start": 1249.88, "end": 1252.88, "text": " So, we have two possible conditions.", "tokens": [407, 11, 321, 362, 732, 1944, 4487, 13], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 303, "seek": 124588, "start": 1252.88, "end": 1257.88, "text": " We emit a file either because we have enough data", "tokens": [492, 32084, 257, 3991, 2139, 570, 321, 362, 1547, 1412], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 304, "seek": 124588, "start": 1257.88, "end": 1261.88, "text": " and it's enough to cut a file.", "tokens": [293, 309, 311, 1547, 281, 1723, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 305, "seek": 124588, "start": 1261.88, "end": 1263.88, "text": " So, let's say if you have 100 messages,", "tokens": [407, 11, 718, 311, 584, 498, 291, 362, 2319, 7897, 11], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 306, "seek": 124588, "start": 1263.88, "end": 1265.88, "text": " it's not 100 in relative,", "tokens": [309, 311, 406, 2319, 294, 4972, 11], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 307, "seek": 124588, "start": 1265.88, "end": 1268.88, "text": " but if you have 100 messages, then you emit a file.", "tokens": [457, 498, 291, 362, 2319, 7897, 11, 550, 291, 32084, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 308, "seek": 124588, "start": 1268.88, "end": 1272.88, "text": " Or, if 30 seconds has elapsed in the reception", "tokens": [1610, 11, 498, 2217, 3949, 575, 806, 2382, 292, 294, 264, 21682], "temperature": 0.0, "avg_logprob": -0.12552130104291556, "compression_ratio": 1.6633663366336633, "no_speech_prob": 8.311407145811245e-05}, {"id": 309, "seek": 127288, "start": 1272.88, "end": 1279.88, "text": " of the first document of the batch.", "tokens": [295, 264, 700, 4166, 295, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 310, "seek": 127288, "start": 1279.88, "end": 1281.88, "text": " So, let's start slow and easy.", "tokens": [407, 11, 718, 311, 722, 2964, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 311, "seek": 127288, "start": 1281.88, "end": 1287.88, "text": " So, we have our actor here.", "tokens": [407, 11, 321, 362, 527, 8747, 510, 13], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 312, "seek": 127288, "start": 1287.88, "end": 1290.88, "text": " So, this is the state of the actor,", "tokens": [407, 11, 341, 307, 264, 1785, 295, 264, 8747, 11], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 313, "seek": 127288, "start": 1290.88, "end": 1293.88, "text": " but this is the actor as well.", "tokens": [457, 341, 307, 264, 8747, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 314, "seek": 127288, "start": 1293.88, "end": 1298.88, "text": " So, something that is obvious is it will have to have some", "tokens": [407, 11, 746, 300, 307, 6322, 307, 309, 486, 362, 281, 362, 512], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 315, "seek": 127288, "start": 1298.88, "end": 1301.88, "text": " mailbox to push the speed that it produced to.", "tokens": [43602, 281, 2944, 264, 3073, 300, 309, 7126, 281, 13], "temperature": 0.0, "avg_logprob": -0.12661857139773486, "compression_ratio": 1.66875, "no_speech_prob": 0.0001185874207294546}, {"id": 316, "seek": 130188, "start": 1301.88, "end": 1305.88, "text": " It will have a document batch which will be a vex of string.", "tokens": [467, 486, 362, 257, 4166, 15245, 597, 486, 312, 257, 1241, 87, 295, 6798, 13], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 317, "seek": 130188, "start": 1305.88, "end": 1307.88, "text": " So, document will be just string.", "tokens": [407, 11, 4166, 486, 312, 445, 6798, 13], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 318, "seek": 130188, "start": 1307.88, "end": 1309.88, "text": " It will append document to that.", "tokens": [467, 486, 34116, 4166, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 319, "seek": 130188, "start": 1309.88, "end": 1311.88, "text": " When it's big enough, it's now flash it", "tokens": [1133, 309, 311, 955, 1547, 11, 309, 311, 586, 7319, 309], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 320, "seek": 130188, "start": 1311.88, "end": 1314.88, "text": " and send it to the mailbox of the consumer.", "tokens": [293, 2845, 309, 281, 264, 43602, 295, 264, 9711, 13], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 321, "seek": 130188, "start": 1314.88, "end": 1319.88, "text": " And one thing that is new here is we added some counters", "tokens": [400, 472, 551, 300, 307, 777, 510, 307, 321, 3869, 512, 39338], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 322, "seek": 130188, "start": 1319.88, "end": 1324.88, "text": " and we will be able in the unit test to do some assert", "tokens": [293, 321, 486, 312, 1075, 294, 264, 4985, 1500, 281, 360, 512, 19810], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 323, "seek": 130188, "start": 1324.88, "end": 1330.88, "text": " on this internal state.", "tokens": [322, 341, 6920, 1785, 13], "temperature": 0.0, "avg_logprob": -0.16971719144570707, "compression_ratio": 1.6926829268292682, "no_speech_prob": 5.6120308727258816e-05}, {"id": 324, "seek": 133088, "start": 1330.88, "end": 1333.88, "text": " I didn't talk about it,", "tokens": [286, 994, 380, 751, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 325, "seek": 133088, "start": 1333.88, "end": 1337.88, "text": " but the actor trait actually has an associated type", "tokens": [457, 264, 8747, 22538, 767, 575, 364, 6615, 2010], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 326, "seek": 133088, "start": 1337.88, "end": 1339.88, "text": " which is called observable state.", "tokens": [597, 307, 1219, 9951, 712, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 327, "seek": 133088, "start": 1339.88, "end": 1343.88, "text": " Of course, the whole idea of actors is to encapsulate your", "tokens": [2720, 1164, 11, 264, 1379, 1558, 295, 10037, 307, 281, 38745, 5256, 428], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 328, "seek": 133088, "start": 1343.88, "end": 1346.88, "text": " state into your thread or your token task", "tokens": [1785, 666, 428, 7207, 420, 428, 14862, 5633], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 329, "seek": 133088, "start": 1346.88, "end": 1348.88, "text": " and you're not supposed to be able to mutate it", "tokens": [293, 291, 434, 406, 3442, 281, 312, 1075, 281, 5839, 473, 309], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 330, "seek": 133088, "start": 1348.88, "end": 1351.88, "text": " or even read it from the external world.", "tokens": [420, 754, 1401, 309, 490, 264, 8320, 1002, 13], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 331, "seek": 133088, "start": 1351.88, "end": 1357.88, "text": " But we have some thing that makes it possible to ask", "tokens": [583, 321, 362, 512, 551, 300, 1669, 309, 1944, 281, 1029], "temperature": 0.0, "avg_logprob": -0.12285720541122112, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00023067070287652314}, {"id": 332, "seek": 135788, "start": 1357.88, "end": 1362.88, "text": " from outside the actor what is your observable state", "tokens": [490, 2380, 264, 8747, 437, 307, 428, 9951, 712, 1785], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 333, "seek": 135788, "start": 1362.88, "end": 1365.88, "text": " and it will send a message to the actor", "tokens": [293, 309, 486, 2845, 257, 3636, 281, 264, 8747], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 334, "seek": 135788, "start": 1365.88, "end": 1368.88, "text": " and the actor will send back the result", "tokens": [293, 264, 8747, 486, 2845, 646, 264, 1874], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 335, "seek": 135788, "start": 1368.88, "end": 1370.88, "text": " of the observable state method here", "tokens": [295, 264, 9951, 712, 1785, 3170, 510], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 336, "seek": 135788, "start": 1370.88, "end": 1375.88, "text": " which is nifty for observability and for unit test.", "tokens": [597, 307, 297, 37177, 337, 9951, 2310, 293, 337, 4985, 1500, 13], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 337, "seek": 135788, "start": 1375.88, "end": 1378.88, "text": " And then there is our handler.", "tokens": [400, 550, 456, 307, 527, 41967, 13], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 338, "seek": 135788, "start": 1378.88, "end": 1380.88, "text": " So, we will have two messages.", "tokens": [407, 11, 321, 486, 362, 732, 7897, 13], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 339, "seek": 135788, "start": 1380.88, "end": 1383.88, "text": " One message will be receiving a document here.", "tokens": [1485, 3636, 486, 312, 10040, 257, 4166, 510, 13], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 340, "seek": 135788, "start": 1383.88, "end": 1385.88, "text": " It was just a string as I told you.", "tokens": [467, 390, 445, 257, 6798, 382, 286, 1907, 291, 13], "temperature": 0.0, "avg_logprob": -0.08378094673156739, "compression_ratio": 1.7136150234741785, "no_speech_prob": 9.103165211854503e-05}, {"id": 341, "seek": 138588, "start": 1385.88, "end": 1387.88, "text": " I wanted to keep stuff simple.", "tokens": [286, 1415, 281, 1066, 1507, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 342, "seek": 138588, "start": 1387.88, "end": 1391.88, "text": " And we will do several things.", "tokens": [400, 321, 486, 360, 2940, 721, 13], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 343, "seek": 138588, "start": 1391.88, "end": 1395.88, "text": " The first thing that we do is if this was the first document", "tokens": [440, 700, 551, 300, 321, 360, 307, 498, 341, 390, 264, 700, 4166], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 344, "seek": 138588, "start": 1395.88, "end": 1399.88, "text": " in the batch, we need to register our callback message", "tokens": [294, 264, 15245, 11, 321, 643, 281, 7280, 527, 818, 3207, 3636], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 345, "seek": 138588, "start": 1399.88, "end": 1403.88, "text": " using the schedule self message.", "tokens": [1228, 264, 7567, 2698, 3636, 13], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 346, "seek": 138588, "start": 1403.88, "end": 1406.88, "text": " We will append this document to our batch", "tokens": [492, 486, 34116, 341, 4166, 281, 527, 15245], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 347, "seek": 138588, "start": 1406.88, "end": 1409.88, "text": " and then we check for the conditions.", "tokens": [293, 550, 321, 1520, 337, 264, 4487, 13], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 348, "seek": 138588, "start": 1409.88, "end": 1411.88, "text": " We have enough documents in the batch", "tokens": [492, 362, 1547, 8512, 294, 264, 15245], "temperature": 0.0, "avg_logprob": -0.10237089423246162, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00012900770525448024}, {"id": 349, "seek": 141188, "start": 1411.88, "end": 1419.88, "text": " to actually emit a batch using our second batch emission", "tokens": [281, 767, 32084, 257, 15245, 1228, 527, 1150, 15245, 29513], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 350, "seek": 141188, "start": 1419.88, "end": 1421.88, "text": " condition.", "tokens": [4188, 13], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 351, "seek": 141188, "start": 1421.88, "end": 1423.88, "text": " And in that case, we call emit batch.", "tokens": [400, 294, 300, 1389, 11, 321, 818, 32084, 15245, 13], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 352, "seek": 141188, "start": 1423.88, "end": 1427.88, "text": " I didn't put the code of emit batch because it was too easy.", "tokens": [286, 994, 380, 829, 264, 3089, 295, 32084, 15245, 570, 309, 390, 886, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 353, "seek": 141188, "start": 1427.88, "end": 1429.88, "text": " Not very interesting.", "tokens": [1726, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 354, "seek": 141188, "start": 1429.88, "end": 1440.88, "text": " And then I didn't put the handler", "tokens": [400, 550, 286, 994, 380, 829, 264, 41967], "temperature": 0.0, "avg_logprob": -0.15577057429722377, "compression_ratio": 1.5310344827586206, "no_speech_prob": 0.00013442744966596365}, {"id": 355, "seek": 144088, "start": 1440.88, "end": 1443.88, "text": " of the time out message, but you can guess it basically", "tokens": [295, 264, 565, 484, 3636, 11, 457, 291, 393, 2041, 309, 1936], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 356, "seek": 144088, "start": 1443.88, "end": 1447.88, "text": " it's emitting the batch.", "tokens": [309, 311, 846, 2414, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 357, "seek": 144088, "start": 1447.88, "end": 1450.88, "text": " And then when we want to unit that stuff,", "tokens": [400, 550, 562, 321, 528, 281, 4985, 300, 1507, 11], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 358, "seek": 144088, "start": 1450.88, "end": 1453.88, "text": " we write things like this.", "tokens": [321, 2464, 721, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 359, "seek": 144088, "start": 1453.88, "end": 1457.88, "text": " So, we have a universe in our unit test.", "tokens": [407, 11, 321, 362, 257, 6445, 294, 527, 4985, 1500, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 360, "seek": 144088, "start": 1457.88, "end": 1459.88, "text": " It's a very important thing.", "tokens": [467, 311, 257, 588, 1021, 551, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 361, "seek": 144088, "start": 1459.88, "end": 1463.88, "text": " We want to isolate our unit test one from each other.", "tokens": [492, 528, 281, 25660, 527, 4985, 1500, 472, 490, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 362, "seek": 144088, "start": 1463.88, "end": 1465.88, "text": " And the universe is in charge of this.", "tokens": [400, 264, 6445, 307, 294, 4602, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 363, "seek": 144088, "start": 1465.88, "end": 1468.88, "text": " So, all of the actors of your program", "tokens": [407, 11, 439, 295, 264, 10037, 295, 428, 1461], "temperature": 0.0, "avg_logprob": -0.1176692536733683, "compression_ratio": 1.6908212560386473, "no_speech_prob": 7.056848698994145e-05}, {"id": 364, "seek": 146888, "start": 1468.88, "end": 1470.88, "text": " have to belong to the same universe.", "tokens": [362, 281, 5784, 281, 264, 912, 6445, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 365, "seek": 146888, "start": 1470.88, "end": 1473.88, "text": " Otherwise, they're not supposed to communicate together.", "tokens": [10328, 11, 436, 434, 406, 3442, 281, 7890, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 366, "seek": 146888, "start": 1473.88, "end": 1477.88, "text": " And we will see that this isolation will make it possible", "tokens": [400, 321, 486, 536, 300, 341, 16001, 486, 652, 309, 1944], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 367, "seek": 146888, "start": 1477.88, "end": 1480.88, "text": " to do something really cool in the next slide.", "tokens": [281, 360, 746, 534, 1627, 294, 264, 958, 4137, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 368, "seek": 146888, "start": 1480.88, "end": 1484.88, "text": " And so, this universe makes it possible to make a fake mailbox", "tokens": [400, 370, 11, 341, 6445, 1669, 309, 1944, 281, 652, 257, 7592, 43602], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 369, "seek": 146888, "start": 1484.88, "end": 1489.88, "text": " that we create like the consumer side of things.", "tokens": [300, 321, 1884, 411, 264, 9711, 1252, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 370, "seek": 146888, "start": 1489.88, "end": 1492.88, "text": " We can create our batch builder and it's alone", "tokens": [492, 393, 1884, 527, 15245, 27377, 293, 309, 311, 3312], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 371, "seek": 146888, "start": 1492.88, "end": 1494.88, "text": " and send message to it.", "tokens": [293, 2845, 3636, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 372, "seek": 146888, "start": 1494.88, "end": 1496.88, "text": " That's what we do there.", "tokens": [663, 311, 437, 321, 360, 456, 13], "temperature": 0.0, "avg_logprob": -0.10271382104782831, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.00010750264482339844}, {"id": 373, "seek": 149688, "start": 1496.88, "end": 1499.88, "text": " So, yeah, I usually like to jump and point at the screen,", "tokens": [407, 11, 1338, 11, 286, 2673, 411, 281, 3012, 293, 935, 412, 264, 2568, 11], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 374, "seek": 149688, "start": 1499.88, "end": 1504.88, "text": " but I've been told that I cannot cross the wait line.", "tokens": [457, 286, 600, 668, 1907, 300, 286, 2644, 3278, 264, 1699, 1622, 13], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 375, "seek": 149688, "start": 1504.88, "end": 1511.88, "text": " So, yeah, and then what we do when we want to create an assert", "tokens": [407, 11, 1338, 11, 293, 550, 437, 321, 360, 562, 321, 528, 281, 1884, 364, 19810], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 376, "seek": 149688, "start": 1511.88, "end": 1514.88, "text": " is that we call this function called process pending and observe,", "tokens": [307, 300, 321, 818, 341, 2445, 1219, 1399, 32110, 293, 11441, 11], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 377, "seek": 149688, "start": 1514.88, "end": 1519.88, "text": " which just means that we wait for all of the messages", "tokens": [597, 445, 1355, 300, 321, 1699, 337, 439, 295, 264, 7897], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 378, "seek": 149688, "start": 1519.88, "end": 1522.88, "text": " that are currently in the queue of the actor.", "tokens": [300, 366, 4362, 294, 264, 18639, 295, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 379, "seek": 149688, "start": 1522.88, "end": 1525.88, "text": " We have all of those process and then we call observe", "tokens": [492, 362, 439, 295, 729, 1399, 293, 550, 321, 818, 11441], "temperature": 0.0, "avg_logprob": -0.14255092694209173, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.00015863453154452145}, {"id": 380, "seek": 152588, "start": 1525.88, "end": 1530.88, "text": " and we get a snapshot of what is the observable state of the actor.", "tokens": [293, 321, 483, 257, 30163, 295, 437, 307, 264, 9951, 712, 1785, 295, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 381, "seek": 152588, "start": 1530.88, "end": 1533.88, "text": " And here, the observable state was a counter,", "tokens": [400, 510, 11, 264, 9951, 712, 1785, 390, 257, 5682, 11], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 382, "seek": 152588, "start": 1533.88, "end": 1537.88, "text": " so we check that it's equal to the number of documents that we wanted.", "tokens": [370, 321, 1520, 300, 309, 311, 2681, 281, 264, 1230, 295, 8512, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 383, "seek": 152588, "start": 1537.88, "end": 1543.88, "text": " And then we check that the consumer mailbox does contain", "tokens": [400, 550, 321, 1520, 300, 264, 9711, 43602, 775, 5304], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 384, "seek": 152588, "start": 1543.88, "end": 1550.88, "text": " our two batches, because 250 is 100 and 102 batches.", "tokens": [527, 732, 15245, 279, 11, 570, 11650, 307, 2319, 293, 45937, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 385, "seek": 152588, "start": 1550.88, "end": 1553.88, "text": " And we also want to be able to check the timeout,", "tokens": [400, 321, 611, 528, 281, 312, 1075, 281, 1520, 264, 565, 346, 11], "temperature": 0.0, "avg_logprob": -0.13147536744462682, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.0001928120182128623}, {"id": 386, "seek": 155388, "start": 1553.88, "end": 1556.88, "text": " because the timeout counter is working well.", "tokens": [570, 264, 565, 346, 5682, 307, 1364, 731, 13], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 387, "seek": 155388, "start": 1556.88, "end": 1560.88, "text": " So, here, what is interesting is we created our universe,", "tokens": [407, 11, 510, 11, 437, 307, 1880, 307, 321, 2942, 527, 6445, 11], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 388, "seek": 155388, "start": 1560.88, "end": 1564.88, "text": " but with the method with accelerated time,", "tokens": [457, 365, 264, 3170, 365, 29763, 565, 11], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 389, "seek": 155388, "start": 1564.88, "end": 1567.88, "text": " and we would be marking time at this point.", "tokens": [293, 321, 576, 312, 25482, 565, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 390, "seek": 155388, "start": 1567.88, "end": 1570.88, "text": " So, you won't have to wait 30 seconds to have your unit test", "tokens": [407, 11, 291, 1582, 380, 362, 281, 1699, 2217, 3949, 281, 362, 428, 4985, 1500], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 391, "seek": 155388, "start": 1570.88, "end": 1572.88, "text": " to run for 30 seconds.", "tokens": [281, 1190, 337, 2217, 3949, 13], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 392, "seek": 155388, "start": 1572.88, "end": 1577.88, "text": " It will do magic and the result will be exactly the same", "tokens": [467, 486, 360, 5585, 293, 264, 1874, 486, 312, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 393, "seek": 155388, "start": 1577.88, "end": 1580.88, "text": " as if you were not accelerating time,", "tokens": [382, 498, 291, 645, 406, 34391, 565, 11], "temperature": 0.0, "avg_logprob": -0.14019733968407216, "compression_ratio": 1.6727272727272726, "no_speech_prob": 7.71386839915067e-05}, {"id": 394, "seek": 158088, "start": 1580.88, "end": 1583.88, "text": " but it will just be faster.", "tokens": [457, 309, 486, 445, 312, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 395, "seek": 158088, "start": 1583.88, "end": 1586.88, "text": " And I will explain a little bit how it works.", "tokens": [400, 286, 486, 2903, 257, 707, 857, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 396, "seek": 158088, "start": 1586.88, "end": 1590.88, "text": " And so, for the unit test, obviously,", "tokens": [400, 370, 11, 337, 264, 4985, 1500, 11, 2745, 11], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 397, "seek": 158088, "start": 1590.88, "end": 1594.88, "text": " we have to call some way to wait,", "tokens": [321, 362, 281, 818, 512, 636, 281, 1699, 11], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 398, "seek": 158088, "start": 1594.88, "end": 1597.88, "text": " and we call universe.sleep to do that.", "tokens": [293, 321, 818, 6445, 13, 82, 7927, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 399, "seek": 158088, "start": 1597.88, "end": 1602.88, "text": " And it's important to use the universe.sleep and time.sleep", "tokens": [400, 309, 311, 1021, 281, 764, 264, 6445, 13, 82, 7927, 293, 565, 13, 82, 7927], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 400, "seek": 158088, "start": 1602.88, "end": 1606.88, "text": " because we are marking stuff, obviously.", "tokens": [570, 321, 366, 25482, 1507, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 401, "seek": 158088, "start": 1606.88, "end": 1609.88, "text": " We cannot use the marking facilities that we have in Tokyo", "tokens": [492, 2644, 764, 264, 25482, 9406, 300, 321, 362, 294, 15147], "temperature": 0.0, "avg_logprob": -0.1274679958230198, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0002233765844721347}, {"id": 402, "seek": 160988, "start": 1609.88, "end": 1611.88, "text": " because we use several runtimes.", "tokens": [570, 321, 764, 2940, 49435, 1532, 13], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 403, "seek": 160988, "start": 1611.88, "end": 1615.88, "text": " And also, what we do is similar in semantics", "tokens": [400, 611, 11, 437, 321, 360, 307, 2531, 294, 4361, 45298], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 404, "seek": 160988, "start": 1615.88, "end": 1619.88, "text": " as pose and auto-advance if you're familiar with it,", "tokens": [382, 10774, 293, 8399, 12, 345, 85, 719, 498, 291, 434, 4963, 365, 309, 11], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 405, "seek": 160988, "start": 1619.88, "end": 1621.88, "text": " except we never freeze time.", "tokens": [3993, 321, 1128, 15959, 565, 13], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 406, "seek": 160988, "start": 1621.88, "end": 1623.88, "text": " We keep time flowing,", "tokens": [492, 1066, 565, 13974, 11], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 407, "seek": 160988, "start": 1623.88, "end": 1625.88, "text": " but what we do is tiny bit different.", "tokens": [457, 437, 321, 360, 307, 5870, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 408, "seek": 160988, "start": 1625.88, "end": 1630.88, "text": " So, you can imagine that if you were not accelerating time,", "tokens": [407, 11, 291, 393, 3811, 300, 498, 291, 645, 406, 34391, 565, 11], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 409, "seek": 160988, "start": 1630.88, "end": 1632.88, "text": " your actor execution would look like this.", "tokens": [428, 8747, 15058, 576, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 410, "seek": 160988, "start": 1632.88, "end": 1634.88, "text": " So, actor are processing stuff,", "tokens": [407, 11, 8747, 366, 9007, 1507, 11], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 411, "seek": 160988, "start": 1634.88, "end": 1637.88, "text": " and sometimes you don't have any message in any queue,", "tokens": [293, 2171, 291, 500, 380, 362, 604, 3636, 294, 604, 18639, 11], "temperature": 0.0, "avg_logprob": -0.13117439435875933, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.00012144120410084724}, {"id": 412, "seek": 163788, "start": 1637.88, "end": 1639.88, "text": " or actor are either,", "tokens": [420, 8747, 366, 2139, 11], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 413, "seek": 163788, "start": 1639.88, "end": 1643.88, "text": " and the only thing that will resume the processing", "tokens": [293, 264, 787, 551, 300, 486, 15358, 264, 9007], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 414, "seek": 163788, "start": 1643.88, "end": 1646.88, "text": " is some time out to happen and the scheduler saying,", "tokens": [307, 512, 565, 484, 281, 1051, 293, 264, 12000, 260, 1566, 11], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 415, "seek": 163788, "start": 1646.88, "end": 1648.88, "text": " okay, I have a message for you,", "tokens": [1392, 11, 286, 362, 257, 3636, 337, 291, 11], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 416, "seek": 163788, "start": 1648.88, "end": 1652.88, "text": " you asked for a self-scheduled message.", "tokens": [291, 2351, 337, 257, 2698, 12, 6145, 292, 45893, 3636, 13], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 417, "seek": 163788, "start": 1652.88, "end": 1654.88, "text": " It's happening now.", "tokens": [467, 311, 2737, 586, 13], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 418, "seek": 163788, "start": 1654.88, "end": 1656.88, "text": " So, our framework detects that we are in a phase", "tokens": [407, 11, 527, 8388, 5531, 82, 300, 321, 366, 294, 257, 5574], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 419, "seek": 163788, "start": 1656.88, "end": 1659.88, "text": " where no one is working and waiting for the scheduler,", "tokens": [689, 572, 472, 307, 1364, 293, 3806, 337, 264, 12000, 260, 11], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 420, "seek": 163788, "start": 1659.88, "end": 1662.88, "text": " and in that case, and only in that case, we accelerate time.", "tokens": [293, 294, 300, 1389, 11, 293, 787, 294, 300, 1389, 11, 321, 21341, 565, 13], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 421, "seek": 163788, "start": 1662.88, "end": 1666.88, "text": " And that's why we get a result that is exactly the same", "tokens": [400, 300, 311, 983, 321, 483, 257, 1874, 300, 307, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.12448192596435546, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00010567552817519754}, {"id": 422, "seek": 166688, "start": 1666.88, "end": 1670.88, "text": " as if we didn't accelerate time, but just faster.", "tokens": [382, 498, 321, 994, 380, 21341, 565, 11, 457, 445, 4663, 13], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 423, "seek": 166688, "start": 1670.88, "end": 1675.88, "text": " So, we compress our execution before, after.", "tokens": [407, 11, 321, 14778, 527, 15058, 949, 11, 934, 13], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 424, "seek": 166688, "start": 1675.88, "end": 1677.88, "text": " That's how it works.", "tokens": [663, 311, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 425, "seek": 166688, "start": 1682.88, "end": 1686.88, "text": " I wanted to show you the actual indexing pipeline", "tokens": [286, 1415, 281, 855, 291, 264, 3539, 8186, 278, 15517], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 426, "seek": 166688, "start": 1686.88, "end": 1688.88, "text": " in its full complexity.", "tokens": [294, 1080, 1577, 14024, 13], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 427, "seek": 166688, "start": 1688.88, "end": 1690.88, "text": " I said 25 actors, it's not 25 actors here,", "tokens": [286, 848, 3552, 10037, 11, 309, 311, 406, 3552, 10037, 510, 11], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 428, "seek": 166688, "start": 1690.88, "end": 1693.88, "text": " but we have other actors in other parts of the code", "tokens": [457, 321, 362, 661, 10037, 294, 661, 3166, 295, 264, 3089], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 429, "seek": 166688, "start": 1693.88, "end": 1695.88, "text": " because the pattern got quite popular.", "tokens": [570, 264, 5102, 658, 1596, 3743, 13], "temperature": 0.0, "avg_logprob": -0.10596382224952781, "compression_ratio": 1.5308056872037914, "no_speech_prob": 0.00013120855146553367}, {"id": 430, "seek": 169588, "start": 1695.88, "end": 1700.88, "text": " It's quite complex, but it makes me extremely good.", "tokens": [467, 311, 1596, 3997, 11, 457, 309, 1669, 385, 4664, 665, 13], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 431, "seek": 169588, "start": 1700.88, "end": 1704.88, "text": " It makes me feel good to be able to show that", "tokens": [467, 1669, 385, 841, 665, 281, 312, 1075, 281, 855, 300], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 432, "seek": 169588, "start": 1704.88, "end": 1707.88, "text": " when we have to explain a new developer", "tokens": [562, 321, 362, 281, 2903, 257, 777, 10754], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 433, "seek": 169588, "start": 1707.88, "end": 1712.88, "text": " what the indexing is doing.", "tokens": [437, 264, 8186, 278, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 434, "seek": 169588, "start": 1712.88, "end": 1714.88, "text": " We can point at things.", "tokens": [492, 393, 935, 412, 721, 13], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 435, "seek": 169588, "start": 1714.88, "end": 1718.88, "text": " Every single one of these box is doing one very simple thing.", "tokens": [2048, 2167, 472, 295, 613, 2424, 307, 884, 472, 588, 2199, 551, 13], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 436, "seek": 169588, "start": 1718.88, "end": 1721.88, "text": " It has its own file, it has its own unit test.", "tokens": [467, 575, 1080, 1065, 3991, 11, 309, 575, 1080, 1065, 4985, 1500, 13], "temperature": 0.0, "avg_logprob": -0.10240817624469135, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.478587890043855e-05}, {"id": 437, "seek": 172188, "start": 1721.88, "end": 1726.88, "text": " It makes me happy to have this very simple figure", "tokens": [467, 1669, 385, 2055, 281, 362, 341, 588, 2199, 2573], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 438, "seek": 172188, "start": 1726.88, "end": 1729.88, "text": " that we can discuss around.", "tokens": [300, 321, 393, 2248, 926, 13], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 439, "seek": 172188, "start": 1729.88, "end": 1733.88, "text": " One thing that I need to talk to you about is one problem", "tokens": [1485, 551, 300, 286, 643, 281, 751, 281, 291, 466, 307, 472, 1154], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 440, "seek": 172188, "start": 1733.88, "end": 1744.88, "text": " with Actors is if you have cycles in your Actors network,", "tokens": [365, 3251, 830, 307, 498, 291, 362, 17796, 294, 428, 3251, 830, 3209, 11], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 441, "seek": 172188, "start": 1744.88, "end": 1746.88, "text": " you might experience deadlock.", "tokens": [291, 1062, 1752, 3116, 4102, 13], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 442, "seek": 172188, "start": 1746.88, "end": 1748.88, "text": " And it's a pretty terrible thing that kind of deadlock", "tokens": [400, 309, 311, 257, 1238, 6237, 551, 300, 733, 295, 3116, 4102], "temperature": 0.0, "avg_logprob": -0.12283971150716146, "compression_ratio": 1.508108108108108, "no_speech_prob": 8.53578167152591e-05}, {"id": 443, "seek": 174888, "start": 1748.88, "end": 1751.88, "text": " because it can happen at any time in production,", "tokens": [570, 309, 393, 1051, 412, 604, 565, 294, 4265, 11], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 444, "seek": 174888, "start": 1751.88, "end": 1753.88, "text": " like it can work for one week", "tokens": [411, 309, 393, 589, 337, 472, 1243], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 445, "seek": 174888, "start": 1753.88, "end": 1759.88, "text": " and then you experience a deadlock and it's a scary thing.", "tokens": [293, 550, 291, 1752, 257, 3116, 4102, 293, 309, 311, 257, 6958, 551, 13], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 446, "seek": 174888, "start": 1759.88, "end": 1763.88, "text": " So there's a sufficient condition to have deadlocks.", "tokens": [407, 456, 311, 257, 11563, 4188, 281, 362, 3116, 34896, 13], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 447, "seek": 174888, "start": 1763.88, "end": 1765.88, "text": " If you don't have any cycles, right?", "tokens": [759, 291, 500, 380, 362, 604, 17796, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 448, "seek": 174888, "start": 1765.88, "end": 1769.88, "text": " And usually that's the case when you are writing a pipeline.", "tokens": [400, 2673, 300, 311, 264, 1389, 562, 291, 366, 3579, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 449, "seek": 174888, "start": 1769.88, "end": 1772.88, "text": " In the graph before, there was actually a cycle.", "tokens": [682, 264, 4295, 949, 11, 456, 390, 767, 257, 6586, 13], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 450, "seek": 174888, "start": 1772.88, "end": 1775.88, "text": " We will have a look at it in a second.", "tokens": [492, 486, 362, 257, 574, 412, 309, 294, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.1659565153576079, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.00010831312101799995}, {"id": 451, "seek": 177588, "start": 1775.88, "end": 1779.88, "text": " There is another, there is a nicer condition to have deadlocks.", "tokens": [821, 307, 1071, 11, 456, 307, 257, 22842, 4188, 281, 362, 3116, 34896, 13], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 452, "seek": 177588, "start": 1779.88, "end": 1786.88, "text": " It's if the graph of your Actors where you removed all of the queue", "tokens": [467, 311, 498, 264, 4295, 295, 428, 3251, 830, 689, 291, 7261, 439, 295, 264, 18639], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 453, "seek": 177588, "start": 1786.88, "end": 1788.88, "text": " where you had an infinite capacity,", "tokens": [689, 291, 632, 364, 13785, 6042, 11], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 454, "seek": 177588, "start": 1788.88, "end": 1791.88, "text": " if that one is a DAG, then you won't have deadlocks.", "tokens": [498, 300, 472, 307, 257, 9578, 38, 11, 550, 291, 1582, 380, 362, 3116, 34896, 13], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 455, "seek": 177588, "start": 1791.88, "end": 1794.88, "text": " And that's what we are doing here.", "tokens": [400, 300, 311, 437, 321, 366, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 456, "seek": 177588, "start": 1794.88, "end": 1799.88, "text": " So the loop, the cycle that we had was due to the fact", "tokens": [407, 264, 6367, 11, 264, 6586, 300, 321, 632, 390, 3462, 281, 264, 1186], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 457, "seek": 177588, "start": 1799.88, "end": 1803.88, "text": " that we have like an auxiliary pipeline", "tokens": [300, 321, 362, 411, 364, 43741, 15517], "temperature": 0.0, "avg_logprob": -0.14649512551047586, "compression_ratio": 1.674641148325359, "no_speech_prob": 9.022658923640847e-05}, {"id": 458, "seek": 180388, "start": 1803.88, "end": 1805.88, "text": " that is merging the file together", "tokens": [300, 307, 44559, 264, 3991, 1214], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 459, "seek": 180388, "start": 1805.88, "end": 1808.88, "text": " and there is an arrow over there.", "tokens": [293, 456, 307, 364, 11610, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 460, "seek": 180388, "start": 1808.88, "end": 1811.88, "text": " Sorry, I'm going to cross the line.", "tokens": [4919, 11, 286, 478, 516, 281, 3278, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 461, "seek": 180388, "start": 1811.88, "end": 1815.88, "text": " I did it, I apologize.", "tokens": [286, 630, 309, 11, 286, 12328, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 462, "seek": 180388, "start": 1815.88, "end": 1818.88, "text": " If you remove this arrow, then it's a DAG", "tokens": [759, 291, 4159, 341, 11610, 11, 550, 309, 311, 257, 9578, 38], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 463, "seek": 180388, "start": 1818.88, "end": 1822.88, "text": " and that's sufficient condition to never experience deadlock.", "tokens": [293, 300, 311, 11563, 4188, 281, 1128, 1752, 3116, 4102, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 464, "seek": 180388, "start": 1822.88, "end": 1825.88, "text": " It helps me sleep at night.", "tokens": [467, 3665, 385, 2817, 412, 1818, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 465, "seek": 180388, "start": 1825.88, "end": 1828.88, "text": " And yeah, we have a bunch of other features.", "tokens": [400, 1338, 11, 321, 362, 257, 3840, 295, 661, 4122, 13], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 466, "seek": 180388, "start": 1828.88, "end": 1831.88, "text": " The most important one I think I want to tell you about", "tokens": [440, 881, 1021, 472, 286, 519, 286, 528, 281, 980, 291, 466], "temperature": 0.0, "avg_logprob": -0.1301288237938514, "compression_ratio": 1.5276595744680852, "no_speech_prob": 6.168795516714454e-05}, {"id": 467, "seek": 183188, "start": 1831.88, "end": 1834.88, "text": " is that we measure back pressure.", "tokens": [307, 300, 321, 3481, 646, 3321, 13], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 468, "seek": 183188, "start": 1834.88, "end": 1837.88, "text": " So the framework is automatically measuring back pressure", "tokens": [407, 264, 8388, 307, 6772, 13389, 646, 3321], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 469, "seek": 183188, "start": 1837.88, "end": 1841.88, "text": " and expose it as a promoter counter.", "tokens": [293, 19219, 309, 382, 257, 6609, 260, 5682, 13], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 470, "seek": 183188, "start": 1841.88, "end": 1843.88, "text": " That's really neat.", "tokens": [663, 311, 534, 10654, 13], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 471, "seek": 183188, "start": 1843.88, "end": 1845.88, "text": " Very useful for us.", "tokens": [4372, 4420, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 472, "seek": 183188, "start": 1845.88, "end": 1850.88, "text": " And let's use the rest of the time for questions.", "tokens": [400, 718, 311, 764, 264, 1472, 295, 264, 565, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 473, "seek": 183188, "start": 1850.88, "end": 1852.88, "text": " So...", "tokens": [407, 485], "temperature": 0.0, "avg_logprob": -0.20877443253993988, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.00022558105411008}, {"id": 474, "seek": 185288, "start": 1852.88, "end": 1861.88, "text": " APPLAUSE", "tokens": [35298], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 475, "seek": 185288, "start": 1861.88, "end": 1864.88, "text": " So are there any questions in the room?", "tokens": [407, 366, 456, 604, 1651, 294, 264, 1808, 30], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 476, "seek": 185288, "start": 1864.88, "end": 1866.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 477, "seek": 185288, "start": 1866.88, "end": 1868.88, "text": " The last slide of the previous slide was", "tokens": [440, 1036, 4137, 295, 264, 3894, 4137, 390], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 478, "seek": 185288, "start": 1868.88, "end": 1870.88, "text": " you didn't need parallel actors.", "tokens": [291, 994, 380, 643, 8952, 10037, 13], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 479, "seek": 185288, "start": 1870.88, "end": 1873.88, "text": " What did you need like Fanny and Fanna out", "tokens": [708, 630, 291, 643, 411, 479, 11612, 293, 479, 1795, 484], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 480, "seek": 185288, "start": 1873.88, "end": 1876.88, "text": " for having any parallel work?", "tokens": [337, 1419, 604, 8952, 589, 30], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 481, "seek": 185288, "start": 1876.88, "end": 1879.88, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.2901898875381007, "compression_ratio": 1.3397435897435896, "no_speech_prob": 0.0008700966718606651}, {"id": 482, "seek": 187988, "start": 1879.88, "end": 1882.88, "text": " So there is something that I didn't read", "tokens": [407, 456, 307, 746, 300, 286, 994, 380, 1401], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 483, "seek": 187988, "start": 1882.88, "end": 1885.88, "text": " but Sylvain was very fast", "tokens": [457, 3902, 14574, 491, 390, 588, 2370], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 484, "seek": 187988, "start": 1885.88, "end": 1888.88, "text": " and noticed that we don't have anything", "tokens": [293, 5694, 300, 321, 500, 380, 362, 1340], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 485, "seek": 187988, "start": 1888.88, "end": 1892.88, "text": " to be able to have several actor work on the same queue", "tokens": [281, 312, 1075, 281, 362, 2940, 8747, 589, 322, 264, 912, 18639], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 486, "seek": 187988, "start": 1892.88, "end": 1897.88, "text": " or work concurrently to process stuff faster.", "tokens": [420, 589, 37702, 356, 281, 1399, 1507, 4663, 13], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 487, "seek": 187988, "start": 1897.88, "end": 1902.88, "text": " So yeah, strongly, we haven't needed that", "tokens": [407, 1338, 11, 10613, 11, 321, 2378, 380, 2978, 300], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 488, "seek": 187988, "start": 1902.88, "end": 1904.88, "text": " strongly enough to actually implement it.", "tokens": [10613, 1547, 281, 767, 4445, 309, 13], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 489, "seek": 187988, "start": 1904.88, "end": 1907.88, "text": " I wrote an implementation and never managed it", "tokens": [286, 4114, 364, 11420, 293, 1128, 6453, 309], "temperature": 0.0, "avg_logprob": -0.18897379528392444, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.00042865052819252014}, {"id": 490, "seek": 190788, "start": 1907.88, "end": 1909.88, "text": " because we didn't really need it.", "tokens": [570, 321, 994, 380, 534, 643, 309, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 491, "seek": 190788, "start": 1909.88, "end": 1914.88, "text": " So indexing, we just spawn several pipelines", "tokens": [407, 8186, 278, 11, 321, 445, 17088, 2940, 40168], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 492, "seek": 190788, "start": 1914.88, "end": 1916.88, "text": " on the same machine, not too much.", "tokens": [322, 264, 912, 3479, 11, 406, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 493, "seek": 190788, "start": 1916.88, "end": 1919.88, "text": " So that part, it's unparalleled.", "tokens": [407, 300, 644, 11, 309, 311, 517, 2181, 336, 31689, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 494, "seek": 190788, "start": 1919.88, "end": 1925.88, "text": " But yeah, we just never need...", "tokens": [583, 1338, 11, 321, 445, 1128, 643, 485], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 495, "seek": 190788, "start": 1925.88, "end": 1927.88, "text": " We haven't needed it yet.", "tokens": [492, 2378, 380, 2978, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 496, "seek": 190788, "start": 1927.88, "end": 1929.88, "text": " I can't really tell where.", "tokens": [286, 393, 380, 534, 980, 689, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 497, "seek": 190788, "start": 1929.88, "end": 1934.88, "text": " Yes, exactly.", "tokens": [1079, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.22126308488257138, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0006153464782983065}, {"id": 498, "seek": 193488, "start": 1934.88, "end": 1939.88, "text": " So the parallel behavior.", "tokens": [407, 264, 8952, 5223, 13], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 499, "seek": 193488, "start": 1939.88, "end": 1942.88, "text": " Sorry, you want me to repeat.", "tokens": [4919, 11, 291, 528, 385, 281, 7149, 13], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 500, "seek": 193488, "start": 1942.88, "end": 1944.88, "text": " So Sylvain was saying,", "tokens": [407, 3902, 14574, 491, 390, 1566, 11], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 501, "seek": 193488, "start": 1944.88, "end": 1947.88, "text": " we use more than one core", "tokens": [321, 764, 544, 813, 472, 4965], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 502, "seek": 193488, "start": 1947.88, "end": 1949.88, "text": " just because within the pipeline", "tokens": [445, 570, 1951, 264, 15517], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 503, "seek": 193488, "start": 1949.88, "end": 1950.88, "text": " we do the streamlining thing.", "tokens": [321, 360, 264, 4309, 31079, 551, 13], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 504, "seek": 193488, "start": 1950.88, "end": 1952.88, "text": " So we may have different actors", "tokens": [407, 321, 815, 362, 819, 10037], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 505, "seek": 193488, "start": 1952.88, "end": 1955.88, "text": " that are consuming the CPU at the same time", "tokens": [300, 366, 19867, 264, 13199, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 506, "seek": 193488, "start": 1955.88, "end": 1957.88, "text": " but we don't have one actor going,", "tokens": [457, 321, 500, 380, 362, 472, 8747, 516, 11], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 507, "seek": 193488, "start": 1957.88, "end": 1959.88, "text": " oh, there's actually five instance of the actor", "tokens": [1954, 11, 456, 311, 767, 1732, 5197, 295, 264, 8747], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 508, "seek": 193488, "start": 1959.88, "end": 1963.88, "text": " and we are doing the work five times faster.", "tokens": [293, 321, 366, 884, 264, 589, 1732, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.15107282602562094, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0003980190958827734}, {"id": 509, "seek": 196388, "start": 1963.88, "end": 1967.88, "text": " So we didn't need that.", "tokens": [407, 321, 994, 380, 643, 300, 13], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 510, "seek": 196388, "start": 1967.88, "end": 1969.88, "text": " Any more questions?", "tokens": [2639, 544, 1651, 30], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 511, "seek": 196388, "start": 1969.88, "end": 1970.88, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 512, "seek": 196388, "start": 1970.88, "end": 1972.88, "text": " Do you have the fairness system", "tokens": [1144, 291, 362, 264, 29765, 1185], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 513, "seek": 196388, "start": 1972.88, "end": 1974.88, "text": " so that one actor doesn't keep", "tokens": [370, 300, 472, 8747, 1177, 380, 1066], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 514, "seek": 196388, "start": 1974.88, "end": 1976.88, "text": " on the processing restriction", "tokens": [322, 264, 9007, 29529], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 515, "seek": 196388, "start": 1976.88, "end": 1981.88, "text": " other orders in the process?", "tokens": [661, 9470, 294, 264, 1399, 30], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 516, "seek": 196388, "start": 1981.88, "end": 1983.88, "text": " So no, we don't have that.", "tokens": [407, 572, 11, 321, 500, 380, 362, 300, 13], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 517, "seek": 196388, "start": 1983.88, "end": 1985.88, "text": " So one thing that we have,", "tokens": [407, 472, 551, 300, 321, 362, 11], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 518, "seek": 196388, "start": 1985.88, "end": 1987.88, "text": " actually we don't want...", "tokens": [767, 321, 500, 380, 528, 485], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 519, "seek": 196388, "start": 1987.88, "end": 1989.88, "text": " We have the opposite problem.", "tokens": [492, 362, 264, 6182, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 520, "seek": 196388, "start": 1989.88, "end": 1991.88, "text": " We don't want fairness.", "tokens": [492, 500, 380, 528, 29765, 13], "temperature": 0.0, "avg_logprob": -0.24928822120030722, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0002617706486489624}, {"id": 521, "seek": 199188, "start": 1991.88, "end": 1996.88, "text": " So if you look at our pipeline,", "tokens": [407, 498, 291, 574, 412, 527, 15517, 11], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 522, "seek": 199188, "start": 1996.88, "end": 1998.88, "text": " the stuff that is taking a lot of CPU", "tokens": [264, 1507, 300, 307, 1940, 257, 688, 295, 13199], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 523, "seek": 199188, "start": 1998.88, "end": 2001.88, "text": " would be the indexer.", "tokens": [576, 312, 264, 8186, 260, 13], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 524, "seek": 199188, "start": 2001.88, "end": 2003.88, "text": " Sylvain would take a lot of IO", "tokens": [3902, 14574, 491, 576, 747, 257, 688, 295, 39839], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 525, "seek": 199188, "start": 2003.88, "end": 2006.88, "text": " and we want to give it priority", "tokens": [293, 321, 528, 281, 976, 309, 9365], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 526, "seek": 199188, "start": 2006.88, "end": 2010.88, "text": " because it's the thing that we want it to use", "tokens": [570, 309, 311, 264, 551, 300, 321, 528, 309, 281, 764], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 527, "seek": 199188, "start": 2010.88, "end": 2012.88, "text": " as much as CPU as it can.", "tokens": [382, 709, 382, 13199, 382, 309, 393, 13], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 528, "seek": 199188, "start": 2012.88, "end": 2014.88, "text": " So we want to give it one core", "tokens": [407, 321, 528, 281, 976, 309, 472, 4965], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 529, "seek": 199188, "start": 2014.88, "end": 2018.88, "text": " and we want it to use as much IO as it needs.", "tokens": [293, 321, 528, 309, 281, 764, 382, 709, 39839, 382, 309, 2203, 13], "temperature": 0.0, "avg_logprob": -0.12808980845441723, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.0002141544537153095}, {"id": 530, "seek": 201888, "start": 2018.88, "end": 2021.88, "text": " And we would like to give it priority.", "tokens": [400, 321, 576, 411, 281, 976, 309, 9365, 13], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 531, "seek": 201888, "start": 2021.88, "end": 2023.88, "text": " So the way we do that is that we run it", "tokens": [407, 264, 636, 321, 360, 300, 307, 300, 321, 1190, 309], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 532, "seek": 201888, "start": 2023.88, "end": 2025.88, "text": " on a specific runtime", "tokens": [322, 257, 2685, 34474], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 533, "seek": 201888, "start": 2025.88, "end": 2028.88, "text": " and over there it has its dedicated core.", "tokens": [293, 670, 456, 309, 575, 1080, 8374, 4965, 13], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 534, "seek": 201888, "start": 2028.88, "end": 2031.88, "text": " For IO, the framework is actually not really helping.", "tokens": [1171, 39839, 11, 264, 8388, 307, 767, 406, 534, 4315, 13], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 535, "seek": 201888, "start": 2031.88, "end": 2034.88, "text": " So what we do is that we have some IO throttling", "tokens": [407, 437, 321, 360, 307, 300, 321, 362, 512, 39839, 739, 1521, 1688], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 536, "seek": 201888, "start": 2034.88, "end": 2037.88, "text": " that makes it so that the other actors", "tokens": [300, 1669, 309, 370, 300, 264, 661, 10037], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 537, "seek": 201888, "start": 2037.88, "end": 2042.88, "text": " are not able to actually more write on disk faster", "tokens": [366, 406, 1075, 281, 767, 544, 2464, 322, 12355, 4663], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 538, "seek": 201888, "start": 2042.88, "end": 2044.88, "text": " and you can configure that", "tokens": [293, 291, 393, 22162, 300], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 539, "seek": 201888, "start": 2044.88, "end": 2047.88, "text": " and there's some corner of the table computation", "tokens": [293, 456, 311, 512, 4538, 295, 264, 3199, 24903], "temperature": 0.0, "avg_logprob": -0.11956589501183312, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.00015143654309213161}, {"id": 540, "seek": 204788, "start": 2047.88, "end": 2049.88, "text": " to compute what you should do.", "tokens": [281, 14722, 437, 291, 820, 360, 13], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 541, "seek": 204788, "start": 2049.88, "end": 2052.88, "text": " But yeah, other actors will not be able to write on disk", "tokens": [583, 1338, 11, 661, 10037, 486, 406, 312, 1075, 281, 2464, 322, 12355], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 542, "seek": 204788, "start": 2052.88, "end": 2056.88, "text": " faster than, let's say, 80 megabytes per second.", "tokens": [4663, 813, 11, 718, 311, 584, 11, 4688, 10816, 24538, 680, 1150, 13], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 543, "seek": 204788, "start": 2056.88, "end": 2060.88, "text": " And the merge that you have below,", "tokens": [400, 264, 22183, 300, 291, 362, 2507, 11], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 544, "seek": 204788, "start": 2060.88, "end": 2062.88, "text": " it's okay if it lags a little bit.", "tokens": [309, 311, 1392, 498, 309, 8953, 82, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 545, "seek": 204788, "start": 2062.88, "end": 2066.88, "text": " That's the part that we want to be low priority", "tokens": [663, 311, 264, 644, 300, 321, 528, 281, 312, 2295, 9365], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 546, "seek": 204788, "start": 2066.88, "end": 2069.88, "text": " and the part on the top we want to be high priority.", "tokens": [293, 264, 644, 322, 264, 1192, 321, 528, 281, 312, 1090, 9365, 13], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 547, "seek": 204788, "start": 2069.88, "end": 2071.88, "text": " So we don't have any fairness", "tokens": [407, 321, 500, 380, 362, 604, 29765], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 548, "seek": 204788, "start": 2071.88, "end": 2076.88, "text": " and we don't want any fairness.", "tokens": [293, 321, 500, 380, 528, 604, 29765, 13], "temperature": 0.0, "avg_logprob": -0.11910346606830219, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00010318175191059709}, {"id": 549, "seek": 207688, "start": 2076.88, "end": 2077.88, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 550, "seek": 207688, "start": 2077.88, "end": 2079.88, "text": " So I guess you're supervising that", "tokens": [407, 286, 2041, 291, 434, 37971, 3436, 300], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 551, "seek": 207688, "start": 2079.88, "end": 2084.88, "text": " because otherwise the timeouts may also be delayed.", "tokens": [570, 5911, 264, 565, 7711, 815, 611, 312, 20268, 13], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 552, "seek": 207688, "start": 2084.88, "end": 2087.88, "text": " So the supervisor is running on,", "tokens": [407, 264, 24610, 307, 2614, 322, 11], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 553, "seek": 207688, "start": 2087.88, "end": 2089.88, "text": " it's very fast, it doesn't do much.", "tokens": [309, 311, 588, 2370, 11, 309, 1177, 380, 360, 709, 13], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 554, "seek": 207688, "start": 2089.88, "end": 2091.88, "text": " So it's running on a Tokyo runtime", "tokens": [407, 309, 311, 2614, 322, 257, 15147, 34474], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 555, "seek": 207688, "start": 2091.88, "end": 2093.88, "text": " that has a dedicated core", "tokens": [300, 575, 257, 8374, 4965], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 556, "seek": 207688, "start": 2093.88, "end": 2096.88, "text": " and runs one bazillion actors,", "tokens": [293, 6676, 472, 27147, 11836, 10037, 11], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 557, "seek": 207688, "start": 2096.88, "end": 2098.88, "text": " but they're all very light.", "tokens": [457, 436, 434, 439, 588, 1442, 13], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 558, "seek": 207688, "start": 2098.88, "end": 2100.88, "text": " So it doesn't matter at all.", "tokens": [407, 309, 1177, 380, 1871, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 559, "seek": 207688, "start": 2100.88, "end": 2102.88, "text": " Okay, yeah.", "tokens": [1033, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.25477571770696356, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.00030174374114722013}, {"id": 560, "seek": 210288, "start": 2102.88, "end": 2105.88, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 561, "seek": 210288, "start": 2105.88, "end": 2108.88, "text": " Because, I mean, if your actors are very heavy now,", "tokens": [1436, 11, 286, 914, 11, 498, 428, 10037, 366, 588, 4676, 586, 11], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 562, "seek": 210288, "start": 2108.88, "end": 2111.88, "text": " of course, the supervisor will catch you.", "tokens": [295, 1164, 11, 264, 24610, 486, 3745, 291, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 563, "seek": 210288, "start": 2111.88, "end": 2112.88, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 564, "seek": 210288, "start": 2112.88, "end": 2114.88, "text": " At some point, because otherwise your timeouts", "tokens": [1711, 512, 935, 11, 570, 5911, 428, 565, 7711], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 565, "seek": 210288, "start": 2114.88, "end": 2115.88, "text": " will still be delayed.", "tokens": [486, 920, 312, 20268, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 566, "seek": 210288, "start": 2115.88, "end": 2116.88, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 567, "seek": 210288, "start": 2116.88, "end": 2117.88, "text": " You're absolutely right,", "tokens": [509, 434, 3122, 558, 11], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 568, "seek": 210288, "start": 2117.88, "end": 2121.88, "text": " but the thing is it's running on its specific runtime", "tokens": [457, 264, 551, 307, 309, 311, 2614, 322, 1080, 2685, 34474], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 569, "seek": 210288, "start": 2121.88, "end": 2123.88, "text": " and it's not CPU-HV,", "tokens": [293, 309, 311, 406, 13199, 12, 39, 53, 11], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 570, "seek": 210288, "start": 2123.88, "end": 2128.88, "text": " so there's plenty of core to work with.", "tokens": [370, 456, 311, 7140, 295, 4965, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 571, "seek": 210288, "start": 2128.88, "end": 2129.88, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 572, "seek": 210288, "start": 2129.88, "end": 2131.88, "text": " When you accelerate time in the testing universe,", "tokens": [1133, 291, 21341, 565, 294, 264, 4997, 6445, 11], "temperature": 0.0, "avg_logprob": -0.23883779170149463, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0006643303204327822}, {"id": 573, "seek": 213188, "start": 2131.88, "end": 2136.88, "text": " do you have to specify the steps in time you take?", "tokens": [360, 291, 362, 281, 16500, 264, 4439, 294, 565, 291, 747, 30], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 574, "seek": 213188, "start": 2136.88, "end": 2138.88, "text": " You mean the number of times?", "tokens": [509, 914, 264, 1230, 295, 1413, 30], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 575, "seek": 213188, "start": 2138.88, "end": 2140.88, "text": " I assume that when you accelerate time,", "tokens": [286, 6552, 300, 562, 291, 21341, 565, 11], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 576, "seek": 213188, "start": 2140.88, "end": 2142.88, "text": " basically when nothing is happening,", "tokens": [1936, 562, 1825, 307, 2737, 11], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 577, "seek": 213188, "start": 2142.88, "end": 2144.88, "text": " you take a time step and then see if something would have happened", "tokens": [291, 747, 257, 565, 1823, 293, 550, 536, 498, 746, 576, 362, 2011], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 578, "seek": 213188, "start": 2144.88, "end": 2146.88, "text": " at that time point.", "tokens": [412, 300, 565, 935, 13], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 579, "seek": 213188, "start": 2146.88, "end": 2148.88, "text": " Does that mean that you have to specify,", "tokens": [4402, 300, 914, 300, 291, 362, 281, 16500, 11], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 580, "seek": 213188, "start": 2148.88, "end": 2152.88, "text": " we take steps of 100 milliseconds and then test every time", "tokens": [321, 747, 4439, 295, 2319, 34184, 293, 550, 1500, 633, 565], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 581, "seek": 213188, "start": 2152.88, "end": 2154.88, "text": " if something would be happening now?", "tokens": [498, 746, 576, 312, 2737, 586, 30], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 582, "seek": 213188, "start": 2154.88, "end": 2157.88, "text": " No, it's not.", "tokens": [883, 11, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 583, "seek": 213188, "start": 2157.88, "end": 2160.88, "text": " We don't need to say how many steps we take.", "tokens": [492, 500, 380, 643, 281, 584, 577, 867, 4439, 321, 747, 13], "temperature": 0.0, "avg_logprob": -0.13659132321675618, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0003538670716807246}, {"id": 584, "seek": 216088, "start": 2160.88, "end": 2164.88, "text": " We don't need to say what is the resolution of the steps", "tokens": [492, 500, 380, 643, 281, 584, 437, 307, 264, 8669, 295, 264, 4439], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 585, "seek": 216088, "start": 2164.88, "end": 2165.88, "text": " that we take.", "tokens": [300, 321, 747, 13], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 586, "seek": 216088, "start": 2165.88, "end": 2167.88, "text": " So the only thing that we do is that the scheduler,", "tokens": [407, 264, 787, 551, 300, 321, 360, 307, 300, 264, 12000, 260, 11], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 587, "seek": 216088, "start": 2167.88, "end": 2170.88, "text": " when it detects, it needs to accelerate time.", "tokens": [562, 309, 5531, 82, 11, 309, 2203, 281, 21341, 565, 13], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 588, "seek": 216088, "start": 2170.88, "end": 2173.88, "text": " It has some kind of heap that says,", "tokens": [467, 575, 512, 733, 295, 33591, 300, 1619, 11], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 589, "seek": 216088, "start": 2173.88, "end": 2177.88, "text": " OK, the next event is actually in 70 milliseconds.", "tokens": [2264, 11, 264, 958, 2280, 307, 767, 294, 5285, 34184, 13], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 590, "seek": 216088, "start": 2177.88, "end": 2180.88, "text": " So let's jump 70 milliseconds in the future", "tokens": [407, 718, 311, 3012, 5285, 34184, 294, 264, 2027], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 591, "seek": 216088, "start": 2180.88, "end": 2182.88, "text": " and it triggers that event.", "tokens": [293, 309, 22827, 300, 2280, 13], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 592, "seek": 216088, "start": 2182.88, "end": 2186.88, "text": " And then the execution of the actor", "tokens": [400, 550, 264, 15058, 295, 264, 8747], "temperature": 0.0, "avg_logprob": -0.13880319502747174, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0001254741509910673}, {"id": 593, "seek": 218688, "start": 2186.88, "end": 2190.88, "text": " that was supposed to receive this message will go", "tokens": [300, 390, 3442, 281, 4774, 341, 3636, 486, 352], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 594, "seek": 218688, "start": 2190.88, "end": 2193.88, "text": " and if no actor is working anymore,", "tokens": [293, 498, 572, 8747, 307, 1364, 3602, 11], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 595, "seek": 218688, "start": 2193.88, "end": 2195.88, "text": " then we re-accidate them again.", "tokens": [550, 321, 319, 12, 8476, 327, 473, 552, 797, 13], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 596, "seek": 218688, "start": 2195.88, "end": 2202.88, "text": " So it's no steps, no resolution or nothing.", "tokens": [407, 309, 311, 572, 4439, 11, 572, 8669, 420, 1825, 13], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 597, "seek": 218688, "start": 2202.88, "end": 2203.88, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 598, "seek": 218688, "start": 2203.88, "end": 2206.88, "text": " How about reliability if you want to be sure", "tokens": [1012, 466, 24550, 498, 291, 528, 281, 312, 988], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 599, "seek": 218688, "start": 2206.88, "end": 2208.88, "text": " that the bot line will make it to the index,", "tokens": [300, 264, 10592, 1622, 486, 652, 309, 281, 264, 8186, 11], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 600, "seek": 218688, "start": 2208.88, "end": 2212.88, "text": " you count them or how do you know they made it through?", "tokens": [291, 1207, 552, 420, 577, 360, 291, 458, 436, 1027, 309, 807, 30], "temperature": 0.0, "avg_logprob": -0.22258993295522836, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.0005978188710287213}, {"id": 601, "seek": 221288, "start": 2212.88, "end": 2219.88, "text": " So, yeah, it should be the subject of another talk.", "tokens": [407, 11, 1338, 11, 309, 820, 312, 264, 3983, 295, 1071, 751, 13], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 602, "seek": 221288, "start": 2219.88, "end": 2222.88, "text": " Because now that's a super interesting question.", "tokens": [1436, 586, 300, 311, 257, 1687, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 603, "seek": 221288, "start": 2222.88, "end": 2227.88, "text": " So the pipeline, you want to know,", "tokens": [407, 264, 15517, 11, 291, 528, 281, 458, 11], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 604, "seek": 221288, "start": 2227.88, "end": 2229.88, "text": " to have an idea of what kind of semantics,", "tokens": [281, 362, 364, 1558, 295, 437, 733, 295, 4361, 45298, 11], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 605, "seek": 221288, "start": 2229.88, "end": 2232.88, "text": " delivery semantics that you want to have.", "tokens": [8982, 4361, 45298, 300, 291, 528, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 606, "seek": 221288, "start": 2232.88, "end": 2235.88, "text": " We actually offer exactly one semantics", "tokens": [492, 767, 2626, 2293, 472, 4361, 45298], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 607, "seek": 221288, "start": 2235.88, "end": 2238.88, "text": " and the way we deal with that is,", "tokens": [293, 264, 636, 321, 2028, 365, 300, 307, 11], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 608, "seek": 221288, "start": 2238.88, "end": 2240.88, "text": " so we didn't talk about that,", "tokens": [370, 321, 994, 380, 751, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.16518240077521212, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00023114476061891764}, {"id": 609, "seek": 224088, "start": 2240.88, "end": 2244.88, "text": " we have the documents that are coming from a source actor", "tokens": [321, 362, 264, 8512, 300, 366, 1348, 490, 257, 4009, 8747], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 610, "seek": 224088, "start": 2244.88, "end": 2247.88, "text": " and the source actors, when we spawn it,", "tokens": [293, 264, 4009, 10037, 11, 562, 321, 17088, 309, 11], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 611, "seek": 224088, "start": 2247.88, "end": 2252.88, "text": " we tell it, OK, you need to stream messages", "tokens": [321, 980, 309, 11, 2264, 11, 291, 643, 281, 4309, 7897], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 612, "seek": 224088, "start": 2252.88, "end": 2255.88, "text": " from these specific checkpoints.", "tokens": [490, 613, 2685, 1520, 20552, 13], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 613, "seek": 224088, "start": 2255.88, "end": 2259.88, "text": " And when we publish stuff like downstream,", "tokens": [400, 562, 321, 11374, 1507, 411, 30621, 11], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 614, "seek": 224088, "start": 2259.88, "end": 2263.88, "text": " we publish stuff by running a transaction", "tokens": [321, 11374, 1507, 538, 2614, 257, 14425], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 615, "seek": 224088, "start": 2263.88, "end": 2267.88, "text": " on our metadata store backend", "tokens": [322, 527, 26603, 3531, 38087], "temperature": 0.0, "avg_logprob": -0.13942736548346443, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.00021187335369177163}, {"id": 616, "seek": 226788, "start": 2267.88, "end": 2270.88, "text": " and that transaction updates the checkpoints", "tokens": [293, 300, 14425, 9205, 264, 1520, 20552], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 617, "seek": 226788, "start": 2270.88, "end": 2273.88, "text": " of the stuff that we have published", "tokens": [295, 264, 1507, 300, 321, 362, 6572], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 618, "seek": 226788, "start": 2273.88, "end": 2275.88, "text": " and it publishes the speed as well.", "tokens": [293, 309, 11374, 279, 264, 3073, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 619, "seek": 226788, "start": 2275.88, "end": 2278.88, "text": " So that when we restart everything,", "tokens": [407, 300, 562, 321, 21022, 1203, 11], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 620, "seek": 226788, "start": 2278.88, "end": 2281.88, "text": " we can check in the metadata store", "tokens": [321, 393, 1520, 294, 264, 26603, 3531], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 621, "seek": 226788, "start": 2281.88, "end": 2283.88, "text": " what is the last checkpoint and it starts from there.", "tokens": [437, 307, 264, 1036, 42269, 293, 309, 3719, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 622, "seek": 226788, "start": 2283.88, "end": 2285.88, "text": " And if there is an error,", "tokens": [400, 498, 456, 307, 364, 6713, 11], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 623, "seek": 226788, "start": 2285.88, "end": 2289.88, "text": " the metadata store will just yell at us and return an error.", "tokens": [264, 26603, 3531, 486, 445, 20525, 412, 505, 293, 2736, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 624, "seek": 226788, "start": 2289.88, "end": 2291.88, "text": " It will say, OK, no, something weird has been happening,", "tokens": [467, 486, 584, 11, 2264, 11, 572, 11, 746, 3657, 575, 668, 2737, 11], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 625, "seek": 226788, "start": 2291.88, "end": 2294.88, "text": " maybe we all had two pipelines working at the same time", "tokens": [1310, 321, 439, 632, 732, 40168, 1364, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.12295477493949558, "compression_ratio": 1.75, "no_speech_prob": 0.00018608503160066903}, {"id": 626, "seek": 229488, "start": 2294.88, "end": 2300.88, "text": " and your checkpoints are overlapping and you have a problem.", "tokens": [293, 428, 1520, 20552, 366, 33535, 293, 291, 362, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 627, "seek": 229488, "start": 2300.88, "end": 2303.88, "text": " That's the way we work with this problem.", "tokens": [663, 311, 264, 636, 321, 589, 365, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 628, "seek": 229488, "start": 2303.88, "end": 2304.88, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 629, "seek": 229488, "start": 2304.88, "end": 2307.88, "text": " At the universe, is that a special crate", "tokens": [1711, 264, 6445, 11, 307, 300, 257, 2121, 42426], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 630, "seek": 229488, "start": 2307.88, "end": 2309.88, "text": " or is it in the standard category?", "tokens": [420, 307, 309, 294, 264, 3832, 7719, 30], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 631, "seek": 229488, "start": 2309.88, "end": 2313.88, "text": " No, the universe thing is something within our framework", "tokens": [883, 11, 264, 6445, 551, 307, 746, 1951, 527, 8388], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 632, "seek": 229488, "start": 2313.88, "end": 2315.88, "text": " and that's what we use to be able to isolate", "tokens": [293, 300, 311, 437, 321, 764, 281, 312, 1075, 281, 25660], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 633, "seek": 229488, "start": 2315.88, "end": 2318.88, "text": " typically different programs or different unit tests", "tokens": [5850, 819, 4268, 420, 819, 4985, 6921], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 634, "seek": 229488, "start": 2318.88, "end": 2320.88, "text": " or different systems.", "tokens": [420, 819, 3652, 13], "temperature": 0.0, "avg_logprob": -0.20715619159001175, "compression_ratio": 1.6589861751152073, "no_speech_prob": 0.0002560413267929107}, {"id": 635, "seek": 232088, "start": 2320.88, "end": 2324.88, "text": " Yeah, it's within our active framework.", "tokens": [865, 11, 309, 311, 1951, 527, 4967, 8388, 13], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 636, "seek": 232088, "start": 2326.88, "end": 2327.88, "text": " It is.", "tokens": [467, 307, 13], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 637, "seek": 232088, "start": 2330.88, "end": 2332.88, "text": " Do we still have time?", "tokens": [1144, 321, 920, 362, 565, 30], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 638, "seek": 232088, "start": 2332.88, "end": 2336.88, "text": " I think we have one more question or one more minute.", "tokens": [286, 519, 321, 362, 472, 544, 1168, 420, 472, 544, 3456, 13], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 639, "seek": 232088, "start": 2336.88, "end": 2338.88, "text": " I think there was a...", "tokens": [286, 519, 456, 390, 257, 485], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 640, "seek": 232088, "start": 2338.88, "end": 2339.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 641, "seek": 232088, "start": 2339.88, "end": 2341.88, "text": " I understand on this graph,", "tokens": [286, 1223, 322, 341, 4295, 11], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 642, "seek": 232088, "start": 2341.88, "end": 2347.88, "text": " the numbers on the rows indicate the capacity of the queue, right?", "tokens": [264, 3547, 322, 264, 13241, 13330, 264, 6042, 295, 264, 18639, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.313786130202444, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0016603844705969095}, {"id": 643, "seek": 234788, "start": 2347.88, "end": 2350.88, "text": " The capacity of the channel between the actors, right?", "tokens": [440, 6042, 295, 264, 2269, 1296, 264, 10037, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 644, "seek": 234788, "start": 2350.88, "end": 2351.88, "text": " The numbers, yes.", "tokens": [440, 3547, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 645, "seek": 234788, "start": 2351.88, "end": 2352.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 646, "seek": 234788, "start": 2352.88, "end": 2356.88, "text": " So we have a lot of tuning points in this system, right?", "tokens": [407, 321, 362, 257, 688, 295, 15164, 2793, 294, 341, 1185, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 647, "seek": 234788, "start": 2356.88, "end": 2357.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 648, "seek": 234788, "start": 2357.88, "end": 2359.88, "text": " With relation to the back pressure.", "tokens": [2022, 9721, 281, 264, 646, 3321, 13], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 649, "seek": 234788, "start": 2359.88, "end": 2361.88, "text": " So the question is, from your experience,", "tokens": [407, 264, 1168, 307, 11, 490, 428, 1752, 11], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 650, "seek": 234788, "start": 2361.88, "end": 2364.88, "text": " how sensitive the performance of the system", "tokens": [577, 9477, 264, 3389, 295, 264, 1185], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 651, "seek": 234788, "start": 2364.88, "end": 2368.88, "text": " is to the tuning of back pressure on the channels?", "tokens": [307, 281, 264, 15164, 295, 646, 3321, 322, 264, 9235, 30], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 652, "seek": 234788, "start": 2368.88, "end": 2371.88, "text": " And maybe you have some kind of advice", "tokens": [400, 1310, 291, 362, 512, 733, 295, 5192], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 653, "seek": 234788, "start": 2371.88, "end": 2374.88, "text": " or a rule of thumb on what to choose", "tokens": [420, 257, 4978, 295, 9298, 322, 437, 281, 2826], "temperature": 0.0, "avg_logprob": -0.15272437979321962, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0013711544452235103}, {"id": 654, "seek": 237488, "start": 2374.88, "end": 2377.88, "text": " for the best performance.", "tokens": [337, 264, 1151, 3389, 13], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 655, "seek": 237488, "start": 2377.88, "end": 2378.88, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 656, "seek": 237488, "start": 2378.88, "end": 2380.88, "text": " So the question was, on this slide,", "tokens": [407, 264, 1168, 390, 11, 322, 341, 4137, 11], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 657, "seek": 237488, "start": 2380.88, "end": 2383.88, "text": " all of the little numbers that we have on the arrow", "tokens": [439, 295, 264, 707, 3547, 300, 321, 362, 322, 264, 11610], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 658, "seek": 237488, "start": 2383.88, "end": 2387.88, "text": " is the capacity of the different queues between actors", "tokens": [307, 264, 6042, 295, 264, 819, 631, 1247, 1296, 10037], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 659, "seek": 237488, "start": 2387.88, "end": 2389.88, "text": " that's a lot of parameters to tune.", "tokens": [300, 311, 257, 688, 295, 9834, 281, 10864, 13], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 660, "seek": 237488, "start": 2389.88, "end": 2391.88, "text": " They probably have an impact on performance.", "tokens": [814, 1391, 362, 364, 2712, 322, 3389, 13], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 661, "seek": 237488, "start": 2391.88, "end": 2393.88, "text": " Is there a cool recipe to...", "tokens": [1119, 456, 257, 1627, 6782, 281, 485], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 662, "seek": 237488, "start": 2393.88, "end": 2395.88, "text": " So the first question was,", "tokens": [407, 264, 700, 1168, 390, 11], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 663, "seek": 237488, "start": 2395.88, "end": 2397.88, "text": " how much do they impact performance?", "tokens": [577, 709, 360, 436, 2712, 3389, 30], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 664, "seek": 237488, "start": 2397.88, "end": 2399.88, "text": " And the second one is,", "tokens": [400, 264, 1150, 472, 307, 11], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 665, "seek": 237488, "start": 2399.88, "end": 2403.88, "text": " do we have a nice recipe to be able to tune them maybe automatically?", "tokens": [360, 321, 362, 257, 1481, 6782, 281, 312, 1075, 281, 10864, 552, 1310, 6772, 30], "temperature": 0.0, "avg_logprob": -0.13379026050409995, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.00027195908478461206}, {"id": 666, "seek": 240388, "start": 2403.88, "end": 2406.88, "text": " I'll go first because there is no more time.", "tokens": [286, 603, 352, 700, 570, 456, 307, 572, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 667, "seek": 240388, "start": 2406.88, "end": 2411.88, "text": " So they don't impact performance all that much", "tokens": [407, 436, 500, 380, 2712, 3389, 439, 300, 709], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 668, "seek": 240388, "start": 2411.88, "end": 2416.88, "text": " as long as you got them a little bit correct.", "tokens": [382, 938, 382, 291, 658, 552, 257, 707, 857, 3006, 13], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 669, "seek": 240388, "start": 2416.88, "end": 2420.88, "text": " So you usually need to identify the stuff", "tokens": [407, 291, 2673, 643, 281, 5876, 264, 1507], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 670, "seek": 240388, "start": 2420.88, "end": 2423.88, "text": " that should be at one,", "tokens": [300, 820, 312, 412, 472, 11], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 671, "seek": 240388, "start": 2423.88, "end": 2425.88, "text": " and then you put it at one", "tokens": [293, 550, 291, 829, 309, 412, 472], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 672, "seek": 240388, "start": 2425.88, "end": 2427.88, "text": " and where you want a little bit of capacity.", "tokens": [293, 689, 291, 528, 257, 707, 857, 295, 6042, 13], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 673, "seek": 240388, "start": 2427.88, "end": 2431.88, "text": " It should be quite obvious if you know your system.", "tokens": [467, 820, 312, 1596, 6322, 498, 291, 458, 428, 1185, 13], "temperature": 0.0, "avg_logprob": -0.11368802877572867, "compression_ratio": 1.613861386138614, "no_speech_prob": 0.00025684002321213484}, {"id": 674, "seek": 243188, "start": 2431.88, "end": 2434.88, "text": " And I'm sure that there is a nice recipe", "tokens": [400, 286, 478, 988, 300, 456, 307, 257, 1481, 6782], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 675, "seek": 243188, "start": 2434.88, "end": 2436.88, "text": " to auto-detect that.", "tokens": [281, 8399, 12, 17863, 557, 300, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 676, "seek": 243188, "start": 2436.88, "end": 2438.88, "text": " I haven't found it.", "tokens": [286, 2378, 380, 1352, 309, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 677, "seek": 243188, "start": 2438.88, "end": 2440.88, "text": " So if you have ideas, I'd love to...", "tokens": [407, 498, 291, 362, 3487, 11, 286, 1116, 959, 281, 485], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 678, "seek": 243188, "start": 2440.88, "end": 2442.88, "text": " Usually that kind of question is someone", "tokens": [11419, 300, 733, 295, 1168, 307, 1580], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 679, "seek": 243188, "start": 2442.88, "end": 2444.88, "text": " who is thinking about something.", "tokens": [567, 307, 1953, 466, 746, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 680, "seek": 243188, "start": 2444.88, "end": 2447.88, "text": " So please come to me after the talk.", "tokens": [407, 1767, 808, 281, 385, 934, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 681, "seek": 243188, "start": 2447.88, "end": 2450.88, "text": " And I'd love to hear your thoughts.", "tokens": [400, 286, 1116, 959, 281, 1568, 428, 4598, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 682, "seek": 243188, "start": 2450.88, "end": 2452.88, "text": " Thank you, everyone. Time is up.", "tokens": [1044, 291, 11, 1518, 13, 6161, 307, 493, 13], "temperature": 0.0, "avg_logprob": -0.20097393892249282, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.0002949395275209099}, {"id": 683, "seek": 245288, "start": 2452.88, "end": 2462.88, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50864], "temperature": 0.0, "avg_logprob": -0.6623080968856812, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.005263061728328466}], "language": "en"}