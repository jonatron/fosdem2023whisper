{"text": " Okay, we're ready for our next talk. Daniel is going to talk about MetaNet devices. Thanks. All right. Thanks a lot. So, yeah. So, this talk is about MetaNet devices. This work has been done by my colleague and myself. We are at ISO Wayland software engineers working on the kernel and also Cilium. So, really, the goal is what we were, the question we were asking ourselves, like what about, how can we leverage the BPF infrastructure in the kernel and also the networking features to really achieve maximum performance for Kubernetes parts. And before I go into the kernel bits, just a really quick recap around Kubernetes and parts and what it is. So, basically, what you can see here is a host. The host can have one or many parts. In Kubernetes, it's an orchestration system, essentially, and a part is usually defined as a network namespace, and it is connected to, typically, to Weave devices to get traffic in and out of them. A part can have one or many containers that are sharing this network namespace. So, yeah, and CNI is basically a networking plug-in which will set up various things. When a part comes up, for example, it will set up net devices, assign IP addresses. It has an IPAM infrastructure to manage pool of addresses. It will install routes. In the case of Cilium, which is a CNI, it will also set up the BPF data path to basically route traffic in and out. It has various features on top as well, such as policy enforcement, load balancing, bandwidth management, and so on and so forth. But I don't want to make this talk about all the different features about Cilium, but rather about performance. There was an interesting keynote last year at the SAEcon from Brandon Craig, where he talked about computing performance and what's on the horizon, and he had a couple of predictions and one was quite interesting when he was talking about OS performance, and the statement that he made is that, well, given the kernel, it's becoming increasingly complex. The performance defaults are getting worse and worse. Yeah, so he stated that it basically takes the whole US team to make the operating system perform well. And the problem is, given all these performance teams, they are trying to optimize at the larger scale, nobody's actually looking at the defaults anymore and how they can be optimized. So this was quite interesting. So yeah, in case of defaults, we were wondering, given two Kubernetes nodes with part to part and they're connected with 100 gigabit NIC, we wanted to look at the single flow, like a single TCP stream, and we asked, like, what's the default baseline you can get to? Where are bottlenecks? How can they become, how can they be overcome? And actually, can we provide better defaults out of that, what we figure out? Why bothering with single stream performance? Well, first of all, it's interesting for the kernel to be able to cope with growing NIC speeds, so 100, 200 gigabit or more. How can this be maxed out with the single stream? There are lots of data intensive workloads from user and customer sites around machine learning, AI, and so on. But generally, it's also interesting to be able to free up the resources and give them to the application instead of the kernel having to block them. So the assumptions for our test is basically, like, usually the Kubernetes worker nodes that users run, they are quite generic, they can run any kind of workloads. What we are also seeing is a large number of users typically just stick to defaults, they don't tune specifically the kernel. There's an interesting cloud-native usage report where they tried to get some insights into how Kubernetes deployments usually look like. There's definitely an increasing trend to have a higher density of containers per host. So like around 50 or more is expected these days and the number of pods per node is also increasing. So yeah, the question now is, okay, so like a basic, very basic compatibility setting, for example, for the case of Silium, we use the, like if you deploy it in a basic mode, we just use the upper stack for routing and forwarding. There are various reasons why people might want to use that. For example, in case of Kubernetes, there's a component called Qproxy which uses net filter IP tables for service management, for service load balancing. Some people stick to that or they have custom net filter rules, so maybe they require to use the upper stack for that or just simply they for now just went with defaults and might look into more tuning at the later point in time. So when we try to look at the performance for that, what you can see here is on the yellow bar is the host-to-host performance for a single stream, so we got to 44 gigabit per second, but then if you do the pod-to-pod connectivity, it's really reduced dramatically. And so yeah, like one of the reasons is, and I will get into this in a bit, is because the upper stack is giving false feedback to the TCP stack. One thing that we did like a year ago or so is to introduce a feature where we can, which is called BPF host routing, where we don't use the upper stack and for BPF itself, given we attach to physical devices, but also to Veef devices, we added a couple of new helper functions there. One is called BPF redirect peer. What this basically is doing is adding a fast switch into the network namespace for the ingress traffic. So basically we're just like, instead of going the usual X-MID route to the Veef devices, we can retrieve the Veef device inside the pod and just scrub the packet to remove the necessary data that we typically remove in switching network namespaces, but then also to just set the device to the device inside the pod. And then circle that around in the main receive loop without going to a per-CPU backlog queue that you would normally do when you transfer data through Veef devices, and we don't need to use the upper stack because there's all the information already available in BPF context. And for the way out, we added a helper which is called BPF redirect neighbor. So that one will basically insert the packet into the neighboring subsystem of the kernel. So usually we can do a FIP lookup out of BPF, so there's a helper for this as well, and then combined with this for the resolution of neighbors. It will allow that you don't need to go to the upper stack, so the nice benefit you get as well with this is that the socket context for the network packet for the SKB is retained all the way to the physical device until the packet is actually sent out. And this is not the case when you normally go to the upper stack. So then the TCP stack actually thinks that once you go to the upper stack that it already left the node, but it's actually not the case. And this way it can be retained. And this is how the complete picture looks like. And if you look at the performance, it's already much better. So we were able to get almost a 40 gigabit per second under 1.5K MTU. So this was interesting. Now the question is, how could we close the remaining gap? Under 8K MTU, we also did some tests, and one thing to note here is that we were able for a single TCP stream to get to 98 gigabit per second for the host-to-host case, but still the situation looks quite the same for the weave with the BPF host routing. So there's still a small gap that we want to close here as well. And that's where we introduce a new device type as a weave replacement. So we call this meta device because it's programmable to BPF, and you can implement various, like your own business logic into this. So it's flexible. And this time, the main difference is that this also gets a faster switch on the egress side for the egress traffic, so it doesn't need to go to the per-CPU or back-lock-U for the egress as well. So if you look at the flame graphs, so that's the worst-case scenario where we compared the weave and the meta device. So what you can see here on the weave device, like on X-MID, it will basically scrub the packet data, it will un-CUE the packet to a per-CPU back-lock-U, and then at some point there's a network's action. It will pick up the packets from the queue again, and in the worst case, it can be deferred to the kernel software QDemon, and then you see this rescheduling where you have a new stack where this is processed again. And then from the BPF side, it will reach BPF on the TC egress on the host weave, where we then can only forward this to the physical device to leave the node, right? And all of this can be done in one go without rescheduling through this meta device. So it will scrub the packet, it will switch the network namespace, it will reset the device pointers, and it will then directly call the BPF program. And if the BPF program says that based on the FIP lookup and so on, that it will forward the packet directly to the physical device, then it will avoid this rescheduling scenario. So on the right side, I mean, it's really straightforward. That's how the implementation of the driver X-MID routine looks like. So it will basically just call into BPF, and then based on the verdict, push it out. It's really just like 500 lines of code. So it's very simple and straightforward. I think it's just one-fifth of the weave driver that we have right now. And the other focus that we wanted to put into is compatibility as well, so that, like given in Solium, we need to support multiple kernels, the ideal case would be that we don't need to change much of the BPF program and can keep it as is. And in case of XTP, we didn't want to implement it because for the weave case, it really is very complex. It even adds multi-q support, which you normally would not need on a virtual device. So we wanted to keep it as simple as possible and to have the flexibility that this can be added as a single or a paired device. So for the weave replacement, it would be a paired device, but you could also do it as a single device and then implement whatever logic you would want in BPF for that. So looking at the performance, again, like the TCP stream under 8K, so this is really able to reach through this approach the full 98 gigabit per second, and in terms of latency, so we did some net-perf TCP or R measurements as well, where you get the minimum, the P90, 99 latency and so on. So this is really on par with the host. So now we were asking ourselves, so can we push this even further? I mean, well, so we were able to get to 98 gigabit per second, but like the cost for like a megabyte to transfer, can this pushed even more? And there's a relatively recent kernel feature which is called Big TCP. It landed for IPv6 only in 5.19 and was developed by Google, and the whole idea behind Big TCP is to even more aggressively aggregate for GEO and GSO. So normally the aggregation, the kernel will try it basically out of the incoming packet stream, create a super packet and then we'll push it up to the networking stack so it only needs to be traversed once. And the limit up until that point was for 64K packets, simply because in the IP header that's the maximum packet size that you can do. And the idea for Big TCP for IPv6 was that, well, maybe we could create a hop-by-hop header in the GEO layer and then add, and then like the 16-bit packet length field can be overcome because there's a jumbo-gram extension in there which allows for a 32-bit field. So you can do much more aggressive aggregation. And yeah, so this is also now supported with the new studio release where this will be set up for all the devices underneath automatically for IPv6. Actually like this week, that was also merged for IPv4 now. So this will end in kernel 6.3, which is exciting. And when we looked at the performance again under Big TCP, turns out like using the upper stack is currently broken in the kernel, so that still needs to be fixed. We will look into that. So forwarding there wouldn't work. And with the host routing cases, it will basically bump up the regular Veef one to get this on par with the meta and also the host, so it will basically hide those glitches. The latency is still better in terms of like the short packet response type workloads for the meta, so that's still on par with the host. So what is the remaining offender like when you run all these features together? It's basically the copying to users, so like between 60 and 70% of the cycles is really spent on copying all this data to user space. So the next question we ask ourselves actually in this experiment, so what if we combine the whole Big TCP stuff with TCP0 copy, so what if we could leverage the memory map TCP? And it turns out that's currently not possible in the kernel. That's a limitation because in the GRO layer, Big TCP will create a frag list, which is essentially like a list of SKBs as a single big one that is being pushed up the stack. And TCP0 copy only works on the SKB frags, so that's like an internal. So basically you have a single SKB and it has like the pages as read only attached in the non-linear section. So that currently does not work. Combining those two would probably have like really big potential, but what we now try to do is we just looked at just using the TCP0 copy to see how it looks without the Big TCP. Actually speaking it's not as straightforward to deploy because first of all you need to rewrite your application in order to leverage memory map TCP. This can be done for RX but also for TX or both. And it needs driver changes and in particular driver changes to be able to split the header from the data because the data you want to memory map to user space. Some Nix might do this with the hardware and some others you would have to do some kind of pseudo header data splits where you basically just copy the header into a linear section. So this is how it would look like. We tried this for 4K and 8K MTU. There's a great talk from Eric Dumasay, the TCP maintainer in terms of what all the all those things you need to do in order to be able to make use of this for example you also need to align the TCP window scale to 12 segments so that you fill exactly those pages. And yeah the driver support can be very different like we had in our lab MLX500 gigabit Nix and they did not implement the header data splits. So my colleague Nikolai did the POC implementation in the like to change the driver to be able to do that so that we could get some measurements out of this. And if you want to look into an application like an example application and how you can implement that there's one in the networking self-test in the upstream tree called TCP M-MAP which is useful also for the benchmarking. So you really need to align various different settings like for our test implementation we used like for MLX5 the non-striding mode so like the legacy mode so you need to switch that off and EVE tool first because I mean for the POC it was easier to do the way like the packet layout is done there then you need to align the MTU to either one page or two pages and you need to do various other settings like for the course of time I will not go into all of the details. And generally I would think it would be useful addition to the kernel to have like a configuration framework for that and to be able to have more drivers supporting the header data split. There's actually one in Windows kernel turns out so there's some documentation around there that we found while preparing for the talk. The other thing is the caveat is like the TCP-0 copy may like the benefits might be limited if you then actually go in your application and then touch the data because then they need to be pulled into the cache which they would be if like if you would have to copy things to user but they may not be like if you just memory map so the applications there is mostly like on the for example storage side where you wouldn't have to do that. And looking at the data for 4K MTU we tried with the implementation we got to 81 gigabit per second so that's a bit limiting. Could also be that this is mostly because the implementation was you know POC with lots of optimization potential that can still be there but we looked at the 8K MTU and there we were able to get to 98 gigabit per second but the interesting piece here is the cost per megabyte we could we were able to reduce from 85 down to 27 microseconds per megabyte so this really is significant and yeah because of the avoidance that you don't copy anymore. So as a summary so we started with the default and then you know switch to the 8K MTU then we went to the host routing that we covered then the addition of the meta device where we can avoid where we can do the slow latency you know for ingress and egress then the big TCP and that all works without changes to the application and with that you can already get like a 2x improvement and then it gets even more dramatic but it's of course dependent on your application for the zero copy. Some of the future directions as I mentioned earlier it would be useful to have like a generic header data split framework for NIC drivers that they can implement that and expose it to this setting. There's potential here as well to optimize for example like the header pages could be the head page could be packed with the headers it could get recycling which we didn't implement here and in future big TCP would be interesting to combine with the zero copy so that this covers the frag list in GRO and the other thing that is at some point on the horizon is to push the big TCP actually onto the wire as well if the hardware supports that and yeah so with that I'm done with the talk and the prototype for the meta device is currently you can find on this branch we are working on pushing this upstream in the coming months and there's also the prototype public for the MLX5 header data split and yeah so the plan is basically to get this into upstream kernel and then also to get this integrated into Syllium for the next release yeah thank you are there any questions? Thanks we already have two questions in the chat so we can start with them if there are no one in the room. Alright so the first question we got was can we well not really a question as much as a comment I think which is can we please come up with a better name than meta for this just call it BPF or BPFDF or something like that. The other thing was if the perf benefit comes from calling through from inside the network namespace can we just make VIF do that instead? I think we could but the question is like I haven't looked into the like how it would affect the XDP related bits so it felt easier to do something simple and start from scratch like one thing I don't really like about the VIF devices to be honest how complex it got with all the XDP additions and it's not even that beneficial and it added like multi queue support and all of that which was not needed at all for a virtual device so I really wanted to have something simple and yeah it's easier so. Any more questions from the audience here? I have a question you're about big TCP and TCP zero copy it's you're in your lab you only did benchmarks host to host right because so but also part to part so we tried both yeah. Because do you know how it would work with the switches do you need special switch support for this? No no so the big TCP is only local to the node so like the packets on the wire they will still be your MTU size packets it's just that the aggregation on like for the local stack is much bigger so it's it doesn't affect anything on the wire that's a nice thing. And for the zero copy? I mean like for the zero copy you need to change definitely the MTU which also affects the rest of your network of course so that's one thing that would be required for that. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.84, "text": " Okay, we're ready for our next talk.", "tokens": [1033, 11, 321, 434, 1919, 337, 527, 958, 751, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 1, "seek": 0, "start": 7.84, "end": 10.88, "text": " Daniel is going to talk about MetaNet devices.", "tokens": [8033, 307, 516, 281, 751, 466, 6377, 64, 31890, 5759, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 2, "seek": 0, "start": 10.88, "end": 11.88, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 3, "seek": 0, "start": 11.88, "end": 12.88, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 4, "seek": 0, "start": 12.88, "end": 13.88, "text": " Thanks a lot.", "tokens": [2561, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 5, "seek": 0, "start": 13.88, "end": 14.88, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 6, "seek": 0, "start": 14.88, "end": 15.88, "text": " So, this talk is about MetaNet devices.", "tokens": [407, 11, 341, 751, 307, 466, 6377, 64, 31890, 5759, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 7, "seek": 0, "start": 15.88, "end": 19.96, "text": " This work has been done by my colleague and myself.", "tokens": [639, 589, 575, 668, 1096, 538, 452, 13532, 293, 2059, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 8, "seek": 0, "start": 19.96, "end": 24.68, "text": " We are at ISO Wayland software engineers working on the kernel and also Cilium.", "tokens": [492, 366, 412, 25042, 9558, 1661, 4722, 11955, 1364, 322, 264, 28256, 293, 611, 383, 388, 2197, 13], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 9, "seek": 0, "start": 24.68, "end": 29.2, "text": " So, really, the goal is what we were, the question we were asking ourselves, like what", "tokens": [407, 11, 534, 11, 264, 3387, 307, 437, 321, 645, 11, 264, 1168, 321, 645, 3365, 4175, 11, 411, 437], "temperature": 0.0, "avg_logprob": -0.29844691835600756, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.2509976923465729}, {"id": 10, "seek": 2920, "start": 29.2, "end": 35.12, "text": " about, how can we leverage the BPF infrastructure in the kernel and also the networking features", "tokens": [466, 11, 577, 393, 321, 13982, 264, 40533, 37, 6896, 294, 264, 28256, 293, 611, 264, 17985, 4122], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 11, "seek": 2920, "start": 35.12, "end": 40.36, "text": " to really achieve maximum performance for Kubernetes parts.", "tokens": [281, 534, 4584, 6674, 3389, 337, 23145, 3166, 13], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 12, "seek": 2920, "start": 40.36, "end": 46.72, "text": " And before I go into the kernel bits, just a really quick recap around Kubernetes and", "tokens": [400, 949, 286, 352, 666, 264, 28256, 9239, 11, 445, 257, 534, 1702, 20928, 926, 23145, 293], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 13, "seek": 2920, "start": 46.72, "end": 48.28, "text": " parts and what it is.", "tokens": [3166, 293, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 14, "seek": 2920, "start": 48.28, "end": 51.44, "text": " So, basically, what you can see here is a host.", "tokens": [407, 11, 1936, 11, 437, 291, 393, 536, 510, 307, 257, 3975, 13], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 15, "seek": 2920, "start": 51.44, "end": 54.36, "text": " The host can have one or many parts.", "tokens": [440, 3975, 393, 362, 472, 420, 867, 3166, 13], "temperature": 0.0, "avg_logprob": -0.21045547307923781, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.00018705494585447013}, {"id": 16, "seek": 5436, "start": 54.36, "end": 60.24, "text": " In Kubernetes, it's an orchestration system, essentially, and a part is usually defined", "tokens": [682, 23145, 11, 309, 311, 364, 14161, 2405, 1185, 11, 4476, 11, 293, 257, 644, 307, 2673, 7642], "temperature": 0.0, "avg_logprob": -0.19854056581537774, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.00015552765398751944}, {"id": 17, "seek": 5436, "start": 60.24, "end": 67.2, "text": " as a network namespace, and it is connected to, typically, to Weave devices to get traffic", "tokens": [382, 257, 3209, 5288, 17940, 11, 293, 309, 307, 4582, 281, 11, 5850, 11, 281, 492, 946, 5759, 281, 483, 6419], "temperature": 0.0, "avg_logprob": -0.19854056581537774, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.00015552765398751944}, {"id": 18, "seek": 5436, "start": 67.2, "end": 69.32, "text": " in and out of them.", "tokens": [294, 293, 484, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19854056581537774, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.00015552765398751944}, {"id": 19, "seek": 5436, "start": 69.32, "end": 74.48, "text": " A part can have one or many containers that are sharing this network namespace.", "tokens": [316, 644, 393, 362, 472, 420, 867, 17089, 300, 366, 5414, 341, 3209, 5288, 17940, 13], "temperature": 0.0, "avg_logprob": -0.19854056581537774, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.00015552765398751944}, {"id": 20, "seek": 5436, "start": 74.48, "end": 82.76, "text": " So, yeah, and CNI is basically a networking plug-in which will set up various things.", "tokens": [407, 11, 1338, 11, 293, 14589, 40, 307, 1936, 257, 17985, 5452, 12, 259, 597, 486, 992, 493, 3683, 721, 13], "temperature": 0.0, "avg_logprob": -0.19854056581537774, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.00015552765398751944}, {"id": 21, "seek": 8276, "start": 82.76, "end": 88.52000000000001, "text": " When a part comes up, for example, it will set up net devices, assign IP addresses.", "tokens": [1133, 257, 644, 1487, 493, 11, 337, 1365, 11, 309, 486, 992, 493, 2533, 5759, 11, 6269, 8671, 16862, 13], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 22, "seek": 8276, "start": 88.52000000000001, "end": 93.64, "text": " It has an IPAM infrastructure to manage pool of addresses.", "tokens": [467, 575, 364, 8671, 2865, 6896, 281, 3067, 7005, 295, 16862, 13], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 23, "seek": 8276, "start": 93.64, "end": 96.48, "text": " It will install routes.", "tokens": [467, 486, 3625, 18242, 13], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 24, "seek": 8276, "start": 96.48, "end": 102.76, "text": " In the case of Cilium, which is a CNI, it will also set up the BPF data path to basically", "tokens": [682, 264, 1389, 295, 383, 388, 2197, 11, 597, 307, 257, 14589, 40, 11, 309, 486, 611, 992, 493, 264, 40533, 37, 1412, 3100, 281, 1936], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 25, "seek": 8276, "start": 102.76, "end": 105.28, "text": " route traffic in and out.", "tokens": [7955, 6419, 294, 293, 484, 13], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 26, "seek": 8276, "start": 105.28, "end": 110.72, "text": " It has various features on top as well, such as policy enforcement, load balancing, bandwidth", "tokens": [467, 575, 3683, 4122, 322, 1192, 382, 731, 11, 1270, 382, 3897, 11475, 11, 3677, 22495, 11, 23647], "temperature": 0.0, "avg_logprob": -0.1300265057252185, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000108602580439765}, {"id": 27, "seek": 11072, "start": 110.72, "end": 113.03999999999999, "text": " management, and so on and so forth.", "tokens": [4592, 11, 293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1728870603773329, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.00019358660210855305}, {"id": 28, "seek": 11072, "start": 113.03999999999999, "end": 120.08, "text": " But I don't want to make this talk about all the different features about Cilium, but rather", "tokens": [583, 286, 500, 380, 528, 281, 652, 341, 751, 466, 439, 264, 819, 4122, 466, 383, 388, 2197, 11, 457, 2831], "temperature": 0.0, "avg_logprob": -0.1728870603773329, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.00019358660210855305}, {"id": 29, "seek": 11072, "start": 120.08, "end": 122.08, "text": " about performance.", "tokens": [466, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1728870603773329, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.00019358660210855305}, {"id": 30, "seek": 11072, "start": 122.08, "end": 127.64, "text": " There was an interesting keynote last year at the SAEcon from Brandon Craig, where he", "tokens": [821, 390, 364, 1880, 33896, 1036, 1064, 412, 264, 16482, 36, 1671, 490, 22606, 19732, 11, 689, 415], "temperature": 0.0, "avg_logprob": -0.1728870603773329, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.00019358660210855305}, {"id": 31, "seek": 11072, "start": 127.64, "end": 133.16, "text": " talked about computing performance and what's on the horizon, and he had a couple of predictions", "tokens": [2825, 466, 15866, 3389, 293, 437, 311, 322, 264, 18046, 11, 293, 415, 632, 257, 1916, 295, 21264], "temperature": 0.0, "avg_logprob": -0.1728870603773329, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.00019358660210855305}, {"id": 32, "seek": 13316, "start": 133.16, "end": 141.35999999999999, "text": " and one was quite interesting when he was talking about OS performance, and the statement", "tokens": [293, 472, 390, 1596, 1880, 562, 415, 390, 1417, 466, 12731, 3389, 11, 293, 264, 5629], "temperature": 0.0, "avg_logprob": -0.21103556744464033, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.00010193449270445853}, {"id": 33, "seek": 13316, "start": 141.35999999999999, "end": 147.04, "text": " that he made is that, well, given the kernel, it's becoming increasingly complex.", "tokens": [300, 415, 1027, 307, 300, 11, 731, 11, 2212, 264, 28256, 11, 309, 311, 5617, 12980, 3997, 13], "temperature": 0.0, "avg_logprob": -0.21103556744464033, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.00010193449270445853}, {"id": 34, "seek": 13316, "start": 147.04, "end": 150.68, "text": " The performance defaults are getting worse and worse.", "tokens": [440, 3389, 7576, 82, 366, 1242, 5324, 293, 5324, 13], "temperature": 0.0, "avg_logprob": -0.21103556744464033, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.00010193449270445853}, {"id": 35, "seek": 13316, "start": 150.68, "end": 157.96, "text": " Yeah, so he stated that it basically takes the whole US team to make the operating system", "tokens": [865, 11, 370, 415, 11323, 300, 309, 1936, 2516, 264, 1379, 2546, 1469, 281, 652, 264, 7447, 1185], "temperature": 0.0, "avg_logprob": -0.21103556744464033, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.00010193449270445853}, {"id": 36, "seek": 13316, "start": 157.96, "end": 159.92, "text": " perform well.", "tokens": [2042, 731, 13], "temperature": 0.0, "avg_logprob": -0.21103556744464033, "compression_ratio": 1.604878048780488, "no_speech_prob": 0.00010193449270445853}, {"id": 37, "seek": 15992, "start": 159.92, "end": 168.11999999999998, "text": " And the problem is, given all these performance teams, they are trying to optimize at the", "tokens": [400, 264, 1154, 307, 11, 2212, 439, 613, 3389, 5491, 11, 436, 366, 1382, 281, 19719, 412, 264], "temperature": 0.0, "avg_logprob": -0.1778104627454603, "compression_ratio": 1.545, "no_speech_prob": 4.90081038151402e-05}, {"id": 38, "seek": 15992, "start": 168.11999999999998, "end": 174.6, "text": " larger scale, nobody's actually looking at the defaults anymore and how they can be optimized.", "tokens": [4833, 4373, 11, 5079, 311, 767, 1237, 412, 264, 7576, 82, 3602, 293, 577, 436, 393, 312, 26941, 13], "temperature": 0.0, "avg_logprob": -0.1778104627454603, "compression_ratio": 1.545, "no_speech_prob": 4.90081038151402e-05}, {"id": 39, "seek": 15992, "start": 174.6, "end": 177.2, "text": " So this was quite interesting.", "tokens": [407, 341, 390, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1778104627454603, "compression_ratio": 1.545, "no_speech_prob": 4.90081038151402e-05}, {"id": 40, "seek": 15992, "start": 177.2, "end": 184.44, "text": " So yeah, in case of defaults, we were wondering, given two Kubernetes nodes with part to part", "tokens": [407, 1338, 11, 294, 1389, 295, 7576, 82, 11, 321, 645, 6359, 11, 2212, 732, 23145, 13891, 365, 644, 281, 644], "temperature": 0.0, "avg_logprob": -0.1778104627454603, "compression_ratio": 1.545, "no_speech_prob": 4.90081038151402e-05}, {"id": 41, "seek": 18444, "start": 184.44, "end": 189.96, "text": " and they're connected with 100 gigabit NIC, we wanted to look at the single flow, like", "tokens": [293, 436, 434, 4582, 365, 2319, 8741, 455, 270, 426, 2532, 11, 321, 1415, 281, 574, 412, 264, 2167, 3095, 11, 411], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 42, "seek": 18444, "start": 189.96, "end": 198.68, "text": " a single TCP stream, and we asked, like, what's the default baseline you can get to?", "tokens": [257, 2167, 48965, 4309, 11, 293, 321, 2351, 11, 411, 11, 437, 311, 264, 7576, 20518, 291, 393, 483, 281, 30], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 43, "seek": 18444, "start": 198.68, "end": 199.68, "text": " Where are bottlenecks?", "tokens": [2305, 366, 44641, 2761, 30], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 44, "seek": 18444, "start": 199.68, "end": 202.6, "text": " How can they become, how can they be overcome?", "tokens": [1012, 393, 436, 1813, 11, 577, 393, 436, 312, 10473, 30], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 45, "seek": 18444, "start": 202.6, "end": 207.4, "text": " And actually, can we provide better defaults out of that, what we figure out?", "tokens": [400, 767, 11, 393, 321, 2893, 1101, 7576, 82, 484, 295, 300, 11, 437, 321, 2573, 484, 30], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 46, "seek": 18444, "start": 207.4, "end": 210.28, "text": " Why bothering with single stream performance?", "tokens": [1545, 31432, 365, 2167, 4309, 3389, 30], "temperature": 0.0, "avg_logprob": -0.19951100252112564, "compression_ratio": 1.553191489361702, "no_speech_prob": 7.953023305162787e-05}, {"id": 47, "seek": 21028, "start": 210.28, "end": 216.36, "text": " Well, first of all, it's interesting for the kernel to be able to cope with growing", "tokens": [1042, 11, 700, 295, 439, 11, 309, 311, 1880, 337, 264, 28256, 281, 312, 1075, 281, 22598, 365, 4194], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 48, "seek": 21028, "start": 216.36, "end": 219.8, "text": " NIC speeds, so 100, 200 gigabit or more.", "tokens": [426, 2532, 16411, 11, 370, 2319, 11, 2331, 8741, 455, 270, 420, 544, 13], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 49, "seek": 21028, "start": 219.8, "end": 223.04, "text": " How can this be maxed out with the single stream?", "tokens": [1012, 393, 341, 312, 11469, 292, 484, 365, 264, 2167, 4309, 30], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 50, "seek": 21028, "start": 223.04, "end": 229.4, "text": " There are lots of data intensive workloads from user and customer sites around machine", "tokens": [821, 366, 3195, 295, 1412, 18957, 32452, 490, 4195, 293, 5474, 7533, 926, 3479], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 51, "seek": 21028, "start": 229.4, "end": 231.28, "text": " learning, AI, and so on.", "tokens": [2539, 11, 7318, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 52, "seek": 21028, "start": 231.28, "end": 237.16, "text": " But generally, it's also interesting to be able to free up the resources and give them", "tokens": [583, 5101, 11, 309, 311, 611, 1880, 281, 312, 1075, 281, 1737, 493, 264, 3593, 293, 976, 552], "temperature": 0.0, "avg_logprob": -0.1684903424195569, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00010358607687521726}, {"id": 53, "seek": 23716, "start": 237.16, "end": 242.2, "text": " to the application instead of the kernel having to block them.", "tokens": [281, 264, 3861, 2602, 295, 264, 28256, 1419, 281, 3461, 552, 13], "temperature": 0.0, "avg_logprob": -0.16690821819994822, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2988405514042825e-05}, {"id": 54, "seek": 23716, "start": 242.2, "end": 246.92, "text": " So the assumptions for our test is basically, like, usually the Kubernetes worker nodes", "tokens": [407, 264, 17695, 337, 527, 1500, 307, 1936, 11, 411, 11, 2673, 264, 23145, 11346, 13891], "temperature": 0.0, "avg_logprob": -0.16690821819994822, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2988405514042825e-05}, {"id": 55, "seek": 23716, "start": 246.92, "end": 251.16, "text": " that users run, they are quite generic, they can run any kind of workloads.", "tokens": [300, 5022, 1190, 11, 436, 366, 1596, 19577, 11, 436, 393, 1190, 604, 733, 295, 32452, 13], "temperature": 0.0, "avg_logprob": -0.16690821819994822, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2988405514042825e-05}, {"id": 56, "seek": 23716, "start": 251.16, "end": 255.12, "text": " What we are also seeing is a large number of users typically just stick to defaults,", "tokens": [708, 321, 366, 611, 2577, 307, 257, 2416, 1230, 295, 5022, 5850, 445, 2897, 281, 7576, 82, 11], "temperature": 0.0, "avg_logprob": -0.16690821819994822, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2988405514042825e-05}, {"id": 57, "seek": 23716, "start": 255.12, "end": 259.36, "text": " they don't tune specifically the kernel.", "tokens": [436, 500, 380, 10864, 4682, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.16690821819994822, "compression_ratio": 1.6296296296296295, "no_speech_prob": 5.2988405514042825e-05}, {"id": 58, "seek": 25936, "start": 259.36, "end": 267.16, "text": " There's an interesting cloud-native usage report where they tried to get some insights", "tokens": [821, 311, 364, 1880, 4588, 12, 77, 1166, 14924, 2275, 689, 436, 3031, 281, 483, 512, 14310], "temperature": 0.0, "avg_logprob": -0.19554255167643228, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.812481322209351e-05}, {"id": 59, "seek": 25936, "start": 267.16, "end": 273.36, "text": " into how Kubernetes deployments usually look like.", "tokens": [666, 577, 23145, 7274, 1117, 2673, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.19554255167643228, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.812481322209351e-05}, {"id": 60, "seek": 25936, "start": 273.36, "end": 279.0, "text": " There's definitely an increasing trend to have a higher density of containers per host.", "tokens": [821, 311, 2138, 364, 5662, 6028, 281, 362, 257, 2946, 10305, 295, 17089, 680, 3975, 13], "temperature": 0.0, "avg_logprob": -0.19554255167643228, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.812481322209351e-05}, {"id": 61, "seek": 25936, "start": 279.0, "end": 286.16, "text": " So like around 50 or more is expected these days and the number of pods per node is also", "tokens": [407, 411, 926, 2625, 420, 544, 307, 5176, 613, 1708, 293, 264, 1230, 295, 31925, 680, 9984, 307, 611], "temperature": 0.0, "avg_logprob": -0.19554255167643228, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.812481322209351e-05}, {"id": 62, "seek": 25936, "start": 286.16, "end": 288.56, "text": " increasing.", "tokens": [5662, 13], "temperature": 0.0, "avg_logprob": -0.19554255167643228, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.812481322209351e-05}, {"id": 63, "seek": 28856, "start": 288.56, "end": 295.64, "text": " So yeah, the question now is, okay, so like a basic, very basic compatibility setting,", "tokens": [407, 1338, 11, 264, 1168, 586, 307, 11, 1392, 11, 370, 411, 257, 3875, 11, 588, 3875, 34237, 3287, 11], "temperature": 0.0, "avg_logprob": -0.2043108699297664, "compression_ratio": 1.5932203389830508, "no_speech_prob": 7.582066609757021e-05}, {"id": 64, "seek": 28856, "start": 295.64, "end": 302.68, "text": " for example, for the case of Silium, we use the, like if you deploy it in a basic mode,", "tokens": [337, 1365, 11, 337, 264, 1389, 295, 6943, 2197, 11, 321, 764, 264, 11, 411, 498, 291, 7274, 309, 294, 257, 3875, 4391, 11], "temperature": 0.0, "avg_logprob": -0.2043108699297664, "compression_ratio": 1.5932203389830508, "no_speech_prob": 7.582066609757021e-05}, {"id": 65, "seek": 28856, "start": 302.68, "end": 306.68, "text": " we just use the upper stack for routing and forwarding.", "tokens": [321, 445, 764, 264, 6597, 8630, 337, 32722, 293, 2128, 278, 13], "temperature": 0.0, "avg_logprob": -0.2043108699297664, "compression_ratio": 1.5932203389830508, "no_speech_prob": 7.582066609757021e-05}, {"id": 66, "seek": 28856, "start": 306.68, "end": 310.68, "text": " There are various reasons why people might want to use that.", "tokens": [821, 366, 3683, 4112, 983, 561, 1062, 528, 281, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.2043108699297664, "compression_ratio": 1.5932203389830508, "no_speech_prob": 7.582066609757021e-05}, {"id": 67, "seek": 28856, "start": 310.68, "end": 315.44, "text": " For example, in case of Kubernetes, there's a component called Qproxy which uses net", "tokens": [1171, 1365, 11, 294, 1389, 295, 23145, 11, 456, 311, 257, 6542, 1219, 1249, 4318, 12876, 597, 4960, 2533], "temperature": 0.0, "avg_logprob": -0.2043108699297664, "compression_ratio": 1.5932203389830508, "no_speech_prob": 7.582066609757021e-05}, {"id": 68, "seek": 31544, "start": 315.44, "end": 322.71999999999997, "text": " filter IP tables for service management, for service load balancing.", "tokens": [6608, 8671, 8020, 337, 2643, 4592, 11, 337, 2643, 3677, 22495, 13], "temperature": 0.0, "avg_logprob": -0.12758732878643533, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.00013737654080614448}, {"id": 69, "seek": 31544, "start": 322.71999999999997, "end": 327.72, "text": " Some people stick to that or they have custom net filter rules, so maybe they require to", "tokens": [2188, 561, 2897, 281, 300, 420, 436, 362, 2375, 2533, 6608, 4474, 11, 370, 1310, 436, 3651, 281], "temperature": 0.0, "avg_logprob": -0.12758732878643533, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.00013737654080614448}, {"id": 70, "seek": 31544, "start": 327.72, "end": 333.72, "text": " use the upper stack for that or just simply they for now just went with defaults and might", "tokens": [764, 264, 6597, 8630, 337, 300, 420, 445, 2935, 436, 337, 586, 445, 1437, 365, 7576, 82, 293, 1062], "temperature": 0.0, "avg_logprob": -0.12758732878643533, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.00013737654080614448}, {"id": 71, "seek": 31544, "start": 333.72, "end": 337.52, "text": " look into more tuning at the later point in time.", "tokens": [574, 666, 544, 15164, 412, 264, 1780, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.12758732878643533, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.00013737654080614448}, {"id": 72, "seek": 31544, "start": 337.52, "end": 341.76, "text": " So when we try to look at the performance for that, what you can see here is on the", "tokens": [407, 562, 321, 853, 281, 574, 412, 264, 3389, 337, 300, 11, 437, 291, 393, 536, 510, 307, 322, 264], "temperature": 0.0, "avg_logprob": -0.12758732878643533, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.00013737654080614448}, {"id": 73, "seek": 34176, "start": 341.76, "end": 349.36, "text": " yellow bar is the host-to-host performance for a single stream, so we got to 44 gigabit", "tokens": [5566, 2159, 307, 264, 3975, 12, 1353, 12, 6037, 3389, 337, 257, 2167, 4309, 11, 370, 321, 658, 281, 16408, 8741, 455, 270], "temperature": 0.0, "avg_logprob": -0.18582070501227127, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.00011566206376301125}, {"id": 74, "seek": 34176, "start": 349.36, "end": 359.08, "text": " per second, but then if you do the pod-to-pod connectivity, it's really reduced dramatically.", "tokens": [680, 1150, 11, 457, 550, 498, 291, 360, 264, 2497, 12, 1353, 12, 43388, 21095, 11, 309, 311, 534, 9212, 17548, 13], "temperature": 0.0, "avg_logprob": -0.18582070501227127, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.00011566206376301125}, {"id": 75, "seek": 34176, "start": 359.08, "end": 367.08, "text": " And so yeah, like one of the reasons is, and I will get into this in a bit, is because", "tokens": [400, 370, 1338, 11, 411, 472, 295, 264, 4112, 307, 11, 293, 286, 486, 483, 666, 341, 294, 257, 857, 11, 307, 570], "temperature": 0.0, "avg_logprob": -0.18582070501227127, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.00011566206376301125}, {"id": 76, "seek": 36708, "start": 367.08, "end": 376.8, "text": " the upper stack is giving false feedback to the TCP stack.", "tokens": [264, 6597, 8630, 307, 2902, 7908, 5824, 281, 264, 48965, 8630, 13], "temperature": 0.0, "avg_logprob": -0.1662038318694584, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.505211523384787e-05}, {"id": 77, "seek": 36708, "start": 376.8, "end": 384.03999999999996, "text": " One thing that we did like a year ago or so is to introduce a feature where we can, which", "tokens": [1485, 551, 300, 321, 630, 411, 257, 1064, 2057, 420, 370, 307, 281, 5366, 257, 4111, 689, 321, 393, 11, 597], "temperature": 0.0, "avg_logprob": -0.1662038318694584, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.505211523384787e-05}, {"id": 78, "seek": 36708, "start": 384.03999999999996, "end": 392.0, "text": " is called BPF host routing, where we don't use the upper stack and for BPF itself, given", "tokens": [307, 1219, 40533, 37, 3975, 32722, 11, 689, 321, 500, 380, 764, 264, 6597, 8630, 293, 337, 40533, 37, 2564, 11, 2212], "temperature": 0.0, "avg_logprob": -0.1662038318694584, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.505211523384787e-05}, {"id": 79, "seek": 39200, "start": 392.0, "end": 397.18, "text": " we attach to physical devices, but also to Veef devices, we added a couple of new helper", "tokens": [321, 5085, 281, 4001, 5759, 11, 457, 611, 281, 691, 1653, 69, 5759, 11, 321, 3869, 257, 1916, 295, 777, 36133], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 80, "seek": 39200, "start": 397.18, "end": 398.56, "text": " functions there.", "tokens": [6828, 456, 13], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 81, "seek": 39200, "start": 398.56, "end": 401.44, "text": " One is called BPF redirect peer.", "tokens": [1485, 307, 1219, 40533, 37, 29066, 15108, 13], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 82, "seek": 39200, "start": 401.44, "end": 407.36, "text": " What this basically is doing is adding a fast switch into the network namespace for the", "tokens": [708, 341, 1936, 307, 884, 307, 5127, 257, 2370, 3679, 666, 264, 3209, 5288, 17940, 337, 264], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 83, "seek": 39200, "start": 407.36, "end": 408.36, "text": " ingress traffic.", "tokens": [3957, 735, 6419, 13], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 84, "seek": 39200, "start": 408.36, "end": 415.44, "text": " So basically we're just like, instead of going the usual X-MID route to the Veef devices,", "tokens": [407, 1936, 321, 434, 445, 411, 11, 2602, 295, 516, 264, 7713, 1783, 12, 44, 2777, 7955, 281, 264, 691, 1653, 69, 5759, 11], "temperature": 0.0, "avg_logprob": -0.2145636003096025, "compression_ratio": 1.5205479452054795, "no_speech_prob": 3.118156746495515e-05}, {"id": 85, "seek": 41544, "start": 415.44, "end": 424.28, "text": " we can retrieve the Veef device inside the pod and just scrub the packet to remove the", "tokens": [321, 393, 30254, 264, 691, 1653, 69, 4302, 1854, 264, 2497, 293, 445, 24163, 264, 20300, 281, 4159, 264], "temperature": 0.0, "avg_logprob": -0.17867623916780107, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.695092214504257e-05}, {"id": 86, "seek": 41544, "start": 424.28, "end": 428.48, "text": " necessary data that we typically remove in switching network namespaces, but then also", "tokens": [4818, 1412, 300, 321, 5850, 4159, 294, 16493, 3209, 5288, 79, 2116, 11, 457, 550, 611], "temperature": 0.0, "avg_logprob": -0.17867623916780107, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.695092214504257e-05}, {"id": 87, "seek": 41544, "start": 428.48, "end": 432.88, "text": " to just set the device to the device inside the pod.", "tokens": [281, 445, 992, 264, 4302, 281, 264, 4302, 1854, 264, 2497, 13], "temperature": 0.0, "avg_logprob": -0.17867623916780107, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.695092214504257e-05}, {"id": 88, "seek": 41544, "start": 432.88, "end": 438.52, "text": " And then circle that around in the main receive loop without going to a per-CPU backlog queue", "tokens": [400, 550, 6329, 300, 926, 294, 264, 2135, 4774, 6367, 1553, 516, 281, 257, 680, 12, 34, 8115, 47364, 18639], "temperature": 0.0, "avg_logprob": -0.17867623916780107, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.695092214504257e-05}, {"id": 89, "seek": 41544, "start": 438.52, "end": 444.64, "text": " that you would normally do when you transfer data through Veef devices, and we don't need", "tokens": [300, 291, 576, 5646, 360, 562, 291, 5003, 1412, 807, 691, 1653, 69, 5759, 11, 293, 321, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.17867623916780107, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.695092214504257e-05}, {"id": 90, "seek": 44464, "start": 444.64, "end": 452.84, "text": " to use the upper stack because there's all the information already available in BPF context.", "tokens": [281, 764, 264, 6597, 8630, 570, 456, 311, 439, 264, 1589, 1217, 2435, 294, 40533, 37, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1263170778081658, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.282121204189025e-05}, {"id": 91, "seek": 44464, "start": 452.84, "end": 458.15999999999997, "text": " And for the way out, we added a helper which is called BPF redirect neighbor.", "tokens": [400, 337, 264, 636, 484, 11, 321, 3869, 257, 36133, 597, 307, 1219, 40533, 37, 29066, 5987, 13], "temperature": 0.0, "avg_logprob": -0.1263170778081658, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.282121204189025e-05}, {"id": 92, "seek": 44464, "start": 458.15999999999997, "end": 467.76, "text": " So that one will basically insert the packet into the neighboring subsystem of the kernel.", "tokens": [407, 300, 472, 486, 1936, 8969, 264, 20300, 666, 264, 31521, 2090, 9321, 295, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1263170778081658, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.282121204189025e-05}, {"id": 93, "seek": 44464, "start": 467.76, "end": 472.24, "text": " So usually we can do a FIP lookup out of BPF, so there's a helper for this as well, and", "tokens": [407, 2673, 321, 393, 360, 257, 479, 9139, 574, 1010, 484, 295, 40533, 37, 11, 370, 456, 311, 257, 36133, 337, 341, 382, 731, 11, 293], "temperature": 0.0, "avg_logprob": -0.1263170778081658, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.282121204189025e-05}, {"id": 94, "seek": 47224, "start": 472.24, "end": 477.72, "text": " then combined with this for the resolution of neighbors.", "tokens": [550, 9354, 365, 341, 337, 264, 8669, 295, 12512, 13], "temperature": 0.0, "avg_logprob": -0.12442956800046175, "compression_ratio": 1.7168949771689497, "no_speech_prob": 6.394118099706247e-05}, {"id": 95, "seek": 47224, "start": 477.72, "end": 483.56, "text": " It will allow that you don't need to go to the upper stack, so the nice benefit you get", "tokens": [467, 486, 2089, 300, 291, 500, 380, 643, 281, 352, 281, 264, 6597, 8630, 11, 370, 264, 1481, 5121, 291, 483], "temperature": 0.0, "avg_logprob": -0.12442956800046175, "compression_ratio": 1.7168949771689497, "no_speech_prob": 6.394118099706247e-05}, {"id": 96, "seek": 47224, "start": 483.56, "end": 490.8, "text": " as well with this is that the socket context for the network packet for the SKB is retained", "tokens": [382, 731, 365, 341, 307, 300, 264, 19741, 4319, 337, 264, 3209, 20300, 337, 264, 21483, 33, 307, 33438], "temperature": 0.0, "avg_logprob": -0.12442956800046175, "compression_ratio": 1.7168949771689497, "no_speech_prob": 6.394118099706247e-05}, {"id": 97, "seek": 47224, "start": 490.8, "end": 495.92, "text": " all the way to the physical device until the packet is actually sent out.", "tokens": [439, 264, 636, 281, 264, 4001, 4302, 1826, 264, 20300, 307, 767, 2279, 484, 13], "temperature": 0.0, "avg_logprob": -0.12442956800046175, "compression_ratio": 1.7168949771689497, "no_speech_prob": 6.394118099706247e-05}, {"id": 98, "seek": 47224, "start": 495.92, "end": 499.56, "text": " And this is not the case when you normally go to the upper stack.", "tokens": [400, 341, 307, 406, 264, 1389, 562, 291, 5646, 352, 281, 264, 6597, 8630, 13], "temperature": 0.0, "avg_logprob": -0.12442956800046175, "compression_ratio": 1.7168949771689497, "no_speech_prob": 6.394118099706247e-05}, {"id": 99, "seek": 49956, "start": 499.56, "end": 506.2, "text": " So then the TCP stack actually thinks that once you go to the upper stack that it already", "tokens": [407, 550, 264, 48965, 8630, 767, 7309, 300, 1564, 291, 352, 281, 264, 6597, 8630, 300, 309, 1217], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 100, "seek": 49956, "start": 506.2, "end": 510.72, "text": " left the node, but it's actually not the case.", "tokens": [1411, 264, 9984, 11, 457, 309, 311, 767, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 101, "seek": 49956, "start": 510.72, "end": 514.08, "text": " And this way it can be retained.", "tokens": [400, 341, 636, 309, 393, 312, 33438, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 102, "seek": 49956, "start": 514.08, "end": 517.6, "text": " And this is how the complete picture looks like.", "tokens": [400, 341, 307, 577, 264, 3566, 3036, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 103, "seek": 49956, "start": 517.6, "end": 520.96, "text": " And if you look at the performance, it's already much better.", "tokens": [400, 498, 291, 574, 412, 264, 3389, 11, 309, 311, 1217, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 104, "seek": 49956, "start": 520.96, "end": 526.6, "text": " So we were able to get almost a 40 gigabit per second under 1.5K MTU.", "tokens": [407, 321, 645, 1075, 281, 483, 1920, 257, 3356, 8741, 455, 270, 680, 1150, 833, 502, 13, 20, 42, 37333, 52, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 105, "seek": 49956, "start": 526.6, "end": 528.84, "text": " So this was interesting.", "tokens": [407, 341, 390, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12837335495721727, "compression_ratio": 1.6163793103448276, "no_speech_prob": 6.007276533637196e-05}, {"id": 106, "seek": 52884, "start": 528.84, "end": 535.64, "text": " Now the question is, how could we close the remaining gap?", "tokens": [823, 264, 1168, 307, 11, 577, 727, 321, 1998, 264, 8877, 7417, 30], "temperature": 0.0, "avg_logprob": -0.1789996925441698, "compression_ratio": 1.4904761904761905, "no_speech_prob": 5.64250694878865e-05}, {"id": 107, "seek": 52884, "start": 535.64, "end": 543.32, "text": " Under 8K MTU, we also did some tests, and one thing to note here is that we were able", "tokens": [6974, 1649, 42, 37333, 52, 11, 321, 611, 630, 512, 6921, 11, 293, 472, 551, 281, 3637, 510, 307, 300, 321, 645, 1075], "temperature": 0.0, "avg_logprob": -0.1789996925441698, "compression_ratio": 1.4904761904761905, "no_speech_prob": 5.64250694878865e-05}, {"id": 108, "seek": 52884, "start": 543.32, "end": 549.44, "text": " for a single TCP stream to get to 98 gigabit per second for the host-to-host case, but", "tokens": [337, 257, 2167, 48965, 4309, 281, 483, 281, 20860, 8741, 455, 270, 680, 1150, 337, 264, 3975, 12, 1353, 12, 6037, 1389, 11, 457], "temperature": 0.0, "avg_logprob": -0.1789996925441698, "compression_ratio": 1.4904761904761905, "no_speech_prob": 5.64250694878865e-05}, {"id": 109, "seek": 52884, "start": 549.44, "end": 554.5600000000001, "text": " still the situation looks quite the same for the weave with the BPF host routing.", "tokens": [920, 264, 2590, 1542, 1596, 264, 912, 337, 264, 29145, 365, 264, 40533, 37, 3975, 32722, 13], "temperature": 0.0, "avg_logprob": -0.1789996925441698, "compression_ratio": 1.4904761904761905, "no_speech_prob": 5.64250694878865e-05}, {"id": 110, "seek": 55456, "start": 554.56, "end": 558.92, "text": " So there's still a small gap that we want to close here as well.", "tokens": [407, 456, 311, 920, 257, 1359, 7417, 300, 321, 528, 281, 1998, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16649609107475777, "compression_ratio": 1.5026178010471205, "no_speech_prob": 4.606427319231443e-05}, {"id": 111, "seek": 55456, "start": 558.92, "end": 566.52, "text": " And that's where we introduce a new device type as a weave replacement.", "tokens": [400, 300, 311, 689, 321, 5366, 257, 777, 4302, 2010, 382, 257, 29145, 14419, 13], "temperature": 0.0, "avg_logprob": -0.16649609107475777, "compression_ratio": 1.5026178010471205, "no_speech_prob": 4.606427319231443e-05}, {"id": 112, "seek": 55456, "start": 566.52, "end": 574.3199999999999, "text": " So we call this meta device because it's programmable to BPF, and you can implement various, like", "tokens": [407, 321, 818, 341, 19616, 4302, 570, 309, 311, 37648, 712, 281, 40533, 37, 11, 293, 291, 393, 4445, 3683, 11, 411], "temperature": 0.0, "avg_logprob": -0.16649609107475777, "compression_ratio": 1.5026178010471205, "no_speech_prob": 4.606427319231443e-05}, {"id": 113, "seek": 55456, "start": 574.3199999999999, "end": 576.9599999999999, "text": " your own business logic into this.", "tokens": [428, 1065, 1606, 9952, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.16649609107475777, "compression_ratio": 1.5026178010471205, "no_speech_prob": 4.606427319231443e-05}, {"id": 114, "seek": 55456, "start": 576.9599999999999, "end": 578.3599999999999, "text": " So it's flexible.", "tokens": [407, 309, 311, 11358, 13], "temperature": 0.0, "avg_logprob": -0.16649609107475777, "compression_ratio": 1.5026178010471205, "no_speech_prob": 4.606427319231443e-05}, {"id": 115, "seek": 57836, "start": 578.36, "end": 585.04, "text": " And this time, the main difference is that this also gets a faster switch on the egress", "tokens": [400, 341, 565, 11, 264, 2135, 2649, 307, 300, 341, 611, 2170, 257, 4663, 3679, 322, 264, 308, 3091], "temperature": 0.0, "avg_logprob": -0.21087491381299364, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.0225288355723023e-05}, {"id": 116, "seek": 57836, "start": 585.04, "end": 590.6, "text": " side for the egress traffic, so it doesn't need to go to the per-CPU or back-lock-U for", "tokens": [1252, 337, 264, 308, 3091, 6419, 11, 370, 309, 1177, 380, 643, 281, 352, 281, 264, 680, 12, 34, 8115, 420, 646, 12, 4102, 12, 52, 337], "temperature": 0.0, "avg_logprob": -0.21087491381299364, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.0225288355723023e-05}, {"id": 117, "seek": 57836, "start": 590.6, "end": 593.88, "text": " the egress as well.", "tokens": [264, 308, 3091, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21087491381299364, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.0225288355723023e-05}, {"id": 118, "seek": 57836, "start": 593.88, "end": 599.96, "text": " So if you look at the flame graphs, so that's the worst-case scenario where we compared", "tokens": [407, 498, 291, 574, 412, 264, 13287, 24877, 11, 370, 300, 311, 264, 5855, 12, 9765, 9005, 689, 321, 5347], "temperature": 0.0, "avg_logprob": -0.21087491381299364, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.0225288355723023e-05}, {"id": 119, "seek": 57836, "start": 599.96, "end": 602.32, "text": " the weave and the meta device.", "tokens": [264, 29145, 293, 264, 19616, 4302, 13], "temperature": 0.0, "avg_logprob": -0.21087491381299364, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.0225288355723023e-05}, {"id": 120, "seek": 60232, "start": 602.32, "end": 609.1600000000001, "text": " So what you can see here on the weave device, like on X-MID, it will basically scrub the", "tokens": [407, 437, 291, 393, 536, 510, 322, 264, 29145, 4302, 11, 411, 322, 1783, 12, 44, 2777, 11, 309, 486, 1936, 24163, 264], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 121, "seek": 60232, "start": 609.1600000000001, "end": 615.36, "text": " packet data, it will un-CUE the packet to a per-CPU back-lock-U, and then at some point", "tokens": [20300, 1412, 11, 309, 486, 517, 12, 34, 16309, 264, 20300, 281, 257, 680, 12, 34, 8115, 646, 12, 4102, 12, 52, 11, 293, 550, 412, 512, 935], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 122, "seek": 60232, "start": 615.36, "end": 616.5600000000001, "text": " there's a network's action.", "tokens": [456, 311, 257, 3209, 311, 3069, 13], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 123, "seek": 60232, "start": 616.5600000000001, "end": 622.12, "text": " It will pick up the packets from the queue again, and in the worst case, it can be deferred", "tokens": [467, 486, 1888, 493, 264, 30364, 490, 264, 18639, 797, 11, 293, 294, 264, 5855, 1389, 11, 309, 393, 312, 25704, 986], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 124, "seek": 60232, "start": 622.12, "end": 627.1600000000001, "text": " to the kernel software QDemon, and then you see this rescheduling where you have a new", "tokens": [281, 264, 28256, 4722, 1249, 35, 36228, 11, 293, 550, 291, 536, 341, 725, 19318, 425, 278, 689, 291, 362, 257, 777], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 125, "seek": 60232, "start": 627.1600000000001, "end": 631.84, "text": " stack where this is processed again.", "tokens": [8630, 689, 341, 307, 18846, 797, 13], "temperature": 0.0, "avg_logprob": -0.21789566288149453, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.142041194019839e-05}, {"id": 126, "seek": 63184, "start": 631.84, "end": 639.96, "text": " And then from the BPF side, it will reach BPF on the TC egress on the host weave, where", "tokens": [400, 550, 490, 264, 40533, 37, 1252, 11, 309, 486, 2524, 40533, 37, 322, 264, 34150, 308, 3091, 322, 264, 3975, 29145, 11, 689], "temperature": 0.0, "avg_logprob": -0.10974302831685769, "compression_ratio": 1.6991525423728813, "no_speech_prob": 3.940394890378229e-05}, {"id": 127, "seek": 63184, "start": 639.96, "end": 645.96, "text": " we then can only forward this to the physical device to leave the node, right?", "tokens": [321, 550, 393, 787, 2128, 341, 281, 264, 4001, 4302, 281, 1856, 264, 9984, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10974302831685769, "compression_ratio": 1.6991525423728813, "no_speech_prob": 3.940394890378229e-05}, {"id": 128, "seek": 63184, "start": 645.96, "end": 652.6, "text": " And all of this can be done in one go without rescheduling through this meta device.", "tokens": [400, 439, 295, 341, 393, 312, 1096, 294, 472, 352, 1553, 725, 19318, 425, 278, 807, 341, 19616, 4302, 13], "temperature": 0.0, "avg_logprob": -0.10974302831685769, "compression_ratio": 1.6991525423728813, "no_speech_prob": 3.940394890378229e-05}, {"id": 129, "seek": 63184, "start": 652.6, "end": 655.96, "text": " So it will scrub the packet, it will switch the network namespace, it will reset the device", "tokens": [407, 309, 486, 24163, 264, 20300, 11, 309, 486, 3679, 264, 3209, 5288, 17940, 11, 309, 486, 14322, 264, 4302], "temperature": 0.0, "avg_logprob": -0.10974302831685769, "compression_ratio": 1.6991525423728813, "no_speech_prob": 3.940394890378229e-05}, {"id": 130, "seek": 63184, "start": 655.96, "end": 661.36, "text": " pointers, and it will then directly call the BPF program.", "tokens": [44548, 11, 293, 309, 486, 550, 3838, 818, 264, 40533, 37, 1461, 13], "temperature": 0.0, "avg_logprob": -0.10974302831685769, "compression_ratio": 1.6991525423728813, "no_speech_prob": 3.940394890378229e-05}, {"id": 131, "seek": 66136, "start": 661.36, "end": 667.4, "text": " And if the BPF program says that based on the FIP lookup and so on, that it will forward", "tokens": [400, 498, 264, 40533, 37, 1461, 1619, 300, 2361, 322, 264, 479, 9139, 574, 1010, 293, 370, 322, 11, 300, 309, 486, 2128], "temperature": 0.0, "avg_logprob": -0.14786836533319384, "compression_ratio": 1.591093117408907, "no_speech_prob": 1.7756539818947203e-05}, {"id": 132, "seek": 66136, "start": 667.4, "end": 676.64, "text": " the packet directly to the physical device, then it will avoid this rescheduling scenario.", "tokens": [264, 20300, 3838, 281, 264, 4001, 4302, 11, 550, 309, 486, 5042, 341, 725, 19318, 425, 278, 9005, 13], "temperature": 0.0, "avg_logprob": -0.14786836533319384, "compression_ratio": 1.591093117408907, "no_speech_prob": 1.7756539818947203e-05}, {"id": 133, "seek": 66136, "start": 676.64, "end": 681.08, "text": " So on the right side, I mean, it's really straightforward.", "tokens": [407, 322, 264, 558, 1252, 11, 286, 914, 11, 309, 311, 534, 15325, 13], "temperature": 0.0, "avg_logprob": -0.14786836533319384, "compression_ratio": 1.591093117408907, "no_speech_prob": 1.7756539818947203e-05}, {"id": 134, "seek": 66136, "start": 681.08, "end": 684.88, "text": " That's how the implementation of the driver X-MID routine looks like.", "tokens": [663, 311, 577, 264, 11420, 295, 264, 6787, 1783, 12, 44, 2777, 9927, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.14786836533319384, "compression_ratio": 1.591093117408907, "no_speech_prob": 1.7756539818947203e-05}, {"id": 135, "seek": 66136, "start": 684.88, "end": 690.9200000000001, "text": " So it will basically just call into BPF, and then based on the verdict, push it out.", "tokens": [407, 309, 486, 1936, 445, 818, 666, 40533, 37, 11, 293, 550, 2361, 322, 264, 33957, 11, 2944, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.14786836533319384, "compression_ratio": 1.591093117408907, "no_speech_prob": 1.7756539818947203e-05}, {"id": 136, "seek": 69092, "start": 690.92, "end": 693.9599999999999, "text": " It's really just like 500 lines of code.", "tokens": [467, 311, 534, 445, 411, 5923, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 137, "seek": 69092, "start": 693.9599999999999, "end": 696.0799999999999, "text": " So it's very simple and straightforward.", "tokens": [407, 309, 311, 588, 2199, 293, 15325, 13], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 138, "seek": 69092, "start": 696.0799999999999, "end": 701.4399999999999, "text": " I think it's just one-fifth of the weave driver that we have right now.", "tokens": [286, 519, 309, 311, 445, 472, 12, 69, 351, 392, 295, 264, 29145, 6787, 300, 321, 362, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 139, "seek": 69092, "start": 701.4399999999999, "end": 705.8399999999999, "text": " And the other focus that we wanted to put into is compatibility as well, so that, like", "tokens": [400, 264, 661, 1879, 300, 321, 1415, 281, 829, 666, 307, 34237, 382, 731, 11, 370, 300, 11, 411], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 140, "seek": 69092, "start": 705.8399999999999, "end": 714.88, "text": " given in Solium, we need to support multiple kernels, the ideal case would be that we don't", "tokens": [2212, 294, 7026, 2197, 11, 321, 643, 281, 1406, 3866, 23434, 1625, 11, 264, 7157, 1389, 576, 312, 300, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 141, "seek": 69092, "start": 714.88, "end": 719.7199999999999, "text": " need to change much of the BPF program and can keep it as is.", "tokens": [643, 281, 1319, 709, 295, 264, 40533, 37, 1461, 293, 393, 1066, 309, 382, 307, 13], "temperature": 0.0, "avg_logprob": -0.1937794116658902, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.00010372603719588369}, {"id": 142, "seek": 71972, "start": 719.72, "end": 727.28, "text": " And in case of XTP, we didn't want to implement it because for the weave case, it really is", "tokens": [400, 294, 1389, 295, 1783, 16804, 11, 321, 994, 380, 528, 281, 4445, 309, 570, 337, 264, 29145, 1389, 11, 309, 534, 307], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 143, "seek": 71972, "start": 727.28, "end": 728.28, "text": " very complex.", "tokens": [588, 3997, 13], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 144, "seek": 71972, "start": 728.28, "end": 734.28, "text": " It even adds multi-q support, which you normally would not need on a virtual device.", "tokens": [467, 754, 10860, 4825, 12, 80, 1406, 11, 597, 291, 5646, 576, 406, 643, 322, 257, 6374, 4302, 13], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 145, "seek": 71972, "start": 734.28, "end": 739.44, "text": " So we wanted to keep it as simple as possible and to have the flexibility that this can", "tokens": [407, 321, 1415, 281, 1066, 309, 382, 2199, 382, 1944, 293, 281, 362, 264, 12635, 300, 341, 393], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 146, "seek": 71972, "start": 739.44, "end": 742.08, "text": " be added as a single or a paired device.", "tokens": [312, 3869, 382, 257, 2167, 420, 257, 25699, 4302, 13], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 147, "seek": 71972, "start": 742.08, "end": 745.88, "text": " So for the weave replacement, it would be a paired device, but you could also do it", "tokens": [407, 337, 264, 29145, 14419, 11, 309, 576, 312, 257, 25699, 4302, 11, 457, 291, 727, 611, 360, 309], "temperature": 0.0, "avg_logprob": -0.1742868243523364, "compression_ratio": 1.6584362139917694, "no_speech_prob": 3.166749229421839e-05}, {"id": 148, "seek": 74588, "start": 745.88, "end": 754.68, "text": " as a single device and then implement whatever logic you would want in BPF for that.", "tokens": [382, 257, 2167, 4302, 293, 550, 4445, 2035, 9952, 291, 576, 528, 294, 40533, 37, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.15441305245926132, "compression_ratio": 1.3915343915343916, "no_speech_prob": 5.219040758674964e-05}, {"id": 149, "seek": 74588, "start": 754.68, "end": 761.16, "text": " So looking at the performance, again, like the TCP stream under 8K, so this is really", "tokens": [407, 1237, 412, 264, 3389, 11, 797, 11, 411, 264, 48965, 4309, 833, 1649, 42, 11, 370, 341, 307, 534], "temperature": 0.0, "avg_logprob": -0.15441305245926132, "compression_ratio": 1.3915343915343916, "no_speech_prob": 5.219040758674964e-05}, {"id": 150, "seek": 74588, "start": 761.16, "end": 770.6, "text": " able to reach through this approach the full 98 gigabit per second, and in terms of latency,", "tokens": [1075, 281, 2524, 807, 341, 3109, 264, 1577, 20860, 8741, 455, 270, 680, 1150, 11, 293, 294, 2115, 295, 27043, 11], "temperature": 0.0, "avg_logprob": -0.15441305245926132, "compression_ratio": 1.3915343915343916, "no_speech_prob": 5.219040758674964e-05}, {"id": 151, "seek": 77060, "start": 770.6, "end": 776.4, "text": " so we did some net-perf TCP or R measurements as well, where you get the minimum, the P90,", "tokens": [370, 321, 630, 512, 2533, 12, 610, 69, 48965, 420, 497, 15383, 382, 731, 11, 689, 291, 483, 264, 7285, 11, 264, 430, 7771, 11], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 152, "seek": 77060, "start": 776.4, "end": 778.24, "text": " 99 latency and so on.", "tokens": [11803, 27043, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 153, "seek": 77060, "start": 778.24, "end": 784.6, "text": " So this is really on par with the host.", "tokens": [407, 341, 307, 534, 322, 971, 365, 264, 3975, 13], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 154, "seek": 77060, "start": 784.6, "end": 787.64, "text": " So now we were asking ourselves, so can we push this even further?", "tokens": [407, 586, 321, 645, 3365, 4175, 11, 370, 393, 321, 2944, 341, 754, 3052, 30], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 155, "seek": 77060, "start": 787.64, "end": 794.32, "text": " I mean, well, so we were able to get to 98 gigabit per second, but like the cost for", "tokens": [286, 914, 11, 731, 11, 370, 321, 645, 1075, 281, 483, 281, 20860, 8741, 455, 270, 680, 1150, 11, 457, 411, 264, 2063, 337], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 156, "seek": 77060, "start": 794.32, "end": 799.9200000000001, "text": " like a megabyte to transfer, can this pushed even more?", "tokens": [411, 257, 10816, 34529, 281, 5003, 11, 393, 341, 9152, 754, 544, 30], "temperature": 0.0, "avg_logprob": -0.22241943573283257, "compression_ratio": 1.5450643776824033, "no_speech_prob": 3.370274498593062e-05}, {"id": 157, "seek": 79992, "start": 799.92, "end": 804.9599999999999, "text": " And there's a relatively recent kernel feature which is called Big TCP.", "tokens": [400, 456, 311, 257, 7226, 5162, 28256, 4111, 597, 307, 1219, 5429, 48965, 13], "temperature": 0.0, "avg_logprob": -0.18212371826171875, "compression_ratio": 1.5461538461538462, "no_speech_prob": 5.301292549120262e-05}, {"id": 158, "seek": 79992, "start": 804.9599999999999, "end": 812.8, "text": " It landed for IPv6 only in 5.19 and was developed by Google, and the whole idea behind Big", "tokens": [467, 15336, 337, 8671, 85, 21, 787, 294, 1025, 13, 3405, 293, 390, 4743, 538, 3329, 11, 293, 264, 1379, 1558, 2261, 5429], "temperature": 0.0, "avg_logprob": -0.18212371826171875, "compression_ratio": 1.5461538461538462, "no_speech_prob": 5.301292549120262e-05}, {"id": 159, "seek": 79992, "start": 812.8, "end": 819.16, "text": " TCP is to even more aggressively aggregate for GEO and GSO.", "tokens": [48965, 307, 281, 754, 544, 32024, 26118, 337, 460, 6004, 293, 460, 17188, 13], "temperature": 0.0, "avg_logprob": -0.18212371826171875, "compression_ratio": 1.5461538461538462, "no_speech_prob": 5.301292549120262e-05}, {"id": 160, "seek": 79992, "start": 819.16, "end": 825.0, "text": " So normally the aggregation, the kernel will try it basically out of the incoming packet", "tokens": [407, 5646, 264, 16743, 399, 11, 264, 28256, 486, 853, 309, 1936, 484, 295, 264, 22341, 20300], "temperature": 0.0, "avg_logprob": -0.18212371826171875, "compression_ratio": 1.5461538461538462, "no_speech_prob": 5.301292549120262e-05}, {"id": 161, "seek": 79992, "start": 825.0, "end": 829.16, "text": " stream, create a super packet and then we'll push it up to the networking stack so it only", "tokens": [4309, 11, 1884, 257, 1687, 20300, 293, 550, 321, 603, 2944, 309, 493, 281, 264, 17985, 8630, 370, 309, 787], "temperature": 0.0, "avg_logprob": -0.18212371826171875, "compression_ratio": 1.5461538461538462, "no_speech_prob": 5.301292549120262e-05}, {"id": 162, "seek": 82916, "start": 829.16, "end": 831.52, "text": " needs to be traversed once.", "tokens": [2203, 281, 312, 23149, 292, 1564, 13], "temperature": 0.0, "avg_logprob": -0.14752413562892638, "compression_ratio": 1.5175438596491229, "no_speech_prob": 6.703448889311403e-05}, {"id": 163, "seek": 82916, "start": 831.52, "end": 837.92, "text": " And the limit up until that point was for 64K packets, simply because in the IP header", "tokens": [400, 264, 4948, 493, 1826, 300, 935, 390, 337, 12145, 42, 30364, 11, 2935, 570, 294, 264, 8671, 23117], "temperature": 0.0, "avg_logprob": -0.14752413562892638, "compression_ratio": 1.5175438596491229, "no_speech_prob": 6.703448889311403e-05}, {"id": 164, "seek": 82916, "start": 837.92, "end": 842.0, "text": " that's the maximum packet size that you can do.", "tokens": [300, 311, 264, 6674, 20300, 2744, 300, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.14752413562892638, "compression_ratio": 1.5175438596491229, "no_speech_prob": 6.703448889311403e-05}, {"id": 165, "seek": 82916, "start": 842.0, "end": 850.04, "text": " And the idea for Big TCP for IPv6 was that, well, maybe we could create a hop-by-hop header", "tokens": [400, 264, 1558, 337, 5429, 48965, 337, 8671, 85, 21, 390, 300, 11, 731, 11, 1310, 321, 727, 1884, 257, 3818, 12, 2322, 12, 9050, 23117], "temperature": 0.0, "avg_logprob": -0.14752413562892638, "compression_ratio": 1.5175438596491229, "no_speech_prob": 6.703448889311403e-05}, {"id": 166, "seek": 82916, "start": 850.04, "end": 856.56, "text": " in the GEO layer and then add, and then like the 16-bit packet length field can be overcome", "tokens": [294, 264, 460, 6004, 4583, 293, 550, 909, 11, 293, 550, 411, 264, 3165, 12, 5260, 20300, 4641, 2519, 393, 312, 10473], "temperature": 0.0, "avg_logprob": -0.14752413562892638, "compression_ratio": 1.5175438596491229, "no_speech_prob": 6.703448889311403e-05}, {"id": 167, "seek": 85656, "start": 856.56, "end": 863.76, "text": " because there's a jumbo-gram extension in there which allows for a 32-bit field.", "tokens": [570, 456, 311, 257, 29067, 1763, 12, 1342, 10320, 294, 456, 597, 4045, 337, 257, 8858, 12, 5260, 2519, 13], "temperature": 0.0, "avg_logprob": -0.20599889755249023, "compression_ratio": 1.5112107623318385, "no_speech_prob": 3.7619211070705205e-05}, {"id": 168, "seek": 85656, "start": 863.76, "end": 868.4399999999999, "text": " So you can do much more aggressive aggregation.", "tokens": [407, 291, 393, 360, 709, 544, 10762, 16743, 399, 13], "temperature": 0.0, "avg_logprob": -0.20599889755249023, "compression_ratio": 1.5112107623318385, "no_speech_prob": 3.7619211070705205e-05}, {"id": 169, "seek": 85656, "start": 868.4399999999999, "end": 875.14, "text": " And yeah, so this is also now supported with the new studio release where this will be", "tokens": [400, 1338, 11, 370, 341, 307, 611, 586, 8104, 365, 264, 777, 6811, 4374, 689, 341, 486, 312], "temperature": 0.0, "avg_logprob": -0.20599889755249023, "compression_ratio": 1.5112107623318385, "no_speech_prob": 3.7619211070705205e-05}, {"id": 170, "seek": 85656, "start": 875.14, "end": 880.1999999999999, "text": " set up for all the devices underneath automatically for IPv6.", "tokens": [992, 493, 337, 439, 264, 5759, 7223, 6772, 337, 8671, 85, 21, 13], "temperature": 0.0, "avg_logprob": -0.20599889755249023, "compression_ratio": 1.5112107623318385, "no_speech_prob": 3.7619211070705205e-05}, {"id": 171, "seek": 85656, "start": 880.1999999999999, "end": 884.56, "text": " Actually like this week, that was also merged for IPv4 now.", "tokens": [5135, 411, 341, 1243, 11, 300, 390, 611, 36427, 337, 8671, 85, 19, 586, 13], "temperature": 0.0, "avg_logprob": -0.20599889755249023, "compression_ratio": 1.5112107623318385, "no_speech_prob": 3.7619211070705205e-05}, {"id": 172, "seek": 88456, "start": 884.56, "end": 890.2399999999999, "text": " So this will end in kernel 6.3, which is exciting.", "tokens": [407, 341, 486, 917, 294, 28256, 1386, 13, 18, 11, 597, 307, 4670, 13], "temperature": 0.0, "avg_logprob": -0.16068407007165858, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.385135591495782e-05}, {"id": 173, "seek": 88456, "start": 890.2399999999999, "end": 898.16, "text": " And when we looked at the performance again under Big TCP, turns out like using the upper", "tokens": [400, 562, 321, 2956, 412, 264, 3389, 797, 833, 5429, 48965, 11, 4523, 484, 411, 1228, 264, 6597], "temperature": 0.0, "avg_logprob": -0.16068407007165858, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.385135591495782e-05}, {"id": 174, "seek": 88456, "start": 898.16, "end": 901.9599999999999, "text": " stack is currently broken in the kernel, so that still needs to be fixed.", "tokens": [8630, 307, 4362, 5463, 294, 264, 28256, 11, 370, 300, 920, 2203, 281, 312, 6806, 13], "temperature": 0.0, "avg_logprob": -0.16068407007165858, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.385135591495782e-05}, {"id": 175, "seek": 88456, "start": 901.9599999999999, "end": 904.8, "text": " We will look into that.", "tokens": [492, 486, 574, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.16068407007165858, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.385135591495782e-05}, {"id": 176, "seek": 88456, "start": 904.8, "end": 907.88, "text": " So forwarding there wouldn't work.", "tokens": [407, 2128, 278, 456, 2759, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.16068407007165858, "compression_ratio": 1.436842105263158, "no_speech_prob": 5.385135591495782e-05}, {"id": 177, "seek": 90788, "start": 907.88, "end": 916.68, "text": " And with the host routing cases, it will basically bump up the regular Veef one to get this", "tokens": [400, 365, 264, 3975, 32722, 3331, 11, 309, 486, 1936, 9961, 493, 264, 3890, 691, 1653, 69, 472, 281, 483, 341], "temperature": 0.0, "avg_logprob": -0.18956954777240753, "compression_ratio": 1.5654761904761905, "no_speech_prob": 5.053749555372633e-05}, {"id": 178, "seek": 90788, "start": 916.68, "end": 925.4, "text": " on par with the meta and also the host, so it will basically hide those glitches.", "tokens": [322, 971, 365, 264, 19616, 293, 611, 264, 3975, 11, 370, 309, 486, 1936, 6479, 729, 23552, 279, 13], "temperature": 0.0, "avg_logprob": -0.18956954777240753, "compression_ratio": 1.5654761904761905, "no_speech_prob": 5.053749555372633e-05}, {"id": 179, "seek": 90788, "start": 925.4, "end": 937.4, "text": " The latency is still better in terms of like the short packet response type workloads for", "tokens": [440, 27043, 307, 920, 1101, 294, 2115, 295, 411, 264, 2099, 20300, 4134, 2010, 32452, 337], "temperature": 0.0, "avg_logprob": -0.18956954777240753, "compression_ratio": 1.5654761904761905, "no_speech_prob": 5.053749555372633e-05}, {"id": 180, "seek": 93740, "start": 937.4, "end": 941.68, "text": " the meta, so that's still on par with the host.", "tokens": [264, 19616, 11, 370, 300, 311, 920, 322, 971, 365, 264, 3975, 13], "temperature": 0.0, "avg_logprob": -0.1764972751790827, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.354489935096353e-05}, {"id": 181, "seek": 93740, "start": 941.68, "end": 947.88, "text": " So what is the remaining offender like when you run all these features together?", "tokens": [407, 437, 307, 264, 8877, 766, 3216, 411, 562, 291, 1190, 439, 613, 4122, 1214, 30], "temperature": 0.0, "avg_logprob": -0.1764972751790827, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.354489935096353e-05}, {"id": 182, "seek": 93740, "start": 947.88, "end": 953.0799999999999, "text": " It's basically the copying to users, so like between 60 and 70% of the cycles is really", "tokens": [467, 311, 1936, 264, 27976, 281, 5022, 11, 370, 411, 1296, 4060, 293, 5285, 4, 295, 264, 17796, 307, 534], "temperature": 0.0, "avg_logprob": -0.1764972751790827, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.354489935096353e-05}, {"id": 183, "seek": 93740, "start": 953.0799999999999, "end": 957.56, "text": " spent on copying all this data to user space.", "tokens": [4418, 322, 27976, 439, 341, 1412, 281, 4195, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1764972751790827, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.354489935096353e-05}, {"id": 184, "seek": 93740, "start": 957.56, "end": 963.72, "text": " So the next question we ask ourselves actually in this experiment, so what if we combine", "tokens": [407, 264, 958, 1168, 321, 1029, 4175, 767, 294, 341, 5120, 11, 370, 437, 498, 321, 10432], "temperature": 0.0, "avg_logprob": -0.1764972751790827, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.354489935096353e-05}, {"id": 185, "seek": 96372, "start": 963.72, "end": 970.84, "text": " the whole Big TCP stuff with TCP0 copy, so what if we could leverage the memory map TCP?", "tokens": [264, 1379, 5429, 48965, 1507, 365, 48965, 15, 5055, 11, 370, 437, 498, 321, 727, 13982, 264, 4675, 4471, 48965, 30], "temperature": 0.0, "avg_logprob": -0.16599881207501446, "compression_ratio": 1.5776892430278884, "no_speech_prob": 8.604036702308804e-05}, {"id": 186, "seek": 96372, "start": 970.84, "end": 974.36, "text": " And it turns out that's currently not possible in the kernel.", "tokens": [400, 309, 4523, 484, 300, 311, 4362, 406, 1944, 294, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.16599881207501446, "compression_ratio": 1.5776892430278884, "no_speech_prob": 8.604036702308804e-05}, {"id": 187, "seek": 96372, "start": 974.36, "end": 980.2, "text": " That's a limitation because in the GRO layer, Big TCP will create a frag list, which is essentially", "tokens": [663, 311, 257, 27432, 570, 294, 264, 460, 7142, 4583, 11, 5429, 48965, 486, 1884, 257, 9241, 1329, 11, 597, 307, 4476], "temperature": 0.0, "avg_logprob": -0.16599881207501446, "compression_ratio": 1.5776892430278884, "no_speech_prob": 8.604036702308804e-05}, {"id": 188, "seek": 96372, "start": 980.2, "end": 986.2, "text": " like a list of SKBs as a single big one that is being pushed up the stack.", "tokens": [411, 257, 1329, 295, 21483, 33, 82, 382, 257, 2167, 955, 472, 300, 307, 885, 9152, 493, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.16599881207501446, "compression_ratio": 1.5776892430278884, "no_speech_prob": 8.604036702308804e-05}, {"id": 189, "seek": 96372, "start": 986.2, "end": 991.9200000000001, "text": " And TCP0 copy only works on the SKB frags, so that's like an internal.", "tokens": [400, 48965, 15, 5055, 787, 1985, 322, 264, 21483, 33, 9241, 82, 11, 370, 300, 311, 411, 364, 6920, 13], "temperature": 0.0, "avg_logprob": -0.16599881207501446, "compression_ratio": 1.5776892430278884, "no_speech_prob": 8.604036702308804e-05}, {"id": 190, "seek": 99192, "start": 991.92, "end": 1000.04, "text": " So basically you have a single SKB and it has like the pages as read only attached in", "tokens": [407, 1936, 291, 362, 257, 2167, 21483, 33, 293, 309, 575, 411, 264, 7183, 382, 1401, 787, 8570, 294], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 191, "seek": 99192, "start": 1000.04, "end": 1001.8, "text": " the non-linear section.", "tokens": [264, 2107, 12, 28263, 3541, 13], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 192, "seek": 99192, "start": 1001.8, "end": 1005.56, "text": " So that currently does not work.", "tokens": [407, 300, 4362, 775, 406, 589, 13], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 193, "seek": 99192, "start": 1005.56, "end": 1010.52, "text": " Combining those two would probably have like really big potential, but what we now try", "tokens": [25939, 1760, 729, 732, 576, 1391, 362, 411, 534, 955, 3995, 11, 457, 437, 321, 586, 853], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 194, "seek": 99192, "start": 1010.52, "end": 1017.12, "text": " to do is we just looked at just using the TCP0 copy to see how it looks without the", "tokens": [281, 360, 307, 321, 445, 2956, 412, 445, 1228, 264, 48965, 15, 5055, 281, 536, 577, 309, 1542, 1553, 264], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 195, "seek": 99192, "start": 1017.12, "end": 1020.48, "text": " Big TCP.", "tokens": [5429, 48965, 13], "temperature": 0.0, "avg_logprob": -0.19017647587975792, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3204882634454407e-05}, {"id": 196, "seek": 102048, "start": 1020.48, "end": 1025.0, "text": " Actually speaking it's not as straightforward to deploy because first of all you need to", "tokens": [5135, 4124, 309, 311, 406, 382, 15325, 281, 7274, 570, 700, 295, 439, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.15528985191794004, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0235138183343224e-05}, {"id": 197, "seek": 102048, "start": 1025.0, "end": 1031.04, "text": " rewrite your application in order to leverage memory map TCP.", "tokens": [28132, 428, 3861, 294, 1668, 281, 13982, 4675, 4471, 48965, 13], "temperature": 0.0, "avg_logprob": -0.15528985191794004, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0235138183343224e-05}, {"id": 198, "seek": 102048, "start": 1031.04, "end": 1034.56, "text": " This can be done for RX but also for TX or both.", "tokens": [639, 393, 312, 1096, 337, 46197, 457, 611, 337, 314, 55, 420, 1293, 13], "temperature": 0.0, "avg_logprob": -0.15528985191794004, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0235138183343224e-05}, {"id": 199, "seek": 102048, "start": 1034.56, "end": 1041.52, "text": " And it needs driver changes and in particular driver changes to be able to split the header", "tokens": [400, 309, 2203, 6787, 2962, 293, 294, 1729, 6787, 2962, 281, 312, 1075, 281, 7472, 264, 23117], "temperature": 0.0, "avg_logprob": -0.15528985191794004, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0235138183343224e-05}, {"id": 200, "seek": 102048, "start": 1041.52, "end": 1047.56, "text": " from the data because the data you want to memory map to user space.", "tokens": [490, 264, 1412, 570, 264, 1412, 291, 528, 281, 4675, 4471, 281, 4195, 1901, 13], "temperature": 0.0, "avg_logprob": -0.15528985191794004, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0235138183343224e-05}, {"id": 201, "seek": 104756, "start": 1047.56, "end": 1052.36, "text": " Some Nix might do this with the hardware and some others you would have to do some kind", "tokens": [2188, 426, 970, 1062, 360, 341, 365, 264, 8837, 293, 512, 2357, 291, 576, 362, 281, 360, 512, 733], "temperature": 0.0, "avg_logprob": -0.204931154355898, "compression_ratio": 1.4933333333333334, "no_speech_prob": 3.167532850056887e-05}, {"id": 202, "seek": 104756, "start": 1052.36, "end": 1060.08, "text": " of pseudo header data splits where you basically just copy the header into a linear section.", "tokens": [295, 35899, 23117, 1412, 37741, 689, 291, 1936, 445, 5055, 264, 23117, 666, 257, 8213, 3541, 13], "temperature": 0.0, "avg_logprob": -0.204931154355898, "compression_ratio": 1.4933333333333334, "no_speech_prob": 3.167532850056887e-05}, {"id": 203, "seek": 104756, "start": 1060.08, "end": 1061.8799999999999, "text": " So this is how it would look like.", "tokens": [407, 341, 307, 577, 309, 576, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.204931154355898, "compression_ratio": 1.4933333333333334, "no_speech_prob": 3.167532850056887e-05}, {"id": 204, "seek": 104756, "start": 1061.8799999999999, "end": 1066.76, "text": " We tried this for 4K and 8K MTU.", "tokens": [492, 3031, 341, 337, 1017, 42, 293, 1649, 42, 37333, 52, 13], "temperature": 0.0, "avg_logprob": -0.204931154355898, "compression_ratio": 1.4933333333333334, "no_speech_prob": 3.167532850056887e-05}, {"id": 205, "seek": 104756, "start": 1066.76, "end": 1072.72, "text": " There's a great talk from Eric Dumasay, the TCP maintainer in terms of what all the all", "tokens": [821, 311, 257, 869, 751, 490, 9336, 29572, 296, 320, 11, 264, 48965, 6909, 260, 294, 2115, 295, 437, 439, 264, 439], "temperature": 0.0, "avg_logprob": -0.204931154355898, "compression_ratio": 1.4933333333333334, "no_speech_prob": 3.167532850056887e-05}, {"id": 206, "seek": 107272, "start": 1072.72, "end": 1077.88, "text": " those things you need to do in order to be able to make use of this for example you also", "tokens": [729, 721, 291, 643, 281, 360, 294, 1668, 281, 312, 1075, 281, 652, 764, 295, 341, 337, 1365, 291, 611], "temperature": 0.0, "avg_logprob": -0.20514530605740017, "compression_ratio": 1.4837209302325582, "no_speech_prob": 3.880596341332421e-05}, {"id": 207, "seek": 107272, "start": 1077.88, "end": 1086.56, "text": " need to align the TCP window scale to 12 segments so that you fill exactly those pages.", "tokens": [643, 281, 7975, 264, 48965, 4910, 4373, 281, 2272, 19904, 370, 300, 291, 2836, 2293, 729, 7183, 13], "temperature": 0.0, "avg_logprob": -0.20514530605740017, "compression_ratio": 1.4837209302325582, "no_speech_prob": 3.880596341332421e-05}, {"id": 208, "seek": 107272, "start": 1086.56, "end": 1094.24, "text": " And yeah the driver support can be very different like we had in our lab MLX500 gigabit Nix", "tokens": [400, 1338, 264, 6787, 1406, 393, 312, 588, 819, 411, 321, 632, 294, 527, 2715, 21601, 55, 7526, 8741, 455, 270, 426, 970], "temperature": 0.0, "avg_logprob": -0.20514530605740017, "compression_ratio": 1.4837209302325582, "no_speech_prob": 3.880596341332421e-05}, {"id": 209, "seek": 107272, "start": 1094.24, "end": 1097.84, "text": " and they did not implement the header data splits.", "tokens": [293, 436, 630, 406, 4445, 264, 23117, 1412, 37741, 13], "temperature": 0.0, "avg_logprob": -0.20514530605740017, "compression_ratio": 1.4837209302325582, "no_speech_prob": 3.880596341332421e-05}, {"id": 210, "seek": 109784, "start": 1097.84, "end": 1104.84, "text": " So my colleague Nikolai did the POC implementation in the like to change the driver to be able", "tokens": [407, 452, 13532, 13969, 401, 1301, 630, 264, 22299, 34, 11420, 294, 264, 411, 281, 1319, 264, 6787, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1726001780083839, "compression_ratio": 1.6075949367088607, "no_speech_prob": 3.75957642972935e-05}, {"id": 211, "seek": 109784, "start": 1104.84, "end": 1108.6799999999998, "text": " to do that so that we could get some measurements out of this.", "tokens": [281, 360, 300, 370, 300, 321, 727, 483, 512, 15383, 484, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1726001780083839, "compression_ratio": 1.6075949367088607, "no_speech_prob": 3.75957642972935e-05}, {"id": 212, "seek": 109784, "start": 1108.6799999999998, "end": 1113.04, "text": " And if you want to look into an application like an example application and how you can", "tokens": [400, 498, 291, 528, 281, 574, 666, 364, 3861, 411, 364, 1365, 3861, 293, 577, 291, 393], "temperature": 0.0, "avg_logprob": -0.1726001780083839, "compression_ratio": 1.6075949367088607, "no_speech_prob": 3.75957642972935e-05}, {"id": 213, "seek": 109784, "start": 1113.04, "end": 1118.12, "text": " implement that there's one in the networking self-test in the upstream tree called TCP", "tokens": [4445, 300, 456, 311, 472, 294, 264, 17985, 2698, 12, 31636, 294, 264, 33915, 4230, 1219, 48965], "temperature": 0.0, "avg_logprob": -0.1726001780083839, "compression_ratio": 1.6075949367088607, "no_speech_prob": 3.75957642972935e-05}, {"id": 214, "seek": 109784, "start": 1118.12, "end": 1121.4399999999998, "text": " M-MAP which is useful also for the benchmarking.", "tokens": [376, 12, 44, 4715, 597, 307, 4420, 611, 337, 264, 18927, 278, 13], "temperature": 0.0, "avg_logprob": -0.1726001780083839, "compression_ratio": 1.6075949367088607, "no_speech_prob": 3.75957642972935e-05}, {"id": 215, "seek": 112144, "start": 1121.44, "end": 1128.6000000000001, "text": " So you really need to align various different settings like for our test implementation", "tokens": [407, 291, 534, 643, 281, 7975, 3683, 819, 6257, 411, 337, 527, 1500, 11420], "temperature": 0.0, "avg_logprob": -0.16654610365964054, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.5927224012557417e-05}, {"id": 216, "seek": 112144, "start": 1128.6000000000001, "end": 1133.48, "text": " we used like for MLX5 the non-striding mode so like the legacy mode so you need to switch", "tokens": [321, 1143, 411, 337, 21601, 55, 20, 264, 2107, 12, 9733, 2819, 4391, 370, 411, 264, 11711, 4391, 370, 291, 643, 281, 3679], "temperature": 0.0, "avg_logprob": -0.16654610365964054, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.5927224012557417e-05}, {"id": 217, "seek": 112144, "start": 1133.48, "end": 1139.52, "text": " that off and EVE tool first because I mean for the POC it was easier to do the way like", "tokens": [300, 766, 293, 462, 7540, 2290, 700, 570, 286, 914, 337, 264, 22299, 34, 309, 390, 3571, 281, 360, 264, 636, 411], "temperature": 0.0, "avg_logprob": -0.16654610365964054, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.5927224012557417e-05}, {"id": 218, "seek": 112144, "start": 1139.52, "end": 1147.64, "text": " the packet layout is done there then you need to align the MTU to either one page or two", "tokens": [264, 20300, 13333, 307, 1096, 456, 550, 291, 643, 281, 7975, 264, 37333, 52, 281, 2139, 472, 3028, 420, 732], "temperature": 0.0, "avg_logprob": -0.16654610365964054, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.5927224012557417e-05}, {"id": 219, "seek": 114764, "start": 1147.64, "end": 1154.1200000000001, "text": " pages and you need to do various other settings like for the course of time I will not go", "tokens": [7183, 293, 291, 643, 281, 360, 3683, 661, 6257, 411, 337, 264, 1164, 295, 565, 286, 486, 406, 352], "temperature": 0.0, "avg_logprob": -0.14188098907470703, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.1086498236400075e-05}, {"id": 220, "seek": 114764, "start": 1154.1200000000001, "end": 1157.2800000000002, "text": " into all of the details.", "tokens": [666, 439, 295, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.14188098907470703, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.1086498236400075e-05}, {"id": 221, "seek": 114764, "start": 1157.2800000000002, "end": 1163.68, "text": " And generally I would think it would be useful addition to the kernel to have like a configuration", "tokens": [400, 5101, 286, 576, 519, 309, 576, 312, 4420, 4500, 281, 264, 28256, 281, 362, 411, 257, 11694], "temperature": 0.0, "avg_logprob": -0.14188098907470703, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.1086498236400075e-05}, {"id": 222, "seek": 114764, "start": 1163.68, "end": 1170.3600000000001, "text": " framework for that and to be able to have more drivers supporting the header data split.", "tokens": [8388, 337, 300, 293, 281, 312, 1075, 281, 362, 544, 11590, 7231, 264, 23117, 1412, 7472, 13], "temperature": 0.0, "avg_logprob": -0.14188098907470703, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.1086498236400075e-05}, {"id": 223, "seek": 114764, "start": 1170.3600000000001, "end": 1174.3200000000002, "text": " There's actually one in Windows kernel turns out so there's some documentation around", "tokens": [821, 311, 767, 472, 294, 8591, 28256, 4523, 484, 370, 456, 311, 512, 14333, 926], "temperature": 0.0, "avg_logprob": -0.14188098907470703, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.1086498236400075e-05}, {"id": 224, "seek": 117432, "start": 1174.32, "end": 1177.8, "text": " there that we found while preparing for the talk.", "tokens": [456, 300, 321, 1352, 1339, 10075, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.1515809973490607, "compression_ratio": 1.75, "no_speech_prob": 4.90027159685269e-05}, {"id": 225, "seek": 117432, "start": 1177.8, "end": 1183.76, "text": " The other thing is the caveat is like the TCP-0 copy may like the benefits might be", "tokens": [440, 661, 551, 307, 264, 43012, 307, 411, 264, 48965, 12, 15, 5055, 815, 411, 264, 5311, 1062, 312], "temperature": 0.0, "avg_logprob": -0.1515809973490607, "compression_ratio": 1.75, "no_speech_prob": 4.90027159685269e-05}, {"id": 226, "seek": 117432, "start": 1183.76, "end": 1188.2, "text": " limited if you then actually go in your application and then touch the data because then they need", "tokens": [5567, 498, 291, 550, 767, 352, 294, 428, 3861, 293, 550, 2557, 264, 1412, 570, 550, 436, 643], "temperature": 0.0, "avg_logprob": -0.1515809973490607, "compression_ratio": 1.75, "no_speech_prob": 4.90027159685269e-05}, {"id": 227, "seek": 117432, "start": 1188.2, "end": 1192.1599999999999, "text": " to be pulled into the cache which they would be if like if you would have to copy things", "tokens": [281, 312, 7373, 666, 264, 19459, 597, 436, 576, 312, 498, 411, 498, 291, 576, 362, 281, 5055, 721], "temperature": 0.0, "avg_logprob": -0.1515809973490607, "compression_ratio": 1.75, "no_speech_prob": 4.90027159685269e-05}, {"id": 228, "seek": 117432, "start": 1192.1599999999999, "end": 1199.48, "text": " to user but they may not be like if you just memory map so the applications there is mostly", "tokens": [281, 4195, 457, 436, 815, 406, 312, 411, 498, 291, 445, 4675, 4471, 370, 264, 5821, 456, 307, 5240], "temperature": 0.0, "avg_logprob": -0.1515809973490607, "compression_ratio": 1.75, "no_speech_prob": 4.90027159685269e-05}, {"id": 229, "seek": 119948, "start": 1199.48, "end": 1204.76, "text": " like on the for example storage side where you wouldn't have to do that.", "tokens": [411, 322, 264, 337, 1365, 6725, 1252, 689, 291, 2759, 380, 362, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.163699433610246, "compression_ratio": 1.5052631578947369, "no_speech_prob": 2.044071334239561e-05}, {"id": 230, "seek": 119948, "start": 1204.76, "end": 1212.8, "text": " And looking at the data for 4K MTU we tried with the implementation we got to 81 gigabit", "tokens": [400, 1237, 412, 264, 1412, 337, 1017, 42, 37333, 52, 321, 3031, 365, 264, 11420, 321, 658, 281, 30827, 8741, 455, 270], "temperature": 0.0, "avg_logprob": -0.163699433610246, "compression_ratio": 1.5052631578947369, "no_speech_prob": 2.044071334239561e-05}, {"id": 231, "seek": 119948, "start": 1212.8, "end": 1218.92, "text": " per second so that's a bit limiting.", "tokens": [680, 1150, 370, 300, 311, 257, 857, 22083, 13], "temperature": 0.0, "avg_logprob": -0.163699433610246, "compression_ratio": 1.5052631578947369, "no_speech_prob": 2.044071334239561e-05}, {"id": 232, "seek": 119948, "start": 1218.92, "end": 1225.48, "text": " Could also be that this is mostly because the implementation was you know POC with lots", "tokens": [7497, 611, 312, 300, 341, 307, 5240, 570, 264, 11420, 390, 291, 458, 22299, 34, 365, 3195], "temperature": 0.0, "avg_logprob": -0.163699433610246, "compression_ratio": 1.5052631578947369, "no_speech_prob": 2.044071334239561e-05}, {"id": 233, "seek": 122548, "start": 1225.48, "end": 1231.4, "text": " of optimization potential that can still be there but we looked at the 8K MTU and there", "tokens": [295, 19618, 3995, 300, 393, 920, 312, 456, 457, 321, 2956, 412, 264, 1649, 42, 37333, 52, 293, 456], "temperature": 0.0, "avg_logprob": -0.14049738980411144, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.2455453290604055e-05}, {"id": 234, "seek": 122548, "start": 1231.4, "end": 1237.64, "text": " we were able to get to 98 gigabit per second but the interesting piece here is the cost", "tokens": [321, 645, 1075, 281, 483, 281, 20860, 8741, 455, 270, 680, 1150, 457, 264, 1880, 2522, 510, 307, 264, 2063], "temperature": 0.0, "avg_logprob": -0.14049738980411144, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.2455453290604055e-05}, {"id": 235, "seek": 122548, "start": 1237.64, "end": 1244.4, "text": " per megabyte we could we were able to reduce from 85 down to 27 microseconds per megabyte", "tokens": [680, 10816, 34529, 321, 727, 321, 645, 1075, 281, 5407, 490, 14695, 760, 281, 7634, 3123, 37841, 28750, 680, 10816, 34529], "temperature": 0.0, "avg_logprob": -0.14049738980411144, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.2455453290604055e-05}, {"id": 236, "seek": 122548, "start": 1244.4, "end": 1253.64, "text": " so this really is significant and yeah because of the avoidance that you don't copy anymore.", "tokens": [370, 341, 534, 307, 4776, 293, 1338, 570, 295, 264, 5042, 719, 300, 291, 500, 380, 5055, 3602, 13], "temperature": 0.0, "avg_logprob": -0.14049738980411144, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.2455453290604055e-05}, {"id": 237, "seek": 125364, "start": 1253.64, "end": 1263.0800000000002, "text": " So as a summary so we started with the default and then you know switch to the 8K MTU then", "tokens": [407, 382, 257, 12691, 370, 321, 1409, 365, 264, 7576, 293, 550, 291, 458, 3679, 281, 264, 1649, 42, 37333, 52, 550], "temperature": 0.0, "avg_logprob": -0.15576611008754995, "compression_ratio": 1.7033492822966507, "no_speech_prob": 3.0680650525027886e-05}, {"id": 238, "seek": 125364, "start": 1263.0800000000002, "end": 1267.92, "text": " we went to the host routing that we covered then the addition of the meta device where", "tokens": [321, 1437, 281, 264, 3975, 32722, 300, 321, 5343, 550, 264, 4500, 295, 264, 19616, 4302, 689], "temperature": 0.0, "avg_logprob": -0.15576611008754995, "compression_ratio": 1.7033492822966507, "no_speech_prob": 3.0680650525027886e-05}, {"id": 239, "seek": 125364, "start": 1267.92, "end": 1273.5600000000002, "text": " we can avoid where we can do the slow latency you know for ingress and egress then the big", "tokens": [321, 393, 5042, 689, 321, 393, 360, 264, 2964, 27043, 291, 458, 337, 3957, 735, 293, 308, 3091, 550, 264, 955], "temperature": 0.0, "avg_logprob": -0.15576611008754995, "compression_ratio": 1.7033492822966507, "no_speech_prob": 3.0680650525027886e-05}, {"id": 240, "seek": 125364, "start": 1273.5600000000002, "end": 1280.2800000000002, "text": " TCP and that all works without changes to the application and with that you can already", "tokens": [48965, 293, 300, 439, 1985, 1553, 2962, 281, 264, 3861, 293, 365, 300, 291, 393, 1217], "temperature": 0.0, "avg_logprob": -0.15576611008754995, "compression_ratio": 1.7033492822966507, "no_speech_prob": 3.0680650525027886e-05}, {"id": 241, "seek": 128028, "start": 1280.28, "end": 1286.68, "text": " get like a 2x improvement and then it gets even more dramatic but it's of course dependent", "tokens": [483, 411, 257, 568, 87, 10444, 293, 550, 309, 2170, 754, 544, 12023, 457, 309, 311, 295, 1164, 12334], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 242, "seek": 128028, "start": 1286.68, "end": 1290.0, "text": " on your application for the zero copy.", "tokens": [322, 428, 3861, 337, 264, 4018, 5055, 13], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 243, "seek": 128028, "start": 1290.0, "end": 1294.6399999999999, "text": " Some of the future directions as I mentioned earlier it would be useful to have like a", "tokens": [2188, 295, 264, 2027, 11095, 382, 286, 2835, 3071, 309, 576, 312, 4420, 281, 362, 411, 257], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 244, "seek": 128028, "start": 1294.6399999999999, "end": 1300.48, "text": " generic header data split framework for NIC drivers that they can implement that and expose", "tokens": [19577, 23117, 1412, 7472, 8388, 337, 426, 2532, 11590, 300, 436, 393, 4445, 300, 293, 19219], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 245, "seek": 128028, "start": 1300.48, "end": 1302.8799999999999, "text": " it to this setting.", "tokens": [309, 281, 341, 3287, 13], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 246, "seek": 128028, "start": 1302.8799999999999, "end": 1309.3999999999999, "text": " There's potential here as well to optimize for example like the header pages could be", "tokens": [821, 311, 3995, 510, 382, 731, 281, 19719, 337, 1365, 411, 264, 23117, 7183, 727, 312], "temperature": 0.0, "avg_logprob": -0.19815153824655632, "compression_ratio": 1.6108949416342413, "no_speech_prob": 5.1400813390500844e-05}, {"id": 247, "seek": 130940, "start": 1309.4, "end": 1316.96, "text": " the head page could be packed with the headers it could get recycling which we didn't implement", "tokens": [264, 1378, 3028, 727, 312, 13265, 365, 264, 45101, 309, 727, 483, 23363, 597, 321, 994, 380, 4445], "temperature": 0.0, "avg_logprob": -0.15712179620581937, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.958445348776877e-05}, {"id": 248, "seek": 130940, "start": 1316.96, "end": 1324.76, "text": " here and in future big TCP would be interesting to combine with the zero copy so that this", "tokens": [510, 293, 294, 2027, 955, 48965, 576, 312, 1880, 281, 10432, 365, 264, 4018, 5055, 370, 300, 341], "temperature": 0.0, "avg_logprob": -0.15712179620581937, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.958445348776877e-05}, {"id": 249, "seek": 130940, "start": 1324.76, "end": 1330.72, "text": " covers the frag list in GRO and the other thing that is at some point on the horizon", "tokens": [10538, 264, 9241, 1329, 294, 460, 7142, 293, 264, 661, 551, 300, 307, 412, 512, 935, 322, 264, 18046], "temperature": 0.0, "avg_logprob": -0.15712179620581937, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.958445348776877e-05}, {"id": 250, "seek": 130940, "start": 1330.72, "end": 1337.2, "text": " is to push the big TCP actually onto the wire as well if the hardware supports that and", "tokens": [307, 281, 2944, 264, 955, 48965, 767, 3911, 264, 6234, 382, 731, 498, 264, 8837, 9346, 300, 293], "temperature": 0.0, "avg_logprob": -0.15712179620581937, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.958445348776877e-05}, {"id": 251, "seek": 133720, "start": 1337.2, "end": 1344.8400000000001, "text": " yeah so with that I'm done with the talk and the prototype for the meta device is currently", "tokens": [1338, 370, 365, 300, 286, 478, 1096, 365, 264, 751, 293, 264, 19475, 337, 264, 19616, 4302, 307, 4362], "temperature": 0.0, "avg_logprob": -0.17841037703149112, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.3926527723670006e-05}, {"id": 252, "seek": 133720, "start": 1344.8400000000001, "end": 1351.0800000000002, "text": " you can find on this branch we are working on pushing this upstream in the coming months", "tokens": [291, 393, 915, 322, 341, 9819, 321, 366, 1364, 322, 7380, 341, 33915, 294, 264, 1348, 2493], "temperature": 0.0, "avg_logprob": -0.17841037703149112, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.3926527723670006e-05}, {"id": 253, "seek": 133720, "start": 1351.0800000000002, "end": 1357.0800000000002, "text": " and there's also the prototype public for the MLX5 header data split and yeah so the", "tokens": [293, 456, 311, 611, 264, 19475, 1908, 337, 264, 21601, 55, 20, 23117, 1412, 7472, 293, 1338, 370, 264], "temperature": 0.0, "avg_logprob": -0.17841037703149112, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.3926527723670006e-05}, {"id": 254, "seek": 133720, "start": 1357.0800000000002, "end": 1361.8400000000001, "text": " plan is basically to get this into upstream kernel and then also to get this integrated", "tokens": [1393, 307, 1936, 281, 483, 341, 666, 33915, 28256, 293, 550, 611, 281, 483, 341, 10919], "temperature": 0.0, "avg_logprob": -0.17841037703149112, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.3926527723670006e-05}, {"id": 255, "seek": 136184, "start": 1361.84, "end": 1374.3999999999999, "text": " into Syllium for the next release yeah thank you are there any questions?", "tokens": [666, 3902, 285, 2197, 337, 264, 958, 4374, 1338, 1309, 291, 366, 456, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2900938245985243, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00028397972346283495}, {"id": 256, "seek": 136184, "start": 1374.3999999999999, "end": 1379.36, "text": " Thanks we already have two questions in the chat so we can start with them if there are", "tokens": [2561, 321, 1217, 362, 732, 1651, 294, 264, 5081, 370, 321, 393, 722, 365, 552, 498, 456, 366], "temperature": 0.0, "avg_logprob": -0.2900938245985243, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00028397972346283495}, {"id": 257, "seek": 136184, "start": 1379.36, "end": 1381.6799999999998, "text": " no one in the room.", "tokens": [572, 472, 294, 264, 1808, 13], "temperature": 0.0, "avg_logprob": -0.2900938245985243, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00028397972346283495}, {"id": 258, "seek": 136184, "start": 1381.6799999999998, "end": 1385.76, "text": " Alright so the first question we got was can we well not really a question as much as a", "tokens": [2798, 370, 264, 700, 1168, 321, 658, 390, 393, 321, 731, 406, 534, 257, 1168, 382, 709, 382, 257], "temperature": 0.0, "avg_logprob": -0.2900938245985243, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00028397972346283495}, {"id": 259, "seek": 136184, "start": 1385.76, "end": 1390.6399999999999, "text": " comment I think which is can we please come up with a better name than meta for this just", "tokens": [2871, 286, 519, 597, 307, 393, 321, 1767, 808, 493, 365, 257, 1101, 1315, 813, 19616, 337, 341, 445], "temperature": 0.0, "avg_logprob": -0.2900938245985243, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00028397972346283495}, {"id": 260, "seek": 139064, "start": 1390.64, "end": 1395.0400000000002, "text": " call it BPF or BPFDF or something like that.", "tokens": [818, 309, 40533, 37, 420, 40533, 37, 35, 37, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.22930132687746824, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.0004763320030178875}, {"id": 261, "seek": 139064, "start": 1395.0400000000002, "end": 1402.1200000000001, "text": " The other thing was if the perf benefit comes from calling through from inside the network", "tokens": [440, 661, 551, 390, 498, 264, 13826, 5121, 1487, 490, 5141, 807, 490, 1854, 264, 3209], "temperature": 0.0, "avg_logprob": -0.22930132687746824, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.0004763320030178875}, {"id": 262, "seek": 139064, "start": 1402.1200000000001, "end": 1408.3600000000001, "text": " namespace can we just make VIF do that instead?", "tokens": [5288, 17940, 393, 321, 445, 652, 691, 12775, 360, 300, 2602, 30], "temperature": 0.0, "avg_logprob": -0.22930132687746824, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.0004763320030178875}, {"id": 263, "seek": 139064, "start": 1408.3600000000001, "end": 1414.1200000000001, "text": " I think we could but the question is like I haven't looked into the like how it would", "tokens": [286, 519, 321, 727, 457, 264, 1168, 307, 411, 286, 2378, 380, 2956, 666, 264, 411, 577, 309, 576], "temperature": 0.0, "avg_logprob": -0.22930132687746824, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.0004763320030178875}, {"id": 264, "seek": 139064, "start": 1414.1200000000001, "end": 1420.3200000000002, "text": " affect the XDP related bits so it felt easier to do something simple and start from scratch", "tokens": [3345, 264, 1783, 11373, 4077, 9239, 370, 309, 2762, 3571, 281, 360, 746, 2199, 293, 722, 490, 8459], "temperature": 0.0, "avg_logprob": -0.22930132687746824, "compression_ratio": 1.5695652173913044, "no_speech_prob": 0.0004763320030178875}, {"id": 265, "seek": 142032, "start": 1420.32, "end": 1425.6, "text": " like one thing I don't really like about the VIF devices to be honest how complex it got", "tokens": [411, 472, 551, 286, 500, 380, 534, 411, 466, 264, 691, 12775, 5759, 281, 312, 3245, 577, 3997, 309, 658], "temperature": 0.0, "avg_logprob": -0.17797917957547343, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.00015872686344664544}, {"id": 266, "seek": 142032, "start": 1425.6, "end": 1432.52, "text": " with all the XDP additions and it's not even that beneficial and it added like multi queue", "tokens": [365, 439, 264, 1783, 11373, 35113, 293, 309, 311, 406, 754, 300, 14072, 293, 309, 3869, 411, 4825, 18639], "temperature": 0.0, "avg_logprob": -0.17797917957547343, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.00015872686344664544}, {"id": 267, "seek": 142032, "start": 1432.52, "end": 1437.4399999999998, "text": " support and all of that which was not needed at all for a virtual device so I really wanted", "tokens": [1406, 293, 439, 295, 300, 597, 390, 406, 2978, 412, 439, 337, 257, 6374, 4302, 370, 286, 534, 1415], "temperature": 0.0, "avg_logprob": -0.17797917957547343, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.00015872686344664544}, {"id": 268, "seek": 142032, "start": 1437.4399999999998, "end": 1446.6399999999999, "text": " to have something simple and yeah it's easier so.", "tokens": [281, 362, 746, 2199, 293, 1338, 309, 311, 3571, 370, 13], "temperature": 0.0, "avg_logprob": -0.17797917957547343, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.00015872686344664544}, {"id": 269, "seek": 144664, "start": 1446.64, "end": 1459.16, "text": " Any more questions from the audience here?", "tokens": [2639, 544, 1651, 490, 264, 4034, 510, 30], "temperature": 0.0, "avg_logprob": -0.354913970171395, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.00157859455794096}, {"id": 270, "seek": 144664, "start": 1459.16, "end": 1466.2, "text": " I have a question you're about big TCP and TCP zero copy it's you're in your lab you", "tokens": [286, 362, 257, 1168, 291, 434, 466, 955, 48965, 293, 48965, 4018, 5055, 309, 311, 291, 434, 294, 428, 2715, 291], "temperature": 0.0, "avg_logprob": -0.354913970171395, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.00157859455794096}, {"id": 271, "seek": 144664, "start": 1466.2, "end": 1472.72, "text": " only did benchmarks host to host right because so but also part to part so we tried both", "tokens": [787, 630, 43751, 3975, 281, 3975, 558, 570, 370, 457, 611, 644, 281, 644, 370, 321, 3031, 1293], "temperature": 0.0, "avg_logprob": -0.354913970171395, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.00157859455794096}, {"id": 272, "seek": 144664, "start": 1472.72, "end": 1473.72, "text": " yeah.", "tokens": [1338, 13], "temperature": 0.0, "avg_logprob": -0.354913970171395, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.00157859455794096}, {"id": 273, "seek": 147372, "start": 1473.72, "end": 1478.3600000000001, "text": " Because do you know how it would work with the switches do you need special switch support", "tokens": [1436, 360, 291, 458, 577, 309, 576, 589, 365, 264, 19458, 360, 291, 643, 2121, 3679, 1406], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 274, "seek": 147372, "start": 1478.3600000000001, "end": 1479.3600000000001, "text": " for this?", "tokens": [337, 341, 30], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 275, "seek": 147372, "start": 1479.3600000000001, "end": 1483.6000000000001, "text": " No no so the big TCP is only local to the node so like the packets on the wire they", "tokens": [883, 572, 370, 264, 955, 48965, 307, 787, 2654, 281, 264, 9984, 370, 411, 264, 30364, 322, 264, 6234, 436], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 276, "seek": 147372, "start": 1483.6000000000001, "end": 1489.3600000000001, "text": " will still be your MTU size packets it's just that the aggregation on like for the local", "tokens": [486, 920, 312, 428, 37333, 52, 2744, 30364, 309, 311, 445, 300, 264, 16743, 399, 322, 411, 337, 264, 2654], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 277, "seek": 147372, "start": 1489.3600000000001, "end": 1496.96, "text": " stack is much bigger so it's it doesn't affect anything on the wire that's a nice thing.", "tokens": [8630, 307, 709, 3801, 370, 309, 311, 309, 1177, 380, 3345, 1340, 322, 264, 6234, 300, 311, 257, 1481, 551, 13], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 278, "seek": 147372, "start": 1496.96, "end": 1499.84, "text": " And for the zero copy?", "tokens": [400, 337, 264, 4018, 5055, 30], "temperature": 0.0, "avg_logprob": -0.22452278892592628, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.000904497632291168}, {"id": 279, "seek": 149984, "start": 1499.84, "end": 1503.9599999999998, "text": " I mean like for the zero copy you need to change definitely the MTU which also affects", "tokens": [286, 914, 411, 337, 264, 4018, 5055, 291, 643, 281, 1319, 2138, 264, 37333, 52, 597, 611, 11807], "temperature": 0.0, "avg_logprob": -0.15189317946738384, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0002360996586503461}, {"id": 280, "seek": 149984, "start": 1503.9599999999998, "end": 1510.56, "text": " the rest of your network of course so that's one thing that would be required for that.", "tokens": [264, 1472, 295, 428, 3209, 295, 1164, 370, 300, 311, 472, 551, 300, 576, 312, 4739, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.15189317946738384, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0002360996586503461}, {"id": 281, "seek": 151056, "start": 1510.56, "end": 1530.12, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51342], "temperature": 0.0, "avg_logprob": -0.8814024130503336, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0004206979356240481}], "language": "en"}