{"text": " I used to work at Pyra, so maybe some of you know me from my six years at Pyra company. Now I work at Omnifish, where we, with our co-founders and employees, were a support glassfish server, so back to the roots, kind of. But this time I'd like to talk about Java, plain Java and Jakarta E, and how it all fits together when we combine that with AWS. So first, before I talked about AWS, let's ask, why do we want to have Java fast, or do we want to have Java start fast? I think everybody wants that, but why? Because it's cool, or because we need it. So there were times when we really didn't need that, when we had the application servers, it was a pain that it took a while to start, but in production it was already running, so there was no real business need for that, only to make developers happy and be more productive with developing codes. But now we have several use cases where it's really needed, because the more time it takes for Java program to start, it costs more money, and it's not user-friendly. And one example, a perfect example of this is AWS Lambda. So now, what is AWS Lambda? It's basically a service to which you can deploy your code, and this service runs your code only when it's needed, and it also charges you, because we need to pay for the cloud environment. But if we run the code in Lambda, we are charged only for the time when the code is running. And that's pretty nice, especially if we have code that usually just sits there and responds to users just once in a while, or only during certain periods of time, especially during the day or in the morning when there is some business activity. So how does AWS Lambda do that? It basically creates environment and deploys our code when it needs to be executed. And for that, if the code is not already deployed, it needs to create the runtime and initialize our so-called function, because this is how our code is called. It's called a function because it's basically just called by the runtime, it gives some result, and then it's thrown away. In reality, it's not always thrown away because AWS Lambda tries to cache our code so that it doesn't have to re-initialize it every time when it's run more frequently. So sometimes it stays there, and then AWS Lambda can skip the initialization phase. This is called warm start, because the code is already prepared to serve things. But if this doesn't happen, and the code is not available, it has to initialize everything. And this is usually referred to as cold start, just start from scratch. So the whole lifecycle of AWS Lambda is as on the slide, you can see there's init phase. This is only when the code or the function is not initialized. So in case of cold start, then there is a warm phase, which happens even for warm start-ups. There is this invoke phase, which actually is the only productive phase from these three. It actually does some job. The first phase only initialized gets some things ready before the application can process requests. Then the invoke phase does the job. And then when AWS Lambda service decides that it doesn't no longer needs our application running, because it's not doing anything right now, and they need AWS wants to use resources in some other way, it will tear everything away. So it will shut down the environment. And then we'll add square at square zero. And next invocation needs to go through the initial initialization phase. So let's not go back to the roots with plain Java application. And let's see or let's think about how fast we can get with Java on AWS Lambda. Can we start Java really fast? I tried to start a very simple Java program on my local machine. And if you do that too on your computers, you will see that Java really starts fast. In my case, it was 50 milliseconds, 0.05 seconds. So very small fraction of a second, where JVM started, printed something on output and finished. So we see on a local computer, plain Java doesn't start, doesn't take very long to start. If we compare the exactly what's going on in the AWS Lambda, because AWS Lambda needs to initialize the environment and only then it can run Java function. It takes a bit longer in reality. But when we compare it to other languages, I haven't done this. This is done by some other guy who is more experienced with AWS Lambda than me and compared performance in a more sophisticated way than just running on the computer or just several measurements. He did a lot of measurements across all the or various different languages, various different runtimes provided by AWS Lambda. And he found out that Java basically is on the same level as JavaScript, Python and a lot of other languages that there's not much difference. There's a small difference that at that time C sharp was a bit slower. But as AWS improves continually, the AWS Lambda, even these numbers would be probably better now. And C sharp and Docker will be maybe more even with the with the rest because the technology running AWS Lambda is continuously improving. But this is just to compare and show that Java itself or even the implementation of Java AWS function or the environment isn't worse than other languages. So now what is the problem actually? Why a lot of people perceive that Java starts very slow. The problem is how I see it is that many people don't think about Java in this simple way that it's a simple application. A lot of people think about Java as a language that runs enterprise applications. And with enterprise applications, we're used to use frameworks that do a lot of job for us. We run the applications on application servers, which are start to which are slow to start. And this is what we think about when we think when we say Java or when we talk about Java. So now we're coming to that. That thing that if we basically can run our applications that are similar to what we were used to before, but if we can start them fast, we could solve a problem with Java call starts as least as we use Java now. So the question I have now is Jakarta EE or some other frameworks like Springboard or something like that. Can that be as fast as plain Java? Can we run that in AWS Lambda to get good performance and fast startup? And the answer is there are such frameworks and solutions to that. There are several ones. I don't have much time to talk about all of them. So I picked one that I personally like. And it's called Piranha Cloud Framework. And this one is based on entirely Jakarta EE APIs. Previously, it was called Java EE. So it's a very well-known API that a lot of people already know, a lot of tools out there already use. So it's interoperable with existing codebase. But the thing with Piranha Cloud is that the implementation actually the engine of the framework is new, very flexible, and allows our application to stop, start very fast. Piranha Cloud is based on a lot of existing components. A lot of them come from the Glassfish server, which actually sort of proves that the server is not a problem or Jakarta EE is not a problem. The components are there, they are quite fast. But the problem how they are assembled in traditional Jakarta EE servers, Java EE application servers, that is the problem. Because an application server usually has a lot of other things that we don't need in Lambda, like monitoring a lot of vendor features and go on an administration console and a lot of other things. So here is an example, it's basically nothing else than a servlet. But this is an application using already some Jakarta EE APIs. And this application, this servlet, you can run on any Jakarta EE server. You can run it on Tomcat, you can run it on Glassfish, you can run it on anything that supports servlets. So the only difference if we run it with Piranha Cloud is that it starts fast and it uses Piranha's own servlet container, which was designed from scratch. And it's very flexible and fast. What is also nice about Piranha's servlet container is that it can be embedded very easily. And that's the crucial point. When we want to use Jakarta EE in Lambda, we need to basically shave off everything that we don't need. And in AWS Lambda, we don't even need an HTTP listener. Because AWS Lambda basically only wants a method from us that will be called, returns some response. And then AWS Lambda is responsible for mapping the HTTP request to an object that it passes to our method. And then the returned object should be mapped to an HTTP response. And not only HTTP requests and responses, but Lambda can handle any type of basically JSON messages, JSON events. So the only thing that our application needs is to parse some input object and return some output event. And with Piranha, we can create an engine and map our servlet onto it and just listen on some object. This object is usually called or the request response cycle is invoked by a service method, which accepts a request object and returns the response object. And this is exactly how we can use it in AWS Lambda. We just need to add one additional layer to map AWS request object to Piranha request object and back. If we run Piranha Cloud, this simple servlet, which is comparable to our plain Java, we were running before. If you remember with plain Java on my computer, I had startup times. Actually, it was not only startup times, but until the program ended and printed some message and finished, it was around 50 milliseconds. With Piranha, it's a bit longer time. But this already includes the first request. So it's very similar to the plain Java application. It's not only that the engine starts, but it actually serves the request response with text message through HTTP stack. And with that, it takes still comparable time around 130 milliseconds. Now we can compare how it works in AWS Lambda. And in AWS Lambda, I have a picture, but I hope I will be able to show you in a minute. As I said before, it takes a bit longer when we start the other function first time. Because this doesn't really matter if we run Java or any other runtime. AWS Lambda first needs to create some environment to execute our code in. And it takes a little bit of time. But together with creating this environment and running our code, our example Piranha function, it takes under one second to serve the request. Even if nothing was ready before, even on the first time we tried to run the function, it still serves the response under one second. If we tried it again, again, again, then the response times are much faster. This is on the right side here. It's under two milliseconds. Because this is only the code that needs to serve the request. Everything was initialized. Environment was initialized. The Piranha engine was initialized. It's cached in a static variable. So it's part of the process that is already live. AWS just executes a method basically on the Piranha engine that goes through the servlet and creates the server response. And that's it. That's why it takes only two milliseconds. This is only the time required to serve the actual response. So I'll try. I think I have a link here. How it works. Okay. So this is the actual AWS console where I already deployed the application, the function. And AWS console has a nice feature called tester or test button. With that, we can directly invoke the Lambda. Normally, we would have to create an API gateway and map it to Lambda so that we can access Lambda via HTTP from outside. AWS can also generate some URL that we can use to invoke the Lambda. But this is like directly execute the Lambda without actually invoking an HTTP request. So with this, yeah, there is some examples, but the application doesn't read anything from the request. It just responds with some hello world message. And if we execute it, you see it takes a bit of a time. And this is what I had in my slide. Here it's even shorter, 850 milliseconds. But if we try it again, it's already pre-warmed because AWS caches. Where is it here? Caches the environment. And now it's just two milliseconds. So now the question is when the cold starts happen. They happen. I don't have any experience. How much they have an impact. I heard that it's not much of an impact because they happen normally once in a while. So the response is once in a while, takes one more second on top of request processing. But if it takes five seconds, which can happen with normal spring boot application or traditional frameworks or even application service, I don't know, sometimes some application service can be embedded, then you can run them in AWS Lambda. But some of them really are hard to basically map to the method call. So you have to install application server. And for that, it's not even possible application to run application servers in Lambda. But if you did, it would take 10, 20 seconds with some application servers. And that's really a difference. You pay for the execution time, but you also have exposure users to waiting for a couple of seconds. If it's a user facing Lambda. If it's not, you maybe don't care so much. If it's something that's some bad job that takes two, three minutes to finish, then couple of seconds don't really matter. So here's a slide about Piranha Cloud. In short, Piranha Cloud is basically, as I said, based on a new servlet container designed from scratch, and a lot of components built on top of it. The servlet container being servlet implementation can run any servlet out there. And a lot of Jakarta technologies are created as servlets. So for example, Jersey as a servlet can be deployed on Piranha. And that's quite an easy way how to get rest endpoints or rest library on Piranha to deploy Jersey as a servlet. And then we have everything that Jersey provides. We can embed Piranha as I did in my demo, but we can also build a war application and run the war application with Piranha on command line. This is using Jakarta distributions, which already contain this distribution of packages, distribution of functionality of Piranha that are mostly used. And the last thing, it's plain Java. There's no real magic. There's no generated code. Everything is just clean code written by clever people, I think. At least judging on the code, when I looked at the code, it looks like the people were very clever. So with Piranha Cloud, we were able to achieve quite fast startup times, but it still takes a couple of milliseconds, 100, 200. It depends on how our application is complex. It may end up to two seconds even if we add all the Jakarta functionality that Piranha Cloud provides. If we want to reduce that even further, we have some general Java options to do that. We can first increase the CPU and RAM on the Lambda, which we can always do with any language. But we can also use a faster JVM. On the last slide, I have a table where I compared running the same application with Java 11 and Java 17. If you look at the numbers, Java 17 is mostly most of the time a bit faster. So just by deciding which Java version we use, we can get a bit better startup time. Then the last option here is basically a combination. I did some experiments which options work well regarding to startup time or reducing the startup time. And in the end, not many things matter. But what matters is class data sharing, which basically caches class information. So it doesn't have to be loaded and processed in the beginning. It's already pre-computed before cold start. And tinkering with compiler, we can disable second level just-in-time compiler if we want to really focus on startup time. And then there are other more magical options, but they can even reduce performance or reduce startup time almost to zero, either compiling the code to Gravium, with Gravium to a native binary which runs the application almost instantly. Or we can use Crack, which is a co-ordinated restore and checkpoint mechanism. The next talk will be about it also. And yeah, which is also nice is that AWS Lambda integrated that basically in one of their Java run times. And it's called snap start. So you can get it for free, but only with Java 11. But hopefully Java 17 support will be coming soon. And this works in a way that your application basically stores, or you at the build time can store a checkpoint of your application with all the memory or all the information basically like hibernates, you can hibernate your application. And then it started again and again and cold start and warm start in that case basically don't make a difference because they start from the same point. That's all from me. If you have any questions, let me know. Thank you for watching.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.8, "text": " I used to work at Pyra, so maybe some of you know me from my six years at Pyra company.", "tokens": [286, 1143, 281, 589, 412, 9953, 424, 11, 370, 1310, 512, 295, 291, 458, 385, 490, 452, 2309, 924, 412, 9953, 424, 2237, 13], "temperature": 0.0, "avg_logprob": -0.3220227206194842, "compression_ratio": 1.3543307086614174, "no_speech_prob": 0.5797407627105713}, {"id": 1, "seek": 0, "start": 17.8, "end": 26.64, "text": " Now I work at Omnifish, where we, with our co-founders and employees, were a support", "tokens": [823, 286, 589, 412, 9757, 77, 351, 742, 11, 689, 321, 11, 365, 527, 598, 12, 17493, 433, 293, 6619, 11, 645, 257, 1406], "temperature": 0.0, "avg_logprob": -0.3220227206194842, "compression_ratio": 1.3543307086614174, "no_speech_prob": 0.5797407627105713}, {"id": 2, "seek": 2664, "start": 26.64, "end": 34.8, "text": " glassfish server, so back to the roots, kind of. But this time I'd like to talk about Java,", "tokens": [4276, 11608, 7154, 11, 370, 646, 281, 264, 10669, 11, 733, 295, 13, 583, 341, 565, 286, 1116, 411, 281, 751, 466, 10745, 11], "temperature": 0.0, "avg_logprob": -0.2848855590820312, "compression_ratio": 1.2826086956521738, "no_speech_prob": 0.008598231710493565}, {"id": 3, "seek": 2664, "start": 34.8, "end": 45.2, "text": " plain Java and Jakarta E, and how it all fits together when we combine that with AWS.", "tokens": [11121, 10745, 293, 15029, 19061, 462, 11, 293, 577, 309, 439, 9001, 1214, 562, 321, 10432, 300, 365, 17650, 13], "temperature": 0.0, "avg_logprob": -0.2848855590820312, "compression_ratio": 1.2826086956521738, "no_speech_prob": 0.008598231710493565}, {"id": 4, "seek": 4520, "start": 45.2, "end": 56.24, "text": " So first, before I talked about AWS, let's ask, why do we want to have Java fast, or", "tokens": [407, 700, 11, 949, 286, 2825, 466, 17650, 11, 718, 311, 1029, 11, 983, 360, 321, 528, 281, 362, 10745, 2370, 11, 420], "temperature": 0.0, "avg_logprob": -0.15574052810668945, "compression_ratio": 1.3983739837398375, "no_speech_prob": 0.00030686549143865705}, {"id": 5, "seek": 4520, "start": 56.24, "end": 66.96000000000001, "text": " do we want to have Java start fast? I think everybody wants that, but why? Because it's", "tokens": [360, 321, 528, 281, 362, 10745, 722, 2370, 30, 286, 519, 2201, 2738, 300, 11, 457, 983, 30, 1436, 309, 311], "temperature": 0.0, "avg_logprob": -0.15574052810668945, "compression_ratio": 1.3983739837398375, "no_speech_prob": 0.00030686549143865705}, {"id": 6, "seek": 6696, "start": 66.96, "end": 74.88, "text": " cool, or because we need it. So there were times when we really didn't need that, when", "tokens": [1627, 11, 420, 570, 321, 643, 309, 13, 407, 456, 645, 1413, 562, 321, 534, 994, 380, 643, 300, 11, 562], "temperature": 0.0, "avg_logprob": -0.15642490170218729, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0018179035978391767}, {"id": 7, "seek": 6696, "start": 74.88, "end": 80.67999999999999, "text": " we had the application servers, it was a pain that it took a while to start, but in production", "tokens": [321, 632, 264, 3861, 15909, 11, 309, 390, 257, 1822, 300, 309, 1890, 257, 1339, 281, 722, 11, 457, 294, 4265], "temperature": 0.0, "avg_logprob": -0.15642490170218729, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0018179035978391767}, {"id": 8, "seek": 6696, "start": 80.67999999999999, "end": 86.88, "text": " it was already running, so there was no real business need for that, only to make developers", "tokens": [309, 390, 1217, 2614, 11, 370, 456, 390, 572, 957, 1606, 643, 337, 300, 11, 787, 281, 652, 8849], "temperature": 0.0, "avg_logprob": -0.15642490170218729, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0018179035978391767}, {"id": 9, "seek": 6696, "start": 86.88, "end": 93.91999999999999, "text": " happy and be more productive with developing codes. But now we have several use cases where", "tokens": [2055, 293, 312, 544, 13304, 365, 6416, 14211, 13, 583, 586, 321, 362, 2940, 764, 3331, 689], "temperature": 0.0, "avg_logprob": -0.15642490170218729, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0018179035978391767}, {"id": 10, "seek": 9392, "start": 93.92, "end": 102.0, "text": " it's really needed, because the more time it takes for Java program to start, it costs", "tokens": [309, 311, 534, 2978, 11, 570, 264, 544, 565, 309, 2516, 337, 10745, 1461, 281, 722, 11, 309, 5497], "temperature": 0.0, "avg_logprob": -0.148641862731049, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0026765100192278624}, {"id": 11, "seek": 9392, "start": 102.0, "end": 108.12, "text": " more money, and it's not user-friendly. And one example, a perfect example of this is", "tokens": [544, 1460, 11, 293, 309, 311, 406, 4195, 12, 22864, 13, 400, 472, 1365, 11, 257, 2176, 1365, 295, 341, 307], "temperature": 0.0, "avg_logprob": -0.148641862731049, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0026765100192278624}, {"id": 12, "seek": 9392, "start": 108.12, "end": 116.48, "text": " AWS Lambda. So now, what is AWS Lambda? It's basically a service to which you can deploy", "tokens": [17650, 45691, 13, 407, 586, 11, 437, 307, 17650, 45691, 30, 467, 311, 1936, 257, 2643, 281, 597, 291, 393, 7274], "temperature": 0.0, "avg_logprob": -0.148641862731049, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0026765100192278624}, {"id": 13, "seek": 11648, "start": 116.48, "end": 124.08, "text": " your code, and this service runs your code only when it's needed, and it also charges", "tokens": [428, 3089, 11, 293, 341, 2643, 6676, 428, 3089, 787, 562, 309, 311, 2978, 11, 293, 309, 611, 12235], "temperature": 0.0, "avg_logprob": -0.1450204849243164, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.0007838633027859032}, {"id": 14, "seek": 11648, "start": 124.08, "end": 132.0, "text": " you, because we need to pay for the cloud environment. But if we run the code in Lambda,", "tokens": [291, 11, 570, 321, 643, 281, 1689, 337, 264, 4588, 2823, 13, 583, 498, 321, 1190, 264, 3089, 294, 45691, 11], "temperature": 0.0, "avg_logprob": -0.1450204849243164, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.0007838633027859032}, {"id": 15, "seek": 11648, "start": 132.0, "end": 138.12, "text": " we are charged only for the time when the code is running. And that's pretty nice, especially", "tokens": [321, 366, 11109, 787, 337, 264, 565, 562, 264, 3089, 307, 2614, 13, 400, 300, 311, 1238, 1481, 11, 2318], "temperature": 0.0, "avg_logprob": -0.1450204849243164, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.0007838633027859032}, {"id": 16, "seek": 11648, "start": 138.12, "end": 146.4, "text": " if we have code that usually just sits there and responds to users just once in a while,", "tokens": [498, 321, 362, 3089, 300, 2673, 445, 12696, 456, 293, 27331, 281, 5022, 445, 1564, 294, 257, 1339, 11], "temperature": 0.0, "avg_logprob": -0.1450204849243164, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.0007838633027859032}, {"id": 17, "seek": 14640, "start": 146.4, "end": 151.56, "text": " or only during certain periods of time, especially during the day or in the morning when there", "tokens": [420, 787, 1830, 1629, 13804, 295, 565, 11, 2318, 1830, 264, 786, 420, 294, 264, 2446, 562, 456], "temperature": 0.0, "avg_logprob": -0.10792415792291815, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0008504578727297485}, {"id": 18, "seek": 14640, "start": 151.56, "end": 161.24, "text": " is some business activity. So how does AWS Lambda do that? It basically creates environment", "tokens": [307, 512, 1606, 5191, 13, 407, 577, 775, 17650, 45691, 360, 300, 30, 467, 1936, 7829, 2823], "temperature": 0.0, "avg_logprob": -0.10792415792291815, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0008504578727297485}, {"id": 19, "seek": 14640, "start": 161.24, "end": 168.48000000000002, "text": " and deploys our code when it needs to be executed. And for that, if the code is not already deployed,", "tokens": [293, 368, 49522, 527, 3089, 562, 309, 2203, 281, 312, 17577, 13, 400, 337, 300, 11, 498, 264, 3089, 307, 406, 1217, 17826, 11], "temperature": 0.0, "avg_logprob": -0.10792415792291815, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0008504578727297485}, {"id": 20, "seek": 14640, "start": 168.48000000000002, "end": 175.92000000000002, "text": " it needs to create the runtime and initialize our so-called function, because this is how", "tokens": [309, 2203, 281, 1884, 264, 34474, 293, 5883, 1125, 527, 370, 12, 11880, 2445, 11, 570, 341, 307, 577], "temperature": 0.0, "avg_logprob": -0.10792415792291815, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0008504578727297485}, {"id": 21, "seek": 17592, "start": 175.92, "end": 180.88, "text": " our code is called. It's called a function because it's basically just called by the", "tokens": [527, 3089, 307, 1219, 13, 467, 311, 1219, 257, 2445, 570, 309, 311, 1936, 445, 1219, 538, 264], "temperature": 0.0, "avg_logprob": -0.1381580331823328, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.001047002850100398}, {"id": 22, "seek": 17592, "start": 180.88, "end": 188.04, "text": " runtime, it gives some result, and then it's thrown away. In reality, it's not always thrown", "tokens": [34474, 11, 309, 2709, 512, 1874, 11, 293, 550, 309, 311, 11732, 1314, 13, 682, 4103, 11, 309, 311, 406, 1009, 11732], "temperature": 0.0, "avg_logprob": -0.1381580331823328, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.001047002850100398}, {"id": 23, "seek": 17592, "start": 188.04, "end": 196.16, "text": " away because AWS Lambda tries to cache our code so that it doesn't have to re-initialize", "tokens": [1314, 570, 17650, 45691, 9898, 281, 19459, 527, 3089, 370, 300, 309, 1177, 380, 362, 281, 319, 12, 259, 270, 831, 1125], "temperature": 0.0, "avg_logprob": -0.1381580331823328, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.001047002850100398}, {"id": 24, "seek": 17592, "start": 196.16, "end": 204.88, "text": " it every time when it's run more frequently. So sometimes it stays there, and then AWS", "tokens": [309, 633, 565, 562, 309, 311, 1190, 544, 10374, 13, 407, 2171, 309, 10834, 456, 11, 293, 550, 17650], "temperature": 0.0, "avg_logprob": -0.1381580331823328, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.001047002850100398}, {"id": 25, "seek": 20488, "start": 204.88, "end": 209.96, "text": " Lambda can skip the initialization phase. This is called warm start, because the code", "tokens": [45691, 393, 10023, 264, 5883, 2144, 5574, 13, 639, 307, 1219, 4561, 722, 11, 570, 264, 3089], "temperature": 0.0, "avg_logprob": -0.14100830975700826, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.0015546241775155067}, {"id": 26, "seek": 20488, "start": 209.96, "end": 217.07999999999998, "text": " is already prepared to serve things. But if this doesn't happen, and the code is not available,", "tokens": [307, 1217, 4927, 281, 4596, 721, 13, 583, 498, 341, 1177, 380, 1051, 11, 293, 264, 3089, 307, 406, 2435, 11], "temperature": 0.0, "avg_logprob": -0.14100830975700826, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.0015546241775155067}, {"id": 27, "seek": 20488, "start": 217.07999999999998, "end": 222.07999999999998, "text": " it has to initialize everything. And this is usually referred to as cold start, just", "tokens": [309, 575, 281, 5883, 1125, 1203, 13, 400, 341, 307, 2673, 10839, 281, 382, 3554, 722, 11, 445], "temperature": 0.0, "avg_logprob": -0.14100830975700826, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.0015546241775155067}, {"id": 28, "seek": 20488, "start": 222.07999999999998, "end": 229.04, "text": " start from scratch. So the whole lifecycle of AWS Lambda is as on the slide, you can", "tokens": [722, 490, 8459, 13, 407, 264, 1379, 45722, 295, 17650, 45691, 307, 382, 322, 264, 4137, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.14100830975700826, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.0015546241775155067}, {"id": 29, "seek": 22904, "start": 229.04, "end": 235.32, "text": " see there's init phase. This is only when the code or the function is not initialized.", "tokens": [536, 456, 311, 3157, 5574, 13, 639, 307, 787, 562, 264, 3089, 420, 264, 2445, 307, 406, 5883, 1602, 13], "temperature": 0.0, "avg_logprob": -0.15684726320464035, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0014198292046785355}, {"id": 30, "seek": 22904, "start": 235.32, "end": 244.6, "text": " So in case of cold start, then there is a warm phase, which happens even for warm start-ups.", "tokens": [407, 294, 1389, 295, 3554, 722, 11, 550, 456, 307, 257, 4561, 5574, 11, 597, 2314, 754, 337, 4561, 722, 12, 7528, 13], "temperature": 0.0, "avg_logprob": -0.15684726320464035, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0014198292046785355}, {"id": 31, "seek": 22904, "start": 244.6, "end": 250.84, "text": " There is this invoke phase, which actually is the only productive phase from these three.", "tokens": [821, 307, 341, 41117, 5574, 11, 597, 767, 307, 264, 787, 13304, 5574, 490, 613, 1045, 13], "temperature": 0.0, "avg_logprob": -0.15684726320464035, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0014198292046785355}, {"id": 32, "seek": 22904, "start": 250.84, "end": 257.24, "text": " It actually does some job. The first phase only initialized gets some things ready before", "tokens": [467, 767, 775, 512, 1691, 13, 440, 700, 5574, 787, 5883, 1602, 2170, 512, 721, 1919, 949], "temperature": 0.0, "avg_logprob": -0.15684726320464035, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0014198292046785355}, {"id": 33, "seek": 25724, "start": 257.24, "end": 263.2, "text": " the application can process requests. Then the invoke phase does the job. And then when", "tokens": [264, 3861, 393, 1399, 12475, 13, 1396, 264, 41117, 5574, 775, 264, 1691, 13, 400, 550, 562], "temperature": 0.0, "avg_logprob": -0.20057959672881337, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00225779484026134}, {"id": 34, "seek": 25724, "start": 263.2, "end": 270.12, "text": " AWS Lambda service decides that it doesn't no longer needs our application running, because", "tokens": [17650, 45691, 2643, 14898, 300, 309, 1177, 380, 572, 2854, 2203, 527, 3861, 2614, 11, 570], "temperature": 0.0, "avg_logprob": -0.20057959672881337, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00225779484026134}, {"id": 35, "seek": 25724, "start": 270.12, "end": 277.92, "text": " it's not doing anything right now, and they need AWS wants to use resources in some other", "tokens": [309, 311, 406, 884, 1340, 558, 586, 11, 293, 436, 643, 17650, 2738, 281, 764, 3593, 294, 512, 661], "temperature": 0.0, "avg_logprob": -0.20057959672881337, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00225779484026134}, {"id": 36, "seek": 25724, "start": 277.92, "end": 285.40000000000003, "text": " way, it will tear everything away. So it will shut down the environment. And then we'll", "tokens": [636, 11, 309, 486, 12556, 1203, 1314, 13, 407, 309, 486, 5309, 760, 264, 2823, 13, 400, 550, 321, 603], "temperature": 0.0, "avg_logprob": -0.20057959672881337, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00225779484026134}, {"id": 37, "seek": 28540, "start": 285.4, "end": 294.47999999999996, "text": " add square at square zero. And next invocation needs to go through the initial initialization", "tokens": [909, 3732, 412, 3732, 4018, 13, 400, 958, 1048, 27943, 2203, 281, 352, 807, 264, 5883, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.1783705310545106, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.0015826537273824215}, {"id": 38, "seek": 28540, "start": 294.47999999999996, "end": 305.84, "text": " phase. So let's not go back to the roots with plain Java application. And let's see or let's", "tokens": [5574, 13, 407, 718, 311, 406, 352, 646, 281, 264, 10669, 365, 11121, 10745, 3861, 13, 400, 718, 311, 536, 420, 718, 311], "temperature": 0.0, "avg_logprob": -0.1783705310545106, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.0015826537273824215}, {"id": 39, "seek": 28540, "start": 305.84, "end": 314.28, "text": " think about how fast we can get with Java on AWS Lambda. Can we start Java really fast?", "tokens": [519, 466, 577, 2370, 321, 393, 483, 365, 10745, 322, 17650, 45691, 13, 1664, 321, 722, 10745, 534, 2370, 30], "temperature": 0.0, "avg_logprob": -0.1783705310545106, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.0015826537273824215}, {"id": 40, "seek": 31428, "start": 314.28, "end": 322.91999999999996, "text": " I tried to start a very simple Java program on my local machine. And if you do that too", "tokens": [286, 3031, 281, 722, 257, 588, 2199, 10745, 1461, 322, 452, 2654, 3479, 13, 400, 498, 291, 360, 300, 886], "temperature": 0.0, "avg_logprob": -0.15259993416922432, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.003156401449814439}, {"id": 41, "seek": 31428, "start": 322.91999999999996, "end": 329.15999999999997, "text": " on your computers, you will see that Java really starts fast. In my case, it was 50", "tokens": [322, 428, 10807, 11, 291, 486, 536, 300, 10745, 534, 3719, 2370, 13, 682, 452, 1389, 11, 309, 390, 2625], "temperature": 0.0, "avg_logprob": -0.15259993416922432, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.003156401449814439}, {"id": 42, "seek": 31428, "start": 329.15999999999997, "end": 340.28, "text": " milliseconds, 0.05 seconds. So very small fraction of a second, where JVM started, printed", "tokens": [34184, 11, 1958, 13, 13328, 3949, 13, 407, 588, 1359, 14135, 295, 257, 1150, 11, 689, 508, 53, 44, 1409, 11, 13567], "temperature": 0.0, "avg_logprob": -0.15259993416922432, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.003156401449814439}, {"id": 43, "seek": 34028, "start": 340.28, "end": 347.2, "text": " something on output and finished. So we see on a local computer, plain Java doesn't start,", "tokens": [746, 322, 5598, 293, 4335, 13, 407, 321, 536, 322, 257, 2654, 3820, 11, 11121, 10745, 1177, 380, 722, 11], "temperature": 0.0, "avg_logprob": -0.18027171869387573, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0011562105501070619}, {"id": 44, "seek": 34028, "start": 347.2, "end": 357.03999999999996, "text": " doesn't take very long to start. If we compare the exactly what's going on in the AWS Lambda,", "tokens": [1177, 380, 747, 588, 938, 281, 722, 13, 759, 321, 6794, 264, 2293, 437, 311, 516, 322, 294, 264, 17650, 45691, 11], "temperature": 0.0, "avg_logprob": -0.18027171869387573, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0011562105501070619}, {"id": 45, "seek": 34028, "start": 357.03999999999996, "end": 362.44, "text": " because AWS Lambda needs to initialize the environment and only then it can run Java", "tokens": [570, 17650, 45691, 2203, 281, 5883, 1125, 264, 2823, 293, 787, 550, 309, 393, 1190, 10745], "temperature": 0.0, "avg_logprob": -0.18027171869387573, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0011562105501070619}, {"id": 46, "seek": 34028, "start": 362.44, "end": 368.0, "text": " function. It takes a bit longer in reality. But when we compare it to other languages,", "tokens": [2445, 13, 467, 2516, 257, 857, 2854, 294, 4103, 13, 583, 562, 321, 6794, 309, 281, 661, 8650, 11], "temperature": 0.0, "avg_logprob": -0.18027171869387573, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0011562105501070619}, {"id": 47, "seek": 36800, "start": 368.0, "end": 372.76, "text": " I haven't done this. This is done by some other guy who is more experienced with AWS", "tokens": [286, 2378, 380, 1096, 341, 13, 639, 307, 1096, 538, 512, 661, 2146, 567, 307, 544, 6751, 365, 17650], "temperature": 0.0, "avg_logprob": -0.1675328341397372, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0045686326920986176}, {"id": 48, "seek": 36800, "start": 372.76, "end": 379.76, "text": " Lambda than me and compared performance in a more sophisticated way than just running", "tokens": [45691, 813, 385, 293, 5347, 3389, 294, 257, 544, 16950, 636, 813, 445, 2614], "temperature": 0.0, "avg_logprob": -0.1675328341397372, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0045686326920986176}, {"id": 49, "seek": 36800, "start": 379.76, "end": 387.6, "text": " on the computer or just several measurements. He did a lot of measurements across all the", "tokens": [322, 264, 3820, 420, 445, 2940, 15383, 13, 634, 630, 257, 688, 295, 15383, 2108, 439, 264], "temperature": 0.0, "avg_logprob": -0.1675328341397372, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0045686326920986176}, {"id": 50, "seek": 36800, "start": 387.6, "end": 395.36, "text": " or various different languages, various different runtimes provided by AWS Lambda. And he found", "tokens": [420, 3683, 819, 8650, 11, 3683, 819, 49435, 1532, 5649, 538, 17650, 45691, 13, 400, 415, 1352], "temperature": 0.0, "avg_logprob": -0.1675328341397372, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0045686326920986176}, {"id": 51, "seek": 39536, "start": 395.36, "end": 401.40000000000003, "text": " out that Java basically is on the same level as JavaScript, Python and a lot of other languages", "tokens": [484, 300, 10745, 1936, 307, 322, 264, 912, 1496, 382, 15778, 11, 15329, 293, 257, 688, 295, 661, 8650], "temperature": 0.0, "avg_logprob": -0.1498294985571573, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.0015618914039805532}, {"id": 52, "seek": 39536, "start": 401.40000000000003, "end": 405.88, "text": " that there's not much difference. There's a small difference that at that time C sharp", "tokens": [300, 456, 311, 406, 709, 2649, 13, 821, 311, 257, 1359, 2649, 300, 412, 300, 565, 383, 8199], "temperature": 0.0, "avg_logprob": -0.1498294985571573, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.0015618914039805532}, {"id": 53, "seek": 39536, "start": 405.88, "end": 413.96000000000004, "text": " was a bit slower. But as AWS improves continually, the AWS Lambda, even these numbers would", "tokens": [390, 257, 857, 14009, 13, 583, 382, 17650, 24771, 22277, 11, 264, 17650, 45691, 11, 754, 613, 3547, 576], "temperature": 0.0, "avg_logprob": -0.1498294985571573, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.0015618914039805532}, {"id": 54, "seek": 39536, "start": 413.96000000000004, "end": 420.24, "text": " be probably better now. And C sharp and Docker will be maybe more even with the with the rest", "tokens": [312, 1391, 1101, 586, 13, 400, 383, 8199, 293, 33772, 486, 312, 1310, 544, 754, 365, 264, 365, 264, 1472], "temperature": 0.0, "avg_logprob": -0.1498294985571573, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.0015618914039805532}, {"id": 55, "seek": 42024, "start": 420.24, "end": 426.36, "text": " because the technology running AWS Lambda is continuously improving. But this is just", "tokens": [570, 264, 2899, 2614, 17650, 45691, 307, 15684, 11470, 13, 583, 341, 307, 445], "temperature": 0.0, "avg_logprob": -0.1640610276607045, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.0002782859082799405}, {"id": 56, "seek": 42024, "start": 426.36, "end": 434.76, "text": " to compare and show that Java itself or even the implementation of Java AWS function or", "tokens": [281, 6794, 293, 855, 300, 10745, 2564, 420, 754, 264, 11420, 295, 10745, 17650, 2445, 420], "temperature": 0.0, "avg_logprob": -0.1640610276607045, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.0002782859082799405}, {"id": 57, "seek": 42024, "start": 434.76, "end": 443.92, "text": " the environment isn't worse than other languages. So now what is the problem actually? Why a", "tokens": [264, 2823, 1943, 380, 5324, 813, 661, 8650, 13, 407, 586, 437, 307, 264, 1154, 767, 30, 1545, 257], "temperature": 0.0, "avg_logprob": -0.1640610276607045, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.0002782859082799405}, {"id": 58, "seek": 44392, "start": 443.92, "end": 452.04, "text": " lot of people perceive that Java starts very slow. The problem is how I see it is that", "tokens": [688, 295, 561, 20281, 300, 10745, 3719, 588, 2964, 13, 440, 1154, 307, 577, 286, 536, 309, 307, 300], "temperature": 0.0, "avg_logprob": -0.155167809451919, "compression_ratio": 1.7715736040609138, "no_speech_prob": 0.0005629771621897817}, {"id": 59, "seek": 44392, "start": 452.04, "end": 458.72, "text": " many people don't think about Java in this simple way that it's a simple application.", "tokens": [867, 561, 500, 380, 519, 466, 10745, 294, 341, 2199, 636, 300, 309, 311, 257, 2199, 3861, 13], "temperature": 0.0, "avg_logprob": -0.155167809451919, "compression_ratio": 1.7715736040609138, "no_speech_prob": 0.0005629771621897817}, {"id": 60, "seek": 44392, "start": 458.72, "end": 465.20000000000005, "text": " A lot of people think about Java as a language that runs enterprise applications. And with", "tokens": [316, 688, 295, 561, 519, 466, 10745, 382, 257, 2856, 300, 6676, 14132, 5821, 13, 400, 365], "temperature": 0.0, "avg_logprob": -0.155167809451919, "compression_ratio": 1.7715736040609138, "no_speech_prob": 0.0005629771621897817}, {"id": 61, "seek": 44392, "start": 465.20000000000005, "end": 471.28000000000003, "text": " enterprise applications, we're used to use frameworks that do a lot of job for us. We", "tokens": [14132, 5821, 11, 321, 434, 1143, 281, 764, 29834, 300, 360, 257, 688, 295, 1691, 337, 505, 13, 492], "temperature": 0.0, "avg_logprob": -0.155167809451919, "compression_ratio": 1.7715736040609138, "no_speech_prob": 0.0005629771621897817}, {"id": 62, "seek": 47128, "start": 471.28, "end": 476.44, "text": " run the applications on application servers, which are start to which are slow to start.", "tokens": [1190, 264, 5821, 322, 3861, 15909, 11, 597, 366, 722, 281, 597, 366, 2964, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.10288998213681308, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.001072179526090622}, {"id": 63, "seek": 47128, "start": 476.44, "end": 483.4, "text": " And this is what we think about when we think when we say Java or when we talk about Java.", "tokens": [400, 341, 307, 437, 321, 519, 466, 562, 321, 519, 562, 321, 584, 10745, 420, 562, 321, 751, 466, 10745, 13], "temperature": 0.0, "avg_logprob": -0.10288998213681308, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.001072179526090622}, {"id": 64, "seek": 47128, "start": 483.4, "end": 491.88, "text": " So now we're coming to that. That thing that if we basically can run our applications that", "tokens": [407, 586, 321, 434, 1348, 281, 300, 13, 663, 551, 300, 498, 321, 1936, 393, 1190, 527, 5821, 300], "temperature": 0.0, "avg_logprob": -0.10288998213681308, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.001072179526090622}, {"id": 65, "seek": 47128, "start": 491.88, "end": 500.55999999999995, "text": " are similar to what we were used to before, but if we can start them fast, we could solve", "tokens": [366, 2531, 281, 437, 321, 645, 1143, 281, 949, 11, 457, 498, 321, 393, 722, 552, 2370, 11, 321, 727, 5039], "temperature": 0.0, "avg_logprob": -0.10288998213681308, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.001072179526090622}, {"id": 66, "seek": 50056, "start": 500.56, "end": 508.0, "text": " a problem with Java call starts as least as we use Java now. So the question I have now", "tokens": [257, 1154, 365, 10745, 818, 3719, 382, 1935, 382, 321, 764, 10745, 586, 13, 407, 264, 1168, 286, 362, 586], "temperature": 0.0, "avg_logprob": -0.19032559028038612, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.002070406451821327}, {"id": 67, "seek": 50056, "start": 508.0, "end": 514.8, "text": " is Jakarta EE or some other frameworks like Springboard or something like that. Can that", "tokens": [307, 15029, 19061, 33685, 420, 512, 661, 29834, 411, 14013, 3787, 420, 746, 411, 300, 13, 1664, 300], "temperature": 0.0, "avg_logprob": -0.19032559028038612, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.002070406451821327}, {"id": 68, "seek": 50056, "start": 514.8, "end": 524.72, "text": " be as fast as plain Java? Can we run that in AWS Lambda to get good performance and", "tokens": [312, 382, 2370, 382, 11121, 10745, 30, 1664, 321, 1190, 300, 294, 17650, 45691, 281, 483, 665, 3389, 293], "temperature": 0.0, "avg_logprob": -0.19032559028038612, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.002070406451821327}, {"id": 69, "seek": 52472, "start": 524.72, "end": 533.08, "text": " fast startup? And the answer is there are such frameworks and solutions to that. There", "tokens": [2370, 18578, 30, 400, 264, 1867, 307, 456, 366, 1270, 29834, 293, 6547, 281, 300, 13, 821], "temperature": 0.0, "avg_logprob": -0.13718224995171846, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0011226257774978876}, {"id": 70, "seek": 52472, "start": 533.08, "end": 538.8000000000001, "text": " are several ones. I don't have much time to talk about all of them. So I picked one that", "tokens": [366, 2940, 2306, 13, 286, 500, 380, 362, 709, 565, 281, 751, 466, 439, 295, 552, 13, 407, 286, 6183, 472, 300], "temperature": 0.0, "avg_logprob": -0.13718224995171846, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0011226257774978876}, {"id": 71, "seek": 52472, "start": 538.8000000000001, "end": 545.5600000000001, "text": " I personally like. And it's called Piranha Cloud Framework. And this one is based on", "tokens": [286, 5665, 411, 13, 400, 309, 311, 1219, 24161, 37899, 8061, 31628, 1902, 13, 400, 341, 472, 307, 2361, 322], "temperature": 0.0, "avg_logprob": -0.13718224995171846, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0011226257774978876}, {"id": 72, "seek": 54556, "start": 545.56, "end": 555.64, "text": " entirely Jakarta EE APIs. Previously, it was called Java EE. So it's a very well-known", "tokens": [7696, 15029, 19061, 33685, 21445, 13, 33606, 11, 309, 390, 1219, 10745, 33685, 13, 407, 309, 311, 257, 588, 731, 12, 6861], "temperature": 0.0, "avg_logprob": -0.20552824584531112, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0024483322631567717}, {"id": 73, "seek": 54556, "start": 555.64, "end": 562.1999999999999, "text": " API that a lot of people already know, a lot of tools out there already use. So it's interoperable", "tokens": [9362, 300, 257, 688, 295, 561, 1217, 458, 11, 257, 688, 295, 3873, 484, 456, 1217, 764, 13, 407, 309, 311, 728, 7192, 712], "temperature": 0.0, "avg_logprob": -0.20552824584531112, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0024483322631567717}, {"id": 74, "seek": 54556, "start": 562.1999999999999, "end": 569.64, "text": " with existing codebase. But the thing with Piranha Cloud is that the implementation actually", "tokens": [365, 6741, 3089, 17429, 13, 583, 264, 551, 365, 24161, 37899, 8061, 307, 300, 264, 11420, 767], "temperature": 0.0, "avg_logprob": -0.20552824584531112, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0024483322631567717}, {"id": 75, "seek": 56964, "start": 569.64, "end": 578.76, "text": " the engine of the framework is new, very flexible, and allows our application to stop, start", "tokens": [264, 2848, 295, 264, 8388, 307, 777, 11, 588, 11358, 11, 293, 4045, 527, 3861, 281, 1590, 11, 722], "temperature": 0.0, "avg_logprob": -0.1204769648038424, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0007185881258919835}, {"id": 76, "seek": 56964, "start": 578.76, "end": 586.92, "text": " very fast. Piranha Cloud is based on a lot of existing components. A lot of them come", "tokens": [588, 2370, 13, 24161, 37899, 8061, 307, 2361, 322, 257, 688, 295, 6741, 6677, 13, 316, 688, 295, 552, 808], "temperature": 0.0, "avg_logprob": -0.1204769648038424, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0007185881258919835}, {"id": 77, "seek": 56964, "start": 586.92, "end": 594.2, "text": " from the Glassfish server, which actually sort of proves that the server is not a problem", "tokens": [490, 264, 23752, 11608, 7154, 11, 597, 767, 1333, 295, 25019, 300, 264, 7154, 307, 406, 257, 1154], "temperature": 0.0, "avg_logprob": -0.1204769648038424, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.0007185881258919835}, {"id": 78, "seek": 59420, "start": 594.2, "end": 600.0400000000001, "text": " or Jakarta EE is not a problem. The components are there, they are quite fast. But the problem", "tokens": [420, 15029, 19061, 33685, 307, 406, 257, 1154, 13, 440, 6677, 366, 456, 11, 436, 366, 1596, 2370, 13, 583, 264, 1154], "temperature": 0.0, "avg_logprob": -0.13238651411873953, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00033894306397996843}, {"id": 79, "seek": 59420, "start": 600.0400000000001, "end": 605.6800000000001, "text": " how they are assembled in traditional Jakarta EE servers, Java EE application servers,", "tokens": [577, 436, 366, 24204, 294, 5164, 15029, 19061, 33685, 15909, 11, 10745, 33685, 3861, 15909, 11], "temperature": 0.0, "avg_logprob": -0.13238651411873953, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00033894306397996843}, {"id": 80, "seek": 59420, "start": 605.6800000000001, "end": 610.08, "text": " that is the problem. Because an application server usually has a lot of other things that", "tokens": [300, 307, 264, 1154, 13, 1436, 364, 3861, 7154, 2673, 575, 257, 688, 295, 661, 721, 300], "temperature": 0.0, "avg_logprob": -0.13238651411873953, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00033894306397996843}, {"id": 81, "seek": 59420, "start": 610.08, "end": 618.88, "text": " we don't need in Lambda, like monitoring a lot of vendor features and go on an administration", "tokens": [321, 500, 380, 643, 294, 45691, 11, 411, 11028, 257, 688, 295, 24321, 4122, 293, 352, 322, 364, 7236], "temperature": 0.0, "avg_logprob": -0.13238651411873953, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.00033894306397996843}, {"id": 82, "seek": 61888, "start": 618.88, "end": 629.48, "text": " console and a lot of other things. So here is an example, it's basically nothing", "tokens": [11076, 293, 257, 688, 295, 661, 721, 13, 407, 510, 307, 364, 1365, 11, 309, 311, 1936, 1825], "temperature": 0.0, "avg_logprob": -0.1603149226014043, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.000662574078887701}, {"id": 83, "seek": 61888, "start": 629.48, "end": 638.16, "text": " else than a servlet. But this is an application using already some Jakarta EE APIs. And this", "tokens": [1646, 813, 257, 1658, 2631, 13, 583, 341, 307, 364, 3861, 1228, 1217, 512, 15029, 19061, 33685, 21445, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.1603149226014043, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.000662574078887701}, {"id": 84, "seek": 61888, "start": 638.16, "end": 645.76, "text": " application, this servlet, you can run on any Jakarta EE server. You can run it on Tomcat,", "tokens": [3861, 11, 341, 1658, 2631, 11, 291, 393, 1190, 322, 604, 15029, 19061, 33685, 7154, 13, 509, 393, 1190, 309, 322, 5041, 18035, 11], "temperature": 0.0, "avg_logprob": -0.1603149226014043, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.000662574078887701}, {"id": 85, "seek": 64576, "start": 645.76, "end": 651.08, "text": " you can run it on Glassfish, you can run it on anything that supports servlets. So the", "tokens": [291, 393, 1190, 309, 322, 23752, 11608, 11, 291, 393, 1190, 309, 322, 1340, 300, 9346, 1658, 12541, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.14564079242748218, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0008216374553740025}, {"id": 86, "seek": 64576, "start": 651.08, "end": 657.2, "text": " only difference if we run it with Piranha Cloud is that it starts fast and it uses Piranha's", "tokens": [787, 2649, 498, 321, 1190, 309, 365, 24161, 37899, 8061, 307, 300, 309, 3719, 2370, 293, 309, 4960, 24161, 37899, 311], "temperature": 0.0, "avg_logprob": -0.14564079242748218, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0008216374553740025}, {"id": 87, "seek": 64576, "start": 657.2, "end": 665.92, "text": " own servlet container, which was designed from scratch. And it's very flexible and fast.", "tokens": [1065, 1658, 2631, 10129, 11, 597, 390, 4761, 490, 8459, 13, 400, 309, 311, 588, 11358, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.14564079242748218, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0008216374553740025}, {"id": 88, "seek": 64576, "start": 665.92, "end": 674.24, "text": " What is also nice about Piranha's servlet container is that it can be embedded very easily.", "tokens": [708, 307, 611, 1481, 466, 24161, 37899, 311, 1658, 2631, 10129, 307, 300, 309, 393, 312, 16741, 588, 3612, 13], "temperature": 0.0, "avg_logprob": -0.14564079242748218, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0008216374553740025}, {"id": 89, "seek": 67424, "start": 674.24, "end": 681.6800000000001, "text": " And that's the crucial point. When we want to use Jakarta EE in Lambda, we need to basically", "tokens": [400, 300, 311, 264, 11462, 935, 13, 1133, 321, 528, 281, 764, 15029, 19061, 33685, 294, 45691, 11, 321, 643, 281, 1936], "temperature": 0.0, "avg_logprob": -0.1358079798081342, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.002556954277679324}, {"id": 90, "seek": 67424, "start": 681.6800000000001, "end": 687.5600000000001, "text": " shave off everything that we don't need. And in AWS Lambda, we don't even need an HTTP", "tokens": [25544, 766, 1203, 300, 321, 500, 380, 643, 13, 400, 294, 17650, 45691, 11, 321, 500, 380, 754, 643, 364, 33283], "temperature": 0.0, "avg_logprob": -0.1358079798081342, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.002556954277679324}, {"id": 91, "seek": 67424, "start": 687.5600000000001, "end": 694.8, "text": " listener. Because AWS Lambda basically only wants a method from us that will be called,", "tokens": [31569, 13, 1436, 17650, 45691, 1936, 787, 2738, 257, 3170, 490, 505, 300, 486, 312, 1219, 11], "temperature": 0.0, "avg_logprob": -0.1358079798081342, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.002556954277679324}, {"id": 92, "seek": 67424, "start": 694.8, "end": 700.04, "text": " returns some response. And then AWS Lambda is responsible for mapping the HTTP request", "tokens": [11247, 512, 4134, 13, 400, 550, 17650, 45691, 307, 6250, 337, 18350, 264, 33283, 5308], "temperature": 0.0, "avg_logprob": -0.1358079798081342, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.002556954277679324}, {"id": 93, "seek": 70004, "start": 700.04, "end": 706.4, "text": " to an object that it passes to our method. And then the returned object should be mapped", "tokens": [281, 364, 2657, 300, 309, 11335, 281, 527, 3170, 13, 400, 550, 264, 8752, 2657, 820, 312, 33318], "temperature": 0.0, "avg_logprob": -0.1245269775390625, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0013678313698619604}, {"id": 94, "seek": 70004, "start": 706.4, "end": 712.4, "text": " to an HTTP response. And not only HTTP requests and responses, but Lambda can handle any type", "tokens": [281, 364, 33283, 4134, 13, 400, 406, 787, 33283, 12475, 293, 13019, 11, 457, 45691, 393, 4813, 604, 2010], "temperature": 0.0, "avg_logprob": -0.1245269775390625, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0013678313698619604}, {"id": 95, "seek": 70004, "start": 712.4, "end": 719.64, "text": " of basically JSON messages, JSON events. So the only thing that our application needs", "tokens": [295, 1936, 31828, 7897, 11, 31828, 3931, 13, 407, 264, 787, 551, 300, 527, 3861, 2203], "temperature": 0.0, "avg_logprob": -0.1245269775390625, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0013678313698619604}, {"id": 96, "seek": 71964, "start": 719.64, "end": 732.1999999999999, "text": " is to parse some input object and return some output event. And with Piranha, we can create", "tokens": [307, 281, 48377, 512, 4846, 2657, 293, 2736, 512, 5598, 2280, 13, 400, 365, 24161, 37899, 11, 321, 393, 1884], "temperature": 0.0, "avg_logprob": -0.1059811380174425, "compression_ratio": 1.408, "no_speech_prob": 0.0028482081834226847}, {"id": 97, "seek": 71964, "start": 732.1999999999999, "end": 741.76, "text": " an engine and map our servlet onto it and just listen on some object. This object is", "tokens": [364, 2848, 293, 4471, 527, 1658, 2631, 3911, 309, 293, 445, 2140, 322, 512, 2657, 13, 639, 2657, 307], "temperature": 0.0, "avg_logprob": -0.1059811380174425, "compression_ratio": 1.408, "no_speech_prob": 0.0028482081834226847}, {"id": 98, "seek": 74176, "start": 741.76, "end": 750.68, "text": " usually called or the request response cycle is invoked by a service method, which accepts", "tokens": [2673, 1219, 420, 264, 5308, 4134, 6586, 307, 1048, 9511, 538, 257, 2643, 3170, 11, 597, 33538], "temperature": 0.0, "avg_logprob": -0.15472739537556965, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.002494074637070298}, {"id": 99, "seek": 74176, "start": 750.68, "end": 756.16, "text": " a request object and returns the response object. And this is exactly how we can use", "tokens": [257, 5308, 2657, 293, 11247, 264, 4134, 2657, 13, 400, 341, 307, 2293, 577, 321, 393, 764], "temperature": 0.0, "avg_logprob": -0.15472739537556965, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.002494074637070298}, {"id": 100, "seek": 74176, "start": 756.16, "end": 763.56, "text": " it in AWS Lambda. We just need to add one additional layer to map AWS request object", "tokens": [309, 294, 17650, 45691, 13, 492, 445, 643, 281, 909, 472, 4497, 4583, 281, 4471, 17650, 5308, 2657], "temperature": 0.0, "avg_logprob": -0.15472739537556965, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.002494074637070298}, {"id": 101, "seek": 76356, "start": 763.56, "end": 773.28, "text": " to Piranha request object and back. If we run Piranha Cloud, this simple servlet, which", "tokens": [281, 24161, 37899, 5308, 2657, 293, 646, 13, 759, 321, 1190, 24161, 37899, 8061, 11, 341, 2199, 1658, 2631, 11, 597], "temperature": 0.0, "avg_logprob": -0.2001204350415398, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0012720696395263076}, {"id": 102, "seek": 76356, "start": 773.28, "end": 779.76, "text": " is comparable to our plain Java, we were running before. If you remember with plain Java on", "tokens": [307, 25323, 281, 527, 11121, 10745, 11, 321, 645, 2614, 949, 13, 759, 291, 1604, 365, 11121, 10745, 322], "temperature": 0.0, "avg_logprob": -0.2001204350415398, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0012720696395263076}, {"id": 103, "seek": 76356, "start": 779.76, "end": 786.28, "text": " my computer, I had startup times. Actually, it was not only startup times, but until the", "tokens": [452, 3820, 11, 286, 632, 18578, 1413, 13, 5135, 11, 309, 390, 406, 787, 18578, 1413, 11, 457, 1826, 264], "temperature": 0.0, "avg_logprob": -0.2001204350415398, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0012720696395263076}, {"id": 104, "seek": 78628, "start": 786.28, "end": 794.64, "text": " program ended and printed some message and finished, it was around 50 milliseconds. With", "tokens": [1461, 4590, 293, 13567, 512, 3636, 293, 4335, 11, 309, 390, 926, 2625, 34184, 13, 2022], "temperature": 0.0, "avg_logprob": -0.17992019653320312, "compression_ratio": 1.4054054054054055, "no_speech_prob": 0.0006246386328712106}, {"id": 105, "seek": 78628, "start": 794.64, "end": 803.04, "text": " Piranha, it's a bit longer time. But this already includes the first request. So it's", "tokens": [24161, 37899, 11, 309, 311, 257, 857, 2854, 565, 13, 583, 341, 1217, 5974, 264, 700, 5308, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.17992019653320312, "compression_ratio": 1.4054054054054055, "no_speech_prob": 0.0006246386328712106}, {"id": 106, "seek": 78628, "start": 803.04, "end": 808.6, "text": " very similar to the plain Java application. It's not only that the engine starts, but", "tokens": [588, 2531, 281, 264, 11121, 10745, 3861, 13, 467, 311, 406, 787, 300, 264, 2848, 3719, 11, 457], "temperature": 0.0, "avg_logprob": -0.17992019653320312, "compression_ratio": 1.4054054054054055, "no_speech_prob": 0.0006246386328712106}, {"id": 107, "seek": 80860, "start": 808.6, "end": 816.64, "text": " it actually serves the request response with text message through HTTP stack. And with", "tokens": [309, 767, 13451, 264, 5308, 4134, 365, 2487, 3636, 807, 33283, 8630, 13, 400, 365], "temperature": 0.0, "avg_logprob": -0.1622384540618412, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0022913110442459583}, {"id": 108, "seek": 80860, "start": 816.64, "end": 830.4, "text": " that, it takes still comparable time around 130 milliseconds. Now we can compare how it", "tokens": [300, 11, 309, 2516, 920, 25323, 565, 926, 19966, 34184, 13, 823, 321, 393, 6794, 577, 309], "temperature": 0.0, "avg_logprob": -0.1622384540618412, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0022913110442459583}, {"id": 109, "seek": 80860, "start": 830.4, "end": 837.44, "text": " works in AWS Lambda. And in AWS Lambda, I have a picture, but I hope I will be able to", "tokens": [1985, 294, 17650, 45691, 13, 400, 294, 17650, 45691, 11, 286, 362, 257, 3036, 11, 457, 286, 1454, 286, 486, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.1622384540618412, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0022913110442459583}, {"id": 110, "seek": 83744, "start": 837.44, "end": 843.96, "text": " show you in a minute. As I said before, it takes a bit longer when we start the other", "tokens": [855, 291, 294, 257, 3456, 13, 1018, 286, 848, 949, 11, 309, 2516, 257, 857, 2854, 562, 321, 722, 264, 661], "temperature": 0.0, "avg_logprob": -0.13415401364550178, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0012778256786987185}, {"id": 111, "seek": 83744, "start": 843.96, "end": 850.32, "text": " function first time. Because this doesn't really matter if we run Java or any other", "tokens": [2445, 700, 565, 13, 1436, 341, 1177, 380, 534, 1871, 498, 321, 1190, 10745, 420, 604, 661], "temperature": 0.0, "avg_logprob": -0.13415401364550178, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0012778256786987185}, {"id": 112, "seek": 83744, "start": 850.32, "end": 859.6800000000001, "text": " runtime. AWS Lambda first needs to create some environment to execute our code in. And", "tokens": [34474, 13, 17650, 45691, 700, 2203, 281, 1884, 512, 2823, 281, 14483, 527, 3089, 294, 13, 400], "temperature": 0.0, "avg_logprob": -0.13415401364550178, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0012778256786987185}, {"id": 113, "seek": 83744, "start": 859.6800000000001, "end": 867.2, "text": " it takes a little bit of time. But together with creating this environment and running", "tokens": [309, 2516, 257, 707, 857, 295, 565, 13, 583, 1214, 365, 4084, 341, 2823, 293, 2614], "temperature": 0.0, "avg_logprob": -0.13415401364550178, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.0012778256786987185}, {"id": 114, "seek": 86720, "start": 867.2, "end": 874.2800000000001, "text": " our code, our example Piranha function, it takes under one second to serve the request.", "tokens": [527, 3089, 11, 527, 1365, 24161, 37899, 2445, 11, 309, 2516, 833, 472, 1150, 281, 4596, 264, 5308, 13], "temperature": 0.0, "avg_logprob": -0.11647584465112579, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0005545140593312681}, {"id": 115, "seek": 86720, "start": 874.2800000000001, "end": 880.9200000000001, "text": " Even if nothing was ready before, even on the first time we tried to run the function,", "tokens": [2754, 498, 1825, 390, 1919, 949, 11, 754, 322, 264, 700, 565, 321, 3031, 281, 1190, 264, 2445, 11], "temperature": 0.0, "avg_logprob": -0.11647584465112579, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0005545140593312681}, {"id": 116, "seek": 86720, "start": 880.9200000000001, "end": 887.9200000000001, "text": " it still serves the response under one second. If we tried it again, again, again, then the", "tokens": [309, 920, 13451, 264, 4134, 833, 472, 1150, 13, 759, 321, 3031, 309, 797, 11, 797, 11, 797, 11, 550, 264], "temperature": 0.0, "avg_logprob": -0.11647584465112579, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0005545140593312681}, {"id": 117, "seek": 86720, "start": 887.9200000000001, "end": 896.2, "text": " response times are much faster. This is on the right side here. It's under two milliseconds.", "tokens": [4134, 1413, 366, 709, 4663, 13, 639, 307, 322, 264, 558, 1252, 510, 13, 467, 311, 833, 732, 34184, 13], "temperature": 0.0, "avg_logprob": -0.11647584465112579, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0005545140593312681}, {"id": 118, "seek": 89620, "start": 896.2, "end": 902.4000000000001, "text": " Because this is only the code that needs to serve the request. Everything was initialized.", "tokens": [1436, 341, 307, 787, 264, 3089, 300, 2203, 281, 4596, 264, 5308, 13, 5471, 390, 5883, 1602, 13], "temperature": 0.0, "avg_logprob": -0.1413944617085073, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.00136915675830096}, {"id": 119, "seek": 89620, "start": 902.4000000000001, "end": 909.2, "text": " Environment was initialized. The Piranha engine was initialized. It's cached in a static variable.", "tokens": [35354, 390, 5883, 1602, 13, 440, 24161, 37899, 2848, 390, 5883, 1602, 13, 467, 311, 269, 15095, 294, 257, 13437, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1413944617085073, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.00136915675830096}, {"id": 120, "seek": 89620, "start": 909.2, "end": 915.24, "text": " So it's part of the process that is already live. AWS just executes a method basically", "tokens": [407, 309, 311, 644, 295, 264, 1399, 300, 307, 1217, 1621, 13, 17650, 445, 4454, 1819, 257, 3170, 1936], "temperature": 0.0, "avg_logprob": -0.1413944617085073, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.00136915675830096}, {"id": 121, "seek": 89620, "start": 915.24, "end": 922.5200000000001, "text": " on the Piranha engine that goes through the servlet and creates the server response. And", "tokens": [322, 264, 24161, 37899, 2848, 300, 1709, 807, 264, 1658, 2631, 293, 7829, 264, 7154, 4134, 13, 400], "temperature": 0.0, "avg_logprob": -0.1413944617085073, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.00136915675830096}, {"id": 122, "seek": 92252, "start": 922.52, "end": 928.52, "text": " that's it. That's why it takes only two milliseconds. This is only the time required to serve the", "tokens": [300, 311, 309, 13, 663, 311, 983, 309, 2516, 787, 732, 34184, 13, 639, 307, 787, 264, 565, 4739, 281, 4596, 264], "temperature": 0.0, "avg_logprob": -0.19697876771291098, "compression_ratio": 1.330708661417323, "no_speech_prob": 0.001215521595440805}, {"id": 123, "seek": 92252, "start": 928.52, "end": 941.8, "text": " actual response. So I'll try. I think I have a link here. How it works.", "tokens": [3539, 4134, 13, 407, 286, 603, 853, 13, 286, 519, 286, 362, 257, 2113, 510, 13, 1012, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19697876771291098, "compression_ratio": 1.330708661417323, "no_speech_prob": 0.001215521595440805}, {"id": 124, "seek": 94180, "start": 941.8, "end": 961.1999999999999, "text": " Okay. So this is the actual AWS console where I already deployed the application, the function.", "tokens": [1033, 13, 407, 341, 307, 264, 3539, 17650, 11076, 689, 286, 1217, 17826, 264, 3861, 11, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1680851091038097, "compression_ratio": 1.375, "no_speech_prob": 0.0005132972728461027}, {"id": 125, "seek": 94180, "start": 961.1999999999999, "end": 968.4399999999999, "text": " And AWS console has a nice feature called tester or test button. With that, we can directly", "tokens": [400, 17650, 11076, 575, 257, 1481, 4111, 1219, 36101, 420, 1500, 2960, 13, 2022, 300, 11, 321, 393, 3838], "temperature": 0.0, "avg_logprob": -0.1680851091038097, "compression_ratio": 1.375, "no_speech_prob": 0.0005132972728461027}, {"id": 126, "seek": 96844, "start": 968.44, "end": 974.7600000000001, "text": " invoke the Lambda. Normally, we would have to create an API gateway and map it to Lambda", "tokens": [41117, 264, 45691, 13, 17424, 11, 321, 576, 362, 281, 1884, 364, 9362, 28532, 293, 4471, 309, 281, 45691], "temperature": 0.0, "avg_logprob": -0.18117446899414064, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0010746174957603216}, {"id": 127, "seek": 96844, "start": 974.7600000000001, "end": 982.72, "text": " so that we can access Lambda via HTTP from outside. AWS can also generate some URL that", "tokens": [370, 300, 321, 393, 2105, 45691, 5766, 33283, 490, 2380, 13, 17650, 393, 611, 8460, 512, 12905, 300], "temperature": 0.0, "avg_logprob": -0.18117446899414064, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0010746174957603216}, {"id": 128, "seek": 96844, "start": 982.72, "end": 990.1600000000001, "text": " we can use to invoke the Lambda. But this is like directly execute the Lambda without actually", "tokens": [321, 393, 764, 281, 41117, 264, 45691, 13, 583, 341, 307, 411, 3838, 14483, 264, 45691, 1553, 767], "temperature": 0.0, "avg_logprob": -0.18117446899414064, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0010746174957603216}, {"id": 129, "seek": 96844, "start": 990.1600000000001, "end": 998.32, "text": " invoking an HTTP request. So with this, yeah, there is some examples, but the application", "tokens": [1048, 5953, 364, 33283, 5308, 13, 407, 365, 341, 11, 1338, 11, 456, 307, 512, 5110, 11, 457, 264, 3861], "temperature": 0.0, "avg_logprob": -0.18117446899414064, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0010746174957603216}, {"id": 130, "seek": 99832, "start": 998.32, "end": 1006.2, "text": " doesn't read anything from the request. It just responds with some hello world message.", "tokens": [1177, 380, 1401, 1340, 490, 264, 5308, 13, 467, 445, 27331, 365, 512, 7751, 1002, 3636, 13], "temperature": 0.0, "avg_logprob": -0.1429120127360026, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0017150070052593946}, {"id": 131, "seek": 99832, "start": 1006.2, "end": 1014.8000000000001, "text": " And if we execute it, you see it takes a bit of a time. And this is what I had in my slide.", "tokens": [400, 498, 321, 14483, 309, 11, 291, 536, 309, 2516, 257, 857, 295, 257, 565, 13, 400, 341, 307, 437, 286, 632, 294, 452, 4137, 13], "temperature": 0.0, "avg_logprob": -0.1429120127360026, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0017150070052593946}, {"id": 132, "seek": 99832, "start": 1014.8000000000001, "end": 1024.96, "text": " Here it's even shorter, 850 milliseconds. But if we try it again, it's already pre-warmed", "tokens": [1692, 309, 311, 754, 11639, 11, 1649, 2803, 34184, 13, 583, 498, 321, 853, 309, 797, 11, 309, 311, 1217, 659, 12, 6925, 1912], "temperature": 0.0, "avg_logprob": -0.1429120127360026, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0017150070052593946}, {"id": 133, "seek": 102496, "start": 1024.96, "end": 1036.88, "text": " because AWS caches. Where is it here? Caches the environment. And now it's just two milliseconds.", "tokens": [570, 17650, 269, 13272, 13, 2305, 307, 309, 510, 30, 383, 13272, 264, 2823, 13, 400, 586, 309, 311, 445, 732, 34184, 13], "temperature": 0.0, "avg_logprob": -0.18612787458631727, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.002501856302842498}, {"id": 134, "seek": 102496, "start": 1036.88, "end": 1048.56, "text": " So now the question is when the cold starts happen. They happen. I don't have any experience.", "tokens": [407, 586, 264, 1168, 307, 562, 264, 3554, 3719, 1051, 13, 814, 1051, 13, 286, 500, 380, 362, 604, 1752, 13], "temperature": 0.0, "avg_logprob": -0.18612787458631727, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.002501856302842498}, {"id": 135, "seek": 102496, "start": 1048.56, "end": 1053.56, "text": " How much they have an impact. I heard that it's not much of an impact because they happen", "tokens": [1012, 709, 436, 362, 364, 2712, 13, 286, 2198, 300, 309, 311, 406, 709, 295, 364, 2712, 570, 436, 1051], "temperature": 0.0, "avg_logprob": -0.18612787458631727, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.002501856302842498}, {"id": 136, "seek": 105356, "start": 1053.56, "end": 1061.32, "text": " normally once in a while. So the response is once in a while, takes one more second on", "tokens": [5646, 1564, 294, 257, 1339, 13, 407, 264, 4134, 307, 1564, 294, 257, 1339, 11, 2516, 472, 544, 1150, 322], "temperature": 0.0, "avg_logprob": -0.1974524140357971, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0021041131112724543}, {"id": 137, "seek": 105356, "start": 1061.32, "end": 1068.8799999999999, "text": " top of request processing. But if it takes five seconds, which can happen with normal", "tokens": [1192, 295, 5308, 9007, 13, 583, 498, 309, 2516, 1732, 3949, 11, 597, 393, 1051, 365, 2710], "temperature": 0.0, "avg_logprob": -0.1974524140357971, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0021041131112724543}, {"id": 138, "seek": 105356, "start": 1068.8799999999999, "end": 1075.6799999999998, "text": " spring boot application or traditional frameworks or even application service, I don't know,", "tokens": [5587, 11450, 3861, 420, 5164, 29834, 420, 754, 3861, 2643, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.1974524140357971, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0021041131112724543}, {"id": 139, "seek": 105356, "start": 1075.6799999999998, "end": 1081.08, "text": " sometimes some application service can be embedded, then you can run them in AWS Lambda.", "tokens": [2171, 512, 3861, 2643, 393, 312, 16741, 11, 550, 291, 393, 1190, 552, 294, 17650, 45691, 13], "temperature": 0.0, "avg_logprob": -0.1974524140357971, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0021041131112724543}, {"id": 140, "seek": 108108, "start": 1081.08, "end": 1087.6, "text": " But some of them really are hard to basically map to the method call. So you have to install", "tokens": [583, 512, 295, 552, 534, 366, 1152, 281, 1936, 4471, 281, 264, 3170, 818, 13, 407, 291, 362, 281, 3625], "temperature": 0.0, "avg_logprob": -0.15361340840657553, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0008887575240805745}, {"id": 141, "seek": 108108, "start": 1087.6, "end": 1091.52, "text": " application server. And for that, it's not even possible application to run application", "tokens": [3861, 7154, 13, 400, 337, 300, 11, 309, 311, 406, 754, 1944, 3861, 281, 1190, 3861], "temperature": 0.0, "avg_logprob": -0.15361340840657553, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0008887575240805745}, {"id": 142, "seek": 108108, "start": 1091.52, "end": 1098.56, "text": " servers in Lambda. But if you did, it would take 10, 20 seconds with some application", "tokens": [15909, 294, 45691, 13, 583, 498, 291, 630, 11, 309, 576, 747, 1266, 11, 945, 3949, 365, 512, 3861], "temperature": 0.0, "avg_logprob": -0.15361340840657553, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0008887575240805745}, {"id": 143, "seek": 108108, "start": 1098.56, "end": 1106.12, "text": " servers. And that's really a difference. You pay for the execution time, but you also", "tokens": [15909, 13, 400, 300, 311, 534, 257, 2649, 13, 509, 1689, 337, 264, 15058, 565, 11, 457, 291, 611], "temperature": 0.0, "avg_logprob": -0.15361340840657553, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0008887575240805745}, {"id": 144, "seek": 110612, "start": 1106.12, "end": 1114.04, "text": " have exposure users to waiting for a couple of seconds. If it's a user facing Lambda.", "tokens": [362, 10420, 5022, 281, 3806, 337, 257, 1916, 295, 3949, 13, 759, 309, 311, 257, 4195, 7170, 45691, 13], "temperature": 0.0, "avg_logprob": -0.16323818377594448, "compression_ratio": 1.578616352201258, "no_speech_prob": 0.001117944368161261}, {"id": 145, "seek": 110612, "start": 1114.04, "end": 1118.4399999999998, "text": " If it's not, you maybe don't care so much. If it's something that's some bad job that", "tokens": [759, 309, 311, 406, 11, 291, 1310, 500, 380, 1127, 370, 709, 13, 759, 309, 311, 746, 300, 311, 512, 1578, 1691, 300], "temperature": 0.0, "avg_logprob": -0.16323818377594448, "compression_ratio": 1.578616352201258, "no_speech_prob": 0.001117944368161261}, {"id": 146, "seek": 110612, "start": 1118.4399999999998, "end": 1131.52, "text": " takes two, three minutes to finish, then couple of seconds don't really matter.", "tokens": [2516, 732, 11, 1045, 2077, 281, 2413, 11, 550, 1916, 295, 3949, 500, 380, 534, 1871, 13], "temperature": 0.0, "avg_logprob": -0.16323818377594448, "compression_ratio": 1.578616352201258, "no_speech_prob": 0.001117944368161261}, {"id": 147, "seek": 113152, "start": 1131.52, "end": 1139.52, "text": " So here's a slide about Piranha Cloud. In short, Piranha Cloud is basically, as I said,", "tokens": [407, 510, 311, 257, 4137, 466, 24161, 37899, 8061, 13, 682, 2099, 11, 24161, 37899, 8061, 307, 1936, 11, 382, 286, 848, 11], "temperature": 0.0, "avg_logprob": -0.17640696149883847, "compression_ratio": 1.5266272189349113, "no_speech_prob": 0.0013869729591533542}, {"id": 148, "seek": 113152, "start": 1139.52, "end": 1147.28, "text": " based on a new servlet container designed from scratch, and a lot of components built", "tokens": [2361, 322, 257, 777, 1658, 2631, 10129, 4761, 490, 8459, 11, 293, 257, 688, 295, 6677, 3094], "temperature": 0.0, "avg_logprob": -0.17640696149883847, "compression_ratio": 1.5266272189349113, "no_speech_prob": 0.0013869729591533542}, {"id": 149, "seek": 113152, "start": 1147.28, "end": 1154.8799999999999, "text": " on top of it. The servlet container being servlet implementation can run any servlet", "tokens": [322, 1192, 295, 309, 13, 440, 1658, 2631, 10129, 885, 1658, 2631, 11420, 393, 1190, 604, 1658, 2631], "temperature": 0.0, "avg_logprob": -0.17640696149883847, "compression_ratio": 1.5266272189349113, "no_speech_prob": 0.0013869729591533542}, {"id": 150, "seek": 115488, "start": 1154.88, "end": 1161.6000000000001, "text": " out there. And a lot of Jakarta technologies are created as servlets. So for example, Jersey", "tokens": [484, 456, 13, 400, 257, 688, 295, 15029, 19061, 7943, 366, 2942, 382, 1658, 12541, 13, 407, 337, 1365, 11, 16601], "temperature": 0.0, "avg_logprob": -0.14293441772460938, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0022070063278079033}, {"id": 151, "seek": 115488, "start": 1161.6000000000001, "end": 1170.48, "text": " as a servlet can be deployed on Piranha. And that's quite an easy way how to get rest endpoints", "tokens": [382, 257, 1658, 2631, 393, 312, 17826, 322, 24161, 37899, 13, 400, 300, 311, 1596, 364, 1858, 636, 577, 281, 483, 1472, 917, 20552], "temperature": 0.0, "avg_logprob": -0.14293441772460938, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0022070063278079033}, {"id": 152, "seek": 115488, "start": 1170.48, "end": 1176.92, "text": " or rest library on Piranha to deploy Jersey as a servlet. And then we have everything", "tokens": [420, 1472, 6405, 322, 24161, 37899, 281, 7274, 16601, 382, 257, 1658, 2631, 13, 400, 550, 321, 362, 1203], "temperature": 0.0, "avg_logprob": -0.14293441772460938, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0022070063278079033}, {"id": 153, "seek": 115488, "start": 1176.92, "end": 1183.92, "text": " that Jersey provides. We can embed Piranha as I did in my demo, but we can also build", "tokens": [300, 16601, 6417, 13, 492, 393, 12240, 24161, 37899, 382, 286, 630, 294, 452, 10723, 11, 457, 321, 393, 611, 1322], "temperature": 0.0, "avg_logprob": -0.14293441772460938, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.0022070063278079033}, {"id": 154, "seek": 118392, "start": 1183.92, "end": 1188.76, "text": " a war application and run the war application with Piranha on command line. This is using", "tokens": [257, 1516, 3861, 293, 1190, 264, 1516, 3861, 365, 24161, 37899, 322, 5622, 1622, 13, 639, 307, 1228], "temperature": 0.0, "avg_logprob": -0.18844448941425213, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0026633632369339466}, {"id": 155, "seek": 118392, "start": 1188.76, "end": 1195.76, "text": " Jakarta distributions, which already contain this distribution of packages, distribution", "tokens": [15029, 19061, 37870, 11, 597, 1217, 5304, 341, 7316, 295, 17401, 11, 7316], "temperature": 0.0, "avg_logprob": -0.18844448941425213, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0026633632369339466}, {"id": 156, "seek": 118392, "start": 1195.76, "end": 1202.88, "text": " of functionality of Piranha that are mostly used. And the last thing, it's plain Java.", "tokens": [295, 14980, 295, 24161, 37899, 300, 366, 5240, 1143, 13, 400, 264, 1036, 551, 11, 309, 311, 11121, 10745, 13], "temperature": 0.0, "avg_logprob": -0.18844448941425213, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0026633632369339466}, {"id": 157, "seek": 118392, "start": 1202.88, "end": 1207.96, "text": " There's no real magic. There's no generated code. Everything is just clean code written", "tokens": [821, 311, 572, 957, 5585, 13, 821, 311, 572, 10833, 3089, 13, 5471, 307, 445, 2541, 3089, 3720], "temperature": 0.0, "avg_logprob": -0.18844448941425213, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0026633632369339466}, {"id": 158, "seek": 118392, "start": 1207.96, "end": 1213.6000000000001, "text": " by clever people, I think. At least judging on the code, when I looked at the code, it", "tokens": [538, 13494, 561, 11, 286, 519, 13, 1711, 1935, 23587, 322, 264, 3089, 11, 562, 286, 2956, 412, 264, 3089, 11, 309], "temperature": 0.0, "avg_logprob": -0.18844448941425213, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0026633632369339466}, {"id": 159, "seek": 121360, "start": 1213.6, "end": 1221.76, "text": " looks like the people were very clever. So with Piranha Cloud, we were able to achieve", "tokens": [1542, 411, 264, 561, 645, 588, 13494, 13, 407, 365, 24161, 37899, 8061, 11, 321, 645, 1075, 281, 4584], "temperature": 0.0, "avg_logprob": -0.14766369742908697, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0010857301531359553}, {"id": 160, "seek": 121360, "start": 1221.76, "end": 1227.48, "text": " quite fast startup times, but it still takes a couple of milliseconds, 100, 200. It depends", "tokens": [1596, 2370, 18578, 1413, 11, 457, 309, 920, 2516, 257, 1916, 295, 34184, 11, 2319, 11, 2331, 13, 467, 5946], "temperature": 0.0, "avg_logprob": -0.14766369742908697, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0010857301531359553}, {"id": 161, "seek": 121360, "start": 1227.48, "end": 1234.76, "text": " on how our application is complex. It may end up to two seconds even if we add all the", "tokens": [322, 577, 527, 3861, 307, 3997, 13, 467, 815, 917, 493, 281, 732, 3949, 754, 498, 321, 909, 439, 264], "temperature": 0.0, "avg_logprob": -0.14766369742908697, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0010857301531359553}, {"id": 162, "seek": 121360, "start": 1234.76, "end": 1243.32, "text": " Jakarta functionality that Piranha Cloud provides. If we want to reduce that even further,", "tokens": [15029, 19061, 14980, 300, 24161, 37899, 8061, 6417, 13, 759, 321, 528, 281, 5407, 300, 754, 3052, 11], "temperature": 0.0, "avg_logprob": -0.14766369742908697, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0010857301531359553}, {"id": 163, "seek": 124332, "start": 1243.32, "end": 1249.24, "text": " we have some general Java options to do that. We can first increase the CPU and RAM on the", "tokens": [321, 362, 512, 2674, 10745, 3956, 281, 360, 300, 13, 492, 393, 700, 3488, 264, 13199, 293, 14561, 322, 264], "temperature": 0.0, "avg_logprob": -0.1372788926245461, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.0028661235701292753}, {"id": 164, "seek": 124332, "start": 1249.24, "end": 1259.76, "text": " Lambda, which we can always do with any language. But we can also use a faster JVM. On the last", "tokens": [45691, 11, 597, 321, 393, 1009, 360, 365, 604, 2856, 13, 583, 321, 393, 611, 764, 257, 4663, 508, 53, 44, 13, 1282, 264, 1036], "temperature": 0.0, "avg_logprob": -0.1372788926245461, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.0028661235701292753}, {"id": 165, "seek": 124332, "start": 1259.76, "end": 1267.2, "text": " slide, I have a table where I compared running the same application with Java 11 and Java", "tokens": [4137, 11, 286, 362, 257, 3199, 689, 286, 5347, 2614, 264, 912, 3861, 365, 10745, 2975, 293, 10745], "temperature": 0.0, "avg_logprob": -0.1372788926245461, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.0028661235701292753}, {"id": 166, "seek": 126720, "start": 1267.2, "end": 1274.72, "text": " 17. If you look at the numbers, Java 17 is mostly most of the time a bit faster. So just", "tokens": [3282, 13, 759, 291, 574, 412, 264, 3547, 11, 10745, 3282, 307, 5240, 881, 295, 264, 565, 257, 857, 4663, 13, 407, 445], "temperature": 0.0, "avg_logprob": -0.13109797309426702, "compression_ratio": 1.613953488372093, "no_speech_prob": 0.001798244658857584}, {"id": 167, "seek": 126720, "start": 1274.72, "end": 1282.92, "text": " by deciding which Java version we use, we can get a bit better startup time.", "tokens": [538, 17990, 597, 10745, 3037, 321, 764, 11, 321, 393, 483, 257, 857, 1101, 18578, 565, 13], "temperature": 0.0, "avg_logprob": -0.13109797309426702, "compression_ratio": 1.613953488372093, "no_speech_prob": 0.001798244658857584}, {"id": 168, "seek": 126720, "start": 1282.92, "end": 1289.68, "text": " Then the last option here is basically a combination. I did some experiments which options work", "tokens": [1396, 264, 1036, 3614, 510, 307, 1936, 257, 6562, 13, 286, 630, 512, 12050, 597, 3956, 589], "temperature": 0.0, "avg_logprob": -0.13109797309426702, "compression_ratio": 1.613953488372093, "no_speech_prob": 0.001798244658857584}, {"id": 169, "seek": 126720, "start": 1289.68, "end": 1295.24, "text": " well regarding to startup time or reducing the startup time. And in the end, not many", "tokens": [731, 8595, 281, 18578, 565, 420, 12245, 264, 18578, 565, 13, 400, 294, 264, 917, 11, 406, 867], "temperature": 0.0, "avg_logprob": -0.13109797309426702, "compression_ratio": 1.613953488372093, "no_speech_prob": 0.001798244658857584}, {"id": 170, "seek": 129524, "start": 1295.24, "end": 1301.76, "text": " things matter. But what matters is class data sharing, which basically caches class information.", "tokens": [721, 1871, 13, 583, 437, 7001, 307, 1508, 1412, 5414, 11, 597, 1936, 269, 13272, 1508, 1589, 13], "temperature": 0.0, "avg_logprob": -0.16372143349996426, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0009818653343245387}, {"id": 171, "seek": 129524, "start": 1301.76, "end": 1309.1200000000001, "text": " So it doesn't have to be loaded and processed in the beginning. It's already pre-computed", "tokens": [407, 309, 1177, 380, 362, 281, 312, 13210, 293, 18846, 294, 264, 2863, 13, 467, 311, 1217, 659, 12, 1112, 2582, 292], "temperature": 0.0, "avg_logprob": -0.16372143349996426, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0009818653343245387}, {"id": 172, "seek": 129524, "start": 1309.1200000000001, "end": 1317.44, "text": " before cold start. And tinkering with compiler, we can disable second level just-in-time compiler", "tokens": [949, 3554, 722, 13, 400, 256, 475, 1794, 365, 31958, 11, 321, 393, 28362, 1150, 1496, 445, 12, 259, 12, 3766, 31958], "temperature": 0.0, "avg_logprob": -0.16372143349996426, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0009818653343245387}, {"id": 173, "seek": 129524, "start": 1317.44, "end": 1322.08, "text": " if we want to really focus on startup time.", "tokens": [498, 321, 528, 281, 534, 1879, 322, 18578, 565, 13], "temperature": 0.0, "avg_logprob": -0.16372143349996426, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0009818653343245387}, {"id": 174, "seek": 132208, "start": 1322.08, "end": 1329.52, "text": " And then there are other more magical options, but they can even reduce performance or reduce", "tokens": [400, 550, 456, 366, 661, 544, 12066, 3956, 11, 457, 436, 393, 754, 5407, 3389, 420, 5407], "temperature": 0.0, "avg_logprob": -0.22665883555556787, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00208441074937582}, {"id": 175, "seek": 132208, "start": 1329.52, "end": 1335.12, "text": " startup time almost to zero, either compiling the code to Gravium, with Gravium to a native", "tokens": [18578, 565, 1920, 281, 4018, 11, 2139, 715, 4883, 264, 3089, 281, 8985, 85, 2197, 11, 365, 8985, 85, 2197, 281, 257, 8470], "temperature": 0.0, "avg_logprob": -0.22665883555556787, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00208441074937582}, {"id": 176, "seek": 132208, "start": 1335.12, "end": 1342.8799999999999, "text": " binary which runs the application almost instantly. Or we can use Crack, which is a", "tokens": [17434, 597, 6676, 264, 3861, 1920, 13518, 13, 1610, 321, 393, 764, 4779, 501, 11, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.22665883555556787, "compression_ratio": 1.5284090909090908, "no_speech_prob": 0.00208441074937582}, {"id": 177, "seek": 134288, "start": 1342.88, "end": 1354.96, "text": " co-ordinated restore and checkpoint mechanism. The next talk will be about it also. And yeah,", "tokens": [598, 12, 765, 5410, 15227, 293, 42269, 7513, 13, 440, 958, 751, 486, 312, 466, 309, 611, 13, 400, 1338, 11], "temperature": 0.0, "avg_logprob": -0.21183305391123597, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.0012999243335798383}, {"id": 178, "seek": 134288, "start": 1354.96, "end": 1362.44, "text": " which is also nice is that AWS Lambda integrated that basically in one of their Java run times.", "tokens": [597, 307, 611, 1481, 307, 300, 17650, 45691, 10919, 300, 1936, 294, 472, 295, 641, 10745, 1190, 1413, 13], "temperature": 0.0, "avg_logprob": -0.21183305391123597, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.0012999243335798383}, {"id": 179, "seek": 134288, "start": 1362.44, "end": 1370.68, "text": " And it's called snap start. So you can get it for free, but only with Java 11. But hopefully", "tokens": [400, 309, 311, 1219, 13650, 722, 13, 407, 291, 393, 483, 309, 337, 1737, 11, 457, 787, 365, 10745, 2975, 13, 583, 4696], "temperature": 0.0, "avg_logprob": -0.21183305391123597, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.0012999243335798383}, {"id": 180, "seek": 137068, "start": 1370.68, "end": 1376.1200000000001, "text": " Java 17 support will be coming soon. And this works in a way that your application", "tokens": [10745, 3282, 1406, 486, 312, 1348, 2321, 13, 400, 341, 1985, 294, 257, 636, 300, 428, 3861], "temperature": 0.0, "avg_logprob": -0.20565115545213836, "compression_ratio": 1.832618025751073, "no_speech_prob": 0.0018976542633026838}, {"id": 181, "seek": 137068, "start": 1376.1200000000001, "end": 1381.64, "text": " basically stores, or you at the build time can store a checkpoint of your application", "tokens": [1936, 9512, 11, 420, 291, 412, 264, 1322, 565, 393, 3531, 257, 42269, 295, 428, 3861], "temperature": 0.0, "avg_logprob": -0.20565115545213836, "compression_ratio": 1.832618025751073, "no_speech_prob": 0.0018976542633026838}, {"id": 182, "seek": 137068, "start": 1381.64, "end": 1387.3600000000001, "text": " with all the memory or all the information basically like hibernates, you can hibernate", "tokens": [365, 439, 264, 4675, 420, 439, 264, 1589, 1936, 411, 4879, 26848, 1024, 11, 291, 393, 4879, 26848, 473], "temperature": 0.0, "avg_logprob": -0.20565115545213836, "compression_ratio": 1.832618025751073, "no_speech_prob": 0.0018976542633026838}, {"id": 183, "seek": 137068, "start": 1387.3600000000001, "end": 1393.1200000000001, "text": " your application. And then it started again and again and cold start and warm start in", "tokens": [428, 3861, 13, 400, 550, 309, 1409, 797, 293, 797, 293, 3554, 722, 293, 4561, 722, 294], "temperature": 0.0, "avg_logprob": -0.20565115545213836, "compression_ratio": 1.832618025751073, "no_speech_prob": 0.0018976542633026838}, {"id": 184, "seek": 137068, "start": 1393.1200000000001, "end": 1400.52, "text": " that case basically don't make a difference because they start from the same point.", "tokens": [300, 1389, 1936, 500, 380, 652, 257, 2649, 570, 436, 722, 490, 264, 912, 935, 13], "temperature": 0.0, "avg_logprob": -0.20565115545213836, "compression_ratio": 1.832618025751073, "no_speech_prob": 0.0018976542633026838}, {"id": 185, "seek": 140052, "start": 1400.52, "end": 1405.72, "text": " That's all from me. If you have any questions, let me know. Thank you for watching.", "tokens": [50364, 663, 311, 439, 490, 385, 13, 759, 291, 362, 604, 1651, 11, 718, 385, 458, 13, 1044, 291, 337, 1976, 13, 50624], "temperature": 0.0, "avg_logprob": -0.16682040691375732, "compression_ratio": 1.0246913580246915, "no_speech_prob": 0.0028330048080533743}], "language": "en"}