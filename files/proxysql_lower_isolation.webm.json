{"text": " First, because the last time I did a presentation, I forgot to say this and there were people last step after asking. So I am assistant engineer working at ProxySQL, my name is Sorry. So I am assistant engineer working at ProxySQL, my name is Xavier and for any more information if you follow us, so that's g-hub and j-lub and there is what I am usually. But maybe a more interesting question is how am I here and this is just because of this conference because in certain conference I found this a couple years ago and yes, it was the first in 2020 in certain corner so and we are still hiring since then and I am speeding up a little bit here. So if you are a developer and you find yourself meeting any of those conditions, so. You tell me when I can do, okay. So a brief introduction to ProxySQL, I am going to be very brief, so high performance protocol where proxy for my SQL and focus is scalability, flexibility, certain time and just basic topology, right, like any other proxy, our offers. So these slides normally are like three or four but we are going only to carry this presentation about the first three of those, so load balancing, query router, cluster monetization, we are not going to see any of that, even though the three of that today, but yeah, SQL far with statistics, mirroring and the statistics parts you can develop a lot of that but yeah, we are going mostly to care about load balancing and query routing and probably cluster monetization will be also important if you are, you know, using what we are talking about today, but and we have done a lot of recent work there lately, so but let's move on and say what is what is Gallera cluster and what is the cluster that we are, we are experimenting on with today. So multi-primary cluster is in a synchronous application, easy to use, high probability solution, so it has all the goodies of a multi-primary cluster, right? So it's, so in essence it's multi-primary or not a primary REC plaga and it's used in a synchronous application which I put this disclaimer here because it's how they officially announced it, so it's virtual synchronization and even in logical asynchronous, the actual writing and committing happens independently and that's very important, the fact that it's virtual asynchronous because what we are going to care is about the definitions of the isolation levels of per cluster node and per cluster itself, so the typical isolation levels that we have in committed and repeatable read, so Gallera says that this all, this isolation level is available for you at cluster level being the default one repeatable read and you know, solvable, but no, obviously solvable solves all of the problems in this talk, but with KB hats that, you know, make it pointless for the purpose of the talk, so mostly performance. So repeatable reads offers non-digit reads and reads remain repeatable during transactions, so offers from the last update problem. What I am, I am making this distinction here because we have just going back through the isolation levels, we have per cluster, per node and having repeatable read at no level is something that doesn't, how to say it, is something that solves some issues that you are going to expect a lot more at cluster level, so because of that Gallera has another kind of isolation during the application, which is a form of, sorry I am just going to get a little bit more into the interviews, of the loss of date, so the loss of date for what, for any that is not familiar is just that during one transaction you perform a read and then another transaction perform a return update and then the first transaction perform a write and that preview, that update from the second transaction may be lost, and is something that could happen at a lot more cluster level, so in Gallera they have a form of a snapshot isolation that is an enforcement of a repeatable read and is essentially the first committed read, so that leads to deadlocks, but it is a protection for the loss of date problem, and I have just said all this for saying that okay we have a lot of consistency across the cluster, right, across the cluster nodes, but this level isolation is respected as it was in one node itself, but there is something that is missing, what is the next question, what about the semantics, right, there are the semantics of these isolation levels preserved at cluster level, they are not, because for that we have WS3 same weight which enforce the read commit semantics at cluster level, so once you have that you don't have any potential read after write, right, but it is a very, that, you know, at what cost, you have elevated now the semantics to the semantics that you probably wanted from the beginning at cluster level, but what is the cost of that, so we are going to provide some numbers, what is the cost of that, and now as in every measurement marking thing, these are, there are a lot of things that we can discuss here, I am going just to provide some numbers that I think that are representative from what we have tested, but, you know, there can always be discussion, so this is the system, and just saying that because just for the setup that you are going to see, if you have a system that is alike, you will see that CPU, not bottleneck, memory, not bottleneck, and disk, not bottleneck, what we are seeing is the performance in the cluster where it is not fully, there is no resource constraint in more than one resource at the same time, so the versions, these are the versions, probably the infra will be available if someone wants to try itself, so the versions, this is the version that I am using, and the network is a dockeraceous infra with 500 microseconds delay imposed for one millisecond RTT between the nodes, and that is because, you know, you cannot benchmark a cluster with zero network latency, especially because you are killing the whole point of benchmarking, so cluster, let's start talking about some numbers, single primary grids will be against multi-primary grids, so the first you benchmark like this and you see that multi-primary grids outperform the single primary grids, and you are like what, that's wonderful, that's what I wanted, more nodes, more grids, around 70% more grids, that's incredible, right? Okay, this is not the truth, and why is it not the truth? Because there is a lot of lies that we tell ourselves when we do benchmarking, and this is pretty much the super ideal scenario for a replication of a multi-primary, which is transactions, only grids, I don't care about many synchronization problems that will arise of the level of isolation that we have talked before, and all nodes are equally busy because I have decided the load is distributed and the queries are all the same, and all take the same time. Okay, of course, perfect throughput, because everything takes the same time all the time. Same with grids, all the grids, and you have crazy amount of throughput yourself, but it's not also outperforming like before. Now Gaussian, the same ideal, now mix it to read-write, now we start seeing some more real stuff. So what happens when I mix the load? Well, obviously numbers go down very hard. Now this is the cluster grids, cluster reads. We have dropped from 30,000 to around 10,000 reads, and we have dropped from, I think it was like 5,000, 6,000 to what it looks like between 2,000 and 3,000. So it's half the reads and half the grids, reads probably one-third. This is equally distributed load. Now we compare the cluster with just one node of the cluster. Now it's all the same load, this mixed load, but one node. And now we have 15% more grids and 19% more reads, but it reverses. Now the single node is outperforming the whole cluster. This makes sense, because you're in a mixed load, and in a mixed load like that against all the nodes, you have a lot of collisions, you have a lot of problems, and that's all gone in one single node. But this is not anything that you're not suspecting, this is just a problem, this is just usual. This is how it's supposed to be. What we want is to increase the reads, because we have 30,000 reads and now we have 10,000, so let's try to improve that. So let's make a read-reader split over the cluster and see what happens. So if we do a read-reader split, and this is HA proxy, by the way, this field part of the vermin are going to be HA proxy because this is the dumbiest read-reader split. We have a port for reads, we have a port for reads, and that's it. So the cluster reads, we see that they are more the same as we had before, but then we have where a split reads, which go insanely up, and our grades has gone down a little bit. So we are compromising our grades by our reads, and that also makes sense, because by doing that amount of reads in the whole cluster, we are creating a lot of pressure in the other cluster nodes, because this is maximum cluster throughput, like 10%, 50% of the total cluster throughput on reads, where it is fully on reads. So you're compromising a lot, and you're losing some grades. So well, that's okay, that's okay. But we will, in this journey of talking about the semantics and the synchronization, and I have just done the dumbiest read-reader split in which I don't care about any read-after grades or anything, right, the semantics. So what happens if I now enable the synchronization for the readers, and I enable the full cluster read-committed semantics, okay, that's fine. So what happens is that we go back to basically the same performance that we were having against the whole cluster, which makes sense, because now instead of writing against everyone, you are writing against one and reading from the others, but waiting for the replication. So we can see that the split reads go between the same frames, is split reads still win, but it's marginal, and speed writes still win a little bit, but it's also marginal. So we have created, now we have the whole cluster having the nice semantics that we wanted and etc., but we are in, we haven't fixed our performance throughput. So what can we do in this scenario? Because what looks like we need a little bit more complicated logic, we are in square one of the problem. Read-side, what do I say, I'm sorry, read-side above the original read-write, writes are below the original read-write, and we still need protections for our critical reads. Okay, an alternative for avoiding this, we'll be using a single writer for a multi-premier cluster, which looks like, for what we had seen before, it shouldn't be like a very bad performance trade-off in terms of whole cluster grids, and then we re-read the critical reads only to the master, and then we, for to the replica, we choose, we choose to read all the non-critical reads to the other replicas. So critical reads to the primary, okay, and now we're being processed equally to the picture, because that's something that we can do with process equally very easily. Contrary to the Mexico-Purilus, which offers you both of the things that you need, which is reverection and anthropocontrol. The testing scenario, we are going to have a 10-90 ratio of writes versus reads, we are going to have a 5% of critical reads and a 95% of regular reads. Okay, which is, well, I would say that changing this into a more balanced ratio with an impact, okay, with an impact so much, thank you, with an impact so much the performance, the problem is that if the total throughput is what you care about, okay, so the ratio is not as important as the, how much you are going to stress the cluster with the throughput that you want for that ratio. So now we're being processed equally into the picture. In this scenario, these are the non-critical reads, these are the critical reads, and these are the writes. So we have improved almost a 50% on non-critical reads, we are retaining more or less the write load, and we have an extra 1,000 reads in the critical reads. So we are trying to, we have able to preserve the great throughput, increase the throughput of our 50%, and this is against the whole cluster read load and progress with the synchronization enabled. And this is the most important part of it. This is like this, because I decided that it was going to be like this, because if I went back here, and I decided that I am not going to limit the throughput on the reads, I will go as up as almost the whole cluster throughput, and I will compromise the writes again, that will go below what you were seeing before at any time in another benchmark. And why is that? Because the total cluster throughput is what it is. So what you need is to control what you want to do with that throughput. You cannot expect just to get more reads for free. So the conclusions, multi-primary clusters can be a lot of benefit for you, let's please set up. And it's just like this. System measurements analysis is really hard. Really, really hard. And especially benchmarking is also very hard, because most of the things that I have said here, they are right, but they are right in this scenario. And if you change slightly the scenario, it may not be. Adapting the system to your workload is what you always want to do. A different workload will change everything. The final conclusion is that control is everything. Being able to control the throughput and being able to control what you want to do with your load is what is going to decide the performance of the cluster, more than anything else. And for that, process SQL is a great tool. And thank you a lot for your attention, and happy specificity. You have five minutes for the questions. Okay. How do you know the maximum throughput of a cluster? You just measure. Like you create an artificial environment, I would say, where I say that it's very hard, because it's probably not going to be the typical load that you're going to have. You try to replicate, and then you measure, because otherwise it's... I would say that you are not going to know which is the limiting factor until you try. What did you use to measure the workload? Seizebench. I was using Seizebench, the Lua, one with all the scripts, and the old, very old, magical one, because it has, let's use, because you can do the same thing with the Lua one. You can create your own things and et cetera, but I just wanted to benchmark also what if I am selecting from different tables of the one that I am writing for and that thing. And name selection is not something that you have in the Lua one by default. So you don't have throughput limiting by default. And it's something that you have in the old one in the options. So for convenience, I use a mix. Which, by the way, no, it impacts the performance, depending on what you're measuring. If you're measuring in process equal, if you're measuring against the other proxy, it's different depending even on the tool that you're choosing. You said that you used Seizebench, I wanted to hear, because in my test, when I was trying to do multi-write multiple loads, I noticed that the drop of write performance was because of write complete. Yes. So if it's not right here to me, how you're achieving, like, better for multi-writing, because you need to certify all loads. How I achieve better throughput writing to a single note that the whole, to multiple notes. Yeah. Because from what I see, you have better performance writing multiple loads than a single load. Probably because I was having very, very few conflicts, because the size of the table that I choose were very big. And I was having very, very few conflicts during that testing. It was a very, very favorable scenario for writing. That's why I say that it's super ideal. When I say that it was ideal, it was because it's super, super ideal. Yeah. I don't know. Thank you very much. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.36, "text": " First, because the last time I did a presentation, I forgot to say this and there were people", "tokens": [50364, 2386, 11, 570, 264, 1036, 565, 286, 630, 257, 5860, 11, 286, 5298, 281, 584, 341, 293, 456, 645, 561, 50982], "temperature": 0.0, "avg_logprob": -0.4079153302689673, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.4312531650066376}, {"id": 1, "seek": 0, "start": 12.36, "end": 14.16, "text": " last step after asking.", "tokens": [50982, 1036, 1823, 934, 3365, 13, 51072], "temperature": 0.0, "avg_logprob": -0.4079153302689673, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.4312531650066376}, {"id": 2, "seek": 0, "start": 14.16, "end": 23.28, "text": " So I am assistant engineer working at ProxySQL, my name is Sorry.", "tokens": [51072, 407, 286, 669, 10994, 11403, 1364, 412, 1705, 12876, 39934, 11, 452, 1315, 307, 4919, 13, 51528], "temperature": 0.0, "avg_logprob": -0.4079153302689673, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.4312531650066376}, {"id": 3, "seek": 0, "start": 23.28, "end": 28.6, "text": " So I am assistant engineer working at ProxySQL, my name is Xavier and for any more information", "tokens": [51528, 407, 286, 669, 10994, 11403, 1364, 412, 1705, 12876, 39934, 11, 452, 1315, 307, 44653, 293, 337, 604, 544, 1589, 51794], "temperature": 0.0, "avg_logprob": -0.4079153302689673, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.4312531650066376}, {"id": 4, "seek": 2860, "start": 28.6, "end": 37.6, "text": " if you follow us, so that's g-hub and j-lub and there is what I am usually.", "tokens": [50364, 498, 291, 1524, 505, 11, 370, 300, 311, 290, 12, 71, 836, 293, 361, 12, 75, 836, 293, 456, 307, 437, 286, 669, 2673, 13, 50814], "temperature": 0.0, "avg_logprob": -0.412446434818097, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.07770773023366928}, {"id": 5, "seek": 2860, "start": 37.6, "end": 44.08, "text": " But maybe a more interesting question is how am I here and this is just because of this", "tokens": [50814, 583, 1310, 257, 544, 1880, 1168, 307, 577, 669, 286, 510, 293, 341, 307, 445, 570, 295, 341, 51138], "temperature": 0.0, "avg_logprob": -0.412446434818097, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.07770773023366928}, {"id": 6, "seek": 2860, "start": 44.08, "end": 50.36, "text": " conference because in certain conference I found this a couple years ago and yes, it", "tokens": [51138, 7586, 570, 294, 1629, 7586, 286, 1352, 341, 257, 1916, 924, 2057, 293, 2086, 11, 309, 51452], "temperature": 0.0, "avg_logprob": -0.412446434818097, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.07770773023366928}, {"id": 7, "seek": 5036, "start": 50.36, "end": 59.92, "text": " was the first in 2020 in certain corner so and we are still hiring since then and I am", "tokens": [50364, 390, 264, 700, 294, 4808, 294, 1629, 4538, 370, 293, 321, 366, 920, 15335, 1670, 550, 293, 286, 669, 50842], "temperature": 0.0, "avg_logprob": -0.5727970004081726, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.16278575360774994}, {"id": 8, "seek": 5036, "start": 59.92, "end": 62.12, "text": " speeding up a little bit here.", "tokens": [50842, 35593, 493, 257, 707, 857, 510, 13, 50952], "temperature": 0.0, "avg_logprob": -0.5727970004081726, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.16278575360774994}, {"id": 9, "seek": 5036, "start": 62.12, "end": 74.72, "text": " So if you are a developer and you find yourself meeting any of those conditions, so.", "tokens": [50952, 407, 498, 291, 366, 257, 10754, 293, 291, 915, 1803, 3440, 604, 295, 729, 4487, 11, 370, 13, 51582], "temperature": 0.0, "avg_logprob": -0.5727970004081726, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.16278575360774994}, {"id": 10, "seek": 5036, "start": 74.72, "end": 78.52, "text": " You tell me when I can do, okay.", "tokens": [51582, 509, 980, 385, 562, 286, 393, 360, 11, 1392, 13, 51772], "temperature": 0.0, "avg_logprob": -0.5727970004081726, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.16278575360774994}, {"id": 11, "seek": 7852, "start": 78.52, "end": 84.8, "text": " So a brief introduction to ProxySQL, I am going to be very brief, so high performance", "tokens": [50364, 407, 257, 5353, 9339, 281, 1705, 12876, 39934, 11, 286, 669, 516, 281, 312, 588, 5353, 11, 370, 1090, 3389, 50678], "temperature": 0.0, "avg_logprob": -0.3599145253499349, "compression_ratio": 1.4156626506024097, "no_speech_prob": 0.01791861094534397}, {"id": 12, "seek": 7852, "start": 84.8, "end": 91.16, "text": " protocol where proxy for my SQL and focus is scalability, flexibility, certain time", "tokens": [50678, 10336, 689, 29690, 337, 452, 19200, 293, 1879, 307, 15664, 2310, 11, 12635, 11, 1629, 565, 50996], "temperature": 0.0, "avg_logprob": -0.3599145253499349, "compression_ratio": 1.4156626506024097, "no_speech_prob": 0.01791861094534397}, {"id": 13, "seek": 7852, "start": 91.16, "end": 103.72, "text": " and just basic topology, right, like any other proxy, our offers.", "tokens": [50996, 293, 445, 3875, 1192, 1793, 11, 558, 11, 411, 604, 661, 29690, 11, 527, 7736, 13, 51624], "temperature": 0.0, "avg_logprob": -0.3599145253499349, "compression_ratio": 1.4156626506024097, "no_speech_prob": 0.01791861094534397}, {"id": 14, "seek": 10372, "start": 103.72, "end": 109.88, "text": " So these slides normally are like three or four but we are going only to carry this presentation", "tokens": [50364, 407, 613, 9788, 5646, 366, 411, 1045, 420, 1451, 457, 321, 366, 516, 787, 281, 3985, 341, 5860, 50672], "temperature": 0.0, "avg_logprob": -0.29779332597679065, "compression_ratio": 1.9180327868852458, "no_speech_prob": 0.4663008749485016}, {"id": 15, "seek": 10372, "start": 109.88, "end": 115.88, "text": " about the first three of those, so load balancing, query router, cluster monetization, we are", "tokens": [50672, 466, 264, 700, 1045, 295, 729, 11, 370, 3677, 22495, 11, 14581, 22492, 11, 13630, 15556, 2144, 11, 321, 366, 50972], "temperature": 0.0, "avg_logprob": -0.29779332597679065, "compression_ratio": 1.9180327868852458, "no_speech_prob": 0.4663008749485016}, {"id": 16, "seek": 10372, "start": 115.88, "end": 120.96000000000001, "text": " not going to see any of that, even though the three of that today, but yeah, SQL far", "tokens": [50972, 406, 516, 281, 536, 604, 295, 300, 11, 754, 1673, 264, 1045, 295, 300, 965, 11, 457, 1338, 11, 19200, 1400, 51226], "temperature": 0.0, "avg_logprob": -0.29779332597679065, "compression_ratio": 1.9180327868852458, "no_speech_prob": 0.4663008749485016}, {"id": 17, "seek": 10372, "start": 120.96000000000001, "end": 128.12, "text": " with statistics, mirroring and the statistics parts you can develop a lot of that but yeah,", "tokens": [51226, 365, 12523, 11, 8013, 278, 293, 264, 12523, 3166, 291, 393, 1499, 257, 688, 295, 300, 457, 1338, 11, 51584], "temperature": 0.0, "avg_logprob": -0.29779332597679065, "compression_ratio": 1.9180327868852458, "no_speech_prob": 0.4663008749485016}, {"id": 18, "seek": 10372, "start": 128.12, "end": 133.64, "text": " we are going mostly to care about load balancing and query routing and probably cluster monetization", "tokens": [51584, 321, 366, 516, 5240, 281, 1127, 466, 3677, 22495, 293, 14581, 32722, 293, 1391, 13630, 15556, 2144, 51860], "temperature": 0.0, "avg_logprob": -0.29779332597679065, "compression_ratio": 1.9180327868852458, "no_speech_prob": 0.4663008749485016}, {"id": 19, "seek": 13364, "start": 133.64, "end": 141.23999999999998, "text": " will be also important if you are, you know, using what we are talking about today, but", "tokens": [50364, 486, 312, 611, 1021, 498, 291, 366, 11, 291, 458, 11, 1228, 437, 321, 366, 1417, 466, 965, 11, 457, 50744], "temperature": 0.0, "avg_logprob": -0.3233094316847781, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.05687754228711128}, {"id": 20, "seek": 13364, "start": 141.23999999999998, "end": 148.16, "text": " and we have done a lot of recent work there lately, so but let's move on and say what", "tokens": [50744, 293, 321, 362, 1096, 257, 688, 295, 5162, 589, 456, 12881, 11, 370, 457, 718, 311, 1286, 322, 293, 584, 437, 51090], "temperature": 0.0, "avg_logprob": -0.3233094316847781, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.05687754228711128}, {"id": 21, "seek": 13364, "start": 148.16, "end": 152.48, "text": " is what is Gallera cluster and what is the cluster that we are, we are experimenting", "tokens": [51090, 307, 437, 307, 14588, 1663, 13630, 293, 437, 307, 264, 13630, 300, 321, 366, 11, 321, 366, 29070, 51306], "temperature": 0.0, "avg_logprob": -0.3233094316847781, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.05687754228711128}, {"id": 22, "seek": 13364, "start": 152.48, "end": 154.32, "text": " on with today.", "tokens": [51306, 322, 365, 965, 13, 51398], "temperature": 0.0, "avg_logprob": -0.3233094316847781, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.05687754228711128}, {"id": 23, "seek": 13364, "start": 154.32, "end": 159.64, "text": " So multi-primary cluster is in a synchronous application, easy to use, high probability", "tokens": [51398, 407, 4825, 12, 1424, 332, 822, 13630, 307, 294, 257, 44743, 3861, 11, 1858, 281, 764, 11, 1090, 8482, 51664], "temperature": 0.0, "avg_logprob": -0.3233094316847781, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.05687754228711128}, {"id": 24, "seek": 15964, "start": 159.64, "end": 167.67999999999998, "text": " solution, so it has all the goodies of a multi-primary cluster, right?", "tokens": [50364, 3827, 11, 370, 309, 575, 439, 264, 44072, 295, 257, 4825, 12, 1424, 332, 822, 13630, 11, 558, 30, 50766], "temperature": 0.0, "avg_logprob": -0.3782147800221163, "compression_ratio": 1.48125, "no_speech_prob": 0.14501385390758514}, {"id": 25, "seek": 15964, "start": 167.67999999999998, "end": 175.35999999999999, "text": " So it's, so in essence it's multi-primary or not a primary REC plaga and it's used", "tokens": [50766, 407, 309, 311, 11, 370, 294, 12801, 309, 311, 4825, 12, 1424, 332, 822, 420, 406, 257, 6194, 497, 8140, 499, 9286, 293, 309, 311, 1143, 51150], "temperature": 0.0, "avg_logprob": -0.3782147800221163, "compression_ratio": 1.48125, "no_speech_prob": 0.14501385390758514}, {"id": 26, "seek": 15964, "start": 175.35999999999999, "end": 183.6, "text": " in a synchronous application which I put this disclaimer here because it's how they", "tokens": [51150, 294, 257, 44743, 3861, 597, 286, 829, 341, 40896, 510, 570, 309, 311, 577, 436, 51562], "temperature": 0.0, "avg_logprob": -0.3782147800221163, "compression_ratio": 1.48125, "no_speech_prob": 0.14501385390758514}, {"id": 27, "seek": 18360, "start": 183.6, "end": 190.35999999999999, "text": " officially announced it, so it's virtual synchronization and even in logical asynchronous,", "tokens": [50364, 12053, 7548, 309, 11, 370, 309, 311, 6374, 19331, 2144, 293, 754, 294, 14978, 49174, 11, 50702], "temperature": 0.0, "avg_logprob": -0.29670863402517217, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.09519792348146439}, {"id": 28, "seek": 18360, "start": 190.35999999999999, "end": 196.79999999999998, "text": " the actual writing and committing happens independently and that's very important, the fact that it's", "tokens": [50702, 264, 3539, 3579, 293, 26659, 2314, 21761, 293, 300, 311, 588, 1021, 11, 264, 1186, 300, 309, 311, 51024], "temperature": 0.0, "avg_logprob": -0.29670863402517217, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.09519792348146439}, {"id": 29, "seek": 18360, "start": 196.79999999999998, "end": 203.35999999999999, "text": " virtual asynchronous because what we are going to care is about the definitions of the isolation", "tokens": [51024, 6374, 49174, 570, 437, 321, 366, 516, 281, 1127, 307, 466, 264, 21988, 295, 264, 16001, 51352], "temperature": 0.0, "avg_logprob": -0.29670863402517217, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.09519792348146439}, {"id": 30, "seek": 18360, "start": 203.35999999999999, "end": 212.0, "text": " levels of per cluster node and per cluster itself, so the typical isolation levels that", "tokens": [51352, 4358, 295, 680, 13630, 9984, 293, 680, 13630, 2564, 11, 370, 264, 7476, 16001, 4358, 300, 51784], "temperature": 0.0, "avg_logprob": -0.29670863402517217, "compression_ratio": 1.8480392156862746, "no_speech_prob": 0.09519792348146439}, {"id": 31, "seek": 21200, "start": 212.12, "end": 218.32, "text": " we have in committed and repeatable read, so Gallera says that this all, this isolation", "tokens": [50370, 321, 362, 294, 7784, 293, 7149, 712, 1401, 11, 370, 14588, 1663, 1619, 300, 341, 439, 11, 341, 16001, 50680], "temperature": 0.0, "avg_logprob": -0.40400305853949653, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.06969597190618515}, {"id": 32, "seek": 21200, "start": 218.32, "end": 223.28, "text": " level is available for you at cluster level being the default one repeatable read and", "tokens": [50680, 1496, 307, 2435, 337, 291, 412, 13630, 1496, 885, 264, 7576, 472, 7149, 712, 1401, 293, 50928], "temperature": 0.0, "avg_logprob": -0.40400305853949653, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.06969597190618515}, {"id": 33, "seek": 21200, "start": 223.28, "end": 230.6, "text": " you know, solvable, but no, obviously solvable solves all of the problems in this talk, but", "tokens": [50928, 291, 458, 11, 1404, 17915, 11, 457, 572, 11, 2745, 1404, 17915, 39890, 439, 295, 264, 2740, 294, 341, 751, 11, 457, 51294], "temperature": 0.0, "avg_logprob": -0.40400305853949653, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.06969597190618515}, {"id": 34, "seek": 21200, "start": 230.6, "end": 239.56, "text": " with KB hats that, you know, make it pointless for the purpose of the talk, so mostly performance.", "tokens": [51294, 365, 591, 33, 20549, 300, 11, 291, 458, 11, 652, 309, 32824, 337, 264, 4334, 295, 264, 751, 11, 370, 5240, 3389, 13, 51742], "temperature": 0.0, "avg_logprob": -0.40400305853949653, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.06969597190618515}, {"id": 35, "seek": 23956, "start": 239.56, "end": 250.24, "text": " So repeatable reads offers non-digit reads and reads remain repeatable during transactions,", "tokens": [50364, 407, 7149, 712, 15700, 7736, 2107, 12, 25259, 270, 15700, 293, 15700, 6222, 7149, 712, 1830, 16856, 11, 50898], "temperature": 0.0, "avg_logprob": -0.3232129903940054, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.005241426173597574}, {"id": 36, "seek": 23956, "start": 250.24, "end": 253.0, "text": " so offers from the last update problem.", "tokens": [50898, 370, 7736, 490, 264, 1036, 5623, 1154, 13, 51036], "temperature": 0.0, "avg_logprob": -0.3232129903940054, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.005241426173597574}, {"id": 37, "seek": 23956, "start": 253.0, "end": 260.24, "text": " What I am, I am making this distinction here because we have just going back through the", "tokens": [51036, 708, 286, 669, 11, 286, 669, 1455, 341, 16844, 510, 570, 321, 362, 445, 516, 646, 807, 264, 51398], "temperature": 0.0, "avg_logprob": -0.3232129903940054, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.005241426173597574}, {"id": 38, "seek": 26024, "start": 260.36, "end": 271.24, "text": " isolation levels, we have per cluster, per node and having repeatable read at no level", "tokens": [50370, 16001, 4358, 11, 321, 362, 680, 13630, 11, 680, 9984, 293, 1419, 7149, 712, 1401, 412, 572, 1496, 50914], "temperature": 0.0, "avg_logprob": -0.21370267868041992, "compression_ratio": 1.6375, "no_speech_prob": 0.06561363488435745}, {"id": 39, "seek": 26024, "start": 271.24, "end": 280.8, "text": " is something that doesn't, how to say it, is something that solves some issues that you", "tokens": [50914, 307, 746, 300, 1177, 380, 11, 577, 281, 584, 309, 11, 307, 746, 300, 39890, 512, 2663, 300, 291, 51392], "temperature": 0.0, "avg_logprob": -0.21370267868041992, "compression_ratio": 1.6375, "no_speech_prob": 0.06561363488435745}, {"id": 40, "seek": 26024, "start": 280.8, "end": 286.96000000000004, "text": " are going to expect a lot more at cluster level, so because of that Gallera has another", "tokens": [51392, 366, 516, 281, 2066, 257, 688, 544, 412, 13630, 1496, 11, 370, 570, 295, 300, 14588, 1663, 575, 1071, 51700], "temperature": 0.0, "avg_logprob": -0.21370267868041992, "compression_ratio": 1.6375, "no_speech_prob": 0.06561363488435745}, {"id": 41, "seek": 28696, "start": 286.96, "end": 294.44, "text": " kind of isolation during the application, which is a form of, sorry I am just going", "tokens": [50364, 733, 295, 16001, 1830, 264, 3861, 11, 597, 307, 257, 1254, 295, 11, 2597, 286, 669, 445, 516, 50738], "temperature": 0.0, "avg_logprob": -0.33202489217122394, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.03289541229605675}, {"id": 42, "seek": 28696, "start": 294.44, "end": 299.71999999999997, "text": " to get a little bit more into the interviews, of the loss of date, so the loss of date for", "tokens": [50738, 281, 483, 257, 707, 857, 544, 666, 264, 12318, 11, 295, 264, 4470, 295, 4002, 11, 370, 264, 4470, 295, 4002, 337, 51002], "temperature": 0.0, "avg_logprob": -0.33202489217122394, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.03289541229605675}, {"id": 43, "seek": 28696, "start": 299.71999999999997, "end": 304.96, "text": " what, for any that is not familiar is just that during one transaction you perform a", "tokens": [51002, 437, 11, 337, 604, 300, 307, 406, 4963, 307, 445, 300, 1830, 472, 14425, 291, 2042, 257, 51264], "temperature": 0.0, "avg_logprob": -0.33202489217122394, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.03289541229605675}, {"id": 44, "seek": 28696, "start": 304.96, "end": 310.84, "text": " read and then another transaction perform a return update and then the first transaction", "tokens": [51264, 1401, 293, 550, 1071, 14425, 2042, 257, 2736, 5623, 293, 550, 264, 700, 14425, 51558], "temperature": 0.0, "avg_logprob": -0.33202489217122394, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.03289541229605675}, {"id": 45, "seek": 31084, "start": 310.84, "end": 317.15999999999997, "text": " perform a write and that preview, that update from the second transaction may be lost, and", "tokens": [50364, 2042, 257, 2464, 293, 300, 14281, 11, 300, 5623, 490, 264, 1150, 14425, 815, 312, 2731, 11, 293, 50680], "temperature": 0.0, "avg_logprob": -0.3122909457184548, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.02628917433321476}, {"id": 46, "seek": 31084, "start": 317.15999999999997, "end": 322.59999999999997, "text": " is something that could happen at a lot more cluster level, so in Gallera they have a form", "tokens": [50680, 307, 746, 300, 727, 1051, 412, 257, 688, 544, 13630, 1496, 11, 370, 294, 14588, 1663, 436, 362, 257, 1254, 50952], "temperature": 0.0, "avg_logprob": -0.3122909457184548, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.02628917433321476}, {"id": 47, "seek": 31084, "start": 322.59999999999997, "end": 330.08, "text": " of a snapshot isolation that is an enforcement of a repeatable read and is essentially the", "tokens": [50952, 295, 257, 30163, 16001, 300, 307, 364, 11475, 295, 257, 7149, 712, 1401, 293, 307, 4476, 264, 51326], "temperature": 0.0, "avg_logprob": -0.3122909457184548, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.02628917433321476}, {"id": 48, "seek": 31084, "start": 330.08, "end": 339.4, "text": " first committed read, so that leads to deadlocks, but it is a protection for the loss of date", "tokens": [51326, 700, 7784, 1401, 11, 370, 300, 6689, 281, 3116, 34896, 11, 457, 309, 307, 257, 6334, 337, 264, 4470, 295, 4002, 51792], "temperature": 0.0, "avg_logprob": -0.3122909457184548, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.02628917433321476}, {"id": 49, "seek": 33940, "start": 339.4, "end": 348.59999999999997, "text": " problem, and I have just said all this for saying that okay we have a lot of consistency", "tokens": [50364, 1154, 11, 293, 286, 362, 445, 848, 439, 341, 337, 1566, 300, 1392, 321, 362, 257, 688, 295, 14416, 50824], "temperature": 0.0, "avg_logprob": -0.2059282511472702, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.06831125169992447}, {"id": 50, "seek": 33940, "start": 348.59999999999997, "end": 360.15999999999997, "text": " across the cluster, right, across the cluster nodes, but this level isolation is respected", "tokens": [50824, 2108, 264, 13630, 11, 558, 11, 2108, 264, 13630, 13891, 11, 457, 341, 1496, 16001, 307, 20020, 51402], "temperature": 0.0, "avg_logprob": -0.2059282511472702, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.06831125169992447}, {"id": 51, "seek": 33940, "start": 360.15999999999997, "end": 367.64, "text": " as it was in one node itself, but there is something that is missing, what is the next", "tokens": [51402, 382, 309, 390, 294, 472, 9984, 2564, 11, 457, 456, 307, 746, 300, 307, 5361, 11, 437, 307, 264, 958, 51776], "temperature": 0.0, "avg_logprob": -0.2059282511472702, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.06831125169992447}, {"id": 52, "seek": 36764, "start": 367.68, "end": 372.88, "text": " question, what about the semantics, right, there are the semantics of these isolation", "tokens": [50366, 1168, 11, 437, 466, 264, 4361, 45298, 11, 558, 11, 456, 366, 264, 4361, 45298, 295, 613, 16001, 50626], "temperature": 0.0, "avg_logprob": -0.30366310706505406, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.15585967898368835}, {"id": 53, "seek": 36764, "start": 372.88, "end": 380.91999999999996, "text": " levels preserved at cluster level, they are not, because for that we have WS3 same weight", "tokens": [50626, 4358, 22242, 412, 13630, 1496, 11, 436, 366, 406, 11, 570, 337, 300, 321, 362, 343, 50, 18, 912, 3364, 51028], "temperature": 0.0, "avg_logprob": -0.30366310706505406, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.15585967898368835}, {"id": 54, "seek": 36764, "start": 380.91999999999996, "end": 390.68, "text": " which enforce the read commit semantics at cluster level, so once you have that you don't", "tokens": [51028, 597, 24825, 264, 1401, 5599, 4361, 45298, 412, 13630, 1496, 11, 370, 1564, 291, 362, 300, 291, 500, 380, 51516], "temperature": 0.0, "avg_logprob": -0.30366310706505406, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.15585967898368835}, {"id": 55, "seek": 39068, "start": 390.72, "end": 403.88, "text": " have any potential read after write, right, but it is a very, that, you know, at what", "tokens": [50366, 362, 604, 3995, 1401, 934, 2464, 11, 558, 11, 457, 309, 307, 257, 588, 11, 300, 11, 291, 458, 11, 412, 437, 51024], "temperature": 0.0, "avg_logprob": -0.19990027652067296, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.034538283944129944}, {"id": 56, "seek": 39068, "start": 403.88, "end": 408.12, "text": " cost, you have elevated now the semantics to the semantics that you probably wanted from", "tokens": [51024, 2063, 11, 291, 362, 23457, 586, 264, 4361, 45298, 281, 264, 4361, 45298, 300, 291, 1391, 1415, 490, 51236], "temperature": 0.0, "avg_logprob": -0.19990027652067296, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.034538283944129944}, {"id": 57, "seek": 39068, "start": 408.12, "end": 418.0, "text": " the beginning at cluster level, but what is the cost of that, so we are going to provide", "tokens": [51236, 264, 2863, 412, 13630, 1496, 11, 457, 437, 307, 264, 2063, 295, 300, 11, 370, 321, 366, 516, 281, 2893, 51730], "temperature": 0.0, "avg_logprob": -0.19990027652067296, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.034538283944129944}, {"id": 58, "seek": 41800, "start": 418.0, "end": 426.0, "text": " some numbers, what is the cost of that, and now as in every measurement marking thing,", "tokens": [50364, 512, 3547, 11, 437, 307, 264, 2063, 295, 300, 11, 293, 586, 382, 294, 633, 13160, 25482, 551, 11, 50764], "temperature": 0.0, "avg_logprob": -0.27818784820899534, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.11210720241069794}, {"id": 59, "seek": 41800, "start": 426.0, "end": 430.48, "text": " these are, there are a lot of things that we can discuss here, I am going just to provide", "tokens": [50764, 613, 366, 11, 456, 366, 257, 688, 295, 721, 300, 321, 393, 2248, 510, 11, 286, 669, 516, 445, 281, 2893, 50988], "temperature": 0.0, "avg_logprob": -0.27818784820899534, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.11210720241069794}, {"id": 60, "seek": 41800, "start": 430.48, "end": 436.44, "text": " some numbers that I think that are representative from what we have tested, but, you know, there", "tokens": [50988, 512, 3547, 300, 286, 519, 300, 366, 12424, 490, 437, 321, 362, 8246, 11, 457, 11, 291, 458, 11, 456, 51286], "temperature": 0.0, "avg_logprob": -0.27818784820899534, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.11210720241069794}, {"id": 61, "seek": 41800, "start": 436.44, "end": 442.92, "text": " can always be discussion, so this is the system, and just saying that because just for the", "tokens": [51286, 393, 1009, 312, 5017, 11, 370, 341, 307, 264, 1185, 11, 293, 445, 1566, 300, 570, 445, 337, 264, 51610], "temperature": 0.0, "avg_logprob": -0.27818784820899534, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.11210720241069794}, {"id": 62, "seek": 44292, "start": 442.92, "end": 447.2, "text": " setup that you are going to see, if you have a system that is alike, you will see that", "tokens": [50364, 8657, 300, 291, 366, 516, 281, 536, 11, 498, 291, 362, 257, 1185, 300, 307, 20025, 11, 291, 486, 536, 300, 50578], "temperature": 0.0, "avg_logprob": -0.2055120363340273, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.08553853631019592}, {"id": 63, "seek": 44292, "start": 447.2, "end": 452.88, "text": " CPU, not bottleneck, memory, not bottleneck, and disk, not bottleneck, what we are seeing", "tokens": [50578, 13199, 11, 406, 44641, 547, 11, 4675, 11, 406, 44641, 547, 11, 293, 12355, 11, 406, 44641, 547, 11, 437, 321, 366, 2577, 50862], "temperature": 0.0, "avg_logprob": -0.2055120363340273, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.08553853631019592}, {"id": 64, "seek": 44292, "start": 452.88, "end": 459.64, "text": " is the performance in the cluster where it is not fully, there is no resource constraint", "tokens": [50862, 307, 264, 3389, 294, 264, 13630, 689, 309, 307, 406, 4498, 11, 456, 307, 572, 7684, 25534, 51200], "temperature": 0.0, "avg_logprob": -0.2055120363340273, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.08553853631019592}, {"id": 65, "seek": 44292, "start": 459.64, "end": 468.04, "text": " in more than one resource at the same time, so the versions, these are the versions, probably", "tokens": [51200, 294, 544, 813, 472, 7684, 412, 264, 912, 565, 11, 370, 264, 9606, 11, 613, 366, 264, 9606, 11, 1391, 51620], "temperature": 0.0, "avg_logprob": -0.2055120363340273, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.08553853631019592}, {"id": 66, "seek": 46804, "start": 468.04, "end": 474.92, "text": " the infra will be available if someone wants to try itself, so the versions, this is the", "tokens": [50364, 264, 23654, 486, 312, 2435, 498, 1580, 2738, 281, 853, 2564, 11, 370, 264, 9606, 11, 341, 307, 264, 50708], "temperature": 0.0, "avg_logprob": -0.29620670991785386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.4013873040676117}, {"id": 67, "seek": 46804, "start": 474.92, "end": 479.76000000000005, "text": " version that I am using, and the network is a dockeraceous infra with 500 microseconds", "tokens": [50708, 3037, 300, 286, 669, 1228, 11, 293, 264, 3209, 307, 257, 360, 9178, 617, 563, 23654, 365, 5923, 3123, 37841, 28750, 50950], "temperature": 0.0, "avg_logprob": -0.29620670991785386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.4013873040676117}, {"id": 68, "seek": 46804, "start": 479.76000000000005, "end": 486.20000000000005, "text": " delay imposed for one millisecond RTT between the nodes, and that is because, you know,", "tokens": [50950, 8577, 26491, 337, 472, 27940, 18882, 21797, 51, 1296, 264, 13891, 11, 293, 300, 307, 570, 11, 291, 458, 11, 51272], "temperature": 0.0, "avg_logprob": -0.29620670991785386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.4013873040676117}, {"id": 69, "seek": 46804, "start": 486.20000000000005, "end": 492.52000000000004, "text": " you cannot benchmark a cluster with zero network latency, especially because you are killing", "tokens": [51272, 291, 2644, 18927, 257, 13630, 365, 4018, 3209, 27043, 11, 2318, 570, 291, 366, 8011, 51588], "temperature": 0.0, "avg_logprob": -0.29620670991785386, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.4013873040676117}, {"id": 70, "seek": 49252, "start": 492.52, "end": 503.56, "text": " the whole point of benchmarking, so cluster, let's start talking about some numbers, single", "tokens": [50364, 264, 1379, 935, 295, 18927, 278, 11, 370, 13630, 11, 718, 311, 722, 1417, 466, 512, 3547, 11, 2167, 50916], "temperature": 0.0, "avg_logprob": -0.2684436045901876, "compression_ratio": 1.8, "no_speech_prob": 0.258494108915329}, {"id": 71, "seek": 49252, "start": 503.56, "end": 509.84, "text": " primary grids will be against multi-primary grids, so the first you benchmark like this", "tokens": [50916, 6194, 677, 3742, 486, 312, 1970, 4825, 12, 1424, 332, 822, 677, 3742, 11, 370, 264, 700, 291, 18927, 411, 341, 51230], "temperature": 0.0, "avg_logprob": -0.2684436045901876, "compression_ratio": 1.8, "no_speech_prob": 0.258494108915329}, {"id": 72, "seek": 49252, "start": 509.84, "end": 514.4399999999999, "text": " and you see that multi-primary grids outperform the single primary grids, and you are like", "tokens": [51230, 293, 291, 536, 300, 4825, 12, 1424, 332, 822, 677, 3742, 484, 26765, 264, 2167, 6194, 677, 3742, 11, 293, 291, 366, 411, 51460], "temperature": 0.0, "avg_logprob": -0.2684436045901876, "compression_ratio": 1.8, "no_speech_prob": 0.258494108915329}, {"id": 73, "seek": 51444, "start": 514.44, "end": 524.6800000000001, "text": " what, that's wonderful, that's what I wanted, more nodes, more grids, around 70% more grids,", "tokens": [50364, 437, 11, 300, 311, 3715, 11, 300, 311, 437, 286, 1415, 11, 544, 13891, 11, 544, 677, 3742, 11, 926, 5285, 4, 544, 677, 3742, 11, 50876], "temperature": 0.0, "avg_logprob": -0.2834214682530875, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.7788757681846619}, {"id": 74, "seek": 51444, "start": 524.6800000000001, "end": 527.5600000000001, "text": " that's incredible, right?", "tokens": [50876, 300, 311, 4651, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.2834214682530875, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.7788757681846619}, {"id": 75, "seek": 51444, "start": 527.5600000000001, "end": 531.72, "text": " Okay, this is not the truth, and why is it not the truth?", "tokens": [51020, 1033, 11, 341, 307, 406, 264, 3494, 11, 293, 983, 307, 309, 406, 264, 3494, 30, 51228], "temperature": 0.0, "avg_logprob": -0.2834214682530875, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.7788757681846619}, {"id": 76, "seek": 51444, "start": 531.72, "end": 536.9200000000001, "text": " Because there is a lot of lies that we tell ourselves when we do benchmarking, and this", "tokens": [51228, 1436, 456, 307, 257, 688, 295, 9134, 300, 321, 980, 4175, 562, 321, 360, 18927, 278, 11, 293, 341, 51488], "temperature": 0.0, "avg_logprob": -0.2834214682530875, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.7788757681846619}, {"id": 77, "seek": 51444, "start": 536.9200000000001, "end": 542.48, "text": " is pretty much the super ideal scenario for a replication of a multi-primary, which is", "tokens": [51488, 307, 1238, 709, 264, 1687, 7157, 9005, 337, 257, 39911, 295, 257, 4825, 12, 1424, 332, 822, 11, 597, 307, 51766], "temperature": 0.0, "avg_logprob": -0.2834214682530875, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.7788757681846619}, {"id": 78, "seek": 54248, "start": 542.48, "end": 547.76, "text": " transactions, only grids, I don't care about many synchronization problems that will arise", "tokens": [50364, 16856, 11, 787, 677, 3742, 11, 286, 500, 380, 1127, 466, 867, 19331, 2144, 2740, 300, 486, 20288, 50628], "temperature": 0.0, "avg_logprob": -0.25855402512983844, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.21017293632030487}, {"id": 79, "seek": 54248, "start": 547.76, "end": 553.32, "text": " of the level of isolation that we have talked before, and all nodes are equally busy because", "tokens": [50628, 295, 264, 1496, 295, 16001, 300, 321, 362, 2825, 949, 11, 293, 439, 13891, 366, 12309, 5856, 570, 50906], "temperature": 0.0, "avg_logprob": -0.25855402512983844, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.21017293632030487}, {"id": 80, "seek": 54248, "start": 553.32, "end": 559.12, "text": " I have decided the load is distributed and the queries are all the same, and all take", "tokens": [50906, 286, 362, 3047, 264, 3677, 307, 12631, 293, 264, 24109, 366, 439, 264, 912, 11, 293, 439, 747, 51196], "temperature": 0.0, "avg_logprob": -0.25855402512983844, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.21017293632030487}, {"id": 81, "seek": 54248, "start": 559.12, "end": 560.12, "text": " the same time.", "tokens": [51196, 264, 912, 565, 13, 51246], "temperature": 0.0, "avg_logprob": -0.25855402512983844, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.21017293632030487}, {"id": 82, "seek": 54248, "start": 560.12, "end": 567.32, "text": " Okay, of course, perfect throughput, because everything takes the same time all the time.", "tokens": [51246, 1033, 11, 295, 1164, 11, 2176, 44629, 11, 570, 1203, 2516, 264, 912, 565, 439, 264, 565, 13, 51606], "temperature": 0.0, "avg_logprob": -0.25855402512983844, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.21017293632030487}, {"id": 83, "seek": 56732, "start": 567.32, "end": 575.1600000000001, "text": " Same with grids, all the grids, and you have crazy amount of throughput yourself, but it's", "tokens": [50364, 10635, 365, 677, 3742, 11, 439, 264, 677, 3742, 11, 293, 291, 362, 3219, 2372, 295, 44629, 1803, 11, 457, 309, 311, 50756], "temperature": 0.0, "avg_logprob": -0.38074343363444013, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.19529518485069275}, {"id": 84, "seek": 56732, "start": 575.1600000000001, "end": 579.7600000000001, "text": " not also outperforming like before.", "tokens": [50756, 406, 611, 484, 26765, 278, 411, 949, 13, 50986], "temperature": 0.0, "avg_logprob": -0.38074343363444013, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.19529518485069275}, {"id": 85, "seek": 56732, "start": 579.7600000000001, "end": 588.6400000000001, "text": " Now Gaussian, the same ideal, now mix it to read-write, now we start seeing some more", "tokens": [50986, 823, 39148, 11, 264, 912, 7157, 11, 586, 2890, 309, 281, 1401, 12, 21561, 11, 586, 321, 722, 2577, 512, 544, 51430], "temperature": 0.0, "avg_logprob": -0.38074343363444013, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.19529518485069275}, {"id": 86, "seek": 56732, "start": 588.6400000000001, "end": 590.08, "text": " real stuff.", "tokens": [51430, 957, 1507, 13, 51502], "temperature": 0.0, "avg_logprob": -0.38074343363444013, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.19529518485069275}, {"id": 87, "seek": 56732, "start": 590.08, "end": 592.6800000000001, "text": " So what happens when I mix the load?", "tokens": [51502, 407, 437, 2314, 562, 286, 2890, 264, 3677, 30, 51632], "temperature": 0.0, "avg_logprob": -0.38074343363444013, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.19529518485069275}, {"id": 88, "seek": 59268, "start": 592.68, "end": 598.5999999999999, "text": " Well, obviously numbers go down very hard.", "tokens": [50364, 1042, 11, 2745, 3547, 352, 760, 588, 1152, 13, 50660], "temperature": 0.0, "avg_logprob": -0.23327924807866415, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.04956687614321709}, {"id": 89, "seek": 59268, "start": 598.5999999999999, "end": 601.2399999999999, "text": " Now this is the cluster grids, cluster reads.", "tokens": [50660, 823, 341, 307, 264, 13630, 677, 3742, 11, 13630, 15700, 13, 50792], "temperature": 0.0, "avg_logprob": -0.23327924807866415, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.04956687614321709}, {"id": 90, "seek": 59268, "start": 601.2399999999999, "end": 607.1999999999999, "text": " We have dropped from 30,000 to around 10,000 reads, and we have dropped from, I think it", "tokens": [50792, 492, 362, 8119, 490, 2217, 11, 1360, 281, 926, 1266, 11, 1360, 15700, 11, 293, 321, 362, 8119, 490, 11, 286, 519, 309, 51090], "temperature": 0.0, "avg_logprob": -0.23327924807866415, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.04956687614321709}, {"id": 91, "seek": 59268, "start": 607.1999999999999, "end": 612.3599999999999, "text": " was like 5,000, 6,000 to what it looks like between 2,000 and 3,000.", "tokens": [51090, 390, 411, 1025, 11, 1360, 11, 1386, 11, 1360, 281, 437, 309, 1542, 411, 1296, 568, 11, 1360, 293, 805, 11, 1360, 13, 51348], "temperature": 0.0, "avg_logprob": -0.23327924807866415, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.04956687614321709}, {"id": 92, "seek": 59268, "start": 612.3599999999999, "end": 617.28, "text": " So it's half the reads and half the grids, reads probably one-third.", "tokens": [51348, 407, 309, 311, 1922, 264, 15700, 293, 1922, 264, 677, 3742, 11, 15700, 1391, 472, 12, 25095, 13, 51594], "temperature": 0.0, "avg_logprob": -0.23327924807866415, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.04956687614321709}, {"id": 93, "seek": 61728, "start": 617.28, "end": 620.1999999999999, "text": " This is equally distributed load.", "tokens": [50364, 639, 307, 12309, 12631, 3677, 13, 50510], "temperature": 0.0, "avg_logprob": -0.23698717431177066, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.02157573774456978}, {"id": 94, "seek": 61728, "start": 620.1999999999999, "end": 629.56, "text": " Now we compare the cluster with just one node of the cluster.", "tokens": [50510, 823, 321, 6794, 264, 13630, 365, 445, 472, 9984, 295, 264, 13630, 13, 50978], "temperature": 0.0, "avg_logprob": -0.23698717431177066, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.02157573774456978}, {"id": 95, "seek": 61728, "start": 629.56, "end": 636.0799999999999, "text": " Now it's all the same load, this mixed load, but one node.", "tokens": [50978, 823, 309, 311, 439, 264, 912, 3677, 11, 341, 7467, 3677, 11, 457, 472, 9984, 13, 51304], "temperature": 0.0, "avg_logprob": -0.23698717431177066, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.02157573774456978}, {"id": 96, "seek": 61728, "start": 636.0799999999999, "end": 641.64, "text": " And now we have 15% more grids and 19% more reads, but it reverses.", "tokens": [51304, 400, 586, 321, 362, 2119, 4, 544, 677, 3742, 293, 1294, 4, 544, 15700, 11, 457, 309, 14582, 279, 13, 51582], "temperature": 0.0, "avg_logprob": -0.23698717431177066, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.02157573774456978}, {"id": 97, "seek": 61728, "start": 641.64, "end": 645.8, "text": " Now the single node is outperforming the whole cluster.", "tokens": [51582, 823, 264, 2167, 9984, 307, 484, 26765, 278, 264, 1379, 13630, 13, 51790], "temperature": 0.0, "avg_logprob": -0.23698717431177066, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.02157573774456978}, {"id": 98, "seek": 64580, "start": 646.8, "end": 654.4799999999999, "text": " This makes sense, because you're in a mixed load, and in a mixed load like that against", "tokens": [50414, 639, 1669, 2020, 11, 570, 291, 434, 294, 257, 7467, 3677, 11, 293, 294, 257, 7467, 3677, 411, 300, 1970, 50798], "temperature": 0.0, "avg_logprob": -0.22337716558705206, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.1536480039358139}, {"id": 99, "seek": 64580, "start": 654.4799999999999, "end": 658.3199999999999, "text": " all the nodes, you have a lot of collisions, you have a lot of problems, and that's all", "tokens": [50798, 439, 264, 13891, 11, 291, 362, 257, 688, 295, 46537, 11, 291, 362, 257, 688, 295, 2740, 11, 293, 300, 311, 439, 50990], "temperature": 0.0, "avg_logprob": -0.22337716558705206, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.1536480039358139}, {"id": 100, "seek": 64580, "start": 658.3199999999999, "end": 662.4399999999999, "text": " gone in one single node.", "tokens": [50990, 2780, 294, 472, 2167, 9984, 13, 51196], "temperature": 0.0, "avg_logprob": -0.22337716558705206, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.1536480039358139}, {"id": 101, "seek": 64580, "start": 662.4399999999999, "end": 667.8399999999999, "text": " But this is not anything that you're not suspecting, this is just a problem, this is just usual.", "tokens": [51196, 583, 341, 307, 406, 1340, 300, 291, 434, 406, 9091, 278, 11, 341, 307, 445, 257, 1154, 11, 341, 307, 445, 7713, 13, 51466], "temperature": 0.0, "avg_logprob": -0.22337716558705206, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.1536480039358139}, {"id": 102, "seek": 64580, "start": 667.8399999999999, "end": 671.9599999999999, "text": " This is how it's supposed to be.", "tokens": [51466, 639, 307, 577, 309, 311, 3442, 281, 312, 13, 51672], "temperature": 0.0, "avg_logprob": -0.22337716558705206, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.1536480039358139}, {"id": 103, "seek": 67196, "start": 672.12, "end": 680.5600000000001, "text": " What we want is to increase the reads, because we have 30,000 reads and now we have 10,000,", "tokens": [50372, 708, 321, 528, 307, 281, 3488, 264, 15700, 11, 570, 321, 362, 2217, 11, 1360, 15700, 293, 586, 321, 362, 1266, 11, 1360, 11, 50794], "temperature": 0.0, "avg_logprob": -0.2547411565427427, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.015559044666588306}, {"id": 104, "seek": 67196, "start": 680.5600000000001, "end": 683.64, "text": " so let's try to improve that.", "tokens": [50794, 370, 718, 311, 853, 281, 3470, 300, 13, 50948], "temperature": 0.0, "avg_logprob": -0.2547411565427427, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.015559044666588306}, {"id": 105, "seek": 67196, "start": 683.64, "end": 689.48, "text": " So let's make a read-reader split over the cluster and see what happens.", "tokens": [50948, 407, 718, 311, 652, 257, 1401, 12, 2538, 260, 7472, 670, 264, 13630, 293, 536, 437, 2314, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2547411565427427, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.015559044666588306}, {"id": 106, "seek": 67196, "start": 689.48, "end": 695.36, "text": " So if we do a read-reader split, and this is HA proxy, by the way, this field part of", "tokens": [51240, 407, 498, 321, 360, 257, 1401, 12, 2538, 260, 7472, 11, 293, 341, 307, 11979, 29690, 11, 538, 264, 636, 11, 341, 2519, 644, 295, 51534], "temperature": 0.0, "avg_logprob": -0.2547411565427427, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.015559044666588306}, {"id": 107, "seek": 67196, "start": 695.36, "end": 700.8000000000001, "text": " the vermin are going to be HA proxy because this is the dumbiest read-reader split.", "tokens": [51534, 264, 1306, 2367, 366, 516, 281, 312, 11979, 29690, 570, 341, 307, 264, 10316, 6495, 1401, 12, 2538, 260, 7472, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2547411565427427, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.015559044666588306}, {"id": 108, "seek": 70080, "start": 700.8, "end": 706.76, "text": " We have a port for reads, we have a port for reads, and that's it.", "tokens": [50364, 492, 362, 257, 2436, 337, 15700, 11, 321, 362, 257, 2436, 337, 15700, 11, 293, 300, 311, 309, 13, 50662], "temperature": 0.0, "avg_logprob": -0.22643932219474547, "compression_ratio": 1.8162162162162163, "no_speech_prob": 0.004659116733819246}, {"id": 109, "seek": 70080, "start": 706.76, "end": 712.0799999999999, "text": " So the cluster reads, we see that they are more the same as we had before, but then we", "tokens": [50662, 407, 264, 13630, 15700, 11, 321, 536, 300, 436, 366, 544, 264, 912, 382, 321, 632, 949, 11, 457, 550, 321, 50928], "temperature": 0.0, "avg_logprob": -0.22643932219474547, "compression_ratio": 1.8162162162162163, "no_speech_prob": 0.004659116733819246}, {"id": 110, "seek": 70080, "start": 712.0799999999999, "end": 720.52, "text": " have where a split reads, which go insanely up, and our grades has gone down a little", "tokens": [50928, 362, 689, 257, 7472, 15700, 11, 597, 352, 40965, 493, 11, 293, 527, 18041, 575, 2780, 760, 257, 707, 51350], "temperature": 0.0, "avg_logprob": -0.22643932219474547, "compression_ratio": 1.8162162162162163, "no_speech_prob": 0.004659116733819246}, {"id": 111, "seek": 70080, "start": 720.52, "end": 721.52, "text": " bit.", "tokens": [51350, 857, 13, 51400], "temperature": 0.0, "avg_logprob": -0.22643932219474547, "compression_ratio": 1.8162162162162163, "no_speech_prob": 0.004659116733819246}, {"id": 112, "seek": 70080, "start": 721.52, "end": 727.8, "text": " So we are compromising our grades by our reads, and that also makes sense, because by doing", "tokens": [51400, 407, 321, 366, 11482, 3436, 527, 18041, 538, 527, 15700, 11, 293, 300, 611, 1669, 2020, 11, 570, 538, 884, 51714], "temperature": 0.0, "avg_logprob": -0.22643932219474547, "compression_ratio": 1.8162162162162163, "no_speech_prob": 0.004659116733819246}, {"id": 113, "seek": 72780, "start": 727.8, "end": 732.1999999999999, "text": " that amount of reads in the whole cluster, we are creating a lot of pressure in the other", "tokens": [50364, 300, 2372, 295, 15700, 294, 264, 1379, 13630, 11, 321, 366, 4084, 257, 688, 295, 3321, 294, 264, 661, 50584], "temperature": 0.0, "avg_logprob": -0.20819199222257767, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.018606318160891533}, {"id": 114, "seek": 72780, "start": 732.1999999999999, "end": 740.5999999999999, "text": " cluster nodes, because this is maximum cluster throughput, like 10%, 50% of the total cluster", "tokens": [50584, 13630, 13891, 11, 570, 341, 307, 6674, 13630, 44629, 11, 411, 1266, 8923, 2625, 4, 295, 264, 3217, 13630, 51004], "temperature": 0.0, "avg_logprob": -0.20819199222257767, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.018606318160891533}, {"id": 115, "seek": 72780, "start": 740.5999999999999, "end": 745.28, "text": " throughput on reads, where it is fully on reads.", "tokens": [51004, 44629, 322, 15700, 11, 689, 309, 307, 4498, 322, 15700, 13, 51238], "temperature": 0.0, "avg_logprob": -0.20819199222257767, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.018606318160891533}, {"id": 116, "seek": 72780, "start": 745.28, "end": 748.76, "text": " So you're compromising a lot, and you're losing some grades.", "tokens": [51238, 407, 291, 434, 11482, 3436, 257, 688, 11, 293, 291, 434, 7027, 512, 18041, 13, 51412], "temperature": 0.0, "avg_logprob": -0.20819199222257767, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.018606318160891533}, {"id": 117, "seek": 72780, "start": 748.76, "end": 753.04, "text": " So well, that's okay, that's okay.", "tokens": [51412, 407, 731, 11, 300, 311, 1392, 11, 300, 311, 1392, 13, 51626], "temperature": 0.0, "avg_logprob": -0.20819199222257767, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.018606318160891533}, {"id": 118, "seek": 75304, "start": 753.04, "end": 758.24, "text": " But we will, in this journey of talking about the semantics and the synchronization, and", "tokens": [50364, 583, 321, 486, 11, 294, 341, 4671, 295, 1417, 466, 264, 4361, 45298, 293, 264, 19331, 2144, 11, 293, 50624], "temperature": 0.0, "avg_logprob": -0.2646944232103301, "compression_ratio": 1.6243654822335025, "no_speech_prob": 0.23920699954032898}, {"id": 119, "seek": 75304, "start": 758.24, "end": 762.4, "text": " I have just done the dumbiest read-reader split in which I don't care about any read-after", "tokens": [50624, 286, 362, 445, 1096, 264, 10316, 6495, 1401, 12, 2538, 260, 7472, 294, 597, 286, 500, 380, 1127, 466, 604, 1401, 12, 18837, 50832], "temperature": 0.0, "avg_logprob": -0.2646944232103301, "compression_ratio": 1.6243654822335025, "no_speech_prob": 0.23920699954032898}, {"id": 120, "seek": 75304, "start": 762.4, "end": 766.36, "text": " grades or anything, right, the semantics.", "tokens": [50832, 18041, 420, 1340, 11, 558, 11, 264, 4361, 45298, 13, 51030], "temperature": 0.0, "avg_logprob": -0.2646944232103301, "compression_ratio": 1.6243654822335025, "no_speech_prob": 0.23920699954032898}, {"id": 121, "seek": 75304, "start": 766.36, "end": 777.12, "text": " So what happens if I now enable the synchronization for the readers, and I enable the full cluster", "tokens": [51030, 407, 437, 2314, 498, 286, 586, 9528, 264, 19331, 2144, 337, 264, 17147, 11, 293, 286, 9528, 264, 1577, 13630, 51568], "temperature": 0.0, "avg_logprob": -0.2646944232103301, "compression_ratio": 1.6243654822335025, "no_speech_prob": 0.23920699954032898}, {"id": 122, "seek": 77712, "start": 777.12, "end": 783.08, "text": " read-committed semantics, okay, that's fine.", "tokens": [50364, 1401, 12, 13278, 3944, 4361, 45298, 11, 1392, 11, 300, 311, 2489, 13, 50662], "temperature": 0.0, "avg_logprob": -0.15634615356857712, "compression_ratio": 1.5979899497487438, "no_speech_prob": 0.07679389417171478}, {"id": 123, "seek": 77712, "start": 783.08, "end": 790.96, "text": " So what happens is that we go back to basically the same performance that we were having against", "tokens": [50662, 407, 437, 2314, 307, 300, 321, 352, 646, 281, 1936, 264, 912, 3389, 300, 321, 645, 1419, 1970, 51056], "temperature": 0.0, "avg_logprob": -0.15634615356857712, "compression_ratio": 1.5979899497487438, "no_speech_prob": 0.07679389417171478}, {"id": 124, "seek": 77712, "start": 790.96, "end": 796.2, "text": " the whole cluster, which makes sense, because now instead of writing against everyone, you", "tokens": [51056, 264, 1379, 13630, 11, 597, 1669, 2020, 11, 570, 586, 2602, 295, 3579, 1970, 1518, 11, 291, 51318], "temperature": 0.0, "avg_logprob": -0.15634615356857712, "compression_ratio": 1.5979899497487438, "no_speech_prob": 0.07679389417171478}, {"id": 125, "seek": 77712, "start": 796.2, "end": 803.0, "text": " are writing against one and reading from the others, but waiting for the replication.", "tokens": [51318, 366, 3579, 1970, 472, 293, 3760, 490, 264, 2357, 11, 457, 3806, 337, 264, 39911, 13, 51658], "temperature": 0.0, "avg_logprob": -0.15634615356857712, "compression_ratio": 1.5979899497487438, "no_speech_prob": 0.07679389417171478}, {"id": 126, "seek": 80300, "start": 803.0, "end": 812.44, "text": " So we can see that the split reads go between the same frames, is split reads still win,", "tokens": [50364, 407, 321, 393, 536, 300, 264, 7472, 15700, 352, 1296, 264, 912, 12083, 11, 307, 7472, 15700, 920, 1942, 11, 50836], "temperature": 0.0, "avg_logprob": -0.24848548103781307, "compression_ratio": 1.66875, "no_speech_prob": 0.02723112516105175}, {"id": 127, "seek": 80300, "start": 812.44, "end": 819.68, "text": " but it's marginal, and speed writes still win a little bit, but it's also marginal.", "tokens": [50836, 457, 309, 311, 16885, 11, 293, 3073, 13657, 920, 1942, 257, 707, 857, 11, 457, 309, 311, 611, 16885, 13, 51198], "temperature": 0.0, "avg_logprob": -0.24848548103781307, "compression_ratio": 1.66875, "no_speech_prob": 0.02723112516105175}, {"id": 128, "seek": 80300, "start": 819.68, "end": 826.12, "text": " So we have created, now we have the whole cluster having the nice semantics that we wanted and", "tokens": [51198, 407, 321, 362, 2942, 11, 586, 321, 362, 264, 1379, 13630, 1419, 264, 1481, 4361, 45298, 300, 321, 1415, 293, 51520], "temperature": 0.0, "avg_logprob": -0.24848548103781307, "compression_ratio": 1.66875, "no_speech_prob": 0.02723112516105175}, {"id": 129, "seek": 82612, "start": 826.12, "end": 834.4, "text": " etc., but we are in, we haven't fixed our performance throughput.", "tokens": [50364, 5183, 7933, 457, 321, 366, 294, 11, 321, 2378, 380, 6806, 527, 3389, 44629, 13, 50778], "temperature": 0.0, "avg_logprob": -0.3186201762004071, "compression_ratio": 1.494949494949495, "no_speech_prob": 0.11206655204296112}, {"id": 130, "seek": 82612, "start": 834.4, "end": 837.24, "text": " So what can we do in this scenario?", "tokens": [50778, 407, 437, 393, 321, 360, 294, 341, 9005, 30, 50920], "temperature": 0.0, "avg_logprob": -0.3186201762004071, "compression_ratio": 1.494949494949495, "no_speech_prob": 0.11206655204296112}, {"id": 131, "seek": 82612, "start": 837.24, "end": 842.16, "text": " Because what looks like we need a little bit more complicated logic, we are in square one", "tokens": [50920, 1436, 437, 1542, 411, 321, 643, 257, 707, 857, 544, 6179, 9952, 11, 321, 366, 294, 3732, 472, 51166], "temperature": 0.0, "avg_logprob": -0.3186201762004071, "compression_ratio": 1.494949494949495, "no_speech_prob": 0.11206655204296112}, {"id": 132, "seek": 82612, "start": 842.16, "end": 846.68, "text": " of the problem.", "tokens": [51166, 295, 264, 1154, 13, 51392], "temperature": 0.0, "avg_logprob": -0.3186201762004071, "compression_ratio": 1.494949494949495, "no_speech_prob": 0.11206655204296112}, {"id": 133, "seek": 82612, "start": 846.68, "end": 851.36, "text": " Read-side, what do I say, I'm sorry, read-side above the original read-write, writes are", "tokens": [51392, 17604, 12, 1812, 11, 437, 360, 286, 584, 11, 286, 478, 2597, 11, 1401, 12, 1812, 3673, 264, 3380, 1401, 12, 21561, 11, 13657, 366, 51626], "temperature": 0.0, "avg_logprob": -0.3186201762004071, "compression_ratio": 1.494949494949495, "no_speech_prob": 0.11206655204296112}, {"id": 134, "seek": 85136, "start": 851.36, "end": 859.24, "text": " below the original read-write, and we still need protections for our critical reads.", "tokens": [50364, 2507, 264, 3380, 1401, 12, 21561, 11, 293, 321, 920, 643, 29031, 337, 527, 4924, 15700, 13, 50758], "temperature": 0.0, "avg_logprob": -0.316188297914655, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.09549163281917572}, {"id": 135, "seek": 85136, "start": 859.24, "end": 865.0, "text": " Okay, an alternative for avoiding this, we'll be using a single writer for a multi-premier", "tokens": [50758, 1033, 11, 364, 8535, 337, 20220, 341, 11, 321, 603, 312, 1228, 257, 2167, 9936, 337, 257, 4825, 12, 29403, 811, 51046], "temperature": 0.0, "avg_logprob": -0.316188297914655, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.09549163281917572}, {"id": 136, "seek": 85136, "start": 865.0, "end": 871.64, "text": " cluster, which looks like, for what we had seen before, it shouldn't be like a very", "tokens": [51046, 13630, 11, 597, 1542, 411, 11, 337, 437, 321, 632, 1612, 949, 11, 309, 4659, 380, 312, 411, 257, 588, 51378], "temperature": 0.0, "avg_logprob": -0.316188297914655, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.09549163281917572}, {"id": 137, "seek": 85136, "start": 871.64, "end": 878.9200000000001, "text": " bad performance trade-off in terms of whole cluster grids, and then we re-read the critical", "tokens": [51378, 1578, 3389, 4923, 12, 4506, 294, 2115, 295, 1379, 13630, 677, 3742, 11, 293, 550, 321, 319, 12, 2538, 264, 4924, 51742], "temperature": 0.0, "avg_logprob": -0.316188297914655, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.09549163281917572}, {"id": 138, "seek": 87892, "start": 878.92, "end": 889.56, "text": " reads only to the master, and then we, for to the replica, we choose, we choose to read", "tokens": [50364, 15700, 787, 281, 264, 4505, 11, 293, 550, 321, 11, 337, 281, 264, 35456, 11, 321, 2826, 11, 321, 2826, 281, 1401, 50896], "temperature": 0.0, "avg_logprob": -0.23662108466738746, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.09305118024349213}, {"id": 139, "seek": 87892, "start": 889.56, "end": 894.1999999999999, "text": " all the non-critical reads to the other replicas.", "tokens": [50896, 439, 264, 2107, 12, 32255, 804, 15700, 281, 264, 661, 3248, 9150, 13, 51128], "temperature": 0.0, "avg_logprob": -0.23662108466738746, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.09305118024349213}, {"id": 140, "seek": 87892, "start": 894.1999999999999, "end": 902.76, "text": " So critical reads to the primary, okay, and now we're being processed equally to the picture,", "tokens": [51128, 407, 4924, 15700, 281, 264, 6194, 11, 1392, 11, 293, 586, 321, 434, 885, 18846, 12309, 281, 264, 3036, 11, 51556], "temperature": 0.0, "avg_logprob": -0.23662108466738746, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.09305118024349213}, {"id": 141, "seek": 90276, "start": 902.76, "end": 907.56, "text": " because that's something that we can do with process equally very easily.", "tokens": [50364, 570, 300, 311, 746, 300, 321, 393, 360, 365, 1399, 12309, 588, 3612, 13, 50604], "temperature": 0.0, "avg_logprob": -0.3611348599803691, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.03760663419961929}, {"id": 142, "seek": 90276, "start": 907.56, "end": 911.56, "text": " Contrary to the Mexico-Purilus, which offers you both of the things that you need, which", "tokens": [50604, 4839, 81, 822, 281, 264, 8612, 12, 47, 374, 388, 301, 11, 597, 7736, 291, 1293, 295, 264, 721, 300, 291, 643, 11, 597, 50804], "temperature": 0.0, "avg_logprob": -0.3611348599803691, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.03760663419961929}, {"id": 143, "seek": 90276, "start": 911.56, "end": 917.52, "text": " is reverection and anthropocontrol.", "tokens": [50804, 307, 3698, 323, 882, 293, 22727, 905, 896, 6623, 13, 51102], "temperature": 0.0, "avg_logprob": -0.3611348599803691, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.03760663419961929}, {"id": 144, "seek": 90276, "start": 917.52, "end": 925.6, "text": " The testing scenario, we are going to have a 10-90 ratio of writes versus reads, we are", "tokens": [51102, 440, 4997, 9005, 11, 321, 366, 516, 281, 362, 257, 1266, 12, 7771, 8509, 295, 13657, 5717, 15700, 11, 321, 366, 51506], "temperature": 0.0, "avg_logprob": -0.3611348599803691, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.03760663419961929}, {"id": 145, "seek": 90276, "start": 925.6, "end": 930.4399999999999, "text": " going to have a 5% of critical reads and a 95% of regular reads.", "tokens": [51506, 516, 281, 362, 257, 1025, 4, 295, 4924, 15700, 293, 257, 13420, 4, 295, 3890, 15700, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3611348599803691, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.03760663419961929}, {"id": 146, "seek": 93044, "start": 930.8800000000001, "end": 942.4000000000001, "text": " Okay, which is, well, I would say that changing this into a more balanced ratio with an impact,", "tokens": [50386, 1033, 11, 597, 307, 11, 731, 11, 286, 576, 584, 300, 4473, 341, 666, 257, 544, 13902, 8509, 365, 364, 2712, 11, 50962], "temperature": 0.0, "avg_logprob": -0.21429379090018894, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.08099624514579773}, {"id": 147, "seek": 93044, "start": 942.4000000000001, "end": 948.7600000000001, "text": " okay, with an impact so much, thank you, with an impact so much the performance, the problem", "tokens": [50962, 1392, 11, 365, 364, 2712, 370, 709, 11, 1309, 291, 11, 365, 364, 2712, 370, 709, 264, 3389, 11, 264, 1154, 51280], "temperature": 0.0, "avg_logprob": -0.21429379090018894, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.08099624514579773}, {"id": 148, "seek": 93044, "start": 948.7600000000001, "end": 954.5200000000001, "text": " is that if the total throughput is what you care about, okay, so the ratio is not as important", "tokens": [51280, 307, 300, 498, 264, 3217, 44629, 307, 437, 291, 1127, 466, 11, 1392, 11, 370, 264, 8509, 307, 406, 382, 1021, 51568], "temperature": 0.0, "avg_logprob": -0.21429379090018894, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.08099624514579773}, {"id": 149, "seek": 93044, "start": 954.5200000000001, "end": 959.12, "text": " as the, how much you are going to stress the cluster with the throughput that you want", "tokens": [51568, 382, 264, 11, 577, 709, 291, 366, 516, 281, 4244, 264, 13630, 365, 264, 44629, 300, 291, 528, 51798], "temperature": 0.0, "avg_logprob": -0.21429379090018894, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.08099624514579773}, {"id": 150, "seek": 95912, "start": 959.12, "end": 963.08, "text": " for that ratio.", "tokens": [50364, 337, 300, 8509, 13, 50562], "temperature": 0.0, "avg_logprob": -0.21817151387532552, "compression_ratio": 1.7125, "no_speech_prob": 0.016522562131285667}, {"id": 151, "seek": 95912, "start": 963.08, "end": 969.24, "text": " So now we're being processed equally into the picture.", "tokens": [50562, 407, 586, 321, 434, 885, 18846, 12309, 666, 264, 3036, 13, 50870], "temperature": 0.0, "avg_logprob": -0.21817151387532552, "compression_ratio": 1.7125, "no_speech_prob": 0.016522562131285667}, {"id": 152, "seek": 95912, "start": 969.24, "end": 975.16, "text": " In this scenario, these are the non-critical reads, these are the critical reads, and these", "tokens": [50870, 682, 341, 9005, 11, 613, 366, 264, 2107, 12, 32255, 804, 15700, 11, 613, 366, 264, 4924, 15700, 11, 293, 613, 51166], "temperature": 0.0, "avg_logprob": -0.21817151387532552, "compression_ratio": 1.7125, "no_speech_prob": 0.016522562131285667}, {"id": 153, "seek": 95912, "start": 975.16, "end": 978.8, "text": " are the writes.", "tokens": [51166, 366, 264, 13657, 13, 51348], "temperature": 0.0, "avg_logprob": -0.21817151387532552, "compression_ratio": 1.7125, "no_speech_prob": 0.016522562131285667}, {"id": 154, "seek": 95912, "start": 978.8, "end": 985.48, "text": " So we have improved almost a 50% on non-critical reads, we are retaining more or less the write", "tokens": [51348, 407, 321, 362, 9689, 1920, 257, 2625, 4, 322, 2107, 12, 32255, 804, 15700, 11, 321, 366, 34936, 544, 420, 1570, 264, 2464, 51682], "temperature": 0.0, "avg_logprob": -0.21817151387532552, "compression_ratio": 1.7125, "no_speech_prob": 0.016522562131285667}, {"id": 155, "seek": 98548, "start": 985.48, "end": 999.64, "text": " load, and we have an extra 1,000 reads in the critical reads.", "tokens": [50364, 3677, 11, 293, 321, 362, 364, 2857, 502, 11, 1360, 15700, 294, 264, 4924, 15700, 13, 51072], "temperature": 0.0, "avg_logprob": -0.26873245239257815, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01695435680449009}, {"id": 156, "seek": 98548, "start": 999.64, "end": 1003.9200000000001, "text": " So we are trying to, we have able to preserve the great throughput, increase the throughput", "tokens": [51072, 407, 321, 366, 1382, 281, 11, 321, 362, 1075, 281, 15665, 264, 869, 44629, 11, 3488, 264, 44629, 51286], "temperature": 0.0, "avg_logprob": -0.26873245239257815, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01695435680449009}, {"id": 157, "seek": 98548, "start": 1003.9200000000001, "end": 1011.84, "text": " of our 50%, and this is against the whole cluster read load and progress with the synchronization", "tokens": [51286, 295, 527, 2625, 8923, 293, 341, 307, 1970, 264, 1379, 13630, 1401, 3677, 293, 4205, 365, 264, 19331, 2144, 51682], "temperature": 0.0, "avg_logprob": -0.26873245239257815, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01695435680449009}, {"id": 158, "seek": 98548, "start": 1011.84, "end": 1012.84, "text": " enabled.", "tokens": [51682, 15172, 13, 51732], "temperature": 0.0, "avg_logprob": -0.26873245239257815, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01695435680449009}, {"id": 159, "seek": 101284, "start": 1013.84, "end": 1016.32, "text": " And this is the most important part of it.", "tokens": [50414, 400, 341, 307, 264, 881, 1021, 644, 295, 309, 13, 50538], "temperature": 0.0, "avg_logprob": -0.21093533963573222, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03563982620835304}, {"id": 160, "seek": 101284, "start": 1016.32, "end": 1021.88, "text": " This is like this, because I decided that it was going to be like this, because if I", "tokens": [50538, 639, 307, 411, 341, 11, 570, 286, 3047, 300, 309, 390, 516, 281, 312, 411, 341, 11, 570, 498, 286, 50816], "temperature": 0.0, "avg_logprob": -0.21093533963573222, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03563982620835304}, {"id": 161, "seek": 101284, "start": 1021.88, "end": 1028.1200000000001, "text": " went back here, and I decided that I am not going to limit the throughput on the reads,", "tokens": [50816, 1437, 646, 510, 11, 293, 286, 3047, 300, 286, 669, 406, 516, 281, 4948, 264, 44629, 322, 264, 15700, 11, 51128], "temperature": 0.0, "avg_logprob": -0.21093533963573222, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03563982620835304}, {"id": 162, "seek": 101284, "start": 1028.1200000000001, "end": 1034.64, "text": " I will go as up as almost the whole cluster throughput, and I will compromise the writes", "tokens": [51128, 286, 486, 352, 382, 493, 382, 1920, 264, 1379, 13630, 44629, 11, 293, 286, 486, 18577, 264, 13657, 51454], "temperature": 0.0, "avg_logprob": -0.21093533963573222, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03563982620835304}, {"id": 163, "seek": 101284, "start": 1034.64, "end": 1040.8400000000001, "text": " again, that will go below what you were seeing before at any time in another benchmark.", "tokens": [51454, 797, 11, 300, 486, 352, 2507, 437, 291, 645, 2577, 949, 412, 604, 565, 294, 1071, 18927, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21093533963573222, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03563982620835304}, {"id": 164, "seek": 104084, "start": 1041.28, "end": 1042.36, "text": " And why is that?", "tokens": [50386, 400, 983, 307, 300, 30, 50440], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 165, "seek": 104084, "start": 1042.36, "end": 1045.52, "text": " Because the total cluster throughput is what it is.", "tokens": [50440, 1436, 264, 3217, 13630, 44629, 307, 437, 309, 307, 13, 50598], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 166, "seek": 104084, "start": 1045.52, "end": 1049.04, "text": " So what you need is to control what you want to do with that throughput.", "tokens": [50598, 407, 437, 291, 643, 307, 281, 1969, 437, 291, 528, 281, 360, 365, 300, 44629, 13, 50774], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 167, "seek": 104084, "start": 1049.04, "end": 1055.48, "text": " You cannot expect just to get more reads for free.", "tokens": [50774, 509, 2644, 2066, 445, 281, 483, 544, 15700, 337, 1737, 13, 51096], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 168, "seek": 104084, "start": 1055.48, "end": 1062.8, "text": " So the conclusions, multi-primary clusters can be a lot of benefit for you, let's please", "tokens": [51096, 407, 264, 22865, 11, 4825, 12, 1424, 332, 822, 23313, 393, 312, 257, 688, 295, 5121, 337, 291, 11, 718, 311, 1767, 51462], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 169, "seek": 104084, "start": 1062.8, "end": 1063.8, "text": " set up.", "tokens": [51462, 992, 493, 13, 51512], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 170, "seek": 104084, "start": 1063.8, "end": 1066.6399999999999, "text": " And it's just like this.", "tokens": [51512, 400, 309, 311, 445, 411, 341, 13, 51654], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 171, "seek": 104084, "start": 1066.6399999999999, "end": 1069.56, "text": " System measurements analysis is really hard.", "tokens": [51654, 8910, 15383, 5215, 307, 534, 1152, 13, 51800], "temperature": 0.0, "avg_logprob": -0.32766873908765387, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.01577090285718441}, {"id": 172, "seek": 106956, "start": 1070.56, "end": 1072.6399999999999, "text": " Really, really hard.", "tokens": [50414, 4083, 11, 534, 1152, 13, 50518], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 173, "seek": 106956, "start": 1072.6399999999999, "end": 1078.56, "text": " And especially benchmarking is also very hard, because most of the things that I have said", "tokens": [50518, 400, 2318, 18927, 278, 307, 611, 588, 1152, 11, 570, 881, 295, 264, 721, 300, 286, 362, 848, 50814], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 174, "seek": 106956, "start": 1078.56, "end": 1083.32, "text": " here, they are right, but they are right in this scenario.", "tokens": [50814, 510, 11, 436, 366, 558, 11, 457, 436, 366, 558, 294, 341, 9005, 13, 51052], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 175, "seek": 106956, "start": 1083.32, "end": 1088.48, "text": " And if you change slightly the scenario, it may not be.", "tokens": [51052, 400, 498, 291, 1319, 4748, 264, 9005, 11, 309, 815, 406, 312, 13, 51310], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 176, "seek": 106956, "start": 1088.48, "end": 1092.1599999999999, "text": " Adapting the system to your workload is what you always want to do.", "tokens": [51310, 1999, 569, 783, 264, 1185, 281, 428, 20139, 307, 437, 291, 1009, 528, 281, 360, 13, 51494], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 177, "seek": 106956, "start": 1092.1599999999999, "end": 1096.0, "text": " A different workload will change everything.", "tokens": [51494, 316, 819, 20139, 486, 1319, 1203, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2788483630651715, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.009539003483951092}, {"id": 178, "seek": 109600, "start": 1097.0, "end": 1101.12, "text": " The final conclusion is that control is everything.", "tokens": [50414, 440, 2572, 10063, 307, 300, 1969, 307, 1203, 13, 50620], "temperature": 0.0, "avg_logprob": -0.31568155056092795, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.008098393678665161}, {"id": 179, "seek": 109600, "start": 1101.12, "end": 1104.92, "text": " Being able to control the throughput and being able to control what you want to do with your", "tokens": [50620, 8891, 1075, 281, 1969, 264, 44629, 293, 885, 1075, 281, 1969, 437, 291, 528, 281, 360, 365, 428, 50810], "temperature": 0.0, "avg_logprob": -0.31568155056092795, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.008098393678665161}, {"id": 180, "seek": 109600, "start": 1104.92, "end": 1110.6, "text": " load is what is going to decide the performance of the cluster, more than anything else.", "tokens": [50810, 3677, 307, 437, 307, 516, 281, 4536, 264, 3389, 295, 264, 13630, 11, 544, 813, 1340, 1646, 13, 51094], "temperature": 0.0, "avg_logprob": -0.31568155056092795, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.008098393678665161}, {"id": 181, "seek": 109600, "start": 1110.6, "end": 1114.88, "text": " And for that, process SQL is a great tool.", "tokens": [51094, 400, 337, 300, 11, 1399, 19200, 307, 257, 869, 2290, 13, 51308], "temperature": 0.0, "avg_logprob": -0.31568155056092795, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.008098393678665161}, {"id": 182, "seek": 109600, "start": 1114.88, "end": 1121.28, "text": " And thank you a lot for your attention, and happy specificity.", "tokens": [51308, 400, 1309, 291, 257, 688, 337, 428, 3202, 11, 293, 2055, 2685, 507, 13, 51628], "temperature": 0.0, "avg_logprob": -0.31568155056092795, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.008098393678665161}, {"id": 183, "seek": 112128, "start": 1121.28, "end": 1125.76, "text": " You have five minutes for the questions.", "tokens": [50364, 509, 362, 1732, 2077, 337, 264, 1651, 13, 50588], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 184, "seek": 112128, "start": 1125.76, "end": 1126.76, "text": " Okay.", "tokens": [50588, 1033, 13, 50638], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 185, "seek": 112128, "start": 1126.76, "end": 1133.56, "text": " How do you know the maximum throughput of a cluster?", "tokens": [50638, 1012, 360, 291, 458, 264, 6674, 44629, 295, 257, 13630, 30, 50978], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 186, "seek": 112128, "start": 1133.56, "end": 1135.36, "text": " You just measure.", "tokens": [50978, 509, 445, 3481, 13, 51068], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 187, "seek": 112128, "start": 1135.36, "end": 1141.28, "text": " Like you create an artificial environment, I would say, where I say that it's very hard,", "tokens": [51068, 1743, 291, 1884, 364, 11677, 2823, 11, 286, 576, 584, 11, 689, 286, 584, 300, 309, 311, 588, 1152, 11, 51364], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 188, "seek": 112128, "start": 1141.28, "end": 1145.96, "text": " because it's probably not going to be the typical load that you're going to have.", "tokens": [51364, 570, 309, 311, 1391, 406, 516, 281, 312, 264, 7476, 3677, 300, 291, 434, 516, 281, 362, 13, 51598], "temperature": 0.0, "avg_logprob": -0.3016869681222098, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.03553024306893349}, {"id": 189, "seek": 114596, "start": 1145.96, "end": 1153.04, "text": " You try to replicate, and then you measure, because otherwise it's...", "tokens": [50364, 509, 853, 281, 25356, 11, 293, 550, 291, 3481, 11, 570, 5911, 309, 311, 485, 50718], "temperature": 0.0, "avg_logprob": -0.34175856071606014, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0563100203871727}, {"id": 190, "seek": 114596, "start": 1153.04, "end": 1157.96, "text": " I would say that you are not going to know which is the limiting factor until you try.", "tokens": [50718, 286, 576, 584, 300, 291, 366, 406, 516, 281, 458, 597, 307, 264, 22083, 5952, 1826, 291, 853, 13, 50964], "temperature": 0.0, "avg_logprob": -0.34175856071606014, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0563100203871727}, {"id": 191, "seek": 114596, "start": 1161.96, "end": 1165.96, "text": " What did you use to measure the workload?", "tokens": [51164, 708, 630, 291, 764, 281, 3481, 264, 20139, 30, 51364], "temperature": 0.0, "avg_logprob": -0.34175856071606014, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0563100203871727}, {"id": 192, "seek": 114596, "start": 1165.96, "end": 1167.1200000000001, "text": " Seizebench.", "tokens": [51364, 1100, 1125, 47244, 13, 51422], "temperature": 0.0, "avg_logprob": -0.34175856071606014, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0563100203871727}, {"id": 193, "seek": 116712, "start": 1167.12, "end": 1176.3999999999999, "text": " I was using Seizebench, the Lua, one with all the scripts, and the old, very old, magical", "tokens": [50364, 286, 390, 1228, 1100, 1125, 47244, 11, 264, 441, 4398, 11, 472, 365, 439, 264, 23294, 11, 293, 264, 1331, 11, 588, 1331, 11, 12066, 50828], "temperature": 0.0, "avg_logprob": -0.26361875366746335, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.18074524402618408}, {"id": 194, "seek": 116712, "start": 1176.3999999999999, "end": 1183.84, "text": " one, because it has, let's use, because you can do the same thing with the Lua one.", "tokens": [50828, 472, 11, 570, 309, 575, 11, 718, 311, 764, 11, 570, 291, 393, 360, 264, 912, 551, 365, 264, 441, 4398, 472, 13, 51200], "temperature": 0.0, "avg_logprob": -0.26361875366746335, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.18074524402618408}, {"id": 195, "seek": 116712, "start": 1183.84, "end": 1189.4799999999998, "text": " You can create your own things and et cetera, but I just wanted to benchmark also what if", "tokens": [51200, 509, 393, 1884, 428, 1065, 721, 293, 1030, 11458, 11, 457, 286, 445, 1415, 281, 18927, 611, 437, 498, 51482], "temperature": 0.0, "avg_logprob": -0.26361875366746335, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.18074524402618408}, {"id": 196, "seek": 116712, "start": 1189.4799999999998, "end": 1193.6, "text": " I am selecting from different tables of the one that I am writing for and that thing.", "tokens": [51482, 286, 669, 18182, 490, 819, 8020, 295, 264, 472, 300, 286, 669, 3579, 337, 293, 300, 551, 13, 51688], "temperature": 0.0, "avg_logprob": -0.26361875366746335, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.18074524402618408}, {"id": 197, "seek": 116712, "start": 1193.6, "end": 1196.9599999999998, "text": " And name selection is not something that you have in the Lua one by default.", "tokens": [51688, 400, 1315, 9450, 307, 406, 746, 300, 291, 362, 294, 264, 441, 4398, 472, 538, 7576, 13, 51856], "temperature": 0.0, "avg_logprob": -0.26361875366746335, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.18074524402618408}, {"id": 198, "seek": 119696, "start": 1196.96, "end": 1200.1200000000001, "text": " So you don't have throughput limiting by default.", "tokens": [50364, 407, 291, 500, 380, 362, 44629, 22083, 538, 7576, 13, 50522], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 199, "seek": 119696, "start": 1200.1200000000001, "end": 1202.56, "text": " And it's something that you have in the old one in the options.", "tokens": [50522, 400, 309, 311, 746, 300, 291, 362, 294, 264, 1331, 472, 294, 264, 3956, 13, 50644], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 200, "seek": 119696, "start": 1202.56, "end": 1206.8400000000001, "text": " So for convenience, I use a mix.", "tokens": [50644, 407, 337, 19283, 11, 286, 764, 257, 2890, 13, 50858], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 201, "seek": 119696, "start": 1206.8400000000001, "end": 1214.32, "text": " Which, by the way, no, it impacts the performance, depending on what you're measuring.", "tokens": [50858, 3013, 11, 538, 264, 636, 11, 572, 11, 309, 11606, 264, 3389, 11, 5413, 322, 437, 291, 434, 13389, 13, 51232], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 202, "seek": 119696, "start": 1214.32, "end": 1218.76, "text": " If you're measuring in process equal, if you're measuring against the other proxy, it's different", "tokens": [51232, 759, 291, 434, 13389, 294, 1399, 2681, 11, 498, 291, 434, 13389, 1970, 264, 661, 29690, 11, 309, 311, 819, 51454], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 203, "seek": 119696, "start": 1218.76, "end": 1220.64, "text": " depending even on the tool that you're choosing.", "tokens": [51454, 5413, 754, 322, 264, 2290, 300, 291, 434, 10875, 13, 51548], "temperature": 0.0, "avg_logprob": -0.34738470583545916, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.020413056015968323}, {"id": 204, "seek": 122064, "start": 1220.64, "end": 1230.64, "text": " You said that you used Seizebench, I wanted to hear, because in my test, when I was trying", "tokens": [50364, 509, 848, 300, 291, 1143, 1100, 1125, 47244, 11, 286, 1415, 281, 1568, 11, 570, 294, 452, 1500, 11, 562, 286, 390, 1382, 50864], "temperature": 0.0, "avg_logprob": -0.5860991594268055, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.42166760563850403}, {"id": 205, "seek": 122064, "start": 1230.64, "end": 1238.64, "text": " to do multi-write multiple loads, I noticed that the drop of write performance was because", "tokens": [50864, 281, 360, 4825, 12, 21561, 3866, 12668, 11, 286, 5694, 300, 264, 3270, 295, 2464, 3389, 390, 570, 51264], "temperature": 0.0, "avg_logprob": -0.5860991594268055, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.42166760563850403}, {"id": 206, "seek": 122064, "start": 1238.64, "end": 1240.64, "text": " of write complete.", "tokens": [51264, 295, 2464, 3566, 13, 51364], "temperature": 0.0, "avg_logprob": -0.5860991594268055, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.42166760563850403}, {"id": 207, "seek": 122064, "start": 1240.64, "end": 1241.64, "text": " Yes.", "tokens": [51364, 1079, 13, 51414], "temperature": 0.0, "avg_logprob": -0.5860991594268055, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.42166760563850403}, {"id": 208, "seek": 122064, "start": 1241.64, "end": 1247.64, "text": " So if it's not right here to me, how you're achieving, like, better for multi-writing,", "tokens": [51414, 407, 498, 309, 311, 406, 558, 510, 281, 385, 11, 577, 291, 434, 19626, 11, 411, 11, 1101, 337, 4825, 12, 19868, 11, 51714], "temperature": 0.0, "avg_logprob": -0.5860991594268055, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.42166760563850403}, {"id": 209, "seek": 124764, "start": 1248.64, "end": 1252.64, "text": " because you need to certify all loads.", "tokens": [50414, 570, 291, 643, 281, 5351, 2505, 439, 12668, 13, 50614], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 210, "seek": 124764, "start": 1252.64, "end": 1258.64, "text": " How I achieve better throughput writing to a single note that the whole, to multiple", "tokens": [50614, 1012, 286, 4584, 1101, 44629, 3579, 281, 257, 2167, 3637, 300, 264, 1379, 11, 281, 3866, 50914], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 211, "seek": 124764, "start": 1258.64, "end": 1259.64, "text": " notes.", "tokens": [50914, 5570, 13, 50964], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 212, "seek": 124764, "start": 1259.64, "end": 1260.64, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 213, "seek": 124764, "start": 1260.64, "end": 1267.64, "text": " Because from what I see, you have better performance writing multiple loads than a single load.", "tokens": [51014, 1436, 490, 437, 286, 536, 11, 291, 362, 1101, 3389, 3579, 3866, 12668, 813, 257, 2167, 3677, 13, 51364], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 214, "seek": 124764, "start": 1267.64, "end": 1272.64, "text": " Probably because I was having very, very few conflicts, because the size of the table", "tokens": [51364, 9210, 570, 286, 390, 1419, 588, 11, 588, 1326, 19807, 11, 570, 264, 2744, 295, 264, 3199, 51614], "temperature": 0.0, "avg_logprob": -0.28707276857816255, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.08830606192350388}, {"id": 215, "seek": 127264, "start": 1272.64, "end": 1274.64, "text": " that I choose were very big.", "tokens": [50364, 300, 286, 2826, 645, 588, 955, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 216, "seek": 127264, "start": 1274.64, "end": 1278.64, "text": " And I was having very, very few conflicts during that testing.", "tokens": [50464, 400, 286, 390, 1419, 588, 11, 588, 1326, 19807, 1830, 300, 4997, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 217, "seek": 127264, "start": 1278.64, "end": 1283.64, "text": " It was a very, very favorable scenario for writing.", "tokens": [50664, 467, 390, 257, 588, 11, 588, 29557, 9005, 337, 3579, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 218, "seek": 127264, "start": 1283.64, "end": 1285.64, "text": " That's why I say that it's super ideal.", "tokens": [50914, 663, 311, 983, 286, 584, 300, 309, 311, 1687, 7157, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 219, "seek": 127264, "start": 1285.64, "end": 1289.64, "text": " When I say that it was ideal, it was because it's super, super ideal.", "tokens": [51014, 1133, 286, 584, 300, 309, 390, 7157, 11, 309, 390, 570, 309, 311, 1687, 11, 1687, 7157, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 220, "seek": 127264, "start": 1289.64, "end": 1290.64, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 221, "seek": 127264, "start": 1290.64, "end": 1291.64, "text": " I don't know.", "tokens": [51264, 286, 500, 380, 458, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 222, "seek": 127264, "start": 1297.64, "end": 1299.64, "text": " Thank you very much.", "tokens": [51614, 1044, 291, 588, 709, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 223, "seek": 127264, "start": 1299.64, "end": 1300.64, "text": " Thank you.", "tokens": [51714, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1742220736564474, "compression_ratio": 1.7231638418079096, "no_speech_prob": 0.04900284856557846}, {"id": 224, "seek": 130264, "start": 1302.64, "end": 1303.64, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.39135273297627765, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9838893413543701}], "language": "en"}