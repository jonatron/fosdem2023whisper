{"text": " Okay. Thank you, everyone. Our next speaker has some interesting news for your CI. There are better solutions than the YAML you're used to. Mark is going to talk to us about building a CI pipeline with Dagger in Go. Thank you. Thank you. Can you hear me? Okay. So very important information before we get started. I have some Dagger stickers here if you want to pick them up. I don't know. Maybe I can just leave them after the talk or you can come to me and pick them up. I'll leave the stickers over here. Perfect. People can grab them. Thank you. An important thing. Stickers are for your laptop, not for the room. Every sticker you put inside a room involves them we have to pay for. So keep them for yourself. Yeah. Well, that's why we are going to conferences for it. So thank you again for joining me today. My name is Mark. And for the better part of the decade I've been focusing on helping engineering teams focus build, helping them focus on their business applications, building their best business applications instead of worrying about things like CI and how they are being deployed. And I have this fake title at Cisco technical lead that I decided that I would come clean here today that I'm really nothing more than just a YAML engineer. Ooh, that feels good. Anyone else want to onboard them themselves? Any other YAML engineers here? All right. So let's talk about CI-CD a bit. And CI or CI systems improved a lot in the last couple of years. We have new and more evolved CI solutions today. But we still have some challenges that we face every day. Like the one I've been already hinting at, YAML. Obviously, YAML is one of the biggest problems with CI systems today. Admittedly, sometimes, like using YAML to build the declarative pipeline can be fine. But, man, you miss a space. The whole thing just broke. The whole thing just breaks. And you might not even know where to start debugging it. So YAML makes it often really hard for people to even just touch CI. And the other thing is CI tends to break for no obvious reasons. Like, the pipeline that worked yesterday may not work today and you don't really know why. And obviously, as developers, when something breaks in production, we can just tell the ops people to worry about it. But with CI, that's not really the case. Like, we have to interact with CI. And if something goes wrong, we might have to be the ones who fix it. And with the currently available CI solutions today, you can't really, like, everything was running in the cloud or in the remote system. You can't really have or you don't really have tools that you can use to debug effectively. You have to start gassing and start changing some YAML config. And you have to push that to a repository and then wait for the CI to get triggered. And you have to go through this whole and long feedback loop to be able to debug what's going wrong and to be able to fix that. And that's a pain. Like, it takes a lot of time. It's really a huge waste of time. And it's really painful to do that. Now, sometimes it's not the CI that's wrong. Sometimes it's you, like, pushing something that you shouldn't be pushing to the repository like tests are not passing or the linters are not passing or something else goes wrong. And again, you may have the tools locally in your machine, but you may not have the same versions. You may not have the same setup as in the CI. And it may just break in the CI even though you ran the test locally and everything was green, it may still fail in the CI. And you still have to go through the same long feedback loop again and again, trying to fix that, instead of being able to run the whole thing locally and being confident that it will just work in the CI as well. And probably there are other challenges with CI, but these are the ones that wasted hours from my life in the last couple years. So how can Dagger provide an answer to this problem? So first of all, Dagger is a programmable and deportable CI solution, which means you can run your CI pipelines basically anywhere. We will get to how it does that. But the important thing is that you can run your CI pipelines anywhere using the same environment, which means if it runs on your machine, then you can be confident that it will run the same way in your own CI system. Now, that's a great thing for a number of reasons, because when you start building a pipeline, for example, using any of the CI systems today, you still have to go through that feedback loop, like adding some config and then pushing it to the GitHub OO and trying to figure out if it works or not, and then changing until it works. Now, the ability to run this whole thing locally, it's much shorter feedback loop, so you can build your own CI pipelines much more quickly than using some remote system. The other thing is that if something goes wrong, you have the whole thing running locally. So again, shorter feedback loop, you have more tools to debug, so it's much easier to figure out what goes wrong, even if it's either the CI pipeline or your code. The other thing about Dagger is that you can actually write your pipelines in your own preferred language. Now, not any language, obviously. Some of the languages that Dagger supports, but that's already much better than YAML. You can write your pipelines in Go, Python or TypeScript, ThinkQ, EvenQ, but that's already much better than YAML. You can write your own pipelines in code, and you don't have to invent or use some weird syntax, for example, to represent dependencies between steps or between different pipelines. You can just do that in plain code, so that's great. And all those, so the possibility of writing pipelines in your own language points to the fact that you can avoid Pandor locking entirely. You would still have a CI solution like Jenkins or GitHub actions or whatever, and you would still run Dagger on those systems, but you would have to write a very thin integration layer just to run the Dagger pipelines. You can, you would be much more confident that the pipelines would run the same way on the CI system as on your computer, and yeah, you can avoid Pandor locking entirely. You can move to another CI system if you want to, and you may say that it doesn't happen often, but when it does, man, it's really painful, like converting from one YAML to another or one YAML to, I don't know, Groovy or JenkinsFly or something, that hurts. And lastly, costly caching. So every CI system or most CI systems have their own caching solutions where you can cache the dependencies of your language or dependency manager, but that requires configuration. You have to make sure that you configure it right, otherwise, well, it could either like grow the cache endlessly and then you will be paying a lot of money for that, or it would just be non-functional at all and it wouldn't cache anything properly. Now with Dagger, everything is cached by default, like every step is cached. You can think about it like a Docker file. Every instruction or the result of it is basically cached in a separate layer in the Docker file, and if nothing changed between the steps, then when you run it again, it will basically run the same way and it will come from the cache. That's really how Dagger works. Obviously, you have some control over what you want to cache and how you want to do it, but by default, Dagger got that covered for you. Now how does all this work behind the scenes? If I had to describe it in one word, it's obviously containers. Now we can do it ourselves today, right? We could just run everything in a container and it would be reasonable to say that it will run on the CEI the same way. What Dagger adds to the mix here is that you can actually build pipelines with code and that would be translated into build pipelines. So you would use the Dagger SDK, the language SDK that Dagger provides. Again, today, I believe it's for Go, TypeScript, Python, maybe Q as well. But the underlying API is actually the GraphQL. So if you have a language client for GraphQL, you can actually build your own SDK if you want to, or you can just write GraphQL queries and send those directly to the Dagger engine. But basically, you write your own pipelines using this SDK in your own language, and then the SDK will basically send GraphQL queries to the Dagger engine. Now, when you run the whole thing locally first, then the Dagger SDK will actually launch the Dagger engine for you. All it needs is really a Docker-compatible container runtime. So if you have Docker on your computer or in your CEI, then you can run your Dagger pipeline basically. So that's once more the portability of this whole thing. If you have Docker on your machine and Docker basically runs anywhere these days, then you can run the Dagger pipeline there. So locally, when you launch this for the first time, the Dagger SDK will launch the Dagger engine for you and you send these GraphQL queries. You'll see a couple examples how that looks like in the SDK, and the Dagger engine basically builds a DAG, directed basically graph of all those steps, and then sends it through, well, it says an OCI runtime, I believe currently Docker is the only supported runtime, but sends through an OCI runtime and runs the whole thing in containers for you. And then when, obviously, when a pipeline is finished, you get back the results, and you can use the results in another pipeline if you want to. For example, the result of your build pipeline would be used in your deployed pipeline, and you could deploy or project or whatever you have. So that's how Dagger works under the hood. And let's take a look at an actual example. Let's see. So the example will be go because this is the go-to-room. Can you see it from the back? Okay, cool. So the first thing you need to use the Dagger SDK and go is importing this module from Dagger. It's the Dagger SDK for go that you can use to interface with the Dagger engine. And once you have that, you can basically start writing your own program. Now, in this case, I'm using mage. I'm not sure if you're familiar with that, but it's basically like a make file like solution for go. You can write these functions, and mage will basically compile the binary from that and execute it like it would work with make. Now, you can absolutely import this Dagger package in your own application if you want to. In case of applications, it's probably not a huge deal if you have an additional dependency in your go modules. If you're writing the library, though, you might want to create a separate module, for example, called CI, and import the Dagger SDK in that separate module so you don't import Dagger as a development dependency in your libraries go that modified. I know it still won't be built or still won't be in the final binary if you import that library, but some people get to know it if they see dependencies that is actually not necessary for the library. So make the life easier for your peers, and if you develop a library and use Dagger, just create a separate module and put all the Dagger-related code there. The first thing you need to do when you want to write pipeline with Dagger is call this DaggerConnect function, which will basically connect to your Docker runtime, and it will launch the Dagger engine for you and start the so-called session. Now, within that session, you can start building your actual pipelines using these containers. Now, it's pretty similar to how a Docker file would look like for good reason, but what you can do here is basically use some of the same instructions as you would do in a Docker file. You can obviously go from a base image, which will be going in a Go project, for example. You can mount your source code. That's how you would have access to the source code within the container, and then you can run a bunch of commands like test or you can do the same with the linter, for example. And the other two here, these are the mounted caches. You can do that with built-kit, actually. I believe that's a built-kit functionality, so you can mount a cache directory to the container that will not actually be part of the container, but it will be a mounted cache directory from your host. Now, let's see if I can run this. So, I'm using the mage miner here. I'm telling it to change to the CI directory because it's a separate module, and then I'm telling it to use the current jet. Can you hear me? Okay. I don't know what happened there. And then I'm basically just telling it to run the test function here again, similarly how a mage file would look like. Now, let's see what happens. Kind of hope that I don't have to download all those container images. Let's see. Let's get some locks here. I swear this worked like a couple of hours ago. Oh, you know what? I think I don't have Docker running. Yeah. That's a problem. Yeah, maybe we'll, yeah. So, I don't have the Docker engine running at the moment. Let's see. This should start a new container. I mean, this should start a new container. Let's see what's going on. All right. This is not great. Can we all pray to the demo gods, please? Thank you. Okay. You all just have to believe me that this actually works. Okay. So, here are the locks from the previous run. So, this actually worked before. Yeah, it says, let's see. Okay, let's try that. Let's see. Do we have internet connection here? Yeah, we do have internet connection. Okay. Well, we'll have to work with the locks from here. So, well, basically what happens here is when it works is it just runs the whole thing within this goal and the image mounts the source code and then runs the goal that test command and just gives back the results. Obviously, this is the build log, like this is the debug log, but normally it would just output the, output of the go test command. Or the go lxci command. Let's see. It's still not working. Let's try from hotspot. Maybe that works better. Anyway, if, well, if someone wants to get back their money, sorry, folks, this is a free conference. Anyway, yeah, you will just have to believe me that this works, but I will try to make this work after the presentation. Now, if you take a look at the code here, this is still not very user friendly. If you don't know how dagger works or if you don't know what happens here, then it's not really useful to you. You will have to go to the documentation and understand how this whole thing works, when it works. So, but the good thing is that this is like, this is not an arbitrary YAML interface you have to use, so we can actually make this a bit better if we want to. And what I did in the last couple of weeks is that I built a higher-level library over the dagger SDK. So, instead of writing all that container mount nonsense stuff, you can just use this go link package. It's actually called the OCI. You can find it here if you want to give it a try. And instead of, you still have to connect to dagger, obviously, but instead of writing that whole container code, you can just use this much more friendly interface to run your tests or run the go link CILin, for example. And it's much easier for developers to interact with this. Like, if they want to change the cover mode, for example, it's pretty obvious how you would want to do that. In this case, compared to how you would want to do that, it would be the lower-level dagger SDK stuff. Let's give this another try. Now, from the... Oh, still on. Let's go to the... Well, it doesn't work. Anyway, if you want to give this a try at home, you're absolutely welcome to do that. The documentation is getting better by the day. It has a bunch of different examples. You will find these examples on my GitHub as well. I promise it works. They've actually just released a brand-new kickstart guide. So far, the documentation... They had documentation for the different SDKs in different places. Now, they have a kickstart guide that is basically the same for all of the languages. Regardless which one you want to choose or if you want to try all three supported SDKs, you can do that with the kickstart guide. You can actually go ahead and run the code from there. And finally, they have a playground that works with their low-level GraphQL API. So if you want to give that a try, it's fairly similar to the SDK, actually. If you want to give that a try, then you can absolutely do that and see how dagger works without actually installing it on your computer. That's all I had for today. If you have questions, feel free. There is a sticker. Thank you very much for your attention. If there are any questions, raise your hand. I'll try to give you a microphone. I have one over here that's closer. Can you use it with something other than Docker? I'm sorry. Can you use it with something other than Docker underneath? I think you can use it with Docker compatible runtimes. So I think you can use it with Podman at the moment. I think, technically, you can use it at runtime. But I don't know if that's currently available as an option. But we have someone from the Dagger team here who can actually answer that question. Hi. So how does the portability work when parts of your deployment depend on publishing a Docker image to a repo that is external or AWS or Terraform or things that require secrets? How does that fit in running it locally? So that's a great question, actually. So the code itself should be completely portable. So the pipeline itself can run anywhere. What you would need to do in that case is you need some sort of either a central secret store that you can connect to from your own computer or you need to be able to load some sort of secrets or credentials from your environment variables, for example. You can absolutely do that with the Dagger pipeline. So from that perspective, if the processor is here, you can push to another registry or push to a development environment, for example. So you can parametrize pipelines based on where you run them. You would still run the same code, but you could deploy to different environments from locally. We have one more question there. Okay. Thank you. One last applause, please.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.68, "text": " Okay. Thank you, everyone. Our next speaker has some interesting news for your CI. There", "tokens": [1033, 13, 1044, 291, 11, 1518, 13, 2621, 958, 8145, 575, 512, 1880, 2583, 337, 428, 37777, 13, 821], "temperature": 0.0, "avg_logprob": -0.31735461691151495, "compression_ratio": 1.2720588235294117, "no_speech_prob": 0.4881691038608551}, {"id": 1, "seek": 0, "start": 11.68, "end": 17.0, "text": " are better solutions than the YAML you're used to. Mark is going to talk to us about", "tokens": [366, 1101, 6547, 813, 264, 398, 2865, 43, 291, 434, 1143, 281, 13, 3934, 307, 516, 281, 751, 281, 505, 466], "temperature": 0.0, "avg_logprob": -0.31735461691151495, "compression_ratio": 1.2720588235294117, "no_speech_prob": 0.4881691038608551}, {"id": 2, "seek": 1700, "start": 17.0, "end": 30.68, "text": " building a CI pipeline with Dagger in Go. Thank you. Thank you. Can you hear me? Okay.", "tokens": [2390, 257, 37777, 15517, 365, 413, 11062, 294, 1037, 13, 1044, 291, 13, 1044, 291, 13, 1664, 291, 1568, 385, 30, 1033, 13], "temperature": 0.0, "avg_logprob": -0.17186233962791553, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0025363911408931017}, {"id": 3, "seek": 1700, "start": 30.68, "end": 37.6, "text": " So very important information before we get started. I have some Dagger stickers here", "tokens": [407, 588, 1021, 1589, 949, 321, 483, 1409, 13, 286, 362, 512, 413, 11062, 21019, 510], "temperature": 0.0, "avg_logprob": -0.17186233962791553, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0025363911408931017}, {"id": 4, "seek": 1700, "start": 37.6, "end": 41.6, "text": " if you want to pick them up. I don't know. Maybe I can just leave them after the talk", "tokens": [498, 291, 528, 281, 1888, 552, 493, 13, 286, 500, 380, 458, 13, 2704, 286, 393, 445, 1856, 552, 934, 264, 751], "temperature": 0.0, "avg_logprob": -0.17186233962791553, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0025363911408931017}, {"id": 5, "seek": 4160, "start": 41.6, "end": 47.120000000000005, "text": " or you can come to me and pick them up. I'll leave the stickers over here. Perfect. People", "tokens": [420, 291, 393, 808, 281, 385, 293, 1888, 552, 493, 13, 286, 603, 1856, 264, 21019, 670, 510, 13, 10246, 13, 3432], "temperature": 0.0, "avg_logprob": -0.19470559397051412, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0010260086273774505}, {"id": 6, "seek": 4160, "start": 47.120000000000005, "end": 51.36, "text": " can grab them. Thank you. An important thing. Stickers are for your laptop, not for the", "tokens": [393, 4444, 552, 13, 1044, 291, 13, 1107, 1021, 551, 13, 22744, 433, 366, 337, 428, 10732, 11, 406, 337, 264], "temperature": 0.0, "avg_logprob": -0.19470559397051412, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0010260086273774505}, {"id": 7, "seek": 4160, "start": 51.36, "end": 55.6, "text": " room. Every sticker you put inside a room involves them we have to pay for. So keep", "tokens": [1808, 13, 2048, 20400, 291, 829, 1854, 257, 1808, 11626, 552, 321, 362, 281, 1689, 337, 13, 407, 1066], "temperature": 0.0, "avg_logprob": -0.19470559397051412, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0010260086273774505}, {"id": 8, "seek": 4160, "start": 55.6, "end": 62.6, "text": " them for yourself. Yeah. Well, that's why we are going to conferences for it. So thank", "tokens": [552, 337, 1803, 13, 865, 13, 1042, 11, 300, 311, 983, 321, 366, 516, 281, 22032, 337, 309, 13, 407, 1309], "temperature": 0.0, "avg_logprob": -0.19470559397051412, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0010260086273774505}, {"id": 9, "seek": 6260, "start": 62.6, "end": 71.68, "text": " you again for joining me today. My name is Mark. And for the better part of the decade", "tokens": [291, 797, 337, 5549, 385, 965, 13, 1222, 1315, 307, 3934, 13, 400, 337, 264, 1101, 644, 295, 264, 10378], "temperature": 0.0, "avg_logprob": -0.2249030815927606, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00017897381621878594}, {"id": 10, "seek": 6260, "start": 71.68, "end": 78.16, "text": " I've been focusing on helping engineering teams focus build, helping them focus on", "tokens": [286, 600, 668, 8416, 322, 4315, 7043, 5491, 1879, 1322, 11, 4315, 552, 1879, 322], "temperature": 0.0, "avg_logprob": -0.2249030815927606, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00017897381621878594}, {"id": 11, "seek": 6260, "start": 78.16, "end": 82.08, "text": " their business applications, building their best business applications instead of worrying", "tokens": [641, 1606, 5821, 11, 2390, 641, 1151, 1606, 5821, 2602, 295, 18788], "temperature": 0.0, "avg_logprob": -0.2249030815927606, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00017897381621878594}, {"id": 12, "seek": 6260, "start": 82.08, "end": 89.52000000000001, "text": " about things like CI and how they are being deployed. And I have this fake title at Cisco", "tokens": [466, 721, 411, 37777, 293, 577, 436, 366, 885, 17826, 13, 400, 286, 362, 341, 7592, 4876, 412, 38528], "temperature": 0.0, "avg_logprob": -0.2249030815927606, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00017897381621878594}, {"id": 13, "seek": 8952, "start": 89.52, "end": 95.88, "text": " technical lead that I decided that I would come clean here today that I'm really nothing", "tokens": [6191, 1477, 300, 286, 3047, 300, 286, 576, 808, 2541, 510, 965, 300, 286, 478, 534, 1825], "temperature": 0.0, "avg_logprob": -0.33685520717075895, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00020273325208108872}, {"id": 14, "seek": 8952, "start": 95.88, "end": 101.84, "text": " more than just a YAML engineer. Ooh, that feels good. Anyone else want to onboard them", "tokens": [544, 813, 445, 257, 398, 2865, 43, 11403, 13, 7951, 11, 300, 3417, 665, 13, 14643, 1646, 528, 281, 24033, 552], "temperature": 0.0, "avg_logprob": -0.33685520717075895, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00020273325208108872}, {"id": 15, "seek": 8952, "start": 101.84, "end": 112.44, "text": " themselves? Any other YAML engineers here? All right. So let's talk about CI-CD a bit.", "tokens": [2969, 30, 2639, 661, 398, 2865, 43, 11955, 510, 30, 1057, 558, 13, 407, 718, 311, 751, 466, 37777, 12, 16508, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.33685520717075895, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.00020273325208108872}, {"id": 16, "seek": 11244, "start": 112.44, "end": 121.56, "text": " And CI or CI systems improved a lot in the last couple of years. We have new and more", "tokens": [400, 37777, 420, 37777, 3652, 9689, 257, 688, 294, 264, 1036, 1916, 295, 924, 13, 492, 362, 777, 293, 544], "temperature": 0.0, "avg_logprob": -0.1862034327547315, "compression_ratio": 1.4202127659574468, "no_speech_prob": 9.126098302658647e-05}, {"id": 17, "seek": 11244, "start": 121.56, "end": 129.32, "text": " evolved CI solutions today. But we still have some challenges that we face every day. Like", "tokens": [14178, 37777, 6547, 965, 13, 583, 321, 920, 362, 512, 4759, 300, 321, 1851, 633, 786, 13, 1743], "temperature": 0.0, "avg_logprob": -0.1862034327547315, "compression_ratio": 1.4202127659574468, "no_speech_prob": 9.126098302658647e-05}, {"id": 18, "seek": 11244, "start": 129.32, "end": 135.72, "text": " the one I've been already hinting at, YAML. Obviously, YAML is one of the biggest problems", "tokens": [264, 472, 286, 600, 668, 1217, 12075, 278, 412, 11, 398, 2865, 43, 13, 7580, 11, 398, 2865, 43, 307, 472, 295, 264, 3880, 2740], "temperature": 0.0, "avg_logprob": -0.1862034327547315, "compression_ratio": 1.4202127659574468, "no_speech_prob": 9.126098302658647e-05}, {"id": 19, "seek": 13572, "start": 135.72, "end": 143.0, "text": " with CI systems today. Admittedly, sometimes, like using YAML to build the declarative pipeline", "tokens": [365, 37777, 3652, 965, 13, 46292, 3944, 356, 11, 2171, 11, 411, 1228, 398, 2865, 43, 281, 1322, 264, 16694, 1166, 15517], "temperature": 0.0, "avg_logprob": -0.20073476997581688, "compression_ratio": 1.467741935483871, "no_speech_prob": 8.203284960472956e-05}, {"id": 20, "seek": 13572, "start": 143.0, "end": 149.84, "text": " can be fine. But, man, you miss a space. The whole thing just broke. The whole thing just", "tokens": [393, 312, 2489, 13, 583, 11, 587, 11, 291, 1713, 257, 1901, 13, 440, 1379, 551, 445, 6902, 13, 440, 1379, 551, 445], "temperature": 0.0, "avg_logprob": -0.20073476997581688, "compression_ratio": 1.467741935483871, "no_speech_prob": 8.203284960472956e-05}, {"id": 21, "seek": 13572, "start": 149.84, "end": 157.2, "text": " breaks. And you might not even know where to start debugging it. So YAML makes it often", "tokens": [9857, 13, 400, 291, 1062, 406, 754, 458, 689, 281, 722, 45592, 309, 13, 407, 398, 2865, 43, 1669, 309, 2049], "temperature": 0.0, "avg_logprob": -0.20073476997581688, "compression_ratio": 1.467741935483871, "no_speech_prob": 8.203284960472956e-05}, {"id": 22, "seek": 15720, "start": 157.2, "end": 167.6, "text": " really hard for people to even just touch CI. And the other thing is CI tends to break for", "tokens": [534, 1152, 337, 561, 281, 754, 445, 2557, 37777, 13, 400, 264, 661, 551, 307, 37777, 12258, 281, 1821, 337], "temperature": 0.0, "avg_logprob": -0.16676357057359484, "compression_ratio": 1.6123348017621146, "no_speech_prob": 7.74188265495468e-06}, {"id": 23, "seek": 15720, "start": 167.6, "end": 172.07999999999998, "text": " no obvious reasons. Like, the pipeline that worked yesterday may not work today and you", "tokens": [572, 6322, 4112, 13, 1743, 11, 264, 15517, 300, 2732, 5186, 815, 406, 589, 965, 293, 291], "temperature": 0.0, "avg_logprob": -0.16676357057359484, "compression_ratio": 1.6123348017621146, "no_speech_prob": 7.74188265495468e-06}, {"id": 24, "seek": 15720, "start": 172.07999999999998, "end": 177.76, "text": " don't really know why. And obviously, as developers, when something breaks in production, we can", "tokens": [500, 380, 534, 458, 983, 13, 400, 2745, 11, 382, 8849, 11, 562, 746, 9857, 294, 4265, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.16676357057359484, "compression_ratio": 1.6123348017621146, "no_speech_prob": 7.74188265495468e-06}, {"id": 25, "seek": 15720, "start": 177.76, "end": 183.83999999999997, "text": " just tell the ops people to worry about it. But with CI, that's not really the case. Like,", "tokens": [445, 980, 264, 44663, 561, 281, 3292, 466, 309, 13, 583, 365, 37777, 11, 300, 311, 406, 534, 264, 1389, 13, 1743, 11], "temperature": 0.0, "avg_logprob": -0.16676357057359484, "compression_ratio": 1.6123348017621146, "no_speech_prob": 7.74188265495468e-06}, {"id": 26, "seek": 18384, "start": 183.84, "end": 188.72, "text": " we have to interact with CI. And if something goes wrong, we might have to be the ones who", "tokens": [321, 362, 281, 4648, 365, 37777, 13, 400, 498, 746, 1709, 2085, 11, 321, 1062, 362, 281, 312, 264, 2306, 567], "temperature": 0.0, "avg_logprob": -0.1722797359432186, "compression_ratio": 1.7392996108949417, "no_speech_prob": 2.5674211428849958e-05}, {"id": 27, "seek": 18384, "start": 188.72, "end": 197.04, "text": " fix it. And with the currently available CI solutions today, you can't really, like, everything", "tokens": [3191, 309, 13, 400, 365, 264, 4362, 2435, 37777, 6547, 965, 11, 291, 393, 380, 534, 11, 411, 11, 1203], "temperature": 0.0, "avg_logprob": -0.1722797359432186, "compression_ratio": 1.7392996108949417, "no_speech_prob": 2.5674211428849958e-05}, {"id": 28, "seek": 18384, "start": 197.04, "end": 202.08, "text": " was running in the cloud or in the remote system. You can't really have or you don't", "tokens": [390, 2614, 294, 264, 4588, 420, 294, 264, 8607, 1185, 13, 509, 393, 380, 534, 362, 420, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.1722797359432186, "compression_ratio": 1.7392996108949417, "no_speech_prob": 2.5674211428849958e-05}, {"id": 29, "seek": 18384, "start": 202.08, "end": 206.52, "text": " really have tools that you can use to debug effectively. You have to start gassing and", "tokens": [534, 362, 3873, 300, 291, 393, 764, 281, 24083, 8659, 13, 509, 362, 281, 722, 290, 42705, 293], "temperature": 0.0, "avg_logprob": -0.1722797359432186, "compression_ratio": 1.7392996108949417, "no_speech_prob": 2.5674211428849958e-05}, {"id": 30, "seek": 18384, "start": 206.52, "end": 211.96, "text": " start changing some YAML config. And you have to push that to a repository and then wait", "tokens": [722, 4473, 512, 398, 2865, 43, 6662, 13, 400, 291, 362, 281, 2944, 300, 281, 257, 25841, 293, 550, 1699], "temperature": 0.0, "avg_logprob": -0.1722797359432186, "compression_ratio": 1.7392996108949417, "no_speech_prob": 2.5674211428849958e-05}, {"id": 31, "seek": 21196, "start": 211.96, "end": 218.08, "text": " for the CI to get triggered. And you have to go through this whole and long feedback loop", "tokens": [337, 264, 37777, 281, 483, 21710, 13, 400, 291, 362, 281, 352, 807, 341, 1379, 293, 938, 5824, 6367], "temperature": 0.0, "avg_logprob": -0.0905727810329861, "compression_ratio": 1.69377990430622, "no_speech_prob": 3.023377212230116e-05}, {"id": 32, "seek": 21196, "start": 218.08, "end": 223.32, "text": " to be able to debug what's going wrong and to be able to fix that. And that's a pain.", "tokens": [281, 312, 1075, 281, 24083, 437, 311, 516, 2085, 293, 281, 312, 1075, 281, 3191, 300, 13, 400, 300, 311, 257, 1822, 13], "temperature": 0.0, "avg_logprob": -0.0905727810329861, "compression_ratio": 1.69377990430622, "no_speech_prob": 3.023377212230116e-05}, {"id": 33, "seek": 21196, "start": 223.32, "end": 229.08, "text": " Like, it takes a lot of time. It's really a huge waste of time. And it's really painful", "tokens": [1743, 11, 309, 2516, 257, 688, 295, 565, 13, 467, 311, 534, 257, 2603, 5964, 295, 565, 13, 400, 309, 311, 534, 11697], "temperature": 0.0, "avg_logprob": -0.0905727810329861, "compression_ratio": 1.69377990430622, "no_speech_prob": 3.023377212230116e-05}, {"id": 34, "seek": 21196, "start": 229.08, "end": 237.12, "text": " to do that. Now, sometimes it's not the CI that's wrong. Sometimes it's you, like, pushing", "tokens": [281, 360, 300, 13, 823, 11, 2171, 309, 311, 406, 264, 37777, 300, 311, 2085, 13, 4803, 309, 311, 291, 11, 411, 11, 7380], "temperature": 0.0, "avg_logprob": -0.0905727810329861, "compression_ratio": 1.69377990430622, "no_speech_prob": 3.023377212230116e-05}, {"id": 35, "seek": 23712, "start": 237.12, "end": 243.32, "text": " something that you shouldn't be pushing to the repository like tests are not passing", "tokens": [746, 300, 291, 4659, 380, 312, 7380, 281, 264, 25841, 411, 6921, 366, 406, 8437], "temperature": 0.0, "avg_logprob": -0.14268480345260265, "compression_ratio": 1.809278350515464, "no_speech_prob": 7.08892839611508e-05}, {"id": 36, "seek": 23712, "start": 243.32, "end": 249.52, "text": " or the linters are not passing or something else goes wrong. And again, you may have the", "tokens": [420, 264, 22896, 1559, 366, 406, 8437, 420, 746, 1646, 1709, 2085, 13, 400, 797, 11, 291, 815, 362, 264], "temperature": 0.0, "avg_logprob": -0.14268480345260265, "compression_ratio": 1.809278350515464, "no_speech_prob": 7.08892839611508e-05}, {"id": 37, "seek": 23712, "start": 249.52, "end": 255.76, "text": " tools locally in your machine, but you may not have the same versions. You may not have", "tokens": [3873, 16143, 294, 428, 3479, 11, 457, 291, 815, 406, 362, 264, 912, 9606, 13, 509, 815, 406, 362], "temperature": 0.0, "avg_logprob": -0.14268480345260265, "compression_ratio": 1.809278350515464, "no_speech_prob": 7.08892839611508e-05}, {"id": 38, "seek": 23712, "start": 255.76, "end": 261.16, "text": " the same setup as in the CI. And it may just break in the CI even though you ran the test", "tokens": [264, 912, 8657, 382, 294, 264, 37777, 13, 400, 309, 815, 445, 1821, 294, 264, 37777, 754, 1673, 291, 5872, 264, 1500], "temperature": 0.0, "avg_logprob": -0.14268480345260265, "compression_ratio": 1.809278350515464, "no_speech_prob": 7.08892839611508e-05}, {"id": 39, "seek": 26116, "start": 261.16, "end": 267.88000000000005, "text": " locally and everything was green, it may still fail in the CI. And you still have to go through", "tokens": [16143, 293, 1203, 390, 3092, 11, 309, 815, 920, 3061, 294, 264, 37777, 13, 400, 291, 920, 362, 281, 352, 807], "temperature": 0.0, "avg_logprob": -0.14054512423138285, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.00010401943291071802}, {"id": 40, "seek": 26116, "start": 267.88000000000005, "end": 273.48, "text": " the same long feedback loop again and again, trying to fix that, instead of being able", "tokens": [264, 912, 938, 5824, 6367, 797, 293, 797, 11, 1382, 281, 3191, 300, 11, 2602, 295, 885, 1075], "temperature": 0.0, "avg_logprob": -0.14054512423138285, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.00010401943291071802}, {"id": 41, "seek": 26116, "start": 273.48, "end": 280.08000000000004, "text": " to run the whole thing locally and being confident that it will just work in the CI as well.", "tokens": [281, 1190, 264, 1379, 551, 16143, 293, 885, 6679, 300, 309, 486, 445, 589, 294, 264, 37777, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14054512423138285, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.00010401943291071802}, {"id": 42, "seek": 26116, "start": 280.08000000000004, "end": 284.72, "text": " And probably there are other challenges with CI, but these are the ones that wasted hours", "tokens": [400, 1391, 456, 366, 661, 4759, 365, 37777, 11, 457, 613, 366, 264, 2306, 300, 19496, 2496], "temperature": 0.0, "avg_logprob": -0.14054512423138285, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.00010401943291071802}, {"id": 43, "seek": 28472, "start": 284.72, "end": 293.72, "text": " from my life in the last couple years. So how can Dagger provide an answer to this problem?", "tokens": [490, 452, 993, 294, 264, 1036, 1916, 924, 13, 407, 577, 393, 413, 11062, 2893, 364, 1867, 281, 341, 1154, 30], "temperature": 0.0, "avg_logprob": -0.13877403873136673, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.195128945459146e-05}, {"id": 44, "seek": 28472, "start": 293.72, "end": 299.28000000000003, "text": " So first of all, Dagger is a programmable and deportable CI solution, which means you", "tokens": [407, 700, 295, 439, 11, 413, 11062, 307, 257, 37648, 712, 293, 33485, 712, 37777, 3827, 11, 597, 1355, 291], "temperature": 0.0, "avg_logprob": -0.13877403873136673, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.195128945459146e-05}, {"id": 45, "seek": 28472, "start": 299.28000000000003, "end": 305.92, "text": " can run your CI pipelines basically anywhere. We will get to how it does that. But the important", "tokens": [393, 1190, 428, 37777, 40168, 1936, 4992, 13, 492, 486, 483, 281, 577, 309, 775, 300, 13, 583, 264, 1021], "temperature": 0.0, "avg_logprob": -0.13877403873136673, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.195128945459146e-05}, {"id": 46, "seek": 28472, "start": 305.92, "end": 311.68, "text": " thing is that you can run your CI pipelines anywhere using the same environment, which", "tokens": [551, 307, 300, 291, 393, 1190, 428, 37777, 40168, 4992, 1228, 264, 912, 2823, 11, 597], "temperature": 0.0, "avg_logprob": -0.13877403873136673, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.195128945459146e-05}, {"id": 47, "seek": 31168, "start": 311.68, "end": 317.24, "text": " means if it runs on your machine, then you can be confident that it will run the same", "tokens": [1355, 498, 309, 6676, 322, 428, 3479, 11, 550, 291, 393, 312, 6679, 300, 309, 486, 1190, 264, 912], "temperature": 0.0, "avg_logprob": -0.1413330923427235, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.632785466033965e-05}, {"id": 48, "seek": 31168, "start": 317.24, "end": 322.76, "text": " way in your own CI system. Now, that's a great thing for a number of reasons, because when", "tokens": [636, 294, 428, 1065, 37777, 1185, 13, 823, 11, 300, 311, 257, 869, 551, 337, 257, 1230, 295, 4112, 11, 570, 562], "temperature": 0.0, "avg_logprob": -0.1413330923427235, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.632785466033965e-05}, {"id": 49, "seek": 31168, "start": 322.76, "end": 329.8, "text": " you start building a pipeline, for example, using any of the CI systems today, you still", "tokens": [291, 722, 2390, 257, 15517, 11, 337, 1365, 11, 1228, 604, 295, 264, 37777, 3652, 965, 11, 291, 920], "temperature": 0.0, "avg_logprob": -0.1413330923427235, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.632785466033965e-05}, {"id": 50, "seek": 31168, "start": 329.8, "end": 335.16, "text": " have to go through that feedback loop, like adding some config and then pushing it to the", "tokens": [362, 281, 352, 807, 300, 5824, 6367, 11, 411, 5127, 512, 6662, 293, 550, 7380, 309, 281, 264], "temperature": 0.0, "avg_logprob": -0.1413330923427235, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.632785466033965e-05}, {"id": 51, "seek": 33516, "start": 335.16, "end": 341.84000000000003, "text": " GitHub OO and trying to figure out if it works or not, and then changing until it works. Now,", "tokens": [23331, 422, 46, 293, 1382, 281, 2573, 484, 498, 309, 1985, 420, 406, 11, 293, 550, 4473, 1826, 309, 1985, 13, 823, 11], "temperature": 0.0, "avg_logprob": -0.15020458177588453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 6.747854058630764e-05}, {"id": 52, "seek": 33516, "start": 341.84000000000003, "end": 347.52000000000004, "text": " the ability to run this whole thing locally, it's much shorter feedback loop, so you can", "tokens": [264, 3485, 281, 1190, 341, 1379, 551, 16143, 11, 309, 311, 709, 11639, 5824, 6367, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.15020458177588453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 6.747854058630764e-05}, {"id": 53, "seek": 33516, "start": 347.52000000000004, "end": 354.36, "text": " build your own CI pipelines much more quickly than using some remote system. The other thing", "tokens": [1322, 428, 1065, 37777, 40168, 709, 544, 2661, 813, 1228, 512, 8607, 1185, 13, 440, 661, 551], "temperature": 0.0, "avg_logprob": -0.15020458177588453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 6.747854058630764e-05}, {"id": 54, "seek": 33516, "start": 354.36, "end": 359.04, "text": " is that if something goes wrong, you have the whole thing running locally. So again,", "tokens": [307, 300, 498, 746, 1709, 2085, 11, 291, 362, 264, 1379, 551, 2614, 16143, 13, 407, 797, 11], "temperature": 0.0, "avg_logprob": -0.15020458177588453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 6.747854058630764e-05}, {"id": 55, "seek": 35904, "start": 359.04, "end": 365.88, "text": " shorter feedback loop, you have more tools to debug, so it's much easier to figure out", "tokens": [11639, 5824, 6367, 11, 291, 362, 544, 3873, 281, 24083, 11, 370, 309, 311, 709, 3571, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.18547810947193819, "compression_ratio": 1.579185520361991, "no_speech_prob": 6.653406308032572e-05}, {"id": 56, "seek": 35904, "start": 365.88, "end": 372.24, "text": " what goes wrong, even if it's either the CI pipeline or your code. The other thing about", "tokens": [437, 1709, 2085, 11, 754, 498, 309, 311, 2139, 264, 37777, 15517, 420, 428, 3089, 13, 440, 661, 551, 466], "temperature": 0.0, "avg_logprob": -0.18547810947193819, "compression_ratio": 1.579185520361991, "no_speech_prob": 6.653406308032572e-05}, {"id": 57, "seek": 35904, "start": 372.24, "end": 377.28000000000003, "text": " Dagger is that you can actually write your pipelines in your own preferred language.", "tokens": [413, 11062, 307, 300, 291, 393, 767, 2464, 428, 40168, 294, 428, 1065, 16494, 2856, 13], "temperature": 0.0, "avg_logprob": -0.18547810947193819, "compression_ratio": 1.579185520361991, "no_speech_prob": 6.653406308032572e-05}, {"id": 58, "seek": 35904, "start": 377.28000000000003, "end": 384.76, "text": " Now, not any language, obviously. Some of the languages that Dagger supports, but that's", "tokens": [823, 11, 406, 604, 2856, 11, 2745, 13, 2188, 295, 264, 8650, 300, 413, 11062, 9346, 11, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.18547810947193819, "compression_ratio": 1.579185520361991, "no_speech_prob": 6.653406308032572e-05}, {"id": 59, "seek": 38476, "start": 384.76, "end": 391.28, "text": " already much better than YAML. You can write your pipelines in Go, Python or TypeScript,", "tokens": [1217, 709, 1101, 813, 398, 2865, 43, 13, 509, 393, 2464, 428, 40168, 294, 1037, 11, 15329, 420, 15576, 14237, 11], "temperature": 0.0, "avg_logprob": -0.1716961549675983, "compression_ratio": 1.7122641509433962, "no_speech_prob": 3.009704960277304e-05}, {"id": 60, "seek": 38476, "start": 391.28, "end": 398.0, "text": " ThinkQ, EvenQ, but that's already much better than YAML. You can write your own pipelines", "tokens": [6557, 48, 11, 2754, 48, 11, 457, 300, 311, 1217, 709, 1101, 813, 398, 2865, 43, 13, 509, 393, 2464, 428, 1065, 40168], "temperature": 0.0, "avg_logprob": -0.1716961549675983, "compression_ratio": 1.7122641509433962, "no_speech_prob": 3.009704960277304e-05}, {"id": 61, "seek": 38476, "start": 398.0, "end": 404.03999999999996, "text": " in code, and you don't have to invent or use some weird syntax, for example, to represent", "tokens": [294, 3089, 11, 293, 291, 500, 380, 362, 281, 7962, 420, 764, 512, 3657, 28431, 11, 337, 1365, 11, 281, 2906], "temperature": 0.0, "avg_logprob": -0.1716961549675983, "compression_ratio": 1.7122641509433962, "no_speech_prob": 3.009704960277304e-05}, {"id": 62, "seek": 38476, "start": 404.03999999999996, "end": 409.96, "text": " dependencies between steps or between different pipelines. You can just do that in plain code,", "tokens": [36606, 1296, 4439, 420, 1296, 819, 40168, 13, 509, 393, 445, 360, 300, 294, 11121, 3089, 11], "temperature": 0.0, "avg_logprob": -0.1716961549675983, "compression_ratio": 1.7122641509433962, "no_speech_prob": 3.009704960277304e-05}, {"id": 63, "seek": 40996, "start": 409.96, "end": 417.76, "text": " so that's great. And all those, so the possibility of writing pipelines in your own language points", "tokens": [370, 300, 311, 869, 13, 400, 439, 729, 11, 370, 264, 7959, 295, 3579, 40168, 294, 428, 1065, 2856, 2793], "temperature": 0.0, "avg_logprob": -0.20716370235789905, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7450694688013755e-05}, {"id": 64, "seek": 40996, "start": 417.76, "end": 425.64, "text": " to the fact that you can avoid Pandor locking entirely. You would still have a CI solution", "tokens": [281, 264, 1186, 300, 291, 393, 5042, 16995, 284, 23954, 7696, 13, 509, 576, 920, 362, 257, 37777, 3827], "temperature": 0.0, "avg_logprob": -0.20716370235789905, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7450694688013755e-05}, {"id": 65, "seek": 40996, "start": 425.64, "end": 432.4, "text": " like Jenkins or GitHub actions or whatever, and you would still run Dagger on those systems,", "tokens": [411, 41273, 420, 23331, 5909, 420, 2035, 11, 293, 291, 576, 920, 1190, 413, 11062, 322, 729, 3652, 11], "temperature": 0.0, "avg_logprob": -0.20716370235789905, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7450694688013755e-05}, {"id": 66, "seek": 40996, "start": 432.4, "end": 439.35999999999996, "text": " but you would have to write a very thin integration layer just to run the Dagger pipelines. You", "tokens": [457, 291, 576, 362, 281, 2464, 257, 588, 5862, 10980, 4583, 445, 281, 1190, 264, 413, 11062, 40168, 13, 509], "temperature": 0.0, "avg_logprob": -0.20716370235789905, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7450694688013755e-05}, {"id": 67, "seek": 43936, "start": 439.36, "end": 443.96000000000004, "text": " can, you would be much more confident that the pipelines would run the same way on the", "tokens": [393, 11, 291, 576, 312, 709, 544, 6679, 300, 264, 40168, 576, 1190, 264, 912, 636, 322, 264], "temperature": 0.0, "avg_logprob": -0.16311978769826366, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00011208341311430559}, {"id": 68, "seek": 43936, "start": 443.96000000000004, "end": 452.56, "text": " CI system as on your computer, and yeah, you can avoid Pandor locking entirely. You can", "tokens": [37777, 1185, 382, 322, 428, 3820, 11, 293, 1338, 11, 291, 393, 5042, 16995, 284, 23954, 7696, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.16311978769826366, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00011208341311430559}, {"id": 69, "seek": 43936, "start": 452.56, "end": 458.44, "text": " move to another CI system if you want to, and you may say that it doesn't happen often,", "tokens": [1286, 281, 1071, 37777, 1185, 498, 291, 528, 281, 11, 293, 291, 815, 584, 300, 309, 1177, 380, 1051, 2049, 11], "temperature": 0.0, "avg_logprob": -0.16311978769826366, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00011208341311430559}, {"id": 70, "seek": 43936, "start": 458.44, "end": 463.8, "text": " but when it does, man, it's really painful, like converting from one YAML to another or", "tokens": [457, 562, 309, 775, 11, 587, 11, 309, 311, 534, 11697, 11, 411, 29942, 490, 472, 398, 2865, 43, 281, 1071, 420], "temperature": 0.0, "avg_logprob": -0.16311978769826366, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00011208341311430559}, {"id": 71, "seek": 46380, "start": 463.8, "end": 473.72, "text": " one YAML to, I don't know, Groovy or JenkinsFly or something, that hurts. And lastly, costly", "tokens": [472, 398, 2865, 43, 281, 11, 286, 500, 380, 458, 11, 12981, 38223, 420, 41273, 37, 356, 420, 746, 11, 300, 11051, 13, 400, 16386, 11, 28328], "temperature": 0.0, "avg_logprob": -0.22178895132882254, "compression_ratio": 1.46875, "no_speech_prob": 3.142846981063485e-05}, {"id": 72, "seek": 46380, "start": 473.72, "end": 480.64, "text": " caching. So every CI system or most CI systems have their own caching solutions where you", "tokens": [269, 2834, 13, 407, 633, 37777, 1185, 420, 881, 37777, 3652, 362, 641, 1065, 269, 2834, 6547, 689, 291], "temperature": 0.0, "avg_logprob": -0.22178895132882254, "compression_ratio": 1.46875, "no_speech_prob": 3.142846981063485e-05}, {"id": 73, "seek": 46380, "start": 480.64, "end": 489.2, "text": " can cache the dependencies of your language or dependency manager, but that requires configuration.", "tokens": [393, 19459, 264, 36606, 295, 428, 2856, 420, 33621, 6598, 11, 457, 300, 7029, 11694, 13], "temperature": 0.0, "avg_logprob": -0.22178895132882254, "compression_ratio": 1.46875, "no_speech_prob": 3.142846981063485e-05}, {"id": 74, "seek": 48920, "start": 489.2, "end": 494.84, "text": " You have to make sure that you configure it right, otherwise, well, it could either like", "tokens": [509, 362, 281, 652, 988, 300, 291, 22162, 309, 558, 11, 5911, 11, 731, 11, 309, 727, 2139, 411], "temperature": 0.0, "avg_logprob": -0.14033207592663463, "compression_ratio": 1.6741573033707866, "no_speech_prob": 3.606378595577553e-05}, {"id": 75, "seek": 48920, "start": 494.84, "end": 499.08, "text": " grow the cache endlessly and then you will be paying a lot of money for that, or it would", "tokens": [1852, 264, 19459, 44920, 293, 550, 291, 486, 312, 6229, 257, 688, 295, 1460, 337, 300, 11, 420, 309, 576], "temperature": 0.0, "avg_logprob": -0.14033207592663463, "compression_ratio": 1.6741573033707866, "no_speech_prob": 3.606378595577553e-05}, {"id": 76, "seek": 48920, "start": 499.08, "end": 505.52, "text": " just be non-functional at all and it wouldn't cache anything properly. Now with Dagger,", "tokens": [445, 312, 2107, 12, 22845, 304, 412, 439, 293, 309, 2759, 380, 19459, 1340, 6108, 13, 823, 365, 413, 11062, 11], "temperature": 0.0, "avg_logprob": -0.14033207592663463, "compression_ratio": 1.6741573033707866, "no_speech_prob": 3.606378595577553e-05}, {"id": 77, "seek": 48920, "start": 505.52, "end": 510.12, "text": " everything is cached by default, like every step is cached. You can think about it like", "tokens": [1203, 307, 269, 15095, 538, 7576, 11, 411, 633, 1823, 307, 269, 15095, 13, 509, 393, 519, 466, 309, 411], "temperature": 0.0, "avg_logprob": -0.14033207592663463, "compression_ratio": 1.6741573033707866, "no_speech_prob": 3.606378595577553e-05}, {"id": 78, "seek": 48920, "start": 510.12, "end": 516.4, "text": " a Docker file. Every instruction or the result of it is basically cached in a separate layer", "tokens": [257, 33772, 3991, 13, 2048, 10951, 420, 264, 1874, 295, 309, 307, 1936, 269, 15095, 294, 257, 4994, 4583], "temperature": 0.0, "avg_logprob": -0.14033207592663463, "compression_ratio": 1.6741573033707866, "no_speech_prob": 3.606378595577553e-05}, {"id": 79, "seek": 51640, "start": 516.4, "end": 523.0, "text": " in the Docker file, and if nothing changed between the steps, then when you run it again,", "tokens": [294, 264, 33772, 3991, 11, 293, 498, 1825, 3105, 1296, 264, 4439, 11, 550, 562, 291, 1190, 309, 797, 11], "temperature": 0.0, "avg_logprob": -0.14629959849129737, "compression_ratio": 1.6793893129770991, "no_speech_prob": 2.485547520336695e-05}, {"id": 80, "seek": 51640, "start": 523.0, "end": 528.64, "text": " it will basically run the same way and it will come from the cache. That's really how", "tokens": [309, 486, 1936, 1190, 264, 912, 636, 293, 309, 486, 808, 490, 264, 19459, 13, 663, 311, 534, 577], "temperature": 0.0, "avg_logprob": -0.14629959849129737, "compression_ratio": 1.6793893129770991, "no_speech_prob": 2.485547520336695e-05}, {"id": 81, "seek": 51640, "start": 528.64, "end": 532.64, "text": " Dagger works. Obviously, you have some control over what you want to cache and how you want", "tokens": [413, 11062, 1985, 13, 7580, 11, 291, 362, 512, 1969, 670, 437, 291, 528, 281, 19459, 293, 577, 291, 528], "temperature": 0.0, "avg_logprob": -0.14629959849129737, "compression_ratio": 1.6793893129770991, "no_speech_prob": 2.485547520336695e-05}, {"id": 82, "seek": 51640, "start": 532.64, "end": 539.4, "text": " to do it, but by default, Dagger got that covered for you. Now how does all this work", "tokens": [281, 360, 309, 11, 457, 538, 7576, 11, 413, 11062, 658, 300, 5343, 337, 291, 13, 823, 577, 775, 439, 341, 589], "temperature": 0.0, "avg_logprob": -0.14629959849129737, "compression_ratio": 1.6793893129770991, "no_speech_prob": 2.485547520336695e-05}, {"id": 83, "seek": 51640, "start": 539.4, "end": 545.48, "text": " behind the scenes? If I had to describe it in one word, it's obviously containers. Now", "tokens": [2261, 264, 8026, 30, 759, 286, 632, 281, 6786, 309, 294, 472, 1349, 11, 309, 311, 2745, 17089, 13, 823], "temperature": 0.0, "avg_logprob": -0.14629959849129737, "compression_ratio": 1.6793893129770991, "no_speech_prob": 2.485547520336695e-05}, {"id": 84, "seek": 54548, "start": 545.48, "end": 550.32, "text": " we can do it ourselves today, right? We could just run everything in a container and it", "tokens": [321, 393, 360, 309, 4175, 965, 11, 558, 30, 492, 727, 445, 1190, 1203, 294, 257, 10129, 293, 309], "temperature": 0.0, "avg_logprob": -0.16833607534344278, "compression_ratio": 1.635135135135135, "no_speech_prob": 9.90618354990147e-06}, {"id": 85, "seek": 54548, "start": 550.32, "end": 558.08, "text": " would be reasonable to say that it will run on the CEI the same way. What Dagger adds", "tokens": [576, 312, 10585, 281, 584, 300, 309, 486, 1190, 322, 264, 28109, 40, 264, 912, 636, 13, 708, 413, 11062, 10860], "temperature": 0.0, "avg_logprob": -0.16833607534344278, "compression_ratio": 1.635135135135135, "no_speech_prob": 9.90618354990147e-06}, {"id": 86, "seek": 54548, "start": 558.08, "end": 565.44, "text": " to the mix here is that you can actually build pipelines with code and that would be translated", "tokens": [281, 264, 2890, 510, 307, 300, 291, 393, 767, 1322, 40168, 365, 3089, 293, 300, 576, 312, 16805], "temperature": 0.0, "avg_logprob": -0.16833607534344278, "compression_ratio": 1.635135135135135, "no_speech_prob": 9.90618354990147e-06}, {"id": 87, "seek": 54548, "start": 565.44, "end": 574.2, "text": " into build pipelines. So you would use the Dagger SDK, the language SDK that Dagger provides.", "tokens": [666, 1322, 40168, 13, 407, 291, 576, 764, 264, 413, 11062, 37135, 11, 264, 2856, 37135, 300, 413, 11062, 6417, 13], "temperature": 0.0, "avg_logprob": -0.16833607534344278, "compression_ratio": 1.635135135135135, "no_speech_prob": 9.90618354990147e-06}, {"id": 88, "seek": 57420, "start": 574.2, "end": 580.96, "text": " Again, today, I believe it's for Go, TypeScript, Python, maybe Q as well. But the underlying", "tokens": [3764, 11, 965, 11, 286, 1697, 309, 311, 337, 1037, 11, 15576, 14237, 11, 15329, 11, 1310, 1249, 382, 731, 13, 583, 264, 14217], "temperature": 0.0, "avg_logprob": -0.1754831088486538, "compression_ratio": 1.547008547008547, "no_speech_prob": 3.339055911055766e-05}, {"id": 89, "seek": 57420, "start": 580.96, "end": 589.08, "text": " API is actually the GraphQL. So if you have a language client for GraphQL, you can actually", "tokens": [9362, 307, 767, 264, 21884, 13695, 13, 407, 498, 291, 362, 257, 2856, 6423, 337, 21884, 13695, 11, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.1754831088486538, "compression_ratio": 1.547008547008547, "no_speech_prob": 3.339055911055766e-05}, {"id": 90, "seek": 57420, "start": 589.08, "end": 593.48, "text": " build your own SDK if you want to, or you can just write GraphQL queries and send those", "tokens": [1322, 428, 1065, 37135, 498, 291, 528, 281, 11, 420, 291, 393, 445, 2464, 21884, 13695, 24109, 293, 2845, 729], "temperature": 0.0, "avg_logprob": -0.1754831088486538, "compression_ratio": 1.547008547008547, "no_speech_prob": 3.339055911055766e-05}, {"id": 91, "seek": 57420, "start": 593.48, "end": 601.5600000000001, "text": " directly to the Dagger engine. But basically, you write your own pipelines using this SDK", "tokens": [3838, 281, 264, 413, 11062, 2848, 13, 583, 1936, 11, 291, 2464, 428, 1065, 40168, 1228, 341, 37135], "temperature": 0.0, "avg_logprob": -0.1754831088486538, "compression_ratio": 1.547008547008547, "no_speech_prob": 3.339055911055766e-05}, {"id": 92, "seek": 60156, "start": 601.56, "end": 607.28, "text": " in your own language, and then the SDK will basically send GraphQL queries to the Dagger", "tokens": [294, 428, 1065, 2856, 11, 293, 550, 264, 37135, 486, 1936, 2845, 21884, 13695, 24109, 281, 264, 413, 11062], "temperature": 0.0, "avg_logprob": -0.15818948643181913, "compression_ratio": 1.665158371040724, "no_speech_prob": 1.7128752006101422e-05}, {"id": 93, "seek": 60156, "start": 607.28, "end": 612.0799999999999, "text": " engine. Now, when you run the whole thing locally first, then the Dagger SDK will actually", "tokens": [2848, 13, 823, 11, 562, 291, 1190, 264, 1379, 551, 16143, 700, 11, 550, 264, 413, 11062, 37135, 486, 767], "temperature": 0.0, "avg_logprob": -0.15818948643181913, "compression_ratio": 1.665158371040724, "no_speech_prob": 1.7128752006101422e-05}, {"id": 94, "seek": 60156, "start": 612.0799999999999, "end": 619.4399999999999, "text": " launch the Dagger engine for you. All it needs is really a Docker-compatible container runtime.", "tokens": [4025, 264, 413, 11062, 2848, 337, 291, 13, 1057, 309, 2203, 307, 534, 257, 33772, 12, 1112, 11584, 964, 10129, 34474, 13], "temperature": 0.0, "avg_logprob": -0.15818948643181913, "compression_ratio": 1.665158371040724, "no_speech_prob": 1.7128752006101422e-05}, {"id": 95, "seek": 60156, "start": 619.4399999999999, "end": 628.4399999999999, "text": " So if you have Docker on your computer or in your CEI, then you can run your Dagger pipeline", "tokens": [407, 498, 291, 362, 33772, 322, 428, 3820, 420, 294, 428, 28109, 40, 11, 550, 291, 393, 1190, 428, 413, 11062, 15517], "temperature": 0.0, "avg_logprob": -0.15818948643181913, "compression_ratio": 1.665158371040724, "no_speech_prob": 1.7128752006101422e-05}, {"id": 96, "seek": 62844, "start": 628.44, "end": 634.5200000000001, "text": " basically. So that's once more the portability of this whole thing. If you have Docker on", "tokens": [1936, 13, 407, 300, 311, 1564, 544, 264, 2436, 2310, 295, 341, 1379, 551, 13, 759, 291, 362, 33772, 322], "temperature": 0.0, "avg_logprob": -0.2019873098893599, "compression_ratio": 1.625, "no_speech_prob": 1.0899790140683763e-05}, {"id": 97, "seek": 62844, "start": 634.5200000000001, "end": 641.5200000000001, "text": " your machine and Docker basically runs anywhere these days, then you can run the Dagger pipeline", "tokens": [428, 3479, 293, 33772, 1936, 6676, 4992, 613, 1708, 11, 550, 291, 393, 1190, 264, 413, 11062, 15517], "temperature": 0.0, "avg_logprob": -0.2019873098893599, "compression_ratio": 1.625, "no_speech_prob": 1.0899790140683763e-05}, {"id": 98, "seek": 62844, "start": 641.5200000000001, "end": 646.8000000000001, "text": " there. So locally, when you launch this for the first time, the Dagger SDK will launch", "tokens": [456, 13, 407, 16143, 11, 562, 291, 4025, 341, 337, 264, 700, 565, 11, 264, 413, 11062, 37135, 486, 4025], "temperature": 0.0, "avg_logprob": -0.2019873098893599, "compression_ratio": 1.625, "no_speech_prob": 1.0899790140683763e-05}, {"id": 99, "seek": 62844, "start": 646.8000000000001, "end": 652.24, "text": " the Dagger engine for you and you send these GraphQL queries. You'll see a couple examples", "tokens": [264, 413, 11062, 2848, 337, 291, 293, 291, 2845, 613, 21884, 13695, 24109, 13, 509, 603, 536, 257, 1916, 5110], "temperature": 0.0, "avg_logprob": -0.2019873098893599, "compression_ratio": 1.625, "no_speech_prob": 1.0899790140683763e-05}, {"id": 100, "seek": 65224, "start": 652.24, "end": 660.44, "text": " how that looks like in the SDK, and the Dagger engine basically builds a DAG, directed basically", "tokens": [577, 300, 1542, 411, 294, 264, 37135, 11, 293, 264, 413, 11062, 2848, 1936, 15182, 257, 9578, 38, 11, 12898, 1936], "temperature": 0.0, "avg_logprob": -0.19883893610356929, "compression_ratio": 1.6077586206896552, "no_speech_prob": 1.9160388546879403e-05}, {"id": 101, "seek": 65224, "start": 660.44, "end": 667.52, "text": " graph of all those steps, and then sends it through, well, it says an OCI runtime, I believe", "tokens": [4295, 295, 439, 729, 4439, 11, 293, 550, 14790, 309, 807, 11, 731, 11, 309, 1619, 364, 422, 25240, 34474, 11, 286, 1697], "temperature": 0.0, "avg_logprob": -0.19883893610356929, "compression_ratio": 1.6077586206896552, "no_speech_prob": 1.9160388546879403e-05}, {"id": 102, "seek": 65224, "start": 667.52, "end": 673.52, "text": " currently Docker is the only supported runtime, but sends through an OCI runtime and runs the", "tokens": [4362, 33772, 307, 264, 787, 8104, 34474, 11, 457, 14790, 807, 364, 422, 25240, 34474, 293, 6676, 264], "temperature": 0.0, "avg_logprob": -0.19883893610356929, "compression_ratio": 1.6077586206896552, "no_speech_prob": 1.9160388546879403e-05}, {"id": 103, "seek": 65224, "start": 673.52, "end": 678.72, "text": " whole thing in containers for you. And then when, obviously, when a pipeline is finished,", "tokens": [1379, 551, 294, 17089, 337, 291, 13, 400, 550, 562, 11, 2745, 11, 562, 257, 15517, 307, 4335, 11], "temperature": 0.0, "avg_logprob": -0.19883893610356929, "compression_ratio": 1.6077586206896552, "no_speech_prob": 1.9160388546879403e-05}, {"id": 104, "seek": 67872, "start": 678.72, "end": 682.84, "text": " you get back the results, and you can use the results in another pipeline if you want", "tokens": [291, 483, 646, 264, 3542, 11, 293, 291, 393, 764, 264, 3542, 294, 1071, 15517, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.17339873581789852, "compression_ratio": 1.7254901960784315, "no_speech_prob": 7.130847370717674e-05}, {"id": 105, "seek": 67872, "start": 682.84, "end": 688.36, "text": " to. For example, the result of your build pipeline would be used in your deployed pipeline,", "tokens": [281, 13, 1171, 1365, 11, 264, 1874, 295, 428, 1322, 15517, 576, 312, 1143, 294, 428, 17826, 15517, 11], "temperature": 0.0, "avg_logprob": -0.17339873581789852, "compression_ratio": 1.7254901960784315, "no_speech_prob": 7.130847370717674e-05}, {"id": 106, "seek": 67872, "start": 688.36, "end": 694.8000000000001, "text": " and you could deploy or project or whatever you have. So that's how Dagger works under", "tokens": [293, 291, 727, 7274, 420, 1716, 420, 2035, 291, 362, 13, 407, 300, 311, 577, 413, 11062, 1985, 833], "temperature": 0.0, "avg_logprob": -0.17339873581789852, "compression_ratio": 1.7254901960784315, "no_speech_prob": 7.130847370717674e-05}, {"id": 107, "seek": 67872, "start": 694.8000000000001, "end": 706.4, "text": " the hood. And let's take a look at an actual example. Let's see. So the example will be", "tokens": [264, 13376, 13, 400, 718, 311, 747, 257, 574, 412, 364, 3539, 1365, 13, 961, 311, 536, 13, 407, 264, 1365, 486, 312], "temperature": 0.0, "avg_logprob": -0.17339873581789852, "compression_ratio": 1.7254901960784315, "no_speech_prob": 7.130847370717674e-05}, {"id": 108, "seek": 70640, "start": 706.4, "end": 727.0799999999999, "text": " go because this is the go-to-room. Can you see it from the back? Okay, cool. So the first", "tokens": [352, 570, 341, 307, 264, 352, 12, 1353, 12, 2861, 13, 1664, 291, 536, 309, 490, 264, 646, 30, 1033, 11, 1627, 13, 407, 264, 700], "temperature": 0.0, "avg_logprob": -0.34566098339152784, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.6358065446838737e-05}, {"id": 109, "seek": 70640, "start": 727.0799999999999, "end": 734.92, "text": " thing you need to use the Dagger SDK and go is importing this module from Dagger. It's", "tokens": [551, 291, 643, 281, 764, 264, 413, 11062, 37135, 293, 352, 307, 43866, 341, 10088, 490, 413, 11062, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.34566098339152784, "compression_ratio": 1.3333333333333333, "no_speech_prob": 4.6358065446838737e-05}, {"id": 110, "seek": 73492, "start": 734.92, "end": 747.8, "text": " the Dagger SDK for go that you can use to interface with the Dagger engine. And once", "tokens": [264, 413, 11062, 37135, 337, 352, 300, 291, 393, 764, 281, 9226, 365, 264, 413, 11062, 2848, 13, 400, 1564], "temperature": 0.0, "avg_logprob": -0.13941690423986414, "compression_ratio": 1.5963302752293578, "no_speech_prob": 6.174948794068769e-05}, {"id": 111, "seek": 73492, "start": 747.8, "end": 751.8399999999999, "text": " you have that, you can basically start writing your own program. Now, in this case, I'm using", "tokens": [291, 362, 300, 11, 291, 393, 1936, 722, 3579, 428, 1065, 1461, 13, 823, 11, 294, 341, 1389, 11, 286, 478, 1228], "temperature": 0.0, "avg_logprob": -0.13941690423986414, "compression_ratio": 1.5963302752293578, "no_speech_prob": 6.174948794068769e-05}, {"id": 112, "seek": 73492, "start": 751.8399999999999, "end": 756.7199999999999, "text": " mage. I'm not sure if you're familiar with that, but it's basically like a make file", "tokens": [463, 432, 13, 286, 478, 406, 988, 498, 291, 434, 4963, 365, 300, 11, 457, 309, 311, 1936, 411, 257, 652, 3991], "temperature": 0.0, "avg_logprob": -0.13941690423986414, "compression_ratio": 1.5963302752293578, "no_speech_prob": 6.174948794068769e-05}, {"id": 113, "seek": 73492, "start": 756.7199999999999, "end": 763.1999999999999, "text": " like solution for go. You can write these functions, and mage will basically compile", "tokens": [411, 3827, 337, 352, 13, 509, 393, 2464, 613, 6828, 11, 293, 463, 432, 486, 1936, 31413], "temperature": 0.0, "avg_logprob": -0.13941690423986414, "compression_ratio": 1.5963302752293578, "no_speech_prob": 6.174948794068769e-05}, {"id": 114, "seek": 76320, "start": 763.2, "end": 772.36, "text": " the binary from that and execute it like it would work with make. Now, you can absolutely", "tokens": [264, 17434, 490, 300, 293, 14483, 309, 411, 309, 576, 589, 365, 652, 13, 823, 11, 291, 393, 3122], "temperature": 0.0, "avg_logprob": -0.16267559263441297, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.6573856303002685e-05}, {"id": 115, "seek": 76320, "start": 772.36, "end": 776.72, "text": " import this Dagger package in your own application if you want to. In case of applications, it's", "tokens": [974, 341, 413, 11062, 7372, 294, 428, 1065, 3861, 498, 291, 528, 281, 13, 682, 1389, 295, 5821, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.16267559263441297, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.6573856303002685e-05}, {"id": 116, "seek": 76320, "start": 776.72, "end": 782.6800000000001, "text": " probably not a huge deal if you have an additional dependency in your go modules. If you're writing", "tokens": [1391, 406, 257, 2603, 2028, 498, 291, 362, 364, 4497, 33621, 294, 428, 352, 16679, 13, 759, 291, 434, 3579], "temperature": 0.0, "avg_logprob": -0.16267559263441297, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.6573856303002685e-05}, {"id": 117, "seek": 76320, "start": 782.6800000000001, "end": 791.6800000000001, "text": " the library, though, you might want to create a separate module, for example, called CI,", "tokens": [264, 6405, 11, 1673, 11, 291, 1062, 528, 281, 1884, 257, 4994, 10088, 11, 337, 1365, 11, 1219, 37777, 11], "temperature": 0.0, "avg_logprob": -0.16267559263441297, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.6573856303002685e-05}, {"id": 118, "seek": 79168, "start": 791.68, "end": 797.9599999999999, "text": " and import the Dagger SDK in that separate module so you don't import Dagger as a development", "tokens": [293, 974, 264, 413, 11062, 37135, 294, 300, 4994, 10088, 370, 291, 500, 380, 974, 413, 11062, 382, 257, 3250], "temperature": 0.0, "avg_logprob": -0.17482203315286077, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.941181057598442e-05}, {"id": 119, "seek": 79168, "start": 797.9599999999999, "end": 804.8, "text": " dependency in your libraries go that modified. I know it still won't be built or still won't", "tokens": [33621, 294, 428, 15148, 352, 300, 15873, 13, 286, 458, 309, 920, 1582, 380, 312, 3094, 420, 920, 1582, 380], "temperature": 0.0, "avg_logprob": -0.17482203315286077, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.941181057598442e-05}, {"id": 120, "seek": 79168, "start": 804.8, "end": 809.76, "text": " be in the final binary if you import that library, but some people get to know it if", "tokens": [312, 294, 264, 2572, 17434, 498, 291, 974, 300, 6405, 11, 457, 512, 561, 483, 281, 458, 309, 498], "temperature": 0.0, "avg_logprob": -0.17482203315286077, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.941181057598442e-05}, {"id": 121, "seek": 79168, "start": 809.76, "end": 818.64, "text": " they see dependencies that is actually not necessary for the library. So make the life", "tokens": [436, 536, 36606, 300, 307, 767, 406, 4818, 337, 264, 6405, 13, 407, 652, 264, 993], "temperature": 0.0, "avg_logprob": -0.17482203315286077, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.941181057598442e-05}, {"id": 122, "seek": 81864, "start": 818.64, "end": 824.0, "text": " easier for your peers, and if you develop a library and use Dagger, just create a separate", "tokens": [3571, 337, 428, 16739, 11, 293, 498, 291, 1499, 257, 6405, 293, 764, 413, 11062, 11, 445, 1884, 257, 4994], "temperature": 0.0, "avg_logprob": -0.15892186658135776, "compression_ratio": 1.6619718309859155, "no_speech_prob": 2.536878855607938e-05}, {"id": 123, "seek": 81864, "start": 824.0, "end": 829.04, "text": " module and put all the Dagger-related code there. The first thing you need to do when", "tokens": [10088, 293, 829, 439, 264, 413, 11062, 12, 12004, 3089, 456, 13, 440, 700, 551, 291, 643, 281, 360, 562], "temperature": 0.0, "avg_logprob": -0.15892186658135776, "compression_ratio": 1.6619718309859155, "no_speech_prob": 2.536878855607938e-05}, {"id": 124, "seek": 81864, "start": 829.04, "end": 839.48, "text": " you want to write pipeline with Dagger is call this DaggerConnect function, which will", "tokens": [291, 528, 281, 2464, 15517, 365, 413, 11062, 307, 818, 341, 413, 11062, 9838, 1569, 2445, 11, 597, 486], "temperature": 0.0, "avg_logprob": -0.15892186658135776, "compression_ratio": 1.6619718309859155, "no_speech_prob": 2.536878855607938e-05}, {"id": 125, "seek": 81864, "start": 839.48, "end": 848.12, "text": " basically connect to your Docker runtime, and it will launch the Dagger engine for you and", "tokens": [1936, 1745, 281, 428, 33772, 34474, 11, 293, 309, 486, 4025, 264, 413, 11062, 2848, 337, 291, 293], "temperature": 0.0, "avg_logprob": -0.15892186658135776, "compression_ratio": 1.6619718309859155, "no_speech_prob": 2.536878855607938e-05}, {"id": 126, "seek": 84812, "start": 848.12, "end": 854.64, "text": " start the so-called session. Now, within that session, you can start building your actual", "tokens": [722, 264, 370, 12, 11880, 5481, 13, 823, 11, 1951, 300, 5481, 11, 291, 393, 722, 2390, 428, 3539], "temperature": 0.0, "avg_logprob": -0.14766617943258847, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.5571232981747016e-05}, {"id": 127, "seek": 84812, "start": 854.64, "end": 861.48, "text": " pipelines using these containers. Now, it's pretty similar to how a Docker file would", "tokens": [40168, 1228, 613, 17089, 13, 823, 11, 309, 311, 1238, 2531, 281, 577, 257, 33772, 3991, 576], "temperature": 0.0, "avg_logprob": -0.14766617943258847, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.5571232981747016e-05}, {"id": 128, "seek": 84812, "start": 861.48, "end": 870.4, "text": " look like for good reason, but what you can do here is basically use some of the same", "tokens": [574, 411, 337, 665, 1778, 11, 457, 437, 291, 393, 360, 510, 307, 1936, 764, 512, 295, 264, 912], "temperature": 0.0, "avg_logprob": -0.14766617943258847, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.5571232981747016e-05}, {"id": 129, "seek": 84812, "start": 870.4, "end": 876.5600000000001, "text": " instructions as you would do in a Docker file. You can obviously go from a base image, which", "tokens": [9415, 382, 291, 576, 360, 294, 257, 33772, 3991, 13, 509, 393, 2745, 352, 490, 257, 3096, 3256, 11, 597], "temperature": 0.0, "avg_logprob": -0.14766617943258847, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.5571232981747016e-05}, {"id": 130, "seek": 87656, "start": 876.56, "end": 882.1199999999999, "text": " will be going in a Go project, for example. You can mount your source code. That's how", "tokens": [486, 312, 516, 294, 257, 1037, 1716, 11, 337, 1365, 13, 509, 393, 3746, 428, 4009, 3089, 13, 663, 311, 577], "temperature": 0.0, "avg_logprob": -0.16481181158535724, "compression_ratio": 1.5705882352941176, "no_speech_prob": 9.612162102712318e-06}, {"id": 131, "seek": 87656, "start": 882.1199999999999, "end": 887.4799999999999, "text": " you would have access to the source code within the container, and then you can run a bunch", "tokens": [291, 576, 362, 2105, 281, 264, 4009, 3089, 1951, 264, 10129, 11, 293, 550, 291, 393, 1190, 257, 3840], "temperature": 0.0, "avg_logprob": -0.16481181158535724, "compression_ratio": 1.5705882352941176, "no_speech_prob": 9.612162102712318e-06}, {"id": 132, "seek": 87656, "start": 887.4799999999999, "end": 898.28, "text": " of commands like test or you can do the same with the linter, for example. And the other", "tokens": [295, 16901, 411, 1500, 420, 291, 393, 360, 264, 912, 365, 264, 287, 5106, 11, 337, 1365, 13, 400, 264, 661], "temperature": 0.0, "avg_logprob": -0.16481181158535724, "compression_ratio": 1.5705882352941176, "no_speech_prob": 9.612162102712318e-06}, {"id": 133, "seek": 89828, "start": 898.28, "end": 907.4, "text": " two here, these are the mounted caches. You can do that with built-kit, actually. I believe", "tokens": [732, 510, 11, 613, 366, 264, 19138, 269, 13272, 13, 509, 393, 360, 300, 365, 3094, 12, 22681, 11, 767, 13, 286, 1697], "temperature": 0.0, "avg_logprob": -0.2102514575509464, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.3699421944911592e-05}, {"id": 134, "seek": 89828, "start": 907.4, "end": 911.92, "text": " that's a built-kit functionality, so you can mount a cache directory to the container", "tokens": [300, 311, 257, 3094, 12, 22681, 14980, 11, 370, 291, 393, 3746, 257, 19459, 21120, 281, 264, 10129], "temperature": 0.0, "avg_logprob": -0.2102514575509464, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.3699421944911592e-05}, {"id": 135, "seek": 89828, "start": 911.92, "end": 919.8, "text": " that will not actually be part of the container, but it will be a mounted cache directory from", "tokens": [300, 486, 406, 767, 312, 644, 295, 264, 10129, 11, 457, 309, 486, 312, 257, 19138, 19459, 21120, 490], "temperature": 0.0, "avg_logprob": -0.2102514575509464, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.3699421944911592e-05}, {"id": 136, "seek": 91980, "start": 919.8, "end": 933.64, "text": " your host. Now, let's see if I can run this. So, I'm using the mage miner here. I'm telling", "tokens": [428, 3975, 13, 823, 11, 718, 311, 536, 498, 286, 393, 1190, 341, 13, 407, 11, 286, 478, 1228, 264, 463, 432, 18746, 510, 13, 286, 478, 3585], "temperature": 0.0, "avg_logprob": -0.1983767792030617, "compression_ratio": 1.3533834586466165, "no_speech_prob": 5.310526375978952e-06}, {"id": 137, "seek": 91980, "start": 933.64, "end": 939.1999999999999, "text": " it to change to the CI directory because it's a separate module, and then I'm telling it", "tokens": [309, 281, 1319, 281, 264, 37777, 21120, 570, 309, 311, 257, 4994, 10088, 11, 293, 550, 286, 478, 3585, 309], "temperature": 0.0, "avg_logprob": -0.1983767792030617, "compression_ratio": 1.3533834586466165, "no_speech_prob": 5.310526375978952e-06}, {"id": 138, "seek": 93920, "start": 939.2, "end": 955.8000000000001, "text": " to use the current jet. Can you hear me? Okay. I don't know what happened there. And then", "tokens": [281, 764, 264, 2190, 14452, 13, 1664, 291, 1568, 385, 30, 1033, 13, 286, 500, 380, 458, 437, 2011, 456, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.21839368343353271, "compression_ratio": 1.2826086956521738, "no_speech_prob": 1.007689115795074e-05}, {"id": 139, "seek": 93920, "start": 955.8000000000001, "end": 963.08, "text": " I'm basically just telling it to run the test function here again, similarly how a mage", "tokens": [286, 478, 1936, 445, 3585, 309, 281, 1190, 264, 1500, 2445, 510, 797, 11, 14138, 577, 257, 463, 432], "temperature": 0.0, "avg_logprob": -0.21839368343353271, "compression_ratio": 1.2826086956521738, "no_speech_prob": 1.007689115795074e-05}, {"id": 140, "seek": 96308, "start": 963.08, "end": 969.76, "text": " file would look like. Now, let's see what happens. Kind of hope that I don't have to", "tokens": [3991, 576, 574, 411, 13, 823, 11, 718, 311, 536, 437, 2314, 13, 9242, 295, 1454, 300, 286, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.1520840021280142, "compression_ratio": 1.0632911392405062, "no_speech_prob": 7.037031900836155e-05}, {"id": 141, "seek": 96976, "start": 969.76, "end": 998.48, "text": " download all those container images. Let's see. Let's get some locks here. I swear this", "tokens": [5484, 439, 729, 10129, 5267, 13, 961, 311, 536, 13, 961, 311, 483, 512, 20703, 510, 13, 286, 11902, 341], "temperature": 0.0, "avg_logprob": -0.2128694454828898, "compression_ratio": 1.0875, "no_speech_prob": 0.00018642486247699708}, {"id": 142, "seek": 99848, "start": 998.48, "end": 1013.44, "text": " worked like a couple of hours ago. Oh, you know what? I think I don't have Docker running.", "tokens": [2732, 411, 257, 1916, 295, 2496, 2057, 13, 876, 11, 291, 458, 437, 30, 286, 519, 286, 500, 380, 362, 33772, 2614, 13], "temperature": 0.0, "avg_logprob": -0.27143598485876014, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.000348710804246366}, {"id": 143, "seek": 101344, "start": 1013.44, "end": 1031.72, "text": " Yeah. That's a problem. Yeah, maybe we'll, yeah. So, I don't have the Docker engine running", "tokens": [865, 13, 663, 311, 257, 1154, 13, 865, 11, 1310, 321, 603, 11, 1338, 13, 407, 11, 286, 500, 380, 362, 264, 33772, 2848, 2614], "temperature": 0.0, "avg_logprob": -0.3220568689806708, "compression_ratio": 1.0340909090909092, "no_speech_prob": 2.370196489209775e-05}, {"id": 144, "seek": 103172, "start": 1031.72, "end": 1048.0, "text": " at the moment. Let's see. This should start a new container. I mean, this should start", "tokens": [412, 264, 1623, 13, 961, 311, 536, 13, 639, 820, 722, 257, 777, 10129, 13, 286, 914, 11, 341, 820, 722], "temperature": 0.0, "avg_logprob": -0.26611862182617185, "compression_ratio": 1.1168831168831168, "no_speech_prob": 0.000630264519713819}, {"id": 145, "seek": 104800, "start": 1048.0, "end": 1072.56, "text": " a new container. Let's see what's going on. All right. This is not great.", "tokens": [257, 777, 10129, 13, 961, 311, 536, 437, 311, 516, 322, 13, 1057, 558, 13, 639, 307, 406, 869, 13], "temperature": 0.0, "avg_logprob": -0.41362043221791583, "compression_ratio": 1.0138888888888888, "no_speech_prob": 0.0013016985030844808}, {"id": 146, "seek": 107256, "start": 1072.56, "end": 1089.96, "text": " Can we all pray to the demo gods, please? Thank you.", "tokens": [1664, 321, 439, 3690, 281, 264, 10723, 14049, 11, 1767, 30, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.41004575623406303, "compression_ratio": 0.8666666666666667, "no_speech_prob": 0.0056304908357560635}, {"id": 147, "seek": 108996, "start": 1089.96, "end": 1115.72, "text": " Okay. You all just have to believe me that this actually works. Okay. So, here are the", "tokens": [1033, 13, 509, 439, 445, 362, 281, 1697, 385, 300, 341, 767, 1985, 13, 1033, 13, 407, 11, 510, 366, 264], "temperature": 0.0, "avg_logprob": -0.23677377700805663, "compression_ratio": 1.048780487804878, "no_speech_prob": 0.000810803088825196}, {"id": 148, "seek": 111572, "start": 1115.72, "end": 1133.48, "text": " locks from the previous run. So, this actually worked before. Yeah, it says, let's see. Okay,", "tokens": [20703, 490, 264, 3894, 1190, 13, 407, 11, 341, 767, 2732, 949, 13, 865, 11, 309, 1619, 11, 718, 311, 536, 13, 1033, 11], "temperature": 0.0, "avg_logprob": -0.41111561230250765, "compression_ratio": 1.0689655172413792, "no_speech_prob": 0.000914761156309396}, {"id": 149, "seek": 113348, "start": 1133.48, "end": 1163.4, "text": " let's try that. Let's see. Do we have internet connection here? Yeah, we do have internet", "tokens": [718, 311, 853, 300, 13, 961, 311, 536, 13, 1144, 321, 362, 4705, 4984, 510, 30, 865, 11, 321, 360, 362, 4705], "temperature": 0.0, "avg_logprob": -0.34394620015070987, "compression_ratio": 1.141025641025641, "no_speech_prob": 0.001973041333258152}, {"id": 150, "seek": 116340, "start": 1163.4, "end": 1178.2, "text": " connection. Okay. Well, we'll have to work with the locks from here. So, well, basically", "tokens": [4984, 13, 1033, 13, 1042, 11, 321, 603, 362, 281, 589, 365, 264, 20703, 490, 510, 13, 407, 11, 731, 11, 1936], "temperature": 0.0, "avg_logprob": -0.2716507911682129, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.00015245942631736398}, {"id": 151, "seek": 116340, "start": 1178.2, "end": 1186.64, "text": " what happens here is when it works is it just runs the whole thing within this goal and", "tokens": [437, 2314, 510, 307, 562, 309, 1985, 307, 309, 445, 6676, 264, 1379, 551, 1951, 341, 3387, 293], "temperature": 0.0, "avg_logprob": -0.2716507911682129, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.00015245942631736398}, {"id": 152, "seek": 118664, "start": 1186.64, "end": 1194.96, "text": " the image mounts the source code and then runs the goal that test command and just gives", "tokens": [264, 3256, 40982, 264, 4009, 3089, 293, 550, 6676, 264, 3387, 300, 1500, 5622, 293, 445, 2709], "temperature": 0.0, "avg_logprob": -0.38040899193805194, "compression_ratio": 1.6645962732919255, "no_speech_prob": 5.6800836318871006e-05}, {"id": 153, "seek": 118664, "start": 1194.96, "end": 1200.3600000000001, "text": " back the results. Obviously, this is the build log, like this is the debug log, but normally", "tokens": [646, 264, 3542, 13, 7580, 11, 341, 307, 264, 1322, 3565, 11, 411, 341, 307, 264, 24083, 3565, 11, 457, 5646], "temperature": 0.0, "avg_logprob": -0.38040899193805194, "compression_ratio": 1.6645962732919255, "no_speech_prob": 5.6800836318871006e-05}, {"id": 154, "seek": 118664, "start": 1200.3600000000001, "end": 1206.2800000000002, "text": " it would just output the, output of the go test command. Or the go lxci command. Let's", "tokens": [309, 576, 445, 5598, 264, 11, 5598, 295, 264, 352, 1500, 5622, 13, 1610, 264, 352, 287, 87, 537, 5622, 13, 961, 311], "temperature": 0.0, "avg_logprob": -0.38040899193805194, "compression_ratio": 1.6645962732919255, "no_speech_prob": 5.6800836318871006e-05}, {"id": 155, "seek": 120628, "start": 1206.28, "end": 1232.2, "text": " see. It's still not working. Let's try from hotspot. Maybe that works better. Anyway,", "tokens": [536, 13, 467, 311, 920, 406, 1364, 13, 961, 311, 853, 490, 36121, 17698, 13, 2704, 300, 1985, 1101, 13, 5684, 11], "temperature": 0.0, "avg_logprob": -0.2840788731208214, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0001573810150148347}, {"id": 156, "seek": 123220, "start": 1232.2, "end": 1244.0800000000002, "text": " if, well, if someone wants to get back their money, sorry, folks, this is a free conference.", "tokens": [498, 11, 731, 11, 498, 1580, 2738, 281, 483, 646, 641, 1460, 11, 2597, 11, 4024, 11, 341, 307, 257, 1737, 7586, 13], "temperature": 0.0, "avg_logprob": -0.17054884484473695, "compression_ratio": 1.6308411214953271, "no_speech_prob": 4.255835301592015e-05}, {"id": 157, "seek": 123220, "start": 1244.0800000000002, "end": 1249.32, "text": " Anyway, yeah, you will just have to believe me that this works, but I will try to make", "tokens": [5684, 11, 1338, 11, 291, 486, 445, 362, 281, 1697, 385, 300, 341, 1985, 11, 457, 286, 486, 853, 281, 652], "temperature": 0.0, "avg_logprob": -0.17054884484473695, "compression_ratio": 1.6308411214953271, "no_speech_prob": 4.255835301592015e-05}, {"id": 158, "seek": 123220, "start": 1249.32, "end": 1253.88, "text": " this work after the presentation. Now, if you take a look at the code here, this is", "tokens": [341, 589, 934, 264, 5860, 13, 823, 11, 498, 291, 747, 257, 574, 412, 264, 3089, 510, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.17054884484473695, "compression_ratio": 1.6308411214953271, "no_speech_prob": 4.255835301592015e-05}, {"id": 159, "seek": 123220, "start": 1253.88, "end": 1261.92, "text": " still not very user friendly. If you don't know how dagger works or if you don't know", "tokens": [920, 406, 588, 4195, 9208, 13, 759, 291, 500, 380, 458, 577, 36972, 1985, 420, 498, 291, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.17054884484473695, "compression_ratio": 1.6308411214953271, "no_speech_prob": 4.255835301592015e-05}, {"id": 160, "seek": 126192, "start": 1261.92, "end": 1266.2, "text": " what happens here, then it's not really useful to you. You will have to go to the documentation", "tokens": [437, 2314, 510, 11, 550, 309, 311, 406, 534, 4420, 281, 291, 13, 509, 486, 362, 281, 352, 281, 264, 14333], "temperature": 0.0, "avg_logprob": -0.15662826738859478, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.4359276216709986e-05}, {"id": 161, "seek": 126192, "start": 1266.2, "end": 1273.3600000000001, "text": " and understand how this whole thing works, when it works. So, but the good thing is that", "tokens": [293, 1223, 577, 341, 1379, 551, 1985, 11, 562, 309, 1985, 13, 407, 11, 457, 264, 665, 551, 307, 300], "temperature": 0.0, "avg_logprob": -0.15662826738859478, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.4359276216709986e-05}, {"id": 162, "seek": 126192, "start": 1273.3600000000001, "end": 1280.5600000000002, "text": " this is like, this is not an arbitrary YAML interface you have to use, so we can actually", "tokens": [341, 307, 411, 11, 341, 307, 406, 364, 23211, 398, 2865, 43, 9226, 291, 362, 281, 764, 11, 370, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.15662826738859478, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.4359276216709986e-05}, {"id": 163, "seek": 126192, "start": 1280.5600000000002, "end": 1288.0800000000002, "text": " make this a bit better if we want to. And what I did in the last couple of weeks is that", "tokens": [652, 341, 257, 857, 1101, 498, 321, 528, 281, 13, 400, 437, 286, 630, 294, 264, 1036, 1916, 295, 3259, 307, 300], "temperature": 0.0, "avg_logprob": -0.15662826738859478, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.4359276216709986e-05}, {"id": 164, "seek": 128808, "start": 1288.08, "end": 1296.9199999999998, "text": " I built a higher-level library over the dagger SDK. So, instead of writing all that container", "tokens": [286, 3094, 257, 2946, 12, 12418, 6405, 670, 264, 36972, 37135, 13, 407, 11, 2602, 295, 3579, 439, 300, 10129], "temperature": 0.0, "avg_logprob": -0.23393346222353653, "compression_ratio": 1.4010416666666667, "no_speech_prob": 1.2731115020869765e-05}, {"id": 165, "seek": 128808, "start": 1296.9199999999998, "end": 1304.76, "text": " mount nonsense stuff, you can just use this go link package. It's actually called the", "tokens": [3746, 14925, 1507, 11, 291, 393, 445, 764, 341, 352, 2113, 7372, 13, 467, 311, 767, 1219, 264], "temperature": 0.0, "avg_logprob": -0.23393346222353653, "compression_ratio": 1.4010416666666667, "no_speech_prob": 1.2731115020869765e-05}, {"id": 166, "seek": 128808, "start": 1304.76, "end": 1311.84, "text": " OCI. You can find it here if you want to give it a try. And instead of, you still have to", "tokens": [422, 25240, 13, 509, 393, 915, 309, 510, 498, 291, 528, 281, 976, 309, 257, 853, 13, 400, 2602, 295, 11, 291, 920, 362, 281], "temperature": 0.0, "avg_logprob": -0.23393346222353653, "compression_ratio": 1.4010416666666667, "no_speech_prob": 1.2731115020869765e-05}, {"id": 167, "seek": 131184, "start": 1311.84, "end": 1318.32, "text": " connect to dagger, obviously, but instead of writing that whole container code, you can", "tokens": [1745, 281, 36972, 11, 2745, 11, 457, 2602, 295, 3579, 300, 1379, 10129, 3089, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.23747569864446466, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.185699097230099e-05}, {"id": 168, "seek": 131184, "start": 1318.32, "end": 1325.8799999999999, "text": " just use this much more friendly interface to run your tests or run the go link CILin,", "tokens": [445, 764, 341, 709, 544, 9208, 9226, 281, 1190, 428, 6921, 420, 1190, 264, 352, 2113, 383, 4620, 259, 11], "temperature": 0.0, "avg_logprob": -0.23747569864446466, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.185699097230099e-05}, {"id": 169, "seek": 131184, "start": 1325.8799999999999, "end": 1330.24, "text": " for example. And it's much easier for developers to interact with this. Like, if they want", "tokens": [337, 1365, 13, 400, 309, 311, 709, 3571, 337, 8849, 281, 4648, 365, 341, 13, 1743, 11, 498, 436, 528], "temperature": 0.0, "avg_logprob": -0.23747569864446466, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.185699097230099e-05}, {"id": 170, "seek": 131184, "start": 1330.24, "end": 1336.0, "text": " to change the cover mode, for example, it's pretty obvious how you would want to do that.", "tokens": [281, 1319, 264, 2060, 4391, 11, 337, 1365, 11, 309, 311, 1238, 6322, 577, 291, 576, 528, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.23747569864446466, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.185699097230099e-05}, {"id": 171, "seek": 133600, "start": 1336.0, "end": 1342.64, "text": " In this case, compared to how you would want to do that, it would be the lower-level dagger", "tokens": [682, 341, 1389, 11, 5347, 281, 577, 291, 576, 528, 281, 360, 300, 11, 309, 576, 312, 264, 3126, 12, 12418, 36972], "temperature": 0.0, "avg_logprob": -0.2419757843017578, "compression_ratio": 1.123456790123457, "no_speech_prob": 5.198018698138185e-05}, {"id": 172, "seek": 134264, "start": 1342.64, "end": 1372.24, "text": " SDK stuff. Let's give this another try. Now, from the... Oh, still on. Let's go to the...", "tokens": [37135, 1507, 13, 961, 311, 976, 341, 1071, 853, 13, 823, 11, 490, 264, 485, 876, 11, 920, 322, 13, 961, 311, 352, 281, 264, 485], "temperature": 0.0, "avg_logprob": -0.2468203862508138, "compression_ratio": 1.072289156626506, "no_speech_prob": 0.0012263964163139462}, {"id": 173, "seek": 137224, "start": 1372.24, "end": 1382.92, "text": " Well, it doesn't work. Anyway, if you want to give this a try at home, you're absolutely", "tokens": [1042, 11, 309, 1177, 380, 589, 13, 5684, 11, 498, 291, 528, 281, 976, 341, 257, 853, 412, 1280, 11, 291, 434, 3122], "temperature": 0.0, "avg_logprob": -0.12594658220318003, "compression_ratio": 1.421875, "no_speech_prob": 0.00011157568951603025}, {"id": 174, "seek": 137224, "start": 1382.92, "end": 1389.52, "text": " welcome to do that. The documentation is getting better by the day. It has a bunch of different", "tokens": [2928, 281, 360, 300, 13, 440, 14333, 307, 1242, 1101, 538, 264, 786, 13, 467, 575, 257, 3840, 295, 819], "temperature": 0.0, "avg_logprob": -0.12594658220318003, "compression_ratio": 1.421875, "no_speech_prob": 0.00011157568951603025}, {"id": 175, "seek": 137224, "start": 1389.52, "end": 1398.0, "text": " examples. You will find these examples on my GitHub as well. I promise it works. They've", "tokens": [5110, 13, 509, 486, 915, 613, 5110, 322, 452, 23331, 382, 731, 13, 286, 6228, 309, 1985, 13, 814, 600], "temperature": 0.0, "avg_logprob": -0.12594658220318003, "compression_ratio": 1.421875, "no_speech_prob": 0.00011157568951603025}, {"id": 176, "seek": 139800, "start": 1398.0, "end": 1403.12, "text": " actually just released a brand-new kickstart guide. So far, the documentation... They had", "tokens": [767, 445, 4736, 257, 3360, 12, 7686, 4437, 24419, 5934, 13, 407, 1400, 11, 264, 14333, 485, 814, 632], "temperature": 0.0, "avg_logprob": -0.16515973762229638, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0001779622252797708}, {"id": 177, "seek": 139800, "start": 1403.12, "end": 1410.2, "text": " documentation for the different SDKs in different places. Now, they have a kickstart guide that", "tokens": [14333, 337, 264, 819, 37135, 82, 294, 819, 3190, 13, 823, 11, 436, 362, 257, 4437, 24419, 5934, 300], "temperature": 0.0, "avg_logprob": -0.16515973762229638, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0001779622252797708}, {"id": 178, "seek": 139800, "start": 1410.2, "end": 1414.2, "text": " is basically the same for all of the languages. Regardless which one you want to choose or", "tokens": [307, 1936, 264, 912, 337, 439, 295, 264, 8650, 13, 25148, 597, 472, 291, 528, 281, 2826, 420], "temperature": 0.0, "avg_logprob": -0.16515973762229638, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0001779622252797708}, {"id": 179, "seek": 139800, "start": 1414.2, "end": 1420.4, "text": " if you want to try all three supported SDKs, you can do that with the kickstart guide.", "tokens": [498, 291, 528, 281, 853, 439, 1045, 8104, 37135, 82, 11, 291, 393, 360, 300, 365, 264, 4437, 24419, 5934, 13], "temperature": 0.0, "avg_logprob": -0.16515973762229638, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0001779622252797708}, {"id": 180, "seek": 139800, "start": 1420.4, "end": 1426.4, "text": " You can actually go ahead and run the code from there. And finally, they have a playground", "tokens": [509, 393, 767, 352, 2286, 293, 1190, 264, 3089, 490, 456, 13, 400, 2721, 11, 436, 362, 257, 24646], "temperature": 0.0, "avg_logprob": -0.16515973762229638, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0001779622252797708}, {"id": 181, "seek": 142640, "start": 1426.4, "end": 1435.3600000000001, "text": " that works with their low-level GraphQL API. So if you want to give that a try, it's fairly", "tokens": [300, 1985, 365, 641, 2295, 12, 12418, 21884, 13695, 9362, 13, 407, 498, 291, 528, 281, 976, 300, 257, 853, 11, 309, 311, 6457], "temperature": 0.0, "avg_logprob": -0.16029728480747768, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001480186911066994}, {"id": 182, "seek": 142640, "start": 1435.3600000000001, "end": 1441.44, "text": " similar to the SDK, actually. If you want to give that a try, then you can absolutely", "tokens": [2531, 281, 264, 37135, 11, 767, 13, 759, 291, 528, 281, 976, 300, 257, 853, 11, 550, 291, 393, 3122], "temperature": 0.0, "avg_logprob": -0.16029728480747768, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001480186911066994}, {"id": 183, "seek": 142640, "start": 1441.44, "end": 1452.6000000000001, "text": " do that and see how dagger works without actually installing it on your computer. That's all", "tokens": [360, 300, 293, 536, 577, 36972, 1985, 1553, 767, 20762, 309, 322, 428, 3820, 13, 663, 311, 439], "temperature": 0.0, "avg_logprob": -0.16029728480747768, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001480186911066994}, {"id": 184, "seek": 145260, "start": 1452.6, "end": 1459.52, "text": " I had for today. If you have questions, feel free. There is a sticker. Thank you very much", "tokens": [286, 632, 337, 965, 13, 759, 291, 362, 1651, 11, 841, 1737, 13, 821, 307, 257, 20400, 13, 1044, 291, 588, 709], "temperature": 0.0, "avg_logprob": -0.2932303249835968, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0006171720451675355}, {"id": 185, "seek": 145260, "start": 1459.52, "end": 1463.32, "text": " for your attention.", "tokens": [337, 428, 3202, 13], "temperature": 0.0, "avg_logprob": -0.2932303249835968, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0006171720451675355}, {"id": 186, "seek": 145260, "start": 1463.32, "end": 1473.76, "text": " If there are any questions, raise your hand. I'll try to give you a microphone. I have", "tokens": [759, 456, 366, 604, 1651, 11, 5300, 428, 1011, 13, 286, 603, 853, 281, 976, 291, 257, 10952, 13, 286, 362], "temperature": 0.0, "avg_logprob": -0.2932303249835968, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0006171720451675355}, {"id": 187, "seek": 145260, "start": 1473.76, "end": 1475.36, "text": " one over here that's closer.", "tokens": [472, 670, 510, 300, 311, 4966, 13], "temperature": 0.0, "avg_logprob": -0.2932303249835968, "compression_ratio": 1.5066666666666666, "no_speech_prob": 0.0006171720451675355}, {"id": 188, "seek": 147536, "start": 1475.36, "end": 1482.08, "text": " Can you use it with something other than Docker? I'm sorry. Can you use it with something other", "tokens": [1664, 291, 764, 309, 365, 746, 661, 813, 33772, 30, 286, 478, 2597, 13, 1664, 291, 764, 309, 365, 746, 661], "temperature": 0.0, "avg_logprob": -0.3056028951512705, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.0015339636011049151}, {"id": 189, "seek": 147536, "start": 1482.08, "end": 1483.08, "text": " than Docker underneath?", "tokens": [813, 33772, 7223, 30], "temperature": 0.0, "avg_logprob": -0.3056028951512705, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.0015339636011049151}, {"id": 190, "seek": 147536, "start": 1483.08, "end": 1489.1599999999999, "text": " I think you can use it with Docker compatible runtimes. So I think you can use it with Podman", "tokens": [286, 519, 291, 393, 764, 309, 365, 33772, 18218, 49435, 1532, 13, 407, 286, 519, 291, 393, 764, 309, 365, 12646, 1601], "temperature": 0.0, "avg_logprob": -0.3056028951512705, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.0015339636011049151}, {"id": 191, "seek": 147536, "start": 1489.1599999999999, "end": 1497.84, "text": " at the moment. I think, technically, you can use it at runtime. But I don't know if that's", "tokens": [412, 264, 1623, 13, 286, 519, 11, 12120, 11, 291, 393, 764, 309, 412, 34474, 13, 583, 286, 500, 380, 458, 498, 300, 311], "temperature": 0.0, "avg_logprob": -0.3056028951512705, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.0015339636011049151}, {"id": 192, "seek": 147536, "start": 1497.84, "end": 1502.08, "text": " currently available as an option. But we have someone from the Dagger team here who can", "tokens": [4362, 2435, 382, 364, 3614, 13, 583, 321, 362, 1580, 490, 264, 413, 11062, 1469, 510, 567, 393], "temperature": 0.0, "avg_logprob": -0.3056028951512705, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.0015339636011049151}, {"id": 193, "seek": 150208, "start": 1502.08, "end": 1506.28, "text": " actually answer that question.", "tokens": [767, 1867, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19428090580174182, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.001032817643135786}, {"id": 194, "seek": 150208, "start": 1506.28, "end": 1516.12, "text": " Hi. So how does the portability work when parts of your deployment depend on publishing", "tokens": [2421, 13, 407, 577, 775, 264, 2436, 2310, 589, 562, 3166, 295, 428, 19317, 5672, 322, 17832], "temperature": 0.0, "avg_logprob": -0.19428090580174182, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.001032817643135786}, {"id": 195, "seek": 150208, "start": 1516.12, "end": 1523.4399999999998, "text": " a Docker image to a repo that is external or AWS or Terraform or things that require secrets?", "tokens": [257, 33772, 3256, 281, 257, 49040, 300, 307, 8320, 420, 17650, 420, 25366, 837, 420, 721, 300, 3651, 14093, 30], "temperature": 0.0, "avg_logprob": -0.19428090580174182, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.001032817643135786}, {"id": 196, "seek": 150208, "start": 1523.4399999999998, "end": 1526.28, "text": " How does that fit in running it locally?", "tokens": [1012, 775, 300, 3318, 294, 2614, 309, 16143, 30], "temperature": 0.0, "avg_logprob": -0.19428090580174182, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.001032817643135786}, {"id": 197, "seek": 152628, "start": 1526.28, "end": 1534.0, "text": " So that's a great question, actually. So the code itself should be completely portable.", "tokens": [407, 300, 311, 257, 869, 1168, 11, 767, 13, 407, 264, 3089, 2564, 820, 312, 2584, 21800, 13], "temperature": 0.0, "avg_logprob": -0.14552464136263218, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0002523424045648426}, {"id": 198, "seek": 152628, "start": 1534.0, "end": 1540.0, "text": " So the pipeline itself can run anywhere. What you would need to do in that case is you need", "tokens": [407, 264, 15517, 2564, 393, 1190, 4992, 13, 708, 291, 576, 643, 281, 360, 294, 300, 1389, 307, 291, 643], "temperature": 0.0, "avg_logprob": -0.14552464136263218, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0002523424045648426}, {"id": 199, "seek": 152628, "start": 1540.0, "end": 1546.76, "text": " some sort of either a central secret store that you can connect to from your own computer", "tokens": [512, 1333, 295, 2139, 257, 5777, 4054, 3531, 300, 291, 393, 1745, 281, 490, 428, 1065, 3820], "temperature": 0.0, "avg_logprob": -0.14552464136263218, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0002523424045648426}, {"id": 200, "seek": 152628, "start": 1546.76, "end": 1553.84, "text": " or you need to be able to load some sort of secrets or credentials from your environment", "tokens": [420, 291, 643, 281, 312, 1075, 281, 3677, 512, 1333, 295, 14093, 420, 27404, 490, 428, 2823], "temperature": 0.0, "avg_logprob": -0.14552464136263218, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0002523424045648426}, {"id": 201, "seek": 155384, "start": 1553.84, "end": 1557.6399999999999, "text": " variables, for example. You can absolutely do that with the Dagger pipeline. So from", "tokens": [9102, 11, 337, 1365, 13, 509, 393, 3122, 360, 300, 365, 264, 413, 11062, 15517, 13, 407, 490], "temperature": 0.0, "avg_logprob": -0.2977872663928616, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0009347510640509427}, {"id": 202, "seek": 155384, "start": 1557.6399999999999, "end": 1574.08, "text": " that perspective, if the processor is here, you can push to another registry or push to", "tokens": [300, 4585, 11, 498, 264, 15321, 307, 510, 11, 291, 393, 2944, 281, 1071, 36468, 420, 2944, 281], "temperature": 0.0, "avg_logprob": -0.2977872663928616, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0009347510640509427}, {"id": 203, "seek": 155384, "start": 1574.08, "end": 1580.3999999999999, "text": " a development environment, for example. So you can parametrize pipelines based on where", "tokens": [257, 3250, 2823, 11, 337, 1365, 13, 407, 291, 393, 6220, 302, 470, 1381, 40168, 2361, 322, 689], "temperature": 0.0, "avg_logprob": -0.2977872663928616, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0009347510640509427}, {"id": 204, "seek": 158040, "start": 1580.4, "end": 1588.72, "text": " you run them. You would still run the same code, but you could deploy to different environments", "tokens": [291, 1190, 552, 13, 509, 576, 920, 1190, 264, 912, 3089, 11, 457, 291, 727, 7274, 281, 819, 12388], "temperature": 0.0, "avg_logprob": -0.4097317165798611, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00020502640109043568}, {"id": 205, "seek": 158040, "start": 1588.72, "end": 1589.72, "text": " from locally.", "tokens": [490, 16143, 13], "temperature": 0.0, "avg_logprob": -0.4097317165798611, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.00020502640109043568}, {"id": 206, "seek": 158972, "start": 1589.72, "end": 1610.0, "text": " We have one more question there. Okay. Thank you. One last applause, please.", "tokens": [50364, 492, 362, 472, 544, 1168, 456, 13, 1033, 13, 1044, 291, 13, 1485, 1036, 9969, 11, 1767, 13, 51378], "temperature": 0.0, "avg_logprob": -0.33864904585338773, "compression_ratio": 1.0, "no_speech_prob": 0.00035531516186892986}], "language": "en"}