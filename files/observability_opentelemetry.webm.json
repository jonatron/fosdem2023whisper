{"text": " Perfect. Hello everyone. I might need to take a selfie because they're not going to believe me when I get back. They go like, so people like testing, obviously, and they know, you know, sounds. Yeah, I should do a video because they're freaking not going to believe me. So apparently people know what open telemetry is and what testing is. And yeah, I was not expecting this to happen. So you're going to go out on Twitter. That's for sure. But yeah, anyway, let's just take a second to welcome our new guests in. Perfect, perfect. Yeah, this went from fun to stressful really quickly. But yeah, so let me begin. For the next 20 or so minutes, I'll be talking about observability driven development with open telemetry. So a lot of complicated words, a lot of stuff that's going to be happening. And a lot of things I need to explain for you as testers and how you can get started with this new thing of being ODD instead of TDD. So first, a quick rundown of who I am. I'm running DevRel at trace test, which is a, it's like a new tool, new open source tool that we're building for trace based testing. Obviously explain all of that later on. But you're wondering like what am I DevRel person doing at a open source conference when it's kind of because I successfully failed a startup that was doing online education. So I was from there, went into education. And because we're basically educators in DevRel, I was like, maybe, maybe, you know, I write shitty code, I can maybe be good at something like talking. So I figured that might be a good career shift. But I've also been helping build open source DevTools for five or so years. So it's pretty natural for me to be here. So enough about that, you probably think that I know what I'm talking about. Let's keep it, keep it rolling. There are four main topics. So remember these four topics that we will cover in the next 20 or so minutes. And that's, first, we'll talk about the pain of testing microservices. It's a horrible, horrible thing. And we'll also talk about TDD and how integration testing is really hard. We're all doing it. It's terrible. It's hard. But we're still doing it. And then in the last two parts, we'll talk about observability-driven development, how it can help. And then we'll show a code example, a hands-on example of how you can do it as well. So I want you to take something home with you after this 20-minute talk and actually start doing it yourself. So from the beginning, from the top down, let's talk about the pain of testing microservices. So first, the biggest issue is that you have no way of knowing where your HTTP transaction fails. You don't know. You can test an API endpoint. You get a response back. But it might be task failed successfully. You never really know if you have a row of microservices behind that initial service. So that's something you can track. You can track and test how these microservices to microservice communications happen. And of course, the hardest thing, what we all really love to hate, is mocking. It's really hard. It's really, really hard. So the solution that we propose is that we go into doing something called observability-driven development, which means that you're using distributed traces as the test assertions. So you're already using your underlying trace infrastructure to run your tests. And now, because this is a testing dev room, you might not know what tracing is, LightStep has a very nice definition of it. And they say that distributed tracing refers to methods of observing requests as they propagate through a distributed system, which means that if you have a distributed system on the left, you have services that communicate with each other. And on the right, you can see that that entire distributed trace is split into different spans. A span is the smallest unit of a distributed test. So a span can be, it can be a type of stamp. It can be a database interaction or database statement. It can be HTTP codes. It can be objects that you generate in your custom instrumentation itself. So they're literally the smallest form or part of a distributed trace. The distributed system we'll be talking about today, so the samples we will be talking about is very simple. We have two services with a mock database connection. Just to simplify this whole architecture, we will be using this to explain how distributed tracing works and how you run observability-driven development on such a system. Now, just a code sample, because this is JavaScript, the only language I really know, not that well, this is what a trace would look like. You're setting the span, you're adding attributes, and then you're ending the span. So this is the code representation of what we have over here. So just remember that for now, and we'll get into more details as we progress. So the visual queue or the visual layout of a distributed span would look like this. This is taken from the trace test app, but this is any, like any distributed span looks like this, where you have your distributed trace and you can see all of the spans within it. And if you drill into one particular span, you can see, okay, so here are all of the attributes that this span has. It can be available, book ID, the check, and the parent ID. There's a lot of different attributes you have in every span. So the next topic I do want to cover, we have all the basics down. We know why it's hard. We need to figure out why integration testing and TDD really need help. Everybody knows about the red-green feedback loop. It's awesome. It's great. We like it. We don't need to change it. But integration tests are hard. Integration tests are the kicker, where they need access to services and infrastructure. That's the hard part. You need to set up different triggers. You need to access databases. You need to set up environment variables. You need to set up authentication. All of those things that everybody hates doing. And of course, you can track which part of the microservice chain failed, which means that you're writing 90% of your code just as plumbing, just to make sure that the test will run. 10% is actually writing the test, writing the test case, and actually getting value from your TDD process. So here's what a traditional integration test would look like. You have all of your setup. Again, this is JavaScript. It can be any language. You have your setup, a bunch of modules, a bunch of setup, a bunch of plumbing. And then you have more plumbing because you need to mock something. Then you have even more plumbing because you have to figure out how to run this custom freaking syntax that has nothing to do with any language, really. You just have to learn it. So it's a lot of stuff you have to know before you actually run tests. If you compare that to a trace based test, you say, here's my URL. Here's my method. This is what I'm suring against. That's it. No complications, no plumbing, no nothing. It just points to the trace, the trace span you want to target. You have your assertion and it's done. So this is why I think observability driven can help our testing process, where obviously we need to explain what ODD is. The main thing that I think is important to know is that you need to write your code and your observability instrumentation in parallel. So the same way you do the red-green process for TDD, in ODD you write your trace spans and you write your code and your features in parallel. Which is good, first thing because in production that helps your DevOps people when they have troubleshoot, but it's also helping you write better code. And ODD is really powerful because first and foremost, of course, you're not testing mocks. Nothing is artificial. You're not creating black boxes. You're literally testing data from the traces in the real environment. So you can spin up your system, get traces from the system and test on those traces. Of course, it works with all of your existing open telemetry based distributed tracing. So if you have tracing enabled or if you want to enable it, it's really simple nowadays, it'll just work. And then from the ODD definition, we need to figure out what trace-based testing is here. So you basically add assertions against span values. And that's what determines whether the test has failed or the test has passed. It's really straightforward. So you're not just testing against the API response, you're actually testing against the whole distributed trace your system generates. So unlike postman where you trigger a test, you get something back, and then you're asserting on that response, you're literally testing and running assertions against the entire distributed trace. Really, really cool. Now let's go into some practice. How do you do observability during development? Well, you do trace tests because that's the open source tool we're building, you know, shocker. But what's important about trace test is fully open source, 100% open source, CNCF project, and it uses open telemetry trace bands as assertions. Very straightforward. Of course, it does work with any existing tracing solution you might have. You can use vendors, you can use open source tools, you can use whatever. If you have tracing in your system, it'll just work. Also, what's important is it doesn't matter if you're a QA engineer, if you're a backend developer, if you're a DevOps person, it'll just work. You have tools for everybody, web UI, CLI, whatever you want, whatever you need, it's there for you. And then why I think it's powerful, you're not running artificial tests, you're testing against real data, and obviously you have a tool belt that you're really used to. You can run test suites by chaining tests together, have transactions where the standard way you're running integration tests is you have a setup, you connect into a database, you're running an insert, you're checking if the insert works, you're deleting that whole path, that environment, and that's what we provide as well. You can set that whole transaction up through the UI. So it's literally what you're used to, but better. You always have test environments as well, which is a very big thing because you can have one set environment for your dev, for your QA, for your prod, for your whatever. So it's very, very flexible in that way as well. Obviously, I'm going to stress this no mocks because I really like that. I hate mocking. So I'm going to just shove this down your throat. Every slide is going to be no mocking. But also, one thing that I think is massively important is that if anybody's running serverless, I've been running serverless since it was a thing like in 2018 when everybody wanted to run serverless, and it was horrible, it was a horrible experience. So I'd suggest nobody really does it. But if you have to because of PMs, testing events on message queues and testing events on distributed systems and services in AWS or whatever, like it's prayer driven development. You never really know what's going to happen. So that's something that we provide. You can literally see the entire trace from that ASIC message queue from other systems, from other services, and you really know what's happening. Obviously, it's important that you get assertions based on timing. Maybe you want all of your database requests and your database queries to finish within 500 milliseconds. That just works. And you can also set wild color assertions. So the same thing I was saying about the database queries, it works for wild cards as well. So a visual demo, like a representation, what that would mean is literally like this. So you have your test executor, which is you can think of that as a trigger. You're testing your system. That trace data is getting written to your trace data store. It can be pretty much anything you've all heard of. Yeager, OpenSearch, Rufana Tempo, OpenTelemetryCollector, like all of those, even vendors like Datadog or whatever. And then what happens is that once the response gets back, we pick up that response, but we also pick up the trace. So you can run assertions based on both the trace and the response itself. And then, obviously, you get the result back and then you can see if it's passed, if it's not passed, what you need to fix, et cetera. So yeah, let's show up after all of this over 10 minutes and perfect. After all of this, just like theory and understanding what's happening, we want to jump into actual code. So let's go back to the sample of checking our trace-based test. So we have a URL and we're making sure that we're sending a GET request to that URL. We're setting up a span. So we're targeting the books span in the Books API and we're making sure that we want to have a list of books equal to three. So this is our TDD red-green process. We have a test. We want to run the code and we see, okay, so we have a handler here. It's getting some books. We have some books, but you can see that there's no instrumentation. So if we do run the test, it's going to say, okay, the 200 is fine, but we're not getting any books here. Red, let's go ahead and refactor. We're adding in our spans. So we say, okay, so now I'll add an attribute and I want to pass in the book's length into this attribute right here. Perfect. Now it passes. So this is the most banal simple use case that you can see, but you're already seeing value from it because you can pass in a custom value. That's a real data. You don't have to mess about with any marking or anything. And then obviously one thing that I'm stressing is very important is what if you want to add a span duration? So I want this API to finish within 500 milliseconds. Okay. Right now, if we have an issue, even though the code works, it might be performing badly. We can add in the span duration, check for the timing, and then obviously refactor if we need to refactor. And that's the thing in the UI as well. Once you do refactor it, this is what you would see. You go and say, okay, so finally now I have a passing test. This book's API is returning within 500 milliseconds. And then obviously the last and I think crucial thing with using trace-based testing is that you can literally test on a search on every part of an HTTP transaction. So if we go back to our books handler API, instead of calling books, we're now calling available books. So we are calling an external API to see if the books are available or not. So we're having this microservice to microservice communication. And if you check that, get the available books function. So we have some promise thingamajig happening here. We're calling an availability API and we're just checking if it's available or not. So the kicker here is we're calling an external API. The external API is super simple. We're just running some tests whether it's available or not and we're setting this attribute. So it's very, very simple example. But the thing is, what if in the availability check, we have a problem? This is why I don't do live demos. Anyway, so if in the availability check, if you're checking here, you can see, oh, we have a problem. There are books that are out of stock. So this is that down the chain action that would happen. You would never know what the hell is the problem. But now, because we have this set up, we can say, okay, so I'm adding into my trace-based test. I want to make sure the availability API is up. So I'm actually triggering this host. And I also want to make sure that all of these is available attributes is true. If I do run that test, I'll see that, whoopsie, I'll see that they're all passing except for this one because, oh wait, there was actually one node, like one part of the trace, one span that was returning false, because one book was out of stock. And if you jump in here, you can see that everything is literally passing. Everything is passing except for that one span, which is something you would never figure out if you're running the traditional way of running tests. And the last thing I really want to stress before we wrap up is that this will work with any distributed system that has open telemetry instrumentation. So any system that looks like this, you have an app with open telemetry, you're sending to the open telemetry collector, and then you're sending that trace data to any trace data store. Yeager open search doesn't really matter. You hook in your trace test instance, you pick up data on every request, you pick up data from the trace data store, and you run these tests. This is the only setup you really need to do. Install the CLI, install the server, one command, one command, and you're ready. Set up your Docker composer or Kubernetes, all of this works out of the box with the install. We have good engineers, like these guys really try to make the install really simple. You set up the trace data store, you can do that in the UI or in the CLI, doesn't really matter. Connect the data store, and you're done. It just works. So last recap, two minutes left. What did we learn today? We learned that obviously open ODD or observability driven development is really awesome. You don't have to mock, again with the mocking. You're testing against real data, and you don't have any black boxes anymore. You know exactly what's happening in every single microservice. You can assert on every step of the transaction. And as the last recap, I mean, you wouldn't be here if you thought testing was fun or easy or something that you really enjoy doing. It is hard, like we all know it is very hard. Testing distributed systems is even harder. Testing microservices is even harder. So I want to help you elevate that TDD process that you're already doing. You're already doing well that you like to doing ODD as well. That's pretty much it. We're on point. If you have any questions, if you want to check out Trace Test, go just go to githubcubeshop slash trace test. You can download it. You can read a blog post I wrote about this as well. So knock yourselves out, I guess. You can also do, just to make it easier, you can do the, like you can also jump into Discord. You can chat with me or the engineers face-to-face. If you have any questions, if you want to try it out, check out the github. Also, give us a star, you know, because it's kind of why I'm here. I have to earn my salary something, in some way. So questions? Yeah, sure. So test run against the trace from the system. Yes, the way it happens is that, imagine you're running a postman request. That would be called, because this is trace test, that would be called response test. You get a response, you're testing on it. For trace test, you get the response, but you're also tapping into the trace data store and getting the traces that that request generates. So from that distributed trace, then you're running assertions based on the spans within that trace, if that answers your question. Yeah, for sure. The only thing is that, obviously, if you're running locally, you have a setup where your application is sending to either an open telemetry collector or whatever. You can also tap into that, where you configure trace test to be the pipeline endpoint of your open telemetry collector. So you can just run it as a dev tool as well. So also we might, I'm not sure if I'm good saying this on camera, but we might be building a desktop app very soon, because we're like half a year into this, so we're still kind of figuring out what you guys need. So that's why I'm here as well. But yeah, let's see what happens. It's a great question, by the way. It's good to finish early. We have time for questions. This is great. So yeah, the question was measuring SLOs for user journeys. That's actually something we're working on now. I'm not sure if you know about the captain project. So we have an integration with the captain project as of last week, quite literally. So if you want to check that out, you just jump into trace test integrates with captain and you'll get a lot of documentation and sample apps examples and whatnot to set that up as well. So that's an excellent use case and something that we actively have been working on. So 100% like the thing is that whatever you have implemented, if you have hotel traces coming in from that system, it works. So it's language agnostic, setup agnostic, it's literally like just the traces are important. So if you're running hotel, if you're running the data dog agent, elastic agent, literally anything that generates traces, it'll work. Obviously works best with hotel because open source, you know. But yeah, it just works. So if I understand the question correctly, it is, do I run synthetic tests with trace test? Yes. You can, if you have a CI pipeline or like you can have a cron job somewhere running, doesn't really matter. Every five minutes, I want to trigger this test and make sure that all of the assertions are true. That's perfectly fine. Oh, yeah, 100%. It works. You can think of it as testing in production and making sure that the production environment is healthy. That works as well. Hi. Yeah. So the test, trace test test depends on your instrumentation. Yes. Your instrumentation is in your production code. Yes. Do you have any advice on how do you prevent your production code from bloating with the instrumentation to beat these tests? Hmm. I'm going to say, that's a great question, but I think I'm not even close to being good enough of an engineer to answer that question, to be honest. 100%. 100%. 100%. Also, yeah. Go ahead. Yeah. Yeah, You really had to pick black boxes, but now the other hand, all right, writing tasks like this, and then spending might be unnecessarily tackled into the intricate details of the infrastructure right now. So is this necessarily good? So the text actually knows what's under, what's in that black box. So when you're in factory infrastructure, you might have to throw out all your tasks because you don't have the database that's a good question as well. I think the logical solution would be, trace test is just mapping out your infra. So if you're using it, you can also use it just to gain visibility. So it doesn't have to be that it's only focused on the testing. If you're using it to map out your infra, even if you have changes, if you're running the test again, you'll exactly know what changed. So if you're running assertions based on one database table, so to say, and then running an API on one endpoint that has one particular host name, if you change those up, you'll see what fails and you can figure out, oh, okay, so we changed that last week because of XYZ and you can know exactly what changed. So I think the overview, the visibility into your system, because when you're running microservices, when you're running a bunch of stuff, distributed systems, whatever, it's just hard to have a mental model, a mind map, so to say, of everything that's happening. So I think that's a good part of the value there as well. Thank you, no more time. No more time, yeah. Thank you. Thank you. No more time. No more time, yeah. Thank you. Thank you, no more time, yeah, no more time, yeah, no more time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.84, "text": " Perfect. Hello everyone. I might need to take a selfie because they're not going to believe", "tokens": [10246, 13, 2425, 1518, 13, 286, 1062, 643, 281, 747, 257, 22147, 570, 436, 434, 406, 516, 281, 1697], "temperature": 0.0, "avg_logprob": -0.31583582970403856, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.4020726680755615}, {"id": 1, "seek": 0, "start": 10.84, "end": 15.200000000000001, "text": " me when I get back. They go like, so people like testing, obviously, and they know, you", "tokens": [385, 562, 286, 483, 646, 13, 814, 352, 411, 11, 370, 561, 411, 4997, 11, 2745, 11, 293, 436, 458, 11, 291], "temperature": 0.0, "avg_logprob": -0.31583582970403856, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.4020726680755615}, {"id": 2, "seek": 0, "start": 15.200000000000001, "end": 22.32, "text": " know, sounds. Yeah, I should do a video because they're freaking not going to believe me. So", "tokens": [458, 11, 3263, 13, 865, 11, 286, 820, 360, 257, 960, 570, 436, 434, 14612, 406, 516, 281, 1697, 385, 13, 407], "temperature": 0.0, "avg_logprob": -0.31583582970403856, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.4020726680755615}, {"id": 3, "seek": 0, "start": 22.32, "end": 27.8, "text": " apparently people know what open telemetry is and what testing is. And yeah, I was not", "tokens": [7970, 561, 458, 437, 1269, 4304, 5537, 627, 307, 293, 437, 4997, 307, 13, 400, 1338, 11, 286, 390, 406], "temperature": 0.0, "avg_logprob": -0.31583582970403856, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.4020726680755615}, {"id": 4, "seek": 2780, "start": 27.8, "end": 33.52, "text": " expecting this to happen. So you're going to go out on Twitter. That's for sure. But yeah,", "tokens": [9650, 341, 281, 1051, 13, 407, 291, 434, 516, 281, 352, 484, 322, 5794, 13, 663, 311, 337, 988, 13, 583, 1338, 11], "temperature": 0.0, "avg_logprob": -0.1480294167995453, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.00010216250666417181}, {"id": 5, "seek": 2780, "start": 33.52, "end": 42.68, "text": " anyway, let's just take a second to welcome our new guests in. Perfect, perfect. Yeah,", "tokens": [4033, 11, 718, 311, 445, 747, 257, 1150, 281, 2928, 527, 777, 9804, 294, 13, 10246, 11, 2176, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.1480294167995453, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.00010216250666417181}, {"id": 6, "seek": 2780, "start": 42.68, "end": 50.24, "text": " this went from fun to stressful really quickly. But yeah, so let me begin. For the next 20 or", "tokens": [341, 1437, 490, 1019, 281, 19108, 534, 2661, 13, 583, 1338, 11, 370, 718, 385, 1841, 13, 1171, 264, 958, 945, 420], "temperature": 0.0, "avg_logprob": -0.1480294167995453, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.00010216250666417181}, {"id": 7, "seek": 2780, "start": 50.24, "end": 54.8, "text": " so minutes, I'll be talking about observability driven development with open telemetry. So a", "tokens": [370, 2077, 11, 286, 603, 312, 1417, 466, 9951, 2310, 9555, 3250, 365, 1269, 4304, 5537, 627, 13, 407, 257], "temperature": 0.0, "avg_logprob": -0.1480294167995453, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.00010216250666417181}, {"id": 8, "seek": 5480, "start": 54.8, "end": 59.0, "text": " lot of complicated words, a lot of stuff that's going to be happening. And a lot of things I", "tokens": [688, 295, 6179, 2283, 11, 257, 688, 295, 1507, 300, 311, 516, 281, 312, 2737, 13, 400, 257, 688, 295, 721, 286], "temperature": 0.0, "avg_logprob": -0.15274210051288756, "compression_ratio": 1.6413793103448275, "no_speech_prob": 8.734587754588574e-05}, {"id": 9, "seek": 5480, "start": 59.0, "end": 65.24, "text": " need to explain for you as testers and how you can get started with this new thing of being ODD", "tokens": [643, 281, 2903, 337, 291, 382, 1500, 433, 293, 577, 291, 393, 483, 1409, 365, 341, 777, 551, 295, 885, 422, 20818], "temperature": 0.0, "avg_logprob": -0.15274210051288756, "compression_ratio": 1.6413793103448275, "no_speech_prob": 8.734587754588574e-05}, {"id": 10, "seek": 5480, "start": 65.24, "end": 72.6, "text": " instead of TDD. So first, a quick rundown of who I am. I'm running DevRel at trace test, which is", "tokens": [2602, 295, 314, 20818, 13, 407, 700, 11, 257, 1702, 23096, 648, 295, 567, 286, 669, 13, 286, 478, 2614, 9096, 49029, 412, 13508, 1500, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.15274210051288756, "compression_ratio": 1.6413793103448275, "no_speech_prob": 8.734587754588574e-05}, {"id": 11, "seek": 5480, "start": 72.6, "end": 78.16, "text": " a, it's like a new tool, new open source tool that we're building for trace based testing. Obviously", "tokens": [257, 11, 309, 311, 411, 257, 777, 2290, 11, 777, 1269, 4009, 2290, 300, 321, 434, 2390, 337, 13508, 2361, 4997, 13, 7580], "temperature": 0.0, "avg_logprob": -0.15274210051288756, "compression_ratio": 1.6413793103448275, "no_speech_prob": 8.734587754588574e-05}, {"id": 12, "seek": 5480, "start": 78.16, "end": 84.6, "text": " explain all of that later on. But you're wondering like what am I DevRel person doing at", "tokens": [2903, 439, 295, 300, 1780, 322, 13, 583, 291, 434, 6359, 411, 437, 669, 286, 9096, 49029, 954, 884, 412], "temperature": 0.0, "avg_logprob": -0.15274210051288756, "compression_ratio": 1.6413793103448275, "no_speech_prob": 8.734587754588574e-05}, {"id": 13, "seek": 8460, "start": 84.6, "end": 91.28, "text": " a open source conference when it's kind of because I successfully failed a startup that was doing", "tokens": [257, 1269, 4009, 7586, 562, 309, 311, 733, 295, 570, 286, 10727, 7612, 257, 18578, 300, 390, 884], "temperature": 0.0, "avg_logprob": -0.127379874552577, "compression_ratio": 1.6816608996539792, "no_speech_prob": 4.466767859412357e-05}, {"id": 14, "seek": 8460, "start": 91.28, "end": 96.72, "text": " online education. So I was from there, went into education. And because we're basically educators", "tokens": [2950, 3309, 13, 407, 286, 390, 490, 456, 11, 1437, 666, 3309, 13, 400, 570, 321, 434, 1936, 22819], "temperature": 0.0, "avg_logprob": -0.127379874552577, "compression_ratio": 1.6816608996539792, "no_speech_prob": 4.466767859412357e-05}, {"id": 15, "seek": 8460, "start": 96.72, "end": 102.67999999999999, "text": " in DevRel, I was like, maybe, maybe, you know, I write shitty code, I can maybe be good at something", "tokens": [294, 9096, 49029, 11, 286, 390, 411, 11, 1310, 11, 1310, 11, 291, 458, 11, 286, 2464, 30748, 3089, 11, 286, 393, 1310, 312, 665, 412, 746], "temperature": 0.0, "avg_logprob": -0.127379874552577, "compression_ratio": 1.6816608996539792, "no_speech_prob": 4.466767859412357e-05}, {"id": 16, "seek": 8460, "start": 102.67999999999999, "end": 108.0, "text": " like talking. So I figured that might be a good career shift. But I've also been helping build", "tokens": [411, 1417, 13, 407, 286, 8932, 300, 1062, 312, 257, 665, 3988, 5513, 13, 583, 286, 600, 611, 668, 4315, 1322], "temperature": 0.0, "avg_logprob": -0.127379874552577, "compression_ratio": 1.6816608996539792, "no_speech_prob": 4.466767859412357e-05}, {"id": 17, "seek": 8460, "start": 108.0, "end": 112.8, "text": " open source DevTools for five or so years. So it's pretty natural for me to be here. So enough", "tokens": [1269, 4009, 9096, 51, 29298, 337, 1732, 420, 370, 924, 13, 407, 309, 311, 1238, 3303, 337, 385, 281, 312, 510, 13, 407, 1547], "temperature": 0.0, "avg_logprob": -0.127379874552577, "compression_ratio": 1.6816608996539792, "no_speech_prob": 4.466767859412357e-05}, {"id": 18, "seek": 11280, "start": 112.8, "end": 116.28, "text": " about that, you probably think that I know what I'm talking about. Let's keep it, keep it rolling.", "tokens": [466, 300, 11, 291, 1391, 519, 300, 286, 458, 437, 286, 478, 1417, 466, 13, 961, 311, 1066, 309, 11, 1066, 309, 9439, 13], "temperature": 0.0, "avg_logprob": -0.10306978595349216, "compression_ratio": 1.7392857142857143, "no_speech_prob": 1.3407921869657002e-05}, {"id": 19, "seek": 11280, "start": 116.28, "end": 122.12, "text": " There are four main topics. So remember these four topics that we will cover in the next 20 or so", "tokens": [821, 366, 1451, 2135, 8378, 13, 407, 1604, 613, 1451, 8378, 300, 321, 486, 2060, 294, 264, 958, 945, 420, 370], "temperature": 0.0, "avg_logprob": -0.10306978595349216, "compression_ratio": 1.7392857142857143, "no_speech_prob": 1.3407921869657002e-05}, {"id": 20, "seek": 11280, "start": 122.12, "end": 127.44, "text": " minutes. And that's, first, we'll talk about the pain of testing microservices. It's a horrible,", "tokens": [2077, 13, 400, 300, 311, 11, 700, 11, 321, 603, 751, 466, 264, 1822, 295, 4997, 15547, 47480, 13, 467, 311, 257, 9263, 11], "temperature": 0.0, "avg_logprob": -0.10306978595349216, "compression_ratio": 1.7392857142857143, "no_speech_prob": 1.3407921869657002e-05}, {"id": 21, "seek": 11280, "start": 127.44, "end": 133.92, "text": " horrible thing. And we'll also talk about TDD and how integration testing is really hard. We're all", "tokens": [9263, 551, 13, 400, 321, 603, 611, 751, 466, 314, 20818, 293, 577, 10980, 4997, 307, 534, 1152, 13, 492, 434, 439], "temperature": 0.0, "avg_logprob": -0.10306978595349216, "compression_ratio": 1.7392857142857143, "no_speech_prob": 1.3407921869657002e-05}, {"id": 22, "seek": 11280, "start": 133.92, "end": 139.4, "text": " doing it. It's terrible. It's hard. But we're still doing it. And then in the last two parts,", "tokens": [884, 309, 13, 467, 311, 6237, 13, 467, 311, 1152, 13, 583, 321, 434, 920, 884, 309, 13, 400, 550, 294, 264, 1036, 732, 3166, 11], "temperature": 0.0, "avg_logprob": -0.10306978595349216, "compression_ratio": 1.7392857142857143, "no_speech_prob": 1.3407921869657002e-05}, {"id": 23, "seek": 13940, "start": 139.4, "end": 144.32, "text": " we'll talk about observability-driven development, how it can help. And then we'll show a code", "tokens": [321, 603, 751, 466, 9951, 2310, 12, 25456, 3250, 11, 577, 309, 393, 854, 13, 400, 550, 321, 603, 855, 257, 3089], "temperature": 0.0, "avg_logprob": -0.08955576419830322, "compression_ratio": 1.6288659793814433, "no_speech_prob": 4.535541302175261e-05}, {"id": 24, "seek": 13940, "start": 144.32, "end": 148.12, "text": " example, a hands-on example of how you can do it as well. So I want you to take something home", "tokens": [1365, 11, 257, 2377, 12, 266, 1365, 295, 577, 291, 393, 360, 309, 382, 731, 13, 407, 286, 528, 291, 281, 747, 746, 1280], "temperature": 0.0, "avg_logprob": -0.08955576419830322, "compression_ratio": 1.6288659793814433, "no_speech_prob": 4.535541302175261e-05}, {"id": 25, "seek": 13940, "start": 148.12, "end": 153.68, "text": " with you after this 20-minute talk and actually start doing it yourself. So from the beginning,", "tokens": [365, 291, 934, 341, 945, 12, 18256, 751, 293, 767, 722, 884, 309, 1803, 13, 407, 490, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.08955576419830322, "compression_ratio": 1.6288659793814433, "no_speech_prob": 4.535541302175261e-05}, {"id": 26, "seek": 13940, "start": 153.68, "end": 159.08, "text": " from the top down, let's talk about the pain of testing microservices. So first, the biggest", "tokens": [490, 264, 1192, 760, 11, 718, 311, 751, 466, 264, 1822, 295, 4997, 15547, 47480, 13, 407, 700, 11, 264, 3880], "temperature": 0.0, "avg_logprob": -0.08955576419830322, "compression_ratio": 1.6288659793814433, "no_speech_prob": 4.535541302175261e-05}, {"id": 27, "seek": 13940, "start": 159.08, "end": 165.56, "text": " issue is that you have no way of knowing where your HTTP transaction fails. You don't know. You", "tokens": [2734, 307, 300, 291, 362, 572, 636, 295, 5276, 689, 428, 33283, 14425, 18199, 13, 509, 500, 380, 458, 13, 509], "temperature": 0.0, "avg_logprob": -0.08955576419830322, "compression_ratio": 1.6288659793814433, "no_speech_prob": 4.535541302175261e-05}, {"id": 28, "seek": 16556, "start": 165.56, "end": 172.16, "text": " can test an API endpoint. You get a response back. But it might be task failed successfully. You", "tokens": [393, 1500, 364, 9362, 35795, 13, 509, 483, 257, 4134, 646, 13, 583, 309, 1062, 312, 5633, 7612, 10727, 13, 509], "temperature": 0.0, "avg_logprob": -0.151274927546469, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.175440931750927e-05}, {"id": 29, "seek": 16556, "start": 172.16, "end": 178.84, "text": " never really know if you have a row of microservices behind that initial service. So that's", "tokens": [1128, 534, 458, 498, 291, 362, 257, 5386, 295, 15547, 47480, 2261, 300, 5883, 2643, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.151274927546469, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.175440931750927e-05}, {"id": 30, "seek": 16556, "start": 178.84, "end": 183.88, "text": " something you can track. You can track and test how these microservices to microservice", "tokens": [746, 291, 393, 2837, 13, 509, 393, 2837, 293, 1500, 577, 613, 15547, 47480, 281, 15547, 25006], "temperature": 0.0, "avg_logprob": -0.151274927546469, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.175440931750927e-05}, {"id": 31, "seek": 16556, "start": 183.88, "end": 190.72, "text": " communications happen. And of course, the hardest thing, what we all really love to hate, is mocking.", "tokens": [15163, 1051, 13, 400, 295, 1164, 11, 264, 13158, 551, 11, 437, 321, 439, 534, 959, 281, 4700, 11, 307, 49792, 13], "temperature": 0.0, "avg_logprob": -0.151274927546469, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.175440931750927e-05}, {"id": 32, "seek": 19072, "start": 190.72, "end": 197.8, "text": " It's really hard. It's really, really hard. So the solution that we propose is that we go into", "tokens": [467, 311, 534, 1152, 13, 467, 311, 534, 11, 534, 1152, 13, 407, 264, 3827, 300, 321, 17421, 307, 300, 321, 352, 666], "temperature": 0.0, "avg_logprob": -0.11845671709846048, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.8384896722855046e-05}, {"id": 33, "seek": 19072, "start": 197.8, "end": 202.44, "text": " doing something called observability-driven development, which means that you're using", "tokens": [884, 746, 1219, 9951, 2310, 12, 25456, 3250, 11, 597, 1355, 300, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.11845671709846048, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.8384896722855046e-05}, {"id": 34, "seek": 19072, "start": 202.44, "end": 208.96, "text": " distributed traces as the test assertions. So you're already using your underlying trace", "tokens": [12631, 26076, 382, 264, 1500, 19810, 626, 13, 407, 291, 434, 1217, 1228, 428, 14217, 13508], "temperature": 0.0, "avg_logprob": -0.11845671709846048, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.8384896722855046e-05}, {"id": 35, "seek": 19072, "start": 208.96, "end": 214.72, "text": " infrastructure to run your tests. And now, because this is a testing dev room, you might not know", "tokens": [6896, 281, 1190, 428, 6921, 13, 400, 586, 11, 570, 341, 307, 257, 4997, 1905, 1808, 11, 291, 1062, 406, 458], "temperature": 0.0, "avg_logprob": -0.11845671709846048, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.8384896722855046e-05}, {"id": 36, "seek": 21472, "start": 214.72, "end": 220.92, "text": " what tracing is, LightStep has a very nice definition of it. And they say that distributed", "tokens": [437, 25262, 307, 11, 8279, 23624, 575, 257, 588, 1481, 7123, 295, 309, 13, 400, 436, 584, 300, 12631], "temperature": 0.0, "avg_logprob": -0.12404251098632812, "compression_ratio": 1.7757009345794392, "no_speech_prob": 2.5456418370595202e-05}, {"id": 37, "seek": 21472, "start": 220.92, "end": 226.12, "text": " tracing refers to methods of observing requests as they propagate through a distributed system,", "tokens": [25262, 14942, 281, 7150, 295, 22107, 12475, 382, 436, 48256, 807, 257, 12631, 1185, 11], "temperature": 0.0, "avg_logprob": -0.12404251098632812, "compression_ratio": 1.7757009345794392, "no_speech_prob": 2.5456418370595202e-05}, {"id": 38, "seek": 21472, "start": 226.12, "end": 233.2, "text": " which means that if you have a distributed system on the left, you have services that communicate", "tokens": [597, 1355, 300, 498, 291, 362, 257, 12631, 1185, 322, 264, 1411, 11, 291, 362, 3328, 300, 7890], "temperature": 0.0, "avg_logprob": -0.12404251098632812, "compression_ratio": 1.7757009345794392, "no_speech_prob": 2.5456418370595202e-05}, {"id": 39, "seek": 21472, "start": 233.2, "end": 239.4, "text": " with each other. And on the right, you can see that that entire distributed trace is split into", "tokens": [365, 1184, 661, 13, 400, 322, 264, 558, 11, 291, 393, 536, 300, 300, 2302, 12631, 13508, 307, 7472, 666], "temperature": 0.0, "avg_logprob": -0.12404251098632812, "compression_ratio": 1.7757009345794392, "no_speech_prob": 2.5456418370595202e-05}, {"id": 40, "seek": 23940, "start": 239.4, "end": 247.08, "text": " different spans. A span is the smallest unit of a distributed test. So a span can be, it can be a", "tokens": [819, 44086, 13, 316, 16174, 307, 264, 16998, 4985, 295, 257, 12631, 1500, 13, 407, 257, 16174, 393, 312, 11, 309, 393, 312, 257], "temperature": 0.0, "avg_logprob": -0.12144038942125109, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.3628025044454262e-05}, {"id": 41, "seek": 23940, "start": 247.08, "end": 252.6, "text": " type of stamp. It can be a database interaction or database statement. It can be HTTP codes. It", "tokens": [2010, 295, 9921, 13, 467, 393, 312, 257, 8149, 9285, 420, 8149, 5629, 13, 467, 393, 312, 33283, 14211, 13, 467], "temperature": 0.0, "avg_logprob": -0.12144038942125109, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.3628025044454262e-05}, {"id": 42, "seek": 23940, "start": 252.6, "end": 258.72, "text": " can be objects that you generate in your custom instrumentation itself. So they're literally", "tokens": [393, 312, 6565, 300, 291, 8460, 294, 428, 2375, 7198, 399, 2564, 13, 407, 436, 434, 3736], "temperature": 0.0, "avg_logprob": -0.12144038942125109, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.3628025044454262e-05}, {"id": 43, "seek": 23940, "start": 258.72, "end": 266.12, "text": " the smallest form or part of a distributed trace. The distributed system we'll be talking about", "tokens": [264, 16998, 1254, 420, 644, 295, 257, 12631, 13508, 13, 440, 12631, 1185, 321, 603, 312, 1417, 466], "temperature": 0.0, "avg_logprob": -0.12144038942125109, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.3628025044454262e-05}, {"id": 44, "seek": 26612, "start": 266.12, "end": 271.2, "text": " today, so the samples we will be talking about is very simple. We have two services with a mock", "tokens": [965, 11, 370, 264, 10938, 321, 486, 312, 1417, 466, 307, 588, 2199, 13, 492, 362, 732, 3328, 365, 257, 17362], "temperature": 0.0, "avg_logprob": -0.11233265335495407, "compression_ratio": 1.6094276094276094, "no_speech_prob": 1.568591505929362e-05}, {"id": 45, "seek": 26612, "start": 271.2, "end": 276.72, "text": " database connection. Just to simplify this whole architecture, we will be using this to explain", "tokens": [8149, 4984, 13, 1449, 281, 20460, 341, 1379, 9482, 11, 321, 486, 312, 1228, 341, 281, 2903], "temperature": 0.0, "avg_logprob": -0.11233265335495407, "compression_ratio": 1.6094276094276094, "no_speech_prob": 1.568591505929362e-05}, {"id": 46, "seek": 26612, "start": 276.72, "end": 281.36, "text": " how distributed tracing works and how you run observability-driven development on such a system.", "tokens": [577, 12631, 25262, 1985, 293, 577, 291, 1190, 9951, 2310, 12, 25456, 3250, 322, 1270, 257, 1185, 13], "temperature": 0.0, "avg_logprob": -0.11233265335495407, "compression_ratio": 1.6094276094276094, "no_speech_prob": 1.568591505929362e-05}, {"id": 47, "seek": 26612, "start": 281.36, "end": 288.76, "text": " Now, just a code sample, because this is JavaScript, the only language I really know, not that well,", "tokens": [823, 11, 445, 257, 3089, 6889, 11, 570, 341, 307, 15778, 11, 264, 787, 2856, 286, 534, 458, 11, 406, 300, 731, 11], "temperature": 0.0, "avg_logprob": -0.11233265335495407, "compression_ratio": 1.6094276094276094, "no_speech_prob": 1.568591505929362e-05}, {"id": 48, "seek": 26612, "start": 288.76, "end": 293.96, "text": " this is what a trace would look like. You're setting the span, you're adding attributes,", "tokens": [341, 307, 437, 257, 13508, 576, 574, 411, 13, 509, 434, 3287, 264, 16174, 11, 291, 434, 5127, 17212, 11], "temperature": 0.0, "avg_logprob": -0.11233265335495407, "compression_ratio": 1.6094276094276094, "no_speech_prob": 1.568591505929362e-05}, {"id": 49, "seek": 29396, "start": 293.96, "end": 300.91999999999996, "text": " and then you're ending the span. So this is the code representation of what we have over here.", "tokens": [293, 550, 291, 434, 8121, 264, 16174, 13, 407, 341, 307, 264, 3089, 10290, 295, 437, 321, 362, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.13262387503564885, "compression_ratio": 1.7613636363636365, "no_speech_prob": 1.2802532182831783e-05}, {"id": 50, "seek": 29396, "start": 300.91999999999996, "end": 308.03999999999996, "text": " So just remember that for now, and we'll get into more details as we progress. So the visual", "tokens": [407, 445, 1604, 300, 337, 586, 11, 293, 321, 603, 483, 666, 544, 4365, 382, 321, 4205, 13, 407, 264, 5056], "temperature": 0.0, "avg_logprob": -0.13262387503564885, "compression_ratio": 1.7613636363636365, "no_speech_prob": 1.2802532182831783e-05}, {"id": 51, "seek": 29396, "start": 308.03999999999996, "end": 312.08, "text": " queue or the visual layout of a distributed span would look like this. This is taken from the", "tokens": [18639, 420, 264, 5056, 13333, 295, 257, 12631, 16174, 576, 574, 411, 341, 13, 639, 307, 2726, 490, 264], "temperature": 0.0, "avg_logprob": -0.13262387503564885, "compression_ratio": 1.7613636363636365, "no_speech_prob": 1.2802532182831783e-05}, {"id": 52, "seek": 29396, "start": 312.08, "end": 317.0, "text": " trace test app, but this is any, like any distributed span looks like this, where you have", "tokens": [13508, 1500, 724, 11, 457, 341, 307, 604, 11, 411, 604, 12631, 16174, 1542, 411, 341, 11, 689, 291, 362], "temperature": 0.0, "avg_logprob": -0.13262387503564885, "compression_ratio": 1.7613636363636365, "no_speech_prob": 1.2802532182831783e-05}, {"id": 53, "seek": 29396, "start": 317.0, "end": 322.59999999999997, "text": " your distributed trace and you can see all of the spans within it. And if you drill into one", "tokens": [428, 12631, 13508, 293, 291, 393, 536, 439, 295, 264, 44086, 1951, 309, 13, 400, 498, 291, 11392, 666, 472], "temperature": 0.0, "avg_logprob": -0.13262387503564885, "compression_ratio": 1.7613636363636365, "no_speech_prob": 1.2802532182831783e-05}, {"id": 54, "seek": 32260, "start": 322.6, "end": 329.04, "text": " particular span, you can see, okay, so here are all of the attributes that this span has. It can", "tokens": [1729, 16174, 11, 291, 393, 536, 11, 1392, 11, 370, 510, 366, 439, 295, 264, 17212, 300, 341, 16174, 575, 13, 467, 393], "temperature": 0.0, "avg_logprob": -0.09176408607541149, "compression_ratio": 1.6723549488054608, "no_speech_prob": 1.0448662578710355e-05}, {"id": 55, "seek": 32260, "start": 329.04, "end": 334.8, "text": " be available, book ID, the check, and the parent ID. There's a lot of different attributes you have", "tokens": [312, 2435, 11, 1446, 7348, 11, 264, 1520, 11, 293, 264, 2596, 7348, 13, 821, 311, 257, 688, 295, 819, 17212, 291, 362], "temperature": 0.0, "avg_logprob": -0.09176408607541149, "compression_ratio": 1.6723549488054608, "no_speech_prob": 1.0448662578710355e-05}, {"id": 56, "seek": 32260, "start": 334.8, "end": 340.08000000000004, "text": " in every span. So the next topic I do want to cover, we have all the basics down. We know why", "tokens": [294, 633, 16174, 13, 407, 264, 958, 4829, 286, 360, 528, 281, 2060, 11, 321, 362, 439, 264, 14688, 760, 13, 492, 458, 983], "temperature": 0.0, "avg_logprob": -0.09176408607541149, "compression_ratio": 1.6723549488054608, "no_speech_prob": 1.0448662578710355e-05}, {"id": 57, "seek": 32260, "start": 340.08000000000004, "end": 346.88, "text": " it's hard. We need to figure out why integration testing and TDD really need help. Everybody knows", "tokens": [309, 311, 1152, 13, 492, 643, 281, 2573, 484, 983, 10980, 4997, 293, 314, 20818, 534, 643, 854, 13, 7646, 3255], "temperature": 0.0, "avg_logprob": -0.09176408607541149, "compression_ratio": 1.6723549488054608, "no_speech_prob": 1.0448662578710355e-05}, {"id": 58, "seek": 32260, "start": 346.88, "end": 352.0, "text": " about the red-green feedback loop. It's awesome. It's great. We like it. We don't need to change it.", "tokens": [466, 264, 2182, 12, 27399, 5824, 6367, 13, 467, 311, 3476, 13, 467, 311, 869, 13, 492, 411, 309, 13, 492, 500, 380, 643, 281, 1319, 309, 13], "temperature": 0.0, "avg_logprob": -0.09176408607541149, "compression_ratio": 1.6723549488054608, "no_speech_prob": 1.0448662578710355e-05}, {"id": 59, "seek": 35200, "start": 352.0, "end": 360.0, "text": " But integration tests are hard. Integration tests are the kicker, where they need access to services", "tokens": [583, 10980, 6921, 366, 1152, 13, 47713, 6921, 366, 264, 4437, 260, 11, 689, 436, 643, 2105, 281, 3328], "temperature": 0.0, "avg_logprob": -0.11404724288405034, "compression_ratio": 1.8050541516245486, "no_speech_prob": 4.19755197071936e-05}, {"id": 60, "seek": 35200, "start": 360.0, "end": 364.16, "text": " and infrastructure. That's the hard part. You need to set up different triggers. You need to access", "tokens": [293, 6896, 13, 663, 311, 264, 1152, 644, 13, 509, 643, 281, 992, 493, 819, 22827, 13, 509, 643, 281, 2105], "temperature": 0.0, "avg_logprob": -0.11404724288405034, "compression_ratio": 1.8050541516245486, "no_speech_prob": 4.19755197071936e-05}, {"id": 61, "seek": 35200, "start": 364.16, "end": 367.96, "text": " databases. You need to set up environment variables. You need to set up authentication. All of those", "tokens": [22380, 13, 509, 643, 281, 992, 493, 2823, 9102, 13, 509, 643, 281, 992, 493, 26643, 13, 1057, 295, 729], "temperature": 0.0, "avg_logprob": -0.11404724288405034, "compression_ratio": 1.8050541516245486, "no_speech_prob": 4.19755197071936e-05}, {"id": 62, "seek": 35200, "start": 367.96, "end": 374.0, "text": " things that everybody hates doing. And of course, you can track which part of the microservice chain", "tokens": [721, 300, 2201, 23000, 884, 13, 400, 295, 1164, 11, 291, 393, 2837, 597, 644, 295, 264, 15547, 25006, 5021], "temperature": 0.0, "avg_logprob": -0.11404724288405034, "compression_ratio": 1.8050541516245486, "no_speech_prob": 4.19755197071936e-05}, {"id": 63, "seek": 35200, "start": 374.0, "end": 379.72, "text": " failed, which means that you're writing 90% of your code just as plumbing, just to make sure that", "tokens": [7612, 11, 597, 1355, 300, 291, 434, 3579, 4289, 4, 295, 428, 3089, 445, 382, 39993, 11, 445, 281, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.11404724288405034, "compression_ratio": 1.8050541516245486, "no_speech_prob": 4.19755197071936e-05}, {"id": 64, "seek": 37972, "start": 379.72, "end": 386.56, "text": " the test will run. 10% is actually writing the test, writing the test case, and actually getting value", "tokens": [264, 1500, 486, 1190, 13, 1266, 4, 307, 767, 3579, 264, 1500, 11, 3579, 264, 1500, 1389, 11, 293, 767, 1242, 2158], "temperature": 0.0, "avg_logprob": -0.10677593549092611, "compression_ratio": 1.8401486988847584, "no_speech_prob": 7.719893619650975e-05}, {"id": 65, "seek": 37972, "start": 386.56, "end": 392.28000000000003, "text": " from your TDD process. So here's what a traditional integration test would look like. You have all", "tokens": [490, 428, 314, 20818, 1399, 13, 407, 510, 311, 437, 257, 5164, 10980, 1500, 576, 574, 411, 13, 509, 362, 439], "temperature": 0.0, "avg_logprob": -0.10677593549092611, "compression_ratio": 1.8401486988847584, "no_speech_prob": 7.719893619650975e-05}, {"id": 66, "seek": 37972, "start": 392.28000000000003, "end": 397.40000000000003, "text": " of your setup. Again, this is JavaScript. It can be any language. You have your setup, a bunch of", "tokens": [295, 428, 8657, 13, 3764, 11, 341, 307, 15778, 13, 467, 393, 312, 604, 2856, 13, 509, 362, 428, 8657, 11, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.10677593549092611, "compression_ratio": 1.8401486988847584, "no_speech_prob": 7.719893619650975e-05}, {"id": 67, "seek": 37972, "start": 397.40000000000003, "end": 401.04, "text": " modules, a bunch of setup, a bunch of plumbing. And then you have more plumbing because you need to", "tokens": [16679, 11, 257, 3840, 295, 8657, 11, 257, 3840, 295, 39993, 13, 400, 550, 291, 362, 544, 39993, 570, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.10677593549092611, "compression_ratio": 1.8401486988847584, "no_speech_prob": 7.719893619650975e-05}, {"id": 68, "seek": 37972, "start": 401.04, "end": 404.8, "text": " mock something. Then you have even more plumbing because you have to figure out how to run this", "tokens": [17362, 746, 13, 1396, 291, 362, 754, 544, 39993, 570, 291, 362, 281, 2573, 484, 577, 281, 1190, 341], "temperature": 0.0, "avg_logprob": -0.10677593549092611, "compression_ratio": 1.8401486988847584, "no_speech_prob": 7.719893619650975e-05}, {"id": 69, "seek": 40480, "start": 404.8, "end": 410.32, "text": " custom freaking syntax that has nothing to do with any language, really. You just have to learn it. So", "tokens": [2375, 14612, 28431, 300, 575, 1825, 281, 360, 365, 604, 2856, 11, 534, 13, 509, 445, 362, 281, 1466, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.1354051356045705, "compression_ratio": 1.6048387096774193, "no_speech_prob": 9.457808482693508e-05}, {"id": 70, "seek": 40480, "start": 410.32, "end": 415.72, "text": " it's a lot of stuff you have to know before you actually run tests. If you compare that to a trace", "tokens": [309, 311, 257, 688, 295, 1507, 291, 362, 281, 458, 949, 291, 767, 1190, 6921, 13, 759, 291, 6794, 300, 281, 257, 13508], "temperature": 0.0, "avg_logprob": -0.1354051356045705, "compression_ratio": 1.6048387096774193, "no_speech_prob": 9.457808482693508e-05}, {"id": 71, "seek": 40480, "start": 415.72, "end": 427.72, "text": " based test, you say, here's my URL. Here's my method. This is what I'm suring against. That's it. No", "tokens": [2361, 1500, 11, 291, 584, 11, 510, 311, 452, 12905, 13, 1692, 311, 452, 3170, 13, 639, 307, 437, 286, 478, 1022, 278, 1970, 13, 663, 311, 309, 13, 883], "temperature": 0.0, "avg_logprob": -0.1354051356045705, "compression_ratio": 1.6048387096774193, "no_speech_prob": 9.457808482693508e-05}, {"id": 72, "seek": 40480, "start": 427.72, "end": 432.56, "text": " complications, no plumbing, no nothing. It just points to the trace, the trace span you want to", "tokens": [26566, 11, 572, 39993, 11, 572, 1825, 13, 467, 445, 2793, 281, 264, 13508, 11, 264, 13508, 16174, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.1354051356045705, "compression_ratio": 1.6048387096774193, "no_speech_prob": 9.457808482693508e-05}, {"id": 73, "seek": 43256, "start": 432.56, "end": 440.16, "text": " target. You have your assertion and it's done. So this is why I think observability driven can help", "tokens": [3779, 13, 509, 362, 428, 19810, 313, 293, 309, 311, 1096, 13, 407, 341, 307, 983, 286, 519, 9951, 2310, 9555, 393, 854], "temperature": 0.0, "avg_logprob": -0.11866758287567453, "compression_ratio": 1.6398305084745763, "no_speech_prob": 1.8923161405837163e-05}, {"id": 74, "seek": 43256, "start": 440.16, "end": 447.2, "text": " our testing process, where obviously we need to explain what ODD is. The main thing that I think is", "tokens": [527, 4997, 1399, 11, 689, 2745, 321, 643, 281, 2903, 437, 422, 20818, 307, 13, 440, 2135, 551, 300, 286, 519, 307], "temperature": 0.0, "avg_logprob": -0.11866758287567453, "compression_ratio": 1.6398305084745763, "no_speech_prob": 1.8923161405837163e-05}, {"id": 75, "seek": 43256, "start": 447.2, "end": 451.88, "text": " important to know is that you need to write your code and your observability instrumentation in", "tokens": [1021, 281, 458, 307, 300, 291, 643, 281, 2464, 428, 3089, 293, 428, 9951, 2310, 7198, 399, 294], "temperature": 0.0, "avg_logprob": -0.11866758287567453, "compression_ratio": 1.6398305084745763, "no_speech_prob": 1.8923161405837163e-05}, {"id": 76, "seek": 43256, "start": 451.88, "end": 458.48, "text": " parallel. So the same way you do the red-green process for TDD, in ODD you write your trace", "tokens": [8952, 13, 407, 264, 912, 636, 291, 360, 264, 2182, 12, 27399, 1399, 337, 314, 20818, 11, 294, 422, 20818, 291, 2464, 428, 13508], "temperature": 0.0, "avg_logprob": -0.11866758287567453, "compression_ratio": 1.6398305084745763, "no_speech_prob": 1.8923161405837163e-05}, {"id": 77, "seek": 45848, "start": 458.48, "end": 464.16, "text": " spans and you write your code and your features in parallel. Which is good, first thing because in", "tokens": [44086, 293, 291, 2464, 428, 3089, 293, 428, 4122, 294, 8952, 13, 3013, 307, 665, 11, 700, 551, 570, 294], "temperature": 0.0, "avg_logprob": -0.12819698507135566, "compression_ratio": 1.696113074204947, "no_speech_prob": 6.105073407525197e-05}, {"id": 78, "seek": 45848, "start": 464.16, "end": 469.28000000000003, "text": " production that helps your DevOps people when they have troubleshoot, but it's also helping you", "tokens": [4265, 300, 3665, 428, 43051, 561, 562, 436, 362, 15379, 24467, 11, 457, 309, 311, 611, 4315, 291], "temperature": 0.0, "avg_logprob": -0.12819698507135566, "compression_ratio": 1.696113074204947, "no_speech_prob": 6.105073407525197e-05}, {"id": 79, "seek": 45848, "start": 469.28000000000003, "end": 474.04, "text": " write better code. And ODD is really powerful because first and foremost, of course, you're not", "tokens": [2464, 1101, 3089, 13, 400, 422, 20818, 307, 534, 4005, 570, 700, 293, 18864, 11, 295, 1164, 11, 291, 434, 406], "temperature": 0.0, "avg_logprob": -0.12819698507135566, "compression_ratio": 1.696113074204947, "no_speech_prob": 6.105073407525197e-05}, {"id": 80, "seek": 45848, "start": 474.04, "end": 480.52000000000004, "text": " testing mocks. Nothing is artificial. You're not creating black boxes. You're literally testing", "tokens": [4997, 705, 2761, 13, 6693, 307, 11677, 13, 509, 434, 406, 4084, 2211, 9002, 13, 509, 434, 3736, 4997], "temperature": 0.0, "avg_logprob": -0.12819698507135566, "compression_ratio": 1.696113074204947, "no_speech_prob": 6.105073407525197e-05}, {"id": 81, "seek": 45848, "start": 480.52000000000004, "end": 485.84000000000003, "text": " data from the traces in the real environment. So you can spin up your system, get traces from", "tokens": [1412, 490, 264, 26076, 294, 264, 957, 2823, 13, 407, 291, 393, 6060, 493, 428, 1185, 11, 483, 26076, 490], "temperature": 0.0, "avg_logprob": -0.12819698507135566, "compression_ratio": 1.696113074204947, "no_speech_prob": 6.105073407525197e-05}, {"id": 82, "seek": 48584, "start": 485.84, "end": 491.4, "text": " the system and test on those traces. Of course, it works with all of your existing open telemetry", "tokens": [264, 1185, 293, 1500, 322, 729, 26076, 13, 2720, 1164, 11, 309, 1985, 365, 439, 295, 428, 6741, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.12070535620053609, "compression_ratio": 1.6276150627615062, "no_speech_prob": 5.222622348810546e-05}, {"id": 83, "seek": 48584, "start": 491.4, "end": 497.44, "text": " based distributed tracing. So if you have tracing enabled or if you want to enable it, it's really", "tokens": [2361, 12631, 25262, 13, 407, 498, 291, 362, 25262, 15172, 420, 498, 291, 528, 281, 9528, 309, 11, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.12070535620053609, "compression_ratio": 1.6276150627615062, "no_speech_prob": 5.222622348810546e-05}, {"id": 84, "seek": 48584, "start": 497.44, "end": 504.28, "text": " simple nowadays, it'll just work. And then from the ODD definition, we need to figure out what", "tokens": [2199, 13434, 11, 309, 603, 445, 589, 13, 400, 550, 490, 264, 422, 20818, 7123, 11, 321, 643, 281, 2573, 484, 437], "temperature": 0.0, "avg_logprob": -0.12070535620053609, "compression_ratio": 1.6276150627615062, "no_speech_prob": 5.222622348810546e-05}, {"id": 85, "seek": 48584, "start": 504.28, "end": 511.15999999999997, "text": " trace-based testing is here. So you basically add assertions against span values. And that's what", "tokens": [13508, 12, 6032, 4997, 307, 510, 13, 407, 291, 1936, 909, 19810, 626, 1970, 16174, 4190, 13, 400, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.12070535620053609, "compression_ratio": 1.6276150627615062, "no_speech_prob": 5.222622348810546e-05}, {"id": 86, "seek": 51116, "start": 511.16, "end": 517.0, "text": " determines whether the test has failed or the test has passed. It's really straightforward. So you're", "tokens": [24799, 1968, 264, 1500, 575, 7612, 420, 264, 1500, 575, 4678, 13, 467, 311, 534, 15325, 13, 407, 291, 434], "temperature": 0.0, "avg_logprob": -0.1162808182042673, "compression_ratio": 1.881679389312977, "no_speech_prob": 6.70431909384206e-05}, {"id": 87, "seek": 51116, "start": 517.0, "end": 521.96, "text": " not just testing against the API response, you're actually testing against the whole distributed", "tokens": [406, 445, 4997, 1970, 264, 9362, 4134, 11, 291, 434, 767, 4997, 1970, 264, 1379, 12631], "temperature": 0.0, "avg_logprob": -0.1162808182042673, "compression_ratio": 1.881679389312977, "no_speech_prob": 6.70431909384206e-05}, {"id": 88, "seek": 51116, "start": 521.96, "end": 527.24, "text": " trace your system generates. So unlike postman where you trigger a test, you get something back,", "tokens": [13508, 428, 1185, 23815, 13, 407, 8343, 2183, 1601, 689, 291, 7875, 257, 1500, 11, 291, 483, 746, 646, 11], "temperature": 0.0, "avg_logprob": -0.1162808182042673, "compression_ratio": 1.881679389312977, "no_speech_prob": 6.70431909384206e-05}, {"id": 89, "seek": 51116, "start": 527.24, "end": 532.88, "text": " and then you're asserting on that response, you're literally testing and running assertions against", "tokens": [293, 550, 291, 434, 1256, 27187, 322, 300, 4134, 11, 291, 434, 3736, 4997, 293, 2614, 19810, 626, 1970], "temperature": 0.0, "avg_logprob": -0.1162808182042673, "compression_ratio": 1.881679389312977, "no_speech_prob": 6.70431909384206e-05}, {"id": 90, "seek": 51116, "start": 532.88, "end": 539.0400000000001, "text": " the entire distributed trace. Really, really cool. Now let's go into some practice. How do you do", "tokens": [264, 2302, 12631, 13508, 13, 4083, 11, 534, 1627, 13, 823, 718, 311, 352, 666, 512, 3124, 13, 1012, 360, 291, 360], "temperature": 0.0, "avg_logprob": -0.1162808182042673, "compression_ratio": 1.881679389312977, "no_speech_prob": 6.70431909384206e-05}, {"id": 91, "seek": 53904, "start": 539.04, "end": 542.8, "text": " observability during development? Well, you do trace tests because that's the open source tool", "tokens": [9951, 2310, 1830, 3250, 30, 1042, 11, 291, 360, 13508, 6921, 570, 300, 311, 264, 1269, 4009, 2290], "temperature": 0.0, "avg_logprob": -0.12584254272982606, "compression_ratio": 1.6971830985915493, "no_speech_prob": 2.3919445084175095e-05}, {"id": 92, "seek": 53904, "start": 542.8, "end": 548.56, "text": " we're building, you know, shocker. But what's important about trace test is fully open source,", "tokens": [321, 434, 2390, 11, 291, 458, 11, 5588, 260, 13, 583, 437, 311, 1021, 466, 13508, 1500, 307, 4498, 1269, 4009, 11], "temperature": 0.0, "avg_logprob": -0.12584254272982606, "compression_ratio": 1.6971830985915493, "no_speech_prob": 2.3919445084175095e-05}, {"id": 93, "seek": 53904, "start": 548.56, "end": 555.5999999999999, "text": " 100% open source, CNCF project, and it uses open telemetry trace bands as assertions. Very", "tokens": [2319, 4, 1269, 4009, 11, 48714, 37, 1716, 11, 293, 309, 4960, 1269, 4304, 5537, 627, 13508, 13543, 382, 19810, 626, 13, 4372], "temperature": 0.0, "avg_logprob": -0.12584254272982606, "compression_ratio": 1.6971830985915493, "no_speech_prob": 2.3919445084175095e-05}, {"id": 94, "seek": 53904, "start": 555.5999999999999, "end": 561.8399999999999, "text": " straightforward. Of course, it does work with any existing tracing solution you might have. You can", "tokens": [15325, 13, 2720, 1164, 11, 309, 775, 589, 365, 604, 6741, 25262, 3827, 291, 1062, 362, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.12584254272982606, "compression_ratio": 1.6971830985915493, "no_speech_prob": 2.3919445084175095e-05}, {"id": 95, "seek": 53904, "start": 561.8399999999999, "end": 565.3199999999999, "text": " use vendors, you can use open source tools, you can use whatever. If you have tracing in your system,", "tokens": [764, 22056, 11, 291, 393, 764, 1269, 4009, 3873, 11, 291, 393, 764, 2035, 13, 759, 291, 362, 25262, 294, 428, 1185, 11], "temperature": 0.0, "avg_logprob": -0.12584254272982606, "compression_ratio": 1.6971830985915493, "no_speech_prob": 2.3919445084175095e-05}, {"id": 96, "seek": 56532, "start": 565.32, "end": 570.0, "text": " it'll just work. Also, what's important is it doesn't matter if you're a QA engineer, if you're a", "tokens": [309, 603, 445, 589, 13, 2743, 11, 437, 311, 1021, 307, 309, 1177, 380, 1871, 498, 291, 434, 257, 1249, 32, 11403, 11, 498, 291, 434, 257], "temperature": 0.0, "avg_logprob": -0.12131254802378573, "compression_ratio": 1.75, "no_speech_prob": 4.97749206260778e-05}, {"id": 97, "seek": 56532, "start": 570.0, "end": 576.48, "text": " backend developer, if you're a DevOps person, it'll just work. You have tools for everybody,", "tokens": [38087, 10754, 11, 498, 291, 434, 257, 43051, 954, 11, 309, 603, 445, 589, 13, 509, 362, 3873, 337, 2201, 11], "temperature": 0.0, "avg_logprob": -0.12131254802378573, "compression_ratio": 1.75, "no_speech_prob": 4.97749206260778e-05}, {"id": 98, "seek": 56532, "start": 576.48, "end": 583.2, "text": " web UI, CLI, whatever you want, whatever you need, it's there for you. And then why I think it's", "tokens": [3670, 15682, 11, 12855, 40, 11, 2035, 291, 528, 11, 2035, 291, 643, 11, 309, 311, 456, 337, 291, 13, 400, 550, 983, 286, 519, 309, 311], "temperature": 0.0, "avg_logprob": -0.12131254802378573, "compression_ratio": 1.75, "no_speech_prob": 4.97749206260778e-05}, {"id": 99, "seek": 56532, "start": 583.2, "end": 588.9200000000001, "text": " powerful, you're not running artificial tests, you're testing against real data, and obviously you", "tokens": [4005, 11, 291, 434, 406, 2614, 11677, 6921, 11, 291, 434, 4997, 1970, 957, 1412, 11, 293, 2745, 291], "temperature": 0.0, "avg_logprob": -0.12131254802378573, "compression_ratio": 1.75, "no_speech_prob": 4.97749206260778e-05}, {"id": 100, "seek": 56532, "start": 588.9200000000001, "end": 593.6800000000001, "text": " have a tool belt that you're really used to. You can run test suites by chaining tests together,", "tokens": [362, 257, 2290, 10750, 300, 291, 434, 534, 1143, 281, 13, 509, 393, 1190, 1500, 459, 3324, 538, 417, 3686, 6921, 1214, 11], "temperature": 0.0, "avg_logprob": -0.12131254802378573, "compression_ratio": 1.75, "no_speech_prob": 4.97749206260778e-05}, {"id": 101, "seek": 59368, "start": 593.68, "end": 599.1999999999999, "text": " have transactions where the standard way you're running integration tests is you have a setup,", "tokens": [362, 16856, 689, 264, 3832, 636, 291, 434, 2614, 10980, 6921, 307, 291, 362, 257, 8657, 11], "temperature": 0.0, "avg_logprob": -0.11392876080104283, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.8053735402645543e-05}, {"id": 102, "seek": 59368, "start": 599.1999999999999, "end": 602.92, "text": " you connect into a database, you're running an insert, you're checking if the insert works,", "tokens": [291, 1745, 666, 257, 8149, 11, 291, 434, 2614, 364, 8969, 11, 291, 434, 8568, 498, 264, 8969, 1985, 11], "temperature": 0.0, "avg_logprob": -0.11392876080104283, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.8053735402645543e-05}, {"id": 103, "seek": 59368, "start": 602.92, "end": 608.5999999999999, "text": " you're deleting that whole path, that environment, and that's what we provide as well. You can set", "tokens": [291, 434, 48946, 300, 1379, 3100, 11, 300, 2823, 11, 293, 300, 311, 437, 321, 2893, 382, 731, 13, 509, 393, 992], "temperature": 0.0, "avg_logprob": -0.11392876080104283, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.8053735402645543e-05}, {"id": 104, "seek": 59368, "start": 608.5999999999999, "end": 616.64, "text": " that whole transaction up through the UI. So it's literally what you're used to, but better. You", "tokens": [300, 1379, 14425, 493, 807, 264, 15682, 13, 407, 309, 311, 3736, 437, 291, 434, 1143, 281, 11, 457, 1101, 13, 509], "temperature": 0.0, "avg_logprob": -0.11392876080104283, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.8053735402645543e-05}, {"id": 105, "seek": 59368, "start": 616.64, "end": 620.7199999999999, "text": " always have test environments as well, which is a very big thing because you can have one set", "tokens": [1009, 362, 1500, 12388, 382, 731, 11, 597, 307, 257, 588, 955, 551, 570, 291, 393, 362, 472, 992], "temperature": 0.0, "avg_logprob": -0.11392876080104283, "compression_ratio": 1.8521400778210118, "no_speech_prob": 1.8053735402645543e-05}, {"id": 106, "seek": 62072, "start": 620.72, "end": 624.36, "text": " environment for your dev, for your QA, for your prod, for your whatever. So it's very,", "tokens": [2823, 337, 428, 1905, 11, 337, 428, 1249, 32, 11, 337, 428, 15792, 11, 337, 428, 2035, 13, 407, 309, 311, 588, 11], "temperature": 0.0, "avg_logprob": -0.14151776526585097, "compression_ratio": 1.72992700729927, "no_speech_prob": 3.268677755841054e-05}, {"id": 107, "seek": 62072, "start": 624.36, "end": 629.96, "text": " very flexible in that way as well. Obviously, I'm going to stress this no mocks because I really", "tokens": [588, 11358, 294, 300, 636, 382, 731, 13, 7580, 11, 286, 478, 516, 281, 4244, 341, 572, 705, 2761, 570, 286, 534], "temperature": 0.0, "avg_logprob": -0.14151776526585097, "compression_ratio": 1.72992700729927, "no_speech_prob": 3.268677755841054e-05}, {"id": 108, "seek": 62072, "start": 629.96, "end": 636.48, "text": " like that. I hate mocking. So I'm going to just shove this down your throat. Every slide is going", "tokens": [411, 300, 13, 286, 4700, 49792, 13, 407, 286, 478, 516, 281, 445, 35648, 341, 760, 428, 12394, 13, 2048, 4137, 307, 516], "temperature": 0.0, "avg_logprob": -0.14151776526585097, "compression_ratio": 1.72992700729927, "no_speech_prob": 3.268677755841054e-05}, {"id": 109, "seek": 62072, "start": 636.48, "end": 641.84, "text": " to be no mocking. But also, one thing that I think is massively important is that if anybody's", "tokens": [281, 312, 572, 49792, 13, 583, 611, 11, 472, 551, 300, 286, 519, 307, 29379, 1021, 307, 300, 498, 4472, 311], "temperature": 0.0, "avg_logprob": -0.14151776526585097, "compression_ratio": 1.72992700729927, "no_speech_prob": 3.268677755841054e-05}, {"id": 110, "seek": 62072, "start": 641.84, "end": 646.36, "text": " running serverless, I've been running serverless since it was a thing like in 2018 when everybody", "tokens": [2614, 7154, 1832, 11, 286, 600, 668, 2614, 7154, 1832, 1670, 309, 390, 257, 551, 411, 294, 6096, 562, 2201], "temperature": 0.0, "avg_logprob": -0.14151776526585097, "compression_ratio": 1.72992700729927, "no_speech_prob": 3.268677755841054e-05}, {"id": 111, "seek": 64636, "start": 646.36, "end": 650.6800000000001, "text": " wanted to run serverless, and it was horrible, it was a horrible experience. So I'd suggest nobody", "tokens": [1415, 281, 1190, 7154, 1832, 11, 293, 309, 390, 9263, 11, 309, 390, 257, 9263, 1752, 13, 407, 286, 1116, 3402, 5079], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 112, "seek": 64636, "start": 650.6800000000001, "end": 656.96, "text": " really does it. But if you have to because of PMs, testing events on message queues and testing", "tokens": [534, 775, 309, 13, 583, 498, 291, 362, 281, 570, 295, 12499, 82, 11, 4997, 3931, 322, 3636, 631, 1247, 293, 4997], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 113, "seek": 64636, "start": 656.96, "end": 662.6800000000001, "text": " events on distributed systems and services in AWS or whatever, like it's prayer driven", "tokens": [3931, 322, 12631, 3652, 293, 3328, 294, 17650, 420, 2035, 11, 411, 309, 311, 8767, 9555], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 114, "seek": 64636, "start": 662.6800000000001, "end": 666.8000000000001, "text": " development. You never really know what's going to happen. So that's something that we provide.", "tokens": [3250, 13, 509, 1128, 534, 458, 437, 311, 516, 281, 1051, 13, 407, 300, 311, 746, 300, 321, 2893, 13], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 115, "seek": 64636, "start": 666.8000000000001, "end": 671.16, "text": " You can literally see the entire trace from that ASIC message queue from other systems,", "tokens": [509, 393, 3736, 536, 264, 2302, 13508, 490, 300, 7469, 2532, 3636, 18639, 490, 661, 3652, 11], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 116, "seek": 64636, "start": 671.16, "end": 675.28, "text": " from other services, and you really know what's happening. Obviously, it's important that you", "tokens": [490, 661, 3328, 11, 293, 291, 534, 458, 437, 311, 2737, 13, 7580, 11, 309, 311, 1021, 300, 291], "temperature": 0.0, "avg_logprob": -0.15185783092792218, "compression_ratio": 1.797427652733119, "no_speech_prob": 2.246495569124818e-05}, {"id": 117, "seek": 67528, "start": 675.28, "end": 679.76, "text": " get assertions based on timing. Maybe you want all of your database requests and your database", "tokens": [483, 19810, 626, 2361, 322, 10822, 13, 2704, 291, 528, 439, 295, 428, 8149, 12475, 293, 428, 8149], "temperature": 0.0, "avg_logprob": -0.1175491554396493, "compression_ratio": 1.6953405017921146, "no_speech_prob": 3.0234601581469178e-05}, {"id": 118, "seek": 67528, "start": 679.76, "end": 685.12, "text": " queries to finish within 500 milliseconds. That just works. And you can also set wild color", "tokens": [24109, 281, 2413, 1951, 5923, 34184, 13, 663, 445, 1985, 13, 400, 291, 393, 611, 992, 4868, 2017], "temperature": 0.0, "avg_logprob": -0.1175491554396493, "compression_ratio": 1.6953405017921146, "no_speech_prob": 3.0234601581469178e-05}, {"id": 119, "seek": 67528, "start": 685.12, "end": 690.4, "text": " assertions. So the same thing I was saying about the database queries, it works for wild cards", "tokens": [19810, 626, 13, 407, 264, 912, 551, 286, 390, 1566, 466, 264, 8149, 24109, 11, 309, 1985, 337, 4868, 5632], "temperature": 0.0, "avg_logprob": -0.1175491554396493, "compression_ratio": 1.6953405017921146, "no_speech_prob": 3.0234601581469178e-05}, {"id": 120, "seek": 67528, "start": 690.4, "end": 696.28, "text": " as well. So a visual demo, like a representation, what that would mean is literally like this. So", "tokens": [382, 731, 13, 407, 257, 5056, 10723, 11, 411, 257, 10290, 11, 437, 300, 576, 914, 307, 3736, 411, 341, 13, 407], "temperature": 0.0, "avg_logprob": -0.1175491554396493, "compression_ratio": 1.6953405017921146, "no_speech_prob": 3.0234601581469178e-05}, {"id": 121, "seek": 67528, "start": 696.28, "end": 700.68, "text": " you have your test executor, which is you can think of that as a trigger. You're testing your", "tokens": [291, 362, 428, 1500, 7568, 284, 11, 597, 307, 291, 393, 519, 295, 300, 382, 257, 7875, 13, 509, 434, 4997, 428], "temperature": 0.0, "avg_logprob": -0.1175491554396493, "compression_ratio": 1.6953405017921146, "no_speech_prob": 3.0234601581469178e-05}, {"id": 122, "seek": 70068, "start": 700.68, "end": 705.4399999999999, "text": " system. That trace data is getting written to your trace data store. It can be pretty much", "tokens": [1185, 13, 663, 13508, 1412, 307, 1242, 3720, 281, 428, 13508, 1412, 3531, 13, 467, 393, 312, 1238, 709], "temperature": 0.0, "avg_logprob": -0.17958120727539062, "compression_ratio": 1.6978417266187051, "no_speech_prob": 5.3053812735015526e-05}, {"id": 123, "seek": 70068, "start": 705.4399999999999, "end": 711.88, "text": " anything you've all heard of. Yeager, OpenSearch, Rufana Tempo, OpenTelemetryCollector, like all", "tokens": [1340, 291, 600, 439, 2198, 295, 13, 835, 3557, 11, 7238, 10637, 1178, 11, 497, 2947, 2095, 8095, 2259, 11, 7238, 14233, 306, 5537, 627, 35294, 20814, 11, 411, 439], "temperature": 0.0, "avg_logprob": -0.17958120727539062, "compression_ratio": 1.6978417266187051, "no_speech_prob": 5.3053812735015526e-05}, {"id": 124, "seek": 70068, "start": 711.88, "end": 716.8, "text": " of those, even vendors like Datadog or whatever. And then what happens is that once the response", "tokens": [295, 729, 11, 754, 22056, 411, 9315, 345, 664, 420, 2035, 13, 400, 550, 437, 2314, 307, 300, 1564, 264, 4134], "temperature": 0.0, "avg_logprob": -0.17958120727539062, "compression_ratio": 1.6978417266187051, "no_speech_prob": 5.3053812735015526e-05}, {"id": 125, "seek": 70068, "start": 716.8, "end": 722.16, "text": " gets back, we pick up that response, but we also pick up the trace. So you can run assertions", "tokens": [2170, 646, 11, 321, 1888, 493, 300, 4134, 11, 457, 321, 611, 1888, 493, 264, 13508, 13, 407, 291, 393, 1190, 19810, 626], "temperature": 0.0, "avg_logprob": -0.17958120727539062, "compression_ratio": 1.6978417266187051, "no_speech_prob": 5.3053812735015526e-05}, {"id": 126, "seek": 70068, "start": 722.16, "end": 729.04, "text": " based on both the trace and the response itself. And then, obviously, you get the result back", "tokens": [2361, 322, 1293, 264, 13508, 293, 264, 4134, 2564, 13, 400, 550, 11, 2745, 11, 291, 483, 264, 1874, 646], "temperature": 0.0, "avg_logprob": -0.17958120727539062, "compression_ratio": 1.6978417266187051, "no_speech_prob": 5.3053812735015526e-05}, {"id": 127, "seek": 72904, "start": 729.04, "end": 734.16, "text": " and then you can see if it's passed, if it's not passed, what you need to fix, et cetera. So yeah,", "tokens": [293, 550, 291, 393, 536, 498, 309, 311, 4678, 11, 498, 309, 311, 406, 4678, 11, 437, 291, 643, 281, 3191, 11, 1030, 11458, 13, 407, 1338, 11], "temperature": 0.0, "avg_logprob": -0.15145970130151556, "compression_ratio": 1.687719298245614, "no_speech_prob": 3.268926593591459e-05}, {"id": 128, "seek": 72904, "start": 734.16, "end": 739.76, "text": " let's show up after all of this over 10 minutes and perfect. After all of this, just like theory", "tokens": [718, 311, 855, 493, 934, 439, 295, 341, 670, 1266, 2077, 293, 2176, 13, 2381, 439, 295, 341, 11, 445, 411, 5261], "temperature": 0.0, "avg_logprob": -0.15145970130151556, "compression_ratio": 1.687719298245614, "no_speech_prob": 3.268926593591459e-05}, {"id": 129, "seek": 72904, "start": 739.76, "end": 743.4399999999999, "text": " and understanding what's happening, we want to jump into actual code. So let's go back to the", "tokens": [293, 3701, 437, 311, 2737, 11, 321, 528, 281, 3012, 666, 3539, 3089, 13, 407, 718, 311, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.15145970130151556, "compression_ratio": 1.687719298245614, "no_speech_prob": 3.268926593591459e-05}, {"id": 130, "seek": 72904, "start": 743.4399999999999, "end": 751.0, "text": " sample of checking our trace-based test. So we have a URL and we're making sure that we're sending", "tokens": [6889, 295, 8568, 527, 13508, 12, 6032, 1500, 13, 407, 321, 362, 257, 12905, 293, 321, 434, 1455, 988, 300, 321, 434, 7750], "temperature": 0.0, "avg_logprob": -0.15145970130151556, "compression_ratio": 1.687719298245614, "no_speech_prob": 3.268926593591459e-05}, {"id": 131, "seek": 72904, "start": 751.0, "end": 757.48, "text": " a GET request to that URL. We're setting up a span. So we're targeting the books span in the", "tokens": [257, 28091, 5308, 281, 300, 12905, 13, 492, 434, 3287, 493, 257, 16174, 13, 407, 321, 434, 17918, 264, 3642, 16174, 294, 264], "temperature": 0.0, "avg_logprob": -0.15145970130151556, "compression_ratio": 1.687719298245614, "no_speech_prob": 3.268926593591459e-05}, {"id": 132, "seek": 75748, "start": 757.48, "end": 763.44, "text": " Books API and we're making sure that we want to have a list of books equal to three. So this is", "tokens": [33843, 9362, 293, 321, 434, 1455, 988, 300, 321, 528, 281, 362, 257, 1329, 295, 3642, 2681, 281, 1045, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.1009666677834331, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.520583828096278e-05}, {"id": 133, "seek": 75748, "start": 763.44, "end": 768.5600000000001, "text": " our TDD red-green process. We have a test. We want to run the code and we see, okay, so we have a", "tokens": [527, 314, 20818, 2182, 12, 27399, 1399, 13, 492, 362, 257, 1500, 13, 492, 528, 281, 1190, 264, 3089, 293, 321, 536, 11, 1392, 11, 370, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.1009666677834331, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.520583828096278e-05}, {"id": 134, "seek": 75748, "start": 768.5600000000001, "end": 772.9200000000001, "text": " handler here. It's getting some books. We have some books, but you can see that there's no", "tokens": [41967, 510, 13, 467, 311, 1242, 512, 3642, 13, 492, 362, 512, 3642, 11, 457, 291, 393, 536, 300, 456, 311, 572], "temperature": 0.0, "avg_logprob": -0.1009666677834331, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.520583828096278e-05}, {"id": 135, "seek": 75748, "start": 772.9200000000001, "end": 778.6, "text": " instrumentation. So if we do run the test, it's going to say, okay, the 200 is fine, but we're", "tokens": [7198, 399, 13, 407, 498, 321, 360, 1190, 264, 1500, 11, 309, 311, 516, 281, 584, 11, 1392, 11, 264, 2331, 307, 2489, 11, 457, 321, 434], "temperature": 0.0, "avg_logprob": -0.1009666677834331, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.520583828096278e-05}, {"id": 136, "seek": 75748, "start": 778.6, "end": 784.64, "text": " not getting any books here. Red, let's go ahead and refactor. We're adding in our spans. So we", "tokens": [406, 1242, 604, 3642, 510, 13, 4477, 11, 718, 311, 352, 2286, 293, 1895, 15104, 13, 492, 434, 5127, 294, 527, 44086, 13, 407, 321], "temperature": 0.0, "avg_logprob": -0.1009666677834331, "compression_ratio": 1.7555555555555555, "no_speech_prob": 1.520583828096278e-05}, {"id": 137, "seek": 78464, "start": 784.64, "end": 789.64, "text": " say, okay, so now I'll add an attribute and I want to pass in the book's length into this", "tokens": [584, 11, 1392, 11, 370, 586, 286, 603, 909, 364, 19667, 293, 286, 528, 281, 1320, 294, 264, 1446, 311, 4641, 666, 341], "temperature": 0.0, "avg_logprob": -0.10828646264895044, "compression_ratio": 1.569672131147541, "no_speech_prob": 2.7963960747001693e-05}, {"id": 138, "seek": 78464, "start": 789.64, "end": 798.8, "text": " attribute right here. Perfect. Now it passes. So this is the most banal simple use case that you", "tokens": [19667, 558, 510, 13, 10246, 13, 823, 309, 11335, 13, 407, 341, 307, 264, 881, 5643, 304, 2199, 764, 1389, 300, 291], "temperature": 0.0, "avg_logprob": -0.10828646264895044, "compression_ratio": 1.569672131147541, "no_speech_prob": 2.7963960747001693e-05}, {"id": 139, "seek": 78464, "start": 798.8, "end": 803.3199999999999, "text": " can see, but you're already seeing value from it because you can pass in a custom value. That's a", "tokens": [393, 536, 11, 457, 291, 434, 1217, 2577, 2158, 490, 309, 570, 291, 393, 1320, 294, 257, 2375, 2158, 13, 663, 311, 257], "temperature": 0.0, "avg_logprob": -0.10828646264895044, "compression_ratio": 1.569672131147541, "no_speech_prob": 2.7963960747001693e-05}, {"id": 140, "seek": 78464, "start": 803.3199999999999, "end": 809.16, "text": " real data. You don't have to mess about with any marking or anything. And then obviously one thing", "tokens": [957, 1412, 13, 509, 500, 380, 362, 281, 2082, 466, 365, 604, 25482, 420, 1340, 13, 400, 550, 2745, 472, 551], "temperature": 0.0, "avg_logprob": -0.10828646264895044, "compression_ratio": 1.569672131147541, "no_speech_prob": 2.7963960747001693e-05}, {"id": 141, "seek": 80916, "start": 809.16, "end": 815.88, "text": " that I'm stressing is very important is what if you want to add a span duration? So I want this", "tokens": [300, 286, 478, 48233, 307, 588, 1021, 307, 437, 498, 291, 528, 281, 909, 257, 16174, 16365, 30, 407, 286, 528, 341], "temperature": 0.0, "avg_logprob": -0.11006776413114944, "compression_ratio": 1.556, "no_speech_prob": 1.2217265975777991e-05}, {"id": 142, "seek": 80916, "start": 815.88, "end": 824.88, "text": " API to finish within 500 milliseconds. Okay. Right now, if we have an issue, even though the code", "tokens": [9362, 281, 2413, 1951, 5923, 34184, 13, 1033, 13, 1779, 586, 11, 498, 321, 362, 364, 2734, 11, 754, 1673, 264, 3089], "temperature": 0.0, "avg_logprob": -0.11006776413114944, "compression_ratio": 1.556, "no_speech_prob": 1.2217265975777991e-05}, {"id": 143, "seek": 80916, "start": 824.88, "end": 832.0799999999999, "text": " works, it might be performing badly. We can add in the span duration, check for the timing, and", "tokens": [1985, 11, 309, 1062, 312, 10205, 13425, 13, 492, 393, 909, 294, 264, 16174, 16365, 11, 1520, 337, 264, 10822, 11, 293], "temperature": 0.0, "avg_logprob": -0.11006776413114944, "compression_ratio": 1.556, "no_speech_prob": 1.2217265975777991e-05}, {"id": 144, "seek": 80916, "start": 832.0799999999999, "end": 837.56, "text": " then obviously refactor if we need to refactor. And that's the thing in the UI as well. Once you do", "tokens": [550, 2745, 1895, 15104, 498, 321, 643, 281, 1895, 15104, 13, 400, 300, 311, 264, 551, 294, 264, 15682, 382, 731, 13, 3443, 291, 360], "temperature": 0.0, "avg_logprob": -0.11006776413114944, "compression_ratio": 1.556, "no_speech_prob": 1.2217265975777991e-05}, {"id": 145, "seek": 83756, "start": 837.56, "end": 842.56, "text": " refactor it, this is what you would see. You go and say, okay, so finally now I have a passing test.", "tokens": [1895, 15104, 309, 11, 341, 307, 437, 291, 576, 536, 13, 509, 352, 293, 584, 11, 1392, 11, 370, 2721, 586, 286, 362, 257, 8437, 1500, 13], "temperature": 0.0, "avg_logprob": -0.12145020263363616, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.8406499041011557e-05}, {"id": 146, "seek": 83756, "start": 842.56, "end": 849.9599999999999, "text": " This book's API is returning within 500 milliseconds. And then obviously the last and I think crucial", "tokens": [639, 1446, 311, 9362, 307, 12678, 1951, 5923, 34184, 13, 400, 550, 2745, 264, 1036, 293, 286, 519, 11462], "temperature": 0.0, "avg_logprob": -0.12145020263363616, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.8406499041011557e-05}, {"id": 147, "seek": 83756, "start": 849.9599999999999, "end": 855.9599999999999, "text": " thing with using trace-based testing is that you can literally test on a search on every part of an", "tokens": [551, 365, 1228, 13508, 12, 6032, 4997, 307, 300, 291, 393, 3736, 1500, 322, 257, 3164, 322, 633, 644, 295, 364], "temperature": 0.0, "avg_logprob": -0.12145020263363616, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.8406499041011557e-05}, {"id": 148, "seek": 83756, "start": 855.9599999999999, "end": 862.4399999999999, "text": " HTTP transaction. So if we go back to our books handler API, instead of calling books, we're now", "tokens": [33283, 14425, 13, 407, 498, 321, 352, 646, 281, 527, 3642, 41967, 9362, 11, 2602, 295, 5141, 3642, 11, 321, 434, 586], "temperature": 0.0, "avg_logprob": -0.12145020263363616, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.8406499041011557e-05}, {"id": 149, "seek": 86244, "start": 862.44, "end": 868.5200000000001, "text": " calling available books. So we are calling an external API to see if the books are available or", "tokens": [5141, 2435, 3642, 13, 407, 321, 366, 5141, 364, 8320, 9362, 281, 536, 498, 264, 3642, 366, 2435, 420], "temperature": 0.0, "avg_logprob": -0.11870277877402517, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.3920163584989496e-05}, {"id": 150, "seek": 86244, "start": 868.5200000000001, "end": 873.6400000000001, "text": " not. So we're having this microservice to microservice communication. And if you check that,", "tokens": [406, 13, 407, 321, 434, 1419, 341, 15547, 25006, 281, 15547, 25006, 6101, 13, 400, 498, 291, 1520, 300, 11], "temperature": 0.0, "avg_logprob": -0.11870277877402517, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.3920163584989496e-05}, {"id": 151, "seek": 86244, "start": 873.6400000000001, "end": 879.08, "text": " get the available books function. So we have some promise thingamajig happening here. We're", "tokens": [483, 264, 2435, 3642, 2445, 13, 407, 321, 362, 512, 6228, 551, 335, 1805, 328, 2737, 510, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.11870277877402517, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.3920163584989496e-05}, {"id": 152, "seek": 86244, "start": 879.08, "end": 884.2, "text": " calling an availability API and we're just checking if it's available or not. So the kicker here is", "tokens": [5141, 364, 17945, 9362, 293, 321, 434, 445, 8568, 498, 309, 311, 2435, 420, 406, 13, 407, 264, 4437, 260, 510, 307], "temperature": 0.0, "avg_logprob": -0.11870277877402517, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.3920163584989496e-05}, {"id": 153, "seek": 86244, "start": 884.2, "end": 889.5200000000001, "text": " we're calling an external API. The external API is super simple. We're just running some tests", "tokens": [321, 434, 5141, 364, 8320, 9362, 13, 440, 8320, 9362, 307, 1687, 2199, 13, 492, 434, 445, 2614, 512, 6921], "temperature": 0.0, "avg_logprob": -0.11870277877402517, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.3920163584989496e-05}, {"id": 154, "seek": 88952, "start": 889.52, "end": 894.24, "text": " whether it's available or not and we're setting this attribute. So it's very, very simple example.", "tokens": [1968, 309, 311, 2435, 420, 406, 293, 321, 434, 3287, 341, 19667, 13, 407, 309, 311, 588, 11, 588, 2199, 1365, 13], "temperature": 0.0, "avg_logprob": -0.17002719327023155, "compression_ratio": 1.5865921787709498, "no_speech_prob": 4.859923592448467e-06}, {"id": 155, "seek": 88952, "start": 894.24, "end": 908.1999999999999, "text": " But the thing is, what if in the availability check, we have a problem? This is why I don't do", "tokens": [583, 264, 551, 307, 11, 437, 498, 294, 264, 17945, 1520, 11, 321, 362, 257, 1154, 30, 639, 307, 983, 286, 500, 380, 360], "temperature": 0.0, "avg_logprob": -0.17002719327023155, "compression_ratio": 1.5865921787709498, "no_speech_prob": 4.859923592448467e-06}, {"id": 156, "seek": 88952, "start": 908.1999999999999, "end": 912.6, "text": " live demos. Anyway, so if in the availability check, if you're checking here, you can see,", "tokens": [1621, 33788, 13, 5684, 11, 370, 498, 294, 264, 17945, 1520, 11, 498, 291, 434, 8568, 510, 11, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.17002719327023155, "compression_ratio": 1.5865921787709498, "no_speech_prob": 4.859923592448467e-06}, {"id": 157, "seek": 91260, "start": 912.6, "end": 919.5600000000001, "text": " oh, we have a problem. There are books that are out of stock. So this is that down the chain", "tokens": [1954, 11, 321, 362, 257, 1154, 13, 821, 366, 3642, 300, 366, 484, 295, 4127, 13, 407, 341, 307, 300, 760, 264, 5021], "temperature": 0.0, "avg_logprob": -0.12468756525969703, "compression_ratio": 1.6814814814814816, "no_speech_prob": 1.4061884030525107e-05}, {"id": 158, "seek": 91260, "start": 919.5600000000001, "end": 923.9200000000001, "text": " action that would happen. You would never know what the hell is the problem. But now,", "tokens": [3069, 300, 576, 1051, 13, 509, 576, 1128, 458, 437, 264, 4921, 307, 264, 1154, 13, 583, 586, 11], "temperature": 0.0, "avg_logprob": -0.12468756525969703, "compression_ratio": 1.6814814814814816, "no_speech_prob": 1.4061884030525107e-05}, {"id": 159, "seek": 91260, "start": 923.9200000000001, "end": 929.0400000000001, "text": " because we have this set up, we can say, okay, so I'm adding into my trace-based test. I want", "tokens": [570, 321, 362, 341, 992, 493, 11, 321, 393, 584, 11, 1392, 11, 370, 286, 478, 5127, 666, 452, 13508, 12, 6032, 1500, 13, 286, 528], "temperature": 0.0, "avg_logprob": -0.12468756525969703, "compression_ratio": 1.6814814814814816, "no_speech_prob": 1.4061884030525107e-05}, {"id": 160, "seek": 91260, "start": 929.0400000000001, "end": 933.9200000000001, "text": " to make sure the availability API is up. So I'm actually triggering this host. And I also want", "tokens": [281, 652, 988, 264, 17945, 9362, 307, 493, 13, 407, 286, 478, 767, 40406, 341, 3975, 13, 400, 286, 611, 528], "temperature": 0.0, "avg_logprob": -0.12468756525969703, "compression_ratio": 1.6814814814814816, "no_speech_prob": 1.4061884030525107e-05}, {"id": 161, "seek": 91260, "start": 933.9200000000001, "end": 938.8000000000001, "text": " to make sure that all of these is available attributes is true. If I do run that test,", "tokens": [281, 652, 988, 300, 439, 295, 613, 307, 2435, 17212, 307, 2074, 13, 759, 286, 360, 1190, 300, 1500, 11], "temperature": 0.0, "avg_logprob": -0.12468756525969703, "compression_ratio": 1.6814814814814816, "no_speech_prob": 1.4061884030525107e-05}, {"id": 162, "seek": 93880, "start": 938.8, "end": 945.4399999999999, "text": " I'll see that, whoopsie, I'll see that they're all passing except for this one because, oh wait,", "tokens": [286, 603, 536, 300, 11, 567, 3370, 414, 11, 286, 603, 536, 300, 436, 434, 439, 8437, 3993, 337, 341, 472, 570, 11, 1954, 1699, 11], "temperature": 0.0, "avg_logprob": -0.13602420116992706, "compression_ratio": 1.7605633802816902, "no_speech_prob": 4.756989801535383e-05}, {"id": 163, "seek": 93880, "start": 945.4399999999999, "end": 951.3599999999999, "text": " there was actually one node, like one part of the trace, one span that was returning false,", "tokens": [456, 390, 767, 472, 9984, 11, 411, 472, 644, 295, 264, 13508, 11, 472, 16174, 300, 390, 12678, 7908, 11], "temperature": 0.0, "avg_logprob": -0.13602420116992706, "compression_ratio": 1.7605633802816902, "no_speech_prob": 4.756989801535383e-05}, {"id": 164, "seek": 93880, "start": 951.3599999999999, "end": 956.3599999999999, "text": " because one book was out of stock. And if you jump in here, you can see that everything is", "tokens": [570, 472, 1446, 390, 484, 295, 4127, 13, 400, 498, 291, 3012, 294, 510, 11, 291, 393, 536, 300, 1203, 307], "temperature": 0.0, "avg_logprob": -0.13602420116992706, "compression_ratio": 1.7605633802816902, "no_speech_prob": 4.756989801535383e-05}, {"id": 165, "seek": 93880, "start": 956.3599999999999, "end": 963.12, "text": " literally passing. Everything is passing except for that one span, which is something you would", "tokens": [3736, 8437, 13, 5471, 307, 8437, 3993, 337, 300, 472, 16174, 11, 597, 307, 746, 291, 576], "temperature": 0.0, "avg_logprob": -0.13602420116992706, "compression_ratio": 1.7605633802816902, "no_speech_prob": 4.756989801535383e-05}, {"id": 166, "seek": 96312, "start": 963.12, "end": 970.5600000000001, "text": " never figure out if you're running the traditional way of running tests. And the last thing I really", "tokens": [1128, 2573, 484, 498, 291, 434, 2614, 264, 5164, 636, 295, 2614, 6921, 13, 400, 264, 1036, 551, 286, 534], "temperature": 0.0, "avg_logprob": -0.10350198331086533, "compression_ratio": 1.8288973384030418, "no_speech_prob": 1.260556837223703e-05}, {"id": 167, "seek": 96312, "start": 970.5600000000001, "end": 975.5600000000001, "text": " want to stress before we wrap up is that this will work with any distributed system that has", "tokens": [528, 281, 4244, 949, 321, 7019, 493, 307, 300, 341, 486, 589, 365, 604, 12631, 1185, 300, 575], "temperature": 0.0, "avg_logprob": -0.10350198331086533, "compression_ratio": 1.8288973384030418, "no_speech_prob": 1.260556837223703e-05}, {"id": 168, "seek": 96312, "start": 975.5600000000001, "end": 980.76, "text": " open telemetry instrumentation. So any system that looks like this, you have an app with open", "tokens": [1269, 4304, 5537, 627, 7198, 399, 13, 407, 604, 1185, 300, 1542, 411, 341, 11, 291, 362, 364, 724, 365, 1269], "temperature": 0.0, "avg_logprob": -0.10350198331086533, "compression_ratio": 1.8288973384030418, "no_speech_prob": 1.260556837223703e-05}, {"id": 169, "seek": 96312, "start": 980.76, "end": 984.44, "text": " telemetry, you're sending to the open telemetry collector, and then you're sending that trace", "tokens": [4304, 5537, 627, 11, 291, 434, 7750, 281, 264, 1269, 4304, 5537, 627, 23960, 11, 293, 550, 291, 434, 7750, 300, 13508], "temperature": 0.0, "avg_logprob": -0.10350198331086533, "compression_ratio": 1.8288973384030418, "no_speech_prob": 1.260556837223703e-05}, {"id": 170, "seek": 96312, "start": 984.44, "end": 990.5600000000001, "text": " data to any trace data store. Yeager open search doesn't really matter. You hook in your trace test", "tokens": [1412, 281, 604, 13508, 1412, 3531, 13, 835, 3557, 1269, 3164, 1177, 380, 534, 1871, 13, 509, 6328, 294, 428, 13508, 1500], "temperature": 0.0, "avg_logprob": -0.10350198331086533, "compression_ratio": 1.8288973384030418, "no_speech_prob": 1.260556837223703e-05}, {"id": 171, "seek": 99056, "start": 990.56, "end": 996.8, "text": " instance, you pick up data on every request, you pick up data from the trace data store, and you", "tokens": [5197, 11, 291, 1888, 493, 1412, 322, 633, 5308, 11, 291, 1888, 493, 1412, 490, 264, 13508, 1412, 3531, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.1310989950585553, "compression_ratio": 1.8364312267657992, "no_speech_prob": 5.91940333833918e-05}, {"id": 172, "seek": 99056, "start": 996.8, "end": 1003.5999999999999, "text": " run these tests. This is the only setup you really need to do. Install the CLI, install the server,", "tokens": [1190, 613, 6921, 13, 639, 307, 264, 787, 8657, 291, 534, 643, 281, 360, 13, 31982, 264, 12855, 40, 11, 3625, 264, 7154, 11], "temperature": 0.0, "avg_logprob": -0.1310989950585553, "compression_ratio": 1.8364312267657992, "no_speech_prob": 5.91940333833918e-05}, {"id": 173, "seek": 99056, "start": 1003.5999999999999, "end": 1009.1199999999999, "text": " one command, one command, and you're ready. Set up your Docker composer or Kubernetes, all of this", "tokens": [472, 5622, 11, 472, 5622, 11, 293, 291, 434, 1919, 13, 8928, 493, 428, 33772, 26003, 420, 23145, 11, 439, 295, 341], "temperature": 0.0, "avg_logprob": -0.1310989950585553, "compression_ratio": 1.8364312267657992, "no_speech_prob": 5.91940333833918e-05}, {"id": 174, "seek": 99056, "start": 1009.1199999999999, "end": 1014.4399999999999, "text": " works out of the box with the install. We have good engineers, like these guys really try to make", "tokens": [1985, 484, 295, 264, 2424, 365, 264, 3625, 13, 492, 362, 665, 11955, 11, 411, 613, 1074, 534, 853, 281, 652], "temperature": 0.0, "avg_logprob": -0.1310989950585553, "compression_ratio": 1.8364312267657992, "no_speech_prob": 5.91940333833918e-05}, {"id": 175, "seek": 99056, "start": 1014.4399999999999, "end": 1019.88, "text": " the install really simple. You set up the trace data store, you can do that in the UI or in the CLI,", "tokens": [264, 3625, 534, 2199, 13, 509, 992, 493, 264, 13508, 1412, 3531, 11, 291, 393, 360, 300, 294, 264, 15682, 420, 294, 264, 12855, 40, 11], "temperature": 0.0, "avg_logprob": -0.1310989950585553, "compression_ratio": 1.8364312267657992, "no_speech_prob": 5.91940333833918e-05}, {"id": 176, "seek": 101988, "start": 1019.88, "end": 1028.32, "text": " doesn't really matter. Connect the data store, and you're done. It just works. So last recap,", "tokens": [1177, 380, 534, 1871, 13, 11653, 264, 1412, 3531, 11, 293, 291, 434, 1096, 13, 467, 445, 1985, 13, 407, 1036, 20928, 11], "temperature": 0.0, "avg_logprob": -0.16280406586667326, "compression_ratio": 1.5502008032128514, "no_speech_prob": 2.077909084619023e-05}, {"id": 177, "seek": 101988, "start": 1028.32, "end": 1034.04, "text": " two minutes left. What did we learn today? We learned that obviously open ODD or observability", "tokens": [732, 2077, 1411, 13, 708, 630, 321, 1466, 965, 30, 492, 3264, 300, 2745, 1269, 422, 20818, 420, 9951, 2310], "temperature": 0.0, "avg_logprob": -0.16280406586667326, "compression_ratio": 1.5502008032128514, "no_speech_prob": 2.077909084619023e-05}, {"id": 178, "seek": 101988, "start": 1034.04, "end": 1039.04, "text": " driven development is really awesome. You don't have to mock, again with the mocking. You're testing", "tokens": [9555, 3250, 307, 534, 3476, 13, 509, 500, 380, 362, 281, 17362, 11, 797, 365, 264, 49792, 13, 509, 434, 4997], "temperature": 0.0, "avg_logprob": -0.16280406586667326, "compression_ratio": 1.5502008032128514, "no_speech_prob": 2.077909084619023e-05}, {"id": 179, "seek": 101988, "start": 1039.04, "end": 1043.72, "text": " against real data, and you don't have any black boxes anymore. You know exactly what's happening", "tokens": [1970, 957, 1412, 11, 293, 291, 500, 380, 362, 604, 2211, 9002, 3602, 13, 509, 458, 2293, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.16280406586667326, "compression_ratio": 1.5502008032128514, "no_speech_prob": 2.077909084619023e-05}, {"id": 180, "seek": 104372, "start": 1043.72, "end": 1050.56, "text": " in every single microservice. You can assert on every step of the transaction. And as the last", "tokens": [294, 633, 2167, 15547, 25006, 13, 509, 393, 19810, 322, 633, 1823, 295, 264, 14425, 13, 400, 382, 264, 1036], "temperature": 0.0, "avg_logprob": -0.11973531195457945, "compression_ratio": 1.6425531914893616, "no_speech_prob": 1.2604493349499535e-05}, {"id": 181, "seek": 104372, "start": 1050.56, "end": 1056.44, "text": " recap, I mean, you wouldn't be here if you thought testing was fun or easy or something that you", "tokens": [20928, 11, 286, 914, 11, 291, 2759, 380, 312, 510, 498, 291, 1194, 4997, 390, 1019, 420, 1858, 420, 746, 300, 291], "temperature": 0.0, "avg_logprob": -0.11973531195457945, "compression_ratio": 1.6425531914893616, "no_speech_prob": 1.2604493349499535e-05}, {"id": 182, "seek": 104372, "start": 1056.44, "end": 1061.4, "text": " really enjoy doing. It is hard, like we all know it is very hard. Testing distributed systems is", "tokens": [534, 2103, 884, 13, 467, 307, 1152, 11, 411, 321, 439, 458, 309, 307, 588, 1152, 13, 45517, 12631, 3652, 307], "temperature": 0.0, "avg_logprob": -0.11973531195457945, "compression_ratio": 1.6425531914893616, "no_speech_prob": 1.2604493349499535e-05}, {"id": 183, "seek": 104372, "start": 1061.4, "end": 1068.1200000000001, "text": " even harder. Testing microservices is even harder. So I want to help you elevate that TDD process", "tokens": [754, 6081, 13, 45517, 15547, 47480, 307, 754, 6081, 13, 407, 286, 528, 281, 854, 291, 33054, 300, 314, 20818, 1399], "temperature": 0.0, "avg_logprob": -0.11973531195457945, "compression_ratio": 1.6425531914893616, "no_speech_prob": 1.2604493349499535e-05}, {"id": 184, "seek": 106812, "start": 1068.12, "end": 1074.36, "text": " that you're already doing. You're already doing well that you like to doing ODD as well. That's", "tokens": [300, 291, 434, 1217, 884, 13, 509, 434, 1217, 884, 731, 300, 291, 411, 281, 884, 422, 20818, 382, 731, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.18148253017798402, "compression_ratio": 1.7132616487455197, "no_speech_prob": 2.710486478463281e-05}, {"id": 185, "seek": 106812, "start": 1074.36, "end": 1079.6399999999999, "text": " pretty much it. We're on point. If you have any questions, if you want to check out Trace Test,", "tokens": [1238, 709, 309, 13, 492, 434, 322, 935, 13, 759, 291, 362, 604, 1651, 11, 498, 291, 528, 281, 1520, 484, 1765, 617, 9279, 11], "temperature": 0.0, "avg_logprob": -0.18148253017798402, "compression_ratio": 1.7132616487455197, "no_speech_prob": 2.710486478463281e-05}, {"id": 186, "seek": 106812, "start": 1079.6399999999999, "end": 1084.9199999999998, "text": " go just go to githubcubeshop slash trace test. You can download it. You can read a blog post I", "tokens": [352, 445, 352, 281, 290, 355, 836, 66, 836, 14935, 404, 17330, 13508, 1500, 13, 509, 393, 5484, 309, 13, 509, 393, 1401, 257, 6968, 2183, 286], "temperature": 0.0, "avg_logprob": -0.18148253017798402, "compression_ratio": 1.7132616487455197, "no_speech_prob": 2.710486478463281e-05}, {"id": 187, "seek": 106812, "start": 1084.9199999999998, "end": 1090.4399999999998, "text": " wrote about this as well. So knock yourselves out, I guess. You can also do, just to make it", "tokens": [4114, 466, 341, 382, 731, 13, 407, 6728, 14791, 484, 11, 286, 2041, 13, 509, 393, 611, 360, 11, 445, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.18148253017798402, "compression_ratio": 1.7132616487455197, "no_speech_prob": 2.710486478463281e-05}, {"id": 188, "seek": 106812, "start": 1090.4399999999998, "end": 1095.04, "text": " easier, you can do the, like you can also jump into Discord. You can chat with me or the engineers", "tokens": [3571, 11, 291, 393, 360, 264, 11, 411, 291, 393, 611, 3012, 666, 32623, 13, 509, 393, 5081, 365, 385, 420, 264, 11955], "temperature": 0.0, "avg_logprob": -0.18148253017798402, "compression_ratio": 1.7132616487455197, "no_speech_prob": 2.710486478463281e-05}, {"id": 189, "seek": 109504, "start": 1095.04, "end": 1099.68, "text": " face-to-face. If you have any questions, if you want to try it out, check out the github. Also,", "tokens": [1851, 12, 1353, 12, 2868, 13, 759, 291, 362, 604, 1651, 11, 498, 291, 528, 281, 853, 309, 484, 11, 1520, 484, 264, 290, 355, 836, 13, 2743, 11], "temperature": 0.0, "avg_logprob": -0.21319018817338786, "compression_ratio": 1.3426573426573427, "no_speech_prob": 7.133069448173046e-05}, {"id": 190, "seek": 109504, "start": 1099.68, "end": 1104.92, "text": " give us a star, you know, because it's kind of why I'm here. I have to earn my salary something,", "tokens": [976, 505, 257, 3543, 11, 291, 458, 11, 570, 309, 311, 733, 295, 983, 286, 478, 510, 13, 286, 362, 281, 6012, 452, 15360, 746, 11], "temperature": 0.0, "avg_logprob": -0.21319018817338786, "compression_ratio": 1.3426573426573427, "no_speech_prob": 7.133069448173046e-05}, {"id": 191, "seek": 110492, "start": 1104.92, "end": 1132.2, "text": " in some way. So questions? Yeah, sure. So test run against the trace from the system. Yes,", "tokens": [294, 512, 636, 13, 407, 1651, 30, 865, 11, 988, 13, 407, 1500, 1190, 1970, 264, 13508, 490, 264, 1185, 13, 1079, 11], "temperature": 0.0, "avg_logprob": -0.2934028660809552, "compression_ratio": 1.0843373493975903, "no_speech_prob": 7.994472252903506e-05}, {"id": 192, "seek": 113220, "start": 1132.2, "end": 1138.1200000000001, "text": " the way it happens is that, imagine you're running a postman request. That would be called,", "tokens": [264, 636, 309, 2314, 307, 300, 11, 3811, 291, 434, 2614, 257, 2183, 1601, 5308, 13, 663, 576, 312, 1219, 11], "temperature": 0.0, "avg_logprob": -0.11426524066050119, "compression_ratio": 1.9205020920502092, "no_speech_prob": 7.026461389614269e-05}, {"id": 193, "seek": 113220, "start": 1138.1200000000001, "end": 1141.8, "text": " because this is trace test, that would be called response test. You get a response,", "tokens": [570, 341, 307, 13508, 1500, 11, 300, 576, 312, 1219, 4134, 1500, 13, 509, 483, 257, 4134, 11], "temperature": 0.0, "avg_logprob": -0.11426524066050119, "compression_ratio": 1.9205020920502092, "no_speech_prob": 7.026461389614269e-05}, {"id": 194, "seek": 113220, "start": 1141.8, "end": 1147.44, "text": " you're testing on it. For trace test, you get the response, but you're also tapping into the", "tokens": [291, 434, 4997, 322, 309, 13, 1171, 13508, 1500, 11, 291, 483, 264, 4134, 11, 457, 291, 434, 611, 21444, 666, 264], "temperature": 0.0, "avg_logprob": -0.11426524066050119, "compression_ratio": 1.9205020920502092, "no_speech_prob": 7.026461389614269e-05}, {"id": 195, "seek": 113220, "start": 1147.44, "end": 1153.2, "text": " trace data store and getting the traces that that request generates. So from that distributed", "tokens": [13508, 1412, 3531, 293, 1242, 264, 26076, 300, 300, 5308, 23815, 13, 407, 490, 300, 12631], "temperature": 0.0, "avg_logprob": -0.11426524066050119, "compression_ratio": 1.9205020920502092, "no_speech_prob": 7.026461389614269e-05}, {"id": 196, "seek": 113220, "start": 1153.2, "end": 1158.24, "text": " trace, then you're running assertions based on the spans within that trace, if that answers your", "tokens": [13508, 11, 550, 291, 434, 2614, 19810, 626, 2361, 322, 264, 44086, 1951, 300, 13508, 11, 498, 300, 6338, 428], "temperature": 0.0, "avg_logprob": -0.11426524066050119, "compression_ratio": 1.9205020920502092, "no_speech_prob": 7.026461389614269e-05}, {"id": 197, "seek": 115824, "start": 1158.24, "end": 1175.44, "text": " question. Yeah, for sure. The only thing is that, obviously, if you're running locally, you have a", "tokens": [1168, 13, 865, 11, 337, 988, 13, 440, 787, 551, 307, 300, 11, 2745, 11, 498, 291, 434, 2614, 16143, 11, 291, 362, 257], "temperature": 0.0, "avg_logprob": -0.14373831818069238, "compression_ratio": 1.513089005235602, "no_speech_prob": 4.0631468436913565e-05}, {"id": 198, "seek": 115824, "start": 1175.44, "end": 1181.4, "text": " setup where your application is sending to either an open telemetry collector or whatever. You can", "tokens": [8657, 689, 428, 3861, 307, 7750, 281, 2139, 364, 1269, 4304, 5537, 627, 23960, 420, 2035, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.14373831818069238, "compression_ratio": 1.513089005235602, "no_speech_prob": 4.0631468436913565e-05}, {"id": 199, "seek": 115824, "start": 1181.4, "end": 1186.52, "text": " also tap into that, where you configure trace test to be the pipeline endpoint of your open", "tokens": [611, 5119, 666, 300, 11, 689, 291, 22162, 13508, 1500, 281, 312, 264, 15517, 35795, 295, 428, 1269], "temperature": 0.0, "avg_logprob": -0.14373831818069238, "compression_ratio": 1.513089005235602, "no_speech_prob": 4.0631468436913565e-05}, {"id": 200, "seek": 118652, "start": 1186.52, "end": 1193.76, "text": " telemetry collector. So you can just run it as a dev tool as well. So also we might, I'm not sure", "tokens": [4304, 5537, 627, 23960, 13, 407, 291, 393, 445, 1190, 309, 382, 257, 1905, 2290, 382, 731, 13, 407, 611, 321, 1062, 11, 286, 478, 406, 988], "temperature": 0.0, "avg_logprob": -0.10532649074281965, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00015825768059585243}, {"id": 201, "seek": 118652, "start": 1193.76, "end": 1198.2, "text": " if I'm good saying this on camera, but we might be building a desktop app very soon, because we're", "tokens": [498, 286, 478, 665, 1566, 341, 322, 2799, 11, 457, 321, 1062, 312, 2390, 257, 14502, 724, 588, 2321, 11, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.10532649074281965, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00015825768059585243}, {"id": 202, "seek": 118652, "start": 1198.2, "end": 1202.8799999999999, "text": " like half a year into this, so we're still kind of figuring out what you guys need. So that's why", "tokens": [411, 1922, 257, 1064, 666, 341, 11, 370, 321, 434, 920, 733, 295, 15213, 484, 437, 291, 1074, 643, 13, 407, 300, 311, 983], "temperature": 0.0, "avg_logprob": -0.10532649074281965, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00015825768059585243}, {"id": 203, "seek": 118652, "start": 1202.8799999999999, "end": 1211.84, "text": " I'm here as well. But yeah, let's see what happens. It's a great question, by the way. It's good", "tokens": [286, 478, 510, 382, 731, 13, 583, 1338, 11, 718, 311, 536, 437, 2314, 13, 467, 311, 257, 869, 1168, 11, 538, 264, 636, 13, 467, 311, 665], "temperature": 0.0, "avg_logprob": -0.10532649074281965, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00015825768059585243}, {"id": 204, "seek": 121184, "start": 1211.84, "end": 1228.84, "text": " to finish early. We have time for questions. This is great. So yeah, the question was measuring", "tokens": [281, 2413, 2440, 13, 492, 362, 565, 337, 1651, 13, 639, 307, 869, 13, 407, 1338, 11, 264, 1168, 390, 13389], "temperature": 0.0, "avg_logprob": -0.15094280242919922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 4.0618033381178975e-05}, {"id": 205, "seek": 121184, "start": 1228.84, "end": 1233.84, "text": " SLOs for user journeys. That's actually something we're working on now. I'm not sure if you know", "tokens": [318, 20184, 82, 337, 4195, 36736, 13, 663, 311, 767, 746, 321, 434, 1364, 322, 586, 13, 286, 478, 406, 988, 498, 291, 458], "temperature": 0.0, "avg_logprob": -0.15094280242919922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 4.0618033381178975e-05}, {"id": 206, "seek": 121184, "start": 1233.84, "end": 1239.3999999999999, "text": " about the captain project. So we have an integration with the captain project as of last week, quite", "tokens": [466, 264, 14871, 1716, 13, 407, 321, 362, 364, 10980, 365, 264, 14871, 1716, 382, 295, 1036, 1243, 11, 1596], "temperature": 0.0, "avg_logprob": -0.15094280242919922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 4.0618033381178975e-05}, {"id": 207, "seek": 123940, "start": 1239.4, "end": 1244.24, "text": " literally. So if you want to check that out, you just jump into trace test integrates with captain", "tokens": [3736, 13, 407, 498, 291, 528, 281, 1520, 300, 484, 11, 291, 445, 3012, 666, 13508, 1500, 3572, 1024, 365, 14871], "temperature": 0.0, "avg_logprob": -0.16410193723790786, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.3171627617557533e-05}, {"id": 208, "seek": 123940, "start": 1244.24, "end": 1249.8400000000001, "text": " and you'll get a lot of documentation and sample apps examples and whatnot to set that up as well.", "tokens": [293, 291, 603, 483, 257, 688, 295, 14333, 293, 6889, 7733, 5110, 293, 25882, 281, 992, 300, 493, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16410193723790786, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.3171627617557533e-05}, {"id": 209, "seek": 123940, "start": 1249.8400000000001, "end": 1253.52, "text": " So that's an excellent use case and something that we actively have been working on. So", "tokens": [407, 300, 311, 364, 7103, 764, 1389, 293, 746, 300, 321, 13022, 362, 668, 1364, 322, 13, 407], "temperature": 0.0, "avg_logprob": -0.16410193723790786, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.3171627617557533e-05}, {"id": 210, "seek": 125352, "start": 1253.52, "end": 1267.8799999999999, "text": " 100% like the thing is that whatever you have implemented, if you have hotel traces coming in", "tokens": [2319, 4, 411, 264, 551, 307, 300, 2035, 291, 362, 12270, 11, 498, 291, 362, 7622, 26076, 1348, 294], "temperature": 0.0, "avg_logprob": -0.18606580446844231, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.477814607322216e-05}, {"id": 211, "seek": 125352, "start": 1267.8799999999999, "end": 1275.2, "text": " from that system, it works. So it's language agnostic, setup agnostic, it's literally like just", "tokens": [490, 300, 1185, 11, 309, 1985, 13, 407, 309, 311, 2856, 623, 77, 19634, 11, 8657, 623, 77, 19634, 11, 309, 311, 3736, 411, 445], "temperature": 0.0, "avg_logprob": -0.18606580446844231, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.477814607322216e-05}, {"id": 212, "seek": 125352, "start": 1275.2, "end": 1279.28, "text": " the traces are important. So if you're running hotel, if you're running the data dog agent,", "tokens": [264, 26076, 366, 1021, 13, 407, 498, 291, 434, 2614, 7622, 11, 498, 291, 434, 2614, 264, 1412, 3000, 9461, 11], "temperature": 0.0, "avg_logprob": -0.18606580446844231, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.477814607322216e-05}, {"id": 213, "seek": 127928, "start": 1279.28, "end": 1283.68, "text": " elastic agent, literally anything that generates traces, it'll work. Obviously works best with", "tokens": [17115, 9461, 11, 3736, 1340, 300, 23815, 26076, 11, 309, 603, 589, 13, 7580, 1985, 1151, 365], "temperature": 0.0, "avg_logprob": -0.21878433227539062, "compression_ratio": 1.2892561983471074, "no_speech_prob": 6.794821820221841e-05}, {"id": 214, "seek": 127928, "start": 1283.68, "end": 1288.56, "text": " hotel because open source, you know. But yeah, it just works.", "tokens": [7622, 570, 1269, 4009, 11, 291, 458, 13, 583, 1338, 11, 309, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.21878433227539062, "compression_ratio": 1.2892561983471074, "no_speech_prob": 6.794821820221841e-05}, {"id": 215, "seek": 128856, "start": 1288.56, "end": 1317.24, "text": " So if I understand the question correctly, it is, do I run synthetic tests with trace test? Yes.", "tokens": [407, 498, 286, 1223, 264, 1168, 8944, 11, 309, 307, 11, 360, 286, 1190, 23420, 6921, 365, 13508, 1500, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.18809252518873948, "compression_ratio": 1.103448275862069, "no_speech_prob": 8.751761197345331e-05}, {"id": 216, "seek": 131724, "start": 1317.24, "end": 1321.44, "text": " You can, if you have a CI pipeline or like you can have a cron job somewhere running,", "tokens": [509, 393, 11, 498, 291, 362, 257, 37777, 15517, 420, 411, 291, 393, 362, 257, 941, 266, 1691, 4079, 2614, 11], "temperature": 0.0, "avg_logprob": -0.20884225723591257, "compression_ratio": 1.5625, "no_speech_prob": 0.00013508570555131882}, {"id": 217, "seek": 131724, "start": 1321.44, "end": 1326.04, "text": " doesn't really matter. Every five minutes, I want to trigger this test and make sure that all of", "tokens": [1177, 380, 534, 1871, 13, 2048, 1732, 2077, 11, 286, 528, 281, 7875, 341, 1500, 293, 652, 988, 300, 439, 295], "temperature": 0.0, "avg_logprob": -0.20884225723591257, "compression_ratio": 1.5625, "no_speech_prob": 0.00013508570555131882}, {"id": 218, "seek": 131724, "start": 1326.04, "end": 1340.36, "text": " the assertions are true. That's perfectly fine. Oh, yeah, 100%. It works. You can think of it", "tokens": [264, 19810, 626, 366, 2074, 13, 663, 311, 6239, 2489, 13, 876, 11, 1338, 11, 2319, 6856, 467, 1985, 13, 509, 393, 519, 295, 309], "temperature": 0.0, "avg_logprob": -0.20884225723591257, "compression_ratio": 1.5625, "no_speech_prob": 0.00013508570555131882}, {"id": 219, "seek": 131724, "start": 1340.36, "end": 1345.84, "text": " as testing in production and making sure that the production environment is healthy. That works as", "tokens": [382, 4997, 294, 4265, 293, 1455, 988, 300, 264, 4265, 2823, 307, 4627, 13, 663, 1985, 382], "temperature": 0.0, "avg_logprob": -0.20884225723591257, "compression_ratio": 1.5625, "no_speech_prob": 0.00013508570555131882}, {"id": 220, "seek": 134584, "start": 1345.84, "end": 1356.56, "text": " well. Hi. Yeah. So the test, trace test test depends on your instrumentation. Yes. Your", "tokens": [731, 13, 2421, 13, 865, 13, 407, 264, 1500, 11, 13508, 1500, 1500, 5946, 322, 428, 7198, 399, 13, 1079, 13, 2260], "temperature": 0.0, "avg_logprob": -0.247581664830038, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.00046450927038677037}, {"id": 221, "seek": 134584, "start": 1356.56, "end": 1361.3999999999999, "text": " instrumentation is in your production code. Yes. Do you have any advice on how do you prevent your", "tokens": [7198, 399, 307, 294, 428, 4265, 3089, 13, 1079, 13, 1144, 291, 362, 604, 5192, 322, 577, 360, 291, 4871, 428], "temperature": 0.0, "avg_logprob": -0.247581664830038, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.00046450927038677037}, {"id": 222, "seek": 134584, "start": 1361.3999999999999, "end": 1371.9599999999998, "text": " production code from bloating with the instrumentation to beat these tests? Hmm. I'm going to say,", "tokens": [4265, 3089, 490, 1749, 990, 365, 264, 7198, 399, 281, 4224, 613, 6921, 30, 8239, 13, 286, 478, 516, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.247581664830038, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.00046450927038677037}, {"id": 223, "seek": 137196, "start": 1371.96, "end": 1377.96, "text": " that's a great question, but I think I'm not even close to being good enough of an engineer", "tokens": [300, 311, 257, 869, 1168, 11, 457, 286, 519, 286, 478, 406, 754, 1998, 281, 885, 665, 1547, 295, 364, 11403], "temperature": 0.0, "avg_logprob": -0.20204984664916992, "compression_ratio": 1.1375, "no_speech_prob": 0.00014992826618254185}, {"id": 224, "seek": 137796, "start": 1377.96, "end": 1404.04, "text": " to answer that question, to be honest. 100%. 100%. 100%. Also, yeah. Go ahead. Yeah. Yeah,", "tokens": [281, 1867, 300, 1168, 11, 281, 312, 3245, 13, 2319, 6856, 2319, 6856, 2319, 6856, 2743, 11, 1338, 13, 1037, 2286, 13, 865, 13, 865, 11], "temperature": 0.6000000000000001, "avg_logprob": -0.6194867451985677, "compression_ratio": 1.1538461538461537, "no_speech_prob": 0.00045867785229347646}, {"id": 225, "seek": 140404, "start": 1404.04, "end": 1407.8799999999999, "text": " You really had to pick black boxes, but now the other hand,", "tokens": [509, 534, 632, 281, 1888, 2211, 9002, 11, 457, 586, 264, 661, 1011, 11], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 226, "seek": 140404, "start": 1407.8799999999999, "end": 1409.72, "text": " all right, writing tasks like this,", "tokens": [439, 558, 11, 3579, 9608, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 227, "seek": 140404, "start": 1409.72, "end": 1413.2, "text": " and then spending might be unnecessarily", "tokens": [293, 550, 6434, 1062, 312, 16799, 3289], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 228, "seek": 140404, "start": 1413.2, "end": 1415.6, "text": " tackled into the intricate details", "tokens": [9426, 1493, 666, 264, 38015, 4365], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 229, "seek": 140404, "start": 1415.6, "end": 1417.24, "text": " of the infrastructure right now.", "tokens": [295, 264, 6896, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 230, "seek": 140404, "start": 1417.24, "end": 1419.2, "text": " So is this necessarily good?", "tokens": [407, 307, 341, 4725, 665, 30], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 231, "seek": 140404, "start": 1419.2, "end": 1422.68, "text": " So the text actually knows what's under,", "tokens": [407, 264, 2487, 767, 3255, 437, 311, 833, 11], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 232, "seek": 140404, "start": 1422.68, "end": 1424.36, "text": " what's in that black box.", "tokens": [437, 311, 294, 300, 2211, 2424, 13], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 233, "seek": 140404, "start": 1424.36, "end": 1426.8799999999999, "text": " So when you're in factory infrastructure,", "tokens": [407, 562, 291, 434, 294, 9265, 6896, 11], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 234, "seek": 140404, "start": 1426.8799999999999, "end": 1430.48, "text": " you might have to throw out all your tasks", "tokens": [291, 1062, 362, 281, 3507, 484, 439, 428, 9608], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 235, "seek": 140404, "start": 1430.48, "end": 1432.44, "text": " because you don't have the database", "tokens": [570, 291, 500, 380, 362, 264, 8149], "temperature": 0.0, "avg_logprob": -0.560110074979765, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.689531147480011}, {"id": 236, "seek": 143244, "start": 1432.44, "end": 1436.52, "text": " that's a good question as well.", "tokens": [300, 311, 257, 665, 1168, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 237, "seek": 143244, "start": 1436.52, "end": 1441.1200000000001, "text": " I think the logical solution would be,", "tokens": [286, 519, 264, 14978, 3827, 576, 312, 11], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 238, "seek": 143244, "start": 1441.1200000000001, "end": 1444.52, "text": " trace test is just mapping out your infra.", "tokens": [13508, 1500, 307, 445, 18350, 484, 428, 23654, 13], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 239, "seek": 143244, "start": 1444.52, "end": 1445.96, "text": " So if you're using it,", "tokens": [407, 498, 291, 434, 1228, 309, 11], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 240, "seek": 143244, "start": 1445.96, "end": 1448.72, "text": " you can also use it just to gain visibility.", "tokens": [291, 393, 611, 764, 309, 445, 281, 6052, 19883, 13], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 241, "seek": 143244, "start": 1448.72, "end": 1451.64, "text": " So it doesn't have to be that it's only focused", "tokens": [407, 309, 1177, 380, 362, 281, 312, 300, 309, 311, 787, 5178], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 242, "seek": 143244, "start": 1451.64, "end": 1452.68, "text": " on the testing.", "tokens": [322, 264, 4997, 13], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 243, "seek": 143244, "start": 1452.68, "end": 1454.56, "text": " If you're using it to map out your infra,", "tokens": [759, 291, 434, 1228, 309, 281, 4471, 484, 428, 23654, 11], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 244, "seek": 143244, "start": 1454.56, "end": 1455.96, "text": " even if you have changes,", "tokens": [754, 498, 291, 362, 2962, 11], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 245, "seek": 143244, "start": 1455.96, "end": 1457.28, "text": " if you're running the test again,", "tokens": [498, 291, 434, 2614, 264, 1500, 797, 11], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 246, "seek": 143244, "start": 1457.28, "end": 1459.28, "text": " you'll exactly know what changed.", "tokens": [291, 603, 2293, 458, 437, 3105, 13], "temperature": 0.0, "avg_logprob": -0.16628730506227726, "compression_ratio": 1.7162162162162162, "no_speech_prob": 2.794947795337066e-05}, {"id": 247, "seek": 145928, "start": 1459.28, "end": 1463.48, "text": " So if you're running assertions based on one database table,", "tokens": [407, 498, 291, 434, 2614, 19810, 626, 2361, 322, 472, 8149, 3199, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 248, "seek": 145928, "start": 1463.48, "end": 1466.28, "text": " so to say, and then running an API on one endpoint", "tokens": [370, 281, 584, 11, 293, 550, 2614, 364, 9362, 322, 472, 35795], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 249, "seek": 145928, "start": 1466.28, "end": 1468.8799999999999, "text": " that has one particular host name,", "tokens": [300, 575, 472, 1729, 3975, 1315, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 250, "seek": 145928, "start": 1468.8799999999999, "end": 1471.32, "text": " if you change those up, you'll see what fails", "tokens": [498, 291, 1319, 729, 493, 11, 291, 603, 536, 437, 18199], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 251, "seek": 145928, "start": 1471.32, "end": 1472.32, "text": " and you can figure out, oh, okay,", "tokens": [293, 291, 393, 2573, 484, 11, 1954, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 252, "seek": 145928, "start": 1472.32, "end": 1475.76, "text": " so we changed that last week because of XYZ", "tokens": [370, 321, 3105, 300, 1036, 1243, 570, 295, 48826, 57], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 253, "seek": 145928, "start": 1475.76, "end": 1477.52, "text": " and you can know exactly what changed.", "tokens": [293, 291, 393, 458, 2293, 437, 3105, 13], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 254, "seek": 145928, "start": 1477.52, "end": 1481.84, "text": " So I think the overview, the visibility into your system,", "tokens": [407, 286, 519, 264, 12492, 11, 264, 19883, 666, 428, 1185, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 255, "seek": 145928, "start": 1481.84, "end": 1483.24, "text": " because when you're running microservices,", "tokens": [570, 562, 291, 434, 2614, 15547, 47480, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 256, "seek": 145928, "start": 1483.24, "end": 1485.52, "text": " when you're running a bunch of stuff,", "tokens": [562, 291, 434, 2614, 257, 3840, 295, 1507, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 257, "seek": 145928, "start": 1485.52, "end": 1487.16, "text": " distributed systems, whatever,", "tokens": [12631, 3652, 11, 2035, 11], "temperature": 0.0, "avg_logprob": -0.0943077146544937, "compression_ratio": 1.7481751824817517, "no_speech_prob": 1.3625758583657444e-05}, {"id": 258, "seek": 148716, "start": 1487.16, "end": 1490.48, "text": " it's just hard to have a mental model,", "tokens": [309, 311, 445, 1152, 281, 362, 257, 4973, 2316, 11], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 259, "seek": 148716, "start": 1490.48, "end": 1492.96, "text": " a mind map, so to say, of everything that's happening.", "tokens": [257, 1575, 4471, 11, 370, 281, 584, 11, 295, 1203, 300, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 260, "seek": 148716, "start": 1492.96, "end": 1496.0800000000002, "text": " So I think that's a good part of the value there as well.", "tokens": [407, 286, 519, 300, 311, 257, 665, 644, 295, 264, 2158, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 261, "seek": 148716, "start": 1497.4, "end": 1499.2, "text": " Thank you, no more time.", "tokens": [1044, 291, 11, 572, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 262, "seek": 148716, "start": 1499.2, "end": 1500.0400000000002, "text": " No more time, yeah.", "tokens": [883, 544, 565, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 263, "seek": 148716, "start": 1500.0400000000002, "end": 1500.88, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 264, "seek": 148716, "start": 1500.88, "end": 1501.72, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 265, "seek": 148716, "start": 1501.72, "end": 1502.5600000000002, "text": " No more time.", "tokens": [883, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 266, "seek": 148716, "start": 1502.5600000000002, "end": 1503.4, "text": " No more time, yeah.", "tokens": [883, 544, 565, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 267, "seek": 148716, "start": 1503.4, "end": 1504.24, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2166232276208622, "compression_ratio": 1.8639455782312926, "no_speech_prob": 3.956207729061134e-05}, {"id": 268, "seek": 150424, "start": 1504.24, "end": 1519.24, "text": " Thank you, no more time, yeah, no more time, yeah, no more time.", "tokens": [50364, 1044, 291, 11, 572, 544, 565, 11, 1338, 11, 572, 544, 565, 11, 1338, 11, 572, 544, 565, 13, 51114], "temperature": 0.0, "avg_logprob": -0.8337135314941406, "compression_ratio": 1.6, "no_speech_prob": 0.0008690308895893395}], "language": "en"}