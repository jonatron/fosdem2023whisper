{"text": " Hi, everyone, and welcome to our talk about operator monitoring and how to do it correctly. My name is Shirley. I work at Red Hat. I'm Jean Villassa. I also work at Red Hat for about one year and a half. So today we're going to talk about operators' observability, Kubernetes operators, and we're going to talk about when to start, the maturity levels of metrics, why we want to monitor, what we want to monitor, and the best practices and code examples that we created for it. So when we want to talk about, when should we start to think about the observability for operators? You can see here in the chart the life cycle of creating an operator, which is starting in basic installation, and the most mature step is autopilot. So when do you think we should start thinking about observability for a new operator? Anyone? When? From the start. From the start. That's correct. Really deep insights, talks about metrics, alerts, which is being able to monitor your operator fully. And people think maybe we should start thinking about it in full life cycle. Maybe that's the case. But you should pretty much start at the beginning, because the metrics that you are adding first are usually not the metrics that are for your users. They are internal. There are a few steps for the maturity of metrics. The first step is initial. You start with your operator, you want to understand how it works, if it works correctly. So the developers start to add hot metrics. I've been working for a few years on an operator in Red Hat called Qvert. And when I joined the project, it was already in the life cycle phase, full life cycle. And when I joined, already a lot of metrics were implemented in this operator. The problem was that there was no, the developers that added the metrics didn't fall best practices. And a lot of the metrics, it was hard to understand which metrics were ours. It's important to understand that your operator is not the only one inside of the Kubernetes system. So when someone, when a user or even other developers want to understand which metrics your operator is exposing, it should be easy for them to identify your metrics. So the first step, as I said, is initial. The second step is basic monitoring. You start adding your monitoring, and you're starting to think about your users, what they want to understand about your operator. And the third step is you have a process for implementing metrics and new metrics, and you are focused about health and performance for your operator. And the last step is actually autopilot. Taking those metrics and doing smart actions with them in order to do stuff like auto healing and auto scaling for your operator. And this is the part that we are actually on in our operator. So as Shirley said, when we first start, we look very much at internal metrics for the operators themselves. So at this point, we might start, for example, looking at the health of the operator. For example, can it connect to the Kubernetes API, or if it's using external resources, can it connect to those providers' API? Is it experiencing any errors? So we can also start by looking at, for example, its behavior. How often is the operator reconciling? What actions is the operator performing? So this is the kind of stuff that, as we are developing, we are very interested in. But we should start, as Shirley said, thinking more in the future about having these good standards, because later we will not be only tracking these, and could also be, like, resource metrics. And then why should then, why operator observability, and what are the steps that we'll be taking? So starting from the performance and health, here we want to detect the issues that come up early. We try to, obviously, reduce both operator and application downtime, and try to detect some regressions that might happen. Also we can start looking at, for example, planning and billing to improve planification, to also improve profitability, or then build users. At this point, we start looking more at infrastructure metrics also. For example, we want to track resource utilization. This might be, like, CPU, memory, this, and we can also start looking at the health of the infrastructure itself, maybe hardware failures, or trying to detect some network issues. Then we also start looking at, use these metrics to create alerts, to send notifications about the problems that come up as early as possible. So we obviously want to take appropriate actions to not let them go around. And after this, at this point, we go into more detail about metrics. Maybe we start looking at application metrics. So what's the availability of our application? What's the time? What's the error rates? And also its behavior. What type of request is the application receiving? What types of responses is sending? And it's important to monitor all of these things. And when we start building up all this information, then at a certain point in time, as Shirley said, we'll be able to give, like, this new life to the operator by having the autopilot capabilities, such as auto scaling, auto wheeling capabilities. Because at this point, if we did everything correctly, you'll be able to know, like, almost all the states that we are in. And we also start looking at metrics functionality metrics. We can provide the expected, are we providing the expected functionality to users? For example, checking that application features are working correctly. We want to see if there are any performance or reliability issues by checking service levels, and that everything is, it's working in the expected way by checking response to the airhorse and the data that it's responding to. Okay. So I hope you are convinced that the observability is important. If you are in this room, I guess you are. And for the past three years, we've been working on observability on our operator. What's important to understand is that our operator is considerably complex. It has a few sub-operators that it's managing. And each sub-operator has its own team, dedicated team, that is maintaining it. And having the insight of looking at those teams working on implementing observability, each team separately gave us a higher level of the possibility of understanding the pitfalls that they all share when implementing monitoring. So we decided to contribute from our knowledge of how to do this correctly in order for others not to do the same, to fall to the same pitfalls as us. So we decided to create best practices and to share with the community our findings. We hope to shorten the onboarding time for others and to create better documentation and to create reusable code for others to be able to use and save time and money, of course. So we reached out to the operator framework SDK team to collaborate with them and to publish there our best practices. As you can see here, this is the operator observability best practices. The operator SDK itself is the first step when someone wants to create a new operator. It gives them tools, how to create it easily, how to build, test the packages, and provides best practices for all steps of the operator life cycle. So we found that this was the best place for others to also go for monitoring. And in these best practices, I will now share with you a few examples. It may sound simple, but simple things have a big impact, both on the users that are using the system and both on developers that are trying to work with the metrics. So for example, a naming convention for metrics. One of the things that is mentioned in the document is having a name prefix for your metrics. This is very simple action that will help you identify, that will help the developers, the users to identify that the metrics are coming from the specific operator or a company. In this case, you can see that all of the metrics here have a cube width prefix, a cube width, as I said, has sub-operators. So under this prefix, we also have a sub-prefix for each individual operator, a CDI network and so on. And this is another example, which does not have this prefix. We can see here a container CPU, for example, prefix, but we can't understand where it's coming from. In this case, it's the advisor. But if you're a user and you're trying to understand where this metric came from, it's very hard, and also you cannot search in Grafana, for example, for all of the C-advisor metrics together. So that's a problem. Another thing that is mentioned in the best practices is about help text. Each metric has a dedicated place to add the help for this metric. And as you can see in Grafana and other visualization tools, the user will be able to see when hovering on the metrics, the description of it. It's very important because if not, you need to go somewhere else to search for it. Also this gives you the ability to create auto-generated documentation for all of your metrics in your site. Another example is the base units. So Prometheus recommends using base units for metrics. For example, you can see here for time to use seconds, not milliseconds, temperature, Celsius, not Fahrenheit, this gives the users a fluent experience when they are using the metrics, they don't need to do conversions, deviations of the data, and they are saying if you want to use milliseconds, use a floating point number. This removes the concern of magnitude of the number, and Grafana can handle it, and it will still show you the same precision, but the consistency in the UI and how to use the metrics will stay the same. Here you can see an example for metrics that are using seconds. And here we see that each CD are not using it. So this is not as recommended, and we would actually recommend to switch it, but they started with milliseconds. And now doing the change will cause issues with the UI that is based on it and everything. So it's a problem to change the names of the metrics once they are created. So when I joined the operator, we didn't have name prefixes. I tried to understand which metrics are ours and which are not, it was very hard. So we needed to go and do breaking changes for the metrics and add those prefixes, change the units, and this is what we want others to be able to avoid, this duplicate of work. Additional information in the best practices is about alerts. This is an example of an alert. You can see here that we have the alert name. We have an expression which is based on a metric, and once the expression is met, the alert either starts firing or is in pending state until the evaluation time. There is a description. There is also a possibility to add a summary. This is the evaluation time. It has a severity. And a link to a runbook URL. There could be other information that you can add to it, but this is the basic. And what we're saying in the best practice is that there's supposed to be, for example, for the labels of severity, there should only be three valid options, critical, warning, and info alerts. If you're using something else, it would be problematic. You can see here in this example, I don't know if you're seeing it, but we see that this is our example in the cluster. We have info, warning, and critical, and we have one non-severity, which is the watchdog. It's part of Prometheus alerts. It's just making sure that the alerts are working as expected. It should always stay one. There should never be alerts that don't have severity. And this is a bad example of using a severity label. In this case, they are using major instead of critical. The impact of that is that if someone is setting up alert manager to notify the support team that something critical happened to the system, and they were to get notified by Slack or by a pager, they will miss out on this alert because it doesn't meet with the convention of severities, values for severities. So what we have at the moment for best practices, we have for a metrics naming convention. We have how to create documentation for metrics, alerts, information about alert labels, run books. By the way, run books are a way to provide more information about the alert. You have a link in the alert where you can send the user to go and find more details. What's it about? What's the impact? How to diagnose it? And how to mitigate the issue. And then additional information about how to test metrics and how to test alerts. We plan to enrich this information, add information about dashboards, logging events, tracing in the future. So Shirley gave an overview about an eye-level situation about metrics and alerts. But how do we translate some of these best practices into code? So one of the problems that we faced was that logic code and monitoring code were becoming very intertwined. Code like this becomes harder to maintain. Obviously it becomes more difficult in understanding what the code does and to modify it. This leads obviously to longer development times, potential bugs, and it's also more challenging to onboard new team members or to contribute to one of these projects. In this specific snippet, there was like 16.4% of monitoring code intertwined with migration logic code. So what we did was try to refactor this code to try to separate these concerns, one from the other. In this specific case, we used a Prometheus collector that's just iterating the existing virtual machines migrations, and then it's just pushing the metrics according to the status of the virtual machines, whether they are successful or not, or the accounts of the pending schedule and running migrations. And obviously this snippet is much easier to understand how the monitoring is being done, and we take all of this out of the migration logic code. And to help other developers that are starting to avoid the same mistakes as we had to solve, we are creating a monitoring example in the memcached operator. We already have an initial example that is already thinking about all these concerns in separation between logic code and monitoring code. Our idea with this example is to make it as clear as possible, especially this is especially important when we are working with large and complex code bases, also make it more modular. It's easier to understand both the logic code and the monitoring code without affecting each other's functionality in the application in general, also make it more reusable. Since like, for example, the way we are doing monitoring in different operators will always be more or less the same. So if we find a more or less common way to do this, it will make it easier to reuse this code in other applications and projects, which will save them time and effort. And also, it will become more performant. If we mix all the monitoring concerns with the migration code, it's trivial that the time it will take to make a migration will take longer because we are calculating metric values and doing some Prometheus operations while we are trying to calculate the state of a migration. So having this separation will also help these questions. Our idea for the structure of the code will be by creating a package. And for example, here we can see a migration example, a central place where we will be registering all migrations and all migrations, sorry, no, all metrics, obviously, and then we will have files that will separate these metrics by their types. For example, in this example, you can see one operator metrics file, which will have all the operator-related metrics, as we talked in the beginning, and then we could have one specific file only for the migration metrics and then register them in one place. And why do we think about this structure and what benefits could this bring us? The first one is to automate the metric and the alert code generation. As we saw, much of the work that a developer needs to do that, it's like creating a file with a specific name, then go to the metrics.go file and register that file there. So this is very structured and always the same. It will be easier to automate and then allow developers to have a command line tool to generate new metrics and generate new alerts easier. We are also looking forward to create a linter for the metrics name. As Shirley said, a lot of the concerns that happen when operators are becoming more advanced is looking back at the metrics and see everything we did wrong with their naming. And even, as she said, it's a simple change, but can have a lot of impact. So a linter that follows all these conventions will also be important. Also automated metrics documentations, we are already doing this. And one thing that we faced was that a lot of metrics were very scattered in the code. So it was easy to automate and find all of them. And with a structure like the previous one, it will be even more easier to create a full list of metrics and that description that will help both developers, newcomers, and users. And lastly, have an easier structure for both unit and end-to-end testing, because if we have, like, this clear structure for where the metrics are, we can test there and test exactly those functions and not code intertwined in logic code. And just to conclude, if you are starting to create an operator or if you already have an operator, we invite you to go and to look at the operator SDK, to look at the best practices, to try to avoid the pitfalls that we had. And I really hope it will help you. And you should really just consider that when you're creating a new operator, it starts small, but it can become really robust. And you cannot tell that in the beginning. So think ahead and try to build it correctly from the beginning. I hope it will be helpful for you. And thank you. Thank you. Do you have any recommendations on how you would log out the decision points within your operator? So if you wanted to retrospectively see why it's done certain things, like the decision points, how it's decided which Kubernetes API calls to make, if your operator did something crazy and you wanted to look back and see why it did that, is there anything you would do in advance to the logging? I think this is the summary of what we've learned is in these documents. Because as I said, for example, the developers that started this project, they didn't have where to go and the best practices of how to name a metric. So they just named it how they thought. And they did follow the Prometheus recommendations, but having a prefix of the operator has a big impact for the users. And not even the users. When we are trying to understand how to use internal metrics for our uses, we also are struggling to understand where a metric came from, where is the code for it. So all of the summary of what we've learned is in those documents, and we plan to enrich it even further. Thank you for your talk. It was very interesting. You mentioned code generation for the metrics package. My question is, do you plan on adding that to QBuilder and the operator SDK? Yeah, basically we are working on the operator SDK right now, because we want to build all these tools, and we are thinking about them, but obviously this needs a lot of help of the community. And I am saying this because I'll enter like a personal note here and an idea, right? Because the way I see it is like on QBuilder and on operator SDK, being able to, you just go there and you say that you want to generate a project with monitoring, and it creates the monitoring package. Or if the operator already exists, you have a command to generate the monitoring package, and then on QBuilder, like you use it to create an API or a controller, you'll have a similar command, but to create a new metric. And you pass the type of the metric, the help, and the same for alerts. At least that's the way I see it. And for me, so it makes sense. I agree. Thank you. Hey, thank you for the talk. How much of a conventions that you talked about, aligned with open telemetry, is my opinion? How much? What? Aligned with open telemetry. Most of them are aligned with open telemetry, actually. But these are specific for operators. That's the idea. The idea is that you have a central place where you can get the information. And by the way, if someone is creating a new operator and has an insight, we encourage others to contribute to the documentation and teach others and share the information. So yeah. Basically, I think we align with open telemetry conventions, but we add more ideas to it to operate. I think that's it. Thank you. Thank you. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.92, "text": " Hi, everyone, and welcome to our talk about operator monitoring and how to do it correctly.", "tokens": [2421, 11, 1518, 11, 293, 2928, 281, 527, 751, 466, 12973, 11028, 293, 577, 281, 360, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.32309449103570753, "compression_ratio": 1.4, "no_speech_prob": 0.27195724844932556}, {"id": 1, "seek": 0, "start": 12.92, "end": 14.0, "text": " My name is Shirley.", "tokens": [1222, 1315, 307, 43275, 13], "temperature": 0.0, "avg_logprob": -0.32309449103570753, "compression_ratio": 1.4, "no_speech_prob": 0.27195724844932556}, {"id": 2, "seek": 0, "start": 14.0, "end": 17.400000000000002, "text": " I work at Red Hat.", "tokens": [286, 589, 412, 4477, 15867, 13], "temperature": 0.0, "avg_logprob": -0.32309449103570753, "compression_ratio": 1.4, "no_speech_prob": 0.27195724844932556}, {"id": 3, "seek": 0, "start": 17.400000000000002, "end": 18.400000000000002, "text": " I'm Jean Villassa.", "tokens": [286, 478, 13854, 14244, 18948, 13], "temperature": 0.0, "avg_logprob": -0.32309449103570753, "compression_ratio": 1.4, "no_speech_prob": 0.27195724844932556}, {"id": 4, "seek": 0, "start": 18.400000000000002, "end": 25.16, "text": " I also work at Red Hat for about one year and a half.", "tokens": [286, 611, 589, 412, 4477, 15867, 337, 466, 472, 1064, 293, 257, 1922, 13], "temperature": 0.0, "avg_logprob": -0.32309449103570753, "compression_ratio": 1.4, "no_speech_prob": 0.27195724844932556}, {"id": 5, "seek": 2516, "start": 25.16, "end": 33.08, "text": " So today we're going to talk about operators' observability, Kubernetes operators, and we're", "tokens": [407, 965, 321, 434, 516, 281, 751, 466, 19077, 6, 9951, 2310, 11, 23145, 19077, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.15046165928696142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0001749462098814547}, {"id": 6, "seek": 2516, "start": 33.08, "end": 43.8, "text": " going to talk about when to start, the maturity levels of metrics, why we want to monitor,", "tokens": [516, 281, 751, 466, 562, 281, 722, 11, 264, 28874, 4358, 295, 16367, 11, 983, 321, 528, 281, 6002, 11], "temperature": 0.0, "avg_logprob": -0.15046165928696142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0001749462098814547}, {"id": 7, "seek": 2516, "start": 43.8, "end": 54.28, "text": " what we want to monitor, and the best practices and code examples that we created for it.", "tokens": [437, 321, 528, 281, 6002, 11, 293, 264, 1151, 7525, 293, 3089, 5110, 300, 321, 2942, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.15046165928696142, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0001749462098814547}, {"id": 8, "seek": 5428, "start": 54.28, "end": 63.44, "text": " So when we want to talk about, when should we start to think about the observability", "tokens": [407, 562, 321, 528, 281, 751, 466, 11, 562, 820, 321, 722, 281, 519, 466, 264, 9951, 2310], "temperature": 0.0, "avg_logprob": -0.15713600958547286, "compression_ratio": 1.5276073619631902, "no_speech_prob": 5.8771769545273855e-05}, {"id": 9, "seek": 5428, "start": 63.44, "end": 65.6, "text": " for operators?", "tokens": [337, 19077, 30], "temperature": 0.0, "avg_logprob": -0.15713600958547286, "compression_ratio": 1.5276073619631902, "no_speech_prob": 5.8771769545273855e-05}, {"id": 10, "seek": 5428, "start": 65.6, "end": 73.72, "text": " You can see here in the chart the life cycle of creating an operator, which is starting", "tokens": [509, 393, 536, 510, 294, 264, 6927, 264, 993, 6586, 295, 4084, 364, 12973, 11, 597, 307, 2891], "temperature": 0.0, "avg_logprob": -0.15713600958547286, "compression_ratio": 1.5276073619631902, "no_speech_prob": 5.8771769545273855e-05}, {"id": 11, "seek": 5428, "start": 73.72, "end": 80.6, "text": " in basic installation, and the most mature step is autopilot.", "tokens": [294, 3875, 13260, 11, 293, 264, 881, 14442, 1823, 307, 31090, 31516, 13], "temperature": 0.0, "avg_logprob": -0.15713600958547286, "compression_ratio": 1.5276073619631902, "no_speech_prob": 5.8771769545273855e-05}, {"id": 12, "seek": 8060, "start": 80.6, "end": 87.44, "text": " So when do you think we should start thinking about observability for a new operator?", "tokens": [407, 562, 360, 291, 519, 321, 820, 722, 1953, 466, 9951, 2310, 337, 257, 777, 12973, 30], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 13, "seek": 8060, "start": 87.44, "end": 88.44, "text": " Anyone?", "tokens": [14643, 30], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 14, "seek": 8060, "start": 88.44, "end": 89.44, "text": " When?", "tokens": [1133, 30], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 15, "seek": 8060, "start": 89.44, "end": 93.56, "text": " From the start.", "tokens": [3358, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 16, "seek": 8060, "start": 93.56, "end": 94.56, "text": " From the start.", "tokens": [3358, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 17, "seek": 8060, "start": 94.56, "end": 97.6, "text": " That's correct.", "tokens": [663, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.32257019205296295, "compression_ratio": 1.2894736842105263, "no_speech_prob": 7.519371865782887e-05}, {"id": 18, "seek": 9760, "start": 97.6, "end": 111.39999999999999, "text": " Really deep insights, talks about metrics, alerts, which is being able to monitor your", "tokens": [4083, 2452, 14310, 11, 6686, 466, 16367, 11, 28061, 11, 597, 307, 885, 1075, 281, 6002, 428], "temperature": 0.0, "avg_logprob": -0.19544234745939013, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001774218399077654}, {"id": 19, "seek": 9760, "start": 111.39999999999999, "end": 112.39999999999999, "text": " operator fully.", "tokens": [12973, 4498, 13], "temperature": 0.0, "avg_logprob": -0.19544234745939013, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001774218399077654}, {"id": 20, "seek": 9760, "start": 112.39999999999999, "end": 117.32, "text": " And people think maybe we should start thinking about it in full life cycle.", "tokens": [400, 561, 519, 1310, 321, 820, 722, 1953, 466, 309, 294, 1577, 993, 6586, 13], "temperature": 0.0, "avg_logprob": -0.19544234745939013, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001774218399077654}, {"id": 21, "seek": 9760, "start": 117.32, "end": 118.32, "text": " Maybe that's the case.", "tokens": [2704, 300, 311, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.19544234745939013, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001774218399077654}, {"id": 22, "seek": 9760, "start": 118.32, "end": 126.52, "text": " But you should pretty much start at the beginning, because the metrics that you are adding first", "tokens": [583, 291, 820, 1238, 709, 722, 412, 264, 2863, 11, 570, 264, 16367, 300, 291, 366, 5127, 700], "temperature": 0.0, "avg_logprob": -0.19544234745939013, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001774218399077654}, {"id": 23, "seek": 12652, "start": 126.52, "end": 131.4, "text": " are usually not the metrics that are for your users.", "tokens": [366, 2673, 406, 264, 16367, 300, 366, 337, 428, 5022, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 24, "seek": 12652, "start": 131.4, "end": 132.4, "text": " They are internal.", "tokens": [814, 366, 6920, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 25, "seek": 12652, "start": 132.4, "end": 136.76, "text": " There are a few steps for the maturity of metrics.", "tokens": [821, 366, 257, 1326, 4439, 337, 264, 28874, 295, 16367, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 26, "seek": 12652, "start": 136.76, "end": 138.28, "text": " The first step is initial.", "tokens": [440, 700, 1823, 307, 5883, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 27, "seek": 12652, "start": 138.28, "end": 143.88, "text": " You start with your operator, you want to understand how it works, if it works correctly.", "tokens": [509, 722, 365, 428, 12973, 11, 291, 528, 281, 1223, 577, 309, 1985, 11, 498, 309, 1985, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 28, "seek": 12652, "start": 143.88, "end": 150.96, "text": " So the developers start to add hot metrics.", "tokens": [407, 264, 8849, 722, 281, 909, 2368, 16367, 13], "temperature": 0.0, "avg_logprob": -0.16358648764120565, "compression_ratio": 1.6453488372093024, "no_speech_prob": 7.316176925087348e-05}, {"id": 29, "seek": 15096, "start": 150.96, "end": 157.72, "text": " I've been working for a few years on an operator in Red Hat called Qvert.", "tokens": [286, 600, 668, 1364, 337, 257, 1326, 924, 322, 364, 12973, 294, 4477, 15867, 1219, 1249, 3281, 13], "temperature": 0.0, "avg_logprob": -0.18402625668433406, "compression_ratio": 1.518987341772152, "no_speech_prob": 3.407971962587908e-05}, {"id": 30, "seek": 15096, "start": 157.72, "end": 167.12, "text": " And when I joined the project, it was already in the life cycle phase, full life cycle.", "tokens": [400, 562, 286, 6869, 264, 1716, 11, 309, 390, 1217, 294, 264, 993, 6586, 5574, 11, 1577, 993, 6586, 13], "temperature": 0.0, "avg_logprob": -0.18402625668433406, "compression_ratio": 1.518987341772152, "no_speech_prob": 3.407971962587908e-05}, {"id": 31, "seek": 15096, "start": 167.12, "end": 172.08, "text": " And when I joined, already a lot of metrics were implemented in this operator.", "tokens": [400, 562, 286, 6869, 11, 1217, 257, 688, 295, 16367, 645, 12270, 294, 341, 12973, 13], "temperature": 0.0, "avg_logprob": -0.18402625668433406, "compression_ratio": 1.518987341772152, "no_speech_prob": 3.407971962587908e-05}, {"id": 32, "seek": 17208, "start": 172.08, "end": 182.60000000000002, "text": " The problem was that there was no, the developers that added the metrics didn't fall best practices.", "tokens": [440, 1154, 390, 300, 456, 390, 572, 11, 264, 8849, 300, 3869, 264, 16367, 994, 380, 2100, 1151, 7525, 13], "temperature": 0.0, "avg_logprob": -0.15020145069469104, "compression_ratio": 1.5977011494252873, "no_speech_prob": 6.874166865600273e-05}, {"id": 33, "seek": 17208, "start": 182.60000000000002, "end": 189.16000000000003, "text": " And a lot of the metrics, it was hard to understand which metrics were ours.", "tokens": [400, 257, 688, 295, 264, 16367, 11, 309, 390, 1152, 281, 1223, 597, 16367, 645, 11896, 13], "temperature": 0.0, "avg_logprob": -0.15020145069469104, "compression_ratio": 1.5977011494252873, "no_speech_prob": 6.874166865600273e-05}, {"id": 34, "seek": 17208, "start": 189.16000000000003, "end": 195.8, "text": " It's important to understand that your operator is not the only one inside of the Kubernetes", "tokens": [467, 311, 1021, 281, 1223, 300, 428, 12973, 307, 406, 264, 787, 472, 1854, 295, 264, 23145], "temperature": 0.0, "avg_logprob": -0.15020145069469104, "compression_ratio": 1.5977011494252873, "no_speech_prob": 6.874166865600273e-05}, {"id": 35, "seek": 17208, "start": 195.8, "end": 197.04000000000002, "text": " system.", "tokens": [1185, 13], "temperature": 0.0, "avg_logprob": -0.15020145069469104, "compression_ratio": 1.5977011494252873, "no_speech_prob": 6.874166865600273e-05}, {"id": 36, "seek": 19704, "start": 197.04, "end": 202.88, "text": " So when someone, when a user or even other developers want to understand which metrics", "tokens": [407, 562, 1580, 11, 562, 257, 4195, 420, 754, 661, 8849, 528, 281, 1223, 597, 16367], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 37, "seek": 19704, "start": 202.88, "end": 210.16, "text": " your operator is exposing, it should be easy for them to identify your metrics.", "tokens": [428, 12973, 307, 33178, 11, 309, 820, 312, 1858, 337, 552, 281, 5876, 428, 16367, 13], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 38, "seek": 19704, "start": 210.16, "end": 213.84, "text": " So the first step, as I said, is initial.", "tokens": [407, 264, 700, 1823, 11, 382, 286, 848, 11, 307, 5883, 13], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 39, "seek": 19704, "start": 213.84, "end": 216.88, "text": " The second step is basic monitoring.", "tokens": [440, 1150, 1823, 307, 3875, 11028, 13], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 40, "seek": 19704, "start": 216.88, "end": 221.32, "text": " You start adding your monitoring, and you're starting to think about your users, what they", "tokens": [509, 722, 5127, 428, 11028, 11, 293, 291, 434, 2891, 281, 519, 466, 428, 5022, 11, 437, 436], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 41, "seek": 19704, "start": 221.32, "end": 225.79999999999998, "text": " want to understand about your operator.", "tokens": [528, 281, 1223, 466, 428, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1399765756395128, "compression_ratio": 1.724770642201835, "no_speech_prob": 3.072653271374293e-05}, {"id": 42, "seek": 22580, "start": 225.8, "end": 233.48000000000002, "text": " And the third step is you have a process for implementing metrics and new metrics, and you", "tokens": [400, 264, 2636, 1823, 307, 291, 362, 257, 1399, 337, 18114, 16367, 293, 777, 16367, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.1455127245759311, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00013233810022938997}, {"id": 43, "seek": 22580, "start": 233.48000000000002, "end": 237.12, "text": " are focused about health and performance for your operator.", "tokens": [366, 5178, 466, 1585, 293, 3389, 337, 428, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1455127245759311, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00013233810022938997}, {"id": 44, "seek": 22580, "start": 237.12, "end": 241.04000000000002, "text": " And the last step is actually autopilot.", "tokens": [400, 264, 1036, 1823, 307, 767, 31090, 31516, 13], "temperature": 0.0, "avg_logprob": -0.1455127245759311, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00013233810022938997}, {"id": 45, "seek": 22580, "start": 241.04000000000002, "end": 248.12, "text": " Taking those metrics and doing smart actions with them in order to do stuff like auto healing", "tokens": [17837, 729, 16367, 293, 884, 4069, 5909, 365, 552, 294, 1668, 281, 360, 1507, 411, 8399, 9745], "temperature": 0.0, "avg_logprob": -0.1455127245759311, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00013233810022938997}, {"id": 46, "seek": 22580, "start": 248.12, "end": 250.84, "text": " and auto scaling for your operator.", "tokens": [293, 8399, 21589, 337, 428, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1455127245759311, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00013233810022938997}, {"id": 47, "seek": 25084, "start": 250.84, "end": 259.84000000000003, "text": " And this is the part that we are actually on in our operator.", "tokens": [400, 341, 307, 264, 644, 300, 321, 366, 767, 322, 294, 527, 12973, 13], "temperature": 0.0, "avg_logprob": -0.13637172216656565, "compression_ratio": 1.6, "no_speech_prob": 0.0005730448756366968}, {"id": 48, "seek": 25084, "start": 259.84000000000003, "end": 266.6, "text": " So as Shirley said, when we first start, we look very much at internal metrics for the", "tokens": [407, 382, 43275, 848, 11, 562, 321, 700, 722, 11, 321, 574, 588, 709, 412, 6920, 16367, 337, 264], "temperature": 0.0, "avg_logprob": -0.13637172216656565, "compression_ratio": 1.6, "no_speech_prob": 0.0005730448756366968}, {"id": 49, "seek": 25084, "start": 266.6, "end": 268.04, "text": " operators themselves.", "tokens": [19077, 2969, 13], "temperature": 0.0, "avg_logprob": -0.13637172216656565, "compression_ratio": 1.6, "no_speech_prob": 0.0005730448756366968}, {"id": 50, "seek": 25084, "start": 268.04, "end": 272.88, "text": " So at this point, we might start, for example, looking at the health of the operator.", "tokens": [407, 412, 341, 935, 11, 321, 1062, 722, 11, 337, 1365, 11, 1237, 412, 264, 1585, 295, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.13637172216656565, "compression_ratio": 1.6, "no_speech_prob": 0.0005730448756366968}, {"id": 51, "seek": 25084, "start": 272.88, "end": 278.6, "text": " For example, can it connect to the Kubernetes API, or if it's using external resources,", "tokens": [1171, 1365, 11, 393, 309, 1745, 281, 264, 23145, 9362, 11, 420, 498, 309, 311, 1228, 8320, 3593, 11], "temperature": 0.0, "avg_logprob": -0.13637172216656565, "compression_ratio": 1.6, "no_speech_prob": 0.0005730448756366968}, {"id": 52, "seek": 27860, "start": 278.6, "end": 282.24, "text": " can it connect to those providers' API?", "tokens": [393, 309, 1745, 281, 729, 11330, 6, 9362, 30], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 53, "seek": 27860, "start": 282.24, "end": 284.76000000000005, "text": " Is it experiencing any errors?", "tokens": [1119, 309, 11139, 604, 13603, 30], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 54, "seek": 27860, "start": 284.76000000000005, "end": 289.84000000000003, "text": " So we can also start by looking at, for example, its behavior.", "tokens": [407, 321, 393, 611, 722, 538, 1237, 412, 11, 337, 1365, 11, 1080, 5223, 13], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 55, "seek": 27860, "start": 289.84000000000003, "end": 292.28000000000003, "text": " How often is the operator reconciling?", "tokens": [1012, 2049, 307, 264, 12973, 9993, 3208, 278, 30], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 56, "seek": 27860, "start": 292.28000000000003, "end": 294.44, "text": " What actions is the operator performing?", "tokens": [708, 5909, 307, 264, 12973, 10205, 30], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 57, "seek": 27860, "start": 294.44, "end": 299.08000000000004, "text": " So this is the kind of stuff that, as we are developing, we are very interested in.", "tokens": [407, 341, 307, 264, 733, 295, 1507, 300, 11, 382, 321, 366, 6416, 11, 321, 366, 588, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 58, "seek": 27860, "start": 299.08000000000004, "end": 307.36, "text": " But we should start, as Shirley said, thinking more in the future about having these good", "tokens": [583, 321, 820, 722, 11, 382, 43275, 848, 11, 1953, 544, 294, 264, 2027, 466, 1419, 613, 665], "temperature": 0.0, "avg_logprob": -0.15234037399291991, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0010831559775397182}, {"id": 59, "seek": 30736, "start": 307.36, "end": 313.68, "text": " standards, because later we will not be only tracking these, and could also be, like, resource", "tokens": [7787, 11, 570, 1780, 321, 486, 406, 312, 787, 11603, 613, 11, 293, 727, 611, 312, 11, 411, 11, 7684], "temperature": 0.0, "avg_logprob": -0.2663645995290656, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0017953224014490843}, {"id": 60, "seek": 30736, "start": 313.68, "end": 317.52000000000004, "text": " metrics.", "tokens": [16367, 13], "temperature": 0.0, "avg_logprob": -0.2663645995290656, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0017953224014490843}, {"id": 61, "seek": 30736, "start": 317.52000000000004, "end": 325.40000000000003, "text": " And then why should then, why operator observability, and what are the steps that we'll be taking?", "tokens": [400, 550, 983, 820, 550, 11, 983, 12973, 9951, 2310, 11, 293, 437, 366, 264, 4439, 300, 321, 603, 312, 1940, 30], "temperature": 0.0, "avg_logprob": -0.2663645995290656, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0017953224014490843}, {"id": 62, "seek": 30736, "start": 325.40000000000003, "end": 331.08000000000004, "text": " So starting from the performance and health, here we want to detect the issues that come", "tokens": [407, 2891, 490, 264, 3389, 293, 1585, 11, 510, 321, 528, 281, 5531, 264, 2663, 300, 808], "temperature": 0.0, "avg_logprob": -0.2663645995290656, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0017953224014490843}, {"id": 63, "seek": 30736, "start": 331.08000000000004, "end": 332.08000000000004, "text": " up early.", "tokens": [493, 2440, 13], "temperature": 0.0, "avg_logprob": -0.2663645995290656, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.0017953224014490843}, {"id": 64, "seek": 33208, "start": 332.08, "end": 339.32, "text": " We try to, obviously, reduce both operator and application downtime, and try to detect", "tokens": [492, 853, 281, 11, 2745, 11, 5407, 1293, 12973, 293, 3861, 49648, 11, 293, 853, 281, 5531], "temperature": 0.0, "avg_logprob": -0.18200786297137922, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0030925741884857416}, {"id": 65, "seek": 33208, "start": 339.32, "end": 342.24, "text": " some regressions that might happen.", "tokens": [512, 1121, 735, 626, 300, 1062, 1051, 13], "temperature": 0.0, "avg_logprob": -0.18200786297137922, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0030925741884857416}, {"id": 66, "seek": 33208, "start": 342.24, "end": 349.68, "text": " Also we can start looking at, for example, planning and billing to improve planification,", "tokens": [2743, 321, 393, 722, 1237, 412, 11, 337, 1365, 11, 5038, 293, 35618, 281, 3470, 1393, 3774, 11], "temperature": 0.0, "avg_logprob": -0.18200786297137922, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0030925741884857416}, {"id": 67, "seek": 33208, "start": 349.68, "end": 354.4, "text": " to also improve profitability, or then build users.", "tokens": [281, 611, 3470, 46249, 11, 420, 550, 1322, 5022, 13], "temperature": 0.0, "avg_logprob": -0.18200786297137922, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0030925741884857416}, {"id": 68, "seek": 33208, "start": 354.4, "end": 359.71999999999997, "text": " At this point, we start looking more at infrastructure metrics also.", "tokens": [1711, 341, 935, 11, 321, 722, 1237, 544, 412, 6896, 16367, 611, 13], "temperature": 0.0, "avg_logprob": -0.18200786297137922, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0030925741884857416}, {"id": 69, "seek": 35972, "start": 359.72, "end": 362.8, "text": " For example, we want to track resource utilization.", "tokens": [1171, 1365, 11, 321, 528, 281, 2837, 7684, 37074, 13], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 70, "seek": 35972, "start": 362.8, "end": 369.36, "text": " This might be, like, CPU, memory, this, and we can also start looking at the health of", "tokens": [639, 1062, 312, 11, 411, 11, 13199, 11, 4675, 11, 341, 11, 293, 321, 393, 611, 722, 1237, 412, 264, 1585, 295], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 71, "seek": 35972, "start": 369.36, "end": 374.8, "text": " the infrastructure itself, maybe hardware failures, or trying to detect some network", "tokens": [264, 6896, 2564, 11, 1310, 8837, 20774, 11, 420, 1382, 281, 5531, 512, 3209], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 72, "seek": 35972, "start": 374.8, "end": 376.16, "text": " issues.", "tokens": [2663, 13], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 73, "seek": 35972, "start": 376.16, "end": 383.68, "text": " Then we also start looking at, use these metrics to create alerts, to send notifications", "tokens": [1396, 321, 611, 722, 1237, 412, 11, 764, 613, 16367, 281, 1884, 28061, 11, 281, 2845, 13426], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 74, "seek": 35972, "start": 383.68, "end": 387.0, "text": " about the problems that come up as early as possible.", "tokens": [466, 264, 2740, 300, 808, 493, 382, 2440, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1637983004252116, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0022948759142309427}, {"id": 75, "seek": 38700, "start": 387.0, "end": 392.08, "text": " So we obviously want to take appropriate actions to not let them go around.", "tokens": [407, 321, 2745, 528, 281, 747, 6854, 5909, 281, 406, 718, 552, 352, 926, 13], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 76, "seek": 38700, "start": 392.08, "end": 397.0, "text": " And after this, at this point, we go into more detail about metrics.", "tokens": [400, 934, 341, 11, 412, 341, 935, 11, 321, 352, 666, 544, 2607, 466, 16367, 13], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 77, "seek": 38700, "start": 397.0, "end": 399.32, "text": " Maybe we start looking at application metrics.", "tokens": [2704, 321, 722, 1237, 412, 3861, 16367, 13], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 78, "seek": 38700, "start": 399.32, "end": 402.12, "text": " So what's the availability of our application?", "tokens": [407, 437, 311, 264, 17945, 295, 527, 3861, 30], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 79, "seek": 38700, "start": 402.12, "end": 403.12, "text": " What's the time?", "tokens": [708, 311, 264, 565, 30], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 80, "seek": 38700, "start": 403.12, "end": 404.68, "text": " What's the error rates?", "tokens": [708, 311, 264, 6713, 6846, 30], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 81, "seek": 38700, "start": 404.68, "end": 406.76, "text": " And also its behavior.", "tokens": [400, 611, 1080, 5223, 13], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 82, "seek": 38700, "start": 406.76, "end": 409.92, "text": " What type of request is the application receiving?", "tokens": [708, 2010, 295, 5308, 307, 264, 3861, 10040, 30], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 83, "seek": 38700, "start": 409.92, "end": 411.76, "text": " What types of responses is sending?", "tokens": [708, 3467, 295, 13019, 307, 7750, 30], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 84, "seek": 38700, "start": 411.76, "end": 415.08, "text": " And it's important to monitor all of these things.", "tokens": [400, 309, 311, 1021, 281, 6002, 439, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.1589925310253042, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0010852687992155552}, {"id": 85, "seek": 41508, "start": 415.08, "end": 421.59999999999997, "text": " And when we start building up all this information, then at a certain point in time, as Shirley", "tokens": [400, 562, 321, 722, 2390, 493, 439, 341, 1589, 11, 550, 412, 257, 1629, 935, 294, 565, 11, 382, 43275], "temperature": 0.0, "avg_logprob": -0.14608984495464125, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0028691203333437443}, {"id": 86, "seek": 41508, "start": 421.59999999999997, "end": 430.96, "text": " said, we'll be able to give, like, this new life to the operator by having the autopilot", "tokens": [848, 11, 321, 603, 312, 1075, 281, 976, 11, 411, 11, 341, 777, 993, 281, 264, 12973, 538, 1419, 264, 31090, 31516], "temperature": 0.0, "avg_logprob": -0.14608984495464125, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0028691203333437443}, {"id": 87, "seek": 41508, "start": 430.96, "end": 436.56, "text": " capabilities, such as auto scaling, auto wheeling capabilities.", "tokens": [10862, 11, 1270, 382, 8399, 21589, 11, 8399, 5589, 278, 10862, 13], "temperature": 0.0, "avg_logprob": -0.14608984495464125, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0028691203333437443}, {"id": 88, "seek": 41508, "start": 436.56, "end": 441.64, "text": " Because at this point, if we did everything correctly, you'll be able to know, like, almost", "tokens": [1436, 412, 341, 935, 11, 498, 321, 630, 1203, 8944, 11, 291, 603, 312, 1075, 281, 458, 11, 411, 11, 1920], "temperature": 0.0, "avg_logprob": -0.14608984495464125, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0028691203333437443}, {"id": 89, "seek": 41508, "start": 441.64, "end": 444.0, "text": " all the states that we are in.", "tokens": [439, 264, 4368, 300, 321, 366, 294, 13], "temperature": 0.0, "avg_logprob": -0.14608984495464125, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0028691203333437443}, {"id": 90, "seek": 44400, "start": 444.0, "end": 447.76, "text": " And we also start looking at metrics functionality metrics.", "tokens": [400, 321, 611, 722, 1237, 412, 16367, 14980, 16367, 13], "temperature": 0.0, "avg_logprob": -0.20712671162169657, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0020548615138977766}, {"id": 91, "seek": 44400, "start": 447.76, "end": 452.76, "text": " We can provide the expected, are we providing the expected functionality to users?", "tokens": [492, 393, 2893, 264, 5176, 11, 366, 321, 6530, 264, 5176, 14980, 281, 5022, 30], "temperature": 0.0, "avg_logprob": -0.20712671162169657, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0020548615138977766}, {"id": 92, "seek": 44400, "start": 452.76, "end": 456.84, "text": " For example, checking that application features are working correctly.", "tokens": [1171, 1365, 11, 8568, 300, 3861, 4122, 366, 1364, 8944, 13], "temperature": 0.0, "avg_logprob": -0.20712671162169657, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0020548615138977766}, {"id": 93, "seek": 44400, "start": 456.84, "end": 462.12, "text": " We want to see if there are any performance or reliability issues by checking service", "tokens": [492, 528, 281, 536, 498, 456, 366, 604, 3389, 420, 24550, 2663, 538, 8568, 2643], "temperature": 0.0, "avg_logprob": -0.20712671162169657, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0020548615138977766}, {"id": 94, "seek": 44400, "start": 462.12, "end": 468.92, "text": " levels, and that everything is, it's working in the expected way by checking response to", "tokens": [4358, 11, 293, 300, 1203, 307, 11, 309, 311, 1364, 294, 264, 5176, 636, 538, 8568, 4134, 281], "temperature": 0.0, "avg_logprob": -0.20712671162169657, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.0020548615138977766}, {"id": 95, "seek": 46892, "start": 468.92, "end": 476.0, "text": " the airhorse and the data that it's responding to.", "tokens": [264, 1988, 45079, 293, 264, 1412, 300, 309, 311, 16670, 281, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 96, "seek": 46892, "start": 476.0, "end": 477.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 97, "seek": 46892, "start": 477.0, "end": 480.40000000000003, "text": " So I hope you are convinced that the observability is important.", "tokens": [407, 286, 1454, 291, 366, 12561, 300, 264, 9951, 2310, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 98, "seek": 46892, "start": 480.40000000000003, "end": 484.20000000000005, "text": " If you are in this room, I guess you are.", "tokens": [759, 291, 366, 294, 341, 1808, 11, 286, 2041, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 99, "seek": 46892, "start": 484.20000000000005, "end": 489.8, "text": " And for the past three years, we've been working on observability on our operator.", "tokens": [400, 337, 264, 1791, 1045, 924, 11, 321, 600, 668, 1364, 322, 9951, 2310, 322, 527, 12973, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 100, "seek": 46892, "start": 489.8, "end": 494.20000000000005, "text": " What's important to understand is that our operator is considerably complex.", "tokens": [708, 311, 1021, 281, 1223, 307, 300, 527, 12973, 307, 31308, 3997, 13], "temperature": 0.0, "avg_logprob": -0.2482375871567499, "compression_ratio": 1.5911330049261083, "no_speech_prob": 0.0008752304129302502}, {"id": 101, "seek": 49420, "start": 494.2, "end": 499.47999999999996, "text": " It has a few sub-operators that it's managing.", "tokens": [467, 575, 257, 1326, 1422, 12, 7192, 3391, 300, 309, 311, 11642, 13], "temperature": 0.0, "avg_logprob": -0.1497193769975142, "compression_ratio": 1.4965034965034965, "no_speech_prob": 9.777270315680653e-05}, {"id": 102, "seek": 49420, "start": 499.47999999999996, "end": 508.0, "text": " And each sub-operator has its own team, dedicated team, that is maintaining it.", "tokens": [400, 1184, 1422, 12, 7192, 1639, 575, 1080, 1065, 1469, 11, 8374, 1469, 11, 300, 307, 14916, 309, 13], "temperature": 0.0, "avg_logprob": -0.1497193769975142, "compression_ratio": 1.4965034965034965, "no_speech_prob": 9.777270315680653e-05}, {"id": 103, "seek": 49420, "start": 508.0, "end": 516.2, "text": " And having the insight of looking at those teams working on implementing observability,", "tokens": [400, 1419, 264, 11269, 295, 1237, 412, 729, 5491, 1364, 322, 18114, 9951, 2310, 11], "temperature": 0.0, "avg_logprob": -0.1497193769975142, "compression_ratio": 1.4965034965034965, "no_speech_prob": 9.777270315680653e-05}, {"id": 104, "seek": 51620, "start": 516.2, "end": 524.32, "text": " each team separately gave us a higher level of the possibility of understanding the pitfalls", "tokens": [1184, 1469, 14759, 2729, 505, 257, 2946, 1496, 295, 264, 7959, 295, 3701, 264, 10147, 18542], "temperature": 0.0, "avg_logprob": -0.1077716210309197, "compression_ratio": 1.5837837837837838, "no_speech_prob": 9.302610123995692e-05}, {"id": 105, "seek": 51620, "start": 524.32, "end": 529.6400000000001, "text": " that they all share when implementing monitoring.", "tokens": [300, 436, 439, 2073, 562, 18114, 11028, 13], "temperature": 0.0, "avg_logprob": -0.1077716210309197, "compression_ratio": 1.5837837837837838, "no_speech_prob": 9.302610123995692e-05}, {"id": 106, "seek": 51620, "start": 529.6400000000001, "end": 536.6400000000001, "text": " So we decided to contribute from our knowledge of how to do this correctly in order for others", "tokens": [407, 321, 3047, 281, 10586, 490, 527, 3601, 295, 577, 281, 360, 341, 8944, 294, 1668, 337, 2357], "temperature": 0.0, "avg_logprob": -0.1077716210309197, "compression_ratio": 1.5837837837837838, "no_speech_prob": 9.302610123995692e-05}, {"id": 107, "seek": 51620, "start": 536.6400000000001, "end": 542.96, "text": " not to do the same, to fall to the same pitfalls as us.", "tokens": [406, 281, 360, 264, 912, 11, 281, 2100, 281, 264, 912, 10147, 18542, 382, 505, 13], "temperature": 0.0, "avg_logprob": -0.1077716210309197, "compression_ratio": 1.5837837837837838, "no_speech_prob": 9.302610123995692e-05}, {"id": 108, "seek": 54296, "start": 542.96, "end": 550.08, "text": " So we decided to create best practices and to share with the community our findings.", "tokens": [407, 321, 3047, 281, 1884, 1151, 7525, 293, 281, 2073, 365, 264, 1768, 527, 16483, 13], "temperature": 0.0, "avg_logprob": -0.13941940184562437, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.0001985961280297488}, {"id": 109, "seek": 54296, "start": 550.08, "end": 557.6800000000001, "text": " We hope to shorten the onboarding time for others and to create better documentation", "tokens": [492, 1454, 281, 39632, 264, 24033, 278, 565, 337, 2357, 293, 281, 1884, 1101, 14333], "temperature": 0.0, "avg_logprob": -0.13941940184562437, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.0001985961280297488}, {"id": 110, "seek": 54296, "start": 557.6800000000001, "end": 565.4000000000001, "text": " and to create reusable code for others to be able to use and save time and money, of", "tokens": [293, 281, 1884, 41807, 3089, 337, 2357, 281, 312, 1075, 281, 764, 293, 3155, 565, 293, 1460, 11, 295], "temperature": 0.0, "avg_logprob": -0.13941940184562437, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.0001985961280297488}, {"id": 111, "seek": 54296, "start": 565.4000000000001, "end": 568.6, "text": " course.", "tokens": [1164, 13], "temperature": 0.0, "avg_logprob": -0.13941940184562437, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.0001985961280297488}, {"id": 112, "seek": 56860, "start": 568.6, "end": 577.4, "text": " So we reached out to the operator framework SDK team to collaborate with them and to publish", "tokens": [407, 321, 6488, 484, 281, 264, 12973, 8388, 37135, 1469, 281, 18338, 365, 552, 293, 281, 11374], "temperature": 0.0, "avg_logprob": -0.151948481798172, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0001971425663214177}, {"id": 113, "seek": 56860, "start": 577.4, "end": 580.08, "text": " there our best practices.", "tokens": [456, 527, 1151, 7525, 13], "temperature": 0.0, "avg_logprob": -0.151948481798172, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0001971425663214177}, {"id": 114, "seek": 56860, "start": 580.08, "end": 587.4, "text": " As you can see here, this is the operator observability best practices.", "tokens": [1018, 291, 393, 536, 510, 11, 341, 307, 264, 12973, 9951, 2310, 1151, 7525, 13], "temperature": 0.0, "avg_logprob": -0.151948481798172, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0001971425663214177}, {"id": 115, "seek": 56860, "start": 587.4, "end": 593.1600000000001, "text": " The operator SDK itself is the first step when someone wants to create a new operator.", "tokens": [440, 12973, 37135, 2564, 307, 264, 700, 1823, 562, 1580, 2738, 281, 1884, 257, 777, 12973, 13], "temperature": 0.0, "avg_logprob": -0.151948481798172, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0001971425663214177}, {"id": 116, "seek": 59316, "start": 593.16, "end": 600.3199999999999, "text": " It gives them tools, how to create it easily, how to build, test the packages, and provides", "tokens": [467, 2709, 552, 3873, 11, 577, 281, 1884, 309, 3612, 11, 577, 281, 1322, 11, 1500, 264, 17401, 11, 293, 6417], "temperature": 0.0, "avg_logprob": -0.1334276580810547, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00030922985752113163}, {"id": 117, "seek": 59316, "start": 600.3199999999999, "end": 605.12, "text": " best practices for all steps of the operator life cycle.", "tokens": [1151, 7525, 337, 439, 4439, 295, 264, 12973, 993, 6586, 13], "temperature": 0.0, "avg_logprob": -0.1334276580810547, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00030922985752113163}, {"id": 118, "seek": 59316, "start": 605.12, "end": 613.1999999999999, "text": " So we found that this was the best place for others to also go for monitoring.", "tokens": [407, 321, 1352, 300, 341, 390, 264, 1151, 1081, 337, 2357, 281, 611, 352, 337, 11028, 13], "temperature": 0.0, "avg_logprob": -0.1334276580810547, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00030922985752113163}, {"id": 119, "seek": 59316, "start": 613.1999999999999, "end": 617.12, "text": " And in these best practices, I will now share with you a few examples.", "tokens": [400, 294, 613, 1151, 7525, 11, 286, 486, 586, 2073, 365, 291, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1334276580810547, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00030922985752113163}, {"id": 120, "seek": 61712, "start": 617.12, "end": 624.96, "text": " It may sound simple, but simple things have a big impact, both on the users that are using", "tokens": [467, 815, 1626, 2199, 11, 457, 2199, 721, 362, 257, 955, 2712, 11, 1293, 322, 264, 5022, 300, 366, 1228], "temperature": 0.0, "avg_logprob": -0.14205367941605418, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00032100811949931085}, {"id": 121, "seek": 61712, "start": 624.96, "end": 630.72, "text": " the system and both on developers that are trying to work with the metrics.", "tokens": [264, 1185, 293, 1293, 322, 8849, 300, 366, 1382, 281, 589, 365, 264, 16367, 13], "temperature": 0.0, "avg_logprob": -0.14205367941605418, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00032100811949931085}, {"id": 122, "seek": 61712, "start": 630.72, "end": 637.04, "text": " So for example, a naming convention for metrics.", "tokens": [407, 337, 1365, 11, 257, 25290, 10286, 337, 16367, 13], "temperature": 0.0, "avg_logprob": -0.14205367941605418, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00032100811949931085}, {"id": 123, "seek": 61712, "start": 637.04, "end": 641.5600000000001, "text": " One of the things that is mentioned in the document is having a name prefix for your", "tokens": [1485, 295, 264, 721, 300, 307, 2835, 294, 264, 4166, 307, 1419, 257, 1315, 46969, 337, 428], "temperature": 0.0, "avg_logprob": -0.14205367941605418, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00032100811949931085}, {"id": 124, "seek": 61712, "start": 641.5600000000001, "end": 642.76, "text": " metrics.", "tokens": [16367, 13], "temperature": 0.0, "avg_logprob": -0.14205367941605418, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00032100811949931085}, {"id": 125, "seek": 64276, "start": 642.76, "end": 648.64, "text": " This is very simple action that will help you identify, that will help the developers,", "tokens": [639, 307, 588, 2199, 3069, 300, 486, 854, 291, 5876, 11, 300, 486, 854, 264, 8849, 11], "temperature": 0.0, "avg_logprob": -0.23047886642755247, "compression_ratio": 1.751111111111111, "no_speech_prob": 7.504082896048203e-05}, {"id": 126, "seek": 64276, "start": 648.64, "end": 654.96, "text": " the users to identify that the metrics are coming from the specific operator or a company.", "tokens": [264, 5022, 281, 5876, 300, 264, 16367, 366, 1348, 490, 264, 2685, 12973, 420, 257, 2237, 13], "temperature": 0.0, "avg_logprob": -0.23047886642755247, "compression_ratio": 1.751111111111111, "no_speech_prob": 7.504082896048203e-05}, {"id": 127, "seek": 64276, "start": 654.96, "end": 660.6, "text": " In this case, you can see that all of the metrics here have a cube width prefix, a cube", "tokens": [682, 341, 1389, 11, 291, 393, 536, 300, 439, 295, 264, 16367, 510, 362, 257, 13728, 11402, 46969, 11, 257, 13728], "temperature": 0.0, "avg_logprob": -0.23047886642755247, "compression_ratio": 1.751111111111111, "no_speech_prob": 7.504082896048203e-05}, {"id": 128, "seek": 64276, "start": 660.6, "end": 664.6, "text": " width, as I said, has sub-operators.", "tokens": [11402, 11, 382, 286, 848, 11, 575, 1422, 12, 7192, 3391, 13], "temperature": 0.0, "avg_logprob": -0.23047886642755247, "compression_ratio": 1.751111111111111, "no_speech_prob": 7.504082896048203e-05}, {"id": 129, "seek": 64276, "start": 664.6, "end": 671.64, "text": " So under this prefix, we also have a sub-prefix for each individual operator, a CDI network", "tokens": [407, 833, 341, 46969, 11, 321, 611, 362, 257, 1422, 12, 3712, 69, 970, 337, 1184, 2609, 12973, 11, 257, 383, 3085, 3209], "temperature": 0.0, "avg_logprob": -0.23047886642755247, "compression_ratio": 1.751111111111111, "no_speech_prob": 7.504082896048203e-05}, {"id": 130, "seek": 67164, "start": 671.64, "end": 678.52, "text": " and so on.", "tokens": [293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 131, "seek": 67164, "start": 678.52, "end": 684.76, "text": " And this is another example, which does not have this prefix.", "tokens": [400, 341, 307, 1071, 1365, 11, 597, 775, 406, 362, 341, 46969, 13], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 132, "seek": 67164, "start": 684.76, "end": 689.84, "text": " We can see here a container CPU, for example, prefix, but we can't understand where it's", "tokens": [492, 393, 536, 510, 257, 10129, 13199, 11, 337, 1365, 11, 46969, 11, 457, 321, 393, 380, 1223, 689, 309, 311], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 133, "seek": 67164, "start": 689.84, "end": 690.84, "text": " coming from.", "tokens": [1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 134, "seek": 67164, "start": 690.84, "end": 692.8, "text": " In this case, it's the advisor.", "tokens": [682, 341, 1389, 11, 309, 311, 264, 19161, 13], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 135, "seek": 67164, "start": 692.8, "end": 696.8, "text": " But if you're a user and you're trying to understand where this metric came from, it's", "tokens": [583, 498, 291, 434, 257, 4195, 293, 291, 434, 1382, 281, 1223, 689, 341, 20678, 1361, 490, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.17125988006591797, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.00017007625137921423}, {"id": 136, "seek": 69680, "start": 696.8, "end": 704.4, "text": " very hard, and also you cannot search in Grafana, for example, for all of the C-advisor metrics", "tokens": [588, 1152, 11, 293, 611, 291, 2644, 3164, 294, 8985, 69, 2095, 11, 337, 1365, 11, 337, 439, 295, 264, 383, 12, 345, 16457, 16367], "temperature": 0.0, "avg_logprob": -0.1667874610587342, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00020941183902323246}, {"id": 137, "seek": 69680, "start": 704.4, "end": 705.4, "text": " together.", "tokens": [1214, 13], "temperature": 0.0, "avg_logprob": -0.1667874610587342, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00020941183902323246}, {"id": 138, "seek": 69680, "start": 705.4, "end": 709.8399999999999, "text": " So that's a problem.", "tokens": [407, 300, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1667874610587342, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00020941183902323246}, {"id": 139, "seek": 69680, "start": 709.8399999999999, "end": 715.24, "text": " Another thing that is mentioned in the best practices is about help text.", "tokens": [3996, 551, 300, 307, 2835, 294, 264, 1151, 7525, 307, 466, 854, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1667874610587342, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00020941183902323246}, {"id": 140, "seek": 69680, "start": 715.24, "end": 724.12, "text": " Each metric has a dedicated place to add the help for this metric.", "tokens": [6947, 20678, 575, 257, 8374, 1081, 281, 909, 264, 854, 337, 341, 20678, 13], "temperature": 0.0, "avg_logprob": -0.1667874610587342, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00020941183902323246}, {"id": 141, "seek": 72412, "start": 724.12, "end": 730.68, "text": " And as you can see in Grafana and other visualization tools, the user will be able to see when hovering", "tokens": [400, 382, 291, 393, 536, 294, 8985, 69, 2095, 293, 661, 25801, 3873, 11, 264, 4195, 486, 312, 1075, 281, 536, 562, 44923], "temperature": 0.0, "avg_logprob": -0.13100197735954733, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.00027032848447561264}, {"id": 142, "seek": 72412, "start": 730.68, "end": 733.96, "text": " on the metrics, the description of it.", "tokens": [322, 264, 16367, 11, 264, 3855, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13100197735954733, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.00027032848447561264}, {"id": 143, "seek": 72412, "start": 733.96, "end": 740.24, "text": " It's very important because if not, you need to go somewhere else to search for it.", "tokens": [467, 311, 588, 1021, 570, 498, 406, 11, 291, 643, 281, 352, 4079, 1646, 281, 3164, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.13100197735954733, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.00027032848447561264}, {"id": 144, "seek": 72412, "start": 740.24, "end": 745.64, "text": " Also this gives you the ability to create auto-generated documentation for all of your", "tokens": [2743, 341, 2709, 291, 264, 3485, 281, 1884, 8399, 12, 21848, 770, 14333, 337, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.13100197735954733, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.00027032848447561264}, {"id": 145, "seek": 72412, "start": 745.64, "end": 751.72, "text": " metrics in your site.", "tokens": [16367, 294, 428, 3621, 13], "temperature": 0.0, "avg_logprob": -0.13100197735954733, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.00027032848447561264}, {"id": 146, "seek": 75172, "start": 751.72, "end": 754.0400000000001, "text": " Another example is the base units.", "tokens": [3996, 1365, 307, 264, 3096, 6815, 13], "temperature": 0.0, "avg_logprob": -0.22932367476205978, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0012864102609455585}, {"id": 147, "seek": 75172, "start": 754.0400000000001, "end": 759.12, "text": " So Prometheus recommends using base units for metrics.", "tokens": [407, 2114, 649, 42209, 34556, 1228, 3096, 6815, 337, 16367, 13], "temperature": 0.0, "avg_logprob": -0.22932367476205978, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0012864102609455585}, {"id": 148, "seek": 75172, "start": 759.12, "end": 767.48, "text": " For example, you can see here for time to use seconds, not milliseconds, temperature,", "tokens": [1171, 1365, 11, 291, 393, 536, 510, 337, 565, 281, 764, 3949, 11, 406, 34184, 11, 4292, 11], "temperature": 0.0, "avg_logprob": -0.22932367476205978, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0012864102609455585}, {"id": 149, "seek": 75172, "start": 767.48, "end": 774.6, "text": " Celsius, not Fahrenheit, this gives the users a fluent experience when they are using the", "tokens": [22658, 11, 406, 31199, 11, 341, 2709, 264, 5022, 257, 40799, 1752, 562, 436, 366, 1228, 264], "temperature": 0.0, "avg_logprob": -0.22932367476205978, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0012864102609455585}, {"id": 150, "seek": 77460, "start": 774.6, "end": 783.0400000000001, "text": " metrics, they don't need to do conversions, deviations of the data, and they are saying", "tokens": [16367, 11, 436, 500, 380, 643, 281, 360, 42256, 11, 31219, 763, 295, 264, 1412, 11, 293, 436, 366, 1566], "temperature": 0.0, "avg_logprob": -0.12108588800197695, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.0005116547690704465}, {"id": 151, "seek": 77460, "start": 783.0400000000001, "end": 787.9200000000001, "text": " if you want to use milliseconds, use a floating point number.", "tokens": [498, 291, 528, 281, 764, 34184, 11, 764, 257, 12607, 935, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12108588800197695, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.0005116547690704465}, {"id": 152, "seek": 77460, "start": 787.9200000000001, "end": 794.52, "text": " This removes the concern of magnitude of the number, and Grafana can handle it, and it", "tokens": [639, 30445, 264, 3136, 295, 15668, 295, 264, 1230, 11, 293, 8985, 69, 2095, 393, 4813, 309, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.12108588800197695, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.0005116547690704465}, {"id": 153, "seek": 77460, "start": 794.52, "end": 801.1600000000001, "text": " will still show you the same precision, but the consistency in the UI and how to use the", "tokens": [486, 920, 855, 291, 264, 912, 18356, 11, 457, 264, 14416, 294, 264, 15682, 293, 577, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.12108588800197695, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.0005116547690704465}, {"id": 154, "seek": 80116, "start": 801.16, "end": 805.76, "text": " metrics will stay the same.", "tokens": [16367, 486, 1754, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.17147293090820312, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.00017607019981369376}, {"id": 155, "seek": 80116, "start": 805.76, "end": 811.92, "text": " Here you can see an example for metrics that are using seconds.", "tokens": [1692, 291, 393, 536, 364, 1365, 337, 16367, 300, 366, 1228, 3949, 13], "temperature": 0.0, "avg_logprob": -0.17147293090820312, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.00017607019981369376}, {"id": 156, "seek": 80116, "start": 811.92, "end": 815.92, "text": " And here we see that each CD are not using it.", "tokens": [400, 510, 321, 536, 300, 1184, 6743, 366, 406, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.17147293090820312, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.00017607019981369376}, {"id": 157, "seek": 80116, "start": 815.92, "end": 824.12, "text": " So this is not as recommended, and we would actually recommend to switch it, but they", "tokens": [407, 341, 307, 406, 382, 9628, 11, 293, 321, 576, 767, 2748, 281, 3679, 309, 11, 457, 436], "temperature": 0.0, "avg_logprob": -0.17147293090820312, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.00017607019981369376}, {"id": 158, "seek": 80116, "start": 824.12, "end": 825.72, "text": " started with milliseconds.", "tokens": [1409, 365, 34184, 13], "temperature": 0.0, "avg_logprob": -0.17147293090820312, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.00017607019981369376}, {"id": 159, "seek": 82572, "start": 825.72, "end": 832.0400000000001, "text": " And now doing the change will cause issues with the UI that is based on it and everything.", "tokens": [400, 586, 884, 264, 1319, 486, 3082, 2663, 365, 264, 15682, 300, 307, 2361, 322, 309, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.15115962028503419, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0004347647773101926}, {"id": 160, "seek": 82572, "start": 832.0400000000001, "end": 838.5600000000001, "text": " So it's a problem to change the names of the metrics once they are created.", "tokens": [407, 309, 311, 257, 1154, 281, 1319, 264, 5288, 295, 264, 16367, 1564, 436, 366, 2942, 13], "temperature": 0.0, "avg_logprob": -0.15115962028503419, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0004347647773101926}, {"id": 161, "seek": 82572, "start": 838.5600000000001, "end": 843.64, "text": " So when I joined the operator, we didn't have name prefixes.", "tokens": [407, 562, 286, 6869, 264, 12973, 11, 321, 994, 380, 362, 1315, 18417, 36005, 13], "temperature": 0.0, "avg_logprob": -0.15115962028503419, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0004347647773101926}, {"id": 162, "seek": 82572, "start": 843.64, "end": 848.64, "text": " I tried to understand which metrics are ours and which are not, it was very hard.", "tokens": [286, 3031, 281, 1223, 597, 16367, 366, 11896, 293, 597, 366, 406, 11, 309, 390, 588, 1152, 13], "temperature": 0.0, "avg_logprob": -0.15115962028503419, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0004347647773101926}, {"id": 163, "seek": 82572, "start": 848.64, "end": 854.0, "text": " So we needed to go and do breaking changes for the metrics and add those prefixes, change", "tokens": [407, 321, 2978, 281, 352, 293, 360, 7697, 2962, 337, 264, 16367, 293, 909, 729, 18417, 36005, 11, 1319], "temperature": 0.0, "avg_logprob": -0.15115962028503419, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0004347647773101926}, {"id": 164, "seek": 85400, "start": 854.0, "end": 864.24, "text": " the units, and this is what we want others to be able to avoid, this duplicate of work.", "tokens": [264, 6815, 11, 293, 341, 307, 437, 321, 528, 2357, 281, 312, 1075, 281, 5042, 11, 341, 23976, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.14234460430380738, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.00030972593231126666}, {"id": 165, "seek": 85400, "start": 864.24, "end": 868.36, "text": " Additional information in the best practices is about alerts.", "tokens": [44272, 1589, 294, 264, 1151, 7525, 307, 466, 28061, 13], "temperature": 0.0, "avg_logprob": -0.14234460430380738, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.00030972593231126666}, {"id": 166, "seek": 85400, "start": 868.36, "end": 871.0, "text": " This is an example of an alert.", "tokens": [639, 307, 364, 1365, 295, 364, 9615, 13], "temperature": 0.0, "avg_logprob": -0.14234460430380738, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.00030972593231126666}, {"id": 167, "seek": 85400, "start": 871.0, "end": 874.6, "text": " You can see here that we have the alert name.", "tokens": [509, 393, 536, 510, 300, 321, 362, 264, 9615, 1315, 13], "temperature": 0.0, "avg_logprob": -0.14234460430380738, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.00030972593231126666}, {"id": 168, "seek": 85400, "start": 874.6, "end": 882.24, "text": " We have an expression which is based on a metric, and once the expression is met, the", "tokens": [492, 362, 364, 6114, 597, 307, 2361, 322, 257, 20678, 11, 293, 1564, 264, 6114, 307, 1131, 11, 264], "temperature": 0.0, "avg_logprob": -0.14234460430380738, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.00030972593231126666}, {"id": 169, "seek": 88224, "start": 882.24, "end": 889.72, "text": " alert either starts firing or is in pending state until the evaluation time.", "tokens": [9615, 2139, 3719, 16045, 420, 307, 294, 32110, 1785, 1826, 264, 13344, 565, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 170, "seek": 88224, "start": 889.72, "end": 890.72, "text": " There is a description.", "tokens": [821, 307, 257, 3855, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 171, "seek": 88224, "start": 890.72, "end": 893.88, "text": " There is also a possibility to add a summary.", "tokens": [821, 307, 611, 257, 7959, 281, 909, 257, 12691, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 172, "seek": 88224, "start": 893.88, "end": 895.6800000000001, "text": " This is the evaluation time.", "tokens": [639, 307, 264, 13344, 565, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 173, "seek": 88224, "start": 895.6800000000001, "end": 898.72, "text": " It has a severity.", "tokens": [467, 575, 257, 35179, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 174, "seek": 88224, "start": 898.72, "end": 901.0, "text": " And a link to a runbook URL.", "tokens": [400, 257, 2113, 281, 257, 1190, 2939, 12905, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 175, "seek": 88224, "start": 901.0, "end": 910.36, "text": " There could be other information that you can add to it, but this is the basic.", "tokens": [821, 727, 312, 661, 1589, 300, 291, 393, 909, 281, 309, 11, 457, 341, 307, 264, 3875, 13], "temperature": 0.0, "avg_logprob": -0.16891848322856856, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.00024844336439855397}, {"id": 176, "seek": 91036, "start": 910.36, "end": 914.36, "text": " And what we're saying in the best practice is that there's supposed to be, for example,", "tokens": [400, 437, 321, 434, 1566, 294, 264, 1151, 3124, 307, 300, 456, 311, 3442, 281, 312, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 177, "seek": 91036, "start": 914.36, "end": 921.28, "text": " for the labels of severity, there should only be three valid options, critical, warning,", "tokens": [337, 264, 16949, 295, 35179, 11, 456, 820, 787, 312, 1045, 7363, 3956, 11, 4924, 11, 9164, 11], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 178, "seek": 91036, "start": 921.28, "end": 923.0, "text": " and info alerts.", "tokens": [293, 13614, 28061, 13], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 179, "seek": 91036, "start": 923.0, "end": 927.24, "text": " If you're using something else, it would be problematic.", "tokens": [759, 291, 434, 1228, 746, 1646, 11, 309, 576, 312, 19011, 13], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 180, "seek": 91036, "start": 927.24, "end": 932.6800000000001, "text": " You can see here in this example, I don't know if you're seeing it, but we see that", "tokens": [509, 393, 536, 510, 294, 341, 1365, 11, 286, 500, 380, 458, 498, 291, 434, 2577, 309, 11, 457, 321, 536, 300], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 181, "seek": 91036, "start": 932.6800000000001, "end": 935.4, "text": " this is our example in the cluster.", "tokens": [341, 307, 527, 1365, 294, 264, 13630, 13], "temperature": 0.0, "avg_logprob": -0.16846590625996494, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.00030384285491891205}, {"id": 182, "seek": 93540, "start": 935.4, "end": 941.8, "text": " We have info, warning, and critical, and we have one non-severity, which is the watchdog.", "tokens": [492, 362, 13614, 11, 9164, 11, 293, 4924, 11, 293, 321, 362, 472, 2107, 12, 405, 331, 507, 11, 597, 307, 264, 1159, 14833, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 183, "seek": 93540, "start": 941.8, "end": 943.88, "text": " It's part of Prometheus alerts.", "tokens": [467, 311, 644, 295, 2114, 649, 42209, 28061, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 184, "seek": 93540, "start": 943.88, "end": 947.24, "text": " It's just making sure that the alerts are working as expected.", "tokens": [467, 311, 445, 1455, 988, 300, 264, 28061, 366, 1364, 382, 5176, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 185, "seek": 93540, "start": 947.24, "end": 949.0799999999999, "text": " It should always stay one.", "tokens": [467, 820, 1009, 1754, 472, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 186, "seek": 93540, "start": 949.0799999999999, "end": 955.84, "text": " There should never be alerts that don't have severity.", "tokens": [821, 820, 1128, 312, 28061, 300, 500, 380, 362, 35179, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 187, "seek": 93540, "start": 955.84, "end": 959.0799999999999, "text": " And this is a bad example of using a severity label.", "tokens": [400, 341, 307, 257, 1578, 1365, 295, 1228, 257, 35179, 7645, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 188, "seek": 93540, "start": 959.0799999999999, "end": 963.04, "text": " In this case, they are using major instead of critical.", "tokens": [682, 341, 1389, 11, 436, 366, 1228, 2563, 2602, 295, 4924, 13], "temperature": 0.0, "avg_logprob": -0.1580646588252141, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.00010736125841503963}, {"id": 189, "seek": 96304, "start": 963.04, "end": 971.04, "text": " The impact of that is that if someone is setting up alert manager to notify the support team", "tokens": [440, 2712, 295, 300, 307, 300, 498, 1580, 307, 3287, 493, 9615, 6598, 281, 36560, 264, 1406, 1469], "temperature": 0.0, "avg_logprob": -0.1871826068775074, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.00034397750278003514}, {"id": 190, "seek": 96304, "start": 971.04, "end": 976.7199999999999, "text": " that something critical happened to the system, and they were to get notified by Slack or by", "tokens": [300, 746, 4924, 2011, 281, 264, 1185, 11, 293, 436, 645, 281, 483, 18013, 538, 37211, 420, 538], "temperature": 0.0, "avg_logprob": -0.1871826068775074, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.00034397750278003514}, {"id": 191, "seek": 96304, "start": 976.7199999999999, "end": 983.12, "text": " a pager, they will miss out on this alert because it doesn't meet with the convention", "tokens": [257, 280, 3557, 11, 436, 486, 1713, 484, 322, 341, 9615, 570, 309, 1177, 380, 1677, 365, 264, 10286], "temperature": 0.0, "avg_logprob": -0.1871826068775074, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.00034397750278003514}, {"id": 192, "seek": 96304, "start": 983.12, "end": 989.7199999999999, "text": " of severities, values for severities.", "tokens": [295, 2802, 1088, 11, 4190, 337, 2802, 1088, 13], "temperature": 0.0, "avg_logprob": -0.1871826068775074, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.00034397750278003514}, {"id": 193, "seek": 98972, "start": 989.72, "end": 997.64, "text": " So what we have at the moment for best practices, we have for a metrics naming convention.", "tokens": [407, 437, 321, 362, 412, 264, 1623, 337, 1151, 7525, 11, 321, 362, 337, 257, 16367, 25290, 10286, 13], "temperature": 0.0, "avg_logprob": -0.1420886379548873, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.00013692688662558794}, {"id": 194, "seek": 98972, "start": 997.64, "end": 1005.0400000000001, "text": " We have how to create documentation for metrics, alerts, information about alert labels, run", "tokens": [492, 362, 577, 281, 1884, 14333, 337, 16367, 11, 28061, 11, 1589, 466, 9615, 16949, 11, 1190], "temperature": 0.0, "avg_logprob": -0.1420886379548873, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.00013692688662558794}, {"id": 195, "seek": 98972, "start": 1005.0400000000001, "end": 1006.0400000000001, "text": " books.", "tokens": [3642, 13], "temperature": 0.0, "avg_logprob": -0.1420886379548873, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.00013692688662558794}, {"id": 196, "seek": 98972, "start": 1006.0400000000001, "end": 1012.0400000000001, "text": " By the way, run books are a way to provide more information about the alert.", "tokens": [3146, 264, 636, 11, 1190, 3642, 366, 257, 636, 281, 2893, 544, 1589, 466, 264, 9615, 13], "temperature": 0.0, "avg_logprob": -0.1420886379548873, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.00013692688662558794}, {"id": 197, "seek": 98972, "start": 1012.0400000000001, "end": 1019.6800000000001, "text": " You have a link in the alert where you can send the user to go and find more details.", "tokens": [509, 362, 257, 2113, 294, 264, 9615, 689, 291, 393, 2845, 264, 4195, 281, 352, 293, 915, 544, 4365, 13], "temperature": 0.0, "avg_logprob": -0.1420886379548873, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.00013692688662558794}, {"id": 198, "seek": 101968, "start": 1019.68, "end": 1020.68, "text": " What's it about?", "tokens": [708, 311, 309, 466, 30], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 199, "seek": 101968, "start": 1020.68, "end": 1021.68, "text": " What's the impact?", "tokens": [708, 311, 264, 2712, 30], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 200, "seek": 101968, "start": 1021.68, "end": 1022.68, "text": " How to diagnose it?", "tokens": [1012, 281, 36238, 309, 30], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 201, "seek": 101968, "start": 1022.68, "end": 1025.6399999999999, "text": " And how to mitigate the issue.", "tokens": [400, 577, 281, 27336, 264, 2734, 13], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 202, "seek": 101968, "start": 1025.6399999999999, "end": 1031.56, "text": " And then additional information about how to test metrics and how to test alerts.", "tokens": [400, 550, 4497, 1589, 466, 577, 281, 1500, 16367, 293, 577, 281, 1500, 28061, 13], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 203, "seek": 101968, "start": 1031.56, "end": 1039.32, "text": " We plan to enrich this information, add information about dashboards, logging events, tracing in", "tokens": [492, 1393, 281, 18849, 341, 1589, 11, 909, 1589, 466, 8240, 17228, 11, 27991, 3931, 11, 25262, 294], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 204, "seek": 101968, "start": 1039.32, "end": 1042.04, "text": " the future.", "tokens": [264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 205, "seek": 101968, "start": 1042.04, "end": 1048.2, "text": " So Shirley gave an overview about an eye-level situation about metrics and alerts.", "tokens": [407, 43275, 2729, 364, 12492, 466, 364, 3313, 12, 12418, 2590, 466, 16367, 293, 28061, 13], "temperature": 0.0, "avg_logprob": -0.19120825891909393, "compression_ratio": 1.7733990147783252, "no_speech_prob": 0.0002532524522393942}, {"id": 206, "seek": 104820, "start": 1048.2, "end": 1053.16, "text": " But how do we translate some of these best practices into code?", "tokens": [583, 577, 360, 321, 13799, 512, 295, 613, 1151, 7525, 666, 3089, 30], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 207, "seek": 104820, "start": 1053.16, "end": 1058.32, "text": " So one of the problems that we faced was that logic code and monitoring code were becoming", "tokens": [407, 472, 295, 264, 2740, 300, 321, 11446, 390, 300, 9952, 3089, 293, 11028, 3089, 645, 5617], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 208, "seek": 104820, "start": 1058.32, "end": 1061.16, "text": " very intertwined.", "tokens": [588, 44400, 2001, 13], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 209, "seek": 104820, "start": 1061.16, "end": 1064.48, "text": " Code like this becomes harder to maintain.", "tokens": [15549, 411, 341, 3643, 6081, 281, 6909, 13], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 210, "seek": 104820, "start": 1064.48, "end": 1071.04, "text": " Obviously it becomes more difficult in understanding what the code does and to modify it.", "tokens": [7580, 309, 3643, 544, 2252, 294, 3701, 437, 264, 3089, 775, 293, 281, 16927, 309, 13], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 211, "seek": 104820, "start": 1071.04, "end": 1076.4, "text": " This leads obviously to longer development times, potential bugs, and it's also more", "tokens": [639, 6689, 2745, 281, 2854, 3250, 1413, 11, 3995, 15120, 11, 293, 309, 311, 611, 544], "temperature": 0.0, "avg_logprob": -0.16426283662969415, "compression_ratio": 1.625, "no_speech_prob": 0.0013871097471565008}, {"id": 212, "seek": 107640, "start": 1076.4, "end": 1083.1200000000001, "text": " challenging to onboard new team members or to contribute to one of these projects.", "tokens": [7595, 281, 24033, 777, 1469, 2679, 420, 281, 10586, 281, 472, 295, 613, 4455, 13], "temperature": 0.0, "avg_logprob": -0.21428728756839283, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00156927271746099}, {"id": 213, "seek": 107640, "start": 1083.1200000000001, "end": 1091.0800000000002, "text": " In this specific snippet, there was like 16.4% of monitoring code intertwined with migration", "tokens": [682, 341, 2685, 35623, 302, 11, 456, 390, 411, 3165, 13, 19, 4, 295, 11028, 3089, 44400, 2001, 365, 17011], "temperature": 0.0, "avg_logprob": -0.21428728756839283, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00156927271746099}, {"id": 214, "seek": 107640, "start": 1091.0800000000002, "end": 1092.48, "text": " logic code.", "tokens": [9952, 3089, 13], "temperature": 0.0, "avg_logprob": -0.21428728756839283, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00156927271746099}, {"id": 215, "seek": 107640, "start": 1092.48, "end": 1099.88, "text": " So what we did was try to refactor this code to try to separate these concerns, one from", "tokens": [407, 437, 321, 630, 390, 853, 281, 1895, 15104, 341, 3089, 281, 853, 281, 4994, 613, 7389, 11, 472, 490], "temperature": 0.0, "avg_logprob": -0.21428728756839283, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00156927271746099}, {"id": 216, "seek": 107640, "start": 1099.88, "end": 1102.72, "text": " the other.", "tokens": [264, 661, 13], "temperature": 0.0, "avg_logprob": -0.21428728756839283, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00156927271746099}, {"id": 217, "seek": 110272, "start": 1102.72, "end": 1109.92, "text": " In this specific case, we used a Prometheus collector that's just iterating the existing", "tokens": [682, 341, 2685, 1389, 11, 321, 1143, 257, 2114, 649, 42209, 23960, 300, 311, 445, 17138, 990, 264, 6741], "temperature": 0.0, "avg_logprob": -0.19307053226164017, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0019778688438236713}, {"id": 218, "seek": 110272, "start": 1109.92, "end": 1116.56, "text": " virtual machines migrations, and then it's just pushing the metrics according to the", "tokens": [6374, 8379, 6186, 12154, 11, 293, 550, 309, 311, 445, 7380, 264, 16367, 4650, 281, 264], "temperature": 0.0, "avg_logprob": -0.19307053226164017, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0019778688438236713}, {"id": 219, "seek": 110272, "start": 1116.56, "end": 1121.76, "text": " status of the virtual machines, whether they are successful or not, or the accounts of", "tokens": [6558, 295, 264, 6374, 8379, 11, 1968, 436, 366, 4406, 420, 406, 11, 420, 264, 9402, 295], "temperature": 0.0, "avg_logprob": -0.19307053226164017, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0019778688438236713}, {"id": 220, "seek": 110272, "start": 1121.76, "end": 1125.48, "text": " the pending schedule and running migrations.", "tokens": [264, 32110, 7567, 293, 2614, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.19307053226164017, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0019778688438236713}, {"id": 221, "seek": 110272, "start": 1125.48, "end": 1130.8, "text": " And obviously this snippet is much easier to understand how the monitoring is being", "tokens": [400, 2745, 341, 35623, 302, 307, 709, 3571, 281, 1223, 577, 264, 11028, 307, 885], "temperature": 0.0, "avg_logprob": -0.19307053226164017, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0019778688438236713}, {"id": 222, "seek": 113080, "start": 1130.8, "end": 1136.6, "text": " done, and we take all of this out of the migration logic code.", "tokens": [1096, 11, 293, 321, 747, 439, 295, 341, 484, 295, 264, 17011, 9952, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1533104802521182, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0005784897948615253}, {"id": 223, "seek": 113080, "start": 1136.6, "end": 1145.12, "text": " And to help other developers that are starting to avoid the same mistakes as we had to solve,", "tokens": [400, 281, 854, 661, 8849, 300, 366, 2891, 281, 5042, 264, 912, 8038, 382, 321, 632, 281, 5039, 11], "temperature": 0.0, "avg_logprob": -0.1533104802521182, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0005784897948615253}, {"id": 224, "seek": 113080, "start": 1145.12, "end": 1150.44, "text": " we are creating a monitoring example in the memcached operator.", "tokens": [321, 366, 4084, 257, 11028, 1365, 294, 264, 1334, 66, 15095, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1533104802521182, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0005784897948615253}, {"id": 225, "seek": 113080, "start": 1150.44, "end": 1158.96, "text": " We already have an initial example that is already thinking about all these concerns", "tokens": [492, 1217, 362, 364, 5883, 1365, 300, 307, 1217, 1953, 466, 439, 613, 7389], "temperature": 0.0, "avg_logprob": -0.1533104802521182, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0005784897948615253}, {"id": 226, "seek": 115896, "start": 1158.96, "end": 1164.72, "text": " in separation between logic code and monitoring code.", "tokens": [294, 14634, 1296, 9952, 3089, 293, 11028, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11548363821847098, "compression_ratio": 1.689119170984456, "no_speech_prob": 0.0012435214594006538}, {"id": 227, "seek": 115896, "start": 1164.72, "end": 1172.16, "text": " Our idea with this example is to make it as clear as possible, especially this is especially", "tokens": [2621, 1558, 365, 341, 1365, 307, 281, 652, 309, 382, 1850, 382, 1944, 11, 2318, 341, 307, 2318], "temperature": 0.0, "avg_logprob": -0.11548363821847098, "compression_ratio": 1.689119170984456, "no_speech_prob": 0.0012435214594006538}, {"id": 228, "seek": 115896, "start": 1172.16, "end": 1179.88, "text": " important when we are working with large and complex code bases, also make it more modular.", "tokens": [1021, 562, 321, 366, 1364, 365, 2416, 293, 3997, 3089, 17949, 11, 611, 652, 309, 544, 31111, 13], "temperature": 0.0, "avg_logprob": -0.11548363821847098, "compression_ratio": 1.689119170984456, "no_speech_prob": 0.0012435214594006538}, {"id": 229, "seek": 115896, "start": 1179.88, "end": 1184.92, "text": " It's easier to understand both the logic code and the monitoring code without affecting", "tokens": [467, 311, 3571, 281, 1223, 1293, 264, 9952, 3089, 293, 264, 11028, 3089, 1553, 17476], "temperature": 0.0, "avg_logprob": -0.11548363821847098, "compression_ratio": 1.689119170984456, "no_speech_prob": 0.0012435214594006538}, {"id": 230, "seek": 118492, "start": 1184.92, "end": 1192.2, "text": " each other's functionality in the application in general, also make it more reusable.", "tokens": [1184, 661, 311, 14980, 294, 264, 3861, 294, 2674, 11, 611, 652, 309, 544, 41807, 13], "temperature": 0.0, "avg_logprob": -0.15509177861588724, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016259317053481936}, {"id": 231, "seek": 118492, "start": 1192.2, "end": 1197.76, "text": " Since like, for example, the way we are doing monitoring in different operators will always", "tokens": [4162, 411, 11, 337, 1365, 11, 264, 636, 321, 366, 884, 11028, 294, 819, 19077, 486, 1009], "temperature": 0.0, "avg_logprob": -0.15509177861588724, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016259317053481936}, {"id": 232, "seek": 118492, "start": 1197.76, "end": 1199.52, "text": " be more or less the same.", "tokens": [312, 544, 420, 1570, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.15509177861588724, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016259317053481936}, {"id": 233, "seek": 118492, "start": 1199.52, "end": 1207.24, "text": " So if we find a more or less common way to do this, it will make it easier to reuse this", "tokens": [407, 498, 321, 915, 257, 544, 420, 1570, 2689, 636, 281, 360, 341, 11, 309, 486, 652, 309, 3571, 281, 26225, 341], "temperature": 0.0, "avg_logprob": -0.15509177861588724, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016259317053481936}, {"id": 234, "seek": 118492, "start": 1207.24, "end": 1213.48, "text": " code in other applications and projects, which will save them time and effort.", "tokens": [3089, 294, 661, 5821, 293, 4455, 11, 597, 486, 3155, 552, 565, 293, 4630, 13], "temperature": 0.0, "avg_logprob": -0.15509177861588724, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016259317053481936}, {"id": 235, "seek": 121348, "start": 1213.48, "end": 1218.2, "text": " And also, it will become more performant.", "tokens": [400, 611, 11, 309, 486, 1813, 544, 2042, 394, 13], "temperature": 0.0, "avg_logprob": -0.1735063680013021, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000771414372138679}, {"id": 236, "seek": 121348, "start": 1218.2, "end": 1225.96, "text": " If we mix all the monitoring concerns with the migration code, it's trivial that the", "tokens": [759, 321, 2890, 439, 264, 11028, 7389, 365, 264, 17011, 3089, 11, 309, 311, 26703, 300, 264], "temperature": 0.0, "avg_logprob": -0.1735063680013021, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000771414372138679}, {"id": 237, "seek": 121348, "start": 1225.96, "end": 1231.84, "text": " time it will take to make a migration will take longer because we are calculating metric", "tokens": [565, 309, 486, 747, 281, 652, 257, 17011, 486, 747, 2854, 570, 321, 366, 28258, 20678], "temperature": 0.0, "avg_logprob": -0.1735063680013021, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000771414372138679}, {"id": 238, "seek": 121348, "start": 1231.84, "end": 1238.52, "text": " values and doing some Prometheus operations while we are trying to calculate the state", "tokens": [4190, 293, 884, 512, 2114, 649, 42209, 7705, 1339, 321, 366, 1382, 281, 8873, 264, 1785], "temperature": 0.0, "avg_logprob": -0.1735063680013021, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000771414372138679}, {"id": 239, "seek": 121348, "start": 1238.52, "end": 1239.64, "text": " of a migration.", "tokens": [295, 257, 17011, 13], "temperature": 0.0, "avg_logprob": -0.1735063680013021, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000771414372138679}, {"id": 240, "seek": 123964, "start": 1239.64, "end": 1246.2800000000002, "text": " So having this separation will also help these questions.", "tokens": [407, 1419, 341, 14634, 486, 611, 854, 613, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2080351210929252, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.000611865078099072}, {"id": 241, "seek": 123964, "start": 1246.2800000000002, "end": 1253.76, "text": " Our idea for the structure of the code will be by creating a package.", "tokens": [2621, 1558, 337, 264, 3877, 295, 264, 3089, 486, 312, 538, 4084, 257, 7372, 13], "temperature": 0.0, "avg_logprob": -0.2080351210929252, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.000611865078099072}, {"id": 242, "seek": 123964, "start": 1253.76, "end": 1260.24, "text": " And for example, here we can see a migration example, a central place where we will be", "tokens": [400, 337, 1365, 11, 510, 321, 393, 536, 257, 17011, 1365, 11, 257, 5777, 1081, 689, 321, 486, 312], "temperature": 0.0, "avg_logprob": -0.2080351210929252, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.000611865078099072}, {"id": 243, "seek": 123964, "start": 1260.24, "end": 1268.0800000000002, "text": " registering all migrations and all migrations, sorry, no, all metrics, obviously, and then", "tokens": [47329, 439, 6186, 12154, 293, 439, 6186, 12154, 11, 2597, 11, 572, 11, 439, 16367, 11, 2745, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.2080351210929252, "compression_ratio": 1.6397849462365592, "no_speech_prob": 0.000611865078099072}, {"id": 244, "seek": 126808, "start": 1268.08, "end": 1274.32, "text": " we will have files that will separate these metrics by their types.", "tokens": [321, 486, 362, 7098, 300, 486, 4994, 613, 16367, 538, 641, 3467, 13], "temperature": 0.0, "avg_logprob": -0.1691763024581106, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.001881380332633853}, {"id": 245, "seek": 126808, "start": 1274.32, "end": 1279.12, "text": " For example, in this example, you can see one operator metrics file, which will have", "tokens": [1171, 1365, 11, 294, 341, 1365, 11, 291, 393, 536, 472, 12973, 16367, 3991, 11, 597, 486, 362], "temperature": 0.0, "avg_logprob": -0.1691763024581106, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.001881380332633853}, {"id": 246, "seek": 126808, "start": 1279.12, "end": 1286.1599999999999, "text": " all the operator-related metrics, as we talked in the beginning, and then we could have one", "tokens": [439, 264, 12973, 12, 12004, 16367, 11, 382, 321, 2825, 294, 264, 2863, 11, 293, 550, 321, 727, 362, 472], "temperature": 0.0, "avg_logprob": -0.1691763024581106, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.001881380332633853}, {"id": 247, "seek": 126808, "start": 1286.1599999999999, "end": 1294.52, "text": " specific file only for the migration metrics and then register them in one place.", "tokens": [2685, 3991, 787, 337, 264, 17011, 16367, 293, 550, 7280, 552, 294, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1691763024581106, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.001881380332633853}, {"id": 248, "seek": 129452, "start": 1294.52, "end": 1301.44, "text": " And why do we think about this structure and what benefits could this bring us?", "tokens": [400, 983, 360, 321, 519, 466, 341, 3877, 293, 437, 5311, 727, 341, 1565, 505, 30], "temperature": 0.0, "avg_logprob": -0.14938744203543958, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.001666978932917118}, {"id": 249, "seek": 129452, "start": 1301.44, "end": 1306.96, "text": " The first one is to automate the metric and the alert code generation.", "tokens": [440, 700, 472, 307, 281, 31605, 264, 20678, 293, 264, 9615, 3089, 5125, 13], "temperature": 0.0, "avg_logprob": -0.14938744203543958, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.001666978932917118}, {"id": 250, "seek": 129452, "start": 1306.96, "end": 1315.68, "text": " As we saw, much of the work that a developer needs to do that, it's like creating a file", "tokens": [1018, 321, 1866, 11, 709, 295, 264, 589, 300, 257, 10754, 2203, 281, 360, 300, 11, 309, 311, 411, 4084, 257, 3991], "temperature": 0.0, "avg_logprob": -0.14938744203543958, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.001666978932917118}, {"id": 251, "seek": 129452, "start": 1315.68, "end": 1322.48, "text": " with a specific name, then go to the metrics.go file and register that file there.", "tokens": [365, 257, 2685, 1315, 11, 550, 352, 281, 264, 16367, 13, 1571, 3991, 293, 7280, 300, 3991, 456, 13], "temperature": 0.0, "avg_logprob": -0.14938744203543958, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.001666978932917118}, {"id": 252, "seek": 132248, "start": 1322.48, "end": 1327.92, "text": " So this is very structured and always the same.", "tokens": [407, 341, 307, 588, 18519, 293, 1009, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16683632135391235, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.001310690538957715}, {"id": 253, "seek": 132248, "start": 1327.92, "end": 1333.24, "text": " It will be easier to automate and then allow developers to have a command line tool to generate", "tokens": [467, 486, 312, 3571, 281, 31605, 293, 550, 2089, 8849, 281, 362, 257, 5622, 1622, 2290, 281, 8460], "temperature": 0.0, "avg_logprob": -0.16683632135391235, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.001310690538957715}, {"id": 254, "seek": 132248, "start": 1333.24, "end": 1337.76, "text": " new metrics and generate new alerts easier.", "tokens": [777, 16367, 293, 8460, 777, 28061, 3571, 13], "temperature": 0.0, "avg_logprob": -0.16683632135391235, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.001310690538957715}, {"id": 255, "seek": 132248, "start": 1337.76, "end": 1343.0, "text": " We are also looking forward to create a linter for the metrics name.", "tokens": [492, 366, 611, 1237, 2128, 281, 1884, 257, 287, 5106, 337, 264, 16367, 1315, 13], "temperature": 0.0, "avg_logprob": -0.16683632135391235, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.001310690538957715}, {"id": 256, "seek": 132248, "start": 1343.0, "end": 1350.52, "text": " As Shirley said, a lot of the concerns that happen when operators are becoming more advanced", "tokens": [1018, 43275, 848, 11, 257, 688, 295, 264, 7389, 300, 1051, 562, 19077, 366, 5617, 544, 7339], "temperature": 0.0, "avg_logprob": -0.16683632135391235, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.001310690538957715}, {"id": 257, "seek": 135052, "start": 1350.52, "end": 1355.92, "text": " is looking back at the metrics and see everything we did wrong with their naming.", "tokens": [307, 1237, 646, 412, 264, 16367, 293, 536, 1203, 321, 630, 2085, 365, 641, 25290, 13], "temperature": 0.0, "avg_logprob": -0.16808595048620345, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0009894781978800893}, {"id": 258, "seek": 135052, "start": 1355.92, "end": 1361.2, "text": " And even, as she said, it's a simple change, but can have a lot of impact.", "tokens": [400, 754, 11, 382, 750, 848, 11, 309, 311, 257, 2199, 1319, 11, 457, 393, 362, 257, 688, 295, 2712, 13], "temperature": 0.0, "avg_logprob": -0.16808595048620345, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0009894781978800893}, {"id": 259, "seek": 135052, "start": 1361.2, "end": 1367.48, "text": " So a linter that follows all these conventions will also be important.", "tokens": [407, 257, 287, 5106, 300, 10002, 439, 613, 33520, 486, 611, 312, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16808595048620345, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0009894781978800893}, {"id": 260, "seek": 135052, "start": 1367.48, "end": 1371.44, "text": " Also automated metrics documentations, we are already doing this.", "tokens": [2743, 18473, 16367, 4166, 763, 11, 321, 366, 1217, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.16808595048620345, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0009894781978800893}, {"id": 261, "seek": 135052, "start": 1371.44, "end": 1378.36, "text": " And one thing that we faced was that a lot of metrics were very scattered in the code.", "tokens": [400, 472, 551, 300, 321, 11446, 390, 300, 257, 688, 295, 16367, 645, 588, 21986, 294, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.16808595048620345, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0009894781978800893}, {"id": 262, "seek": 137836, "start": 1378.36, "end": 1383.52, "text": " So it was easy to automate and find all of them.", "tokens": [407, 309, 390, 1858, 281, 31605, 293, 915, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19231905937194824, "compression_ratio": 1.585, "no_speech_prob": 0.0009914753027260303}, {"id": 263, "seek": 137836, "start": 1383.52, "end": 1390.04, "text": " And with a structure like the previous one, it will be even more easier to create a full", "tokens": [400, 365, 257, 3877, 411, 264, 3894, 472, 11, 309, 486, 312, 754, 544, 3571, 281, 1884, 257, 1577], "temperature": 0.0, "avg_logprob": -0.19231905937194824, "compression_ratio": 1.585, "no_speech_prob": 0.0009914753027260303}, {"id": 264, "seek": 137836, "start": 1390.04, "end": 1398.1999999999998, "text": " list of metrics and that description that will help both developers, newcomers, and users.", "tokens": [1329, 295, 16367, 293, 300, 3855, 300, 486, 854, 1293, 8849, 11, 40014, 433, 11, 293, 5022, 13], "temperature": 0.0, "avg_logprob": -0.19231905937194824, "compression_ratio": 1.585, "no_speech_prob": 0.0009914753027260303}, {"id": 265, "seek": 137836, "start": 1398.1999999999998, "end": 1404.84, "text": " And lastly, have an easier structure for both unit and end-to-end testing, because if we", "tokens": [400, 16386, 11, 362, 364, 3571, 3877, 337, 1293, 4985, 293, 917, 12, 1353, 12, 521, 4997, 11, 570, 498, 321], "temperature": 0.0, "avg_logprob": -0.19231905937194824, "compression_ratio": 1.585, "no_speech_prob": 0.0009914753027260303}, {"id": 266, "seek": 140484, "start": 1404.84, "end": 1413.56, "text": " have, like, this clear structure for where the metrics are, we can test there and test", "tokens": [362, 11, 411, 11, 341, 1850, 3877, 337, 689, 264, 16367, 366, 11, 321, 393, 1500, 456, 293, 1500], "temperature": 0.0, "avg_logprob": -0.21503367075105992, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.00035203612060286105}, {"id": 267, "seek": 140484, "start": 1413.56, "end": 1422.9599999999998, "text": " exactly those functions and not code intertwined in logic code.", "tokens": [2293, 729, 6828, 293, 406, 3089, 44400, 2001, 294, 9952, 3089, 13], "temperature": 0.0, "avg_logprob": -0.21503367075105992, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.00035203612060286105}, {"id": 268, "seek": 140484, "start": 1422.9599999999998, "end": 1427.8, "text": " And just to conclude, if you are starting to create an operator or if you already have", "tokens": [400, 445, 281, 16886, 11, 498, 291, 366, 2891, 281, 1884, 364, 12973, 420, 498, 291, 1217, 362], "temperature": 0.0, "avg_logprob": -0.21503367075105992, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.00035203612060286105}, {"id": 269, "seek": 140484, "start": 1427.8, "end": 1433.6, "text": " an operator, we invite you to go and to look at the operator SDK, to look at the best practices,", "tokens": [364, 12973, 11, 321, 7980, 291, 281, 352, 293, 281, 574, 412, 264, 12973, 37135, 11, 281, 574, 412, 264, 1151, 7525, 11], "temperature": 0.0, "avg_logprob": -0.21503367075105992, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.00035203612060286105}, {"id": 270, "seek": 143360, "start": 1433.6, "end": 1436.3999999999999, "text": " to try to avoid the pitfalls that we had.", "tokens": [281, 853, 281, 5042, 264, 10147, 18542, 300, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 271, "seek": 143360, "start": 1436.3999999999999, "end": 1438.6, "text": " And I really hope it will help you.", "tokens": [400, 286, 534, 1454, 309, 486, 854, 291, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 272, "seek": 143360, "start": 1438.6, "end": 1444.04, "text": " And you should really just consider that when you're creating a new operator, it starts", "tokens": [400, 291, 820, 534, 445, 1949, 300, 562, 291, 434, 4084, 257, 777, 12973, 11, 309, 3719], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 273, "seek": 143360, "start": 1444.04, "end": 1447.76, "text": " small, but it can become really robust.", "tokens": [1359, 11, 457, 309, 393, 1813, 534, 13956, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 274, "seek": 143360, "start": 1447.76, "end": 1449.76, "text": " And you cannot tell that in the beginning.", "tokens": [400, 291, 2644, 980, 300, 294, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 275, "seek": 143360, "start": 1449.76, "end": 1453.9199999999998, "text": " So think ahead and try to build it correctly from the beginning.", "tokens": [407, 519, 2286, 293, 853, 281, 1322, 309, 8944, 490, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 276, "seek": 143360, "start": 1453.9199999999998, "end": 1455.84, "text": " I hope it will be helpful for you.", "tokens": [286, 1454, 309, 486, 312, 4961, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 277, "seek": 143360, "start": 1455.84, "end": 1456.84, "text": " And thank you.", "tokens": [400, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 278, "seek": 143360, "start": 1456.84, "end": 1457.84, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1626570591559777, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0005608093924820423}, {"id": 279, "seek": 145784, "start": 1457.84, "end": 1480.32, "text": " Do you have any recommendations on how you would log out the decision points within your", "tokens": [1144, 291, 362, 604, 10434, 322, 577, 291, 576, 3565, 484, 264, 3537, 2793, 1951, 428], "temperature": 0.0, "avg_logprob": -0.25408756732940674, "compression_ratio": 1.180722891566265, "no_speech_prob": 0.0019527672557160258}, {"id": 280, "seek": 145784, "start": 1480.32, "end": 1481.32, "text": " operator?", "tokens": [12973, 30], "temperature": 0.0, "avg_logprob": -0.25408756732940674, "compression_ratio": 1.180722891566265, "no_speech_prob": 0.0019527672557160258}, {"id": 281, "seek": 148132, "start": 1481.32, "end": 1492.1599999999999, "text": " So if you wanted to retrospectively see why it's done certain things, like the decision", "tokens": [407, 498, 291, 1415, 281, 34997, 3413, 536, 983, 309, 311, 1096, 1629, 721, 11, 411, 264, 3537], "temperature": 0.0, "avg_logprob": -0.1893153569054982, "compression_ratio": 1.52, "no_speech_prob": 0.001162589411251247}, {"id": 282, "seek": 148132, "start": 1492.1599999999999, "end": 1501.52, "text": " points, how it's decided which Kubernetes API calls to make, if your operator did something", "tokens": [2793, 11, 577, 309, 311, 3047, 597, 23145, 9362, 5498, 281, 652, 11, 498, 428, 12973, 630, 746], "temperature": 0.0, "avg_logprob": -0.1893153569054982, "compression_ratio": 1.52, "no_speech_prob": 0.001162589411251247}, {"id": 283, "seek": 148132, "start": 1501.52, "end": 1507.32, "text": " crazy and you wanted to look back and see why it did that, is there anything you would", "tokens": [3219, 293, 291, 1415, 281, 574, 646, 293, 536, 983, 309, 630, 300, 11, 307, 456, 1340, 291, 576], "temperature": 0.0, "avg_logprob": -0.1893153569054982, "compression_ratio": 1.52, "no_speech_prob": 0.001162589411251247}, {"id": 284, "seek": 150732, "start": 1507.32, "end": 1513.36, "text": " do in advance to the logging?", "tokens": [360, 294, 7295, 281, 264, 27991, 30], "temperature": 0.0, "avg_logprob": -0.14572118783926036, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.0006086385110393167}, {"id": 285, "seek": 150732, "start": 1513.36, "end": 1518.32, "text": " I think this is the summary of what we've learned is in these documents.", "tokens": [286, 519, 341, 307, 264, 12691, 295, 437, 321, 600, 3264, 307, 294, 613, 8512, 13], "temperature": 0.0, "avg_logprob": -0.14572118783926036, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.0006086385110393167}, {"id": 286, "seek": 150732, "start": 1518.32, "end": 1525.1599999999999, "text": " Because as I said, for example, the developers that started this project, they didn't have", "tokens": [1436, 382, 286, 848, 11, 337, 1365, 11, 264, 8849, 300, 1409, 341, 1716, 11, 436, 994, 380, 362], "temperature": 0.0, "avg_logprob": -0.14572118783926036, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.0006086385110393167}, {"id": 287, "seek": 150732, "start": 1525.1599999999999, "end": 1529.24, "text": " where to go and the best practices of how to name a metric.", "tokens": [689, 281, 352, 293, 264, 1151, 7525, 295, 577, 281, 1315, 257, 20678, 13], "temperature": 0.0, "avg_logprob": -0.14572118783926036, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.0006086385110393167}, {"id": 288, "seek": 150732, "start": 1529.24, "end": 1532.24, "text": " So they just named it how they thought.", "tokens": [407, 436, 445, 4926, 309, 577, 436, 1194, 13], "temperature": 0.0, "avg_logprob": -0.14572118783926036, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.0006086385110393167}, {"id": 289, "seek": 153224, "start": 1532.24, "end": 1539.96, "text": " And they did follow the Prometheus recommendations, but having a prefix of the operator has a big", "tokens": [400, 436, 630, 1524, 264, 2114, 649, 42209, 10434, 11, 457, 1419, 257, 46969, 295, 264, 12973, 575, 257, 955], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 290, "seek": 153224, "start": 1539.96, "end": 1543.32, "text": " impact for the users.", "tokens": [2712, 337, 264, 5022, 13], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 291, "seek": 153224, "start": 1543.32, "end": 1544.76, "text": " And not even the users.", "tokens": [400, 406, 754, 264, 5022, 13], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 292, "seek": 153224, "start": 1544.76, "end": 1551.1200000000001, "text": " When we are trying to understand how to use internal metrics for our uses, we also are", "tokens": [1133, 321, 366, 1382, 281, 1223, 577, 281, 764, 6920, 16367, 337, 527, 4960, 11, 321, 611, 366], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 293, "seek": 153224, "start": 1551.1200000000001, "end": 1555.44, "text": " struggling to understand where a metric came from, where is the code for it.", "tokens": [9314, 281, 1223, 689, 257, 20678, 1361, 490, 11, 689, 307, 264, 3089, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 294, "seek": 153224, "start": 1555.44, "end": 1561.44, "text": " So all of the summary of what we've learned is in those documents, and we plan to enrich", "tokens": [407, 439, 295, 264, 12691, 295, 437, 321, 600, 3264, 307, 294, 729, 8512, 11, 293, 321, 1393, 281, 18849], "temperature": 0.0, "avg_logprob": -0.14034933032411517, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00046000161091797054}, {"id": 295, "seek": 156144, "start": 1561.44, "end": 1563.44, "text": " it even further.", "tokens": [309, 754, 3052, 13], "temperature": 0.0, "avg_logprob": -0.31908719880240305, "compression_ratio": 1.3026315789473684, "no_speech_prob": 0.0007106483681127429}, {"id": 296, "seek": 156144, "start": 1563.44, "end": 1570.0, "text": " Thank you for your talk.", "tokens": [1044, 291, 337, 428, 751, 13], "temperature": 0.0, "avg_logprob": -0.31908719880240305, "compression_ratio": 1.3026315789473684, "no_speech_prob": 0.0007106483681127429}, {"id": 297, "seek": 156144, "start": 1570.0, "end": 1572.76, "text": " It was very interesting.", "tokens": [467, 390, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.31908719880240305, "compression_ratio": 1.3026315789473684, "no_speech_prob": 0.0007106483681127429}, {"id": 298, "seek": 156144, "start": 1572.76, "end": 1577.88, "text": " You mentioned code generation for the metrics package.", "tokens": [509, 2835, 3089, 5125, 337, 264, 16367, 7372, 13], "temperature": 0.0, "avg_logprob": -0.31908719880240305, "compression_ratio": 1.3026315789473684, "no_speech_prob": 0.0007106483681127429}, {"id": 299, "seek": 156144, "start": 1577.88, "end": 1584.56, "text": " My question is, do you plan on adding that to QBuilder and the operator SDK?", "tokens": [1222, 1168, 307, 11, 360, 291, 1393, 322, 5127, 300, 281, 1249, 28110, 793, 260, 293, 264, 12973, 37135, 30], "temperature": 0.0, "avg_logprob": -0.31908719880240305, "compression_ratio": 1.3026315789473684, "no_speech_prob": 0.0007106483681127429}, {"id": 300, "seek": 158456, "start": 1584.56, "end": 1593.72, "text": " Yeah, basically we are working on the operator SDK right now, because we want to build all", "tokens": [865, 11, 1936, 321, 366, 1364, 322, 264, 12973, 37135, 558, 586, 11, 570, 321, 528, 281, 1322, 439], "temperature": 0.0, "avg_logprob": -0.20874182710942535, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.003998017404228449}, {"id": 301, "seek": 158456, "start": 1593.72, "end": 1598.2, "text": " these tools, and we are thinking about them, but obviously this needs a lot of help of", "tokens": [613, 3873, 11, 293, 321, 366, 1953, 466, 552, 11, 457, 2745, 341, 2203, 257, 688, 295, 854, 295], "temperature": 0.0, "avg_logprob": -0.20874182710942535, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.003998017404228449}, {"id": 302, "seek": 158456, "start": 1598.2, "end": 1599.3999999999999, "text": " the community.", "tokens": [264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.20874182710942535, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.003998017404228449}, {"id": 303, "seek": 158456, "start": 1599.3999999999999, "end": 1607.08, "text": " And I am saying this because I'll enter like a personal note here and an idea, right?", "tokens": [400, 286, 669, 1566, 341, 570, 286, 603, 3242, 411, 257, 2973, 3637, 510, 293, 364, 1558, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20874182710942535, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.003998017404228449}, {"id": 304, "seek": 158456, "start": 1607.08, "end": 1613.6399999999999, "text": " Because the way I see it is like on QBuilder and on operator SDK, being able to, you just", "tokens": [1436, 264, 636, 286, 536, 309, 307, 411, 322, 1249, 28110, 793, 260, 293, 322, 12973, 37135, 11, 885, 1075, 281, 11, 291, 445], "temperature": 0.0, "avg_logprob": -0.20874182710942535, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.003998017404228449}, {"id": 305, "seek": 161364, "start": 1613.64, "end": 1618.1200000000001, "text": " go there and you say that you want to generate a project with monitoring, and it creates", "tokens": [352, 456, 293, 291, 584, 300, 291, 528, 281, 8460, 257, 1716, 365, 11028, 11, 293, 309, 7829], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 306, "seek": 161364, "start": 1618.1200000000001, "end": 1619.8400000000001, "text": " the monitoring package.", "tokens": [264, 11028, 7372, 13], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 307, "seek": 161364, "start": 1619.8400000000001, "end": 1625.92, "text": " Or if the operator already exists, you have a command to generate the monitoring package,", "tokens": [1610, 498, 264, 12973, 1217, 8198, 11, 291, 362, 257, 5622, 281, 8460, 264, 11028, 7372, 11], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 308, "seek": 161364, "start": 1625.92, "end": 1632.2800000000002, "text": " and then on QBuilder, like you use it to create an API or a controller, you'll have", "tokens": [293, 550, 322, 1249, 28110, 793, 260, 11, 411, 291, 764, 309, 281, 1884, 364, 9362, 420, 257, 10561, 11, 291, 603, 362], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 309, "seek": 161364, "start": 1632.2800000000002, "end": 1634.48, "text": " a similar command, but to create a new metric.", "tokens": [257, 2531, 5622, 11, 457, 281, 1884, 257, 777, 20678, 13], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 310, "seek": 161364, "start": 1634.48, "end": 1638.92, "text": " And you pass the type of the metric, the help, and the same for alerts.", "tokens": [400, 291, 1320, 264, 2010, 295, 264, 20678, 11, 264, 854, 11, 293, 264, 912, 337, 28061, 13], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 311, "seek": 161364, "start": 1638.92, "end": 1640.5200000000002, "text": " At least that's the way I see it.", "tokens": [1711, 1935, 300, 311, 264, 636, 286, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.16811517568734977, "compression_ratio": 1.7991803278688525, "no_speech_prob": 0.002084917388856411}, {"id": 312, "seek": 164052, "start": 1640.52, "end": 1643.8, "text": " And for me, so it makes sense.", "tokens": [400, 337, 385, 11, 370, 309, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 313, "seek": 164052, "start": 1643.8, "end": 1644.8, "text": " I agree.", "tokens": [286, 3986, 13], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 314, "seek": 164052, "start": 1644.8, "end": 1645.8, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 315, "seek": 164052, "start": 1645.8, "end": 1656.84, "text": " Hey, thank you for the talk.", "tokens": [1911, 11, 1309, 291, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 316, "seek": 164052, "start": 1656.84, "end": 1661.48, "text": " How much of a conventions that you talked about, aligned with open telemetry, is my", "tokens": [1012, 709, 295, 257, 33520, 300, 291, 2825, 466, 11, 17962, 365, 1269, 4304, 5537, 627, 11, 307, 452], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 317, "seek": 164052, "start": 1661.48, "end": 1662.48, "text": " opinion?", "tokens": [4800, 30], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 318, "seek": 164052, "start": 1662.48, "end": 1663.48, "text": " How much?", "tokens": [1012, 709, 30], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 319, "seek": 164052, "start": 1663.48, "end": 1664.48, "text": " What?", "tokens": [708, 30], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 320, "seek": 164052, "start": 1664.48, "end": 1667.0, "text": " Aligned with open telemetry.", "tokens": [967, 16690, 365, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.43940170089919844, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.0017057100776582956}, {"id": 321, "seek": 166700, "start": 1667.0, "end": 1670.92, "text": " Most of them are aligned with open telemetry, actually.", "tokens": [4534, 295, 552, 366, 17962, 365, 1269, 4304, 5537, 627, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 322, "seek": 166700, "start": 1670.92, "end": 1673.56, "text": " But these are specific for operators.", "tokens": [583, 613, 366, 2685, 337, 19077, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 323, "seek": 166700, "start": 1673.56, "end": 1674.56, "text": " That's the idea.", "tokens": [663, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 324, "seek": 166700, "start": 1674.56, "end": 1678.12, "text": " The idea is that you have a central place where you can get the information.", "tokens": [440, 1558, 307, 300, 291, 362, 257, 5777, 1081, 689, 291, 393, 483, 264, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 325, "seek": 166700, "start": 1678.12, "end": 1683.6, "text": " And by the way, if someone is creating a new operator and has an insight, we encourage", "tokens": [400, 538, 264, 636, 11, 498, 1580, 307, 4084, 257, 777, 12973, 293, 575, 364, 11269, 11, 321, 5373], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 326, "seek": 166700, "start": 1683.6, "end": 1689.64, "text": " others to contribute to the documentation and teach others and share the information.", "tokens": [2357, 281, 10586, 281, 264, 14333, 293, 2924, 2357, 293, 2073, 264, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 327, "seek": 166700, "start": 1689.64, "end": 1690.64, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.13642283408872544, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.001407582312822342}, {"id": 328, "seek": 169064, "start": 1690.64, "end": 1698.8000000000002, "text": " Basically, I think we align with open telemetry conventions, but we add more ideas to it to", "tokens": [8537, 11, 286, 519, 321, 7975, 365, 1269, 4304, 5537, 627, 33520, 11, 457, 321, 909, 544, 3487, 281, 309, 281], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 329, "seek": 169064, "start": 1698.8000000000002, "end": 1709.4, "text": " operate.", "tokens": [9651, 13], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 330, "seek": 169064, "start": 1709.4, "end": 1710.4, "text": " I think that's it.", "tokens": [286, 519, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 331, "seek": 169064, "start": 1710.4, "end": 1711.4, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 332, "seek": 169064, "start": 1711.4, "end": 1712.4, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 333, "seek": 169064, "start": 1712.4, "end": 1713.4, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3091867651258196, "compression_ratio": 1.4553571428571428, "no_speech_prob": 0.0012885385658591986}, {"id": 334, "seek": 171340, "start": 1713.4, "end": 1738.5600000000002, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 1.0, "avg_logprob": -1.3050225121634347, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0013523957459256053}], "language": "en"}