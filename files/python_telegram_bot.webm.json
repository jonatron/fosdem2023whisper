{"text": " Hello, everybody. Welcome to the Python, the room of UsDem. I'm really happy to welcome all of you here and to welcome Marc-Andr\u00e9 for his talk, an introduction to async programming. Thanks for everyone for coming, also, so early for this talk that I'm really looking forward to see with you. Thank you very much and thank you for the introduction. It's really nice to see so many people here on a Sunday morning at 9 o'clock. I'm really happy about this. I wouldn't have expected so many people. I hope this is going to be interesting for you. There is a lot of text on these slides. I uploaded the slides to the website so you don't have to, you know, write down everything and read everything. There's a lot to talk about in async. So what I wanted to do is I wanted to give a short introduction to async and I wanted to frame everything into writing a telegram bot because that was, you know, the motivation behind the talk and how I came to doing this. So a few words about myself. I'm Marc-Andr\u00e9 Lamberg. I'm from D\u00fcsseldorf in Germany. I've been with Python for a very long time since 1994. I'm a core developer. I've done lots of work in the various organizations around Python. So I was your Python chair, for example. I was on the board of the PSF. I'm a PSF fellow and I've done lots of work in that area. In my day job, I am a consulting CTO or do senior architectures. Also do coaching a bit. So if you have a need in that area, just ping me. So the motivation for the talk is writing a telegram and a spam bot. Now, why did we have to do that? We have a user group in Germany, the Python meeting D\u00fcsseldorf, and we're using a telegram group to communicate. And early last year, we started seeing lots of signups to that group because it's a public group. Anyone can just sign up to that group. We started seeing lots of signups from strange people. And the people then usually started to, you know, send spam, send crypto links, you know, link spam. Many of those people were actually not people, but bots, and they scraped the contact info and started sending DMs to the various members. So it was, you know, getting to a point where it was not possible for us as admins to handle this anymore because most of these people or bots, they were actually signing up during the night. So there was no one there to handle these. And so the idea was to write a bot which basically tries to, you know, check whether people are human, check whether the signups are actually from people who know Python. And that's what it did last year. So the idea was to have a scalable bot because it needs to run 24-7. It also needs to be very stable because, you know, at night, no one is there to basically restart it. We needed something that is low resource because we wanted to have it on one of the VMs that we have to run. And so we decided to look out for, you know, a library that we could use. And there is a very nice library called Pyrogram, which you can use for creating these bots. It's LTPL3. It's fairly new. It's well documented, and it's actually maintained. So basically all the checks are there. And we started to use that, and we had great success with it. It is an async library. So what is this asynchronous programming? I'm going to go through the three different models that you have for execution in Python. And let's start with the synchronous execution. So what is synchronous execution? Basically you write your program from top to bottom. The Python interpreter then takes all the different steps that you have in your program and runs them one by one, one after the other. So there's no parallel processing going on. Everything happens one after the other thing. If you have to wait for IO, for example, then you just do the cord just sits there, doesn't do anything. And of course, waiting is not really very efficient. So what can you do about that? If you want to scale up. One way to scale up in Python is to use threads. And probably many of you know about the GIL. I'm going to talk about that in a bit. But let's just mention what was threaded programming is. Thread programming is where the operating system basically assigns slices to your application. And then each slice can run for a certain amount of time. And then the operating system switches to the next slice and the next thread. So everything is controlled by the OS. This is a very nice and very elegant way to scale up. You can use all the cores that you have in your CPU. You can, you know, in the past you usually had multiple processes in the servers and you could use those multiple processes as well. There's one catch though with thread programming because it's controlled by the OS and not by the application. So not by Python. It is possible for two threads to try to access the same object, let's say, or same memory area in your application and do things on those memory areas. For example, you know, take a list, append to it, delete from it and so on. And if two threads start doing that at the same time, you can have clashes. And in order to prevent that, because you don't want to have data loss, for example, you have to put locks around these things to make everything work. So there is a bit of extra work to be done there. You have to consider how things work in the thread environment and you have to put locks around areas that can be shared between the different threads that you have. It is an efficient use of resources. So this is actually something that people try to get working. With Python, it's a bit harder. And because it's a bit harder, some years ago, async support was added to Python. So what is asynchronous? Asynchronous is basically focusing on a single thread, on a single core. It looks very much like a synchronous program, except that whenever you do IO, the application, Python in that case, can then say, okay, I'm going to run this function until I hit a spot where I have IO, for example, or I have to wait for something. And then I give back control to something called an event loop. And that event loop is then going to take, it's going to go through the list of everything that is scheduled to be executed and then just run the next thing that's on that list. And so whenever you wait for IO, you can tell the program, okay, I'm done with this part of the application. And now I'm going to switch to a different part and run that part. So that's a way to work around the threading issues I just talked about. It's also a way to write code that scales up very neatly, very fast. It's a bit limited in terms of focusing on just one core. So you, for example, you cannot use multiple cores that way, not that easily. If you want to use multiple cores, you can push work that is being done in the application that's not running Python on these other cores, that's certainly possible. But if you want to scale up, use all the cores, then you basically have to use multiple of these applications, run them in different processes, and then use up all the cores that you have. There's one catch with this. I mean, there's no free lunch, right? So all the parts in your code have to collaborate. Because it's application driven, all the parts that you have need to have certain places where they say, okay, I'm going to give up control back to the event loop at this point because I'm waiting for something. Because Python cannot know that you're trying to wait for something. And so you have to tell Python that this is a good place to give up control. Now, why do we need this? And this slide is about the global interpreter lock. How many of you know the global interpreter lock? Just a few. That's interesting. So what Python does is it keeps a one big lock around the Python virtual machine that executes the Python bytecode. And it does this because it wants to support multiple threads. But at the time when this was added, threading was actually very new. Python is, it's more than 30 years old now. So there's been a lot of development going on since Python started. And because Guido wanted to start supporting threading right from the beginning, he added this global interpreter lock to make sure that the logic that's inside Python is only used by one thread at any point in time. So what happens is the Python starts running code, Python code in one thread, then reaches a certain point and then gives up control back to the OS and says, okay, you can switch to a different thread now. But it does this under the control of this global interpreter lock. So it makes sure that no other thread is running Python at the moment. When it gives up control to a different thread, then that thread will have been waiting for the Python interpreter lock to get the lock. And then we'll start executing. And this goes on for all the threads that you have in your application. So in a multi-threaded program that's running Python, you can just have one thread execute Python code at any point in time. And this is something that, of course, is not very scalable. It's also not a very big issue, as some may tell you. Because if you're clever and you put all the logic that you need to run a multiple course or multiple threads into parts of the program that don't need Python, for example, if you're running machine learning and you want to train a model, then you can just easily push off everything into C code, which doesn't need Python. And that can very well run next to Python in another thread. So that's certainly possible. But, of course, sometimes you don't have a chance to do that. And then you need to look for other things. And this is where async becomes very nice. So let's have a look at how thread code executes in Python. The image on the right basically explains how Python works. So you have three threads. The orange is Python running. Then the yellow is basically the thread, the Python interpreter in those threads waiting for the gil. And then you have some waiting for IO that happens in between. So if you look closely, you will see that it's not a very efficient use here, because there's lots of waiting, lots of yellow in there waiting for the gil, lots of blue waiting for IO. Let's have a closer look at this. So this again is the picture that I had on the other slide. And I moved out all the waiting and all the execution. And if you move all the execution together, you will see that only one thread is running at any point in time. So the other threads are basically just sitting there doing nothing. Now, how can you work around this? You can use async programming for this. And async programming has the need feature that you can actually saturate a single core very efficiently without doing too much work. So again, you have the execution here. You don't have three threads. This is just one thread that you have for one core, but you have three tasks running in that one thread. And the different tasks, they share the execution. And again, you have the orange here executing Python. You have some waiting for IO in here or could also be waiting for calculations to happen. And if you move everything together, you will see that it's really the thread, the core is saturated. So everything is working out nicely. And it's very efficient. So how does this work? How many of you know coroutines? Okay, about like one third. So a coroutine basically is very much like a normal function, except that it's possible to have certain spots in the coroutine in the function where it says, okay, at this point, you can give up control back to the caller of that function. And this is essentially how async programming works. You have something called an event loop. The event loop calls these coroutines. The coroutine executes until it hits one of these the spots where you can give up control. The function, the coroutine gives back control to the event loop at that point. And then the event loop can execute something else in your application. And then at a later point, it comes back to that coroutine and continues executing where it left off. In order to make that easy to define and easy to use in Python, we have new keywords. We have async def, which is a way to define these coroutines. And we have these await statements in Python, which are basically places where the coroutine says, okay, you can give up control and you can pass back control to the event loop because I'm waiting for, let's say, IO or for a longer running calculation that you want to do. And everything around this, all the support for this is bundled in this package called async IO, which is part of the standard library. So let's have a look at how this works in Python to compare synchronous code and async code. So on the left, you have a very simple function. You have a time sleep in there, which is like a simulation for the IO. So something, the application needs to wait for something. And then you call that function. And if you run the simple example, then you see that, you know, it starts executing, it starts working. Then it sleeps for two seconds, and then it's done. And then it's the end of that function. Now, in the async case, it works a bit differently. So what you do is you put the async in front of the function. So you have to turn it into a coroutine. And then inside that function, we use the await statement to say, okay, at this point, I can give up control back to the event loop. And so what happens here is that you have a special function called async IO sleep, which is a way to, you know, wait for a certain amount of time in async. But when waiting for these two seconds, you can actually go back and you can execute something else. It's not possible to use await and then time dot sleep for this, because time dot sleep is actually a blocking function, right? It doesn't give back control. So you have to make sure that whatever you use with await is actually a coroutine so that it can pass back control to your coroutine that's calling this coroutine. And this is what I meant with everything has to collaborate. If you have places in your application that are not compatible with async, you have to be careful and you have to use certain workarounds to make it happen. So the next thing is that, you know, now you have a coroutine calling the coroutine will do nothing. Basically, all that happens is you get back a coroutine object. So it doesn't run. So in order to run it, you have to actually start the coroutine inside the event loop. And this is what async IO dot run does at the very bottom. And this, if you look at it, it takes, it defines two tasks. So basically two instances of that coroutine puts them into this tuple. The tuple is passed to this async IO gather, which is a special function I'm going to come to in one of the next slides. It basically just takes these, the coroutines, creates task objects, and then executes them until all of them are done and then passes back control. So this is how you would run an async application. I already went through these so I can basically just skip these. So what are the, you know, things in the async IO package or module? A very important function is this async IO run. This is basically the function that needs to be called in order to set up the event loop to run everything in that event loop. You typically just have one of these calls in your application, basically starting the event loop and then running anything that's being scheduled. Then you have this gather function. Gather is, like I already mentioned, it's a function where you can pass in coroutines or tasks. And then it runs all these tasks until completion and then it returns. Async IO sleeper already mentioned. You also have a couple of functions down here for waiting for certain things. So sometimes in an application you need to synchronize between various different parts. So there are some handy functions for this as well. Now what is this task object? I keep mentioning. Task objects are basically just coroutine calls that are being scheduled. And it's a way for the event loop to manage everything that happens in the event loop. So whenever something is scheduled to be run, you create a task object. And this is done behind the scenes for you. You don't have to create these objects yourself. In fact, you should not create these objects yourself. You should always use one of the functions for this, like this create task that you have down here. And then these task objects go into the event loop or run and everything happens for you. There are also some query functions down here if you're interested in what's currently scheduled on the event loop. You can have a look at the documentation for those. So how does this event loop work? It's basically just a way to do the same kind of management as the OS is doing for threads, except that it's done in Python. And the async.io package manages one of these event loops. Now event loops can actually be defined by multiple different libraries. But what's important is that there should only be one event loop per thread. So you can have multiple threads, of course, also run. Then again, you hit the same kind of roadblock as you've seen with the GIL. But there might be ways to, in your application, to make use of that. So that would be possible as well. Typically, what you have in an async program is you just have a single thread. And so you just call this run function once. Now, I mentioned blocking code. So blocking code basically is code that doesn't collaborate with this async logic. And you have that quite often in Python. For example, let's say you're using one of the database modules. Not one of the async ones, but the regular ones. Those will all be synchronous. So you call, let's say, an execute to run some SQL. And that will actually wait until the database comes back with results. So in order to run this kind of code in an async application, you have to use special functions. There's a very nice function called async.io2 thread, which was added in Python 3.9, which makes this easy. So what these functions do is say they spin up a thread in your async application, run the code inside that thread, and then pass back control via the threading logic to your event loop. So you can still run synchronous code because the synchronous code is most likely going to give up the GIL. For example, if you have a good database module, then when you execute something, typically what these database modules do is they give back control to other threads running Python code because they're just running C code at that time. So this is a way to make everything work together. And of course, there's lots more. I'm not going to talk about these things because I don't have enough time for that. In fact, I'm already almost out of time. So I have to speed up a bit. Let's just do a very quick overview of what's in the async ecosystem. So of course, we have the async.io standard library package. We have event loops inside the async.io package. If you want fast loops, then you can use UV loop, which is a faster implementation, speeds up your async by almost four times. You can also have a look at other stacks that implement event loops, like Trio, for example, where you can use the package any.io, which abstracts these things. So you can use basically Trio or you can use async.io loops underneath in your application when using any.io and it abstracts away all the details. Now, there's a rather large system of modules and packages around the async world in Python. Many of these are grouped under the aio-lips. So if you go to GitHub to that URL, then you will find lots of examples there. There are database packages there. There are things for doing HTTP, DNS, and so on. Something to watch out is the database modules typically don't support transactions, which can be a bummer sometimes. At the higher level, you have, of course, web frameworks again because, you know, everyone loves web frameworks. And, of course, you have API frameworks. The most popular one right now is FastAPI for doing the REST APIs, and then Strawberry is coming very strongly as a GraphQL server. Both operate async. Even Django does, or starts, is starting to do async right now. It's not fully there yet. If you're using Flask for synchronous code, then you might want to look at a quad, which is like an async implementation using a similar API. And the most famous one probably in the async space is Tornado, which some of you may know. It's very fast. Right. So let's go back to the bot. If you want to see the bot code, it's on GitHub. Just search for Eugenics Telegram, and then you'll find it. How does it work? Very easy. You just subclass the client of that package. You do some configuration. You do some observability, so logging for things. I'm lazy, so I'm just, you know, put all the admin messages into another telegram chat that I can manage so I can see what's happening without having to go to the server. Because we actually want to catch all the messages of these people signing up to the telegram group, and not just people who want to run bot commands, you know, slash something. We have to use the catch all in here, and that's also why we need something that's very scalable, because it literally, the bot sees all the messages that go into that group, and it has to handle all these messages. And then what you do is basically you just do these awaits to whenever you have to do IO. And if you look at it, it looks very much like synchronous code, except that you have these awaits in front of certain things. Wherever something happens, where you need to do some IO, you put the await in front of it, and then everything else looks very natural, looks like a very, you know, very much like a synchronous program. So what are the results of doing this? Writing this bot, it's actually helped us a lot. We've, you know, had over almost 800 spam signups since April 2022, when we started to use it. And this has, I mean, this is one part, this is just the admin part that saved us a lot of work, but of course, you know, every single signup would have cost spam messages. And so that was very successful. So the time spent on actually writing things is, was well invested and basically mission accomplished. So what's the main takeaway of the talk? I think it's great. And give it a try if you can. Thank you for your attention. Thank you Mark Andre. Thanks everyone for attending this talk. Don't hesitate to reach to Mark Andre if you have any question or want to discuss this further. Thanks a lot. Thank you. Thank you very much for coming. Let me just do a picture. Excellent.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.16, "text": " Hello, everybody. Welcome to the Python, the room of UsDem. I'm really happy to welcome", "tokens": [50364, 2425, 11, 2201, 13, 4027, 281, 264, 15329, 11, 264, 1808, 295, 4958, 35, 443, 13, 286, 478, 534, 2055, 281, 2928, 50922], "temperature": 0.0, "avg_logprob": -0.31294261450055, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.3479524254798889}, {"id": 1, "seek": 0, "start": 11.16, "end": 18.5, "text": " all of you here and to welcome Marc-Andr\u00e9 for his talk, an introduction to async programming.", "tokens": [50922, 439, 295, 291, 510, 293, 281, 2928, 18460, 12, 5289, 10521, 337, 702, 751, 11, 364, 9339, 281, 382, 34015, 9410, 13, 51289], "temperature": 0.0, "avg_logprob": -0.31294261450055, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.3479524254798889}, {"id": 2, "seek": 0, "start": 18.5, "end": 23.92, "text": " Thanks for everyone for coming, also, so early for this talk that I'm really looking forward", "tokens": [51289, 2561, 337, 1518, 337, 1348, 11, 611, 11, 370, 2440, 337, 341, 751, 300, 286, 478, 534, 1237, 2128, 51560], "temperature": 0.0, "avg_logprob": -0.31294261450055, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.3479524254798889}, {"id": 3, "seek": 0, "start": 23.92, "end": 27.560000000000002, "text": " to see with you. Thank you very much and thank you for the", "tokens": [51560, 281, 536, 365, 291, 13, 1044, 291, 588, 709, 293, 1309, 291, 337, 264, 51742], "temperature": 0.0, "avg_logprob": -0.31294261450055, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.3479524254798889}, {"id": 4, "seek": 2756, "start": 27.599999999999998, "end": 32.519999999999996, "text": " introduction. It's really nice to see so many people here on a Sunday morning at 9 o'clock.", "tokens": [50366, 9339, 13, 467, 311, 534, 1481, 281, 536, 370, 867, 561, 510, 322, 257, 7776, 2446, 412, 1722, 277, 6, 9023, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1492873246107644, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.09074888378381729}, {"id": 5, "seek": 2756, "start": 32.519999999999996, "end": 38.96, "text": " I'm really happy about this. I wouldn't have expected so many people. I hope this is going to be", "tokens": [50612, 286, 478, 534, 2055, 466, 341, 13, 286, 2759, 380, 362, 5176, 370, 867, 561, 13, 286, 1454, 341, 307, 516, 281, 312, 50934], "temperature": 0.0, "avg_logprob": -0.1492873246107644, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.09074888378381729}, {"id": 6, "seek": 2756, "start": 38.96, "end": 44.72, "text": " interesting for you. There is a lot of text on these slides. I uploaded the slides to the", "tokens": [50934, 1880, 337, 291, 13, 821, 307, 257, 688, 295, 2487, 322, 613, 9788, 13, 286, 17135, 264, 9788, 281, 264, 51222], "temperature": 0.0, "avg_logprob": -0.1492873246107644, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.09074888378381729}, {"id": 7, "seek": 2756, "start": 44.72, "end": 50.239999999999995, "text": " website so you don't have to, you know, write down everything and read everything. There's a lot", "tokens": [51222, 3144, 370, 291, 500, 380, 362, 281, 11, 291, 458, 11, 2464, 760, 1203, 293, 1401, 1203, 13, 821, 311, 257, 688, 51498], "temperature": 0.0, "avg_logprob": -0.1492873246107644, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.09074888378381729}, {"id": 8, "seek": 2756, "start": 50.239999999999995, "end": 56.120000000000005, "text": " to talk about in async. So what I wanted to do is I wanted to give a short introduction to async", "tokens": [51498, 281, 751, 466, 294, 382, 34015, 13, 407, 437, 286, 1415, 281, 360, 307, 286, 1415, 281, 976, 257, 2099, 9339, 281, 382, 34015, 51792], "temperature": 0.0, "avg_logprob": -0.1492873246107644, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.09074888378381729}, {"id": 9, "seek": 5612, "start": 56.12, "end": 61.68, "text": " and I wanted to frame everything into writing a telegram bot because that was, you know,", "tokens": [50364, 293, 286, 1415, 281, 3920, 1203, 666, 3579, 257, 4304, 1342, 10592, 570, 300, 390, 11, 291, 458, 11, 50642], "temperature": 0.0, "avg_logprob": -0.14390724221455684, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.016442028805613518}, {"id": 10, "seek": 5612, "start": 61.68, "end": 69.24, "text": " the motivation behind the talk and how I came to doing this. So a few words about myself.", "tokens": [50642, 264, 12335, 2261, 264, 751, 293, 577, 286, 1361, 281, 884, 341, 13, 407, 257, 1326, 2283, 466, 2059, 13, 51020], "temperature": 0.0, "avg_logprob": -0.14390724221455684, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.016442028805613518}, {"id": 11, "seek": 5612, "start": 69.24, "end": 75.32, "text": " I'm Marc-Andr\u00e9 Lamberg. I'm from D\u00fcsseldorf in Germany. I've been with Python for a very", "tokens": [51020, 286, 478, 18460, 12, 5289, 10521, 18825, 6873, 13, 286, 478, 490, 413, 37838, 67, 28030, 294, 7244, 13, 286, 600, 668, 365, 15329, 337, 257, 588, 51324], "temperature": 0.0, "avg_logprob": -0.14390724221455684, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.016442028805613518}, {"id": 12, "seek": 5612, "start": 75.32, "end": 82.8, "text": " long time since 1994. I'm a core developer. I've done lots of work in the various organizations", "tokens": [51324, 938, 565, 1670, 22736, 13, 286, 478, 257, 4965, 10754, 13, 286, 600, 1096, 3195, 295, 589, 294, 264, 3683, 6150, 51698], "temperature": 0.0, "avg_logprob": -0.14390724221455684, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.016442028805613518}, {"id": 13, "seek": 8280, "start": 82.88, "end": 89.12, "text": " around Python. So I was your Python chair, for example. I was on the board of the PSF. I'm a", "tokens": [50368, 926, 15329, 13, 407, 286, 390, 428, 15329, 6090, 11, 337, 1365, 13, 286, 390, 322, 264, 3150, 295, 264, 8168, 37, 13, 286, 478, 257, 50680], "temperature": 0.0, "avg_logprob": -0.17635126980868251, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.016011644154787064}, {"id": 14, "seek": 8280, "start": 89.12, "end": 96.2, "text": " PSF fellow and I've done lots of work in that area. In my day job, I am a consulting CTO or do", "tokens": [50680, 8168, 37, 7177, 293, 286, 600, 1096, 3195, 295, 589, 294, 300, 1859, 13, 682, 452, 786, 1691, 11, 286, 669, 257, 23682, 383, 15427, 420, 360, 51034], "temperature": 0.0, "avg_logprob": -0.17635126980868251, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.016011644154787064}, {"id": 15, "seek": 8280, "start": 96.2, "end": 104.47999999999999, "text": " senior architectures. Also do coaching a bit. So if you have a need in that area, just ping me.", "tokens": [51034, 7965, 6331, 1303, 13, 2743, 360, 15818, 257, 857, 13, 407, 498, 291, 362, 257, 643, 294, 300, 1859, 11, 445, 26151, 385, 13, 51448], "temperature": 0.0, "avg_logprob": -0.17635126980868251, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.016011644154787064}, {"id": 16, "seek": 8280, "start": 104.47999999999999, "end": 111.03999999999999, "text": " So the motivation for the talk is writing a telegram and a spam bot. Now, why did we have to", "tokens": [51448, 407, 264, 12335, 337, 264, 751, 307, 3579, 257, 4304, 1342, 293, 257, 24028, 10592, 13, 823, 11, 983, 630, 321, 362, 281, 51776], "temperature": 0.0, "avg_logprob": -0.17635126980868251, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.016011644154787064}, {"id": 17, "seek": 11104, "start": 111.12, "end": 117.36000000000001, "text": " do that? We have a user group in Germany, the Python meeting D\u00fcsseldorf, and we're using a", "tokens": [50368, 360, 300, 30, 492, 362, 257, 4195, 1594, 294, 7244, 11, 264, 15329, 3440, 413, 37838, 67, 28030, 11, 293, 321, 434, 1228, 257, 50680], "temperature": 0.0, "avg_logprob": -0.13925415982482253, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.010785473510622978}, {"id": 18, "seek": 11104, "start": 117.36000000000001, "end": 125.32000000000001, "text": " telegram group to communicate. And early last year, we started seeing lots of signups to that", "tokens": [50680, 4304, 1342, 1594, 281, 7890, 13, 400, 2440, 1036, 1064, 11, 321, 1409, 2577, 3195, 295, 1465, 7528, 281, 300, 51078], "temperature": 0.0, "avg_logprob": -0.13925415982482253, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.010785473510622978}, {"id": 19, "seek": 11104, "start": 125.32000000000001, "end": 130.12, "text": " group because it's a public group. Anyone can just sign up to that group. We started seeing", "tokens": [51078, 1594, 570, 309, 311, 257, 1908, 1594, 13, 14643, 393, 445, 1465, 493, 281, 300, 1594, 13, 492, 1409, 2577, 51318], "temperature": 0.0, "avg_logprob": -0.13925415982482253, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.010785473510622978}, {"id": 20, "seek": 11104, "start": 130.12, "end": 137.60000000000002, "text": " lots of signups from strange people. And the people then usually started to, you know,", "tokens": [51318, 3195, 295, 1465, 7528, 490, 5861, 561, 13, 400, 264, 561, 550, 2673, 1409, 281, 11, 291, 458, 11, 51692], "temperature": 0.0, "avg_logprob": -0.13925415982482253, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.010785473510622978}, {"id": 21, "seek": 13760, "start": 137.68, "end": 147.12, "text": " send spam, send crypto links, you know, link spam. Many of those people were actually not people,", "tokens": [50368, 2845, 24028, 11, 2845, 17240, 6123, 11, 291, 458, 11, 2113, 24028, 13, 5126, 295, 729, 561, 645, 767, 406, 561, 11, 50840], "temperature": 0.0, "avg_logprob": -0.152564831783897, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.013367675244808197}, {"id": 22, "seek": 13760, "start": 147.12, "end": 152.76, "text": " but bots, and they scraped the contact info and started sending DMs to the various members. So", "tokens": [50840, 457, 35410, 11, 293, 436, 13943, 3452, 264, 3385, 13614, 293, 1409, 7750, 15322, 82, 281, 264, 3683, 2679, 13, 407, 51122], "temperature": 0.0, "avg_logprob": -0.152564831783897, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.013367675244808197}, {"id": 23, "seek": 13760, "start": 152.76, "end": 160.68, "text": " it was, you know, getting to a point where it was not possible for us as admins to handle this", "tokens": [51122, 309, 390, 11, 291, 458, 11, 1242, 281, 257, 935, 689, 309, 390, 406, 1944, 337, 505, 382, 5910, 1292, 281, 4813, 341, 51518], "temperature": 0.0, "avg_logprob": -0.152564831783897, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.013367675244808197}, {"id": 24, "seek": 13760, "start": 160.68, "end": 165.72, "text": " anymore because most of these people or bots, they were actually signing up during the night. So", "tokens": [51518, 3602, 570, 881, 295, 613, 561, 420, 35410, 11, 436, 645, 767, 13393, 493, 1830, 264, 1818, 13, 407, 51770], "temperature": 0.0, "avg_logprob": -0.152564831783897, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.013367675244808197}, {"id": 25, "seek": 16572, "start": 165.76, "end": 171.84, "text": " there was no one there to handle these. And so the idea was to write a bot which basically tries to,", "tokens": [50366, 456, 390, 572, 472, 456, 281, 4813, 613, 13, 400, 370, 264, 1558, 390, 281, 2464, 257, 10592, 597, 1936, 9898, 281, 11, 50670], "temperature": 0.0, "avg_logprob": -0.13679451303383738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.004595058970153332}, {"id": 26, "seek": 16572, "start": 171.84, "end": 178.72, "text": " you know, check whether people are human, check whether the signups are actually from people", "tokens": [50670, 291, 458, 11, 1520, 1968, 561, 366, 1952, 11, 1520, 1968, 264, 1465, 7528, 366, 767, 490, 561, 51014], "temperature": 0.0, "avg_logprob": -0.13679451303383738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.004595058970153332}, {"id": 27, "seek": 16572, "start": 178.72, "end": 187.32, "text": " who know Python. And that's what it did last year. So the idea was to have a scalable bot", "tokens": [51014, 567, 458, 15329, 13, 400, 300, 311, 437, 309, 630, 1036, 1064, 13, 407, 264, 1558, 390, 281, 362, 257, 38481, 10592, 51444], "temperature": 0.0, "avg_logprob": -0.13679451303383738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.004595058970153332}, {"id": 28, "seek": 16572, "start": 187.32, "end": 194.68, "text": " because it needs to run 24-7. It also needs to be very stable because, you know, at night,", "tokens": [51444, 570, 309, 2203, 281, 1190, 4022, 12, 22, 13, 467, 611, 2203, 281, 312, 588, 8351, 570, 11, 291, 458, 11, 412, 1818, 11, 51812], "temperature": 0.0, "avg_logprob": -0.13679451303383738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.004595058970153332}, {"id": 29, "seek": 19468, "start": 194.76000000000002, "end": 200.76000000000002, "text": " no one is there to basically restart it. We needed something that is low resource because", "tokens": [50368, 572, 472, 307, 456, 281, 1936, 21022, 309, 13, 492, 2978, 746, 300, 307, 2295, 7684, 570, 50668], "temperature": 0.0, "avg_logprob": -0.14093487805659227, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0011444177944213152}, {"id": 30, "seek": 19468, "start": 200.76000000000002, "end": 207.88, "text": " we wanted to have it on one of the VMs that we have to run. And so we decided to look out for,", "tokens": [50668, 321, 1415, 281, 362, 309, 322, 472, 295, 264, 18038, 82, 300, 321, 362, 281, 1190, 13, 400, 370, 321, 3047, 281, 574, 484, 337, 11, 51024], "temperature": 0.0, "avg_logprob": -0.14093487805659227, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0011444177944213152}, {"id": 31, "seek": 19468, "start": 207.88, "end": 213.16, "text": " you know, a library that we could use. And there is a very nice library called Pyrogram,", "tokens": [51024, 291, 458, 11, 257, 6405, 300, 321, 727, 764, 13, 400, 456, 307, 257, 588, 1481, 6405, 1219, 9953, 340, 1342, 11, 51288], "temperature": 0.0, "avg_logprob": -0.14093487805659227, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0011444177944213152}, {"id": 32, "seek": 19468, "start": 213.16, "end": 222.24, "text": " which you can use for creating these bots. It's LTPL3. It's fairly new. It's well documented,", "tokens": [51288, 597, 291, 393, 764, 337, 4084, 613, 35410, 13, 467, 311, 441, 16804, 43, 18, 13, 467, 311, 6457, 777, 13, 467, 311, 731, 23007, 11, 51742], "temperature": 0.0, "avg_logprob": -0.14093487805659227, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0011444177944213152}, {"id": 33, "seek": 22224, "start": 222.32000000000002, "end": 228.76000000000002, "text": " and it's actually maintained. So basically all the checks are there. And we started to use that,", "tokens": [50368, 293, 309, 311, 767, 17578, 13, 407, 1936, 439, 264, 13834, 366, 456, 13, 400, 321, 1409, 281, 764, 300, 11, 50690], "temperature": 0.0, "avg_logprob": -0.14182341376016305, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0007541780942119658}, {"id": 34, "seek": 22224, "start": 228.76000000000002, "end": 237.60000000000002, "text": " and we had great success with it. It is an async library. So what is this asynchronous", "tokens": [50690, 293, 321, 632, 869, 2245, 365, 309, 13, 467, 307, 364, 382, 34015, 6405, 13, 407, 437, 307, 341, 49174, 51132], "temperature": 0.0, "avg_logprob": -0.14182341376016305, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0007541780942119658}, {"id": 35, "seek": 22224, "start": 237.60000000000002, "end": 243.56, "text": " programming? I'm going to go through the three different models that you have for execution", "tokens": [51132, 9410, 30, 286, 478, 516, 281, 352, 807, 264, 1045, 819, 5245, 300, 291, 362, 337, 15058, 51430], "temperature": 0.0, "avg_logprob": -0.14182341376016305, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0007541780942119658}, {"id": 36, "seek": 22224, "start": 243.56, "end": 250.24, "text": " in Python. And let's start with the synchronous execution. So what is synchronous execution?", "tokens": [51430, 294, 15329, 13, 400, 718, 311, 722, 365, 264, 44743, 15058, 13, 407, 437, 307, 44743, 15058, 30, 51764], "temperature": 0.0, "avg_logprob": -0.14182341376016305, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0007541780942119658}, {"id": 37, "seek": 25024, "start": 250.24, "end": 255.84, "text": " Basically you write your program from top to bottom. The Python interpreter then takes all", "tokens": [50364, 8537, 291, 2464, 428, 1461, 490, 1192, 281, 2767, 13, 440, 15329, 34132, 550, 2516, 439, 50644], "temperature": 0.0, "avg_logprob": -0.10606934143616273, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0033166345674544573}, {"id": 38, "seek": 25024, "start": 255.84, "end": 260.32, "text": " the different steps that you have in your program and runs them one by one, one after the other.", "tokens": [50644, 264, 819, 4439, 300, 291, 362, 294, 428, 1461, 293, 6676, 552, 472, 538, 472, 11, 472, 934, 264, 661, 13, 50868], "temperature": 0.0, "avg_logprob": -0.10606934143616273, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0033166345674544573}, {"id": 39, "seek": 25024, "start": 261.12, "end": 268.40000000000003, "text": " So there's no parallel processing going on. Everything happens one after the other thing.", "tokens": [50908, 407, 456, 311, 572, 8952, 9007, 516, 322, 13, 5471, 2314, 472, 934, 264, 661, 551, 13, 51272], "temperature": 0.0, "avg_logprob": -0.10606934143616273, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0033166345674544573}, {"id": 40, "seek": 25024, "start": 268.40000000000003, "end": 274.08, "text": " If you have to wait for IO, for example, then you just do the cord just sits there, doesn't do", "tokens": [51272, 759, 291, 362, 281, 1699, 337, 39839, 11, 337, 1365, 11, 550, 291, 445, 360, 264, 12250, 445, 12696, 456, 11, 1177, 380, 360, 51556], "temperature": 0.0, "avg_logprob": -0.10606934143616273, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0033166345674544573}, {"id": 41, "seek": 25024, "start": 274.08, "end": 280.0, "text": " anything. And of course, waiting is not really very efficient. So what can you do about that?", "tokens": [51556, 1340, 13, 400, 295, 1164, 11, 3806, 307, 406, 534, 588, 7148, 13, 407, 437, 393, 291, 360, 466, 300, 30, 51852], "temperature": 0.0, "avg_logprob": -0.10606934143616273, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0033166345674544573}, {"id": 42, "seek": 28000, "start": 280.0, "end": 288.48, "text": " If you want to scale up. One way to scale up in Python is to use threads. And probably many of you", "tokens": [50364, 759, 291, 528, 281, 4373, 493, 13, 1485, 636, 281, 4373, 493, 294, 15329, 307, 281, 764, 19314, 13, 400, 1391, 867, 295, 291, 50788], "temperature": 0.0, "avg_logprob": -0.1482747600924584, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007779999868944287}, {"id": 43, "seek": 28000, "start": 288.48, "end": 293.36, "text": " know about the GIL. I'm going to talk about that in a bit. But let's just mention what", "tokens": [50788, 458, 466, 264, 460, 4620, 13, 286, 478, 516, 281, 751, 466, 300, 294, 257, 857, 13, 583, 718, 311, 445, 2152, 437, 51032], "temperature": 0.0, "avg_logprob": -0.1482747600924584, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007779999868944287}, {"id": 44, "seek": 28000, "start": 293.36, "end": 299.68, "text": " was threaded programming is. Thread programming is where the operating system basically assigns", "tokens": [51032, 390, 47493, 9410, 307, 13, 334, 2538, 9410, 307, 689, 264, 7447, 1185, 1936, 6269, 82, 51348], "temperature": 0.0, "avg_logprob": -0.1482747600924584, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007779999868944287}, {"id": 45, "seek": 28000, "start": 299.68, "end": 305.52, "text": " slices to your application. And then each slice can run for a certain amount of time. And then", "tokens": [51348, 19793, 281, 428, 3861, 13, 400, 550, 1184, 13153, 393, 1190, 337, 257, 1629, 2372, 295, 565, 13, 400, 550, 51640], "temperature": 0.0, "avg_logprob": -0.1482747600924584, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007779999868944287}, {"id": 46, "seek": 30552, "start": 305.59999999999997, "end": 311.44, "text": " the operating system switches to the next slice and the next thread. So everything is controlled", "tokens": [50368, 264, 7447, 1185, 19458, 281, 264, 958, 13153, 293, 264, 958, 7207, 13, 407, 1203, 307, 10164, 50660], "temperature": 0.0, "avg_logprob": -0.0897821803669353, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00039137492422014475}, {"id": 47, "seek": 30552, "start": 311.44, "end": 317.35999999999996, "text": " by the OS. This is a very nice and very elegant way to scale up. You can use all the cores that", "tokens": [50660, 538, 264, 12731, 13, 639, 307, 257, 588, 1481, 293, 588, 21117, 636, 281, 4373, 493, 13, 509, 393, 764, 439, 264, 24826, 300, 50956], "temperature": 0.0, "avg_logprob": -0.0897821803669353, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00039137492422014475}, {"id": 48, "seek": 30552, "start": 317.35999999999996, "end": 324.24, "text": " you have in your CPU. You can, you know, in the past you usually had multiple processes in the", "tokens": [50956, 291, 362, 294, 428, 13199, 13, 509, 393, 11, 291, 458, 11, 294, 264, 1791, 291, 2673, 632, 3866, 7555, 294, 264, 51300], "temperature": 0.0, "avg_logprob": -0.0897821803669353, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00039137492422014475}, {"id": 49, "seek": 30552, "start": 324.24, "end": 330.24, "text": " servers and you could use those multiple processes as well. There's one catch though with thread", "tokens": [51300, 15909, 293, 291, 727, 764, 729, 3866, 7555, 382, 731, 13, 821, 311, 472, 3745, 1673, 365, 7207, 51600], "temperature": 0.0, "avg_logprob": -0.0897821803669353, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.00039137492422014475}, {"id": 50, "seek": 33024, "start": 330.24, "end": 334.56, "text": " programming because it's controlled by the OS and not by the application. So not by Python.", "tokens": [50364, 9410, 570, 309, 311, 10164, 538, 264, 12731, 293, 406, 538, 264, 3861, 13, 407, 406, 538, 15329, 13, 50580], "temperature": 0.0, "avg_logprob": -0.10706712761703803, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0031665496062487364}, {"id": 51, "seek": 33024, "start": 336.32, "end": 343.04, "text": " It is possible for two threads to try to access the same object, let's say, or same memory area", "tokens": [50668, 467, 307, 1944, 337, 732, 19314, 281, 853, 281, 2105, 264, 912, 2657, 11, 718, 311, 584, 11, 420, 912, 4675, 1859, 51004], "temperature": 0.0, "avg_logprob": -0.10706712761703803, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0031665496062487364}, {"id": 52, "seek": 33024, "start": 343.04, "end": 349.52, "text": " in your application and do things on those memory areas. For example, you know, take a list, append", "tokens": [51004, 294, 428, 3861, 293, 360, 721, 322, 729, 4675, 3179, 13, 1171, 1365, 11, 291, 458, 11, 747, 257, 1329, 11, 34116, 51328], "temperature": 0.0, "avg_logprob": -0.10706712761703803, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0031665496062487364}, {"id": 53, "seek": 33024, "start": 349.52, "end": 356.0, "text": " to it, delete from it and so on. And if two threads start doing that at the same time, you can have", "tokens": [51328, 281, 309, 11, 12097, 490, 309, 293, 370, 322, 13, 400, 498, 732, 19314, 722, 884, 300, 412, 264, 912, 565, 11, 291, 393, 362, 51652], "temperature": 0.0, "avg_logprob": -0.10706712761703803, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0031665496062487364}, {"id": 54, "seek": 35600, "start": 356.0, "end": 360.56, "text": " clashes. And in order to prevent that, because you don't want to have data loss, for example,", "tokens": [50364, 596, 12808, 13, 400, 294, 1668, 281, 4871, 300, 11, 570, 291, 500, 380, 528, 281, 362, 1412, 4470, 11, 337, 1365, 11, 50592], "temperature": 0.0, "avg_logprob": -0.07441097023212805, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0022363467141985893}, {"id": 55, "seek": 35600, "start": 361.6, "end": 368.16, "text": " you have to put locks around these things to make everything work. So there is a bit of extra work", "tokens": [50644, 291, 362, 281, 829, 20703, 926, 613, 721, 281, 652, 1203, 589, 13, 407, 456, 307, 257, 857, 295, 2857, 589, 50972], "temperature": 0.0, "avg_logprob": -0.07441097023212805, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0022363467141985893}, {"id": 56, "seek": 35600, "start": 368.16, "end": 373.36, "text": " to be done there. You have to consider how things work in the thread environment and you have to", "tokens": [50972, 281, 312, 1096, 456, 13, 509, 362, 281, 1949, 577, 721, 589, 294, 264, 7207, 2823, 293, 291, 362, 281, 51232], "temperature": 0.0, "avg_logprob": -0.07441097023212805, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0022363467141985893}, {"id": 57, "seek": 35600, "start": 373.36, "end": 379.04, "text": " put locks around areas that can be shared between the different threads that you have. It is an", "tokens": [51232, 829, 20703, 926, 3179, 300, 393, 312, 5507, 1296, 264, 819, 19314, 300, 291, 362, 13, 467, 307, 364, 51516], "temperature": 0.0, "avg_logprob": -0.07441097023212805, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0022363467141985893}, {"id": 58, "seek": 35600, "start": 379.04, "end": 385.36, "text": " efficient use of resources. So this is actually something that people try to get working. With", "tokens": [51516, 7148, 764, 295, 3593, 13, 407, 341, 307, 767, 746, 300, 561, 853, 281, 483, 1364, 13, 2022, 51832], "temperature": 0.0, "avg_logprob": -0.07441097023212805, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0022363467141985893}, {"id": 59, "seek": 38536, "start": 385.44, "end": 392.96000000000004, "text": " Python, it's a bit harder. And because it's a bit harder, some years ago, async support was added", "tokens": [50368, 15329, 11, 309, 311, 257, 857, 6081, 13, 400, 570, 309, 311, 257, 857, 6081, 11, 512, 924, 2057, 11, 382, 34015, 1406, 390, 3869, 50744], "temperature": 0.0, "avg_logprob": -0.08846063324899385, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0008038672967813909}, {"id": 60, "seek": 38536, "start": 392.96000000000004, "end": 398.56, "text": " to Python. So what is asynchronous? Asynchronous is basically focusing on a single thread, on a", "tokens": [50744, 281, 15329, 13, 407, 437, 307, 49174, 30, 1018, 36420, 563, 307, 1936, 8416, 322, 257, 2167, 7207, 11, 322, 257, 51024], "temperature": 0.0, "avg_logprob": -0.08846063324899385, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0008038672967813909}, {"id": 61, "seek": 38536, "start": 398.56, "end": 406.16, "text": " single core. It looks very much like a synchronous program, except that whenever you do IO, the", "tokens": [51024, 2167, 4965, 13, 467, 1542, 588, 709, 411, 257, 44743, 1461, 11, 3993, 300, 5699, 291, 360, 39839, 11, 264, 51404], "temperature": 0.0, "avg_logprob": -0.08846063324899385, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0008038672967813909}, {"id": 62, "seek": 38536, "start": 406.16, "end": 412.24, "text": " application, Python in that case, can then say, okay, I'm going to run this function until I hit", "tokens": [51404, 3861, 11, 15329, 294, 300, 1389, 11, 393, 550, 584, 11, 1392, 11, 286, 478, 516, 281, 1190, 341, 2445, 1826, 286, 2045, 51708], "temperature": 0.0, "avg_logprob": -0.08846063324899385, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0008038672967813909}, {"id": 63, "seek": 41224, "start": 412.24, "end": 417.84000000000003, "text": " a spot where I have IO, for example, or I have to wait for something. And then I give back control", "tokens": [50364, 257, 4008, 689, 286, 362, 39839, 11, 337, 1365, 11, 420, 286, 362, 281, 1699, 337, 746, 13, 400, 550, 286, 976, 646, 1969, 50644], "temperature": 0.0, "avg_logprob": -0.10288474139045267, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0017795084277167916}, {"id": 64, "seek": 41224, "start": 417.84000000000003, "end": 422.0, "text": " to something called an event loop. And that event loop is then going to take,", "tokens": [50644, 281, 746, 1219, 364, 2280, 6367, 13, 400, 300, 2280, 6367, 307, 550, 516, 281, 747, 11, 50852], "temperature": 0.0, "avg_logprob": -0.10288474139045267, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0017795084277167916}, {"id": 65, "seek": 41224, "start": 423.12, "end": 427.68, "text": " it's going to go through the list of everything that is scheduled to be executed and then just", "tokens": [50908, 309, 311, 516, 281, 352, 807, 264, 1329, 295, 1203, 300, 307, 15678, 281, 312, 17577, 293, 550, 445, 51136], "temperature": 0.0, "avg_logprob": -0.10288474139045267, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0017795084277167916}, {"id": 66, "seek": 41224, "start": 427.68, "end": 433.84000000000003, "text": " run the next thing that's on that list. And so whenever you wait for IO, you can tell the program,", "tokens": [51136, 1190, 264, 958, 551, 300, 311, 322, 300, 1329, 13, 400, 370, 5699, 291, 1699, 337, 39839, 11, 291, 393, 980, 264, 1461, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10288474139045267, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0017795084277167916}, {"id": 67, "seek": 41224, "start": 433.84000000000003, "end": 440.16, "text": " okay, I'm done with this part of the application. And now I'm going to switch to a different part", "tokens": [51444, 1392, 11, 286, 478, 1096, 365, 341, 644, 295, 264, 3861, 13, 400, 586, 286, 478, 516, 281, 3679, 281, 257, 819, 644, 51760], "temperature": 0.0, "avg_logprob": -0.10288474139045267, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.0017795084277167916}, {"id": 68, "seek": 44016, "start": 440.16, "end": 447.20000000000005, "text": " and run that part. So that's a way to work around the threading issues I just talked about. It's", "tokens": [50364, 293, 1190, 300, 644, 13, 407, 300, 311, 257, 636, 281, 589, 926, 264, 7207, 278, 2663, 286, 445, 2825, 466, 13, 467, 311, 50716], "temperature": 0.0, "avg_logprob": -0.07968858757404366, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0017537017120048404}, {"id": 69, "seek": 44016, "start": 447.20000000000005, "end": 456.32000000000005, "text": " also a way to write code that scales up very neatly, very fast. It's a bit limited in terms of", "tokens": [50716, 611, 257, 636, 281, 2464, 3089, 300, 17408, 493, 588, 36634, 11, 588, 2370, 13, 467, 311, 257, 857, 5567, 294, 2115, 295, 51172], "temperature": 0.0, "avg_logprob": -0.07968858757404366, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0017537017120048404}, {"id": 70, "seek": 44016, "start": 457.12, "end": 463.36, "text": " focusing on just one core. So you, for example, you cannot use multiple cores that way,", "tokens": [51212, 8416, 322, 445, 472, 4965, 13, 407, 291, 11, 337, 1365, 11, 291, 2644, 764, 3866, 24826, 300, 636, 11, 51524], "temperature": 0.0, "avg_logprob": -0.07968858757404366, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0017537017120048404}, {"id": 71, "seek": 44016, "start": 463.36, "end": 469.28000000000003, "text": " not that easily. If you want to use multiple cores, you can push work that is being done in the", "tokens": [51524, 406, 300, 3612, 13, 759, 291, 528, 281, 764, 3866, 24826, 11, 291, 393, 2944, 589, 300, 307, 885, 1096, 294, 264, 51820], "temperature": 0.0, "avg_logprob": -0.07968858757404366, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0017537017120048404}, {"id": 72, "seek": 46928, "start": 469.28, "end": 476.0, "text": " application that's not running Python on these other cores, that's certainly possible. But if you", "tokens": [50364, 3861, 300, 311, 406, 2614, 15329, 322, 613, 661, 24826, 11, 300, 311, 3297, 1944, 13, 583, 498, 291, 50700], "temperature": 0.0, "avg_logprob": -0.10377720991770427, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0004302602610550821}, {"id": 73, "seek": 46928, "start": 476.0, "end": 481.76, "text": " want to scale up, use all the cores, then you basically have to use multiple of these applications,", "tokens": [50700, 528, 281, 4373, 493, 11, 764, 439, 264, 24826, 11, 550, 291, 1936, 362, 281, 764, 3866, 295, 613, 5821, 11, 50988], "temperature": 0.0, "avg_logprob": -0.10377720991770427, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0004302602610550821}, {"id": 74, "seek": 46928, "start": 481.76, "end": 487.84, "text": " run them in different processes, and then use up all the cores that you have. There's one catch with", "tokens": [50988, 1190, 552, 294, 819, 7555, 11, 293, 550, 764, 493, 439, 264, 24826, 300, 291, 362, 13, 821, 311, 472, 3745, 365, 51292], "temperature": 0.0, "avg_logprob": -0.10377720991770427, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0004302602610550821}, {"id": 75, "seek": 46928, "start": 487.84, "end": 493.84, "text": " this. I mean, there's no free lunch, right? So all the parts in your code have to collaborate.", "tokens": [51292, 341, 13, 286, 914, 11, 456, 311, 572, 1737, 6349, 11, 558, 30, 407, 439, 264, 3166, 294, 428, 3089, 362, 281, 18338, 13, 51592], "temperature": 0.0, "avg_logprob": -0.10377720991770427, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0004302602610550821}, {"id": 76, "seek": 49384, "start": 494.71999999999997, "end": 500.32, "text": " Because it's application driven, all the parts that you have need to have certain places where", "tokens": [50408, 1436, 309, 311, 3861, 9555, 11, 439, 264, 3166, 300, 291, 362, 643, 281, 362, 1629, 3190, 689, 50688], "temperature": 0.0, "avg_logprob": -0.08358681720236073, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0043227216228842735}, {"id": 77, "seek": 49384, "start": 500.32, "end": 505.2, "text": " they say, okay, I'm going to give up control back to the event loop at this point because I'm waiting", "tokens": [50688, 436, 584, 11, 1392, 11, 286, 478, 516, 281, 976, 493, 1969, 646, 281, 264, 2280, 6367, 412, 341, 935, 570, 286, 478, 3806, 50932], "temperature": 0.0, "avg_logprob": -0.08358681720236073, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0043227216228842735}, {"id": 78, "seek": 49384, "start": 505.2, "end": 510.4, "text": " for something. Because Python cannot know that you're trying to wait for something. And so you", "tokens": [50932, 337, 746, 13, 1436, 15329, 2644, 458, 300, 291, 434, 1382, 281, 1699, 337, 746, 13, 400, 370, 291, 51192], "temperature": 0.0, "avg_logprob": -0.08358681720236073, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0043227216228842735}, {"id": 79, "seek": 49384, "start": 510.4, "end": 519.4399999999999, "text": " have to tell Python that this is a good place to give up control. Now, why do we need this? And", "tokens": [51192, 362, 281, 980, 15329, 300, 341, 307, 257, 665, 1081, 281, 976, 493, 1969, 13, 823, 11, 983, 360, 321, 643, 341, 30, 400, 51644], "temperature": 0.0, "avg_logprob": -0.08358681720236073, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0043227216228842735}, {"id": 80, "seek": 49384, "start": 519.4399999999999, "end": 523.28, "text": " this slide is about the global interpreter lock. How many of you know the global interpreter lock?", "tokens": [51644, 341, 4137, 307, 466, 264, 4338, 34132, 4017, 13, 1012, 867, 295, 291, 458, 264, 4338, 34132, 4017, 30, 51836], "temperature": 0.0, "avg_logprob": -0.08358681720236073, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0043227216228842735}, {"id": 81, "seek": 52384, "start": 524.8000000000001, "end": 535.0400000000001, "text": " Just a few. That's interesting. So what Python does is it keeps a one big lock around the Python", "tokens": [50412, 1449, 257, 1326, 13, 663, 311, 1880, 13, 407, 437, 15329, 775, 307, 309, 5965, 257, 472, 955, 4017, 926, 264, 15329, 50924], "temperature": 0.0, "avg_logprob": -0.1349931464475744, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.00027775089256465435}, {"id": 82, "seek": 52384, "start": 535.0400000000001, "end": 540.88, "text": " virtual machine that executes the Python bytecode. And it does this because it wants to support", "tokens": [50924, 6374, 3479, 300, 4454, 1819, 264, 15329, 40846, 22332, 13, 400, 309, 775, 341, 570, 309, 2738, 281, 1406, 51216], "temperature": 0.0, "avg_logprob": -0.1349931464475744, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.00027775089256465435}, {"id": 83, "seek": 52384, "start": 540.88, "end": 549.84, "text": " multiple threads. But at the time when this was added, threading was actually very new.", "tokens": [51216, 3866, 19314, 13, 583, 412, 264, 565, 562, 341, 390, 3869, 11, 7207, 278, 390, 767, 588, 777, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1349931464475744, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.00027775089256465435}, {"id": 84, "seek": 54984, "start": 550.8000000000001, "end": 559.52, "text": " Python is, it's more than 30 years old now. So there's been a lot of development going on", "tokens": [50412, 15329, 307, 11, 309, 311, 544, 813, 2217, 924, 1331, 586, 13, 407, 456, 311, 668, 257, 688, 295, 3250, 516, 322, 50848], "temperature": 0.0, "avg_logprob": -0.11437239280113808, "compression_ratio": 1.4607329842931938, "no_speech_prob": 0.001744582667015493}, {"id": 85, "seek": 54984, "start": 559.52, "end": 565.2, "text": " since Python started. And because Guido wanted to start supporting threading right from the", "tokens": [50848, 1670, 15329, 1409, 13, 400, 570, 2694, 2925, 1415, 281, 722, 7231, 7207, 278, 558, 490, 264, 51132], "temperature": 0.0, "avg_logprob": -0.11437239280113808, "compression_ratio": 1.4607329842931938, "no_speech_prob": 0.001744582667015493}, {"id": 86, "seek": 54984, "start": 565.2, "end": 571.12, "text": " beginning, he added this global interpreter lock to make sure that the logic that's inside Python", "tokens": [51132, 2863, 11, 415, 3869, 341, 4338, 34132, 4017, 281, 652, 988, 300, 264, 9952, 300, 311, 1854, 15329, 51428], "temperature": 0.0, "avg_logprob": -0.11437239280113808, "compression_ratio": 1.4607329842931938, "no_speech_prob": 0.001744582667015493}, {"id": 87, "seek": 57112, "start": 571.68, "end": 581.28, "text": " is only used by one thread at any point in time. So what happens is the Python starts running", "tokens": [50392, 307, 787, 1143, 538, 472, 7207, 412, 604, 935, 294, 565, 13, 407, 437, 2314, 307, 264, 15329, 3719, 2614, 50872], "temperature": 0.0, "avg_logprob": -0.08911545011732314, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01737777143716812}, {"id": 88, "seek": 57112, "start": 581.28, "end": 589.2, "text": " code, Python code in one thread, then reaches a certain point and then gives up control back to", "tokens": [50872, 3089, 11, 15329, 3089, 294, 472, 7207, 11, 550, 14235, 257, 1629, 935, 293, 550, 2709, 493, 1969, 646, 281, 51268], "temperature": 0.0, "avg_logprob": -0.08911545011732314, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01737777143716812}, {"id": 89, "seek": 57112, "start": 589.2, "end": 594.48, "text": " the OS and says, okay, you can switch to a different thread now. But it does this under the", "tokens": [51268, 264, 12731, 293, 1619, 11, 1392, 11, 291, 393, 3679, 281, 257, 819, 7207, 586, 13, 583, 309, 775, 341, 833, 264, 51532], "temperature": 0.0, "avg_logprob": -0.08911545011732314, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01737777143716812}, {"id": 90, "seek": 57112, "start": 594.48, "end": 599.36, "text": " control of this global interpreter lock. So it makes sure that no other thread is running Python", "tokens": [51532, 1969, 295, 341, 4338, 34132, 4017, 13, 407, 309, 1669, 988, 300, 572, 661, 7207, 307, 2614, 15329, 51776], "temperature": 0.0, "avg_logprob": -0.08911545011732314, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01737777143716812}, {"id": 91, "seek": 59936, "start": 599.36, "end": 605.6, "text": " at the moment. When it gives up control to a different thread, then that thread will have", "tokens": [50364, 412, 264, 1623, 13, 1133, 309, 2709, 493, 1969, 281, 257, 819, 7207, 11, 550, 300, 7207, 486, 362, 50676], "temperature": 0.0, "avg_logprob": -0.09661713706122504, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0003051677194889635}, {"id": 92, "seek": 59936, "start": 605.6, "end": 611.44, "text": " been waiting for the Python interpreter lock to get the lock. And then we'll start executing.", "tokens": [50676, 668, 3806, 337, 264, 15329, 34132, 4017, 281, 483, 264, 4017, 13, 400, 550, 321, 603, 722, 32368, 13, 50968], "temperature": 0.0, "avg_logprob": -0.09661713706122504, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0003051677194889635}, {"id": 93, "seek": 59936, "start": 611.44, "end": 616.88, "text": " And this goes on for all the threads that you have in your application. So in a multi-threaded", "tokens": [50968, 400, 341, 1709, 322, 337, 439, 264, 19314, 300, 291, 362, 294, 428, 3861, 13, 407, 294, 257, 4825, 12, 392, 2538, 292, 51240], "temperature": 0.0, "avg_logprob": -0.09661713706122504, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0003051677194889635}, {"id": 94, "seek": 59936, "start": 616.88, "end": 623.84, "text": " program that's running Python, you can just have one thread execute Python code at any point in", "tokens": [51240, 1461, 300, 311, 2614, 15329, 11, 291, 393, 445, 362, 472, 7207, 14483, 15329, 3089, 412, 604, 935, 294, 51588], "temperature": 0.0, "avg_logprob": -0.09661713706122504, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0003051677194889635}, {"id": 95, "seek": 62384, "start": 623.84, "end": 633.36, "text": " time. And this is something that, of course, is not very scalable. It's also not a very big issue,", "tokens": [50364, 565, 13, 400, 341, 307, 746, 300, 11, 295, 1164, 11, 307, 406, 588, 38481, 13, 467, 311, 611, 406, 257, 588, 955, 2734, 11, 50840], "temperature": 0.0, "avg_logprob": -0.09001706043879192, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0052097272127866745}, {"id": 96, "seek": 62384, "start": 633.36, "end": 641.6800000000001, "text": " as some may tell you. Because if you're clever and you put all the logic that you need to run", "tokens": [50840, 382, 512, 815, 980, 291, 13, 1436, 498, 291, 434, 13494, 293, 291, 829, 439, 264, 9952, 300, 291, 643, 281, 1190, 51256], "temperature": 0.0, "avg_logprob": -0.09001706043879192, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0052097272127866745}, {"id": 97, "seek": 62384, "start": 641.6800000000001, "end": 648.5600000000001, "text": " a multiple course or multiple threads into parts of the program that don't need Python, for example,", "tokens": [51256, 257, 3866, 1164, 420, 3866, 19314, 666, 3166, 295, 264, 1461, 300, 500, 380, 643, 15329, 11, 337, 1365, 11, 51600], "temperature": 0.0, "avg_logprob": -0.09001706043879192, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0052097272127866745}, {"id": 98, "seek": 62384, "start": 648.5600000000001, "end": 653.76, "text": " if you're running machine learning and you want to train a model, then you can just easily", "tokens": [51600, 498, 291, 434, 2614, 3479, 2539, 293, 291, 528, 281, 3847, 257, 2316, 11, 550, 291, 393, 445, 3612, 51860], "temperature": 0.0, "avg_logprob": -0.09001706043879192, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0052097272127866745}, {"id": 99, "seek": 65376, "start": 653.76, "end": 659.84, "text": " push off everything into C code, which doesn't need Python. And that can very well run next to", "tokens": [50364, 2944, 766, 1203, 666, 383, 3089, 11, 597, 1177, 380, 643, 15329, 13, 400, 300, 393, 588, 731, 1190, 958, 281, 50668], "temperature": 0.0, "avg_logprob": -0.09293888012568156, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0002911374904215336}, {"id": 100, "seek": 65376, "start": 659.84, "end": 667.4399999999999, "text": " Python in another thread. So that's certainly possible. But, of course, sometimes you don't", "tokens": [50668, 15329, 294, 1071, 7207, 13, 407, 300, 311, 3297, 1944, 13, 583, 11, 295, 1164, 11, 2171, 291, 500, 380, 51048], "temperature": 0.0, "avg_logprob": -0.09293888012568156, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0002911374904215336}, {"id": 101, "seek": 65376, "start": 667.4399999999999, "end": 671.28, "text": " have a chance to do that. And then you need to look for other things. And this is where async", "tokens": [51048, 362, 257, 2931, 281, 360, 300, 13, 400, 550, 291, 643, 281, 574, 337, 661, 721, 13, 400, 341, 307, 689, 382, 34015, 51240], "temperature": 0.0, "avg_logprob": -0.09293888012568156, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0002911374904215336}, {"id": 102, "seek": 65376, "start": 671.28, "end": 679.4399999999999, "text": " becomes very nice. So let's have a look at how thread code executes in Python. The image on", "tokens": [51240, 3643, 588, 1481, 13, 407, 718, 311, 362, 257, 574, 412, 577, 7207, 3089, 4454, 1819, 294, 15329, 13, 440, 3256, 322, 51648], "temperature": 0.0, "avg_logprob": -0.09293888012568156, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0002911374904215336}, {"id": 103, "seek": 67944, "start": 679.44, "end": 688.8800000000001, "text": " the right basically explains how Python works. So you have three threads. The orange is Python", "tokens": [50364, 264, 558, 1936, 13948, 577, 15329, 1985, 13, 407, 291, 362, 1045, 19314, 13, 440, 7671, 307, 15329, 50836], "temperature": 0.0, "avg_logprob": -0.1312258584158761, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.002630142727866769}, {"id": 104, "seek": 67944, "start": 688.8800000000001, "end": 694.72, "text": " running. Then the yellow is basically the thread, the Python interpreter in those threads waiting", "tokens": [50836, 2614, 13, 1396, 264, 5566, 307, 1936, 264, 7207, 11, 264, 15329, 34132, 294, 729, 19314, 3806, 51128], "temperature": 0.0, "avg_logprob": -0.1312258584158761, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.002630142727866769}, {"id": 105, "seek": 67944, "start": 694.72, "end": 702.4000000000001, "text": " for the gil. And then you have some waiting for IO that happens in between. So if you look closely,", "tokens": [51128, 337, 264, 290, 388, 13, 400, 550, 291, 362, 512, 3806, 337, 39839, 300, 2314, 294, 1296, 13, 407, 498, 291, 574, 8185, 11, 51512], "temperature": 0.0, "avg_logprob": -0.1312258584158761, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.002630142727866769}, {"id": 106, "seek": 67944, "start": 702.4000000000001, "end": 707.6800000000001, "text": " you will see that it's not a very efficient use here, because there's lots of waiting, lots of", "tokens": [51512, 291, 486, 536, 300, 309, 311, 406, 257, 588, 7148, 764, 510, 11, 570, 456, 311, 3195, 295, 3806, 11, 3195, 295, 51776], "temperature": 0.0, "avg_logprob": -0.1312258584158761, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.002630142727866769}, {"id": 107, "seek": 70768, "start": 707.68, "end": 715.52, "text": " yellow in there waiting for the gil, lots of blue waiting for IO. Let's have a closer look at this.", "tokens": [50364, 5566, 294, 456, 3806, 337, 264, 290, 388, 11, 3195, 295, 3344, 3806, 337, 39839, 13, 961, 311, 362, 257, 4966, 574, 412, 341, 13, 50756], "temperature": 0.0, "avg_logprob": -0.06640278024876371, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.00046433633542619646}, {"id": 108, "seek": 70768, "start": 715.52, "end": 721.3599999999999, "text": " So this again is the picture that I had on the other slide. And I moved out all the waiting", "tokens": [50756, 407, 341, 797, 307, 264, 3036, 300, 286, 632, 322, 264, 661, 4137, 13, 400, 286, 4259, 484, 439, 264, 3806, 51048], "temperature": 0.0, "avg_logprob": -0.06640278024876371, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.00046433633542619646}, {"id": 109, "seek": 70768, "start": 721.3599999999999, "end": 726.8, "text": " and all the execution. And if you move all the execution together, you will see that only one", "tokens": [51048, 293, 439, 264, 15058, 13, 400, 498, 291, 1286, 439, 264, 15058, 1214, 11, 291, 486, 536, 300, 787, 472, 51320], "temperature": 0.0, "avg_logprob": -0.06640278024876371, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.00046433633542619646}, {"id": 110, "seek": 70768, "start": 726.8, "end": 732.7199999999999, "text": " thread is running at any point in time. So the other threads are basically just sitting there", "tokens": [51320, 7207, 307, 2614, 412, 604, 935, 294, 565, 13, 407, 264, 661, 19314, 366, 1936, 445, 3798, 456, 51616], "temperature": 0.0, "avg_logprob": -0.06640278024876371, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.00046433633542619646}, {"id": 111, "seek": 73272, "start": 732.8000000000001, "end": 743.12, "text": " doing nothing. Now, how can you work around this? You can use async programming for this. And async", "tokens": [50368, 884, 1825, 13, 823, 11, 577, 393, 291, 589, 926, 341, 30, 509, 393, 764, 382, 34015, 9410, 337, 341, 13, 400, 382, 34015, 50884], "temperature": 0.0, "avg_logprob": -0.0874599788499915, "compression_ratio": 1.71875, "no_speech_prob": 0.0009388384060002863}, {"id": 112, "seek": 73272, "start": 743.12, "end": 749.28, "text": " programming has the need feature that you can actually saturate a single core very efficiently", "tokens": [50884, 9410, 575, 264, 643, 4111, 300, 291, 393, 767, 21160, 473, 257, 2167, 4965, 588, 19621, 51192], "temperature": 0.0, "avg_logprob": -0.0874599788499915, "compression_ratio": 1.71875, "no_speech_prob": 0.0009388384060002863}, {"id": 113, "seek": 73272, "start": 749.28, "end": 755.36, "text": " without doing too much work. So again, you have the execution here. You don't have three threads.", "tokens": [51192, 1553, 884, 886, 709, 589, 13, 407, 797, 11, 291, 362, 264, 15058, 510, 13, 509, 500, 380, 362, 1045, 19314, 13, 51496], "temperature": 0.0, "avg_logprob": -0.0874599788499915, "compression_ratio": 1.71875, "no_speech_prob": 0.0009388384060002863}, {"id": 114, "seek": 73272, "start": 755.36, "end": 762.1600000000001, "text": " This is just one thread that you have for one core, but you have three tasks running in that", "tokens": [51496, 639, 307, 445, 472, 7207, 300, 291, 362, 337, 472, 4965, 11, 457, 291, 362, 1045, 9608, 2614, 294, 300, 51836], "temperature": 0.0, "avg_logprob": -0.0874599788499915, "compression_ratio": 1.71875, "no_speech_prob": 0.0009388384060002863}, {"id": 115, "seek": 76216, "start": 762.16, "end": 768.9599999999999, "text": " one thread. And the different tasks, they share the execution. And again, you have the orange here", "tokens": [50364, 472, 7207, 13, 400, 264, 819, 9608, 11, 436, 2073, 264, 15058, 13, 400, 797, 11, 291, 362, 264, 7671, 510, 50704], "temperature": 0.0, "avg_logprob": -0.11357141310168851, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0007315647671930492}, {"id": 116, "seek": 76216, "start": 768.9599999999999, "end": 774.16, "text": " executing Python. You have some waiting for IO in here or could also be waiting for calculations", "tokens": [50704, 32368, 15329, 13, 509, 362, 512, 3806, 337, 39839, 294, 510, 420, 727, 611, 312, 3806, 337, 20448, 50964], "temperature": 0.0, "avg_logprob": -0.11357141310168851, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0007315647671930492}, {"id": 117, "seek": 76216, "start": 775.04, "end": 781.4399999999999, "text": " to happen. And if you move everything together, you will see that it's really the thread, the core", "tokens": [51008, 281, 1051, 13, 400, 498, 291, 1286, 1203, 1214, 11, 291, 486, 536, 300, 309, 311, 534, 264, 7207, 11, 264, 4965, 51328], "temperature": 0.0, "avg_logprob": -0.11357141310168851, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0007315647671930492}, {"id": 118, "seek": 76216, "start": 781.4399999999999, "end": 788.48, "text": " is saturated. So everything is working out nicely. And it's very efficient. So how does this work?", "tokens": [51328, 307, 25408, 13, 407, 1203, 307, 1364, 484, 9594, 13, 400, 309, 311, 588, 7148, 13, 407, 577, 775, 341, 589, 30, 51680], "temperature": 0.0, "avg_logprob": -0.11357141310168851, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0007315647671930492}, {"id": 119, "seek": 78848, "start": 788.48, "end": 797.36, "text": " How many of you know coroutines? Okay, about like one third. So a coroutine basically is very much", "tokens": [50364, 1012, 867, 295, 291, 458, 1181, 346, 1652, 30, 1033, 11, 466, 411, 472, 2636, 13, 407, 257, 1181, 45075, 1936, 307, 588, 709, 50808], "temperature": 0.0, "avg_logprob": -0.09809318665535219, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0007199369138106704}, {"id": 120, "seek": 78848, "start": 797.36, "end": 805.76, "text": " like a normal function, except that it's possible to have certain spots in the coroutine in the", "tokens": [50808, 411, 257, 2710, 2445, 11, 3993, 300, 309, 311, 1944, 281, 362, 1629, 10681, 294, 264, 1181, 45075, 294, 264, 51228], "temperature": 0.0, "avg_logprob": -0.09809318665535219, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0007199369138106704}, {"id": 121, "seek": 78848, "start": 805.76, "end": 811.12, "text": " function where it says, okay, at this point, you can give up control back to the caller of that", "tokens": [51228, 2445, 689, 309, 1619, 11, 1392, 11, 412, 341, 935, 11, 291, 393, 976, 493, 1969, 646, 281, 264, 48324, 295, 300, 51496], "temperature": 0.0, "avg_logprob": -0.09809318665535219, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0007199369138106704}, {"id": 122, "seek": 78848, "start": 811.12, "end": 816.24, "text": " function. And this is essentially how async programming works. You have something called an", "tokens": [51496, 2445, 13, 400, 341, 307, 4476, 577, 382, 34015, 9410, 1985, 13, 509, 362, 746, 1219, 364, 51752], "temperature": 0.0, "avg_logprob": -0.09809318665535219, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0007199369138106704}, {"id": 123, "seek": 81624, "start": 816.24, "end": 822.4, "text": " event loop. The event loop calls these coroutines. The coroutine executes until it hits one of these", "tokens": [50364, 2280, 6367, 13, 440, 2280, 6367, 5498, 613, 1181, 346, 1652, 13, 440, 1181, 45075, 4454, 1819, 1826, 309, 8664, 472, 295, 613, 50672], "temperature": 0.0, "avg_logprob": -0.081058192671391, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.0010311624500900507}, {"id": 124, "seek": 81624, "start": 823.52, "end": 828.8, "text": " the spots where you can give up control. The function, the coroutine gives back control", "tokens": [50728, 264, 10681, 689, 291, 393, 976, 493, 1969, 13, 440, 2445, 11, 264, 1181, 45075, 2709, 646, 1969, 50992], "temperature": 0.0, "avg_logprob": -0.081058192671391, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.0010311624500900507}, {"id": 125, "seek": 81624, "start": 828.8, "end": 833.04, "text": " to the event loop at that point. And then the event loop can execute something else in your", "tokens": [50992, 281, 264, 2280, 6367, 412, 300, 935, 13, 400, 550, 264, 2280, 6367, 393, 14483, 746, 1646, 294, 428, 51204], "temperature": 0.0, "avg_logprob": -0.081058192671391, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.0010311624500900507}, {"id": 126, "seek": 81624, "start": 833.04, "end": 838.72, "text": " application. And then at a later point, it comes back to that coroutine and continues executing", "tokens": [51204, 3861, 13, 400, 550, 412, 257, 1780, 935, 11, 309, 1487, 646, 281, 300, 1181, 45075, 293, 6515, 32368, 51488], "temperature": 0.0, "avg_logprob": -0.081058192671391, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.0010311624500900507}, {"id": 127, "seek": 81624, "start": 838.72, "end": 845.04, "text": " where it left off. In order to make that easy to define and easy to use in Python, we have new", "tokens": [51488, 689, 309, 1411, 766, 13, 682, 1668, 281, 652, 300, 1858, 281, 6964, 293, 1858, 281, 764, 294, 15329, 11, 321, 362, 777, 51804], "temperature": 0.0, "avg_logprob": -0.081058192671391, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.0010311624500900507}, {"id": 128, "seek": 84504, "start": 845.04, "end": 853.28, "text": " keywords. We have async def, which is a way to define these coroutines. And we have these await", "tokens": [50364, 21009, 13, 492, 362, 382, 34015, 1060, 11, 597, 307, 257, 636, 281, 6964, 613, 1181, 346, 1652, 13, 400, 321, 362, 613, 19670, 50776], "temperature": 0.0, "avg_logprob": -0.12162971496582031, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0012637157924473286}, {"id": 129, "seek": 84504, "start": 854.8, "end": 862.24, "text": " statements in Python, which are basically places where the coroutine says, okay, you can give up", "tokens": [50852, 12363, 294, 15329, 11, 597, 366, 1936, 3190, 689, 264, 1181, 45075, 1619, 11, 1392, 11, 291, 393, 976, 493, 51224], "temperature": 0.0, "avg_logprob": -0.12162971496582031, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0012637157924473286}, {"id": 130, "seek": 84504, "start": 862.24, "end": 868.3199999999999, "text": " control and you can pass back control to the event loop because I'm waiting for, let's say, IO or for", "tokens": [51224, 1969, 293, 291, 393, 1320, 646, 1969, 281, 264, 2280, 6367, 570, 286, 478, 3806, 337, 11, 718, 311, 584, 11, 39839, 420, 337, 51528], "temperature": 0.0, "avg_logprob": -0.12162971496582031, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0012637157924473286}, {"id": 131, "seek": 86832, "start": 868.32, "end": 877.36, "text": " a longer running calculation that you want to do. And everything around this, all the support for", "tokens": [50364, 257, 2854, 2614, 17108, 300, 291, 528, 281, 360, 13, 400, 1203, 926, 341, 11, 439, 264, 1406, 337, 50816], "temperature": 0.0, "avg_logprob": -0.07566337055630154, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007911117863841355}, {"id": 132, "seek": 86832, "start": 877.36, "end": 882.32, "text": " this is bundled in this package called async IO, which is part of the standard library.", "tokens": [50816, 341, 307, 13882, 1493, 294, 341, 7372, 1219, 382, 34015, 39839, 11, 597, 307, 644, 295, 264, 3832, 6405, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07566337055630154, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007911117863841355}, {"id": 133, "seek": 86832, "start": 884.1600000000001, "end": 890.08, "text": " So let's have a look at how this works in Python to compare synchronous code and async code.", "tokens": [51156, 407, 718, 311, 362, 257, 574, 412, 577, 341, 1985, 294, 15329, 281, 6794, 44743, 3089, 293, 382, 34015, 3089, 13, 51452], "temperature": 0.0, "avg_logprob": -0.07566337055630154, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007911117863841355}, {"id": 134, "seek": 86832, "start": 890.88, "end": 895.2800000000001, "text": " So on the left, you have a very simple function. You have a time sleep in there,", "tokens": [51492, 407, 322, 264, 1411, 11, 291, 362, 257, 588, 2199, 2445, 13, 509, 362, 257, 565, 2817, 294, 456, 11, 51712], "temperature": 0.0, "avg_logprob": -0.07566337055630154, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007911117863841355}, {"id": 135, "seek": 89528, "start": 895.28, "end": 901.04, "text": " which is like a simulation for the IO. So something, the application needs to wait for something.", "tokens": [50364, 597, 307, 411, 257, 16575, 337, 264, 39839, 13, 407, 746, 11, 264, 3861, 2203, 281, 1699, 337, 746, 13, 50652], "temperature": 0.0, "avg_logprob": -0.11373731645487123, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0004232039791531861}, {"id": 136, "seek": 89528, "start": 902.72, "end": 909.52, "text": " And then you call that function. And if you run the simple example, then you see that,", "tokens": [50736, 400, 550, 291, 818, 300, 2445, 13, 400, 498, 291, 1190, 264, 2199, 1365, 11, 550, 291, 536, 300, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11373731645487123, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0004232039791531861}, {"id": 137, "seek": 89528, "start": 909.52, "end": 914.0799999999999, "text": " you know, it starts executing, it starts working. Then it sleeps for two seconds,", "tokens": [51076, 291, 458, 11, 309, 3719, 32368, 11, 309, 3719, 1364, 13, 1396, 309, 37991, 337, 732, 3949, 11, 51304], "temperature": 0.0, "avg_logprob": -0.11373731645487123, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0004232039791531861}, {"id": 138, "seek": 89528, "start": 914.0799999999999, "end": 919.36, "text": " and then it's done. And then it's the end of that function. Now, in the async case,", "tokens": [51304, 293, 550, 309, 311, 1096, 13, 400, 550, 309, 311, 264, 917, 295, 300, 2445, 13, 823, 11, 294, 264, 382, 34015, 1389, 11, 51568], "temperature": 0.0, "avg_logprob": -0.11373731645487123, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0004232039791531861}, {"id": 139, "seek": 89528, "start": 919.36, "end": 924.0799999999999, "text": " it works a bit differently. So what you do is you put the async in front of the function. So you", "tokens": [51568, 309, 1985, 257, 857, 7614, 13, 407, 437, 291, 360, 307, 291, 829, 264, 382, 34015, 294, 1868, 295, 264, 2445, 13, 407, 291, 51804], "temperature": 0.0, "avg_logprob": -0.11373731645487123, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0004232039791531861}, {"id": 140, "seek": 92408, "start": 924.08, "end": 930.08, "text": " have to turn it into a coroutine. And then inside that function, we use the await statement", "tokens": [50364, 362, 281, 1261, 309, 666, 257, 1181, 45075, 13, 400, 550, 1854, 300, 2445, 11, 321, 764, 264, 19670, 5629, 50664], "temperature": 0.0, "avg_logprob": -0.08064096669356029, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0004950739676132798}, {"id": 141, "seek": 92408, "start": 930.88, "end": 935.76, "text": " to say, okay, at this point, I can give up control back to the event loop. And so what", "tokens": [50704, 281, 584, 11, 1392, 11, 412, 341, 935, 11, 286, 393, 976, 493, 1969, 646, 281, 264, 2280, 6367, 13, 400, 370, 437, 50948], "temperature": 0.0, "avg_logprob": -0.08064096669356029, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0004950739676132798}, {"id": 142, "seek": 92408, "start": 935.76, "end": 942.96, "text": " happens here is that you have a special function called async IO sleep, which is a way to, you", "tokens": [50948, 2314, 510, 307, 300, 291, 362, 257, 2121, 2445, 1219, 382, 34015, 39839, 2817, 11, 597, 307, 257, 636, 281, 11, 291, 51308], "temperature": 0.0, "avg_logprob": -0.08064096669356029, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0004950739676132798}, {"id": 143, "seek": 92408, "start": 942.96, "end": 951.5200000000001, "text": " know, wait for a certain amount of time in async. But when waiting for these two seconds,", "tokens": [51308, 458, 11, 1699, 337, 257, 1629, 2372, 295, 565, 294, 382, 34015, 13, 583, 562, 3806, 337, 613, 732, 3949, 11, 51736], "temperature": 0.0, "avg_logprob": -0.08064096669356029, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0004950739676132798}, {"id": 144, "seek": 95152, "start": 951.52, "end": 958.0, "text": " you can actually go back and you can execute something else. It's not possible to use await", "tokens": [50364, 291, 393, 767, 352, 646, 293, 291, 393, 14483, 746, 1646, 13, 467, 311, 406, 1944, 281, 764, 19670, 50688], "temperature": 0.0, "avg_logprob": -0.06564910384430282, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005878297379240394}, {"id": 145, "seek": 95152, "start": 958.0, "end": 963.12, "text": " and then time dot sleep for this, because time dot sleep is actually a blocking function,", "tokens": [50688, 293, 550, 565, 5893, 2817, 337, 341, 11, 570, 565, 5893, 2817, 307, 767, 257, 17776, 2445, 11, 50944], "temperature": 0.0, "avg_logprob": -0.06564910384430282, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005878297379240394}, {"id": 146, "seek": 95152, "start": 963.12, "end": 969.12, "text": " right? It doesn't give back control. So you have to make sure that whatever you use with await", "tokens": [50944, 558, 30, 467, 1177, 380, 976, 646, 1969, 13, 407, 291, 362, 281, 652, 988, 300, 2035, 291, 764, 365, 19670, 51244], "temperature": 0.0, "avg_logprob": -0.06564910384430282, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005878297379240394}, {"id": 147, "seek": 95152, "start": 970.24, "end": 977.28, "text": " is actually a coroutine so that it can pass back control to your coroutine that's calling", "tokens": [51300, 307, 767, 257, 1181, 45075, 370, 300, 309, 393, 1320, 646, 1969, 281, 428, 1181, 45075, 300, 311, 5141, 51652], "temperature": 0.0, "avg_logprob": -0.06564910384430282, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005878297379240394}, {"id": 148, "seek": 97728, "start": 977.28, "end": 982.64, "text": " this coroutine. And this is what I meant with everything has to collaborate. If you have places", "tokens": [50364, 341, 1181, 45075, 13, 400, 341, 307, 437, 286, 4140, 365, 1203, 575, 281, 18338, 13, 759, 291, 362, 3190, 50632], "temperature": 0.0, "avg_logprob": -0.07048571870682087, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0014541603159159422}, {"id": 149, "seek": 97728, "start": 982.64, "end": 987.4399999999999, "text": " in your application that are not compatible with async, you have to be careful and you have to", "tokens": [50632, 294, 428, 3861, 300, 366, 406, 18218, 365, 382, 34015, 11, 291, 362, 281, 312, 5026, 293, 291, 362, 281, 50872], "temperature": 0.0, "avg_logprob": -0.07048571870682087, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0014541603159159422}, {"id": 150, "seek": 97728, "start": 987.4399999999999, "end": 995.1999999999999, "text": " use certain workarounds to make it happen. So the next thing is that, you know, now you have a", "tokens": [50872, 764, 1629, 589, 289, 4432, 281, 652, 309, 1051, 13, 407, 264, 958, 551, 307, 300, 11, 291, 458, 11, 586, 291, 362, 257, 51260], "temperature": 0.0, "avg_logprob": -0.07048571870682087, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0014541603159159422}, {"id": 151, "seek": 97728, "start": 995.1999999999999, "end": 1002.24, "text": " coroutine calling the coroutine will do nothing. Basically, all that happens is you get back a", "tokens": [51260, 1181, 45075, 5141, 264, 1181, 45075, 486, 360, 1825, 13, 8537, 11, 439, 300, 2314, 307, 291, 483, 646, 257, 51612], "temperature": 0.0, "avg_logprob": -0.07048571870682087, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0014541603159159422}, {"id": 152, "seek": 100224, "start": 1002.24, "end": 1008.8, "text": " coroutine object. So it doesn't run. So in order to run it, you have to actually start the coroutine", "tokens": [50364, 1181, 45075, 2657, 13, 407, 309, 1177, 380, 1190, 13, 407, 294, 1668, 281, 1190, 309, 11, 291, 362, 281, 767, 722, 264, 1181, 45075, 50692], "temperature": 0.0, "avg_logprob": -0.09551522226044626, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006188107188791037}, {"id": 153, "seek": 100224, "start": 1008.8, "end": 1013.84, "text": " inside the event loop. And this is what async IO dot run does at the very bottom.", "tokens": [50692, 1854, 264, 2280, 6367, 13, 400, 341, 307, 437, 382, 34015, 39839, 5893, 1190, 775, 412, 264, 588, 2767, 13, 50944], "temperature": 0.0, "avg_logprob": -0.09551522226044626, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006188107188791037}, {"id": 154, "seek": 100224, "start": 1015.6800000000001, "end": 1023.76, "text": " And this, if you look at it, it takes, it defines two tasks. So basically two instances", "tokens": [51036, 400, 341, 11, 498, 291, 574, 412, 309, 11, 309, 2516, 11, 309, 23122, 732, 9608, 13, 407, 1936, 732, 14519, 51440], "temperature": 0.0, "avg_logprob": -0.09551522226044626, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006188107188791037}, {"id": 155, "seek": 100224, "start": 1023.76, "end": 1030.8, "text": " of that coroutine puts them into this tuple. The tuple is passed to this async IO gather,", "tokens": [51440, 295, 300, 1181, 45075, 8137, 552, 666, 341, 2604, 781, 13, 440, 2604, 781, 307, 4678, 281, 341, 382, 34015, 39839, 5448, 11, 51792], "temperature": 0.0, "avg_logprob": -0.09551522226044626, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.006188107188791037}, {"id": 156, "seek": 103080, "start": 1030.8, "end": 1033.36, "text": " which is a special function I'm going to come to in one of the next slides.", "tokens": [50364, 597, 307, 257, 2121, 2445, 286, 478, 516, 281, 808, 281, 294, 472, 295, 264, 958, 9788, 13, 50492], "temperature": 0.0, "avg_logprob": -0.15281704173368565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00026093231281265616}, {"id": 157, "seek": 103080, "start": 1034.3999999999999, "end": 1039.36, "text": " It basically just takes these, the coroutines, creates task objects,", "tokens": [50544, 467, 1936, 445, 2516, 613, 11, 264, 1181, 346, 1652, 11, 7829, 5633, 6565, 11, 50792], "temperature": 0.0, "avg_logprob": -0.15281704173368565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00026093231281265616}, {"id": 158, "seek": 103080, "start": 1040.48, "end": 1045.84, "text": " and then executes them until all of them are done and then passes back control. So this is how you", "tokens": [50848, 293, 550, 4454, 1819, 552, 1826, 439, 295, 552, 366, 1096, 293, 550, 11335, 646, 1969, 13, 407, 341, 307, 577, 291, 51116], "temperature": 0.0, "avg_logprob": -0.15281704173368565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00026093231281265616}, {"id": 159, "seek": 103080, "start": 1045.84, "end": 1054.0, "text": " would run an async application. I already went through these so I can basically just skip these.", "tokens": [51116, 576, 1190, 364, 382, 34015, 3861, 13, 286, 1217, 1437, 807, 613, 370, 286, 393, 1936, 445, 10023, 613, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15281704173368565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00026093231281265616}, {"id": 160, "seek": 105400, "start": 1054.32, "end": 1060.48, "text": " So what are the, you know, things in the async IO package or module?", "tokens": [50380, 407, 437, 366, 264, 11, 291, 458, 11, 721, 294, 264, 382, 34015, 39839, 7372, 420, 10088, 30, 50688], "temperature": 0.0, "avg_logprob": -0.12448232314165901, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.00035688967909663916}, {"id": 161, "seek": 105400, "start": 1061.92, "end": 1067.12, "text": " A very important function is this async IO run. This is basically the function that needs to be", "tokens": [50760, 316, 588, 1021, 2445, 307, 341, 382, 34015, 39839, 1190, 13, 639, 307, 1936, 264, 2445, 300, 2203, 281, 312, 51020], "temperature": 0.0, "avg_logprob": -0.12448232314165901, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.00035688967909663916}, {"id": 162, "seek": 105400, "start": 1067.12, "end": 1072.64, "text": " called in order to set up the event loop to run everything in that event loop. You typically", "tokens": [51020, 1219, 294, 1668, 281, 992, 493, 264, 2280, 6367, 281, 1190, 1203, 294, 300, 2280, 6367, 13, 509, 5850, 51296], "temperature": 0.0, "avg_logprob": -0.12448232314165901, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.00035688967909663916}, {"id": 163, "seek": 105400, "start": 1072.64, "end": 1077.04, "text": " just have one of these calls in your application, basically starting the event loop and then running", "tokens": [51296, 445, 362, 472, 295, 613, 5498, 294, 428, 3861, 11, 1936, 2891, 264, 2280, 6367, 293, 550, 2614, 51516], "temperature": 0.0, "avg_logprob": -0.12448232314165901, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.00035688967909663916}, {"id": 164, "seek": 107704, "start": 1077.04, "end": 1084.1599999999999, "text": " anything that's being scheduled. Then you have this gather function. Gather is, like I already", "tokens": [50364, 1340, 300, 311, 885, 15678, 13, 1396, 291, 362, 341, 5448, 2445, 13, 39841, 307, 11, 411, 286, 1217, 50720], "temperature": 0.0, "avg_logprob": -0.13096458261663263, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.003647006582468748}, {"id": 165, "seek": 107704, "start": 1084.1599999999999, "end": 1092.1599999999999, "text": " mentioned, it's a function where you can pass in coroutines or tasks. And then it runs all these", "tokens": [50720, 2835, 11, 309, 311, 257, 2445, 689, 291, 393, 1320, 294, 1181, 346, 1652, 420, 9608, 13, 400, 550, 309, 6676, 439, 613, 51120], "temperature": 0.0, "avg_logprob": -0.13096458261663263, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.003647006582468748}, {"id": 166, "seek": 107704, "start": 1092.1599999999999, "end": 1097.76, "text": " tasks until completion and then it returns. Async IO sleeper already mentioned. You also have a", "tokens": [51120, 9608, 1826, 19372, 293, 550, 309, 11247, 13, 1018, 34015, 39839, 2817, 260, 1217, 2835, 13, 509, 611, 362, 257, 51400], "temperature": 0.0, "avg_logprob": -0.13096458261663263, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.003647006582468748}, {"id": 167, "seek": 107704, "start": 1097.76, "end": 1102.0, "text": " couple of functions down here for waiting for certain things. So sometimes in an application", "tokens": [51400, 1916, 295, 6828, 760, 510, 337, 3806, 337, 1629, 721, 13, 407, 2171, 294, 364, 3861, 51612], "temperature": 0.0, "avg_logprob": -0.13096458261663263, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.003647006582468748}, {"id": 168, "seek": 110200, "start": 1102.0, "end": 1107.2, "text": " you need to synchronize between various different parts. So there are some handy functions for", "tokens": [50364, 291, 643, 281, 19331, 1125, 1296, 3683, 819, 3166, 13, 407, 456, 366, 512, 13239, 6828, 337, 50624], "temperature": 0.0, "avg_logprob": -0.09023469617997093, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0023944100830703974}, {"id": 169, "seek": 110200, "start": 1107.2, "end": 1114.56, "text": " this as well. Now what is this task object? I keep mentioning. Task objects are basically just", "tokens": [50624, 341, 382, 731, 13, 823, 437, 307, 341, 5633, 2657, 30, 286, 1066, 18315, 13, 30428, 6565, 366, 1936, 445, 50992], "temperature": 0.0, "avg_logprob": -0.09023469617997093, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0023944100830703974}, {"id": 170, "seek": 110200, "start": 1114.56, "end": 1120.4, "text": " coroutine calls that are being scheduled. And it's a way for the event loop to manage everything", "tokens": [50992, 1181, 45075, 5498, 300, 366, 885, 15678, 13, 400, 309, 311, 257, 636, 337, 264, 2280, 6367, 281, 3067, 1203, 51284], "temperature": 0.0, "avg_logprob": -0.09023469617997093, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0023944100830703974}, {"id": 171, "seek": 110200, "start": 1120.4, "end": 1127.76, "text": " that happens in the event loop. So whenever something is scheduled to be run, you create a task", "tokens": [51284, 300, 2314, 294, 264, 2280, 6367, 13, 407, 5699, 746, 307, 15678, 281, 312, 1190, 11, 291, 1884, 257, 5633, 51652], "temperature": 0.0, "avg_logprob": -0.09023469617997093, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0023944100830703974}, {"id": 172, "seek": 112776, "start": 1127.76, "end": 1132.56, "text": " object. And this is done behind the scenes for you. You don't have to create these objects yourself.", "tokens": [50364, 2657, 13, 400, 341, 307, 1096, 2261, 264, 8026, 337, 291, 13, 509, 500, 380, 362, 281, 1884, 613, 6565, 1803, 13, 50604], "temperature": 0.0, "avg_logprob": -0.06910205942339602, "compression_ratio": 1.8841698841698842, "no_speech_prob": 0.0013819154119119048}, {"id": 173, "seek": 112776, "start": 1133.28, "end": 1137.76, "text": " In fact, you should not create these objects yourself. You should always use one of the functions", "tokens": [50640, 682, 1186, 11, 291, 820, 406, 1884, 613, 6565, 1803, 13, 509, 820, 1009, 764, 472, 295, 264, 6828, 50864], "temperature": 0.0, "avg_logprob": -0.06910205942339602, "compression_ratio": 1.8841698841698842, "no_speech_prob": 0.0013819154119119048}, {"id": 174, "seek": 112776, "start": 1137.76, "end": 1145.6, "text": " for this, like this create task that you have down here. And then these task objects go into the", "tokens": [50864, 337, 341, 11, 411, 341, 1884, 5633, 300, 291, 362, 760, 510, 13, 400, 550, 613, 5633, 6565, 352, 666, 264, 51256], "temperature": 0.0, "avg_logprob": -0.06910205942339602, "compression_ratio": 1.8841698841698842, "no_speech_prob": 0.0013819154119119048}, {"id": 175, "seek": 112776, "start": 1145.6, "end": 1151.04, "text": " event loop or run and everything happens for you. There are also some query functions down here if", "tokens": [51256, 2280, 6367, 420, 1190, 293, 1203, 2314, 337, 291, 13, 821, 366, 611, 512, 14581, 6828, 760, 510, 498, 51528], "temperature": 0.0, "avg_logprob": -0.06910205942339602, "compression_ratio": 1.8841698841698842, "no_speech_prob": 0.0013819154119119048}, {"id": 176, "seek": 112776, "start": 1151.04, "end": 1155.68, "text": " you're interested in what's currently scheduled on the event loop. You can have a look at the", "tokens": [51528, 291, 434, 3102, 294, 437, 311, 4362, 15678, 322, 264, 2280, 6367, 13, 509, 393, 362, 257, 574, 412, 264, 51760], "temperature": 0.0, "avg_logprob": -0.06910205942339602, "compression_ratio": 1.8841698841698842, "no_speech_prob": 0.0013819154119119048}, {"id": 177, "seek": 115568, "start": 1155.68, "end": 1163.68, "text": " documentation for those. So how does this event loop work? It's basically just a way to do the", "tokens": [50364, 14333, 337, 729, 13, 407, 577, 775, 341, 2280, 6367, 589, 30, 467, 311, 1936, 445, 257, 636, 281, 360, 264, 50764], "temperature": 0.0, "avg_logprob": -0.10621361348820829, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0007088453276082873}, {"id": 178, "seek": 115568, "start": 1163.68, "end": 1168.88, "text": " same kind of management as the OS is doing for threads, except that it's done in Python.", "tokens": [50764, 912, 733, 295, 4592, 382, 264, 12731, 307, 884, 337, 19314, 11, 3993, 300, 309, 311, 1096, 294, 15329, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10621361348820829, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0007088453276082873}, {"id": 179, "seek": 115568, "start": 1169.76, "end": 1177.8400000000001, "text": " And the async.io package manages one of these event loops. Now event loops can actually be", "tokens": [51068, 400, 264, 382, 34015, 13, 1004, 7372, 22489, 472, 295, 613, 2280, 16121, 13, 823, 2280, 16121, 393, 767, 312, 51472], "temperature": 0.0, "avg_logprob": -0.10621361348820829, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0007088453276082873}, {"id": 180, "seek": 115568, "start": 1177.8400000000001, "end": 1184.0800000000002, "text": " defined by multiple different libraries. But what's important is that there should only be", "tokens": [51472, 7642, 538, 3866, 819, 15148, 13, 583, 437, 311, 1021, 307, 300, 456, 820, 787, 312, 51784], "temperature": 0.0, "avg_logprob": -0.10621361348820829, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0007088453276082873}, {"id": 181, "seek": 118408, "start": 1184.08, "end": 1191.1999999999998, "text": " one event loop per thread. So you can have multiple threads, of course, also run. Then again,", "tokens": [50364, 472, 2280, 6367, 680, 7207, 13, 407, 291, 393, 362, 3866, 19314, 11, 295, 1164, 11, 611, 1190, 13, 1396, 797, 11, 50720], "temperature": 0.0, "avg_logprob": -0.12162994851871413, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0011314782314002514}, {"id": 182, "seek": 118408, "start": 1191.1999999999998, "end": 1199.4399999999998, "text": " you hit the same kind of roadblock as you've seen with the GIL. But there might be ways to,", "tokens": [50720, 291, 2045, 264, 912, 733, 295, 3060, 28830, 382, 291, 600, 1612, 365, 264, 460, 4620, 13, 583, 456, 1062, 312, 2098, 281, 11, 51132], "temperature": 0.0, "avg_logprob": -0.12162994851871413, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0011314782314002514}, {"id": 183, "seek": 118408, "start": 1199.4399999999998, "end": 1204.3999999999999, "text": " in your application, to make use of that. So that would be possible as well. Typically,", "tokens": [51132, 294, 428, 3861, 11, 281, 652, 764, 295, 300, 13, 407, 300, 576, 312, 1944, 382, 731, 13, 23129, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12162994851871413, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0011314782314002514}, {"id": 184, "seek": 118408, "start": 1204.3999999999999, "end": 1210.24, "text": " what you have in an async program is you just have a single thread. And so you just call this", "tokens": [51380, 437, 291, 362, 294, 364, 382, 34015, 1461, 307, 291, 445, 362, 257, 2167, 7207, 13, 400, 370, 291, 445, 818, 341, 51672], "temperature": 0.0, "avg_logprob": -0.12162994851871413, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0011314782314002514}, {"id": 185, "seek": 121024, "start": 1210.24, "end": 1217.2, "text": " run function once. Now, I mentioned blocking code. So blocking code basically is code that", "tokens": [50364, 1190, 2445, 1564, 13, 823, 11, 286, 2835, 17776, 3089, 13, 407, 17776, 3089, 1936, 307, 3089, 300, 50712], "temperature": 0.0, "avg_logprob": -0.08651642896691147, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.000698157527949661}, {"id": 186, "seek": 121024, "start": 1217.2, "end": 1224.0, "text": " doesn't collaborate with this async logic. And you have that quite often in Python. For example,", "tokens": [50712, 1177, 380, 18338, 365, 341, 382, 34015, 9952, 13, 400, 291, 362, 300, 1596, 2049, 294, 15329, 13, 1171, 1365, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08651642896691147, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.000698157527949661}, {"id": 187, "seek": 121024, "start": 1224.0, "end": 1229.28, "text": " let's say you're using one of the database modules. Not one of the async ones, but the regular ones.", "tokens": [51052, 718, 311, 584, 291, 434, 1228, 472, 295, 264, 8149, 16679, 13, 1726, 472, 295, 264, 382, 34015, 2306, 11, 457, 264, 3890, 2306, 13, 51316], "temperature": 0.0, "avg_logprob": -0.08651642896691147, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.000698157527949661}, {"id": 188, "seek": 121024, "start": 1229.28, "end": 1236.72, "text": " Those will all be synchronous. So you call, let's say, an execute to run some SQL. And that will", "tokens": [51316, 3950, 486, 439, 312, 44743, 13, 407, 291, 818, 11, 718, 311, 584, 11, 364, 14483, 281, 1190, 512, 19200, 13, 400, 300, 486, 51688], "temperature": 0.0, "avg_logprob": -0.08651642896691147, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.000698157527949661}, {"id": 189, "seek": 123672, "start": 1236.72, "end": 1242.8, "text": " actually wait until the database comes back with results. So in order to run this kind of code", "tokens": [50364, 767, 1699, 1826, 264, 8149, 1487, 646, 365, 3542, 13, 407, 294, 1668, 281, 1190, 341, 733, 295, 3089, 50668], "temperature": 0.0, "avg_logprob": -0.10574538740393234, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.0025486727245151997}, {"id": 190, "seek": 123672, "start": 1242.8, "end": 1249.1200000000001, "text": " in an async application, you have to use special functions. There's a very nice function called", "tokens": [50668, 294, 364, 382, 34015, 3861, 11, 291, 362, 281, 764, 2121, 6828, 13, 821, 311, 257, 588, 1481, 2445, 1219, 50984], "temperature": 0.0, "avg_logprob": -0.10574538740393234, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.0025486727245151997}, {"id": 191, "seek": 123672, "start": 1249.1200000000001, "end": 1258.8, "text": " async.io2 thread, which was added in Python 3.9, which makes this easy. So what these functions do", "tokens": [50984, 382, 34015, 13, 1004, 17, 7207, 11, 597, 390, 3869, 294, 15329, 805, 13, 24, 11, 597, 1669, 341, 1858, 13, 407, 437, 613, 6828, 360, 51468], "temperature": 0.0, "avg_logprob": -0.10574538740393234, "compression_ratio": 1.4896907216494846, "no_speech_prob": 0.0025486727245151997}, {"id": 192, "seek": 125880, "start": 1258.8, "end": 1267.2, "text": " is say they spin up a thread in your async application, run the code inside that thread,", "tokens": [50364, 307, 584, 436, 6060, 493, 257, 7207, 294, 428, 382, 34015, 3861, 11, 1190, 264, 3089, 1854, 300, 7207, 11, 50784], "temperature": 0.0, "avg_logprob": -0.07481119550507644, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0009692661697044969}, {"id": 193, "seek": 125880, "start": 1267.2, "end": 1273.28, "text": " and then pass back control via the threading logic to your event loop. So you can still run", "tokens": [50784, 293, 550, 1320, 646, 1969, 5766, 264, 7207, 278, 9952, 281, 428, 2280, 6367, 13, 407, 291, 393, 920, 1190, 51088], "temperature": 0.0, "avg_logprob": -0.07481119550507644, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0009692661697044969}, {"id": 194, "seek": 125880, "start": 1273.28, "end": 1277.9199999999998, "text": " synchronous code because the synchronous code is most likely going to give up the GIL. For example,", "tokens": [51088, 44743, 3089, 570, 264, 44743, 3089, 307, 881, 3700, 516, 281, 976, 493, 264, 460, 4620, 13, 1171, 1365, 11, 51320], "temperature": 0.0, "avg_logprob": -0.07481119550507644, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0009692661697044969}, {"id": 195, "seek": 125880, "start": 1278.8, "end": 1284.32, "text": " if you have a good database module, then when you execute something, typically what these", "tokens": [51364, 498, 291, 362, 257, 665, 8149, 10088, 11, 550, 562, 291, 14483, 746, 11, 5850, 437, 613, 51640], "temperature": 0.0, "avg_logprob": -0.07481119550507644, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0009692661697044969}, {"id": 196, "seek": 128432, "start": 1284.32, "end": 1290.72, "text": " database modules do is they give back control to other threads running Python code because they're", "tokens": [50364, 8149, 16679, 360, 307, 436, 976, 646, 1969, 281, 661, 19314, 2614, 15329, 3089, 570, 436, 434, 50684], "temperature": 0.0, "avg_logprob": -0.06764806310335796, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00028219138039276004}, {"id": 197, "seek": 128432, "start": 1290.72, "end": 1295.6799999999998, "text": " just running C code at that time. So this is a way to make everything work together.", "tokens": [50684, 445, 2614, 383, 3089, 412, 300, 565, 13, 407, 341, 307, 257, 636, 281, 652, 1203, 589, 1214, 13, 50932], "temperature": 0.0, "avg_logprob": -0.06764806310335796, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00028219138039276004}, {"id": 198, "seek": 128432, "start": 1297.9199999999998, "end": 1302.08, "text": " And of course, there's lots more. I'm not going to talk about these things because I don't have", "tokens": [51044, 400, 295, 1164, 11, 456, 311, 3195, 544, 13, 286, 478, 406, 516, 281, 751, 466, 613, 721, 570, 286, 500, 380, 362, 51252], "temperature": 0.0, "avg_logprob": -0.06764806310335796, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00028219138039276004}, {"id": 199, "seek": 128432, "start": 1302.08, "end": 1308.48, "text": " enough time for that. In fact, I'm already almost out of time. So I have to speed up a bit. Let's", "tokens": [51252, 1547, 565, 337, 300, 13, 682, 1186, 11, 286, 478, 1217, 1920, 484, 295, 565, 13, 407, 286, 362, 281, 3073, 493, 257, 857, 13, 961, 311, 51572], "temperature": 0.0, "avg_logprob": -0.06764806310335796, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.00028219138039276004}, {"id": 200, "seek": 130848, "start": 1308.48, "end": 1315.52, "text": " just do a very quick overview of what's in the async ecosystem. So of course, we have the", "tokens": [50364, 445, 360, 257, 588, 1702, 12492, 295, 437, 311, 294, 264, 382, 34015, 11311, 13, 407, 295, 1164, 11, 321, 362, 264, 50716], "temperature": 0.0, "avg_logprob": -0.0931736193205181, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007912211585789919}, {"id": 201, "seek": 130848, "start": 1315.52, "end": 1322.0, "text": " async.io standard library package. We have event loops inside the async.io package. If you want", "tokens": [50716, 382, 34015, 13, 1004, 3832, 6405, 7372, 13, 492, 362, 2280, 16121, 1854, 264, 382, 34015, 13, 1004, 7372, 13, 759, 291, 528, 51040], "temperature": 0.0, "avg_logprob": -0.0931736193205181, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007912211585789919}, {"id": 202, "seek": 130848, "start": 1322.0, "end": 1327.04, "text": " fast loops, then you can use UV loop, which is a faster implementation, speeds up your async", "tokens": [51040, 2370, 16121, 11, 550, 291, 393, 764, 17887, 6367, 11, 597, 307, 257, 4663, 11420, 11, 16411, 493, 428, 382, 34015, 51292], "temperature": 0.0, "avg_logprob": -0.0931736193205181, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007912211585789919}, {"id": 203, "seek": 130848, "start": 1327.6, "end": 1333.28, "text": " by almost four times. You can also have a look at other stacks that implement event loops,", "tokens": [51320, 538, 1920, 1451, 1413, 13, 509, 393, 611, 362, 257, 574, 412, 661, 30792, 300, 4445, 2280, 16121, 11, 51604], "temperature": 0.0, "avg_logprob": -0.0931736193205181, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0007912211585789919}, {"id": 204, "seek": 133328, "start": 1333.36, "end": 1339.12, "text": " like Trio, for example, where you can use the package any.io, which abstracts these things. So", "tokens": [50368, 411, 314, 6584, 11, 337, 1365, 11, 689, 291, 393, 764, 264, 7372, 604, 13, 1004, 11, 597, 12649, 82, 613, 721, 13, 407, 50656], "temperature": 0.0, "avg_logprob": -0.13656501567110102, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0011681984178721905}, {"id": 205, "seek": 133328, "start": 1339.12, "end": 1345.28, "text": " you can use basically Trio or you can use async.io loops underneath in your application", "tokens": [50656, 291, 393, 764, 1936, 314, 6584, 420, 291, 393, 764, 382, 34015, 13, 1004, 16121, 7223, 294, 428, 3861, 50964], "temperature": 0.0, "avg_logprob": -0.13656501567110102, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0011681984178721905}, {"id": 206, "seek": 133328, "start": 1346.24, "end": 1353.2, "text": " when using any.io and it abstracts away all the details. Now, there's a rather large system of", "tokens": [51012, 562, 1228, 604, 13, 1004, 293, 309, 12649, 82, 1314, 439, 264, 4365, 13, 823, 11, 456, 311, 257, 2831, 2416, 1185, 295, 51360], "temperature": 0.0, "avg_logprob": -0.13656501567110102, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0011681984178721905}, {"id": 207, "seek": 133328, "start": 1354.3999999999999, "end": 1361.52, "text": " modules and packages around the async world in Python. Many of these are grouped under the", "tokens": [51420, 16679, 293, 17401, 926, 264, 382, 34015, 1002, 294, 15329, 13, 5126, 295, 613, 366, 41877, 833, 264, 51776], "temperature": 0.0, "avg_logprob": -0.13656501567110102, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0011681984178721905}, {"id": 208, "seek": 136152, "start": 1361.52, "end": 1369.28, "text": " aio-lips. So if you go to GitHub to that URL, then you will find lots of examples there.", "tokens": [50364, 257, 1004, 12, 75, 2600, 13, 407, 498, 291, 352, 281, 23331, 281, 300, 12905, 11, 550, 291, 486, 915, 3195, 295, 5110, 456, 13, 50752], "temperature": 0.0, "avg_logprob": -0.15075983750192742, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0004234996740706265}, {"id": 209, "seek": 136152, "start": 1369.28, "end": 1374.6399999999999, "text": " There are database packages there. There are things for doing HTTP, DNS, and so on. Something to", "tokens": [50752, 821, 366, 8149, 17401, 456, 13, 821, 366, 721, 337, 884, 33283, 11, 35153, 11, 293, 370, 322, 13, 6595, 281, 51020], "temperature": 0.0, "avg_logprob": -0.15075983750192742, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0004234996740706265}, {"id": 210, "seek": 136152, "start": 1374.6399999999999, "end": 1382.4, "text": " watch out is the database modules typically don't support transactions, which can be a bummer", "tokens": [51020, 1159, 484, 307, 264, 8149, 16679, 5850, 500, 380, 1406, 16856, 11, 597, 393, 312, 257, 13309, 936, 51408], "temperature": 0.0, "avg_logprob": -0.15075983750192742, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0004234996740706265}, {"id": 211, "seek": 136152, "start": 1382.4, "end": 1388.16, "text": " sometimes. At the higher level, you have, of course, web frameworks again because, you know,", "tokens": [51408, 2171, 13, 1711, 264, 2946, 1496, 11, 291, 362, 11, 295, 1164, 11, 3670, 29834, 797, 570, 11, 291, 458, 11, 51696], "temperature": 0.0, "avg_logprob": -0.15075983750192742, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0004234996740706265}, {"id": 212, "seek": 138816, "start": 1388.24, "end": 1394.8000000000002, "text": " everyone loves web frameworks. And, of course, you have API frameworks. The most popular one right", "tokens": [50368, 1518, 6752, 3670, 29834, 13, 400, 11, 295, 1164, 11, 291, 362, 9362, 29834, 13, 440, 881, 3743, 472, 558, 50696], "temperature": 0.0, "avg_logprob": -0.13683338165283204, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.00048008051817305386}, {"id": 213, "seek": 138816, "start": 1394.8000000000002, "end": 1402.64, "text": " now is FastAPI for doing the REST APIs, and then Strawberry is coming very strongly as a GraphQL", "tokens": [50696, 586, 307, 15968, 4715, 40, 337, 884, 264, 497, 14497, 21445, 11, 293, 550, 45906, 307, 1348, 588, 10613, 382, 257, 21884, 13695, 51088], "temperature": 0.0, "avg_logprob": -0.13683338165283204, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.00048008051817305386}, {"id": 214, "seek": 138816, "start": 1402.64, "end": 1410.96, "text": " server. Both operate async. Even Django does, or starts, is starting to do async right now. It's", "tokens": [51088, 7154, 13, 6767, 9651, 382, 34015, 13, 2754, 33464, 17150, 775, 11, 420, 3719, 11, 307, 2891, 281, 360, 382, 34015, 558, 586, 13, 467, 311, 51504], "temperature": 0.0, "avg_logprob": -0.13683338165283204, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.00048008051817305386}, {"id": 215, "seek": 138816, "start": 1410.96, "end": 1417.1200000000001, "text": " not fully there yet. If you're using Flask for synchronous code, then you might want to look", "tokens": [51504, 406, 4498, 456, 1939, 13, 759, 291, 434, 1228, 3235, 3863, 337, 44743, 3089, 11, 550, 291, 1062, 528, 281, 574, 51812], "temperature": 0.0, "avg_logprob": -0.13683338165283204, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.00048008051817305386}, {"id": 216, "seek": 141712, "start": 1417.12, "end": 1423.84, "text": " at a quad, which is like an async implementation using a similar API. And the most famous one", "tokens": [50364, 412, 257, 10787, 11, 597, 307, 411, 364, 382, 34015, 11420, 1228, 257, 2531, 9362, 13, 400, 264, 881, 4618, 472, 50700], "temperature": 0.0, "avg_logprob": -0.1218413344217003, "compression_ratio": 1.484375, "no_speech_prob": 0.00037377182161435485}, {"id": 217, "seek": 141712, "start": 1423.84, "end": 1431.4399999999998, "text": " probably in the async space is Tornado, which some of you may know. It's very fast. Right. So", "tokens": [50700, 1391, 294, 264, 382, 34015, 1901, 307, 314, 1865, 1573, 11, 597, 512, 295, 291, 815, 458, 13, 467, 311, 588, 2370, 13, 1779, 13, 407, 51080], "temperature": 0.0, "avg_logprob": -0.1218413344217003, "compression_ratio": 1.484375, "no_speech_prob": 0.00037377182161435485}, {"id": 218, "seek": 141712, "start": 1431.4399999999998, "end": 1437.1999999999998, "text": " let's go back to the bot. If you want to see the bot code, it's on GitHub. Just search for", "tokens": [51080, 718, 311, 352, 646, 281, 264, 10592, 13, 759, 291, 528, 281, 536, 264, 10592, 3089, 11, 309, 311, 322, 23331, 13, 1449, 3164, 337, 51368], "temperature": 0.0, "avg_logprob": -0.1218413344217003, "compression_ratio": 1.484375, "no_speech_prob": 0.00037377182161435485}, {"id": 219, "seek": 141712, "start": 1437.1999999999998, "end": 1444.0, "text": " Eugenics Telegram, and then you'll find it. How does it work? Very easy. You just subclass the client", "tokens": [51368, 462, 27915, 1167, 14889, 1342, 11, 293, 550, 291, 603, 915, 309, 13, 1012, 775, 309, 589, 30, 4372, 1858, 13, 509, 445, 1422, 11665, 264, 6423, 51708], "temperature": 0.0, "avg_logprob": -0.1218413344217003, "compression_ratio": 1.484375, "no_speech_prob": 0.00037377182161435485}, {"id": 220, "seek": 144400, "start": 1444.0, "end": 1449.76, "text": " of that package. You do some configuration. You do some observability, so logging for things.", "tokens": [50364, 295, 300, 7372, 13, 509, 360, 512, 11694, 13, 509, 360, 512, 9951, 2310, 11, 370, 27991, 337, 721, 13, 50652], "temperature": 0.0, "avg_logprob": -0.09967194480457525, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.0022462711203843355}, {"id": 221, "seek": 144400, "start": 1450.64, "end": 1456.08, "text": " I'm lazy, so I'm just, you know, put all the admin messages into another telegram chat that I can", "tokens": [50696, 286, 478, 14847, 11, 370, 286, 478, 445, 11, 291, 458, 11, 829, 439, 264, 24236, 7897, 666, 1071, 4304, 1342, 5081, 300, 286, 393, 50968], "temperature": 0.0, "avg_logprob": -0.09967194480457525, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.0022462711203843355}, {"id": 222, "seek": 144400, "start": 1456.08, "end": 1459.68, "text": " manage so I can see what's happening without having to go to the server.", "tokens": [50968, 3067, 370, 286, 393, 536, 437, 311, 2737, 1553, 1419, 281, 352, 281, 264, 7154, 13, 51148], "temperature": 0.0, "avg_logprob": -0.09967194480457525, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.0022462711203843355}, {"id": 223, "seek": 144400, "start": 1462.72, "end": 1469.12, "text": " Because we actually want to catch all the messages of these people signing up to the", "tokens": [51300, 1436, 321, 767, 528, 281, 3745, 439, 264, 7897, 295, 613, 561, 13393, 493, 281, 264, 51620], "temperature": 0.0, "avg_logprob": -0.09967194480457525, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.0022462711203843355}, {"id": 224, "seek": 146912, "start": 1469.12, "end": 1474.3999999999999, "text": " telegram group, and not just people who want to run bot commands, you know, slash something.", "tokens": [50364, 4304, 1342, 1594, 11, 293, 406, 445, 561, 567, 528, 281, 1190, 10592, 16901, 11, 291, 458, 11, 17330, 746, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10640392632320009, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0016979336505755782}, {"id": 225, "seek": 146912, "start": 1475.6799999999998, "end": 1479.9199999999998, "text": " We have to use the catch all in here, and that's also why we need something that's very scalable,", "tokens": [50692, 492, 362, 281, 764, 264, 3745, 439, 294, 510, 11, 293, 300, 311, 611, 983, 321, 643, 746, 300, 311, 588, 38481, 11, 50904], "temperature": 0.0, "avg_logprob": -0.10640392632320009, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0016979336505755782}, {"id": 226, "seek": 146912, "start": 1479.9199999999998, "end": 1485.6799999999998, "text": " because it literally, the bot sees all the messages that go into that group, and it has to", "tokens": [50904, 570, 309, 3736, 11, 264, 10592, 8194, 439, 264, 7897, 300, 352, 666, 300, 1594, 11, 293, 309, 575, 281, 51192], "temperature": 0.0, "avg_logprob": -0.10640392632320009, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0016979336505755782}, {"id": 227, "seek": 146912, "start": 1485.6799999999998, "end": 1491.12, "text": " handle all these messages. And then what you do is basically you just do these awaits to", "tokens": [51192, 4813, 439, 613, 7897, 13, 400, 550, 437, 291, 360, 307, 1936, 291, 445, 360, 613, 45955, 281, 51464], "temperature": 0.0, "avg_logprob": -0.10640392632320009, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0016979336505755782}, {"id": 228, "seek": 146912, "start": 1492.6399999999999, "end": 1498.08, "text": " whenever you have to do IO. And if you look at it, it looks very much like synchronous code,", "tokens": [51540, 5699, 291, 362, 281, 360, 39839, 13, 400, 498, 291, 574, 412, 309, 11, 309, 1542, 588, 709, 411, 44743, 3089, 11, 51812], "temperature": 0.0, "avg_logprob": -0.10640392632320009, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0016979336505755782}, {"id": 229, "seek": 149808, "start": 1498.3999999999999, "end": 1502.48, "text": " except that you have these awaits in front of certain things. Wherever something happens,", "tokens": [50380, 3993, 300, 291, 362, 613, 45955, 294, 1868, 295, 1629, 721, 13, 30903, 746, 2314, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10472786161634658, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00037386445910669863}, {"id": 230, "seek": 149808, "start": 1502.48, "end": 1508.0, "text": " where you need to do some IO, you put the await in front of it, and then everything", "tokens": [50584, 689, 291, 643, 281, 360, 512, 39839, 11, 291, 829, 264, 19670, 294, 1868, 295, 309, 11, 293, 550, 1203, 50860], "temperature": 0.0, "avg_logprob": -0.10472786161634658, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00037386445910669863}, {"id": 231, "seek": 149808, "start": 1508.0, "end": 1513.12, "text": " else looks very natural, looks like a very, you know, very much like a synchronous program.", "tokens": [50860, 1646, 1542, 588, 3303, 11, 1542, 411, 257, 588, 11, 291, 458, 11, 588, 709, 411, 257, 44743, 1461, 13, 51116], "temperature": 0.0, "avg_logprob": -0.10472786161634658, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00037386445910669863}, {"id": 232, "seek": 149808, "start": 1516.0, "end": 1524.1599999999999, "text": " So what are the results of doing this? Writing this bot, it's actually helped us a lot. We've,", "tokens": [51260, 407, 437, 366, 264, 3542, 295, 884, 341, 30, 32774, 341, 10592, 11, 309, 311, 767, 4254, 505, 257, 688, 13, 492, 600, 11, 51668], "temperature": 0.0, "avg_logprob": -0.10472786161634658, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00037386445910669863}, {"id": 233, "seek": 152416, "start": 1524.16, "end": 1532.3200000000002, "text": " you know, had over almost 800 spam signups since April 2022, when we started to use it.", "tokens": [50364, 291, 458, 11, 632, 670, 1920, 13083, 24028, 1465, 7528, 1670, 6929, 20229, 11, 562, 321, 1409, 281, 764, 309, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1190368566620216, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.011304175481200218}, {"id": 234, "seek": 152416, "start": 1534.4, "end": 1539.2, "text": " And this has, I mean, this is one part, this is just the admin part that saved us a lot of work,", "tokens": [50876, 400, 341, 575, 11, 286, 914, 11, 341, 307, 472, 644, 11, 341, 307, 445, 264, 24236, 644, 300, 6624, 505, 257, 688, 295, 589, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1190368566620216, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.011304175481200218}, {"id": 235, "seek": 152416, "start": 1539.2, "end": 1542.96, "text": " but of course, you know, every single signup would have cost spam messages.", "tokens": [51116, 457, 295, 1164, 11, 291, 458, 11, 633, 2167, 1465, 1010, 576, 362, 2063, 24028, 7897, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1190368566620216, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.011304175481200218}, {"id": 236, "seek": 152416, "start": 1543.76, "end": 1548.48, "text": " And so that was very successful. So the time spent on actually writing things", "tokens": [51344, 400, 370, 300, 390, 588, 4406, 13, 407, 264, 565, 4418, 322, 767, 3579, 721, 51580], "temperature": 0.0, "avg_logprob": -0.1190368566620216, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.011304175481200218}, {"id": 237, "seek": 154848, "start": 1549.44, "end": 1555.52, "text": " is, was well invested and basically mission accomplished. So what's the main takeaway of", "tokens": [50412, 307, 11, 390, 731, 13104, 293, 1936, 4447, 15419, 13, 407, 437, 311, 264, 2135, 30681, 295, 50716], "temperature": 0.0, "avg_logprob": -0.2300918254446476, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.008054422214627266}, {"id": 238, "seek": 154848, "start": 1555.52, "end": 1563.3600000000001, "text": " the talk? I think it's great. And give it a try if you can. Thank you for your attention.", "tokens": [50716, 264, 751, 30, 286, 519, 309, 311, 869, 13, 400, 976, 309, 257, 853, 498, 291, 393, 13, 1044, 291, 337, 428, 3202, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2300918254446476, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.008054422214627266}, {"id": 239, "seek": 156336, "start": 1564.32, "end": 1581.76, "text": " Thank you Mark Andre. Thanks everyone for attending this talk. Don't hesitate to reach to Mark Andre", "tokens": [50412, 1044, 291, 3934, 20667, 13, 2561, 1518, 337, 15862, 341, 751, 13, 1468, 380, 20842, 281, 2524, 281, 3934, 20667, 51284], "temperature": 0.0, "avg_logprob": -0.2226189374923706, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.01445120107382536}, {"id": 240, "seek": 156336, "start": 1581.76, "end": 1588.56, "text": " if you have any question or want to discuss this further. Thanks a lot. Thank you. Thank you very", "tokens": [51284, 498, 291, 362, 604, 1168, 420, 528, 281, 2248, 341, 3052, 13, 2561, 257, 688, 13, 1044, 291, 13, 1044, 291, 588, 51624], "temperature": 0.0, "avg_logprob": -0.2226189374923706, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.01445120107382536}, {"id": 241, "seek": 158856, "start": 1588.56, "end": 1603.76, "text": " much for coming. Let me just do a picture. Excellent.", "tokens": [50368, 709, 337, 1348, 13, 961, 385, 445, 360, 257, 3036, 13, 16723, 13, 51124], "temperature": 0.0, "avg_logprob": -0.19105175137519836, "compression_ratio": 0.8688524590163934, "no_speech_prob": 0.04319901764392853}], "language": "en"}