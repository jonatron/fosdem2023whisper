{"text": " Hello, everyone, could you hear me well? How do you feel today, Sunday, the fourth day? Did you wake up with energy? Or you are like... We have energy, right? Okay, that's good. I'm Ida Pugja from Peru. I traveled to Europe for the first time for this year. It's the first time also doing a talk in English. So if I make some mistake, I hope you can understand this. And I am a technology evangelist at Percona. I started six months ago. It's a really great company focusing on databases. And if you want to follow me on Twitter or want to connect, I will be really happy by LinkedIn. So I used to publish things about open source containers, Kubernetes. About me, I am from Peru. I am a Google woman tech maker. I was nominated as a docker captain the last year. And I am a container and database enthusiast. So this talk is about monitoring your database with open source tools. We are going to focus in PMM. How many of you did you hear about PMM before? Percona monitoring and management. Okay, so it's something new for this room. And this is a perspective of a beginner view. So this is not something advanced. I will see this monitoring tool as a perspective, because I am learning about databases in the company where I am working, Percona. So we are going to evaluate why it's important the value of monitoring databases. Also, we will see PMM, Percona monitoring and management, and how we can effectively see the dashboards and graphs that we can get to monitor and manage our databases. All this time I was working in a database company. I asked myself, and I realized the importance is to monitor databases because we can just have one database running for us. We can have several of them. We can have it in cloud, in infra. So it's very important, and it's why we should ask why we should care about databases, why we should care about monitoring these databases. And we can ask most questions like, is my database performing well? So as we start to work with databases, there are several queries that we make. So this queries maybe is not executing in the time that we expected. It could be taking more time, and it's going to have bottleneck in the time that we are executing. So we should care about this and know that there are metrics that we should detect the problem for the performance of our databases. Another question that could ask ourselves is, are databases available and accepting connections? We can have several databases, and many connections could be made, but if we don't put a limit in these connections, it could just crash. But if we put a limit also, we should be aware when we are achieving or we are reaching that limit to increase or maybe to stop the connection. But if we pass the limit, so this is going to be a problem in our databases, right? If this is an e-commerce company, it can happen because the user is going to wait just two seconds or maybe one second to go to the next page faster. But if no, if we did this in three seconds, five seconds that the user is going to go to another page and we lost as a business. Is my sister estable? We also monitor the infrastructure where our database is running, not just the databases, because it is running over an infrastructure. We are using CPUs, memory, this, and we are not able to provision these resources on time. We will be in problem for our databases. I am having avoidable time, so if our hardware is not enough, so our application could crash and we can have hardware failures or network outage. We should be aware of these metrics to avoid these problems. So we also can have human errors or these crashes. So there are metrics that we can see to identify these before. But we can just see what are the problems when we are having those problems. We can also prevent these problems, asking these questions. I am minimising performance issues that can impact my business. I am able to identify these issues before they happen because there is a way to prevent. As a previous example, if we are reaching the limit, we can see it before we are reaching it. So we can prevent provisioning more resources, maybe checking the query that we already saw that is taking too much time executing. So there are ways we can prevent these problems. Nowadays, there are challenges that we have when we think to monitor databases. For example, the data volume grow. We don't talk about gigabytes now. We are talking about terabytes or maybe more because the database has a lot. It's a challenge to monitor these days. The complexity of the model databases. Right now, it's SQL databases, not SQL databases. The database has different models. It could run in different cloud providers. It could have different models. So the complexity just grow. Now it's different than before. The downtime and data loss. So it's one race that we should try to monitor to prevent it. Lack of visibility. If we don't have these things ready to be, to check it, if we have to do another thing, like maybe create scripts, Linux script or bash script to check it and to get this metrics, we don't have that data on time. We don't have that metrics on time. But if we have that visibility, it's going to be easy for us to detect these problems to monitor our databases. Even better, if we have everything in one single dashboard where we can visualize it. I learned fatigue. We can monitor many things. But with this, we have different databases. They can have MySQL, they can have other kind of databases. So it depends on the business, what we want to monitor, what metrics we are going to get to monitor. But if we don't know in all these things that we have for databases, we are going to get a lot of things that maybe we don't need and create alerts for maybe things that we really don't need. We are not focusing in that business exactly. We should try to focus to monitor, we should try to focus in the business. What's the metrics that are important for us, for our business? Integration with other tools. So with that time, we can do continuous integration, continuous delivery things. So it increases also the complexity to monitor our databases because this is not in a single place. This go over a process that is also devolved. These things are being automated. Now that we know why we should care about monitor databases, we will talk about one of the solutions that could help us to monitor. This is Percona monitoring and management, which is PMM. This is an open source tools and free tools, also based in other open source tools like MySQL. This led us to monitor databases like MySQL, MariaDB, PostgreSQL, AmongoDB, but not just that. As I said before, this also led us to monitor the infrastructure where our database is running. It's important to know about that. And it also helped us to performance our databases to simplify the management of these databases and we can exchange the security. Percona monitoring and management is built on top of other open source tools like Grafana. I know many of you use Grafana, right? Who use Grafana? Okay, a lot. And Victoria metrics also to storage this data, that metrics we collect for a long time. We are using clickhose to create these reports in real time with all these metrics that we collect in the time. We are using PostgreSQL to storage all the metadata and all these metrics for databases, all the important data that we have in PMM. And everything that we visualize is saved in this database. And we use Docker to install PMM. We can containerize the installation of PMM, the client and the server, and use it in different platforms. We also use Kubernetes operators for scaling our databases. There are three levels of deep when we talk about PMM interface. This is the big one, which is a dashboard that we all know, but we can go deeper and see the graphs, that is a graphical representation of the metrics in long period of time. And we have the metrics also, which is a countable number that represents some value, some important value of our infrastructure of our database. Okay, as I tell you before, what we want to monitor is going to depend of our business. We are not going to monitor the same metrics as another business. It depends a lot on that. And we also should aware of the alerts that we create that should be focused on what we do as a business and create this alertness and notifications that could be notified when we need it. So it could be integrated with Slack, with many other tools that we have in the hand to know when the problem is happening exactly. Some important metrics, some of that that we can check with PMM, is query performance, high CPU, high memory usage, and this high disk part of the infrastructure, the amount of user connections. We can know when the data grows and other kind of metrics that we can have. Could somebody tell me what other kind of metrics we can check with PMM? With some monitoring tool? A part of the infrastructure or... Okay, we'll check. If we see the long query response times, as I say, some queries could take some time, PMM has a very good dashboard, which is this, is Khan, query analytics dashboard, where we can see for a specific... We can see all the queries for our databases. In this case, I'm seeing all the 10 top queries that is running in our databases, but also we have an option to check it here if we want to just check for MySQL, for Postgres, or MongoDB, and we will see the amount of queries per second for example that we are running and how time is taking. So if we open this dashboard when we are working in databases, the first thing that we are going to see is, okay, this query is taking too much time. In this case, no, but we have a query that is taking too much time, or it's running a lot of queries per second. We can see the first one, and we can start troubleshooting from that point. The high CPU utilization is part of the infrastructure. Also, it's important to know how is this going. For example, this dashboard in PMA, in Percona Monitoring and Management, we can see for a specific note, we can check a note because we can have different notes running in our infrastructure, and in this case, for example, we have 25% of our disk that is using. This may be not a good example because I checked last 12 hours, but we can check our disk usage during six months or more, and then it's when we can see and take decisions. Let's see if this is like six months. We are using just 25% of our disk. It could be a problem because we have a lot of infrastructure that we are not using and we are wasting money because of this space that we are not using in six months. We can reduce our CPU and save money. High memory usage is this dashboard where we can see the amount of memory that I have for my databases and also can see what is using for Kakache, what has been using, what is going to be ready to be free, and this is good because we can also see when we are reaching the limit of the memory and we can take actions to provisioning another disk. We can say that this is very easy when we are working in cloud because just we click in a button and say, hey, provision another disk or increment the memory, but if we are working in infra, maybe in a private cloud, this is hard because we have to prepare the logistics to get another disk, another memory is going to take time, and have this kind of visualization helps. The amount of input or output that we make in our disk, you also can check it in this graph. We can see that your latency here, in this case, is stable, but we can have peaks to see where we are detecting these problems. User connections, as I say, it helps to monitor the number of active database connections and size it appropriately, and also put limits in our connections for our databases. We have, in this case, MySQL connections. We can see that you are for other databases too, but in this case, it's also stable. We are going to have peaks when we can see that we are working with a lot of transactions on our databases, and we can take actions with that. The maximum of connections allowed is 151. We are in 150. If this is going to be 151, this is going to alert us. You have to check this. The data grow also. We can see a dashboard where we can... Where you can see where our data is a lot when we are inserting a lot of data in our databases, or we are just removing things. In this case, it's going to show when my databases start, and it's not like too good to see it here, but if I have time, yes, I still have time. I still have time, right? To show something, to show the dashboard. I have time. If you want to try it right now, we can check this PMM demo per con graph. You can enter. Right now, we are going to check the dashboard, but what we learned now, it was some aspects that let me think what we should keep away and monitoring our databases, and also how to explore PMM, which is an open source tool, is available there. You have to double down and start to check it and explore it, and it's easy to visualize things, so we are going to check now PMM. You can also enter to this link, so it's free to experiment. Let's see if this is going to work. Yes, this is the dashboard that we have. We have several nodes that we can choose here. A lot of them, so it depends on the database. We have nodes of MongoDB, MySQL, Postgres, and MySQL proxy also. We can check the details for a specific time. 12 hours is not enough, maybe, for some things, but maybe six months, three months, we have a lot of things here. So we also can check things about the system operator. I don't have access right now to see that, but we can also register alertings for this. In this case, we have three databases that are being monitored for Postgres. One of that is the database for PMM itself. We have nine databases in Mongo and 15 databases in MySQL. This is a good thing for PMM, because you can see everything in one single dashboard, and we can go deeper for each node or for each database as you want. Yeah, this is all. Thank you so much if you have some questions. So thanks a lot for the interesting talk. Does anyone have questions? Hi, hello, good morning. The query monitoring dashboard do have some advice on how to perform these queries, the bad queries, the slow queries. Some advice on how to perform the bad queries, the slow queries, how to rewrite them. If you go deeper into that query, you will open another dashboard where you are going to be able to see suggestions. You are doing something bad here, and you can fix it with that. Hi, thank you for the talk, first of all. One question regarding the PMM query analytics. Is it possible to filter by connection and not by database? Say it again, please. I want all queries from one connection instead of... Yeah, is it possible? Yeah, you can have just one connection. Okay, then we have to talk. Yeah, okay, thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Hello, everyone, could you hear me well?", "tokens": [2425, 11, 1518, 11, 727, 291, 1568, 385, 731, 30], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 1, "seek": 0, "start": 7.0, "end": 12.0, "text": " How do you feel today, Sunday, the fourth day?", "tokens": [1012, 360, 291, 841, 965, 11, 7776, 11, 264, 6409, 786, 30], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 2, "seek": 0, "start": 12.0, "end": 14.0, "text": " Did you wake up with energy?", "tokens": [2589, 291, 6634, 493, 365, 2281, 30], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 3, "seek": 0, "start": 14.0, "end": 17.0, "text": " Or you are like...", "tokens": [1610, 291, 366, 411, 485], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 4, "seek": 0, "start": 17.0, "end": 19.0, "text": " We have energy, right?", "tokens": [492, 362, 2281, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 5, "seek": 0, "start": 19.0, "end": 21.0, "text": " Okay, that's good.", "tokens": [1033, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 6, "seek": 0, "start": 21.0, "end": 23.0, "text": " I'm Ida Pugja from Peru.", "tokens": [286, 478, 286, 2675, 430, 697, 2938, 490, 31571, 13], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 7, "seek": 0, "start": 23.0, "end": 27.0, "text": " I traveled to Europe for the first time for this year.", "tokens": [286, 16147, 281, 3315, 337, 264, 700, 565, 337, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.41439641908157704, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.5189971923828125}, {"id": 8, "seek": 2700, "start": 27.0, "end": 32.0, "text": " It's the first time also doing a talk in English.", "tokens": [467, 311, 264, 700, 565, 611, 884, 257, 751, 294, 3669, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 9, "seek": 2700, "start": 32.0, "end": 36.0, "text": " So if I make some mistake, I hope you can understand this.", "tokens": [407, 498, 286, 652, 512, 6146, 11, 286, 1454, 291, 393, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 10, "seek": 2700, "start": 36.0, "end": 38.0, "text": " And I am a technology evangelist at Percona.", "tokens": [400, 286, 669, 257, 2899, 24546, 468, 412, 3026, 1671, 64, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 11, "seek": 2700, "start": 38.0, "end": 40.0, "text": " I started six months ago.", "tokens": [286, 1409, 2309, 2493, 2057, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 12, "seek": 2700, "start": 40.0, "end": 43.0, "text": " It's a really great company focusing on databases.", "tokens": [467, 311, 257, 534, 869, 2237, 8416, 322, 22380, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 13, "seek": 2700, "start": 43.0, "end": 46.0, "text": " And if you want to follow me on Twitter or want to connect,", "tokens": [400, 498, 291, 528, 281, 1524, 385, 322, 5794, 420, 528, 281, 1745, 11], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 14, "seek": 2700, "start": 46.0, "end": 48.0, "text": " I will be really happy by LinkedIn.", "tokens": [286, 486, 312, 534, 2055, 538, 20657, 13], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 15, "seek": 2700, "start": 48.0, "end": 52.0, "text": " So I used to publish things about open source containers,", "tokens": [407, 286, 1143, 281, 11374, 721, 466, 1269, 4009, 17089, 11], "temperature": 0.0, "avg_logprob": -0.11842429751441592, "compression_ratio": 1.517786561264822, "no_speech_prob": 0.002507275901734829}, {"id": 16, "seek": 5200, "start": 52.0, "end": 55.0, "text": " Kubernetes.", "tokens": [23145, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 17, "seek": 5200, "start": 55.0, "end": 57.0, "text": " About me, I am from Peru.", "tokens": [7769, 385, 11, 286, 669, 490, 31571, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 18, "seek": 5200, "start": 57.0, "end": 61.0, "text": " I am a Google woman tech maker.", "tokens": [286, 669, 257, 3329, 3059, 7553, 17127, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 19, "seek": 5200, "start": 61.0, "end": 65.0, "text": " I was nominated as a docker captain the last year.", "tokens": [286, 390, 25159, 382, 257, 360, 9178, 14871, 264, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 20, "seek": 5200, "start": 65.0, "end": 68.0, "text": " And I am a container and database enthusiast.", "tokens": [400, 286, 669, 257, 10129, 293, 8149, 18076, 525, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 21, "seek": 5200, "start": 68.0, "end": 77.0, "text": " So this talk is about monitoring your database with open source tools.", "tokens": [407, 341, 751, 307, 466, 11028, 428, 8149, 365, 1269, 4009, 3873, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 22, "seek": 5200, "start": 77.0, "end": 79.0, "text": " We are going to focus in PMM.", "tokens": [492, 366, 516, 281, 1879, 294, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.15902250241010618, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.0037809545174241066}, {"id": 23, "seek": 7900, "start": 79.0, "end": 83.0, "text": " How many of you did you hear about PMM before?", "tokens": [1012, 867, 295, 291, 630, 291, 1568, 466, 12499, 44, 949, 30], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 24, "seek": 7900, "start": 83.0, "end": 86.0, "text": " Percona monitoring and management.", "tokens": [3026, 1671, 64, 11028, 293, 4592, 13], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 25, "seek": 7900, "start": 86.0, "end": 90.0, "text": " Okay, so it's something new for this room.", "tokens": [1033, 11, 370, 309, 311, 746, 777, 337, 341, 1808, 13], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 26, "seek": 7900, "start": 90.0, "end": 98.0, "text": " And this is a perspective of a beginner view.", "tokens": [400, 341, 307, 257, 4585, 295, 257, 22080, 1910, 13], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 27, "seek": 7900, "start": 98.0, "end": 100.0, "text": " So this is not something advanced.", "tokens": [407, 341, 307, 406, 746, 7339, 13], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 28, "seek": 7900, "start": 100.0, "end": 106.0, "text": " I will see this monitoring tool as a perspective,", "tokens": [286, 486, 536, 341, 11028, 2290, 382, 257, 4585, 11], "temperature": 0.0, "avg_logprob": -0.14016809597821303, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.004036161117255688}, {"id": 29, "seek": 10600, "start": 106.0, "end": 109.0, "text": " because I am learning about databases in the company", "tokens": [570, 286, 669, 2539, 466, 22380, 294, 264, 2237], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 30, "seek": 10600, "start": 109.0, "end": 111.0, "text": " where I am working, Percona.", "tokens": [689, 286, 669, 1364, 11, 3026, 1671, 64, 13], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 31, "seek": 10600, "start": 111.0, "end": 117.0, "text": " So we are going to evaluate why it's important the value of monitoring databases.", "tokens": [407, 321, 366, 516, 281, 13059, 983, 309, 311, 1021, 264, 2158, 295, 11028, 22380, 13], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 32, "seek": 10600, "start": 117.0, "end": 123.0, "text": " Also, we will see PMM, Percona monitoring and management,", "tokens": [2743, 11, 321, 486, 536, 12499, 44, 11, 3026, 1671, 64, 11028, 293, 4592, 11], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 33, "seek": 10600, "start": 123.0, "end": 127.0, "text": " and how we can effectively see the dashboards and graphs", "tokens": [293, 577, 321, 393, 8659, 536, 264, 8240, 17228, 293, 24877], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 34, "seek": 10600, "start": 127.0, "end": 133.0, "text": " that we can get to monitor and manage our databases.", "tokens": [300, 321, 393, 483, 281, 6002, 293, 3067, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.13084042493034811, "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.0004151859611738473}, {"id": 35, "seek": 13300, "start": 133.0, "end": 136.0, "text": " All this time I was working in a database company.", "tokens": [1057, 341, 565, 286, 390, 1364, 294, 257, 8149, 2237, 13], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 36, "seek": 13300, "start": 136.0, "end": 143.0, "text": " I asked myself, and I realized the importance is to monitor databases", "tokens": [286, 2351, 2059, 11, 293, 286, 5334, 264, 7379, 307, 281, 6002, 22380], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 37, "seek": 13300, "start": 143.0, "end": 147.0, "text": " because we can just have one database running for us.", "tokens": [570, 321, 393, 445, 362, 472, 8149, 2614, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 38, "seek": 13300, "start": 147.0, "end": 149.0, "text": " We can have several of them.", "tokens": [492, 393, 362, 2940, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 39, "seek": 13300, "start": 149.0, "end": 152.0, "text": " We can have it in cloud, in infra.", "tokens": [492, 393, 362, 309, 294, 4588, 11, 294, 23654, 13], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 40, "seek": 13300, "start": 152.0, "end": 157.0, "text": " So it's very important, and it's why we should ask", "tokens": [407, 309, 311, 588, 1021, 11, 293, 309, 311, 983, 321, 820, 1029], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 41, "seek": 13300, "start": 157.0, "end": 159.0, "text": " why we should care about databases,", "tokens": [983, 321, 820, 1127, 466, 22380, 11], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 42, "seek": 13300, "start": 159.0, "end": 162.0, "text": " why we should care about monitoring these databases.", "tokens": [983, 321, 820, 1127, 466, 11028, 613, 22380, 13], "temperature": 0.0, "avg_logprob": -0.10403312336314809, "compression_ratio": 1.8, "no_speech_prob": 0.0008489693864248693}, {"id": 43, "seek": 16200, "start": 162.0, "end": 165.0, "text": " And we can ask most questions like,", "tokens": [400, 321, 393, 1029, 881, 1651, 411, 11], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 44, "seek": 16200, "start": 165.0, "end": 167.0, "text": " is my database performing well?", "tokens": [307, 452, 8149, 10205, 731, 30], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 45, "seek": 16200, "start": 167.0, "end": 170.0, "text": " So as we start to work with databases,", "tokens": [407, 382, 321, 722, 281, 589, 365, 22380, 11], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 46, "seek": 16200, "start": 170.0, "end": 172.0, "text": " there are several queries that we make.", "tokens": [456, 366, 2940, 24109, 300, 321, 652, 13], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 47, "seek": 16200, "start": 172.0, "end": 177.0, "text": " So this queries maybe is not executing in the time that we expected.", "tokens": [407, 341, 24109, 1310, 307, 406, 32368, 294, 264, 565, 300, 321, 5176, 13], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 48, "seek": 16200, "start": 177.0, "end": 179.0, "text": " It could be taking more time,", "tokens": [467, 727, 312, 1940, 544, 565, 11], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 49, "seek": 16200, "start": 179.0, "end": 184.0, "text": " and it's going to have bottleneck in the time that we are executing.", "tokens": [293, 309, 311, 516, 281, 362, 44641, 547, 294, 264, 565, 300, 321, 366, 32368, 13], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 50, "seek": 16200, "start": 184.0, "end": 189.0, "text": " So we should care about this", "tokens": [407, 321, 820, 1127, 466, 341], "temperature": 0.0, "avg_logprob": -0.11123632348102072, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.000663818558678031}, {"id": 51, "seek": 18900, "start": 189.0, "end": 194.0, "text": " and know that there are metrics that we should detect", "tokens": [293, 458, 300, 456, 366, 16367, 300, 321, 820, 5531], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 52, "seek": 18900, "start": 194.0, "end": 198.0, "text": " the problem for the performance of our databases.", "tokens": [264, 1154, 337, 264, 3389, 295, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 53, "seek": 18900, "start": 198.0, "end": 202.0, "text": " Another question that could ask ourselves is,", "tokens": [3996, 1168, 300, 727, 1029, 4175, 307, 11], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 54, "seek": 18900, "start": 202.0, "end": 206.0, "text": " are databases available and accepting connections?", "tokens": [366, 22380, 2435, 293, 17391, 9271, 30], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 55, "seek": 18900, "start": 206.0, "end": 208.0, "text": " We can have several databases,", "tokens": [492, 393, 362, 2940, 22380, 11], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 56, "seek": 18900, "start": 208.0, "end": 210.0, "text": " and many connections could be made,", "tokens": [293, 867, 9271, 727, 312, 1027, 11], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 57, "seek": 18900, "start": 210.0, "end": 214.0, "text": " but if we don't put a limit in these connections,", "tokens": [457, 498, 321, 500, 380, 829, 257, 4948, 294, 613, 9271, 11], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 58, "seek": 18900, "start": 214.0, "end": 216.0, "text": " it could just crash.", "tokens": [309, 727, 445, 8252, 13], "temperature": 0.0, "avg_logprob": -0.1167258867403356, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.0005208876100368798}, {"id": 59, "seek": 21600, "start": 216.0, "end": 219.0, "text": " But if we put a limit also,", "tokens": [583, 498, 321, 829, 257, 4948, 611, 11], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 60, "seek": 21600, "start": 219.0, "end": 222.0, "text": " we should be aware when we are achieving", "tokens": [321, 820, 312, 3650, 562, 321, 366, 19626], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 61, "seek": 21600, "start": 222.0, "end": 225.0, "text": " or we are reaching that limit to increase", "tokens": [420, 321, 366, 9906, 300, 4948, 281, 3488], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 62, "seek": 21600, "start": 225.0, "end": 227.0, "text": " or maybe to stop the connection.", "tokens": [420, 1310, 281, 1590, 264, 4984, 13], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 63, "seek": 21600, "start": 227.0, "end": 234.0, "text": " But if we pass the limit,", "tokens": [583, 498, 321, 1320, 264, 4948, 11], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 64, "seek": 21600, "start": 234.0, "end": 237.0, "text": " so this is going to be a problem in our databases, right?", "tokens": [370, 341, 307, 516, 281, 312, 257, 1154, 294, 527, 22380, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 65, "seek": 21600, "start": 237.0, "end": 240.0, "text": " If this is an e-commerce company,", "tokens": [759, 341, 307, 364, 308, 12, 26926, 2237, 11], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 66, "seek": 21600, "start": 240.0, "end": 244.0, "text": " it can happen because the user is going to wait just two seconds", "tokens": [309, 393, 1051, 570, 264, 4195, 307, 516, 281, 1699, 445, 732, 3949], "temperature": 0.0, "avg_logprob": -0.10267741783805516, "compression_ratio": 1.63, "no_speech_prob": 0.001376712811179459}, {"id": 67, "seek": 24400, "start": 244.0, "end": 248.0, "text": " or maybe one second to go to the next page faster.", "tokens": [420, 1310, 472, 1150, 281, 352, 281, 264, 958, 3028, 4663, 13], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 68, "seek": 24400, "start": 248.0, "end": 251.0, "text": " But if no, if we did this in three seconds,", "tokens": [583, 498, 572, 11, 498, 321, 630, 341, 294, 1045, 3949, 11], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 69, "seek": 24400, "start": 251.0, "end": 254.0, "text": " five seconds that the user is going to go to another page", "tokens": [1732, 3949, 300, 264, 4195, 307, 516, 281, 352, 281, 1071, 3028], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 70, "seek": 24400, "start": 254.0, "end": 258.0, "text": " and we lost as a business.", "tokens": [293, 321, 2731, 382, 257, 1606, 13], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 71, "seek": 24400, "start": 258.0, "end": 260.0, "text": " Is my sister estable?", "tokens": [1119, 452, 4892, 871, 712, 30], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 72, "seek": 24400, "start": 260.0, "end": 270.0, "text": " We also monitor the infrastructure where our database is running,", "tokens": [492, 611, 6002, 264, 6896, 689, 527, 8149, 307, 2614, 11], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 73, "seek": 24400, "start": 270.0, "end": 271.0, "text": " not just the databases,", "tokens": [406, 445, 264, 22380, 11], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 74, "seek": 24400, "start": 271.0, "end": 273.0, "text": " because it is running over an infrastructure.", "tokens": [570, 309, 307, 2614, 670, 364, 6896, 13], "temperature": 0.0, "avg_logprob": -0.15304130512279468, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.0004912817967124283}, {"id": 75, "seek": 27300, "start": 273.0, "end": 276.0, "text": " We are using CPUs, memory, this,", "tokens": [492, 366, 1228, 13199, 82, 11, 4675, 11, 341, 11], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 76, "seek": 27300, "start": 276.0, "end": 282.0, "text": " and we are not able to provision these resources on time.", "tokens": [293, 321, 366, 406, 1075, 281, 17225, 613, 3593, 322, 565, 13], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 77, "seek": 27300, "start": 282.0, "end": 287.0, "text": " We will be in problem for our databases.", "tokens": [492, 486, 312, 294, 1154, 337, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 78, "seek": 27300, "start": 287.0, "end": 290.0, "text": " I am having avoidable time,", "tokens": [286, 669, 1419, 5042, 712, 565, 11], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 79, "seek": 27300, "start": 290.0, "end": 296.0, "text": " so if our hardware is not enough,", "tokens": [370, 498, 527, 8837, 307, 406, 1547, 11], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 80, "seek": 27300, "start": 296.0, "end": 300.0, "text": " so our application could crash", "tokens": [370, 527, 3861, 727, 8252], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 81, "seek": 27300, "start": 300.0, "end": 302.0, "text": " and we can have hardware failures", "tokens": [293, 321, 393, 362, 8837, 20774], "temperature": 0.0, "avg_logprob": -0.17268136429460082, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.00013930202112533152}, {"id": 82, "seek": 30200, "start": 302.0, "end": 305.0, "text": " or network outage.", "tokens": [420, 3209, 484, 609, 13], "temperature": 0.0, "avg_logprob": -0.19103205437753715, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.0008896348299458623}, {"id": 83, "seek": 30200, "start": 305.0, "end": 312.0, "text": " We should be aware of these metrics to avoid these problems.", "tokens": [492, 820, 312, 3650, 295, 613, 16367, 281, 5042, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.19103205437753715, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.0008896348299458623}, {"id": 84, "seek": 30200, "start": 312.0, "end": 319.0, "text": " So we also can have human errors or these crashes.", "tokens": [407, 321, 611, 393, 362, 1952, 13603, 420, 613, 28642, 13], "temperature": 0.0, "avg_logprob": -0.19103205437753715, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.0008896348299458623}, {"id": 85, "seek": 30200, "start": 319.0, "end": 328.0, "text": " So there are metrics that we can see to identify these before.", "tokens": [407, 456, 366, 16367, 300, 321, 393, 536, 281, 5876, 613, 949, 13], "temperature": 0.0, "avg_logprob": -0.19103205437753715, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.0008896348299458623}, {"id": 86, "seek": 32800, "start": 328.0, "end": 332.0, "text": " But we can just see what are the problems", "tokens": [583, 321, 393, 445, 536, 437, 366, 264, 2740], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 87, "seek": 32800, "start": 332.0, "end": 334.0, "text": " when we are having those problems.", "tokens": [562, 321, 366, 1419, 729, 2740, 13], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 88, "seek": 32800, "start": 334.0, "end": 336.0, "text": " We can also prevent these problems,", "tokens": [492, 393, 611, 4871, 613, 2740, 11], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 89, "seek": 32800, "start": 336.0, "end": 337.0, "text": " asking these questions.", "tokens": [3365, 613, 1651, 13], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 90, "seek": 32800, "start": 337.0, "end": 344.0, "text": " I am minimising performance issues that can impact my business.", "tokens": [286, 669, 4464, 3436, 3389, 2663, 300, 393, 2712, 452, 1606, 13], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 91, "seek": 32800, "start": 344.0, "end": 348.0, "text": " I am able to identify these issues before they happen", "tokens": [286, 669, 1075, 281, 5876, 613, 2663, 949, 436, 1051], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 92, "seek": 32800, "start": 348.0, "end": 351.0, "text": " because there is a way to prevent.", "tokens": [570, 456, 307, 257, 636, 281, 4871, 13], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 93, "seek": 32800, "start": 351.0, "end": 355.0, "text": " As a previous example, if we are reaching the limit,", "tokens": [1018, 257, 3894, 1365, 11, 498, 321, 366, 9906, 264, 4948, 11], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 94, "seek": 32800, "start": 355.0, "end": 357.0, "text": " we can see it before we are reaching it.", "tokens": [321, 393, 536, 309, 949, 321, 366, 9906, 309, 13], "temperature": 0.0, "avg_logprob": -0.13482179545392894, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0003177616454195231}, {"id": 95, "seek": 35700, "start": 357.0, "end": 360.0, "text": " So we can prevent provisioning more resources,", "tokens": [407, 321, 393, 4871, 17225, 278, 544, 3593, 11], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 96, "seek": 35700, "start": 360.0, "end": 364.0, "text": " maybe checking the query that we already saw", "tokens": [1310, 8568, 264, 14581, 300, 321, 1217, 1866], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 97, "seek": 35700, "start": 364.0, "end": 368.0, "text": " that is taking too much time executing.", "tokens": [300, 307, 1940, 886, 709, 565, 32368, 13], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 98, "seek": 35700, "start": 368.0, "end": 375.0, "text": " So there are ways we can prevent these problems.", "tokens": [407, 456, 366, 2098, 321, 393, 4871, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 99, "seek": 35700, "start": 375.0, "end": 378.0, "text": " Nowadays, there are challenges that we have", "tokens": [28908, 11, 456, 366, 4759, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 100, "seek": 35700, "start": 378.0, "end": 381.0, "text": " when we think to monitor databases.", "tokens": [562, 321, 519, 281, 6002, 22380, 13], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 101, "seek": 35700, "start": 381.0, "end": 383.0, "text": " For example, the data volume grow.", "tokens": [1171, 1365, 11, 264, 1412, 5523, 1852, 13], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 102, "seek": 35700, "start": 383.0, "end": 386.0, "text": " We don't talk about gigabytes now.", "tokens": [492, 500, 380, 751, 466, 42741, 586, 13], "temperature": 0.0, "avg_logprob": -0.132599637621925, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0005887252627871931}, {"id": 103, "seek": 38600, "start": 386.0, "end": 390.0, "text": " We are talking about terabytes or maybe more", "tokens": [492, 366, 1417, 466, 1796, 24538, 420, 1310, 544], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 104, "seek": 38600, "start": 390.0, "end": 396.0, "text": " because the database has a lot.", "tokens": [570, 264, 8149, 575, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 105, "seek": 38600, "start": 396.0, "end": 399.0, "text": " It's a challenge to monitor these days.", "tokens": [467, 311, 257, 3430, 281, 6002, 613, 1708, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 106, "seek": 38600, "start": 399.0, "end": 402.0, "text": " The complexity of the model databases.", "tokens": [440, 14024, 295, 264, 2316, 22380, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 107, "seek": 38600, "start": 402.0, "end": 407.0, "text": " Right now, it's SQL databases, not SQL databases.", "tokens": [1779, 586, 11, 309, 311, 19200, 22380, 11, 406, 19200, 22380, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 108, "seek": 38600, "start": 407.0, "end": 409.0, "text": " The database has different models.", "tokens": [440, 8149, 575, 819, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 109, "seek": 38600, "start": 409.0, "end": 413.0, "text": " It could run in different cloud providers.", "tokens": [467, 727, 1190, 294, 819, 4588, 11330, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 110, "seek": 38600, "start": 413.0, "end": 415.0, "text": " It could have different models.", "tokens": [467, 727, 362, 819, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11205325475553186, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.0006756306975148618}, {"id": 111, "seek": 41500, "start": 415.0, "end": 417.0, "text": " So the complexity just grow.", "tokens": [407, 264, 14024, 445, 1852, 13], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 112, "seek": 41500, "start": 417.0, "end": 420.0, "text": " Now it's different than before.", "tokens": [823, 309, 311, 819, 813, 949, 13], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 113, "seek": 41500, "start": 420.0, "end": 423.0, "text": " The downtime and data loss.", "tokens": [440, 49648, 293, 1412, 4470, 13], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 114, "seek": 41500, "start": 423.0, "end": 428.0, "text": " So it's one race that we should try to monitor to prevent it.", "tokens": [407, 309, 311, 472, 4569, 300, 321, 820, 853, 281, 6002, 281, 4871, 309, 13], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 115, "seek": 41500, "start": 428.0, "end": 430.0, "text": " Lack of visibility.", "tokens": [441, 501, 295, 19883, 13], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 116, "seek": 41500, "start": 430.0, "end": 437.0, "text": " If we don't have these things ready to be, to check it,", "tokens": [759, 321, 500, 380, 362, 613, 721, 1919, 281, 312, 11, 281, 1520, 309, 11], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 117, "seek": 41500, "start": 437.0, "end": 441.0, "text": " if we have to do another thing, like maybe create scripts,", "tokens": [498, 321, 362, 281, 360, 1071, 551, 11, 411, 1310, 1884, 23294, 11], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 118, "seek": 41500, "start": 441.0, "end": 443.0, "text": " Linux script or bash script to check it", "tokens": [18734, 5755, 420, 46183, 5755, 281, 1520, 309], "temperature": 0.0, "avg_logprob": -0.20829190490066365, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0006578334723599255}, {"id": 119, "seek": 44300, "start": 443.0, "end": 445.0, "text": " and to get this metrics,", "tokens": [293, 281, 483, 341, 16367, 11], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 120, "seek": 44300, "start": 445.0, "end": 447.0, "text": " we don't have that data on time.", "tokens": [321, 500, 380, 362, 300, 1412, 322, 565, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 121, "seek": 44300, "start": 447.0, "end": 449.0, "text": " We don't have that metrics on time.", "tokens": [492, 500, 380, 362, 300, 16367, 322, 565, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 122, "seek": 44300, "start": 449.0, "end": 451.0, "text": " But if we have that visibility,", "tokens": [583, 498, 321, 362, 300, 19883, 11], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 123, "seek": 44300, "start": 451.0, "end": 453.0, "text": " it's going to be easy for us to detect these problems", "tokens": [309, 311, 516, 281, 312, 1858, 337, 505, 281, 5531, 613, 2740], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 124, "seek": 44300, "start": 453.0, "end": 455.0, "text": " to monitor our databases.", "tokens": [281, 6002, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 125, "seek": 44300, "start": 455.0, "end": 458.0, "text": " Even better, if we have everything in one single dashboard", "tokens": [2754, 1101, 11, 498, 321, 362, 1203, 294, 472, 2167, 18342], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 126, "seek": 44300, "start": 458.0, "end": 460.0, "text": " where we can visualize it.", "tokens": [689, 321, 393, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 127, "seek": 44300, "start": 460.0, "end": 462.0, "text": " I learned fatigue.", "tokens": [286, 3264, 20574, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 128, "seek": 44300, "start": 462.0, "end": 466.0, "text": " We can monitor many things.", "tokens": [492, 393, 6002, 867, 721, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 129, "seek": 44300, "start": 466.0, "end": 470.0, "text": " But with this, we have different databases.", "tokens": [583, 365, 341, 11, 321, 362, 819, 22380, 13], "temperature": 0.0, "avg_logprob": -0.1451607898429588, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003550779656507075}, {"id": 130, "seek": 47000, "start": 470.0, "end": 474.0, "text": " They can have MySQL, they can have other kind of databases.", "tokens": [814, 393, 362, 1222, 39934, 11, 436, 393, 362, 661, 733, 295, 22380, 13], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 131, "seek": 47000, "start": 474.0, "end": 477.0, "text": " So it depends on the business, what we want to monitor,", "tokens": [407, 309, 5946, 322, 264, 1606, 11, 437, 321, 528, 281, 6002, 11], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 132, "seek": 47000, "start": 477.0, "end": 480.0, "text": " what metrics we are going to get to monitor.", "tokens": [437, 16367, 321, 366, 516, 281, 483, 281, 6002, 13], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 133, "seek": 47000, "start": 480.0, "end": 484.0, "text": " But if we don't know in all these things that we have for databases,", "tokens": [583, 498, 321, 500, 380, 458, 294, 439, 613, 721, 300, 321, 362, 337, 22380, 11], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 134, "seek": 47000, "start": 484.0, "end": 487.0, "text": " we are going to get a lot of things that maybe we don't need", "tokens": [321, 366, 516, 281, 483, 257, 688, 295, 721, 300, 1310, 321, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 135, "seek": 47000, "start": 487.0, "end": 491.0, "text": " and create alerts for maybe things that we really don't need.", "tokens": [293, 1884, 28061, 337, 1310, 721, 300, 321, 534, 500, 380, 643, 13], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 136, "seek": 47000, "start": 491.0, "end": 495.0, "text": " We are not focusing in that business exactly.", "tokens": [492, 366, 406, 8416, 294, 300, 1606, 2293, 13], "temperature": 0.0, "avg_logprob": -0.09235304706501511, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.0009744075941853225}, {"id": 137, "seek": 49500, "start": 495.0, "end": 500.0, "text": " We should try to focus to monitor, we should try to focus in the business.", "tokens": [492, 820, 853, 281, 1879, 281, 6002, 11, 321, 820, 853, 281, 1879, 294, 264, 1606, 13], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 138, "seek": 49500, "start": 500.0, "end": 504.0, "text": " What's the metrics that are important for us, for our business?", "tokens": [708, 311, 264, 16367, 300, 366, 1021, 337, 505, 11, 337, 527, 1606, 30], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 139, "seek": 49500, "start": 504.0, "end": 506.0, "text": " Integration with other tools.", "tokens": [47713, 365, 661, 3873, 13], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 140, "seek": 49500, "start": 506.0, "end": 509.0, "text": " So with that time, we can do continuous integration,", "tokens": [407, 365, 300, 565, 11, 321, 393, 360, 10957, 10980, 11], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 141, "seek": 49500, "start": 509.0, "end": 511.0, "text": " continuous delivery things.", "tokens": [10957, 8982, 721, 13], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 142, "seek": 49500, "start": 511.0, "end": 515.0, "text": " So it increases also the complexity to monitor our databases", "tokens": [407, 309, 8637, 611, 264, 14024, 281, 6002, 527, 22380], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 143, "seek": 49500, "start": 515.0, "end": 517.0, "text": " because this is not in a single place.", "tokens": [570, 341, 307, 406, 294, 257, 2167, 1081, 13], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 144, "seek": 49500, "start": 517.0, "end": 521.0, "text": " This go over a process that is also devolved.", "tokens": [639, 352, 670, 257, 1399, 300, 307, 611, 1905, 29110, 13], "temperature": 0.0, "avg_logprob": -0.14805401696099174, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0001746701600495726}, {"id": 145, "seek": 52100, "start": 521.0, "end": 528.0, "text": " These things are being automated.", "tokens": [1981, 721, 366, 885, 18473, 13], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 146, "seek": 52100, "start": 528.0, "end": 533.0, "text": " Now that we know why we should care about monitor databases,", "tokens": [823, 300, 321, 458, 983, 321, 820, 1127, 466, 6002, 22380, 11], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 147, "seek": 52100, "start": 533.0, "end": 537.0, "text": " we will talk about one of the solutions that could help us to monitor.", "tokens": [321, 486, 751, 466, 472, 295, 264, 6547, 300, 727, 854, 505, 281, 6002, 13], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 148, "seek": 52100, "start": 537.0, "end": 542.0, "text": " This is Percona monitoring and management, which is PMM.", "tokens": [639, 307, 3026, 1671, 64, 11028, 293, 4592, 11, 597, 307, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 149, "seek": 52100, "start": 542.0, "end": 545.0, "text": " This is an open source tools and free tools,", "tokens": [639, 307, 364, 1269, 4009, 3873, 293, 1737, 3873, 11], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 150, "seek": 52100, "start": 545.0, "end": 549.0, "text": " also based in other open source tools like MySQL.", "tokens": [611, 2361, 294, 661, 1269, 4009, 3873, 411, 1222, 39934, 13], "temperature": 0.0, "avg_logprob": -0.17023442431194027, "compression_ratio": 1.634020618556701, "no_speech_prob": 0.0004038516490254551}, {"id": 151, "seek": 54900, "start": 549.0, "end": 553.0, "text": " This led us to monitor databases like MySQL, MariaDB,", "tokens": [639, 4684, 505, 281, 6002, 22380, 411, 1222, 39934, 11, 12734, 27735, 11], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 152, "seek": 54900, "start": 553.0, "end": 556.0, "text": " PostgreSQL, AmongoDB, but not just that.", "tokens": [10223, 33248, 39934, 11, 2012, 25729, 27735, 11, 457, 406, 445, 300, 13], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 153, "seek": 54900, "start": 556.0, "end": 560.0, "text": " As I said before, this also led us to monitor the infrastructure", "tokens": [1018, 286, 848, 949, 11, 341, 611, 4684, 505, 281, 6002, 264, 6896], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 154, "seek": 54900, "start": 560.0, "end": 562.0, "text": " where our database is running.", "tokens": [689, 527, 8149, 307, 2614, 13], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 155, "seek": 54900, "start": 562.0, "end": 564.0, "text": " It's important to know about that.", "tokens": [467, 311, 1021, 281, 458, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 156, "seek": 54900, "start": 564.0, "end": 568.0, "text": " And it also helped us to performance our databases", "tokens": [400, 309, 611, 4254, 505, 281, 3389, 527, 22380], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 157, "seek": 54900, "start": 568.0, "end": 572.0, "text": " to simplify the management of these databases", "tokens": [281, 20460, 264, 4592, 295, 613, 22380], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 158, "seek": 54900, "start": 572.0, "end": 578.0, "text": " and we can exchange the security.", "tokens": [293, 321, 393, 7742, 264, 3825, 13], "temperature": 0.0, "avg_logprob": -0.14344163651162006, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0008984118467196822}, {"id": 159, "seek": 57800, "start": 578.0, "end": 583.0, "text": " Percona monitoring and management is built on top of other open source tools", "tokens": [3026, 1671, 64, 11028, 293, 4592, 307, 3094, 322, 1192, 295, 661, 1269, 4009, 3873], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 160, "seek": 57800, "start": 583.0, "end": 585.0, "text": " like Grafana.", "tokens": [411, 8985, 69, 2095, 13], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 161, "seek": 57800, "start": 585.0, "end": 588.0, "text": " I know many of you use Grafana, right?", "tokens": [286, 458, 867, 295, 291, 764, 8985, 69, 2095, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 162, "seek": 57800, "start": 588.0, "end": 590.0, "text": " Who use Grafana?", "tokens": [2102, 764, 8985, 69, 2095, 30], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 163, "seek": 57800, "start": 590.0, "end": 592.0, "text": " Okay, a lot.", "tokens": [1033, 11, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 164, "seek": 57800, "start": 592.0, "end": 596.0, "text": " And Victoria metrics also to storage this data,", "tokens": [400, 16656, 16367, 611, 281, 6725, 341, 1412, 11], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 165, "seek": 57800, "start": 596.0, "end": 599.0, "text": " that metrics we collect for a long time.", "tokens": [300, 16367, 321, 2500, 337, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 166, "seek": 57800, "start": 599.0, "end": 603.0, "text": " We are using clickhose to create these reports in real time", "tokens": [492, 366, 1228, 2052, 71, 541, 281, 1884, 613, 7122, 294, 957, 565], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 167, "seek": 57800, "start": 603.0, "end": 606.0, "text": " with all these metrics that we collect in the time.", "tokens": [365, 439, 613, 16367, 300, 321, 2500, 294, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.161859621320452, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0011485167779028416}, {"id": 168, "seek": 60600, "start": 606.0, "end": 610.0, "text": " We are using PostgreSQL to storage all the metadata", "tokens": [492, 366, 1228, 10223, 33248, 39934, 281, 6725, 439, 264, 26603], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 169, "seek": 60600, "start": 610.0, "end": 613.0, "text": " and all these metrics for databases,", "tokens": [293, 439, 613, 16367, 337, 22380, 11], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 170, "seek": 60600, "start": 613.0, "end": 616.0, "text": " all the important data that we have in PMM.", "tokens": [439, 264, 1021, 1412, 300, 321, 362, 294, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 171, "seek": 60600, "start": 616.0, "end": 621.0, "text": " And everything that we visualize is saved in this database.", "tokens": [400, 1203, 300, 321, 23273, 307, 6624, 294, 341, 8149, 13], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 172, "seek": 60600, "start": 621.0, "end": 624.0, "text": " And we use Docker to install PMM.", "tokens": [400, 321, 764, 33772, 281, 3625, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 173, "seek": 60600, "start": 624.0, "end": 628.0, "text": " We can containerize the installation of PMM, the client and the server,", "tokens": [492, 393, 10129, 1125, 264, 13260, 295, 12499, 44, 11, 264, 6423, 293, 264, 7154, 11], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 174, "seek": 60600, "start": 628.0, "end": 633.0, "text": " and use it in different platforms.", "tokens": [293, 764, 309, 294, 819, 9473, 13], "temperature": 0.0, "avg_logprob": -0.0969446138902144, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00014162521983962506}, {"id": 175, "seek": 63300, "start": 633.0, "end": 641.0, "text": " We also use Kubernetes operators for scaling our databases.", "tokens": [492, 611, 764, 23145, 19077, 337, 21589, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.12426263173421224, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0002907953748945147}, {"id": 176, "seek": 63300, "start": 641.0, "end": 645.0, "text": " There are three levels of deep when we talk about PMM interface.", "tokens": [821, 366, 1045, 4358, 295, 2452, 562, 321, 751, 466, 12499, 44, 9226, 13], "temperature": 0.0, "avg_logprob": -0.12426263173421224, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0002907953748945147}, {"id": 177, "seek": 63300, "start": 645.0, "end": 649.0, "text": " This is the big one, which is a dashboard that we all know,", "tokens": [639, 307, 264, 955, 472, 11, 597, 307, 257, 18342, 300, 321, 439, 458, 11], "temperature": 0.0, "avg_logprob": -0.12426263173421224, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0002907953748945147}, {"id": 178, "seek": 63300, "start": 649.0, "end": 654.0, "text": " but we can go deeper and see the graphs,", "tokens": [457, 321, 393, 352, 7731, 293, 536, 264, 24877, 11], "temperature": 0.0, "avg_logprob": -0.12426263173421224, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0002907953748945147}, {"id": 179, "seek": 63300, "start": 654.0, "end": 661.0, "text": " that is a graphical representation of the metrics in long period of time.", "tokens": [300, 307, 257, 35942, 10290, 295, 264, 16367, 294, 938, 2896, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.12426263173421224, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0002907953748945147}, {"id": 180, "seek": 66100, "start": 661.0, "end": 667.0, "text": " And we have the metrics also, which is a countable number", "tokens": [400, 321, 362, 264, 16367, 611, 11, 597, 307, 257, 1207, 712, 1230], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 181, "seek": 66100, "start": 667.0, "end": 672.0, "text": " that represents some value, some important value", "tokens": [300, 8855, 512, 2158, 11, 512, 1021, 2158], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 182, "seek": 66100, "start": 672.0, "end": 678.0, "text": " of our infrastructure of our database.", "tokens": [295, 527, 6896, 295, 527, 8149, 13], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 183, "seek": 66100, "start": 678.0, "end": 682.0, "text": " Okay, as I tell you before, what we want to monitor", "tokens": [1033, 11, 382, 286, 980, 291, 949, 11, 437, 321, 528, 281, 6002], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 184, "seek": 66100, "start": 682.0, "end": 686.0, "text": " is going to depend of our business.", "tokens": [307, 516, 281, 5672, 295, 527, 1606, 13], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 185, "seek": 66100, "start": 686.0, "end": 690.0, "text": " We are not going to monitor the same metrics as another business.", "tokens": [492, 366, 406, 516, 281, 6002, 264, 912, 16367, 382, 1071, 1606, 13], "temperature": 0.0, "avg_logprob": -0.10094000791248522, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.00036697337054647505}, {"id": 186, "seek": 69000, "start": 690.0, "end": 692.0, "text": " It depends a lot on that.", "tokens": [467, 5946, 257, 688, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 187, "seek": 69000, "start": 692.0, "end": 695.0, "text": " And we also should aware of the alerts that we create", "tokens": [400, 321, 611, 820, 3650, 295, 264, 28061, 300, 321, 1884], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 188, "seek": 69000, "start": 695.0, "end": 698.0, "text": " that should be focused on what we do as a business", "tokens": [300, 820, 312, 5178, 322, 437, 321, 360, 382, 257, 1606], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 189, "seek": 69000, "start": 698.0, "end": 702.0, "text": " and create this alertness and notifications", "tokens": [293, 1884, 341, 9615, 1287, 293, 13426], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 190, "seek": 69000, "start": 702.0, "end": 707.0, "text": " that could be notified when we need it.", "tokens": [300, 727, 312, 18013, 562, 321, 643, 309, 13], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 191, "seek": 69000, "start": 707.0, "end": 709.0, "text": " So it could be integrated with Slack,", "tokens": [407, 309, 727, 312, 10919, 365, 37211, 11], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 192, "seek": 69000, "start": 709.0, "end": 712.0, "text": " with many other tools that we have in the hand", "tokens": [365, 867, 661, 3873, 300, 321, 362, 294, 264, 1011], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 193, "seek": 69000, "start": 712.0, "end": 719.0, "text": " to know when the problem is happening exactly.", "tokens": [281, 458, 562, 264, 1154, 307, 2737, 2293, 13], "temperature": 0.0, "avg_logprob": -0.11952284706963433, "compression_ratio": 1.73, "no_speech_prob": 0.00033745597465895116}, {"id": 194, "seek": 71900, "start": 719.0, "end": 725.0, "text": " Some important metrics, some of that that we can check with PMM,", "tokens": [2188, 1021, 16367, 11, 512, 295, 300, 300, 321, 393, 1520, 365, 12499, 44, 11], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 195, "seek": 71900, "start": 725.0, "end": 730.0, "text": " is query performance, high CPU, high memory usage,", "tokens": [307, 14581, 3389, 11, 1090, 13199, 11, 1090, 4675, 14924, 11], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 196, "seek": 71900, "start": 730.0, "end": 734.0, "text": " and this high disk part of the infrastructure,", "tokens": [293, 341, 1090, 12355, 644, 295, 264, 6896, 11], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 197, "seek": 71900, "start": 734.0, "end": 737.0, "text": " the amount of user connections.", "tokens": [264, 2372, 295, 4195, 9271, 13], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 198, "seek": 71900, "start": 737.0, "end": 739.0, "text": " We can know when the data grows", "tokens": [492, 393, 458, 562, 264, 1412, 13156], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 199, "seek": 71900, "start": 739.0, "end": 744.0, "text": " and other kind of metrics that we can have.", "tokens": [293, 661, 733, 295, 16367, 300, 321, 393, 362, 13], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 200, "seek": 71900, "start": 744.0, "end": 748.0, "text": " Could somebody tell me what other kind of metrics we can check with PMM?", "tokens": [7497, 2618, 980, 385, 437, 661, 733, 295, 16367, 321, 393, 1520, 365, 12499, 44, 30], "temperature": 0.0, "avg_logprob": -0.1758655760023329, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.00035019699134863913}, {"id": 201, "seek": 74800, "start": 748.0, "end": 753.0, "text": " With some monitoring tool?", "tokens": [2022, 512, 11028, 2290, 30], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 202, "seek": 74800, "start": 753.0, "end": 759.0, "text": " A part of the infrastructure or...", "tokens": [316, 644, 295, 264, 6896, 420, 485], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 203, "seek": 74800, "start": 759.0, "end": 762.0, "text": " Okay, we'll check.", "tokens": [1033, 11, 321, 603, 1520, 13], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 204, "seek": 74800, "start": 762.0, "end": 764.0, "text": " If we see the long query response times,", "tokens": [759, 321, 536, 264, 938, 14581, 4134, 1413, 11], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 205, "seek": 74800, "start": 764.0, "end": 768.0, "text": " as I say, some queries could take some time,", "tokens": [382, 286, 584, 11, 512, 24109, 727, 747, 512, 565, 11], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 206, "seek": 74800, "start": 768.0, "end": 772.0, "text": " PMM has a very good dashboard, which is this, is Khan,", "tokens": [12499, 44, 575, 257, 588, 665, 18342, 11, 597, 307, 341, 11, 307, 18136, 11], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 207, "seek": 74800, "start": 772.0, "end": 775.0, "text": " query analytics dashboard,", "tokens": [14581, 15370, 18342, 11], "temperature": 0.0, "avg_logprob": -0.20188889438158844, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.000148962892126292}, {"id": 208, "seek": 77500, "start": 775.0, "end": 781.0, "text": " where we can see for a specific...", "tokens": [689, 321, 393, 536, 337, 257, 2685, 485], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 209, "seek": 77500, "start": 781.0, "end": 787.0, "text": " We can see all the queries for our databases.", "tokens": [492, 393, 536, 439, 264, 24109, 337, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 210, "seek": 77500, "start": 787.0, "end": 791.0, "text": " In this case, I'm seeing all the 10 top queries", "tokens": [682, 341, 1389, 11, 286, 478, 2577, 439, 264, 1266, 1192, 24109], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 211, "seek": 77500, "start": 791.0, "end": 793.0, "text": " that is running in our databases,", "tokens": [300, 307, 2614, 294, 527, 22380, 11], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 212, "seek": 77500, "start": 793.0, "end": 796.0, "text": " but also we have an option to check it here", "tokens": [457, 611, 321, 362, 364, 3614, 281, 1520, 309, 510], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 213, "seek": 77500, "start": 796.0, "end": 801.0, "text": " if we want to just check for MySQL, for Postgres, or MongoDB,", "tokens": [498, 321, 528, 281, 445, 1520, 337, 1222, 39934, 11, 337, 10223, 45189, 11, 420, 48380, 27735, 11], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 214, "seek": 77500, "start": 801.0, "end": 804.0, "text": " and we will see the amount of queries per second", "tokens": [293, 321, 486, 536, 264, 2372, 295, 24109, 680, 1150], "temperature": 0.0, "avg_logprob": -0.12334923167805095, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.0002439044910715893}, {"id": 215, "seek": 80400, "start": 804.0, "end": 808.0, "text": " for example that we are running and how time is taking.", "tokens": [337, 1365, 300, 321, 366, 2614, 293, 577, 565, 307, 1940, 13], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 216, "seek": 80400, "start": 808.0, "end": 812.0, "text": " So if we open this dashboard when we are working in databases,", "tokens": [407, 498, 321, 1269, 341, 18342, 562, 321, 366, 1364, 294, 22380, 11], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 217, "seek": 80400, "start": 812.0, "end": 815.0, "text": " the first thing that we are going to see is,", "tokens": [264, 700, 551, 300, 321, 366, 516, 281, 536, 307, 11], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 218, "seek": 80400, "start": 815.0, "end": 817.0, "text": " okay, this query is taking too much time.", "tokens": [1392, 11, 341, 14581, 307, 1940, 886, 709, 565, 13], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 219, "seek": 80400, "start": 817.0, "end": 821.0, "text": " In this case, no, but we have a query that is taking too much time,", "tokens": [682, 341, 1389, 11, 572, 11, 457, 321, 362, 257, 14581, 300, 307, 1940, 886, 709, 565, 11], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 220, "seek": 80400, "start": 821.0, "end": 823.0, "text": " or it's running a lot of queries per second.", "tokens": [420, 309, 311, 2614, 257, 688, 295, 24109, 680, 1150, 13], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 221, "seek": 80400, "start": 823.0, "end": 825.0, "text": " We can see the first one,", "tokens": [492, 393, 536, 264, 700, 472, 11], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 222, "seek": 80400, "start": 825.0, "end": 830.0, "text": " and we can start troubleshooting from that point.", "tokens": [293, 321, 393, 722, 15379, 47011, 490, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.12200949408791283, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0006574987783096731}, {"id": 223, "seek": 83000, "start": 830.0, "end": 834.0, "text": " The high CPU utilization is part of the infrastructure.", "tokens": [440, 1090, 13199, 37074, 307, 644, 295, 264, 6896, 13], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 224, "seek": 83000, "start": 834.0, "end": 838.0, "text": " Also, it's important to know how is this going.", "tokens": [2743, 11, 309, 311, 1021, 281, 458, 577, 307, 341, 516, 13], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 225, "seek": 83000, "start": 838.0, "end": 840.0, "text": " For example, this dashboard in PMA,", "tokens": [1171, 1365, 11, 341, 18342, 294, 430, 9998, 11], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 226, "seek": 83000, "start": 840.0, "end": 842.0, "text": " in Percona Monitoring and Management,", "tokens": [294, 3026, 1671, 64, 33799, 278, 293, 14781, 11], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 227, "seek": 83000, "start": 842.0, "end": 845.0, "text": " we can see for a specific note,", "tokens": [321, 393, 536, 337, 257, 2685, 3637, 11], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 228, "seek": 83000, "start": 845.0, "end": 850.0, "text": " we can check a note because we can have different notes running", "tokens": [321, 393, 1520, 257, 3637, 570, 321, 393, 362, 819, 5570, 2614], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 229, "seek": 83000, "start": 850.0, "end": 854.0, "text": " in our infrastructure,", "tokens": [294, 527, 6896, 11], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 230, "seek": 83000, "start": 854.0, "end": 856.0, "text": " and in this case, for example,", "tokens": [293, 294, 341, 1389, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.12793143590291342, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.00046991801355034113}, {"id": 231, "seek": 85600, "start": 856.0, "end": 861.0, "text": " we have 25% of our disk that is using.", "tokens": [321, 362, 3552, 4, 295, 527, 12355, 300, 307, 1228, 13], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 232, "seek": 85600, "start": 861.0, "end": 866.0, "text": " This may be not a good example because I checked last 12 hours,", "tokens": [639, 815, 312, 406, 257, 665, 1365, 570, 286, 10033, 1036, 2272, 2496, 11], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 233, "seek": 85600, "start": 866.0, "end": 872.0, "text": " but we can check our disk usage during six months or more,", "tokens": [457, 321, 393, 1520, 527, 12355, 14924, 1830, 2309, 2493, 420, 544, 11], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 234, "seek": 85600, "start": 872.0, "end": 876.0, "text": " and then it's when we can see and take decisions.", "tokens": [293, 550, 309, 311, 562, 321, 393, 536, 293, 747, 5327, 13], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 235, "seek": 85600, "start": 876.0, "end": 878.0, "text": " Let's see if this is like six months.", "tokens": [961, 311, 536, 498, 341, 307, 411, 2309, 2493, 13], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 236, "seek": 85600, "start": 878.0, "end": 881.0, "text": " We are using just 25% of our disk.", "tokens": [492, 366, 1228, 445, 3552, 4, 295, 527, 12355, 13], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 237, "seek": 85600, "start": 881.0, "end": 884.0, "text": " It could be a problem because we have a lot of infrastructure", "tokens": [467, 727, 312, 257, 1154, 570, 321, 362, 257, 688, 295, 6896], "temperature": 0.0, "avg_logprob": -0.10664143854258012, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.00016730683273635805}, {"id": 238, "seek": 88400, "start": 884.0, "end": 887.0, "text": " that we are not using and we are wasting money", "tokens": [300, 321, 366, 406, 1228, 293, 321, 366, 20457, 1460], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 239, "seek": 88400, "start": 887.0, "end": 892.0, "text": " because of this space that we are not using in six months.", "tokens": [570, 295, 341, 1901, 300, 321, 366, 406, 1228, 294, 2309, 2493, 13], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 240, "seek": 88400, "start": 892.0, "end": 897.0, "text": " We can reduce our CPU and save money.", "tokens": [492, 393, 5407, 527, 13199, 293, 3155, 1460, 13], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 241, "seek": 88400, "start": 897.0, "end": 905.0, "text": " High memory usage is this dashboard where we can see the amount", "tokens": [5229, 4675, 14924, 307, 341, 18342, 689, 321, 393, 536, 264, 2372], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 242, "seek": 88400, "start": 905.0, "end": 909.0, "text": " of memory that I have for my databases", "tokens": [295, 4675, 300, 286, 362, 337, 452, 22380], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 243, "seek": 88400, "start": 909.0, "end": 912.0, "text": " and also can see what is using for Kakache,", "tokens": [293, 611, 393, 536, 437, 307, 1228, 337, 36775, 6000, 11], "temperature": 0.0, "avg_logprob": -0.1064983219295353, "compression_ratio": 1.6292134831460674, "no_speech_prob": 0.00026931092725135386}, {"id": 244, "seek": 91200, "start": 912.0, "end": 918.0, "text": " what has been using, what is going to be ready to be free,", "tokens": [437, 575, 668, 1228, 11, 437, 307, 516, 281, 312, 1919, 281, 312, 1737, 11], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 245, "seek": 91200, "start": 918.0, "end": 922.0, "text": " and this is good because we can also see", "tokens": [293, 341, 307, 665, 570, 321, 393, 611, 536], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 246, "seek": 91200, "start": 922.0, "end": 925.0, "text": " when we are reaching the limit of the memory", "tokens": [562, 321, 366, 9906, 264, 4948, 295, 264, 4675], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 247, "seek": 91200, "start": 925.0, "end": 931.0, "text": " and we can take actions to provisioning another disk.", "tokens": [293, 321, 393, 747, 5909, 281, 17225, 278, 1071, 12355, 13], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 248, "seek": 91200, "start": 931.0, "end": 935.0, "text": " We can say that this is very easy when we are working in cloud", "tokens": [492, 393, 584, 300, 341, 307, 588, 1858, 562, 321, 366, 1364, 294, 4588], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 249, "seek": 91200, "start": 935.0, "end": 937.0, "text": " because just we click in a button and say,", "tokens": [570, 445, 321, 2052, 294, 257, 2960, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 250, "seek": 91200, "start": 937.0, "end": 941.0, "text": " hey, provision another disk or increment the memory,", "tokens": [4177, 11, 17225, 1071, 12355, 420, 26200, 264, 4675, 11], "temperature": 0.0, "avg_logprob": -0.12896391685972822, "compression_ratio": 1.7939698492462313, "no_speech_prob": 0.0006490513915196061}, {"id": 251, "seek": 94100, "start": 941.0, "end": 948.0, "text": " but if we are working in infra, maybe in a private cloud,", "tokens": [457, 498, 321, 366, 1364, 294, 23654, 11, 1310, 294, 257, 4551, 4588, 11], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 252, "seek": 94100, "start": 948.0, "end": 951.0, "text": " this is hard because we have to prepare the logistics", "tokens": [341, 307, 1152, 570, 321, 362, 281, 5940, 264, 27420], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 253, "seek": 94100, "start": 951.0, "end": 955.0, "text": " to get another disk, another memory is going to take time,", "tokens": [281, 483, 1071, 12355, 11, 1071, 4675, 307, 516, 281, 747, 565, 11], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 254, "seek": 94100, "start": 955.0, "end": 962.0, "text": " and have this kind of visualization helps.", "tokens": [293, 362, 341, 733, 295, 25801, 3665, 13], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 255, "seek": 94100, "start": 962.0, "end": 966.0, "text": " The amount of input or output that we make in our disk,", "tokens": [440, 2372, 295, 4846, 420, 5598, 300, 321, 652, 294, 527, 12355, 11], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 256, "seek": 94100, "start": 966.0, "end": 970.0, "text": " you also can check it in this graph.", "tokens": [291, 611, 393, 1520, 309, 294, 341, 4295, 13], "temperature": 0.0, "avg_logprob": -0.15261027253704307, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000606886635068804}, {"id": 257, "seek": 97000, "start": 970.0, "end": 973.0, "text": " We can see that your latency here,", "tokens": [492, 393, 536, 300, 428, 27043, 510, 11], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 258, "seek": 97000, "start": 973.0, "end": 977.0, "text": " in this case, is stable,", "tokens": [294, 341, 1389, 11, 307, 8351, 11], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 259, "seek": 97000, "start": 977.0, "end": 985.0, "text": " but we can have peaks to see where we are detecting these problems.", "tokens": [457, 321, 393, 362, 26897, 281, 536, 689, 321, 366, 40237, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 260, "seek": 97000, "start": 985.0, "end": 987.0, "text": " User connections, as I say,", "tokens": [32127, 9271, 11, 382, 286, 584, 11], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 261, "seek": 97000, "start": 987.0, "end": 991.0, "text": " it helps to monitor the number of active database connections", "tokens": [309, 3665, 281, 6002, 264, 1230, 295, 4967, 8149, 9271], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 262, "seek": 97000, "start": 991.0, "end": 996.0, "text": " and size it appropriately,", "tokens": [293, 2744, 309, 23505, 11], "temperature": 0.0, "avg_logprob": -0.22622668926532452, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00017759799084160477}, {"id": 263, "seek": 99600, "start": 996.0, "end": 1001.0, "text": " and also put limits in our connections for our databases.", "tokens": [293, 611, 829, 10406, 294, 527, 9271, 337, 527, 22380, 13], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 264, "seek": 99600, "start": 1001.0, "end": 1005.0, "text": " We have, in this case, MySQL connections.", "tokens": [492, 362, 11, 294, 341, 1389, 11, 1222, 39934, 9271, 13], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 265, "seek": 99600, "start": 1005.0, "end": 1009.0, "text": " We can see that you are for other databases too,", "tokens": [492, 393, 536, 300, 291, 366, 337, 661, 22380, 886, 11], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 266, "seek": 99600, "start": 1009.0, "end": 1011.0, "text": " but in this case, it's also stable.", "tokens": [457, 294, 341, 1389, 11, 309, 311, 611, 8351, 13], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 267, "seek": 99600, "start": 1011.0, "end": 1014.0, "text": " We are going to have peaks when we can see", "tokens": [492, 366, 516, 281, 362, 26897, 562, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 268, "seek": 99600, "start": 1014.0, "end": 1018.0, "text": " that we are working with a lot of transactions on our databases,", "tokens": [300, 321, 366, 1364, 365, 257, 688, 295, 16856, 322, 527, 22380, 11], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 269, "seek": 99600, "start": 1018.0, "end": 1024.0, "text": " and we can take actions with that.", "tokens": [293, 321, 393, 747, 5909, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1496715545654297, "compression_ratio": 1.8066298342541436, "no_speech_prob": 0.0003961971087846905}, {"id": 270, "seek": 102400, "start": 1024.0, "end": 1029.0, "text": " The maximum of connections allowed is 151.", "tokens": [440, 6674, 295, 9271, 4350, 307, 2119, 16, 13], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 271, "seek": 102400, "start": 1029.0, "end": 1031.0, "text": " We are in 150.", "tokens": [492, 366, 294, 8451, 13], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 272, "seek": 102400, "start": 1031.0, "end": 1036.0, "text": " If this is going to be 151, this is going to alert us.", "tokens": [759, 341, 307, 516, 281, 312, 2119, 16, 11, 341, 307, 516, 281, 9615, 505, 13], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 273, "seek": 102400, "start": 1036.0, "end": 1042.0, "text": " You have to check this.", "tokens": [509, 362, 281, 1520, 341, 13], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 274, "seek": 102400, "start": 1042.0, "end": 1044.0, "text": " The data grow also.", "tokens": [440, 1412, 1852, 611, 13], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 275, "seek": 102400, "start": 1044.0, "end": 1047.0, "text": " We can see a dashboard where we can...", "tokens": [492, 393, 536, 257, 18342, 689, 321, 393, 485], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 276, "seek": 102400, "start": 1047.0, "end": 1051.0, "text": " Where you can see where our data is a lot", "tokens": [2305, 291, 393, 536, 689, 527, 1412, 307, 257, 688], "temperature": 0.0, "avg_logprob": -0.17978656919378982, "compression_ratio": 1.5, "no_speech_prob": 0.0006476407288573682}, {"id": 277, "seek": 105100, "start": 1051.0, "end": 1054.0, "text": " when we are inserting a lot of data in our databases,", "tokens": [562, 321, 366, 46567, 257, 688, 295, 1412, 294, 527, 22380, 11], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 278, "seek": 105100, "start": 1054.0, "end": 1057.0, "text": " or we are just removing things.", "tokens": [420, 321, 366, 445, 12720, 721, 13], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 279, "seek": 105100, "start": 1057.0, "end": 1062.0, "text": " In this case, it's going to show when my databases start,", "tokens": [682, 341, 1389, 11, 309, 311, 516, 281, 855, 562, 452, 22380, 722, 11], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 280, "seek": 105100, "start": 1062.0, "end": 1067.0, "text": " and it's not like too good to see it here,", "tokens": [293, 309, 311, 406, 411, 886, 665, 281, 536, 309, 510, 11], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 281, "seek": 105100, "start": 1067.0, "end": 1072.0, "text": " but if I have time, yes, I still have time.", "tokens": [457, 498, 286, 362, 565, 11, 2086, 11, 286, 920, 362, 565, 13], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 282, "seek": 105100, "start": 1072.0, "end": 1075.0, "text": " I still have time, right?", "tokens": [286, 920, 362, 565, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 283, "seek": 105100, "start": 1075.0, "end": 1078.0, "text": " To show something, to show the dashboard.", "tokens": [1407, 855, 746, 11, 281, 855, 264, 18342, 13], "temperature": 0.0, "avg_logprob": -0.127508364783393, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.0005002692341804504}, {"id": 284, "seek": 107800, "start": 1078.0, "end": 1082.0, "text": " I have time.", "tokens": [286, 362, 565, 13], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 285, "seek": 107800, "start": 1082.0, "end": 1086.0, "text": " If you want to try it right now,", "tokens": [759, 291, 528, 281, 853, 309, 558, 586, 11], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 286, "seek": 107800, "start": 1086.0, "end": 1089.0, "text": " we can check this PMM demo per con graph.", "tokens": [321, 393, 1520, 341, 12499, 44, 10723, 680, 416, 4295, 13], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 287, "seek": 107800, "start": 1089.0, "end": 1090.0, "text": " You can enter.", "tokens": [509, 393, 3242, 13], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 288, "seek": 107800, "start": 1090.0, "end": 1093.0, "text": " Right now, we are going to check the dashboard,", "tokens": [1779, 586, 11, 321, 366, 516, 281, 1520, 264, 18342, 11], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 289, "seek": 107800, "start": 1093.0, "end": 1100.0, "text": " but what we learned now, it was some aspects", "tokens": [457, 437, 321, 3264, 586, 11, 309, 390, 512, 7270], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 290, "seek": 107800, "start": 1100.0, "end": 1104.0, "text": " that let me think what we should keep away", "tokens": [300, 718, 385, 519, 437, 321, 820, 1066, 1314], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 291, "seek": 107800, "start": 1104.0, "end": 1106.0, "text": " and monitoring our databases,", "tokens": [293, 11028, 527, 22380, 11], "temperature": 0.0, "avg_logprob": -0.21192381117078993, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.0005737962783314288}, {"id": 292, "seek": 110600, "start": 1106.0, "end": 1109.0, "text": " and also how to explore PMM,", "tokens": [293, 611, 577, 281, 6839, 12499, 44, 11], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 293, "seek": 110600, "start": 1109.0, "end": 1111.0, "text": " which is an open source tool, is available there.", "tokens": [597, 307, 364, 1269, 4009, 2290, 11, 307, 2435, 456, 13], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 294, "seek": 110600, "start": 1111.0, "end": 1114.0, "text": " You have to double down and start to check it and explore it,", "tokens": [509, 362, 281, 3834, 760, 293, 722, 281, 1520, 309, 293, 6839, 309, 11], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 295, "seek": 110600, "start": 1114.0, "end": 1117.0, "text": " and it's easy to visualize things,", "tokens": [293, 309, 311, 1858, 281, 23273, 721, 11], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 296, "seek": 110600, "start": 1117.0, "end": 1122.0, "text": " so we are going to check now PMM.", "tokens": [370, 321, 366, 516, 281, 1520, 586, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 297, "seek": 110600, "start": 1122.0, "end": 1127.0, "text": " You can also enter to this link, so it's free to experiment.", "tokens": [509, 393, 611, 3242, 281, 341, 2113, 11, 370, 309, 311, 1737, 281, 5120, 13], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 298, "seek": 110600, "start": 1127.0, "end": 1131.0, "text": " Let's see if this is going to work.", "tokens": [961, 311, 536, 498, 341, 307, 516, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 299, "seek": 110600, "start": 1131.0, "end": 1135.0, "text": " Yes, this is the dashboard that we have.", "tokens": [1079, 11, 341, 307, 264, 18342, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.1323757813527034, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.0006445064209401608}, {"id": 300, "seek": 113500, "start": 1135.0, "end": 1139.0, "text": " We have several nodes that we can choose here.", "tokens": [492, 362, 2940, 13891, 300, 321, 393, 2826, 510, 13], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 301, "seek": 113500, "start": 1139.0, "end": 1142.0, "text": " A lot of them, so it depends on the database.", "tokens": [316, 688, 295, 552, 11, 370, 309, 5946, 322, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 302, "seek": 113500, "start": 1142.0, "end": 1146.0, "text": " We have nodes of MongoDB, MySQL, Postgres,", "tokens": [492, 362, 13891, 295, 48380, 27735, 11, 1222, 39934, 11, 10223, 45189, 11], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 303, "seek": 113500, "start": 1146.0, "end": 1149.0, "text": " and MySQL proxy also.", "tokens": [293, 1222, 39934, 29690, 611, 13], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 304, "seek": 113500, "start": 1149.0, "end": 1152.0, "text": " We can check the details for a specific time.", "tokens": [492, 393, 1520, 264, 4365, 337, 257, 2685, 565, 13], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 305, "seek": 113500, "start": 1152.0, "end": 1157.0, "text": " 12 hours is not enough, maybe, for some things,", "tokens": [2272, 2496, 307, 406, 1547, 11, 1310, 11, 337, 512, 721, 11], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 306, "seek": 113500, "start": 1157.0, "end": 1161.0, "text": " but maybe six months, three months,", "tokens": [457, 1310, 2309, 2493, 11, 1045, 2493, 11], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 307, "seek": 113500, "start": 1161.0, "end": 1164.0, "text": " we have a lot of things here.", "tokens": [321, 362, 257, 688, 295, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.1131497904197457, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0006893320824019611}, {"id": 308, "seek": 116400, "start": 1164.0, "end": 1168.0, "text": " So we also can check things about the system operator.", "tokens": [407, 321, 611, 393, 1520, 721, 466, 264, 1185, 12973, 13], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 309, "seek": 116400, "start": 1168.0, "end": 1170.0, "text": " I don't have access right now to see that,", "tokens": [286, 500, 380, 362, 2105, 558, 586, 281, 536, 300, 11], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 310, "seek": 116400, "start": 1170.0, "end": 1177.0, "text": " but we can also register alertings for this.", "tokens": [457, 321, 393, 611, 7280, 9615, 1109, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 311, "seek": 116400, "start": 1177.0, "end": 1183.0, "text": " In this case, we have three databases", "tokens": [682, 341, 1389, 11, 321, 362, 1045, 22380], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 312, "seek": 116400, "start": 1183.0, "end": 1185.0, "text": " that are being monitored for Postgres.", "tokens": [300, 366, 885, 36255, 337, 10223, 45189, 13], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 313, "seek": 116400, "start": 1185.0, "end": 1189.0, "text": " One of that is the database for PMM itself.", "tokens": [1485, 295, 300, 307, 264, 8149, 337, 12499, 44, 2564, 13], "temperature": 0.0, "avg_logprob": -0.0747313826051477, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.00040625824476592243}, {"id": 314, "seek": 118900, "start": 1189.0, "end": 1195.0, "text": " We have nine databases in Mongo and 15 databases in MySQL.", "tokens": [492, 362, 4949, 22380, 294, 48380, 293, 2119, 22380, 294, 1222, 39934, 13], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 315, "seek": 118900, "start": 1195.0, "end": 1197.0, "text": " This is a good thing for PMM,", "tokens": [639, 307, 257, 665, 551, 337, 12499, 44, 11], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 316, "seek": 118900, "start": 1197.0, "end": 1200.0, "text": " because you can see everything in one single dashboard,", "tokens": [570, 291, 393, 536, 1203, 294, 472, 2167, 18342, 11], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 317, "seek": 118900, "start": 1200.0, "end": 1202.0, "text": " and we can go deeper for each node", "tokens": [293, 321, 393, 352, 7731, 337, 1184, 9984], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 318, "seek": 118900, "start": 1202.0, "end": 1211.0, "text": " or for each database as you want.", "tokens": [420, 337, 1184, 8149, 382, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 319, "seek": 118900, "start": 1211.0, "end": 1213.0, "text": " Yeah, this is all.", "tokens": [865, 11, 341, 307, 439, 13], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 320, "seek": 118900, "start": 1213.0, "end": 1215.0, "text": " Thank you so much if you have some questions.", "tokens": [1044, 291, 370, 709, 498, 291, 362, 512, 1651, 13], "temperature": 0.0, "avg_logprob": -0.11959317922592164, "compression_ratio": 1.5444444444444445, "no_speech_prob": 0.00024269658024422824}, {"id": 321, "seek": 121500, "start": 1215.0, "end": 1224.0, "text": " So thanks a lot for the interesting talk.", "tokens": [407, 3231, 257, 688, 337, 264, 1880, 751, 13], "temperature": 0.0, "avg_logprob": -0.18337170282999674, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.0010374553967267275}, {"id": 322, "seek": 121500, "start": 1224.0, "end": 1226.0, "text": " Does anyone have questions?", "tokens": [4402, 2878, 362, 1651, 30], "temperature": 0.0, "avg_logprob": -0.18337170282999674, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.0010374553967267275}, {"id": 323, "seek": 121500, "start": 1233.0, "end": 1235.0, "text": " Hi, hello, good morning.", "tokens": [2421, 11, 7751, 11, 665, 2446, 13], "temperature": 0.0, "avg_logprob": -0.18337170282999674, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.0010374553967267275}, {"id": 324, "seek": 121500, "start": 1235.0, "end": 1238.0, "text": " The query monitoring dashboard", "tokens": [440, 14581, 11028, 18342], "temperature": 0.0, "avg_logprob": -0.18337170282999674, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.0010374553967267275}, {"id": 325, "seek": 121500, "start": 1238.0, "end": 1243.0, "text": " do have some advice on how to perform these queries,", "tokens": [360, 362, 512, 5192, 322, 577, 281, 2042, 613, 24109, 11], "temperature": 0.0, "avg_logprob": -0.18337170282999674, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.0010374553967267275}, {"id": 326, "seek": 124300, "start": 1243.0, "end": 1249.0, "text": " the bad queries, the slow queries.", "tokens": [264, 1578, 24109, 11, 264, 2964, 24109, 13], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 327, "seek": 124300, "start": 1249.0, "end": 1256.0, "text": " Some advice on how to perform the bad queries,", "tokens": [2188, 5192, 322, 577, 281, 2042, 264, 1578, 24109, 11], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 328, "seek": 124300, "start": 1256.0, "end": 1262.0, "text": " the slow queries, how to rewrite them.", "tokens": [264, 2964, 24109, 11, 577, 281, 28132, 552, 13], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 329, "seek": 124300, "start": 1262.0, "end": 1265.0, "text": " If you go deeper into that query,", "tokens": [759, 291, 352, 7731, 666, 300, 14581, 11], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 330, "seek": 124300, "start": 1265.0, "end": 1267.0, "text": " you will open another dashboard", "tokens": [291, 486, 1269, 1071, 18342], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 331, "seek": 124300, "start": 1267.0, "end": 1270.0, "text": " where you are going to be able to see suggestions.", "tokens": [689, 291, 366, 516, 281, 312, 1075, 281, 536, 13396, 13], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 332, "seek": 124300, "start": 1270.0, "end": 1272.0, "text": " You are doing something bad here,", "tokens": [509, 366, 884, 746, 1578, 510, 11], "temperature": 0.0, "avg_logprob": -0.12884901665352486, "compression_ratio": 1.6625766871165644, "no_speech_prob": 0.001136532868258655}, {"id": 333, "seek": 127200, "start": 1272.0, "end": 1277.0, "text": " and you can fix it with that.", "tokens": [293, 291, 393, 3191, 309, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 334, "seek": 127200, "start": 1277.0, "end": 1280.0, "text": " Hi, thank you for the talk, first of all.", "tokens": [2421, 11, 1309, 291, 337, 264, 751, 11, 700, 295, 439, 13], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 335, "seek": 127200, "start": 1280.0, "end": 1284.0, "text": " One question regarding the PMM query analytics.", "tokens": [1485, 1168, 8595, 264, 12499, 44, 14581, 15370, 13], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 336, "seek": 127200, "start": 1284.0, "end": 1289.0, "text": " Is it possible to filter by connection and not by database?", "tokens": [1119, 309, 1944, 281, 6608, 538, 4984, 293, 406, 538, 8149, 30], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 337, "seek": 127200, "start": 1289.0, "end": 1291.0, "text": " Say it again, please.", "tokens": [6463, 309, 797, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 338, "seek": 127200, "start": 1291.0, "end": 1295.0, "text": " I want all queries from one connection instead of...", "tokens": [286, 528, 439, 24109, 490, 472, 4984, 2602, 295, 485], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 339, "seek": 127200, "start": 1295.0, "end": 1297.0, "text": " Yeah, is it possible?", "tokens": [865, 11, 307, 309, 1944, 30], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 340, "seek": 127200, "start": 1297.0, "end": 1300.0, "text": " Yeah, you can have just one connection.", "tokens": [865, 11, 291, 393, 362, 445, 472, 4984, 13], "temperature": 0.0, "avg_logprob": -0.16612736384073892, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.0011829138966277242}, {"id": 341, "seek": 130000, "start": 1300.0, "end": 1302.0, "text": " Okay, then we have to talk.", "tokens": [1033, 11, 550, 321, 362, 281, 751, 13], "temperature": 0.0, "avg_logprob": -0.18432530489834872, "compression_ratio": 1.018181818181818, "no_speech_prob": 0.005713433027267456}, {"id": 342, "seek": 130200, "start": 1302.0, "end": 1331.0, "text": " Yeah, okay, thank you.", "tokens": [50364, 865, 11, 1392, 11, 1309, 291, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16012130975723265, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.0013179414672777057}], "language": "en"}