The speaker begins by introducing themselves and their company, Giscard, which is building a collaborative and open source software platform for testing the quality of AI models. They then discuss the importance of testing machine learning systems, highlighting the risks and incidents that can occur when they are not properly tested. They explain that traditional software testing methods do not work well for AI, as AI systems are probabilistic and involve large amounts of data inputs. The speaker then focuses on two important quality criteria for machine learning, robustness and fairness. They discuss methods such as metamorphic testing and disparate impact to test these criteria. They also touch on the importance of human feedback and inspection before testing. The speaker concludes by inviting feedback and directing the audience to their GitHub for more information.