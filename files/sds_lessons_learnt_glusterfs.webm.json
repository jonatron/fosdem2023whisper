{"text": " Please welcome Sanju, Sanju and Pranin for us and enjoy. Thank you guys, thank you. Good morning guys, I am Sanju and he is Pranin, we work at Foonpei, yeah today we are going to discuss about the lessons that we learnt while we manage the cluster first cluster at the scale and the some of the problems we have faced and the solutions that we have came up with, yeah Foonpei is the leading Indian digital payments and technology company headquartered in Bangalore, India and it uses unified payments interface which is introduced by government of India, so in India if you are thinking of any payment you can do it using Foonpei app, this is how our Foonpei app home screen looks like. And we have like a we see 800 k rps on our edge layer every day and we do 130 million daily transactions, so this will generate lots of records and this will generate lots of records in the documents that we have to store and as per the regulations in India we have to store all of them in India only, so Foonpei has a private cloud where we store all these things and we need a service to store and retrieve the files from the cloud, we have developed a service called darkstore which will write the data to Glustreface and which will fetch the data from the Glustreface, so coming to the question why did we choose the Glustreface, we didn't wanted to have a metadata server because like we have lots of small files and storing all the metadata, we didn't wanted it, so Glustreface has no metadata server, so we went ahead with it and our team had earlier success in the Glustreface project, so they were confident that Glustreface will work for our use case, so we are here and this is the data flow to and from the Glustreface, so all the traffic is fronted up by CDM and the request is forwarded to nginx and nginx will send the request to the API gateway and API gateway can choose to store or retrieve any file from the, any file or it can choose to send the request to any back-end service, now if the back-end service wants to store this file or if it wants a file it can be a post or get request I mean like it can store or it can retrieve, it will send the request to darkstore, now the darkstore will store the data or retrieve the data from Glustreface servers and darkstore also uses elastic search to store some of the metadata and it uses aero spike to store the earth related info and some of the rate limiting features, it uses RMQ for asynchronous jobs like deletions and batch operations and this is our team, yeah today's our agenda is an introduction to Glustreface and then we will discuss about different problems that I have faced and the solutions that we are using and we have some proposals as a roadmap. What is Glustreface? Glustreface it is a distributed file system that means whenever you do some write the data is distributed across multiple servers, these servers have some of the directories we call them as BRICS and this is where the data is actually getting stored, yes so this is a typical Glustreface server, each server can have multiple BRICS, the BRICS will have underlying file system where the data will be stored and in the root partition we store the Glustreface configuration, go ahead, yeah this is how a 3 by 3 Glustreface volume looks like, when I say 3 by 3 whenever a write comes to Glustreface mount point, so how Mr. 1 point like we can mount Glustreface volume on any mission over the network and you can read and write from that mission, now from the client where the mount is happened if any write comes, so it is distributed across 3 sub volumes based on the hash range allocation, we will talk more about the hash range in a coming slides and another 3 is transfer the data is replicated 3 times, so whenever a write comes the data will choose one of the sub volume and in a sub volume, sub volume is a replica set, here it is a 3, so it is replicated thrice, over to Pranip, hello, yeah so let us look at some numbers that we see at phone pay for dock store service and then to Glustreface, in a day we see about 4.3 million uploads and downloads are 9 million, with peak upload rps as 200 and download rps as 800, the aggregate upload size per day is just 150 GB, not a lot but the download size is 2.5 TB, so it is completely read heavy workload and this is after a Syrian is fronting it, that means only when the file is not available in your CDN, the call will come to Glustreface which will download the file onto the CDN and then it will be served and this is how the rps is distributed throughout the day, rps is request per second, so the uploads actually are reasonably uniform from 6 am to 5 in the evening, then it tapers off for the rest of the day, whereas the downloads are in bimodal distribution with one peak at around 12 pm and another at around 7 pm, the latencies are function of the size of the file, so we have post upload latencies with mean of about 50 ms to the p99 at around 250 ms, similarly for gates the mean is around 10 ms and p99 is around 100 ms, let us look at the configuration that we use at phone pay for Glustreface, we have 30 nodes in the cluster, each node contributes 2 bricks and one brick corresponds to 10 TB and that is a ZFS pool, so 30 into 20 that is 600 TB of available capacity and we use replica 3, so the available size is 200 TB out of which 130 TB is in use at the moment, let us now go to the problems that we face and how we solved it, I will start off with the capacity expansion problem that we solved, then Sanju will take over and talk about the data migration problem that we solved, I will talk about how performance issues are debugged and how we solved the problems using that method, then Sanju will finish it off with maintenance activities that we do to prevent the problems, before we talk about the capacity expansion problem, let us try to understand a bit about the distribution, so the data is distributed across the servers based on hashes, in this diagram we have 3 distribute sub volumes, each sub volume is a replica 3, so when you create a directory, each of the directory in these 3 replica sets will get a hash range and whenever you create a file or try to read a file, it will actually compute the hash of the name and it will figure out which of these directories in these 3 sub volumes has that hash range and tries to get that file or store that file in that node, so for folks who are well versed with database, this is more like sharding but the entity here that is getting sharded is the directory based on the file names, alright, so the files actually can have varying sizes, for example in our setup, the minimum size would be less than a kb but the maximum size is like 26gb, so you will run into this problem where some of the shards or distributes of volumes that you have would fill up the space before the others, so you need to handle that part as well, so there is a feature in Glouceref is called min-free disk where if you hit that level, when you create the directory again, the hash range will not be allocated for the ones that met the threshold, so for example here, even though there are 3 distribute sub volumes, data is going to only 2 because the middle one actually has met the threshold, so the hash range will only be distributed between the 2, 50% and 50% instead of one third that you would expect normally, so let's talk about the actual process of increasing the capacity and why it didn't work for us, when you want to increase the capacity that is you bring in more distributes of volumes or shards, the way that you do it is you first you do something called as cluster peer probe, that will bring the new machines into the cluster, then you do another operation called add brick that will add the bricks to your volume, then you have to do something called as cluster volume rebalance to redistribute the data among the nodes equally, so what are the problems that we faced, when we did the benchmark, the rebalance had this application latency impact in some cases up to 25 seconds and as I mentioned most of the P99 latencies were just in milliseconds, so this is this will be like a partial timeout partial outage for us, so this is not going to work for us, the other thing that we notice is for large volumes the rebalance may take up to months and at the moment cluster FS rebalance does not have pause and resume, so we can't do the maintenance activity in off peak hours, that is one more problem, the other one that we have seen is when you do the data migration when it is going from one distribute sub volume or shard to two distribute sub volumes, you would expect 50 percent of the data to be transferred that's all right, but when you are going from 9 shards slash distributed sub volumes to 10, you want to only migrate like 10 percent of the data, but less than FS is still like transferring about 30 percent to 40 percent like irrespective of what is the number of sub volumes are, so the rebalance itself may take so much time with our workload that by the time we want to do the next capacity expansion the rebalance may not even complete, so that is also not going to work for us, so these are the three main problems that we have seen, so this is the solution that we are using now, then there is a proposal as well, since we know that the hash range allocation is based on the based on both the number of sub volumes and number of free sub volumes, what we are doing is in our doxor application every day in the night we create directories with a new basically, so the directory structure will be something like the namespace that the clients are going to use slash year slash month slash day, so each day you are going to create new directories, so based on the size that is available only the ones that have space will get the hash range allocation, so you will never run into the problem where you will have to do rebalance that much, because we have seen that with our workloads reads are distributed uniformly and as we have seen the it is read heavy workload and writes are just a few, so we were okay with the solution in the interim, but long term the solution that we are we have proposed and this is something that is yet to be accepted, but there are some POC that we did very few use jump consistent hash instead of the one that we have when you are going from 9 to 10 here it is only about 10 percent that is getting rebalanced, so that is what we want to get to this is something that we are focusing on this year, alright over to you Sanju, so let us look at the problems that we have faced while migrating the data, so we had a use case where we wanted to move complete data which is present in one server to another server, so in clusterface the standard way of doing this is to use a rebalance operation, sorry replace brick operation, so when you do replace brick operation there is a process called a self filled demon which will copy all the data which is present in the old server to new server, so to copy 10 TB data it takes around 2 to 3 weeks, so that is like a huge time we wanted to reduce this time so we came up with a new approach so let us understand few aspects of clusterface before we jump to the solution, so that we understand our approach better, so the right flow in clusterface is something like this whenever a right comes based on the hash range allocation plan is just spoke it will choose one of the sub volume, so the data will go to all the servers in that sub volume, now let us say we have chosen replicas at 0 and the right will go to all the machines in that sub volume, it is a client side replication so the client will send the right to all the machines and it will wait for the success response to come, so client will assume the right is successful only when quorum number of success responses has come, let us say one of the node is down, in our case we see like a server 2 either it can be a node down or the brick process is unhealthy this can be unresponsive at times, so something happened the right came to one of the sub volume and it went to all the three replica servers, but server 2 did not responded with the success response, now server 1 and server 3 has responded with the success response, so client it assumes that the right is successful, now when the server 2 is back up we to have the consistency of the data server 2 should get the data which it has missed while it was down, so who will take care of the job of doing this it is SHD, so SHD is a daemon process which will read the pending heal data like whatever the data that was missing we call it as a pending heal, so it will read from one of the good copy in our case server 1 and server 3 are the good copies and server 2 is a bad copy, so SHD will read the data from one of the good copy and it will write to server 2, so server 2 will have all the data once the self heal is completed healing the data, we will use this as part of our approach as well, our approach is we will kill the brick which we want to migrate like we want to migrate from the server 3 to server 4, so we have to copy all the data right, so self heal is taking 2 to 3 weeks, here in our case we will kill the brick and we have a ZFS, we are using ZFS file system, so we will take a ZFS snapshot and we will transfer this snapshot from the server 3 to server 4, it is like a old server to the new server and now we will perform the replace brick operation, while we are performing the replace brick operation server 4 that is a new server will already have all the data which server 3 had, once the replace brick operation is performed server 4 is now part of the sub volume and the heals will take place from server 1 and server 2 to server 4, so now we have reduced the amount of data that we are healing, previously we are copying all the data that is like a 10 TB of data from server 3 to server 4, but here in our case we are healing only the data which came after killing the brick before doing the rebalance replace brick operation, so the data we heal is reduced hugely, with this approach now it is taking only 50 hours to complete this, that is also if we are using the spinning discs it will take 48 hours to transfer the snapshot of 10 TB and 2 hours for the healing of data, but it is only 8 to 9 hours if we are using SSDs, if we are using SSD it takes like a 8 hours to transfer the snapshot and it takes around 40 minutes to complete the heals, so that is like we came from 2 to 3 weeks to 1 or 2 days or 9 hours we can say, we are using netcat utility, it gave us very good performance, it is like a 60% performance optimization and we have in flight checksum at both the ends in the old server and also in the new server, so that it is like we are checking whether we are transferring the snapshot perfectly or not, we are not using any data and yeah it is at the time, I have kept the commands that we have exactly used in this link and we also have a rollback plan, so let us say that we have started with this activity but we have not performed the replace brick yet, because once the replace brick is performed it will be something like this, the sub volume will already have the server 4 as a part of it, before we perform the replace brick that means when we are here, we can we do not want to do this anymore, all we need to do is start the volume with the force, so that the brick process that we have killed will come up, once it is up the good copies that we have SSD will copy the data from good copies to bad copy are the old server, so that we will have the consistent data across all of our replicated servers, yeah that is so easy and we want to popularize this method so that it helps the community, yeah over to Prenet, yeah so this we will now talk about the performance issues that we faced and how we solved them, this is the graph that we have seen in our prod setup, while doing this migration when something happened that we did not account for, so the latencies have shot up to 1 minute here and I have said that it is supposed to be only milliseconds, so this is horrible, there was like 2 hours of partial voltage because of this, so let us see how these things can be debugged and how they can be fixed, so we have a method called GlusterVolumeProfile in GlusterFS, so what you do is you start profiling on the volume, then you run your benchmark or whatever is your workload, then you keep executing GlusterVolumeProfile in for incremental and it will keep giving you the stats of what is happening to the volume during that time, for each of the bricks that are there in the volume you will get an output like this, where for that interval in this case interval 9, for each of the block size you will see the number of reads and writes that came and for all of the internal file operations that you see on the volume, you will get the number of calls and the latency distribution, min max average latency and what is the percentage latency that is taken by each of your file operation internally. So, what we have seen when this ZFS issue happened is the lookup call is taking more than a second which is not what we generally see, so we knew something was happening during lookup operation, so we did an stress on the brick and we have found that there is one internal directory called GlusterFS indices XRTROP, to list three entries it is basically taking 0.35 seconds, so we so imagine this, so you do LS it will just show you three entries, but it will take like 0.35 seconds sometimes it even takes a second, so we after looking at this we found that ZFS has this behavior where if you create a lot of files in one directory like millions and then you delete most of them and then if you do LS it takes up to a second, so this bug is open for more than like two years I think, so we did not know whether ZFS would fix this issue anytime soon, so in GlusterFS we patched it by caching this information, so that we do not have to keep doing this operation, so now you would not see it if you are using any of the latest GlusterFS releases, but yeah this is one issue that we found and fixed. The second one is about increasing the RPS that we have on our volume, so the there was a new application that was getting launched at the time and the RPS that they wanted was not what we are giving, so basically they wanted something like 300, 360 RPS or something like that, but when we did the benchmark we were getting only like 250 RPS, so we wanted to figure out what is happening, so we ran benchmarks on Prod Gluster itself and we saw that one of the threads is getting saturated, so there is a feature in GlusterFS called client IO threads where multiple threads would take the responsibility of sending it over the network, so we thought let us just enable it and it would solve all our problems, we enabled it and it made it worse like from 250 it went down, so we realized that there is a continuation problem in the client side that we are yet to fix, so for now what we did is to on the containers of Dockstore where it was doing only one mount, we are now doing three mounts and distributing the uploads and downloads over yes, so can you repeat the, oh yeah, no I didn't, it is a fuse mount, yeah the thread that is saturating is fuse thread, yeah so the question is which GlusterFS client we are using, the answer is fuse client and the thread that is saturating is fuse thread, so what we are doing is we have created multiple mounts on the container and we are distributing the load in the application itself like the uploads will go to all three and even downloads will go to all three, that is one thing that we did to solve the CPU saturation problem, the other thing that we noticed this is like part of the Gluster volume profile output where it will tell you for each block what is the number of reads and writes, we have seen that most of the writes are coming as 8KB, so later when we looked at the Java application Dockstore we saw that the IO block that Java is using the default size is 8KB, so we just increased it to 128KB, so these two combined has given us 2X to 3X the number and we also increased the number of VMs that we are using to mount the client, so put all together we got something like 10X performance improvement compared to the earlier one, so we are set for maybe 2, 3 KB all right, so let us now go on to health checks, so for any production cluster some of the health checks are needed, so I will talk about the minimal health checks that needed for GlusterFace cluster, so GlusterFace already provides POSIX health checks, so it is a health checker thread which will do a write of 1KB for every 15 or 30 minutes, I mean seconds, so there is one option to set the time interval in which you want to do this, so if you set it as a 0 that means you are disabling the health check, so you can set it as like a 10 seconds or something, so it sends a write and check if the disk is responsive enough and brick is healthy or not, if it did not get a response in a particular time, it will kill the brick process, so that like we will get to know that something is wrong with the brick process, so the other one we have is the rest of the things are we have a script and we have some config, these are the things we have kept externally kind of thing, the POSIX health checks are the one which come with the GlusterFace project, so the cluster health checks that we have are like we have a config where we will specify number of nodes in the cluster, so that is like a expected number of nodes in the cluster and using the Gluster peer status or GlusterPoorList command, we can check the number of nodes that are present in the cluster and we will check if both of them are equal, if not we will write an alert saying something unexpected is happening and we will also check whether the node is in connected state or not, so in the GlusterFace cluster the nodes can be in different state, so it can be connected or rejected or disconnected based on how the GlusterFace management daemon is working, so now we will see whether, so the expected is all the nodes should be in a connected state, we will check whether the nodes are connected or not, if the nodes are not connected then we will get an alert saying okay one of your node is not in a connected state and we have some of the health checks for the BRICS as well, so we have number of BRICS that are present in each volume in the config and in the GlusterVolume info output you will get how many number of volumes that are present in that volume and you will check if they are equal, the another check we have on the BRICS, if the BRICS is not online we will get to know it by checking the GlusterVolume status command and if it is not online you will get an alert saying that one of your BRICS is down and so whenever the server is down or the BRICS is down there will be some of the pending heels and you can check the pending heels using the GlusterVolumeHealInfo command and if there are any pending heels you will see an entry, so if the entry is non-zero then you will get an alert saying that okay you have some pending heels in your cluster that means something unexpected, unwanted is going on that can be like a BRICS down or node is down anything and we always lock profile info incremental to our debug locks using the health check so that whenever we see some issue like the Prandit just spoke about some of the issues that we can solve by looking at the profile info output, so in such cases this output will be helpful so we always log into our log backup servers and the exact commands that we are using are listed in this link, so we have some of the maintenance activities so things can go back sometimes, so we have a replica 3 setup in our production, so at any point of time quorum number of BRICS process should be up so that the reads and writes can go on smoothly, so whenever we are doing something which might take some downtime of the BRICS process or which can have some load on particular server at that time we do it only on one of the server from each replica set so that even if that server goes down or the BRICS process running on that server goes down we won't be having an issue because there are two other replica servers which can like do all the reads and writes, so we are doing few activities in this way, one is ZFS scrubbing, ZFS scrubbing is about doing the checksum of the data, it will see if the data is in a proper condition or not and we do migrations in this way only, so we are doing it on one server from each replica set so that even if it is down for some time or something didn't work out we are in a good place and upgrades also we will do in the same manner, we have done some contributions so the data migration part that I have spoke it's a production ready we have used it in our production and Pranit has given some of the developer sessions which has many internals of Glastrophase, they are very useful for any Glastrophase developers who wants to learn about many translators that we have in Glastrophase and recently we have fixed one of the single point of failure which was present in the geo-replication feature, it was merged into the upstream very recently last week and this year we are looking at another thing the hashing strategy that Pranit has proposed, once it is accepted at the community we will take it and develop it, yeah that's all we had folks, thank you. Just want to let you guys know that the production ready thing, we actually migrated like in total 375 TB using the method that Sanju talked about so it is ready, so yeah you guys can use it, I think it should work even with butter, basically any file system that has a snapshot feature it should work, yeah thank you guys, yeah I think we have a few minutes for questions if you have any otherwise you guys can catch us there, yeah so the question is how do you handle a disk failure, so basically the problem that I showed you where we had the ZFS issue where it was taking like minutes of latency that was the first time it happened on production for us and initially we were waiting for the machine itself to be fixed so that it will come back again and it went for like a week or so and the amount of data that needed to be healed became too much that it coincided with our peak hours, so now the standard operating procedure that we have come up with after this issue is if a machine goes down or disk goes down we can just get it back online in 9 hours so why do we have to wait, so we just consider that node dead, we get a new machine we do whatever Sanju mentioned using ZFS snapshot migration and we just bring it up, so do you have the ZFS backup somewhere, do you have the ZFS backup somewhere, the answer is no you have the ZFS data on the active bricks so you take a snapshot on the active bricks and do the snapshot trend, yeah one of the good ones yes, any other questions, I think that's it I think, thank you guys, thanks a lot. Thank you. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " Please welcome Sanju, Sanju and Pranin for us and enjoy.", "tokens": [2555, 2928, 5271, 8954, 11, 5271, 8954, 293, 2114, 282, 259, 337, 505, 293, 2103, 13], "temperature": 0.0, "avg_logprob": -0.36379539489746093, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.4094523787498474}, {"id": 1, "seek": 0, "start": 13.0, "end": 16.84, "text": " Thank you guys, thank you.", "tokens": [1044, 291, 1074, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.36379539489746093, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.4094523787498474}, {"id": 2, "seek": 0, "start": 16.84, "end": 23.8, "text": " Good morning guys, I am Sanju and he is Pranin, we work at Foonpei, yeah today we are going", "tokens": [2205, 2446, 1074, 11, 286, 669, 5271, 8954, 293, 415, 307, 2114, 282, 259, 11, 321, 589, 412, 479, 4106, 46238, 11, 1338, 965, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.36379539489746093, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.4094523787498474}, {"id": 3, "seek": 0, "start": 23.8, "end": 28.36, "text": " to discuss about the lessons that we learnt while we manage the cluster first cluster", "tokens": [281, 2248, 466, 264, 8820, 300, 321, 18991, 1339, 321, 3067, 264, 13630, 700, 13630], "temperature": 0.0, "avg_logprob": -0.36379539489746093, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.4094523787498474}, {"id": 4, "seek": 2836, "start": 28.36, "end": 33.88, "text": " at the scale and the some of the problems we have faced and the solutions that we have", "tokens": [412, 264, 4373, 293, 264, 512, 295, 264, 2740, 321, 362, 11446, 293, 264, 6547, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.18752242297661015, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0024876391980797052}, {"id": 5, "seek": 2836, "start": 33.88, "end": 44.64, "text": " came up with, yeah Foonpei is the leading Indian digital payments and technology company", "tokens": [1361, 493, 365, 11, 1338, 479, 4106, 46238, 307, 264, 5775, 6427, 4562, 14348, 293, 2899, 2237], "temperature": 0.0, "avg_logprob": -0.18752242297661015, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0024876391980797052}, {"id": 6, "seek": 2836, "start": 44.64, "end": 50.32, "text": " headquartered in Bangalore, India and it uses unified payments interface which is introduced", "tokens": [1378, 43363, 292, 294, 11538, 304, 418, 11, 5282, 293, 309, 4960, 26787, 14348, 9226, 597, 307, 7268], "temperature": 0.0, "avg_logprob": -0.18752242297661015, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0024876391980797052}, {"id": 7, "seek": 2836, "start": 50.32, "end": 56.2, "text": " by government of India, so in India if you are thinking of any payment you can do it", "tokens": [538, 2463, 295, 5282, 11, 370, 294, 5282, 498, 291, 366, 1953, 295, 604, 10224, 291, 393, 360, 309], "temperature": 0.0, "avg_logprob": -0.18752242297661015, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.0024876391980797052}, {"id": 8, "seek": 5620, "start": 56.2, "end": 62.24, "text": " using Foonpei app, this is how our Foonpei app home screen looks like.", "tokens": [1228, 479, 4106, 46238, 724, 11, 341, 307, 577, 527, 479, 4106, 46238, 724, 1280, 2568, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.16977913408394318, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.0014671595999971032}, {"id": 9, "seek": 5620, "start": 62.24, "end": 69.44, "text": " And we have like a we see 800 k rps on our edge layer every day and we do 130 million", "tokens": [400, 321, 362, 411, 257, 321, 536, 13083, 350, 367, 1878, 322, 527, 4691, 4583, 633, 786, 293, 321, 360, 19966, 2459], "temperature": 0.0, "avg_logprob": -0.16977913408394318, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.0014671595999971032}, {"id": 10, "seek": 5620, "start": 69.44, "end": 79.72, "text": " daily transactions, so this will generate lots of records and this will generate lots", "tokens": [5212, 16856, 11, 370, 341, 486, 8460, 3195, 295, 7724, 293, 341, 486, 8460, 3195], "temperature": 0.0, "avg_logprob": -0.16977913408394318, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.0014671595999971032}, {"id": 11, "seek": 5620, "start": 79.72, "end": 85.56, "text": " of records in the documents that we have to store and as per the regulations in India", "tokens": [295, 7724, 294, 264, 8512, 300, 321, 362, 281, 3531, 293, 382, 680, 264, 12563, 294, 5282], "temperature": 0.0, "avg_logprob": -0.16977913408394318, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.0014671595999971032}, {"id": 12, "seek": 8556, "start": 85.56, "end": 91.4, "text": " we have to store all of them in India only, so Foonpei has a private cloud where we store", "tokens": [321, 362, 281, 3531, 439, 295, 552, 294, 5282, 787, 11, 370, 479, 4106, 46238, 575, 257, 4551, 4588, 689, 321, 3531], "temperature": 0.0, "avg_logprob": -0.1774369818823678, "compression_ratio": 1.8893617021276596, "no_speech_prob": 0.0002480859402567148}, {"id": 13, "seek": 8556, "start": 91.4, "end": 98.36, "text": " all these things and we need a service to store and retrieve the files from the cloud,", "tokens": [439, 613, 721, 293, 321, 643, 257, 2643, 281, 3531, 293, 30254, 264, 7098, 490, 264, 4588, 11], "temperature": 0.0, "avg_logprob": -0.1774369818823678, "compression_ratio": 1.8893617021276596, "no_speech_prob": 0.0002480859402567148}, {"id": 14, "seek": 8556, "start": 98.36, "end": 104.4, "text": " we have developed a service called darkstore which will write the data to Glustreface and", "tokens": [321, 362, 4743, 257, 2643, 1219, 2877, 21624, 597, 486, 2464, 264, 1412, 281, 5209, 381, 33115, 617, 293], "temperature": 0.0, "avg_logprob": -0.1774369818823678, "compression_ratio": 1.8893617021276596, "no_speech_prob": 0.0002480859402567148}, {"id": 15, "seek": 8556, "start": 104.4, "end": 109.16, "text": " which will fetch the data from the Glustreface, so coming to the question why did we choose", "tokens": [597, 486, 23673, 264, 1412, 490, 264, 5209, 381, 33115, 617, 11, 370, 1348, 281, 264, 1168, 983, 630, 321, 2826], "temperature": 0.0, "avg_logprob": -0.1774369818823678, "compression_ratio": 1.8893617021276596, "no_speech_prob": 0.0002480859402567148}, {"id": 16, "seek": 8556, "start": 109.16, "end": 114.64, "text": " the Glustreface, we didn't wanted to have a metadata server because like we have lots", "tokens": [264, 5209, 381, 33115, 617, 11, 321, 994, 380, 1415, 281, 362, 257, 26603, 7154, 570, 411, 321, 362, 3195], "temperature": 0.0, "avg_logprob": -0.1774369818823678, "compression_ratio": 1.8893617021276596, "no_speech_prob": 0.0002480859402567148}, {"id": 17, "seek": 11464, "start": 114.64, "end": 120.76, "text": " of small files and storing all the metadata, we didn't wanted it, so Glustreface has no", "tokens": [295, 1359, 7098, 293, 26085, 439, 264, 26603, 11, 321, 994, 380, 1415, 309, 11, 370, 5209, 381, 33115, 617, 575, 572], "temperature": 0.0, "avg_logprob": -0.12242101885608792, "compression_ratio": 1.79, "no_speech_prob": 5.4622567404294387e-05}, {"id": 18, "seek": 11464, "start": 120.76, "end": 126.84, "text": " metadata server, so we went ahead with it and our team had earlier success in the Glustreface", "tokens": [26603, 7154, 11, 370, 321, 1437, 2286, 365, 309, 293, 527, 1469, 632, 3071, 2245, 294, 264, 5209, 381, 33115, 617], "temperature": 0.0, "avg_logprob": -0.12242101885608792, "compression_ratio": 1.79, "no_speech_prob": 5.4622567404294387e-05}, {"id": 19, "seek": 11464, "start": 126.84, "end": 135.08, "text": " project, so they were confident that Glustreface will work for our use case, so we are here", "tokens": [1716, 11, 370, 436, 645, 6679, 300, 5209, 381, 33115, 617, 486, 589, 337, 527, 764, 1389, 11, 370, 321, 366, 510], "temperature": 0.0, "avg_logprob": -0.12242101885608792, "compression_ratio": 1.79, "no_speech_prob": 5.4622567404294387e-05}, {"id": 20, "seek": 11464, "start": 135.08, "end": 141.08, "text": " and this is the data flow to and from the Glustreface, so all the traffic is fronted", "tokens": [293, 341, 307, 264, 1412, 3095, 281, 293, 490, 264, 5209, 381, 33115, 617, 11, 370, 439, 264, 6419, 307, 1868, 292], "temperature": 0.0, "avg_logprob": -0.12242101885608792, "compression_ratio": 1.79, "no_speech_prob": 5.4622567404294387e-05}, {"id": 21, "seek": 14108, "start": 141.08, "end": 147.32000000000002, "text": " up by CDM and the request is forwarded to nginx and nginx will send the request to the API", "tokens": [493, 538, 6743, 44, 293, 264, 5308, 307, 2128, 292, 281, 297, 1494, 87, 293, 297, 1494, 87, 486, 2845, 264, 5308, 281, 264, 9362], "temperature": 0.0, "avg_logprob": -0.1627958337056268, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.000211020334973}, {"id": 22, "seek": 14108, "start": 147.32000000000002, "end": 153.96, "text": " gateway and API gateway can choose to store or retrieve any file from the, any file or", "tokens": [28532, 293, 9362, 28532, 393, 2826, 281, 3531, 420, 30254, 604, 3991, 490, 264, 11, 604, 3991, 420], "temperature": 0.0, "avg_logprob": -0.1627958337056268, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.000211020334973}, {"id": 23, "seek": 14108, "start": 153.96, "end": 158.92000000000002, "text": " it can choose to send the request to any back-end service, now if the back-end service wants", "tokens": [309, 393, 2826, 281, 2845, 264, 5308, 281, 604, 646, 12, 521, 2643, 11, 586, 498, 264, 646, 12, 521, 2643, 2738], "temperature": 0.0, "avg_logprob": -0.1627958337056268, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.000211020334973}, {"id": 24, "seek": 14108, "start": 158.92000000000002, "end": 165.96, "text": " to store this file or if it wants a file it can be a post or get request I mean like it", "tokens": [281, 3531, 341, 3991, 420, 498, 309, 2738, 257, 3991, 309, 393, 312, 257, 2183, 420, 483, 5308, 286, 914, 411, 309], "temperature": 0.0, "avg_logprob": -0.1627958337056268, "compression_ratio": 1.924731182795699, "no_speech_prob": 0.000211020334973}, {"id": 25, "seek": 16596, "start": 165.96, "end": 172.04000000000002, "text": " can store or it can retrieve, it will send the request to darkstore, now the darkstore", "tokens": [393, 3531, 420, 309, 393, 30254, 11, 309, 486, 2845, 264, 5308, 281, 2877, 21624, 11, 586, 264, 2877, 21624], "temperature": 0.0, "avg_logprob": -0.17934743924574417, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.00020534729992505163}, {"id": 26, "seek": 16596, "start": 172.04000000000002, "end": 179.96, "text": " will store the data or retrieve the data from Glustreface servers and darkstore also uses", "tokens": [486, 3531, 264, 1412, 420, 30254, 264, 1412, 490, 5209, 381, 33115, 617, 15909, 293, 2877, 21624, 611, 4960], "temperature": 0.0, "avg_logprob": -0.17934743924574417, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.00020534729992505163}, {"id": 27, "seek": 16596, "start": 179.96, "end": 187.56, "text": " elastic search to store some of the metadata and it uses aero spike to store the earth related", "tokens": [17115, 3164, 281, 3531, 512, 295, 264, 26603, 293, 309, 4960, 257, 2032, 21053, 281, 3531, 264, 4120, 4077], "temperature": 0.0, "avg_logprob": -0.17934743924574417, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.00020534729992505163}, {"id": 28, "seek": 16596, "start": 187.56, "end": 195.84, "text": " info and some of the rate limiting features, it uses RMQ for asynchronous jobs like deletions", "tokens": [13614, 293, 512, 295, 264, 3314, 22083, 4122, 11, 309, 4960, 23790, 48, 337, 49174, 4782, 411, 1103, 302, 626], "temperature": 0.0, "avg_logprob": -0.17934743924574417, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.00020534729992505163}, {"id": 29, "seek": 19584, "start": 195.84, "end": 205.28, "text": " and batch operations and this is our team, yeah today's our agenda is an introduction", "tokens": [293, 15245, 7705, 293, 341, 307, 527, 1469, 11, 1338, 965, 311, 527, 9829, 307, 364, 9339], "temperature": 0.0, "avg_logprob": -0.20043110025340113, "compression_ratio": 1.644859813084112, "no_speech_prob": 9.142435010289773e-05}, {"id": 30, "seek": 19584, "start": 205.28, "end": 210.88, "text": " to Glustreface and then we will discuss about different problems that I have faced and the", "tokens": [281, 5209, 381, 33115, 617, 293, 550, 321, 486, 2248, 466, 819, 2740, 300, 286, 362, 11446, 293, 264], "temperature": 0.0, "avg_logprob": -0.20043110025340113, "compression_ratio": 1.644859813084112, "no_speech_prob": 9.142435010289773e-05}, {"id": 31, "seek": 19584, "start": 210.88, "end": 217.44, "text": " solutions that we are using and we have some proposals as a roadmap.", "tokens": [6547, 300, 321, 366, 1228, 293, 321, 362, 512, 20198, 382, 257, 35738, 13], "temperature": 0.0, "avg_logprob": -0.20043110025340113, "compression_ratio": 1.644859813084112, "no_speech_prob": 9.142435010289773e-05}, {"id": 32, "seek": 19584, "start": 217.44, "end": 218.44, "text": " What is Glustreface?", "tokens": [708, 307, 5209, 381, 33115, 617, 30], "temperature": 0.0, "avg_logprob": -0.20043110025340113, "compression_ratio": 1.644859813084112, "no_speech_prob": 9.142435010289773e-05}, {"id": 33, "seek": 19584, "start": 218.44, "end": 224.92000000000002, "text": " Glustreface it is a distributed file system that means whenever you do some write the", "tokens": [5209, 381, 33115, 617, 309, 307, 257, 12631, 3991, 1185, 300, 1355, 5699, 291, 360, 512, 2464, 264], "temperature": 0.0, "avg_logprob": -0.20043110025340113, "compression_ratio": 1.644859813084112, "no_speech_prob": 9.142435010289773e-05}, {"id": 34, "seek": 22492, "start": 224.92, "end": 231.76, "text": " data is distributed across multiple servers, these servers have some of the directories", "tokens": [1412, 307, 12631, 2108, 3866, 15909, 11, 613, 15909, 362, 512, 295, 264, 5391, 530], "temperature": 0.0, "avg_logprob": -0.15553320285885833, "compression_ratio": 1.726829268292683, "no_speech_prob": 0.00010739978461060673}, {"id": 35, "seek": 22492, "start": 231.76, "end": 240.79999999999998, "text": " we call them as BRICS and this is where the data is actually getting stored, yes so this", "tokens": [321, 818, 552, 382, 10262, 2532, 50, 293, 341, 307, 689, 264, 1412, 307, 767, 1242, 12187, 11, 2086, 370, 341], "temperature": 0.0, "avg_logprob": -0.15553320285885833, "compression_ratio": 1.726829268292683, "no_speech_prob": 0.00010739978461060673}, {"id": 36, "seek": 22492, "start": 240.79999999999998, "end": 247.92, "text": " is a typical Glustreface server, each server can have multiple BRICS, the BRICS will have", "tokens": [307, 257, 7476, 5209, 381, 33115, 617, 7154, 11, 1184, 7154, 393, 362, 3866, 10262, 2532, 50, 11, 264, 10262, 2532, 50, 486, 362], "temperature": 0.0, "avg_logprob": -0.15553320285885833, "compression_ratio": 1.726829268292683, "no_speech_prob": 0.00010739978461060673}, {"id": 37, "seek": 22492, "start": 247.92, "end": 254.48, "text": " underlying file system where the data will be stored and in the root partition we store", "tokens": [14217, 3991, 1185, 689, 264, 1412, 486, 312, 12187, 293, 294, 264, 5593, 24808, 321, 3531], "temperature": 0.0, "avg_logprob": -0.15553320285885833, "compression_ratio": 1.726829268292683, "no_speech_prob": 0.00010739978461060673}, {"id": 38, "seek": 25448, "start": 254.48, "end": 262.92, "text": " the Glustreface configuration, go ahead, yeah this is how a 3 by 3 Glustreface volume looks", "tokens": [264, 5209, 381, 33115, 617, 11694, 11, 352, 2286, 11, 1338, 341, 307, 577, 257, 805, 538, 805, 5209, 381, 33115, 617, 5523, 1542], "temperature": 0.0, "avg_logprob": -0.17449340572604885, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.00013035301526542753}, {"id": 39, "seek": 25448, "start": 262.92, "end": 271.64, "text": " like, when I say 3 by 3 whenever a write comes to Glustreface mount point, so how Mr. 1 point", "tokens": [411, 11, 562, 286, 584, 805, 538, 805, 5699, 257, 2464, 1487, 281, 5209, 381, 33115, 617, 3746, 935, 11, 370, 577, 2221, 13, 502, 935], "temperature": 0.0, "avg_logprob": -0.17449340572604885, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.00013035301526542753}, {"id": 40, "seek": 25448, "start": 271.64, "end": 280.64, "text": " like we can mount Glustreface volume on any mission over the network and you can read", "tokens": [411, 321, 393, 3746, 5209, 381, 33115, 617, 5523, 322, 604, 4447, 670, 264, 3209, 293, 291, 393, 1401], "temperature": 0.0, "avg_logprob": -0.17449340572604885, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.00013035301526542753}, {"id": 41, "seek": 28064, "start": 280.64, "end": 287.47999999999996, "text": " and write from that mission, now from the client where the mount is happened if any write", "tokens": [293, 2464, 490, 300, 4447, 11, 586, 490, 264, 6423, 689, 264, 3746, 307, 2011, 498, 604, 2464], "temperature": 0.0, "avg_logprob": -0.18754671465966008, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.00016239890828728676}, {"id": 42, "seek": 28064, "start": 287.47999999999996, "end": 295.03999999999996, "text": " comes, so it is distributed across 3 sub volumes based on the hash range allocation, we will", "tokens": [1487, 11, 370, 309, 307, 12631, 2108, 805, 1422, 22219, 2361, 322, 264, 22019, 3613, 27599, 11, 321, 486], "temperature": 0.0, "avg_logprob": -0.18754671465966008, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.00016239890828728676}, {"id": 43, "seek": 28064, "start": 295.03999999999996, "end": 305.0, "text": " talk more about the hash range in a coming slides and another 3 is transfer the data", "tokens": [751, 544, 466, 264, 22019, 3613, 294, 257, 1348, 9788, 293, 1071, 805, 307, 5003, 264, 1412], "temperature": 0.0, "avg_logprob": -0.18754671465966008, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.00016239890828728676}, {"id": 44, "seek": 30500, "start": 305.0, "end": 312.64, "text": " is replicated 3 times, so whenever a write comes the data will choose one of the sub", "tokens": [307, 46365, 805, 1413, 11, 370, 5699, 257, 2464, 1487, 264, 1412, 486, 2826, 472, 295, 264, 1422], "temperature": 0.0, "avg_logprob": -0.19611345755087362, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.00014145238674245775}, {"id": 45, "seek": 30500, "start": 312.64, "end": 320.24, "text": " volume and in a sub volume, sub volume is a replica set, here it is a 3, so it is replicated", "tokens": [5523, 293, 294, 257, 1422, 5523, 11, 1422, 5523, 307, 257, 35456, 992, 11, 510, 309, 307, 257, 805, 11, 370, 309, 307, 46365], "temperature": 0.0, "avg_logprob": -0.19611345755087362, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.00014145238674245775}, {"id": 46, "seek": 30500, "start": 320.24, "end": 332.44, "text": " thrice, over to Pranip, hello, yeah so let us look at some numbers that we see at phone", "tokens": [739, 573, 11, 670, 281, 2114, 282, 647, 11, 7751, 11, 1338, 370, 718, 505, 574, 412, 512, 3547, 300, 321, 536, 412, 2593], "temperature": 0.0, "avg_logprob": -0.19611345755087362, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.00014145238674245775}, {"id": 47, "seek": 33244, "start": 332.44, "end": 340.0, "text": " pay for dock store service and then to Glustreface, in a day we see about 4.3 million uploads", "tokens": [1689, 337, 20929, 3531, 2643, 293, 550, 281, 5209, 381, 33115, 617, 11, 294, 257, 786, 321, 536, 466, 1017, 13, 18, 2459, 48611], "temperature": 0.0, "avg_logprob": -0.15293582762130584, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0008608127245679498}, {"id": 48, "seek": 33244, "start": 340.0, "end": 348.16, "text": " and downloads are 9 million, with peak upload rps as 200 and download rps as 800, the aggregate", "tokens": [293, 36553, 366, 1722, 2459, 11, 365, 10651, 6580, 367, 1878, 382, 2331, 293, 5484, 367, 1878, 382, 13083, 11, 264, 26118], "temperature": 0.0, "avg_logprob": -0.15293582762130584, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0008608127245679498}, {"id": 49, "seek": 33244, "start": 348.16, "end": 355.12, "text": " upload size per day is just 150 GB, not a lot but the download size is 2.5 TB, so it", "tokens": [6580, 2744, 680, 786, 307, 445, 8451, 26809, 11, 406, 257, 688, 457, 264, 5484, 2744, 307, 568, 13, 20, 29711, 11, 370, 309], "temperature": 0.0, "avg_logprob": -0.15293582762130584, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0008608127245679498}, {"id": 50, "seek": 33244, "start": 355.12, "end": 361.72, "text": " is completely read heavy workload and this is after a Syrian is fronting it, that means", "tokens": [307, 2584, 1401, 4676, 20139, 293, 341, 307, 934, 257, 24081, 307, 431, 266, 783, 309, 11, 300, 1355], "temperature": 0.0, "avg_logprob": -0.15293582762130584, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.0008608127245679498}, {"id": 51, "seek": 36172, "start": 361.72, "end": 367.0, "text": " only when the file is not available in your CDN, the call will come to Glustreface which", "tokens": [787, 562, 264, 3991, 307, 406, 2435, 294, 428, 6743, 45, 11, 264, 818, 486, 808, 281, 5209, 381, 33115, 617, 597], "temperature": 0.0, "avg_logprob": -0.14607882237696385, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00019429411622695625}, {"id": 52, "seek": 36172, "start": 367.0, "end": 373.6, "text": " will download the file onto the CDN and then it will be served and this is how the rps", "tokens": [486, 5484, 264, 3991, 3911, 264, 6743, 45, 293, 550, 309, 486, 312, 7584, 293, 341, 307, 577, 264, 367, 1878], "temperature": 0.0, "avg_logprob": -0.14607882237696385, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00019429411622695625}, {"id": 53, "seek": 36172, "start": 373.6, "end": 381.36, "text": " is distributed throughout the day, rps is request per second, so the uploads actually", "tokens": [307, 12631, 3710, 264, 786, 11, 367, 1878, 307, 5308, 680, 1150, 11, 370, 264, 48611, 767], "temperature": 0.0, "avg_logprob": -0.14607882237696385, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00019429411622695625}, {"id": 54, "seek": 36172, "start": 381.36, "end": 388.92, "text": " are reasonably uniform from 6 am to 5 in the evening, then it tapers off for the rest of", "tokens": [366, 23551, 9452, 490, 1386, 669, 281, 1025, 294, 264, 5634, 11, 550, 309, 5119, 433, 766, 337, 264, 1472, 295], "temperature": 0.0, "avg_logprob": -0.14607882237696385, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00019429411622695625}, {"id": 55, "seek": 38892, "start": 388.92, "end": 395.08000000000004, "text": " the day, whereas the downloads are in bimodal distribution with one peak at around 12 pm", "tokens": [264, 786, 11, 9735, 264, 36553, 366, 294, 272, 332, 378, 304, 7316, 365, 472, 10651, 412, 926, 2272, 23023], "temperature": 0.0, "avg_logprob": -0.1778519418504503, "compression_ratio": 1.5963855421686748, "no_speech_prob": 3.5339042369741946e-05}, {"id": 56, "seek": 38892, "start": 395.08000000000004, "end": 404.24, "text": " and another at around 7 pm, the latencies are function of the size of the file, so we", "tokens": [293, 1071, 412, 926, 1614, 23023, 11, 264, 4465, 6464, 366, 2445, 295, 264, 2744, 295, 264, 3991, 11, 370, 321], "temperature": 0.0, "avg_logprob": -0.1778519418504503, "compression_ratio": 1.5963855421686748, "no_speech_prob": 3.5339042369741946e-05}, {"id": 57, "seek": 38892, "start": 404.24, "end": 414.24, "text": " have post upload latencies with mean of about 50 ms to the p99 at around 250 ms, similarly", "tokens": [362, 2183, 6580, 4465, 6464, 365, 914, 295, 466, 2625, 275, 82, 281, 264, 280, 8494, 412, 926, 11650, 275, 82, 11, 14138], "temperature": 0.0, "avg_logprob": -0.1778519418504503, "compression_ratio": 1.5963855421686748, "no_speech_prob": 3.5339042369741946e-05}, {"id": 58, "seek": 41424, "start": 414.24, "end": 426.68, "text": " for gates the mean is around 10 ms and p99 is around 100 ms, let us look at the configuration", "tokens": [337, 19792, 264, 914, 307, 926, 1266, 275, 82, 293, 280, 8494, 307, 926, 2319, 275, 82, 11, 718, 505, 574, 412, 264, 11694], "temperature": 0.0, "avg_logprob": -0.16516373952229818, "compression_ratio": 1.4944444444444445, "no_speech_prob": 4.3193089368287474e-05}, {"id": 59, "seek": 41424, "start": 426.68, "end": 432.48, "text": " that we use at phone pay for Glustreface, we have 30 nodes in the cluster, each node", "tokens": [300, 321, 764, 412, 2593, 1689, 337, 5209, 381, 33115, 617, 11, 321, 362, 2217, 13891, 294, 264, 13630, 11, 1184, 9984], "temperature": 0.0, "avg_logprob": -0.16516373952229818, "compression_ratio": 1.4944444444444445, "no_speech_prob": 4.3193089368287474e-05}, {"id": 60, "seek": 41424, "start": 432.48, "end": 441.04, "text": " contributes 2 bricks and one brick corresponds to 10 TB and that is a ZFS pool, so 30 into", "tokens": [32035, 568, 25497, 293, 472, 16725, 23249, 281, 1266, 29711, 293, 300, 307, 257, 1176, 29318, 7005, 11, 370, 2217, 666], "temperature": 0.0, "avg_logprob": -0.16516373952229818, "compression_ratio": 1.4944444444444445, "no_speech_prob": 4.3193089368287474e-05}, {"id": 61, "seek": 44104, "start": 441.04, "end": 447.88, "text": " 20 that is 600 TB of available capacity and we use replica 3, so the available size is", "tokens": [945, 300, 307, 11849, 29711, 295, 2435, 6042, 293, 321, 764, 35456, 805, 11, 370, 264, 2435, 2744, 307], "temperature": 0.0, "avg_logprob": -0.13208397854579967, "compression_ratio": 1.7198067632850242, "no_speech_prob": 7.00829696143046e-05}, {"id": 62, "seek": 44104, "start": 447.88, "end": 457.04, "text": " 200 TB out of which 130 TB is in use at the moment, let us now go to the problems that", "tokens": [2331, 29711, 484, 295, 597, 19966, 29711, 307, 294, 764, 412, 264, 1623, 11, 718, 505, 586, 352, 281, 264, 2740, 300], "temperature": 0.0, "avg_logprob": -0.13208397854579967, "compression_ratio": 1.7198067632850242, "no_speech_prob": 7.00829696143046e-05}, {"id": 63, "seek": 44104, "start": 457.04, "end": 462.28000000000003, "text": " we face and how we solved it, I will start off with the capacity expansion problem that", "tokens": [321, 1851, 293, 577, 321, 13041, 309, 11, 286, 486, 722, 766, 365, 264, 6042, 11260, 1154, 300], "temperature": 0.0, "avg_logprob": -0.13208397854579967, "compression_ratio": 1.7198067632850242, "no_speech_prob": 7.00829696143046e-05}, {"id": 64, "seek": 44104, "start": 462.28000000000003, "end": 469.76, "text": " we solved, then Sanju will take over and talk about the data migration problem that we solved,", "tokens": [321, 13041, 11, 550, 5271, 8954, 486, 747, 670, 293, 751, 466, 264, 1412, 17011, 1154, 300, 321, 13041, 11], "temperature": 0.0, "avg_logprob": -0.13208397854579967, "compression_ratio": 1.7198067632850242, "no_speech_prob": 7.00829696143046e-05}, {"id": 65, "seek": 46976, "start": 469.76, "end": 476.36, "text": " I will talk about how performance issues are debugged and how we solved the problems using", "tokens": [286, 486, 751, 466, 577, 3389, 2663, 366, 24083, 3004, 293, 577, 321, 13041, 264, 2740, 1228], "temperature": 0.0, "avg_logprob": -0.12251265425431102, "compression_ratio": 1.6875, "no_speech_prob": 0.0002322887012269348}, {"id": 66, "seek": 46976, "start": 476.36, "end": 481.32, "text": " that method, then Sanju will finish it off with maintenance activities that we do to", "tokens": [300, 3170, 11, 550, 5271, 8954, 486, 2413, 309, 766, 365, 11258, 5354, 300, 321, 360, 281], "temperature": 0.0, "avg_logprob": -0.12251265425431102, "compression_ratio": 1.6875, "no_speech_prob": 0.0002322887012269348}, {"id": 67, "seek": 46976, "start": 481.32, "end": 488.24, "text": " prevent the problems, before we talk about the capacity expansion problem, let us try", "tokens": [4871, 264, 2740, 11, 949, 321, 751, 466, 264, 6042, 11260, 1154, 11, 718, 505, 853], "temperature": 0.0, "avg_logprob": -0.12251265425431102, "compression_ratio": 1.6875, "no_speech_prob": 0.0002322887012269348}, {"id": 68, "seek": 46976, "start": 488.24, "end": 498.15999999999997, "text": " to understand a bit about the distribution, so the data is distributed across the servers", "tokens": [281, 1223, 257, 857, 466, 264, 7316, 11, 370, 264, 1412, 307, 12631, 2108, 264, 15909], "temperature": 0.0, "avg_logprob": -0.12251265425431102, "compression_ratio": 1.6875, "no_speech_prob": 0.0002322887012269348}, {"id": 69, "seek": 49816, "start": 498.16, "end": 506.44, "text": " based on hashes, in this diagram we have 3 distribute sub volumes, each sub volume is", "tokens": [2361, 322, 575, 8076, 11, 294, 341, 10686, 321, 362, 805, 20594, 1422, 22219, 11, 1184, 1422, 5523, 307], "temperature": 0.0, "avg_logprob": -0.13043585945578182, "compression_ratio": 1.68125, "no_speech_prob": 0.0001289178617298603}, {"id": 70, "seek": 49816, "start": 506.44, "end": 514.6, "text": " a replica 3, so when you create a directory, each of the directory in these 3 replica sets", "tokens": [257, 35456, 805, 11, 370, 562, 291, 1884, 257, 21120, 11, 1184, 295, 264, 21120, 294, 613, 805, 35456, 6352], "temperature": 0.0, "avg_logprob": -0.13043585945578182, "compression_ratio": 1.68125, "no_speech_prob": 0.0001289178617298603}, {"id": 71, "seek": 49816, "start": 514.6, "end": 523.5600000000001, "text": " will get a hash range and whenever you create a file or try to read a file, it will actually", "tokens": [486, 483, 257, 22019, 3613, 293, 5699, 291, 1884, 257, 3991, 420, 853, 281, 1401, 257, 3991, 11, 309, 486, 767], "temperature": 0.0, "avg_logprob": -0.13043585945578182, "compression_ratio": 1.68125, "no_speech_prob": 0.0001289178617298603}, {"id": 72, "seek": 52356, "start": 523.56, "end": 528.92, "text": " compute the hash of the name and it will figure out which of these directories in these 3", "tokens": [14722, 264, 22019, 295, 264, 1315, 293, 309, 486, 2573, 484, 597, 295, 613, 5391, 530, 294, 613, 805], "temperature": 0.0, "avg_logprob": -0.15471655746986127, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.00018492242088541389}, {"id": 73, "seek": 52356, "start": 528.92, "end": 533.8399999999999, "text": " sub volumes has that hash range and tries to get that file or store that file in that", "tokens": [1422, 22219, 575, 300, 22019, 3613, 293, 9898, 281, 483, 300, 3991, 420, 3531, 300, 3991, 294, 300], "temperature": 0.0, "avg_logprob": -0.15471655746986127, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.00018492242088541389}, {"id": 74, "seek": 52356, "start": 533.8399999999999, "end": 542.3599999999999, "text": " node, so for folks who are well versed with database, this is more like sharding but the", "tokens": [9984, 11, 370, 337, 4024, 567, 366, 731, 1774, 292, 365, 8149, 11, 341, 307, 544, 411, 402, 515, 278, 457, 264], "temperature": 0.0, "avg_logprob": -0.15471655746986127, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.00018492242088541389}, {"id": 75, "seek": 52356, "start": 542.3599999999999, "end": 547.9599999999999, "text": " entity here that is getting sharded is the directory based on the file names, alright,", "tokens": [13977, 510, 300, 307, 1242, 402, 22803, 307, 264, 21120, 2361, 322, 264, 3991, 5288, 11, 5845, 11], "temperature": 0.0, "avg_logprob": -0.15471655746986127, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.00018492242088541389}, {"id": 76, "seek": 54796, "start": 547.96, "end": 554.6, "text": " so the files actually can have varying sizes, for example in our setup, the minimum size", "tokens": [370, 264, 7098, 767, 393, 362, 22984, 11602, 11, 337, 1365, 294, 527, 8657, 11, 264, 7285, 2744], "temperature": 0.0, "avg_logprob": -0.18762518962224325, "compression_ratio": 1.6367713004484306, "no_speech_prob": 8.666540088597685e-05}, {"id": 77, "seek": 54796, "start": 554.6, "end": 560.5600000000001, "text": " would be less than a kb but the maximum size is like 26gb, so you will run into this problem", "tokens": [576, 312, 1570, 813, 257, 350, 65, 457, 264, 6674, 2744, 307, 411, 7551, 70, 65, 11, 370, 291, 486, 1190, 666, 341, 1154], "temperature": 0.0, "avg_logprob": -0.18762518962224325, "compression_ratio": 1.6367713004484306, "no_speech_prob": 8.666540088597685e-05}, {"id": 78, "seek": 54796, "start": 560.5600000000001, "end": 566.84, "text": " where some of the shards or distributes of volumes that you have would fill up the space", "tokens": [689, 512, 295, 264, 402, 2287, 420, 4400, 1819, 295, 22219, 300, 291, 362, 576, 2836, 493, 264, 1901], "temperature": 0.0, "avg_logprob": -0.18762518962224325, "compression_ratio": 1.6367713004484306, "no_speech_prob": 8.666540088597685e-05}, {"id": 79, "seek": 54796, "start": 566.84, "end": 572.64, "text": " before the others, so you need to handle that part as well, so there is a feature in Glouceref", "tokens": [949, 264, 2357, 11, 370, 291, 643, 281, 4813, 300, 644, 382, 731, 11, 370, 456, 307, 257, 4111, 294, 5209, 263, 66, 323, 69], "temperature": 0.0, "avg_logprob": -0.18762518962224325, "compression_ratio": 1.6367713004484306, "no_speech_prob": 8.666540088597685e-05}, {"id": 80, "seek": 57264, "start": 572.64, "end": 578.1999999999999, "text": " is called min-free disk where if you hit that level, when you create the directory again,", "tokens": [307, 1219, 923, 12, 10792, 12355, 689, 498, 291, 2045, 300, 1496, 11, 562, 291, 1884, 264, 21120, 797, 11], "temperature": 0.0, "avg_logprob": -0.16782936417912864, "compression_ratio": 1.7438423645320198, "no_speech_prob": 9.753098856890574e-05}, {"id": 81, "seek": 57264, "start": 578.1999999999999, "end": 582.76, "text": " the hash range will not be allocated for the ones that met the threshold, so for example", "tokens": [264, 22019, 3613, 486, 406, 312, 29772, 337, 264, 2306, 300, 1131, 264, 14678, 11, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.16782936417912864, "compression_ratio": 1.7438423645320198, "no_speech_prob": 9.753098856890574e-05}, {"id": 82, "seek": 57264, "start": 582.76, "end": 589.04, "text": " here, even though there are 3 distribute sub volumes, data is going to only 2 because", "tokens": [510, 11, 754, 1673, 456, 366, 805, 20594, 1422, 22219, 11, 1412, 307, 516, 281, 787, 568, 570], "temperature": 0.0, "avg_logprob": -0.16782936417912864, "compression_ratio": 1.7438423645320198, "no_speech_prob": 9.753098856890574e-05}, {"id": 83, "seek": 57264, "start": 589.04, "end": 596.92, "text": " the middle one actually has met the threshold, so the hash range will only be distributed", "tokens": [264, 2808, 472, 767, 575, 1131, 264, 14678, 11, 370, 264, 22019, 3613, 486, 787, 312, 12631], "temperature": 0.0, "avg_logprob": -0.16782936417912864, "compression_ratio": 1.7438423645320198, "no_speech_prob": 9.753098856890574e-05}, {"id": 84, "seek": 59692, "start": 596.92, "end": 604.4, "text": " between the 2, 50% and 50% instead of one third that you would expect normally, so let's", "tokens": [1296, 264, 568, 11, 2625, 4, 293, 2625, 4, 2602, 295, 472, 2636, 300, 291, 576, 2066, 5646, 11, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.1696252198976891, "compression_ratio": 1.7490196078431373, "no_speech_prob": 7.959530194057152e-05}, {"id": 85, "seek": 59692, "start": 604.4, "end": 611.0, "text": " talk about the actual process of increasing the capacity and why it didn't work for us,", "tokens": [751, 466, 264, 3539, 1399, 295, 5662, 264, 6042, 293, 983, 309, 994, 380, 589, 337, 505, 11], "temperature": 0.0, "avg_logprob": -0.1696252198976891, "compression_ratio": 1.7490196078431373, "no_speech_prob": 7.959530194057152e-05}, {"id": 86, "seek": 59692, "start": 611.0, "end": 614.88, "text": " when you want to increase the capacity that is you bring in more distributes of volumes", "tokens": [562, 291, 528, 281, 3488, 264, 6042, 300, 307, 291, 1565, 294, 544, 4400, 1819, 295, 22219], "temperature": 0.0, "avg_logprob": -0.1696252198976891, "compression_ratio": 1.7490196078431373, "no_speech_prob": 7.959530194057152e-05}, {"id": 87, "seek": 59692, "start": 614.88, "end": 622.12, "text": " or shards, the way that you do it is you first you do something called as cluster peer probe,", "tokens": [420, 402, 2287, 11, 264, 636, 300, 291, 360, 309, 307, 291, 700, 291, 360, 746, 1219, 382, 13630, 15108, 22715, 11], "temperature": 0.0, "avg_logprob": -0.1696252198976891, "compression_ratio": 1.7490196078431373, "no_speech_prob": 7.959530194057152e-05}, {"id": 88, "seek": 59692, "start": 622.12, "end": 626.36, "text": " that will bring the new machines into the cluster, then you do another operation called", "tokens": [300, 486, 1565, 264, 777, 8379, 666, 264, 13630, 11, 550, 291, 360, 1071, 6916, 1219], "temperature": 0.0, "avg_logprob": -0.1696252198976891, "compression_ratio": 1.7490196078431373, "no_speech_prob": 7.959530194057152e-05}, {"id": 89, "seek": 62636, "start": 626.36, "end": 632.4, "text": " add brick that will add the bricks to your volume, then you have to do something called", "tokens": [909, 16725, 300, 486, 909, 264, 25497, 281, 428, 5523, 11, 550, 291, 362, 281, 360, 746, 1219], "temperature": 0.0, "avg_logprob": -0.15123759998994715, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.00011924761201953515}, {"id": 90, "seek": 62636, "start": 632.4, "end": 641.08, "text": " as cluster volume rebalance to redistribute the data among the nodes equally, so what", "tokens": [382, 13630, 5523, 319, 29215, 281, 36198, 2024, 1169, 264, 1412, 3654, 264, 13891, 12309, 11, 370, 437], "temperature": 0.0, "avg_logprob": -0.15123759998994715, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.00011924761201953515}, {"id": 91, "seek": 62636, "start": 641.08, "end": 647.28, "text": " are the problems that we faced, when we did the benchmark, the rebalance had this application", "tokens": [366, 264, 2740, 300, 321, 11446, 11, 562, 321, 630, 264, 18927, 11, 264, 319, 29215, 632, 341, 3861], "temperature": 0.0, "avg_logprob": -0.15123759998994715, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.00011924761201953515}, {"id": 92, "seek": 62636, "start": 647.28, "end": 654.76, "text": " latency impact in some cases up to 25 seconds and as I mentioned most of the P99 latencies", "tokens": [27043, 2712, 294, 512, 3331, 493, 281, 3552, 3949, 293, 382, 286, 2835, 881, 295, 264, 430, 8494, 4465, 6464], "temperature": 0.0, "avg_logprob": -0.15123759998994715, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.00011924761201953515}, {"id": 93, "seek": 65476, "start": 654.76, "end": 660.48, "text": " were just in milliseconds, so this is this will be like a partial timeout partial outage", "tokens": [645, 445, 294, 34184, 11, 370, 341, 307, 341, 486, 312, 411, 257, 14641, 565, 346, 14641, 484, 609], "temperature": 0.0, "avg_logprob": -0.16576899181712756, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.00016741498257033527}, {"id": 94, "seek": 65476, "start": 660.48, "end": 666.88, "text": " for us, so this is not going to work for us, the other thing that we notice is for large", "tokens": [337, 505, 11, 370, 341, 307, 406, 516, 281, 589, 337, 505, 11, 264, 661, 551, 300, 321, 3449, 307, 337, 2416], "temperature": 0.0, "avg_logprob": -0.16576899181712756, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.00016741498257033527}, {"id": 95, "seek": 65476, "start": 666.88, "end": 674.08, "text": " volumes the rebalance may take up to months and at the moment cluster FS rebalance does", "tokens": [22219, 264, 319, 29215, 815, 747, 493, 281, 2493, 293, 412, 264, 1623, 13630, 41138, 319, 29215, 775], "temperature": 0.0, "avg_logprob": -0.16576899181712756, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.00016741498257033527}, {"id": 96, "seek": 65476, "start": 674.08, "end": 681.72, "text": " not have pause and resume, so we can't do the maintenance activity in off peak hours,", "tokens": [406, 362, 10465, 293, 15358, 11, 370, 321, 393, 380, 360, 264, 11258, 5191, 294, 766, 10651, 2496, 11], "temperature": 0.0, "avg_logprob": -0.16576899181712756, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.00016741498257033527}, {"id": 97, "seek": 68172, "start": 681.72, "end": 689.8000000000001, "text": " that is one more problem, the other one that we have seen is when you do the data migration", "tokens": [300, 307, 472, 544, 1154, 11, 264, 661, 472, 300, 321, 362, 1612, 307, 562, 291, 360, 264, 1412, 17011], "temperature": 0.0, "avg_logprob": -0.16955643822165095, "compression_ratio": 1.77, "no_speech_prob": 0.00028362241573631763}, {"id": 98, "seek": 68172, "start": 689.8000000000001, "end": 694.6800000000001, "text": " when it is going from one distribute sub volume or shard to two distribute sub volumes, you", "tokens": [562, 309, 307, 516, 490, 472, 20594, 1422, 5523, 420, 402, 515, 281, 732, 20594, 1422, 22219, 11, 291], "temperature": 0.0, "avg_logprob": -0.16955643822165095, "compression_ratio": 1.77, "no_speech_prob": 0.00028362241573631763}, {"id": 99, "seek": 68172, "start": 694.6800000000001, "end": 699.1600000000001, "text": " would expect 50 percent of the data to be transferred that's all right, but when you", "tokens": [576, 2066, 2625, 3043, 295, 264, 1412, 281, 312, 15809, 300, 311, 439, 558, 11, 457, 562, 291], "temperature": 0.0, "avg_logprob": -0.16955643822165095, "compression_ratio": 1.77, "no_speech_prob": 0.00028362241573631763}, {"id": 100, "seek": 68172, "start": 699.1600000000001, "end": 705.0, "text": " are going from 9 shards slash distributed sub volumes to 10, you want to only migrate", "tokens": [366, 516, 490, 1722, 402, 2287, 17330, 12631, 1422, 22219, 281, 1266, 11, 291, 528, 281, 787, 31821], "temperature": 0.0, "avg_logprob": -0.16955643822165095, "compression_ratio": 1.77, "no_speech_prob": 0.00028362241573631763}, {"id": 101, "seek": 70500, "start": 705.0, "end": 711.72, "text": " like 10 percent of the data, but less than FS is still like transferring about 30 percent", "tokens": [411, 1266, 3043, 295, 264, 1412, 11, 457, 1570, 813, 41138, 307, 920, 411, 31437, 466, 2217, 3043], "temperature": 0.0, "avg_logprob": -0.13268136442377326, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.7219230358023196e-05}, {"id": 102, "seek": 70500, "start": 711.72, "end": 720.84, "text": " to 40 percent like irrespective of what is the number of sub volumes are, so the rebalance", "tokens": [281, 3356, 3043, 411, 3418, 19575, 488, 295, 437, 307, 264, 1230, 295, 1422, 22219, 366, 11, 370, 264, 319, 29215], "temperature": 0.0, "avg_logprob": -0.13268136442377326, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.7219230358023196e-05}, {"id": 103, "seek": 70500, "start": 720.84, "end": 728.6, "text": " itself may take so much time with our workload that by the time we want to do the next capacity", "tokens": [2564, 815, 747, 370, 709, 565, 365, 527, 20139, 300, 538, 264, 565, 321, 528, 281, 360, 264, 958, 6042], "temperature": 0.0, "avg_logprob": -0.13268136442377326, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.7219230358023196e-05}, {"id": 104, "seek": 70500, "start": 728.6, "end": 733.48, "text": " expansion the rebalance may not even complete, so that is also not going to work for us,", "tokens": [11260, 264, 319, 29215, 815, 406, 754, 3566, 11, 370, 300, 307, 611, 406, 516, 281, 589, 337, 505, 11], "temperature": 0.0, "avg_logprob": -0.13268136442377326, "compression_ratio": 1.6441441441441442, "no_speech_prob": 1.7219230358023196e-05}, {"id": 105, "seek": 73348, "start": 733.48, "end": 739.8000000000001, "text": " so these are the three main problems that we have seen, so this is the solution that", "tokens": [370, 613, 366, 264, 1045, 2135, 2740, 300, 321, 362, 1612, 11, 370, 341, 307, 264, 3827, 300], "temperature": 0.0, "avg_logprob": -0.14789556902508402, "compression_ratio": 1.8031088082901554, "no_speech_prob": 0.00018455188546795398}, {"id": 106, "seek": 73348, "start": 739.8000000000001, "end": 746.04, "text": " we are using now, then there is a proposal as well, since we know that the hash range", "tokens": [321, 366, 1228, 586, 11, 550, 456, 307, 257, 11494, 382, 731, 11, 1670, 321, 458, 300, 264, 22019, 3613], "temperature": 0.0, "avg_logprob": -0.14789556902508402, "compression_ratio": 1.8031088082901554, "no_speech_prob": 0.00018455188546795398}, {"id": 107, "seek": 73348, "start": 746.04, "end": 752.12, "text": " allocation is based on the based on both the number of sub volumes and number of free sub", "tokens": [27599, 307, 2361, 322, 264, 2361, 322, 1293, 264, 1230, 295, 1422, 22219, 293, 1230, 295, 1737, 1422], "temperature": 0.0, "avg_logprob": -0.14789556902508402, "compression_ratio": 1.8031088082901554, "no_speech_prob": 0.00018455188546795398}, {"id": 108, "seek": 73348, "start": 752.12, "end": 758.5600000000001, "text": " volumes, what we are doing is in our doxor application every day in the night we create", "tokens": [22219, 11, 437, 321, 366, 884, 307, 294, 527, 360, 87, 284, 3861, 633, 786, 294, 264, 1818, 321, 1884], "temperature": 0.0, "avg_logprob": -0.14789556902508402, "compression_ratio": 1.8031088082901554, "no_speech_prob": 0.00018455188546795398}, {"id": 109, "seek": 75856, "start": 758.56, "end": 766.4, "text": " directories with a new basically, so the directory structure will be something like the namespace", "tokens": [5391, 530, 365, 257, 777, 1936, 11, 370, 264, 21120, 3877, 486, 312, 746, 411, 264, 5288, 17940], "temperature": 0.0, "avg_logprob": -0.0948542072659447, "compression_ratio": 1.7951219512195122, "no_speech_prob": 6.912138633197173e-05}, {"id": 110, "seek": 75856, "start": 766.4, "end": 773.1999999999999, "text": " that the clients are going to use slash year slash month slash day, so each day you are", "tokens": [300, 264, 6982, 366, 516, 281, 764, 17330, 1064, 17330, 1618, 17330, 786, 11, 370, 1184, 786, 291, 366], "temperature": 0.0, "avg_logprob": -0.0948542072659447, "compression_ratio": 1.7951219512195122, "no_speech_prob": 6.912138633197173e-05}, {"id": 111, "seek": 75856, "start": 773.1999999999999, "end": 779.4799999999999, "text": " going to create new directories, so based on the size that is available only the ones", "tokens": [516, 281, 1884, 777, 5391, 530, 11, 370, 2361, 322, 264, 2744, 300, 307, 2435, 787, 264, 2306], "temperature": 0.0, "avg_logprob": -0.0948542072659447, "compression_ratio": 1.7951219512195122, "no_speech_prob": 6.912138633197173e-05}, {"id": 112, "seek": 75856, "start": 779.4799999999999, "end": 785.7199999999999, "text": " that have space will get the hash range allocation, so you will never run into the problem where", "tokens": [300, 362, 1901, 486, 483, 264, 22019, 3613, 27599, 11, 370, 291, 486, 1128, 1190, 666, 264, 1154, 689], "temperature": 0.0, "avg_logprob": -0.0948542072659447, "compression_ratio": 1.7951219512195122, "no_speech_prob": 6.912138633197173e-05}, {"id": 113, "seek": 78572, "start": 785.72, "end": 790.24, "text": " you will have to do rebalance that much, because we have seen that with our workloads reads", "tokens": [291, 486, 362, 281, 360, 319, 29215, 300, 709, 11, 570, 321, 362, 1612, 300, 365, 527, 32452, 15700], "temperature": 0.0, "avg_logprob": -0.15789401253988578, "compression_ratio": 1.801980198019802, "no_speech_prob": 0.00014851086598355323}, {"id": 114, "seek": 78572, "start": 790.24, "end": 797.48, "text": " are distributed uniformly and as we have seen the it is read heavy workload and writes are", "tokens": [366, 12631, 48806, 293, 382, 321, 362, 1612, 264, 309, 307, 1401, 4676, 20139, 293, 13657, 366], "temperature": 0.0, "avg_logprob": -0.15789401253988578, "compression_ratio": 1.801980198019802, "no_speech_prob": 0.00014851086598355323}, {"id": 115, "seek": 78572, "start": 797.48, "end": 804.5600000000001, "text": " just a few, so we were okay with the solution in the interim, but long term the solution", "tokens": [445, 257, 1326, 11, 370, 321, 645, 1392, 365, 264, 3827, 294, 264, 33500, 11, 457, 938, 1433, 264, 3827], "temperature": 0.0, "avg_logprob": -0.15789401253988578, "compression_ratio": 1.801980198019802, "no_speech_prob": 0.00014851086598355323}, {"id": 116, "seek": 78572, "start": 804.5600000000001, "end": 810.08, "text": " that we are we have proposed and this is something that is yet to be accepted, but there are", "tokens": [300, 321, 366, 321, 362, 10348, 293, 341, 307, 746, 300, 307, 1939, 281, 312, 9035, 11, 457, 456, 366], "temperature": 0.0, "avg_logprob": -0.15789401253988578, "compression_ratio": 1.801980198019802, "no_speech_prob": 0.00014851086598355323}, {"id": 117, "seek": 81008, "start": 810.08, "end": 816.72, "text": " some POC that we did very few use jump consistent hash instead of the one that we have when", "tokens": [512, 22299, 34, 300, 321, 630, 588, 1326, 764, 3012, 8398, 22019, 2602, 295, 264, 472, 300, 321, 362, 562], "temperature": 0.0, "avg_logprob": -0.20320653915405273, "compression_ratio": 1.52, "no_speech_prob": 0.00029052994796074927}, {"id": 118, "seek": 81008, "start": 816.72, "end": 821.76, "text": " you are going from 9 to 10 here it is only about 10 percent that is getting rebalanced,", "tokens": [291, 366, 516, 490, 1722, 281, 1266, 510, 309, 307, 787, 466, 1266, 3043, 300, 307, 1242, 319, 40251, 11], "temperature": 0.0, "avg_logprob": -0.20320653915405273, "compression_ratio": 1.52, "no_speech_prob": 0.00029052994796074927}, {"id": 119, "seek": 81008, "start": 821.76, "end": 827.0400000000001, "text": " so that is what we want to get to this is something that we are focusing on this year,", "tokens": [370, 300, 307, 437, 321, 528, 281, 483, 281, 341, 307, 746, 300, 321, 366, 8416, 322, 341, 1064, 11], "temperature": 0.0, "avg_logprob": -0.20320653915405273, "compression_ratio": 1.52, "no_speech_prob": 0.00029052994796074927}, {"id": 120, "seek": 82704, "start": 827.04, "end": 840.28, "text": " alright over to you Sanju, so let us look at the problems that we have faced while migrating", "tokens": [5845, 670, 281, 291, 5271, 8954, 11, 370, 718, 505, 574, 412, 264, 2740, 300, 321, 362, 11446, 1339, 6186, 8754], "temperature": 0.0, "avg_logprob": -0.18278162819998606, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0002998934651259333}, {"id": 121, "seek": 82704, "start": 840.28, "end": 846.1999999999999, "text": " the data, so we had a use case where we wanted to move complete data which is present in", "tokens": [264, 1412, 11, 370, 321, 632, 257, 764, 1389, 689, 321, 1415, 281, 1286, 3566, 1412, 597, 307, 1974, 294], "temperature": 0.0, "avg_logprob": -0.18278162819998606, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0002998934651259333}, {"id": 122, "seek": 82704, "start": 846.1999999999999, "end": 854.28, "text": " one server to another server, so in clusterface the standard way of doing this is to use a", "tokens": [472, 7154, 281, 1071, 7154, 11, 370, 294, 13630, 69, 617, 264, 3832, 636, 295, 884, 341, 307, 281, 764, 257], "temperature": 0.0, "avg_logprob": -0.18278162819998606, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0002998934651259333}, {"id": 123, "seek": 85428, "start": 854.28, "end": 862.04, "text": " rebalance operation, sorry replace brick operation, so when you do replace brick operation there", "tokens": [319, 29215, 6916, 11, 2597, 7406, 16725, 6916, 11, 370, 562, 291, 360, 7406, 16725, 6916, 456], "temperature": 0.0, "avg_logprob": -0.19549233886017198, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0006125718355178833}, {"id": 124, "seek": 85428, "start": 862.04, "end": 867.6, "text": " is a process called a self filled demon which will copy all the data which is present in", "tokens": [307, 257, 1399, 1219, 257, 2698, 6412, 14283, 597, 486, 5055, 439, 264, 1412, 597, 307, 1974, 294], "temperature": 0.0, "avg_logprob": -0.19549233886017198, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0006125718355178833}, {"id": 125, "seek": 85428, "start": 867.6, "end": 878.04, "text": " the old server to new server, so to copy 10 TB data it takes around 2 to 3 weeks, so", "tokens": [264, 1331, 7154, 281, 777, 7154, 11, 370, 281, 5055, 1266, 29711, 1412, 309, 2516, 926, 568, 281, 805, 3259, 11, 370], "temperature": 0.0, "avg_logprob": -0.19549233886017198, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0006125718355178833}, {"id": 126, "seek": 85428, "start": 878.04, "end": 883.64, "text": " that is like a huge time we wanted to reduce this time so we came up with a new approach", "tokens": [300, 307, 411, 257, 2603, 565, 321, 1415, 281, 5407, 341, 565, 370, 321, 1361, 493, 365, 257, 777, 3109], "temperature": 0.0, "avg_logprob": -0.19549233886017198, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0006125718355178833}, {"id": 127, "seek": 88364, "start": 883.64, "end": 891.04, "text": " so let us understand few aspects of clusterface before we jump to the solution, so that we", "tokens": [370, 718, 505, 1223, 1326, 7270, 295, 13630, 69, 617, 949, 321, 3012, 281, 264, 3827, 11, 370, 300, 321], "temperature": 0.0, "avg_logprob": -0.22548543253252584, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.0004413153219502419}, {"id": 128, "seek": 88364, "start": 891.04, "end": 896.52, "text": " understand our approach better, so the right flow in clusterface is something like this", "tokens": [1223, 527, 3109, 1101, 11, 370, 264, 558, 3095, 294, 13630, 69, 617, 307, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.22548543253252584, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.0004413153219502419}, {"id": 129, "seek": 88364, "start": 896.52, "end": 903.68, "text": " whenever a right comes based on the hash range allocation plan is just spoke it will choose", "tokens": [5699, 257, 558, 1487, 2361, 322, 264, 22019, 3613, 27599, 1393, 307, 445, 7179, 309, 486, 2826], "temperature": 0.0, "avg_logprob": -0.22548543253252584, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.0004413153219502419}, {"id": 130, "seek": 90368, "start": 903.68, "end": 916.3199999999999, "text": " one of the sub volume, so the data will go to all the servers in that sub volume, now", "tokens": [472, 295, 264, 1422, 5523, 11, 370, 264, 1412, 486, 352, 281, 439, 264, 15909, 294, 300, 1422, 5523, 11, 586], "temperature": 0.0, "avg_logprob": -0.17139000823532324, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004879649495705962}, {"id": 131, "seek": 90368, "start": 916.3199999999999, "end": 926.16, "text": " let us say we have chosen replicas at 0 and the right will go to all the machines in", "tokens": [718, 505, 584, 321, 362, 8614, 3248, 9150, 412, 1958, 293, 264, 558, 486, 352, 281, 439, 264, 8379, 294], "temperature": 0.0, "avg_logprob": -0.17139000823532324, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004879649495705962}, {"id": 132, "seek": 90368, "start": 926.16, "end": 932.88, "text": " that sub volume, it is a client side replication so the client will send the right to all the", "tokens": [300, 1422, 5523, 11, 309, 307, 257, 6423, 1252, 39911, 370, 264, 6423, 486, 2845, 264, 558, 281, 439, 264], "temperature": 0.0, "avg_logprob": -0.17139000823532324, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004879649495705962}, {"id": 133, "seek": 93288, "start": 932.88, "end": 940.4399999999999, "text": " machines and it will wait for the success response to come, so client will assume the", "tokens": [8379, 293, 309, 486, 1699, 337, 264, 2245, 4134, 281, 808, 11, 370, 6423, 486, 6552, 264], "temperature": 0.0, "avg_logprob": -0.14930575444148136, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0007364825578406453}, {"id": 134, "seek": 93288, "start": 940.4399999999999, "end": 948.24, "text": " right is successful only when quorum number of success responses has come, let us say", "tokens": [558, 307, 4406, 787, 562, 421, 36543, 1230, 295, 2245, 13019, 575, 808, 11, 718, 505, 584], "temperature": 0.0, "avg_logprob": -0.14930575444148136, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0007364825578406453}, {"id": 135, "seek": 93288, "start": 948.24, "end": 955.6, "text": " one of the node is down, in our case we see like a server 2 either it can be a node down", "tokens": [472, 295, 264, 9984, 307, 760, 11, 294, 527, 1389, 321, 536, 411, 257, 7154, 568, 2139, 309, 393, 312, 257, 9984, 760], "temperature": 0.0, "avg_logprob": -0.14930575444148136, "compression_ratio": 1.6149068322981366, "no_speech_prob": 0.0007364825578406453}, {"id": 136, "seek": 95560, "start": 955.6, "end": 963.44, "text": " or the brick process is unhealthy this can be unresponsive at times, so something happened", "tokens": [420, 264, 16725, 1399, 307, 29147, 341, 393, 312, 517, 28930, 488, 412, 1413, 11, 370, 746, 2011], "temperature": 0.0, "avg_logprob": -0.1563014757065546, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0004566069401334971}, {"id": 137, "seek": 95560, "start": 963.44, "end": 968.6, "text": " the right came to one of the sub volume and it went to all the three replica servers,", "tokens": [264, 558, 1361, 281, 472, 295, 264, 1422, 5523, 293, 309, 1437, 281, 439, 264, 1045, 35456, 15909, 11], "temperature": 0.0, "avg_logprob": -0.1563014757065546, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0004566069401334971}, {"id": 138, "seek": 95560, "start": 968.6, "end": 975.84, "text": " but server 2 did not responded with the success response, now server 1 and server 3 has responded", "tokens": [457, 7154, 568, 630, 406, 15806, 365, 264, 2245, 4134, 11, 586, 7154, 502, 293, 7154, 805, 575, 15806], "temperature": 0.0, "avg_logprob": -0.1563014757065546, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0004566069401334971}, {"id": 139, "seek": 95560, "start": 975.84, "end": 982.44, "text": " with the success response, so client it assumes that the right is successful, now when the", "tokens": [365, 264, 2245, 4134, 11, 370, 6423, 309, 37808, 300, 264, 558, 307, 4406, 11, 586, 562, 264], "temperature": 0.0, "avg_logprob": -0.1563014757065546, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0004566069401334971}, {"id": 140, "seek": 98244, "start": 982.44, "end": 989.5200000000001, "text": " server 2 is back up we to have the consistency of the data server 2 should get the data which", "tokens": [7154, 568, 307, 646, 493, 321, 281, 362, 264, 14416, 295, 264, 1412, 7154, 568, 820, 483, 264, 1412, 597], "temperature": 0.0, "avg_logprob": -0.13978855660621156, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.0005846975254826248}, {"id": 141, "seek": 98244, "start": 989.5200000000001, "end": 996.96, "text": " it has missed while it was down, so who will take care of the job of doing this it is SHD,", "tokens": [309, 575, 6721, 1339, 309, 390, 760, 11, 370, 567, 486, 747, 1127, 295, 264, 1691, 295, 884, 341, 309, 307, 7405, 35, 11], "temperature": 0.0, "avg_logprob": -0.13978855660621156, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.0005846975254826248}, {"id": 142, "seek": 98244, "start": 996.96, "end": 1004.36, "text": " so SHD is a daemon process which will read the pending heal data like whatever the data", "tokens": [370, 7405, 35, 307, 257, 1120, 36228, 1399, 597, 486, 1401, 264, 32110, 10526, 1412, 411, 2035, 264, 1412], "temperature": 0.0, "avg_logprob": -0.13978855660621156, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.0005846975254826248}, {"id": 143, "seek": 98244, "start": 1004.36, "end": 1010.36, "text": " that was missing we call it as a pending heal, so it will read from one of the good copy", "tokens": [300, 390, 5361, 321, 818, 309, 382, 257, 32110, 10526, 11, 370, 309, 486, 1401, 490, 472, 295, 264, 665, 5055], "temperature": 0.0, "avg_logprob": -0.13978855660621156, "compression_ratio": 1.7524271844660195, "no_speech_prob": 0.0005846975254826248}, {"id": 144, "seek": 101036, "start": 1010.36, "end": 1017.12, "text": " in our case server 1 and server 3 are the good copies and server 2 is a bad copy, so", "tokens": [294, 527, 1389, 7154, 502, 293, 7154, 805, 366, 264, 665, 14341, 293, 7154, 568, 307, 257, 1578, 5055, 11, 370], "temperature": 0.0, "avg_logprob": -0.15532603630652794, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.0007702336297370493}, {"id": 145, "seek": 101036, "start": 1017.12, "end": 1024.64, "text": " SHD will read the data from one of the good copy and it will write to server 2, so server", "tokens": [7405, 35, 486, 1401, 264, 1412, 490, 472, 295, 264, 665, 5055, 293, 309, 486, 2464, 281, 7154, 568, 11, 370, 7154], "temperature": 0.0, "avg_logprob": -0.15532603630652794, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.0007702336297370493}, {"id": 146, "seek": 101036, "start": 1024.64, "end": 1032.04, "text": " 2 will have all the data once the self heal is completed healing the data, we will use", "tokens": [568, 486, 362, 439, 264, 1412, 1564, 264, 2698, 10526, 307, 7365, 9745, 264, 1412, 11, 321, 486, 764], "temperature": 0.0, "avg_logprob": -0.15532603630652794, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.0007702336297370493}, {"id": 147, "seek": 101036, "start": 1032.04, "end": 1039.76, "text": " this as part of our approach as well, our approach is we will kill the brick which we", "tokens": [341, 382, 644, 295, 527, 3109, 382, 731, 11, 527, 3109, 307, 321, 486, 1961, 264, 16725, 597, 321], "temperature": 0.0, "avg_logprob": -0.15532603630652794, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.0007702336297370493}, {"id": 148, "seek": 103976, "start": 1039.76, "end": 1048.08, "text": " want to migrate like we want to migrate from the server 3 to server 4, so we have to copy", "tokens": [528, 281, 31821, 411, 321, 528, 281, 31821, 490, 264, 7154, 805, 281, 7154, 1017, 11, 370, 321, 362, 281, 5055], "temperature": 0.0, "avg_logprob": -0.14122191228364644, "compression_ratio": 1.592814371257485, "no_speech_prob": 0.0002685773652046919}, {"id": 149, "seek": 103976, "start": 1048.08, "end": 1056.12, "text": " all the data right, so self heal is taking 2 to 3 weeks, here in our case we will kill", "tokens": [439, 264, 1412, 558, 11, 370, 2698, 10526, 307, 1940, 568, 281, 805, 3259, 11, 510, 294, 527, 1389, 321, 486, 1961], "temperature": 0.0, "avg_logprob": -0.14122191228364644, "compression_ratio": 1.592814371257485, "no_speech_prob": 0.0002685773652046919}, {"id": 150, "seek": 103976, "start": 1056.12, "end": 1064.92, "text": " the brick and we have a ZFS, we are using ZFS file system, so we will take a ZFS snapshot", "tokens": [264, 16725, 293, 321, 362, 257, 1176, 29318, 11, 321, 366, 1228, 1176, 29318, 3991, 1185, 11, 370, 321, 486, 747, 257, 1176, 29318, 30163], "temperature": 0.0, "avg_logprob": -0.14122191228364644, "compression_ratio": 1.592814371257485, "no_speech_prob": 0.0002685773652046919}, {"id": 151, "seek": 106492, "start": 1064.92, "end": 1071.04, "text": " and we will transfer this snapshot from the server 3 to server 4, it is like a old server", "tokens": [293, 321, 486, 5003, 341, 30163, 490, 264, 7154, 805, 281, 7154, 1017, 11, 309, 307, 411, 257, 1331, 7154], "temperature": 0.0, "avg_logprob": -0.16120151237205224, "compression_ratio": 2.063953488372093, "no_speech_prob": 0.0006070661474950612}, {"id": 152, "seek": 106492, "start": 1071.04, "end": 1077.76, "text": " to the new server and now we will perform the replace brick operation, while we are", "tokens": [281, 264, 777, 7154, 293, 586, 321, 486, 2042, 264, 7406, 16725, 6916, 11, 1339, 321, 366], "temperature": 0.0, "avg_logprob": -0.16120151237205224, "compression_ratio": 2.063953488372093, "no_speech_prob": 0.0006070661474950612}, {"id": 153, "seek": 106492, "start": 1077.76, "end": 1083.8400000000001, "text": " performing the replace brick operation server 4 that is a new server will already have all", "tokens": [10205, 264, 7406, 16725, 6916, 7154, 1017, 300, 307, 257, 777, 7154, 486, 1217, 362, 439], "temperature": 0.0, "avg_logprob": -0.16120151237205224, "compression_ratio": 2.063953488372093, "no_speech_prob": 0.0006070661474950612}, {"id": 154, "seek": 106492, "start": 1083.8400000000001, "end": 1092.16, "text": " the data which server 3 had, once the replace brick operation is performed server 4 is now", "tokens": [264, 1412, 597, 7154, 805, 632, 11, 1564, 264, 7406, 16725, 6916, 307, 10332, 7154, 1017, 307, 586], "temperature": 0.0, "avg_logprob": -0.16120151237205224, "compression_ratio": 2.063953488372093, "no_speech_prob": 0.0006070661474950612}, {"id": 155, "seek": 109216, "start": 1092.16, "end": 1100.24, "text": " part of the sub volume and the heals will take place from server 1 and server 2 to server", "tokens": [644, 295, 264, 1422, 5523, 293, 264, 45653, 486, 747, 1081, 490, 7154, 502, 293, 7154, 568, 281, 7154], "temperature": 0.0, "avg_logprob": -0.1588212303493334, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00018047877529170364}, {"id": 156, "seek": 109216, "start": 1100.24, "end": 1109.88, "text": " 4, so now we have reduced the amount of data that we are healing, previously we are copying", "tokens": [1017, 11, 370, 586, 321, 362, 9212, 264, 2372, 295, 1412, 300, 321, 366, 9745, 11, 8046, 321, 366, 27976], "temperature": 0.0, "avg_logprob": -0.1588212303493334, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00018047877529170364}, {"id": 157, "seek": 109216, "start": 1109.88, "end": 1116.5600000000002, "text": " all the data that is like a 10 TB of data from server 3 to server 4, but here in our", "tokens": [439, 264, 1412, 300, 307, 411, 257, 1266, 29711, 295, 1412, 490, 7154, 805, 281, 7154, 1017, 11, 457, 510, 294, 527], "temperature": 0.0, "avg_logprob": -0.1588212303493334, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00018047877529170364}, {"id": 158, "seek": 111656, "start": 1116.56, "end": 1124.44, "text": " case we are healing only the data which came after killing the brick before doing the rebalance", "tokens": [1389, 321, 366, 9745, 787, 264, 1412, 597, 1361, 934, 8011, 264, 16725, 949, 884, 264, 319, 29215], "temperature": 0.0, "avg_logprob": -0.12479221436285204, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.0003137284948024899}, {"id": 159, "seek": 111656, "start": 1124.44, "end": 1133.12, "text": " replace brick operation, so the data we heal is reduced hugely, with this approach now it", "tokens": [7406, 16725, 6916, 11, 370, 264, 1412, 321, 10526, 307, 9212, 27417, 11, 365, 341, 3109, 586, 309], "temperature": 0.0, "avg_logprob": -0.12479221436285204, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.0003137284948024899}, {"id": 160, "seek": 111656, "start": 1133.12, "end": 1140.12, "text": " is taking only 50 hours to complete this, that is also if we are using the spinning", "tokens": [307, 1940, 787, 2625, 2496, 281, 3566, 341, 11, 300, 307, 611, 498, 321, 366, 1228, 264, 15640], "temperature": 0.0, "avg_logprob": -0.12479221436285204, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.0003137284948024899}, {"id": 161, "seek": 114012, "start": 1140.12, "end": 1147.2399999999998, "text": " discs it will take 48 hours to transfer the snapshot of 10 TB and 2 hours for the healing", "tokens": [37525, 309, 486, 747, 11174, 2496, 281, 5003, 264, 30163, 295, 1266, 29711, 293, 568, 2496, 337, 264, 9745], "temperature": 0.0, "avg_logprob": -0.15015996020773184, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00025057789753191173}, {"id": 162, "seek": 114012, "start": 1147.2399999999998, "end": 1155.1599999999999, "text": " of data, but it is only 8 to 9 hours if we are using SSDs, if we are using SSD it takes", "tokens": [295, 1412, 11, 457, 309, 307, 787, 1649, 281, 1722, 2496, 498, 321, 366, 1228, 30262, 82, 11, 498, 321, 366, 1228, 30262, 309, 2516], "temperature": 0.0, "avg_logprob": -0.15015996020773184, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00025057789753191173}, {"id": 163, "seek": 114012, "start": 1155.1599999999999, "end": 1161.6799999999998, "text": " like a 8 hours to transfer the snapshot and it takes around 40 minutes to complete the", "tokens": [411, 257, 1649, 2496, 281, 5003, 264, 30163, 293, 309, 2516, 926, 3356, 2077, 281, 3566, 264], "temperature": 0.0, "avg_logprob": -0.15015996020773184, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00025057789753191173}, {"id": 164, "seek": 116168, "start": 1161.68, "end": 1170.5600000000002, "text": " heals, so that is like we came from 2 to 3 weeks to 1 or 2 days or 9 hours we can say,", "tokens": [45653, 11, 370, 300, 307, 411, 321, 1361, 490, 568, 281, 805, 3259, 281, 502, 420, 568, 1708, 420, 1722, 2496, 321, 393, 584, 11], "temperature": 0.0, "avg_logprob": -0.1619471888388357, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.00031595813925378025}, {"id": 165, "seek": 116168, "start": 1170.5600000000002, "end": 1176.48, "text": " we are using netcat utility, it gave us very good performance, it is like a 60% performance", "tokens": [321, 366, 1228, 2533, 18035, 14877, 11, 309, 2729, 505, 588, 665, 3389, 11, 309, 307, 411, 257, 4060, 4, 3389], "temperature": 0.0, "avg_logprob": -0.1619471888388357, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.00031595813925378025}, {"id": 166, "seek": 116168, "start": 1176.48, "end": 1183.5600000000002, "text": " optimization and we have in flight checksum at both the ends in the old server and also", "tokens": [19618, 293, 321, 362, 294, 7018, 13834, 449, 412, 1293, 264, 5314, 294, 264, 1331, 7154, 293, 611], "temperature": 0.0, "avg_logprob": -0.1619471888388357, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.00031595813925378025}, {"id": 167, "seek": 116168, "start": 1183.5600000000002, "end": 1189.76, "text": " in the new server, so that it is like we are checking whether we are transferring the snapshot", "tokens": [294, 264, 777, 7154, 11, 370, 300, 309, 307, 411, 321, 366, 8568, 1968, 321, 366, 31437, 264, 30163], "temperature": 0.0, "avg_logprob": -0.1619471888388357, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.00031595813925378025}, {"id": 168, "seek": 118976, "start": 1189.76, "end": 1199.68, "text": " perfectly or not, we are not using any data and yeah it is at the time, I have kept the", "tokens": [6239, 420, 406, 11, 321, 366, 406, 1228, 604, 1412, 293, 1338, 309, 307, 412, 264, 565, 11, 286, 362, 4305, 264], "temperature": 0.0, "avg_logprob": -0.17062717886532056, "compression_ratio": 1.7487437185929648, "no_speech_prob": 0.00019046537636313587}, {"id": 169, "seek": 118976, "start": 1199.68, "end": 1206.8, "text": " commands that we have exactly used in this link and we also have a rollback plan, so", "tokens": [16901, 300, 321, 362, 2293, 1143, 294, 341, 2113, 293, 321, 611, 362, 257, 3373, 3207, 1393, 11, 370], "temperature": 0.0, "avg_logprob": -0.17062717886532056, "compression_ratio": 1.7487437185929648, "no_speech_prob": 0.00019046537636313587}, {"id": 170, "seek": 118976, "start": 1206.8, "end": 1212.2, "text": " let us say that we have started with this activity but we have not performed the replace", "tokens": [718, 505, 584, 300, 321, 362, 1409, 365, 341, 5191, 457, 321, 362, 406, 10332, 264, 7406], "temperature": 0.0, "avg_logprob": -0.17062717886532056, "compression_ratio": 1.7487437185929648, "no_speech_prob": 0.00019046537636313587}, {"id": 171, "seek": 118976, "start": 1212.2, "end": 1217.48, "text": " brick yet, because once the replace brick is performed it will be something like this,", "tokens": [16725, 1939, 11, 570, 1564, 264, 7406, 16725, 307, 10332, 309, 486, 312, 746, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.17062717886532056, "compression_ratio": 1.7487437185929648, "no_speech_prob": 0.00019046537636313587}, {"id": 172, "seek": 121748, "start": 1217.48, "end": 1224.04, "text": " the sub volume will already have the server 4 as a part of it, before we perform the replace", "tokens": [264, 1422, 5523, 486, 1217, 362, 264, 7154, 1017, 382, 257, 644, 295, 309, 11, 949, 321, 2042, 264, 7406], "temperature": 0.0, "avg_logprob": -0.1284605938455333, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0001605840225238353}, {"id": 173, "seek": 121748, "start": 1224.04, "end": 1231.28, "text": " brick that means when we are here, we can we do not want to do this anymore, all we", "tokens": [16725, 300, 1355, 562, 321, 366, 510, 11, 321, 393, 321, 360, 406, 528, 281, 360, 341, 3602, 11, 439, 321], "temperature": 0.0, "avg_logprob": -0.1284605938455333, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0001605840225238353}, {"id": 174, "seek": 121748, "start": 1231.28, "end": 1237.28, "text": " need to do is start the volume with the force, so that the brick process that we have killed", "tokens": [643, 281, 360, 307, 722, 264, 5523, 365, 264, 3464, 11, 370, 300, 264, 16725, 1399, 300, 321, 362, 4652], "temperature": 0.0, "avg_logprob": -0.1284605938455333, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0001605840225238353}, {"id": 175, "seek": 121748, "start": 1237.28, "end": 1245.84, "text": " will come up, once it is up the good copies that we have SSD will copy the data from good", "tokens": [486, 808, 493, 11, 1564, 309, 307, 493, 264, 665, 14341, 300, 321, 362, 30262, 486, 5055, 264, 1412, 490, 665], "temperature": 0.0, "avg_logprob": -0.1284605938455333, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.0001605840225238353}, {"id": 176, "seek": 124584, "start": 1245.84, "end": 1252.52, "text": " copies to bad copy are the old server, so that we will have the consistent data across", "tokens": [14341, 281, 1578, 5055, 366, 264, 1331, 7154, 11, 370, 300, 321, 486, 362, 264, 8398, 1412, 2108], "temperature": 0.0, "avg_logprob": -0.19799233906304659, "compression_ratio": 1.6358024691358024, "no_speech_prob": 8.869434532243758e-05}, {"id": 177, "seek": 124584, "start": 1252.52, "end": 1259.84, "text": " all of our replicated servers, yeah that is so easy and we want to popularize this method", "tokens": [439, 295, 527, 46365, 15909, 11, 1338, 300, 307, 370, 1858, 293, 321, 528, 281, 3743, 1125, 341, 3170], "temperature": 0.0, "avg_logprob": -0.19799233906304659, "compression_ratio": 1.6358024691358024, "no_speech_prob": 8.869434532243758e-05}, {"id": 178, "seek": 124584, "start": 1259.84, "end": 1271.36, "text": " so that it helps the community, yeah over to Prenet, yeah so this we will now talk about", "tokens": [370, 300, 309, 3665, 264, 1768, 11, 1338, 670, 281, 430, 1095, 302, 11, 1338, 370, 341, 321, 486, 586, 751, 466], "temperature": 0.0, "avg_logprob": -0.19799233906304659, "compression_ratio": 1.6358024691358024, "no_speech_prob": 8.869434532243758e-05}, {"id": 179, "seek": 127136, "start": 1271.36, "end": 1277.1999999999998, "text": " the performance issues that we faced and how we solved them, this is the graph that we", "tokens": [264, 3389, 2663, 300, 321, 11446, 293, 577, 321, 13041, 552, 11, 341, 307, 264, 4295, 300, 321], "temperature": 0.0, "avg_logprob": -0.18011353436638328, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0001251165522262454}, {"id": 180, "seek": 127136, "start": 1277.1999999999998, "end": 1284.52, "text": " have seen in our prod setup, while doing this migration when something happened that we did", "tokens": [362, 1612, 294, 527, 15792, 8657, 11, 1339, 884, 341, 17011, 562, 746, 2011, 300, 321, 630], "temperature": 0.0, "avg_logprob": -0.18011353436638328, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0001251165522262454}, {"id": 181, "seek": 127136, "start": 1284.52, "end": 1290.8799999999999, "text": " not account for, so the latencies have shot up to 1 minute here and I have said that it", "tokens": [406, 2696, 337, 11, 370, 264, 4465, 6464, 362, 3347, 493, 281, 502, 3456, 510, 293, 286, 362, 848, 300, 309], "temperature": 0.0, "avg_logprob": -0.18011353436638328, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0001251165522262454}, {"id": 182, "seek": 127136, "start": 1290.8799999999999, "end": 1295.04, "text": " is supposed to be only milliseconds, so this is horrible, there was like 2 hours of partial", "tokens": [307, 3442, 281, 312, 787, 34184, 11, 370, 341, 307, 9263, 11, 456, 390, 411, 568, 2496, 295, 14641], "temperature": 0.0, "avg_logprob": -0.18011353436638328, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0001251165522262454}, {"id": 183, "seek": 129504, "start": 1295.04, "end": 1301.76, "text": " voltage because of this, so let us see how these things can be debugged and how they", "tokens": [8352, 570, 295, 341, 11, 370, 718, 505, 536, 577, 613, 721, 393, 312, 24083, 3004, 293, 577, 436], "temperature": 0.0, "avg_logprob": -0.165509521320302, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.944601722760126e-05}, {"id": 184, "seek": 129504, "start": 1301.76, "end": 1310.0, "text": " can be fixed, so we have a method called GlusterVolumeProfile in GlusterFS, so what", "tokens": [393, 312, 6806, 11, 370, 321, 362, 257, 3170, 1219, 5209, 8393, 53, 401, 2540, 43227, 794, 294, 5209, 8393, 29318, 11, 370, 437], "temperature": 0.0, "avg_logprob": -0.165509521320302, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.944601722760126e-05}, {"id": 185, "seek": 129504, "start": 1310.0, "end": 1315.84, "text": " you do is you start profiling on the volume, then you run your benchmark or whatever is", "tokens": [291, 360, 307, 291, 722, 1740, 4883, 322, 264, 5523, 11, 550, 291, 1190, 428, 18927, 420, 2035, 307], "temperature": 0.0, "avg_logprob": -0.165509521320302, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.944601722760126e-05}, {"id": 186, "seek": 129504, "start": 1315.84, "end": 1321.68, "text": " your workload, then you keep executing GlusterVolumeProfile in for incremental and it will keep", "tokens": [428, 20139, 11, 550, 291, 1066, 32368, 5209, 8393, 53, 401, 2540, 43227, 794, 294, 337, 35759, 293, 309, 486, 1066], "temperature": 0.0, "avg_logprob": -0.165509521320302, "compression_ratio": 1.7170731707317073, "no_speech_prob": 8.944601722760126e-05}, {"id": 187, "seek": 132168, "start": 1321.68, "end": 1327.96, "text": " giving you the stats of what is happening to the volume during that time, for each of", "tokens": [2902, 291, 264, 18152, 295, 437, 307, 2737, 281, 264, 5523, 1830, 300, 565, 11, 337, 1184, 295], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 188, "seek": 132168, "start": 1327.96, "end": 1332.3200000000002, "text": " the bricks that are there in the volume you will get an output like this, where for that", "tokens": [264, 25497, 300, 366, 456, 294, 264, 5523, 291, 486, 483, 364, 5598, 411, 341, 11, 689, 337, 300], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 189, "seek": 132168, "start": 1332.3200000000002, "end": 1336.92, "text": " interval in this case interval 9, for each of the block size you will see the number", "tokens": [15035, 294, 341, 1389, 15035, 1722, 11, 337, 1184, 295, 264, 3461, 2744, 291, 486, 536, 264, 1230], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 190, "seek": 132168, "start": 1336.92, "end": 1341.96, "text": " of reads and writes that came and for all of the internal file operations that you see", "tokens": [295, 15700, 293, 13657, 300, 1361, 293, 337, 439, 295, 264, 6920, 3991, 7705, 300, 291, 536], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 191, "seek": 132168, "start": 1341.96, "end": 1345.96, "text": " on the volume, you will get the number of calls and the latency distribution, min max", "tokens": [322, 264, 5523, 11, 291, 486, 483, 264, 1230, 295, 5498, 293, 264, 27043, 7316, 11, 923, 11469], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 192, "seek": 132168, "start": 1345.96, "end": 1350.76, "text": " average latency and what is the percentage latency that is taken by each of your file", "tokens": [4274, 27043, 293, 437, 307, 264, 9668, 27043, 300, 307, 2726, 538, 1184, 295, 428, 3991], "temperature": 0.0, "avg_logprob": -0.11274211406707764, "compression_ratio": 2.0637450199203187, "no_speech_prob": 0.00017097567615564913}, {"id": 193, "seek": 135076, "start": 1350.76, "end": 1352.44, "text": " operation internally.", "tokens": [6916, 19501, 13], "temperature": 0.0, "avg_logprob": -0.14814973213303256, "compression_ratio": 1.6285714285714286, "no_speech_prob": 5.553619848797098e-05}, {"id": 194, "seek": 135076, "start": 1352.44, "end": 1360.76, "text": " So, what we have seen when this ZFS issue happened is the lookup call is taking more", "tokens": [407, 11, 437, 321, 362, 1612, 562, 341, 1176, 29318, 2734, 2011, 307, 264, 574, 1010, 818, 307, 1940, 544], "temperature": 0.0, "avg_logprob": -0.14814973213303256, "compression_ratio": 1.6285714285714286, "no_speech_prob": 5.553619848797098e-05}, {"id": 195, "seek": 135076, "start": 1360.76, "end": 1367.84, "text": " than a second which is not what we generally see, so we knew something was happening during", "tokens": [813, 257, 1150, 597, 307, 406, 437, 321, 5101, 536, 11, 370, 321, 2586, 746, 390, 2737, 1830], "temperature": 0.0, "avg_logprob": -0.14814973213303256, "compression_ratio": 1.6285714285714286, "no_speech_prob": 5.553619848797098e-05}, {"id": 196, "seek": 135076, "start": 1367.84, "end": 1375.8799999999999, "text": " lookup operation, so we did an stress on the brick and we have found that there is one", "tokens": [574, 1010, 6916, 11, 370, 321, 630, 364, 4244, 322, 264, 16725, 293, 321, 362, 1352, 300, 456, 307, 472], "temperature": 0.0, "avg_logprob": -0.14814973213303256, "compression_ratio": 1.6285714285714286, "no_speech_prob": 5.553619848797098e-05}, {"id": 197, "seek": 137588, "start": 1375.88, "end": 1383.0400000000002, "text": " internal directory called GlusterFS indices XRTROP, to list three entries it is basically", "tokens": [6920, 21120, 1219, 5209, 8393, 29318, 43840, 1783, 49, 51, 7142, 47, 11, 281, 1329, 1045, 23041, 309, 307, 1936], "temperature": 0.0, "avg_logprob": -0.19556733395190948, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0209297619876452e-05}, {"id": 198, "seek": 137588, "start": 1383.0400000000002, "end": 1390.8000000000002, "text": " taking 0.35 seconds, so we so imagine this, so you do LS it will just show you three entries,", "tokens": [1940, 1958, 13, 8794, 3949, 11, 370, 321, 370, 3811, 341, 11, 370, 291, 360, 36657, 309, 486, 445, 855, 291, 1045, 23041, 11], "temperature": 0.0, "avg_logprob": -0.19556733395190948, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0209297619876452e-05}, {"id": 199, "seek": 137588, "start": 1390.8000000000002, "end": 1398.44, "text": " but it will take like 0.35 seconds sometimes it even takes a second, so we after looking", "tokens": [457, 309, 486, 747, 411, 1958, 13, 8794, 3949, 2171, 309, 754, 2516, 257, 1150, 11, 370, 321, 934, 1237], "temperature": 0.0, "avg_logprob": -0.19556733395190948, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0209297619876452e-05}, {"id": 200, "seek": 137588, "start": 1398.44, "end": 1403.4, "text": " at this we found that ZFS has this behavior where if you create a lot of files in one", "tokens": [412, 341, 321, 1352, 300, 1176, 29318, 575, 341, 5223, 689, 498, 291, 1884, 257, 688, 295, 7098, 294, 472], "temperature": 0.0, "avg_logprob": -0.19556733395190948, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0209297619876452e-05}, {"id": 201, "seek": 140340, "start": 1403.4, "end": 1409.44, "text": " directory like millions and then you delete most of them and then if you do LS it takes", "tokens": [21120, 411, 6803, 293, 550, 291, 12097, 881, 295, 552, 293, 550, 498, 291, 360, 36657, 309, 2516], "temperature": 0.0, "avg_logprob": -0.12911705304217594, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00011934340727748349}, {"id": 202, "seek": 140340, "start": 1409.44, "end": 1417.88, "text": " up to a second, so this bug is open for more than like two years I think, so we did not", "tokens": [493, 281, 257, 1150, 11, 370, 341, 7426, 307, 1269, 337, 544, 813, 411, 732, 924, 286, 519, 11, 370, 321, 630, 406], "temperature": 0.0, "avg_logprob": -0.12911705304217594, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00011934340727748349}, {"id": 203, "seek": 140340, "start": 1417.88, "end": 1423.8400000000001, "text": " know whether ZFS would fix this issue anytime soon, so in GlusterFS we patched it by caching", "tokens": [458, 1968, 1176, 29318, 576, 3191, 341, 2734, 13038, 2321, 11, 370, 294, 5209, 8393, 29318, 321, 9972, 292, 309, 538, 269, 2834], "temperature": 0.0, "avg_logprob": -0.12911705304217594, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00011934340727748349}, {"id": 204, "seek": 140340, "start": 1423.8400000000001, "end": 1428.5600000000002, "text": " this information, so that we do not have to keep doing this operation, so now you would", "tokens": [341, 1589, 11, 370, 300, 321, 360, 406, 362, 281, 1066, 884, 341, 6916, 11, 370, 586, 291, 576], "temperature": 0.0, "avg_logprob": -0.12911705304217594, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00011934340727748349}, {"id": 205, "seek": 142856, "start": 1428.56, "end": 1436.52, "text": " not see it if you are using any of the latest GlusterFS releases, but yeah this is one issue", "tokens": [406, 536, 309, 498, 291, 366, 1228, 604, 295, 264, 6792, 5209, 8393, 29318, 16952, 11, 457, 1338, 341, 307, 472, 2734], "temperature": 0.0, "avg_logprob": -0.08989921369050678, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00010221901175100356}, {"id": 206, "seek": 142856, "start": 1436.52, "end": 1439.28, "text": " that we found and fixed.", "tokens": [300, 321, 1352, 293, 6806, 13], "temperature": 0.0, "avg_logprob": -0.08989921369050678, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00010221901175100356}, {"id": 207, "seek": 142856, "start": 1439.28, "end": 1446.6399999999999, "text": " The second one is about increasing the RPS that we have on our volume, so the there was", "tokens": [440, 1150, 472, 307, 466, 5662, 264, 497, 6273, 300, 321, 362, 322, 527, 5523, 11, 370, 264, 456, 390], "temperature": 0.0, "avg_logprob": -0.08989921369050678, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00010221901175100356}, {"id": 208, "seek": 142856, "start": 1446.6399999999999, "end": 1453.1599999999999, "text": " a new application that was getting launched at the time and the RPS that they wanted was", "tokens": [257, 777, 3861, 300, 390, 1242, 8730, 412, 264, 565, 293, 264, 497, 6273, 300, 436, 1415, 390], "temperature": 0.0, "avg_logprob": -0.08989921369050678, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00010221901175100356}, {"id": 209, "seek": 145316, "start": 1453.16, "end": 1460.28, "text": " not what we are giving, so basically they wanted something like 300, 360 RPS or something", "tokens": [406, 437, 321, 366, 2902, 11, 370, 1936, 436, 1415, 746, 411, 6641, 11, 13898, 497, 6273, 420, 746], "temperature": 0.0, "avg_logprob": -0.1259788234582108, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.00017339222540613264}, {"id": 210, "seek": 145316, "start": 1460.28, "end": 1465.72, "text": " like that, but when we did the benchmark we were getting only like 250 RPS, so we wanted", "tokens": [411, 300, 11, 457, 562, 321, 630, 264, 18927, 321, 645, 1242, 787, 411, 11650, 497, 6273, 11, 370, 321, 1415], "temperature": 0.0, "avg_logprob": -0.1259788234582108, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.00017339222540613264}, {"id": 211, "seek": 145316, "start": 1465.72, "end": 1472.4, "text": " to figure out what is happening, so we ran benchmarks on Prod Gluster itself and we saw", "tokens": [281, 2573, 484, 437, 307, 2737, 11, 370, 321, 5872, 43751, 322, 1705, 67, 5209, 8393, 2564, 293, 321, 1866], "temperature": 0.0, "avg_logprob": -0.1259788234582108, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.00017339222540613264}, {"id": 212, "seek": 145316, "start": 1472.4, "end": 1482.0400000000002, "text": " that one of the threads is getting saturated, so there is a feature in GlusterFS called", "tokens": [300, 472, 295, 264, 19314, 307, 1242, 25408, 11, 370, 456, 307, 257, 4111, 294, 5209, 8393, 29318, 1219], "temperature": 0.0, "avg_logprob": -0.1259788234582108, "compression_ratio": 1.6542056074766356, "no_speech_prob": 0.00017339222540613264}, {"id": 213, "seek": 148204, "start": 1482.04, "end": 1488.76, "text": " client IO threads where multiple threads would take the responsibility of sending it over", "tokens": [6423, 39839, 19314, 689, 3866, 19314, 576, 747, 264, 6357, 295, 7750, 309, 670], "temperature": 0.0, "avg_logprob": -0.14659509314111915, "compression_ratio": 1.6372093023255814, "no_speech_prob": 6.10315146332141e-05}, {"id": 214, "seek": 148204, "start": 1488.76, "end": 1493.84, "text": " the network, so we thought let us just enable it and it would solve all our problems, we", "tokens": [264, 3209, 11, 370, 321, 1194, 718, 505, 445, 9528, 309, 293, 309, 576, 5039, 439, 527, 2740, 11, 321], "temperature": 0.0, "avg_logprob": -0.14659509314111915, "compression_ratio": 1.6372093023255814, "no_speech_prob": 6.10315146332141e-05}, {"id": 215, "seek": 148204, "start": 1493.84, "end": 1499.96, "text": " enabled it and it made it worse like from 250 it went down, so we realized that there", "tokens": [15172, 309, 293, 309, 1027, 309, 5324, 411, 490, 11650, 309, 1437, 760, 11, 370, 321, 5334, 300, 456], "temperature": 0.0, "avg_logprob": -0.14659509314111915, "compression_ratio": 1.6372093023255814, "no_speech_prob": 6.10315146332141e-05}, {"id": 216, "seek": 148204, "start": 1499.96, "end": 1505.76, "text": " is a continuation problem in the client side that we are yet to fix, so for now what we", "tokens": [307, 257, 29357, 1154, 294, 264, 6423, 1252, 300, 321, 366, 1939, 281, 3191, 11, 370, 337, 586, 437, 321], "temperature": 0.0, "avg_logprob": -0.14659509314111915, "compression_ratio": 1.6372093023255814, "no_speech_prob": 6.10315146332141e-05}, {"id": 217, "seek": 150576, "start": 1505.76, "end": 1512.8799999999999, "text": " did is to on the containers of Dockstore where it was doing only one mount, we are now doing", "tokens": [630, 307, 281, 322, 264, 17089, 295, 1144, 547, 21624, 689, 309, 390, 884, 787, 472, 3746, 11, 321, 366, 586, 884], "temperature": 0.0, "avg_logprob": -0.33184310368129183, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.156934538623318e-05}, {"id": 218, "seek": 150576, "start": 1512.8799999999999, "end": 1523.08, "text": " three mounts and distributing the uploads and downloads over yes, so can you repeat", "tokens": [1045, 40982, 293, 41406, 264, 48611, 293, 36553, 670, 2086, 11, 370, 393, 291, 7149], "temperature": 0.0, "avg_logprob": -0.33184310368129183, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.156934538623318e-05}, {"id": 219, "seek": 150576, "start": 1523.08, "end": 1533.08, "text": " the, oh yeah, no I didn't, it is a fuse mount, yeah the thread that is saturating is fuse", "tokens": [264, 11, 1954, 1338, 11, 572, 286, 994, 380, 11, 309, 307, 257, 31328, 3746, 11, 1338, 264, 7207, 300, 307, 21160, 990, 307, 31328], "temperature": 0.0, "avg_logprob": -0.33184310368129183, "compression_ratio": 1.6024096385542168, "no_speech_prob": 9.156934538623318e-05}, {"id": 220, "seek": 153308, "start": 1533.08, "end": 1541.24, "text": " thread, yeah so the question is which GlusterFS client we are using, the answer is fuse client", "tokens": [7207, 11, 1338, 370, 264, 1168, 307, 597, 5209, 8393, 29318, 6423, 321, 366, 1228, 11, 264, 1867, 307, 31328, 6423], "temperature": 0.0, "avg_logprob": -0.09589594144087571, "compression_ratio": 1.8661087866108788, "no_speech_prob": 7.012900459812954e-05}, {"id": 221, "seek": 153308, "start": 1541.24, "end": 1548.1999999999998, "text": " and the thread that is saturating is fuse thread, so what we are doing is we have created", "tokens": [293, 264, 7207, 300, 307, 21160, 990, 307, 31328, 7207, 11, 370, 437, 321, 366, 884, 307, 321, 362, 2942], "temperature": 0.0, "avg_logprob": -0.09589594144087571, "compression_ratio": 1.8661087866108788, "no_speech_prob": 7.012900459812954e-05}, {"id": 222, "seek": 153308, "start": 1548.1999999999998, "end": 1552.8, "text": " multiple mounts on the container and we are distributing the load in the application itself", "tokens": [3866, 40982, 322, 264, 10129, 293, 321, 366, 41406, 264, 3677, 294, 264, 3861, 2564], "temperature": 0.0, "avg_logprob": -0.09589594144087571, "compression_ratio": 1.8661087866108788, "no_speech_prob": 7.012900459812954e-05}, {"id": 223, "seek": 153308, "start": 1552.8, "end": 1557.72, "text": " like the uploads will go to all three and even downloads will go to all three, that", "tokens": [411, 264, 48611, 486, 352, 281, 439, 1045, 293, 754, 36553, 486, 352, 281, 439, 1045, 11, 300], "temperature": 0.0, "avg_logprob": -0.09589594144087571, "compression_ratio": 1.8661087866108788, "no_speech_prob": 7.012900459812954e-05}, {"id": 224, "seek": 153308, "start": 1557.72, "end": 1562.6799999999998, "text": " is one thing that we did to solve the CPU saturation problem, the other thing that we", "tokens": [307, 472, 551, 300, 321, 630, 281, 5039, 264, 13199, 27090, 1154, 11, 264, 661, 551, 300, 321], "temperature": 0.0, "avg_logprob": -0.09589594144087571, "compression_ratio": 1.8661087866108788, "no_speech_prob": 7.012900459812954e-05}, {"id": 225, "seek": 156268, "start": 1562.68, "end": 1567.8400000000001, "text": " noticed this is like part of the Gluster volume profile output where it will tell you for", "tokens": [5694, 341, 307, 411, 644, 295, 264, 5209, 8393, 5523, 7964, 5598, 689, 309, 486, 980, 291, 337], "temperature": 0.0, "avg_logprob": -0.12760086262479742, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.00010773443500511348}, {"id": 226, "seek": 156268, "start": 1567.8400000000001, "end": 1573.52, "text": " each block what is the number of reads and writes, we have seen that most of the writes", "tokens": [1184, 3461, 437, 307, 264, 1230, 295, 15700, 293, 13657, 11, 321, 362, 1612, 300, 881, 295, 264, 13657], "temperature": 0.0, "avg_logprob": -0.12760086262479742, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.00010773443500511348}, {"id": 227, "seek": 156268, "start": 1573.52, "end": 1580.88, "text": " are coming as 8KB, so later when we looked at the Java application Dockstore we saw that", "tokens": [366, 1348, 382, 1649, 42, 33, 11, 370, 1780, 562, 321, 2956, 412, 264, 10745, 3861, 1144, 547, 21624, 321, 1866, 300], "temperature": 0.0, "avg_logprob": -0.12760086262479742, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.00010773443500511348}, {"id": 228, "seek": 156268, "start": 1580.88, "end": 1588.24, "text": " the IO block that Java is using the default size is 8KB, so we just increased it to 128KB,", "tokens": [264, 39839, 3461, 300, 10745, 307, 1228, 264, 7576, 2744, 307, 1649, 42, 33, 11, 370, 321, 445, 6505, 309, 281, 29810, 42, 33, 11], "temperature": 0.0, "avg_logprob": -0.12760086262479742, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.00010773443500511348}, {"id": 229, "seek": 158824, "start": 1588.24, "end": 1595.92, "text": " so these two combined has given us 2X to 3X the number and we also increased the number", "tokens": [370, 613, 732, 9354, 575, 2212, 505, 568, 55, 281, 805, 55, 264, 1230, 293, 321, 611, 6505, 264, 1230], "temperature": 0.0, "avg_logprob": -0.19565047079057835, "compression_ratio": 1.5, "no_speech_prob": 0.00016276871610898525}, {"id": 230, "seek": 158824, "start": 1595.92, "end": 1603.4, "text": " of VMs that we are using to mount the client, so put all together we got something like", "tokens": [295, 18038, 82, 300, 321, 366, 1228, 281, 3746, 264, 6423, 11, 370, 829, 439, 1214, 321, 658, 746, 411], "temperature": 0.0, "avg_logprob": -0.19565047079057835, "compression_ratio": 1.5, "no_speech_prob": 0.00016276871610898525}, {"id": 231, "seek": 158824, "start": 1603.4, "end": 1610.72, "text": " 10X performance improvement compared to the earlier one, so we are set for maybe 2, 3", "tokens": [1266, 55, 3389, 10444, 5347, 281, 264, 3071, 472, 11, 370, 321, 366, 992, 337, 1310, 568, 11, 805], "temperature": 0.0, "avg_logprob": -0.19565047079057835, "compression_ratio": 1.5, "no_speech_prob": 0.00016276871610898525}, {"id": 232, "seek": 161072, "start": 1610.72, "end": 1622.96, "text": " KB all right, so let us now go on to health checks, so for any production cluster some", "tokens": [591, 33, 439, 558, 11, 370, 718, 505, 586, 352, 322, 281, 1585, 13834, 11, 370, 337, 604, 4265, 13630, 512], "temperature": 0.0, "avg_logprob": -0.23486493981402853, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.002092137234285474}, {"id": 233, "seek": 161072, "start": 1622.96, "end": 1627.16, "text": " of the health checks are needed, so I will talk about the minimal health checks that", "tokens": [295, 264, 1585, 13834, 366, 2978, 11, 370, 286, 486, 751, 466, 264, 13206, 1585, 13834, 300], "temperature": 0.0, "avg_logprob": -0.23486493981402853, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.002092137234285474}, {"id": 234, "seek": 161072, "start": 1627.16, "end": 1634.76, "text": " needed for GlusterFace cluster, so GlusterFace already provides POSIX health checks, so it", "tokens": [2978, 337, 5209, 8393, 37, 617, 13630, 11, 370, 5209, 8393, 37, 617, 1217, 6417, 430, 4367, 21124, 1585, 13834, 11, 370, 309], "temperature": 0.0, "avg_logprob": -0.23486493981402853, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.002092137234285474}, {"id": 235, "seek": 163476, "start": 1634.76, "end": 1642.96, "text": " is a health checker thread which will do a write of 1KB for every 15 or 30 minutes,", "tokens": [307, 257, 1585, 1520, 260, 7207, 597, 486, 360, 257, 2464, 295, 502, 42, 33, 337, 633, 2119, 420, 2217, 2077, 11], "temperature": 0.0, "avg_logprob": -0.10842222916452508, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0007352125830948353}, {"id": 236, "seek": 163476, "start": 1642.96, "end": 1649.96, "text": " I mean seconds, so there is one option to set the time interval in which you want to", "tokens": [286, 914, 3949, 11, 370, 456, 307, 472, 3614, 281, 992, 264, 565, 15035, 294, 597, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.10842222916452508, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0007352125830948353}, {"id": 237, "seek": 163476, "start": 1649.96, "end": 1656.56, "text": " do this, so if you set it as a 0 that means you are disabling the health check, so you", "tokens": [360, 341, 11, 370, 498, 291, 992, 309, 382, 257, 1958, 300, 1355, 291, 366, 717, 20112, 264, 1585, 1520, 11, 370, 291], "temperature": 0.0, "avg_logprob": -0.10842222916452508, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0007352125830948353}, {"id": 238, "seek": 163476, "start": 1656.56, "end": 1662.0, "text": " can set it as like a 10 seconds or something, so it sends a write and check if the disk", "tokens": [393, 992, 309, 382, 411, 257, 1266, 3949, 420, 746, 11, 370, 309, 14790, 257, 2464, 293, 1520, 498, 264, 12355], "temperature": 0.0, "avg_logprob": -0.10842222916452508, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0007352125830948353}, {"id": 239, "seek": 166200, "start": 1662.0, "end": 1668.84, "text": " is responsive enough and brick is healthy or not, if it did not get a response in a", "tokens": [307, 21826, 1547, 293, 16725, 307, 4627, 420, 406, 11, 498, 309, 630, 406, 483, 257, 4134, 294, 257], "temperature": 0.0, "avg_logprob": -0.11556205530276244, "compression_ratio": 1.8298969072164948, "no_speech_prob": 0.0005261253099888563}, {"id": 240, "seek": 166200, "start": 1668.84, "end": 1675.24, "text": " particular time, it will kill the brick process, so that like we will get to know that something", "tokens": [1729, 565, 11, 309, 486, 1961, 264, 16725, 1399, 11, 370, 300, 411, 321, 486, 483, 281, 458, 300, 746], "temperature": 0.0, "avg_logprob": -0.11556205530276244, "compression_ratio": 1.8298969072164948, "no_speech_prob": 0.0005261253099888563}, {"id": 241, "seek": 166200, "start": 1675.24, "end": 1683.44, "text": " is wrong with the brick process, so the other one we have is the rest of the things are", "tokens": [307, 2085, 365, 264, 16725, 1399, 11, 370, 264, 661, 472, 321, 362, 307, 264, 1472, 295, 264, 721, 366], "temperature": 0.0, "avg_logprob": -0.11556205530276244, "compression_ratio": 1.8298969072164948, "no_speech_prob": 0.0005261253099888563}, {"id": 242, "seek": 166200, "start": 1683.44, "end": 1689.4, "text": " we have a script and we have some config, these are the things we have kept externally", "tokens": [321, 362, 257, 5755, 293, 321, 362, 512, 6662, 11, 613, 366, 264, 721, 321, 362, 4305, 40899], "temperature": 0.0, "avg_logprob": -0.11556205530276244, "compression_ratio": 1.8298969072164948, "no_speech_prob": 0.0005261253099888563}, {"id": 243, "seek": 168940, "start": 1689.4, "end": 1694.76, "text": " kind of thing, the POSIX health checks are the one which come with the GlusterFace project,", "tokens": [733, 295, 551, 11, 264, 430, 4367, 21124, 1585, 13834, 366, 264, 472, 597, 808, 365, 264, 5209, 8393, 37, 617, 1716, 11], "temperature": 0.0, "avg_logprob": -0.1274222199634839, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0002980345452670008}, {"id": 244, "seek": 168940, "start": 1694.76, "end": 1700.92, "text": " so the cluster health checks that we have are like we have a config where we will specify", "tokens": [370, 264, 13630, 1585, 13834, 300, 321, 362, 366, 411, 321, 362, 257, 6662, 689, 321, 486, 16500], "temperature": 0.0, "avg_logprob": -0.1274222199634839, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0002980345452670008}, {"id": 245, "seek": 168940, "start": 1700.92, "end": 1706.16, "text": " number of nodes in the cluster, so that is like a expected number of nodes in the cluster", "tokens": [1230, 295, 13891, 294, 264, 13630, 11, 370, 300, 307, 411, 257, 5176, 1230, 295, 13891, 294, 264, 13630], "temperature": 0.0, "avg_logprob": -0.1274222199634839, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0002980345452670008}, {"id": 246, "seek": 168940, "start": 1706.16, "end": 1713.48, "text": " and using the Gluster peer status or GlusterPoorList command, we can check the number of nodes", "tokens": [293, 1228, 264, 5209, 8393, 15108, 6558, 420, 5209, 8393, 47, 28656, 43, 468, 5622, 11, 321, 393, 1520, 264, 1230, 295, 13891], "temperature": 0.0, "avg_logprob": -0.1274222199634839, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0002980345452670008}, {"id": 247, "seek": 171348, "start": 1713.48, "end": 1720.84, "text": " that are present in the cluster and we will check if both of them are equal, if not we", "tokens": [300, 366, 1974, 294, 264, 13630, 293, 321, 486, 1520, 498, 1293, 295, 552, 366, 2681, 11, 498, 406, 321], "temperature": 0.0, "avg_logprob": -0.13504180312156677, "compression_ratio": 1.65, "no_speech_prob": 0.0005618706927634776}, {"id": 248, "seek": 171348, "start": 1720.84, "end": 1728.24, "text": " will write an alert saying something unexpected is happening and we will also check whether", "tokens": [486, 2464, 364, 9615, 1566, 746, 13106, 307, 2737, 293, 321, 486, 611, 1520, 1968], "temperature": 0.0, "avg_logprob": -0.13504180312156677, "compression_ratio": 1.65, "no_speech_prob": 0.0005618706927634776}, {"id": 249, "seek": 171348, "start": 1728.24, "end": 1734.88, "text": " the node is in connected state or not, so in the GlusterFace cluster the nodes can be", "tokens": [264, 9984, 307, 294, 4582, 1785, 420, 406, 11, 370, 294, 264, 5209, 8393, 37, 617, 13630, 264, 13891, 393, 312], "temperature": 0.0, "avg_logprob": -0.13504180312156677, "compression_ratio": 1.65, "no_speech_prob": 0.0005618706927634776}, {"id": 250, "seek": 173488, "start": 1734.88, "end": 1743.5600000000002, "text": " in different state, so it can be connected or rejected or disconnected based on how the", "tokens": [294, 819, 1785, 11, 370, 309, 393, 312, 4582, 420, 15749, 420, 29426, 2361, 322, 577, 264], "temperature": 0.0, "avg_logprob": -0.14324196179707846, "compression_ratio": 1.8864864864864865, "no_speech_prob": 0.00010762053716462106}, {"id": 251, "seek": 173488, "start": 1743.5600000000002, "end": 1751.7600000000002, "text": " GlusterFace management daemon is working, so now we will see whether, so the expected", "tokens": [5209, 8393, 37, 617, 4592, 1120, 36228, 307, 1364, 11, 370, 586, 321, 486, 536, 1968, 11, 370, 264, 5176], "temperature": 0.0, "avg_logprob": -0.14324196179707846, "compression_ratio": 1.8864864864864865, "no_speech_prob": 0.00010762053716462106}, {"id": 252, "seek": 173488, "start": 1751.7600000000002, "end": 1755.8400000000001, "text": " is all the nodes should be in a connected state, we will check whether the nodes are", "tokens": [307, 439, 264, 13891, 820, 312, 294, 257, 4582, 1785, 11, 321, 486, 1520, 1968, 264, 13891, 366], "temperature": 0.0, "avg_logprob": -0.14324196179707846, "compression_ratio": 1.8864864864864865, "no_speech_prob": 0.00010762053716462106}, {"id": 253, "seek": 173488, "start": 1755.8400000000001, "end": 1761.5600000000002, "text": " connected or not, if the nodes are not connected then we will get an alert saying okay one", "tokens": [4582, 420, 406, 11, 498, 264, 13891, 366, 406, 4582, 550, 321, 486, 483, 364, 9615, 1566, 1392, 472], "temperature": 0.0, "avg_logprob": -0.14324196179707846, "compression_ratio": 1.8864864864864865, "no_speech_prob": 0.00010762053716462106}, {"id": 254, "seek": 176156, "start": 1761.56, "end": 1767.0, "text": " of your node is not in a connected state and we have some of the health checks for the", "tokens": [295, 428, 9984, 307, 406, 294, 257, 4582, 1785, 293, 321, 362, 512, 295, 264, 1585, 13834, 337, 264], "temperature": 0.0, "avg_logprob": -0.13308523495992025, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.0002769510028883815}, {"id": 255, "seek": 176156, "start": 1767.0, "end": 1773.72, "text": " BRICS as well, so we have number of BRICS that are present in each volume in the config", "tokens": [10262, 2532, 50, 382, 731, 11, 370, 321, 362, 1230, 295, 10262, 2532, 50, 300, 366, 1974, 294, 1184, 5523, 294, 264, 6662], "temperature": 0.0, "avg_logprob": -0.13308523495992025, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.0002769510028883815}, {"id": 256, "seek": 176156, "start": 1773.72, "end": 1779.12, "text": " and in the GlusterVolume info output you will get how many number of volumes that are present", "tokens": [293, 294, 264, 5209, 8393, 53, 401, 2540, 13614, 5598, 291, 486, 483, 577, 867, 1230, 295, 22219, 300, 366, 1974], "temperature": 0.0, "avg_logprob": -0.13308523495992025, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.0002769510028883815}, {"id": 257, "seek": 176156, "start": 1779.12, "end": 1784.48, "text": " in that volume and you will check if they are equal, the another check we have on the", "tokens": [294, 300, 5523, 293, 291, 486, 1520, 498, 436, 366, 2681, 11, 264, 1071, 1520, 321, 362, 322, 264], "temperature": 0.0, "avg_logprob": -0.13308523495992025, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.0002769510028883815}, {"id": 258, "seek": 176156, "start": 1784.48, "end": 1789.84, "text": " BRICS, if the BRICS is not online we will get to know it by checking the GlusterVolume", "tokens": [10262, 2532, 50, 11, 498, 264, 10262, 2532, 50, 307, 406, 2950, 321, 486, 483, 281, 458, 309, 538, 8568, 264, 5209, 8393, 53, 401, 2540], "temperature": 0.0, "avg_logprob": -0.13308523495992025, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.0002769510028883815}, {"id": 259, "seek": 178984, "start": 1789.84, "end": 1795.1999999999998, "text": " status command and if it is not online you will get an alert saying that one of your", "tokens": [6558, 5622, 293, 498, 309, 307, 406, 2950, 291, 486, 483, 364, 9615, 1566, 300, 472, 295, 428], "temperature": 0.0, "avg_logprob": -0.13946316553198773, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.00028089643456041813}, {"id": 260, "seek": 178984, "start": 1795.1999999999998, "end": 1803.04, "text": " BRICS is down and so whenever the server is down or the BRICS is down there will be some", "tokens": [10262, 2532, 50, 307, 760, 293, 370, 5699, 264, 7154, 307, 760, 420, 264, 10262, 2532, 50, 307, 760, 456, 486, 312, 512], "temperature": 0.0, "avg_logprob": -0.13946316553198773, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.00028089643456041813}, {"id": 261, "seek": 178984, "start": 1803.04, "end": 1809.3999999999999, "text": " of the pending heels and you can check the pending heels using the GlusterVolumeHealInfo", "tokens": [295, 264, 32110, 19502, 293, 291, 393, 1520, 264, 32110, 19502, 1228, 264, 5209, 8393, 53, 401, 2540, 5205, 304, 4575, 16931], "temperature": 0.0, "avg_logprob": -0.13946316553198773, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.00028089643456041813}, {"id": 262, "seek": 178984, "start": 1809.3999999999999, "end": 1816.6799999999998, "text": " command and if there are any pending heels you will see an entry, so if the entry is", "tokens": [5622, 293, 498, 456, 366, 604, 32110, 19502, 291, 486, 536, 364, 8729, 11, 370, 498, 264, 8729, 307], "temperature": 0.0, "avg_logprob": -0.13946316553198773, "compression_ratio": 1.8263157894736841, "no_speech_prob": 0.00028089643456041813}, {"id": 263, "seek": 181668, "start": 1816.68, "end": 1822.0, "text": " non-zero then you will get an alert saying that okay you have some pending heels in your", "tokens": [2107, 12, 32226, 550, 291, 486, 483, 364, 9615, 1566, 300, 1392, 291, 362, 512, 32110, 19502, 294, 428], "temperature": 0.0, "avg_logprob": -0.19596269654064644, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.0004292419762350619}, {"id": 264, "seek": 181668, "start": 1822.0, "end": 1827.68, "text": " cluster that means something unexpected, unwanted is going on that can be like a BRICS down", "tokens": [13630, 300, 1355, 746, 13106, 11, 33745, 307, 516, 322, 300, 393, 312, 411, 257, 10262, 2532, 50, 760], "temperature": 0.0, "avg_logprob": -0.19596269654064644, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.0004292419762350619}, {"id": 265, "seek": 181668, "start": 1827.68, "end": 1835.48, "text": " or node is down anything and we always lock profile info incremental to our debug locks", "tokens": [420, 9984, 307, 760, 1340, 293, 321, 1009, 4017, 7964, 13614, 35759, 281, 527, 24083, 20703], "temperature": 0.0, "avg_logprob": -0.19596269654064644, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.0004292419762350619}, {"id": 266, "seek": 181668, "start": 1835.48, "end": 1841.96, "text": " using the health check so that whenever we see some issue like the Prandit just spoke", "tokens": [1228, 264, 1585, 1520, 370, 300, 5699, 321, 536, 512, 2734, 411, 264, 430, 3699, 270, 445, 7179], "temperature": 0.0, "avg_logprob": -0.19596269654064644, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.0004292419762350619}, {"id": 267, "seek": 184196, "start": 1841.96, "end": 1847.68, "text": " about some of the issues that we can solve by looking at the profile info output, so", "tokens": [466, 512, 295, 264, 2663, 300, 321, 393, 5039, 538, 1237, 412, 264, 7964, 13614, 5598, 11, 370], "temperature": 0.0, "avg_logprob": -0.14355872926257907, "compression_ratio": 1.6352201257861636, "no_speech_prob": 0.00012676746700890362}, {"id": 268, "seek": 184196, "start": 1847.68, "end": 1854.96, "text": " in such cases this output will be helpful so we always log into our log backup servers", "tokens": [294, 1270, 3331, 341, 5598, 486, 312, 4961, 370, 321, 1009, 3565, 666, 527, 3565, 14807, 15909], "temperature": 0.0, "avg_logprob": -0.14355872926257907, "compression_ratio": 1.6352201257861636, "no_speech_prob": 0.00012676746700890362}, {"id": 269, "seek": 184196, "start": 1854.96, "end": 1866.44, "text": " and the exact commands that we are using are listed in this link, so we have some of the", "tokens": [293, 264, 1900, 16901, 300, 321, 366, 1228, 366, 10052, 294, 341, 2113, 11, 370, 321, 362, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.14355872926257907, "compression_ratio": 1.6352201257861636, "no_speech_prob": 0.00012676746700890362}, {"id": 270, "seek": 186644, "start": 1866.44, "end": 1875.24, "text": " maintenance activities so things can go back sometimes, so we have a replica 3 setup in", "tokens": [11258, 5354, 370, 721, 393, 352, 646, 2171, 11, 370, 321, 362, 257, 35456, 805, 8657, 294], "temperature": 0.0, "avg_logprob": -0.16971942782402039, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.0002448013983666897}, {"id": 271, "seek": 186644, "start": 1875.24, "end": 1881.88, "text": " our production, so at any point of time quorum number of BRICS process should be up so that", "tokens": [527, 4265, 11, 370, 412, 604, 935, 295, 565, 421, 36543, 1230, 295, 10262, 2532, 50, 1399, 820, 312, 493, 370, 300], "temperature": 0.0, "avg_logprob": -0.16971942782402039, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.0002448013983666897}, {"id": 272, "seek": 186644, "start": 1881.88, "end": 1890.6000000000001, "text": " the reads and writes can go on smoothly, so whenever we are doing something which might", "tokens": [264, 15700, 293, 13657, 393, 352, 322, 19565, 11, 370, 5699, 321, 366, 884, 746, 597, 1062], "temperature": 0.0, "avg_logprob": -0.16971942782402039, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.0002448013983666897}, {"id": 273, "seek": 189060, "start": 1890.6, "end": 1898.48, "text": " take some downtime of the BRICS process or which can have some load on particular server", "tokens": [747, 512, 49648, 295, 264, 10262, 2532, 50, 1399, 420, 597, 393, 362, 512, 3677, 322, 1729, 7154], "temperature": 0.0, "avg_logprob": -0.12898725687071336, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0007926043472252786}, {"id": 274, "seek": 189060, "start": 1898.48, "end": 1905.6, "text": " at that time we do it only on one of the server from each replica set so that even if that", "tokens": [412, 300, 565, 321, 360, 309, 787, 322, 472, 295, 264, 7154, 490, 1184, 35456, 992, 370, 300, 754, 498, 300], "temperature": 0.0, "avg_logprob": -0.12898725687071336, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0007926043472252786}, {"id": 275, "seek": 189060, "start": 1905.6, "end": 1911.36, "text": " server goes down or the BRICS process running on that server goes down we won't be having", "tokens": [7154, 1709, 760, 420, 264, 10262, 2532, 50, 1399, 2614, 322, 300, 7154, 1709, 760, 321, 1582, 380, 312, 1419], "temperature": 0.0, "avg_logprob": -0.12898725687071336, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0007926043472252786}, {"id": 276, "seek": 189060, "start": 1911.36, "end": 1918.84, "text": " an issue because there are two other replica servers which can like do all the reads and", "tokens": [364, 2734, 570, 456, 366, 732, 661, 35456, 15909, 597, 393, 411, 360, 439, 264, 15700, 293], "temperature": 0.0, "avg_logprob": -0.12898725687071336, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0007926043472252786}, {"id": 277, "seek": 191884, "start": 1918.84, "end": 1925.6, "text": " writes, so we are doing few activities in this way, one is ZFS scrubbing, ZFS scrubbing", "tokens": [13657, 11, 370, 321, 366, 884, 1326, 5354, 294, 341, 636, 11, 472, 307, 1176, 29318, 24163, 4324, 11, 1176, 29318, 24163, 4324], "temperature": 0.0, "avg_logprob": -0.1305099306879817, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0007501214859075844}, {"id": 278, "seek": 191884, "start": 1925.6, "end": 1933.28, "text": " is about doing the checksum of the data, it will see if the data is in a proper condition", "tokens": [307, 466, 884, 264, 13834, 449, 295, 264, 1412, 11, 309, 486, 536, 498, 264, 1412, 307, 294, 257, 2296, 4188], "temperature": 0.0, "avg_logprob": -0.1305099306879817, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0007501214859075844}, {"id": 279, "seek": 191884, "start": 1933.28, "end": 1942.8799999999999, "text": " or not and we do migrations in this way only, so we are doing it on one server from each", "tokens": [420, 406, 293, 321, 360, 6186, 12154, 294, 341, 636, 787, 11, 370, 321, 366, 884, 309, 322, 472, 7154, 490, 1184], "temperature": 0.0, "avg_logprob": -0.1305099306879817, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0007501214859075844}, {"id": 280, "seek": 194288, "start": 1942.88, "end": 1949.64, "text": " replica set so that even if it is down for some time or something didn't work out we", "tokens": [35456, 992, 370, 300, 754, 498, 309, 307, 760, 337, 512, 565, 420, 746, 994, 380, 589, 484, 321], "temperature": 0.0, "avg_logprob": -0.129301432905526, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00032664131140336394}, {"id": 281, "seek": 194288, "start": 1949.64, "end": 1958.5600000000002, "text": " are in a good place and upgrades also we will do in the same manner, we have done some contributions", "tokens": [366, 294, 257, 665, 1081, 293, 24868, 611, 321, 486, 360, 294, 264, 912, 9060, 11, 321, 362, 1096, 512, 15725], "temperature": 0.0, "avg_logprob": -0.129301432905526, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00032664131140336394}, {"id": 282, "seek": 194288, "start": 1958.5600000000002, "end": 1965.24, "text": " so the data migration part that I have spoke it's a production ready we have used it in", "tokens": [370, 264, 1412, 17011, 644, 300, 286, 362, 7179, 309, 311, 257, 4265, 1919, 321, 362, 1143, 309, 294], "temperature": 0.0, "avg_logprob": -0.129301432905526, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00032664131140336394}, {"id": 283, "seek": 194288, "start": 1965.24, "end": 1971.92, "text": " our production and Pranit has given some of the developer sessions which has many internals", "tokens": [527, 4265, 293, 2114, 282, 270, 575, 2212, 512, 295, 264, 10754, 11081, 597, 575, 867, 2154, 1124], "temperature": 0.0, "avg_logprob": -0.129301432905526, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00032664131140336394}, {"id": 284, "seek": 197192, "start": 1971.92, "end": 1977.8000000000002, "text": " of Glastrophase, they are very useful for any Glastrophase developers who wants to learn", "tokens": [295, 5209, 525, 11741, 651, 11, 436, 366, 588, 4420, 337, 604, 5209, 525, 11741, 651, 8849, 567, 2738, 281, 1466], "temperature": 0.0, "avg_logprob": -0.19180042824048674, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00041973876068368554}, {"id": 285, "seek": 197192, "start": 1977.8000000000002, "end": 1985.76, "text": " about many translators that we have in Glastrophase and recently we have fixed one of the single", "tokens": [466, 867, 5105, 3391, 300, 321, 362, 294, 5209, 525, 11741, 651, 293, 3938, 321, 362, 6806, 472, 295, 264, 2167], "temperature": 0.0, "avg_logprob": -0.19180042824048674, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00041973876068368554}, {"id": 286, "seek": 197192, "start": 1985.76, "end": 1991.72, "text": " point of failure which was present in the geo-replication feature, it was merged into", "tokens": [935, 295, 7763, 597, 390, 1974, 294, 264, 43198, 12, 265, 4770, 399, 4111, 11, 309, 390, 36427, 666], "temperature": 0.0, "avg_logprob": -0.19180042824048674, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00041973876068368554}, {"id": 287, "seek": 197192, "start": 1991.72, "end": 1998.92, "text": " the upstream very recently last week and this year we are looking at another thing the hashing", "tokens": [264, 33915, 588, 3938, 1036, 1243, 293, 341, 1064, 321, 366, 1237, 412, 1071, 551, 264, 575, 571], "temperature": 0.0, "avg_logprob": -0.19180042824048674, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00041973876068368554}, {"id": 288, "seek": 199892, "start": 1998.92, "end": 2006.3200000000002, "text": " strategy that Pranit has proposed, once it is accepted at the community we will take", "tokens": [5206, 300, 2114, 282, 270, 575, 10348, 11, 1564, 309, 307, 9035, 412, 264, 1768, 321, 486, 747], "temperature": 0.0, "avg_logprob": -0.21976321084158762, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003953847917728126}, {"id": 289, "seek": 199892, "start": 2006.3200000000002, "end": 2013.88, "text": " it and develop it, yeah that's all we had folks, thank you.", "tokens": [309, 293, 1499, 309, 11, 1338, 300, 311, 439, 321, 632, 4024, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.21976321084158762, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003953847917728126}, {"id": 290, "seek": 199892, "start": 2013.88, "end": 2020.52, "text": " Just want to let you guys know that the production ready thing, we actually migrated like in", "tokens": [1449, 528, 281, 718, 291, 1074, 458, 300, 264, 4265, 1919, 551, 11, 321, 767, 48329, 411, 294], "temperature": 0.0, "avg_logprob": -0.21976321084158762, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003953847917728126}, {"id": 291, "seek": 199892, "start": 2020.52, "end": 2027.3200000000002, "text": " total 375 TB using the method that Sanju talked about so it is ready, so yeah you guys can", "tokens": [3217, 805, 11901, 29711, 1228, 264, 3170, 300, 5271, 8954, 2825, 466, 370, 309, 307, 1919, 11, 370, 1338, 291, 1074, 393], "temperature": 0.0, "avg_logprob": -0.21976321084158762, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003953847917728126}, {"id": 292, "seek": 202732, "start": 2027.32, "end": 2032.0, "text": " use it, I think it should work even with butter, basically any file system that has a snapshot", "tokens": [764, 309, 11, 286, 519, 309, 820, 589, 754, 365, 5517, 11, 1936, 604, 3991, 1185, 300, 575, 257, 30163], "temperature": 0.0, "avg_logprob": -0.24628282629925272, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.0001998476218432188}, {"id": 293, "seek": 202732, "start": 2032.0, "end": 2043.4399999999998, "text": " feature it should work, yeah thank you guys, yeah I think we have a few minutes for questions", "tokens": [4111, 309, 820, 589, 11, 1338, 1309, 291, 1074, 11, 1338, 286, 519, 321, 362, 257, 1326, 2077, 337, 1651], "temperature": 0.0, "avg_logprob": -0.24628282629925272, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.0001998476218432188}, {"id": 294, "seek": 204344, "start": 2043.44, "end": 2058.2000000000003, "text": " if you have any otherwise you guys can catch us there, yeah so the question is how do you", "tokens": [498, 291, 362, 604, 5911, 291, 1074, 393, 3745, 505, 456, 11, 1338, 370, 264, 1168, 307, 577, 360, 291], "temperature": 0.0, "avg_logprob": -0.16104674706092248, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.818752106279135e-05}, {"id": 295, "seek": 204344, "start": 2058.2000000000003, "end": 2067.32, "text": " handle a disk failure, so basically the problem that I showed you where we had the ZFS issue", "tokens": [4813, 257, 12355, 7763, 11, 370, 1936, 264, 1154, 300, 286, 4712, 291, 689, 321, 632, 264, 1176, 29318, 2734], "temperature": 0.0, "avg_logprob": -0.16104674706092248, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.818752106279135e-05}, {"id": 296, "seek": 204344, "start": 2067.32, "end": 2072.68, "text": " where it was taking like minutes of latency that was the first time it happened on production", "tokens": [689, 309, 390, 1940, 411, 2077, 295, 27043, 300, 390, 264, 700, 565, 309, 2011, 322, 4265], "temperature": 0.0, "avg_logprob": -0.16104674706092248, "compression_ratio": 1.5164835164835164, "no_speech_prob": 5.818752106279135e-05}, {"id": 297, "seek": 207268, "start": 2072.68, "end": 2078.68, "text": " for us and initially we were waiting for the machine itself to be fixed so that it will", "tokens": [337, 505, 293, 9105, 321, 645, 3806, 337, 264, 3479, 2564, 281, 312, 6806, 370, 300, 309, 486], "temperature": 0.0, "avg_logprob": -0.09882260490866268, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0005506923771463335}, {"id": 298, "seek": 207268, "start": 2078.68, "end": 2084.8399999999997, "text": " come back again and it went for like a week or so and the amount of data that needed to", "tokens": [808, 646, 797, 293, 309, 1437, 337, 411, 257, 1243, 420, 370, 293, 264, 2372, 295, 1412, 300, 2978, 281], "temperature": 0.0, "avg_logprob": -0.09882260490866268, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0005506923771463335}, {"id": 299, "seek": 207268, "start": 2084.8399999999997, "end": 2094.72, "text": " be healed became too much that it coincided with our peak hours, so now the standard operating", "tokens": [312, 20482, 3062, 886, 709, 300, 309, 13001, 2112, 365, 527, 10651, 2496, 11, 370, 586, 264, 3832, 7447], "temperature": 0.0, "avg_logprob": -0.09882260490866268, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0005506923771463335}, {"id": 300, "seek": 207268, "start": 2094.72, "end": 2099.3999999999996, "text": " procedure that we have come up with after this issue is if a machine goes down or disk", "tokens": [10747, 300, 321, 362, 808, 493, 365, 934, 341, 2734, 307, 498, 257, 3479, 1709, 760, 420, 12355], "temperature": 0.0, "avg_logprob": -0.09882260490866268, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0005506923771463335}, {"id": 301, "seek": 209940, "start": 2099.4, "end": 2105.64, "text": " goes down we can just get it back online in 9 hours so why do we have to wait, so we just", "tokens": [1709, 760, 321, 393, 445, 483, 309, 646, 2950, 294, 1722, 2496, 370, 983, 360, 321, 362, 281, 1699, 11, 370, 321, 445], "temperature": 0.0, "avg_logprob": -0.16369742535530252, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0001533864124212414}, {"id": 302, "seek": 209940, "start": 2105.64, "end": 2111.64, "text": " consider that node dead, we get a new machine we do whatever Sanju mentioned using ZFS snapshot", "tokens": [1949, 300, 9984, 3116, 11, 321, 483, 257, 777, 3479, 321, 360, 2035, 5271, 8954, 2835, 1228, 1176, 29318, 30163], "temperature": 0.0, "avg_logprob": -0.16369742535530252, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0001533864124212414}, {"id": 303, "seek": 209940, "start": 2111.64, "end": 2120.88, "text": " migration and we just bring it up, so do you have the ZFS backup somewhere, do you have", "tokens": [17011, 293, 321, 445, 1565, 309, 493, 11, 370, 360, 291, 362, 264, 1176, 29318, 14807, 4079, 11, 360, 291, 362], "temperature": 0.0, "avg_logprob": -0.16369742535530252, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0001533864124212414}, {"id": 304, "seek": 209940, "start": 2120.88, "end": 2128.12, "text": " the ZFS backup somewhere, the answer is no you have the ZFS data on the active bricks", "tokens": [264, 1176, 29318, 14807, 4079, 11, 264, 1867, 307, 572, 291, 362, 264, 1176, 29318, 1412, 322, 264, 4967, 25497], "temperature": 0.0, "avg_logprob": -0.16369742535530252, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.0001533864124212414}, {"id": 305, "seek": 212812, "start": 2128.12, "end": 2133.24, "text": " so you take a snapshot on the active bricks and do the snapshot trend, yeah one of the", "tokens": [370, 291, 747, 257, 30163, 322, 264, 4967, 25497, 293, 360, 264, 30163, 6028, 11, 1338, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.3240545304095159, "compression_ratio": 1.5461538461538462, "no_speech_prob": 0.00021886920148972422}, {"id": 306, "seek": 212812, "start": 2133.24, "end": 2143.16, "text": " good ones yes, any other questions, I think that's it I think, thank you guys, thanks", "tokens": [665, 2306, 2086, 11, 604, 661, 1651, 11, 286, 519, 300, 311, 309, 286, 519, 11, 1309, 291, 1074, 11, 3231], "temperature": 0.0, "avg_logprob": -0.3240545304095159, "compression_ratio": 1.5461538461538462, "no_speech_prob": 0.00021886920148972422}, {"id": 307, "seek": 212812, "start": 2143.16, "end": 2144.16, "text": " a lot.", "tokens": [257, 688, 13], "temperature": 0.0, "avg_logprob": -0.3240545304095159, "compression_ratio": 1.5461538461538462, "no_speech_prob": 0.00021886920148972422}, {"id": 308, "seek": 212812, "start": 2144.16, "end": 2145.16, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3240545304095159, "compression_ratio": 1.5461538461538462, "no_speech_prob": 0.00021886920148972422}, {"id": 309, "seek": 212812, "start": 2145.16, "end": 2146.16, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3240545304095159, "compression_ratio": 1.5461538461538462, "no_speech_prob": 0.00021886920148972422}, {"id": 310, "seek": 214616, "start": 2146.16, "end": 2158.64, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 1.0, "avg_logprob": -1.1004437037876673, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0024766286369413137}], "language": "en"}