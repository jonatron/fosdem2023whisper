{"text": " Thank you for the opportunity to present our project, Namba MPI. Let me first acknowledge the co-authors. My name is Sylvester Arrabas and we are here with Olex Ibulenok and Kacper Darlatka from Jagiellonian University in Krak\u00f3w, Poland, Maciej Manna from the same university contributed to this project and we have also, we will be presenting some work from David Zwicker from Max Planck Institute for Dynamics and Self-Organisation in G\u00f6ttingen. So let's start with a maybe controversial, provocative question, Python and HPC. And let's try to look for answers to this question in a very respected journal, okay? So maybe you have some guesses what's written there. 2019, in scripting languages such as Python, users type code into an interactive editor line by line. It doesn't sound like HPC. Next year, level of computational performance that Python simply couldn't deliver. Same year, same journal, Namba runs on machines ranging from embedded devices to the world's largest supercomputers with performance approaching that of compiled languages. Same year, nature astronomy. Astronomers should avoid interpreted scripting languages such as Python. In principle, Namba and Nampa can lead to enormous increase in speed, but please reconsider teaching Python to university students. Same year, nature methods. Implementing new functionality into SciPy, Python is still the language of choice. Full test should pass with the PyPy just-in-time compiler as of 1.0 SciPy. Are they talking about the same language? No. The left-hand side are papers about Rust and Julia. The right-hand side are papers about Python. So maybe that's the reason. So just to set the stage, let me present, I think, a way that is apt for thinking about Python. So Python as a language lacks any support for multi-dimensional arrays or number crunching because it leaves it to packages to be handled. Python also leaves it to implementations to actually interpret its syntax. And SciPy, of course, the major, the main implementation, but it's not the only one and actually solutions exist that streamline, for example, just-in-time compilation of Python code. Moreover, Nampa, while de facto standard, is not the implementation of the Nampa API. And alternatives are embedded in just-in-time frameworks, just-in-time compilation frameworks, GPU frameworks for Python, and they leverage typing and concurrency. So probably here the highlight is that Python lets you glue these technologies together and package them together, leveraging some of the Python ecosystem and its popularity, et cetera. And probably, arguably, I would say that's an advantage. I'm not saying that please use Python for HPC instead of Julia. Probably vice versa, actually, but still it's an interesting question to see how it can perform. OK, so let's check it. I will present a brief benchmark, a very tiny one, that we have come up with in relation with this project, and it uses Nampa. Nampa is just-in-time compiler that translates a subset of Python and Nampa into machine code that is compiled at runtime using LLVM, OK? So here is the story about the super simple benchmark problem. It's related to a numerical weather prediction. So you can imagine a grid of, well, numbers representing weather here. And numerical weather prediction, or part of numerical weather prediction, the integration part involves solving equations for the hydrodynamics that is the transport of such pattern in space and time, and, of course, thermodynamics that tell you what's happening in the atmosphere. Super simplified picture. I'm not saying that's the whole story about NWP, but for benchmarking Nampa, let's simplify it down to, in this case, two-dimensional simple problem. You have a grid, x, y, some signal. And if we look at just the transport problem, a partial differential equation for transport, we can see what happens if we move around such signal, which could be some, I don't know, humidity, temperature, whatever, in the atmosphere, OK? So we have a sample problem. Here I'm showing results from a three-dimensional version of what was just shown. And let's start with the right-hand side plot, x-axis, the size of the grid. So if it's 8, it means 8 by 8, super tiny. If it's 128, it's 128 by 128 by 128, and wall time per time step on the y-axis, OK? Green. C++ implementation of one particular algorithm for this kind of problems, and orange, pi MP data, the same algorithm, numerically, but a Python implementation. So here you see that actually Namba, just compiled version outperformed C++, maintaining even better scaling for the tiny matrices, but they are kind of irrelevant for the problem. And please note that in both cases we have used multi-threading. So here on the left-hand side, you can see actually on the x-axis number of threads, y-axis wall time per time step. And again, the green line is the C++ implementation. These two are two variants of the Python 1.jit compiled with Namba, almost an order of magnitude a faster execution, five times faster. And what's probably most interesting for now is that when you compare with just setting the environment variable for Namba.jit to disabled, we jump more than two orders of magnitude up in wall time. So this is how Namba timing compares with plain Python timing. But there are two important things to be mentioned here. The Python package is written having Namba in mind, that is, everything is loop-based, which is the reason why plain C Python with Nampa performs badly. This line is kind of irrelevant, just as a curiosity. On the other hand, the C++ version is kind of legacy, it's based on Blitz++ library. Back then, when it was developed, IGN didn't have support for multiple dimensions. And it's object-oriented RIProcessing, which was reported and measured to be kind of five times slower than 4777 for these kind of small domains. It's not the same for larger domains. Anyhow, we can achieve high performance with Python. But what if we need MPI? We need message passing in our code. How would we use it? Let's say we divide in a domain that can position spirit our domain in two parts. So the same problem, same setup, just half of the domain is computed by one process or node or anything that has distributed, has different memory addressing than another work. So this is how we want to use it, why we want to use MPI? Well, because despite expansion in parallel computation, both in the number of machines and the number of cores, no other parallel programming paradigm has replaced MPI. At least as of 2013. And already in 2013, people were writing that this is, even though it's universally acknowledged that MPI is rather a crude way of programming these machines. Anyhow, still, let's try it. And let's try it with Python. So here is a seven-line snippet of code where we try to import Namba to get the jit compilation of Python code. And then we use MPI for pi, which is Python interface to MPI. What do we do? We define some number crunching routine, and we try to use MPI from it. And then we try to Njit. Njit means the highest performance variant of Namba jit compilation. We try to jit compile this function and straight ahead execute it. What happens? It doesn't work. It cannot compile because Namba cannot determine type of MPI for pi.mpi.intra.com because it's a class. Classes do not work with Namba, at least not the ordinary Python classes. So something doesn't work. So the problem is that we have Namba, which is one of the leading solutions to speed up Python. MPI, which is clearly the de facto standard for distributed memory parallelization. We try to work with them together, but it doesn't work. So stack overflow. Let's go it. Nothing. Let's quant it. Nothing. Wrong search engine, right? Someone must have solved the problem. Nothing. Let's ask Namba guys and MPI for pi guys. In 2020, you will not be able to use MPI for pi's siton code. It was not designed for such low-level usage. Well, okay, it's siton. But I mean, it must be doable, right? We have two established packages. The aim is kind of solid and makes sense. So it must be doable. And 30 months later, 120 comments later, 50 PR slater from five contributors on a totally unplanned site project, we are introducing Namba MPI. Namba MPI is an open source, kind of small Python project, which allows you to, let's jump here to the Hello World example, which allows you to use the Namba NGIT decorator on a function that involves rank, size, or any other MPI API calls within the Python code. As of now, we cover size rank, send, receive, or reduce broadcast barrier. The API for Namba MPI is based on NumPy. We have auto-generated documentation. We are on PyPy and Conda Forge. Few words about how it's implemented. Essentially we start with Ctypes built into Python to try to address the C API. There are some things related with passing addresses, memories, void pointers, et cetera, super interesting. Probably the key message here is that we are offering the send function that is already NGITed, which means that you can use it from other NGITed functions. We handle non-continuous arrays from NumPy, so we try to be user-friendly. We then call the underline C function, and kind of that's all. But really, there is the key line number 30. This one. Well, that's nothing but, in principle, without it, Namba optimizes out all our code. Anyhow, these are kind of things that you see when trying to implement such things. Unfortunately, there are quite more of such hacks inside Namba MPI. The next slide is kind of a thing that you prefer to never see, but they cannot be unseen, in a way, if you work with it. So please just think of it as a picture of some problems that we have challenged and essentially wrote to Namba guys asking how can it be done, and we got this kind of tools for handling void pointers from C types in Namba with Python, NumPy, et cetera. But well, that's utilspy, and that's it, and it kind of works, and why do we know it works? Because we test it, and let me handle the mic to Olexi to tell you more about testing. Okay, it's focused. So I'm going to tell you about the CI that we have set up for our project for Namba MPI. So the CI was set up at Github Actions, as I said, and this is the screen of the workflow. We start from running the PDoc, Precommit, and PyLint. PDoc is for automatic documentation generation, PyLint for static code analysis, and Precommit for styling. After that, if these steps were successfully moving to the main part where we run our unit tests, this is the example, not example, but actually the workflow file that we run. As you can see, when we run the CI against multiple systems, different Python versions and different MPI implementations, and here we should say a big thank you to MPI for PyTeam for providing set up MPI Github Action, because this has saved us a lot of time. So thank you, MPI for Py. And as of operation systems and MPI implementations, we are running, in case of Linux, we're testing against OpenMPI, MPICH, and Intel MPI, Mac OS, MPI, and MPICH, and in case of Windows, of course, MSMPI implementation. But when we are talking about MPICH, there is a problem that has recently occurred, namely starting from version 4 of MPICH, it fails for on Ubuntu on our CI for Python version less than 3.10. So if anyone has ideas how to fix it, please contact us, we will appreciate any help. Okay, so sample, we are running the unit tests on different systems and so on. Let's see the sample unit test. In this test, we are testing the logic of the wrapper of the broadcast function of MPI and the main thing that you should remember from this slide is that we are testing this function in plain Python implementation as well as Github compiled by Namba. We have also set up an integration test, the integration test is in another project named isopredoperate-les, and this is just a scheme of this test. We are starting from providing the initial conditions for the APDS solver, and these initial conditions are written to the HDF5 file. After that, we are running three runs, the first one we run with only one process, the second we have two processes, the third three processes, and in each we divide, well, in the first we don't divide the domain, but the other ones we divide the domain accordingly, and in the assert state we just compare the results and we want the results to be the same for different runs. And also these results are also written to HDF5 file. Interesting fact that everything works on Windows except installing HDF5 package for concurrent file access, HDF5 package was enabled in PIO, we have troubles setting up on Windows, but everything else works fine, and there is also an independent use case, the PyPD project that uses our library, our package, and it's not developed by us, so there is a user, and this is the Python package for solving partial differential equation, it focuses on finite differencing, and these are defined by, I provide it as strings, and the solution strategy as follows, we start from partitioning the grid onto different nodes using number MPI after add that with partial expressions using the SIMPY and compile the results using number, and then we trade the PDE exchange in boundary information between the nodes using number MPI. Take home messages, there is a common mismatch between the Python language and Python ecosystem, we should remember that the language could be slow, but we also should consider the ecosystem around this language, the libraries that are available, the libraries that are available, and probably different implementations, and Python has a range of global HPC solutions such as just-in-time compilation, GPU programming, multi-trading, and MPI, and in case of number MPI, this is the package to glue the MPI with LFMG compiled Python code, it is tested on CI, on GitHub Actions, we are aiming for 100% unit test coverage, and also there is also already the two projects that are dependent on this package, here you can find the links for number MPI, the GitHub links, and also the links to the packages at PyPy and Anaconda, and we also welcome contributions, the first two issues I have mentioned earlier, and we also welcome and encourage to provide the logo for number MPI, as well as adding support for the other functions, or we are also aiming for dropping dependency on MPI for Py in our project, and also the plan is to benchmark the performance of this package, and we also we wanted to acknowledge funding, the project was funded by National Science Centre of Poland, so thank you for your attention, and probably we now have time for questions. Thank you very much, any questions? Question from an MPI expert? Hello, thank you for the talk, so the interface you are proposing is very close to the let's say CMPI interface, let's say when you do a send you work with a buffer, or do you try to provide a bit higher level interface, for example, serializing some Python object, or it could be very useful. Yeah, the interface is as slim thin as possible probably, very close to the CMPI, one of the reasons being that within Namba and Jitted Code, probably things like serialization might not be that easy to do, there is no problem in combining MPI for Py and Namba MPI in one code base, so when you are out of the Jitted Code, you can use MPI for Py, which has high level things like serialization, et cetera, so you can use it there, but within LLVM compiled blocks, you can use Namba MPI for simple send, receive, already use, I mean, without higher level array functioning, having said that we, for example, handle transparently non-contiguous devices of arrays, we also, yeah, there are some things that are higher level than C interface, but in general, we try to provide wrapper around the C routines. Okay, thank you. Any other questions? Thanks for a great talk, it seems really interesting what you are working on, I have got a couple of questions, probably born out of ignorance, but I just kind of wondered if you could help me with them, so firstly, I was wondering why you went with making a separate package rather than sort of trying to build this functionality on top of MPI for Py, would it have been possible to sort of add this, add the feature of making things jit-compilable into MPI for Py, and secondly, I was kind of wondering with the MPI IO thing that you were looking at with Windows, if that requires kind of concurrent file access from separate processes in Windows, is that just a complete, completely a no-go for Windows, because I understand that's something that Windows kernel doesn't support. Thank you. Thanks, let me start from the second one. So here our, well, essentially it's a fun fact that everything else worked for Windows, we do not really target Windows, but it was nice to observe that all works, it's kind of one of these advantages of Python that you code and you don't really need to take too much care about the targeted platforms, because the underlying packages are meant to work on all of them, and here everything works with Microsoft MPI, the only thing that actually was a problem for us was to even install H5py on Windows with MPI support. So we don't really know what's the true bottleneck, but even the documentation of H5py suggests against trying. For the first question, why do we create, why do we develop a separate package instead of adding it on top of MPI 4Py? So I think even on the slide with the story of the package, there was a link to, yeah, there's a link to MPI 4Py issue, the bottom footnote, where we suggested would it be possible to add it, and in relation to the first question, so probably the scope, the goal of MPI 4Py is to provide very high level API for MPI in Python. So with discussing with the developers there, we realized that it's probably not within the scope of a very high level interface, so we started off with just, well, small separate project, but I mean, well, great idea, it could be glued together, as of now we aim for dropping dependency on MPI 4Py, which we now use just for some utility routine, not for the communication or nothing that is used by Namba, and probably that might be an advantage, because you can eventually you should be able to install Namba MPI with very little other dependencies, and Namba MPI is written purely in Python, so installing it, you do not need to have any Python related C-compiled code, and you can do it quite easily. Okay, thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.36, "text": " Thank you for the opportunity to present our project, Namba MPI.", "tokens": [50364, 1044, 291, 337, 264, 2650, 281, 1974, 527, 1716, 11, 426, 23337, 14146, 40, 13, 50882], "temperature": 0.0, "avg_logprob": -0.3189150855654762, "compression_ratio": 1.45703125, "no_speech_prob": 0.45846283435821533}, {"id": 1, "seek": 0, "start": 10.36, "end": 12.76, "text": " Let me first acknowledge the co-authors.", "tokens": [50882, 961, 385, 700, 10692, 264, 598, 12, 40198, 830, 13, 51002], "temperature": 0.0, "avg_logprob": -0.3189150855654762, "compression_ratio": 1.45703125, "no_speech_prob": 0.45846283435821533}, {"id": 2, "seek": 0, "start": 12.76, "end": 18.88, "text": " My name is Sylvester Arrabas and we are here with Olex Ibulenok and Kacper Darlatka from", "tokens": [51002, 1222, 1315, 307, 3902, 14574, 3011, 1587, 5305, 296, 293, 321, 366, 510, 365, 422, 2021, 286, 12176, 268, 453, 293, 591, 326, 610, 413, 6843, 267, 2330, 490, 51308], "temperature": 0.0, "avg_logprob": -0.3189150855654762, "compression_ratio": 1.45703125, "no_speech_prob": 0.45846283435821533}, {"id": 3, "seek": 0, "start": 18.88, "end": 24.2, "text": " Jagiellonian University in Krak\u00f3w, Poland, Maciej Manna from the same university contributed", "tokens": [51308, 9014, 32562, 43294, 3535, 294, 591, 11272, 3901, 11, 15950, 11, 5707, 7764, 2458, 629, 490, 264, 912, 5454, 18434, 51574], "temperature": 0.0, "avg_logprob": -0.3189150855654762, "compression_ratio": 1.45703125, "no_speech_prob": 0.45846283435821533}, {"id": 4, "seek": 0, "start": 24.2, "end": 28.68, "text": " to this project and we have also, we will be presenting some work from David Zwicker", "tokens": [51574, 281, 341, 1716, 293, 321, 362, 611, 11, 321, 486, 312, 15578, 512, 589, 490, 4389, 1176, 16038, 260, 51798], "temperature": 0.0, "avg_logprob": -0.3189150855654762, "compression_ratio": 1.45703125, "no_speech_prob": 0.45846283435821533}, {"id": 5, "seek": 2868, "start": 28.68, "end": 33.32, "text": " from Max Planck Institute for Dynamics and Self-Organisation in G\u00f6ttingen.", "tokens": [50364, 490, 7402, 8112, 547, 9446, 337, 22947, 1167, 293, 16348, 12, 21520, 1275, 7623, 294, 460, 12082, 783, 268, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2547408512660435, "compression_ratio": 1.3981042654028435, "no_speech_prob": 0.0689401626586914}, {"id": 6, "seek": 2868, "start": 33.32, "end": 40.96, "text": " So let's start with a maybe controversial, provocative question, Python and HPC.", "tokens": [50596, 407, 718, 311, 722, 365, 257, 1310, 17323, 11, 47663, 1168, 11, 15329, 293, 12557, 34, 13, 50978], "temperature": 0.0, "avg_logprob": -0.2547408512660435, "compression_ratio": 1.3981042654028435, "no_speech_prob": 0.0689401626586914}, {"id": 7, "seek": 2868, "start": 40.96, "end": 48.32, "text": " And let's try to look for answers to this question in a very respected journal, okay?", "tokens": [50978, 400, 718, 311, 853, 281, 574, 337, 6338, 281, 341, 1168, 294, 257, 588, 20020, 6708, 11, 1392, 30, 51346], "temperature": 0.0, "avg_logprob": -0.2547408512660435, "compression_ratio": 1.3981042654028435, "no_speech_prob": 0.0689401626586914}, {"id": 8, "seek": 2868, "start": 48.32, "end": 53.120000000000005, "text": " So maybe you have some guesses what's written there.", "tokens": [51346, 407, 1310, 291, 362, 512, 42703, 437, 311, 3720, 456, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2547408512660435, "compression_ratio": 1.3981042654028435, "no_speech_prob": 0.0689401626586914}, {"id": 9, "seek": 5312, "start": 53.12, "end": 59.72, "text": " 2019, in scripting languages such as Python, users type code into an interactive editor", "tokens": [50364, 6071, 11, 294, 5755, 278, 8650, 1270, 382, 15329, 11, 5022, 2010, 3089, 666, 364, 15141, 9839, 50694], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 10, "seek": 5312, "start": 59.72, "end": 60.839999999999996, "text": " line by line.", "tokens": [50694, 1622, 538, 1622, 13, 50750], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 11, "seek": 5312, "start": 60.839999999999996, "end": 63.4, "text": " It doesn't sound like HPC.", "tokens": [50750, 467, 1177, 380, 1626, 411, 12557, 34, 13, 50878], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 12, "seek": 5312, "start": 63.4, "end": 68.6, "text": " Next year, level of computational performance that Python simply couldn't deliver.", "tokens": [50878, 3087, 1064, 11, 1496, 295, 28270, 3389, 300, 15329, 2935, 2809, 380, 4239, 13, 51138], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 13, "seek": 5312, "start": 68.6, "end": 74.52, "text": " Same year, same journal, Namba runs on machines ranging from embedded devices to the world's", "tokens": [51138, 10635, 1064, 11, 912, 6708, 11, 426, 23337, 6676, 322, 8379, 25532, 490, 16741, 5759, 281, 264, 1002, 311, 51434], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 14, "seek": 5312, "start": 74.52, "end": 81.6, "text": " largest supercomputers with performance approaching that of compiled languages.", "tokens": [51434, 6443, 27839, 2582, 433, 365, 3389, 14908, 300, 295, 36548, 8650, 13, 51788], "temperature": 0.0, "avg_logprob": -0.25540820035067474, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.06650044769048691}, {"id": 15, "seek": 8160, "start": 81.6, "end": 83.83999999999999, "text": " Same year, nature astronomy.", "tokens": [50364, 10635, 1064, 11, 3687, 37844, 13, 50476], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 16, "seek": 8160, "start": 83.83999999999999, "end": 87.39999999999999, "text": " Astronomers should avoid interpreted scripting languages such as Python.", "tokens": [50476, 36819, 298, 433, 820, 5042, 26749, 5755, 278, 8650, 1270, 382, 15329, 13, 50654], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 17, "seek": 8160, "start": 87.39999999999999, "end": 92.47999999999999, "text": " In principle, Namba and Nampa can lead to enormous increase in speed, but please reconsider", "tokens": [50654, 682, 8665, 11, 426, 23337, 293, 426, 26625, 393, 1477, 281, 11322, 3488, 294, 3073, 11, 457, 1767, 40497, 50908], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 18, "seek": 8160, "start": 92.47999999999999, "end": 96.08, "text": " teaching Python to university students.", "tokens": [50908, 4571, 15329, 281, 5454, 1731, 13, 51088], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 19, "seek": 8160, "start": 96.08, "end": 98.32, "text": " Same year, nature methods.", "tokens": [51088, 10635, 1064, 11, 3687, 7150, 13, 51200], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 20, "seek": 8160, "start": 98.32, "end": 104.0, "text": " Implementing new functionality into SciPy, Python is still the language of choice.", "tokens": [51200, 4331, 43704, 278, 777, 14980, 666, 16942, 47, 88, 11, 15329, 307, 920, 264, 2856, 295, 3922, 13, 51484], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 21, "seek": 8160, "start": 104.0, "end": 110.0, "text": " Full test should pass with the PyPy just-in-time compiler as of 1.0 SciPy.", "tokens": [51484, 13841, 1500, 820, 1320, 365, 264, 9953, 47, 88, 445, 12, 259, 12, 3766, 31958, 382, 295, 502, 13, 15, 16942, 47, 88, 13, 51784], "temperature": 0.0, "avg_logprob": -0.18537466614334672, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.08976344764232635}, {"id": 22, "seek": 11000, "start": 110.0, "end": 112.6, "text": " Are they talking about the same language?", "tokens": [50364, 2014, 436, 1417, 466, 264, 912, 2856, 30, 50494], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 23, "seek": 11000, "start": 112.6, "end": 114.08, "text": " No.", "tokens": [50494, 883, 13, 50568], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 24, "seek": 11000, "start": 114.08, "end": 116.88, "text": " The left-hand side are papers about Rust and Julia.", "tokens": [50568, 440, 1411, 12, 5543, 1252, 366, 10577, 466, 34952, 293, 18551, 13, 50708], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 25, "seek": 11000, "start": 116.88, "end": 119.72, "text": " The right-hand side are papers about Python.", "tokens": [50708, 440, 558, 12, 5543, 1252, 366, 10577, 466, 15329, 13, 50850], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 26, "seek": 11000, "start": 119.72, "end": 121.88, "text": " So maybe that's the reason.", "tokens": [50850, 407, 1310, 300, 311, 264, 1778, 13, 50958], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 27, "seek": 11000, "start": 121.88, "end": 129.0, "text": " So just to set the stage, let me present, I think, a way that is apt for thinking about", "tokens": [50958, 407, 445, 281, 992, 264, 3233, 11, 718, 385, 1974, 11, 286, 519, 11, 257, 636, 300, 307, 29427, 337, 1953, 466, 51314], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 28, "seek": 11000, "start": 129.0, "end": 130.0, "text": " Python.", "tokens": [51314, 15329, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 29, "seek": 11000, "start": 130.0, "end": 136.76, "text": " So Python as a language lacks any support for multi-dimensional arrays or number crunching", "tokens": [51364, 407, 15329, 382, 257, 2856, 31132, 604, 1406, 337, 4825, 12, 18759, 41011, 420, 1230, 13386, 278, 51702], "temperature": 0.0, "avg_logprob": -0.19732991043402223, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.24200260639190674}, {"id": 30, "seek": 13676, "start": 136.76, "end": 141.67999999999998, "text": " because it leaves it to packages to be handled.", "tokens": [50364, 570, 309, 5510, 309, 281, 17401, 281, 312, 18033, 13, 50610], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 31, "seek": 13676, "start": 141.67999999999998, "end": 147.67999999999998, "text": " Python also leaves it to implementations to actually interpret its syntax.", "tokens": [50610, 15329, 611, 5510, 309, 281, 4445, 763, 281, 767, 7302, 1080, 28431, 13, 50910], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 32, "seek": 13676, "start": 147.67999999999998, "end": 152.14, "text": " And SciPy, of course, the major, the main implementation, but it's not the only one", "tokens": [50910, 400, 16942, 47, 88, 11, 295, 1164, 11, 264, 2563, 11, 264, 2135, 11420, 11, 457, 309, 311, 406, 264, 787, 472, 51133], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 33, "seek": 13676, "start": 152.14, "end": 157.64, "text": " and actually solutions exist that streamline, for example, just-in-time compilation of Python", "tokens": [51133, 293, 767, 6547, 2514, 300, 47141, 11, 337, 1365, 11, 445, 12, 259, 12, 3766, 40261, 295, 15329, 51408], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 34, "seek": 13676, "start": 157.64, "end": 158.64, "text": " code.", "tokens": [51408, 3089, 13, 51458], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 35, "seek": 13676, "start": 158.64, "end": 166.07999999999998, "text": " Moreover, Nampa, while de facto standard, is not the implementation of the Nampa API.", "tokens": [51458, 19838, 11, 426, 26625, 11, 1339, 368, 42225, 3832, 11, 307, 406, 264, 11420, 295, 264, 426, 26625, 9362, 13, 51830], "temperature": 0.0, "avg_logprob": -0.1688672075367937, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.009650452062487602}, {"id": 36, "seek": 16608, "start": 166.08, "end": 172.76000000000002, "text": " And alternatives are embedded in just-in-time frameworks, just-in-time compilation frameworks,", "tokens": [50364, 400, 20478, 366, 16741, 294, 445, 12, 259, 12, 3766, 29834, 11, 445, 12, 259, 12, 3766, 40261, 29834, 11, 50698], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 37, "seek": 16608, "start": 172.76000000000002, "end": 178.20000000000002, "text": " GPU frameworks for Python, and they leverage typing and concurrency.", "tokens": [50698, 18407, 29834, 337, 15329, 11, 293, 436, 13982, 18444, 293, 23702, 10457, 13, 50970], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 38, "seek": 16608, "start": 178.20000000000002, "end": 184.16000000000003, "text": " So probably here the highlight is that Python lets you glue these technologies together", "tokens": [50970, 407, 1391, 510, 264, 5078, 307, 300, 15329, 6653, 291, 8998, 613, 7943, 1214, 51268], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 39, "seek": 16608, "start": 184.16000000000003, "end": 189.60000000000002, "text": " and package them together, leveraging some of the Python ecosystem and its popularity,", "tokens": [51268, 293, 7372, 552, 1214, 11, 32666, 512, 295, 264, 15329, 11311, 293, 1080, 19301, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 40, "seek": 16608, "start": 189.60000000000002, "end": 191.8, "text": " et cetera.", "tokens": [51540, 1030, 11458, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 41, "seek": 16608, "start": 191.8, "end": 195.36, "text": " And probably, arguably, I would say that's an advantage.", "tokens": [51650, 400, 1391, 11, 26771, 11, 286, 576, 584, 300, 311, 364, 5002, 13, 51828], "temperature": 0.0, "avg_logprob": -0.1747831676317298, "compression_ratio": 1.6437246963562753, "no_speech_prob": 0.011419142596423626}, {"id": 42, "seek": 19536, "start": 195.36, "end": 199.8, "text": " I'm not saying that please use Python for HPC instead of Julia.", "tokens": [50364, 286, 478, 406, 1566, 300, 1767, 764, 15329, 337, 12557, 34, 2602, 295, 18551, 13, 50586], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 43, "seek": 19536, "start": 199.8, "end": 205.52, "text": " Probably vice versa, actually, but still it's an interesting question to see how it can", "tokens": [50586, 9210, 11964, 25650, 11, 767, 11, 457, 920, 309, 311, 364, 1880, 1168, 281, 536, 577, 309, 393, 50872], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 44, "seek": 19536, "start": 205.52, "end": 206.52, "text": " perform.", "tokens": [50872, 2042, 13, 50922], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 45, "seek": 19536, "start": 206.52, "end": 210.08, "text": " OK, so let's check it.", "tokens": [50922, 2264, 11, 370, 718, 311, 1520, 309, 13, 51100], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 46, "seek": 19536, "start": 210.08, "end": 216.32000000000002, "text": " I will present a brief benchmark, a very tiny one, that we have come up with in relation", "tokens": [51100, 286, 486, 1974, 257, 5353, 18927, 11, 257, 588, 5870, 472, 11, 300, 321, 362, 808, 493, 365, 294, 9721, 51412], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 47, "seek": 19536, "start": 216.32000000000002, "end": 219.08, "text": " with this project, and it uses Nampa.", "tokens": [51412, 365, 341, 1716, 11, 293, 309, 4960, 426, 26625, 13, 51550], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 48, "seek": 19536, "start": 219.08, "end": 224.8, "text": " Nampa is just-in-time compiler that translates a subset of Python and Nampa into machine", "tokens": [51550, 426, 26625, 307, 445, 12, 259, 12, 3766, 31958, 300, 28468, 257, 25993, 295, 15329, 293, 426, 26625, 666, 3479, 51836], "temperature": 0.0, "avg_logprob": -0.18234007949129158, "compression_ratio": 1.5346153846153847, "no_speech_prob": 0.001908632810227573}, {"id": 49, "seek": 22480, "start": 224.8, "end": 230.20000000000002, "text": " code that is compiled at runtime using LLVM, OK?", "tokens": [50364, 3089, 300, 307, 36548, 412, 34474, 1228, 441, 43, 53, 44, 11, 2264, 30, 50634], "temperature": 0.0, "avg_logprob": -0.17404758302788986, "compression_ratio": 1.671875, "no_speech_prob": 0.002312101423740387}, {"id": 50, "seek": 22480, "start": 230.20000000000002, "end": 235.56, "text": " So here is the story about the super simple benchmark problem.", "tokens": [50634, 407, 510, 307, 264, 1657, 466, 264, 1687, 2199, 18927, 1154, 13, 50902], "temperature": 0.0, "avg_logprob": -0.17404758302788986, "compression_ratio": 1.671875, "no_speech_prob": 0.002312101423740387}, {"id": 51, "seek": 22480, "start": 235.56, "end": 238.32000000000002, "text": " It's related to a numerical weather prediction.", "tokens": [50902, 467, 311, 4077, 281, 257, 29054, 5503, 17630, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17404758302788986, "compression_ratio": 1.671875, "no_speech_prob": 0.002312101423740387}, {"id": 52, "seek": 22480, "start": 238.32000000000002, "end": 245.64000000000001, "text": " So you can imagine a grid of, well, numbers representing weather here.", "tokens": [51040, 407, 291, 393, 3811, 257, 10748, 295, 11, 731, 11, 3547, 13460, 5503, 510, 13, 51406], "temperature": 0.0, "avg_logprob": -0.17404758302788986, "compression_ratio": 1.671875, "no_speech_prob": 0.002312101423740387}, {"id": 53, "seek": 22480, "start": 245.64000000000001, "end": 249.92000000000002, "text": " And numerical weather prediction, or part of numerical weather prediction, the integration", "tokens": [51406, 400, 29054, 5503, 17630, 11, 420, 644, 295, 29054, 5503, 17630, 11, 264, 10980, 51620], "temperature": 0.0, "avg_logprob": -0.17404758302788986, "compression_ratio": 1.671875, "no_speech_prob": 0.002312101423740387}, {"id": 54, "seek": 24992, "start": 249.92, "end": 257.12, "text": " part involves solving equations for the hydrodynamics that is the transport of such pattern in", "tokens": [50364, 644, 11626, 12606, 11787, 337, 264, 5796, 11452, 5216, 1167, 300, 307, 264, 5495, 295, 1270, 5102, 294, 50724], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 55, "seek": 24992, "start": 257.12, "end": 262.76, "text": " space and time, and, of course, thermodynamics that tell you what's happening in the atmosphere.", "tokens": [50724, 1901, 293, 565, 11, 293, 11, 295, 1164, 11, 8810, 35483, 300, 980, 291, 437, 311, 2737, 294, 264, 8018, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 56, "seek": 24992, "start": 262.76, "end": 263.96, "text": " Super simplified picture.", "tokens": [51006, 4548, 26335, 3036, 13, 51066], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 57, "seek": 24992, "start": 263.96, "end": 270.28, "text": " I'm not saying that's the whole story about NWP, but for benchmarking Nampa, let's simplify", "tokens": [51066, 286, 478, 406, 1566, 300, 311, 264, 1379, 1657, 466, 426, 54, 47, 11, 457, 337, 18927, 278, 426, 26625, 11, 718, 311, 20460, 51382], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 58, "seek": 24992, "start": 270.28, "end": 274.2, "text": " it down to, in this case, two-dimensional simple problem.", "tokens": [51382, 309, 760, 281, 11, 294, 341, 1389, 11, 732, 12, 18759, 2199, 1154, 13, 51578], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 59, "seek": 24992, "start": 274.2, "end": 277.52, "text": " You have a grid, x, y, some signal.", "tokens": [51578, 509, 362, 257, 10748, 11, 2031, 11, 288, 11, 512, 6358, 13, 51744], "temperature": 0.0, "avg_logprob": -0.14326847825094918, "compression_ratio": 1.562015503875969, "no_speech_prob": 0.01967032626271248}, {"id": 60, "seek": 27752, "start": 277.52, "end": 283.52, "text": " And if we look at just the transport problem, a partial differential equation for transport,", "tokens": [50364, 400, 498, 321, 574, 412, 445, 264, 5495, 1154, 11, 257, 14641, 15756, 5367, 337, 5495, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 61, "seek": 27752, "start": 283.52, "end": 289.0, "text": " we can see what happens if we move around such signal, which could be some, I don't", "tokens": [50664, 321, 393, 536, 437, 2314, 498, 321, 1286, 926, 1270, 6358, 11, 597, 727, 312, 512, 11, 286, 500, 380, 50938], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 62, "seek": 27752, "start": 289.0, "end": 292.52, "text": " know, humidity, temperature, whatever, in the atmosphere, OK?", "tokens": [50938, 458, 11, 24751, 11, 4292, 11, 2035, 11, 294, 264, 8018, 11, 2264, 30, 51114], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 63, "seek": 27752, "start": 292.52, "end": 295.44, "text": " So we have a sample problem.", "tokens": [51114, 407, 321, 362, 257, 6889, 1154, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 64, "seek": 27752, "start": 295.44, "end": 301.35999999999996, "text": " Here I'm showing results from a three-dimensional version of what was just shown.", "tokens": [51260, 1692, 286, 478, 4099, 3542, 490, 257, 1045, 12, 18759, 3037, 295, 437, 390, 445, 4898, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 65, "seek": 27752, "start": 301.35999999999996, "end": 307.12, "text": " And let's start with the right-hand side plot, x-axis, the size of the grid.", "tokens": [51556, 400, 718, 311, 722, 365, 264, 558, 12, 5543, 1252, 7542, 11, 2031, 12, 24633, 11, 264, 2744, 295, 264, 10748, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1440421095839492, "compression_ratio": 1.549090909090909, "no_speech_prob": 0.0002295907324878499}, {"id": 66, "seek": 30712, "start": 307.12, "end": 310.52, "text": " So if it's 8, it means 8 by 8, super tiny.", "tokens": [50364, 407, 498, 309, 311, 1649, 11, 309, 1355, 1649, 538, 1649, 11, 1687, 5870, 13, 50534], "temperature": 0.0, "avg_logprob": -0.21938382495533337, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0031067347154021263}, {"id": 67, "seek": 30712, "start": 310.52, "end": 318.8, "text": " If it's 128, it's 128 by 128 by 128, and wall time per time step on the y-axis, OK?", "tokens": [50534, 759, 309, 311, 29810, 11, 309, 311, 29810, 538, 29810, 538, 29810, 11, 293, 2929, 565, 680, 565, 1823, 322, 264, 288, 12, 24633, 11, 2264, 30, 50948], "temperature": 0.0, "avg_logprob": -0.21938382495533337, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0031067347154021263}, {"id": 68, "seek": 30712, "start": 318.8, "end": 319.8, "text": " Green.", "tokens": [50948, 6969, 13, 50998], "temperature": 0.0, "avg_logprob": -0.21938382495533337, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0031067347154021263}, {"id": 69, "seek": 30712, "start": 319.8, "end": 326.4, "text": " C++ implementation of one particular algorithm for this kind of problems, and orange, pi", "tokens": [50998, 383, 25472, 11420, 295, 472, 1729, 9284, 337, 341, 733, 295, 2740, 11, 293, 7671, 11, 3895, 51328], "temperature": 0.0, "avg_logprob": -0.21938382495533337, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0031067347154021263}, {"id": 70, "seek": 30712, "start": 326.4, "end": 332.48, "text": " MP data, the same algorithm, numerically, but a Python implementation.", "tokens": [51328, 14146, 1412, 11, 264, 912, 9284, 11, 7866, 984, 11, 457, 257, 15329, 11420, 13, 51632], "temperature": 0.0, "avg_logprob": -0.21938382495533337, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0031067347154021263}, {"id": 71, "seek": 33248, "start": 332.48, "end": 342.28000000000003, "text": " So here you see that actually Namba, just compiled version outperformed C++, maintaining", "tokens": [50364, 407, 510, 291, 536, 300, 767, 426, 23337, 11, 445, 36548, 3037, 484, 610, 22892, 383, 25472, 11, 14916, 50854], "temperature": 0.0, "avg_logprob": -0.1719259161698191, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0010368169751018286}, {"id": 72, "seek": 33248, "start": 342.28000000000003, "end": 348.96000000000004, "text": " even better scaling for the tiny matrices, but they are kind of irrelevant for the problem.", "tokens": [50854, 754, 1101, 21589, 337, 264, 5870, 32284, 11, 457, 436, 366, 733, 295, 28682, 337, 264, 1154, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1719259161698191, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0010368169751018286}, {"id": 73, "seek": 33248, "start": 348.96000000000004, "end": 353.08000000000004, "text": " And please note that in both cases we have used multi-threading.", "tokens": [51188, 400, 1767, 3637, 300, 294, 1293, 3331, 321, 362, 1143, 4825, 12, 392, 35908, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1719259161698191, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0010368169751018286}, {"id": 74, "seek": 33248, "start": 353.08000000000004, "end": 358.40000000000003, "text": " So here on the left-hand side, you can see actually on the x-axis number of threads, y-axis", "tokens": [51394, 407, 510, 322, 264, 1411, 12, 5543, 1252, 11, 291, 393, 536, 767, 322, 264, 2031, 12, 24633, 1230, 295, 19314, 11, 288, 12, 24633, 51660], "temperature": 0.0, "avg_logprob": -0.1719259161698191, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0010368169751018286}, {"id": 75, "seek": 33248, "start": 358.40000000000003, "end": 360.72, "text": " wall time per time step.", "tokens": [51660, 2929, 565, 680, 565, 1823, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1719259161698191, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0010368169751018286}, {"id": 76, "seek": 36072, "start": 360.72, "end": 364.08000000000004, "text": " And again, the green line is the C++ implementation.", "tokens": [50364, 400, 797, 11, 264, 3092, 1622, 307, 264, 383, 25472, 11420, 13, 50532], "temperature": 0.0, "avg_logprob": -0.15183240911933812, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0053305597975850105}, {"id": 77, "seek": 36072, "start": 364.08000000000004, "end": 370.0, "text": " These two are two variants of the Python 1.jit compiled with Namba, almost an order of magnitude", "tokens": [50532, 1981, 732, 366, 732, 21669, 295, 264, 15329, 502, 13, 73, 270, 36548, 365, 426, 23337, 11, 1920, 364, 1668, 295, 15668, 50828], "temperature": 0.0, "avg_logprob": -0.15183240911933812, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0053305597975850105}, {"id": 78, "seek": 36072, "start": 370.0, "end": 372.68, "text": " a faster execution, five times faster.", "tokens": [50828, 257, 4663, 15058, 11, 1732, 1413, 4663, 13, 50962], "temperature": 0.0, "avg_logprob": -0.15183240911933812, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0053305597975850105}, {"id": 79, "seek": 36072, "start": 372.68, "end": 379.04, "text": " And what's probably most interesting for now is that when you compare with just setting", "tokens": [50962, 400, 437, 311, 1391, 881, 1880, 337, 586, 307, 300, 562, 291, 6794, 365, 445, 3287, 51280], "temperature": 0.0, "avg_logprob": -0.15183240911933812, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0053305597975850105}, {"id": 80, "seek": 36072, "start": 379.04, "end": 386.90000000000003, "text": " the environment variable for Namba.jit to disabled, we jump more than two orders of", "tokens": [51280, 264, 2823, 7006, 337, 426, 23337, 13, 73, 270, 281, 15191, 11, 321, 3012, 544, 813, 732, 9470, 295, 51673], "temperature": 0.0, "avg_logprob": -0.15183240911933812, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0053305597975850105}, {"id": 81, "seek": 38690, "start": 386.9, "end": 391.23999999999995, "text": " magnitude up in wall time.", "tokens": [50364, 15668, 493, 294, 2929, 565, 13, 50581], "temperature": 0.0, "avg_logprob": -0.14407766660054525, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.04757596552371979}, {"id": 82, "seek": 38690, "start": 391.23999999999995, "end": 399.97999999999996, "text": " So this is how Namba timing compares with plain Python timing.", "tokens": [50581, 407, 341, 307, 577, 426, 23337, 10822, 38334, 365, 11121, 15329, 10822, 13, 51018], "temperature": 0.0, "avg_logprob": -0.14407766660054525, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.04757596552371979}, {"id": 83, "seek": 38690, "start": 399.97999999999996, "end": 403.26, "text": " But there are two important things to be mentioned here.", "tokens": [51018, 583, 456, 366, 732, 1021, 721, 281, 312, 2835, 510, 13, 51182], "temperature": 0.0, "avg_logprob": -0.14407766660054525, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.04757596552371979}, {"id": 84, "seek": 38690, "start": 403.26, "end": 410.53999999999996, "text": " The Python package is written having Namba in mind, that is, everything is loop-based,", "tokens": [51182, 440, 15329, 7372, 307, 3720, 1419, 426, 23337, 294, 1575, 11, 300, 307, 11, 1203, 307, 6367, 12, 6032, 11, 51546], "temperature": 0.0, "avg_logprob": -0.14407766660054525, "compression_ratio": 1.4382716049382716, "no_speech_prob": 0.04757596552371979}, {"id": 85, "seek": 41054, "start": 410.54, "end": 416.98, "text": " which is the reason why plain C Python with Nampa performs badly.", "tokens": [50364, 597, 307, 264, 1778, 983, 11121, 383, 15329, 365, 426, 26625, 26213, 13425, 13, 50686], "temperature": 0.0, "avg_logprob": -0.21395936342749264, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.07237206399440765}, {"id": 86, "seek": 41054, "start": 416.98, "end": 421.22, "text": " This line is kind of irrelevant, just as a curiosity.", "tokens": [50686, 639, 1622, 307, 733, 295, 28682, 11, 445, 382, 257, 18769, 13, 50898], "temperature": 0.0, "avg_logprob": -0.21395936342749264, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.07237206399440765}, {"id": 87, "seek": 41054, "start": 421.22, "end": 426.34000000000003, "text": " On the other hand, the C++ version is kind of legacy, it's based on Blitz++ library.", "tokens": [50898, 1282, 264, 661, 1011, 11, 264, 383, 25472, 3037, 307, 733, 295, 11711, 11, 309, 311, 2361, 322, 2177, 6862, 25472, 6405, 13, 51154], "temperature": 0.0, "avg_logprob": -0.21395936342749264, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.07237206399440765}, {"id": 88, "seek": 41054, "start": 426.34000000000003, "end": 430.54, "text": " Back then, when it was developed, IGN didn't have support for multiple dimensions.", "tokens": [51154, 5833, 550, 11, 562, 309, 390, 4743, 11, 26367, 45, 994, 380, 362, 1406, 337, 3866, 12819, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21395936342749264, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.07237206399440765}, {"id": 89, "seek": 41054, "start": 430.54, "end": 436.86, "text": " And it's object-oriented RIProcessing, which was reported and measured to be kind of five", "tokens": [51364, 400, 309, 311, 2657, 12, 27414, 497, 9139, 340, 780, 278, 11, 597, 390, 7055, 293, 12690, 281, 312, 733, 295, 1732, 51680], "temperature": 0.0, "avg_logprob": -0.21395936342749264, "compression_ratio": 1.4784313725490197, "no_speech_prob": 0.07237206399440765}, {"id": 90, "seek": 43686, "start": 436.86, "end": 441.3, "text": " times slower than 4777 for these kind of small domains.", "tokens": [50364, 1413, 14009, 813, 16953, 17512, 337, 613, 733, 295, 1359, 25514, 13, 50586], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 91, "seek": 43686, "start": 441.3, "end": 444.22, "text": " It's not the same for larger domains.", "tokens": [50586, 467, 311, 406, 264, 912, 337, 4833, 25514, 13, 50732], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 92, "seek": 43686, "start": 444.22, "end": 448.7, "text": " Anyhow, we can achieve high performance with Python.", "tokens": [50732, 2639, 4286, 11, 321, 393, 4584, 1090, 3389, 365, 15329, 13, 50956], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 93, "seek": 43686, "start": 448.7, "end": 451.1, "text": " But what if we need MPI?", "tokens": [50956, 583, 437, 498, 321, 643, 14146, 40, 30, 51076], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 94, "seek": 43686, "start": 451.1, "end": 455.38, "text": " We need message passing in our code.", "tokens": [51076, 492, 643, 3636, 8437, 294, 527, 3089, 13, 51290], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 95, "seek": 43686, "start": 455.38, "end": 456.98, "text": " How would we use it?", "tokens": [51290, 1012, 576, 321, 764, 309, 30, 51370], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 96, "seek": 43686, "start": 456.98, "end": 462.7, "text": " Let's say we divide in a domain that can position spirit our domain in two parts.", "tokens": [51370, 961, 311, 584, 321, 9845, 294, 257, 9274, 300, 393, 2535, 3797, 527, 9274, 294, 732, 3166, 13, 51656], "temperature": 0.0, "avg_logprob": -0.22459472309459338, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.21264047920703888}, {"id": 97, "seek": 46270, "start": 462.7, "end": 469.46, "text": " So the same problem, same setup, just half of the domain is computed by one process or", "tokens": [50364, 407, 264, 912, 1154, 11, 912, 8657, 11, 445, 1922, 295, 264, 9274, 307, 40610, 538, 472, 1399, 420, 50702], "temperature": 0.0, "avg_logprob": -0.1748239416825144, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.001227180240675807}, {"id": 98, "seek": 46270, "start": 469.46, "end": 479.26, "text": " node or anything that has distributed, has different memory addressing than another work.", "tokens": [50702, 9984, 420, 1340, 300, 575, 12631, 11, 575, 819, 4675, 14329, 813, 1071, 589, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1748239416825144, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.001227180240675807}, {"id": 99, "seek": 46270, "start": 479.26, "end": 482.38, "text": " So this is how we want to use it, why we want to use MPI?", "tokens": [51192, 407, 341, 307, 577, 321, 528, 281, 764, 309, 11, 983, 321, 528, 281, 764, 14146, 40, 30, 51348], "temperature": 0.0, "avg_logprob": -0.1748239416825144, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.001227180240675807}, {"id": 100, "seek": 46270, "start": 482.38, "end": 487.7, "text": " Well, because despite expansion in parallel computation, both in the number of machines", "tokens": [51348, 1042, 11, 570, 7228, 11260, 294, 8952, 24903, 11, 1293, 294, 264, 1230, 295, 8379, 51614], "temperature": 0.0, "avg_logprob": -0.1748239416825144, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.001227180240675807}, {"id": 101, "seek": 46270, "start": 487.7, "end": 492.41999999999996, "text": " and the number of cores, no other parallel programming paradigm has replaced MPI.", "tokens": [51614, 293, 264, 1230, 295, 24826, 11, 572, 661, 8952, 9410, 24709, 575, 10772, 14146, 40, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1748239416825144, "compression_ratio": 1.6557377049180328, "no_speech_prob": 0.001227180240675807}, {"id": 102, "seek": 49242, "start": 492.46000000000004, "end": 495.06, "text": " At least as of 2013.", "tokens": [50366, 1711, 1935, 382, 295, 9012, 13, 50496], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 103, "seek": 49242, "start": 495.06, "end": 501.34000000000003, "text": " And already in 2013, people were writing that this is, even though it's universally acknowledged", "tokens": [50496, 400, 1217, 294, 9012, 11, 561, 645, 3579, 300, 341, 307, 11, 754, 1673, 309, 311, 43995, 27262, 50810], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 104, "seek": 49242, "start": 501.34000000000003, "end": 504.3, "text": " that MPI is rather a crude way of programming these machines.", "tokens": [50810, 300, 14146, 40, 307, 2831, 257, 30796, 636, 295, 9410, 613, 8379, 13, 50958], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 105, "seek": 49242, "start": 504.3, "end": 507.02000000000004, "text": " Anyhow, still, let's try it.", "tokens": [50958, 2639, 4286, 11, 920, 11, 718, 311, 853, 309, 13, 51094], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 106, "seek": 49242, "start": 507.02000000000004, "end": 509.06, "text": " And let's try it with Python.", "tokens": [51094, 400, 718, 311, 853, 309, 365, 15329, 13, 51196], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 107, "seek": 49242, "start": 509.06, "end": 517.1800000000001, "text": " So here is a seven-line snippet of code where we try to import Namba to get the jit compilation", "tokens": [51196, 407, 510, 307, 257, 3407, 12, 1889, 35623, 302, 295, 3089, 689, 321, 853, 281, 974, 426, 23337, 281, 483, 264, 361, 270, 40261, 51602], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 108, "seek": 49242, "start": 517.1800000000001, "end": 518.78, "text": " of Python code.", "tokens": [51602, 295, 15329, 3089, 13, 51682], "temperature": 0.0, "avg_logprob": -0.20458976668540876, "compression_ratio": 1.5283842794759825, "no_speech_prob": 0.005046055186539888}, {"id": 109, "seek": 51878, "start": 518.78, "end": 523.3, "text": " And then we use MPI for pi, which is Python interface to MPI.", "tokens": [50364, 400, 550, 321, 764, 14146, 40, 337, 3895, 11, 597, 307, 15329, 9226, 281, 14146, 40, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 110, "seek": 51878, "start": 523.3, "end": 524.3, "text": " What do we do?", "tokens": [50590, 708, 360, 321, 360, 30, 50640], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 111, "seek": 51878, "start": 524.3, "end": 528.54, "text": " We define some number crunching routine, and we try to use MPI from it.", "tokens": [50640, 492, 6964, 512, 1230, 13386, 278, 9927, 11, 293, 321, 853, 281, 764, 14146, 40, 490, 309, 13, 50852], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 112, "seek": 51878, "start": 528.54, "end": 531.3399999999999, "text": " And then we try to Njit.", "tokens": [50852, 400, 550, 321, 853, 281, 426, 73, 270, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 113, "seek": 51878, "start": 531.3399999999999, "end": 538.8199999999999, "text": " Njit means the highest performance variant of Namba jit compilation.", "tokens": [50992, 426, 73, 270, 1355, 264, 6343, 3389, 17501, 295, 426, 23337, 361, 270, 40261, 13, 51366], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 114, "seek": 51878, "start": 538.8199999999999, "end": 542.8199999999999, "text": " We try to jit compile this function and straight ahead execute it.", "tokens": [51366, 492, 853, 281, 361, 270, 31413, 341, 2445, 293, 2997, 2286, 14483, 309, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 115, "seek": 51878, "start": 542.8199999999999, "end": 543.8199999999999, "text": " What happens?", "tokens": [51566, 708, 2314, 30, 51616], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 116, "seek": 51878, "start": 543.8199999999999, "end": 546.5, "text": " It doesn't work.", "tokens": [51616, 467, 1177, 380, 589, 13, 51750], "temperature": 0.0, "avg_logprob": -0.1827821548168476, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0010008735116571188}, {"id": 117, "seek": 54650, "start": 546.5, "end": 554.78, "text": " It cannot compile because Namba cannot determine type of MPI for pi.mpi.intra.com because it's", "tokens": [50364, 467, 2644, 31413, 570, 426, 23337, 2644, 6997, 2010, 295, 14146, 40, 337, 3895, 13, 2455, 72, 13, 686, 424, 13, 1112, 570, 309, 311, 50778], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 118, "seek": 54650, "start": 554.78, "end": 556.42, "text": " a class.", "tokens": [50778, 257, 1508, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 119, "seek": 54650, "start": 556.42, "end": 562.1, "text": " Classes do not work with Namba, at least not the ordinary Python classes.", "tokens": [50860, 9471, 279, 360, 406, 589, 365, 426, 23337, 11, 412, 1935, 406, 264, 10547, 15329, 5359, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 120, "seek": 54650, "start": 562.1, "end": 564.1, "text": " So something doesn't work.", "tokens": [51144, 407, 746, 1177, 380, 589, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 121, "seek": 54650, "start": 564.1, "end": 569.3, "text": " So the problem is that we have Namba, which is one of the leading solutions to speed up", "tokens": [51244, 407, 264, 1154, 307, 300, 321, 362, 426, 23337, 11, 597, 307, 472, 295, 264, 5775, 6547, 281, 3073, 493, 51504], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 122, "seek": 54650, "start": 569.3, "end": 570.3, "text": " Python.", "tokens": [51504, 15329, 13, 51554], "temperature": 0.0, "avg_logprob": -0.1925349180725799, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0011476778890937567}, {"id": 123, "seek": 57030, "start": 570.3, "end": 576.62, "text": " MPI, which is clearly the de facto standard for distributed memory parallelization.", "tokens": [50364, 14146, 40, 11, 597, 307, 4448, 264, 368, 42225, 3832, 337, 12631, 4675, 8952, 2144, 13, 50680], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 124, "seek": 57030, "start": 576.62, "end": 580.5, "text": " We try to work with them together, but it doesn't work.", "tokens": [50680, 492, 853, 281, 589, 365, 552, 1214, 11, 457, 309, 1177, 380, 589, 13, 50874], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 125, "seek": 57030, "start": 580.5, "end": 583.78, "text": " So stack overflow.", "tokens": [50874, 407, 8630, 37772, 13, 51038], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 126, "seek": 57030, "start": 583.78, "end": 585.9399999999999, "text": " Let's go it.", "tokens": [51038, 961, 311, 352, 309, 13, 51146], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 127, "seek": 57030, "start": 585.9399999999999, "end": 586.9399999999999, "text": " Nothing.", "tokens": [51146, 6693, 13, 51196], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 128, "seek": 57030, "start": 586.9399999999999, "end": 588.14, "text": " Let's quant it.", "tokens": [51196, 961, 311, 4426, 309, 13, 51256], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 129, "seek": 57030, "start": 588.14, "end": 589.78, "text": " Nothing.", "tokens": [51256, 6693, 13, 51338], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 130, "seek": 57030, "start": 589.78, "end": 591.4599999999999, "text": " Wrong search engine, right?", "tokens": [51338, 28150, 3164, 2848, 11, 558, 30, 51422], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 131, "seek": 57030, "start": 591.4599999999999, "end": 593.9, "text": " Someone must have solved the problem.", "tokens": [51422, 8734, 1633, 362, 13041, 264, 1154, 13, 51544], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 132, "seek": 57030, "start": 593.9, "end": 594.9, "text": " Nothing.", "tokens": [51544, 6693, 13, 51594], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 133, "seek": 57030, "start": 594.9, "end": 598.38, "text": " Let's ask Namba guys and MPI for pi guys.", "tokens": [51594, 961, 311, 1029, 426, 23337, 1074, 293, 14146, 40, 337, 3895, 1074, 13, 51768], "temperature": 0.0, "avg_logprob": -0.30573909759521484, "compression_ratio": 1.4907407407407407, "no_speech_prob": 0.01717156171798706}, {"id": 134, "seek": 59838, "start": 598.46, "end": 603.66, "text": " In 2020, you will not be able to use MPI for pi's siton code.", "tokens": [50368, 682, 4808, 11, 291, 486, 406, 312, 1075, 281, 764, 14146, 40, 337, 3895, 311, 1394, 266, 3089, 13, 50628], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 135, "seek": 59838, "start": 603.66, "end": 605.98, "text": " It was not designed for such low-level usage.", "tokens": [50628, 467, 390, 406, 4761, 337, 1270, 2295, 12, 12418, 14924, 13, 50744], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 136, "seek": 59838, "start": 605.98, "end": 609.5, "text": " Well, okay, it's siton.", "tokens": [50744, 1042, 11, 1392, 11, 309, 311, 1394, 266, 13, 50920], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 137, "seek": 59838, "start": 609.5, "end": 612.82, "text": " But I mean, it must be doable, right?", "tokens": [50920, 583, 286, 914, 11, 309, 1633, 312, 41183, 11, 558, 30, 51086], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 138, "seek": 59838, "start": 612.82, "end": 616.1, "text": " We have two established packages.", "tokens": [51086, 492, 362, 732, 7545, 17401, 13, 51250], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 139, "seek": 59838, "start": 616.1, "end": 618.98, "text": " The aim is kind of solid and makes sense.", "tokens": [51250, 440, 5939, 307, 733, 295, 5100, 293, 1669, 2020, 13, 51394], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 140, "seek": 59838, "start": 618.98, "end": 622.06, "text": " So it must be doable.", "tokens": [51394, 407, 309, 1633, 312, 41183, 13, 51548], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 141, "seek": 59838, "start": 622.06, "end": 628.34, "text": " And 30 months later, 120 comments later, 50 PR slater from five contributors on a totally", "tokens": [51548, 400, 2217, 2493, 1780, 11, 10411, 3053, 1780, 11, 2625, 11568, 1061, 771, 490, 1732, 45627, 322, 257, 3879, 51862], "temperature": 0.0, "avg_logprob": -0.3218625619870807, "compression_ratio": 1.4813278008298756, "no_speech_prob": 0.029169760644435883}, {"id": 142, "seek": 62834, "start": 628.34, "end": 632.5, "text": " unplanned site project, we are introducing Namba MPI.", "tokens": [50364, 32816, 5943, 3621, 1716, 11, 321, 366, 15424, 426, 23337, 14146, 40, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2283734265495749, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0020215511322021484}, {"id": 143, "seek": 62834, "start": 632.5, "end": 642.46, "text": " Namba MPI is an open source, kind of small Python project, which allows you to, let's", "tokens": [50572, 426, 23337, 14146, 40, 307, 364, 1269, 4009, 11, 733, 295, 1359, 15329, 1716, 11, 597, 4045, 291, 281, 11, 718, 311, 51070], "temperature": 0.0, "avg_logprob": -0.2283734265495749, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0020215511322021484}, {"id": 144, "seek": 62834, "start": 642.46, "end": 648.34, "text": " jump here to the Hello World example, which allows you to use the Namba NGIT decorator", "tokens": [51070, 3012, 510, 281, 264, 2425, 3937, 1365, 11, 597, 4045, 291, 281, 764, 264, 426, 23337, 426, 38, 3927, 7919, 1639, 51364], "temperature": 0.0, "avg_logprob": -0.2283734265495749, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0020215511322021484}, {"id": 145, "seek": 62834, "start": 648.34, "end": 657.0600000000001, "text": " on a function that involves rank, size, or any other MPI API calls within the Python", "tokens": [51364, 322, 257, 2445, 300, 11626, 6181, 11, 2744, 11, 420, 604, 661, 14146, 40, 9362, 5498, 1951, 264, 15329, 51800], "temperature": 0.0, "avg_logprob": -0.2283734265495749, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0020215511322021484}, {"id": 146, "seek": 65706, "start": 657.06, "end": 659.26, "text": " code.", "tokens": [50364, 3089, 13, 50474], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 147, "seek": 65706, "start": 659.26, "end": 663.8599999999999, "text": " As of now, we cover size rank, send, receive, or reduce broadcast barrier.", "tokens": [50474, 1018, 295, 586, 11, 321, 2060, 2744, 6181, 11, 2845, 11, 4774, 11, 420, 5407, 9975, 13357, 13, 50704], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 148, "seek": 65706, "start": 663.8599999999999, "end": 667.3399999999999, "text": " The API for Namba MPI is based on NumPy.", "tokens": [50704, 440, 9362, 337, 426, 23337, 14146, 40, 307, 2361, 322, 22592, 47, 88, 13, 50878], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 149, "seek": 65706, "start": 667.3399999999999, "end": 669.42, "text": " We have auto-generated documentation.", "tokens": [50878, 492, 362, 8399, 12, 21848, 770, 14333, 13, 50982], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 150, "seek": 65706, "start": 669.42, "end": 672.5, "text": " We are on PyPy and Conda Forge.", "tokens": [50982, 492, 366, 322, 9953, 47, 88, 293, 383, 12233, 1171, 432, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 151, "seek": 65706, "start": 672.5, "end": 675.54, "text": " Few words about how it's implemented.", "tokens": [51136, 33468, 2283, 466, 577, 309, 311, 12270, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 152, "seek": 65706, "start": 675.54, "end": 682.26, "text": " Essentially we start with Ctypes built into Python to try to address the C API.", "tokens": [51288, 23596, 321, 722, 365, 383, 874, 5190, 3094, 666, 15329, 281, 853, 281, 2985, 264, 383, 9362, 13, 51624], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 153, "seek": 65706, "start": 682.26, "end": 686.78, "text": " There are some things related with passing addresses, memories, void pointers, et cetera,", "tokens": [51624, 821, 366, 512, 721, 4077, 365, 8437, 16862, 11, 8495, 11, 22009, 44548, 11, 1030, 11458, 11, 51850], "temperature": 0.0, "avg_logprob": -0.2740652498991593, "compression_ratio": 1.5, "no_speech_prob": 0.0015524146147072315}, {"id": 154, "seek": 68678, "start": 687.62, "end": 691.22, "text": " super interesting.", "tokens": [50406, 1687, 1880, 13, 50586], "temperature": 0.0, "avg_logprob": -0.22972205515657917, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.019031301140785217}, {"id": 155, "seek": 68678, "start": 691.22, "end": 696.5799999999999, "text": " Probably the key message here is that we are offering the send function that is already", "tokens": [50586, 9210, 264, 2141, 3636, 510, 307, 300, 321, 366, 8745, 264, 2845, 2445, 300, 307, 1217, 50854], "temperature": 0.0, "avg_logprob": -0.22972205515657917, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.019031301140785217}, {"id": 156, "seek": 68678, "start": 696.5799999999999, "end": 701.4599999999999, "text": " NGITed, which means that you can use it from other NGITed functions.", "tokens": [50854, 426, 38, 3927, 292, 11, 597, 1355, 300, 291, 393, 764, 309, 490, 661, 426, 38, 3927, 292, 6828, 13, 51098], "temperature": 0.0, "avg_logprob": -0.22972205515657917, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.019031301140785217}, {"id": 157, "seek": 68678, "start": 701.4599999999999, "end": 706.54, "text": " We handle non-continuous arrays from NumPy, so we try to be user-friendly.", "tokens": [51098, 492, 4813, 2107, 12, 9000, 259, 12549, 41011, 490, 22592, 47, 88, 11, 370, 321, 853, 281, 312, 4195, 12, 22864, 13, 51352], "temperature": 0.0, "avg_logprob": -0.22972205515657917, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.019031301140785217}, {"id": 158, "seek": 68678, "start": 706.54, "end": 713.54, "text": " We then call the underline C function, and kind of that's all.", "tokens": [51352, 492, 550, 818, 264, 833, 1889, 383, 2445, 11, 293, 733, 295, 300, 311, 439, 13, 51702], "temperature": 0.0, "avg_logprob": -0.22972205515657917, "compression_ratio": 1.5194174757281553, "no_speech_prob": 0.019031301140785217}, {"id": 159, "seek": 71354, "start": 713.54, "end": 717.86, "text": " But really, there is the key line number 30.", "tokens": [50364, 583, 534, 11, 456, 307, 264, 2141, 1622, 1230, 2217, 13, 50580], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 160, "seek": 71354, "start": 717.86, "end": 718.86, "text": " This one.", "tokens": [50580, 639, 472, 13, 50630], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 161, "seek": 71354, "start": 718.86, "end": 724.38, "text": " Well, that's nothing but, in principle, without it, Namba optimizes out all our code.", "tokens": [50630, 1042, 11, 300, 311, 1825, 457, 11, 294, 8665, 11, 1553, 309, 11, 426, 23337, 5028, 5660, 484, 439, 527, 3089, 13, 50906], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 162, "seek": 71354, "start": 724.38, "end": 729.9, "text": " Anyhow, these are kind of things that you see when trying to implement such things.", "tokens": [50906, 2639, 4286, 11, 613, 366, 733, 295, 721, 300, 291, 536, 562, 1382, 281, 4445, 1270, 721, 13, 51182], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 163, "seek": 71354, "start": 729.9, "end": 734.62, "text": " Unfortunately, there are quite more of such hacks inside Namba MPI.", "tokens": [51182, 8590, 11, 456, 366, 1596, 544, 295, 1270, 33617, 1854, 426, 23337, 14146, 40, 13, 51418], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 164, "seek": 71354, "start": 734.62, "end": 740.8199999999999, "text": " The next slide is kind of a thing that you prefer to never see, but they cannot be unseen,", "tokens": [51418, 440, 958, 4137, 307, 733, 295, 257, 551, 300, 291, 4382, 281, 1128, 536, 11, 457, 436, 2644, 312, 40608, 11, 51728], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 165, "seek": 71354, "start": 740.8199999999999, "end": 742.78, "text": " in a way, if you work with it.", "tokens": [51728, 294, 257, 636, 11, 498, 291, 589, 365, 309, 13, 51826], "temperature": 0.0, "avg_logprob": -0.23265613358596277, "compression_ratio": 1.5923076923076922, "no_speech_prob": 0.006207320373505354}, {"id": 166, "seek": 74278, "start": 742.78, "end": 748.9399999999999, "text": " So please just think of it as a picture of some problems that we have challenged and", "tokens": [50364, 407, 1767, 445, 519, 295, 309, 382, 257, 3036, 295, 512, 2740, 300, 321, 362, 17737, 293, 50672], "temperature": 0.0, "avg_logprob": -0.23058590136076274, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.00358464359305799}, {"id": 167, "seek": 74278, "start": 748.9399999999999, "end": 756.3399999999999, "text": " essentially wrote to Namba guys asking how can it be done, and we got this kind of tools", "tokens": [50672, 4476, 4114, 281, 426, 23337, 1074, 3365, 577, 393, 309, 312, 1096, 11, 293, 321, 658, 341, 733, 295, 3873, 51042], "temperature": 0.0, "avg_logprob": -0.23058590136076274, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.00358464359305799}, {"id": 168, "seek": 74278, "start": 756.3399999999999, "end": 764.86, "text": " for handling void pointers from C types in Namba with Python, NumPy, et cetera.", "tokens": [51042, 337, 13175, 22009, 44548, 490, 383, 3467, 294, 426, 23337, 365, 15329, 11, 22592, 47, 88, 11, 1030, 11458, 13, 51468], "temperature": 0.0, "avg_logprob": -0.23058590136076274, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.00358464359305799}, {"id": 169, "seek": 74278, "start": 764.86, "end": 771.6999999999999, "text": " But well, that's utilspy, and that's it, and it kind of works, and why do we know it works?", "tokens": [51468, 583, 731, 11, 300, 311, 2839, 4174, 8200, 11, 293, 300, 311, 309, 11, 293, 309, 733, 295, 1985, 11, 293, 983, 360, 321, 458, 309, 1985, 30, 51810], "temperature": 0.0, "avg_logprob": -0.23058590136076274, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.00358464359305799}, {"id": 170, "seek": 77170, "start": 771.7, "end": 780.5, "text": " Because we test it, and let me handle the mic to Olexi to tell you more about testing.", "tokens": [50364, 1436, 321, 1500, 309, 11, 293, 718, 385, 4813, 264, 3123, 281, 422, 2021, 72, 281, 980, 291, 544, 466, 4997, 13, 50804], "temperature": 0.0, "avg_logprob": -0.3246891975402832, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.07428121566772461}, {"id": 171, "seek": 77170, "start": 780.5, "end": 790.6600000000001, "text": " Okay, it's focused.", "tokens": [50804, 1033, 11, 309, 311, 5178, 13, 51312], "temperature": 0.0, "avg_logprob": -0.3246891975402832, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.07428121566772461}, {"id": 172, "seek": 77170, "start": 790.6600000000001, "end": 798.1800000000001, "text": " So I'm going to tell you about the CI that we have set up for our project for Namba MPI.", "tokens": [51312, 407, 286, 478, 516, 281, 980, 291, 466, 264, 37777, 300, 321, 362, 992, 493, 337, 527, 1716, 337, 426, 23337, 14146, 40, 13, 51688], "temperature": 0.0, "avg_logprob": -0.3246891975402832, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.07428121566772461}, {"id": 173, "seek": 79818, "start": 798.18, "end": 814.8599999999999, "text": " So the CI was set up at Github Actions, as I said, and this is the screen of the workflow.", "tokens": [50364, 407, 264, 37777, 390, 992, 493, 412, 460, 355, 836, 3251, 626, 11, 382, 286, 848, 11, 293, 341, 307, 264, 2568, 295, 264, 20993, 13, 51198], "temperature": 0.0, "avg_logprob": -0.3411917350661587, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1641129106283188}, {"id": 174, "seek": 79818, "start": 814.8599999999999, "end": 820.06, "text": " We start from running the PDoc, Precommit, and PyLint.", "tokens": [51198, 492, 722, 490, 2614, 264, 10464, 905, 11, 6001, 1112, 3508, 11, 293, 9953, 43, 686, 13, 51458], "temperature": 0.0, "avg_logprob": -0.3411917350661587, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1641129106283188}, {"id": 175, "seek": 79818, "start": 820.06, "end": 826.5799999999999, "text": " PDoc is for automatic documentation generation, PyLint for static code analysis, and Precommit", "tokens": [51458, 10464, 905, 307, 337, 12509, 14333, 5125, 11, 9953, 43, 686, 337, 13437, 3089, 5215, 11, 293, 6001, 1112, 3508, 51784], "temperature": 0.0, "avg_logprob": -0.3411917350661587, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1641129106283188}, {"id": 176, "seek": 82658, "start": 826.58, "end": 829.14, "text": " for styling.", "tokens": [50364, 337, 27944, 13, 50492], "temperature": 0.0, "avg_logprob": -0.21111529060963835, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.06360592693090439}, {"id": 177, "seek": 82658, "start": 829.14, "end": 833.5400000000001, "text": " After that, if these steps were successfully moving to the main part where we run our unit", "tokens": [50492, 2381, 300, 11, 498, 613, 4439, 645, 10727, 2684, 281, 264, 2135, 644, 689, 321, 1190, 527, 4985, 50712], "temperature": 0.0, "avg_logprob": -0.21111529060963835, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.06360592693090439}, {"id": 178, "seek": 82658, "start": 833.5400000000001, "end": 843.4200000000001, "text": " tests, this is the example, not example, but actually the workflow file that we run.", "tokens": [50712, 6921, 11, 341, 307, 264, 1365, 11, 406, 1365, 11, 457, 767, 264, 20993, 3991, 300, 321, 1190, 13, 51206], "temperature": 0.0, "avg_logprob": -0.21111529060963835, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.06360592693090439}, {"id": 179, "seek": 82658, "start": 843.4200000000001, "end": 850.14, "text": " As you can see, when we run the CI against multiple systems, different Python versions", "tokens": [51206, 1018, 291, 393, 536, 11, 562, 321, 1190, 264, 37777, 1970, 3866, 3652, 11, 819, 15329, 9606, 51542], "temperature": 0.0, "avg_logprob": -0.21111529060963835, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.06360592693090439}, {"id": 180, "seek": 82658, "start": 850.14, "end": 856.5400000000001, "text": " and different MPI implementations, and here we should say a big thank you to MPI for", "tokens": [51542, 293, 819, 14146, 40, 4445, 763, 11, 293, 510, 321, 820, 584, 257, 955, 1309, 291, 281, 14146, 40, 337, 51862], "temperature": 0.0, "avg_logprob": -0.21111529060963835, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.06360592693090439}, {"id": 181, "seek": 85654, "start": 856.6999999999999, "end": 865.3, "text": " PyTeam for providing set up MPI Github Action, because this has saved us a lot of time.", "tokens": [50372, 9953, 38588, 337, 6530, 992, 493, 14146, 40, 460, 355, 836, 16261, 11, 570, 341, 575, 6624, 505, 257, 688, 295, 565, 13, 50802], "temperature": 0.0, "avg_logprob": -0.3255894382794698, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0521126464009285}, {"id": 182, "seek": 85654, "start": 865.3, "end": 869.26, "text": " So thank you, MPI for Py.", "tokens": [50802, 407, 1309, 291, 11, 14146, 40, 337, 9953, 13, 51000], "temperature": 0.0, "avg_logprob": -0.3255894382794698, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0521126464009285}, {"id": 183, "seek": 85654, "start": 869.26, "end": 876.62, "text": " And as of operation systems and MPI implementations, we are running, in case of Linux, we're testing", "tokens": [51000, 400, 382, 295, 6916, 3652, 293, 14146, 40, 4445, 763, 11, 321, 366, 2614, 11, 294, 1389, 295, 18734, 11, 321, 434, 4997, 51368], "temperature": 0.0, "avg_logprob": -0.3255894382794698, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0521126464009285}, {"id": 184, "seek": 85654, "start": 876.62, "end": 884.0999999999999, "text": " against OpenMPI, MPICH, and Intel MPI, Mac OS, MPI, and MPICH, and in case of Windows,", "tokens": [51368, 1970, 7238, 12224, 40, 11, 14146, 40, 5462, 11, 293, 19762, 14146, 40, 11, 5707, 12731, 11, 14146, 40, 11, 293, 14146, 40, 5462, 11, 293, 294, 1389, 295, 8591, 11, 51742], "temperature": 0.0, "avg_logprob": -0.3255894382794698, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0521126464009285}, {"id": 185, "seek": 88410, "start": 884.34, "end": 888.1800000000001, "text": " of course, MSMPI implementation.", "tokens": [50376, 295, 1164, 11, 7395, 12224, 40, 11420, 13, 50568], "temperature": 0.0, "avg_logprob": -0.32786143376277044, "compression_ratio": 1.355421686746988, "no_speech_prob": 0.03500109165906906}, {"id": 186, "seek": 88410, "start": 888.1800000000001, "end": 901.3000000000001, "text": " But when we are talking about MPICH, there is a problem that has recently occurred, namely", "tokens": [50568, 583, 562, 321, 366, 1417, 466, 14146, 40, 5462, 11, 456, 307, 257, 1154, 300, 575, 3938, 11068, 11, 20926, 51224], "temperature": 0.0, "avg_logprob": -0.32786143376277044, "compression_ratio": 1.355421686746988, "no_speech_prob": 0.03500109165906906}, {"id": 187, "seek": 88410, "start": 901.3000000000001, "end": 907.94, "text": " starting from version 4 of MPICH, it fails for on Ubuntu on our CI for Python version", "tokens": [51224, 2891, 490, 3037, 1017, 295, 14146, 40, 5462, 11, 309, 18199, 337, 322, 30230, 45605, 322, 527, 37777, 337, 15329, 3037, 51556], "temperature": 0.0, "avg_logprob": -0.32786143376277044, "compression_ratio": 1.355421686746988, "no_speech_prob": 0.03500109165906906}, {"id": 188, "seek": 88410, "start": 907.94, "end": 910.6600000000001, "text": " less than 3.10.", "tokens": [51556, 1570, 813, 805, 13, 3279, 13, 51692], "temperature": 0.0, "avg_logprob": -0.32786143376277044, "compression_ratio": 1.355421686746988, "no_speech_prob": 0.03500109165906906}, {"id": 189, "seek": 91066, "start": 910.66, "end": 918.02, "text": " So if anyone has ideas how to fix it, please contact us, we will appreciate any help.", "tokens": [50364, 407, 498, 2878, 575, 3487, 577, 281, 3191, 309, 11, 1767, 3385, 505, 11, 321, 486, 4449, 604, 854, 13, 50732], "temperature": 0.0, "avg_logprob": -0.31578206380208335, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.00522764865309}, {"id": 190, "seek": 91066, "start": 918.02, "end": 926.74, "text": " Okay, so sample, we are running the unit tests on different systems and so on.", "tokens": [50732, 1033, 11, 370, 6889, 11, 321, 366, 2614, 264, 4985, 6921, 322, 819, 3652, 293, 370, 322, 13, 51168], "temperature": 0.0, "avg_logprob": -0.31578206380208335, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.00522764865309}, {"id": 191, "seek": 91066, "start": 926.74, "end": 929.06, "text": " Let's see the sample unit test.", "tokens": [51168, 961, 311, 536, 264, 6889, 4985, 1500, 13, 51284], "temperature": 0.0, "avg_logprob": -0.31578206380208335, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.00522764865309}, {"id": 192, "seek": 91066, "start": 929.06, "end": 940.22, "text": " In this test, we are testing the logic of the wrapper of the broadcast function of MPI", "tokens": [51284, 682, 341, 1500, 11, 321, 366, 4997, 264, 9952, 295, 264, 46906, 295, 264, 9975, 2445, 295, 14146, 40, 51842], "temperature": 0.0, "avg_logprob": -0.31578206380208335, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.00522764865309}, {"id": 193, "seek": 94022, "start": 940.4200000000001, "end": 946.4200000000001, "text": " and the main thing that you should remember from this slide is that we are testing this", "tokens": [50374, 293, 264, 2135, 551, 300, 291, 820, 1604, 490, 341, 4137, 307, 300, 321, 366, 4997, 341, 50674], "temperature": 0.0, "avg_logprob": -0.3163472866189891, "compression_ratio": 1.5301204819277108, "no_speech_prob": 0.011302469298243523}, {"id": 194, "seek": 94022, "start": 946.4200000000001, "end": 956.34, "text": " function in plain Python implementation as well as Github compiled by Namba.", "tokens": [50674, 2445, 294, 11121, 15329, 11420, 382, 731, 382, 460, 355, 836, 36548, 538, 426, 23337, 13, 51170], "temperature": 0.0, "avg_logprob": -0.3163472866189891, "compression_ratio": 1.5301204819277108, "no_speech_prob": 0.011302469298243523}, {"id": 195, "seek": 94022, "start": 956.34, "end": 964.94, "text": " We have also set up an integration test, the integration test is in another project named", "tokens": [51170, 492, 362, 611, 992, 493, 364, 10980, 1500, 11, 264, 10980, 1500, 307, 294, 1071, 1716, 4926, 51600], "temperature": 0.0, "avg_logprob": -0.3163472866189891, "compression_ratio": 1.5301204819277108, "no_speech_prob": 0.011302469298243523}, {"id": 196, "seek": 96494, "start": 964.94, "end": 970.9000000000001, "text": " isopredoperate-les, and this is just a scheme of this test.", "tokens": [50364, 307, 404, 986, 7192, 473, 12, 904, 11, 293, 341, 307, 445, 257, 12232, 295, 341, 1500, 13, 50662], "temperature": 0.0, "avg_logprob": -0.37333601399471883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0336303636431694}, {"id": 197, "seek": 96494, "start": 970.9000000000001, "end": 979.62, "text": " We are starting from providing the initial conditions for the APDS solver, and these initial", "tokens": [50662, 492, 366, 2891, 490, 6530, 264, 5883, 4487, 337, 264, 5372, 11844, 1404, 331, 11, 293, 613, 5883, 51098], "temperature": 0.0, "avg_logprob": -0.37333601399471883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0336303636431694}, {"id": 198, "seek": 96494, "start": 979.62, "end": 987.0600000000001, "text": " conditions are written to the HDF5 file.", "tokens": [51098, 4487, 366, 3720, 281, 264, 12149, 37, 20, 3991, 13, 51470], "temperature": 0.0, "avg_logprob": -0.37333601399471883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0336303636431694}, {"id": 199, "seek": 96494, "start": 987.0600000000001, "end": 994.46, "text": " After that, we are running three runs, the first one we run with only one process, the", "tokens": [51470, 2381, 300, 11, 321, 366, 2614, 1045, 6676, 11, 264, 700, 472, 321, 1190, 365, 787, 472, 1399, 11, 264, 51840], "temperature": 0.0, "avg_logprob": -0.37333601399471883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0336303636431694}, {"id": 200, "seek": 99446, "start": 994.46, "end": 1001.34, "text": " second we have two processes, the third three processes, and in each we divide, well, in", "tokens": [50364, 1150, 321, 362, 732, 7555, 11, 264, 2636, 1045, 7555, 11, 293, 294, 1184, 321, 9845, 11, 731, 11, 294, 50708], "temperature": 0.0, "avg_logprob": -0.24010252952575684, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.019423212856054306}, {"id": 201, "seek": 99446, "start": 1001.34, "end": 1010.02, "text": " the first we don't divide the domain, but the other ones we divide the domain accordingly,", "tokens": [50708, 264, 700, 321, 500, 380, 9845, 264, 9274, 11, 457, 264, 661, 2306, 321, 9845, 264, 9274, 19717, 11, 51142], "temperature": 0.0, "avg_logprob": -0.24010252952575684, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.019423212856054306}, {"id": 202, "seek": 99446, "start": 1010.02, "end": 1017.82, "text": " and in the assert state we just compare the results and we want the results to be the", "tokens": [51142, 293, 294, 264, 19810, 1785, 321, 445, 6794, 264, 3542, 293, 321, 528, 264, 3542, 281, 312, 264, 51532], "temperature": 0.0, "avg_logprob": -0.24010252952575684, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.019423212856054306}, {"id": 203, "seek": 99446, "start": 1017.82, "end": 1020.74, "text": " same for different runs.", "tokens": [51532, 912, 337, 819, 6676, 13, 51678], "temperature": 0.0, "avg_logprob": -0.24010252952575684, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.019423212856054306}, {"id": 204, "seek": 102074, "start": 1020.74, "end": 1030.02, "text": " And also these results are also written to HDF5 file.", "tokens": [50364, 400, 611, 613, 3542, 366, 611, 3720, 281, 12149, 37, 20, 3991, 13, 50828], "temperature": 0.0, "avg_logprob": -0.36407544320089774, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.0352126881480217}, {"id": 205, "seek": 102074, "start": 1030.02, "end": 1039.66, "text": " Interesting fact that everything works on Windows except installing HDF5 package for", "tokens": [50828, 14711, 1186, 300, 1203, 1985, 322, 8591, 3993, 20762, 12149, 37, 20, 7372, 337, 51310], "temperature": 0.0, "avg_logprob": -0.36407544320089774, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.0352126881480217}, {"id": 206, "seek": 102074, "start": 1039.66, "end": 1048.78, "text": " concurrent file access, HDF5 package was enabled in PIO, we have troubles setting up on Windows,", "tokens": [51310, 37702, 3991, 2105, 11, 12149, 37, 20, 7372, 390, 15172, 294, 430, 15167, 11, 321, 362, 15379, 3287, 493, 322, 8591, 11, 51766], "temperature": 0.0, "avg_logprob": -0.36407544320089774, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.0352126881480217}, {"id": 207, "seek": 104878, "start": 1048.78, "end": 1056.94, "text": " but everything else works fine, and there is also an independent use case, the PyPD", "tokens": [50364, 457, 1203, 1646, 1985, 2489, 11, 293, 456, 307, 611, 364, 6695, 764, 1389, 11, 264, 9953, 17349, 50772], "temperature": 0.0, "avg_logprob": -0.2769197648571384, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.15085090696811676}, {"id": 208, "seek": 104878, "start": 1056.94, "end": 1066.02, "text": " project that uses our library, our package, and it's not developed by us, so there is", "tokens": [50772, 1716, 300, 4960, 527, 6405, 11, 527, 7372, 11, 293, 309, 311, 406, 4743, 538, 505, 11, 370, 456, 307, 51226], "temperature": 0.0, "avg_logprob": -0.2769197648571384, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.15085090696811676}, {"id": 209, "seek": 104878, "start": 1066.02, "end": 1072.06, "text": " a user, and this is the Python package for solving partial differential equation, it", "tokens": [51226, 257, 4195, 11, 293, 341, 307, 264, 15329, 7372, 337, 12606, 14641, 15756, 5367, 11, 309, 51528], "temperature": 0.0, "avg_logprob": -0.2769197648571384, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.15085090696811676}, {"id": 210, "seek": 107206, "start": 1072.06, "end": 1084.34, "text": " focuses on finite differencing, and these are defined by, I provide it as strings, and", "tokens": [50364, 16109, 322, 19362, 743, 13644, 11, 293, 613, 366, 7642, 538, 11, 286, 2893, 309, 382, 13985, 11, 293, 50978], "temperature": 0.0, "avg_logprob": -0.4249351183573405, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.18411701917648315}, {"id": 211, "seek": 107206, "start": 1084.34, "end": 1089.22, "text": " the solution strategy as follows, we start from partitioning the grid onto different", "tokens": [50978, 264, 3827, 5206, 382, 10002, 11, 321, 722, 490, 24808, 278, 264, 10748, 3911, 819, 51222], "temperature": 0.0, "avg_logprob": -0.4249351183573405, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.18411701917648315}, {"id": 212, "seek": 107206, "start": 1089.22, "end": 1098.54, "text": " nodes using number MPI after add that with partial expressions using the SIMPY and compile", "tokens": [51222, 13891, 1228, 1230, 14146, 40, 934, 909, 300, 365, 14641, 15277, 1228, 264, 24738, 47, 56, 293, 31413, 51688], "temperature": 0.0, "avg_logprob": -0.4249351183573405, "compression_ratio": 1.5144508670520231, "no_speech_prob": 0.18411701917648315}, {"id": 213, "seek": 109854, "start": 1098.54, "end": 1106.3799999999999, "text": " the results using number, and then we trade the PDE exchange in boundary information between", "tokens": [50364, 264, 3542, 1228, 1230, 11, 293, 550, 321, 4923, 264, 10464, 36, 7742, 294, 12866, 1589, 1296, 50756], "temperature": 0.0, "avg_logprob": -0.27332622864667105, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.17358015477657318}, {"id": 214, "seek": 109854, "start": 1106.3799999999999, "end": 1111.42, "text": " the nodes using number MPI.", "tokens": [50756, 264, 13891, 1228, 1230, 14146, 40, 13, 51008], "temperature": 0.0, "avg_logprob": -0.27332622864667105, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.17358015477657318}, {"id": 215, "seek": 109854, "start": 1111.42, "end": 1118.98, "text": " Take home messages, there is a common mismatch between the Python language and Python ecosystem,", "tokens": [51008, 3664, 1280, 7897, 11, 456, 307, 257, 2689, 23220, 852, 1296, 264, 15329, 2856, 293, 15329, 11311, 11, 51386], "temperature": 0.0, "avg_logprob": -0.27332622864667105, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.17358015477657318}, {"id": 216, "seek": 109854, "start": 1118.98, "end": 1125.26, "text": " we should remember that the language could be slow, but we also should consider the", "tokens": [51386, 321, 820, 1604, 300, 264, 2856, 727, 312, 2964, 11, 457, 321, 611, 820, 1949, 264, 51700], "temperature": 0.0, "avg_logprob": -0.27332622864667105, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.17358015477657318}, {"id": 217, "seek": 112526, "start": 1125.26, "end": 1132.62, "text": " ecosystem around this language, the libraries that are available, the libraries that are", "tokens": [50364, 11311, 926, 341, 2856, 11, 264, 15148, 300, 366, 2435, 11, 264, 15148, 300, 366, 50732], "temperature": 0.0, "avg_logprob": -0.23296599601631734, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.12975242733955383}, {"id": 218, "seek": 112526, "start": 1132.62, "end": 1140.14, "text": " available, and probably different implementations, and Python has a range of global HPC solutions", "tokens": [50732, 2435, 11, 293, 1391, 819, 4445, 763, 11, 293, 15329, 575, 257, 3613, 295, 4338, 12557, 34, 6547, 51108], "temperature": 0.0, "avg_logprob": -0.23296599601631734, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.12975242733955383}, {"id": 219, "seek": 112526, "start": 1140.14, "end": 1147.3, "text": " such as just-in-time compilation, GPU programming, multi-trading, and MPI, and in case of number", "tokens": [51108, 1270, 382, 445, 12, 259, 12, 3766, 40261, 11, 18407, 9410, 11, 4825, 12, 43831, 278, 11, 293, 14146, 40, 11, 293, 294, 1389, 295, 1230, 51466], "temperature": 0.0, "avg_logprob": -0.23296599601631734, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.12975242733955383}, {"id": 220, "seek": 114730, "start": 1147.3, "end": 1160.3, "text": " MPI, this is the package to glue the MPI with LFMG compiled Python code, it is tested", "tokens": [50364, 14146, 40, 11, 341, 307, 264, 7372, 281, 8998, 264, 14146, 40, 365, 441, 37, 44, 38, 36548, 15329, 3089, 11, 309, 307, 8246, 51014], "temperature": 0.0, "avg_logprob": -0.29427051544189453, "compression_ratio": 1.2285714285714286, "no_speech_prob": 0.43146565556526184}, {"id": 221, "seek": 114730, "start": 1160.3, "end": 1174.98, "text": " on CI, on GitHub Actions, we are aiming for 100% unit test coverage, and also there is", "tokens": [51014, 322, 37777, 11, 322, 23331, 3251, 626, 11, 321, 366, 20253, 337, 2319, 4, 4985, 1500, 9645, 11, 293, 611, 456, 307, 51748], "temperature": 0.0, "avg_logprob": -0.29427051544189453, "compression_ratio": 1.2285714285714286, "no_speech_prob": 0.43146565556526184}, {"id": 222, "seek": 117498, "start": 1174.98, "end": 1185.26, "text": " also already the two projects that are dependent on this package, here you can find the links", "tokens": [50364, 611, 1217, 264, 732, 4455, 300, 366, 12334, 322, 341, 7372, 11, 510, 291, 393, 915, 264, 6123, 50878], "temperature": 0.0, "avg_logprob": -0.2678125325371237, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.2354009747505188}, {"id": 223, "seek": 117498, "start": 1185.26, "end": 1192.18, "text": " for number MPI, the GitHub links, and also the links to the packages at PyPy and Anaconda,", "tokens": [50878, 337, 1230, 14146, 40, 11, 264, 23331, 6123, 11, 293, 611, 264, 6123, 281, 264, 17401, 412, 9953, 47, 88, 293, 1107, 326, 12233, 11, 51224], "temperature": 0.0, "avg_logprob": -0.2678125325371237, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.2354009747505188}, {"id": 224, "seek": 117498, "start": 1192.18, "end": 1200.82, "text": " and we also welcome contributions, the first two issues I have mentioned earlier, and we", "tokens": [51224, 293, 321, 611, 2928, 15725, 11, 264, 700, 732, 2663, 286, 362, 2835, 3071, 11, 293, 321, 51656], "temperature": 0.0, "avg_logprob": -0.2678125325371237, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.2354009747505188}, {"id": 225, "seek": 120082, "start": 1200.82, "end": 1207.6599999999999, "text": " also welcome and encourage to provide the logo for number MPI, as well as adding support", "tokens": [50364, 611, 2928, 293, 5373, 281, 2893, 264, 9699, 337, 1230, 14146, 40, 11, 382, 731, 382, 5127, 1406, 50706], "temperature": 0.0, "avg_logprob": -0.25991296768188477, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.04040191322565079}, {"id": 226, "seek": 120082, "start": 1207.6599999999999, "end": 1215.78, "text": " for the other functions, or we are also aiming for dropping dependency on MPI for Py in our", "tokens": [50706, 337, 264, 661, 6828, 11, 420, 321, 366, 611, 20253, 337, 13601, 33621, 322, 14146, 40, 337, 9953, 294, 527, 51112], "temperature": 0.0, "avg_logprob": -0.25991296768188477, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.04040191322565079}, {"id": 227, "seek": 120082, "start": 1215.78, "end": 1225.46, "text": " project, and also the plan is to benchmark the performance of this package, and we also", "tokens": [51112, 1716, 11, 293, 611, 264, 1393, 307, 281, 18927, 264, 3389, 295, 341, 7372, 11, 293, 321, 611, 51596], "temperature": 0.0, "avg_logprob": -0.25991296768188477, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.04040191322565079}, {"id": 228, "seek": 122546, "start": 1225.46, "end": 1232.18, "text": " we wanted to acknowledge funding, the project was funded by National Science Centre of Poland,", "tokens": [50364, 321, 1415, 281, 10692, 6137, 11, 264, 1716, 390, 14385, 538, 4862, 8976, 20764, 295, 15950, 11, 50700], "temperature": 0.0, "avg_logprob": -0.34254105885823566, "compression_ratio": 1.303030303030303, "no_speech_prob": 0.1473127156496048}, {"id": 229, "seek": 122546, "start": 1232.18, "end": 1246.98, "text": " so thank you for your attention, and probably we now have time for questions.", "tokens": [50700, 370, 1309, 291, 337, 428, 3202, 11, 293, 1391, 321, 586, 362, 565, 337, 1651, 13, 51440], "temperature": 0.0, "avg_logprob": -0.34254105885823566, "compression_ratio": 1.303030303030303, "no_speech_prob": 0.1473127156496048}, {"id": 230, "seek": 124698, "start": 1246.98, "end": 1256.98, "text": " Thank you very much, any questions?", "tokens": [50364, 1044, 291, 588, 709, 11, 604, 1651, 30, 50864], "temperature": 0.0, "avg_logprob": -0.2774068832397461, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.5198334455490112}, {"id": 231, "seek": 124698, "start": 1256.98, "end": 1259.3, "text": " Question from an MPI expert?", "tokens": [50864, 14464, 490, 364, 14146, 40, 5844, 30, 50980], "temperature": 0.0, "avg_logprob": -0.2774068832397461, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.5198334455490112}, {"id": 232, "seek": 124698, "start": 1259.3, "end": 1266.94, "text": " Hello, thank you for the talk, so the interface you are proposing is very close to the let's", "tokens": [50980, 2425, 11, 1309, 291, 337, 264, 751, 11, 370, 264, 9226, 291, 366, 29939, 307, 588, 1998, 281, 264, 718, 311, 51362], "temperature": 0.0, "avg_logprob": -0.2774068832397461, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.5198334455490112}, {"id": 233, "seek": 124698, "start": 1266.94, "end": 1272.7, "text": " say CMPI interface, let's say when you do a send you work with a buffer, or do you try", "tokens": [51362, 584, 383, 12224, 40, 9226, 11, 718, 311, 584, 562, 291, 360, 257, 2845, 291, 589, 365, 257, 21762, 11, 420, 360, 291, 853, 51650], "temperature": 0.0, "avg_logprob": -0.2774068832397461, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.5198334455490112}, {"id": 234, "seek": 127270, "start": 1272.7, "end": 1279.42, "text": " to provide a bit higher level interface, for example, serializing some Python object,", "tokens": [50364, 281, 2893, 257, 857, 2946, 1496, 9226, 11, 337, 1365, 11, 17436, 3319, 512, 15329, 2657, 11, 50700], "temperature": 0.0, "avg_logprob": -0.3442156011408026, "compression_ratio": 1.3642384105960266, "no_speech_prob": 0.13970163464546204}, {"id": 235, "seek": 127270, "start": 1279.42, "end": 1289.42, "text": " or it could be very useful.", "tokens": [50700, 420, 309, 727, 312, 588, 4420, 13, 51200], "temperature": 0.0, "avg_logprob": -0.3442156011408026, "compression_ratio": 1.3642384105960266, "no_speech_prob": 0.13970163464546204}, {"id": 236, "seek": 127270, "start": 1289.42, "end": 1297.02, "text": " Yeah, the interface is as slim thin as possible probably, very close to the CMPI, one of the", "tokens": [51200, 865, 11, 264, 9226, 307, 382, 25357, 5862, 382, 1944, 1391, 11, 588, 1998, 281, 264, 383, 12224, 40, 11, 472, 295, 264, 51580], "temperature": 0.0, "avg_logprob": -0.3442156011408026, "compression_ratio": 1.3642384105960266, "no_speech_prob": 0.13970163464546204}, {"id": 237, "seek": 129702, "start": 1297.02, "end": 1304.82, "text": " reasons being that within Namba and Jitted Code, probably things like serialization might", "tokens": [50364, 4112, 885, 300, 1951, 426, 23337, 293, 508, 3944, 15549, 11, 1391, 721, 411, 17436, 2144, 1062, 50754], "temperature": 0.0, "avg_logprob": -0.19962567252081795, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.1633974313735962}, {"id": 238, "seek": 129702, "start": 1304.82, "end": 1312.86, "text": " not be that easy to do, there is no problem in combining MPI for Py and Namba MPI in one", "tokens": [50754, 406, 312, 300, 1858, 281, 360, 11, 456, 307, 572, 1154, 294, 21928, 14146, 40, 337, 9953, 293, 426, 23337, 14146, 40, 294, 472, 51156], "temperature": 0.0, "avg_logprob": -0.19962567252081795, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.1633974313735962}, {"id": 239, "seek": 129702, "start": 1312.86, "end": 1318.1399999999999, "text": " code base, so when you are out of the Jitted Code, you can use MPI for Py, which has high", "tokens": [51156, 3089, 3096, 11, 370, 562, 291, 366, 484, 295, 264, 508, 3944, 15549, 11, 291, 393, 764, 14146, 40, 337, 9953, 11, 597, 575, 1090, 51420], "temperature": 0.0, "avg_logprob": -0.19962567252081795, "compression_ratio": 1.5402298850574712, "no_speech_prob": 0.1633974313735962}, {"id": 240, "seek": 131814, "start": 1318.14, "end": 1326.3000000000002, "text": " level things like serialization, et cetera, so you can use it there, but within LLVM compiled", "tokens": [50364, 1496, 721, 411, 17436, 2144, 11, 1030, 11458, 11, 370, 291, 393, 764, 309, 456, 11, 457, 1951, 441, 43, 53, 44, 36548, 50772], "temperature": 0.0, "avg_logprob": -0.23971142180978436, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.31884142756462097}, {"id": 241, "seek": 131814, "start": 1326.3000000000002, "end": 1333.5400000000002, "text": " blocks, you can use Namba MPI for simple send, receive, already use, I mean, without higher", "tokens": [50772, 8474, 11, 291, 393, 764, 426, 23337, 14146, 40, 337, 2199, 2845, 11, 4774, 11, 1217, 764, 11, 286, 914, 11, 1553, 2946, 51134], "temperature": 0.0, "avg_logprob": -0.23971142180978436, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.31884142756462097}, {"id": 242, "seek": 131814, "start": 1333.5400000000002, "end": 1342.8200000000002, "text": " level array functioning, having said that we, for example, handle transparently non-contiguous", "tokens": [51134, 1496, 10225, 18483, 11, 1419, 848, 300, 321, 11, 337, 1365, 11, 4813, 7132, 6420, 2107, 12, 9000, 30525, 51598], "temperature": 0.0, "avg_logprob": -0.23971142180978436, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.31884142756462097}, {"id": 243, "seek": 134282, "start": 1343.02, "end": 1352.4199999999998, "text": " devices of arrays, we also, yeah, there are some things that are higher level than C interface,", "tokens": [50374, 5759, 295, 41011, 11, 321, 611, 11, 1338, 11, 456, 366, 512, 721, 300, 366, 2946, 1496, 813, 383, 9226, 11, 50844], "temperature": 0.0, "avg_logprob": -0.3238096237182617, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.09617593884468079}, {"id": 244, "seek": 134282, "start": 1352.4199999999998, "end": 1357.4199999999998, "text": " but in general, we try to provide wrapper around the C routines.", "tokens": [50844, 457, 294, 2674, 11, 321, 853, 281, 2893, 46906, 926, 264, 383, 33827, 13, 51094], "temperature": 0.0, "avg_logprob": -0.3238096237182617, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.09617593884468079}, {"id": 245, "seek": 134282, "start": 1357.4199999999998, "end": 1362.78, "text": " Okay, thank you.", "tokens": [51094, 1033, 11, 1309, 291, 13, 51362], "temperature": 0.0, "avg_logprob": -0.3238096237182617, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.09617593884468079}, {"id": 246, "seek": 134282, "start": 1362.78, "end": 1365.78, "text": " Any other questions?", "tokens": [51362, 2639, 661, 1651, 30, 51512], "temperature": 0.0, "avg_logprob": -0.3238096237182617, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.09617593884468079}, {"id": 247, "seek": 136578, "start": 1366.74, "end": 1375.22, "text": " Thanks for a great talk, it seems really interesting what you are working on, I have got a couple", "tokens": [50412, 2561, 337, 257, 869, 751, 11, 309, 2544, 534, 1880, 437, 291, 366, 1364, 322, 11, 286, 362, 658, 257, 1916, 50836], "temperature": 0.0, "avg_logprob": -0.21555103725857205, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.02024347335100174}, {"id": 248, "seek": 136578, "start": 1375.22, "end": 1380.58, "text": " of questions, probably born out of ignorance, but I just kind of wondered if you could help", "tokens": [50836, 295, 1651, 11, 1391, 4232, 484, 295, 25390, 11, 457, 286, 445, 733, 295, 17055, 498, 291, 727, 854, 51104], "temperature": 0.0, "avg_logprob": -0.21555103725857205, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.02024347335100174}, {"id": 249, "seek": 136578, "start": 1380.58, "end": 1386.1399999999999, "text": " me with them, so firstly, I was wondering why you went with making a separate package", "tokens": [51104, 385, 365, 552, 11, 370, 27376, 11, 286, 390, 6359, 983, 291, 1437, 365, 1455, 257, 4994, 7372, 51382], "temperature": 0.0, "avg_logprob": -0.21555103725857205, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.02024347335100174}, {"id": 250, "seek": 136578, "start": 1386.1399999999999, "end": 1394.74, "text": " rather than sort of trying to build this functionality on top of MPI for Py, would it have been possible", "tokens": [51382, 2831, 813, 1333, 295, 1382, 281, 1322, 341, 14980, 322, 1192, 295, 14146, 40, 337, 9953, 11, 576, 309, 362, 668, 1944, 51812], "temperature": 0.0, "avg_logprob": -0.21555103725857205, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.02024347335100174}, {"id": 251, "seek": 139474, "start": 1394.82, "end": 1401.18, "text": " to sort of add this, add the feature of making things jit-compilable into MPI for Py, and", "tokens": [50368, 281, 1333, 295, 909, 341, 11, 909, 264, 4111, 295, 1455, 721, 361, 270, 12, 21541, 388, 712, 666, 14146, 40, 337, 9953, 11, 293, 50686], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 252, "seek": 139474, "start": 1401.18, "end": 1406.5, "text": " secondly, I was kind of wondering with the MPI IO thing that you were looking at with", "tokens": [50686, 26246, 11, 286, 390, 733, 295, 6359, 365, 264, 14146, 40, 39839, 551, 300, 291, 645, 1237, 412, 365, 50952], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 253, "seek": 139474, "start": 1406.5, "end": 1414.06, "text": " Windows, if that requires kind of concurrent file access from separate processes in Windows,", "tokens": [50952, 8591, 11, 498, 300, 7029, 733, 295, 37702, 3991, 2105, 490, 4994, 7555, 294, 8591, 11, 51330], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 254, "seek": 139474, "start": 1414.06, "end": 1419.3, "text": " is that just a complete, completely a no-go for Windows, because I understand that's something", "tokens": [51330, 307, 300, 445, 257, 3566, 11, 2584, 257, 572, 12, 1571, 337, 8591, 11, 570, 286, 1223, 300, 311, 746, 51592], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 255, "seek": 139474, "start": 1419.3, "end": 1422.18, "text": " that Windows kernel doesn't support.", "tokens": [51592, 300, 8591, 28256, 1177, 380, 1406, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 256, "seek": 139474, "start": 1422.18, "end": 1423.18, "text": " Thank you.", "tokens": [51736, 1044, 291, 13, 51786], "temperature": 0.0, "avg_logprob": -0.2511038596813495, "compression_ratio": 1.6639676113360324, "no_speech_prob": 0.027994977310299873}, {"id": 257, "seek": 142318, "start": 1423.42, "end": 1426.38, "text": " Thanks, let me start from the second one.", "tokens": [50376, 2561, 11, 718, 385, 722, 490, 264, 1150, 472, 13, 50524], "temperature": 0.0, "avg_logprob": -0.18767801920572916, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00989059079438448}, {"id": 258, "seek": 142318, "start": 1426.38, "end": 1434.38, "text": " So here our, well, essentially it's a fun fact that everything else worked for Windows,", "tokens": [50524, 407, 510, 527, 11, 731, 11, 4476, 309, 311, 257, 1019, 1186, 300, 1203, 1646, 2732, 337, 8591, 11, 50924], "temperature": 0.0, "avg_logprob": -0.18767801920572916, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00989059079438448}, {"id": 259, "seek": 142318, "start": 1434.38, "end": 1439.46, "text": " we do not really target Windows, but it was nice to observe that all works, it's kind", "tokens": [50924, 321, 360, 406, 534, 3779, 8591, 11, 457, 309, 390, 1481, 281, 11441, 300, 439, 1985, 11, 309, 311, 733, 51178], "temperature": 0.0, "avg_logprob": -0.18767801920572916, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00989059079438448}, {"id": 260, "seek": 142318, "start": 1439.46, "end": 1447.3400000000001, "text": " of one of these advantages of Python that you code and you don't really need to take", "tokens": [51178, 295, 472, 295, 613, 14906, 295, 15329, 300, 291, 3089, 293, 291, 500, 380, 534, 643, 281, 747, 51572], "temperature": 0.0, "avg_logprob": -0.18767801920572916, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00989059079438448}, {"id": 261, "seek": 142318, "start": 1447.3400000000001, "end": 1452.42, "text": " too much care about the targeted platforms, because the underlying packages are meant", "tokens": [51572, 886, 709, 1127, 466, 264, 15045, 9473, 11, 570, 264, 14217, 17401, 366, 4140, 51826], "temperature": 0.0, "avg_logprob": -0.18767801920572916, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00989059079438448}, {"id": 262, "seek": 145242, "start": 1452.42, "end": 1458.6200000000001, "text": " to work on all of them, and here everything works with Microsoft MPI, the only thing that", "tokens": [50364, 281, 589, 322, 439, 295, 552, 11, 293, 510, 1203, 1985, 365, 8116, 14146, 40, 11, 264, 787, 551, 300, 50674], "temperature": 0.0, "avg_logprob": -0.14204092661539713, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.006653532851487398}, {"id": 263, "seek": 145242, "start": 1458.6200000000001, "end": 1465.54, "text": " actually was a problem for us was to even install H5py on Windows with MPI support.", "tokens": [50674, 767, 390, 257, 1154, 337, 505, 390, 281, 754, 3625, 389, 20, 8200, 322, 8591, 365, 14146, 40, 1406, 13, 51020], "temperature": 0.0, "avg_logprob": -0.14204092661539713, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.006653532851487398}, {"id": 264, "seek": 145242, "start": 1465.54, "end": 1472.66, "text": " So we don't really know what's the true bottleneck, but even the documentation of H5py suggests", "tokens": [51020, 407, 321, 500, 380, 534, 458, 437, 311, 264, 2074, 44641, 547, 11, 457, 754, 264, 14333, 295, 389, 20, 8200, 13409, 51376], "temperature": 0.0, "avg_logprob": -0.14204092661539713, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.006653532851487398}, {"id": 265, "seek": 145242, "start": 1472.66, "end": 1475.0600000000002, "text": " against trying.", "tokens": [51376, 1970, 1382, 13, 51496], "temperature": 0.0, "avg_logprob": -0.14204092661539713, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.006653532851487398}, {"id": 266, "seek": 147506, "start": 1475.06, "end": 1481.82, "text": " For the first question, why do we create, why do we develop a separate package instead", "tokens": [50364, 1171, 264, 700, 1168, 11, 983, 360, 321, 1884, 11, 983, 360, 321, 1499, 257, 4994, 7372, 2602, 50702], "temperature": 0.0, "avg_logprob": -0.14932043607844864, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0617901049554348}, {"id": 267, "seek": 147506, "start": 1481.82, "end": 1485.8999999999999, "text": " of adding it on top of MPI 4Py?", "tokens": [50702, 295, 5127, 309, 322, 1192, 295, 14146, 40, 1017, 47, 88, 30, 50906], "temperature": 0.0, "avg_logprob": -0.14932043607844864, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0617901049554348}, {"id": 268, "seek": 147506, "start": 1485.8999999999999, "end": 1495.26, "text": " So I think even on the slide with the story of the package, there was a link to, yeah,", "tokens": [50906, 407, 286, 519, 754, 322, 264, 4137, 365, 264, 1657, 295, 264, 7372, 11, 456, 390, 257, 2113, 281, 11, 1338, 11, 51374], "temperature": 0.0, "avg_logprob": -0.14932043607844864, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0617901049554348}, {"id": 269, "seek": 147506, "start": 1495.26, "end": 1501.7, "text": " there's a link to MPI 4Py issue, the bottom footnote, where we suggested would it be possible", "tokens": [51374, 456, 311, 257, 2113, 281, 14146, 40, 1017, 47, 88, 2734, 11, 264, 2767, 2671, 22178, 11, 689, 321, 10945, 576, 309, 312, 1944, 51696], "temperature": 0.0, "avg_logprob": -0.14932043607844864, "compression_ratio": 1.5736842105263158, "no_speech_prob": 0.0617901049554348}, {"id": 270, "seek": 150170, "start": 1501.7, "end": 1510.54, "text": " to add it, and in relation to the first question, so probably the scope, the goal of MPI 4Py", "tokens": [50364, 281, 909, 309, 11, 293, 294, 9721, 281, 264, 700, 1168, 11, 370, 1391, 264, 11923, 11, 264, 3387, 295, 14146, 40, 1017, 47, 88, 50806], "temperature": 0.0, "avg_logprob": -0.16464031896283549, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.01830868050456047}, {"id": 271, "seek": 150170, "start": 1510.54, "end": 1520.5800000000002, "text": " is to provide very high level API for MPI in Python.", "tokens": [50806, 307, 281, 2893, 588, 1090, 1496, 9362, 337, 14146, 40, 294, 15329, 13, 51308], "temperature": 0.0, "avg_logprob": -0.16464031896283549, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.01830868050456047}, {"id": 272, "seek": 150170, "start": 1520.5800000000002, "end": 1526.42, "text": " So with discussing with the developers there, we realized that it's probably not within", "tokens": [51308, 407, 365, 10850, 365, 264, 8849, 456, 11, 321, 5334, 300, 309, 311, 1391, 406, 1951, 51600], "temperature": 0.0, "avg_logprob": -0.16464031896283549, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.01830868050456047}, {"id": 273, "seek": 152642, "start": 1526.42, "end": 1534.94, "text": " the scope of a very high level interface, so we started off with just, well, small separate", "tokens": [50364, 264, 11923, 295, 257, 588, 1090, 1496, 9226, 11, 370, 321, 1409, 766, 365, 445, 11, 731, 11, 1359, 4994, 50790], "temperature": 0.0, "avg_logprob": -0.188560684521993, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.0210159569978714}, {"id": 274, "seek": 152642, "start": 1534.94, "end": 1541.94, "text": " project, but I mean, well, great idea, it could be glued together, as of now we aim for dropping", "tokens": [50790, 1716, 11, 457, 286, 914, 11, 731, 11, 869, 1558, 11, 309, 727, 312, 28008, 1214, 11, 382, 295, 586, 321, 5939, 337, 13601, 51140], "temperature": 0.0, "avg_logprob": -0.188560684521993, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.0210159569978714}, {"id": 275, "seek": 152642, "start": 1541.94, "end": 1548.02, "text": " dependency on MPI 4Py, which we now use just for some utility routine, not for the communication", "tokens": [51140, 33621, 322, 14146, 40, 1017, 47, 88, 11, 597, 321, 586, 764, 445, 337, 512, 14877, 9927, 11, 406, 337, 264, 6101, 51444], "temperature": 0.0, "avg_logprob": -0.188560684521993, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.0210159569978714}, {"id": 276, "seek": 152642, "start": 1548.02, "end": 1555.7, "text": " or nothing that is used by Namba, and probably that might be an advantage, because you can", "tokens": [51444, 420, 1825, 300, 307, 1143, 538, 426, 23337, 11, 293, 1391, 300, 1062, 312, 364, 5002, 11, 570, 291, 393, 51828], "temperature": 0.0, "avg_logprob": -0.188560684521993, "compression_ratio": 1.5537190082644627, "no_speech_prob": 0.0210159569978714}, {"id": 277, "seek": 155570, "start": 1556.46, "end": 1562.98, "text": " eventually you should be able to install Namba MPI with very little other dependencies,", "tokens": [50402, 4728, 291, 820, 312, 1075, 281, 3625, 426, 23337, 14146, 40, 365, 588, 707, 661, 36606, 11, 50728], "temperature": 0.0, "avg_logprob": -0.2507432480932961, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.03976329043507576}, {"id": 278, "seek": 155570, "start": 1562.98, "end": 1568.7, "text": " and Namba MPI is written purely in Python, so installing it, you do not need to have", "tokens": [50728, 293, 426, 23337, 14146, 40, 307, 3720, 17491, 294, 15329, 11, 370, 20762, 309, 11, 291, 360, 406, 643, 281, 362, 51014], "temperature": 0.0, "avg_logprob": -0.2507432480932961, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.03976329043507576}, {"id": 279, "seek": 155570, "start": 1568.7, "end": 1573.82, "text": " any Python related C-compiled code, and you can do it quite easily.", "tokens": [51014, 604, 15329, 4077, 383, 12, 21541, 7292, 3089, 11, 293, 291, 393, 360, 309, 1596, 3612, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2507432480932961, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.03976329043507576}, {"id": 280, "seek": 155570, "start": 1573.82, "end": 1577.42, "text": " Okay, thank you very much.", "tokens": [51270, 1033, 11, 1309, 291, 588, 709, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2507432480932961, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.03976329043507576}], "language": "en"}