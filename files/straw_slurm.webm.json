{"text": " Okay, next talk is Pablo. Who is going to explain us how to set up slurm client environments more easily. My name is Pablo and I have been running the HBC Clusters since I was 9 years. I was running the HBC Clusters at CERN and got involved mostly in slurm, running slurm. That's when I came up with the idea for this tool since about 8 or 9 months ago I started the HBC Clusters and I'm also participating in the SKA project, hence the pretty background, where we do also things related to the HBC infrastructure. So just a brief introduction to slurm in case anybody is not familiar with it. Slurm is basically both a resource manager and a job scheduler, meaning slurm will manage their allocations, it will track which machines are used to which jobs and which users own, which CPUs and which nodes, etc. And it's also the job scheduler, meaning it will, when users submit jobs, you have your happy users over there, or hopefully it will be happy users. And they can be one-on-one on your cluster, so they make a job submission, usually writing a script that launches some workloads. And they will basically interact with slurm and slurm will manage all of these job submissions. You won't just have one by one, you will have hundreds or even thousands of jobs that are scheduled to run on your infrastructure and slurm will manage the views and the priorities and the accounting, etc. So basically it's a batch manager, but there's both resource managing and the scheduling of the jobs. Building a bit deeper into how slurm works, because this is relevant for this talk, there's basically two main components, two units that are the most relevant, and those are the controller, which is called the slurm CTLD, and then the deals that run on the worker nodes at the bottom, which is the slurm VDU. And then you have other demons like the slurm VD, slurm RST, slurm RST. Those are not relevant for this talk, I will mostly focus on the part on the left here. So users and client tools, they basically interact with the controller over a slurm protocol. There's nowadays a slurm RST, so you can also interact with the rest with some scripts, but mostly all the user lab tools, and mostly almost everything in the slurm ecosystem just talks to the slurm CTLD, and this controller handles the source of truth for slurm, so it knows which resources are allocated where, it knows which jobs exist, and knows who the users are, etc. The controller talks to the slurm units, and talking to the nodes and the slurm units are in charge of launching the jobs, so you do the cleanup, setting up the seed routes for the jobs, whatever you have. Now, what's important here is to know that for all of this to work, you need at least the same thing. You need the slurm conflict files, and they need to be instinct between the whole cluster, so you may have some difference, but mostly it should be the same. There was no audio online? Okay. So as I was saying, the slurm CTLD handles the source of truth. The slurm units are in charge of launching the jobs, and the two important things are that you need the slurm configuration files. It's mostly the slurm.conf file, but there's other files as well. Those need to be in sync in the whole cluster, and they need to be basically the same. They should have the same hash, ideally. And then you should also have a shared secret so that nobody can, a rogue client cannot just add a worker node to the cluster and start doing malicious things. So you have usually it's a munch secret. It's called the demon called munch, and you have a shared secret as well for the whole cluster. And this fact is important, is very relevant for this talk. Now, up to containers. So containers are increasingly becoming a super popular tool to run infrastructure for reproducibility, for automating deployments. And just in general, they're becoming super ubiquitous in our industry. I think for good reasons. And there are, I think, very good use cases for using containers with slurm. In this talk, I will focus on the use case where you use containers on the user and client side of things. So those tools that will talk to slurm, to the controller mostly, to do things on the cluster. So this could be some automation that you have run to do whatever. For instance, you could use it for monitoring purposes. You could write a tool that does health checks on the cluster for accounting. I've used it extensively for accounting as well. But also integration with other services, right? Or if you want to connect the Jupyter notebook with slurm, you will end up with some tools that talk to the controller. Now, there are basically two scenarios in which you can use containers with slurm. On the left, we have the local use case. That means imagine you have a frontend mode, you have a machine that's configured where it uses SSH2. And from there, they can run the slurm commands to launch jobs, to track their job usage, et cetera. It's conventionally called frontend mode for the cluster. So if you just add the slurm client container on that node, it's very simple. Because you can just, as I said, you need a secret with munch, and you need the config files. And that scenario is very simple because you can just do bind mounts, and you can access the munch socket to talk to slurm. And you might bind mount the slurm config directory, and you're done, basically. So that's sort of easy. However, what if you have, for the use case on the right, you have the distributed or remote use case. And in that case, you may run your slurm client container in a different service. That's a different network, or you may run it on Kubernetes or somewhere else. In that case, you obviously can't just do the bind mounts because you need to give it all those things. So you would have to give it all the slurm config files and somehow the munch shared key so that your external service can talk to your cluster, right, specifically to the slurm controller. Now, this is an extraction from a Docker file. This is the naive approach. This is how I started trying things. Easy, right? You just take the slurm config, and you just copy it to the destination, right? And this will absolutely work. But I was not happy with this approach because then you end up managing two copies of your slurm config. And I really like having a single source of truth for when you do configuration management and automation of your infrastructure, I really like having a single source of truth. And managing this in this way with containers is very fiddly because it's very easy that you will forget to update it or something that will fail to update it automatically. It's just not ideal. I didn't like this approach, but it will work. It will work. And some of you who know slurm may say, oh, but Pablo, why wouldn't you just use slurm's config less feature? So slurm config less is a new feature since slurm 20 or so that will basically allow a client to just pull the config files from slurm. So the slurm ddemons that run on the worker nodes, when they start, they will just grab the slurm config files. So you can just remove the needs to even copy the slurm config, right? Well, that's a trick question. Not necessarily because then you need to run a slurm ddemon in your container. And you also need the munch demon. And it sounds easy, but it's really not. You will need to do a lot of hacks. This is an instruction from a container that I was creating. And you run in lots of awful things. Like the slurm ddemon expects this release agent file to exist in the C group and the containers, they just don't create it. I tried it on Docker. I tried it on different Kubernetes versions. It just doesn't exist. I don't know why. I couldn't find out why. If anybody knows, please tell me. I googled around a found that could have been related to some privilege escalation issues. However, if you just remount the C groups, the file appears. So I'm not sure what's going on there. Another fun story is that, for instance, if you're using Kubernetes, Kubernetes likes to give a sim link to your secrets, and munch refuses to take the secret from a sim link for security reasons. It makes sense. So there's no more. So you will need to put in hacks. And it's hacks on top of hacks on top of hacks just to run these two demons. And yeah, I was not very happy with this approach either. So basically I was faced with two options. We arrived at this situation. You're faced with two options. Either you basically do the first naive approach where you just copy all the stuff into your slurm container. You manage a copy of your slurm config files. But as I said, if you want a single source of truth, this might not be ideal. You also need, of course, in the case of use case, unless you need munch, and you need to supply the munch key. Or you can try the configless approach, but then you need to add slurm d to your container so it can pull via configless your config files. But then anyway, you also need munch. And you need to add the munch key to your container somehow and managing secrets. I mean, if you're running Kubernetes, it might not be a big issue or some other container manager. But you will still need to maintain all these extra demons with nasty hacks. And we don't always like all these having lots of hacks in our infrastructure. There's a third option, by the way, which is trying to go secret less. It doesn't work in combination with configless, where you try to use JSON web tokens. But it gives a lot of issues. It doesn't really work. I tried it. So I didn't include it here. Just mentioning it in case somebody thought about it. So Pablo, you talked about the bad and the ugly. What about the good? Is there any good part to this? I'm glad you asked. Yes. What if we had a single shot CLI tool, that just a very simple tool that just was able to authenticate to the controller, either using munch or JSON web tokens, which Slurm also supports, and just fetch the config files, and then it's done. That's all you really want to do, right? Because then your tools, the Slurm tools can work, because they have the Slurm config files, and just by having the JSON web token in your environment, you can just talk to the Slurm controller. And yeah, that's the tool that I wrote. It's a very simple tool. It just does exactly what I described there. And it's open source. You can find it on GitHub. I uploaded it in the past month. Fun story about this. As I said, I had the idea for this when I was back at CERN. I worked on this a year ago already. But then I somehow lost the source. I don't know what happened. Just before I left CERN, the source was just lost. I don't know why. I must have deleted it by accident. I don't know what happened. So after I left CERN, I kept in contact with my ex-colleagues, and they were telling me that they wanted to do this integration between the swan, which is the who here knows swan? Anybody? Okay, one, two, three. Yeah, so it's the Jupyter Notebook Service for CERN, which also does analytics. And we wanted to connect it to Slurm, and we run into all these issues, because this is a service that's exposed to the whole internet. So we didn't want to have the munchkey for the Slurm cluster in the container, et cetera. Anyway, so then I left CERN, and then, yeah, my colleagues were telling me, oh, it would have been so useful to have this at Watapiti. And then a few months ago, I just didn't like the fact that I had lost the source and all these days. I spent a couple of days reverse-engineering the Slurm protocol, and I just didn't like losing it, so I just rewrote it more properly in Python and just made it public. So if you're interested in making client containers like this, feel free to give it a try. It looks a bit like this. It's very simple. You can choose between munch or JWT, JSON WebToken's authentication. If you choose JWT, which is the most simple one, you just need an environment variable with a token, and you can tell it where you want to store the config files, and then you have verbosity as an option. So it's very simple. It has very little dependencies. So the tool talks several Slurm protocol versions, because with every major release, Slurm changes the protocol versions. So you can list them with minus L, and it will show you basically all the versions that it supports. So imagine you have a Slurm WebToken in this variable. You can just tell it to do JSON WebToken authentication with the server. It supports multiple controllers in case you have high availability set up in your Slurm cluster, so you can specify a list of servers that it will retry until it succeeds, and then you tell it the protocol version of the Slurm CTLD, because it needs to know what protocol it should talk. The protocol version negotiation, I think it doesn't exist in the Slurm protocol, so you have to tell it which version you want it to talk, and that's it, and then it will just download the Slurm config files and happy days for your containers. Conclusions, I think I'm ahead of time. So this tool called straw, it can simplify the cost of creating and maintaining your Slurm client containers. It can also increase the security, because you don't need to put the Munch key everywhere, where you're running your client containers. JSON WebToken's surface. Caveats, caveats. I think this tool should not exist, because ideally this would be supported upstream. So, you know, if anybody has any influence on SCADMD Slurm development, yeah, I think it would be nice if we had this built-in into Slurm. And then the second caveat is that the JSON WebToken, the token needs to be associated with a Slurm user, basically. So ideally, you would be able to just generate a JSON WebToken for a user that's going to run on the Slurm cluster, and then if the secret for some reason is exposed, you've only exposed the JSON WebToken of a single user. However, this is a limitation built into the Slurm, into Slurm, basically. You cannot pull over the protocol the Slurm config file unless the token belongs to the Slurm user, or to root. Still, I think it's an improvement over having your Munch key available everywhere. If you're free to try it out, that was it. I'm happy to answer any questions you might have. APPLAUSE Thank you very much, Pablo. Time for questions. So what kind of clients do need the config file? Could you do everything over REST nowadays? Is it still necessary to use the config file? Yes, so anything that wants to run srun, sbatch, sq, sinfo. For instance, if you have the Jupyter Notebook plugins, they will just run those commands. Or if you want to run a client that uses PySlurm, for instance, or any library really, anything that uses lipslurm underneath will automatically read the config files, right? So, of course, you can write your own client, handwritten from scratch, that just interacts with the Slurm REST to do stuff. Yes, but you cannot leverage all the existing user client tools, and the lipslurm, PySlurm, etc. So if you want to create a Python tool, for instance, that leverages PySlurm, this would be, I think, a good solution. I think Slurm does have, like, a REST API, but it's considered very insecure. So even the documentation tells you, like, don't use this. I just didn't understand, like, for a long time now, why everyone needs the config file, right? I mean, why doesn't it need to be in sync? Like, couldn't they just exchange the information over the protocols now and just say, like, this is your Slurm server? Yeah, that's a configless feature. That's a configless feature, essentially. Yeah, but the configless feature just downloads the config. Yes. Next, like, config less OK. Yes. I download the config. I don't need the config beforehand. It's like serverless. There's always a server somewhere. Yes. Yeah, exactly. So that's just how Slurm works. Yeah. So I'm still a little confused about the Slurm client container. So the container is an application on the actual Slurm client, because you have to document in the SlurmConf, you have to sort of say what your clients are so that the scheduler can intelligently decide how to schedule jobs, right? I'm missing something. No, you don't really need to declare all the clients for Slurm. You just need to declare the worker nodes that are part of it. But you can have any... I mean, it depends on how you've configured it. You can limit it. You can limit in Slurm which clients are allowed to connect, but you don't have to. So you could just... But even if you do, you will need this, because you will... Even if you authorize a host name to connect as a client, it will need to have the munch key and the SlurmConf files, et cetera. Does this answer your question? Well, no, so when you... In the Slurm.conf, you sort of detail what your positions are, and you have to kind of tell it what the capabilities are of your clients, of your Slurm clients, right? So that Slurm can decide how to schedule jobs. I'm missing something. Well, I think you're thinking about the compute nodes. Yeah, I am. Yeah, the node names part of the SlurmConf. So the containers run on the compute nodes? No, the containers would be... Let me go back to one of the slides where... So you're thinking maybe about the compute nodes, each of which runs a Slurm DDemon, and those you have to declare. Yes, I think in 2023, by the way, you will be able to dynamically spawn compute nodes, but that's the future. What I'm talking about is all the users and client tools that connect to the controller to run SQ as info, like when you use Slurm and you... Hello. So if you had some tooling that you automated to gather metrics from Slurm or, yeah, a Jupyter notebook service, for instance, that connects to your cluster that wants to launch jobs, that wants to run as batch SQ, whatever, that's in that domain. Yeah, I mean, the newest werewolf runs containers on my back for the stream. I mean, I think the newest version of werewolf is set up to run containers on the Slurm clients, right? It's sort of, you're actually launching containers as applications, so that was kind of... That's on the compute nodes. On the compute nodes, yeah. Yeah, yeah, that's the compute nodes. Thank you for your talk. So I have a question. You are telling that you can pull the configuration with your tool, but there are many... Fine, you can't pull with configless. For example, all the spank plugins, or I think topology, you can pull it, but various, like I said, spank plugins and so on. So how do you manage this kind of config file that are not ended by default by Slurm? Right, that's correct. So when you use the configless feature, it will download the, you know, the Slurm Conf, the C Group Conf, a lot of config files, but it will not download your plugins, your plugin files. But I think those are usually not needed if you're running a client, because those are usually just needed for the Slurm D demons, right? Even for the worker nodes. Like the epilogue, the prologue, you mean all of those plugin scripts, right? The authentication plugins. Those are usually needed by the Slurm D demon, but if you're just writing a client, but say you're automating something with PySlurm to interact with it, you don't need those files. And Slurm will happily... You can happily run... all of those commands without those files. Yeah, okay, so if I just summarize, the idea is just to create some frontend nodes, but not really work nodes. That's right? So you... So if you want to use configless to set up a frontend node, you might need those files from somewhere else. But if you're just creating a container to just interact with Slurm and send Slurm commands, you don't need them, basically. Because the plugin files are usually the... Yeah, the epilogue prologue for the Slurm D or the Slurm CTLD. And that's not what these Slurm client containers are about. So short answer, you usually don't need them. Hello, thank you for the talk. I'm wondering, in huge institutions, like in CERN or EPFL, would you run your own forked or patched Slurm so you could fix maybe the authentication privileges? Or is it just not done because it's... I've never carried any Slurm patches, to be honest. I've always, both at Slurm and at EPFL, we just use Slurm out of the box. It works well enough for our use cases. It is true that you could, for instance, do a patch to enable finer granularity for the permissions. For instance, you could enable any user to pull the config file. That would be a nice patch. We don't do it. Okay, thank you. We have time for one short question. Hi, thanks. We actually are very interested in this because we are applying... We have a Jupyter Hub frontend that actually talks to a Slurm cluster through SSH because we don't want to install all that stuff, like the munch and the full Slurm deployment into the Jupyter Hub host. And I'm wondering, how does it talk actually to Slurm control? So is the Slurm control always listening to any... any of the hosts that will talk to it? Yes. Or is there any restrictions to who is connecting to the Slurm control demo? So there's an alloc nodes setting in the SlurmConf, I believe, which will allow you to restrict from which nodes you can allocate resources. Okay. So you can limit it. However, if you don't have that, the Slurm will happily accept anything because if you have the shared secret, it's considered good enough. Okay. Or a valid JSON web token. Okay. Yeah. Thank you. Thank you very much, Pablo. Thanks. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 29.92, "text": " Okay, next talk is Pablo.", "tokens": [1033, 11, 958, 751, 307, 31554, 13], "temperature": 0.0, "avg_logprob": -0.6123781637711958, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.6253361701965332}, {"id": 1, "seek": 2992, "start": 29.92, "end": 36.92, "text": " Who is going to explain us how to set up slurm client environments more easily.", "tokens": [50364, 2102, 307, 516, 281, 2903, 505, 577, 281, 992, 493, 1061, 26717, 6423, 12388, 544, 3612, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5100153923034668, "compression_ratio": 1.0675675675675675, "no_speech_prob": 0.012230413034558296}, {"id": 2, "seek": 5992, "start": 59.92, "end": 66.92, "text": " My name is Pablo and I have been running the HBC Clusters since I was 9 years.", "tokens": [1222, 1315, 307, 31554, 293, 286, 362, 668, 2614, 264, 389, 7869, 2033, 17181, 1670, 286, 390, 1722, 924, 13], "temperature": 0.0, "avg_logprob": -0.5619735195212168, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.9237033128738403}, {"id": 3, "seek": 5992, "start": 66.92, "end": 74.92, "text": " I was running the HBC Clusters at CERN and got involved mostly in slurm, running slurm.", "tokens": [286, 390, 2614, 264, 389, 7869, 2033, 17181, 412, 383, 1598, 45, 293, 658, 3288, 5240, 294, 1061, 26717, 11, 2614, 1061, 26717, 13], "temperature": 0.0, "avg_logprob": -0.5619735195212168, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.9237033128738403}, {"id": 4, "seek": 5992, "start": 74.92, "end": 83.92, "text": " That's when I came up with the idea for this tool since about 8 or 9 months ago I started", "tokens": [663, 311, 562, 286, 1361, 493, 365, 264, 1558, 337, 341, 2290, 1670, 466, 1649, 420, 1722, 2493, 2057, 286, 1409], "temperature": 0.0, "avg_logprob": -0.5619735195212168, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.9237033128738403}, {"id": 5, "seek": 8392, "start": 83.92, "end": 89.92, "text": " the HBC Clusters and I'm also participating in the SKA project, hence the pretty background,", "tokens": [264, 389, 7869, 2033, 17181, 293, 286, 478, 611, 13950, 294, 264, 318, 16135, 1716, 11, 16678, 264, 1238, 3678, 11], "temperature": 0.0, "avg_logprob": -0.22140775752972952, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0009138474124483764}, {"id": 6, "seek": 8392, "start": 89.92, "end": 97.92, "text": " where we do also things related to the HBC infrastructure.", "tokens": [689, 321, 360, 611, 721, 4077, 281, 264, 389, 7869, 6896, 13], "temperature": 0.0, "avg_logprob": -0.22140775752972952, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0009138474124483764}, {"id": 7, "seek": 8392, "start": 97.92, "end": 104.92, "text": " So just a brief introduction to slurm in case anybody is not familiar with it.", "tokens": [407, 445, 257, 5353, 9339, 281, 1061, 26717, 294, 1389, 4472, 307, 406, 4963, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.22140775752972952, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0009138474124483764}, {"id": 8, "seek": 8392, "start": 104.92, "end": 112.92, "text": " Slurm is basically both a resource manager and a job scheduler, meaning slurm will manage", "tokens": [6187, 26717, 307, 1936, 1293, 257, 7684, 6598, 293, 257, 1691, 12000, 260, 11, 3620, 1061, 26717, 486, 3067], "temperature": 0.0, "avg_logprob": -0.22140775752972952, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0009138474124483764}, {"id": 9, "seek": 11292, "start": 112.92, "end": 120.92, "text": " their allocations, it will track which machines are used to which jobs and which users own,", "tokens": [641, 12660, 763, 11, 309, 486, 2837, 597, 8379, 366, 1143, 281, 597, 4782, 293, 597, 5022, 1065, 11], "temperature": 0.0, "avg_logprob": -0.311662495136261, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0006495967390947044}, {"id": 10, "seek": 11292, "start": 120.92, "end": 124.92, "text": " which CPUs and which nodes, etc.", "tokens": [597, 13199, 82, 293, 597, 13891, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.311662495136261, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0006495967390947044}, {"id": 11, "seek": 11292, "start": 124.92, "end": 130.92000000000002, "text": " And it's also the job scheduler, meaning it will, when users submit jobs, you have your", "tokens": [400, 309, 311, 611, 264, 1691, 12000, 260, 11, 3620, 309, 486, 11, 562, 5022, 10315, 4782, 11, 291, 362, 428], "temperature": 0.0, "avg_logprob": -0.311662495136261, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0006495967390947044}, {"id": 12, "seek": 11292, "start": 130.92000000000002, "end": 134.92000000000002, "text": " happy users over there, or hopefully it will be happy users.", "tokens": [2055, 5022, 670, 456, 11, 420, 4696, 309, 486, 312, 2055, 5022, 13], "temperature": 0.0, "avg_logprob": -0.311662495136261, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0006495967390947044}, {"id": 13, "seek": 11292, "start": 134.92000000000002, "end": 139.92000000000002, "text": " And they can be one-on-one on your cluster, so they make a job submission, usually writing", "tokens": [400, 436, 393, 312, 472, 12, 266, 12, 546, 322, 428, 13630, 11, 370, 436, 652, 257, 1691, 23689, 11, 2673, 3579], "temperature": 0.0, "avg_logprob": -0.311662495136261, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0006495967390947044}, {"id": 14, "seek": 13992, "start": 139.92, "end": 145.92, "text": " a script that launches some workloads.", "tokens": [257, 5755, 300, 31841, 512, 32452, 13], "temperature": 0.0, "avg_logprob": -0.1719695210456848, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0008055442594923079}, {"id": 15, "seek": 13992, "start": 145.92, "end": 150.92, "text": " And they will basically interact with slurm and slurm will manage all of these job submissions.", "tokens": [400, 436, 486, 1936, 4648, 365, 1061, 26717, 293, 1061, 26717, 486, 3067, 439, 295, 613, 1691, 40429, 13], "temperature": 0.0, "avg_logprob": -0.1719695210456848, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0008055442594923079}, {"id": 16, "seek": 13992, "start": 150.92, "end": 154.92, "text": " You won't just have one by one, you will have hundreds or even thousands of jobs that are", "tokens": [509, 1582, 380, 445, 362, 472, 538, 472, 11, 291, 486, 362, 6779, 420, 754, 5383, 295, 4782, 300, 366], "temperature": 0.0, "avg_logprob": -0.1719695210456848, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0008055442594923079}, {"id": 17, "seek": 13992, "start": 154.92, "end": 160.92, "text": " scheduled to run on your infrastructure and slurm will manage the views and the priorities", "tokens": [15678, 281, 1190, 322, 428, 6896, 293, 1061, 26717, 486, 3067, 264, 6809, 293, 264, 15503], "temperature": 0.0, "avg_logprob": -0.1719695210456848, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0008055442594923079}, {"id": 18, "seek": 13992, "start": 160.92, "end": 163.92, "text": " and the accounting, etc.", "tokens": [293, 264, 19163, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.1719695210456848, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0008055442594923079}, {"id": 19, "seek": 16392, "start": 163.92, "end": 170.92, "text": " So basically it's a batch manager, but there's both resource managing and the scheduling", "tokens": [407, 1936, 309, 311, 257, 15245, 6598, 11, 457, 456, 311, 1293, 7684, 11642, 293, 264, 29055], "temperature": 0.0, "avg_logprob": -0.15559273607590618, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008514255750924349}, {"id": 20, "seek": 16392, "start": 170.92, "end": 173.92, "text": " of the jobs.", "tokens": [295, 264, 4782, 13], "temperature": 0.0, "avg_logprob": -0.15559273607590618, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008514255750924349}, {"id": 21, "seek": 16392, "start": 173.92, "end": 178.92, "text": " Building a bit deeper into how slurm works, because this is relevant for this talk, there's", "tokens": [18974, 257, 857, 7731, 666, 577, 1061, 26717, 1985, 11, 570, 341, 307, 7340, 337, 341, 751, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.15559273607590618, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008514255750924349}, {"id": 22, "seek": 16392, "start": 178.92, "end": 184.92, "text": " basically two main components, two units that are the most relevant, and those are the", "tokens": [1936, 732, 2135, 6677, 11, 732, 6815, 300, 366, 264, 881, 7340, 11, 293, 729, 366, 264], "temperature": 0.0, "avg_logprob": -0.15559273607590618, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008514255750924349}, {"id": 23, "seek": 18492, "start": 184.92, "end": 194.92, "text": " controller, which is called the slurm CTLD, and then the deals that run on the worker nodes", "tokens": [10561, 11, 597, 307, 1219, 264, 1061, 26717, 19529, 23704, 11, 293, 550, 264, 11215, 300, 1190, 322, 264, 11346, 13891], "temperature": 0.0, "avg_logprob": -0.29943527572456446, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0009285164996981621}, {"id": 24, "seek": 18492, "start": 194.92, "end": 196.92, "text": " at the bottom, which is the slurm VDU.", "tokens": [412, 264, 2767, 11, 597, 307, 264, 1061, 26717, 691, 35, 52, 13], "temperature": 0.0, "avg_logprob": -0.29943527572456446, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0009285164996981621}, {"id": 25, "seek": 18492, "start": 196.92, "end": 200.92, "text": " And then you have other demons like the slurm VD, slurm RST, slurm RST.", "tokens": [400, 550, 291, 362, 661, 19733, 411, 264, 1061, 26717, 691, 35, 11, 1061, 26717, 497, 6840, 11, 1061, 26717, 497, 6840, 13], "temperature": 0.0, "avg_logprob": -0.29943527572456446, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0009285164996981621}, {"id": 26, "seek": 18492, "start": 200.92, "end": 206.92, "text": " Those are not relevant for this talk, I will mostly focus on the part on the left here.", "tokens": [3950, 366, 406, 7340, 337, 341, 751, 11, 286, 486, 5240, 1879, 322, 264, 644, 322, 264, 1411, 510, 13], "temperature": 0.0, "avg_logprob": -0.29943527572456446, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0009285164996981621}, {"id": 27, "seek": 20692, "start": 206.92, "end": 214.92, "text": " So users and client tools, they basically interact with the controller over a slurm protocol.", "tokens": [407, 5022, 293, 6423, 3873, 11, 436, 1936, 4648, 365, 264, 10561, 670, 257, 1061, 26717, 10336, 13], "temperature": 0.0, "avg_logprob": -0.17940227637130224, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.000628386449534446}, {"id": 28, "seek": 20692, "start": 214.92, "end": 219.92, "text": " There's nowadays a slurm RST, so you can also interact with the rest with some scripts,", "tokens": [821, 311, 13434, 257, 1061, 26717, 497, 6840, 11, 370, 291, 393, 611, 4648, 365, 264, 1472, 365, 512, 23294, 11], "temperature": 0.0, "avg_logprob": -0.17940227637130224, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.000628386449534446}, {"id": 29, "seek": 20692, "start": 219.92, "end": 226.92, "text": " but mostly all the user lab tools, and mostly almost everything in the slurm ecosystem just", "tokens": [457, 5240, 439, 264, 4195, 2715, 3873, 11, 293, 5240, 1920, 1203, 294, 264, 1061, 26717, 11311, 445], "temperature": 0.0, "avg_logprob": -0.17940227637130224, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.000628386449534446}, {"id": 30, "seek": 20692, "start": 226.92, "end": 232.92, "text": " talks to the slurm CTLD, and this controller handles the source of truth for slurm, so it", "tokens": [6686, 281, 264, 1061, 26717, 19529, 23704, 11, 293, 341, 10561, 18722, 264, 4009, 295, 3494, 337, 1061, 26717, 11, 370, 309], "temperature": 0.0, "avg_logprob": -0.17940227637130224, "compression_ratio": 1.7122641509433962, "no_speech_prob": 0.000628386449534446}, {"id": 31, "seek": 23292, "start": 232.92, "end": 237.92, "text": " knows which resources are allocated where, it knows which jobs exist, and knows who the", "tokens": [3255, 597, 3593, 366, 29772, 689, 11, 309, 3255, 597, 4782, 2514, 11, 293, 3255, 567, 264], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 32, "seek": 23292, "start": 237.92, "end": 240.92, "text": " users are, etc.", "tokens": [5022, 366, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 33, "seek": 23292, "start": 240.92, "end": 245.92, "text": " The controller talks to the slurm units, and talking to the nodes and the slurm units are", "tokens": [440, 10561, 6686, 281, 264, 1061, 26717, 6815, 11, 293, 1417, 281, 264, 13891, 293, 264, 1061, 26717, 6815, 366], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 34, "seek": 23292, "start": 245.92, "end": 249.92, "text": " in charge of launching the jobs, so you do the cleanup, setting up the seed routes for", "tokens": [294, 4602, 295, 18354, 264, 4782, 11, 370, 291, 360, 264, 40991, 11, 3287, 493, 264, 8871, 18242, 337], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 35, "seek": 23292, "start": 249.92, "end": 252.92, "text": " the jobs, whatever you have.", "tokens": [264, 4782, 11, 2035, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 36, "seek": 23292, "start": 252.92, "end": 257.91999999999996, "text": " Now, what's important here is to know that for all of this to work, you need at least", "tokens": [823, 11, 437, 311, 1021, 510, 307, 281, 458, 300, 337, 439, 295, 341, 281, 589, 11, 291, 643, 412, 1935], "temperature": 0.0, "avg_logprob": -0.20118722637880196, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.0001565302663948387}, {"id": 37, "seek": 25792, "start": 257.92, "end": 262.92, "text": " the same thing. You need the slurm conflict files, and they need to be instinct between", "tokens": [264, 912, 551, 13, 509, 643, 264, 1061, 26717, 6596, 7098, 11, 293, 436, 643, 281, 312, 16556, 1296], "temperature": 0.0, "avg_logprob": -0.2035012179858064, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0007248155307024717}, {"id": 38, "seek": 25792, "start": 262.92, "end": 274.92, "text": " the whole cluster, so you may have some difference, but mostly it should be the same.", "tokens": [264, 1379, 13630, 11, 370, 291, 815, 362, 512, 2649, 11, 457, 5240, 309, 820, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2035012179858064, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0007248155307024717}, {"id": 39, "seek": 25792, "start": 274.92, "end": 278.92, "text": " There was no audio online? Okay.", "tokens": [821, 390, 572, 6278, 2950, 30, 1033, 13], "temperature": 0.0, "avg_logprob": -0.2035012179858064, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0007248155307024717}, {"id": 40, "seek": 25792, "start": 278.92, "end": 284.92, "text": " So as I was saying, the slurm CTLD handles the source of truth.", "tokens": [407, 382, 286, 390, 1566, 11, 264, 1061, 26717, 19529, 23704, 18722, 264, 4009, 295, 3494, 13], "temperature": 0.0, "avg_logprob": -0.2035012179858064, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0007248155307024717}, {"id": 41, "seek": 28492, "start": 284.92, "end": 290.92, "text": " The slurm units are in charge of launching the jobs, and the two important things are", "tokens": [440, 1061, 26717, 6815, 366, 294, 4602, 295, 18354, 264, 4782, 11, 293, 264, 732, 1021, 721, 366], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 42, "seek": 28492, "start": 290.92, "end": 294.92, "text": " that you need the slurm configuration files.", "tokens": [300, 291, 643, 264, 1061, 26717, 11694, 7098, 13], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 43, "seek": 28492, "start": 294.92, "end": 297.92, "text": " It's mostly the slurm.conf file, but there's other files as well.", "tokens": [467, 311, 5240, 264, 1061, 26717, 13, 24697, 3991, 11, 457, 456, 311, 661, 7098, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 44, "seek": 28492, "start": 297.92, "end": 303.92, "text": " Those need to be in sync in the whole cluster, and they need to be basically the same.", "tokens": [3950, 643, 281, 312, 294, 20271, 294, 264, 1379, 13630, 11, 293, 436, 643, 281, 312, 1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 45, "seek": 28492, "start": 303.92, "end": 305.92, "text": " They should have the same hash, ideally.", "tokens": [814, 820, 362, 264, 912, 22019, 11, 22915, 13], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 46, "seek": 28492, "start": 305.92, "end": 311.92, "text": " And then you should also have a shared secret so that nobody can, a rogue client cannot just", "tokens": [400, 550, 291, 820, 611, 362, 257, 5507, 4054, 370, 300, 5079, 393, 11, 257, 39100, 6423, 2644, 445], "temperature": 0.0, "avg_logprob": -0.08334202632725796, "compression_ratio": 1.7447698744769875, "no_speech_prob": 8.164057362591848e-05}, {"id": 47, "seek": 31192, "start": 311.92, "end": 315.92, "text": " add a worker node to the cluster and start doing malicious things.", "tokens": [909, 257, 11346, 9984, 281, 264, 13630, 293, 722, 884, 33496, 721, 13], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 48, "seek": 31192, "start": 315.92, "end": 318.92, "text": " So you have usually it's a munch secret.", "tokens": [407, 291, 362, 2673, 309, 311, 257, 275, 1680, 4054, 13], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 49, "seek": 31192, "start": 318.92, "end": 324.92, "text": " It's called the demon called munch, and you have a shared secret as well for the whole cluster.", "tokens": [467, 311, 1219, 264, 14283, 1219, 275, 1680, 11, 293, 291, 362, 257, 5507, 4054, 382, 731, 337, 264, 1379, 13630, 13], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 50, "seek": 31192, "start": 324.92, "end": 329.92, "text": " And this fact is important, is very relevant for this talk.", "tokens": [400, 341, 1186, 307, 1021, 11, 307, 588, 7340, 337, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 51, "seek": 31192, "start": 329.92, "end": 332.92, "text": " Now, up to containers.", "tokens": [823, 11, 493, 281, 17089, 13], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 52, "seek": 31192, "start": 332.92, "end": 337.92, "text": " So containers are increasingly becoming a super popular tool to run infrastructure for", "tokens": [407, 17089, 366, 12980, 5617, 257, 1687, 3743, 2290, 281, 1190, 6896, 337], "temperature": 0.0, "avg_logprob": -0.12886455784673276, "compression_ratio": 1.6217391304347826, "no_speech_prob": 9.670636063674465e-05}, {"id": 53, "seek": 33792, "start": 337.92, "end": 341.92, "text": " reproducibility, for automating deployments.", "tokens": [11408, 537, 39802, 11, 337, 3553, 990, 7274, 1117, 13], "temperature": 0.0, "avg_logprob": -0.11880489631935402, "compression_ratio": 1.597938144329897, "no_speech_prob": 8.329973934451118e-05}, {"id": 54, "seek": 33792, "start": 341.92, "end": 348.92, "text": " And just in general, they're becoming super ubiquitous in our industry.", "tokens": [400, 445, 294, 2674, 11, 436, 434, 5617, 1687, 43868, 39831, 294, 527, 3518, 13], "temperature": 0.0, "avg_logprob": -0.11880489631935402, "compression_ratio": 1.597938144329897, "no_speech_prob": 8.329973934451118e-05}, {"id": 55, "seek": 33792, "start": 348.92, "end": 352.92, "text": " I think for good reasons.", "tokens": [286, 519, 337, 665, 4112, 13], "temperature": 0.0, "avg_logprob": -0.11880489631935402, "compression_ratio": 1.597938144329897, "no_speech_prob": 8.329973934451118e-05}, {"id": 56, "seek": 33792, "start": 352.92, "end": 359.92, "text": " And there are, I think, very good use cases for using containers with slurm.", "tokens": [400, 456, 366, 11, 286, 519, 11, 588, 665, 764, 3331, 337, 1228, 17089, 365, 1061, 26717, 13], "temperature": 0.0, "avg_logprob": -0.11880489631935402, "compression_ratio": 1.597938144329897, "no_speech_prob": 8.329973934451118e-05}, {"id": 57, "seek": 33792, "start": 359.92, "end": 366.92, "text": " In this talk, I will focus on the use case where you use containers on the user and client", "tokens": [682, 341, 751, 11, 286, 486, 1879, 322, 264, 764, 1389, 689, 291, 764, 17089, 322, 264, 4195, 293, 6423], "temperature": 0.0, "avg_logprob": -0.11880489631935402, "compression_ratio": 1.597938144329897, "no_speech_prob": 8.329973934451118e-05}, {"id": 58, "seek": 36692, "start": 366.92, "end": 367.92, "text": " side of things.", "tokens": [1252, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 59, "seek": 36692, "start": 367.92, "end": 373.92, "text": " So those tools that will talk to slurm, to the controller mostly, to do things on the cluster.", "tokens": [407, 729, 3873, 300, 486, 751, 281, 1061, 26717, 11, 281, 264, 10561, 5240, 11, 281, 360, 721, 322, 264, 13630, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 60, "seek": 36692, "start": 373.92, "end": 377.92, "text": " So this could be some automation that you have run to do whatever.", "tokens": [407, 341, 727, 312, 512, 17769, 300, 291, 362, 1190, 281, 360, 2035, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 61, "seek": 36692, "start": 377.92, "end": 381.92, "text": " For instance, you could use it for monitoring purposes.", "tokens": [1171, 5197, 11, 291, 727, 764, 309, 337, 11028, 9932, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 62, "seek": 36692, "start": 381.92, "end": 387.92, "text": " You could write a tool that does health checks on the cluster for accounting.", "tokens": [509, 727, 2464, 257, 2290, 300, 775, 1585, 13834, 322, 264, 13630, 337, 19163, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 63, "seek": 36692, "start": 387.92, "end": 390.92, "text": " I've used it extensively for accounting as well.", "tokens": [286, 600, 1143, 309, 32636, 337, 19163, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 64, "seek": 36692, "start": 390.92, "end": 393.92, "text": " But also integration with other services, right?", "tokens": [583, 611, 10980, 365, 661, 3328, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09618325752787071, "compression_ratio": 1.6970954356846473, "no_speech_prob": 5.053535278420895e-05}, {"id": 65, "seek": 39392, "start": 393.92, "end": 399.92, "text": " Or if you want to connect the Jupyter notebook with slurm, you will end up with some tools", "tokens": [1610, 498, 291, 528, 281, 1745, 264, 22125, 88, 391, 21060, 365, 1061, 26717, 11, 291, 486, 917, 493, 365, 512, 3873], "temperature": 0.0, "avg_logprob": -0.11360289833762428, "compression_ratio": 1.51875, "no_speech_prob": 4.4555781641975045e-05}, {"id": 66, "seek": 39392, "start": 399.92, "end": 405.92, "text": " that talk to the controller.", "tokens": [300, 751, 281, 264, 10561, 13], "temperature": 0.0, "avg_logprob": -0.11360289833762428, "compression_ratio": 1.51875, "no_speech_prob": 4.4555781641975045e-05}, {"id": 67, "seek": 39392, "start": 405.92, "end": 415.92, "text": " Now, there are basically two scenarios in which you can use containers with slurm.", "tokens": [823, 11, 456, 366, 1936, 732, 15077, 294, 597, 291, 393, 764, 17089, 365, 1061, 26717, 13], "temperature": 0.0, "avg_logprob": -0.11360289833762428, "compression_ratio": 1.51875, "no_speech_prob": 4.4555781641975045e-05}, {"id": 68, "seek": 39392, "start": 415.92, "end": 417.92, "text": " On the left, we have the local use case.", "tokens": [1282, 264, 1411, 11, 321, 362, 264, 2654, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.11360289833762428, "compression_ratio": 1.51875, "no_speech_prob": 4.4555781641975045e-05}, {"id": 69, "seek": 41792, "start": 417.92, "end": 423.92, "text": " That means imagine you have a frontend mode, you have a machine that's configured where it uses SSH2.", "tokens": [663, 1355, 3811, 291, 362, 257, 1868, 521, 4391, 11, 291, 362, 257, 3479, 300, 311, 30538, 689, 309, 4960, 12238, 39, 17, 13], "temperature": 0.0, "avg_logprob": -0.1336801528930664, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.457923104288056e-05}, {"id": 70, "seek": 41792, "start": 423.92, "end": 430.92, "text": " And from there, they can run the slurm commands to launch jobs, to track their job usage, et cetera.", "tokens": [400, 490, 456, 11, 436, 393, 1190, 264, 1061, 26717, 16901, 281, 4025, 4782, 11, 281, 2837, 641, 1691, 14924, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1336801528930664, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.457923104288056e-05}, {"id": 71, "seek": 41792, "start": 430.92, "end": 433.92, "text": " It's conventionally called frontend mode for the cluster.", "tokens": [467, 311, 10286, 379, 1219, 1868, 521, 4391, 337, 264, 13630, 13], "temperature": 0.0, "avg_logprob": -0.1336801528930664, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.457923104288056e-05}, {"id": 72, "seek": 41792, "start": 433.92, "end": 439.92, "text": " So if you just add the slurm client container on that node, it's very simple.", "tokens": [407, 498, 291, 445, 909, 264, 1061, 26717, 6423, 10129, 322, 300, 9984, 11, 309, 311, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1336801528930664, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.457923104288056e-05}, {"id": 73, "seek": 41792, "start": 439.92, "end": 446.92, "text": " Because you can just, as I said, you need a secret with munch, and you need the config files.", "tokens": [1436, 291, 393, 445, 11, 382, 286, 848, 11, 291, 643, 257, 4054, 365, 275, 1680, 11, 293, 291, 643, 264, 6662, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1336801528930664, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.457923104288056e-05}, {"id": 74, "seek": 44692, "start": 446.92, "end": 449.92, "text": " And that scenario is very simple because you can just do bind mounts,", "tokens": [400, 300, 9005, 307, 588, 2199, 570, 291, 393, 445, 360, 14786, 40982, 11], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 75, "seek": 44692, "start": 449.92, "end": 452.92, "text": " and you can access the munch socket to talk to slurm.", "tokens": [293, 291, 393, 2105, 264, 275, 1680, 19741, 281, 751, 281, 1061, 26717, 13], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 76, "seek": 44692, "start": 452.92, "end": 458.92, "text": " And you might bind mount the slurm config directory, and you're done, basically.", "tokens": [400, 291, 1062, 14786, 3746, 264, 1061, 26717, 6662, 21120, 11, 293, 291, 434, 1096, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 77, "seek": 44692, "start": 458.92, "end": 460.92, "text": " So that's sort of easy.", "tokens": [407, 300, 311, 1333, 295, 1858, 13], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 78, "seek": 44692, "start": 460.92, "end": 467.92, "text": " However, what if you have, for the use case on the right, you have the distributed or remote use case.", "tokens": [2908, 11, 437, 498, 291, 362, 11, 337, 264, 764, 1389, 322, 264, 558, 11, 291, 362, 264, 12631, 420, 8607, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 79, "seek": 44692, "start": 467.92, "end": 475.92, "text": " And in that case, you may run your slurm client container in a different service.", "tokens": [400, 294, 300, 1389, 11, 291, 815, 1190, 428, 1061, 26717, 6423, 10129, 294, 257, 819, 2643, 13], "temperature": 0.0, "avg_logprob": -0.10074465865388922, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.3791194659424946e-05}, {"id": 80, "seek": 47592, "start": 475.92, "end": 480.92, "text": " That's a different network, or you may run it on Kubernetes or somewhere else.", "tokens": [663, 311, 257, 819, 3209, 11, 420, 291, 815, 1190, 309, 322, 23145, 420, 4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.0858722898695204, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.2798404895875137e-05}, {"id": 81, "seek": 47592, "start": 480.92, "end": 487.92, "text": " In that case, you obviously can't just do the bind mounts because you need to give it all those things.", "tokens": [682, 300, 1389, 11, 291, 2745, 393, 380, 445, 360, 264, 14786, 40982, 570, 291, 643, 281, 976, 309, 439, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.0858722898695204, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.2798404895875137e-05}, {"id": 82, "seek": 47592, "start": 487.92, "end": 493.92, "text": " So you would have to give it all the slurm config files and somehow the munch shared key", "tokens": [407, 291, 576, 362, 281, 976, 309, 439, 264, 1061, 26717, 6662, 7098, 293, 6063, 264, 275, 1680, 5507, 2141], "temperature": 0.0, "avg_logprob": -0.0858722898695204, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.2798404895875137e-05}, {"id": 83, "seek": 47592, "start": 493.92, "end": 501.92, "text": " so that your external service can talk to your cluster, right, specifically to the slurm controller.", "tokens": [370, 300, 428, 8320, 2643, 393, 751, 281, 428, 13630, 11, 558, 11, 4682, 281, 264, 1061, 26717, 10561, 13], "temperature": 0.0, "avg_logprob": -0.0858722898695204, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.2798404895875137e-05}, {"id": 84, "seek": 50192, "start": 501.92, "end": 505.92, "text": " Now, this is an extraction from a Docker file.", "tokens": [823, 11, 341, 307, 364, 30197, 490, 257, 33772, 3991, 13], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 85, "seek": 50192, "start": 505.92, "end": 506.92, "text": " This is the naive approach.", "tokens": [639, 307, 264, 29052, 3109, 13], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 86, "seek": 50192, "start": 506.92, "end": 509.92, "text": " This is how I started trying things.", "tokens": [639, 307, 577, 286, 1409, 1382, 721, 13], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 87, "seek": 50192, "start": 509.92, "end": 510.92, "text": " Easy, right?", "tokens": [16002, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 88, "seek": 50192, "start": 510.92, "end": 516.9200000000001, "text": " You just take the slurm config, and you just copy it to the destination, right?", "tokens": [509, 445, 747, 264, 1061, 26717, 6662, 11, 293, 291, 445, 5055, 309, 281, 264, 12236, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 89, "seek": 50192, "start": 516.9200000000001, "end": 518.9200000000001, "text": " And this will absolutely work.", "tokens": [400, 341, 486, 3122, 589, 13], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 90, "seek": 50192, "start": 518.9200000000001, "end": 525.9200000000001, "text": " But I was not happy with this approach because then you end up managing two copies of your slurm config.", "tokens": [583, 286, 390, 406, 2055, 365, 341, 3109, 570, 550, 291, 917, 493, 11642, 732, 14341, 295, 428, 1061, 26717, 6662, 13], "temperature": 0.0, "avg_logprob": -0.10344617263130519, "compression_ratio": 1.5813953488372092, "no_speech_prob": 8.082335261860862e-05}, {"id": 91, "seek": 52592, "start": 525.92, "end": 532.92, "text": " And I really like having a single source of truth for when you do configuration management and automation of your infrastructure,", "tokens": [400, 286, 534, 411, 1419, 257, 2167, 4009, 295, 3494, 337, 562, 291, 360, 11694, 4592, 293, 17769, 295, 428, 6896, 11], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 92, "seek": 52592, "start": 532.92, "end": 534.92, "text": " I really like having a single source of truth.", "tokens": [286, 534, 411, 1419, 257, 2167, 4009, 295, 3494, 13], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 93, "seek": 52592, "start": 534.92, "end": 542.92, "text": " And managing this in this way with containers is very fiddly because it's very easy that you will forget to update it", "tokens": [400, 11642, 341, 294, 341, 636, 365, 17089, 307, 588, 283, 14273, 356, 570, 309, 311, 588, 1858, 300, 291, 486, 2870, 281, 5623, 309], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 94, "seek": 52592, "start": 542.92, "end": 544.92, "text": " or something that will fail to update it automatically.", "tokens": [420, 746, 300, 486, 3061, 281, 5623, 309, 6772, 13], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 95, "seek": 52592, "start": 544.92, "end": 546.92, "text": " It's just not ideal.", "tokens": [467, 311, 445, 406, 7157, 13], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 96, "seek": 52592, "start": 546.92, "end": 548.92, "text": " I didn't like this approach, but it will work.", "tokens": [286, 994, 380, 411, 341, 3109, 11, 457, 309, 486, 589, 13], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 97, "seek": 52592, "start": 548.92, "end": 551.92, "text": " It will work.", "tokens": [467, 486, 589, 13], "temperature": 0.0, "avg_logprob": -0.08866509028843471, "compression_ratio": 1.8382978723404255, "no_speech_prob": 4.902047658106312e-05}, {"id": 98, "seek": 55192, "start": 551.92, "end": 557.92, "text": " And some of you who know slurm may say, oh, but Pablo, why wouldn't you just use slurm's config less feature?", "tokens": [400, 512, 295, 291, 567, 458, 1061, 26717, 815, 584, 11, 1954, 11, 457, 31554, 11, 983, 2759, 380, 291, 445, 764, 1061, 26717, 311, 6662, 1570, 4111, 30], "temperature": 0.0, "avg_logprob": -0.11673409740130107, "compression_ratio": 1.6839622641509433, "no_speech_prob": 8.460151730105281e-05}, {"id": 99, "seek": 55192, "start": 557.92, "end": 568.92, "text": " So slurm config less is a new feature since slurm 20 or so that will basically allow a client to just pull the config files from slurm.", "tokens": [407, 1061, 26717, 6662, 1570, 307, 257, 777, 4111, 1670, 1061, 26717, 945, 420, 370, 300, 486, 1936, 2089, 257, 6423, 281, 445, 2235, 264, 6662, 7098, 490, 1061, 26717, 13], "temperature": 0.0, "avg_logprob": -0.11673409740130107, "compression_ratio": 1.6839622641509433, "no_speech_prob": 8.460151730105281e-05}, {"id": 100, "seek": 55192, "start": 568.92, "end": 576.92, "text": " So the slurm ddemons that run on the worker nodes, when they start, they will just grab the slurm config files.", "tokens": [407, 264, 1061, 26717, 274, 10730, 892, 300, 1190, 322, 264, 11346, 13891, 11, 562, 436, 722, 11, 436, 486, 445, 4444, 264, 1061, 26717, 6662, 7098, 13], "temperature": 0.0, "avg_logprob": -0.11673409740130107, "compression_ratio": 1.6839622641509433, "no_speech_prob": 8.460151730105281e-05}, {"id": 101, "seek": 57692, "start": 576.92, "end": 582.92, "text": " So you can just remove the needs to even copy the slurm config, right?", "tokens": [407, 291, 393, 445, 4159, 264, 2203, 281, 754, 5055, 264, 1061, 26717, 6662, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1358490244547526, "compression_ratio": 1.4628571428571429, "no_speech_prob": 3.627319892984815e-05}, {"id": 102, "seek": 57692, "start": 582.92, "end": 586.92, "text": " Well, that's a trick question.", "tokens": [1042, 11, 300, 311, 257, 4282, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1358490244547526, "compression_ratio": 1.4628571428571429, "no_speech_prob": 3.627319892984815e-05}, {"id": 103, "seek": 57692, "start": 586.92, "end": 593.92, "text": " Not necessarily because then you need to run a slurm ddemon in your container.", "tokens": [1726, 4725, 570, 550, 291, 643, 281, 1190, 257, 1061, 26717, 274, 10730, 266, 294, 428, 10129, 13], "temperature": 0.0, "avg_logprob": -0.1358490244547526, "compression_ratio": 1.4628571428571429, "no_speech_prob": 3.627319892984815e-05}, {"id": 104, "seek": 57692, "start": 593.92, "end": 596.92, "text": " And you also need the munch demon.", "tokens": [400, 291, 611, 643, 264, 275, 1680, 14283, 13], "temperature": 0.0, "avg_logprob": -0.1358490244547526, "compression_ratio": 1.4628571428571429, "no_speech_prob": 3.627319892984815e-05}, {"id": 105, "seek": 57692, "start": 596.92, "end": 600.92, "text": " And it sounds easy, but it's really not.", "tokens": [400, 309, 3263, 1858, 11, 457, 309, 311, 534, 406, 13], "temperature": 0.0, "avg_logprob": -0.1358490244547526, "compression_ratio": 1.4628571428571429, "no_speech_prob": 3.627319892984815e-05}, {"id": 106, "seek": 60092, "start": 600.92, "end": 608.92, "text": " You will need to do a lot of hacks. This is an instruction from a container that I was creating.", "tokens": [509, 486, 643, 281, 360, 257, 688, 295, 33617, 13, 639, 307, 364, 10951, 490, 257, 10129, 300, 286, 390, 4084, 13], "temperature": 0.0, "avg_logprob": -0.18985246022542318, "compression_ratio": 1.5535714285714286, "no_speech_prob": 7.022990757832304e-05}, {"id": 107, "seek": 60092, "start": 608.92, "end": 611.92, "text": " And you run in lots of awful things.", "tokens": [400, 291, 1190, 294, 3195, 295, 11232, 721, 13], "temperature": 0.0, "avg_logprob": -0.18985246022542318, "compression_ratio": 1.5535714285714286, "no_speech_prob": 7.022990757832304e-05}, {"id": 108, "seek": 60092, "start": 611.92, "end": 621.92, "text": " Like the slurm ddemon expects this release agent file to exist in the C group and the containers, they just don't create it.", "tokens": [1743, 264, 1061, 26717, 274, 10730, 266, 33280, 341, 4374, 9461, 3991, 281, 2514, 294, 264, 383, 1594, 293, 264, 17089, 11, 436, 445, 500, 380, 1884, 309, 13], "temperature": 0.0, "avg_logprob": -0.18985246022542318, "compression_ratio": 1.5535714285714286, "no_speech_prob": 7.022990757832304e-05}, {"id": 109, "seek": 60092, "start": 621.92, "end": 626.92, "text": " I tried it on Docker. I tried it on different Kubernetes versions. It just doesn't exist.", "tokens": [286, 3031, 309, 322, 33772, 13, 286, 3031, 309, 322, 819, 23145, 9606, 13, 467, 445, 1177, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.18985246022542318, "compression_ratio": 1.5535714285714286, "no_speech_prob": 7.022990757832304e-05}, {"id": 110, "seek": 62692, "start": 626.92, "end": 631.92, "text": " I don't know why. I couldn't find out why. If anybody knows, please tell me.", "tokens": [286, 500, 380, 458, 983, 13, 286, 2809, 380, 915, 484, 983, 13, 759, 4472, 3255, 11, 1767, 980, 385, 13], "temperature": 0.0, "avg_logprob": -0.09934381816698157, "compression_ratio": 1.506726457399103, "no_speech_prob": 5.8239900681655854e-05}, {"id": 111, "seek": 62692, "start": 631.92, "end": 636.92, "text": " I googled around a found that could have been related to some privilege escalation issues.", "tokens": [286, 50061, 1493, 926, 257, 1352, 300, 727, 362, 668, 4077, 281, 512, 12122, 17871, 399, 2663, 13], "temperature": 0.0, "avg_logprob": -0.09934381816698157, "compression_ratio": 1.506726457399103, "no_speech_prob": 5.8239900681655854e-05}, {"id": 112, "seek": 62692, "start": 636.92, "end": 639.92, "text": " However, if you just remount the C groups, the file appears.", "tokens": [2908, 11, 498, 291, 445, 890, 792, 264, 383, 3935, 11, 264, 3991, 7038, 13], "temperature": 0.0, "avg_logprob": -0.09934381816698157, "compression_ratio": 1.506726457399103, "no_speech_prob": 5.8239900681655854e-05}, {"id": 113, "seek": 62692, "start": 639.92, "end": 642.92, "text": " So I'm not sure what's going on there.", "tokens": [407, 286, 478, 406, 988, 437, 311, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.09934381816698157, "compression_ratio": 1.506726457399103, "no_speech_prob": 5.8239900681655854e-05}, {"id": 114, "seek": 62692, "start": 642.92, "end": 647.92, "text": " Another fun story is that, for instance, if you're using Kubernetes,", "tokens": [3996, 1019, 1657, 307, 300, 11, 337, 5197, 11, 498, 291, 434, 1228, 23145, 11], "temperature": 0.0, "avg_logprob": -0.09934381816698157, "compression_ratio": 1.506726457399103, "no_speech_prob": 5.8239900681655854e-05}, {"id": 115, "seek": 64792, "start": 647.92, "end": 657.92, "text": " Kubernetes likes to give a sim link to your secrets, and munch refuses to take the secret from a sim link for security reasons.", "tokens": [23145, 5902, 281, 976, 257, 1034, 2113, 281, 428, 14093, 11, 293, 275, 1680, 33222, 281, 747, 264, 4054, 490, 257, 1034, 2113, 337, 3825, 4112, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 116, "seek": 64792, "start": 657.92, "end": 659.92, "text": " It makes sense. So there's no more.", "tokens": [467, 1669, 2020, 13, 407, 456, 311, 572, 544, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 117, "seek": 64792, "start": 659.92, "end": 661.92, "text": " So you will need to put in hacks.", "tokens": [407, 291, 486, 643, 281, 829, 294, 33617, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 118, "seek": 64792, "start": 661.92, "end": 665.92, "text": " And it's hacks on top of hacks on top of hacks just to run these two demons.", "tokens": [400, 309, 311, 33617, 322, 1192, 295, 33617, 322, 1192, 295, 33617, 445, 281, 1190, 613, 732, 19733, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 119, "seek": 64792, "start": 665.92, "end": 670.92, "text": " And yeah, I was not very happy with this approach either.", "tokens": [400, 1338, 11, 286, 390, 406, 588, 2055, 365, 341, 3109, 2139, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 120, "seek": 64792, "start": 670.92, "end": 675.92, "text": " So basically I was faced with two options.", "tokens": [407, 1936, 286, 390, 11446, 365, 732, 3956, 13], "temperature": 0.0, "avg_logprob": -0.17977601230734647, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00016576406778767705}, {"id": 121, "seek": 67592, "start": 675.92, "end": 677.92, "text": " We arrived at this situation. You're faced with two options.", "tokens": [492, 6678, 412, 341, 2590, 13, 509, 434, 11446, 365, 732, 3956, 13], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 122, "seek": 67592, "start": 677.92, "end": 683.92, "text": " Either you basically do the first naive approach where you just copy all the stuff into your slurm container.", "tokens": [13746, 291, 1936, 360, 264, 700, 29052, 3109, 689, 291, 445, 5055, 439, 264, 1507, 666, 428, 1061, 26717, 10129, 13], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 123, "seek": 67592, "start": 683.92, "end": 686.92, "text": " You manage a copy of your slurm config files.", "tokens": [509, 3067, 257, 5055, 295, 428, 1061, 26717, 6662, 7098, 13], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 124, "seek": 67592, "start": 686.92, "end": 693.92, "text": " But as I said, if you want a single source of truth, this might not be ideal.", "tokens": [583, 382, 286, 848, 11, 498, 291, 528, 257, 2167, 4009, 295, 3494, 11, 341, 1062, 406, 312, 7157, 13], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 125, "seek": 67592, "start": 693.92, "end": 697.92, "text": " You also need, of course, in the case of use case, unless you need munch,", "tokens": [509, 611, 643, 11, 295, 1164, 11, 294, 264, 1389, 295, 764, 1389, 11, 5969, 291, 643, 275, 1680, 11], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 126, "seek": 67592, "start": 697.92, "end": 699.92, "text": " and you need to supply the munch key.", "tokens": [293, 291, 643, 281, 5847, 264, 275, 1680, 2141, 13], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 127, "seek": 67592, "start": 699.92, "end": 703.92, "text": " Or you can try the configless approach, but then you need to add slurm d to your container", "tokens": [1610, 291, 393, 853, 264, 6662, 1832, 3109, 11, 457, 550, 291, 643, 281, 909, 1061, 26717, 274, 281, 428, 10129], "temperature": 0.0, "avg_logprob": -0.15404322653105765, "compression_ratio": 1.7686832740213523, "no_speech_prob": 6.291520548984408e-05}, {"id": 128, "seek": 70392, "start": 703.92, "end": 706.92, "text": " so it can pull via configless your config files.", "tokens": [370, 309, 393, 2235, 5766, 6662, 1832, 428, 6662, 7098, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 129, "seek": 70392, "start": 706.92, "end": 708.92, "text": " But then anyway, you also need munch.", "tokens": [583, 550, 4033, 11, 291, 611, 643, 275, 1680, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 130, "seek": 70392, "start": 708.92, "end": 712.92, "text": " And you need to add the munch key to your container somehow and managing secrets.", "tokens": [400, 291, 643, 281, 909, 264, 275, 1680, 2141, 281, 428, 10129, 6063, 293, 11642, 14093, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 131, "seek": 70392, "start": 712.92, "end": 716.92, "text": " I mean, if you're running Kubernetes, it might not be a big issue or some other container manager.", "tokens": [286, 914, 11, 498, 291, 434, 2614, 23145, 11, 309, 1062, 406, 312, 257, 955, 2734, 420, 512, 661, 10129, 6598, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 132, "seek": 70392, "start": 716.92, "end": 722.92, "text": " But you will still need to maintain all these extra demons with nasty hacks.", "tokens": [583, 291, 486, 920, 643, 281, 6909, 439, 613, 2857, 19733, 365, 17923, 33617, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 133, "seek": 70392, "start": 722.92, "end": 728.92, "text": " And we don't always like all these having lots of hacks in our infrastructure.", "tokens": [400, 321, 500, 380, 1009, 411, 439, 613, 1419, 3195, 295, 33617, 294, 527, 6896, 13], "temperature": 0.0, "avg_logprob": -0.088771729242234, "compression_ratio": 1.6987951807228916, "no_speech_prob": 4.5289070840226486e-05}, {"id": 134, "seek": 72892, "start": 728.92, "end": 733.92, "text": " There's a third option, by the way, which is trying to go secret less.", "tokens": [821, 311, 257, 2636, 3614, 11, 538, 264, 636, 11, 597, 307, 1382, 281, 352, 4054, 1570, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 135, "seek": 72892, "start": 733.92, "end": 738.92, "text": " It doesn't work in combination with configless, where you try to use JSON web tokens.", "tokens": [467, 1177, 380, 589, 294, 6562, 365, 6662, 1832, 11, 689, 291, 853, 281, 764, 31828, 3670, 22667, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 136, "seek": 72892, "start": 738.92, "end": 741.92, "text": " But it gives a lot of issues. It doesn't really work. I tried it.", "tokens": [583, 309, 2709, 257, 688, 295, 2663, 13, 467, 1177, 380, 534, 589, 13, 286, 3031, 309, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 137, "seek": 72892, "start": 741.92, "end": 743.92, "text": " So I didn't include it here.", "tokens": [407, 286, 994, 380, 4090, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 138, "seek": 72892, "start": 743.92, "end": 748.92, "text": " Just mentioning it in case somebody thought about it.", "tokens": [1449, 18315, 309, 294, 1389, 2618, 1194, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 139, "seek": 72892, "start": 748.92, "end": 751.92, "text": " So Pablo, you talked about the bad and the ugly. What about the good?", "tokens": [407, 31554, 11, 291, 2825, 466, 264, 1578, 293, 264, 12246, 13, 708, 466, 264, 665, 30], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 140, "seek": 72892, "start": 751.92, "end": 753.92, "text": " Is there any good part to this?", "tokens": [1119, 456, 604, 665, 644, 281, 341, 30], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 141, "seek": 72892, "start": 753.92, "end": 754.92, "text": " I'm glad you asked.", "tokens": [286, 478, 5404, 291, 2351, 13], "temperature": 0.0, "avg_logprob": -0.09344390962944656, "compression_ratio": 1.599250936329588, "no_speech_prob": 9.102649346459657e-05}, {"id": 142, "seek": 75492, "start": 754.92, "end": 759.92, "text": " Yes. What if we had a single shot CLI tool,", "tokens": [1079, 13, 708, 498, 321, 632, 257, 2167, 3347, 12855, 40, 2290, 11], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 143, "seek": 75492, "start": 759.92, "end": 764.92, "text": " that just a very simple tool that just was able to authenticate to the controller,", "tokens": [300, 445, 257, 588, 2199, 2290, 300, 445, 390, 1075, 281, 9214, 8700, 281, 264, 10561, 11], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 144, "seek": 75492, "start": 764.92, "end": 768.92, "text": " either using munch or JSON web tokens, which Slurm also supports,", "tokens": [2139, 1228, 275, 1680, 420, 31828, 3670, 22667, 11, 597, 6187, 26717, 611, 9346, 11], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 145, "seek": 75492, "start": 768.92, "end": 772.92, "text": " and just fetch the config files, and then it's done.", "tokens": [293, 445, 23673, 264, 6662, 7098, 11, 293, 550, 309, 311, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 146, "seek": 75492, "start": 772.92, "end": 775.92, "text": " That's all you really want to do, right?", "tokens": [663, 311, 439, 291, 534, 528, 281, 360, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 147, "seek": 75492, "start": 775.92, "end": 779.92, "text": " Because then your tools, the Slurm tools can work,", "tokens": [1436, 550, 428, 3873, 11, 264, 6187, 26717, 3873, 393, 589, 11], "temperature": 0.0, "avg_logprob": -0.11920016439337479, "compression_ratio": 1.5248868778280542, "no_speech_prob": 4.6018598368391395e-05}, {"id": 148, "seek": 77992, "start": 779.92, "end": 784.92, "text": " because they have the Slurm config files, and just by having the JSON web token in your environment,", "tokens": [570, 436, 362, 264, 6187, 26717, 6662, 7098, 11, 293, 445, 538, 1419, 264, 31828, 3670, 14862, 294, 428, 2823, 11], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 149, "seek": 77992, "start": 784.92, "end": 788.92, "text": " you can just talk to the Slurm controller.", "tokens": [291, 393, 445, 751, 281, 264, 6187, 26717, 10561, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 150, "seek": 77992, "start": 788.92, "end": 791.92, "text": " And yeah, that's the tool that I wrote.", "tokens": [400, 1338, 11, 300, 311, 264, 2290, 300, 286, 4114, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 151, "seek": 77992, "start": 791.92, "end": 795.92, "text": " It's a very simple tool. It just does exactly what I described there.", "tokens": [467, 311, 257, 588, 2199, 2290, 13, 467, 445, 775, 2293, 437, 286, 7619, 456, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 152, "seek": 77992, "start": 795.92, "end": 799.92, "text": " And it's open source. You can find it on GitHub.", "tokens": [400, 309, 311, 1269, 4009, 13, 509, 393, 915, 309, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 153, "seek": 77992, "start": 799.92, "end": 802.92, "text": " I uploaded it in the past month.", "tokens": [286, 17135, 309, 294, 264, 1791, 1618, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 154, "seek": 77992, "start": 802.92, "end": 804.92, "text": " Fun story about this.", "tokens": [11166, 1657, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 155, "seek": 77992, "start": 804.92, "end": 807.92, "text": " As I said, I had the idea for this when I was back at CERN.", "tokens": [1018, 286, 848, 11, 286, 632, 264, 1558, 337, 341, 562, 286, 390, 646, 412, 383, 1598, 45, 13], "temperature": 0.0, "avg_logprob": -0.10604123044605097, "compression_ratio": 1.5916030534351144, "no_speech_prob": 9.70536275417544e-05}, {"id": 156, "seek": 80792, "start": 807.92, "end": 811.92, "text": " I worked on this a year ago already.", "tokens": [286, 2732, 322, 341, 257, 1064, 2057, 1217, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 157, "seek": 80792, "start": 811.92, "end": 814.92, "text": " But then I somehow lost the source.", "tokens": [583, 550, 286, 6063, 2731, 264, 4009, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 158, "seek": 80792, "start": 814.92, "end": 816.92, "text": " I don't know what happened.", "tokens": [286, 500, 380, 458, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 159, "seek": 80792, "start": 816.92, "end": 819.92, "text": " Just before I left CERN, the source was just lost.", "tokens": [1449, 949, 286, 1411, 383, 1598, 45, 11, 264, 4009, 390, 445, 2731, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 160, "seek": 80792, "start": 819.92, "end": 822.92, "text": " I don't know why. I must have deleted it by accident.", "tokens": [286, 500, 380, 458, 983, 13, 286, 1633, 362, 22981, 309, 538, 6398, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 161, "seek": 80792, "start": 822.92, "end": 824.92, "text": " I don't know what happened.", "tokens": [286, 500, 380, 458, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 162, "seek": 80792, "start": 824.92, "end": 827.92, "text": " So after I left CERN, I kept in contact with my ex-colleagues,", "tokens": [407, 934, 286, 1411, 383, 1598, 45, 11, 286, 4305, 294, 3385, 365, 452, 454, 12, 1291, 2447, 7063, 11], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 163, "seek": 80792, "start": 827.92, "end": 831.92, "text": " and they were telling me that they wanted to do this integration between the swan,", "tokens": [293, 436, 645, 3585, 385, 300, 436, 1415, 281, 360, 341, 10980, 1296, 264, 1693, 282, 11], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 164, "seek": 80792, "start": 831.92, "end": 835.92, "text": " which is the who here knows swan? Anybody?", "tokens": [597, 307, 264, 567, 510, 3255, 1693, 282, 30, 19082, 30], "temperature": 0.0, "avg_logprob": -0.08228425904521793, "compression_ratio": 1.6947791164658634, "no_speech_prob": 8.329861157108098e-05}, {"id": 165, "seek": 83592, "start": 835.92, "end": 837.92, "text": " Okay, one, two, three.", "tokens": [1033, 11, 472, 11, 732, 11, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 166, "seek": 83592, "start": 837.92, "end": 840.92, "text": " Yeah, so it's the Jupyter Notebook Service for CERN,", "tokens": [865, 11, 370, 309, 311, 264, 22125, 88, 391, 11633, 2939, 9561, 337, 383, 1598, 45, 11], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 167, "seek": 83592, "start": 840.92, "end": 843.92, "text": " which also does analytics.", "tokens": [597, 611, 775, 15370, 13], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 168, "seek": 83592, "start": 843.92, "end": 845.92, "text": " And we wanted to connect it to Slurm,", "tokens": [400, 321, 1415, 281, 1745, 309, 281, 6187, 26717, 11], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 169, "seek": 83592, "start": 845.92, "end": 849.92, "text": " and we run into all these issues, because this is a service that's exposed to the whole internet.", "tokens": [293, 321, 1190, 666, 439, 613, 2663, 11, 570, 341, 307, 257, 2643, 300, 311, 9495, 281, 264, 1379, 4705, 13], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 170, "seek": 83592, "start": 849.92, "end": 854.92, "text": " So we didn't want to have the munchkey for the Slurm cluster in the container, et cetera.", "tokens": [407, 321, 994, 380, 528, 281, 362, 264, 275, 1680, 4119, 337, 264, 6187, 26717, 13630, 294, 264, 10129, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 171, "seek": 83592, "start": 854.92, "end": 859.92, "text": " Anyway, so then I left CERN, and then, yeah, my colleagues were telling me,", "tokens": [5684, 11, 370, 550, 286, 1411, 383, 1598, 45, 11, 293, 550, 11, 1338, 11, 452, 7734, 645, 3585, 385, 11], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 172, "seek": 83592, "start": 859.92, "end": 862.92, "text": " oh, it would have been so useful to have this at Watapiti.", "tokens": [1954, 11, 309, 576, 362, 668, 370, 4420, 281, 362, 341, 412, 12593, 569, 8707, 13], "temperature": 0.0, "avg_logprob": -0.1405203153761171, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.00012898088607471436}, {"id": 173, "seek": 86292, "start": 862.92, "end": 869.92, "text": " And then a few months ago, I just didn't like the fact that I had lost the source and all these days.", "tokens": [400, 550, 257, 1326, 2493, 2057, 11, 286, 445, 994, 380, 411, 264, 1186, 300, 286, 632, 2731, 264, 4009, 293, 439, 613, 1708, 13], "temperature": 0.0, "avg_logprob": -0.08179346395998585, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.697205339558423e-05}, {"id": 174, "seek": 86292, "start": 869.92, "end": 874.92, "text": " I spent a couple of days reverse-engineering the Slurm protocol,", "tokens": [286, 4418, 257, 1916, 295, 1708, 9943, 12, 25609, 1794, 264, 6187, 26717, 10336, 11], "temperature": 0.0, "avg_logprob": -0.08179346395998585, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.697205339558423e-05}, {"id": 175, "seek": 86292, "start": 874.92, "end": 882.92, "text": " and I just didn't like losing it, so I just rewrote it more properly in Python and just made it public.", "tokens": [293, 286, 445, 994, 380, 411, 7027, 309, 11, 370, 286, 445, 319, 7449, 1370, 309, 544, 6108, 294, 15329, 293, 445, 1027, 309, 1908, 13], "temperature": 0.0, "avg_logprob": -0.08179346395998585, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.697205339558423e-05}, {"id": 176, "seek": 86292, "start": 882.92, "end": 887.92, "text": " So if you're interested in making client containers like this,", "tokens": [407, 498, 291, 434, 3102, 294, 1455, 6423, 17089, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.08179346395998585, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.697205339558423e-05}, {"id": 177, "seek": 86292, "start": 887.92, "end": 891.92, "text": " feel free to give it a try.", "tokens": [841, 1737, 281, 976, 309, 257, 853, 13], "temperature": 0.0, "avg_logprob": -0.08179346395998585, "compression_ratio": 1.5973451327433628, "no_speech_prob": 8.697205339558423e-05}, {"id": 178, "seek": 89192, "start": 891.92, "end": 893.92, "text": " It looks a bit like this.", "tokens": [467, 1542, 257, 857, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 179, "seek": 89192, "start": 893.92, "end": 895.92, "text": " It's very simple.", "tokens": [467, 311, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 180, "seek": 89192, "start": 895.92, "end": 901.92, "text": " You can choose between munch or JWT, JSON WebToken's authentication.", "tokens": [509, 393, 2826, 1296, 275, 1680, 420, 49885, 51, 11, 31828, 9573, 51, 8406, 311, 26643, 13], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 181, "seek": 89192, "start": 901.92, "end": 904.92, "text": " If you choose JWT, which is the most simple one,", "tokens": [759, 291, 2826, 49885, 51, 11, 597, 307, 264, 881, 2199, 472, 11], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 182, "seek": 89192, "start": 904.92, "end": 908.92, "text": " you just need an environment variable with a token,", "tokens": [291, 445, 643, 364, 2823, 7006, 365, 257, 14862, 11], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 183, "seek": 89192, "start": 908.92, "end": 912.92, "text": " and you can tell it where you want to store the config files,", "tokens": [293, 291, 393, 980, 309, 689, 291, 528, 281, 3531, 264, 6662, 7098, 11], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 184, "seek": 89192, "start": 912.92, "end": 916.92, "text": " and then you have verbosity as an option.", "tokens": [293, 550, 291, 362, 9595, 20373, 382, 364, 3614, 13], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 185, "seek": 89192, "start": 916.92, "end": 918.92, "text": " So it's very simple.", "tokens": [407, 309, 311, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10047112464904785, "compression_ratio": 1.5504587155963303, "no_speech_prob": 7.122454553609714e-05}, {"id": 186, "seek": 91892, "start": 918.92, "end": 924.92, "text": " It has very little dependencies.", "tokens": [467, 575, 588, 707, 36606, 13], "temperature": 0.0, "avg_logprob": -0.1320049043685671, "compression_ratio": 1.5548780487804879, "no_speech_prob": 0.00010506359103601426}, {"id": 187, "seek": 91892, "start": 924.92, "end": 931.92, "text": " So the tool talks several Slurm protocol versions,", "tokens": [407, 264, 2290, 6686, 2940, 6187, 26717, 10336, 9606, 11], "temperature": 0.0, "avg_logprob": -0.1320049043685671, "compression_ratio": 1.5548780487804879, "no_speech_prob": 0.00010506359103601426}, {"id": 188, "seek": 91892, "start": 931.92, "end": 937.92, "text": " because with every major release, Slurm changes the protocol versions.", "tokens": [570, 365, 633, 2563, 4374, 11, 6187, 26717, 2962, 264, 10336, 9606, 13], "temperature": 0.0, "avg_logprob": -0.1320049043685671, "compression_ratio": 1.5548780487804879, "no_speech_prob": 0.00010506359103601426}, {"id": 189, "seek": 91892, "start": 937.92, "end": 940.92, "text": " So you can list them with minus L,", "tokens": [407, 291, 393, 1329, 552, 365, 3175, 441, 11], "temperature": 0.0, "avg_logprob": -0.1320049043685671, "compression_ratio": 1.5548780487804879, "no_speech_prob": 0.00010506359103601426}, {"id": 190, "seek": 91892, "start": 940.92, "end": 946.92, "text": " and it will show you basically all the versions that it supports.", "tokens": [293, 309, 486, 855, 291, 1936, 439, 264, 9606, 300, 309, 9346, 13], "temperature": 0.0, "avg_logprob": -0.1320049043685671, "compression_ratio": 1.5548780487804879, "no_speech_prob": 0.00010506359103601426}, {"id": 191, "seek": 94692, "start": 946.92, "end": 950.92, "text": " So imagine you have a Slurm WebToken in this variable.", "tokens": [407, 3811, 291, 362, 257, 6187, 26717, 9573, 51, 8406, 294, 341, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 192, "seek": 94692, "start": 950.92, "end": 955.92, "text": " You can just tell it to do JSON WebToken authentication with the server.", "tokens": [509, 393, 445, 980, 309, 281, 360, 31828, 9573, 51, 8406, 26643, 365, 264, 7154, 13], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 193, "seek": 94692, "start": 955.92, "end": 960.92, "text": " It supports multiple controllers in case you have high availability set up in your Slurm cluster,", "tokens": [467, 9346, 3866, 26903, 294, 1389, 291, 362, 1090, 17945, 992, 493, 294, 428, 6187, 26717, 13630, 11], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 194, "seek": 94692, "start": 960.92, "end": 964.92, "text": " so you can specify a list of servers that it will retry until it succeeds,", "tokens": [370, 291, 393, 16500, 257, 1329, 295, 15909, 300, 309, 486, 1533, 627, 1826, 309, 49263, 11], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 195, "seek": 94692, "start": 964.92, "end": 967.92, "text": " and then you tell it the protocol version of the Slurm CTLD,", "tokens": [293, 550, 291, 980, 309, 264, 10336, 3037, 295, 264, 6187, 26717, 19529, 23704, 11], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 196, "seek": 94692, "start": 967.92, "end": 971.92, "text": " because it needs to know what protocol it should talk.", "tokens": [570, 309, 2203, 281, 458, 437, 10336, 309, 820, 751, 13], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 197, "seek": 94692, "start": 971.92, "end": 975.92, "text": " The protocol version negotiation, I think it doesn't exist in the Slurm protocol,", "tokens": [440, 10336, 3037, 27573, 11, 286, 519, 309, 1177, 380, 2514, 294, 264, 6187, 26717, 10336, 11], "temperature": 0.0, "avg_logprob": -0.08258853420134514, "compression_ratio": 1.688135593220339, "no_speech_prob": 4.39579198427964e-05}, {"id": 198, "seek": 97592, "start": 975.92, "end": 978.92, "text": " so you have to tell it which version you want it to talk,", "tokens": [370, 291, 362, 281, 980, 309, 597, 3037, 291, 528, 309, 281, 751, 11], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 199, "seek": 97592, "start": 978.92, "end": 982.92, "text": " and that's it, and then it will just download the Slurm config files", "tokens": [293, 300, 311, 309, 11, 293, 550, 309, 486, 445, 5484, 264, 6187, 26717, 6662, 7098], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 200, "seek": 97592, "start": 982.92, "end": 987.92, "text": " and happy days for your containers.", "tokens": [293, 2055, 1708, 337, 428, 17089, 13], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 201, "seek": 97592, "start": 987.92, "end": 992.92, "text": " Conclusions, I think I'm ahead of time.", "tokens": [18200, 3063, 626, 11, 286, 519, 286, 478, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 202, "seek": 97592, "start": 992.92, "end": 998.92, "text": " So this tool called straw, it can simplify the cost of creating and maintaining", "tokens": [407, 341, 2290, 1219, 10099, 11, 309, 393, 20460, 264, 2063, 295, 4084, 293, 14916], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 203, "seek": 97592, "start": 998.92, "end": 1000.92, "text": " your Slurm client containers.", "tokens": [428, 6187, 26717, 6423, 17089, 13], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 204, "seek": 97592, "start": 1000.92, "end": 1002.92, "text": " It can also increase the security,", "tokens": [467, 393, 611, 3488, 264, 3825, 11], "temperature": 0.0, "avg_logprob": -0.12212435404459636, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.862032149801962e-05}, {"id": 205, "seek": 100292, "start": 1002.92, "end": 1005.92, "text": " because you don't need to put the Munch key everywhere,", "tokens": [570, 291, 500, 380, 643, 281, 829, 264, 376, 1680, 2141, 5315, 11], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 206, "seek": 100292, "start": 1005.92, "end": 1008.92, "text": " where you're running your client containers.", "tokens": [689, 291, 434, 2614, 428, 6423, 17089, 13], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 207, "seek": 100292, "start": 1008.92, "end": 1011.92, "text": " JSON WebToken's surface.", "tokens": [31828, 9573, 51, 8406, 311, 3753, 13], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 208, "seek": 100292, "start": 1011.92, "end": 1014.92, "text": " Caveats, caveats.", "tokens": [41100, 1720, 11, 11730, 1720, 13], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 209, "seek": 100292, "start": 1014.92, "end": 1018.92, "text": " I think this tool should not exist,", "tokens": [286, 519, 341, 2290, 820, 406, 2514, 11], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 210, "seek": 100292, "start": 1018.92, "end": 1021.92, "text": " because ideally this would be supported upstream.", "tokens": [570, 22915, 341, 576, 312, 8104, 33915, 13], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 211, "seek": 100292, "start": 1021.92, "end": 1027.92, "text": " So, you know, if anybody has any influence on SCADMD Slurm development,", "tokens": [407, 11, 291, 458, 11, 498, 4472, 575, 604, 6503, 322, 9028, 6112, 44, 35, 6187, 26717, 3250, 11], "temperature": 0.0, "avg_logprob": -0.2058176826028263, "compression_ratio": 1.4, "no_speech_prob": 7.225826266221702e-05}, {"id": 212, "seek": 102792, "start": 1027.92, "end": 1033.92, "text": " yeah, I think it would be nice if we had this built-in into Slurm.", "tokens": [1338, 11, 286, 519, 309, 576, 312, 1481, 498, 321, 632, 341, 3094, 12, 259, 666, 6187, 26717, 13], "temperature": 0.0, "avg_logprob": -0.11336475679244118, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.1758291040896438e-05}, {"id": 213, "seek": 102792, "start": 1033.92, "end": 1038.92, "text": " And then the second caveat is that the JSON WebToken,", "tokens": [400, 550, 264, 1150, 43012, 307, 300, 264, 31828, 9573, 51, 8406, 11], "temperature": 0.0, "avg_logprob": -0.11336475679244118, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.1758291040896438e-05}, {"id": 214, "seek": 102792, "start": 1038.92, "end": 1044.92, "text": " the token needs to be associated with a Slurm user, basically.", "tokens": [264, 14862, 2203, 281, 312, 6615, 365, 257, 6187, 26717, 4195, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.11336475679244118, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.1758291040896438e-05}, {"id": 215, "seek": 102792, "start": 1044.92, "end": 1050.92, "text": " So ideally, you would be able to just generate a JSON WebToken for a user", "tokens": [407, 22915, 11, 291, 576, 312, 1075, 281, 445, 8460, 257, 31828, 9573, 51, 8406, 337, 257, 4195], "temperature": 0.0, "avg_logprob": -0.11336475679244118, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.1758291040896438e-05}, {"id": 216, "seek": 102792, "start": 1050.92, "end": 1053.92, "text": " that's going to run on the Slurm cluster,", "tokens": [300, 311, 516, 281, 1190, 322, 264, 6187, 26717, 13630, 11], "temperature": 0.0, "avg_logprob": -0.11336475679244118, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.1758291040896438e-05}, {"id": 217, "seek": 105392, "start": 1053.92, "end": 1057.92, "text": " and then if the secret for some reason is exposed, you've only exposed", "tokens": [293, 550, 498, 264, 4054, 337, 512, 1778, 307, 9495, 11, 291, 600, 787, 9495], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 218, "seek": 105392, "start": 1057.92, "end": 1060.92, "text": " the JSON WebToken of a single user.", "tokens": [264, 31828, 9573, 51, 8406, 295, 257, 2167, 4195, 13], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 219, "seek": 105392, "start": 1060.92, "end": 1066.92, "text": " However, this is a limitation built into the Slurm, into Slurm, basically.", "tokens": [2908, 11, 341, 307, 257, 27432, 3094, 666, 264, 6187, 26717, 11, 666, 6187, 26717, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 220, "seek": 105392, "start": 1066.92, "end": 1069.92, "text": " You cannot pull over the protocol the Slurm config file", "tokens": [509, 2644, 2235, 670, 264, 10336, 264, 6187, 26717, 6662, 3991], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 221, "seek": 105392, "start": 1069.92, "end": 1074.92, "text": " unless the token belongs to the Slurm user, or to root.", "tokens": [5969, 264, 14862, 12953, 281, 264, 6187, 26717, 4195, 11, 420, 281, 5593, 13], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 222, "seek": 105392, "start": 1074.92, "end": 1079.92, "text": " Still, I think it's an improvement over having your Munch key available everywhere.", "tokens": [8291, 11, 286, 519, 309, 311, 364, 10444, 670, 1419, 428, 376, 1680, 2141, 2435, 5315, 13], "temperature": 0.0, "avg_logprob": -0.10231830134536281, "compression_ratio": 1.551440329218107, "no_speech_prob": 1.6942905858741142e-05}, {"id": 223, "seek": 107992, "start": 1079.92, "end": 1083.92, "text": " If you're free to try it out, that was it.", "tokens": [759, 291, 434, 1737, 281, 853, 309, 484, 11, 300, 390, 309, 13], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 224, "seek": 107992, "start": 1083.92, "end": 1087.92, "text": " I'm happy to answer any questions you might have.", "tokens": [286, 478, 2055, 281, 1867, 604, 1651, 291, 1062, 362, 13], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 225, "seek": 107992, "start": 1087.92, "end": 1090.92, "text": " APPLAUSE", "tokens": [35298], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 226, "seek": 107992, "start": 1093.92, "end": 1096.92, "text": " Thank you very much, Pablo.", "tokens": [1044, 291, 588, 709, 11, 31554, 13], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 227, "seek": 107992, "start": 1096.92, "end": 1099.92, "text": " Time for questions.", "tokens": [6161, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 228, "seek": 107992, "start": 1099.92, "end": 1103.92, "text": " So what kind of clients do need the config file?", "tokens": [407, 437, 733, 295, 6982, 360, 643, 264, 6662, 3991, 30], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 229, "seek": 107992, "start": 1103.92, "end": 1105.92, "text": " Could you do everything over REST nowadays?", "tokens": [7497, 291, 360, 1203, 670, 497, 14497, 13434, 30], "temperature": 0.0, "avg_logprob": -0.17306760946909586, "compression_ratio": 1.3224043715846994, "no_speech_prob": 0.0005451298202387989}, {"id": 230, "seek": 110592, "start": 1105.92, "end": 1109.92, "text": " Is it still necessary to use the config file?", "tokens": [1119, 309, 920, 4818, 281, 764, 264, 6662, 3991, 30], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 231, "seek": 110592, "start": 1109.92, "end": 1114.92, "text": " Yes, so anything that wants to run srun, sbatch, sq, sinfo.", "tokens": [1079, 11, 370, 1340, 300, 2738, 281, 1190, 262, 12997, 11, 262, 65, 852, 11, 262, 80, 11, 262, 259, 16931, 13], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 232, "seek": 110592, "start": 1114.92, "end": 1117.92, "text": " For instance, if you have the Jupyter Notebook plugins,", "tokens": [1171, 5197, 11, 498, 291, 362, 264, 22125, 88, 391, 11633, 2939, 33759, 11], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 233, "seek": 110592, "start": 1117.92, "end": 1119.92, "text": " they will just run those commands.", "tokens": [436, 486, 445, 1190, 729, 16901, 13], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 234, "seek": 110592, "start": 1119.92, "end": 1123.92, "text": " Or if you want to run a client that uses PySlurm, for instance,", "tokens": [1610, 498, 291, 528, 281, 1190, 257, 6423, 300, 4960, 9953, 50, 75, 26717, 11, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 235, "seek": 110592, "start": 1123.92, "end": 1127.92, "text": " or any library really, anything that uses lipslurm underneath", "tokens": [420, 604, 6405, 534, 11, 1340, 300, 4960, 10118, 75, 26717, 7223], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 236, "seek": 110592, "start": 1127.92, "end": 1130.92, "text": " will automatically read the config files, right?", "tokens": [486, 6772, 1401, 264, 6662, 7098, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15583554020634405, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00013252552889753133}, {"id": 237, "seek": 113092, "start": 1130.92, "end": 1135.92, "text": " So, of course, you can write your own client,", "tokens": [407, 11, 295, 1164, 11, 291, 393, 2464, 428, 1065, 6423, 11], "temperature": 0.0, "avg_logprob": -0.14413498224837057, "compression_ratio": 1.582010582010582, "no_speech_prob": 7.822597399353981e-05}, {"id": 238, "seek": 113092, "start": 1135.92, "end": 1141.92, "text": " handwritten from scratch, that just interacts with the Slurm REST to do stuff.", "tokens": [1011, 26859, 490, 8459, 11, 300, 445, 43582, 365, 264, 6187, 26717, 497, 14497, 281, 360, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14413498224837057, "compression_ratio": 1.582010582010582, "no_speech_prob": 7.822597399353981e-05}, {"id": 239, "seek": 113092, "start": 1141.92, "end": 1147.92, "text": " Yes, but you cannot leverage all the existing user client tools,", "tokens": [1079, 11, 457, 291, 2644, 13982, 439, 264, 6741, 4195, 6423, 3873, 11], "temperature": 0.0, "avg_logprob": -0.14413498224837057, "compression_ratio": 1.582010582010582, "no_speech_prob": 7.822597399353981e-05}, {"id": 240, "seek": 113092, "start": 1147.92, "end": 1150.92, "text": " and the lipslurm, PySlurm, etc.", "tokens": [293, 264, 10118, 75, 26717, 11, 9953, 50, 75, 26717, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.14413498224837057, "compression_ratio": 1.582010582010582, "no_speech_prob": 7.822597399353981e-05}, {"id": 241, "seek": 113092, "start": 1150.92, "end": 1156.92, "text": " So if you want to create a Python tool, for instance, that leverages PySlurm,", "tokens": [407, 498, 291, 528, 281, 1884, 257, 15329, 2290, 11, 337, 5197, 11, 300, 12451, 1660, 9953, 50, 75, 26717, 11], "temperature": 0.0, "avg_logprob": -0.14413498224837057, "compression_ratio": 1.582010582010582, "no_speech_prob": 7.822597399353981e-05}, {"id": 242, "seek": 115692, "start": 1156.92, "end": 1160.92, "text": " this would be, I think, a good solution.", "tokens": [341, 576, 312, 11, 286, 519, 11, 257, 665, 3827, 13], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 243, "seek": 115692, "start": 1160.92, "end": 1165.92, "text": " I think Slurm does have, like, a REST API, but it's considered very insecure.", "tokens": [286, 519, 6187, 26717, 775, 362, 11, 411, 11, 257, 497, 14497, 9362, 11, 457, 309, 311, 4888, 588, 32215, 13], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 244, "seek": 115692, "start": 1165.92, "end": 1169.92, "text": " So even the documentation tells you, like, don't use this.", "tokens": [407, 754, 264, 14333, 5112, 291, 11, 411, 11, 500, 380, 764, 341, 13], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 245, "seek": 115692, "start": 1169.92, "end": 1173.92, "text": " I just didn't understand, like, for a long time now,", "tokens": [286, 445, 994, 380, 1223, 11, 411, 11, 337, 257, 938, 565, 586, 11], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 246, "seek": 115692, "start": 1173.92, "end": 1176.92, "text": " why everyone needs the config file, right?", "tokens": [983, 1518, 2203, 264, 6662, 3991, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 247, "seek": 115692, "start": 1176.92, "end": 1178.92, "text": " I mean, why doesn't it need to be in sync?", "tokens": [286, 914, 11, 983, 1177, 380, 309, 643, 281, 312, 294, 20271, 30], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 248, "seek": 115692, "start": 1178.92, "end": 1181.92, "text": " Like, couldn't they just exchange the information over the protocols now", "tokens": [1743, 11, 2809, 380, 436, 445, 7742, 264, 1589, 670, 264, 20618, 586], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 249, "seek": 115692, "start": 1181.92, "end": 1183.92, "text": " and just say, like, this is your Slurm server?", "tokens": [293, 445, 584, 11, 411, 11, 341, 307, 428, 6187, 26717, 7154, 30], "temperature": 0.0, "avg_logprob": -0.12811736455039371, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.00016221919213421643}, {"id": 250, "seek": 118392, "start": 1183.92, "end": 1186.92, "text": " Yeah, that's a configless feature. That's a configless feature, essentially.", "tokens": [865, 11, 300, 311, 257, 6662, 1832, 4111, 13, 663, 311, 257, 6662, 1832, 4111, 11, 4476, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 251, "seek": 118392, "start": 1186.92, "end": 1188.92, "text": " Yeah, but the configless feature just downloads the config.", "tokens": [865, 11, 457, 264, 6662, 1832, 4111, 445, 36553, 264, 6662, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 252, "seek": 118392, "start": 1188.92, "end": 1189.92, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 253, "seek": 118392, "start": 1189.92, "end": 1190.92, "text": " Next, like, config less OK.", "tokens": [3087, 11, 411, 11, 6662, 1570, 2264, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 254, "seek": 118392, "start": 1190.92, "end": 1191.92, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 255, "seek": 118392, "start": 1191.92, "end": 1194.92, "text": " I download the config. I don't need the config beforehand.", "tokens": [286, 5484, 264, 6662, 13, 286, 500, 380, 643, 264, 6662, 22893, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 256, "seek": 118392, "start": 1194.92, "end": 1197.92, "text": " It's like serverless. There's always a server somewhere.", "tokens": [467, 311, 411, 7154, 1832, 13, 821, 311, 1009, 257, 7154, 4079, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 257, "seek": 118392, "start": 1197.92, "end": 1200.92, "text": " Yes. Yeah, exactly.", "tokens": [1079, 13, 865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 258, "seek": 118392, "start": 1200.92, "end": 1203.92, "text": " So that's just how Slurm works.", "tokens": [407, 300, 311, 445, 577, 6187, 26717, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 259, "seek": 118392, "start": 1203.92, "end": 1204.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.17303456101462106, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.00037774431984871626}, {"id": 260, "seek": 120492, "start": 1204.92, "end": 1214.92, "text": " So I'm still a little confused about the Slurm client container.", "tokens": [407, 286, 478, 920, 257, 707, 9019, 466, 264, 6187, 26717, 6423, 10129, 13], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 261, "seek": 120492, "start": 1214.92, "end": 1217.92, "text": " So the container is an application on the actual Slurm client,", "tokens": [407, 264, 10129, 307, 364, 3861, 322, 264, 3539, 6187, 26717, 6423, 11], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 262, "seek": 120492, "start": 1217.92, "end": 1219.92, "text": " because you have to document in the SlurmConf,", "tokens": [570, 291, 362, 281, 4166, 294, 264, 6187, 26717, 43874, 11], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 263, "seek": 120492, "start": 1219.92, "end": 1221.92, "text": " you have to sort of say what your clients are", "tokens": [291, 362, 281, 1333, 295, 584, 437, 428, 6982, 366], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 264, "seek": 120492, "start": 1221.92, "end": 1226.92, "text": " so that the scheduler can intelligently decide how to schedule jobs, right?", "tokens": [370, 300, 264, 12000, 260, 393, 5613, 2276, 4536, 577, 281, 7567, 4782, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 265, "seek": 120492, "start": 1226.92, "end": 1227.92, "text": " I'm missing something.", "tokens": [286, 478, 5361, 746, 13], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 266, "seek": 120492, "start": 1227.92, "end": 1231.92, "text": " No, you don't really need to declare all the clients for Slurm.", "tokens": [883, 11, 291, 500, 380, 534, 643, 281, 19710, 439, 264, 6982, 337, 6187, 26717, 13], "temperature": 0.0, "avg_logprob": -0.11030167872362798, "compression_ratio": 1.6652173913043478, "no_speech_prob": 6.293348269537091e-05}, {"id": 267, "seek": 123192, "start": 1231.92, "end": 1236.92, "text": " You just need to declare the worker nodes that are part of it.", "tokens": [509, 445, 643, 281, 19710, 264, 11346, 13891, 300, 366, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 268, "seek": 123192, "start": 1236.92, "end": 1238.92, "text": " But you can have any...", "tokens": [583, 291, 393, 362, 604, 485], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 269, "seek": 123192, "start": 1238.92, "end": 1240.92, "text": " I mean, it depends on how you've configured it.", "tokens": [286, 914, 11, 309, 5946, 322, 577, 291, 600, 30538, 309, 13], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 270, "seek": 123192, "start": 1240.92, "end": 1244.92, "text": " You can limit it. You can limit in Slurm which clients are allowed to connect,", "tokens": [509, 393, 4948, 309, 13, 509, 393, 4948, 294, 6187, 26717, 597, 6982, 366, 4350, 281, 1745, 11], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 271, "seek": 123192, "start": 1244.92, "end": 1245.92, "text": " but you don't have to.", "tokens": [457, 291, 500, 380, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 272, "seek": 123192, "start": 1245.92, "end": 1247.92, "text": " So you could just...", "tokens": [407, 291, 727, 445, 485], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 273, "seek": 123192, "start": 1247.92, "end": 1249.92, "text": " But even if you do, you will need this,", "tokens": [583, 754, 498, 291, 360, 11, 291, 486, 643, 341, 11], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 274, "seek": 123192, "start": 1249.92, "end": 1251.92, "text": " because you will...", "tokens": [570, 291, 486, 485], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 275, "seek": 123192, "start": 1251.92, "end": 1254.92, "text": " Even if you authorize a host name to connect as a client,", "tokens": [2754, 498, 291, 3793, 1125, 257, 3975, 1315, 281, 1745, 382, 257, 6423, 11], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 276, "seek": 123192, "start": 1254.92, "end": 1259.92, "text": " it will need to have the munch key and the SlurmConf files, et cetera.", "tokens": [309, 486, 643, 281, 362, 264, 275, 1680, 2141, 293, 264, 6187, 26717, 43874, 7098, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.10445266058950713, "compression_ratio": 1.7628458498023716, "no_speech_prob": 6.196383765200153e-05}, {"id": 277, "seek": 125992, "start": 1259.92, "end": 1261.92, "text": " Does this answer your question?", "tokens": [4402, 341, 1867, 428, 1168, 30], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 278, "seek": 125992, "start": 1261.92, "end": 1263.92, "text": " Well, no, so when you...", "tokens": [1042, 11, 572, 11, 370, 562, 291, 485], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 279, "seek": 125992, "start": 1263.92, "end": 1266.92, "text": " In the Slurm.conf, you sort of detail what your positions are,", "tokens": [682, 264, 6187, 26717, 13, 24697, 11, 291, 1333, 295, 2607, 437, 428, 8432, 366, 11], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 280, "seek": 125992, "start": 1266.92, "end": 1269.92, "text": " and you have to kind of tell it what the capabilities are of your clients,", "tokens": [293, 291, 362, 281, 733, 295, 980, 309, 437, 264, 10862, 366, 295, 428, 6982, 11], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 281, "seek": 125992, "start": 1269.92, "end": 1270.92, "text": " of your Slurm clients, right?", "tokens": [295, 428, 6187, 26717, 6982, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 282, "seek": 125992, "start": 1270.92, "end": 1272.92, "text": " So that Slurm can decide how to schedule jobs.", "tokens": [407, 300, 6187, 26717, 393, 4536, 577, 281, 7567, 4782, 13], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 283, "seek": 125992, "start": 1272.92, "end": 1274.92, "text": " I'm missing something.", "tokens": [286, 478, 5361, 746, 13], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 284, "seek": 125992, "start": 1274.92, "end": 1276.92, "text": " Well, I think you're thinking about the compute nodes.", "tokens": [1042, 11, 286, 519, 291, 434, 1953, 466, 264, 14722, 13891, 13], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 285, "seek": 125992, "start": 1276.92, "end": 1277.92, "text": " Yeah, I am.", "tokens": [865, 11, 286, 669, 13], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 286, "seek": 125992, "start": 1277.92, "end": 1280.92, "text": " Yeah, the node names part of the SlurmConf.", "tokens": [865, 11, 264, 9984, 5288, 644, 295, 264, 6187, 26717, 43874, 13], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 287, "seek": 125992, "start": 1280.92, "end": 1282.92, "text": " So the containers run on the compute nodes?", "tokens": [407, 264, 17089, 1190, 322, 264, 14722, 13891, 30], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 288, "seek": 125992, "start": 1282.92, "end": 1284.92, "text": " No, the containers would be...", "tokens": [883, 11, 264, 17089, 576, 312, 485], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 289, "seek": 125992, "start": 1284.92, "end": 1287.92, "text": " Let me go back to one of the slides where...", "tokens": [961, 385, 352, 646, 281, 472, 295, 264, 9788, 689, 485], "temperature": 0.0, "avg_logprob": -0.12902071568873022, "compression_ratio": 1.7736486486486487, "no_speech_prob": 8.335513120982796e-05}, {"id": 290, "seek": 128792, "start": 1287.92, "end": 1292.92, "text": " So you're thinking maybe about the compute nodes,", "tokens": [407, 291, 434, 1953, 1310, 466, 264, 14722, 13891, 11], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 291, "seek": 128792, "start": 1292.92, "end": 1294.92, "text": " each of which runs a Slurm DDemon,", "tokens": [1184, 295, 597, 6676, 257, 6187, 26717, 30778, 36228, 11], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 292, "seek": 128792, "start": 1294.92, "end": 1296.92, "text": " and those you have to declare.", "tokens": [293, 729, 291, 362, 281, 19710, 13], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 293, "seek": 128792, "start": 1296.92, "end": 1298.92, "text": " Yes, I think in 2023, by the way,", "tokens": [1079, 11, 286, 519, 294, 44377, 11, 538, 264, 636, 11], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 294, "seek": 128792, "start": 1298.92, "end": 1301.92, "text": " you will be able to dynamically spawn compute nodes,", "tokens": [291, 486, 312, 1075, 281, 43492, 17088, 14722, 13891, 11], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 295, "seek": 128792, "start": 1301.92, "end": 1305.92, "text": " but that's the future.", "tokens": [457, 300, 311, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 296, "seek": 128792, "start": 1305.92, "end": 1308.92, "text": " What I'm talking about is all the users and client tools", "tokens": [708, 286, 478, 1417, 466, 307, 439, 264, 5022, 293, 6423, 3873], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 297, "seek": 128792, "start": 1308.92, "end": 1311.92, "text": " that connect to the controller to run SQ as info,", "tokens": [300, 1745, 281, 264, 10561, 281, 1190, 318, 48, 382, 13614, 11], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 298, "seek": 128792, "start": 1311.92, "end": 1313.92, "text": " like when you use Slurm and you...", "tokens": [411, 562, 291, 764, 6187, 26717, 293, 291, 485], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 299, "seek": 128792, "start": 1313.92, "end": 1314.92, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.13927565394221125, "compression_ratio": 1.5847457627118644, "no_speech_prob": 6.195338210090995e-05}, {"id": 300, "seek": 131492, "start": 1314.92, "end": 1318.92, "text": " So if you had some tooling that you automated", "tokens": [407, 498, 291, 632, 512, 46593, 300, 291, 18473], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 301, "seek": 131492, "start": 1318.92, "end": 1322.92, "text": " to gather metrics from Slurm or, yeah,", "tokens": [281, 5448, 16367, 490, 6187, 26717, 420, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 302, "seek": 131492, "start": 1322.92, "end": 1324.92, "text": " a Jupyter notebook service, for instance,", "tokens": [257, 22125, 88, 391, 21060, 2643, 11, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 303, "seek": 131492, "start": 1324.92, "end": 1327.92, "text": " that connects to your cluster that wants to launch jobs,", "tokens": [300, 16967, 281, 428, 13630, 300, 2738, 281, 4025, 4782, 11], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 304, "seek": 131492, "start": 1327.92, "end": 1330.92, "text": " that wants to run as batch SQ, whatever,", "tokens": [300, 2738, 281, 1190, 382, 15245, 318, 48, 11, 2035, 11], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 305, "seek": 131492, "start": 1330.92, "end": 1332.92, "text": " that's in that domain.", "tokens": [300, 311, 294, 300, 9274, 13], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 306, "seek": 131492, "start": 1332.92, "end": 1335.92, "text": " Yeah, I mean, the newest werewolf runs containers", "tokens": [865, 11, 286, 914, 11, 264, 17569, 645, 34763, 6676, 17089], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 307, "seek": 131492, "start": 1335.92, "end": 1338.92, "text": " on my back for the stream.", "tokens": [322, 452, 646, 337, 264, 4309, 13], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 308, "seek": 131492, "start": 1338.92, "end": 1340.92, "text": " I mean, I think the newest version of werewolf", "tokens": [286, 914, 11, 286, 519, 264, 17569, 3037, 295, 645, 34763], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 309, "seek": 131492, "start": 1340.92, "end": 1343.92, "text": " is set up to run containers on the Slurm clients, right?", "tokens": [307, 992, 493, 281, 1190, 17089, 322, 264, 6187, 26717, 6982, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1448180401911501, "compression_ratio": 1.7398373983739837, "no_speech_prob": 1.3819289961247705e-05}, {"id": 310, "seek": 134392, "start": 1343.92, "end": 1346.92, "text": " It's sort of, you're actually launching containers", "tokens": [467, 311, 1333, 295, 11, 291, 434, 767, 18354, 17089], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 311, "seek": 134392, "start": 1346.92, "end": 1348.92, "text": " as applications, so that was kind of...", "tokens": [382, 5821, 11, 370, 300, 390, 733, 295, 485], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 312, "seek": 134392, "start": 1348.92, "end": 1350.92, "text": " That's on the compute nodes.", "tokens": [663, 311, 322, 264, 14722, 13891, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 313, "seek": 134392, "start": 1350.92, "end": 1351.92, "text": " On the compute nodes, yeah.", "tokens": [1282, 264, 14722, 13891, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 314, "seek": 134392, "start": 1351.92, "end": 1353.92, "text": " Yeah, yeah, that's the compute nodes.", "tokens": [865, 11, 1338, 11, 300, 311, 264, 14722, 13891, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 315, "seek": 134392, "start": 1356.92, "end": 1358.92, "text": " Thank you for your talk.", "tokens": [1044, 291, 337, 428, 751, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 316, "seek": 134392, "start": 1358.92, "end": 1360.92, "text": " So I have a question.", "tokens": [407, 286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 317, "seek": 134392, "start": 1360.92, "end": 1363.92, "text": " You are telling that you can pull the configuration", "tokens": [509, 366, 3585, 300, 291, 393, 2235, 264, 11694], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 318, "seek": 134392, "start": 1363.92, "end": 1367.92, "text": " with your tool, but there are many...", "tokens": [365, 428, 2290, 11, 457, 456, 366, 867, 485], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 319, "seek": 134392, "start": 1367.92, "end": 1370.92, "text": " Fine, you can't pull with configless.", "tokens": [12024, 11, 291, 393, 380, 2235, 365, 6662, 1832, 13], "temperature": 0.0, "avg_logprob": -0.17136915297735306, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008225367055274546}, {"id": 320, "seek": 137092, "start": 1370.92, "end": 1373.92, "text": " For example, all the spank plugins,", "tokens": [1171, 1365, 11, 439, 264, 637, 657, 33759, 11], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 321, "seek": 137092, "start": 1373.92, "end": 1375.92, "text": " or I think topology, you can pull it,", "tokens": [420, 286, 519, 1192, 1793, 11, 291, 393, 2235, 309, 11], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 322, "seek": 137092, "start": 1375.92, "end": 1378.92, "text": " but various, like I said, spank plugins and so on.", "tokens": [457, 3683, 11, 411, 286, 848, 11, 637, 657, 33759, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 323, "seek": 137092, "start": 1378.92, "end": 1381.92, "text": " So how do you manage this kind of config file", "tokens": [407, 577, 360, 291, 3067, 341, 733, 295, 6662, 3991], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 324, "seek": 137092, "start": 1381.92, "end": 1384.92, "text": " that are not ended by default by Slurm?", "tokens": [300, 366, 406, 4590, 538, 7576, 538, 6187, 26717, 30], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 325, "seek": 137092, "start": 1384.92, "end": 1386.92, "text": " Right, that's correct.", "tokens": [1779, 11, 300, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 326, "seek": 137092, "start": 1386.92, "end": 1388.92, "text": " So when you use the configless feature,", "tokens": [407, 562, 291, 764, 264, 6662, 1832, 4111, 11], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 327, "seek": 137092, "start": 1388.92, "end": 1390.92, "text": " it will download the, you know, the Slurm Conf,", "tokens": [309, 486, 5484, 264, 11, 291, 458, 11, 264, 6187, 26717, 11701, 11], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 328, "seek": 137092, "start": 1390.92, "end": 1392.92, "text": " the C Group Conf, a lot of config files,", "tokens": [264, 383, 10500, 11701, 11, 257, 688, 295, 6662, 7098, 11], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 329, "seek": 137092, "start": 1392.92, "end": 1396.92, "text": " but it will not download your plugins, your plugin files.", "tokens": [457, 309, 486, 406, 5484, 428, 33759, 11, 428, 23407, 7098, 13], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 330, "seek": 137092, "start": 1396.92, "end": 1398.92, "text": " But I think those are usually not needed", "tokens": [583, 286, 519, 729, 366, 2673, 406, 2978], "temperature": 0.0, "avg_logprob": -0.15114632488167198, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0002839886292349547}, {"id": 331, "seek": 139892, "start": 1398.92, "end": 1400.92, "text": " if you're running a client,", "tokens": [498, 291, 434, 2614, 257, 6423, 11], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 332, "seek": 139892, "start": 1400.92, "end": 1402.92, "text": " because those are usually just needed", "tokens": [570, 729, 366, 2673, 445, 2978], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 333, "seek": 139892, "start": 1402.92, "end": 1404.92, "text": " for the Slurm D demons, right?", "tokens": [337, 264, 6187, 26717, 413, 19733, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 334, "seek": 139892, "start": 1404.92, "end": 1406.92, "text": " Even for the worker nodes.", "tokens": [2754, 337, 264, 11346, 13891, 13], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 335, "seek": 139892, "start": 1406.92, "end": 1408.92, "text": " Like the epilogue, the prologue,", "tokens": [1743, 264, 2388, 388, 7213, 11, 264, 447, 4987, 622, 11], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 336, "seek": 139892, "start": 1408.92, "end": 1410.92, "text": " you mean all of those plugin scripts, right?", "tokens": [291, 914, 439, 295, 729, 23407, 23294, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 337, "seek": 139892, "start": 1410.92, "end": 1412.92, "text": " The authentication plugins.", "tokens": [440, 26643, 33759, 13], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 338, "seek": 139892, "start": 1412.92, "end": 1414.92, "text": " Those are usually needed by the Slurm D demon,", "tokens": [3950, 366, 2673, 2978, 538, 264, 6187, 26717, 413, 14283, 11], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 339, "seek": 139892, "start": 1414.92, "end": 1416.92, "text": " but if you're just writing a client,", "tokens": [457, 498, 291, 434, 445, 3579, 257, 6423, 11], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 340, "seek": 139892, "start": 1416.92, "end": 1418.92, "text": " but say you're automating something", "tokens": [457, 584, 291, 434, 3553, 990, 746], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 341, "seek": 139892, "start": 1418.92, "end": 1420.92, "text": " with PySlurm to interact with it,", "tokens": [365, 9953, 50, 75, 26717, 281, 4648, 365, 309, 11], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 342, "seek": 139892, "start": 1420.92, "end": 1422.92, "text": " you don't need those files.", "tokens": [291, 500, 380, 643, 729, 7098, 13], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 343, "seek": 139892, "start": 1422.92, "end": 1424.92, "text": " And Slurm will happily...", "tokens": [400, 6187, 26717, 486, 19909, 485], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 344, "seek": 139892, "start": 1424.92, "end": 1426.92, "text": " You can happily run...", "tokens": [509, 393, 19909, 1190, 485], "temperature": 0.0, "avg_logprob": -0.14267665752466174, "compression_ratio": 1.796875, "no_speech_prob": 4.194836219539866e-05}, {"id": 345, "seek": 142692, "start": 1426.92, "end": 1428.92, "text": " all of those commands without those files.", "tokens": [439, 295, 729, 16901, 1553, 729, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 346, "seek": 142692, "start": 1428.92, "end": 1431.92, "text": " Yeah, okay, so if I just summarize,", "tokens": [865, 11, 1392, 11, 370, 498, 286, 445, 20858, 11], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 347, "seek": 142692, "start": 1431.92, "end": 1433.92, "text": " the idea is just to create some frontend nodes,", "tokens": [264, 1558, 307, 445, 281, 1884, 512, 1868, 521, 13891, 11], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 348, "seek": 142692, "start": 1433.92, "end": 1436.92, "text": " but not really work nodes.", "tokens": [457, 406, 534, 589, 13891, 13], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 349, "seek": 142692, "start": 1436.92, "end": 1438.92, "text": " That's right?", "tokens": [663, 311, 558, 30], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 350, "seek": 142692, "start": 1438.92, "end": 1440.92, "text": " So you...", "tokens": [407, 291, 485], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 351, "seek": 142692, "start": 1440.92, "end": 1445.92, "text": " So if you want to use configless to set up a frontend node,", "tokens": [407, 498, 291, 528, 281, 764, 6662, 1832, 281, 992, 493, 257, 1868, 521, 9984, 11], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 352, "seek": 142692, "start": 1445.92, "end": 1448.92, "text": " you might need those files from somewhere else.", "tokens": [291, 1062, 643, 729, 7098, 490, 4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 353, "seek": 142692, "start": 1448.92, "end": 1451.92, "text": " But if you're just creating a container", "tokens": [583, 498, 291, 434, 445, 4084, 257, 10129], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 354, "seek": 142692, "start": 1451.92, "end": 1453.92, "text": " to just interact with Slurm and send Slurm commands,", "tokens": [281, 445, 4648, 365, 6187, 26717, 293, 2845, 6187, 26717, 16901, 11], "temperature": 0.0, "avg_logprob": -0.1201676009991847, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00044398003956303}, {"id": 355, "seek": 145392, "start": 1453.92, "end": 1456.92, "text": " you don't need them, basically.", "tokens": [291, 500, 380, 643, 552, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 356, "seek": 145392, "start": 1456.92, "end": 1459.92, "text": " Because the plugin files are usually the...", "tokens": [1436, 264, 23407, 7098, 366, 2673, 264, 485], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 357, "seek": 145392, "start": 1459.92, "end": 1462.92, "text": " Yeah, the epilogue prologue for the Slurm D", "tokens": [865, 11, 264, 2388, 388, 7213, 447, 4987, 622, 337, 264, 6187, 26717, 413], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 358, "seek": 145392, "start": 1462.92, "end": 1464.92, "text": " or the Slurm CTLD.", "tokens": [420, 264, 6187, 26717, 19529, 23704, 13], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 359, "seek": 145392, "start": 1464.92, "end": 1471.92, "text": " And that's not what these Slurm client containers are about.", "tokens": [400, 300, 311, 406, 437, 613, 6187, 26717, 6423, 17089, 366, 466, 13], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 360, "seek": 145392, "start": 1471.92, "end": 1474.92, "text": " So short answer, you usually don't need them.", "tokens": [407, 2099, 1867, 11, 291, 2673, 500, 380, 643, 552, 13], "temperature": 0.0, "avg_logprob": -0.10567899703979493, "compression_ratio": 1.53125, "no_speech_prob": 0.0010559124639257789}, {"id": 361, "seek": 147492, "start": 1474.92, "end": 1476.92, "text": " Hello, thank you for the talk.", "tokens": [2425, 11, 1309, 291, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 362, "seek": 147492, "start": 1476.92, "end": 1479.92, "text": " I'm wondering, in huge institutions,", "tokens": [286, 478, 6359, 11, 294, 2603, 8142, 11], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 363, "seek": 147492, "start": 1479.92, "end": 1481.92, "text": " like in CERN or EPFL,", "tokens": [411, 294, 383, 1598, 45, 420, 25330, 31455, 11], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 364, "seek": 147492, "start": 1481.92, "end": 1494.92, "text": " would you run your own forked or patched Slurm", "tokens": [576, 291, 1190, 428, 1065, 17716, 292, 420, 9972, 292, 6187, 26717], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 365, "seek": 147492, "start": 1494.92, "end": 1499.92, "text": " so you could fix maybe the authentication privileges?", "tokens": [370, 291, 727, 3191, 1310, 264, 26643, 32588, 30], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 366, "seek": 147492, "start": 1499.92, "end": 1502.92, "text": " Or is it just not done because it's...", "tokens": [1610, 307, 309, 445, 406, 1096, 570, 309, 311, 485], "temperature": 0.0, "avg_logprob": -0.20420333317347936, "compression_ratio": 1.3391812865497077, "no_speech_prob": 0.004350181668996811}, {"id": 367, "seek": 150292, "start": 1502.92, "end": 1505.92, "text": " I've never carried any Slurm patches, to be honest.", "tokens": [286, 600, 1128, 9094, 604, 6187, 26717, 26531, 11, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 368, "seek": 150292, "start": 1505.92, "end": 1508.92, "text": " I've always, both at Slurm and at EPFL,", "tokens": [286, 600, 1009, 11, 1293, 412, 6187, 26717, 293, 412, 25330, 31455, 11], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 369, "seek": 150292, "start": 1508.92, "end": 1510.92, "text": " we just use Slurm out of the box.", "tokens": [321, 445, 764, 6187, 26717, 484, 295, 264, 2424, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 370, "seek": 150292, "start": 1510.92, "end": 1513.92, "text": " It works well enough for our use cases.", "tokens": [467, 1985, 731, 1547, 337, 527, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 371, "seek": 150292, "start": 1513.92, "end": 1516.92, "text": " It is true that you could, for instance, do a patch", "tokens": [467, 307, 2074, 300, 291, 727, 11, 337, 5197, 11, 360, 257, 9972], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 372, "seek": 150292, "start": 1516.92, "end": 1521.92, "text": " to enable finer granularity for the permissions.", "tokens": [281, 9528, 39130, 39962, 507, 337, 264, 32723, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 373, "seek": 150292, "start": 1521.92, "end": 1524.92, "text": " For instance, you could enable any user to pull the config file.", "tokens": [1171, 5197, 11, 291, 727, 9528, 604, 4195, 281, 2235, 264, 6662, 3991, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 374, "seek": 150292, "start": 1524.92, "end": 1526.92, "text": " That would be a nice patch.", "tokens": [663, 576, 312, 257, 1481, 9972, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 375, "seek": 150292, "start": 1526.92, "end": 1528.92, "text": " We don't do it.", "tokens": [492, 500, 380, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 376, "seek": 150292, "start": 1528.92, "end": 1531.92, "text": " Okay, thank you.", "tokens": [1033, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.07231122797185724, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0007128845900297165}, {"id": 377, "seek": 153192, "start": 1531.92, "end": 1534.92, "text": " We have time for one short question.", "tokens": [492, 362, 565, 337, 472, 2099, 1168, 13], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 378, "seek": 153192, "start": 1534.92, "end": 1535.92, "text": " Hi, thanks.", "tokens": [2421, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 379, "seek": 153192, "start": 1535.92, "end": 1537.92, "text": " We actually are very interested in this", "tokens": [492, 767, 366, 588, 3102, 294, 341], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 380, "seek": 153192, "start": 1537.92, "end": 1541.92, "text": " because we are applying...", "tokens": [570, 321, 366, 9275, 485], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 381, "seek": 153192, "start": 1541.92, "end": 1543.92, "text": " We have a Jupyter Hub frontend", "tokens": [492, 362, 257, 22125, 88, 391, 18986, 1868, 521], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 382, "seek": 153192, "start": 1543.92, "end": 1547.92, "text": " that actually talks to a Slurm cluster through SSH", "tokens": [300, 767, 6686, 281, 257, 6187, 26717, 13630, 807, 12238, 39], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 383, "seek": 153192, "start": 1547.92, "end": 1549.92, "text": " because we don't want to install all that stuff,", "tokens": [570, 321, 500, 380, 528, 281, 3625, 439, 300, 1507, 11], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 384, "seek": 153192, "start": 1549.92, "end": 1552.92, "text": " like the munch and the full Slurm deployment", "tokens": [411, 264, 275, 1680, 293, 264, 1577, 6187, 26717, 19317], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 385, "seek": 153192, "start": 1552.92, "end": 1554.92, "text": " into the Jupyter Hub host.", "tokens": [666, 264, 22125, 88, 391, 18986, 3975, 13], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 386, "seek": 153192, "start": 1554.92, "end": 1557.92, "text": " And I'm wondering, how does it talk actually to Slurm control?", "tokens": [400, 286, 478, 6359, 11, 577, 775, 309, 751, 767, 281, 6187, 26717, 1969, 30], "temperature": 0.0, "avg_logprob": -0.15329435521906074, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.0027719095814973116}, {"id": 387, "seek": 155792, "start": 1557.92, "end": 1562.92, "text": " So is the Slurm control always listening to any...", "tokens": [407, 307, 264, 6187, 26717, 1969, 1009, 4764, 281, 604, 485], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 388, "seek": 155792, "start": 1562.92, "end": 1565.92, "text": " any of the hosts that will talk to it?", "tokens": [604, 295, 264, 21573, 300, 486, 751, 281, 309, 30], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 389, "seek": 155792, "start": 1565.92, "end": 1566.92, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 390, "seek": 155792, "start": 1566.92, "end": 1568.92, "text": " Or is there any restrictions to who is connecting", "tokens": [1610, 307, 456, 604, 14191, 281, 567, 307, 11015], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 391, "seek": 155792, "start": 1568.92, "end": 1570.92, "text": " to the Slurm control demo?", "tokens": [281, 264, 6187, 26717, 1969, 10723, 30], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 392, "seek": 155792, "start": 1570.92, "end": 1575.92, "text": " So there's an alloc nodes setting in the SlurmConf, I believe,", "tokens": [407, 456, 311, 364, 12660, 13891, 3287, 294, 264, 6187, 26717, 43874, 11, 286, 1697, 11], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 393, "seek": 155792, "start": 1575.92, "end": 1579.92, "text": " which will allow you to restrict from which nodes", "tokens": [597, 486, 2089, 291, 281, 7694, 490, 597, 13891], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 394, "seek": 155792, "start": 1579.92, "end": 1581.92, "text": " you can allocate resources.", "tokens": [291, 393, 35713, 3593, 13], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 395, "seek": 155792, "start": 1581.92, "end": 1582.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 396, "seek": 155792, "start": 1582.92, "end": 1584.92, "text": " So you can limit it.", "tokens": [407, 291, 393, 4948, 309, 13], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 397, "seek": 155792, "start": 1584.92, "end": 1586.92, "text": " However, if you don't have that,", "tokens": [2908, 11, 498, 291, 500, 380, 362, 300, 11], "temperature": 0.0, "avg_logprob": -0.18068927418101918, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00045887206215411425}, {"id": 398, "seek": 158692, "start": 1586.92, "end": 1589.92, "text": " the Slurm will happily accept anything", "tokens": [264, 6187, 26717, 486, 19909, 3241, 1340], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 399, "seek": 158692, "start": 1589.92, "end": 1591.92, "text": " because if you have the shared secret,", "tokens": [570, 498, 291, 362, 264, 5507, 4054, 11], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 400, "seek": 158692, "start": 1591.92, "end": 1593.92, "text": " it's considered good enough.", "tokens": [309, 311, 4888, 665, 1547, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 401, "seek": 158692, "start": 1593.92, "end": 1594.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 402, "seek": 158692, "start": 1594.92, "end": 1595.92, "text": " Or a valid JSON web token.", "tokens": [1610, 257, 7363, 31828, 3670, 14862, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 403, "seek": 158692, "start": 1595.92, "end": 1596.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 404, "seek": 158692, "start": 1596.92, "end": 1597.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 405, "seek": 158692, "start": 1597.92, "end": 1598.92, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 406, "seek": 158692, "start": 1598.92, "end": 1599.92, "text": " Thank you very much, Pablo.", "tokens": [1044, 291, 588, 709, 11, 31554, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 407, "seek": 158692, "start": 1599.92, "end": 1600.92, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.11899235669304342, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.00024168497475329787}, {"id": 408, "seek": 160092, "start": 1600.92, "end": 1617.92, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 51214], "temperature": 0.0, "avg_logprob": -0.6531133651733398, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.0003886721679009497}], "language": "en"}