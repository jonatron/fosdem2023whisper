{"text": " This is the largest room I've ever given a presentation to, so thank you for showing up. Also, let's see. How many of you are using logging right now in your job? Wow. Okay, that's a lot of you. This is the wrong talk. No, I'm just kidding. How many of you would say you're doing Python logging well? Yeah, that's what I thought. Well, it's okay. You're in the right conversation because we're going to tell you all about how you get started with Python logging and all the wonderful things that, hold on, that's my boss, hold on. Hey, yeah, I'm in the middle of a presentation right now. What's up? He deployed on a Saturday? Are you kidding me? What do you mean, production's broken? Yeah, tell me you check the logs. He's not logging. Oh my goodness. Well, please don't fire him. Tell him to tune into my talk. We're going to go over the basics of logging and then we're going to get all the way down into some more advanced logging configurations. Okay? Yeah. Ciao. So, nobody wants to be in that situation. Honestly, that went over way better than I thought it would. Nobody wants to be in that situation. So before we get too deep though, I'm going to just say hi. I'm David. I'm a developer advocate for OpenSearch. Thanks to the OpenSearch group for letting me come here. OpenSearch is commonly used as a log store, so it fits in nice with Python logging. Previous to this, I'd worked as a data engineer, network automation engineer, DevOps engineer. I've done a lot of engineering and all of them had one thing in common and it was I needed a lot of logs to really understand what was going on at any point in my application. So you might want to get your phones out at this point. I have lots of QR codes. Just kidding. There are only three. And we're going to go to code right after this one. So that's my link tree if you want to connect with me and hear more about OpenSearch because I'll talk your ear off, I promise. So with that, you can follow along. There's a gist for this whole presentation. It can all run for the most part. I was working on it last night till very late. And we're going to go ahead and get started. So as with anything, we're going to import logging off to the races. And now you can check off your JIRA card and put it on the backlog. Just kidding. We all know that logging is a lot more than just importing logging. So we're going to start with the logger. Logger is kind of the core concept of Python logging and all loggers start with a name. So we're going to do underscore underscore name. Does anyone know what underscore underscore name actually will resolve to in this instance? Any guesses? It's underscore underscore main. Come on, guys. So we create a logger. We give it a name. And fun fact of the day, that logger is global. I can import Python logging and do get logger, of course, from anywhere and use this logger. As it's too small. Yes. There we go. Try not to delete code while I'm at it. There we go. We're going to make this smaller. We're going to make this up here. So as we know with logging, there are several different levels you can set. These levels are a filtering mechanism for Python logging. Fun fact of the day, though, most people don't know this, is these logger or these levels actually resolve two numbers. And those numbers are used to do filtering. And fun fact of the day, you can actually make your own log levels as if five, six, six log levels, seven log levels isn't enough. You could add more potentially if you needed that. So we're going to go ahead and send this to terminal. And we're going to take a look at all those log levels. Here we are. As we can see, they resolve to 10, 20, 30, 30, 30, 30, 40, and 50. So warn and warning both resolve to the same log level. So they're both there in case you need two log levels that have the same level. And these levels, again, are used for filtering. So as you are sending out logs, they're used to filter out logs that maybe are less important. So the number, the lower the importance of the log. So let's talk about emitting a log. Here we're going to go ahead and send a log with logger.info. So that's going to emit a log from the logger with the info level attached to it. Upper case info is for sending logs, upper case info is the level. And nothing actually happens. It just gets sent to the terminal and nothing happens. And the reason why nothing happens is because we've not told it where it needs to go. We need log handlers. So that is what we do. We're going to build a simple syslog handler for, sorry, streaming handler for sending to standard out now. So handlers receive logs from the logger. It's pretty straightforward. I'm going to go ahead and create this real quick. We're going to set the level of our handler. Remember our levels are our wonderful filtering mechanism. Set the level to warning. And the top one will not show up, but the bottom one should because it is greater than warning. And there it is. This will. That's pretty dull. Nobody really wants a log that says this will. That tells us nothing. You have no clue what's going on in your application. What time did this will? What time will it won't? So we need to add some context. And the way we get context is, of course, with log formatters. And I've jumped ahead too far in my presentation. We're just going to take a quick back step real quick. We're going to talk about some of the other handlers that are built in. So there's the rotating file handle. This one's particularly useful if you have too many logs and your file gets too large. It can actually automatically rotate that file every X amount of size into a new file. And then you can specify for it to delete logs after a certain amount of time or a certain amount of files. We have the syslog handler. Syslog is a standard for logging. HTTP lets you send logs to arbitrary HTTP endpoints. Time rotating file handler, believe it or not, is a timed rotation of your log files. Whether it be every day, every minute, or please God forbid every second, do not do that. You will end up with thousands of files. I did actually every hour once, and that was a huge mistake. I ended up with like 15,000 files after a little bit. And the SMTP handler, if you are masochistic. Now we'll talk about formatters. OK. So let's go ahead. We're going to create, set our console handle to the info level. We're going to create a formatter. The formatter is going to include the date and time, the name of the logger. Which again, if you're using underscore underscore name, that's going to be the name of your module, whether that's like search dot util dot whatever. And then the level name that it was admitted at, and a message. Pretty straightforward. Then we set the formatter. Formatter gets attached to handlers again. Logger dot info. Look at my pretty log. Well, just kidding. There's no log there. Is it because I have a bug? Because I wrote this way too late at night? Probably. Or more likely because there was something I actually tricked you on earlier. The truth is, the reason our first log wasn't admitted wasn't because there wasn't a handler attached to it, but it's because Python's logging library by default sets loggers and handlers to the warning level right out of the gate, and we admitted at the warning level. So fun fact, you actually need to set the level for both the logger and your handlers. So we'll go ahead and do that. We'll set the logger level, and we're off to the races. Let's look at this pretty log. Man, that is so beautiful. Now I know exactly when I had a pretty log. I know exactly where it happened in the main. I know what level it was at. It was informational log. So we've talked about a lot so far. We've talked about loggers, handlers. So let's do a top to bottom just real quick to make sure we're all talking about the same thing. So we have loggers. Loggers emit a log at some level. They also filter out logs at some level. So if your logger is set to filter out warning logs and you send it an info, it's never going to get to the handler. Handlers receive logs and then send them to some specified output, whatever that may be. And then formatters attach to handlers, and they enrich the output. So they add context. And there's a bunch of other contexts. That's not mentioned here. It's in a Python logging library. I guess it's like at a certain time in the presentation, it just decides it wants to do that. So these are all wonderful. Wait a minute. I'm getting there. Logging. Logging. Yes. All right. Here we go. We're just going to scroll on. Oh, yes. Here we go. So setting up logs from, you know, in each individual module is a pain. So you can actually pre-create loggers ahead of time with a dictionary config or a YAML config file because I know all of us just love creating YAML configs and having them everywhere. So with this, you can create as many logs as you want and specify them with dictionary config. One other really important thing to mention, and I probably should have hit it when I was talking about loggers at the get-go, there's a specific reason why you can set it at the logger level and at the handler level. So handlers give you very fine-tune access over what you're looking at. So where it's going, which output, you know. So a lot of times people will specify certain levels for handlers and then they will use their global debug level to set their loggers. So say, for example, you're debugging an application locally, you're going to set that to debug, of course, but when you push it to production, you don't want that. So a lot of people will have, you know, a production logging config and a development environment logging config. So with that, there is actually one other slight challenge with loggers, and that is they're blocking operations. So like I said earlier, if you're a masochist and you like the SMTP log handler, you could be in a real pinch. So say, for example, I'm on your application, you have this nice web server, and all of a sudden it hits a critical error, it sends a message to your SMTP server, and your SMTP server is slow, it's chugging, so you're taking five to ten seconds for it to register that and send a response. Do you think I'm going to stay on your web page for five to ten seconds while it sends an error log? Heck, no, I'm closing out and I'm going to somewhere else, I don't know, amazon.com to buy whatever I needed. So we have to handle this. We have to understand that, hey, this could potentially block, so how do we unblock our applications? Well, it's by making our applications simpler, obviously, and using multi-threading. That was a joke, you can laugh. So with this, we can actually import queues, and what happens is there's a queue handler and a queue listener. So the queue is the shared memory space that can be accessed both the handler and the listener. You create the handler, the handler receives all the logs, and then distributes them to the queue. The queue listener starts up on its own independent thread, and it's going to listen to the log queue, and then distribute it to any of the handlers that you specified it should. So let's go through that end to end again. We've got queue handlers, receive the logs, place them on a queue, the queue hands it over to the queue listener, which then hands it on to your other loggers. Now your application is unblocked. It drops that queue, or that log on the queue, and then it's off to the races. You can use SMTPlib if you really wanted to. So let's talk about pulling this all together now, right? So we have these logs, they sit on our local machines, and that's fine, but if you are a large organization, you might have hundreds of servers. Take a second to breathe. You might have hundreds of servers, hundreds of network devices or whatever, and I'll give a real example of when I could have used this. So when I was a network automation engineer, we had a particular log that my boss found on some of the servers, or router switches, et cetera, and I spent the next three hours logging into each individual one. We had thousands of network devices. So I logged into enough of them, wrote down the logs, and then correlated, and I said, oh, look, every Thursday and Saturday at the exact same time, this log happens. One email later finds out security team is pen testing against us, and we don't need to worry about it. Again, three hours, just trying to correlate what log was happening where, when. So this is exactly what you can avoid by using something like OpenSearch, ElasticSearch, Loki to aggregate your logs. And again, if you do want to follow along with this later, this just is a Docker compose file that will let you spin up some sample containers with OpenSearch. So we'll import logging in our OpenSearch library, create an OpenSearch client, and this is where I'm going to break for just a moment and talk about custom handlers. So we mentioned handlers are where you send your logs. You can implement custom handlers, believe it or not. All you need to do is, I was going to say, inherit from, there we go, that's probably the right word, logging.handler. Then you create and emit definition, and that needs to have self and record, and that will send the record wherever you specify. So in our case, we have, it's going to take, and it's going to format created time. Also I did not implement the formatting library, because I wanted to send it as a dictionary to OpenSearch just because that's what it works with. You can also use something like FluentDit, FluentBit, or FluentDlogStash to parse out your logs later, and we'll talk about that just briefly. So we've got our created time. We've created this wonderful record, OpenSearchClient.index, we'll send that log to OpenSearch. There we go. And then we'll set it up. So we're going to create our logger named log. We're going to set the logger's level to info so that we get the info records. We're going to create the OpenSearchHandler and add logging.info, set that level, and add our handler. And we're off to the races. Boom. Well, that was kind of anticlimactic. You can't actually see it going into OpenSearch. But I promise you, it chugged along and it went into OpenSearch and, ha-za, let's go into OpenSearch. And this is actually OpenSearch dashboards. So again, if you do this on your local machine, passwords admin, usernames admin. Very secure. We're actually looking to change that, but that is coming soon. So we go here and we're going to go into stack management, hang with me for just a second. So we created a custom index with this, and that index is named based off of the logger that sent it. So we've got logger name, which was log, and then with the date time so that we can roll those off after a certain amount of days. We're going to create an index pattern. And we are going to say, we're going to say absolutely nothing because this is the problem with doing things like, oh, just kidding, I just don't know how to use OpenSearch. Here we go. It's only my job. Don't worry. Boss, I swear, if you're watching this, no, I'm just kidding. So there we go. So we have these two indexes that have been created because I was testing yesterday and today. So logstar just says, hey, any index that looks like log with anything after it, group them together. Ask for a time field, which is our created field, and create index pattern. There we go. OpenSearch, auto-detects, all of these different files, types, et cetera. You can actually specify mappings, and I actually really recommend that because different visualizations require different types of mappings, but that is not what we're talking about today. We're talking about Python logs, darn it. So we're going to go over to Discover and go into our logs, and we are going to make sure we're looking at today's logs. Actually, well, let's look at this week. Here we go. And we have three hits. There we are. These are our logs. Look ma, I'm logging. And you could actually see the exact point in time when I switch from this is my log to look ma, I'm logging. So that was, yeah, I had 2.37 p.m., so anyhow, with this, you can actually go ahead and then visualize spikes, peaks, valleys, when this is happening. You can enrich it with device information. So you can say, hey, when was this log sent? Is there a particular device that's having a lot of issues? So now instead of logging into all of your servers, you can go and bounce the correct ones. And if you saw Doton's presentation yesterday, you'll know you can use this for monitoring anything, whether it's CI CD pipelines, Python logs, network logs, et cetera. So let's see. Look at that. You're doing more logging now than 99% of the population. So congratulations. Clap your hands for yourselves. So I'm going to talk real quick about just a very simple, common logging architecture for capturing distributed logs and why you would want to do that. So more often than not, you're probably actually going to want to log your, you don't want to put your logs locally on the file system. And the reason why is because your file system is not going to go down, and God forbid if it goes down, there's no hope for that log getting out anyways. Your service remotely could disconnect because whether you're doing an upgrade or something along those lines, so it acts as a little bit of a caching mechanism. And then you'll normally have it logged to a file, and then you could use something like Fluent Bit or Beats, and those will ship your logs out. And the wonderful thing about this architecture is, again, if OpenSearch was to go down because you're doing an update, or if something was to happen critical with your Python app, it can quickly write that log out, and you can also do enrichment. So I talked about getting which server sent that log. So Fluent D, Data Prepper, and Log Stash all can take and enrich your logs with the context information that came with them. So say, for example, it says, I received this log from X, Y, and Z server, 10, 20, 90, 32, 83, or something like that. Then it can go and do a reverse DNS lookup and say, hey, who has that? Which service is assigned to that? And then it can add that information in and then push it into OpenSearch. So now you've all of a sudden gone from having a log that says, hello world, to or got here, please, please, do not. And you can have it pushed into OpenSearch, know what service is causing the issues, and visualize on dashboards. With that, I'm finished. Please scan, look at OpenSearch if you're curious. It is open source, Apache 2 licensed. All of our features are being developed in the open. And with that, I want to ask it, does anyone have any questions? Yeah. Thank you. Hello? Okay. How would you... I'm mostly familiar with Sentry, and I'm very curious how would you compare this to that, because as far as my friends told me, they're pretty different products, but they do similar stuff. Yeah. So Sentry is more of an APM, isn't it? Is that word familiar? Okay. So OpenSearch has some APM capabilities, which is, I don't remember the word for it, but it's for app monitoring and specific to the application. So this has some APM capabilities. It might not go as deep as some of your auto configurations for other APM tools though, but it can ingest APM logs. So that's a good question. Thank you. You had two async questions from the chats. The first one is, what about f-strings and logging? Because I wrote this presentation in like a couple hours, and yeah. So I'm trying to modernize, but again, it's old habits die hard, so I'm still using f-strings. Please don't get on me. It was the next one, sorry. Yeah. The question is, what about structured logging, and in particular, strike log? I'm not actually familiar with strut log. So I'm actually moving myself more towards using OpenTelemetry instead of logging, or alongside logging, we'll say. So OpenTelemetry gives you a trace, actually, which can tell you the full stack of what happened during your application. So everything down from which function was called to which load balancer sent the information over. So you get an end-to-end trace of what happened, which in my opinion, I think, is a little bit more handy than just logs. I think we have one more question in the middle. Just a quick question from the discussion I'm having with the code worker. So we're thinking about moving our logs to JSON format, because it's easy to understand for non-Python people and searchable. If we were to switch to OpenSearch, and I really liked the presentation, do you think it's still feasible to make logs searchable in and of itself, or is OpenSearch that's stable and usable and from your experience, there's no need to search on your logs? Yeah. No, that's a great question. You do need to search your logs. OpenSearch is a search engine at its core, and that is why it is as good as it is with logs. As for JSON versus other formats, I think there's no particular preference, but OpenSearch is certainly stable. We have 150 million downloads, so we are here to stay. It's been adopted by a lot of companies such as Oracle, Ivan, Instacluster, Opster, Amazon Web Services, of course, because I work there, and many others. So I would say it is very stable, production-ready to use, and yeah, it's a really great way to search your logs. In fact, I have a lightning talk at 1655 in another room, so I think it's a Kubernetes room. So if you want to talk more about searching your logs, I'm going to be talking about search relevance for logs. So thank you. Thank you, online people. Thanks a lot.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " This is the largest room I've ever given a presentation to, so thank you for showing", "tokens": [639, 307, 264, 6443, 1808, 286, 600, 1562, 2212, 257, 5860, 281, 11, 370, 1309, 291, 337, 4099], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 1, "seek": 0, "start": 10.0, "end": 11.0, "text": " up.", "tokens": [493, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 2, "seek": 0, "start": 11.0, "end": 14.200000000000001, "text": " Also, let's see.", "tokens": [2743, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 3, "seek": 0, "start": 14.200000000000001, "end": 17.72, "text": " How many of you are using logging right now in your job?", "tokens": [1012, 867, 295, 291, 366, 1228, 27991, 558, 586, 294, 428, 1691, 30], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 4, "seek": 0, "start": 17.72, "end": 18.72, "text": " Wow.", "tokens": [3153, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 5, "seek": 0, "start": 18.72, "end": 20.96, "text": " Okay, that's a lot of you.", "tokens": [1033, 11, 300, 311, 257, 688, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 6, "seek": 0, "start": 20.96, "end": 21.96, "text": " This is the wrong talk.", "tokens": [639, 307, 264, 2085, 751, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 7, "seek": 0, "start": 21.96, "end": 23.8, "text": " No, I'm just kidding.", "tokens": [883, 11, 286, 478, 445, 9287, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 8, "seek": 0, "start": 23.8, "end": 27.2, "text": " How many of you would say you're doing Python logging well?", "tokens": [1012, 867, 295, 291, 576, 584, 291, 434, 884, 15329, 27991, 731, 30], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 9, "seek": 0, "start": 27.2, "end": 29.88, "text": " Yeah, that's what I thought.", "tokens": [865, 11, 300, 311, 437, 286, 1194, 13], "temperature": 0.0, "avg_logprob": -0.2347372702832492, "compression_ratio": 1.5231481481481481, "no_speech_prob": 0.18138059973716736}, {"id": 10, "seek": 2988, "start": 29.88, "end": 30.88, "text": " Well, it's okay.", "tokens": [1042, 11, 309, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 11, "seek": 2988, "start": 30.88, "end": 34.4, "text": " You're in the right conversation because we're going to tell you all about how you get started", "tokens": [509, 434, 294, 264, 558, 3761, 570, 321, 434, 516, 281, 980, 291, 439, 466, 577, 291, 483, 1409], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 12, "seek": 2988, "start": 34.4, "end": 41.32, "text": " with Python logging and all the wonderful things that, hold on, that's my boss, hold", "tokens": [365, 15329, 27991, 293, 439, 264, 3715, 721, 300, 11, 1797, 322, 11, 300, 311, 452, 5741, 11, 1797], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 13, "seek": 2988, "start": 41.32, "end": 42.32, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 14, "seek": 2988, "start": 42.32, "end": 45.44, "text": " Hey, yeah, I'm in the middle of a presentation right now.", "tokens": [1911, 11, 1338, 11, 286, 478, 294, 264, 2808, 295, 257, 5860, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 15, "seek": 2988, "start": 45.44, "end": 47.480000000000004, "text": " What's up?", "tokens": [708, 311, 493, 30], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 16, "seek": 2988, "start": 47.480000000000004, "end": 49.32, "text": " He deployed on a Saturday?", "tokens": [634, 17826, 322, 257, 8803, 30], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 17, "seek": 2988, "start": 49.32, "end": 50.32, "text": " Are you kidding me?", "tokens": [2014, 291, 9287, 385, 30], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 18, "seek": 2988, "start": 50.32, "end": 52.72, "text": " What do you mean, production's broken?", "tokens": [708, 360, 291, 914, 11, 4265, 311, 5463, 30], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 19, "seek": 2988, "start": 52.72, "end": 56.4, "text": " Yeah, tell me you check the logs.", "tokens": [865, 11, 980, 385, 291, 1520, 264, 20820, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 20, "seek": 2988, "start": 56.4, "end": 57.4, "text": " He's not logging.", "tokens": [634, 311, 406, 27991, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 21, "seek": 2988, "start": 57.4, "end": 58.4, "text": " Oh my goodness.", "tokens": [876, 452, 8387, 13], "temperature": 0.0, "avg_logprob": -0.2509897512982982, "compression_ratio": 1.5962264150943397, "no_speech_prob": 2.707981548155658e-05}, {"id": 22, "seek": 5840, "start": 58.4, "end": 61.0, "text": " Well, please don't fire him.", "tokens": [1042, 11, 1767, 500, 380, 2610, 796, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 23, "seek": 5840, "start": 61.0, "end": 62.92, "text": " Tell him to tune into my talk.", "tokens": [5115, 796, 281, 10864, 666, 452, 751, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 24, "seek": 5840, "start": 62.92, "end": 66.24, "text": " We're going to go over the basics of logging and then we're going to get all the way down", "tokens": [492, 434, 516, 281, 352, 670, 264, 14688, 295, 27991, 293, 550, 321, 434, 516, 281, 483, 439, 264, 636, 760], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 25, "seek": 5840, "start": 66.24, "end": 68.64, "text": " into some more advanced logging configurations.", "tokens": [666, 512, 544, 7339, 27991, 31493, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 26, "seek": 5840, "start": 68.64, "end": 69.64, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 27, "seek": 5840, "start": 69.64, "end": 70.64, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 28, "seek": 5840, "start": 70.64, "end": 71.64, "text": " Ciao.", "tokens": [28473, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 29, "seek": 5840, "start": 71.64, "end": 75.28, "text": " So, nobody wants to be in that situation.", "tokens": [407, 11, 5079, 2738, 281, 312, 294, 300, 2590, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 30, "seek": 5840, "start": 75.28, "end": 82.6, "text": " Honestly, that went over way better than I thought it would.", "tokens": [12348, 11, 300, 1437, 670, 636, 1101, 813, 286, 1194, 309, 576, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 31, "seek": 5840, "start": 82.6, "end": 84.0, "text": " Nobody wants to be in that situation.", "tokens": [9297, 2738, 281, 312, 294, 300, 2590, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 32, "seek": 5840, "start": 84.0, "end": 86.36, "text": " So before we get too deep though, I'm going to just say hi.", "tokens": [407, 949, 321, 483, 886, 2452, 1673, 11, 286, 478, 516, 281, 445, 584, 4879, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 33, "seek": 5840, "start": 86.36, "end": 87.36, "text": " I'm David.", "tokens": [286, 478, 4389, 13], "temperature": 0.0, "avg_logprob": -0.1824703967477393, "compression_ratio": 1.701195219123506, "no_speech_prob": 1.643008909013588e-05}, {"id": 34, "seek": 8736, "start": 87.36, "end": 89.96, "text": " I'm a developer advocate for OpenSearch.", "tokens": [286, 478, 257, 10754, 14608, 337, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 35, "seek": 8736, "start": 89.96, "end": 92.76, "text": " Thanks to the OpenSearch group for letting me come here.", "tokens": [2561, 281, 264, 7238, 10637, 1178, 1594, 337, 8295, 385, 808, 510, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 36, "seek": 8736, "start": 92.76, "end": 99.44, "text": " OpenSearch is commonly used as a log store, so it fits in nice with Python logging.", "tokens": [7238, 10637, 1178, 307, 12719, 1143, 382, 257, 3565, 3531, 11, 370, 309, 9001, 294, 1481, 365, 15329, 27991, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 37, "seek": 8736, "start": 99.44, "end": 104.2, "text": " Previous to this, I'd worked as a data engineer, network automation engineer, DevOps engineer.", "tokens": [6001, 1502, 281, 341, 11, 286, 1116, 2732, 382, 257, 1412, 11403, 11, 3209, 17769, 11403, 11, 43051, 11403, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 38, "seek": 8736, "start": 104.2, "end": 108.4, "text": " I've done a lot of engineering and all of them had one thing in common and it was I", "tokens": [286, 600, 1096, 257, 688, 295, 7043, 293, 439, 295, 552, 632, 472, 551, 294, 2689, 293, 309, 390, 286], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 39, "seek": 8736, "start": 108.4, "end": 113.36, "text": " needed a lot of logs to really understand what was going on at any point in my application.", "tokens": [2978, 257, 688, 295, 20820, 281, 534, 1223, 437, 390, 516, 322, 412, 604, 935, 294, 452, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 40, "seek": 8736, "start": 113.36, "end": 115.72, "text": " So you might want to get your phones out at this point.", "tokens": [407, 291, 1062, 528, 281, 483, 428, 10216, 484, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.1829363990376014, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.4280718460213393e-05}, {"id": 41, "seek": 11572, "start": 115.72, "end": 117.28, "text": " I have lots of QR codes.", "tokens": [286, 362, 3195, 295, 32784, 14211, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 42, "seek": 11572, "start": 117.28, "end": 118.28, "text": " Just kidding.", "tokens": [1449, 9287, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 43, "seek": 11572, "start": 118.28, "end": 119.28, "text": " There are only three.", "tokens": [821, 366, 787, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 44, "seek": 11572, "start": 119.28, "end": 121.0, "text": " And we're going to go to code right after this one.", "tokens": [400, 321, 434, 516, 281, 352, 281, 3089, 558, 934, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 45, "seek": 11572, "start": 121.0, "end": 125.24, "text": " So that's my link tree if you want to connect with me and hear more about OpenSearch because", "tokens": [407, 300, 311, 452, 2113, 4230, 498, 291, 528, 281, 1745, 365, 385, 293, 1568, 544, 466, 7238, 10637, 1178, 570], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 46, "seek": 11572, "start": 125.24, "end": 127.72, "text": " I'll talk your ear off, I promise.", "tokens": [286, 603, 751, 428, 1273, 766, 11, 286, 6228, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 47, "seek": 11572, "start": 127.72, "end": 131.0, "text": " So with that, you can follow along.", "tokens": [407, 365, 300, 11, 291, 393, 1524, 2051, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 48, "seek": 11572, "start": 131.0, "end": 132.92, "text": " There's a gist for this whole presentation.", "tokens": [821, 311, 257, 290, 468, 337, 341, 1379, 5860, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 49, "seek": 11572, "start": 132.92, "end": 134.72, "text": " It can all run for the most part.", "tokens": [467, 393, 439, 1190, 337, 264, 881, 644, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 50, "seek": 11572, "start": 134.72, "end": 138.64, "text": " I was working on it last night till very late.", "tokens": [286, 390, 1364, 322, 309, 1036, 1818, 4288, 588, 3469, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 51, "seek": 11572, "start": 138.64, "end": 140.28, "text": " And we're going to go ahead and get started.", "tokens": [400, 321, 434, 516, 281, 352, 2286, 293, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.1580499061068198, "compression_ratio": 1.6457564575645756, "no_speech_prob": 1.0609684977680445e-05}, {"id": 52, "seek": 14028, "start": 140.28, "end": 146.12, "text": " So as with anything, we're going to import logging off to the races.", "tokens": [407, 382, 365, 1340, 11, 321, 434, 516, 281, 974, 27991, 766, 281, 264, 15484, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 53, "seek": 14028, "start": 146.12, "end": 150.76, "text": " And now you can check off your JIRA card and put it on the backlog.", "tokens": [400, 586, 291, 393, 1520, 766, 428, 50172, 3750, 2920, 293, 829, 309, 322, 264, 47364, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 54, "seek": 14028, "start": 150.76, "end": 151.76, "text": " Just kidding.", "tokens": [1449, 9287, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 55, "seek": 14028, "start": 151.76, "end": 154.2, "text": " We all know that logging is a lot more than just importing logging.", "tokens": [492, 439, 458, 300, 27991, 307, 257, 688, 544, 813, 445, 43866, 27991, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 56, "seek": 14028, "start": 154.2, "end": 156.44, "text": " So we're going to start with the logger.", "tokens": [407, 321, 434, 516, 281, 722, 365, 264, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 57, "seek": 14028, "start": 156.44, "end": 161.48, "text": " Logger is kind of the core concept of Python logging and all loggers start with a name.", "tokens": [10824, 1321, 307, 733, 295, 264, 4965, 3410, 295, 15329, 27991, 293, 439, 3565, 9458, 722, 365, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 58, "seek": 14028, "start": 161.48, "end": 164.52, "text": " So we're going to do underscore underscore name.", "tokens": [407, 321, 434, 516, 281, 360, 37556, 37556, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 59, "seek": 14028, "start": 164.52, "end": 170.08, "text": " Does anyone know what underscore underscore name actually will resolve to in this instance?", "tokens": [4402, 2878, 458, 437, 37556, 37556, 1315, 767, 486, 14151, 281, 294, 341, 5197, 30], "temperature": 0.0, "avg_logprob": -0.12062481910951676, "compression_ratio": 1.855513307984791, "no_speech_prob": 1.7322990970569663e-06}, {"id": 60, "seek": 17008, "start": 170.08, "end": 172.44000000000003, "text": " Any guesses?", "tokens": [2639, 42703, 30], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 61, "seek": 17008, "start": 172.44000000000003, "end": 173.64000000000001, "text": " It's underscore underscore main.", "tokens": [467, 311, 37556, 37556, 2135, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 62, "seek": 17008, "start": 173.64000000000001, "end": 175.76000000000002, "text": " Come on, guys.", "tokens": [2492, 322, 11, 1074, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 63, "seek": 17008, "start": 175.76000000000002, "end": 178.04000000000002, "text": " So we create a logger.", "tokens": [407, 321, 1884, 257, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 64, "seek": 17008, "start": 178.04000000000002, "end": 179.04000000000002, "text": " We give it a name.", "tokens": [492, 976, 309, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 65, "seek": 17008, "start": 179.04000000000002, "end": 181.64000000000001, "text": " And fun fact of the day, that logger is global.", "tokens": [400, 1019, 1186, 295, 264, 786, 11, 300, 3565, 1321, 307, 4338, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 66, "seek": 17008, "start": 181.64000000000001, "end": 190.68, "text": " I can import Python logging and do get logger, of course, from anywhere and use this logger.", "tokens": [286, 393, 974, 15329, 27991, 293, 360, 483, 3565, 1321, 11, 295, 1164, 11, 490, 4992, 293, 764, 341, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 67, "seek": 17008, "start": 190.68, "end": 191.92000000000002, "text": " As it's too small.", "tokens": [1018, 309, 311, 886, 1359, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 68, "seek": 17008, "start": 191.92000000000002, "end": 192.92000000000002, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 69, "seek": 17008, "start": 192.92000000000002, "end": 194.24, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 70, "seek": 17008, "start": 194.24, "end": 197.88000000000002, "text": " Try not to delete code while I'm at it.", "tokens": [6526, 406, 281, 12097, 3089, 1339, 286, 478, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 71, "seek": 17008, "start": 197.88000000000002, "end": 198.88000000000002, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1874644072159477, "compression_ratio": 1.5633802816901408, "no_speech_prob": 3.3716150937834755e-05}, {"id": 72, "seek": 19888, "start": 198.88, "end": 200.64, "text": " We're going to make this smaller.", "tokens": [492, 434, 516, 281, 652, 341, 4356, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 73, "seek": 19888, "start": 200.64, "end": 202.96, "text": " We're going to make this up here.", "tokens": [492, 434, 516, 281, 652, 341, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 74, "seek": 19888, "start": 202.96, "end": 206.68, "text": " So as we know with logging, there are several different levels you can set.", "tokens": [407, 382, 321, 458, 365, 27991, 11, 456, 366, 2940, 819, 4358, 291, 393, 992, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 75, "seek": 19888, "start": 206.68, "end": 210.48, "text": " These levels are a filtering mechanism for Python logging.", "tokens": [1981, 4358, 366, 257, 30822, 7513, 337, 15329, 27991, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 76, "seek": 19888, "start": 210.48, "end": 215.35999999999999, "text": " Fun fact of the day, though, most people don't know this, is these logger or these levels", "tokens": [11166, 1186, 295, 264, 786, 11, 1673, 11, 881, 561, 500, 380, 458, 341, 11, 307, 613, 3565, 1321, 420, 613, 4358], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 77, "seek": 19888, "start": 215.35999999999999, "end": 217.35999999999999, "text": " actually resolve two numbers.", "tokens": [767, 14151, 732, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 78, "seek": 19888, "start": 217.35999999999999, "end": 220.2, "text": " And those numbers are used to do filtering.", "tokens": [400, 729, 3547, 366, 1143, 281, 360, 30822, 13], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 79, "seek": 19888, "start": 220.2, "end": 226.88, "text": " And fun fact of the day, you can actually make your own log levels as if five, six,", "tokens": [400, 1019, 1186, 295, 264, 786, 11, 291, 393, 767, 652, 428, 1065, 3565, 4358, 382, 498, 1732, 11, 2309, 11], "temperature": 0.0, "avg_logprob": -0.16057108216366525, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.166041278745979e-05}, {"id": 80, "seek": 22688, "start": 226.88, "end": 229.16, "text": " six log levels, seven log levels isn't enough.", "tokens": [2309, 3565, 4358, 11, 3407, 3565, 4358, 1943, 380, 1547, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 81, "seek": 22688, "start": 229.16, "end": 231.6, "text": " You could add more potentially if you needed that.", "tokens": [509, 727, 909, 544, 7263, 498, 291, 2978, 300, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 82, "seek": 22688, "start": 231.6, "end": 235.16, "text": " So we're going to go ahead and send this to terminal.", "tokens": [407, 321, 434, 516, 281, 352, 2286, 293, 2845, 341, 281, 14709, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 83, "seek": 22688, "start": 235.16, "end": 237.72, "text": " And we're going to take a look at all those log levels.", "tokens": [400, 321, 434, 516, 281, 747, 257, 574, 412, 439, 729, 3565, 4358, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 84, "seek": 22688, "start": 237.72, "end": 240.12, "text": " Here we are.", "tokens": [1692, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 85, "seek": 22688, "start": 240.12, "end": 245.92, "text": " As we can see, they resolve to 10, 20, 30, 30, 30, 30, 40, and 50.", "tokens": [1018, 321, 393, 536, 11, 436, 14151, 281, 1266, 11, 945, 11, 2217, 11, 2217, 11, 2217, 11, 2217, 11, 3356, 11, 293, 2625, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 86, "seek": 22688, "start": 245.92, "end": 249.12, "text": " So warn and warning both resolve to the same log level.", "tokens": [407, 12286, 293, 9164, 1293, 14151, 281, 264, 912, 3565, 1496, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 87, "seek": 22688, "start": 249.12, "end": 255.2, "text": " So they're both there in case you need two log levels that have the same level.", "tokens": [407, 436, 434, 1293, 456, 294, 1389, 291, 643, 732, 3565, 4358, 300, 362, 264, 912, 1496, 13], "temperature": 0.0, "avg_logprob": -0.16079049682617189, "compression_ratio": 1.815450643776824, "no_speech_prob": 5.589656666415976e-06}, {"id": 88, "seek": 25520, "start": 255.2, "end": 257.08, "text": " And these levels, again, are used for filtering.", "tokens": [400, 613, 4358, 11, 797, 11, 366, 1143, 337, 30822, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 89, "seek": 25520, "start": 257.08, "end": 263.03999999999996, "text": " So as you are sending out logs, they're used to filter out logs that maybe are less important.", "tokens": [407, 382, 291, 366, 7750, 484, 20820, 11, 436, 434, 1143, 281, 6608, 484, 20820, 300, 1310, 366, 1570, 1021, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 90, "seek": 25520, "start": 263.03999999999996, "end": 267.88, "text": " So the number, the lower the importance of the log.", "tokens": [407, 264, 1230, 11, 264, 3126, 264, 7379, 295, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 91, "seek": 25520, "start": 267.88, "end": 269.56, "text": " So let's talk about emitting a log.", "tokens": [407, 718, 311, 751, 466, 846, 2414, 257, 3565, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 92, "seek": 25520, "start": 269.56, "end": 274.2, "text": " Here we're going to go ahead and send a log with logger.info.", "tokens": [1692, 321, 434, 516, 281, 352, 2286, 293, 2845, 257, 3565, 365, 3565, 1321, 13, 259, 16931, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 93, "seek": 25520, "start": 274.2, "end": 280.44, "text": " So that's going to emit a log from the logger with the info level attached to it.", "tokens": [407, 300, 311, 516, 281, 32084, 257, 3565, 490, 264, 3565, 1321, 365, 264, 13614, 1496, 8570, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.09571877065694558, "compression_ratio": 1.7772511848341233, "no_speech_prob": 1.0181911420659162e-06}, {"id": 94, "seek": 28044, "start": 280.44, "end": 285.84, "text": " Upper case info is for sending logs, upper case info is the level.", "tokens": [36926, 1389, 13614, 307, 337, 7750, 20820, 11, 6597, 1389, 13614, 307, 264, 1496, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 95, "seek": 28044, "start": 285.84, "end": 287.56, "text": " And nothing actually happens.", "tokens": [400, 1825, 767, 2314, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 96, "seek": 28044, "start": 287.56, "end": 290.68, "text": " It just gets sent to the terminal and nothing happens.", "tokens": [467, 445, 2170, 2279, 281, 264, 14709, 293, 1825, 2314, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 97, "seek": 28044, "start": 290.68, "end": 295.32, "text": " And the reason why nothing happens is because we've not told it where it needs to go.", "tokens": [400, 264, 1778, 983, 1825, 2314, 307, 570, 321, 600, 406, 1907, 309, 689, 309, 2203, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 98, "seek": 28044, "start": 295.32, "end": 297.48, "text": " We need log handlers.", "tokens": [492, 643, 3565, 1011, 11977, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 99, "seek": 28044, "start": 297.48, "end": 300.96, "text": " So that is what we do.", "tokens": [407, 300, 307, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 100, "seek": 28044, "start": 300.96, "end": 306.88, "text": " We're going to build a simple syslog handler for, sorry, streaming handler for sending to", "tokens": [492, 434, 516, 281, 1322, 257, 2199, 262, 749, 4987, 41967, 337, 11, 2597, 11, 11791, 41967, 337, 7750, 281], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 101, "seek": 28044, "start": 306.88, "end": 308.56, "text": " standard out now.", "tokens": [3832, 484, 586, 13], "temperature": 0.0, "avg_logprob": -0.11682110741024926, "compression_ratio": 1.7647058823529411, "no_speech_prob": 4.636210633179871e-06}, {"id": 102, "seek": 30856, "start": 308.56, "end": 311.24, "text": " So handlers receive logs from the logger.", "tokens": [407, 1011, 11977, 4774, 20820, 490, 264, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 103, "seek": 30856, "start": 311.24, "end": 312.76, "text": " It's pretty straightforward.", "tokens": [467, 311, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 104, "seek": 30856, "start": 312.76, "end": 317.84, "text": " I'm going to go ahead and create this real quick.", "tokens": [286, 478, 516, 281, 352, 2286, 293, 1884, 341, 957, 1702, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 105, "seek": 30856, "start": 317.84, "end": 319.92, "text": " We're going to set the level of our handler.", "tokens": [492, 434, 516, 281, 992, 264, 1496, 295, 527, 41967, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 106, "seek": 30856, "start": 319.92, "end": 323.84000000000003, "text": " Remember our levels are our wonderful filtering mechanism.", "tokens": [5459, 527, 4358, 366, 527, 3715, 30822, 7513, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 107, "seek": 30856, "start": 323.84000000000003, "end": 325.92, "text": " Set the level to warning.", "tokens": [8928, 264, 1496, 281, 9164, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 108, "seek": 30856, "start": 325.92, "end": 331.76, "text": " And the top one will not show up, but the bottom one should because it is greater than", "tokens": [400, 264, 1192, 472, 486, 406, 855, 493, 11, 457, 264, 2767, 472, 820, 570, 309, 307, 5044, 813], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 109, "seek": 30856, "start": 331.76, "end": 333.24, "text": " warning.", "tokens": [9164, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 110, "seek": 30856, "start": 333.24, "end": 334.24, "text": " And there it is.", "tokens": [400, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 111, "seek": 30856, "start": 334.24, "end": 335.96, "text": " This will.", "tokens": [639, 486, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 112, "seek": 30856, "start": 335.96, "end": 337.68, "text": " That's pretty dull.", "tokens": [663, 311, 1238, 23471, 13], "temperature": 0.0, "avg_logprob": -0.19197259507737718, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.8548497539304662e-06}, {"id": 113, "seek": 33768, "start": 337.68, "end": 339.92, "text": " Nobody really wants a log that says this will.", "tokens": [9297, 534, 2738, 257, 3565, 300, 1619, 341, 486, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 114, "seek": 33768, "start": 339.92, "end": 341.68, "text": " That tells us nothing.", "tokens": [663, 5112, 505, 1825, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 115, "seek": 33768, "start": 341.68, "end": 344.04, "text": " You have no clue what's going on in your application.", "tokens": [509, 362, 572, 13602, 437, 311, 516, 322, 294, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 116, "seek": 33768, "start": 344.04, "end": 346.12, "text": " What time did this will?", "tokens": [708, 565, 630, 341, 486, 30], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 117, "seek": 33768, "start": 346.12, "end": 348.88, "text": " What time will it won't?", "tokens": [708, 565, 486, 309, 1582, 380, 30], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 118, "seek": 33768, "start": 348.88, "end": 350.36, "text": " So we need to add some context.", "tokens": [407, 321, 643, 281, 909, 512, 4319, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 119, "seek": 33768, "start": 350.36, "end": 354.6, "text": " And the way we get context is, of course, with log formatters.", "tokens": [400, 264, 636, 321, 483, 4319, 307, 11, 295, 1164, 11, 365, 3565, 1254, 37690, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 120, "seek": 33768, "start": 354.6, "end": 358.84000000000003, "text": " And I've jumped ahead too far in my presentation.", "tokens": [400, 286, 600, 13864, 2286, 886, 1400, 294, 452, 5860, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 121, "seek": 33768, "start": 358.84000000000003, "end": 361.72, "text": " We're just going to take a quick back step real quick.", "tokens": [492, 434, 445, 516, 281, 747, 257, 1702, 646, 1823, 957, 1702, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 122, "seek": 33768, "start": 361.72, "end": 364.64, "text": " We're going to talk about some of the other handlers that are built in.", "tokens": [492, 434, 516, 281, 751, 466, 512, 295, 264, 661, 1011, 11977, 300, 366, 3094, 294, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 123, "seek": 33768, "start": 364.64, "end": 366.52, "text": " So there's the rotating file handle.", "tokens": [407, 456, 311, 264, 19627, 3991, 4813, 13], "temperature": 0.0, "avg_logprob": -0.13604822646092324, "compression_ratio": 1.6912280701754385, "no_speech_prob": 2.259802613480133e-06}, {"id": 124, "seek": 36652, "start": 366.52, "end": 371.4, "text": " This one's particularly useful if you have too many logs and your file gets too large.", "tokens": [639, 472, 311, 4098, 4420, 498, 291, 362, 886, 867, 20820, 293, 428, 3991, 2170, 886, 2416, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 125, "seek": 36652, "start": 371.4, "end": 378.24, "text": " It can actually automatically rotate that file every X amount of size into a new file.", "tokens": [467, 393, 767, 6772, 13121, 300, 3991, 633, 1783, 2372, 295, 2744, 666, 257, 777, 3991, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 126, "seek": 36652, "start": 378.24, "end": 382.79999999999995, "text": " And then you can specify for it to delete logs after a certain amount of time or a certain", "tokens": [400, 550, 291, 393, 16500, 337, 309, 281, 12097, 20820, 934, 257, 1629, 2372, 295, 565, 420, 257, 1629], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 127, "seek": 36652, "start": 382.79999999999995, "end": 383.79999999999995, "text": " amount of files.", "tokens": [2372, 295, 7098, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 128, "seek": 36652, "start": 383.79999999999995, "end": 386.15999999999997, "text": " We have the syslog handler.", "tokens": [492, 362, 264, 262, 749, 4987, 41967, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 129, "seek": 36652, "start": 386.15999999999997, "end": 388.24, "text": " Syslog is a standard for logging.", "tokens": [318, 749, 4987, 307, 257, 3832, 337, 27991, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 130, "seek": 36652, "start": 388.24, "end": 393.03999999999996, "text": " HTTP lets you send logs to arbitrary HTTP endpoints.", "tokens": [33283, 6653, 291, 2845, 20820, 281, 23211, 33283, 917, 20552, 13], "temperature": 0.0, "avg_logprob": -0.15936743044385723, "compression_ratio": 1.6569037656903767, "no_speech_prob": 1.8441370457367157e-06}, {"id": 131, "seek": 39304, "start": 393.04, "end": 398.76000000000005, "text": " Time rotating file handler, believe it or not, is a timed rotation of your log files.", "tokens": [6161, 19627, 3991, 41967, 11, 1697, 309, 420, 406, 11, 307, 257, 44696, 12447, 295, 428, 3565, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 132, "seek": 39304, "start": 398.76000000000005, "end": 402.72, "text": " Whether it be every day, every minute, or please God forbid every second, do not do", "tokens": [8503, 309, 312, 633, 786, 11, 633, 3456, 11, 420, 1767, 1265, 34117, 633, 1150, 11, 360, 406, 360], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 133, "seek": 39304, "start": 402.72, "end": 403.72, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 134, "seek": 39304, "start": 403.72, "end": 405.0, "text": " You will end up with thousands of files.", "tokens": [509, 486, 917, 493, 365, 5383, 295, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 135, "seek": 39304, "start": 405.0, "end": 408.28000000000003, "text": " I did actually every hour once, and that was a huge mistake.", "tokens": [286, 630, 767, 633, 1773, 1564, 11, 293, 300, 390, 257, 2603, 6146, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 136, "seek": 39304, "start": 408.28000000000003, "end": 412.56, "text": " I ended up with like 15,000 files after a little bit.", "tokens": [286, 4590, 493, 365, 411, 2119, 11, 1360, 7098, 934, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 137, "seek": 39304, "start": 412.56, "end": 418.16, "text": " And the SMTP handler, if you are masochistic.", "tokens": [400, 264, 13115, 16804, 41967, 11, 498, 291, 366, 2300, 8997, 3142, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 138, "seek": 39304, "start": 418.16, "end": 420.40000000000003, "text": " Now we'll talk about formatters.", "tokens": [823, 321, 603, 751, 466, 1254, 37690, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 139, "seek": 39304, "start": 420.40000000000003, "end": 421.56, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1927142302195231, "compression_ratio": 1.5741444866920151, "no_speech_prob": 3.6665064726548735e-06}, {"id": 140, "seek": 42156, "start": 421.56, "end": 422.96, "text": " So let's go ahead.", "tokens": [407, 718, 311, 352, 2286, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 141, "seek": 42156, "start": 422.96, "end": 427.64, "text": " We're going to create, set our console handle to the info level.", "tokens": [492, 434, 516, 281, 1884, 11, 992, 527, 11076, 4813, 281, 264, 13614, 1496, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 142, "seek": 42156, "start": 427.64, "end": 429.04, "text": " We're going to create a formatter.", "tokens": [492, 434, 516, 281, 1884, 257, 1254, 1161, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 143, "seek": 42156, "start": 429.04, "end": 433.0, "text": " The formatter is going to include the date and time, the name of the logger.", "tokens": [440, 1254, 1161, 307, 516, 281, 4090, 264, 4002, 293, 565, 11, 264, 1315, 295, 264, 3565, 1321, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 144, "seek": 42156, "start": 433.0, "end": 436.24, "text": " Which again, if you're using underscore underscore name, that's going to be the name of your", "tokens": [3013, 797, 11, 498, 291, 434, 1228, 37556, 37556, 1315, 11, 300, 311, 516, 281, 312, 264, 1315, 295, 428], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 145, "seek": 42156, "start": 436.24, "end": 440.52, "text": " module, whether that's like search dot util dot whatever.", "tokens": [10088, 11, 1968, 300, 311, 411, 3164, 5893, 4976, 5893, 2035, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 146, "seek": 42156, "start": 440.52, "end": 445.68, "text": " And then the level name that it was admitted at, and a message.", "tokens": [400, 550, 264, 1496, 1315, 300, 309, 390, 14920, 412, 11, 293, 257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 147, "seek": 42156, "start": 445.68, "end": 446.8, "text": " Pretty straightforward.", "tokens": [10693, 15325, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 148, "seek": 42156, "start": 446.8, "end": 448.8, "text": " Then we set the formatter.", "tokens": [1396, 321, 992, 264, 1254, 1161, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 149, "seek": 42156, "start": 448.8, "end": 451.4, "text": " Formatter gets attached to handlers again.", "tokens": [10126, 1161, 2170, 8570, 281, 1011, 11977, 797, 13], "temperature": 0.0, "avg_logprob": -0.16607695252355867, "compression_ratio": 1.8327272727272728, "no_speech_prob": 5.768852133769542e-06}, {"id": 150, "seek": 45140, "start": 451.4, "end": 453.59999999999997, "text": " Logger dot info.", "tokens": [10824, 1321, 5893, 13614, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 151, "seek": 45140, "start": 453.59999999999997, "end": 455.91999999999996, "text": " Look at my pretty log.", "tokens": [2053, 412, 452, 1238, 3565, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 152, "seek": 45140, "start": 455.91999999999996, "end": 457.79999999999995, "text": " Well, just kidding.", "tokens": [1042, 11, 445, 9287, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 153, "seek": 45140, "start": 457.79999999999995, "end": 459.64, "text": " There's no log there.", "tokens": [821, 311, 572, 3565, 456, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 154, "seek": 45140, "start": 459.64, "end": 461.76, "text": " Is it because I have a bug?", "tokens": [1119, 309, 570, 286, 362, 257, 7426, 30], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 155, "seek": 45140, "start": 461.76, "end": 464.03999999999996, "text": " Because I wrote this way too late at night?", "tokens": [1436, 286, 4114, 341, 636, 886, 3469, 412, 1818, 30], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 156, "seek": 45140, "start": 464.03999999999996, "end": 465.03999999999996, "text": " Probably.", "tokens": [9210, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 157, "seek": 45140, "start": 465.03999999999996, "end": 469.32, "text": " Or more likely because there was something I actually tricked you on earlier.", "tokens": [1610, 544, 3700, 570, 456, 390, 746, 286, 767, 39345, 291, 322, 3071, 13], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 158, "seek": 45140, "start": 469.32, "end": 473.64, "text": " The truth is, the reason our first log wasn't admitted wasn't because there wasn't a handler", "tokens": [440, 3494, 307, 11, 264, 1778, 527, 700, 3565, 2067, 380, 14920, 2067, 380, 570, 456, 2067, 380, 257, 41967], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 159, "seek": 45140, "start": 473.64, "end": 479.47999999999996, "text": " attached to it, but it's because Python's logging library by default sets loggers and", "tokens": [8570, 281, 309, 11, 457, 309, 311, 570, 15329, 311, 27991, 6405, 538, 7576, 6352, 450, 1615, 433, 293], "temperature": 0.0, "avg_logprob": -0.17277535006531283, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.2795742804883048e-05}, {"id": 160, "seek": 47948, "start": 479.48, "end": 484.68, "text": " handlers to the warning level right out of the gate, and we admitted at the warning level.", "tokens": [1011, 11977, 281, 264, 9164, 1496, 558, 484, 295, 264, 8539, 11, 293, 321, 14920, 412, 264, 9164, 1496, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 161, "seek": 47948, "start": 484.68, "end": 491.68, "text": " So fun fact, you actually need to set the level for both the logger and your handlers.", "tokens": [407, 1019, 1186, 11, 291, 767, 643, 281, 992, 264, 1496, 337, 1293, 264, 3565, 1321, 293, 428, 1011, 11977, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 162, "seek": 47948, "start": 491.68, "end": 492.68, "text": " So we'll go ahead and do that.", "tokens": [407, 321, 603, 352, 2286, 293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 163, "seek": 47948, "start": 492.68, "end": 496.52000000000004, "text": " We'll set the logger level, and we're off to the races.", "tokens": [492, 603, 992, 264, 3565, 1321, 1496, 11, 293, 321, 434, 766, 281, 264, 15484, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 164, "seek": 47948, "start": 496.52000000000004, "end": 498.20000000000005, "text": " Let's look at this pretty log.", "tokens": [961, 311, 574, 412, 341, 1238, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 165, "seek": 47948, "start": 498.20000000000005, "end": 501.12, "text": " Man, that is so beautiful.", "tokens": [2458, 11, 300, 307, 370, 2238, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 166, "seek": 47948, "start": 501.12, "end": 504.32, "text": " Now I know exactly when I had a pretty log.", "tokens": [823, 286, 458, 2293, 562, 286, 632, 257, 1238, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 167, "seek": 47948, "start": 504.32, "end": 506.72, "text": " I know exactly where it happened in the main.", "tokens": [286, 458, 2293, 689, 309, 2011, 294, 264, 2135, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 168, "seek": 47948, "start": 506.72, "end": 508.28000000000003, "text": " I know what level it was at.", "tokens": [286, 458, 437, 1496, 309, 390, 412, 13], "temperature": 0.0, "avg_logprob": -0.11957328502948468, "compression_ratio": 1.829875518672199, "no_speech_prob": 2.190409304603236e-06}, {"id": 169, "seek": 50828, "start": 508.28, "end": 511.71999999999997, "text": " It was informational log.", "tokens": [467, 390, 49391, 3565, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 170, "seek": 50828, "start": 511.71999999999997, "end": 514.04, "text": " So we've talked about a lot so far.", "tokens": [407, 321, 600, 2825, 466, 257, 688, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 171, "seek": 50828, "start": 514.04, "end": 517.12, "text": " We've talked about loggers, handlers.", "tokens": [492, 600, 2825, 466, 450, 1615, 433, 11, 1011, 11977, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 172, "seek": 50828, "start": 517.12, "end": 521.72, "text": " So let's do a top to bottom just real quick to make sure we're all talking about the", "tokens": [407, 718, 311, 360, 257, 1192, 281, 2767, 445, 957, 1702, 281, 652, 988, 321, 434, 439, 1417, 466, 264], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 173, "seek": 50828, "start": 521.72, "end": 523.3199999999999, "text": " same thing.", "tokens": [912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 174, "seek": 50828, "start": 523.3199999999999, "end": 525.76, "text": " So we have loggers.", "tokens": [407, 321, 362, 450, 1615, 433, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 175, "seek": 50828, "start": 525.76, "end": 528.76, "text": " Loggers emit a log at some level.", "tokens": [10824, 9458, 32084, 257, 3565, 412, 512, 1496, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 176, "seek": 50828, "start": 528.76, "end": 531.56, "text": " They also filter out logs at some level.", "tokens": [814, 611, 6608, 484, 20820, 412, 512, 1496, 13], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 177, "seek": 50828, "start": 531.56, "end": 536.68, "text": " So if your logger is set to filter out warning logs and you send it an info, it's never", "tokens": [407, 498, 428, 3565, 1321, 307, 992, 281, 6608, 484, 9164, 20820, 293, 291, 2845, 309, 364, 13614, 11, 309, 311, 1128], "temperature": 0.0, "avg_logprob": -0.1028671511288347, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.4051560103544034e-06}, {"id": 178, "seek": 53668, "start": 536.68, "end": 539.4399999999999, "text": " going to get to the handler.", "tokens": [516, 281, 483, 281, 264, 41967, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 179, "seek": 53668, "start": 539.4399999999999, "end": 543.64, "text": " Handlers receive logs and then send them to some specified output, whatever that may", "tokens": [8854, 11977, 4774, 20820, 293, 550, 2845, 552, 281, 512, 22206, 5598, 11, 2035, 300, 815], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 180, "seek": 53668, "start": 543.64, "end": 544.64, "text": " be.", "tokens": [312, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 181, "seek": 53668, "start": 544.64, "end": 548.3199999999999, "text": " And then formatters attach to handlers, and they enrich the output.", "tokens": [400, 550, 1254, 37690, 5085, 281, 1011, 11977, 11, 293, 436, 18849, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 182, "seek": 53668, "start": 548.3199999999999, "end": 549.92, "text": " So they add context.", "tokens": [407, 436, 909, 4319, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 183, "seek": 53668, "start": 549.92, "end": 551.92, "text": " And there's a bunch of other contexts.", "tokens": [400, 456, 311, 257, 3840, 295, 661, 30628, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 184, "seek": 53668, "start": 551.92, "end": 552.92, "text": " That's not mentioned here.", "tokens": [663, 311, 406, 2835, 510, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 185, "seek": 53668, "start": 552.92, "end": 557.04, "text": " It's in a Python logging library.", "tokens": [467, 311, 294, 257, 15329, 27991, 6405, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 186, "seek": 53668, "start": 557.04, "end": 560.4399999999999, "text": " I guess it's like at a certain time in the presentation, it just decides it wants to", "tokens": [286, 2041, 309, 311, 411, 412, 257, 1629, 565, 294, 264, 5860, 11, 309, 445, 14898, 309, 2738, 281], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 187, "seek": 53668, "start": 560.4399999999999, "end": 563.0799999999999, "text": " do that.", "tokens": [360, 300, 13], "temperature": 0.0, "avg_logprob": -0.19153857231140137, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.564577011478832e-06}, {"id": 188, "seek": 56308, "start": 563.08, "end": 569.4000000000001, "text": " So these are all wonderful.", "tokens": [407, 613, 366, 439, 3715, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 189, "seek": 56308, "start": 569.4000000000001, "end": 570.4000000000001, "text": " Wait a minute.", "tokens": [3802, 257, 3456, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 190, "seek": 56308, "start": 570.4000000000001, "end": 571.4000000000001, "text": " I'm getting there.", "tokens": [286, 478, 1242, 456, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 191, "seek": 56308, "start": 571.4000000000001, "end": 572.4000000000001, "text": " Logging.", "tokens": [10824, 3249, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 192, "seek": 56308, "start": 572.4000000000001, "end": 573.4000000000001, "text": " Logging.", "tokens": [10824, 3249, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 193, "seek": 56308, "start": 573.4000000000001, "end": 574.4000000000001, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 194, "seek": 56308, "start": 574.4000000000001, "end": 575.4000000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 195, "seek": 56308, "start": 575.4000000000001, "end": 576.4000000000001, "text": " Here we go.", "tokens": [1692, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 196, "seek": 56308, "start": 576.4000000000001, "end": 577.4000000000001, "text": " We're just going to scroll on.", "tokens": [492, 434, 445, 516, 281, 11369, 322, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 197, "seek": 56308, "start": 577.4000000000001, "end": 578.4000000000001, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 198, "seek": 56308, "start": 578.4000000000001, "end": 579.4000000000001, "text": " Here we go.", "tokens": [1692, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 199, "seek": 56308, "start": 579.4000000000001, "end": 582.44, "text": " So setting up logs from, you know, in each individual module is a pain.", "tokens": [407, 3287, 493, 20820, 490, 11, 291, 458, 11, 294, 1184, 2609, 10088, 307, 257, 1822, 13], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 200, "seek": 56308, "start": 582.44, "end": 587.96, "text": " So you can actually pre-create loggers ahead of time with a dictionary config or a YAML", "tokens": [407, 291, 393, 767, 659, 12, 14066, 473, 450, 1615, 433, 2286, 295, 565, 365, 257, 25890, 6662, 420, 257, 398, 2865, 43], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 201, "seek": 56308, "start": 587.96, "end": 592.88, "text": " config file because I know all of us just love creating YAML configs and having them", "tokens": [6662, 3991, 570, 286, 458, 439, 295, 505, 445, 959, 4084, 398, 2865, 43, 6662, 82, 293, 1419, 552], "temperature": 0.0, "avg_logprob": -0.15561609621401187, "compression_ratio": 1.5968379446640317, "no_speech_prob": 1.993520299947704e-06}, {"id": 202, "seek": 59288, "start": 592.88, "end": 594.28, "text": " everywhere.", "tokens": [5315, 13], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 203, "seek": 59288, "start": 594.28, "end": 602.24, "text": " So with this, you can create as many logs as you want and specify them with dictionary", "tokens": [407, 365, 341, 11, 291, 393, 1884, 382, 867, 20820, 382, 291, 528, 293, 16500, 552, 365, 25890], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 204, "seek": 59288, "start": 602.24, "end": 603.5, "text": " config.", "tokens": [6662, 13], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 205, "seek": 59288, "start": 603.5, "end": 606.96, "text": " One other really important thing to mention, and I probably should have hit it when I was", "tokens": [1485, 661, 534, 1021, 551, 281, 2152, 11, 293, 286, 1391, 820, 362, 2045, 309, 562, 286, 390], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 206, "seek": 59288, "start": 606.96, "end": 610.96, "text": " talking about loggers at the get-go, there's a specific reason why you can set it at the", "tokens": [1417, 466, 450, 1615, 433, 412, 264, 483, 12, 1571, 11, 456, 311, 257, 2685, 1778, 983, 291, 393, 992, 309, 412, 264], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 207, "seek": 59288, "start": 610.96, "end": 613.96, "text": " logger level and at the handler level.", "tokens": [3565, 1321, 1496, 293, 412, 264, 41967, 1496, 13], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 208, "seek": 59288, "start": 613.96, "end": 618.32, "text": " So handlers give you very fine-tune access over what you're looking at.", "tokens": [407, 1011, 11977, 976, 291, 588, 2489, 12, 83, 2613, 2105, 670, 437, 291, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 209, "seek": 59288, "start": 618.32, "end": 621.28, "text": " So where it's going, which output, you know.", "tokens": [407, 689, 309, 311, 516, 11, 597, 5598, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.11993858019510904, "compression_ratio": 1.6896551724137931, "no_speech_prob": 2.9468449156411225e-06}, {"id": 210, "seek": 62128, "start": 621.28, "end": 625.64, "text": " So a lot of times people will specify certain levels for handlers and then they will use", "tokens": [407, 257, 688, 295, 1413, 561, 486, 16500, 1629, 4358, 337, 1011, 11977, 293, 550, 436, 486, 764], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 211, "seek": 62128, "start": 625.64, "end": 629.1999999999999, "text": " their global debug level to set their loggers.", "tokens": [641, 4338, 24083, 1496, 281, 992, 641, 450, 1615, 433, 13], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 212, "seek": 62128, "start": 629.1999999999999, "end": 633.9599999999999, "text": " So say, for example, you're debugging an application locally, you're going to set that to debug,", "tokens": [407, 584, 11, 337, 1365, 11, 291, 434, 45592, 364, 3861, 16143, 11, 291, 434, 516, 281, 992, 300, 281, 24083, 11], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 213, "seek": 62128, "start": 633.9599999999999, "end": 638.3199999999999, "text": " of course, but when you push it to production, you don't want that.", "tokens": [295, 1164, 11, 457, 562, 291, 2944, 309, 281, 4265, 11, 291, 500, 380, 528, 300, 13], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 214, "seek": 62128, "start": 638.3199999999999, "end": 644.1999999999999, "text": " So a lot of people will have, you know, a production logging config and a development", "tokens": [407, 257, 688, 295, 561, 486, 362, 11, 291, 458, 11, 257, 4265, 27991, 6662, 293, 257, 3250], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 215, "seek": 62128, "start": 644.1999999999999, "end": 646.4399999999999, "text": " environment logging config.", "tokens": [2823, 27991, 6662, 13], "temperature": 0.0, "avg_logprob": -0.10243226014650784, "compression_ratio": 1.7922077922077921, "no_speech_prob": 9.274456829189148e-07}, {"id": 216, "seek": 64644, "start": 646.44, "end": 655.48, "text": " So with that, there is actually one other slight challenge with loggers, and that is", "tokens": [407, 365, 300, 11, 456, 307, 767, 472, 661, 4036, 3430, 365, 450, 1615, 433, 11, 293, 300, 307], "temperature": 0.0, "avg_logprob": -0.12181949615478516, "compression_ratio": 1.5145631067961165, "no_speech_prob": 5.625153676191985e-07}, {"id": 217, "seek": 64644, "start": 655.48, "end": 657.6400000000001, "text": " they're blocking operations.", "tokens": [436, 434, 17776, 7705, 13], "temperature": 0.0, "avg_logprob": -0.12181949615478516, "compression_ratio": 1.5145631067961165, "no_speech_prob": 5.625153676191985e-07}, {"id": 218, "seek": 64644, "start": 657.6400000000001, "end": 664.0, "text": " So like I said earlier, if you're a masochist and you like the SMTP log handler, you could", "tokens": [407, 411, 286, 848, 3071, 11, 498, 291, 434, 257, 2300, 8997, 468, 293, 291, 411, 264, 13115, 16804, 3565, 41967, 11, 291, 727], "temperature": 0.0, "avg_logprob": -0.12181949615478516, "compression_ratio": 1.5145631067961165, "no_speech_prob": 5.625153676191985e-07}, {"id": 219, "seek": 64644, "start": 664.0, "end": 665.2, "text": " be in a real pinch.", "tokens": [312, 294, 257, 957, 14614, 13], "temperature": 0.0, "avg_logprob": -0.12181949615478516, "compression_ratio": 1.5145631067961165, "no_speech_prob": 5.625153676191985e-07}, {"id": 220, "seek": 64644, "start": 665.2, "end": 670.6, "text": " So say, for example, I'm on your application, you have this nice web server, and all of", "tokens": [407, 584, 11, 337, 1365, 11, 286, 478, 322, 428, 3861, 11, 291, 362, 341, 1481, 3670, 7154, 11, 293, 439, 295], "temperature": 0.0, "avg_logprob": -0.12181949615478516, "compression_ratio": 1.5145631067961165, "no_speech_prob": 5.625153676191985e-07}, {"id": 221, "seek": 67060, "start": 670.6, "end": 677.12, "text": " a sudden it hits a critical error, it sends a message to your SMTP server, and your SMTP", "tokens": [257, 3990, 309, 8664, 257, 4924, 6713, 11, 309, 14790, 257, 3636, 281, 428, 13115, 16804, 7154, 11, 293, 428, 13115, 16804], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 222, "seek": 67060, "start": 677.12, "end": 682.88, "text": " server is slow, it's chugging, so you're taking five to ten seconds for it to register", "tokens": [7154, 307, 2964, 11, 309, 311, 417, 697, 3249, 11, 370, 291, 434, 1940, 1732, 281, 2064, 3949, 337, 309, 281, 7280], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 223, "seek": 67060, "start": 682.88, "end": 684.72, "text": " that and send a response.", "tokens": [300, 293, 2845, 257, 4134, 13], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 224, "seek": 67060, "start": 684.72, "end": 688.6800000000001, "text": " Do you think I'm going to stay on your web page for five to ten seconds while it sends", "tokens": [1144, 291, 519, 286, 478, 516, 281, 1754, 322, 428, 3670, 3028, 337, 1732, 281, 2064, 3949, 1339, 309, 14790], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 225, "seek": 67060, "start": 688.6800000000001, "end": 689.6800000000001, "text": " an error log?", "tokens": [364, 6713, 3565, 30], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 226, "seek": 67060, "start": 689.6800000000001, "end": 693.72, "text": " Heck, no, I'm closing out and I'm going to somewhere else, I don't know, amazon.com", "tokens": [41948, 11, 572, 11, 286, 478, 10377, 484, 293, 286, 478, 516, 281, 4079, 1646, 11, 286, 500, 380, 458, 11, 47010, 13, 1112], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 227, "seek": 67060, "start": 693.72, "end": 695.72, "text": " to buy whatever I needed.", "tokens": [281, 2256, 2035, 286, 2978, 13], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 228, "seek": 67060, "start": 695.72, "end": 698.32, "text": " So we have to handle this.", "tokens": [407, 321, 362, 281, 4813, 341, 13], "temperature": 0.0, "avg_logprob": -0.16022266713223715, "compression_ratio": 1.7081712062256809, "no_speech_prob": 9.21744776860578e-06}, {"id": 229, "seek": 69832, "start": 698.32, "end": 702.32, "text": " We have to understand that, hey, this could potentially block, so how do we unblock our", "tokens": [492, 362, 281, 1223, 300, 11, 4177, 11, 341, 727, 7263, 3461, 11, 370, 577, 360, 321, 517, 28830, 527], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 230, "seek": 69832, "start": 702.32, "end": 703.32, "text": " applications?", "tokens": [5821, 30], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 231, "seek": 69832, "start": 703.32, "end": 708.88, "text": " Well, it's by making our applications simpler, obviously, and using multi-threading.", "tokens": [1042, 11, 309, 311, 538, 1455, 527, 5821, 18587, 11, 2745, 11, 293, 1228, 4825, 12, 392, 35908, 13], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 232, "seek": 69832, "start": 708.88, "end": 712.84, "text": " That was a joke, you can laugh.", "tokens": [663, 390, 257, 7647, 11, 291, 393, 5801, 13], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 233, "seek": 69832, "start": 712.84, "end": 718.48, "text": " So with this, we can actually import queues, and what happens is there's a queue handler", "tokens": [407, 365, 341, 11, 321, 393, 767, 974, 631, 1247, 11, 293, 437, 2314, 307, 456, 311, 257, 18639, 41967], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 234, "seek": 69832, "start": 718.48, "end": 720.4000000000001, "text": " and a queue listener.", "tokens": [293, 257, 18639, 31569, 13], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 235, "seek": 69832, "start": 720.4000000000001, "end": 726.36, "text": " So the queue is the shared memory space that can be accessed both the handler and the listener.", "tokens": [407, 264, 18639, 307, 264, 5507, 4675, 1901, 300, 393, 312, 34211, 1293, 264, 41967, 293, 264, 31569, 13], "temperature": 0.0, "avg_logprob": -0.1270231853831898, "compression_ratio": 1.6932270916334662, "no_speech_prob": 6.036582817614544e-06}, {"id": 236, "seek": 72636, "start": 726.36, "end": 731.88, "text": " You create the handler, the handler receives all the logs, and then distributes them to", "tokens": [509, 1884, 264, 41967, 11, 264, 41967, 20717, 439, 264, 20820, 11, 293, 550, 4400, 1819, 552, 281], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 237, "seek": 72636, "start": 731.88, "end": 732.88, "text": " the queue.", "tokens": [264, 18639, 13], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 238, "seek": 72636, "start": 732.88, "end": 737.96, "text": " The queue listener starts up on its own independent thread, and it's going to listen to the log", "tokens": [440, 18639, 31569, 3719, 493, 322, 1080, 1065, 6695, 7207, 11, 293, 309, 311, 516, 281, 2140, 281, 264, 3565], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 239, "seek": 72636, "start": 737.96, "end": 743.6, "text": " queue, and then distribute it to any of the handlers that you specified it should.", "tokens": [18639, 11, 293, 550, 20594, 309, 281, 604, 295, 264, 1011, 11977, 300, 291, 22206, 309, 820, 13], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 240, "seek": 72636, "start": 743.6, "end": 747.0, "text": " So let's go through that end to end again.", "tokens": [407, 718, 311, 352, 807, 300, 917, 281, 917, 797, 13], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 241, "seek": 72636, "start": 747.0, "end": 750.84, "text": " We've got queue handlers, receive the logs, place them on a queue, the queue hands it", "tokens": [492, 600, 658, 18639, 1011, 11977, 11, 4774, 264, 20820, 11, 1081, 552, 322, 257, 18639, 11, 264, 18639, 2377, 309], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 242, "seek": 72636, "start": 750.84, "end": 755.52, "text": " over to the queue listener, which then hands it on to your other loggers.", "tokens": [670, 281, 264, 18639, 31569, 11, 597, 550, 2377, 309, 322, 281, 428, 661, 450, 1615, 433, 13], "temperature": 0.0, "avg_logprob": -0.11941311645507813, "compression_ratio": 1.9834710743801653, "no_speech_prob": 1.815691121009877e-06}, {"id": 243, "seek": 75552, "start": 755.52, "end": 757.3199999999999, "text": " Now your application is unblocked.", "tokens": [823, 428, 3861, 307, 517, 28830, 292, 13], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 244, "seek": 75552, "start": 757.3199999999999, "end": 761.3199999999999, "text": " It drops that queue, or that log on the queue, and then it's off to the races.", "tokens": [467, 11438, 300, 18639, 11, 420, 300, 3565, 322, 264, 18639, 11, 293, 550, 309, 311, 766, 281, 264, 15484, 13], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 245, "seek": 75552, "start": 761.3199999999999, "end": 767.76, "text": " You can use SMTPlib if you really wanted to.", "tokens": [509, 393, 764, 13115, 16804, 38270, 498, 291, 534, 1415, 281, 13], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 246, "seek": 75552, "start": 767.76, "end": 771.52, "text": " So let's talk about pulling this all together now, right?", "tokens": [407, 718, 311, 751, 466, 8407, 341, 439, 1214, 586, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 247, "seek": 75552, "start": 771.52, "end": 777.48, "text": " So we have these logs, they sit on our local machines, and that's fine, but if you are", "tokens": [407, 321, 362, 613, 20820, 11, 436, 1394, 322, 527, 2654, 8379, 11, 293, 300, 311, 2489, 11, 457, 498, 291, 366], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 248, "seek": 75552, "start": 777.48, "end": 783.16, "text": " a large organization, you might have hundreds of servers.", "tokens": [257, 2416, 4475, 11, 291, 1062, 362, 6779, 295, 15909, 13], "temperature": 0.0, "avg_logprob": -0.12441208338973546, "compression_ratio": 1.5296610169491525, "no_speech_prob": 3.5552911867853254e-06}, {"id": 249, "seek": 78316, "start": 783.16, "end": 785.56, "text": " Take a second to breathe.", "tokens": [3664, 257, 1150, 281, 10192, 13], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 250, "seek": 78316, "start": 785.56, "end": 790.24, "text": " You might have hundreds of servers, hundreds of network devices or whatever, and I'll give", "tokens": [509, 1062, 362, 6779, 295, 15909, 11, 6779, 295, 3209, 5759, 420, 2035, 11, 293, 286, 603, 976], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 251, "seek": 78316, "start": 790.24, "end": 793.9599999999999, "text": " a real example of when I could have used this.", "tokens": [257, 957, 1365, 295, 562, 286, 727, 362, 1143, 341, 13], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 252, "seek": 78316, "start": 793.9599999999999, "end": 799.0799999999999, "text": " So when I was a network automation engineer, we had a particular log that my boss found", "tokens": [407, 562, 286, 390, 257, 3209, 17769, 11403, 11, 321, 632, 257, 1729, 3565, 300, 452, 5741, 1352], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 253, "seek": 78316, "start": 799.0799999999999, "end": 805.1999999999999, "text": " on some of the servers, or router switches, et cetera, and I spent the next three hours", "tokens": [322, 512, 295, 264, 15909, 11, 420, 22492, 19458, 11, 1030, 11458, 11, 293, 286, 4418, 264, 958, 1045, 2496], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 254, "seek": 78316, "start": 805.1999999999999, "end": 807.4399999999999, "text": " logging into each individual one.", "tokens": [27991, 666, 1184, 2609, 472, 13], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 255, "seek": 78316, "start": 807.4399999999999, "end": 811.24, "text": " We had thousands of network devices.", "tokens": [492, 632, 5383, 295, 3209, 5759, 13], "temperature": 0.0, "avg_logprob": -0.13475905212701536, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.743847279722104e-06}, {"id": 256, "seek": 81124, "start": 811.24, "end": 816.44, "text": " So I logged into enough of them, wrote down the logs, and then correlated, and I said,", "tokens": [407, 286, 27231, 666, 1547, 295, 552, 11, 4114, 760, 264, 20820, 11, 293, 550, 38574, 11, 293, 286, 848, 11], "temperature": 0.0, "avg_logprob": -0.17317109579568382, "compression_ratio": 1.5669642857142858, "no_speech_prob": 2.9020557121839374e-06}, {"id": 257, "seek": 81124, "start": 816.44, "end": 824.04, "text": " oh, look, every Thursday and Saturday at the exact same time, this log happens.", "tokens": [1954, 11, 574, 11, 633, 10383, 293, 8803, 412, 264, 1900, 912, 565, 11, 341, 3565, 2314, 13], "temperature": 0.0, "avg_logprob": -0.17317109579568382, "compression_ratio": 1.5669642857142858, "no_speech_prob": 2.9020557121839374e-06}, {"id": 258, "seek": 81124, "start": 824.04, "end": 829.08, "text": " One email later finds out security team is pen testing against us, and we don't need", "tokens": [1485, 3796, 1780, 10704, 484, 3825, 1469, 307, 3435, 4997, 1970, 505, 11, 293, 321, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.17317109579568382, "compression_ratio": 1.5669642857142858, "no_speech_prob": 2.9020557121839374e-06}, {"id": 259, "seek": 81124, "start": 829.08, "end": 830.72, "text": " to worry about it.", "tokens": [281, 3292, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.17317109579568382, "compression_ratio": 1.5669642857142858, "no_speech_prob": 2.9020557121839374e-06}, {"id": 260, "seek": 81124, "start": 830.72, "end": 836.52, "text": " Again, three hours, just trying to correlate what log was happening where, when.", "tokens": [3764, 11, 1045, 2496, 11, 445, 1382, 281, 48742, 437, 3565, 390, 2737, 689, 11, 562, 13], "temperature": 0.0, "avg_logprob": -0.17317109579568382, "compression_ratio": 1.5669642857142858, "no_speech_prob": 2.9020557121839374e-06}, {"id": 261, "seek": 83652, "start": 836.52, "end": 842.6, "text": " So this is exactly what you can avoid by using something like OpenSearch, ElasticSearch,", "tokens": [407, 341, 307, 2293, 437, 291, 393, 5042, 538, 1228, 746, 411, 7238, 10637, 1178, 11, 2699, 2750, 10637, 1178, 11], "temperature": 0.0, "avg_logprob": -0.16324957621466255, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.9939708383608377e-06}, {"id": 262, "seek": 83652, "start": 842.6, "end": 846.0799999999999, "text": " Loki to aggregate your logs.", "tokens": [37940, 281, 26118, 428, 20820, 13], "temperature": 0.0, "avg_logprob": -0.16324957621466255, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.9939708383608377e-06}, {"id": 263, "seek": 83652, "start": 846.0799999999999, "end": 850.24, "text": " And again, if you do want to follow along with this later, this just is a Docker compose", "tokens": [400, 797, 11, 498, 291, 360, 528, 281, 1524, 2051, 365, 341, 1780, 11, 341, 445, 307, 257, 33772, 35925], "temperature": 0.0, "avg_logprob": -0.16324957621466255, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.9939708383608377e-06}, {"id": 264, "seek": 83652, "start": 850.24, "end": 854.6, "text": " file that will let you spin up some sample containers with OpenSearch.", "tokens": [3991, 300, 486, 718, 291, 6060, 493, 512, 6889, 17089, 365, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.16324957621466255, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.9939708383608377e-06}, {"id": 265, "seek": 83652, "start": 854.6, "end": 865.4, "text": " So we'll import logging in our OpenSearch library, create an OpenSearch client, and this is where", "tokens": [407, 321, 603, 974, 27991, 294, 527, 7238, 10637, 1178, 6405, 11, 1884, 364, 7238, 10637, 1178, 6423, 11, 293, 341, 307, 689], "temperature": 0.0, "avg_logprob": -0.16324957621466255, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.9939708383608377e-06}, {"id": 266, "seek": 86540, "start": 865.4, "end": 870.28, "text": " I'm going to break for just a moment and talk about custom handlers.", "tokens": [286, 478, 516, 281, 1821, 337, 445, 257, 1623, 293, 751, 466, 2375, 1011, 11977, 13], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 267, "seek": 86540, "start": 870.28, "end": 873.4, "text": " So we mentioned handlers are where you send your logs.", "tokens": [407, 321, 2835, 1011, 11977, 366, 689, 291, 2845, 428, 20820, 13], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 268, "seek": 86540, "start": 873.4, "end": 876.84, "text": " You can implement custom handlers, believe it or not.", "tokens": [509, 393, 4445, 2375, 1011, 11977, 11, 1697, 309, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 269, "seek": 86540, "start": 876.84, "end": 882.28, "text": " All you need to do is, I was going to say, inherit from, there we go, that's probably", "tokens": [1057, 291, 643, 281, 360, 307, 11, 286, 390, 516, 281, 584, 11, 21389, 490, 11, 456, 321, 352, 11, 300, 311, 1391], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 270, "seek": 86540, "start": 882.28, "end": 885.24, "text": " the right word, logging.handler.", "tokens": [264, 558, 1349, 11, 27991, 13, 5543, 1918, 13], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 271, "seek": 86540, "start": 885.24, "end": 892.8, "text": " Then you create and emit definition, and that needs to have self and record, and that will", "tokens": [1396, 291, 1884, 293, 32084, 7123, 11, 293, 300, 2203, 281, 362, 2698, 293, 2136, 11, 293, 300, 486], "temperature": 0.0, "avg_logprob": -0.12066428774879093, "compression_ratio": 1.646808510638298, "no_speech_prob": 5.8611444728740025e-06}, {"id": 272, "seek": 89280, "start": 892.8, "end": 895.64, "text": " send the record wherever you specify.", "tokens": [2845, 264, 2136, 8660, 291, 16500, 13], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 273, "seek": 89280, "start": 895.64, "end": 900.8399999999999, "text": " So in our case, we have, it's going to take, and it's going to format created time.", "tokens": [407, 294, 527, 1389, 11, 321, 362, 11, 309, 311, 516, 281, 747, 11, 293, 309, 311, 516, 281, 7877, 2942, 565, 13], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 274, "seek": 89280, "start": 900.8399999999999, "end": 906.0, "text": " Also I did not implement the formatting library, because I wanted to send it as a dictionary", "tokens": [2743, 286, 630, 406, 4445, 264, 39366, 6405, 11, 570, 286, 1415, 281, 2845, 309, 382, 257, 25890], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 275, "seek": 89280, "start": 906.0, "end": 908.7199999999999, "text": " to OpenSearch just because that's what it works with.", "tokens": [281, 7238, 10637, 1178, 445, 570, 300, 311, 437, 309, 1985, 365, 13], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 276, "seek": 89280, "start": 908.7199999999999, "end": 914.8399999999999, "text": " You can also use something like FluentDit, FluentBit, or FluentDlogStash to parse out", "tokens": [509, 393, 611, 764, 746, 411, 33612, 317, 35, 270, 11, 33612, 317, 33, 270, 11, 420, 33612, 317, 35, 4987, 4520, 1299, 281, 48377, 484], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 277, "seek": 89280, "start": 914.8399999999999, "end": 918.4799999999999, "text": " your logs later, and we'll talk about that just briefly.", "tokens": [428, 20820, 1780, 11, 293, 321, 603, 751, 466, 300, 445, 10515, 13], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 278, "seek": 89280, "start": 918.4799999999999, "end": 920.28, "text": " So we've got our created time.", "tokens": [407, 321, 600, 658, 527, 2942, 565, 13], "temperature": 0.0, "avg_logprob": -0.1191020627175608, "compression_ratio": 1.6679245283018869, "no_speech_prob": 3.5547304833016824e-06}, {"id": 279, "seek": 92028, "start": 920.28, "end": 930.28, "text": " We've created this wonderful record, OpenSearchClient.index, we'll send that log to OpenSearch.", "tokens": [492, 600, 2942, 341, 3715, 2136, 11, 7238, 10637, 1178, 9966, 1196, 13, 471, 3121, 11, 321, 603, 2845, 300, 3565, 281, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 280, "seek": 92028, "start": 930.28, "end": 932.28, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 281, "seek": 92028, "start": 932.28, "end": 933.28, "text": " And then we'll set it up.", "tokens": [400, 550, 321, 603, 992, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 282, "seek": 92028, "start": 933.28, "end": 936.36, "text": " So we're going to create our logger named log.", "tokens": [407, 321, 434, 516, 281, 1884, 527, 3565, 1321, 4926, 3565, 13], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 283, "seek": 92028, "start": 936.36, "end": 940.88, "text": " We're going to set the logger's level to info so that we get the info records.", "tokens": [492, 434, 516, 281, 992, 264, 3565, 1321, 311, 1496, 281, 13614, 370, 300, 321, 483, 264, 13614, 7724, 13], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 284, "seek": 92028, "start": 940.88, "end": 947.04, "text": " We're going to create the OpenSearchHandler and add logging.info, set that level, and", "tokens": [492, 434, 516, 281, 1884, 264, 7238, 10637, 1178, 39, 474, 1918, 293, 909, 27991, 13, 259, 16931, 11, 992, 300, 1496, 11, 293], "temperature": 0.0, "avg_logprob": -0.1448125927536576, "compression_ratio": 1.7743589743589743, "no_speech_prob": 2.4433650196442613e-06}, {"id": 285, "seek": 94704, "start": 947.04, "end": 950.4, "text": " add our handler.", "tokens": [909, 527, 41967, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 286, "seek": 94704, "start": 950.4, "end": 952.76, "text": " And we're off to the races.", "tokens": [400, 321, 434, 766, 281, 264, 15484, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 287, "seek": 94704, "start": 952.76, "end": 953.76, "text": " Boom.", "tokens": [15523, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 288, "seek": 94704, "start": 953.76, "end": 956.4, "text": " Well, that was kind of anticlimactic.", "tokens": [1042, 11, 300, 390, 733, 295, 49172, 4197, 19892, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 289, "seek": 94704, "start": 956.4, "end": 958.52, "text": " You can't actually see it going into OpenSearch.", "tokens": [509, 393, 380, 767, 536, 309, 516, 666, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 290, "seek": 94704, "start": 958.52, "end": 963.12, "text": " But I promise you, it chugged along and it went into OpenSearch and, ha-za, let's go", "tokens": [583, 286, 6228, 291, 11, 309, 417, 697, 3004, 2051, 293, 309, 1437, 666, 7238, 10637, 1178, 293, 11, 324, 12, 2394, 11, 718, 311, 352], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 291, "seek": 94704, "start": 963.12, "end": 965.3199999999999, "text": " into OpenSearch.", "tokens": [666, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 292, "seek": 94704, "start": 965.3199999999999, "end": 967.64, "text": " And this is actually OpenSearch dashboards.", "tokens": [400, 341, 307, 767, 7238, 10637, 1178, 8240, 17228, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 293, "seek": 94704, "start": 967.64, "end": 974.56, "text": " So again, if you do this on your local machine, passwords admin, usernames admin.", "tokens": [407, 797, 11, 498, 291, 360, 341, 322, 428, 2654, 3479, 11, 33149, 24236, 11, 505, 1248, 1632, 24236, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 294, "seek": 94704, "start": 974.56, "end": 975.56, "text": " Very secure.", "tokens": [4372, 7144, 13], "temperature": 0.0, "avg_logprob": -0.19691453214551582, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.8555725748447003e-06}, {"id": 295, "seek": 97556, "start": 975.56, "end": 979.52, "text": " We're actually looking to change that, but that is coming soon.", "tokens": [492, 434, 767, 1237, 281, 1319, 300, 11, 457, 300, 307, 1348, 2321, 13], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 296, "seek": 97556, "start": 979.52, "end": 986.76, "text": " So we go here and we're going to go into stack management, hang with me for just a second.", "tokens": [407, 321, 352, 510, 293, 321, 434, 516, 281, 352, 666, 8630, 4592, 11, 3967, 365, 385, 337, 445, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 297, "seek": 97556, "start": 986.76, "end": 992.64, "text": " So we created a custom index with this, and that index is named based off of the logger", "tokens": [407, 321, 2942, 257, 2375, 8186, 365, 341, 11, 293, 300, 8186, 307, 4926, 2361, 766, 295, 264, 3565, 1321], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 298, "seek": 97556, "start": 992.64, "end": 993.8, "text": " that sent it.", "tokens": [300, 2279, 309, 13], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 299, "seek": 97556, "start": 993.8, "end": 1002.1999999999999, "text": " So we've got logger name, which was log, and then with the date time so that we can roll", "tokens": [407, 321, 600, 658, 3565, 1321, 1315, 11, 597, 390, 3565, 11, 293, 550, 365, 264, 4002, 565, 370, 300, 321, 393, 3373], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 300, "seek": 97556, "start": 1002.1999999999999, "end": 1005.52, "text": " those off after a certain amount of days.", "tokens": [729, 766, 934, 257, 1629, 2372, 295, 1708, 13], "temperature": 0.0, "avg_logprob": -0.15027481654904923, "compression_ratio": 1.7048458149779735, "no_speech_prob": 2.224579702669871e-06}, {"id": 301, "seek": 100552, "start": 1005.52, "end": 1009.1999999999999, "text": " We're going to create an index pattern.", "tokens": [492, 434, 516, 281, 1884, 364, 8186, 5102, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 302, "seek": 100552, "start": 1009.1999999999999, "end": 1015.56, "text": " And we are going to say, we're going to say absolutely nothing because this is the problem", "tokens": [400, 321, 366, 516, 281, 584, 11, 321, 434, 516, 281, 584, 3122, 1825, 570, 341, 307, 264, 1154], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 303, "seek": 100552, "start": 1015.56, "end": 1018.88, "text": " with doing things like, oh, just kidding, I just don't know how to use OpenSearch.", "tokens": [365, 884, 721, 411, 11, 1954, 11, 445, 9287, 11, 286, 445, 500, 380, 458, 577, 281, 764, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 304, "seek": 100552, "start": 1018.88, "end": 1019.88, "text": " Here we go.", "tokens": [1692, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 305, "seek": 100552, "start": 1019.88, "end": 1020.88, "text": " It's only my job.", "tokens": [467, 311, 787, 452, 1691, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 306, "seek": 100552, "start": 1020.88, "end": 1021.88, "text": " Don't worry.", "tokens": [1468, 380, 3292, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 307, "seek": 100552, "start": 1021.88, "end": 1024.6, "text": " Boss, I swear, if you're watching this, no, I'm just kidding.", "tokens": [15215, 11, 286, 11902, 11, 498, 291, 434, 1976, 341, 11, 572, 11, 286, 478, 445, 9287, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 308, "seek": 100552, "start": 1024.6, "end": 1025.6, "text": " So there we go.", "tokens": [407, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 309, "seek": 100552, "start": 1025.6, "end": 1029.52, "text": " So we have these two indexes that have been created because I was testing yesterday and", "tokens": [407, 321, 362, 613, 732, 8186, 279, 300, 362, 668, 2942, 570, 286, 390, 4997, 5186, 293], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 310, "seek": 100552, "start": 1029.52, "end": 1031.28, "text": " today.", "tokens": [965, 13], "temperature": 0.0, "avg_logprob": -0.13110505044460297, "compression_ratio": 1.688976377952756, "no_speech_prob": 3.3924552553799003e-06}, {"id": 311, "seek": 103128, "start": 1031.28, "end": 1036.04, "text": " So logstar just says, hey, any index that looks like log with anything after it, group", "tokens": [407, 3565, 9710, 445, 1619, 11, 4177, 11, 604, 8186, 300, 1542, 411, 3565, 365, 1340, 934, 309, 11, 1594], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 312, "seek": 103128, "start": 1036.04, "end": 1039.2, "text": " them together.", "tokens": [552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 313, "seek": 103128, "start": 1039.2, "end": 1043.68, "text": " Ask for a time field, which is our created field, and create index pattern.", "tokens": [12320, 337, 257, 565, 2519, 11, 597, 307, 527, 2942, 2519, 11, 293, 1884, 8186, 5102, 13], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 314, "seek": 103128, "start": 1043.68, "end": 1044.84, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 315, "seek": 103128, "start": 1044.84, "end": 1050.72, "text": " OpenSearch, auto-detects, all of these different files, types, et cetera.", "tokens": [7238, 10637, 1178, 11, 8399, 12, 17863, 557, 82, 11, 439, 295, 613, 819, 7098, 11, 3467, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 316, "seek": 103128, "start": 1050.72, "end": 1055.0, "text": " You can actually specify mappings, and I actually really recommend that because different visualizations", "tokens": [509, 393, 767, 16500, 463, 28968, 11, 293, 286, 767, 534, 2748, 300, 570, 819, 5056, 14455], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 317, "seek": 103128, "start": 1055.0, "end": 1058.8, "text": " require different types of mappings, but that is not what we're talking about today.", "tokens": [3651, 819, 3467, 295, 463, 28968, 11, 457, 300, 307, 406, 437, 321, 434, 1417, 466, 965, 13], "temperature": 0.0, "avg_logprob": -0.14482902658396754, "compression_ratio": 1.663003663003663, "no_speech_prob": 1.22852156891895e-06}, {"id": 318, "seek": 105880, "start": 1058.8, "end": 1061.36, "text": " We're talking about Python logs, darn it.", "tokens": [492, 434, 1417, 466, 15329, 20820, 11, 29063, 309, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 319, "seek": 105880, "start": 1061.36, "end": 1068.76, "text": " So we're going to go over to Discover and go into our logs, and we are going to make", "tokens": [407, 321, 434, 516, 281, 352, 670, 281, 40386, 293, 352, 666, 527, 20820, 11, 293, 321, 366, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 320, "seek": 105880, "start": 1068.76, "end": 1070.76, "text": " sure we're looking at today's logs.", "tokens": [988, 321, 434, 1237, 412, 965, 311, 20820, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 321, "seek": 105880, "start": 1070.76, "end": 1073.28, "text": " Actually, well, let's look at this week.", "tokens": [5135, 11, 731, 11, 718, 311, 574, 412, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 322, "seek": 105880, "start": 1073.28, "end": 1074.76, "text": " Here we go.", "tokens": [1692, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 323, "seek": 105880, "start": 1074.76, "end": 1076.48, "text": " And we have three hits.", "tokens": [400, 321, 362, 1045, 8664, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 324, "seek": 105880, "start": 1076.48, "end": 1077.48, "text": " There we are.", "tokens": [821, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 325, "seek": 105880, "start": 1077.48, "end": 1078.48, "text": " These are our logs.", "tokens": [1981, 366, 527, 20820, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 326, "seek": 105880, "start": 1078.48, "end": 1079.48, "text": " Look ma, I'm logging.", "tokens": [2053, 463, 11, 286, 478, 27991, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 327, "seek": 105880, "start": 1079.48, "end": 1082.8799999999999, "text": " And you could actually see the exact point in time when I switch from this is my log", "tokens": [400, 291, 727, 767, 536, 264, 1900, 935, 294, 565, 562, 286, 3679, 490, 341, 307, 452, 3565], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 328, "seek": 105880, "start": 1082.8799999999999, "end": 1084.72, "text": " to look ma, I'm logging.", "tokens": [281, 574, 463, 11, 286, 478, 27991, 13], "temperature": 0.0, "avg_logprob": -0.1567566188301627, "compression_ratio": 1.7161016949152543, "no_speech_prob": 3.905117409885861e-06}, {"id": 329, "seek": 108472, "start": 1084.72, "end": 1092.4, "text": " So that was, yeah, I had 2.37 p.m., so anyhow, with this, you can actually go ahead and", "tokens": [407, 300, 390, 11, 1338, 11, 286, 632, 568, 13, 12851, 280, 13, 76, 7933, 370, 44995, 11, 365, 341, 11, 291, 393, 767, 352, 2286, 293], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 330, "seek": 108472, "start": 1092.4, "end": 1096.28, "text": " then visualize spikes, peaks, valleys, when this is happening.", "tokens": [550, 23273, 28997, 11, 26897, 11, 45614, 11, 562, 341, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 331, "seek": 108472, "start": 1096.28, "end": 1098.28, "text": " You can enrich it with device information.", "tokens": [509, 393, 18849, 309, 365, 4302, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 332, "seek": 108472, "start": 1098.28, "end": 1101.46, "text": " So you can say, hey, when was this log sent?", "tokens": [407, 291, 393, 584, 11, 4177, 11, 562, 390, 341, 3565, 2279, 30], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 333, "seek": 108472, "start": 1101.46, "end": 1103.96, "text": " Is there a particular device that's having a lot of issues?", "tokens": [1119, 456, 257, 1729, 4302, 300, 311, 1419, 257, 688, 295, 2663, 30], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 334, "seek": 108472, "start": 1103.96, "end": 1107.84, "text": " So now instead of logging into all of your servers, you can go and bounce the correct", "tokens": [407, 586, 2602, 295, 27991, 666, 439, 295, 428, 15909, 11, 291, 393, 352, 293, 15894, 264, 3006], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 335, "seek": 108472, "start": 1107.84, "end": 1109.88, "text": " ones.", "tokens": [2306, 13], "temperature": 0.0, "avg_logprob": -0.13179255398837003, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.4430369194305968e-06}, {"id": 336, "seek": 110988, "start": 1109.88, "end": 1115.3600000000001, "text": " And if you saw Doton's presentation yesterday, you'll know you can use this for monitoring", "tokens": [400, 498, 291, 1866, 413, 27794, 311, 5860, 5186, 11, 291, 603, 458, 291, 393, 764, 341, 337, 11028], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 337, "seek": 110988, "start": 1115.3600000000001, "end": 1122.8400000000001, "text": " anything, whether it's CI CD pipelines, Python logs, network logs, et cetera.", "tokens": [1340, 11, 1968, 309, 311, 37777, 6743, 40168, 11, 15329, 20820, 11, 3209, 20820, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 338, "seek": 110988, "start": 1122.8400000000001, "end": 1124.88, "text": " So let's see.", "tokens": [407, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 339, "seek": 110988, "start": 1124.88, "end": 1125.88, "text": " Look at that.", "tokens": [2053, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 340, "seek": 110988, "start": 1125.88, "end": 1128.16, "text": " You're doing more logging now than 99% of the population.", "tokens": [509, 434, 884, 544, 27991, 586, 813, 11803, 4, 295, 264, 4415, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 341, "seek": 110988, "start": 1128.16, "end": 1129.16, "text": " So congratulations.", "tokens": [407, 13568, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 342, "seek": 110988, "start": 1129.16, "end": 1137.16, "text": " Clap your hands for yourselves.", "tokens": [45297, 428, 2377, 337, 14791, 13], "temperature": 0.0, "avg_logprob": -0.14983054569789342, "compression_ratio": 1.4854368932038835, "no_speech_prob": 1.902049916679971e-06}, {"id": 343, "seek": 113716, "start": 1137.16, "end": 1141.28, "text": " So I'm going to talk real quick about just a very simple, common logging architecture", "tokens": [407, 286, 478, 516, 281, 751, 957, 1702, 466, 445, 257, 588, 2199, 11, 2689, 27991, 9482], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 344, "seek": 113716, "start": 1141.28, "end": 1147.3200000000002, "text": " for capturing distributed logs and why you would want to do that.", "tokens": [337, 23384, 12631, 20820, 293, 983, 291, 576, 528, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 345, "seek": 113716, "start": 1147.3200000000002, "end": 1154.96, "text": " So more often than not, you're probably actually going to want to log your, you don't want", "tokens": [407, 544, 2049, 813, 406, 11, 291, 434, 1391, 767, 516, 281, 528, 281, 3565, 428, 11, 291, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 346, "seek": 113716, "start": 1154.96, "end": 1157.76, "text": " to put your logs locally on the file system.", "tokens": [281, 829, 428, 20820, 16143, 322, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 347, "seek": 113716, "start": 1157.76, "end": 1162.76, "text": " And the reason why is because your file system is not going to go down, and God forbid if", "tokens": [400, 264, 1778, 983, 307, 570, 428, 3991, 1185, 307, 406, 516, 281, 352, 760, 11, 293, 1265, 34117, 498], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 348, "seek": 113716, "start": 1162.76, "end": 1166.28, "text": " it goes down, there's no hope for that log getting out anyways.", "tokens": [309, 1709, 760, 11, 456, 311, 572, 1454, 337, 300, 3565, 1242, 484, 13448, 13], "temperature": 0.0, "avg_logprob": -0.14059571352871983, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.272588386011194e-07}, {"id": 349, "seek": 116628, "start": 1166.28, "end": 1170.72, "text": " Your service remotely could disconnect because whether you're doing an upgrade or something", "tokens": [2260, 2643, 20824, 727, 14299, 570, 1968, 291, 434, 884, 364, 11484, 420, 746], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 350, "seek": 116628, "start": 1170.72, "end": 1174.8799999999999, "text": " along those lines, so it acts as a little bit of a caching mechanism.", "tokens": [2051, 729, 3876, 11, 370, 309, 10672, 382, 257, 707, 857, 295, 257, 269, 2834, 7513, 13], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 351, "seek": 116628, "start": 1174.8799999999999, "end": 1178.56, "text": " And then you'll normally have it logged to a file, and then you could use something like", "tokens": [400, 550, 291, 603, 5646, 362, 309, 27231, 281, 257, 3991, 11, 293, 550, 291, 727, 764, 746, 411], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 352, "seek": 116628, "start": 1178.56, "end": 1183.0, "text": " Fluent Bit or Beats, and those will ship your logs out.", "tokens": [33612, 317, 9101, 420, 879, 1720, 11, 293, 729, 486, 5374, 428, 20820, 484, 13], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 353, "seek": 116628, "start": 1183.0, "end": 1187.36, "text": " And the wonderful thing about this architecture is, again, if OpenSearch was to go down because", "tokens": [400, 264, 3715, 551, 466, 341, 9482, 307, 11, 797, 11, 498, 7238, 10637, 1178, 390, 281, 352, 760, 570], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 354, "seek": 116628, "start": 1187.36, "end": 1193.24, "text": " you're doing an update, or if something was to happen critical with your Python app, it", "tokens": [291, 434, 884, 364, 5623, 11, 420, 498, 746, 390, 281, 1051, 4924, 365, 428, 15329, 724, 11, 309], "temperature": 0.0, "avg_logprob": -0.13127193612567448, "compression_ratio": 1.7437722419928825, "no_speech_prob": 1.7878772951007704e-06}, {"id": 355, "seek": 119324, "start": 1193.24, "end": 1198.2, "text": " can quickly write that log out, and you can also do enrichment.", "tokens": [393, 2661, 2464, 300, 3565, 484, 11, 293, 291, 393, 611, 360, 49900, 13], "temperature": 0.0, "avg_logprob": -0.18230095315486827, "compression_ratio": 1.48868778280543, "no_speech_prob": 2.8556746656249743e-06}, {"id": 356, "seek": 119324, "start": 1198.2, "end": 1203.68, "text": " So I talked about getting which server sent that log.", "tokens": [407, 286, 2825, 466, 1242, 597, 7154, 2279, 300, 3565, 13], "temperature": 0.0, "avg_logprob": -0.18230095315486827, "compression_ratio": 1.48868778280543, "no_speech_prob": 2.8556746656249743e-06}, {"id": 357, "seek": 119324, "start": 1203.68, "end": 1211.32, "text": " So Fluent D, Data Prepper, and Log Stash all can take and enrich your logs with the context", "tokens": [407, 33612, 317, 413, 11, 11888, 6001, 3717, 11, 293, 10824, 745, 1299, 439, 393, 747, 293, 18849, 428, 20820, 365, 264, 4319], "temperature": 0.0, "avg_logprob": -0.18230095315486827, "compression_ratio": 1.48868778280543, "no_speech_prob": 2.8556746656249743e-06}, {"id": 358, "seek": 119324, "start": 1211.32, "end": 1213.2, "text": " information that came with them.", "tokens": [1589, 300, 1361, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.18230095315486827, "compression_ratio": 1.48868778280543, "no_speech_prob": 2.8556746656249743e-06}, {"id": 359, "seek": 119324, "start": 1213.2, "end": 1220.52, "text": " So say, for example, it says, I received this log from X, Y, and Z server, 10, 20, 90,", "tokens": [407, 584, 11, 337, 1365, 11, 309, 1619, 11, 286, 4613, 341, 3565, 490, 1783, 11, 398, 11, 293, 1176, 7154, 11, 1266, 11, 945, 11, 4289, 11], "temperature": 0.0, "avg_logprob": -0.18230095315486827, "compression_ratio": 1.48868778280543, "no_speech_prob": 2.8556746656249743e-06}, {"id": 360, "seek": 122052, "start": 1220.52, "end": 1223.48, "text": " 32, 83, or something like that.", "tokens": [8858, 11, 30997, 11, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 361, "seek": 122052, "start": 1223.48, "end": 1228.24, "text": " Then it can go and do a reverse DNS lookup and say, hey, who has that?", "tokens": [1396, 309, 393, 352, 293, 360, 257, 9943, 35153, 574, 1010, 293, 584, 11, 4177, 11, 567, 575, 300, 30], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 362, "seek": 122052, "start": 1228.24, "end": 1230.44, "text": " Which service is assigned to that?", "tokens": [3013, 2643, 307, 13279, 281, 300, 30], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 363, "seek": 122052, "start": 1230.44, "end": 1233.8, "text": " And then it can add that information in and then push it into OpenSearch.", "tokens": [400, 550, 309, 393, 909, 300, 1589, 294, 293, 550, 2944, 309, 666, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 364, "seek": 122052, "start": 1233.8, "end": 1240.16, "text": " So now you've all of a sudden gone from having a log that says, hello world, to or got here,", "tokens": [407, 586, 291, 600, 439, 295, 257, 3990, 2780, 490, 1419, 257, 3565, 300, 1619, 11, 7751, 1002, 11, 281, 420, 658, 510, 11], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 365, "seek": 122052, "start": 1240.16, "end": 1243.16, "text": " please, please, do not.", "tokens": [1767, 11, 1767, 11, 360, 406, 13], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 366, "seek": 122052, "start": 1243.16, "end": 1247.72, "text": " And you can have it pushed into OpenSearch, know what service is causing the issues, and", "tokens": [400, 291, 393, 362, 309, 9152, 666, 7238, 10637, 1178, 11, 458, 437, 2643, 307, 9853, 264, 2663, 11, 293], "temperature": 0.0, "avg_logprob": -0.1656655788421631, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.038892600670806e-06}, {"id": 367, "seek": 124772, "start": 1247.72, "end": 1250.88, "text": " visualize on dashboards.", "tokens": [23273, 322, 8240, 17228, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 368, "seek": 124772, "start": 1250.88, "end": 1252.6000000000001, "text": " With that, I'm finished.", "tokens": [2022, 300, 11, 286, 478, 4335, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 369, "seek": 124772, "start": 1252.6000000000001, "end": 1255.08, "text": " Please scan, look at OpenSearch if you're curious.", "tokens": [2555, 11049, 11, 574, 412, 7238, 10637, 1178, 498, 291, 434, 6369, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 370, "seek": 124772, "start": 1255.08, "end": 1258.2, "text": " It is open source, Apache 2 licensed.", "tokens": [467, 307, 1269, 4009, 11, 46597, 568, 25225, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 371, "seek": 124772, "start": 1258.2, "end": 1260.96, "text": " All of our features are being developed in the open.", "tokens": [1057, 295, 527, 4122, 366, 885, 4743, 294, 264, 1269, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 372, "seek": 124772, "start": 1260.96, "end": 1265.04, "text": " And with that, I want to ask it, does anyone have any questions?", "tokens": [400, 365, 300, 11, 286, 528, 281, 1029, 309, 11, 775, 2878, 362, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 373, "seek": 124772, "start": 1265.04, "end": 1266.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2697389578517479, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.4256023607449606e-05}, {"id": 374, "seek": 126604, "start": 1266.04, "end": 1278.04, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 375, "seek": 126604, "start": 1278.04, "end": 1280.6399999999999, "text": " Hello?", "tokens": [2425, 30], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 376, "seek": 126604, "start": 1280.6399999999999, "end": 1281.76, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 377, "seek": 126604, "start": 1281.76, "end": 1283.1599999999999, "text": " How would you...", "tokens": [1012, 576, 291, 485], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 378, "seek": 126604, "start": 1283.1599999999999, "end": 1287.8, "text": " I'm mostly familiar with Sentry, and I'm very curious how would you compare this to that,", "tokens": [286, 478, 5240, 4963, 365, 23652, 627, 11, 293, 286, 478, 588, 6369, 577, 576, 291, 6794, 341, 281, 300, 11], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 379, "seek": 126604, "start": 1287.8, "end": 1291.68, "text": " because as far as my friends told me, they're pretty different products, but they do similar", "tokens": [570, 382, 1400, 382, 452, 1855, 1907, 385, 11, 436, 434, 1238, 819, 3383, 11, 457, 436, 360, 2531], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 380, "seek": 126604, "start": 1291.68, "end": 1292.68, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 381, "seek": 126604, "start": 1292.68, "end": 1293.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.37762333595589415, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.001108034746721387}, {"id": 382, "seek": 129368, "start": 1293.68, "end": 1296.04, "text": " So Sentry is more of an APM, isn't it?", "tokens": [407, 23652, 627, 307, 544, 295, 364, 5372, 44, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 383, "seek": 129368, "start": 1296.04, "end": 1297.04, "text": " Is that word familiar?", "tokens": [1119, 300, 1349, 4963, 30], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 384, "seek": 129368, "start": 1297.04, "end": 1298.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 385, "seek": 129368, "start": 1298.04, "end": 1303.6000000000001, "text": " So OpenSearch has some APM capabilities, which is, I don't remember the word for it, but", "tokens": [407, 7238, 10637, 1178, 575, 512, 5372, 44, 10862, 11, 597, 307, 11, 286, 500, 380, 1604, 264, 1349, 337, 309, 11, 457], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 386, "seek": 129368, "start": 1303.6000000000001, "end": 1306.64, "text": " it's for app monitoring and specific to the application.", "tokens": [309, 311, 337, 724, 11028, 293, 2685, 281, 264, 3861, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 387, "seek": 129368, "start": 1306.64, "end": 1310.88, "text": " So this has some APM capabilities.", "tokens": [407, 341, 575, 512, 5372, 44, 10862, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 388, "seek": 129368, "start": 1310.88, "end": 1315.8400000000001, "text": " It might not go as deep as some of your auto configurations for other APM tools though,", "tokens": [467, 1062, 406, 352, 382, 2452, 382, 512, 295, 428, 8399, 31493, 337, 661, 5372, 44, 3873, 1673, 11], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 389, "seek": 129368, "start": 1315.8400000000001, "end": 1317.6000000000001, "text": " but it can ingest APM logs.", "tokens": [457, 309, 393, 3957, 377, 5372, 44, 20820, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 390, "seek": 129368, "start": 1317.6000000000001, "end": 1319.68, "text": " So that's a good question.", "tokens": [407, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 391, "seek": 129368, "start": 1319.68, "end": 1320.8400000000001, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19432863375035728, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.241006824187934e-05}, {"id": 392, "seek": 132084, "start": 1320.84, "end": 1323.6799999999998, "text": " You had two async questions from the chats.", "tokens": [509, 632, 732, 382, 34015, 1651, 490, 264, 38057, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 393, "seek": 132084, "start": 1323.6799999999998, "end": 1328.84, "text": " The first one is, what about f-strings and logging?", "tokens": [440, 700, 472, 307, 11, 437, 466, 283, 12, 50035, 293, 27991, 30], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 394, "seek": 132084, "start": 1328.84, "end": 1333.76, "text": " Because I wrote this presentation in like a couple hours, and yeah.", "tokens": [1436, 286, 4114, 341, 5860, 294, 411, 257, 1916, 2496, 11, 293, 1338, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 395, "seek": 132084, "start": 1333.76, "end": 1339.28, "text": " So I'm trying to modernize, but again, it's old habits die hard, so I'm still using f-strings.", "tokens": [407, 286, 478, 1382, 281, 4363, 1125, 11, 457, 797, 11, 309, 311, 1331, 14100, 978, 1152, 11, 370, 286, 478, 920, 1228, 283, 12, 50035, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 396, "seek": 132084, "start": 1339.28, "end": 1341.04, "text": " Please don't get on me.", "tokens": [2555, 500, 380, 483, 322, 385, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 397, "seek": 132084, "start": 1341.04, "end": 1344.4399999999998, "text": " It was the next one, sorry.", "tokens": [467, 390, 264, 958, 472, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 398, "seek": 132084, "start": 1344.4399999999998, "end": 1345.4399999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24305115532629268, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.00023254039115272462}, {"id": 399, "seek": 134544, "start": 1345.44, "end": 1351.2, "text": " The question is, what about structured logging, and in particular, strike log?", "tokens": [440, 1168, 307, 11, 437, 466, 18519, 27991, 11, 293, 294, 1729, 11, 9302, 3565, 30], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 400, "seek": 134544, "start": 1351.2, "end": 1352.88, "text": " I'm not actually familiar with strut log.", "tokens": [286, 478, 406, 767, 4963, 365, 1056, 325, 3565, 13], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 401, "seek": 134544, "start": 1352.88, "end": 1358.6000000000001, "text": " So I'm actually moving myself more towards using OpenTelemetry instead of logging, or", "tokens": [407, 286, 478, 767, 2684, 2059, 544, 3030, 1228, 7238, 14233, 306, 5537, 627, 2602, 295, 27991, 11, 420], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 402, "seek": 134544, "start": 1358.6000000000001, "end": 1360.2, "text": " alongside logging, we'll say.", "tokens": [12385, 27991, 11, 321, 603, 584, 13], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 403, "seek": 134544, "start": 1360.2, "end": 1366.44, "text": " So OpenTelemetry gives you a trace, actually, which can tell you the full stack of what", "tokens": [407, 7238, 14233, 306, 5537, 627, 2709, 291, 257, 13508, 11, 767, 11, 597, 393, 980, 291, 264, 1577, 8630, 295, 437], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 404, "seek": 134544, "start": 1366.44, "end": 1368.48, "text": " happened during your application.", "tokens": [2011, 1830, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 405, "seek": 134544, "start": 1368.48, "end": 1375.4, "text": " So everything down from which function was called to which load balancer sent the information", "tokens": [407, 1203, 760, 490, 597, 2445, 390, 1219, 281, 597, 3677, 3119, 28347, 2279, 264, 1589], "temperature": 0.0, "avg_logprob": -0.1749998041101404, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.279069202719256e-05}, {"id": 406, "seek": 137540, "start": 1375.4, "end": 1376.4, "text": " over.", "tokens": [670, 13], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 407, "seek": 137540, "start": 1376.4, "end": 1379.92, "text": " So you get an end-to-end trace of what happened, which in my opinion, I think, is a little", "tokens": [407, 291, 483, 364, 917, 12, 1353, 12, 521, 13508, 295, 437, 2011, 11, 597, 294, 452, 4800, 11, 286, 519, 11, 307, 257, 707], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 408, "seek": 137540, "start": 1379.92, "end": 1382.2800000000002, "text": " bit more handy than just logs.", "tokens": [857, 544, 13239, 813, 445, 20820, 13], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 409, "seek": 137540, "start": 1382.2800000000002, "end": 1390.64, "text": " I think we have one more question in the middle.", "tokens": [286, 519, 321, 362, 472, 544, 1168, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 410, "seek": 137540, "start": 1390.64, "end": 1394.0400000000002, "text": " Just a quick question from the discussion I'm having with the code worker.", "tokens": [1449, 257, 1702, 1168, 490, 264, 5017, 286, 478, 1419, 365, 264, 3089, 11346, 13], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 411, "seek": 137540, "start": 1394.0400000000002, "end": 1399.52, "text": " So we're thinking about moving our logs to JSON format, because it's easy to understand", "tokens": [407, 321, 434, 1953, 466, 2684, 527, 20820, 281, 31828, 7877, 11, 570, 309, 311, 1858, 281, 1223], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 412, "seek": 137540, "start": 1399.52, "end": 1402.96, "text": " for non-Python people and searchable.", "tokens": [337, 2107, 12, 47, 88, 11943, 561, 293, 3164, 712, 13], "temperature": 0.0, "avg_logprob": -0.16375399998256138, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.7781144278123975e-05}, {"id": 413, "seek": 140296, "start": 1402.96, "end": 1407.56, "text": " If we were to switch to OpenSearch, and I really liked the presentation, do you think", "tokens": [759, 321, 645, 281, 3679, 281, 7238, 10637, 1178, 11, 293, 286, 534, 4501, 264, 5860, 11, 360, 291, 519], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 414, "seek": 140296, "start": 1407.56, "end": 1414.6000000000001, "text": " it's still feasible to make logs searchable in and of itself, or is OpenSearch that's", "tokens": [309, 311, 920, 26648, 281, 652, 20820, 3164, 712, 294, 293, 295, 2564, 11, 420, 307, 7238, 10637, 1178, 300, 311], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 415, "seek": 140296, "start": 1414.6000000000001, "end": 1420.44, "text": " stable and usable and from your experience, there's no need to search on your logs?", "tokens": [8351, 293, 29975, 293, 490, 428, 1752, 11, 456, 311, 572, 643, 281, 3164, 322, 428, 20820, 30], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 416, "seek": 140296, "start": 1420.44, "end": 1421.44, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 417, "seek": 140296, "start": 1421.44, "end": 1422.44, "text": " No, that's a great question.", "tokens": [883, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 418, "seek": 140296, "start": 1422.44, "end": 1424.1200000000001, "text": " You do need to search your logs.", "tokens": [509, 360, 643, 281, 3164, 428, 20820, 13], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 419, "seek": 140296, "start": 1424.1200000000001, "end": 1428.56, "text": " OpenSearch is a search engine at its core, and that is why it is as good as it is with", "tokens": [7238, 10637, 1178, 307, 257, 3164, 2848, 412, 1080, 4965, 11, 293, 300, 307, 983, 309, 307, 382, 665, 382, 309, 307, 365], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 420, "seek": 140296, "start": 1428.56, "end": 1430.2, "text": " logs.", "tokens": [20820, 13], "temperature": 0.0, "avg_logprob": -0.14436070124308267, "compression_ratio": 1.7627118644067796, "no_speech_prob": 3.401857611606829e-05}, {"id": 421, "seek": 143020, "start": 1430.2, "end": 1435.88, "text": " As for JSON versus other formats, I think there's no particular preference, but OpenSearch", "tokens": [1018, 337, 31828, 5717, 661, 25879, 11, 286, 519, 456, 311, 572, 1729, 17502, 11, 457, 7238, 10637, 1178], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 422, "seek": 143020, "start": 1435.88, "end": 1436.88, "text": " is certainly stable.", "tokens": [307, 3297, 8351, 13], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 423, "seek": 143020, "start": 1436.88, "end": 1441.56, "text": " We have 150 million downloads, so we are here to stay.", "tokens": [492, 362, 8451, 2459, 36553, 11, 370, 321, 366, 510, 281, 1754, 13], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 424, "seek": 143020, "start": 1441.56, "end": 1447.8400000000001, "text": " It's been adopted by a lot of companies such as Oracle, Ivan, Instacluster, Opster, Amazon", "tokens": [467, 311, 668, 12175, 538, 257, 688, 295, 3431, 1270, 382, 25654, 11, 28893, 11, 2730, 326, 75, 8393, 11, 12011, 3120, 11, 6795], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 425, "seek": 143020, "start": 1447.8400000000001, "end": 1452.4, "text": " Web Services, of course, because I work there, and many others.", "tokens": [9573, 12124, 11, 295, 1164, 11, 570, 286, 589, 456, 11, 293, 867, 2357, 13], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 426, "seek": 143020, "start": 1452.4, "end": 1457.96, "text": " So I would say it is very stable, production-ready to use, and yeah, it's a really great way", "tokens": [407, 286, 576, 584, 309, 307, 588, 8351, 11, 4265, 12, 1201, 281, 764, 11, 293, 1338, 11, 309, 311, 257, 534, 869, 636], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 427, "seek": 143020, "start": 1457.96, "end": 1458.96, "text": " to search your logs.", "tokens": [281, 3164, 428, 20820, 13], "temperature": 0.0, "avg_logprob": -0.09905734856923422, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.352550149633316e-06}, {"id": 428, "seek": 145896, "start": 1458.96, "end": 1464.48, "text": " In fact, I have a lightning talk at 1655 in another room, so I think it's a Kubernetes", "tokens": [682, 1186, 11, 286, 362, 257, 16589, 751, 412, 3165, 13622, 294, 1071, 1808, 11, 370, 286, 519, 309, 311, 257, 23145], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 429, "seek": 145896, "start": 1464.48, "end": 1465.48, "text": " room.", "tokens": [1808, 13], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 430, "seek": 145896, "start": 1465.48, "end": 1468.16, "text": " So if you want to talk more about searching your logs, I'm going to be talking about search", "tokens": [407, 498, 291, 528, 281, 751, 544, 466, 10808, 428, 20820, 11, 286, 478, 516, 281, 312, 1417, 466, 3164], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 431, "seek": 145896, "start": 1468.16, "end": 1469.16, "text": " relevance for logs.", "tokens": [32684, 337, 20820, 13], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 432, "seek": 145896, "start": 1469.16, "end": 1470.16, "text": " So thank you.", "tokens": [407, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 433, "seek": 145896, "start": 1470.16, "end": 1471.16, "text": " Thank you, online people.", "tokens": [1044, 291, 11, 2950, 561, 13], "temperature": 0.0, "avg_logprob": -0.21829243449421673, "compression_ratio": 1.5, "no_speech_prob": 4.66062811028678e-05}, {"id": 434, "seek": 147116, "start": 1471.16, "end": 1488.88, "text": " Thanks a lot.", "tokens": [50364, 2561, 257, 688, 13, 51250], "temperature": 0.0, "avg_logprob": -0.46464071954999653, "compression_ratio": 0.6190476190476191, "no_speech_prob": 0.003173217875882983}], "language": "en"}