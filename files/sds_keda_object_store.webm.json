{"text": " So, good doing everyone. Before going to my presentation, I just want to thank Jan and Nils. I guess these folks are organizing the show at the room for last six years or seven. So, yeah, today I hope I'm audible and yeah, the network is, yeah, sure. So, today I'm going to talk about autoscaling feature in Kubernetes and how it can be done if using CADA for RGW specifically. So, most of the presentation covered Sef and Rook and last presentation from Gaurav and Alexander was about Rook. So, it's bit advanced topic over that. So, myself, Jifin Tonythotta and I am working as backend engineer in IBM storage and I work closely with Rook and Sef community. So, as I mentioned before, like most of the talks already covered Sef and Rook topics. So, I mostly focus on CADA. So, my section covers what is CADA and basic CADA concepts. Then, a brief thing about Rook operator and finally, a demo how the autoscaling works. So, what is CADA? So, in the last presentation, one of the things Alexander mentioned, you can change the configuration, right, if you are in the deployment. With help of autoscaling, like, you don't want to change it. Like, it basically, the autoscaler will find and scale those for you. And, Kubernetes inbuilt have the HPA and VPS scalers. So, CADA is kind of a bit, what you say, advanced version for the HPA. So, what CADA does? It makes the Kubernetes even driving autoscaling that symbol. I don't know if you folks use HPA. So, HPA by default support, CPU and memory kind of scale. Like, if your port is using most, lot of CPUs or something like that, it will scale or if you use memory, it will scale. But, it also supports custom metrics. And, CADA, and the one of the issues with the custom metrics, it does not have a standard version for that. Like, even Prometheus has an implementation, I mean, Kubernetes has an implementation, but nothing is standardized. So, this is where CADA comes. And, it started as a partnership between Microsoft and other that, and it was donated to CNCF. And, during the two-point-o version, like, it may be, go through a major design change and all that stuff. And, last couple of years back, like, it got become a, like, incubation project. And, currently, like, it's on the 2.9 release, like, that's the latest version. And, I hope the next major change will happen in the third version coming years. Now, going a bit about CADA concepts. So, as I mentioned, it automatically scales the Kubernetes resources, like deployments, or stateful sets, like replica, or even customer sources. Then, it has, like, inbuilt 50-scalers, like, like a plug-in which we can attach. Like, for example, you have Prometheus, or like Kafka, and RabbitMQ, AWS, and Azure, all those big players are there. Then, other thing is, it just scales the resources. It does not manipulate your data. Then, the scaling is done on the event basis. So, it doesn't want anything to do with your data. Like, it won't manipulate your, like, your side unit or something like that. It just scales based on the events which it's facing. And, you can install CADA via OLM or Helm, whatever. Now, this is basic architecture with CADA. So, CADA just enhances the HPF feature of the Kubernetes. So, you need to have the Kubernetes cluster. Then, there's an, on the bottom side, you can see an external source, like, which you, which you have given the information about, like, what resources, or, like, what even you need to scale. And, how the, like, how the deployment will scale, it based on a customer source, not a scaled object or a scaled job. Now, the scaled object will be created by CADA. And, CADA has, like, the three components, like, one is a scalar, like, as I mentioned, like, there are a lot of scalers, like, it's a plug-in for CADA. Then, you have the CADA controller, basically, manage the CADA, I mean, CADA services and demons. Then, you have the metrics adapter. So, the custom, like, in the case of HPA, the custom metrics is driven by a metrics adapter, like, you need to provide a metrics adapter for the custom scaling. So, CADA itself brings up a metrics adapter for that. Now, you have your workload, and after that, based on the events, it may increase your deployment or it may decrease your deployment. And, it does it with the help of HPA, like, even though you define a scaled object or a scaled job, then, in general, it creates an HPA. Now, one of the differences is that, like, it can scale down to zero, like, normally, with HPA, the scaling starts on one or, like, and it ends on the maximum which you have defined. Then, yeah, that's all. And, the metrics adapter is covered. So, yeah, the custom metrics is provided by the auto-scaler, like, provided to auto-scaler by this metrics adapter. Now, I am just giving an example about scaled objects, because that's what I'm going to cover into this presentation. So, basically, it has a name, the metadata information. Then, the, you can see the target of, which will mention what type of resource you want to scale. Like, by default, it will be deployment. But, you can also add the replica sets or stateful resources. Then, if you have a customer resource with a scale defined, you can also define that. So, these, then, you need to mention the type of resource. Like, by default, if you give a name, it will be deployment. Then, you can mention about the Plick account. Then, you can give Triggers. So, Triggers is an event which, the based on scaling will happen and you can define multiple Triggers as well. So, so far, any questions or, like, okay, I'll move forward. Now, I am just mentioning a few, most of them I already covered, like, CADAP features. It can scale down to zero if you want. Another part is, like, if there's a failback, the Plick account, if something happens to the cluster, you can failback to one number. So, it's not min or max. You can define a failback value. Maybe, say, your min is three and your five is your max and you can set a failback one, in some of the cases, if you want to failback two. It can really, say, like, pose for autoscaling. Like, you can start and stop. Like, you don't delete those sources. You can just, I mean, you don't want to delete the scaled object or something like that. You can keep those sources and you can just do a pose. Then, CADAP, by default, can expose the Prometheus metrics in this adapter as well as the Kafka events. And one thing is, like, you can also use secure connections, like, potentials in things, like, it can be defined by another subclass or, like, a subsection in the scaled, I mean, in the scale, like, in the Prometheus, I mean, the type, in the scalar type, you can mention a subclass about the trigger authentication, which you'll refer, like, how you can authenticate with the server. Now, today, in the latest version, even, you can have the events or metrics from the GRPC or, like, from the other JPA, but I have never tried or I have never used those things. Now, coming to RGW. So, RGBL case is very simple case for CADAP. First of all, it just proves through the Wootkend RGW stuff, like, so, in the last presentation, they mentioned Wootkiston orchestrator, which conducts the self storage and it simplifies the deployment and management services for the self cluster. Now, here, for RGW, like, the access can be given as an OBCs, like, something similar to PBO PVC for the fly and file and block. Other source is known as self-object storage. So, OBC, like, you will get a bucket, but in case of self-object storage, like, you will get the entire user credential, too. So, you can create multiple buckets and all those features can be done. Then, other part is, like, Wootk also have a service monitor. So, if you're familiar with Pomerateus and all, like, if Pomerateus want to fetch the metrics from your, so, I mean, your DMN, like, you need to have a service monitor. So, what the service monitor does, the self-manager supports the metrics and this metrics will be passed to the Pomerateus service with the help of the service monitor. Now, for my test case, I am using HS bench. So, it's a performance-evaluating tool for S3 workloads. So, yeah, that will be tool, like, that will be S3 client, which I will be using for HW. Now, yeah, so, in the demo, I cover, like, the Pomerateus scale I will be using and the self-cluster will be already deployed by a hook and HW is configured. Also, the Pomerateus server is up and have defined the Pomerateus, like, the requirements for service monitors and all those stuff. Then, I need to define a scaled object so that my HW can scale. And the scaling is based on the metrics provided by the manager. So, for HW, most of the metrics are related to performance count. It's based on the how many requests we are receiving on the HW server. It's nothing depends on the backend or something like that. So, that's one thing which we may need to change. But currently, it's like a web server, like, when you are getting the request based on request a lot, like, the scaling will happen. Now, I will go to the demo. So, okay. I hope it's visible. So, I am running a mini cube cluster for my demo purpose, like, everything is up and running. And I already installed the look cluster. So, you can see look operator is running. Then, HW is also running. Currently, I have only one HW on my cluster. Then, this is the service monitor, like, the format is, which look deploys. Then, if you check the services, there are two HW service, like, one is the internal service, which can be accessed for the humanities workloads. And for my HS1, I need to expose the HW service. Hence, I am using the channel HW service. It's just, it's an old port, like, it will just expose the HW service on the remission. Then, yeah, there is also the Udpometheus service as well. So, I have created SF subject show user, and I need to pass these credentials for my demo, I mean, for my workload. This is the Pometheus operator running on the default name space. Now, I will install the CADA via Helm. It's deployed. Just checking the ports are up and running. It's nothing fancy. So, everything is up and running. Now, I need to define the scaled object to source for HW. Sorry. I'm just taking the Pometheus web's console UI. So, I am doing the scaling based on the request, like, SFHJ request. So, just showing the current value. That's it. Yeah. Now, I need to define the scaled object. So, so, this is the, in the GitHub repo, like, this example is the lamified is the, so, I have given the name for my scaled object, and I am doing the scale, like, on the scaling, on the deployment, on the SF object show, like, HW show, set the minimum of pick account 1 and maximum 3. Then, this is the Pometheus endpoint, I mean, the metrics endpoint, which I need to fetch the metrics, and this is the metrics name which I am giving, like, so, this is based on, this is a, like, definition for the triggers, nothing else, and the threshold value is 10. So, basically, it's 10 million requests, not normal 10. I mean, now, the scaled object is created. If you check for the scaled object and HPA, you can, we have defined the scaled object, and HPA is automatically created. So, this is a scaled object, and yeah, it is not active, that's why the state is unknown, and you have not defined the fallback, that's why, but it's ready for scaling. If you go for the, so, for the scaled object, and HPA will be triggered internally. So, this is the current load on the RW deployment, and current status, like, you can see them in NMAX and the pick accounts. Now, I am doing a watch on the ports, and the scaled object and HPA. So, yeah. Now, I am triggering the load. So, for that, I need to fetch the credential from the subfuser. So, just getting the access key and secret key. So, I am triggering the HS bench. So, HS bench has the access key, so you can see the endpoint details. Then, currently, I am running a load of, like, so, this is, 1 mb is the size of the object, and 1 dollar tree, I mean, 1 hierarchy. This is 10 clients it will be running, and 1000 objects. This is triggered. Now, on the left-hand side, like, still the, like, it may take some time to flatten that side, like, the watch part. So, you can see the load is increasing. So, it is still not reaching that 10 million, as I mentioned before. Like, if it costs 10,000 or something like that, then it triggers the scaling. So, still I have one port. Yeah. Now, it costs the limit. So, if you look closely, like, so, there is a kind of, like, what you said. School of period or something like that, it will wait for some time, 90 seconds or something like that before scaling, then only the scale will happen. As you can see, like, after costing the limit, it just scaled one. Now, it will scale again, still increasing the two ports is not satisfying the request. So, it scaled again. Now, the workload become bit less. Still, it is about 10 million requests. So, now, if I execute here on the probability server, like, you can see three requests, three instances providing the same request. Another fourth port is up. That is a four instances, like, if I go back to the terminal, yeah. So, you can see a bit decrease in the load, but, yeah, the load is never become less than 10. That says it was still increasing. So, yeah, I guess that is all I have and I do not have the, like, the scaling down part, like, before that it was taking a lot of time. So, that is it. Any questions or, like, what is the use case of scaling down to zero? So, I have a question. The question is, what is the use case of scaling down to zero? Maybe you can save those sources or see if you are not using that service up. It depends on you, like, if you want to defend zero, like, then it will, if it is ideal, it will scale to down to zero. That is it. But will it then scale up fast enough? Yeah, if it causes the threshold is coming up, I have played zero. So, I do not know whether, whether RGW will have bug because if you scale down, then the server is not the right for RGW. So, I am not sure whether it will work for RGW. But, yeah, majorly, it will save those sources, if you have anything else. Yeah, sure. With something like that, work for scaling OSDs? OSDs, I am not quite sure because OSD has the dependency of hard disk. So, I do not know how it will play in case of OSDs. But it can obviously work for NFS or it can work for MDSs. Scaling OSDs is very expensive. You need to move better. It does not fit this, this method of scaling up and down or momentarily, you think. If you already moved it to OSD, you need a big event to move it down. Just whether that would be more of an upgrade for the server. I think the usual argument for OSDs is that you have to have hard drives, like, in storage, to put OSDs on, and then you might as well deploy them right away, because then the server will operate better. But in public cloud, it is different. And then it is cost. You do not want to scale up because it is cost you more and you want to do it in the last minute. So, it makes sense, but not using this. And the problem scaling down OSDs is also not a simple task. You need to do it all in a manner, otherwise you will be at risk of errors, trying to take it down. I try to write once the process how to, in the public cloud, how to scale down OSDs, scale out OSDs. It will document the speech of something like 45 pages. If you want to do it in a safe manner, it is not simple. So, the question is whether we can use CADA for Cepheidium, something like that. So, I guess Cepheidium mostly works with Portman. It does not need the Kubernetes, and this is specific to Kubernetes. No, not with CADA. Maybe if we have defined something for the Cepheidium step, then yeah, that is like, based on like, we need to, yeah, like, we need to see the scalar like, based on like, if this is a question that we had, hit this request, then Cepheidium can trigger or Ingressor, etc. That is possible, but not with CADA. Kubernetes, yeah. Okay, yeah, sure. Is there anything similar for tuning, for suggesting tuning? Sorry. So, the question is anything? So, I think he's asking if there is, I mean, if you can scale or, I mean, if I understood correctly, you can suggest configuration tuning based on the workload. So, the tuning comes from the, like, kind of a YAML, like, which preferred YAML you need to use or something like that. It's kind of a data based on the scaling, right? And, like, it's, I don't know, HPA has some mechanism, like, it's based, the scaling is based on HPA only. So, HPA does not look up for the configuration. It just look up for the deployment, like, how the deployment see, just see the counts. About tuning, right, a little long topic. But I recently read a research paper where Luster was doing machine learning performance analysis based on ML and AM workloads and suggesting what configuration is that you are doing. There was some, I think I shared that research paper in this performance weekly once and shared that research paper. But this is a really nice idea that, I mean, based on AI, I mean, you use AI as models to list the workload and everything, and just configuration tuning that I want to, in this F cluster. Okay. Discussion was, it's more of a, the discussion more of, like, to which I have a performance weekly discussion. We have more discussion in the upstream community meetings or in the network. We could work for them for that. Sounds fun initially, but I don't know how much Okay. So, thank you, folks. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.72, "text": " So, good doing everyone. Before going to my presentation, I just want to thank Jan and", "tokens": [50364, 407, 11, 665, 884, 1518, 13, 4546, 516, 281, 452, 5860, 11, 286, 445, 528, 281, 1309, 4956, 293, 51250], "temperature": 0.0, "avg_logprob": -0.4950434643289317, "compression_ratio": 1.3014705882352942, "no_speech_prob": 0.36416471004486084}, {"id": 1, "seek": 0, "start": 17.72, "end": 27.2, "text": " Nils. I guess these folks are organizing the show at the room for last six years or seven.", "tokens": [51250, 426, 4174, 13, 286, 2041, 613, 4024, 366, 17608, 264, 855, 412, 264, 1808, 337, 1036, 2309, 924, 420, 3407, 13, 51724], "temperature": 0.0, "avg_logprob": -0.4950434643289317, "compression_ratio": 1.3014705882352942, "no_speech_prob": 0.36416471004486084}, {"id": 2, "seek": 2720, "start": 27.2, "end": 39.2, "text": " So, yeah, today I hope I'm audible and yeah, the network is, yeah, sure. So, today I'm going", "tokens": [50364, 407, 11, 1338, 11, 965, 286, 1454, 286, 478, 41317, 293, 1338, 11, 264, 3209, 307, 11, 1338, 11, 988, 13, 407, 11, 965, 286, 478, 516, 50964], "temperature": 0.0, "avg_logprob": -0.40720461527506513, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.07495594024658203}, {"id": 3, "seek": 2720, "start": 39.2, "end": 47.84, "text": " to talk about autoscaling feature in Kubernetes and how it can be done if using CADA for RGW", "tokens": [50964, 281, 751, 466, 1476, 10466, 4270, 4111, 294, 23145, 293, 577, 309, 393, 312, 1096, 498, 1228, 41143, 32, 337, 497, 38, 54, 51396], "temperature": 0.0, "avg_logprob": -0.40720461527506513, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.07495594024658203}, {"id": 4, "seek": 2720, "start": 47.84, "end": 54.480000000000004, "text": " specifically. So, most of the presentation covered Sef and Rook and last presentation", "tokens": [51396, 4682, 13, 407, 11, 881, 295, 264, 5860, 5343, 1100, 69, 293, 497, 1212, 293, 1036, 5860, 51728], "temperature": 0.0, "avg_logprob": -0.40720461527506513, "compression_ratio": 1.4808743169398908, "no_speech_prob": 0.07495594024658203}, {"id": 5, "seek": 5448, "start": 54.48, "end": 61.279999999999994, "text": " from Gaurav and Alexander was about Rook. So, it's bit advanced topic over that. So,", "tokens": [50364, 490, 460, 3463, 706, 293, 14845, 390, 466, 497, 1212, 13, 407, 11, 309, 311, 857, 7339, 4829, 670, 300, 13, 407, 11, 50704], "temperature": 0.0, "avg_logprob": -0.2727497061904596, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.023300884291529655}, {"id": 6, "seek": 5448, "start": 61.279999999999994, "end": 68.28, "text": " myself, Jifin Tonythotta and I am working as backend engineer in IBM storage and I work", "tokens": [50704, 2059, 11, 508, 351, 259, 10902, 392, 22967, 293, 286, 669, 1364, 382, 38087, 11403, 294, 23487, 6725, 293, 286, 589, 51054], "temperature": 0.0, "avg_logprob": -0.2727497061904596, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.023300884291529655}, {"id": 7, "seek": 5448, "start": 68.28, "end": 74.4, "text": " closely with Rook and Sef community. So, as I mentioned before, like most of the talks", "tokens": [51054, 8185, 365, 497, 1212, 293, 1100, 69, 1768, 13, 407, 11, 382, 286, 2835, 949, 11, 411, 881, 295, 264, 6686, 51360], "temperature": 0.0, "avg_logprob": -0.2727497061904596, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.023300884291529655}, {"id": 8, "seek": 5448, "start": 74.4, "end": 80.08, "text": " already covered Sef and Rook topics. So, I mostly focus on CADA. So, my section covers", "tokens": [51360, 1217, 5343, 1100, 69, 293, 497, 1212, 8378, 13, 407, 11, 286, 5240, 1879, 322, 41143, 32, 13, 407, 11, 452, 3541, 10538, 51644], "temperature": 0.0, "avg_logprob": -0.2727497061904596, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.023300884291529655}, {"id": 9, "seek": 8008, "start": 80.08, "end": 87.36, "text": " what is CADA and basic CADA concepts. Then, a brief thing about Rook operator and finally,", "tokens": [50364, 437, 307, 41143, 32, 293, 3875, 41143, 32, 10392, 13, 1396, 11, 257, 5353, 551, 466, 497, 1212, 12973, 293, 2721, 11, 50728], "temperature": 0.0, "avg_logprob": -0.19967878707731612, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.01728697121143341}, {"id": 10, "seek": 8008, "start": 87.36, "end": 94.67999999999999, "text": " a demo how the autoscaling works. So, what is CADA? So, in the last presentation, one of the", "tokens": [50728, 257, 10723, 577, 264, 1476, 10466, 4270, 1985, 13, 407, 11, 437, 307, 41143, 32, 30, 407, 11, 294, 264, 1036, 5860, 11, 472, 295, 264, 51094], "temperature": 0.0, "avg_logprob": -0.19967878707731612, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.01728697121143341}, {"id": 11, "seek": 8008, "start": 94.67999999999999, "end": 98.84, "text": " things Alexander mentioned, you can change the configuration, right, if you are in the", "tokens": [51094, 721, 14845, 2835, 11, 291, 393, 1319, 264, 11694, 11, 558, 11, 498, 291, 366, 294, 264, 51302], "temperature": 0.0, "avg_logprob": -0.19967878707731612, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.01728697121143341}, {"id": 12, "seek": 8008, "start": 98.84, "end": 104.16, "text": " deployment. With help of autoscaling, like, you don't want to change it. Like, it basically,", "tokens": [51302, 19317, 13, 2022, 854, 295, 1476, 10466, 4270, 11, 411, 11, 291, 500, 380, 528, 281, 1319, 309, 13, 1743, 11, 309, 1936, 11, 51568], "temperature": 0.0, "avg_logprob": -0.19967878707731612, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.01728697121143341}, {"id": 13, "seek": 10416, "start": 105.03999999999999, "end": 112.32, "text": " the autoscaler will find and scale those for you. And, Kubernetes inbuilt have the HPA and", "tokens": [50408, 264, 1476, 10466, 17148, 486, 915, 293, 4373, 729, 337, 291, 13, 400, 11, 23145, 294, 23018, 362, 264, 389, 10297, 293, 50772], "temperature": 0.0, "avg_logprob": -0.20975239477424978, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.020075244829058647}, {"id": 14, "seek": 10416, "start": 112.32, "end": 119.08, "text": " VPS scalers. So, CADA is kind of a bit, what you say, advanced version for the HPA. So,", "tokens": [50772, 691, 6273, 15664, 433, 13, 407, 11, 41143, 32, 307, 733, 295, 257, 857, 11, 437, 291, 584, 11, 7339, 3037, 337, 264, 389, 10297, 13, 407, 11, 51110], "temperature": 0.0, "avg_logprob": -0.20975239477424978, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.020075244829058647}, {"id": 15, "seek": 10416, "start": 119.08, "end": 126.16, "text": " what CADA does? It makes the Kubernetes even driving autoscaling that symbol. I don't know", "tokens": [51110, 437, 41143, 32, 775, 30, 467, 1669, 264, 23145, 754, 4840, 1476, 10466, 4270, 300, 5986, 13, 286, 500, 380, 458, 51464], "temperature": 0.0, "avg_logprob": -0.20975239477424978, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.020075244829058647}, {"id": 16, "seek": 10416, "start": 126.16, "end": 132.51999999999998, "text": " if you folks use HPA. So, HPA by default support, CPU and memory kind of scale. Like, if your", "tokens": [51464, 498, 291, 4024, 764, 389, 10297, 13, 407, 11, 389, 10297, 538, 7576, 1406, 11, 13199, 293, 4675, 733, 295, 4373, 13, 1743, 11, 498, 428, 51782], "temperature": 0.0, "avg_logprob": -0.20975239477424978, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.020075244829058647}, {"id": 17, "seek": 13252, "start": 132.52, "end": 136.92000000000002, "text": " port is using most, lot of CPUs or something like that, it will scale or if you use memory,", "tokens": [50364, 2436, 307, 1228, 881, 11, 688, 295, 13199, 82, 420, 746, 411, 300, 11, 309, 486, 4373, 420, 498, 291, 764, 4675, 11, 50584], "temperature": 0.0, "avg_logprob": -0.2595305038710772, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.009622108191251755}, {"id": 18, "seek": 13252, "start": 136.92000000000002, "end": 144.20000000000002, "text": " it will scale. But, it also supports custom metrics. And, CADA, and the one of the issues", "tokens": [50584, 309, 486, 4373, 13, 583, 11, 309, 611, 9346, 2375, 16367, 13, 400, 11, 41143, 32, 11, 293, 264, 472, 295, 264, 2663, 50948], "temperature": 0.0, "avg_logprob": -0.2595305038710772, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.009622108191251755}, {"id": 19, "seek": 13252, "start": 144.20000000000002, "end": 148.12, "text": " with the custom metrics, it does not have a standard version for that. Like, even Prometheus", "tokens": [50948, 365, 264, 2375, 16367, 11, 309, 775, 406, 362, 257, 3832, 3037, 337, 300, 13, 1743, 11, 754, 2114, 649, 42209, 51144], "temperature": 0.0, "avg_logprob": -0.2595305038710772, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.009622108191251755}, {"id": 20, "seek": 13252, "start": 148.12, "end": 153.04000000000002, "text": " has an implementation, I mean, Kubernetes has an implementation, but nothing is standardized. So,", "tokens": [51144, 575, 364, 11420, 11, 286, 914, 11, 23145, 575, 364, 11420, 11, 457, 1825, 307, 31677, 13, 407, 11, 51390], "temperature": 0.0, "avg_logprob": -0.2595305038710772, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.009622108191251755}, {"id": 21, "seek": 13252, "start": 153.04000000000002, "end": 158.72, "text": " this is where CADA comes. And, it started as a partnership between Microsoft and other that,", "tokens": [51390, 341, 307, 689, 41143, 32, 1487, 13, 400, 11, 309, 1409, 382, 257, 9982, 1296, 8116, 293, 661, 300, 11, 51674], "temperature": 0.0, "avg_logprob": -0.2595305038710772, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.009622108191251755}, {"id": 22, "seek": 15872, "start": 159.0, "end": 165.84, "text": " and it was donated to CNCF. And, during the two-point-o version, like, it may be,", "tokens": [50378, 293, 309, 390, 23723, 281, 48714, 37, 13, 400, 11, 1830, 264, 732, 12, 6053, 12, 78, 3037, 11, 411, 11, 309, 815, 312, 11, 50720], "temperature": 0.0, "avg_logprob": -0.2424702875822493, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.01367745827883482}, {"id": 23, "seek": 15872, "start": 165.84, "end": 171.56, "text": " go through a major design change and all that stuff. And, last couple of years back, like,", "tokens": [50720, 352, 807, 257, 2563, 1715, 1319, 293, 439, 300, 1507, 13, 400, 11, 1036, 1916, 295, 924, 646, 11, 411, 11, 51006], "temperature": 0.0, "avg_logprob": -0.2424702875822493, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.01367745827883482}, {"id": 24, "seek": 15872, "start": 171.56, "end": 178.24, "text": " it got become a, like, incubation project. And, currently, like, it's on the 2.9 release,", "tokens": [51006, 309, 658, 1813, 257, 11, 411, 11, 33345, 399, 1716, 13, 400, 11, 4362, 11, 411, 11, 309, 311, 322, 264, 568, 13, 24, 4374, 11, 51340], "temperature": 0.0, "avg_logprob": -0.2424702875822493, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.01367745827883482}, {"id": 25, "seek": 15872, "start": 178.24, "end": 183.32, "text": " like, that's the latest version. And, I hope the next major change will happen in the third", "tokens": [51340, 411, 11, 300, 311, 264, 6792, 3037, 13, 400, 11, 286, 1454, 264, 958, 2563, 1319, 486, 1051, 294, 264, 2636, 51594], "temperature": 0.0, "avg_logprob": -0.2424702875822493, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.01367745827883482}, {"id": 26, "seek": 18332, "start": 183.35999999999999, "end": 190.28, "text": " version coming years. Now, going a bit about CADA concepts. So, as I mentioned,", "tokens": [50366, 3037, 1348, 924, 13, 823, 11, 516, 257, 857, 466, 41143, 32, 10392, 13, 407, 11, 382, 286, 2835, 11, 50712], "temperature": 0.0, "avg_logprob": -0.28088430148452076, "compression_ratio": 1.4, "no_speech_prob": 0.006440867204219103}, {"id": 27, "seek": 18332, "start": 190.28, "end": 199.04, "text": " it automatically scales the Kubernetes resources, like deployments, or stateful sets,", "tokens": [50712, 309, 6772, 17408, 264, 23145, 3593, 11, 411, 7274, 1117, 11, 420, 1785, 906, 6352, 11, 51150], "temperature": 0.0, "avg_logprob": -0.28088430148452076, "compression_ratio": 1.4, "no_speech_prob": 0.006440867204219103}, {"id": 28, "seek": 18332, "start": 199.04, "end": 205.28, "text": " like replica, or even customer sources. Then, it has, like, inbuilt 50-scalers,", "tokens": [51150, 411, 35456, 11, 420, 754, 5474, 7139, 13, 1396, 11, 309, 575, 11, 411, 11, 294, 23018, 2625, 12, 4417, 304, 433, 11, 51462], "temperature": 0.0, "avg_logprob": -0.28088430148452076, "compression_ratio": 1.4, "no_speech_prob": 0.006440867204219103}, {"id": 29, "seek": 20528, "start": 205.4, "end": 212.6, "text": " like, like a plug-in which we can attach. Like, for example, you have Prometheus,", "tokens": [50370, 411, 11, 411, 257, 5452, 12, 259, 597, 321, 393, 5085, 13, 1743, 11, 337, 1365, 11, 291, 362, 2114, 649, 42209, 11, 50730], "temperature": 0.0, "avg_logprob": -0.24901962280273438, "compression_ratio": 1.52, "no_speech_prob": 0.0045263199135661125}, {"id": 30, "seek": 20528, "start": 212.6, "end": 220.16, "text": " or like Kafka, and RabbitMQ, AWS, and Azure, all those big players are there. Then,", "tokens": [50730, 420, 411, 47064, 11, 293, 42092, 44, 48, 11, 17650, 11, 293, 11969, 11, 439, 729, 955, 4150, 366, 456, 13, 1396, 11, 51108], "temperature": 0.0, "avg_logprob": -0.24901962280273438, "compression_ratio": 1.52, "no_speech_prob": 0.0045263199135661125}, {"id": 31, "seek": 20528, "start": 220.16, "end": 225.48, "text": " other thing is, it just scales the resources. It does not manipulate your data. Then,", "tokens": [51108, 661, 551, 307, 11, 309, 445, 17408, 264, 3593, 13, 467, 775, 406, 20459, 428, 1412, 13, 1396, 11, 51374], "temperature": 0.0, "avg_logprob": -0.24901962280273438, "compression_ratio": 1.52, "no_speech_prob": 0.0045263199135661125}, {"id": 32, "seek": 20528, "start": 225.48, "end": 232.84, "text": " the scaling is done on the event basis. So, it doesn't want anything to do with your data.", "tokens": [51374, 264, 21589, 307, 1096, 322, 264, 2280, 5143, 13, 407, 11, 309, 1177, 380, 528, 1340, 281, 360, 365, 428, 1412, 13, 51742], "temperature": 0.0, "avg_logprob": -0.24901962280273438, "compression_ratio": 1.52, "no_speech_prob": 0.0045263199135661125}, {"id": 33, "seek": 23284, "start": 232.88, "end": 237.56, "text": " Like, it won't manipulate your, like, your side unit or something like that. It just scales", "tokens": [50366, 1743, 11, 309, 1582, 380, 20459, 428, 11, 411, 11, 428, 1252, 4985, 420, 746, 411, 300, 13, 467, 445, 17408, 50600], "temperature": 0.0, "avg_logprob": -0.2546926252912767, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.004838260821998119}, {"id": 34, "seek": 23284, "start": 237.56, "end": 243.48000000000002, "text": " based on the events which it's facing. And, you can install CADA via OLM or Helm, whatever.", "tokens": [50600, 2361, 322, 264, 3931, 597, 309, 311, 7170, 13, 400, 11, 291, 393, 3625, 41143, 32, 5766, 39191, 44, 420, 6128, 76, 11, 2035, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2546926252912767, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.004838260821998119}, {"id": 35, "seek": 23284, "start": 243.48000000000002, "end": 252.64000000000001, "text": " Now, this is basic architecture with CADA. So, CADA just enhances the HPF feature of the", "tokens": [50896, 823, 11, 341, 307, 3875, 9482, 365, 41143, 32, 13, 407, 11, 41143, 32, 445, 46628, 264, 12557, 37, 4111, 295, 264, 51354], "temperature": 0.0, "avg_logprob": -0.2546926252912767, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.004838260821998119}, {"id": 36, "seek": 23284, "start": 252.64000000000001, "end": 261.44, "text": " Kubernetes. So, you need to have the Kubernetes cluster. Then, there's an, on the bottom side,", "tokens": [51354, 23145, 13, 407, 11, 291, 643, 281, 362, 264, 23145, 13630, 13, 1396, 11, 456, 311, 364, 11, 322, 264, 2767, 1252, 11, 51794], "temperature": 0.0, "avg_logprob": -0.2546926252912767, "compression_ratio": 1.5485232067510548, "no_speech_prob": 0.004838260821998119}, {"id": 37, "seek": 26144, "start": 261.44, "end": 265.96, "text": " you can see an external source, like, which you, which you have given the information about,", "tokens": [50364, 291, 393, 536, 364, 8320, 4009, 11, 411, 11, 597, 291, 11, 597, 291, 362, 2212, 264, 1589, 466, 11, 50590], "temperature": 0.0, "avg_logprob": -0.22834623221195105, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.003503260435536504}, {"id": 38, "seek": 26144, "start": 265.96, "end": 274.8, "text": " like, what resources, or, like, what even you need to scale. And, how the, like, how the deployment", "tokens": [50590, 411, 11, 437, 3593, 11, 420, 11, 411, 11, 437, 754, 291, 643, 281, 4373, 13, 400, 11, 577, 264, 11, 411, 11, 577, 264, 19317, 51032], "temperature": 0.0, "avg_logprob": -0.22834623221195105, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.003503260435536504}, {"id": 39, "seek": 26144, "start": 274.8, "end": 278.4, "text": " will scale, it based on a customer source, not a scaled object or a scaled job. Now,", "tokens": [51032, 486, 4373, 11, 309, 2361, 322, 257, 5474, 4009, 11, 406, 257, 36039, 2657, 420, 257, 36039, 1691, 13, 823, 11, 51212], "temperature": 0.0, "avg_logprob": -0.22834623221195105, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.003503260435536504}, {"id": 40, "seek": 26144, "start": 278.4, "end": 285.2, "text": " the scaled object will be created by CADA. And, CADA has, like, the three components,", "tokens": [51212, 264, 36039, 2657, 486, 312, 2942, 538, 41143, 32, 13, 400, 11, 41143, 32, 575, 11, 411, 11, 264, 1045, 6677, 11, 51552], "temperature": 0.0, "avg_logprob": -0.22834623221195105, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.003503260435536504}, {"id": 41, "seek": 26144, "start": 285.2, "end": 288.96, "text": " like, one is a scalar, like, as I mentioned, like, there are a lot of scalers, like, it's a plug-in", "tokens": [51552, 411, 11, 472, 307, 257, 39684, 11, 411, 11, 382, 286, 2835, 11, 411, 11, 456, 366, 257, 688, 295, 15664, 433, 11, 411, 11, 309, 311, 257, 5452, 12, 259, 51740], "temperature": 0.0, "avg_logprob": -0.22834623221195105, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.003503260435536504}, {"id": 42, "seek": 28896, "start": 289.15999999999997, "end": 295.44, "text": " for CADA. Then, you have the CADA controller, basically, manage the CADA, I mean, CADA services", "tokens": [50374, 337, 41143, 32, 13, 1396, 11, 291, 362, 264, 41143, 32, 10561, 11, 1936, 11, 3067, 264, 41143, 32, 11, 286, 914, 11, 41143, 32, 3328, 50688], "temperature": 0.0, "avg_logprob": -0.1502221301921363, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.01334457565099001}, {"id": 43, "seek": 28896, "start": 295.44, "end": 305.12, "text": " and demons. Then, you have the metrics adapter. So, the custom, like, in the case of HPA, the custom", "tokens": [50688, 293, 19733, 13, 1396, 11, 291, 362, 264, 16367, 22860, 13, 407, 11, 264, 2375, 11, 411, 11, 294, 264, 1389, 295, 389, 10297, 11, 264, 2375, 51172], "temperature": 0.0, "avg_logprob": -0.1502221301921363, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.01334457565099001}, {"id": 44, "seek": 28896, "start": 305.12, "end": 309.84, "text": " metrics is driven by a metrics adapter, like, you need to provide a metrics adapter for the", "tokens": [51172, 16367, 307, 9555, 538, 257, 16367, 22860, 11, 411, 11, 291, 643, 281, 2893, 257, 16367, 22860, 337, 264, 51408], "temperature": 0.0, "avg_logprob": -0.1502221301921363, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.01334457565099001}, {"id": 45, "seek": 28896, "start": 309.84, "end": 316.44, "text": " custom scaling. So, CADA itself brings up a metrics adapter for that. Now, you have your", "tokens": [51408, 2375, 21589, 13, 407, 11, 41143, 32, 2564, 5607, 493, 257, 16367, 22860, 337, 300, 13, 823, 11, 291, 362, 428, 51738], "temperature": 0.0, "avg_logprob": -0.1502221301921363, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.01334457565099001}, {"id": 46, "seek": 31644, "start": 316.52, "end": 323.32, "text": " workload, and after that, based on the events, it may increase your deployment or it may decrease", "tokens": [50368, 20139, 11, 293, 934, 300, 11, 2361, 322, 264, 3931, 11, 309, 815, 3488, 428, 19317, 420, 309, 815, 11514, 50708], "temperature": 0.0, "avg_logprob": -0.19889774500766647, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.006099145393818617}, {"id": 47, "seek": 31644, "start": 323.32, "end": 328.68, "text": " your deployment. And, it does it with the help of HPA, like, even though you define a scaled object", "tokens": [50708, 428, 19317, 13, 400, 11, 309, 775, 309, 365, 264, 854, 295, 389, 10297, 11, 411, 11, 754, 1673, 291, 6964, 257, 36039, 2657, 50976], "temperature": 0.0, "avg_logprob": -0.19889774500766647, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.006099145393818617}, {"id": 48, "seek": 31644, "start": 328.68, "end": 334.84, "text": " or a scaled job, then, in general, it creates an HPA. Now, one of the differences is that, like,", "tokens": [50976, 420, 257, 36039, 1691, 11, 550, 11, 294, 2674, 11, 309, 7829, 364, 389, 10297, 13, 823, 11, 472, 295, 264, 7300, 307, 300, 11, 411, 11, 51284], "temperature": 0.0, "avg_logprob": -0.19889774500766647, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.006099145393818617}, {"id": 49, "seek": 31644, "start": 334.84, "end": 342.0, "text": " it can scale down to zero, like, normally, with HPA, the scaling starts on one or, like, and it", "tokens": [51284, 309, 393, 4373, 760, 281, 4018, 11, 411, 11, 5646, 11, 365, 389, 10297, 11, 264, 21589, 3719, 322, 472, 420, 11, 411, 11, 293, 309, 51642], "temperature": 0.0, "avg_logprob": -0.19889774500766647, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.006099145393818617}, {"id": 50, "seek": 34200, "start": 342.04, "end": 349.24, "text": " ends on the maximum which you have defined. Then, yeah, that's all. And, the metrics adapter", "tokens": [50366, 5314, 322, 264, 6674, 597, 291, 362, 7642, 13, 1396, 11, 1338, 11, 300, 311, 439, 13, 400, 11, 264, 16367, 22860, 50726], "temperature": 0.0, "avg_logprob": -0.20769603884949975, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00173535430803895}, {"id": 51, "seek": 34200, "start": 349.24, "end": 354.88, "text": " is covered. So, yeah, the custom metrics is provided by the auto-scaler, like, provided", "tokens": [50726, 307, 5343, 13, 407, 11, 1338, 11, 264, 2375, 16367, 307, 5649, 538, 264, 8399, 12, 4417, 17148, 11, 411, 11, 5649, 51008], "temperature": 0.0, "avg_logprob": -0.20769603884949975, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00173535430803895}, {"id": 52, "seek": 34200, "start": 354.88, "end": 361.6, "text": " to auto-scaler by this metrics adapter. Now, I am just giving an example about scaled objects,", "tokens": [51008, 281, 8399, 12, 4417, 17148, 538, 341, 16367, 22860, 13, 823, 11, 286, 669, 445, 2902, 364, 1365, 466, 36039, 6565, 11, 51344], "temperature": 0.0, "avg_logprob": -0.20769603884949975, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00173535430803895}, {"id": 53, "seek": 34200, "start": 361.6, "end": 368.64, "text": " because that's what I'm going to cover into this presentation. So, basically, it has a name,", "tokens": [51344, 570, 300, 311, 437, 286, 478, 516, 281, 2060, 666, 341, 5860, 13, 407, 11, 1936, 11, 309, 575, 257, 1315, 11, 51696], "temperature": 0.0, "avg_logprob": -0.20769603884949975, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00173535430803895}, {"id": 54, "seek": 36864, "start": 368.76, "end": 374.84, "text": " the metadata information. Then, the, you can see the target of, which will mention what type of", "tokens": [50370, 264, 26603, 1589, 13, 1396, 11, 264, 11, 291, 393, 536, 264, 3779, 295, 11, 597, 486, 2152, 437, 2010, 295, 50674], "temperature": 0.0, "avg_logprob": -0.2481443660775411, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0050529069267213345}, {"id": 55, "seek": 36864, "start": 374.84, "end": 380.88, "text": " resource you want to scale. Like, by default, it will be deployment. But, you can also add the", "tokens": [50674, 7684, 291, 528, 281, 4373, 13, 1743, 11, 538, 7576, 11, 309, 486, 312, 19317, 13, 583, 11, 291, 393, 611, 909, 264, 50976], "temperature": 0.0, "avg_logprob": -0.2481443660775411, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0050529069267213345}, {"id": 56, "seek": 36864, "start": 380.88, "end": 387.84, "text": " replica sets or stateful resources. Then, if you have a customer resource with a scale defined,", "tokens": [50976, 35456, 6352, 420, 1785, 906, 3593, 13, 1396, 11, 498, 291, 362, 257, 5474, 7684, 365, 257, 4373, 7642, 11, 51324], "temperature": 0.0, "avg_logprob": -0.2481443660775411, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0050529069267213345}, {"id": 57, "seek": 36864, "start": 387.84, "end": 393.71999999999997, "text": " you can also define that. So, these, then, you need to mention the type of resource. Like,", "tokens": [51324, 291, 393, 611, 6964, 300, 13, 407, 11, 613, 11, 550, 11, 291, 643, 281, 2152, 264, 2010, 295, 7684, 13, 1743, 11, 51618], "temperature": 0.0, "avg_logprob": -0.2481443660775411, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0050529069267213345}, {"id": 58, "seek": 39372, "start": 393.72, "end": 398.20000000000005, "text": " by default, if you give a name, it will be deployment. Then, you can mention about the", "tokens": [50364, 538, 7576, 11, 498, 291, 976, 257, 1315, 11, 309, 486, 312, 19317, 13, 1396, 11, 291, 393, 2152, 466, 264, 50588], "temperature": 0.0, "avg_logprob": -0.23544124528473498, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.003952433355152607}, {"id": 59, "seek": 39372, "start": 398.20000000000005, "end": 403.88000000000005, "text": " Plick account. Then, you can give Triggers. So, Triggers is an event which, the based on", "tokens": [50588, 2149, 618, 2696, 13, 1396, 11, 291, 393, 976, 1765, 20942, 13, 407, 11, 1765, 20942, 307, 364, 2280, 597, 11, 264, 2361, 322, 50872], "temperature": 0.0, "avg_logprob": -0.23544124528473498, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.003952433355152607}, {"id": 60, "seek": 39372, "start": 403.88000000000005, "end": 409.32000000000005, "text": " scaling will happen and you can define multiple Triggers as well. So, so far, any questions or,", "tokens": [50872, 21589, 486, 1051, 293, 291, 393, 6964, 3866, 1765, 20942, 382, 731, 13, 407, 11, 370, 1400, 11, 604, 1651, 420, 11, 51144], "temperature": 0.0, "avg_logprob": -0.23544124528473498, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.003952433355152607}, {"id": 61, "seek": 39372, "start": 409.32000000000005, "end": 421.72, "text": " like, okay, I'll move forward. Now, I am just mentioning a few, most of them I already covered,", "tokens": [51144, 411, 11, 1392, 11, 286, 603, 1286, 2128, 13, 823, 11, 286, 669, 445, 18315, 257, 1326, 11, 881, 295, 552, 286, 1217, 5343, 11, 51764], "temperature": 0.0, "avg_logprob": -0.23544124528473498, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.003952433355152607}, {"id": 62, "seek": 42172, "start": 421.72, "end": 427.96000000000004, "text": " like, CADAP features. It can scale down to zero if you want. Another part is, like, if there's a", "tokens": [50364, 411, 11, 41143, 4715, 4122, 13, 467, 393, 4373, 760, 281, 4018, 498, 291, 528, 13, 3996, 644, 307, 11, 411, 11, 498, 456, 311, 257, 50676], "temperature": 0.0, "avg_logprob": -0.20869349090146347, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.023835550993680954}, {"id": 63, "seek": 42172, "start": 427.96000000000004, "end": 433.48, "text": " failback, the Plick account, if something happens to the cluster, you can failback to one number.", "tokens": [50676, 3061, 3207, 11, 264, 2149, 618, 2696, 11, 498, 746, 2314, 281, 264, 13630, 11, 291, 393, 3061, 3207, 281, 472, 1230, 13, 50952], "temperature": 0.0, "avg_logprob": -0.20869349090146347, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.023835550993680954}, {"id": 64, "seek": 42172, "start": 433.48, "end": 439.88000000000005, "text": " So, it's not min or max. You can define a failback value. Maybe, say, your min is three and your", "tokens": [50952, 407, 11, 309, 311, 406, 923, 420, 11469, 13, 509, 393, 6964, 257, 3061, 3207, 2158, 13, 2704, 11, 584, 11, 428, 923, 307, 1045, 293, 428, 51272], "temperature": 0.0, "avg_logprob": -0.20869349090146347, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.023835550993680954}, {"id": 65, "seek": 42172, "start": 439.88000000000005, "end": 444.52000000000004, "text": " five is your max and you can set a failback one, in some of the cases, if you want to failback two.", "tokens": [51272, 1732, 307, 428, 11469, 293, 291, 393, 992, 257, 3061, 3207, 472, 11, 294, 512, 295, 264, 3331, 11, 498, 291, 528, 281, 3061, 3207, 732, 13, 51504], "temperature": 0.0, "avg_logprob": -0.20869349090146347, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.023835550993680954}, {"id": 66, "seek": 42172, "start": 445.32000000000005, "end": 449.96000000000004, "text": " It can really, say, like, pose for autoscaling. Like, you can start and stop. Like, you don't", "tokens": [51544, 467, 393, 534, 11, 584, 11, 411, 11, 10774, 337, 1476, 10466, 4270, 13, 1743, 11, 291, 393, 722, 293, 1590, 13, 1743, 11, 291, 500, 380, 51776], "temperature": 0.0, "avg_logprob": -0.20869349090146347, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.023835550993680954}, {"id": 67, "seek": 44996, "start": 450.03999999999996, "end": 455.64, "text": " delete those sources. You can just, I mean, you don't want to delete the scaled object or something", "tokens": [50368, 12097, 729, 7139, 13, 509, 393, 445, 11, 286, 914, 11, 291, 500, 380, 528, 281, 12097, 264, 36039, 2657, 420, 746, 50648], "temperature": 0.0, "avg_logprob": -0.235692226525509, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.005554462317377329}, {"id": 68, "seek": 44996, "start": 455.64, "end": 461.23999999999995, "text": " like that. You can keep those sources and you can just do a pose. Then, CADAP, by default, can", "tokens": [50648, 411, 300, 13, 509, 393, 1066, 729, 7139, 293, 291, 393, 445, 360, 257, 10774, 13, 1396, 11, 41143, 4715, 11, 538, 7576, 11, 393, 50928], "temperature": 0.0, "avg_logprob": -0.235692226525509, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.005554462317377329}, {"id": 69, "seek": 44996, "start": 461.23999999999995, "end": 469.08, "text": " expose the Prometheus metrics in this adapter as well as the Kafka events. And one thing is,", "tokens": [50928, 19219, 264, 2114, 649, 42209, 16367, 294, 341, 22860, 382, 731, 382, 264, 47064, 3931, 13, 400, 472, 551, 307, 11, 51320], "temperature": 0.0, "avg_logprob": -0.235692226525509, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.005554462317377329}, {"id": 70, "seek": 44996, "start": 469.08, "end": 473.64, "text": " like, you can also use secure connections, like, potentials in things, like, it can be", "tokens": [51320, 411, 11, 291, 393, 611, 764, 7144, 9271, 11, 411, 11, 3995, 82, 294, 721, 11, 411, 11, 309, 393, 312, 51548], "temperature": 0.0, "avg_logprob": -0.235692226525509, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.005554462317377329}, {"id": 71, "seek": 47364, "start": 474.2, "end": 480.76, "text": " defined by another subclass or, like, a subsection in the scaled, I mean,", "tokens": [50392, 7642, 538, 1071, 1422, 11665, 420, 11, 411, 11, 257, 1422, 11963, 294, 264, 36039, 11, 286, 914, 11, 50720], "temperature": 0.0, "avg_logprob": -0.20373296737670898, "compression_ratio": 1.8457446808510638, "no_speech_prob": 0.011548887006938457}, {"id": 72, "seek": 47364, "start": 482.12, "end": 487.24, "text": " in the scale, like, in the Prometheus, I mean, the type, in the scalar type, you can mention a", "tokens": [50788, 294, 264, 4373, 11, 411, 11, 294, 264, 2114, 649, 42209, 11, 286, 914, 11, 264, 2010, 11, 294, 264, 39684, 2010, 11, 291, 393, 2152, 257, 51044], "temperature": 0.0, "avg_logprob": -0.20373296737670898, "compression_ratio": 1.8457446808510638, "no_speech_prob": 0.011548887006938457}, {"id": 73, "seek": 47364, "start": 487.24, "end": 492.68, "text": " subclass about the trigger authentication, which you'll refer, like, how you can authenticate", "tokens": [51044, 1422, 11665, 466, 264, 7875, 26643, 11, 597, 291, 603, 2864, 11, 411, 11, 577, 291, 393, 9214, 8700, 51316], "temperature": 0.0, "avg_logprob": -0.20373296737670898, "compression_ratio": 1.8457446808510638, "no_speech_prob": 0.011548887006938457}, {"id": 74, "seek": 47364, "start": 492.68, "end": 499.47999999999996, "text": " with the server. Now, today, in the latest version, even, you can have the events or", "tokens": [51316, 365, 264, 7154, 13, 823, 11, 965, 11, 294, 264, 6792, 3037, 11, 754, 11, 291, 393, 362, 264, 3931, 420, 51656], "temperature": 0.0, "avg_logprob": -0.20373296737670898, "compression_ratio": 1.8457446808510638, "no_speech_prob": 0.011548887006938457}, {"id": 75, "seek": 49948, "start": 499.48, "end": 505.48, "text": " metrics from the GRPC or, like, from the other JPA, but I have never tried or I have never used", "tokens": [50364, 16367, 490, 264, 10903, 12986, 420, 11, 411, 11, 490, 264, 661, 508, 10297, 11, 457, 286, 362, 1128, 3031, 420, 286, 362, 1128, 1143, 50664], "temperature": 0.0, "avg_logprob": -0.26749921728063514, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.01061095017939806}, {"id": 76, "seek": 49948, "start": 505.48, "end": 514.44, "text": " those things. Now, coming to RGW. So, RGBL case is very simple case for CADAP. First of all,", "tokens": [50664, 729, 721, 13, 823, 11, 1348, 281, 497, 38, 54, 13, 407, 11, 31231, 43, 1389, 307, 588, 2199, 1389, 337, 41143, 4715, 13, 2386, 295, 439, 11, 51112], "temperature": 0.0, "avg_logprob": -0.26749921728063514, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.01061095017939806}, {"id": 77, "seek": 49948, "start": 514.44, "end": 520.28, "text": " it just proves through the Wootkend RGW stuff, like, so, in the last presentation, they mentioned", "tokens": [51112, 309, 445, 25019, 807, 264, 343, 6259, 74, 521, 497, 38, 54, 1507, 11, 411, 11, 370, 11, 294, 264, 1036, 5860, 11, 436, 2835, 51404], "temperature": 0.0, "avg_logprob": -0.26749921728063514, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.01061095017939806}, {"id": 78, "seek": 49948, "start": 520.28, "end": 525.32, "text": " Wootkiston orchestrator, which conducts the self storage and it simplifies the deployment and", "tokens": [51404, 343, 6259, 74, 47345, 14161, 19802, 11, 597, 6018, 82, 264, 2698, 6725, 293, 309, 6883, 11221, 264, 19317, 293, 51656], "temperature": 0.0, "avg_logprob": -0.26749921728063514, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.01061095017939806}, {"id": 79, "seek": 52532, "start": 525.32, "end": 535.1600000000001, "text": " management services for the self cluster. Now, here, for RGW, like, the access can be given as an", "tokens": [50364, 4592, 3328, 337, 264, 2698, 13630, 13, 823, 11, 510, 11, 337, 497, 38, 54, 11, 411, 11, 264, 2105, 393, 312, 2212, 382, 364, 50856], "temperature": 0.0, "avg_logprob": -0.17941401829229336, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0057755825109779835}, {"id": 80, "seek": 52532, "start": 535.1600000000001, "end": 541.72, "text": " OBCs, like, something similar to PBO PVC for the fly and file and block. Other source is known as", "tokens": [50856, 422, 7869, 82, 11, 411, 11, 746, 2531, 281, 430, 15893, 46700, 337, 264, 3603, 293, 3991, 293, 3461, 13, 5358, 4009, 307, 2570, 382, 51184], "temperature": 0.0, "avg_logprob": -0.17941401829229336, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0057755825109779835}, {"id": 81, "seek": 52532, "start": 541.72, "end": 548.7600000000001, "text": " self-object storage. So, OBC, like, you will get a bucket, but in case of self-object storage, like,", "tokens": [51184, 2698, 12, 41070, 6725, 13, 407, 11, 422, 7869, 11, 411, 11, 291, 486, 483, 257, 13058, 11, 457, 294, 1389, 295, 2698, 12, 41070, 6725, 11, 411, 11, 51536], "temperature": 0.0, "avg_logprob": -0.17941401829229336, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0057755825109779835}, {"id": 82, "seek": 52532, "start": 548.7600000000001, "end": 554.5200000000001, "text": " you will get the entire user credential, too. So, you can create multiple buckets and", "tokens": [51536, 291, 486, 483, 264, 2302, 4195, 22034, 11, 886, 13, 407, 11, 291, 393, 1884, 3866, 32191, 293, 51824], "temperature": 0.0, "avg_logprob": -0.17941401829229336, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.0057755825109779835}, {"id": 83, "seek": 55452, "start": 554.52, "end": 559.64, "text": " all those features can be done. Then, other part is, like, Wootk also have a service monitor. So,", "tokens": [50364, 439, 729, 4122, 393, 312, 1096, 13, 1396, 11, 661, 644, 307, 11, 411, 11, 343, 6259, 74, 611, 362, 257, 2643, 6002, 13, 407, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1710298112828097, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.005090869497507811}, {"id": 84, "seek": 55452, "start": 559.64, "end": 563.56, "text": " if you're familiar with Pomerateus and all, like, if Pomerateus want to fetch the metrics from your,", "tokens": [50620, 498, 291, 434, 4963, 365, 430, 14301, 473, 301, 293, 439, 11, 411, 11, 498, 430, 14301, 473, 301, 528, 281, 23673, 264, 16367, 490, 428, 11, 50816], "temperature": 0.0, "avg_logprob": -0.1710298112828097, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.005090869497507811}, {"id": 85, "seek": 55452, "start": 564.76, "end": 570.52, "text": " so, I mean, your DMN, like, you need to have a service monitor. So, what the service monitor does,", "tokens": [50876, 370, 11, 286, 914, 11, 428, 15322, 45, 11, 411, 11, 291, 643, 281, 362, 257, 2643, 6002, 13, 407, 11, 437, 264, 2643, 6002, 775, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1710298112828097, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.005090869497507811}, {"id": 86, "seek": 55452, "start": 570.52, "end": 574.52, "text": " the self-manager supports the metrics and this metrics will be passed to the Pomerateus service", "tokens": [51164, 264, 2698, 12, 1601, 3557, 9346, 264, 16367, 293, 341, 16367, 486, 312, 4678, 281, 264, 430, 14301, 473, 301, 2643, 51364], "temperature": 0.0, "avg_logprob": -0.1710298112828097, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.005090869497507811}, {"id": 87, "seek": 55452, "start": 574.52, "end": 579.64, "text": " with the help of the service monitor. Now, for my test case, I am using HS bench. So, it's a", "tokens": [51364, 365, 264, 854, 295, 264, 2643, 6002, 13, 823, 11, 337, 452, 1500, 1389, 11, 286, 669, 1228, 34194, 10638, 13, 407, 11, 309, 311, 257, 51620], "temperature": 0.0, "avg_logprob": -0.1710298112828097, "compression_ratio": 1.8270676691729324, "no_speech_prob": 0.005090869497507811}, {"id": 88, "seek": 57964, "start": 579.64, "end": 585.4, "text": " performance-evaluating tool for S3 workloads. So, yeah, that will be tool, like, that will", "tokens": [50364, 3389, 12, 68, 3337, 32438, 2290, 337, 318, 18, 32452, 13, 407, 11, 1338, 11, 300, 486, 312, 2290, 11, 411, 11, 300, 486, 50652], "temperature": 0.0, "avg_logprob": -0.23507092450116132, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0067786783911287785}, {"id": 89, "seek": 57964, "start": 585.4, "end": 594.28, "text": " be S3 client, which I will be using for HW. Now, yeah, so, in the demo, I cover, like,", "tokens": [50652, 312, 318, 18, 6423, 11, 597, 286, 486, 312, 1228, 337, 389, 54, 13, 823, 11, 1338, 11, 370, 11, 294, 264, 10723, 11, 286, 2060, 11, 411, 11, 51096], "temperature": 0.0, "avg_logprob": -0.23507092450116132, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0067786783911287785}, {"id": 90, "seek": 57964, "start": 594.28, "end": 599.96, "text": " the Pomerateus scale I will be using and the self-cluster will be already deployed by a hook", "tokens": [51096, 264, 430, 14301, 473, 301, 4373, 286, 486, 312, 1228, 293, 264, 2698, 12, 3474, 8393, 486, 312, 1217, 17826, 538, 257, 6328, 51380], "temperature": 0.0, "avg_logprob": -0.23507092450116132, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0067786783911287785}, {"id": 91, "seek": 57964, "start": 599.96, "end": 606.4399999999999, "text": " and HW is configured. Also, the Pomerateus server is up and have defined the Pomerateus,", "tokens": [51380, 293, 389, 54, 307, 30538, 13, 2743, 11, 264, 430, 14301, 473, 301, 7154, 307, 493, 293, 362, 7642, 264, 430, 14301, 473, 301, 11, 51704], "temperature": 0.0, "avg_logprob": -0.23507092450116132, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0067786783911287785}, {"id": 92, "seek": 60644, "start": 607.0, "end": 611.72, "text": " like, the requirements for service monitors and all those stuff. Then, I need to define a scaled", "tokens": [50392, 411, 11, 264, 7728, 337, 2643, 26518, 293, 439, 729, 1507, 13, 1396, 11, 286, 643, 281, 6964, 257, 36039, 50628], "temperature": 0.0, "avg_logprob": -0.12831435438062325, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.0087733818218112}, {"id": 93, "seek": 60644, "start": 611.72, "end": 618.6, "text": " object so that my HW can scale. And the scaling is based on the metrics provided by the manager.", "tokens": [50628, 2657, 370, 300, 452, 389, 54, 393, 4373, 13, 400, 264, 21589, 307, 2361, 322, 264, 16367, 5649, 538, 264, 6598, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12831435438062325, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.0087733818218112}, {"id": 94, "seek": 60644, "start": 618.6, "end": 624.2800000000001, "text": " So, for HW, most of the metrics are related to performance count. It's based on the how many", "tokens": [50972, 407, 11, 337, 389, 54, 11, 881, 295, 264, 16367, 366, 4077, 281, 3389, 1207, 13, 467, 311, 2361, 322, 264, 577, 867, 51256], "temperature": 0.0, "avg_logprob": -0.12831435438062325, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.0087733818218112}, {"id": 95, "seek": 60644, "start": 624.2800000000001, "end": 628.7600000000001, "text": " requests we are receiving on the HW server. It's nothing depends on the backend or something like", "tokens": [51256, 12475, 321, 366, 10040, 322, 264, 389, 54, 7154, 13, 467, 311, 1825, 5946, 322, 264, 38087, 420, 746, 411, 51480], "temperature": 0.0, "avg_logprob": -0.12831435438062325, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.0087733818218112}, {"id": 96, "seek": 60644, "start": 628.7600000000001, "end": 633.72, "text": " that. So, that's one thing which we may need to change. But currently, it's like a web server,", "tokens": [51480, 300, 13, 407, 11, 300, 311, 472, 551, 597, 321, 815, 643, 281, 1319, 13, 583, 4362, 11, 309, 311, 411, 257, 3670, 7154, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12831435438062325, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.0087733818218112}, {"id": 97, "seek": 63372, "start": 633.72, "end": 637.72, "text": " like, when you are getting the request based on request a lot, like, the scaling will happen.", "tokens": [50364, 411, 11, 562, 291, 366, 1242, 264, 5308, 2361, 322, 5308, 257, 688, 11, 411, 11, 264, 21589, 486, 1051, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18423374385049898, "compression_ratio": 1.5, "no_speech_prob": 0.0011130692437291145}, {"id": 98, "seek": 63372, "start": 639.08, "end": 653.48, "text": " Now, I will go to the demo. So, okay. I hope it's visible. So, I am running a mini cube cluster", "tokens": [50632, 823, 11, 286, 486, 352, 281, 264, 10723, 13, 407, 11, 1392, 13, 286, 1454, 309, 311, 8974, 13, 407, 11, 286, 669, 2614, 257, 8382, 13728, 13630, 51352], "temperature": 0.0, "avg_logprob": -0.18423374385049898, "compression_ratio": 1.5, "no_speech_prob": 0.0011130692437291145}, {"id": 99, "seek": 63372, "start": 655.96, "end": 660.28, "text": " for my demo purpose, like, everything is up and running. And I already installed", "tokens": [51476, 337, 452, 10723, 4334, 11, 411, 11, 1203, 307, 493, 293, 2614, 13, 400, 286, 1217, 8899, 51692], "temperature": 0.0, "avg_logprob": -0.18423374385049898, "compression_ratio": 1.5, "no_speech_prob": 0.0011130692437291145}, {"id": 100, "seek": 66372, "start": 664.28, "end": 673.64, "text": " the look cluster. So, you can see look operator is running. Then,", "tokens": [50392, 264, 574, 13630, 13, 407, 11, 291, 393, 536, 574, 12973, 307, 2614, 13, 1396, 11, 50860], "temperature": 0.0, "avg_logprob": -0.27842576952948084, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.002708932152017951}, {"id": 101, "seek": 66372, "start": 674.6, "end": 679.64, "text": " HW is also running. Currently, I have only one HW on my cluster. Then,", "tokens": [50908, 389, 54, 307, 611, 2614, 13, 19964, 11, 286, 362, 787, 472, 389, 54, 322, 452, 13630, 13, 1396, 11, 51160], "temperature": 0.0, "avg_logprob": -0.27842576952948084, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.002708932152017951}, {"id": 102, "seek": 66372, "start": 680.6, "end": 687.72, "text": " this is the service monitor, like, the format is, which look deploys. Then, if you check the services,", "tokens": [51208, 341, 307, 264, 2643, 6002, 11, 411, 11, 264, 7877, 307, 11, 597, 574, 368, 49522, 13, 1396, 11, 498, 291, 1520, 264, 3328, 11, 51564], "temperature": 0.0, "avg_logprob": -0.27842576952948084, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.002708932152017951}, {"id": 103, "seek": 68772, "start": 688.6800000000001, "end": 694.36, "text": " there are two HW service, like, one is the internal service, which can be accessed for the", "tokens": [50412, 456, 366, 732, 389, 54, 2643, 11, 411, 11, 472, 307, 264, 6920, 2643, 11, 597, 393, 312, 34211, 337, 264, 50696], "temperature": 0.0, "avg_logprob": -0.23731278438194126, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.0030137139838188887}, {"id": 104, "seek": 68772, "start": 695.24, "end": 700.36, "text": " humanities workloads. And for my HS1, I need to expose the HW service. Hence,", "tokens": [50740, 36140, 32452, 13, 400, 337, 452, 34194, 16, 11, 286, 643, 281, 19219, 264, 389, 54, 2643, 13, 22229, 11, 50996], "temperature": 0.0, "avg_logprob": -0.23731278438194126, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.0030137139838188887}, {"id": 105, "seek": 68772, "start": 701.08, "end": 706.12, "text": " I am using the channel HW service. It's just, it's an old port, like, it will just expose the", "tokens": [51032, 286, 669, 1228, 264, 2269, 389, 54, 2643, 13, 467, 311, 445, 11, 309, 311, 364, 1331, 2436, 11, 411, 11, 309, 486, 445, 19219, 264, 51284], "temperature": 0.0, "avg_logprob": -0.23731278438194126, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.0030137139838188887}, {"id": 106, "seek": 68772, "start": 706.12, "end": 714.0400000000001, "text": " HW service on the remission. Then, yeah, there is also the Udpometheus service as well.", "tokens": [51284, 389, 54, 2643, 322, 264, 890, 3106, 13, 1396, 11, 1338, 11, 456, 307, 611, 264, 624, 67, 79, 649, 42209, 2643, 382, 731, 13, 51680], "temperature": 0.0, "avg_logprob": -0.23731278438194126, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.0030137139838188887}, {"id": 107, "seek": 71404, "start": 714.28, "end": 721.56, "text": " So, I have created SF subject show user, and I need to pass these credentials for my demo,", "tokens": [50376, 407, 11, 286, 362, 2942, 31095, 3983, 855, 4195, 11, 293, 286, 643, 281, 1320, 613, 27404, 337, 452, 10723, 11, 50740], "temperature": 0.0, "avg_logprob": -0.3239577015240987, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0015596459852531552}, {"id": 108, "seek": 71404, "start": 721.56, "end": 729.7199999999999, "text": " I mean, for my workload. This is the Pometheus operator running on the default name space.", "tokens": [50740, 286, 914, 11, 337, 452, 20139, 13, 639, 307, 264, 430, 649, 42209, 12973, 2614, 322, 264, 7576, 1315, 1901, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3239577015240987, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0015596459852531552}, {"id": 109, "seek": 72972, "start": 730.6800000000001, "end": 737.4, "text": " Now, I will install the CADA via Helm. It's deployed.", "tokens": [50412, 823, 11, 286, 486, 3625, 264, 41143, 32, 5766, 6128, 76, 13, 467, 311, 17826, 13, 50748], "temperature": 0.0, "avg_logprob": -0.3297642072041829, "compression_ratio": 1.1037735849056605, "no_speech_prob": 0.018629342317581177}, {"id": 110, "seek": 72972, "start": 751.0, "end": 754.6800000000001, "text": " Just checking the ports are up and running. It's nothing fancy.", "tokens": [51428, 1449, 8568, 264, 18160, 366, 493, 293, 2614, 13, 467, 311, 1825, 10247, 13, 51612], "temperature": 0.0, "avg_logprob": -0.3297642072041829, "compression_ratio": 1.1037735849056605, "no_speech_prob": 0.018629342317581177}, {"id": 111, "seek": 75468, "start": 755.0, "end": 762.5999999999999, "text": " So, everything is up and running. Now, I need to define the scaled object to source for HW.", "tokens": [50380, 407, 11, 1203, 307, 493, 293, 2614, 13, 823, 11, 286, 643, 281, 6964, 264, 36039, 2657, 281, 4009, 337, 389, 54, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2787916505491579, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006924115587025881}, {"id": 112, "seek": 75468, "start": 764.5999999999999, "end": 772.92, "text": " Sorry. I'm just taking the Pometheus web's console UI. So, I am doing the scaling based on the", "tokens": [50860, 4919, 13, 286, 478, 445, 1940, 264, 430, 649, 42209, 3670, 311, 11076, 15682, 13, 407, 11, 286, 669, 884, 264, 21589, 2361, 322, 264, 51276], "temperature": 0.0, "avg_logprob": -0.2787916505491579, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006924115587025881}, {"id": 113, "seek": 75468, "start": 773.56, "end": 777.4, "text": " request, like, SFHJ request. So, just showing the current value. That's it.", "tokens": [51308, 5308, 11, 411, 11, 31095, 39, 41, 5308, 13, 407, 11, 445, 4099, 264, 2190, 2158, 13, 663, 311, 309, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2787916505491579, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.006924115587025881}, {"id": 114, "seek": 77740, "start": 778.12, "end": 784.52, "text": " Yeah. Now, I need to define the scaled object. So,", "tokens": [50400, 865, 13, 823, 11, 286, 643, 281, 6964, 264, 36039, 2657, 13, 407, 11, 50720], "temperature": 0.0, "avg_logprob": -0.4212545667375837, "compression_ratio": 1.25, "no_speech_prob": 0.004361379891633987}, {"id": 115, "seek": 77740, "start": 799.48, "end": 803.8, "text": " so, this is the, in the GitHub repo, like, this example is the lamified is the,", "tokens": [51468, 370, 11, 341, 307, 264, 11, 294, 264, 23331, 49040, 11, 411, 11, 341, 1365, 307, 264, 24688, 2587, 307, 264, 11, 51684], "temperature": 0.0, "avg_logprob": -0.4212545667375837, "compression_ratio": 1.25, "no_speech_prob": 0.004361379891633987}, {"id": 116, "seek": 80380, "start": 804.3599999999999, "end": 810.12, "text": " so, I have given the name for my scaled object, and I am doing the scale, like, on the scaling,", "tokens": [50392, 370, 11, 286, 362, 2212, 264, 1315, 337, 452, 36039, 2657, 11, 293, 286, 669, 884, 264, 4373, 11, 411, 11, 322, 264, 21589, 11, 50680], "temperature": 0.0, "avg_logprob": -0.24095832940303918, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.006105958018451929}, {"id": 117, "seek": 80380, "start": 810.12, "end": 816.1999999999999, "text": " on the deployment, on the SF object show, like, HW show, set the minimum of pick account 1 and", "tokens": [50680, 322, 264, 19317, 11, 322, 264, 31095, 2657, 855, 11, 411, 11, 389, 54, 855, 11, 992, 264, 7285, 295, 1888, 2696, 502, 293, 50984], "temperature": 0.0, "avg_logprob": -0.24095832940303918, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.006105958018451929}, {"id": 118, "seek": 80380, "start": 816.1999999999999, "end": 821.4799999999999, "text": " maximum 3. Then, this is the Pometheus endpoint, I mean, the metrics endpoint, which I need to", "tokens": [50984, 6674, 805, 13, 1396, 11, 341, 307, 264, 430, 649, 42209, 35795, 11, 286, 914, 11, 264, 16367, 35795, 11, 597, 286, 643, 281, 51248], "temperature": 0.0, "avg_logprob": -0.24095832940303918, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.006105958018451929}, {"id": 119, "seek": 80380, "start": 821.4799999999999, "end": 825.56, "text": " fetch the metrics, and this is the metrics name which I am giving, like, so, this is based on,", "tokens": [51248, 23673, 264, 16367, 11, 293, 341, 307, 264, 16367, 1315, 597, 286, 669, 2902, 11, 411, 11, 370, 11, 341, 307, 2361, 322, 11, 51452], "temperature": 0.0, "avg_logprob": -0.24095832940303918, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.006105958018451929}, {"id": 120, "seek": 80380, "start": 826.4399999999999, "end": 832.92, "text": " this is a, like, definition for the triggers, nothing else, and the threshold value is 10. So,", "tokens": [51496, 341, 307, 257, 11, 411, 11, 7123, 337, 264, 22827, 11, 1825, 1646, 11, 293, 264, 14678, 2158, 307, 1266, 13, 407, 11, 51820], "temperature": 0.0, "avg_logprob": -0.24095832940303918, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.006105958018451929}, {"id": 121, "seek": 83292, "start": 832.92, "end": 837.8, "text": " basically, it's 10 million requests, not normal 10. I mean,", "tokens": [50364, 1936, 11, 309, 311, 1266, 2459, 12475, 11, 406, 2710, 1266, 13, 286, 914, 11, 50608], "temperature": 0.0, "avg_logprob": -0.19690967250514674, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.002096824813634157}, {"id": 122, "seek": 83292, "start": 850.36, "end": 855.4, "text": " now, the scaled object is created. If you check for the scaled object and", "tokens": [51236, 586, 11, 264, 36039, 2657, 307, 2942, 13, 759, 291, 1520, 337, 264, 36039, 2657, 293, 51488], "temperature": 0.0, "avg_logprob": -0.19690967250514674, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.002096824813634157}, {"id": 123, "seek": 85540, "start": 856.12, "end": 863.8, "text": " HPA, you can, we have defined the scaled object, and HPA is automatically created. So,", "tokens": [50400, 389, 10297, 11, 291, 393, 11, 321, 362, 7642, 264, 36039, 2657, 11, 293, 389, 10297, 307, 6772, 2942, 13, 407, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15710713001007728, "compression_ratio": 1.7268041237113403, "no_speech_prob": 0.0066963862627744675}, {"id": 124, "seek": 85540, "start": 863.8, "end": 869.9599999999999, "text": " this is a scaled object, and yeah, it is not active, that's why the state is unknown,", "tokens": [50784, 341, 307, 257, 36039, 2657, 11, 293, 1338, 11, 309, 307, 406, 4967, 11, 300, 311, 983, 264, 1785, 307, 9841, 11, 51092], "temperature": 0.0, "avg_logprob": -0.15710713001007728, "compression_ratio": 1.7268041237113403, "no_speech_prob": 0.0066963862627744675}, {"id": 125, "seek": 85540, "start": 870.76, "end": 876.92, "text": " and you have not defined the fallback, that's why, but it's ready for scaling. If you go for the, so,", "tokens": [51132, 293, 291, 362, 406, 7642, 264, 2100, 3207, 11, 300, 311, 983, 11, 457, 309, 311, 1919, 337, 21589, 13, 759, 291, 352, 337, 264, 11, 370, 11, 51440], "temperature": 0.0, "avg_logprob": -0.15710713001007728, "compression_ratio": 1.7268041237113403, "no_speech_prob": 0.0066963862627744675}, {"id": 126, "seek": 85540, "start": 879.56, "end": 882.6, "text": " for the scaled object, and HPA will be triggered internally.", "tokens": [51572, 337, 264, 36039, 2657, 11, 293, 389, 10297, 486, 312, 21710, 19501, 13, 51724], "temperature": 0.0, "avg_logprob": -0.15710713001007728, "compression_ratio": 1.7268041237113403, "no_speech_prob": 0.0066963862627744675}, {"id": 127, "seek": 88540, "start": 886.12, "end": 893.16, "text": " So, this is the current load on the RW deployment, and current status, like,", "tokens": [50400, 407, 11, 341, 307, 264, 2190, 3677, 322, 264, 42513, 19317, 11, 293, 2190, 6558, 11, 411, 11, 50752], "temperature": 0.0, "avg_logprob": -0.28807081255996436, "compression_ratio": 1.3496503496503496, "no_speech_prob": 0.00844221469014883}, {"id": 128, "seek": 88540, "start": 893.16, "end": 895.48, "text": " you can see them in NMAX and the pick accounts.", "tokens": [50752, 291, 393, 536, 552, 294, 426, 9998, 55, 293, 264, 1888, 9402, 13, 50868], "temperature": 0.0, "avg_logprob": -0.28807081255996436, "compression_ratio": 1.3496503496503496, "no_speech_prob": 0.00844221469014883}, {"id": 129, "seek": 88540, "start": 900.12, "end": 905.48, "text": " Now, I am doing a watch on the ports, and the scaled object and HPA.", "tokens": [51100, 823, 11, 286, 669, 884, 257, 1159, 322, 264, 18160, 11, 293, 264, 36039, 2657, 293, 389, 10297, 13, 51368], "temperature": 0.0, "avg_logprob": -0.28807081255996436, "compression_ratio": 1.3496503496503496, "no_speech_prob": 0.00844221469014883}, {"id": 130, "seek": 91540, "start": 916.36, "end": 925.8, "text": " So, yeah. Now, I am triggering the load. So, for that, I need to fetch the", "tokens": [50412, 407, 11, 1338, 13, 823, 11, 286, 669, 40406, 264, 3677, 13, 407, 11, 337, 300, 11, 286, 643, 281, 23673, 264, 50884], "temperature": 0.0, "avg_logprob": -0.20300964208749625, "compression_ratio": 1.0, "no_speech_prob": 0.006868032272905111}, {"id": 131, "seek": 92580, "start": 925.8, "end": 936.1999999999999, "text": " credential from the subfuser. So, just getting the access key and secret key.", "tokens": [50364, 22034, 490, 264, 1422, 69, 18088, 13, 407, 11, 445, 1242, 264, 2105, 2141, 293, 4054, 2141, 13, 50884], "temperature": 0.0, "avg_logprob": -0.47502656416459516, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.05018236115574837}, {"id": 132, "seek": 93620, "start": 936.6, "end": 956.6, "text": " So, I am triggering the HS bench. So, HS bench has the access key, so you can", "tokens": [50384, 407, 11, 286, 669, 40406, 264, 34194, 10638, 13, 407, 11, 34194, 10638, 575, 264, 2105, 2141, 11, 370, 291, 393, 51384], "temperature": 0.0, "avg_logprob": -0.2990266799926758, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.005296234041452408}, {"id": 133, "seek": 93620, "start": 956.6, "end": 961.1600000000001, "text": " see the endpoint details. Then, currently, I am running a load of, like,", "tokens": [51384, 536, 264, 35795, 4365, 13, 1396, 11, 4362, 11, 286, 669, 2614, 257, 3677, 295, 11, 411, 11, 51612], "temperature": 0.0, "avg_logprob": -0.2990266799926758, "compression_ratio": 1.3043478260869565, "no_speech_prob": 0.005296234041452408}, {"id": 134, "seek": 96116, "start": 962.12, "end": 968.92, "text": " so, this is, 1 mb is the size of the object, and 1 dollar tree, I mean, 1 hierarchy.", "tokens": [50412, 370, 11, 341, 307, 11, 502, 275, 65, 307, 264, 2744, 295, 264, 2657, 11, 293, 502, 7241, 4230, 11, 286, 914, 11, 502, 22333, 13, 50752], "temperature": 0.0, "avg_logprob": -0.2728977818642893, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.00303965643979609}, {"id": 135, "seek": 96116, "start": 968.92, "end": 971.9599999999999, "text": " This is 10 clients it will be running, and 1000 objects.", "tokens": [50752, 639, 307, 1266, 6982, 309, 486, 312, 2614, 11, 293, 9714, 6565, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2728977818642893, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.00303965643979609}, {"id": 136, "seek": 96116, "start": 976.8399999999999, "end": 982.68, "text": " This is triggered. Now, on the left-hand side, like,", "tokens": [51148, 639, 307, 21710, 13, 823, 11, 322, 264, 1411, 12, 5543, 1252, 11, 411, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2728977818642893, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.00303965643979609}, {"id": 137, "seek": 98268, "start": 982.68, "end": 987.4, "text": " still the, like, it may take some time to flatten that side, like, the watch part.", "tokens": [50364, 920, 264, 11, 411, 11, 309, 815, 747, 512, 565, 281, 24183, 300, 1252, 11, 411, 11, 264, 1159, 644, 13, 50600], "temperature": 0.0, "avg_logprob": -0.15526847581605654, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.009351318702101707}, {"id": 138, "seek": 98268, "start": 996.68, "end": 1001.4, "text": " So, you can see the load is increasing. So, it is still not reaching that 10 million, as I", "tokens": [51064, 407, 11, 291, 393, 536, 264, 3677, 307, 5662, 13, 407, 11, 309, 307, 920, 406, 9906, 300, 1266, 2459, 11, 382, 286, 51300], "temperature": 0.0, "avg_logprob": -0.15526847581605654, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.009351318702101707}, {"id": 139, "seek": 98268, "start": 1001.4, "end": 1006.52, "text": " mentioned before. Like, if it costs 10,000 or something like that, then it triggers the scaling.", "tokens": [51300, 2835, 949, 13, 1743, 11, 498, 309, 5497, 1266, 11, 1360, 420, 746, 411, 300, 11, 550, 309, 22827, 264, 21589, 13, 51556], "temperature": 0.0, "avg_logprob": -0.15526847581605654, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.009351318702101707}, {"id": 140, "seek": 100652, "start": 1007.3199999999999, "end": 1009.24, "text": " So, still I have one port.", "tokens": [50404, 407, 11, 920, 286, 362, 472, 2436, 13, 50500], "temperature": 0.0, "avg_logprob": -0.18855501104284217, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.003919584210962057}, {"id": 141, "seek": 100652, "start": 1013.64, "end": 1025.16, "text": " Yeah. Now, it costs the limit. So, if you look closely, like, so, there is a kind of, like,", "tokens": [50720, 865, 13, 823, 11, 309, 5497, 264, 4948, 13, 407, 11, 498, 291, 574, 8185, 11, 411, 11, 370, 11, 456, 307, 257, 733, 295, 11, 411, 11, 51296], "temperature": 0.0, "avg_logprob": -0.18855501104284217, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.003919584210962057}, {"id": 142, "seek": 100652, "start": 1025.16, "end": 1029.72, "text": " what you said. School of period or something like that, it will wait for some time,", "tokens": [51296, 437, 291, 848, 13, 5070, 295, 2896, 420, 746, 411, 300, 11, 309, 486, 1699, 337, 512, 565, 11, 51524], "temperature": 0.0, "avg_logprob": -0.18855501104284217, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.003919584210962057}, {"id": 143, "seek": 100652, "start": 1030.36, "end": 1033.72, "text": " 90 seconds or something like that before scaling, then only the scale will happen.", "tokens": [51556, 4289, 3949, 420, 746, 411, 300, 949, 21589, 11, 550, 787, 264, 4373, 486, 1051, 13, 51724], "temperature": 0.0, "avg_logprob": -0.18855501104284217, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.003919584210962057}, {"id": 144, "seek": 103372, "start": 1034.44, "end": 1040.52, "text": " As you can see, like, after costing the limit, it just scaled one. Now, it will scale again,", "tokens": [50400, 1018, 291, 393, 536, 11, 411, 11, 934, 37917, 264, 4948, 11, 309, 445, 36039, 472, 13, 823, 11, 309, 486, 4373, 797, 11, 50704], "temperature": 0.0, "avg_logprob": -0.15297803125883402, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.000353350886143744}, {"id": 145, "seek": 103372, "start": 1040.52, "end": 1046.28, "text": " still increasing the two ports is not satisfying the request. So, it scaled again. Now, the workload", "tokens": [50704, 920, 5662, 264, 732, 18160, 307, 406, 18348, 264, 5308, 13, 407, 11, 309, 36039, 797, 13, 823, 11, 264, 20139, 50992], "temperature": 0.0, "avg_logprob": -0.15297803125883402, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.000353350886143744}, {"id": 146, "seek": 103372, "start": 1046.28, "end": 1053.88, "text": " become bit less. Still, it is about 10 million requests. So, now, if I execute here on the", "tokens": [50992, 1813, 857, 1570, 13, 8291, 11, 309, 307, 466, 1266, 2459, 12475, 13, 407, 11, 586, 11, 498, 286, 14483, 510, 322, 264, 51372], "temperature": 0.0, "avg_logprob": -0.15297803125883402, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.000353350886143744}, {"id": 147, "seek": 105388, "start": 1053.88, "end": 1060.1200000000001, "text": " probability server, like, you can see three requests, three instances providing the same request.", "tokens": [50364, 8482, 7154, 11, 411, 11, 291, 393, 536, 1045, 12475, 11, 1045, 14519, 6530, 264, 912, 5308, 13, 50676], "temperature": 0.0, "avg_logprob": -0.29248470306396485, "compression_ratio": 1.443609022556391, "no_speech_prob": 0.006352552212774754}, {"id": 148, "seek": 105388, "start": 1067.4, "end": 1068.8400000000001, "text": " Another fourth port is up.", "tokens": [51040, 3996, 6409, 2436, 307, 493, 13, 51112], "temperature": 0.0, "avg_logprob": -0.29248470306396485, "compression_ratio": 1.443609022556391, "no_speech_prob": 0.006352552212774754}, {"id": 149, "seek": 105388, "start": 1073.16, "end": 1078.6000000000001, "text": " That is a four instances, like, if I go back to the terminal, yeah.", "tokens": [51328, 663, 307, 257, 1451, 14519, 11, 411, 11, 498, 286, 352, 646, 281, 264, 14709, 11, 1338, 13, 51600], "temperature": 0.0, "avg_logprob": -0.29248470306396485, "compression_ratio": 1.443609022556391, "no_speech_prob": 0.006352552212774754}, {"id": 150, "seek": 107860, "start": 1079.48, "end": 1086.36, "text": " So, you can see a bit decrease in the load, but, yeah, the load is never become less than 10.", "tokens": [50408, 407, 11, 291, 393, 536, 257, 857, 11514, 294, 264, 3677, 11, 457, 11, 1338, 11, 264, 3677, 307, 1128, 1813, 1570, 813, 1266, 13, 50752], "temperature": 0.0, "avg_logprob": -0.20868467982811265, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.001045919954776764}, {"id": 151, "seek": 107860, "start": 1086.36, "end": 1097.08, "text": " That says it was still increasing. So, yeah, I guess that is all I have and I do not have the,", "tokens": [50752, 663, 1619, 309, 390, 920, 5662, 13, 407, 11, 1338, 11, 286, 2041, 300, 307, 439, 286, 362, 293, 286, 360, 406, 362, 264, 11, 51288], "temperature": 0.0, "avg_logprob": -0.20868467982811265, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.001045919954776764}, {"id": 152, "seek": 107860, "start": 1097.8799999999999, "end": 1102.52, "text": " like, the scaling down part, like, before that it was taking a lot of time. So,", "tokens": [51328, 411, 11, 264, 21589, 760, 644, 11, 411, 11, 949, 300, 309, 390, 1940, 257, 688, 295, 565, 13, 407, 11, 51560], "temperature": 0.0, "avg_logprob": -0.20868467982811265, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.001045919954776764}, {"id": 153, "seek": 110252, "start": 1103.4, "end": 1107.96, "text": " that is it. Any questions or, like,", "tokens": [50408, 300, 307, 309, 13, 2639, 1651, 420, 11, 411, 11, 50636], "temperature": 0.0, "avg_logprob": -0.30217503566367954, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.0013405076460912824}, {"id": 154, "seek": 110252, "start": 1116.44, "end": 1119.4, "text": " what is the use case of scaling down to zero?", "tokens": [51060, 437, 307, 264, 764, 1389, 295, 21589, 760, 281, 4018, 30, 51208], "temperature": 0.0, "avg_logprob": -0.30217503566367954, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.0013405076460912824}, {"id": 155, "seek": 110252, "start": 1123.56, "end": 1128.2, "text": " So, I have a question. The question is, what is the use case of scaling down to zero?", "tokens": [51416, 407, 11, 286, 362, 257, 1168, 13, 440, 1168, 307, 11, 437, 307, 264, 764, 1389, 295, 21589, 760, 281, 4018, 30, 51648], "temperature": 0.0, "avg_logprob": -0.30217503566367954, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.0013405076460912824}, {"id": 156, "seek": 112820, "start": 1129.16, "end": 1132.8400000000001, "text": " Maybe you can save those sources or see if you are not using that service up.", "tokens": [50412, 2704, 291, 393, 3155, 729, 7139, 420, 536, 498, 291, 366, 406, 1228, 300, 2643, 493, 13, 50596], "temperature": 0.0, "avg_logprob": -0.19497789875153573, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.010585172101855278}, {"id": 157, "seek": 112820, "start": 1132.8400000000001, "end": 1137.56, "text": " It depends on you, like, if you want to defend zero, like, then it will, if it is ideal,", "tokens": [50596, 467, 5946, 322, 291, 11, 411, 11, 498, 291, 528, 281, 8602, 4018, 11, 411, 11, 550, 309, 486, 11, 498, 309, 307, 7157, 11, 50832], "temperature": 0.0, "avg_logprob": -0.19497789875153573, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.010585172101855278}, {"id": 158, "seek": 112820, "start": 1137.56, "end": 1142.92, "text": " it will scale to down to zero. That is it. But will it then scale up fast enough?", "tokens": [50832, 309, 486, 4373, 281, 760, 281, 4018, 13, 663, 307, 309, 13, 583, 486, 309, 550, 4373, 493, 2370, 1547, 30, 51100], "temperature": 0.0, "avg_logprob": -0.19497789875153573, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.010585172101855278}, {"id": 159, "seek": 112820, "start": 1142.92, "end": 1150.2, "text": " Yeah, if it causes the threshold is coming up, I have played zero. So, I do not know whether,", "tokens": [51100, 865, 11, 498, 309, 7700, 264, 14678, 307, 1348, 493, 11, 286, 362, 3737, 4018, 13, 407, 11, 286, 360, 406, 458, 1968, 11, 51464], "temperature": 0.0, "avg_logprob": -0.19497789875153573, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.010585172101855278}, {"id": 160, "seek": 112820, "start": 1150.2, "end": 1154.8400000000001, "text": " whether RGW will have bug because if you scale down, then the server is not the right for RGW.", "tokens": [51464, 1968, 497, 38, 54, 486, 362, 7426, 570, 498, 291, 4373, 760, 11, 550, 264, 7154, 307, 406, 264, 558, 337, 497, 38, 54, 13, 51696], "temperature": 0.0, "avg_logprob": -0.19497789875153573, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.010585172101855278}, {"id": 161, "seek": 115484, "start": 1155.72, "end": 1160.6799999999998, "text": " So, I am not sure whether it will work for RGW. But, yeah, majorly, it will save those sources,", "tokens": [50408, 407, 11, 286, 669, 406, 988, 1968, 309, 486, 589, 337, 497, 38, 54, 13, 583, 11, 1338, 11, 2563, 356, 11, 309, 486, 3155, 729, 7139, 11, 50656], "temperature": 0.0, "avg_logprob": -0.22857353422376844, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.006871001794934273}, {"id": 162, "seek": 115484, "start": 1160.6799999999998, "end": 1165.1599999999999, "text": " if you have anything else. Yeah, sure.", "tokens": [50656, 498, 291, 362, 1340, 1646, 13, 865, 11, 988, 13, 50880], "temperature": 0.0, "avg_logprob": -0.22857353422376844, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.006871001794934273}, {"id": 163, "seek": 115484, "start": 1165.1599999999999, "end": 1169.3999999999999, "text": " With something like that, work for scaling OSDs?", "tokens": [50880, 2022, 746, 411, 300, 11, 589, 337, 21589, 12731, 35, 82, 30, 51092], "temperature": 0.0, "avg_logprob": -0.22857353422376844, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.006871001794934273}, {"id": 164, "seek": 115484, "start": 1169.3999999999999, "end": 1173.32, "text": " OSDs, I am not quite sure because OSD has the dependency of hard disk.", "tokens": [51092, 12731, 35, 82, 11, 286, 669, 406, 1596, 988, 570, 12731, 35, 575, 264, 33621, 295, 1152, 12355, 13, 51288], "temperature": 0.0, "avg_logprob": -0.22857353422376844, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.006871001794934273}, {"id": 165, "seek": 115484, "start": 1173.32, "end": 1179.1599999999999, "text": " So, I do not know how it will play in case of OSDs. But it can obviously work for NFS or it", "tokens": [51288, 407, 11, 286, 360, 406, 458, 577, 309, 486, 862, 294, 1389, 295, 12731, 35, 82, 13, 583, 309, 393, 2745, 589, 337, 13576, 50, 420, 309, 51580], "temperature": 0.0, "avg_logprob": -0.22857353422376844, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.006871001794934273}, {"id": 166, "seek": 117916, "start": 1179.24, "end": 1181.48, "text": " can work for MDSs.", "tokens": [50368, 393, 589, 337, 376, 11844, 82, 13, 50480], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 167, "seek": 117916, "start": 1181.48, "end": 1186.52, "text": " Scaling OSDs is very expensive. You need to move better. It does not fit this,", "tokens": [50480, 2747, 4270, 12731, 35, 82, 307, 588, 5124, 13, 509, 643, 281, 1286, 1101, 13, 467, 775, 406, 3318, 341, 11, 50732], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 168, "seek": 117916, "start": 1186.52, "end": 1192.28, "text": " this method of scaling up and down or momentarily, you think. If you already moved it to OSD,", "tokens": [50732, 341, 3170, 295, 21589, 493, 293, 760, 420, 1623, 3289, 11, 291, 519, 13, 759, 291, 1217, 4259, 309, 281, 12731, 35, 11, 51020], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 169, "seek": 117916, "start": 1192.28, "end": 1194.6000000000001, "text": " you need a big event to move it down.", "tokens": [51020, 291, 643, 257, 955, 2280, 281, 1286, 309, 760, 13, 51136], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 170, "seek": 117916, "start": 1194.6000000000001, "end": 1198.28, "text": " Just whether that would be more of an upgrade for the server.", "tokens": [51136, 1449, 1968, 300, 576, 312, 544, 295, 364, 11484, 337, 264, 7154, 13, 51320], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 171, "seek": 117916, "start": 1198.28, "end": 1206.2, "text": " I think the usual argument for OSDs is that you have to have hard drives, like, in storage,", "tokens": [51320, 286, 519, 264, 7713, 6770, 337, 12731, 35, 82, 307, 300, 291, 362, 281, 362, 1152, 11754, 11, 411, 11, 294, 6725, 11, 51716], "temperature": 0.0, "avg_logprob": -0.36682166709556235, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.25734224915504456}, {"id": 172, "seek": 120620, "start": 1206.68, "end": 1210.8400000000001, "text": " to put OSDs on, and then you might as well deploy them right away, because then the server", "tokens": [50388, 281, 829, 12731, 35, 82, 322, 11, 293, 550, 291, 1062, 382, 731, 7274, 552, 558, 1314, 11, 570, 550, 264, 7154, 50596], "temperature": 0.0, "avg_logprob": -0.2936295444525561, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.1253613829612732}, {"id": 173, "seek": 120620, "start": 1210.8400000000001, "end": 1216.92, "text": " will operate better. But in public cloud, it is different. And then it is cost. You do not want to", "tokens": [50596, 486, 9651, 1101, 13, 583, 294, 1908, 4588, 11, 309, 307, 819, 13, 400, 550, 309, 307, 2063, 13, 509, 360, 406, 528, 281, 50900], "temperature": 0.0, "avg_logprob": -0.2936295444525561, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.1253613829612732}, {"id": 174, "seek": 120620, "start": 1216.92, "end": 1222.92, "text": " scale up because it is cost you more and you want to do it in the last minute. So, it makes sense,", "tokens": [50900, 4373, 493, 570, 309, 307, 2063, 291, 544, 293, 291, 528, 281, 360, 309, 294, 264, 1036, 3456, 13, 407, 11, 309, 1669, 2020, 11, 51200], "temperature": 0.0, "avg_logprob": -0.2936295444525561, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.1253613829612732}, {"id": 175, "seek": 120620, "start": 1222.92, "end": 1231.24, "text": " but not using this. And the problem scaling down OSDs is also not a simple task. You need", "tokens": [51200, 457, 406, 1228, 341, 13, 400, 264, 1154, 21589, 760, 12731, 35, 82, 307, 611, 406, 257, 2199, 5633, 13, 509, 643, 51616], "temperature": 0.0, "avg_logprob": -0.2936295444525561, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.1253613829612732}, {"id": 176, "seek": 123124, "start": 1231.24, "end": 1240.04, "text": " to do it all in a manner, otherwise you will be at risk of errors, trying to take it down.", "tokens": [50364, 281, 360, 309, 439, 294, 257, 9060, 11, 5911, 291, 486, 312, 412, 3148, 295, 13603, 11, 1382, 281, 747, 309, 760, 13, 50804], "temperature": 0.0, "avg_logprob": -0.5294476898623185, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.3334854245185852}, {"id": 177, "seek": 123124, "start": 1243.8, "end": 1249.48, "text": " I try to write once the process how to, in the public cloud, how to scale down OSDs,", "tokens": [50992, 286, 853, 281, 2464, 1564, 264, 1399, 577, 281, 11, 294, 264, 1908, 4588, 11, 577, 281, 4373, 760, 12731, 35, 82, 11, 51276], "temperature": 0.0, "avg_logprob": -0.5294476898623185, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.3334854245185852}, {"id": 178, "seek": 123124, "start": 1249.48, "end": 1254.28, "text": " scale out OSDs. It will document the speech of something like 45 pages.", "tokens": [51276, 4373, 484, 12731, 35, 82, 13, 467, 486, 4166, 264, 6218, 295, 746, 411, 6905, 7183, 13, 51516], "temperature": 0.0, "avg_logprob": -0.5294476898623185, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.3334854245185852}, {"id": 179, "seek": 125428, "start": 1255.0, "end": 1258.68, "text": " If you want to do it in a safe manner, it is not simple.", "tokens": [50400, 759, 291, 528, 281, 360, 309, 294, 257, 3273, 9060, 11, 309, 307, 406, 2199, 13, 50584], "temperature": 0.0, "avg_logprob": -0.49421007492963004, "compression_ratio": 1.118279569892473, "no_speech_prob": 0.08648458123207092}, {"id": 180, "seek": 125428, "start": 1279.0, "end": 1283.24, "text": " So, the question is whether we can use CADA for", "tokens": [51600, 407, 11, 264, 1168, 307, 1968, 321, 393, 764, 41143, 32, 337, 51812], "temperature": 0.0, "avg_logprob": -0.49421007492963004, "compression_ratio": 1.118279569892473, "no_speech_prob": 0.08648458123207092}, {"id": 181, "seek": 128324, "start": 1283.32, "end": 1287.48, "text": " Cepheidium, something like that. So, I guess Cepheidium mostly works with", "tokens": [50368, 383, 595, 26013, 2197, 11, 746, 411, 300, 13, 407, 11, 286, 2041, 383, 595, 26013, 2197, 5240, 1985, 365, 50576], "temperature": 0.0, "avg_logprob": -0.2972055937114515, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.06034848093986511}, {"id": 182, "seek": 128324, "start": 1287.48, "end": 1291.16, "text": " Portman. It does not need the Kubernetes, and this is specific to Kubernetes.", "tokens": [50576, 6733, 1601, 13, 467, 775, 406, 643, 264, 23145, 11, 293, 341, 307, 2685, 281, 23145, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2972055937114515, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.06034848093986511}, {"id": 183, "seek": 128324, "start": 1296.04, "end": 1303.88, "text": " No, not with CADA. Maybe if we have defined something for the Cepheidium step, then yeah,", "tokens": [51004, 883, 11, 406, 365, 41143, 32, 13, 2704, 498, 321, 362, 7642, 746, 337, 264, 383, 595, 26013, 2197, 1823, 11, 550, 1338, 11, 51396], "temperature": 0.0, "avg_logprob": -0.2972055937114515, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.06034848093986511}, {"id": 184, "seek": 128324, "start": 1304.52, "end": 1310.1200000000001, "text": " that is like, based on like, we need to, yeah, like, we need to see the scalar like,", "tokens": [51428, 300, 307, 411, 11, 2361, 322, 411, 11, 321, 643, 281, 11, 1338, 11, 411, 11, 321, 643, 281, 536, 264, 39684, 411, 11, 51708], "temperature": 0.0, "avg_logprob": -0.2972055937114515, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.06034848093986511}, {"id": 185, "seek": 131012, "start": 1310.6799999999998, "end": 1315.4799999999998, "text": " based on like, if this is a question that we had, hit this request, then Cepheidium can trigger", "tokens": [50392, 2361, 322, 411, 11, 498, 341, 307, 257, 1168, 300, 321, 632, 11, 2045, 341, 5308, 11, 550, 383, 595, 26013, 2197, 393, 7875, 50632], "temperature": 0.0, "avg_logprob": -0.5298966579749936, "compression_ratio": 1.3169014084507042, "no_speech_prob": 0.02089371345937252}, {"id": 186, "seek": 131012, "start": 1315.4799999999998, "end": 1318.84, "text": " or Ingressor, etc. That is possible, but not with CADA.", "tokens": [50632, 420, 682, 3091, 284, 11, 5183, 13, 663, 307, 1944, 11, 457, 406, 365, 41143, 32, 13, 50800], "temperature": 0.0, "avg_logprob": -0.5298966579749936, "compression_ratio": 1.3169014084507042, "no_speech_prob": 0.02089371345937252}, {"id": 187, "seek": 131012, "start": 1321.32, "end": 1322.28, "text": " Kubernetes, yeah.", "tokens": [50924, 23145, 11, 1338, 13, 50972], "temperature": 0.0, "avg_logprob": -0.5298966579749936, "compression_ratio": 1.3169014084507042, "no_speech_prob": 0.02089371345937252}, {"id": 188, "seek": 131012, "start": 1326.28, "end": 1327.56, "text": " Okay, yeah, sure.", "tokens": [51172, 1033, 11, 1338, 11, 988, 13, 51236], "temperature": 0.0, "avg_logprob": -0.5298966579749936, "compression_ratio": 1.3169014084507042, "no_speech_prob": 0.02089371345937252}, {"id": 189, "seek": 132756, "start": 1328.52, "end": 1334.12, "text": " Is there anything similar for tuning, for suggesting tuning?", "tokens": [50412, 1119, 456, 1340, 2531, 337, 15164, 11, 337, 18094, 15164, 30, 50692], "temperature": 0.0, "avg_logprob": -0.3390381476458381, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.04916578531265259}, {"id": 190, "seek": 132756, "start": 1337.3999999999999, "end": 1339.72, "text": " Sorry. So, the question is anything?", "tokens": [50856, 4919, 13, 407, 11, 264, 1168, 307, 1340, 30, 50972], "temperature": 0.0, "avg_logprob": -0.3390381476458381, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.04916578531265259}, {"id": 191, "seek": 132756, "start": 1343.0, "end": 1349.0, "text": " So, I think he's asking if there is, I mean, if you can scale or, I mean, if I understood", "tokens": [51136, 407, 11, 286, 519, 415, 311, 3365, 498, 456, 307, 11, 286, 914, 11, 498, 291, 393, 4373, 420, 11, 286, 914, 11, 498, 286, 7320, 51436], "temperature": 0.0, "avg_logprob": -0.3390381476458381, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.04916578531265259}, {"id": 192, "seek": 132756, "start": 1349.0, "end": 1353.32, "text": " correctly, you can suggest configuration tuning based on the workload.", "tokens": [51436, 8944, 11, 291, 393, 3402, 11694, 15164, 2361, 322, 264, 20139, 13, 51652], "temperature": 0.0, "avg_logprob": -0.3390381476458381, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.04916578531265259}, {"id": 193, "seek": 135332, "start": 1353.32, "end": 1360.4399999999998, "text": " So, the tuning comes from the, like, kind of a YAML, like, which preferred YAML you need to", "tokens": [50364, 407, 11, 264, 15164, 1487, 490, 264, 11, 411, 11, 733, 295, 257, 398, 2865, 43, 11, 411, 11, 597, 16494, 398, 2865, 43, 291, 643, 281, 50720], "temperature": 0.0, "avg_logprob": -0.22644317440870332, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0022800618316978216}, {"id": 194, "seek": 135332, "start": 1360.4399999999998, "end": 1366.28, "text": " use or something like that. It's kind of a data based on the scaling, right? And, like, it's,", "tokens": [50720, 764, 420, 746, 411, 300, 13, 467, 311, 733, 295, 257, 1412, 2361, 322, 264, 21589, 11, 558, 30, 400, 11, 411, 11, 309, 311, 11, 51012], "temperature": 0.0, "avg_logprob": -0.22644317440870332, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0022800618316978216}, {"id": 195, "seek": 135332, "start": 1367.24, "end": 1371.32, "text": " I don't know, HPA has some mechanism, like, it's based, the scaling is based on HPA only. So,", "tokens": [51060, 286, 500, 380, 458, 11, 389, 10297, 575, 512, 7513, 11, 411, 11, 309, 311, 2361, 11, 264, 21589, 307, 2361, 322, 389, 10297, 787, 13, 407, 11, 51264], "temperature": 0.0, "avg_logprob": -0.22644317440870332, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0022800618316978216}, {"id": 196, "seek": 135332, "start": 1371.32, "end": 1377.0, "text": " HPA does not look up for the configuration. It just look up for the deployment, like,", "tokens": [51264, 389, 10297, 775, 406, 574, 493, 337, 264, 11694, 13, 467, 445, 574, 493, 337, 264, 19317, 11, 411, 11, 51548], "temperature": 0.0, "avg_logprob": -0.22644317440870332, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0022800618316978216}, {"id": 197, "seek": 135332, "start": 1377.0, "end": 1379.32, "text": " how the deployment see, just see the counts.", "tokens": [51548, 577, 264, 19317, 536, 11, 445, 536, 264, 14893, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22644317440870332, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0022800618316978216}, {"id": 198, "seek": 137932, "start": 1379.8799999999999, "end": 1385.6399999999999, "text": " About tuning, right, a little long topic. But I recently read a research paper where Luster", "tokens": [50392, 7769, 15164, 11, 558, 11, 257, 707, 938, 4829, 13, 583, 286, 3938, 1401, 257, 2132, 3035, 689, 441, 8393, 50680], "temperature": 0.0, "avg_logprob": -0.4668145237198795, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.14167678356170654}, {"id": 199, "seek": 137932, "start": 1386.28, "end": 1391.8, "text": " was doing machine learning performance analysis based on ML and AM workloads and suggesting what", "tokens": [50712, 390, 884, 3479, 2539, 3389, 5215, 2361, 322, 21601, 293, 6475, 32452, 293, 18094, 437, 50988], "temperature": 0.0, "avg_logprob": -0.4668145237198795, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.14167678356170654}, {"id": 200, "seek": 137932, "start": 1391.8, "end": 1398.6, "text": " configuration is that you are doing. There was some, I think I shared that research paper in", "tokens": [50988, 11694, 307, 300, 291, 366, 884, 13, 821, 390, 512, 11, 286, 519, 286, 5507, 300, 2132, 3035, 294, 51328], "temperature": 0.0, "avg_logprob": -0.4668145237198795, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.14167678356170654}, {"id": 201, "seek": 137932, "start": 1398.6, "end": 1405.08, "text": " this performance weekly once and shared that research paper. But this is a really nice idea that,", "tokens": [51328, 341, 3389, 12460, 1564, 293, 5507, 300, 2132, 3035, 13, 583, 341, 307, 257, 534, 1481, 1558, 300, 11, 51652], "temperature": 0.0, "avg_logprob": -0.4668145237198795, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.14167678356170654}, {"id": 202, "seek": 140508, "start": 1405.8, "end": 1411.8, "text": " I mean, based on AI, I mean, you use AI as models to list the workload and everything,", "tokens": [50400, 286, 914, 11, 2361, 322, 7318, 11, 286, 914, 11, 291, 764, 7318, 382, 5245, 281, 1329, 264, 20139, 293, 1203, 11, 50700], "temperature": 0.0, "avg_logprob": -0.5866335088556464, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.15134760737419128}, {"id": 203, "seek": 140508, "start": 1411.8, "end": 1415.24, "text": " and just configuration tuning that I want to, in this F cluster.", "tokens": [50700, 293, 445, 11694, 15164, 300, 286, 528, 281, 11, 294, 341, 479, 13630, 13, 50872], "temperature": 0.0, "avg_logprob": -0.5866335088556464, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.15134760737419128}, {"id": 204, "seek": 140508, "start": 1417.96, "end": 1418.28, "text": " Okay.", "tokens": [51008, 1033, 13, 51024], "temperature": 0.0, "avg_logprob": -0.5866335088556464, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.15134760737419128}, {"id": 205, "seek": 140508, "start": 1418.28, "end": 1422.28, "text": " Discussion was, it's more of a, the discussion more of, like, to which I have a performance", "tokens": [51024, 4208, 25049, 390, 11, 309, 311, 544, 295, 257, 11, 264, 5017, 544, 295, 11, 411, 11, 281, 597, 286, 362, 257, 3389, 51224], "temperature": 0.0, "avg_logprob": -0.5866335088556464, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.15134760737419128}, {"id": 206, "seek": 140508, "start": 1422.28, "end": 1428.9199999999998, "text": " weekly discussion. We have more discussion in the upstream community meetings or in the", "tokens": [51224, 12460, 5017, 13, 492, 362, 544, 5017, 294, 264, 33915, 1768, 8410, 420, 294, 264, 51556], "temperature": 0.0, "avg_logprob": -0.5866335088556464, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.15134760737419128}, {"id": 207, "seek": 142892, "start": 1429.0800000000002, "end": 1432.1200000000001, "text": " network. We could work for them for that.", "tokens": [50372, 3209, 13, 492, 727, 589, 337, 552, 337, 300, 13, 50524], "temperature": 0.2, "avg_logprob": -0.6792667388916016, "compression_ratio": 1.1962616822429906, "no_speech_prob": 0.022870905697345734}, {"id": 208, "seek": 142892, "start": 1435.24, "end": 1438.92, "text": " Sounds fun initially, but I don't know how much", "tokens": [50680, 14576, 1019, 9105, 11, 457, 286, 500, 380, 458, 577, 709, 50864], "temperature": 0.2, "avg_logprob": -0.6792667388916016, "compression_ratio": 1.1962616822429906, "no_speech_prob": 0.022870905697345734}, {"id": 209, "seek": 142892, "start": 1445.72, "end": 1449.0800000000002, "text": " Okay. So, thank you, folks. Thank you.", "tokens": [51204, 1033, 13, 407, 11, 1309, 291, 11, 4024, 13, 1044, 291, 13, 51372], "temperature": 0.2, "avg_logprob": -0.6792667388916016, "compression_ratio": 1.1962616822429906, "no_speech_prob": 0.022870905697345734}], "language": "en"}