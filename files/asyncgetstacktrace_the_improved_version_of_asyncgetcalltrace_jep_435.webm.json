{"text": " Yes. Hi. I'm Johannes Pechberger. As I was already introduced, I work at the sub machine. It's another great distribution of the OpenTraderK. So I worked since the beginning of last year on my new project on Async Get Stack Trace. It's essentially an improved version of the Async Get Call Trace API. And I think many of you probably don't know this API. I didn't know it before I started this project. But essentially, it's related to profiling. So how does profiling work? Some of you might have already seen Flamecraft. If not, there are some other talks on profiling in the Mozilla left room that you can look it up. But essentially, what profiling is, you want to see which parts of your applications are so, for example, here, wanted to see, I can see that some JDK stuff is probably a thing that takes time. But essentially, how it works under the hood is that we have a selection of threads, like for example, here, five threads. Then we randomly select three threads because we cannot usually sample all threads because it would be too costly. Then we pre-allocate some traces. There's just a data structure where we store the stack frame information in. And then we ping the first thread. And with ping, I mean, we send it the signal. And then the signal handle. We walk the stack because in the single handler, the thread is stopped. So we can walk the stack. We do this with the thread two, with the thread five. And we have the traces. And then we store it. And then we do some post-processing. That's essentially how I think profile works, but just in a loop. So in a loop, we already do this. And so we need an API because we need an API. It's called, I think it called trace because we could use JVMTR libraries. They are safe from bias. So they let the threads wait till they're ready, till they're at a safe point. But we want to have the call trace at a certain point where we want it. And so I think that call trace is quite a cool API. So how it works, here we have the stack, how it's on your system. We have at the bottom the pthread start. It's on the Unix system. And on top, we have like some Java frames. And then it goes up till the top to write, write bytes method because it writes to a buffered output stream. It's essentially, hello world, just print some strings. And in the single handler, we get the top frame. That's where the U context from the single handler points to. And then we do some stack walking. And as in get call trace does it for us. And essentially it returns us in a preallocated data structure, the frames. And the number of frames that we got. And it also stores a number of frames in error code if there was an error. And so what we get for every frame is the line number. So it's called line number, but it's essentially the byte code index. I don't know. It's historically this way because this API is like from 2003 around. And we get a method ID. But we only get this information on Java frames. So what are these problems? So don't get missed out. They worked on it for long enough time. So it's unofficial. So it's there in 2003, like for three months. And then Oracle put it out, sun at the time put it away. It's now just lying around as an exported symbol but doesn't have its own header. It's unsupported. So if there's a change in another part of the JVM that potentially breaks it, nobody notices it because there's only one single test that doesn't test that much. So there's also missing information. So it only gives us information on the stack frames of the Java stack, of the Java frames, but not on anything else. And it misses information like inlining, which isn't that great. And so in the beginning of last year, I started to work on a new API because this, I think, is the best we have. And maybe we could do something better. And so I worked, I started to work on Async et cetera. It's now a CHEP candidate. It's 435. So if you want to see the CHEP in its entirety, just go on the OpenTedicay website or read the blog post for this talk and you get a picture of what it does. And so the idea was to create a better API that gives us more information and is far more supported, so with lots of tests with its own header. And so again, we have the stack, our stack, but we then get more information. For example, we get at its most basic level, we also get the kind of the thread that we're running on. So is this thread like in Java mode or is this in GC mode or what is this thread, which is quite neat. And we got more information. For example, we get the BCI. It's not called BCI because, yeah, it's the byte code index. We get the method ID. We get also the type. Is it inlined? Is it native? With native, I mean not CC++, but these boundary methods that are defined in Java, but which code is implemented in CC++. And we also get a compilation level. So is it C1, C2, compiled, or don't compile at all? So this is quite neat because we get more information. But the cool thing is we have options now. With this API, we can set in an integer. Hey, we want to have non-Java frames and we also want to walk non-Java threads, which leads us to this situation where we get information also on the thread on these CC++ frames, which is quite nice. Because for these frames, we get also the type. So it's a CC++ and we also get a program counter. So we can then go back, do some of our own analysis and use DL-SIM to get methods of the DL family and get the method name. And we can also walk with these options non-Java threads. So we see more information. It essentially makes the life of a profile developer far easier because we can now just use this API. It will be supported if it gets in. It will be supported. I'm working on lots and lots of tests. And yeah, I hope it gets in. And as a bonus, what I also introduced is new methods for OpenShiftedHead developers to walk stacks because currently the code is like spread between a few different places. Some of them are copies of others. So it's quite hard when you change some port. You have to change other parts too. So it's essentially technical depth. There were good reasons in the years before, but still I want to make stack walking easier. So the new API that I used in the implementation of my chat proposal allows us to just give a stack walker some options like, hey, I want to walk stacks. I want to skip. I want to walk also non-Java frames. And I can just go over it and say, oh, give me the next frame. And on this next frame, we can ask all the information. Is this a Java frame? Is this a native frame? Which is this compilation level? And this makes it far easier to walk stacks and hopefully makes it easier to combine all the stack walking from some ever-related stack traces from AsyncGetCallTrace, from JVR using one API. And so when you make an improvement in one of these APIs and implementations, you get an improvement on all. So what I've done is that I improved AsyncGetCallTrace with the help of my colleagues to be much safer. So I wrote testing code that used SafeFed so that it checks the pointer. So it kind of checks the pointer before it exists. So it's far safer than I did here for AsyncGetStackTrace. Lots of testing, for example. I did some fuzzing. So I called AsyncGetStackTrace with random u-context, so with randomized frame pointers and stack pointers. And it doesn't crash like for hours on a large machine, which is quite cool. And so this covers AsyncGetAsync profile when it modifies the frame and stack pointer to alleviate some concerns when the VM is like an undefined state. It needs a lot of convincing, so I'm still in the process where I have to talk with all the people from Oracle, all the JVR people. It's a long drawn-out process, but I hope I can convince them. But clearly, because clearly the people on the profile side are really happy to have this because it has many advantages for them. And of course, again, testing because the whole point of this API is that you get more information, but also that it's a better tested API. Currently, I have six tests, and I'm working on more. So I hope that it gets in. Till then, you can see on GitHub, there's a draft PR on the step. Just search in the PRs for draft PR with ISKST in the name. And then you can, yeah, you can follow me on Twitter on our team at SpeedSubmachine. And that's all. Oh, yes, yes, yeah. And I'm also blogging like on mostly nerdlers, and all the blog posts I like also put on Fujay. But yeah, you can follow me there and read on all the topics that they talk today. So, thanks. The question was, can safehatch be called from signal hunters because it uses signals? I think it uses different signals because I didn't have any problems using it from signal hunters. So I have tests. To use us and get stack drives, you have to use signal hunters. So I didn't see any problems so far. I think that's probably, it's even weird because from signal hunters, you can, you cannot do any malloc. So you have to preallocate, but you can call fork. So it's quite, quite interesting. So any other questions? Does it handle in both dynamics, especially, because within that stack, you get like the whole stack of deciding how to dispatch the call? So the question was, does it handle in work dynamics specifically? Now, it just uses, it just is based on the frame stack walking and like the internal mechanism of stack walking. So it doesn't handle it differently than, for example, I think get call trace and trade for. Yeah, that's all Java frames. So that's, that's probably fine. Do you have to change the native parts? Or does it go on all platforms? So the question was, does it work on all platforms? It's known that it doesn't really work on windows just because windows hasn't really a concept of signals. If you have any ideas on getting something like this to work on windows, feel free to drop me a message. So no, I didn't have to change any native parts. I had to change some, I had to create some native parts for testing to modify like the U context because this is highly applications, highly operating systems specific. So the changes to the whole OpenJDK are fairly minimal. So they aren't that large besides passing through some bullets to configure stuff. And the code itself is just a few couple hundred lines. So it's quite simple also to understand. And there's a blog post that describes like reasoning behind it. Any other questions? Yes? Is it already a sub machine? No, it's not yet on the sub machine because I'm still in the process of testing it. So there's of course a podcast. You can already use the JVM when you compile it yourself. I'm in the process of updating my demo repository which contains a modified sync profile that uses it. So you can try it out yourself. I should be right in the next few weeks. It still has some bugs. Yeah. Anything else? Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.4, "text": " Yes. Hi. I'm Johannes Pechberger. As I was already introduced, I work at the sub machine.", "tokens": [1079, 13, 2421, 13, 286, 478, 48455, 2396, 339, 42226, 13, 1018, 286, 390, 1217, 7268, 11, 286, 589, 412, 264, 1422, 3479, 13], "temperature": 0.0, "avg_logprob": -0.34776468534727356, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.17283830046653748}, {"id": 1, "seek": 0, "start": 12.4, "end": 19.240000000000002, "text": " It's another great distribution of the OpenTraderK. So I worked since the beginning of last year", "tokens": [467, 311, 1071, 869, 7316, 295, 264, 7238, 51, 6206, 260, 42, 13, 407, 286, 2732, 1670, 264, 2863, 295, 1036, 1064], "temperature": 0.0, "avg_logprob": -0.34776468534727356, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.17283830046653748}, {"id": 2, "seek": 0, "start": 19.240000000000002, "end": 25.0, "text": " on my new project on Async Get Stack Trace. It's essentially an improved version of the", "tokens": [322, 452, 777, 1716, 322, 1018, 34015, 3240, 37649, 1765, 617, 13, 467, 311, 4476, 364, 9689, 3037, 295, 264], "temperature": 0.0, "avg_logprob": -0.34776468534727356, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.17283830046653748}, {"id": 3, "seek": 2500, "start": 25.0, "end": 32.16, "text": " Async Get Call Trace API. And I think many of you probably don't know this API. I didn't know it", "tokens": [1018, 34015, 3240, 7807, 1765, 617, 9362, 13, 400, 286, 519, 867, 295, 291, 1391, 500, 380, 458, 341, 9362, 13, 286, 994, 380, 458, 309], "temperature": 0.0, "avg_logprob": -0.2356061085616008, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00011185110633959994}, {"id": 4, "seek": 2500, "start": 32.16, "end": 39.04, "text": " before I started this project. But essentially, it's related to profiling. So how does profiling", "tokens": [949, 286, 1409, 341, 1716, 13, 583, 4476, 11, 309, 311, 4077, 281, 1740, 4883, 13, 407, 577, 775, 1740, 4883], "temperature": 0.0, "avg_logprob": -0.2356061085616008, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00011185110633959994}, {"id": 5, "seek": 2500, "start": 39.04, "end": 45.92, "text": " work? Some of you might have already seen Flamecraft. If not, there are some other talks on", "tokens": [589, 30, 2188, 295, 291, 1062, 362, 1217, 1612, 42792, 5611, 13, 759, 406, 11, 456, 366, 512, 661, 6686, 322], "temperature": 0.0, "avg_logprob": -0.2356061085616008, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00011185110633959994}, {"id": 6, "seek": 2500, "start": 45.92, "end": 53.04, "text": " profiling in the Mozilla left room that you can look it up. But essentially, what profiling is,", "tokens": [1740, 4883, 294, 264, 3335, 26403, 1411, 1808, 300, 291, 393, 574, 309, 493, 13, 583, 4476, 11, 437, 1740, 4883, 307, 11], "temperature": 0.0, "avg_logprob": -0.2356061085616008, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00011185110633959994}, {"id": 7, "seek": 5304, "start": 53.04, "end": 59.32, "text": " you want to see which parts of your applications are so, for example, here, wanted to see, I can", "tokens": [291, 528, 281, 536, 597, 3166, 295, 428, 5821, 366, 370, 11, 337, 1365, 11, 510, 11, 1415, 281, 536, 11, 286, 393], "temperature": 0.0, "avg_logprob": -0.2067914061493926, "compression_ratio": 1.7022222222222223, "no_speech_prob": 2.6681183953769505e-05}, {"id": 8, "seek": 5304, "start": 59.32, "end": 65.0, "text": " see that some JDK stuff is probably a thing that takes time. But essentially, how it works under", "tokens": [536, 300, 512, 37082, 42, 1507, 307, 1391, 257, 551, 300, 2516, 565, 13, 583, 4476, 11, 577, 309, 1985, 833], "temperature": 0.0, "avg_logprob": -0.2067914061493926, "compression_ratio": 1.7022222222222223, "no_speech_prob": 2.6681183953769505e-05}, {"id": 9, "seek": 5304, "start": 65.0, "end": 69.72, "text": " the hood is that we have a selection of threads, like for example, here, five threads. Then we", "tokens": [264, 13376, 307, 300, 321, 362, 257, 9450, 295, 19314, 11, 411, 337, 1365, 11, 510, 11, 1732, 19314, 13, 1396, 321], "temperature": 0.0, "avg_logprob": -0.2067914061493926, "compression_ratio": 1.7022222222222223, "no_speech_prob": 2.6681183953769505e-05}, {"id": 10, "seek": 5304, "start": 69.72, "end": 77.75999999999999, "text": " randomly select three threads because we cannot usually sample all threads because it would be", "tokens": [16979, 3048, 1045, 19314, 570, 321, 2644, 2673, 6889, 439, 19314, 570, 309, 576, 312], "temperature": 0.0, "avg_logprob": -0.2067914061493926, "compression_ratio": 1.7022222222222223, "no_speech_prob": 2.6681183953769505e-05}, {"id": 11, "seek": 7776, "start": 77.76, "end": 83.60000000000001, "text": " too costly. Then we pre-allocate some traces. There's just a data structure where we store the", "tokens": [886, 28328, 13, 1396, 321, 659, 12, 336, 42869, 512, 26076, 13, 821, 311, 445, 257, 1412, 3877, 689, 321, 3531, 264], "temperature": 0.0, "avg_logprob": -0.22120077133178712, "compression_ratio": 1.8169014084507042, "no_speech_prob": 6.397734978236258e-05}, {"id": 12, "seek": 7776, "start": 83.60000000000001, "end": 89.84, "text": " stack frame information in. And then we ping the first thread. And with ping, I mean, we send it", "tokens": [8630, 3920, 1589, 294, 13, 400, 550, 321, 26151, 264, 700, 7207, 13, 400, 365, 26151, 11, 286, 914, 11, 321, 2845, 309], "temperature": 0.0, "avg_logprob": -0.22120077133178712, "compression_ratio": 1.8169014084507042, "no_speech_prob": 6.397734978236258e-05}, {"id": 13, "seek": 7776, "start": 89.84, "end": 95.36000000000001, "text": " the signal. And then the signal handle. We walk the stack because in the single handler, the thread", "tokens": [264, 6358, 13, 400, 550, 264, 6358, 4813, 13, 492, 1792, 264, 8630, 570, 294, 264, 2167, 41967, 11, 264, 7207], "temperature": 0.0, "avg_logprob": -0.22120077133178712, "compression_ratio": 1.8169014084507042, "no_speech_prob": 6.397734978236258e-05}, {"id": 14, "seek": 7776, "start": 95.36000000000001, "end": 101.96000000000001, "text": " is stopped. So we can walk the stack. We do this with the thread two, with the thread five. And", "tokens": [307, 5936, 13, 407, 321, 393, 1792, 264, 8630, 13, 492, 360, 341, 365, 264, 7207, 732, 11, 365, 264, 7207, 1732, 13, 400], "temperature": 0.0, "avg_logprob": -0.22120077133178712, "compression_ratio": 1.8169014084507042, "no_speech_prob": 6.397734978236258e-05}, {"id": 15, "seek": 10196, "start": 101.96, "end": 109.6, "text": " we have the traces. And then we store it. And then we do some post-processing. That's", "tokens": [321, 362, 264, 26076, 13, 400, 550, 321, 3531, 309, 13, 400, 550, 321, 360, 512, 2183, 12, 41075, 278, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.2105093735914964, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.9516019165166654e-05}, {"id": 16, "seek": 10196, "start": 109.6, "end": 116.08, "text": " essentially how I think profile works, but just in a loop. So in a loop, we already do this. And", "tokens": [4476, 577, 286, 519, 7964, 1985, 11, 457, 445, 294, 257, 6367, 13, 407, 294, 257, 6367, 11, 321, 1217, 360, 341, 13, 400], "temperature": 0.0, "avg_logprob": -0.2105093735914964, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.9516019165166654e-05}, {"id": 17, "seek": 10196, "start": 116.08, "end": 124.75999999999999, "text": " so we need an API because we need an API. It's called, I think it called trace because we could", "tokens": [370, 321, 643, 364, 9362, 570, 321, 643, 364, 9362, 13, 467, 311, 1219, 11, 286, 519, 309, 1219, 13508, 570, 321, 727], "temperature": 0.0, "avg_logprob": -0.2105093735914964, "compression_ratio": 1.6257309941520468, "no_speech_prob": 1.9516019165166654e-05}, {"id": 18, "seek": 12476, "start": 124.76, "end": 132.36, "text": " use JVMTR libraries. They are safe from bias. So they let the threads wait till they're ready,", "tokens": [764, 508, 53, 44, 25936, 15148, 13, 814, 366, 3273, 490, 12577, 13, 407, 436, 718, 264, 19314, 1699, 4288, 436, 434, 1919, 11], "temperature": 0.0, "avg_logprob": -0.21726027480116836, "compression_ratio": 1.7123893805309736, "no_speech_prob": 2.013208177231718e-05}, {"id": 19, "seek": 12476, "start": 132.36, "end": 138.8, "text": " till they're at a safe point. But we want to have the call trace at a certain point where we", "tokens": [4288, 436, 434, 412, 257, 3273, 935, 13, 583, 321, 528, 281, 362, 264, 818, 13508, 412, 257, 1629, 935, 689, 321], "temperature": 0.0, "avg_logprob": -0.21726027480116836, "compression_ratio": 1.7123893805309736, "no_speech_prob": 2.013208177231718e-05}, {"id": 20, "seek": 12476, "start": 138.8, "end": 143.8, "text": " want it. And so I think that call trace is quite a cool API. So how it works, here we have the stack,", "tokens": [528, 309, 13, 400, 370, 286, 519, 300, 818, 13508, 307, 1596, 257, 1627, 9362, 13, 407, 577, 309, 1985, 11, 510, 321, 362, 264, 8630, 11], "temperature": 0.0, "avg_logprob": -0.21726027480116836, "compression_ratio": 1.7123893805309736, "no_speech_prob": 2.013208177231718e-05}, {"id": 21, "seek": 12476, "start": 143.8, "end": 151.4, "text": " how it's on your system. We have at the bottom the pthread start. It's on the Unix system. And on", "tokens": [577, 309, 311, 322, 428, 1185, 13, 492, 362, 412, 264, 2767, 264, 280, 392, 2538, 722, 13, 467, 311, 322, 264, 1156, 970, 1185, 13, 400, 322], "temperature": 0.0, "avg_logprob": -0.21726027480116836, "compression_ratio": 1.7123893805309736, "no_speech_prob": 2.013208177231718e-05}, {"id": 22, "seek": 15140, "start": 151.4, "end": 159.20000000000002, "text": " top, we have like some Java frames. And then it goes up till the top to write, write bytes method", "tokens": [1192, 11, 321, 362, 411, 512, 10745, 12083, 13, 400, 550, 309, 1709, 493, 4288, 264, 1192, 281, 2464, 11, 2464, 36088, 3170], "temperature": 0.0, "avg_logprob": -0.26053905487060547, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.1085206753923558e-05}, {"id": 23, "seek": 15140, "start": 159.20000000000002, "end": 167.6, "text": " because it writes to a buffered output stream. It's essentially, hello world, just print some", "tokens": [570, 309, 13657, 281, 257, 9204, 4073, 5598, 4309, 13, 467, 311, 4476, 11, 7751, 1002, 11, 445, 4482, 512], "temperature": 0.0, "avg_logprob": -0.26053905487060547, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.1085206753923558e-05}, {"id": 24, "seek": 15140, "start": 167.6, "end": 173.72, "text": " strings. And in the single handler, we get the top frame. That's where the U context from the", "tokens": [13985, 13, 400, 294, 264, 2167, 41967, 11, 321, 483, 264, 1192, 3920, 13, 663, 311, 689, 264, 624, 4319, 490, 264], "temperature": 0.0, "avg_logprob": -0.26053905487060547, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.1085206753923558e-05}, {"id": 25, "seek": 15140, "start": 173.72, "end": 179.04000000000002, "text": " single handler points to. And then we do some stack walking. And as in get call trace does it", "tokens": [2167, 41967, 2793, 281, 13, 400, 550, 321, 360, 512, 8630, 4494, 13, 400, 382, 294, 483, 818, 13508, 775, 309], "temperature": 0.0, "avg_logprob": -0.26053905487060547, "compression_ratio": 1.662280701754386, "no_speech_prob": 2.1085206753923558e-05}, {"id": 26, "seek": 17904, "start": 179.04, "end": 187.92, "text": " for us. And essentially it returns us in a preallocated data structure, the frames. And the", "tokens": [337, 505, 13, 400, 4476, 309, 11247, 505, 294, 257, 659, 336, 905, 770, 1412, 3877, 11, 264, 12083, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.17763404456936582, "compression_ratio": 1.7710280373831775, "no_speech_prob": 3.701950845425017e-05}, {"id": 27, "seek": 17904, "start": 187.92, "end": 192.51999999999998, "text": " number of frames that we got. And it also stores a number of frames in error code if there was an", "tokens": [1230, 295, 12083, 300, 321, 658, 13, 400, 309, 611, 9512, 257, 1230, 295, 12083, 294, 6713, 3089, 498, 456, 390, 364], "temperature": 0.0, "avg_logprob": -0.17763404456936582, "compression_ratio": 1.7710280373831775, "no_speech_prob": 3.701950845425017e-05}, {"id": 28, "seek": 17904, "start": 192.51999999999998, "end": 198.48, "text": " error. And so what we get for every frame is the line number. So it's called line number,", "tokens": [6713, 13, 400, 370, 437, 321, 483, 337, 633, 3920, 307, 264, 1622, 1230, 13, 407, 309, 311, 1219, 1622, 1230, 11], "temperature": 0.0, "avg_logprob": -0.17763404456936582, "compression_ratio": 1.7710280373831775, "no_speech_prob": 3.701950845425017e-05}, {"id": 29, "seek": 17904, "start": 198.48, "end": 205.68, "text": " but it's essentially the byte code index. I don't know. It's historically this way because this API", "tokens": [457, 309, 311, 4476, 264, 40846, 3089, 8186, 13, 286, 500, 380, 458, 13, 467, 311, 16180, 341, 636, 570, 341, 9362], "temperature": 0.0, "avg_logprob": -0.17763404456936582, "compression_ratio": 1.7710280373831775, "no_speech_prob": 3.701950845425017e-05}, {"id": 30, "seek": 20568, "start": 205.68, "end": 213.4, "text": " is like from 2003 around. And we get a method ID. But we only get this information on Java frames.", "tokens": [307, 411, 490, 16416, 926, 13, 400, 321, 483, 257, 3170, 7348, 13, 583, 321, 787, 483, 341, 1589, 322, 10745, 12083, 13], "temperature": 0.0, "avg_logprob": -0.2268210190993089, "compression_ratio": 1.4321608040201006, "no_speech_prob": 7.709948113188148e-05}, {"id": 31, "seek": 20568, "start": 213.4, "end": 221.68, "text": " So what are these problems? So don't get missed out. They worked on it for long enough time. So", "tokens": [407, 437, 366, 613, 2740, 30, 407, 500, 380, 483, 6721, 484, 13, 814, 2732, 322, 309, 337, 938, 1547, 565, 13, 407], "temperature": 0.0, "avg_logprob": -0.2268210190993089, "compression_ratio": 1.4321608040201006, "no_speech_prob": 7.709948113188148e-05}, {"id": 32, "seek": 20568, "start": 221.68, "end": 229.76000000000002, "text": " it's unofficial. So it's there in 2003, like for three months. And then Oracle put it out,", "tokens": [309, 311, 8526, 37661, 13, 407, 309, 311, 456, 294, 16416, 11, 411, 337, 1045, 2493, 13, 400, 550, 25654, 829, 309, 484, 11], "temperature": 0.0, "avg_logprob": -0.2268210190993089, "compression_ratio": 1.4321608040201006, "no_speech_prob": 7.709948113188148e-05}, {"id": 33, "seek": 22976, "start": 229.76, "end": 236.79999999999998, "text": " sun at the time put it away. It's now just lying around as an exported symbol but doesn't have", "tokens": [3295, 412, 264, 565, 829, 309, 1314, 13, 467, 311, 586, 445, 8493, 926, 382, 364, 42055, 5986, 457, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.17617898173146435, "compression_ratio": 1.5340314136125655, "no_speech_prob": 5.0616810767678544e-05}, {"id": 34, "seek": 22976, "start": 236.79999999999998, "end": 245.72, "text": " its own header. It's unsupported. So if there's a change in another part of the JVM that potentially", "tokens": [1080, 1065, 23117, 13, 467, 311, 2693, 10504, 14813, 13, 407, 498, 456, 311, 257, 1319, 294, 1071, 644, 295, 264, 508, 53, 44, 300, 7263], "temperature": 0.0, "avg_logprob": -0.17617898173146435, "compression_ratio": 1.5340314136125655, "no_speech_prob": 5.0616810767678544e-05}, {"id": 35, "seek": 22976, "start": 245.72, "end": 253.92, "text": " breaks it, nobody notices it because there's only one single test that doesn't test that much. So", "tokens": [9857, 309, 11, 5079, 32978, 309, 570, 456, 311, 787, 472, 2167, 1500, 300, 1177, 380, 1500, 300, 709, 13, 407], "temperature": 0.0, "avg_logprob": -0.17617898173146435, "compression_ratio": 1.5340314136125655, "no_speech_prob": 5.0616810767678544e-05}, {"id": 36, "seek": 25392, "start": 253.92, "end": 261.47999999999996, "text": " there's also missing information. So it only gives us information on the stack frames of the", "tokens": [456, 311, 611, 5361, 1589, 13, 407, 309, 787, 2709, 505, 1589, 322, 264, 8630, 12083, 295, 264], "temperature": 0.0, "avg_logprob": -0.19772495107447846, "compression_ratio": 1.6986301369863013, "no_speech_prob": 1.6686997696524486e-05}, {"id": 37, "seek": 25392, "start": 261.47999999999996, "end": 267.59999999999997, "text": " Java stack, of the Java frames, but not on anything else. And it misses information like inlining,", "tokens": [10745, 8630, 11, 295, 264, 10745, 12083, 11, 457, 406, 322, 1340, 1646, 13, 400, 309, 29394, 1589, 411, 294, 31079, 11], "temperature": 0.0, "avg_logprob": -0.19772495107447846, "compression_ratio": 1.6986301369863013, "no_speech_prob": 1.6686997696524486e-05}, {"id": 38, "seek": 25392, "start": 267.59999999999997, "end": 274.96, "text": " which isn't that great. And so in the beginning of last year, I started to work on a new API", "tokens": [597, 1943, 380, 300, 869, 13, 400, 370, 294, 264, 2863, 295, 1036, 1064, 11, 286, 1409, 281, 589, 322, 257, 777, 9362], "temperature": 0.0, "avg_logprob": -0.19772495107447846, "compression_ratio": 1.6986301369863013, "no_speech_prob": 1.6686997696524486e-05}, {"id": 39, "seek": 25392, "start": 274.96, "end": 283.4, "text": " because this, I think, is the best we have. And maybe we could do something better. And", "tokens": [570, 341, 11, 286, 519, 11, 307, 264, 1151, 321, 362, 13, 400, 1310, 321, 727, 360, 746, 1101, 13, 400], "temperature": 0.0, "avg_logprob": -0.19772495107447846, "compression_ratio": 1.6986301369863013, "no_speech_prob": 1.6686997696524486e-05}, {"id": 40, "seek": 28340, "start": 283.4, "end": 292.96, "text": " so I worked, I started to work on Async et cetera. It's now a CHEP candidate. It's 435. So if you", "tokens": [370, 286, 2732, 11, 286, 1409, 281, 589, 322, 1018, 34015, 1030, 11458, 13, 467, 311, 586, 257, 5995, 8929, 11532, 13, 467, 311, 1017, 8794, 13, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.2778523467307867, "compression_ratio": 1.4472361809045227, "no_speech_prob": 3.526032014633529e-05}, {"id": 41, "seek": 28340, "start": 292.96, "end": 299.56, "text": " want to see the CHEP in its entirety, just go on the OpenTedicay website or read the blog post", "tokens": [528, 281, 536, 264, 5995, 8929, 294, 1080, 31557, 11, 445, 352, 322, 264, 7238, 51, 292, 299, 320, 3144, 420, 1401, 264, 6968, 2183], "temperature": 0.0, "avg_logprob": -0.2778523467307867, "compression_ratio": 1.4472361809045227, "no_speech_prob": 3.526032014633529e-05}, {"id": 42, "seek": 28340, "start": 299.56, "end": 307.52, "text": " for this talk and you get a picture of what it does. And so the idea was to create a better API", "tokens": [337, 341, 751, 293, 291, 483, 257, 3036, 295, 437, 309, 775, 13, 400, 370, 264, 1558, 390, 281, 1884, 257, 1101, 9362], "temperature": 0.0, "avg_logprob": -0.2778523467307867, "compression_ratio": 1.4472361809045227, "no_speech_prob": 3.526032014633529e-05}, {"id": 43, "seek": 30752, "start": 307.52, "end": 313.76, "text": " that gives us more information and is far more supported, so with lots of tests with its own", "tokens": [300, 2709, 505, 544, 1589, 293, 307, 1400, 544, 8104, 11, 370, 365, 3195, 295, 6921, 365, 1080, 1065], "temperature": 0.0, "avg_logprob": -0.1379921273009418, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.981456004315987e-05}, {"id": 44, "seek": 30752, "start": 313.76, "end": 323.68, "text": " header. And so again, we have the stack, our stack, but we then get more information. For", "tokens": [23117, 13, 400, 370, 797, 11, 321, 362, 264, 8630, 11, 527, 8630, 11, 457, 321, 550, 483, 544, 1589, 13, 1171], "temperature": 0.0, "avg_logprob": -0.1379921273009418, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.981456004315987e-05}, {"id": 45, "seek": 30752, "start": 323.68, "end": 330.32, "text": " example, we get at its most basic level, we also get the kind of the thread that we're running on.", "tokens": [1365, 11, 321, 483, 412, 1080, 881, 3875, 1496, 11, 321, 611, 483, 264, 733, 295, 264, 7207, 300, 321, 434, 2614, 322, 13], "temperature": 0.0, "avg_logprob": -0.1379921273009418, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.981456004315987e-05}, {"id": 46, "seek": 33032, "start": 330.32, "end": 339.15999999999997, "text": " So is this thread like in Java mode or is this in GC mode or what is this thread, which is quite", "tokens": [407, 307, 341, 7207, 411, 294, 10745, 4391, 420, 307, 341, 294, 29435, 4391, 420, 437, 307, 341, 7207, 11, 597, 307, 1596], "temperature": 0.0, "avg_logprob": -0.21329183356706485, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.7770817066775635e-05}, {"id": 47, "seek": 33032, "start": 339.15999999999997, "end": 344.84, "text": " neat. And we got more information. For example, we get the BCI. It's not called BCI because, yeah,", "tokens": [10654, 13, 400, 321, 658, 544, 1589, 13, 1171, 1365, 11, 321, 483, 264, 14359, 40, 13, 467, 311, 406, 1219, 14359, 40, 570, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.21329183356706485, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.7770817066775635e-05}, {"id": 48, "seek": 33032, "start": 344.84, "end": 351.15999999999997, "text": " it's the byte code index. We get the method ID. We get also the type. Is it inlined? Is it native?", "tokens": [309, 311, 264, 40846, 3089, 8186, 13, 492, 483, 264, 3170, 7348, 13, 492, 483, 611, 264, 2010, 13, 1119, 309, 294, 13564, 30, 1119, 309, 8470, 30], "temperature": 0.0, "avg_logprob": -0.21329183356706485, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.7770817066775635e-05}, {"id": 49, "seek": 35116, "start": 351.16, "end": 362.16, "text": " With native, I mean not CC++, but these boundary methods that are defined in Java, but which code", "tokens": [2022, 8470, 11, 286, 914, 406, 12630, 25472, 11, 457, 613, 12866, 7150, 300, 366, 7642, 294, 10745, 11, 457, 597, 3089], "temperature": 0.0, "avg_logprob": -0.19505253591035543, "compression_ratio": 1.4479166666666667, "no_speech_prob": 3.4797129046637565e-05}, {"id": 50, "seek": 35116, "start": 362.16, "end": 368.88, "text": " is implemented in CC++. And we also get a compilation level. So is it C1, C2, compiled,", "tokens": [307, 12270, 294, 12630, 25472, 13, 400, 321, 611, 483, 257, 40261, 1496, 13, 407, 307, 309, 383, 16, 11, 383, 17, 11, 36548, 11], "temperature": 0.0, "avg_logprob": -0.19505253591035543, "compression_ratio": 1.4479166666666667, "no_speech_prob": 3.4797129046637565e-05}, {"id": 51, "seek": 35116, "start": 368.88, "end": 375.88, "text": " or don't compile at all? So this is quite neat because we get more information. But the cool", "tokens": [420, 500, 380, 31413, 412, 439, 30, 407, 341, 307, 1596, 10654, 570, 321, 483, 544, 1589, 13, 583, 264, 1627], "temperature": 0.0, "avg_logprob": -0.19505253591035543, "compression_ratio": 1.4479166666666667, "no_speech_prob": 3.4797129046637565e-05}, {"id": 52, "seek": 37588, "start": 375.88, "end": 383.56, "text": " thing is we have options now. With this API, we can set in an integer. Hey, we want to have", "tokens": [551, 307, 321, 362, 3956, 586, 13, 2022, 341, 9362, 11, 321, 393, 992, 294, 364, 24922, 13, 1911, 11, 321, 528, 281, 362], "temperature": 0.0, "avg_logprob": -0.1662729162918894, "compression_ratio": 1.6114285714285714, "no_speech_prob": 8.089353650575504e-05}, {"id": 53, "seek": 37588, "start": 383.56, "end": 392.2, "text": " non-Java frames and we also want to walk non-Java threads, which leads us to this situation where", "tokens": [2107, 12, 41, 4061, 12083, 293, 321, 611, 528, 281, 1792, 2107, 12, 41, 4061, 19314, 11, 597, 6689, 505, 281, 341, 2590, 689], "temperature": 0.0, "avg_logprob": -0.1662729162918894, "compression_ratio": 1.6114285714285714, "no_speech_prob": 8.089353650575504e-05}, {"id": 54, "seek": 37588, "start": 392.2, "end": 400.2, "text": " we get information also on the thread on these CC++ frames, which is quite nice. Because for", "tokens": [321, 483, 1589, 611, 322, 264, 7207, 322, 613, 12630, 25472, 12083, 11, 597, 307, 1596, 1481, 13, 1436, 337], "temperature": 0.0, "avg_logprob": -0.1662729162918894, "compression_ratio": 1.6114285714285714, "no_speech_prob": 8.089353650575504e-05}, {"id": 55, "seek": 40020, "start": 400.2, "end": 406.64, "text": " these frames, we get also the type. So it's a CC++ and we also get a program counter. So we can", "tokens": [613, 12083, 11, 321, 483, 611, 264, 2010, 13, 407, 309, 311, 257, 12630, 25472, 293, 321, 611, 483, 257, 1461, 5682, 13, 407, 321, 393], "temperature": 0.0, "avg_logprob": -0.19495363796458526, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.4057462067285087e-05}, {"id": 56, "seek": 40020, "start": 406.64, "end": 417.4, "text": " then go back, do some of our own analysis and use DL-SIM to get methods of the DL family and get", "tokens": [550, 352, 646, 11, 360, 512, 295, 527, 1065, 5215, 293, 764, 413, 43, 12, 50, 6324, 281, 483, 7150, 295, 264, 413, 43, 1605, 293, 483], "temperature": 0.0, "avg_logprob": -0.19495363796458526, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.4057462067285087e-05}, {"id": 57, "seek": 40020, "start": 417.4, "end": 423.76, "text": " the method name. And we can also walk with these options non-Java threads. So we see more", "tokens": [264, 3170, 1315, 13, 400, 321, 393, 611, 1792, 365, 613, 3956, 2107, 12, 41, 4061, 19314, 13, 407, 321, 536, 544], "temperature": 0.0, "avg_logprob": -0.19495363796458526, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.4057462067285087e-05}, {"id": 58, "seek": 40020, "start": 423.76, "end": 429.96, "text": " information. It essentially makes the life of a profile developer far easier because we can now", "tokens": [1589, 13, 467, 4476, 1669, 264, 993, 295, 257, 7964, 10754, 1400, 3571, 570, 321, 393, 586], "temperature": 0.0, "avg_logprob": -0.19495363796458526, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.4057462067285087e-05}, {"id": 59, "seek": 42996, "start": 429.96, "end": 436.56, "text": " just use this API. It will be supported if it gets in. It will be supported. I'm working on lots", "tokens": [445, 764, 341, 9362, 13, 467, 486, 312, 8104, 498, 309, 2170, 294, 13, 467, 486, 312, 8104, 13, 286, 478, 1364, 322, 3195], "temperature": 0.0, "avg_logprob": -0.21921475728352866, "compression_ratio": 1.5875, "no_speech_prob": 7.958282367326319e-05}, {"id": 60, "seek": 42996, "start": 436.56, "end": 447.44, "text": " and lots of tests. And yeah, I hope it gets in. And as a bonus, what I also introduced is new", "tokens": [293, 3195, 295, 6921, 13, 400, 1338, 11, 286, 1454, 309, 2170, 294, 13, 400, 382, 257, 10882, 11, 437, 286, 611, 7268, 307, 777], "temperature": 0.0, "avg_logprob": -0.21921475728352866, "compression_ratio": 1.5875, "no_speech_prob": 7.958282367326319e-05}, {"id": 61, "seek": 42996, "start": 447.44, "end": 452.59999999999997, "text": " methods for OpenShiftedHead developers to walk stacks because currently the code is like spread", "tokens": [7150, 337, 7238, 7774, 2008, 292, 39, 2056, 8849, 281, 1792, 30792, 570, 4362, 264, 3089, 307, 411, 3974], "temperature": 0.0, "avg_logprob": -0.21921475728352866, "compression_ratio": 1.5875, "no_speech_prob": 7.958282367326319e-05}, {"id": 62, "seek": 42996, "start": 452.59999999999997, "end": 457.64, "text": " between a few different places. Some of them are copies of others. So it's quite hard when you", "tokens": [1296, 257, 1326, 819, 3190, 13, 2188, 295, 552, 366, 14341, 295, 2357, 13, 407, 309, 311, 1596, 1152, 562, 291], "temperature": 0.0, "avg_logprob": -0.21921475728352866, "compression_ratio": 1.5875, "no_speech_prob": 7.958282367326319e-05}, {"id": 63, "seek": 45764, "start": 457.64, "end": 465.8, "text": " change some port. You have to change other parts too. So it's essentially technical depth. There", "tokens": [1319, 512, 2436, 13, 509, 362, 281, 1319, 661, 3166, 886, 13, 407, 309, 311, 4476, 6191, 7161, 13, 821], "temperature": 0.0, "avg_logprob": -0.13961067752561707, "compression_ratio": 1.4595959595959596, "no_speech_prob": 2.4662538635311648e-05}, {"id": 64, "seek": 45764, "start": 465.8, "end": 475.76, "text": " were good reasons in the years before, but still I want to make stack walking easier. So the new", "tokens": [645, 665, 4112, 294, 264, 924, 949, 11, 457, 920, 286, 528, 281, 652, 8630, 4494, 3571, 13, 407, 264, 777], "temperature": 0.0, "avg_logprob": -0.13961067752561707, "compression_ratio": 1.4595959595959596, "no_speech_prob": 2.4662538635311648e-05}, {"id": 65, "seek": 45764, "start": 475.76, "end": 484.64, "text": " API that I used in the implementation of my chat proposal allows us to just give a stack walker", "tokens": [9362, 300, 286, 1143, 294, 264, 11420, 295, 452, 5081, 11494, 4045, 505, 281, 445, 976, 257, 8630, 1792, 260], "temperature": 0.0, "avg_logprob": -0.13961067752561707, "compression_ratio": 1.4595959595959596, "no_speech_prob": 2.4662538635311648e-05}, {"id": 66, "seek": 48464, "start": 484.64, "end": 490.64, "text": " some options like, hey, I want to walk stacks. I want to skip. I want to walk also non-Java", "tokens": [512, 3956, 411, 11, 4177, 11, 286, 528, 281, 1792, 30792, 13, 286, 528, 281, 10023, 13, 286, 528, 281, 1792, 611, 2107, 12, 41, 4061], "temperature": 0.0, "avg_logprob": -0.1641983518413469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 6.19784914306365e-05}, {"id": 67, "seek": 48464, "start": 490.64, "end": 496.12, "text": " frames. And I can just go over it and say, oh, give me the next frame. And on this next frame,", "tokens": [12083, 13, 400, 286, 393, 445, 352, 670, 309, 293, 584, 11, 1954, 11, 976, 385, 264, 958, 3920, 13, 400, 322, 341, 958, 3920, 11], "temperature": 0.0, "avg_logprob": -0.1641983518413469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 6.19784914306365e-05}, {"id": 68, "seek": 48464, "start": 496.12, "end": 501.52, "text": " we can ask all the information. Is this a Java frame? Is this a native frame? Which is this", "tokens": [321, 393, 1029, 439, 264, 1589, 13, 1119, 341, 257, 10745, 3920, 30, 1119, 341, 257, 8470, 3920, 30, 3013, 307, 341], "temperature": 0.0, "avg_logprob": -0.1641983518413469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 6.19784914306365e-05}, {"id": 69, "seek": 48464, "start": 501.52, "end": 509.64, "text": " compilation level? And this makes it far easier to walk stacks and hopefully makes it easier to", "tokens": [40261, 1496, 30, 400, 341, 1669, 309, 1400, 3571, 281, 1792, 30792, 293, 4696, 1669, 309, 3571, 281], "temperature": 0.0, "avg_logprob": -0.1641983518413469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 6.19784914306365e-05}, {"id": 70, "seek": 50964, "start": 509.64, "end": 516.76, "text": " combine all the stack walking from some ever-related stack traces from AsyncGetCallTrace,", "tokens": [10432, 439, 264, 8630, 4494, 490, 512, 1562, 12, 12004, 8630, 26076, 490, 1018, 34015, 18133, 46113, 14252, 617, 11], "temperature": 0.0, "avg_logprob": -0.20521610661556847, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.836006387951784e-05}, {"id": 71, "seek": 50964, "start": 516.76, "end": 526.4399999999999, "text": " from JVR using one API. And so when you make an improvement in one of these APIs and implementations,", "tokens": [490, 508, 53, 49, 1228, 472, 9362, 13, 400, 370, 562, 291, 652, 364, 10444, 294, 472, 295, 613, 21445, 293, 4445, 763, 11], "temperature": 0.0, "avg_logprob": -0.20521610661556847, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.836006387951784e-05}, {"id": 72, "seek": 50964, "start": 526.4399999999999, "end": 534.4399999999999, "text": " you get an improvement on all. So what I've done is that I improved AsyncGetCallTrace with the", "tokens": [291, 483, 364, 10444, 322, 439, 13, 407, 437, 286, 600, 1096, 307, 300, 286, 9689, 1018, 34015, 18133, 46113, 14252, 617, 365, 264], "temperature": 0.0, "avg_logprob": -0.20521610661556847, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.836006387951784e-05}, {"id": 73, "seek": 53444, "start": 534.44, "end": 544.08, "text": " help of my colleagues to be much safer. So I wrote testing code that used SafeFed so that it", "tokens": [854, 295, 452, 7734, 281, 312, 709, 15856, 13, 407, 286, 4114, 4997, 3089, 300, 1143, 27030, 37, 292, 370, 300, 309], "temperature": 0.0, "avg_logprob": -0.23503358336700791, "compression_ratio": 1.6236559139784945, "no_speech_prob": 5.140981375006959e-05}, {"id": 74, "seek": 53444, "start": 544.08, "end": 552.2, "text": " checks the pointer. So it kind of checks the pointer before it exists. So it's far safer than I did", "tokens": [13834, 264, 23918, 13, 407, 309, 733, 295, 13834, 264, 23918, 949, 309, 8198, 13, 407, 309, 311, 1400, 15856, 813, 286, 630], "temperature": 0.0, "avg_logprob": -0.23503358336700791, "compression_ratio": 1.6236559139784945, "no_speech_prob": 5.140981375006959e-05}, {"id": 75, "seek": 53444, "start": 552.2, "end": 558.6400000000001, "text": " here for AsyncGetStackTrace. Lots of testing, for example. I did some fuzzing. So I called AsyncGetStackTrace", "tokens": [510, 337, 1018, 34015, 18133, 4520, 501, 14252, 617, 13, 15908, 295, 4997, 11, 337, 1365, 13, 286, 630, 512, 283, 3334, 8781, 13, 407, 286, 1219, 1018, 34015, 18133, 4520, 501, 14252, 617], "temperature": 0.0, "avg_logprob": -0.23503358336700791, "compression_ratio": 1.6236559139784945, "no_speech_prob": 5.140981375006959e-05}, {"id": 76, "seek": 55864, "start": 558.64, "end": 566.04, "text": " with random u-context, so with randomized frame pointers and stack pointers. And it doesn't crash", "tokens": [365, 4974, 344, 12, 9000, 3828, 11, 370, 365, 38513, 3920, 44548, 293, 8630, 44548, 13, 400, 309, 1177, 380, 8252], "temperature": 0.0, "avg_logprob": -0.22499512935030289, "compression_ratio": 1.543956043956044, "no_speech_prob": 3.16847836074885e-05}, {"id": 77, "seek": 55864, "start": 566.04, "end": 574.48, "text": " like for hours on a large machine, which is quite cool. And so this covers AsyncGetAsync", "tokens": [411, 337, 2496, 322, 257, 2416, 3479, 11, 597, 307, 1596, 1627, 13, 400, 370, 341, 10538, 1018, 34015, 18133, 10884, 34015], "temperature": 0.0, "avg_logprob": -0.22499512935030289, "compression_ratio": 1.543956043956044, "no_speech_prob": 3.16847836074885e-05}, {"id": 78, "seek": 55864, "start": 574.48, "end": 584.2, "text": " profile when it modifies the frame and stack pointer to alleviate some concerns when the VM is", "tokens": [7964, 562, 309, 1072, 11221, 264, 3920, 293, 8630, 23918, 281, 42701, 512, 7389, 562, 264, 18038, 307], "temperature": 0.0, "avg_logprob": -0.22499512935030289, "compression_ratio": 1.543956043956044, "no_speech_prob": 3.16847836074885e-05}, {"id": 79, "seek": 58420, "start": 584.2, "end": 590.6400000000001, "text": " like an undefined state. It needs a lot of convincing, so I'm still in the process where I", "tokens": [411, 364, 674, 5666, 2001, 1785, 13, 467, 2203, 257, 688, 295, 24823, 11, 370, 286, 478, 920, 294, 264, 1399, 689, 286], "temperature": 0.0, "avg_logprob": -0.18035103877385458, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.8283491196343675e-05}, {"id": 80, "seek": 58420, "start": 590.6400000000001, "end": 596.0, "text": " have to talk with all the people from Oracle, all the JVR people. It's a long drawn-out process,", "tokens": [362, 281, 751, 365, 439, 264, 561, 490, 25654, 11, 439, 264, 508, 53, 49, 561, 13, 467, 311, 257, 938, 10117, 12, 346, 1399, 11], "temperature": 0.0, "avg_logprob": -0.18035103877385458, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.8283491196343675e-05}, {"id": 81, "seek": 58420, "start": 596.0, "end": 603.12, "text": " but I hope I can convince them. But clearly, because clearly the people on the profile", "tokens": [457, 286, 1454, 286, 393, 13447, 552, 13, 583, 4448, 11, 570, 4448, 264, 561, 322, 264, 7964], "temperature": 0.0, "avg_logprob": -0.18035103877385458, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.8283491196343675e-05}, {"id": 82, "seek": 58420, "start": 603.12, "end": 610.2800000000001, "text": " side are really happy to have this because it has many advantages for them. And of course,", "tokens": [1252, 366, 534, 2055, 281, 362, 341, 570, 309, 575, 867, 14906, 337, 552, 13, 400, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.18035103877385458, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.8283491196343675e-05}, {"id": 83, "seek": 61028, "start": 610.28, "end": 615.8, "text": " again, testing because the whole point of this API is that you get more information,", "tokens": [797, 11, 4997, 570, 264, 1379, 935, 295, 341, 9362, 307, 300, 291, 483, 544, 1589, 11], "temperature": 0.0, "avg_logprob": -0.16268406175587274, "compression_ratio": 1.4324324324324325, "no_speech_prob": 5.5587737733731046e-05}, {"id": 84, "seek": 61028, "start": 615.8, "end": 622.1999999999999, "text": " but also that it's a better tested API. Currently, I have six tests, and I'm working on more.", "tokens": [457, 611, 300, 309, 311, 257, 1101, 8246, 9362, 13, 19964, 11, 286, 362, 2309, 6921, 11, 293, 286, 478, 1364, 322, 544, 13], "temperature": 0.0, "avg_logprob": -0.16268406175587274, "compression_ratio": 1.4324324324324325, "no_speech_prob": 5.5587737733731046e-05}, {"id": 85, "seek": 61028, "start": 622.1999999999999, "end": 637.0799999999999, "text": " So I hope that it gets in. Till then, you can see on GitHub, there's a draft PR on the", "tokens": [407, 286, 1454, 300, 309, 2170, 294, 13, 20227, 550, 11, 291, 393, 536, 322, 23331, 11, 456, 311, 257, 11206, 11568, 322, 264], "temperature": 0.0, "avg_logprob": -0.16268406175587274, "compression_ratio": 1.4324324324324325, "no_speech_prob": 5.5587737733731046e-05}, {"id": 86, "seek": 63708, "start": 637.08, "end": 644.84, "text": " step. Just search in the PRs for draft PR with ISKST in the name. And then you can, yeah,", "tokens": [1823, 13, 1449, 3164, 294, 264, 11568, 82, 337, 11206, 11568, 365, 6205, 42, 6840, 294, 264, 1315, 13, 400, 550, 291, 393, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.30825930508700283, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012520317977759987}, {"id": 87, "seek": 63708, "start": 644.84, "end": 652.2, "text": " you can follow me on Twitter on our team at SpeedSubmachine. And that's all. Oh, yes, yes,", "tokens": [291, 393, 1524, 385, 322, 5794, 322, 527, 1469, 412, 18774, 39582, 46061, 13, 400, 300, 311, 439, 13, 876, 11, 2086, 11, 2086, 11], "temperature": 0.0, "avg_logprob": -0.30825930508700283, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012520317977759987}, {"id": 88, "seek": 63708, "start": 652.2, "end": 659.0, "text": " yeah. And I'm also blogging like on mostly nerdlers, and all the blog posts I like also put", "tokens": [1338, 13, 400, 286, 478, 611, 6968, 3249, 411, 322, 5240, 23229, 11977, 11, 293, 439, 264, 6968, 12300, 286, 411, 611, 829], "temperature": 0.0, "avg_logprob": -0.30825930508700283, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012520317977759987}, {"id": 89, "seek": 65900, "start": 659.0, "end": 668.12, "text": " on Fujay. But yeah, you can follow me there and read on all the topics that they talk today. So, thanks.", "tokens": [322, 43915, 320, 13, 583, 1338, 11, 291, 393, 1524, 385, 456, 293, 1401, 322, 439, 264, 8378, 300, 436, 751, 965, 13, 407, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.36192281784549835, "compression_ratio": 1.1304347826086956, "no_speech_prob": 0.00024449455668218434}, {"id": 90, "seek": 66812, "start": 668.12, "end": 692.04, "text": " The question was, can safehatch be called from signal hunters because it uses signals? I think", "tokens": [440, 1168, 390, 11, 393, 3273, 71, 852, 312, 1219, 490, 6358, 29509, 570, 309, 4960, 12354, 30, 286, 519], "temperature": 0.0, "avg_logprob": -0.2646768358018663, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.0013838380109518766}, {"id": 91, "seek": 66812, "start": 692.04, "end": 696.84, "text": " it uses different signals because I didn't have any problems using it from signal hunters. So I", "tokens": [309, 4960, 819, 12354, 570, 286, 994, 380, 362, 604, 2740, 1228, 309, 490, 6358, 29509, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.2646768358018663, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.0013838380109518766}, {"id": 92, "seek": 69684, "start": 696.84, "end": 704.84, "text": " have tests. To use us and get stack drives, you have to use signal hunters. So I didn't see any", "tokens": [362, 6921, 13, 1407, 764, 505, 293, 483, 8630, 11754, 11, 291, 362, 281, 764, 6358, 29509, 13, 407, 286, 994, 380, 536, 604], "temperature": 0.0, "avg_logprob": -0.23976954065188014, "compression_ratio": 1.65625, "no_speech_prob": 0.00022639438975602388}, {"id": 93, "seek": 69684, "start": 704.84, "end": 709.24, "text": " problems so far. I think that's probably, it's even weird because from signal hunters, you can,", "tokens": [2740, 370, 1400, 13, 286, 519, 300, 311, 1391, 11, 309, 311, 754, 3657, 570, 490, 6358, 29509, 11, 291, 393, 11], "temperature": 0.0, "avg_logprob": -0.23976954065188014, "compression_ratio": 1.65625, "no_speech_prob": 0.00022639438975602388}, {"id": 94, "seek": 69684, "start": 709.24, "end": 714.84, "text": " you cannot do any malloc. So you have to preallocate, but you can call fork. So it's quite,", "tokens": [291, 2644, 360, 604, 16026, 905, 13, 407, 291, 362, 281, 659, 336, 42869, 11, 457, 291, 393, 818, 17716, 13, 407, 309, 311, 1596, 11], "temperature": 0.0, "avg_logprob": -0.23976954065188014, "compression_ratio": 1.65625, "no_speech_prob": 0.00022639438975602388}, {"id": 95, "seek": 69684, "start": 714.84, "end": 721.96, "text": " quite interesting. So any other questions? Does it handle in both dynamics, especially,", "tokens": [1596, 1880, 13, 407, 604, 661, 1651, 30, 4402, 309, 4813, 294, 1293, 15679, 11, 2318, 11], "temperature": 0.0, "avg_logprob": -0.23976954065188014, "compression_ratio": 1.65625, "no_speech_prob": 0.00022639438975602388}, {"id": 96, "seek": 72196, "start": 721.96, "end": 728.76, "text": " because within that stack, you get like the whole stack of deciding how to dispatch the call?", "tokens": [570, 1951, 300, 8630, 11, 291, 483, 411, 264, 1379, 8630, 295, 17990, 577, 281, 36729, 264, 818, 30], "temperature": 0.0, "avg_logprob": -0.27333676537802054, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0002232570550404489}, {"id": 97, "seek": 72196, "start": 731.88, "end": 738.2, "text": " So the question was, does it handle in work dynamics specifically? Now, it just uses,", "tokens": [407, 264, 1168, 390, 11, 775, 309, 4813, 294, 589, 15679, 4682, 30, 823, 11, 309, 445, 4960, 11], "temperature": 0.0, "avg_logprob": -0.27333676537802054, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0002232570550404489}, {"id": 98, "seek": 72196, "start": 738.2, "end": 744.6, "text": " it just is based on the frame stack walking and like the internal mechanism of stack walking. So", "tokens": [309, 445, 307, 2361, 322, 264, 3920, 8630, 4494, 293, 411, 264, 6920, 7513, 295, 8630, 4494, 13, 407], "temperature": 0.0, "avg_logprob": -0.27333676537802054, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0002232570550404489}, {"id": 99, "seek": 74460, "start": 744.6, "end": 752.12, "text": " it doesn't handle it differently than, for example, I think get call trace and trade for. Yeah,", "tokens": [309, 1177, 380, 4813, 309, 7614, 813, 11, 337, 1365, 11, 286, 519, 483, 818, 13508, 293, 4923, 337, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.2937390776886337, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.8160829212283716e-05}, {"id": 100, "seek": 74460, "start": 752.12, "end": 755.48, "text": " that's all Java frames. So that's, that's probably fine.", "tokens": [300, 311, 439, 10745, 12083, 13, 407, 300, 311, 11, 300, 311, 1391, 2489, 13], "temperature": 0.0, "avg_logprob": -0.2937390776886337, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.8160829212283716e-05}, {"id": 101, "seek": 74460, "start": 760.12, "end": 765.08, "text": " Do you have to change the native parts? Or does it go on all platforms?", "tokens": [1144, 291, 362, 281, 1319, 264, 8470, 3166, 30, 1610, 775, 309, 352, 322, 439, 9473, 30], "temperature": 0.0, "avg_logprob": -0.2937390776886337, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.8160829212283716e-05}, {"id": 102, "seek": 74460, "start": 766.12, "end": 771.48, "text": " So the question was, does it work on all platforms? It's known that it doesn't really work on windows", "tokens": [407, 264, 1168, 390, 11, 775, 309, 589, 322, 439, 9473, 30, 467, 311, 2570, 300, 309, 1177, 380, 534, 589, 322, 9309], "temperature": 0.0, "avg_logprob": -0.2937390776886337, "compression_ratio": 1.5980392156862746, "no_speech_prob": 5.8160829212283716e-05}, {"id": 103, "seek": 77148, "start": 771.48, "end": 776.52, "text": " just because windows hasn't really a concept of signals. If you have any ideas on getting", "tokens": [445, 570, 9309, 6132, 380, 534, 257, 3410, 295, 12354, 13, 759, 291, 362, 604, 3487, 322, 1242], "temperature": 0.0, "avg_logprob": -0.1448942228805187, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.746562864165753e-05}, {"id": 104, "seek": 77148, "start": 776.52, "end": 783.4, "text": " something like this to work on windows, feel free to drop me a message. So no,", "tokens": [746, 411, 341, 281, 589, 322, 9309, 11, 841, 1737, 281, 3270, 385, 257, 3636, 13, 407, 572, 11], "temperature": 0.0, "avg_logprob": -0.1448942228805187, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.746562864165753e-05}, {"id": 105, "seek": 77148, "start": 783.4, "end": 789.0, "text": " I didn't have to change any native parts. I had to change some, I had to create some native parts", "tokens": [286, 994, 380, 362, 281, 1319, 604, 8470, 3166, 13, 286, 632, 281, 1319, 512, 11, 286, 632, 281, 1884, 512, 8470, 3166], "temperature": 0.0, "avg_logprob": -0.1448942228805187, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.746562864165753e-05}, {"id": 106, "seek": 77148, "start": 789.0, "end": 795.16, "text": " for testing to modify like the U context because this is highly applications, highly operating", "tokens": [337, 4997, 281, 16927, 411, 264, 624, 4319, 570, 341, 307, 5405, 5821, 11, 5405, 7447], "temperature": 0.0, "avg_logprob": -0.1448942228805187, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.746562864165753e-05}, {"id": 107, "seek": 79516, "start": 795.16, "end": 803.3199999999999, "text": " systems specific. So the changes to the whole OpenJDK are fairly minimal. So they aren't that", "tokens": [3652, 2685, 13, 407, 264, 2962, 281, 264, 1379, 7238, 41, 35, 42, 366, 6457, 13206, 13, 407, 436, 3212, 380, 300], "temperature": 0.0, "avg_logprob": -0.17583361793966854, "compression_ratio": 1.4842105263157894, "no_speech_prob": 3.21530933433678e-05}, {"id": 108, "seek": 79516, "start": 803.3199999999999, "end": 809.88, "text": " large besides passing through some bullets to configure stuff. And the code itself is just", "tokens": [2416, 11868, 8437, 807, 512, 20132, 281, 22162, 1507, 13, 400, 264, 3089, 2564, 307, 445], "temperature": 0.0, "avg_logprob": -0.17583361793966854, "compression_ratio": 1.4842105263157894, "no_speech_prob": 3.21530933433678e-05}, {"id": 109, "seek": 79516, "start": 809.88, "end": 815.88, "text": " a few couple hundred lines. So it's quite simple also to understand. And there's a blog post that", "tokens": [257, 1326, 1916, 3262, 3876, 13, 407, 309, 311, 1596, 2199, 611, 281, 1223, 13, 400, 456, 311, 257, 6968, 2183, 300], "temperature": 0.0, "avg_logprob": -0.17583361793966854, "compression_ratio": 1.4842105263157894, "no_speech_prob": 3.21530933433678e-05}, {"id": 110, "seek": 81588, "start": 815.88, "end": 826.2, "text": " describes like reasoning behind it. Any other questions? Yes? Is it already a sub machine?", "tokens": [15626, 411, 21577, 2261, 309, 13, 2639, 661, 1651, 30, 1079, 30, 1119, 309, 1217, 257, 1422, 3479, 30], "temperature": 0.0, "avg_logprob": -0.1679879887898763, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.53459178970661e-05}, {"id": 111, "seek": 81588, "start": 826.2, "end": 833.4, "text": " No, it's not yet on the sub machine because I'm still in the process of testing it. So there's", "tokens": [883, 11, 309, 311, 406, 1939, 322, 264, 1422, 3479, 570, 286, 478, 920, 294, 264, 1399, 295, 4997, 309, 13, 407, 456, 311], "temperature": 0.0, "avg_logprob": -0.1679879887898763, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.53459178970661e-05}, {"id": 112, "seek": 81588, "start": 833.4, "end": 841.08, "text": " of course a podcast. You can already use the JVM when you compile it yourself. I'm in the process", "tokens": [295, 1164, 257, 7367, 13, 509, 393, 1217, 764, 264, 508, 53, 44, 562, 291, 31413, 309, 1803, 13, 286, 478, 294, 264, 1399], "temperature": 0.0, "avg_logprob": -0.1679879887898763, "compression_ratio": 1.5297297297297296, "no_speech_prob": 4.53459178970661e-05}, {"id": 113, "seek": 84108, "start": 841.08, "end": 848.12, "text": " of updating my demo repository which contains a modified sync profile that uses it. So you can", "tokens": [295, 25113, 452, 10723, 25841, 597, 8306, 257, 15873, 20271, 7964, 300, 4960, 309, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.24711976732526506, "compression_ratio": 1.361963190184049, "no_speech_prob": 4.7511999582638964e-05}, {"id": 114, "seek": 84108, "start": 848.12, "end": 855.96, "text": " try it out yourself. I should be right in the next few weeks. It still has some bugs. Yeah.", "tokens": [853, 309, 484, 1803, 13, 286, 820, 312, 558, 294, 264, 958, 1326, 3259, 13, 467, 920, 575, 512, 15120, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.24711976732526506, "compression_ratio": 1.361963190184049, "no_speech_prob": 4.7511999582638964e-05}, {"id": 115, "seek": 85596, "start": 855.96, "end": 868.6800000000001, "text": " Anything else? Thank you very much.", "tokens": [50364, 11998, 1646, 30, 1044, 291, 588, 709, 13, 51000], "temperature": 0.0, "avg_logprob": -0.4197273254394531, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.00047354178968816996}], "language": "en"}