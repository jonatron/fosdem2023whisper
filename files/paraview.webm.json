{"text": " All right. Good morning, everyone. Welcome to the HPC Dev Room. Thanks for being here so early in the morning, maybe not entirely sober. We'll let Nicolas get started with opening a Dev Room with a talk on paraffin. Thank you, Nicolas. Hello, everyone. Thanks to be here early in the morning to begin this HPC day. I trust them. A big thanks to everyone who organized this room. That's really great to be here. Thanks, Kenneth, on all your team. So, about me, I'm Nicolas Vieille. I'm a C++ developer, and I have the chance to make my job about making first-code contributions. I'm working at Kitware Europe, and I work mainly on ParaView, so developing the software, but also interacting with the community. So, as my may want to reach me later. So, ParaView, it's an end-user applications that work for scientific data analysis and visualizations. We have an open community on the GitLab for the code and discourse for discussions. It's supported by Kitware, which is also behind VTK for visualization toolkits, and to make you potentially already know about. So, what do we do with ParaView? It's for displaying and analyzing scientific data sets. So, it's mainly a 3D visualizer, but you've got also some charts and spreadsheets and so on. It's also intended for data processing. So, we have a concept of filters to take an input and compute some stuff on it and get your outputs. So, basically, you can extract the data of interest of your raw data. We also have some other module like realistic rendering. So, you can do your communications with directly your real data sets or not with just some kind of fake one. So, here is some basic screenshot of the applications. Who is using ParaView? We cover its generic application. So, we cover a large range of domains like fluid dynamics. So, we can compute streamlines, particle tracking, and so on. We have also volume rendering that is real nice for our medical applications with you have some 3D scan and you want to understand what happened inside. So, that's just, that's non-finite list. We have a lot of domains that can be covered, but here is the more well-known one. Oh, we do use ParaView. So, as I said, that's an application. So, basically, the first way to learn is to use the GUI. So, you click on buttons, you do some stuff, and you're happy. But you can also use the Python wrapping to write some scripts. And so, you can run the script on that processing without having to be behind the computer. It has a framework because you can code your own extensions, your own derivative work from ParaView in the native simplest language, but also some features can be done in Python code. And it's all based on the visualization tool kit. I mean, all the hard work of processing the data and do the rendering come from VTK. So, as I said, that's also supported by Kitware. So, I do work sometimes on VTK to have some bug fix or some small new features. Where we can run ParaView, on which hardware is it possible to use it? Basically, on your small, classical, big stop, we have some official binaries you can try to download and just run. It should be, it's cross-platform, I didn't say, but you can run it on, as well, on proprietary software like Mac or Windows, but it should be out of the box for Linux 2. You can also build it. We have a large selection of build options, depending on what you want to do exactly. You can enable or disable it. So, if you want to have Python data distribution, parallelizations, or custom rendering, a lot of stuff. We have some documentation about it, and we can help you on the discourse if you have to try to achieve some specific build of it. And which kind of usage do we have of ParaView? So, either a research and industry are using it. For instance, recently, there were the Super... So, before winter, a supercomputing conference in the US, and they organized a service contest where people are asked to upload some nice videos made about their scientific analysis, and most of them are using ParaView, either for just the data processing, but also sometimes for the video generations, and animate their data. So, why all those people are using ParaView? Because ParaView does some stuff efficiently to process the data and their infrastructures. So, that's what I want to talk in the next part of my... of my talk. So, what's ParaView used behind the hood to make it possible? So, first, we have a client-server architecture. So, I always say that you can do it from Python, but the degree on Python are just two clients. So, you can do exactly the same stuff with one or the other. There's no real limitation about using one or the other second. You can also run with remote server, so, and you can run in a distributed environment your server. So, in that case, you can connect your... you can either just run the server parts to analysis with a script analysis, but you can also connect your local clients to your distance server, and again, using the graphical interface to do the stuff as if it was on your local machine, but instead, it's... yeah, the supercomputer or the remote architecture. At the bottom, two other modes that are available. If you... typically, if you have some graphic nodes on your server, you can use them for just the rendering part and stay the data management on the CPU nodes, and still, you can connect your client on it to see what happened and to control from a graphical interface. And last mode, I will go back on it later. We have an institute infrastructure, so, basically, your simulation can call an IPI that's Fluid Paraview script analysis, and you can even connect with a graphical client to see time step per time step what is happening on your simulation. So, that's for the different mode of use for Paraview. So, first, to run on HP infrastructure, we implement data distributions for the analysis. So, basically, we rely on the MPI standards. So, our readers are MPI aware, so they can distribute the data over the rank early in the process when you read your data on the disk. Then, most of the filters are okay to run just with their support of data, but some other filters need to know about the neighborhood to execute correctly. So, for that, we support the concept of ghost cells, where each rank knows a little bit about the rank that is next to it. In that case, mainly, we split the data geometrically. So, a subset is really a geometric subset of your data, and different one can know and communicate with the other for specific tasks. At least, we have some filters. So, what I call a filter is really something that the user can instantiate from the client and ask to process. So, we can ensure a load balancing by redistributing the data during the process. The visualizations can also be distributed over several ranks. So, for that, we use an inner library that call IST, that's also based on the MPI process to do that, and parallel view support has a different kind of model of rendering. So, you can, if you have dedicated rendering node, you can, as I said, create parallel view server just for the rendering part and connect it to the data server. You can have multiple GPUs per rank, yeah, multiple GPUs per rank to do the rendering, but that's also possible locally if you have just one machine that have multiple GPUs, you can ask to do a rendering on both simil-tune-y. Concerning the performances now, so that distributions is not about performance, it's just about running with too big data so you cannot just run on your machine, that's a requirement when you have huge data to be able to distribute it over your computer or your supercomputer. Now, we're talking a little bit about performance because if you have big data, you also need to be performant on how you analyze it, on how you are proceed with it. So, for that, we have a thin layer for CPU parallelism, we call that a simple tool in our code base. The goal is to parallelize, do code parallelizations for many for loop, and main purpose is that you can choose at build time and then at run time, if you enable the OpenMP or TBB backends, and if you don't want external live, you can also use the C++ thread to do that. And so, as it just, for instance, to parallelize a for loop or field operations, it's really widely used in a lot of our, in our algorithm, and you have some environment variable that can control the back end on some of the number of, of thread, the size of the thread pools, or if you allow nested pools or so on, depending on the, on your resources and back end. So, it has some documentation on it, and we made some improvements last year about that. Still, still about performances, we also use as an optional dependency, the VTKM, VTKM projects that stand for, yes, somewhat some many core that is intended to be used on heterogeneous systems. So, basically, when you want to have performance on supercomputer or even, you, you still need to be aware of the current technology and the state of the art, and as we saw in the past decades, a lot of new architectures emerging. We, we think about using a dedicated library to, to be able to use this, this new architecture. So, with VTKM, the goal, inside VTKM library, the goal is to split all operations into really atomic operations, and then the, the, at runtime, it can dispatch all, all that on the hardware you find on the back end that are available. So, with VTKM, you can use CUDA, OpenMP, TBB also, to do the computation. This time, with VTKM is not just accelerating some specific loop inside an algorithm, it's more about VTKM is implementing some whole algorithm like extracting ISO control or, or so. And then, we embed this into, into Paraview with some kind of wrapper to, to communicate with all VTKM works. So, that's optional, that's enabled by default in the binaries we, we provide. Another point about performance, but that's really depending on the use case, on, on the data you are using is the in-situ wall. So, basically, when you, traditionally, when you have your simulation, it dumps every time step or every end time step some data on the disk. And then, to analyze, you have to load back to the data with post-processing tools. But that adds a cost of writing and reading from your disk. And you, you should have the size on your disk, the whole size of the disk. So, you should have big disk. And then, it's, it have really a cost in term of time, when you should write a full, a full mesh or full data on disk. And then, read back with another process. So, basically, the goal of in-situ is to, to make the simulation communicate directly with the processing tools. And then, the processing tools can wrap the memory in place and analyze directly in, in the RAM without writing on the disk to save some higher time. So, in the context of ParaView, we have a standalone API that's called Catalysts. That was recently released, as we make big improvements into Catalysts past years. And the goal of Catalysts is to have a really minimal API and stable API. So, you can choose and run time the implementation you, you want. And one other goal is to minimize the instrumentation you need to do in your simulation code directly. So, it's really easy for simulation developer to understand the few key places where they have to put a new code to call our API. So, here is a really basic example from one on the tutorial we have. We need to initialize, of course, and you need to call some method at each time step where you want the processing to happen. And finalizations. Of course, the, you still have to do a little layer to describe your, your data. For that, we do some sort of a partial library to, to help us. So, ParaView, so Catalyst is a standard, I say standalone, is not, is independent project, no, independent of ParaView. But of course, the first real implementation is an implementation for ParaView. So, we, sorry. So, yes, we, we implement Catalyst. So, the back end, so you can run ParaView pipeline directly from your simulation. It's each time step or when, when you call it. So, how does it, how does it work? Or do you, you can, the idea is that you are, are called the communication between your simulation and the, on Catalyst through the API. But then the actual script that is executed, the actual pipeline and visualization, visualization you want to produce. It's all scriptable thanks to the Python wrapping of, of ParaView. You can even, you can even, sorry, load some representative data in the graphical interface of ParaView. Do some analysis, export this as a Python script and use the script to feed Catalyst. And then, when you run your simulation with Catalyst enable, it will reuse the script you produce directly from the GUI. So, people that are not at all developers still can do some stuff with, with Catalyst. And last point is that, when you have a running simulation with the Catalyst pipeline on your dedicated server, you also can use the GUI to connect to this ParaView server and to see real-time get some screenshots of the visualization on the analysis that is proceeding on the server. So, you can have a feedback, a time step per test, a time step on what happened on the simulation. So, if you see that something is diverging or going wrong, you can stop your simulation directly and you don't waste all the time before seeing that something went wrong and that you should tweak the parameter and start again. So, yeah, I was quite faster than expected for me. So, in the conclusions of to, to be able to run efficiently on the, on the supercomputer with ParaView, we implemented a client-server mode. The server can be, is MPIO rare and can be run on distributed environments. We are relying on old, on well-known libraries such as implementation of MPI to the distributions, but we are also really looking for, toward new, new library that can help us. Yeah, and we, we are able to integrate new library to, to do some performance analysis on new library that is aware of a new architecture of supercomputer or new technology. That's okay with, for instance, with VTKM or, or others. And we have this API to do institute that can save a lot of time and disk space. Yeah, just a slide to summarize the organize. So, we have different kind of way to interact with ParaView. Yeah, the grid, the Python scripting, the catalyst in city stuff. You can also build some custom one. We have some web example of clients. And at the bottom, we have a list, a non-finite list of library on which we will like to, to, to do the effective work. So, basically, open GL, MPI, open, open MP. And so, concerning roadmap, we have several improvements that are coming. First, I talk about in, in situ, in the current implementation, you have each rank that does the simulation, does also the, the co-processing work. So, that's not always what is intended. Sometimes, you want to do the co-processing on the other rank. Just because, for instance, you have dedicated the rank for visualization. So, you want to do all the processing on the visualization nodes. That's for not possible just with the in situ implementation, but we have an in-transit implementation where the simulation can communicate with those different nodes. And, and the analysis can happen on other ranks than the simulation. So, the simulation can go forward directly. We use, we also use some new library, recently used a library called DIY. That's here to do some wrapper for us. It's, we take it as a wrapper around the MPI. So, DIY, I love to do some to, to cut the data into different blocks. And then, the, at runtime DIY itself is a rare to do. Okay, I should put three blocks on each rank. So, only one block. And, yeah, it's a, yeah, just a new abstraction over cutting your, your data for distribution. We are also looking for better VTK on always, yeah, better VTK integrations to, to be able to, to run on a lot of hardware. And something very cool that is very new. It's just in the development branch of VTK. So, absolutely not in Paraview yet. That was merged, I think, one or two weeks ago. It's what we call implicit arrays. And, basically, it's really cool for memory point of view because we, it's some kind of views on memory. For now, in the Paraview process, your data is really an array in the, in the memory, in your memory. So, with the implicit array, we have some views. So, you can implement an open, open pattern on it. For instance, when you do an isocontrol of your data, you know that the, the, that, the resulting data will all have the same values. So, if you want, if you, after the isocontrol, you still have one million points, you will know that all the points will share the same value. For now, it's one million times duplicate in your memory. So, that's a not-efficient. With implicit array, you can sort only one time the value and say, okay, this should, this should be an array of size one million. And the value you should return is this one. But you can imagine as a, a compressed array in your memory and have an on-the-fly, uncompressed algorithm to when you just want, just when you want to, to read your data. So, it has a cost in terms of time of computations. But if you run out of memory with too huge data, that's, that can be really great. Okay, I still can have a lot of things to, to say, but that's what is the, the end of what I, I put in the slides. So, thanks for attending these songs to be here early in the morning. And if you have any questions, I think it will, it will be the time. I put just a lot of resources at the end of the slides so you can get it from the website of the phone. Thank you. Thanks, everyone. Thank you very much, Nikol. Do we have any questions? Thank you. In our group, we are happy users of ParaView. One thing that maybe I could add to a wishlist or some, well, maybe just for discussion is that we have quite some headache when using ParaView on GitHub Actions for multiple platforms. So, like to set up environments for Linux, Mac and Windows with the same version of ParaView coupling with Python just, just to be ready to use it. It's a bit of a headache, especially when you go to Windows and you need to download things, brew things, up to get install things and then they not necessarily work all together. So, wishlist thing, GitHub Actions, ParaView, set up a thing. Unless it doesn't, it exists already but I haven't found it. And the truth is, if there are questions here? The use of ParaView and GitHub Actions, so in like a continuous integration, limited environment, I guess? Yeah, it's a wishlist. Yeah, well, we don't choose GitHub directly. We have a, we have a, the GitLab where you can find a lot of stuff with our CI and CDO. We produce, we produce nightly releases of ParaView through the GitLab. So, I don't know if I, if it's some sort of part of the question, but. So, what kind of stuff are you doing with ParaView and GitHub Actions? Is it rendering or, rendering with Python? The fact is, I don't really know about GitHub Actions because I don't choose GitHub anymore. So, I don't see what you can do with that, that you should not able to do otherwise. Any other questions? There's a question on the chat. Okay. Yeah, there's a question on the chat. If I want to put Catalyst in my simulation, what is the first step? Oh, sorry. If you want to use Catalyst. In your, yeah. What's the first step? We have some, I think we have some tutorials on example in the code base of ParaView. We have some examples where there are some dummy simulations with just a main, so you can enter from it to, to see how it is organized. And, and yeah, the first, one first thing is to be able to know what do you want, which data do you want to, to send through, through Catalyst and where you can access it in your code. And then, it's, and then so you, at this time you, you have located the entry points from your simulation code and then you will be able to, to start writing the, the small wrapper you need to wrap your data on the need to the actual API of ParaView. Thanks. Okay. Any other burning questions? Maybe one last, yeah. Last question? Thank you for the talk. A very naive question, because I, is it working? A very naive question because I know almost nothing about, about ParaView. You had many components there. One of them was the client that does the visualizations. Yeah. Is it, would it be possible at some point in the future to be like a web client where you just log into the website and it just displays everything? Or is it just, due to the architecture is it like super complicated to do it that way? We, so the question is, yeah, the question is, are we able to use a web client for ParaView? Just, just for the part that does the visualization, if that could be like a, we are, we have some web client for ParaView already. So we have a framework called Trame, T-R-R-M-E, that's intended to, to connect to a ParaView server. And then you build your own front end for these applications. So basically it's, we don't have a, yeah, you should build your own. But it can be, okay, I have a server on, I open the, always this data and the front end is just a 3D round of view. That's already possible quite easily, I think, with the Trame framework. And Jupyter Notebooks also, right? Jupyter Notebooks, I think I saw it on the user interface line. Yeah. Well, we are, yeah, as we use intensively Python, we also make the step to, to be supported from a Jupyter Notebook and we also have a plugin that allows you to control a ParaView GUI. So you can do some stuff in the Notebook. And if something goes wrong or you don't understand, you can launch a magic command run ParaView that's open the ParaView client with all your Python, Python, and you can introspect in the GUI. And then you can go back to your, to your Notebook. Okay. Thank you very much, Nicholas. Thanks. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.120000000000001, "text": " All right. Good morning, everyone. Welcome to the HPC Dev Room. Thanks for being here", "tokens": [1057, 558, 13, 2205, 2446, 11, 1518, 13, 4027, 281, 264, 12557, 34, 9096, 19190, 13, 2561, 337, 885, 510], "temperature": 0.0, "avg_logprob": -0.3230693506640057, "compression_ratio": 1.61, "no_speech_prob": 0.3742370009422302}, {"id": 1, "seek": 0, "start": 10.120000000000001, "end": 15.6, "text": " so early in the morning, maybe not entirely sober. We'll let Nicolas get started with", "tokens": [370, 2440, 294, 264, 2446, 11, 1310, 406, 7696, 26212, 13, 492, 603, 718, 38268, 483, 1409, 365], "temperature": 0.0, "avg_logprob": -0.3230693506640057, "compression_ratio": 1.61, "no_speech_prob": 0.3742370009422302}, {"id": 2, "seek": 0, "start": 15.6, "end": 19.68, "text": " opening a Dev Room with a talk on paraffin. Thank you, Nicolas.", "tokens": [5193, 257, 9096, 19190, 365, 257, 751, 322, 1690, 602, 259, 13, 1044, 291, 11, 38268, 13], "temperature": 0.0, "avg_logprob": -0.3230693506640057, "compression_ratio": 1.61, "no_speech_prob": 0.3742370009422302}, {"id": 3, "seek": 0, "start": 19.68, "end": 29.68, "text": " Hello, everyone. Thanks to be here early in the morning to begin this HPC day. I trust", "tokens": [2425, 11, 1518, 13, 2561, 281, 312, 510, 2440, 294, 264, 2446, 281, 1841, 341, 12557, 34, 786, 13, 286, 3361], "temperature": 0.0, "avg_logprob": -0.3230693506640057, "compression_ratio": 1.61, "no_speech_prob": 0.3742370009422302}, {"id": 4, "seek": 2968, "start": 29.68, "end": 34.56, "text": " them. A big thanks to everyone who organized this room. That's really great to be here.", "tokens": [552, 13, 316, 955, 3231, 281, 1518, 567, 9983, 341, 1808, 13, 663, 311, 534, 869, 281, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.2957107330037055, "compression_ratio": 1.5369649805447472, "no_speech_prob": 0.0005334910820238292}, {"id": 5, "seek": 2968, "start": 34.56, "end": 40.86, "text": " Thanks, Kenneth, on all your team. So, about me, I'm Nicolas Vieille. I'm a", "tokens": [2561, 11, 33735, 11, 322, 439, 428, 1469, 13, 407, 11, 466, 385, 11, 286, 478, 38268, 24130, 3409, 13, 286, 478, 257], "temperature": 0.0, "avg_logprob": -0.2957107330037055, "compression_ratio": 1.5369649805447472, "no_speech_prob": 0.0005334910820238292}, {"id": 6, "seek": 2968, "start": 40.86, "end": 46.120000000000005, "text": " C++ developer, and I have the chance to make my job about making first-code", "tokens": [383, 25472, 10754, 11, 293, 286, 362, 264, 2931, 281, 652, 452, 1691, 466, 1455, 700, 12, 22332], "temperature": 0.0, "avg_logprob": -0.2957107330037055, "compression_ratio": 1.5369649805447472, "no_speech_prob": 0.0005334910820238292}, {"id": 7, "seek": 2968, "start": 46.120000000000005, "end": 51.64, "text": " contributions. I'm working at Kitware Europe, and I work mainly on ParaView,", "tokens": [15725, 13, 286, 478, 1364, 412, 23037, 3039, 3315, 11, 293, 286, 589, 8704, 322, 11107, 30203, 11], "temperature": 0.0, "avg_logprob": -0.2957107330037055, "compression_ratio": 1.5369649805447472, "no_speech_prob": 0.0005334910820238292}, {"id": 8, "seek": 2968, "start": 51.64, "end": 57.84, "text": " so developing the software, but also interacting with the community. So, as my", "tokens": [370, 6416, 264, 4722, 11, 457, 611, 18017, 365, 264, 1768, 13, 407, 11, 382, 452], "temperature": 0.0, "avg_logprob": -0.2957107330037055, "compression_ratio": 1.5369649805447472, "no_speech_prob": 0.0005334910820238292}, {"id": 9, "seek": 5784, "start": 57.84, "end": 64.72, "text": " may want to reach me later. So, ParaView, it's an end-user applications that work", "tokens": [815, 528, 281, 2524, 385, 1780, 13, 407, 11, 11107, 30203, 11, 309, 311, 364, 917, 12, 18088, 5821, 300, 589], "temperature": 0.0, "avg_logprob": -0.2398080587387085, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.0004962626262567937}, {"id": 10, "seek": 5784, "start": 64.72, "end": 70.88000000000001, "text": " for scientific data analysis and visualizations. We have an open community", "tokens": [337, 8134, 1412, 5215, 293, 5056, 14455, 13, 492, 362, 364, 1269, 1768], "temperature": 0.0, "avg_logprob": -0.2398080587387085, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.0004962626262567937}, {"id": 11, "seek": 5784, "start": 70.88000000000001, "end": 76.64, "text": " on the GitLab for the code and discourse for discussions. It's supported by", "tokens": [322, 264, 16939, 37880, 337, 264, 3089, 293, 23938, 337, 11088, 13, 467, 311, 8104, 538], "temperature": 0.0, "avg_logprob": -0.2398080587387085, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.0004962626262567937}, {"id": 12, "seek": 5784, "start": 76.64, "end": 81.60000000000001, "text": " Kitware, which is also behind VTK for visualization toolkits, and to make you", "tokens": [23037, 3039, 11, 597, 307, 611, 2261, 691, 51, 42, 337, 25801, 2290, 74, 1208, 11, 293, 281, 652, 291], "temperature": 0.0, "avg_logprob": -0.2398080587387085, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.0004962626262567937}, {"id": 13, "seek": 8160, "start": 81.6, "end": 89.6, "text": " potentially already know about. So, what do we do with ParaView? It's for", "tokens": [7263, 1217, 458, 466, 13, 407, 11, 437, 360, 321, 360, 365, 11107, 30203, 30, 467, 311, 337], "temperature": 0.0, "avg_logprob": -0.182495958664838, "compression_ratio": 1.5697211155378485, "no_speech_prob": 0.00011129011545563117}, {"id": 14, "seek": 8160, "start": 89.6, "end": 94.24, "text": " displaying and analyzing scientific data sets. So, it's mainly a 3D visualizer, but", "tokens": [36834, 293, 23663, 8134, 1412, 6352, 13, 407, 11, 309, 311, 8704, 257, 805, 35, 5056, 6545, 11, 457], "temperature": 0.0, "avg_logprob": -0.182495958664838, "compression_ratio": 1.5697211155378485, "no_speech_prob": 0.00011129011545563117}, {"id": 15, "seek": 8160, "start": 94.24, "end": 98.88, "text": " you've got also some charts and spreadsheets and so on. It's also intended for", "tokens": [291, 600, 658, 611, 512, 17767, 293, 23651, 1385, 293, 370, 322, 13, 467, 311, 611, 10226, 337], "temperature": 0.0, "avg_logprob": -0.182495958664838, "compression_ratio": 1.5697211155378485, "no_speech_prob": 0.00011129011545563117}, {"id": 16, "seek": 8160, "start": 98.88, "end": 104.36, "text": " data processing. So, we have a concept of filters to take an input and compute", "tokens": [1412, 9007, 13, 407, 11, 321, 362, 257, 3410, 295, 15995, 281, 747, 364, 4846, 293, 14722], "temperature": 0.0, "avg_logprob": -0.182495958664838, "compression_ratio": 1.5697211155378485, "no_speech_prob": 0.00011129011545563117}, {"id": 17, "seek": 8160, "start": 104.36, "end": 108.88, "text": " some stuff on it and get your outputs. So, basically, you can extract the data", "tokens": [512, 1507, 322, 309, 293, 483, 428, 23930, 13, 407, 11, 1936, 11, 291, 393, 8947, 264, 1412], "temperature": 0.0, "avg_logprob": -0.182495958664838, "compression_ratio": 1.5697211155378485, "no_speech_prob": 0.00011129011545563117}, {"id": 18, "seek": 10888, "start": 108.88, "end": 114.19999999999999, "text": " of interest of your raw data. We also have some other module like realistic", "tokens": [295, 1179, 295, 428, 8936, 1412, 13, 492, 611, 362, 512, 661, 10088, 411, 12465], "temperature": 0.0, "avg_logprob": -0.24677800460600516, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00025338996783830225}, {"id": 19, "seek": 10888, "start": 114.19999999999999, "end": 119.6, "text": " rendering. So, you can do your communications with directly your real", "tokens": [22407, 13, 407, 11, 291, 393, 360, 428, 15163, 365, 3838, 428, 957], "temperature": 0.0, "avg_logprob": -0.24677800460600516, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00025338996783830225}, {"id": 20, "seek": 10888, "start": 119.6, "end": 126.16, "text": " data sets or not with just some kind of fake one. So, here is some basic", "tokens": [1412, 6352, 420, 406, 365, 445, 512, 733, 295, 7592, 472, 13, 407, 11, 510, 307, 512, 3875], "temperature": 0.0, "avg_logprob": -0.24677800460600516, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00025338996783830225}, {"id": 21, "seek": 10888, "start": 126.16, "end": 134.32, "text": " screenshot of the applications. Who is using ParaView? We cover its generic", "tokens": [27712, 295, 264, 5821, 13, 2102, 307, 1228, 11107, 30203, 30, 492, 2060, 1080, 19577], "temperature": 0.0, "avg_logprob": -0.24677800460600516, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00025338996783830225}, {"id": 22, "seek": 13432, "start": 134.32, "end": 139.04, "text": " application. So, we cover a large range of domains like fluid dynamics. So, we can", "tokens": [3861, 13, 407, 11, 321, 2060, 257, 2416, 3613, 295, 25514, 411, 9113, 15679, 13, 407, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.2440829890789372, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.0002666048821993172}, {"id": 23, "seek": 13432, "start": 139.04, "end": 145.16, "text": " compute streamlines, particle tracking, and so on. We have also volume rendering", "tokens": [14722, 4309, 11045, 11, 12359, 11603, 11, 293, 370, 322, 13, 492, 362, 611, 5523, 22407], "temperature": 0.0, "avg_logprob": -0.2440829890789372, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.0002666048821993172}, {"id": 24, "seek": 13432, "start": 145.16, "end": 149.28, "text": " that is real nice for our medical applications with you have some 3D scan", "tokens": [300, 307, 957, 1481, 337, 527, 4625, 5821, 365, 291, 362, 512, 805, 35, 11049], "temperature": 0.0, "avg_logprob": -0.2440829890789372, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.0002666048821993172}, {"id": 25, "seek": 13432, "start": 149.28, "end": 154.32, "text": " and you want to understand what happened inside. So, that's just, that's", "tokens": [293, 291, 528, 281, 1223, 437, 2011, 1854, 13, 407, 11, 300, 311, 445, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.2440829890789372, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.0002666048821993172}, {"id": 26, "seek": 13432, "start": 154.32, "end": 160.0, "text": " non-finite list. We have a lot of domains that can be covered, but here is the more", "tokens": [2107, 12, 5194, 642, 1329, 13, 492, 362, 257, 688, 295, 25514, 300, 393, 312, 5343, 11, 457, 510, 307, 264, 544], "temperature": 0.0, "avg_logprob": -0.2440829890789372, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.0002666048821993172}, {"id": 27, "seek": 16000, "start": 160.0, "end": 167.08, "text": " well-known one. Oh, we do use ParaView. So, as I said, that's an application. So,", "tokens": [731, 12, 6861, 472, 13, 876, 11, 321, 360, 764, 11107, 30203, 13, 407, 11, 382, 286, 848, 11, 300, 311, 364, 3861, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.1747526689009233, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.00021191716950852424}, {"id": 28, "seek": 16000, "start": 167.08, "end": 172.0, "text": " basically, the first way to learn is to use the GUI. So, you click on buttons, you", "tokens": [1936, 11, 264, 700, 636, 281, 1466, 307, 281, 764, 264, 17917, 40, 13, 407, 11, 291, 2052, 322, 9905, 11, 291], "temperature": 0.0, "avg_logprob": -0.1747526689009233, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.00021191716950852424}, {"id": 29, "seek": 16000, "start": 172.0, "end": 177.84, "text": " do some stuff, and you're happy. But you can also use the Python wrapping to", "tokens": [360, 512, 1507, 11, 293, 291, 434, 2055, 13, 583, 291, 393, 611, 764, 264, 15329, 21993, 281], "temperature": 0.0, "avg_logprob": -0.1747526689009233, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.00021191716950852424}, {"id": 30, "seek": 16000, "start": 177.84, "end": 182.68, "text": " write some scripts. And so, you can run the script on that processing without", "tokens": [2464, 512, 23294, 13, 400, 370, 11, 291, 393, 1190, 264, 5755, 322, 300, 9007, 1553], "temperature": 0.0, "avg_logprob": -0.1747526689009233, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.00021191716950852424}, {"id": 31, "seek": 16000, "start": 182.68, "end": 188.48, "text": " having to be behind the computer. It has a framework because you can code your", "tokens": [1419, 281, 312, 2261, 264, 3820, 13, 467, 575, 257, 8388, 570, 291, 393, 3089, 428], "temperature": 0.0, "avg_logprob": -0.1747526689009233, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.00021191716950852424}, {"id": 32, "seek": 18848, "start": 188.48, "end": 193.64, "text": " own extensions, your own derivative work from ParaView in the native", "tokens": [1065, 25129, 11, 428, 1065, 13760, 589, 490, 11107, 30203, 294, 264, 8470], "temperature": 0.0, "avg_logprob": -0.2618910532731276, "compression_ratio": 1.568, "no_speech_prob": 0.00037316096131689847}, {"id": 33, "seek": 18848, "start": 193.64, "end": 199.2, "text": " simplest language, but also some features can be done in Python code. And it's", "tokens": [22811, 2856, 11, 457, 611, 512, 4122, 393, 312, 1096, 294, 15329, 3089, 13, 400, 309, 311], "temperature": 0.0, "avg_logprob": -0.2618910532731276, "compression_ratio": 1.568, "no_speech_prob": 0.00037316096131689847}, {"id": 34, "seek": 18848, "start": 199.2, "end": 204.04, "text": " all based on the visualization tool kit. I mean, all the hard work of processing", "tokens": [439, 2361, 322, 264, 25801, 2290, 8260, 13, 286, 914, 11, 439, 264, 1152, 589, 295, 9007], "temperature": 0.0, "avg_logprob": -0.2618910532731276, "compression_ratio": 1.568, "no_speech_prob": 0.00037316096131689847}, {"id": 35, "seek": 18848, "start": 204.04, "end": 211.72, "text": " the data and do the rendering come from VTK. So, as I said, that's also supported", "tokens": [264, 1412, 293, 360, 264, 22407, 808, 490, 691, 51, 42, 13, 407, 11, 382, 286, 848, 11, 300, 311, 611, 8104], "temperature": 0.0, "avg_logprob": -0.2618910532731276, "compression_ratio": 1.568, "no_speech_prob": 0.00037316096131689847}, {"id": 36, "seek": 18848, "start": 211.72, "end": 217.23999999999998, "text": " by Kitware. So, I do work sometimes on VTK to have some bug fix or some small new", "tokens": [538, 23037, 3039, 13, 407, 11, 286, 360, 589, 2171, 322, 691, 51, 42, 281, 362, 512, 7426, 3191, 420, 512, 1359, 777], "temperature": 0.0, "avg_logprob": -0.2618910532731276, "compression_ratio": 1.568, "no_speech_prob": 0.00037316096131689847}, {"id": 37, "seek": 21724, "start": 217.24, "end": 224.84, "text": " features. Where we can run ParaView, on which hardware is it possible to use it?", "tokens": [4122, 13, 2305, 321, 393, 1190, 11107, 30203, 11, 322, 597, 8837, 307, 309, 1944, 281, 764, 309, 30], "temperature": 0.0, "avg_logprob": -0.272567727592554, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.0003624757518991828}, {"id": 38, "seek": 21724, "start": 224.84, "end": 230.60000000000002, "text": " Basically, on your small, classical, big stop, we have some official binaries you", "tokens": [8537, 11, 322, 428, 1359, 11, 13735, 11, 955, 1590, 11, 321, 362, 512, 4783, 5171, 4889, 291], "temperature": 0.0, "avg_logprob": -0.272567727592554, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.0003624757518991828}, {"id": 39, "seek": 21724, "start": 230.60000000000002, "end": 234.88, "text": " can try to download and just run. It should be, it's cross-platform, I didn't say,", "tokens": [393, 853, 281, 5484, 293, 445, 1190, 13, 467, 820, 312, 11, 309, 311, 3278, 12, 39975, 837, 11, 286, 994, 380, 584, 11], "temperature": 0.0, "avg_logprob": -0.272567727592554, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.0003624757518991828}, {"id": 40, "seek": 21724, "start": 234.88, "end": 241.28, "text": " but you can run it on, as well, on proprietary software like Mac or Windows,", "tokens": [457, 291, 393, 1190, 309, 322, 11, 382, 731, 11, 322, 38992, 4722, 411, 5707, 420, 8591, 11], "temperature": 0.0, "avg_logprob": -0.272567727592554, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.0003624757518991828}, {"id": 41, "seek": 24128, "start": 241.28, "end": 248.04, "text": " but it should be out of the box for Linux 2. You can also build it. We have a large", "tokens": [457, 309, 820, 312, 484, 295, 264, 2424, 337, 18734, 568, 13, 509, 393, 611, 1322, 309, 13, 492, 362, 257, 2416], "temperature": 0.0, "avg_logprob": -0.22500417709350587, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.00011465495481388643}, {"id": 42, "seek": 24128, "start": 248.04, "end": 252.08, "text": " selection of build options, depending on what you want to do exactly. You can", "tokens": [9450, 295, 1322, 3956, 11, 5413, 322, 437, 291, 528, 281, 360, 2293, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.22500417709350587, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.00011465495481388643}, {"id": 43, "seek": 24128, "start": 252.08, "end": 256.36, "text": " enable or disable it. So, if you want to have Python data distribution,", "tokens": [9528, 420, 28362, 309, 13, 407, 11, 498, 291, 528, 281, 362, 15329, 1412, 7316, 11], "temperature": 0.0, "avg_logprob": -0.22500417709350587, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.00011465495481388643}, {"id": 44, "seek": 24128, "start": 256.36, "end": 262.56, "text": " parallelizations, or custom rendering, a lot of stuff. We have some documentation", "tokens": [8952, 14455, 11, 420, 2375, 22407, 11, 257, 688, 295, 1507, 13, 492, 362, 512, 14333], "temperature": 0.0, "avg_logprob": -0.22500417709350587, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.00011465495481388643}, {"id": 45, "seek": 24128, "start": 262.56, "end": 267.16, "text": " about it, and we can help you on the discourse if you have to try to achieve", "tokens": [466, 309, 11, 293, 321, 393, 854, 291, 322, 264, 23938, 498, 291, 362, 281, 853, 281, 4584], "temperature": 0.0, "avg_logprob": -0.22500417709350587, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.00011465495481388643}, {"id": 46, "seek": 26716, "start": 267.16, "end": 274.04, "text": " some specific build of it. And which kind of usage do we have of ParaView?", "tokens": [512, 2685, 1322, 295, 309, 13, 400, 597, 733, 295, 14924, 360, 321, 362, 295, 11107, 30203, 30], "temperature": 0.0, "avg_logprob": -0.24816094274106232, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0002320756611879915}, {"id": 47, "seek": 26716, "start": 274.04, "end": 280.68, "text": " So, either a research and industry are using it. For instance, recently, there", "tokens": [407, 11, 2139, 257, 2132, 293, 3518, 366, 1228, 309, 13, 1171, 5197, 11, 3938, 11, 456], "temperature": 0.0, "avg_logprob": -0.24816094274106232, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0002320756611879915}, {"id": 48, "seek": 26716, "start": 280.68, "end": 286.84000000000003, "text": " were the Super... So, before winter, a supercomputing conference in the US, and", "tokens": [645, 264, 4548, 485, 407, 11, 949, 6355, 11, 257, 27839, 2582, 278, 7586, 294, 264, 2546, 11, 293], "temperature": 0.0, "avg_logprob": -0.24816094274106232, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0002320756611879915}, {"id": 49, "seek": 26716, "start": 286.84000000000003, "end": 292.0, "text": " they organized a service contest where people are asked to upload some nice", "tokens": [436, 9983, 257, 2643, 10287, 689, 561, 366, 2351, 281, 6580, 512, 1481], "temperature": 0.0, "avg_logprob": -0.24816094274106232, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0002320756611879915}, {"id": 50, "seek": 26716, "start": 292.0, "end": 296.48, "text": " videos made about their scientific analysis, and most of them are using", "tokens": [2145, 1027, 466, 641, 8134, 5215, 11, 293, 881, 295, 552, 366, 1228], "temperature": 0.0, "avg_logprob": -0.24816094274106232, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0002320756611879915}, {"id": 51, "seek": 29648, "start": 296.48, "end": 300.8, "text": " ParaView, either for just the data processing, but also sometimes for the", "tokens": [11107, 30203, 11, 2139, 337, 445, 264, 1412, 9007, 11, 457, 611, 2171, 337, 264], "temperature": 0.0, "avg_logprob": -0.1919755033544592, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00039967880002222955}, {"id": 52, "seek": 29648, "start": 300.8, "end": 307.16, "text": " video generations, and animate their data. So, why all those people are using", "tokens": [960, 10593, 11, 293, 36439, 641, 1412, 13, 407, 11, 983, 439, 729, 561, 366, 1228], "temperature": 0.0, "avg_logprob": -0.1919755033544592, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00039967880002222955}, {"id": 53, "seek": 29648, "start": 307.16, "end": 313.40000000000003, "text": " ParaView? Because ParaView does some stuff efficiently to process the data", "tokens": [11107, 30203, 30, 1436, 11107, 30203, 775, 512, 1507, 19621, 281, 1399, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1919755033544592, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00039967880002222955}, {"id": 54, "seek": 29648, "start": 313.40000000000003, "end": 319.72, "text": " and their infrastructures. So, that's what I want to talk in the next part of", "tokens": [293, 641, 6534, 44513, 13, 407, 11, 300, 311, 437, 286, 528, 281, 751, 294, 264, 958, 644, 295], "temperature": 0.0, "avg_logprob": -0.1919755033544592, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00039967880002222955}, {"id": 55, "seek": 31972, "start": 319.72, "end": 327.08000000000004, "text": " my... of my talk. So, what's ParaView used behind the hood to make it", "tokens": [452, 485, 295, 452, 751, 13, 407, 11, 437, 311, 11107, 30203, 1143, 2261, 264, 13376, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.2148367264691521, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.00017230099183507264}, {"id": 56, "seek": 31972, "start": 327.08000000000004, "end": 333.12, "text": " possible? So, first, we have a client-server architecture. So, I always say", "tokens": [1944, 30, 407, 11, 700, 11, 321, 362, 257, 6423, 12, 12484, 331, 9482, 13, 407, 11, 286, 1009, 584], "temperature": 0.0, "avg_logprob": -0.2148367264691521, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.00017230099183507264}, {"id": 57, "seek": 31972, "start": 333.12, "end": 339.08000000000004, "text": " that you can do it from Python, but the degree on Python are just two", "tokens": [300, 291, 393, 360, 309, 490, 15329, 11, 457, 264, 4314, 322, 15329, 366, 445, 732], "temperature": 0.0, "avg_logprob": -0.2148367264691521, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.00017230099183507264}, {"id": 58, "seek": 31972, "start": 339.08000000000004, "end": 345.12, "text": " clients. So, you can do exactly the same stuff with one or the other. There's no", "tokens": [6982, 13, 407, 11, 291, 393, 360, 2293, 264, 912, 1507, 365, 472, 420, 264, 661, 13, 821, 311, 572], "temperature": 0.0, "avg_logprob": -0.2148367264691521, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.00017230099183507264}, {"id": 59, "seek": 34512, "start": 345.12, "end": 353.4, "text": " real limitation about using one or the other second. You can also run with", "tokens": [957, 27432, 466, 1228, 472, 420, 264, 661, 1150, 13, 509, 393, 611, 1190, 365], "temperature": 0.0, "avg_logprob": -0.28045985573216486, "compression_ratio": 1.7228260869565217, "no_speech_prob": 0.00039435410872101784}, {"id": 60, "seek": 34512, "start": 353.4, "end": 360.6, "text": " remote server, so, and you can run in a distributed environment your server. So,", "tokens": [8607, 7154, 11, 370, 11, 293, 291, 393, 1190, 294, 257, 12631, 2823, 428, 7154, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.28045985573216486, "compression_ratio": 1.7228260869565217, "no_speech_prob": 0.00039435410872101784}, {"id": 61, "seek": 34512, "start": 360.6, "end": 366.08, "text": " in that case, you can connect your... you can either just run the server parts to", "tokens": [294, 300, 1389, 11, 291, 393, 1745, 428, 485, 291, 393, 2139, 445, 1190, 264, 7154, 3166, 281], "temperature": 0.0, "avg_logprob": -0.28045985573216486, "compression_ratio": 1.7228260869565217, "no_speech_prob": 0.00039435410872101784}, {"id": 62, "seek": 34512, "start": 366.08, "end": 370.56, "text": " analysis with a script analysis, but you can also connect your local clients to", "tokens": [5215, 365, 257, 5755, 5215, 11, 457, 291, 393, 611, 1745, 428, 2654, 6982, 281], "temperature": 0.0, "avg_logprob": -0.28045985573216486, "compression_ratio": 1.7228260869565217, "no_speech_prob": 0.00039435410872101784}, {"id": 63, "seek": 37056, "start": 370.56, "end": 375.96, "text": " your distance server, and again, using the graphical interface to do the stuff", "tokens": [428, 4560, 7154, 11, 293, 797, 11, 1228, 264, 35942, 9226, 281, 360, 264, 1507], "temperature": 0.0, "avg_logprob": -0.23468555874294705, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0004000527842435986}, {"id": 64, "seek": 37056, "start": 375.96, "end": 382.88, "text": " as if it was on your local machine, but instead, it's... yeah, the", "tokens": [382, 498, 309, 390, 322, 428, 2654, 3479, 11, 457, 2602, 11, 309, 311, 485, 1338, 11, 264], "temperature": 0.0, "avg_logprob": -0.23468555874294705, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0004000527842435986}, {"id": 65, "seek": 37056, "start": 382.88, "end": 389.04, "text": " supercomputer or the remote architecture. At the bottom, two other", "tokens": [36708, 420, 264, 8607, 9482, 13, 1711, 264, 2767, 11, 732, 661], "temperature": 0.0, "avg_logprob": -0.23468555874294705, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0004000527842435986}, {"id": 66, "seek": 37056, "start": 389.04, "end": 393.24, "text": " modes that are available. If you... typically, if you have some graphic nodes on", "tokens": [14068, 300, 366, 2435, 13, 759, 291, 485, 5850, 11, 498, 291, 362, 512, 14089, 13891, 322], "temperature": 0.0, "avg_logprob": -0.23468555874294705, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0004000527842435986}, {"id": 67, "seek": 37056, "start": 393.24, "end": 399.96, "text": " your server, you can use them for just the rendering part and stay the data", "tokens": [428, 7154, 11, 291, 393, 764, 552, 337, 445, 264, 22407, 644, 293, 1754, 264, 1412], "temperature": 0.0, "avg_logprob": -0.23468555874294705, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0004000527842435986}, {"id": 68, "seek": 39996, "start": 399.96, "end": 405.76, "text": " management on the CPU nodes, and still, you can connect your client on it to see", "tokens": [4592, 322, 264, 13199, 13891, 11, 293, 920, 11, 291, 393, 1745, 428, 6423, 322, 309, 281, 536], "temperature": 0.0, "avg_logprob": -0.19634750208903834, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003142200584989041}, {"id": 69, "seek": 39996, "start": 405.76, "end": 410.88, "text": " what happened and to control from a graphical interface. And last mode, I will", "tokens": [437, 2011, 293, 281, 1969, 490, 257, 35942, 9226, 13, 400, 1036, 4391, 11, 286, 486], "temperature": 0.0, "avg_logprob": -0.19634750208903834, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003142200584989041}, {"id": 70, "seek": 39996, "start": 410.88, "end": 416.44, "text": " go back on it later. We have an institute infrastructure, so, basically, your", "tokens": [352, 646, 322, 309, 1780, 13, 492, 362, 364, 26860, 6896, 11, 370, 11, 1936, 11, 428], "temperature": 0.0, "avg_logprob": -0.19634750208903834, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003142200584989041}, {"id": 71, "seek": 39996, "start": 416.44, "end": 422.64, "text": " simulation can call an IPI that's Fluid Paraview script analysis, and you can", "tokens": [16575, 393, 818, 364, 8671, 40, 300, 311, 33612, 327, 3457, 706, 1093, 5755, 5215, 11, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.19634750208903834, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003142200584989041}, {"id": 72, "seek": 39996, "start": 422.64, "end": 426.84, "text": " even connect with a graphical client to see time step per time step what is", "tokens": [754, 1745, 365, 257, 35942, 6423, 281, 536, 565, 1823, 680, 565, 1823, 437, 307], "temperature": 0.0, "avg_logprob": -0.19634750208903834, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003142200584989041}, {"id": 73, "seek": 42684, "start": 426.84, "end": 434.44, "text": " happening on your simulation. So, that's for the different mode of use for", "tokens": [2737, 322, 428, 16575, 13, 407, 11, 300, 311, 337, 264, 819, 4391, 295, 764, 337], "temperature": 0.0, "avg_logprob": -0.19821117401123048, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0001531478192191571}, {"id": 74, "seek": 42684, "start": 434.44, "end": 441.96, "text": " Paraview. So, first, to run on HP infrastructure, we", "tokens": [3457, 706, 1093, 13, 407, 11, 700, 11, 281, 1190, 322, 12557, 6896, 11, 321], "temperature": 0.0, "avg_logprob": -0.19821117401123048, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0001531478192191571}, {"id": 75, "seek": 42684, "start": 441.96, "end": 446.52, "text": " implement data distributions for the analysis. So, basically, we rely on the", "tokens": [4445, 1412, 37870, 337, 264, 5215, 13, 407, 11, 1936, 11, 321, 10687, 322, 264], "temperature": 0.0, "avg_logprob": -0.19821117401123048, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0001531478192191571}, {"id": 76, "seek": 42684, "start": 446.52, "end": 453.28, "text": " MPI standards. So, our readers are MPI aware, so they can distribute the data", "tokens": [14146, 40, 7787, 13, 407, 11, 527, 17147, 366, 14146, 40, 3650, 11, 370, 436, 393, 20594, 264, 1412], "temperature": 0.0, "avg_logprob": -0.19821117401123048, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0001531478192191571}, {"id": 77, "seek": 45328, "start": 453.28, "end": 458.91999999999996, "text": " over the rank early in the process when you read your data on the disk. Then,", "tokens": [670, 264, 6181, 2440, 294, 264, 1399, 562, 291, 1401, 428, 1412, 322, 264, 12355, 13, 1396, 11], "temperature": 0.0, "avg_logprob": -0.17222844337930485, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.00022977222397457808}, {"id": 78, "seek": 45328, "start": 458.91999999999996, "end": 465.67999999999995, "text": " most of the filters are okay to run just with their support of data, but some", "tokens": [881, 295, 264, 15995, 366, 1392, 281, 1190, 445, 365, 641, 1406, 295, 1412, 11, 457, 512], "temperature": 0.0, "avg_logprob": -0.17222844337930485, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.00022977222397457808}, {"id": 79, "seek": 45328, "start": 465.67999999999995, "end": 470.59999999999997, "text": " other filters need to know about the neighborhood to execute correctly. So, for", "tokens": [661, 15995, 643, 281, 458, 466, 264, 7630, 281, 14483, 8944, 13, 407, 11, 337], "temperature": 0.0, "avg_logprob": -0.17222844337930485, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.00022977222397457808}, {"id": 80, "seek": 45328, "start": 470.59999999999997, "end": 475.91999999999996, "text": " that, we support the concept of ghost cells, where each rank knows a little bit", "tokens": [300, 11, 321, 1406, 264, 3410, 295, 8359, 5438, 11, 689, 1184, 6181, 3255, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17222844337930485, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.00022977222397457808}, {"id": 81, "seek": 45328, "start": 475.91999999999996, "end": 482.15999999999997, "text": " about the rank that is next to it. In that case, mainly, we split the data", "tokens": [466, 264, 6181, 300, 307, 958, 281, 309, 13, 682, 300, 1389, 11, 8704, 11, 321, 7472, 264, 1412], "temperature": 0.0, "avg_logprob": -0.17222844337930485, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.00022977222397457808}, {"id": 82, "seek": 48216, "start": 482.16, "end": 487.68, "text": " geometrically. So, a subset is really a geometric subset of your data, and", "tokens": [12956, 81, 984, 13, 407, 11, 257, 25993, 307, 534, 257, 33246, 25993, 295, 428, 1412, 11, 293], "temperature": 0.0, "avg_logprob": -0.2089483830835912, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.00019312002405058593}, {"id": 83, "seek": 48216, "start": 487.68, "end": 496.6, "text": " different one can know and communicate with the other for specific tasks. At", "tokens": [819, 472, 393, 458, 293, 7890, 365, 264, 661, 337, 2685, 9608, 13, 1711], "temperature": 0.0, "avg_logprob": -0.2089483830835912, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.00019312002405058593}, {"id": 84, "seek": 48216, "start": 496.6, "end": 501.48, "text": " least, we have some filters. So, what I call a filter is really something that", "tokens": [1935, 11, 321, 362, 512, 15995, 13, 407, 11, 437, 286, 818, 257, 6608, 307, 534, 746, 300], "temperature": 0.0, "avg_logprob": -0.2089483830835912, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.00019312002405058593}, {"id": 85, "seek": 48216, "start": 501.48, "end": 507.16, "text": " the user can instantiate from the client and ask to process. So, we can", "tokens": [264, 4195, 393, 9836, 13024, 490, 264, 6423, 293, 1029, 281, 1399, 13, 407, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.2089483830835912, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.00019312002405058593}, {"id": 86, "seek": 50716, "start": 507.16, "end": 516.1600000000001, "text": " ensure a load balancing by redistributing the data during the process. The", "tokens": [5586, 257, 3677, 22495, 538, 36198, 2024, 10861, 264, 1412, 1830, 264, 1399, 13, 440], "temperature": 0.0, "avg_logprob": -0.2618991196757615, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00015603608335368335}, {"id": 87, "seek": 50716, "start": 516.1600000000001, "end": 520.44, "text": " visualizations can also be distributed over several ranks. So, for that, we use", "tokens": [5056, 14455, 393, 611, 312, 12631, 670, 2940, 21406, 13, 407, 11, 337, 300, 11, 321, 764], "temperature": 0.0, "avg_logprob": -0.2618991196757615, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00015603608335368335}, {"id": 88, "seek": 50716, "start": 520.44, "end": 526.0, "text": " an inner library that call IST, that's also based on the MPI process to do that,", "tokens": [364, 7284, 6405, 300, 818, 6205, 51, 11, 300, 311, 611, 2361, 322, 264, 14146, 40, 1399, 281, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.2618991196757615, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00015603608335368335}, {"id": 89, "seek": 50716, "start": 526.0, "end": 531.76, "text": " and parallel view support has a different kind of model of rendering. So, you", "tokens": [293, 8952, 1910, 1406, 575, 257, 819, 733, 295, 2316, 295, 22407, 13, 407, 11, 291], "temperature": 0.0, "avg_logprob": -0.2618991196757615, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00015603608335368335}, {"id": 90, "seek": 50716, "start": 531.76, "end": 536.5600000000001, "text": " can, if you have dedicated rendering node, you can, as I said, create parallel", "tokens": [393, 11, 498, 291, 362, 8374, 22407, 9984, 11, 291, 393, 11, 382, 286, 848, 11, 1884, 8952], "temperature": 0.0, "avg_logprob": -0.2618991196757615, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00015603608335368335}, {"id": 91, "seek": 53656, "start": 536.56, "end": 541.2399999999999, "text": " view server just for the rendering part and connect it to the data server. You", "tokens": [1910, 7154, 445, 337, 264, 22407, 644, 293, 1745, 309, 281, 264, 1412, 7154, 13, 509], "temperature": 0.0, "avg_logprob": -0.4057531474549093, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00045425505959428847}, {"id": 92, "seek": 53656, "start": 541.2399999999999, "end": 546.04, "text": " can have multiple GPUs per rank, yeah, multiple GPUs per rank to do the", "tokens": [393, 362, 3866, 18407, 82, 680, 6181, 11, 1338, 11, 3866, 18407, 82, 680, 6181, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.4057531474549093, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00045425505959428847}, {"id": 93, "seek": 53656, "start": 546.04, "end": 551.8, "text": " rendering, but that's also possible locally if you have just one machine that", "tokens": [22407, 11, 457, 300, 311, 611, 1944, 16143, 498, 291, 362, 445, 472, 3479, 300], "temperature": 0.0, "avg_logprob": -0.4057531474549093, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00045425505959428847}, {"id": 94, "seek": 53656, "start": 551.8, "end": 559.4399999999999, "text": " have multiple GPUs, you can ask to do a rendering on both simil-tune-y.", "tokens": [362, 3866, 18407, 82, 11, 291, 393, 1029, 281, 360, 257, 22407, 322, 1293, 1034, 388, 12, 83, 2613, 12, 88, 13], "temperature": 0.0, "avg_logprob": -0.4057531474549093, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00045425505959428847}, {"id": 95, "seek": 55944, "start": 559.44, "end": 567.6800000000001, "text": " Concerning the performances now, so that distributions is not about", "tokens": [2656, 1776, 773, 264, 16087, 586, 11, 370, 300, 37870, 307, 406, 466], "temperature": 0.0, "avg_logprob": -0.248217614848962, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.00021890399511903524}, {"id": 96, "seek": 55944, "start": 567.6800000000001, "end": 572.8800000000001, "text": " performance, it's just about running with too big data so you cannot just run on", "tokens": [3389, 11, 309, 311, 445, 466, 2614, 365, 886, 955, 1412, 370, 291, 2644, 445, 1190, 322], "temperature": 0.0, "avg_logprob": -0.248217614848962, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.00021890399511903524}, {"id": 97, "seek": 55944, "start": 572.8800000000001, "end": 578.6400000000001, "text": " your machine, that's a requirement when you have huge data to be able to", "tokens": [428, 3479, 11, 300, 311, 257, 11695, 562, 291, 362, 2603, 1412, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.248217614848962, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.00021890399511903524}, {"id": 98, "seek": 55944, "start": 578.6400000000001, "end": 584.6400000000001, "text": " distribute it over your computer or your supercomputer. Now, we're talking a", "tokens": [20594, 309, 670, 428, 3820, 420, 428, 36708, 13, 823, 11, 321, 434, 1417, 257], "temperature": 0.0, "avg_logprob": -0.248217614848962, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.00021890399511903524}, {"id": 99, "seek": 55944, "start": 584.6400000000001, "end": 588.8000000000001, "text": " little bit about performance because if you have big data, you also need to be", "tokens": [707, 857, 466, 3389, 570, 498, 291, 362, 955, 1412, 11, 291, 611, 643, 281, 312], "temperature": 0.0, "avg_logprob": -0.248217614848962, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.00021890399511903524}, {"id": 100, "seek": 58880, "start": 588.8, "end": 595.68, "text": " performant on how you analyze it, on how you are proceed with it. So, for that, we", "tokens": [2042, 394, 322, 577, 291, 12477, 309, 11, 322, 577, 291, 366, 8991, 365, 309, 13, 407, 11, 337, 300, 11, 321], "temperature": 0.0, "avg_logprob": -0.30620489449336613, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.00033317977795377374}, {"id": 101, "seek": 58880, "start": 595.68, "end": 601.52, "text": " have a thin layer for CPU parallelism, we call that a simple tool in our code", "tokens": [362, 257, 5862, 4583, 337, 13199, 8952, 1434, 11, 321, 818, 300, 257, 2199, 2290, 294, 527, 3089], "temperature": 0.0, "avg_logprob": -0.30620489449336613, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.00033317977795377374}, {"id": 102, "seek": 58880, "start": 601.52, "end": 610.3599999999999, "text": " base. The goal is to parallelize, do code parallelizations for many for loop, and", "tokens": [3096, 13, 440, 3387, 307, 281, 8952, 1125, 11, 360, 3089, 8952, 14455, 337, 867, 337, 6367, 11, 293], "temperature": 0.0, "avg_logprob": -0.30620489449336613, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.00033317977795377374}, {"id": 103, "seek": 58880, "start": 610.3599999999999, "end": 616.3599999999999, "text": " main purpose is that you can choose at build time and then at run time, if you", "tokens": [2135, 4334, 307, 300, 291, 393, 2826, 412, 1322, 565, 293, 550, 412, 1190, 565, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.30620489449336613, "compression_ratio": 1.6377551020408163, "no_speech_prob": 0.00033317977795377374}, {"id": 104, "seek": 61636, "start": 616.36, "end": 621.72, "text": " enable the OpenMP or TBB backends, and if you don't want external live, you can", "tokens": [9528, 264, 7238, 12224, 420, 29711, 33, 646, 2581, 11, 293, 498, 291, 500, 380, 528, 8320, 1621, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.23242142086937315, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.0003691876772791147}, {"id": 105, "seek": 61636, "start": 621.72, "end": 632.6800000000001, "text": " also use the C++ thread to do that. And so, as it just, for instance, to", "tokens": [611, 764, 264, 383, 25472, 7207, 281, 360, 300, 13, 400, 370, 11, 382, 309, 445, 11, 337, 5197, 11, 281], "temperature": 0.0, "avg_logprob": -0.23242142086937315, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.0003691876772791147}, {"id": 106, "seek": 61636, "start": 632.6800000000001, "end": 638.6, "text": " parallelize a for loop or field operations, it's really widely used in a lot", "tokens": [8952, 1125, 257, 337, 6367, 420, 2519, 7705, 11, 309, 311, 534, 13371, 1143, 294, 257, 688], "temperature": 0.0, "avg_logprob": -0.23242142086937315, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.0003691876772791147}, {"id": 107, "seek": 61636, "start": 638.6, "end": 644.36, "text": " of our, in our algorithm, and you have some environment variable that can", "tokens": [295, 527, 11, 294, 527, 9284, 11, 293, 291, 362, 512, 2823, 7006, 300, 393], "temperature": 0.0, "avg_logprob": -0.23242142086937315, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.0003691876772791147}, {"id": 108, "seek": 64436, "start": 644.36, "end": 650.2, "text": " control the back end on some of the number of, of thread, the size of the", "tokens": [1969, 264, 646, 917, 322, 512, 295, 264, 1230, 295, 11, 295, 7207, 11, 264, 2744, 295, 264], "temperature": 0.0, "avg_logprob": -0.2551963852673042, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.00027899135602638125}, {"id": 109, "seek": 64436, "start": 650.2, "end": 655.84, "text": " thread pools, or if you allow nested pools or so on, depending on the, on your", "tokens": [7207, 28688, 11, 420, 498, 291, 2089, 15646, 292, 28688, 420, 370, 322, 11, 5413, 322, 264, 11, 322, 428], "temperature": 0.0, "avg_logprob": -0.2551963852673042, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.00027899135602638125}, {"id": 110, "seek": 64436, "start": 655.84, "end": 661.96, "text": " resources and back end. So, it has some documentation on it, and we made some", "tokens": [3593, 293, 646, 917, 13, 407, 11, 309, 575, 512, 14333, 322, 309, 11, 293, 321, 1027, 512], "temperature": 0.0, "avg_logprob": -0.2551963852673042, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.00027899135602638125}, {"id": 111, "seek": 64436, "start": 661.96, "end": 672.12, "text": " improvements last year about that. Still, still about performances, we also use as", "tokens": [13797, 1036, 1064, 466, 300, 13, 8291, 11, 920, 466, 16087, 11, 321, 611, 764, 382], "temperature": 0.0, "avg_logprob": -0.2551963852673042, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.00027899135602638125}, {"id": 112, "seek": 67212, "start": 672.12, "end": 679.2, "text": " an optional dependency, the VTKM, VTKM projects that stand for, yes, somewhat", "tokens": [364, 17312, 33621, 11, 264, 691, 51, 42, 44, 11, 691, 51, 42, 44, 4455, 300, 1463, 337, 11, 2086, 11, 8344], "temperature": 0.0, "avg_logprob": -0.22578890183392694, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00042180949822068214}, {"id": 113, "seek": 67212, "start": 679.2, "end": 685.6, "text": " some many core that is intended to be used on heterogeneous systems. So, basically,", "tokens": [512, 867, 4965, 300, 307, 10226, 281, 312, 1143, 322, 20789, 31112, 3652, 13, 407, 11, 1936, 11], "temperature": 0.0, "avg_logprob": -0.22578890183392694, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00042180949822068214}, {"id": 114, "seek": 67212, "start": 685.6, "end": 691.92, "text": " when you want to have performance on supercomputer or even, you, you still need", "tokens": [562, 291, 528, 281, 362, 3389, 322, 36708, 420, 754, 11, 291, 11, 291, 920, 643], "temperature": 0.0, "avg_logprob": -0.22578890183392694, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00042180949822068214}, {"id": 115, "seek": 67212, "start": 691.92, "end": 697.28, "text": " to be aware of the current technology and the state of the art, and as we saw in", "tokens": [281, 312, 3650, 295, 264, 2190, 2899, 293, 264, 1785, 295, 264, 1523, 11, 293, 382, 321, 1866, 294], "temperature": 0.0, "avg_logprob": -0.22578890183392694, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00042180949822068214}, {"id": 116, "seek": 69728, "start": 697.28, "end": 703.76, "text": " the past decades, a lot of new architectures emerging. We, we think about", "tokens": [264, 1791, 7878, 11, 257, 688, 295, 777, 6331, 1303, 14989, 13, 492, 11, 321, 519, 466], "temperature": 0.0, "avg_logprob": -0.21749613144818475, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.00020131457131356}, {"id": 117, "seek": 69728, "start": 703.76, "end": 712.24, "text": " using a dedicated library to, to be able to use this, this new architecture. So,", "tokens": [1228, 257, 8374, 6405, 281, 11, 281, 312, 1075, 281, 764, 341, 11, 341, 777, 9482, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.21749613144818475, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.00020131457131356}, {"id": 118, "seek": 69728, "start": 712.24, "end": 717.36, "text": " with VTKM, the goal, inside VTKM library, the goal is to split all", "tokens": [365, 691, 51, 42, 44, 11, 264, 3387, 11, 1854, 691, 51, 42, 44, 6405, 11, 264, 3387, 307, 281, 7472, 439], "temperature": 0.0, "avg_logprob": -0.21749613144818475, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.00020131457131356}, {"id": 119, "seek": 69728, "start": 717.36, "end": 723.4, "text": " operations into really atomic operations, and then the, the, at runtime, it can", "tokens": [7705, 666, 534, 22275, 7705, 11, 293, 550, 264, 11, 264, 11, 412, 34474, 11, 309, 393], "temperature": 0.0, "avg_logprob": -0.21749613144818475, "compression_ratio": 1.6096256684491979, "no_speech_prob": 0.00020131457131356}, {"id": 120, "seek": 72340, "start": 723.4, "end": 729.0799999999999, "text": " dispatch all, all that on the hardware you find on the back end that are", "tokens": [36729, 439, 11, 439, 300, 322, 264, 8837, 291, 915, 322, 264, 646, 917, 300, 366], "temperature": 0.0, "avg_logprob": -0.29904463124829667, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00012048330245306715}, {"id": 121, "seek": 72340, "start": 729.0799999999999, "end": 734.8, "text": " available. So, with VTKM, you can use CUDA, OpenMP, TBB also, to do the", "tokens": [2435, 13, 407, 11, 365, 691, 51, 42, 44, 11, 291, 393, 764, 29777, 7509, 11, 7238, 12224, 11, 29711, 33, 611, 11, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.29904463124829667, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00012048330245306715}, {"id": 122, "seek": 72340, "start": 734.8, "end": 741.9599999999999, "text": " computation. This time, with VTKM is not just accelerating some specific loop", "tokens": [24903, 13, 639, 565, 11, 365, 691, 51, 42, 44, 307, 406, 445, 34391, 512, 2685, 6367], "temperature": 0.0, "avg_logprob": -0.29904463124829667, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00012048330245306715}, {"id": 123, "seek": 72340, "start": 741.9599999999999, "end": 747.96, "text": " inside an algorithm, it's more about VTKM is implementing some whole algorithm", "tokens": [1854, 364, 9284, 11, 309, 311, 544, 466, 691, 51, 42, 44, 307, 18114, 512, 1379, 9284], "temperature": 0.0, "avg_logprob": -0.29904463124829667, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00012048330245306715}, {"id": 124, "seek": 74796, "start": 747.96, "end": 757.72, "text": " like extracting ISO control or, or so. And then, we embed this into, into", "tokens": [411, 49844, 25042, 1969, 420, 11, 420, 370, 13, 400, 550, 11, 321, 12240, 341, 666, 11, 666], "temperature": 0.0, "avg_logprob": -0.2709510398633552, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.00016660451365169138}, {"id": 125, "seek": 74796, "start": 757.72, "end": 764.84, "text": " Paraview with some kind of wrapper to, to communicate with all VTKM works. So,", "tokens": [3457, 706, 1093, 365, 512, 733, 295, 46906, 281, 11, 281, 7890, 365, 439, 691, 51, 42, 44, 1985, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.2709510398633552, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.00016660451365169138}, {"id": 126, "seek": 74796, "start": 764.84, "end": 771.32, "text": " that's optional, that's enabled by default in the binaries we, we provide.", "tokens": [300, 311, 17312, 11, 300, 311, 15172, 538, 7576, 294, 264, 5171, 4889, 321, 11, 321, 2893, 13], "temperature": 0.0, "avg_logprob": -0.2709510398633552, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.00016660451365169138}, {"id": 127, "seek": 77132, "start": 771.32, "end": 778.5200000000001, "text": " Another point about performance, but that's really depending on the use case,", "tokens": [3996, 935, 466, 3389, 11, 457, 300, 311, 534, 5413, 322, 264, 764, 1389, 11], "temperature": 0.0, "avg_logprob": -0.21206889106231985, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00019822007743641734}, {"id": 128, "seek": 77132, "start": 778.5200000000001, "end": 784.4000000000001, "text": " on, on the data you are using is the in-situ wall. So, basically, when you,", "tokens": [322, 11, 322, 264, 1412, 291, 366, 1228, 307, 264, 294, 12, 82, 6380, 2929, 13, 407, 11, 1936, 11, 562, 291, 11], "temperature": 0.0, "avg_logprob": -0.21206889106231985, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00019822007743641734}, {"id": 129, "seek": 77132, "start": 784.4000000000001, "end": 789.08, "text": " traditionally, when you have your simulation, it dumps every time step or", "tokens": [19067, 11, 562, 291, 362, 428, 16575, 11, 309, 11430, 82, 633, 565, 1823, 420], "temperature": 0.0, "avg_logprob": -0.21206889106231985, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00019822007743641734}, {"id": 130, "seek": 77132, "start": 789.08, "end": 793.48, "text": " every end time step some data on the disk. And then, to analyze, you have to load", "tokens": [633, 917, 565, 1823, 512, 1412, 322, 264, 12355, 13, 400, 550, 11, 281, 12477, 11, 291, 362, 281, 3677], "temperature": 0.0, "avg_logprob": -0.21206889106231985, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00019822007743641734}, {"id": 131, "seek": 77132, "start": 793.48, "end": 799.6800000000001, "text": " back to the data with post-processing tools. But that adds a cost of writing", "tokens": [646, 281, 264, 1412, 365, 2183, 12, 41075, 278, 3873, 13, 583, 300, 10860, 257, 2063, 295, 3579], "temperature": 0.0, "avg_logprob": -0.21206889106231985, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00019822007743641734}, {"id": 132, "seek": 79968, "start": 799.68, "end": 805.3199999999999, "text": " and reading from your disk. And you, you should have the size on your disk, the", "tokens": [293, 3760, 490, 428, 12355, 13, 400, 291, 11, 291, 820, 362, 264, 2744, 322, 428, 12355, 11, 264], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 133, "seek": 79968, "start": 805.3199999999999, "end": 809.7199999999999, "text": " whole size of the disk. So, you should have big disk. And then, it's, it have", "tokens": [1379, 2744, 295, 264, 12355, 13, 407, 11, 291, 820, 362, 955, 12355, 13, 400, 550, 11, 309, 311, 11, 309, 362], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 134, "seek": 79968, "start": 809.7199999999999, "end": 815.0, "text": " really a cost in term of time, when you should write a full, a full mesh or full", "tokens": [534, 257, 2063, 294, 1433, 295, 565, 11, 562, 291, 820, 2464, 257, 1577, 11, 257, 1577, 17407, 420, 1577], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 135, "seek": 79968, "start": 815.0, "end": 819.5999999999999, "text": " data on disk. And then, read back with another process. So, basically, the goal", "tokens": [1412, 322, 12355, 13, 400, 550, 11, 1401, 646, 365, 1071, 1399, 13, 407, 11, 1936, 11, 264, 3387], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 136, "seek": 79968, "start": 819.5999999999999, "end": 825.0, "text": " of in-situ is to, to make the simulation communicate directly with the", "tokens": [295, 294, 12, 82, 6380, 307, 281, 11, 281, 652, 264, 16575, 7890, 3838, 365, 264], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 137, "seek": 79968, "start": 825.0, "end": 829.56, "text": " processing tools. And then, the processing tools can wrap the memory in place and", "tokens": [9007, 3873, 13, 400, 550, 11, 264, 9007, 3873, 393, 7019, 264, 4675, 294, 1081, 293], "temperature": 0.0, "avg_logprob": -0.21905926295689174, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.00021945447952020913}, {"id": 138, "seek": 82956, "start": 829.56, "end": 835.4799999999999, "text": " analyze directly in, in the RAM without writing on the disk to save some", "tokens": [12477, 3838, 294, 11, 294, 264, 14561, 1553, 3579, 322, 264, 12355, 281, 3155, 512], "temperature": 0.0, "avg_logprob": -0.27539861613306504, "compression_ratio": 1.3411764705882352, "no_speech_prob": 0.0002774650347419083}, {"id": 139, "seek": 82956, "start": 835.4799999999999, "end": 845.0, "text": " higher time. So, in the context of ParaView, we have a standalone API that's", "tokens": [2946, 565, 13, 407, 11, 294, 264, 4319, 295, 11107, 30203, 11, 321, 362, 257, 37454, 9362, 300, 311], "temperature": 0.0, "avg_logprob": -0.27539861613306504, "compression_ratio": 1.3411764705882352, "no_speech_prob": 0.0002774650347419083}, {"id": 140, "seek": 82956, "start": 845.0, "end": 851.1999999999999, "text": " called Catalysts. That was recently released, as we make big improvements into", "tokens": [1219, 9565, 19530, 82, 13, 663, 390, 3938, 4736, 11, 382, 321, 652, 955, 13797, 666], "temperature": 0.0, "avg_logprob": -0.27539861613306504, "compression_ratio": 1.3411764705882352, "no_speech_prob": 0.0002774650347419083}, {"id": 141, "seek": 85120, "start": 851.2, "end": 859.72, "text": " Catalysts past years. And the goal of Catalysts is to have a really minimal API", "tokens": [9565, 19530, 82, 1791, 924, 13, 400, 264, 3387, 295, 9565, 19530, 82, 307, 281, 362, 257, 534, 13206, 9362], "temperature": 0.0, "avg_logprob": -0.20812410177643767, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.000253570411587134}, {"id": 142, "seek": 85120, "start": 859.72, "end": 866.4000000000001, "text": " and stable API. So, you can choose and run time the implementation you, you want.", "tokens": [293, 8351, 9362, 13, 407, 11, 291, 393, 2826, 293, 1190, 565, 264, 11420, 291, 11, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.20812410177643767, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.000253570411587134}, {"id": 143, "seek": 85120, "start": 866.4000000000001, "end": 872.24, "text": " And one other goal is to minimize the instrumentation you need to do in your", "tokens": [400, 472, 661, 3387, 307, 281, 17522, 264, 7198, 399, 291, 643, 281, 360, 294, 428], "temperature": 0.0, "avg_logprob": -0.20812410177643767, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.000253570411587134}, {"id": 144, "seek": 85120, "start": 872.24, "end": 876.1600000000001, "text": " simulation code directly. So, it's really easy for simulation developer to", "tokens": [16575, 3089, 3838, 13, 407, 11, 309, 311, 534, 1858, 337, 16575, 10754, 281], "temperature": 0.0, "avg_logprob": -0.20812410177643767, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.000253570411587134}, {"id": 145, "seek": 85120, "start": 876.1600000000001, "end": 881.12, "text": " understand the few key places where they have to put a new code to call our", "tokens": [1223, 264, 1326, 2141, 3190, 689, 436, 362, 281, 829, 257, 777, 3089, 281, 818, 527], "temperature": 0.0, "avg_logprob": -0.20812410177643767, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.000253570411587134}, {"id": 146, "seek": 88112, "start": 881.12, "end": 887.64, "text": " API. So, here is a really basic example from one on the tutorial we have. We need", "tokens": [9362, 13, 407, 11, 510, 307, 257, 534, 3875, 1365, 490, 472, 322, 264, 7073, 321, 362, 13, 492, 643], "temperature": 0.0, "avg_logprob": -0.14290055774507068, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.00021775199274998158}, {"id": 147, "seek": 88112, "start": 887.64, "end": 891.36, "text": " to initialize, of course, and you need to call some method at each time step", "tokens": [281, 5883, 1125, 11, 295, 1164, 11, 293, 291, 643, 281, 818, 512, 3170, 412, 1184, 565, 1823], "temperature": 0.0, "avg_logprob": -0.14290055774507068, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.00021775199274998158}, {"id": 148, "seek": 88112, "start": 891.36, "end": 899.68, "text": " where you want the processing to happen. And finalizations. Of course, the, you", "tokens": [689, 291, 528, 264, 9007, 281, 1051, 13, 400, 2572, 14455, 13, 2720, 1164, 11, 264, 11, 291], "temperature": 0.0, "avg_logprob": -0.14290055774507068, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.00021775199274998158}, {"id": 149, "seek": 88112, "start": 899.68, "end": 905.8, "text": " still have to do a little layer to describe your, your data. For that, we", "tokens": [920, 362, 281, 360, 257, 707, 4583, 281, 6786, 428, 11, 428, 1412, 13, 1171, 300, 11, 321], "temperature": 0.0, "avg_logprob": -0.14290055774507068, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.00021775199274998158}, {"id": 150, "seek": 90580, "start": 905.8, "end": 915.16, "text": " do some sort of a partial library to, to help us. So, ParaView, so Catalyst is a", "tokens": [360, 512, 1333, 295, 257, 14641, 6405, 281, 11, 281, 854, 505, 13, 407, 11, 11107, 30203, 11, 370, 9565, 19530, 307, 257], "temperature": 0.0, "avg_logprob": -0.3304699489048549, "compression_ratio": 1.5419354838709678, "no_speech_prob": 0.0002644882188178599}, {"id": 151, "seek": 90580, "start": 915.16, "end": 921.4399999999999, "text": " standard, I say standalone, is not, is independent project, no, independent of", "tokens": [3832, 11, 286, 584, 37454, 11, 307, 406, 11, 307, 6695, 1716, 11, 572, 11, 6695, 295], "temperature": 0.0, "avg_logprob": -0.3304699489048549, "compression_ratio": 1.5419354838709678, "no_speech_prob": 0.0002644882188178599}, {"id": 152, "seek": 90580, "start": 921.4399999999999, "end": 925.92, "text": " ParaView. But of course, the first real implementation is an implementation for", "tokens": [11107, 30203, 13, 583, 295, 1164, 11, 264, 700, 957, 11420, 307, 364, 11420, 337], "temperature": 0.0, "avg_logprob": -0.3304699489048549, "compression_ratio": 1.5419354838709678, "no_speech_prob": 0.0002644882188178599}, {"id": 153, "seek": 92592, "start": 925.92, "end": 938.5999999999999, "text": " ParaView. So, we, sorry. So, yes, we, we implement Catalyst. So, the back end, so", "tokens": [11107, 30203, 13, 407, 11, 321, 11, 2597, 13, 407, 11, 2086, 11, 321, 11, 321, 4445, 9565, 19530, 13, 407, 11, 264, 646, 917, 11, 370], "temperature": 0.0, "avg_logprob": -0.2542401377360026, "compression_ratio": 1.49375, "no_speech_prob": 0.00035361855407245457}, {"id": 154, "seek": 92592, "start": 938.5999999999999, "end": 944.04, "text": " you can run ParaView pipeline directly from your simulation. It's each time", "tokens": [291, 393, 1190, 11107, 30203, 15517, 3838, 490, 428, 16575, 13, 467, 311, 1184, 565], "temperature": 0.0, "avg_logprob": -0.2542401377360026, "compression_ratio": 1.49375, "no_speech_prob": 0.00035361855407245457}, {"id": 155, "seek": 92592, "start": 944.04, "end": 953.48, "text": " step or when, when you call it. So, how does it, how does it work? Or do you, you", "tokens": [1823, 420, 562, 11, 562, 291, 818, 309, 13, 407, 11, 577, 775, 309, 11, 577, 775, 309, 589, 30, 1610, 360, 291, 11, 291], "temperature": 0.0, "avg_logprob": -0.2542401377360026, "compression_ratio": 1.49375, "no_speech_prob": 0.00035361855407245457}, {"id": 156, "seek": 95348, "start": 953.48, "end": 958.24, "text": " can, the idea is that you are, are called the communication between your", "tokens": [393, 11, 264, 1558, 307, 300, 291, 366, 11, 366, 1219, 264, 6101, 1296, 428], "temperature": 0.0, "avg_logprob": -0.2276964809583581, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00018406295566819608}, {"id": 157, "seek": 95348, "start": 958.24, "end": 963.16, "text": " simulation and the, on Catalyst through the API. But then the actual script that", "tokens": [16575, 293, 264, 11, 322, 9565, 19530, 807, 264, 9362, 13, 583, 550, 264, 3539, 5755, 300], "temperature": 0.0, "avg_logprob": -0.2276964809583581, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00018406295566819608}, {"id": 158, "seek": 95348, "start": 963.16, "end": 967.5600000000001, "text": " is executed, the actual pipeline and visualization, visualization you want to", "tokens": [307, 17577, 11, 264, 3539, 15517, 293, 25801, 11, 25801, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.2276964809583581, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00018406295566819608}, {"id": 159, "seek": 95348, "start": 967.5600000000001, "end": 974.28, "text": " produce. It's all scriptable thanks to the Python wrapping of, of ParaView. You", "tokens": [5258, 13, 467, 311, 439, 5755, 712, 3231, 281, 264, 15329, 21993, 295, 11, 295, 11107, 30203, 13, 509], "temperature": 0.0, "avg_logprob": -0.2276964809583581, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00018406295566819608}, {"id": 160, "seek": 95348, "start": 974.28, "end": 979.04, "text": " can even, you can even, sorry, load some representative data in the graphical", "tokens": [393, 754, 11, 291, 393, 754, 11, 2597, 11, 3677, 512, 12424, 1412, 294, 264, 35942], "temperature": 0.0, "avg_logprob": -0.2276964809583581, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.00018406295566819608}, {"id": 161, "seek": 97904, "start": 979.04, "end": 983.7199999999999, "text": " interface of ParaView. Do some analysis, export this as a Python script and use", "tokens": [9226, 295, 11107, 30203, 13, 1144, 512, 5215, 11, 10725, 341, 382, 257, 15329, 5755, 293, 764], "temperature": 0.0, "avg_logprob": -0.19224770863850912, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0002924492582678795}, {"id": 162, "seek": 97904, "start": 983.7199999999999, "end": 988.88, "text": " the script to feed Catalyst. And then, when you run your simulation with", "tokens": [264, 5755, 281, 3154, 9565, 19530, 13, 400, 550, 11, 562, 291, 1190, 428, 16575, 365], "temperature": 0.0, "avg_logprob": -0.19224770863850912, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0002924492582678795}, {"id": 163, "seek": 97904, "start": 988.88, "end": 994.3199999999999, "text": " Catalyst enable, it will reuse the script you produce directly from the GUI. So,", "tokens": [9565, 19530, 9528, 11, 309, 486, 26225, 264, 5755, 291, 5258, 3838, 490, 264, 17917, 40, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.19224770863850912, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0002924492582678795}, {"id": 164, "seek": 97904, "start": 994.3199999999999, "end": 1000.16, "text": " people that are not at all developers still can do some stuff with, with", "tokens": [561, 300, 366, 406, 412, 439, 8849, 920, 393, 360, 512, 1507, 365, 11, 365], "temperature": 0.0, "avg_logprob": -0.19224770863850912, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0002924492582678795}, {"id": 165, "seek": 97904, "start": 1000.16, "end": 1005.48, "text": " Catalyst. And last point is that, when you have a running simulation with the", "tokens": [9565, 19530, 13, 400, 1036, 935, 307, 300, 11, 562, 291, 362, 257, 2614, 16575, 365, 264], "temperature": 0.0, "avg_logprob": -0.19224770863850912, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0002924492582678795}, {"id": 166, "seek": 100548, "start": 1005.48, "end": 1009.88, "text": " Catalyst pipeline on your dedicated server, you also can use the GUI to", "tokens": [9565, 19530, 15517, 322, 428, 8374, 7154, 11, 291, 611, 393, 764, 264, 17917, 40, 281], "temperature": 0.0, "avg_logprob": -0.14383327838071844, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00021435289818327874}, {"id": 167, "seek": 100548, "start": 1009.88, "end": 1016.24, "text": " connect to this ParaView server and to see real-time get some screenshots of the", "tokens": [1745, 281, 341, 11107, 30203, 7154, 293, 281, 536, 957, 12, 3766, 483, 512, 40661, 295, 264], "temperature": 0.0, "avg_logprob": -0.14383327838071844, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00021435289818327874}, {"id": 168, "seek": 100548, "start": 1016.24, "end": 1021.04, "text": " visualization on the analysis that is proceeding on the server. So, you can have", "tokens": [25801, 322, 264, 5215, 300, 307, 41163, 322, 264, 7154, 13, 407, 11, 291, 393, 362], "temperature": 0.0, "avg_logprob": -0.14383327838071844, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00021435289818327874}, {"id": 169, "seek": 100548, "start": 1021.04, "end": 1026.44, "text": " a feedback, a time step per test, a time step on what happened on the simulation.", "tokens": [257, 5824, 11, 257, 565, 1823, 680, 1500, 11, 257, 565, 1823, 322, 437, 2011, 322, 264, 16575, 13], "temperature": 0.0, "avg_logprob": -0.14383327838071844, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00021435289818327874}, {"id": 170, "seek": 100548, "start": 1026.44, "end": 1030.64, "text": " So, if you see that something is diverging or going wrong, you can stop", "tokens": [407, 11, 498, 291, 536, 300, 746, 307, 18558, 3249, 420, 516, 2085, 11, 291, 393, 1590], "temperature": 0.0, "avg_logprob": -0.14383327838071844, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00021435289818327874}, {"id": 171, "seek": 103064, "start": 1030.64, "end": 1036.2, "text": " your simulation directly and you don't waste all the time before seeing that", "tokens": [428, 16575, 3838, 293, 291, 500, 380, 5964, 439, 264, 565, 949, 2577, 300], "temperature": 0.0, "avg_logprob": -0.24495604163721987, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00027599456370808184}, {"id": 172, "seek": 103064, "start": 1036.2, "end": 1041.88, "text": " something went wrong and that you should tweak the parameter and start again.", "tokens": [746, 1437, 2085, 293, 300, 291, 820, 29879, 264, 13075, 293, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.24495604163721987, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00027599456370808184}, {"id": 173, "seek": 103064, "start": 1043.64, "end": 1052.76, "text": " So, yeah, I was quite faster than expected for me. So, in the conclusions of", "tokens": [407, 11, 1338, 11, 286, 390, 1596, 4663, 813, 5176, 337, 385, 13, 407, 11, 294, 264, 22865, 295], "temperature": 0.0, "avg_logprob": -0.24495604163721987, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00027599456370808184}, {"id": 174, "seek": 103064, "start": 1052.76, "end": 1059.2800000000002, "text": " to, to be able to run efficiently on the, on the supercomputer with ParaView, we", "tokens": [281, 11, 281, 312, 1075, 281, 1190, 19621, 322, 264, 11, 322, 264, 36708, 365, 11107, 30203, 11, 321], "temperature": 0.0, "avg_logprob": -0.24495604163721987, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00027599456370808184}, {"id": 175, "seek": 105928, "start": 1059.28, "end": 1064.44, "text": " implemented a client-server mode. The server can be, is MPIO rare and can be run", "tokens": [12270, 257, 6423, 12, 12484, 331, 4391, 13, 440, 7154, 393, 312, 11, 307, 14146, 15167, 5892, 293, 393, 312, 1190], "temperature": 0.0, "avg_logprob": -0.31146534379706325, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.0001922151423059404}, {"id": 176, "seek": 105928, "start": 1064.44, "end": 1069.6, "text": " on distributed environments. We are relying on old, on well-known libraries", "tokens": [322, 12631, 12388, 13, 492, 366, 24140, 322, 1331, 11, 322, 731, 12, 6861, 15148], "temperature": 0.0, "avg_logprob": -0.31146534379706325, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.0001922151423059404}, {"id": 177, "seek": 105928, "start": 1069.6, "end": 1077.16, "text": " such as implementation of MPI to the distributions, but we are also really", "tokens": [1270, 382, 11420, 295, 14146, 40, 281, 264, 37870, 11, 457, 321, 366, 611, 534], "temperature": 0.0, "avg_logprob": -0.31146534379706325, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.0001922151423059404}, {"id": 178, "seek": 105928, "start": 1077.16, "end": 1087.16, "text": " looking for, toward new, new library that can help us. Yeah, and we, we are able to", "tokens": [1237, 337, 11, 7361, 777, 11, 777, 6405, 300, 393, 854, 505, 13, 865, 11, 293, 321, 11, 321, 366, 1075, 281], "temperature": 0.0, "avg_logprob": -0.31146534379706325, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.0001922151423059404}, {"id": 179, "seek": 108716, "start": 1087.16, "end": 1093.64, "text": " integrate new library to, to do some performance analysis on new library that", "tokens": [13365, 777, 6405, 281, 11, 281, 360, 512, 3389, 5215, 322, 777, 6405, 300], "temperature": 0.0, "avg_logprob": -0.16636672201035899, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.00033380085369572043}, {"id": 180, "seek": 108716, "start": 1093.64, "end": 1100.0800000000002, "text": " is aware of a new architecture of supercomputer or new technology. That's", "tokens": [307, 3650, 295, 257, 777, 9482, 295, 36708, 420, 777, 2899, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.16636672201035899, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.00033380085369572043}, {"id": 181, "seek": 108716, "start": 1100.0800000000002, "end": 1107.0800000000002, "text": " okay with, for instance, with VTKM or, or others. And we have this API to do", "tokens": [1392, 365, 11, 337, 5197, 11, 365, 691, 51, 42, 44, 420, 11, 420, 2357, 13, 400, 321, 362, 341, 9362, 281, 360], "temperature": 0.0, "avg_logprob": -0.16636672201035899, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.00033380085369572043}, {"id": 182, "seek": 108716, "start": 1107.0800000000002, "end": 1114.1200000000001, "text": " institute that can save a lot of time and disk space. Yeah, just a slide to", "tokens": [26860, 300, 393, 3155, 257, 688, 295, 565, 293, 12355, 1901, 13, 865, 11, 445, 257, 4137, 281], "temperature": 0.0, "avg_logprob": -0.16636672201035899, "compression_ratio": 1.504950495049505, "no_speech_prob": 0.00033380085369572043}, {"id": 183, "seek": 111412, "start": 1114.12, "end": 1118.56, "text": " summarize the organize. So, we have different kind of way to interact with", "tokens": [20858, 264, 13859, 13, 407, 11, 321, 362, 819, 733, 295, 636, 281, 4648, 365], "temperature": 0.0, "avg_logprob": -0.28502720799939385, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.0004090989532414824}, {"id": 184, "seek": 111412, "start": 1118.56, "end": 1124.0, "text": " ParaView. Yeah, the grid, the Python scripting, the catalyst in city stuff. You", "tokens": [11107, 30203, 13, 865, 11, 264, 10748, 11, 264, 15329, 5755, 278, 11, 264, 23868, 294, 2307, 1507, 13, 509], "temperature": 0.0, "avg_logprob": -0.28502720799939385, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.0004090989532414824}, {"id": 185, "seek": 111412, "start": 1124.0, "end": 1130.12, "text": " can also build some custom one. We have some web example of clients. And at the", "tokens": [393, 611, 1322, 512, 2375, 472, 13, 492, 362, 512, 3670, 1365, 295, 6982, 13, 400, 412, 264], "temperature": 0.0, "avg_logprob": -0.28502720799939385, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.0004090989532414824}, {"id": 186, "seek": 111412, "start": 1130.12, "end": 1136.6799999999998, "text": " bottom, we have a list, a non-finite list of library on which we will like to, to,", "tokens": [2767, 11, 321, 362, 257, 1329, 11, 257, 2107, 12, 5194, 642, 1329, 295, 6405, 322, 597, 321, 486, 411, 281, 11, 281, 11], "temperature": 0.0, "avg_logprob": -0.28502720799939385, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.0004090989532414824}, {"id": 187, "seek": 113668, "start": 1136.68, "end": 1145.3200000000002, "text": " to do the effective work. So, basically, open GL, MPI, open, open MP. And so,", "tokens": [281, 360, 264, 4942, 589, 13, 407, 11, 1936, 11, 1269, 16225, 11, 14146, 40, 11, 1269, 11, 1269, 14146, 13, 400, 370, 11], "temperature": 0.0, "avg_logprob": -0.28381498469862826, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.00010861378541449085}, {"id": 188, "seek": 113668, "start": 1145.3200000000002, "end": 1151.3600000000001, "text": " concerning roadmap, we have several improvements that are coming. First, I", "tokens": [18087, 35738, 11, 321, 362, 2940, 13797, 300, 366, 1348, 13, 2386, 11, 286], "temperature": 0.0, "avg_logprob": -0.28381498469862826, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.00010861378541449085}, {"id": 189, "seek": 113668, "start": 1151.3600000000001, "end": 1157.72, "text": " talk about in, in situ, in the current implementation, you have each rank that", "tokens": [751, 466, 294, 11, 294, 2054, 11, 294, 264, 2190, 11420, 11, 291, 362, 1184, 6181, 300], "temperature": 0.0, "avg_logprob": -0.28381498469862826, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.00010861378541449085}, {"id": 190, "seek": 113668, "start": 1157.72, "end": 1163.0800000000002, "text": " does the simulation, does also the, the co-processing work. So, that's not always", "tokens": [775, 264, 16575, 11, 775, 611, 264, 11, 264, 598, 12, 41075, 278, 589, 13, 407, 11, 300, 311, 406, 1009], "temperature": 0.0, "avg_logprob": -0.28381498469862826, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.00010861378541449085}, {"id": 191, "seek": 116308, "start": 1163.08, "end": 1168.1999999999998, "text": " what is intended. Sometimes, you want to do the co-processing on the other rank.", "tokens": [437, 307, 10226, 13, 4803, 11, 291, 528, 281, 360, 264, 598, 12, 41075, 278, 322, 264, 661, 6181, 13], "temperature": 0.0, "avg_logprob": -0.26081900544218967, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.00019923927902709693}, {"id": 192, "seek": 116308, "start": 1168.1999999999998, "end": 1173.1999999999998, "text": " Just because, for instance, you have dedicated the rank for visualization. So,", "tokens": [1449, 570, 11, 337, 5197, 11, 291, 362, 8374, 264, 6181, 337, 25801, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.26081900544218967, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.00019923927902709693}, {"id": 193, "seek": 116308, "start": 1173.1999999999998, "end": 1178.36, "text": " you want to do all the processing on the visualization nodes. That's for not", "tokens": [291, 528, 281, 360, 439, 264, 9007, 322, 264, 25801, 13891, 13, 663, 311, 337, 406], "temperature": 0.0, "avg_logprob": -0.26081900544218967, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.00019923927902709693}, {"id": 194, "seek": 116308, "start": 1178.36, "end": 1181.76, "text": " possible just with the in situ implementation, but we have an in-transit", "tokens": [1944, 445, 365, 264, 294, 2054, 11420, 11, 457, 321, 362, 364, 294, 12, 24999, 270], "temperature": 0.0, "avg_logprob": -0.26081900544218967, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.00019923927902709693}, {"id": 195, "seek": 116308, "start": 1181.76, "end": 1187.8799999999999, "text": " implementation where the simulation can communicate with those different nodes.", "tokens": [11420, 689, 264, 16575, 393, 7890, 365, 729, 819, 13891, 13], "temperature": 0.0, "avg_logprob": -0.26081900544218967, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.00019923927902709693}, {"id": 196, "seek": 118788, "start": 1187.88, "end": 1194.72, "text": " And, and the analysis can happen on other ranks than the simulation. So, the", "tokens": [400, 11, 293, 264, 5215, 393, 1051, 322, 661, 21406, 813, 264, 16575, 13, 407, 11, 264], "temperature": 0.0, "avg_logprob": -0.24816370010375977, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.00045029009925201535}, {"id": 197, "seek": 118788, "start": 1194.72, "end": 1201.6000000000001, "text": " simulation can go forward directly. We use, we also use some new library,", "tokens": [16575, 393, 352, 2128, 3838, 13, 492, 764, 11, 321, 611, 764, 512, 777, 6405, 11], "temperature": 0.0, "avg_logprob": -0.24816370010375977, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.00045029009925201535}, {"id": 198, "seek": 118788, "start": 1201.6000000000001, "end": 1208.3200000000002, "text": " recently used a library called DIY. That's here to do some wrapper for us. It's, we", "tokens": [3938, 1143, 257, 6405, 1219, 22194, 13, 663, 311, 510, 281, 360, 512, 46906, 337, 505, 13, 467, 311, 11, 321], "temperature": 0.0, "avg_logprob": -0.24816370010375977, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.00045029009925201535}, {"id": 199, "seek": 118788, "start": 1208.3200000000002, "end": 1217.1200000000001, "text": " take it as a wrapper around the MPI. So, DIY, I love to do some to, to cut the", "tokens": [747, 309, 382, 257, 46906, 926, 264, 14146, 40, 13, 407, 11, 22194, 11, 286, 959, 281, 360, 512, 281, 11, 281, 1723, 264], "temperature": 0.0, "avg_logprob": -0.24816370010375977, "compression_ratio": 1.5728643216080402, "no_speech_prob": 0.00045029009925201535}, {"id": 200, "seek": 121712, "start": 1217.12, "end": 1223.56, "text": " data into different blocks. And then, the, at runtime DIY itself is a rare to do.", "tokens": [1412, 666, 819, 8474, 13, 400, 550, 11, 264, 11, 412, 34474, 22194, 2564, 307, 257, 5892, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.3061499494187375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003430137876421213}, {"id": 201, "seek": 121712, "start": 1223.56, "end": 1230.08, "text": " Okay, I should put three blocks on each rank. So, only one block. And, yeah, it's a,", "tokens": [1033, 11, 286, 820, 829, 1045, 8474, 322, 1184, 6181, 13, 407, 11, 787, 472, 3461, 13, 400, 11, 1338, 11, 309, 311, 257, 11], "temperature": 0.0, "avg_logprob": -0.3061499494187375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003430137876421213}, {"id": 202, "seek": 121712, "start": 1230.08, "end": 1238.1999999999998, "text": " yeah, just a new abstraction over cutting your, your data for distribution. We", "tokens": [1338, 11, 445, 257, 777, 37765, 670, 6492, 428, 11, 428, 1412, 337, 7316, 13, 492], "temperature": 0.0, "avg_logprob": -0.3061499494187375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003430137876421213}, {"id": 203, "seek": 121712, "start": 1238.1999999999998, "end": 1245.4399999999998, "text": " are also looking for better VTK on always, yeah, better VTK integrations to, to be", "tokens": [366, 611, 1237, 337, 1101, 691, 51, 42, 322, 1009, 11, 1338, 11, 1101, 691, 51, 42, 3572, 763, 281, 11, 281, 312], "temperature": 0.0, "avg_logprob": -0.3061499494187375, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.0003430137876421213}, {"id": 204, "seek": 124544, "start": 1245.44, "end": 1251.16, "text": " able to, to run on a lot of hardware. And something very cool that is very new. It's", "tokens": [1075, 281, 11, 281, 1190, 322, 257, 688, 295, 8837, 13, 400, 746, 588, 1627, 300, 307, 588, 777, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.22809242580247963, "compression_ratio": 1.6095617529880477, "no_speech_prob": 0.0004986713174730539}, {"id": 205, "seek": 124544, "start": 1251.16, "end": 1256.16, "text": " just in the development branch of VTK. So, absolutely not in Paraview yet. That was", "tokens": [445, 294, 264, 3250, 9819, 295, 691, 51, 42, 13, 407, 11, 3122, 406, 294, 3457, 706, 1093, 1939, 13, 663, 390], "temperature": 0.0, "avg_logprob": -0.22809242580247963, "compression_ratio": 1.6095617529880477, "no_speech_prob": 0.0004986713174730539}, {"id": 206, "seek": 124544, "start": 1256.16, "end": 1260.3600000000001, "text": " merged, I think, one or two weeks ago. It's what we call implicit arrays. And,", "tokens": [36427, 11, 286, 519, 11, 472, 420, 732, 3259, 2057, 13, 467, 311, 437, 321, 818, 26947, 41011, 13, 400, 11], "temperature": 0.0, "avg_logprob": -0.22809242580247963, "compression_ratio": 1.6095617529880477, "no_speech_prob": 0.0004986713174730539}, {"id": 207, "seek": 124544, "start": 1260.3600000000001, "end": 1267.2, "text": " basically, it's really cool for memory point of view because we, it's some kind", "tokens": [1936, 11, 309, 311, 534, 1627, 337, 4675, 935, 295, 1910, 570, 321, 11, 309, 311, 512, 733], "temperature": 0.0, "avg_logprob": -0.22809242580247963, "compression_ratio": 1.6095617529880477, "no_speech_prob": 0.0004986713174730539}, {"id": 208, "seek": 124544, "start": 1267.2, "end": 1273.56, "text": " of views on memory. For now, in the Paraview process, your data is really an", "tokens": [295, 6809, 322, 4675, 13, 1171, 586, 11, 294, 264, 3457, 706, 1093, 1399, 11, 428, 1412, 307, 534, 364], "temperature": 0.0, "avg_logprob": -0.22809242580247963, "compression_ratio": 1.6095617529880477, "no_speech_prob": 0.0004986713174730539}, {"id": 209, "seek": 127356, "start": 1273.56, "end": 1281.0, "text": " array in the, in the memory, in your memory. So, with the implicit array, we have", "tokens": [10225, 294, 264, 11, 294, 264, 4675, 11, 294, 428, 4675, 13, 407, 11, 365, 264, 26947, 10225, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.25129173942234206, "compression_ratio": 1.875, "no_speech_prob": 0.0002830294251907617}, {"id": 210, "seek": 127356, "start": 1281.0, "end": 1287.6799999999998, "text": " some views. So, you can implement an open, open pattern on it. For instance, when", "tokens": [512, 6809, 13, 407, 11, 291, 393, 4445, 364, 1269, 11, 1269, 5102, 322, 309, 13, 1171, 5197, 11, 562], "temperature": 0.0, "avg_logprob": -0.25129173942234206, "compression_ratio": 1.875, "no_speech_prob": 0.0002830294251907617}, {"id": 211, "seek": 127356, "start": 1287.6799999999998, "end": 1293.52, "text": " you do an isocontrol of your data, you know that the, the, that, the resulting", "tokens": [291, 360, 364, 307, 905, 896, 6623, 295, 428, 1412, 11, 291, 458, 300, 264, 11, 264, 11, 300, 11, 264, 16505], "temperature": 0.0, "avg_logprob": -0.25129173942234206, "compression_ratio": 1.875, "no_speech_prob": 0.0002830294251907617}, {"id": 212, "seek": 127356, "start": 1293.52, "end": 1298.8, "text": " data will all have the same values. So, if you want, if you, after the isocontrol,", "tokens": [1412, 486, 439, 362, 264, 912, 4190, 13, 407, 11, 498, 291, 528, 11, 498, 291, 11, 934, 264, 307, 905, 896, 6623, 11], "temperature": 0.0, "avg_logprob": -0.25129173942234206, "compression_ratio": 1.875, "no_speech_prob": 0.0002830294251907617}, {"id": 213, "seek": 127356, "start": 1298.8, "end": 1302.6, "text": " you still have one million points, you will know that all the points will share", "tokens": [291, 920, 362, 472, 2459, 2793, 11, 291, 486, 458, 300, 439, 264, 2793, 486, 2073], "temperature": 0.0, "avg_logprob": -0.25129173942234206, "compression_ratio": 1.875, "no_speech_prob": 0.0002830294251907617}, {"id": 214, "seek": 130260, "start": 1302.6, "end": 1308.28, "text": " the same value. For now, it's one million times duplicate in your memory. So, that's", "tokens": [264, 912, 2158, 13, 1171, 586, 11, 309, 311, 472, 2459, 1413, 23976, 294, 428, 4675, 13, 407, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.21603790026032524, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.00044110120506957173}, {"id": 215, "seek": 130260, "start": 1308.28, "end": 1312.1999999999998, "text": " a not-efficient. With implicit array, you can sort only one time the value and say,", "tokens": [257, 406, 12, 68, 7816, 13, 2022, 26947, 10225, 11, 291, 393, 1333, 787, 472, 565, 264, 2158, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.21603790026032524, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.00044110120506957173}, {"id": 216, "seek": 130260, "start": 1312.1999999999998, "end": 1317.9199999999998, "text": " okay, this should, this should be an array of size one million. And the value you", "tokens": [1392, 11, 341, 820, 11, 341, 820, 312, 364, 10225, 295, 2744, 472, 2459, 13, 400, 264, 2158, 291], "temperature": 0.0, "avg_logprob": -0.21603790026032524, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.00044110120506957173}, {"id": 217, "seek": 130260, "start": 1317.9199999999998, "end": 1324.7199999999998, "text": " should return is this one. But you can imagine as a, a compressed array in your", "tokens": [820, 2736, 307, 341, 472, 13, 583, 291, 393, 3811, 382, 257, 11, 257, 30353, 10225, 294, 428], "temperature": 0.0, "avg_logprob": -0.21603790026032524, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.00044110120506957173}, {"id": 218, "seek": 132472, "start": 1324.72, "end": 1332.84, "text": " memory and have an on-the-fly, uncompressed algorithm to when you just want, just", "tokens": [4675, 293, 362, 364, 322, 12, 3322, 12, 14061, 11, 8585, 79, 3805, 9284, 281, 562, 291, 445, 528, 11, 445], "temperature": 0.0, "avg_logprob": -0.174809445832905, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.00015520627493970096}, {"id": 219, "seek": 132472, "start": 1332.84, "end": 1336.88, "text": " when you want to, to read your data. So, it has a cost in terms of time of", "tokens": [562, 291, 528, 281, 11, 281, 1401, 428, 1412, 13, 407, 11, 309, 575, 257, 2063, 294, 2115, 295, 565, 295], "temperature": 0.0, "avg_logprob": -0.174809445832905, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.00015520627493970096}, {"id": 220, "seek": 132472, "start": 1336.88, "end": 1341.84, "text": " computations. But if you run out of memory with too huge data, that's, that can be", "tokens": [2807, 763, 13, 583, 498, 291, 1190, 484, 295, 4675, 365, 886, 2603, 1412, 11, 300, 311, 11, 300, 393, 312], "temperature": 0.0, "avg_logprob": -0.174809445832905, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.00015520627493970096}, {"id": 221, "seek": 132472, "start": 1341.84, "end": 1351.28, "text": " really great. Okay, I still can have a lot of things to, to say, but that's what", "tokens": [534, 869, 13, 1033, 11, 286, 920, 393, 362, 257, 688, 295, 721, 281, 11, 281, 584, 11, 457, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.174809445832905, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.00015520627493970096}, {"id": 222, "seek": 135128, "start": 1351.28, "end": 1355.68, "text": " is the, the end of what I, I put in the slides. So, thanks for attending these", "tokens": [307, 264, 11, 264, 917, 295, 437, 286, 11, 286, 829, 294, 264, 9788, 13, 407, 11, 3231, 337, 15862, 613], "temperature": 0.0, "avg_logprob": -0.26654716159986414, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.0006169803091324866}, {"id": 223, "seek": 135128, "start": 1355.68, "end": 1359.56, "text": " songs to be here early in the morning. And if you have any questions, I think it", "tokens": [5781, 281, 312, 510, 2440, 294, 264, 2446, 13, 400, 498, 291, 362, 604, 1651, 11, 286, 519, 309], "temperature": 0.0, "avg_logprob": -0.26654716159986414, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.0006169803091324866}, {"id": 224, "seek": 135128, "start": 1359.56, "end": 1364.2, "text": " will, it will be the time. I put just a lot of resources at the end of the", "tokens": [486, 11, 309, 486, 312, 264, 565, 13, 286, 829, 445, 257, 688, 295, 3593, 412, 264, 917, 295, 264], "temperature": 0.0, "avg_logprob": -0.26654716159986414, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.0006169803091324866}, {"id": 225, "seek": 135128, "start": 1364.2, "end": 1369.2, "text": " slides so you can get it from the website of the phone. Thank you.", "tokens": [9788, 370, 291, 393, 483, 309, 490, 264, 3144, 295, 264, 2593, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.26654716159986414, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.0006169803091324866}, {"id": 226, "seek": 135128, "start": 1369.2, "end": 1375.2, "text": " Thanks, everyone.", "tokens": [2561, 11, 1518, 13], "temperature": 0.0, "avg_logprob": -0.26654716159986414, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.0006169803091324866}, {"id": 227, "seek": 137520, "start": 1375.2, "end": 1385.64, "text": " Thank you very much, Nikol. Do we have any questions? Thank you. In our group, we", "tokens": [1044, 291, 588, 709, 11, 13969, 401, 13, 1144, 321, 362, 604, 1651, 30, 1044, 291, 13, 682, 527, 1594, 11, 321], "temperature": 0.0, "avg_logprob": -0.21398092416616588, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.005401130765676498}, {"id": 228, "seek": 137520, "start": 1385.64, "end": 1391.76, "text": " are happy users of ParaView. One thing that maybe I could add to a wishlist or", "tokens": [366, 2055, 5022, 295, 11107, 30203, 13, 1485, 551, 300, 1310, 286, 727, 909, 281, 257, 3172, 8264, 420], "temperature": 0.0, "avg_logprob": -0.21398092416616588, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.005401130765676498}, {"id": 229, "seek": 137520, "start": 1391.76, "end": 1402.0, "text": " some, well, maybe just for discussion is that we have quite some headache when", "tokens": [512, 11, 731, 11, 1310, 445, 337, 5017, 307, 300, 321, 362, 1596, 512, 23520, 562], "temperature": 0.0, "avg_logprob": -0.21398092416616588, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.005401130765676498}, {"id": 230, "seek": 140200, "start": 1402.0, "end": 1408.0, "text": " using ParaView on GitHub Actions for multiple platforms. So, like to set up", "tokens": [1228, 11107, 30203, 322, 23331, 3251, 626, 337, 3866, 9473, 13, 407, 11, 411, 281, 992, 493], "temperature": 0.0, "avg_logprob": -0.18817854900749362, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0024823129642754793}, {"id": 231, "seek": 140200, "start": 1408.0, "end": 1411.76, "text": " environments for Linux, Mac and Windows with the same version of ParaView", "tokens": [12388, 337, 18734, 11, 5707, 293, 8591, 365, 264, 912, 3037, 295, 11107, 30203], "temperature": 0.0, "avg_logprob": -0.18817854900749362, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0024823129642754793}, {"id": 232, "seek": 140200, "start": 1411.76, "end": 1419.28, "text": " coupling with Python just, just to be ready to use it. It's a bit of a headache,", "tokens": [37447, 365, 15329, 445, 11, 445, 281, 312, 1919, 281, 764, 309, 13, 467, 311, 257, 857, 295, 257, 23520, 11], "temperature": 0.0, "avg_logprob": -0.18817854900749362, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0024823129642754793}, {"id": 233, "seek": 140200, "start": 1419.28, "end": 1424.12, "text": " especially when you go to Windows and you need to download things, brew things, up", "tokens": [2318, 562, 291, 352, 281, 8591, 293, 291, 643, 281, 5484, 721, 11, 34619, 721, 11, 493], "temperature": 0.0, "avg_logprob": -0.18817854900749362, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0024823129642754793}, {"id": 234, "seek": 140200, "start": 1424.12, "end": 1428.84, "text": " to get install things and then they not necessarily work all together. So, wishlist", "tokens": [281, 483, 3625, 721, 293, 550, 436, 406, 4725, 589, 439, 1214, 13, 407, 11, 3172, 8264], "temperature": 0.0, "avg_logprob": -0.18817854900749362, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.0024823129642754793}, {"id": 235, "seek": 142884, "start": 1428.84, "end": 1434.9599999999998, "text": " thing, GitHub Actions, ParaView, set up a thing. Unless it doesn't, it exists", "tokens": [551, 11, 23331, 3251, 626, 11, 11107, 30203, 11, 992, 493, 257, 551, 13, 16581, 309, 1177, 380, 11, 309, 8198], "temperature": 0.0, "avg_logprob": -0.3586006164550781, "compression_ratio": 1.4830917874396135, "no_speech_prob": 0.0021160528995096684}, {"id": 236, "seek": 142884, "start": 1434.9599999999998, "end": 1442.8799999999999, "text": " already but I haven't found it. And the truth is, if there are questions here?", "tokens": [1217, 457, 286, 2378, 380, 1352, 309, 13, 400, 264, 3494, 307, 11, 498, 456, 366, 1651, 510, 30], "temperature": 0.0, "avg_logprob": -0.3586006164550781, "compression_ratio": 1.4830917874396135, "no_speech_prob": 0.0021160528995096684}, {"id": 237, "seek": 142884, "start": 1442.8799999999999, "end": 1447.56, "text": " The use of ParaView and GitHub Actions, so in like a continuous integration,", "tokens": [440, 764, 295, 11107, 30203, 293, 23331, 3251, 626, 11, 370, 294, 411, 257, 10957, 10980, 11], "temperature": 0.0, "avg_logprob": -0.3586006164550781, "compression_ratio": 1.4830917874396135, "no_speech_prob": 0.0021160528995096684}, {"id": 238, "seek": 142884, "start": 1447.56, "end": 1453.48, "text": " limited environment, I guess? Yeah, it's a wishlist. Yeah, well, we don't", "tokens": [5567, 2823, 11, 286, 2041, 30, 865, 11, 309, 311, 257, 3172, 8264, 13, 865, 11, 731, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.3586006164550781, "compression_ratio": 1.4830917874396135, "no_speech_prob": 0.0021160528995096684}, {"id": 239, "seek": 145348, "start": 1453.48, "end": 1460.4, "text": " choose GitHub directly. We have a, we have a, the GitLab where you can find a lot", "tokens": [2826, 23331, 3838, 13, 492, 362, 257, 11, 321, 362, 257, 11, 264, 16939, 37880, 689, 291, 393, 915, 257, 688], "temperature": 0.0, "avg_logprob": -0.37665168526246373, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0005331241409294307}, {"id": 240, "seek": 145348, "start": 1460.4, "end": 1465.96, "text": " of stuff with our CI and CDO. We produce, we produce nightly releases of", "tokens": [295, 1507, 365, 527, 37777, 293, 6743, 46, 13, 492, 5258, 11, 321, 5258, 1818, 356, 16952, 295], "temperature": 0.0, "avg_logprob": -0.37665168526246373, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0005331241409294307}, {"id": 241, "seek": 145348, "start": 1465.96, "end": 1475.76, "text": " ParaView through the GitLab. So, I don't know if I, if it's some sort of part of the question, but.", "tokens": [11107, 30203, 807, 264, 16939, 37880, 13, 407, 11, 286, 500, 380, 458, 498, 286, 11, 498, 309, 311, 512, 1333, 295, 644, 295, 264, 1168, 11, 457, 13], "temperature": 0.0, "avg_logprob": -0.37665168526246373, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0005331241409294307}, {"id": 242, "seek": 145348, "start": 1475.76, "end": 1479.96, "text": " So, what kind of stuff are you doing with ParaView and GitHub Actions? Is it", "tokens": [407, 11, 437, 733, 295, 1507, 366, 291, 884, 365, 11107, 30203, 293, 23331, 3251, 626, 30, 1119, 309], "temperature": 0.0, "avg_logprob": -0.37665168526246373, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0005331241409294307}, {"id": 243, "seek": 147996, "start": 1479.96, "end": 1485.64, "text": " rendering or, rendering with Python? The fact is, I don't really know about GitHub", "tokens": [22407, 420, 11, 22407, 365, 15329, 30, 440, 1186, 307, 11, 286, 500, 380, 534, 458, 466, 23331], "temperature": 0.0, "avg_logprob": -0.2512838170769509, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0013511693105101585}, {"id": 244, "seek": 147996, "start": 1485.64, "end": 1489.96, "text": " Actions because I don't choose GitHub anymore. So, I don't see what you can do", "tokens": [3251, 626, 570, 286, 500, 380, 2826, 23331, 3602, 13, 407, 11, 286, 500, 380, 536, 437, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.2512838170769509, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0013511693105101585}, {"id": 245, "seek": 147996, "start": 1489.96, "end": 1499.0, "text": " with that, that you should not able to do otherwise. Any other questions?", "tokens": [365, 300, 11, 300, 291, 820, 406, 1075, 281, 360, 5911, 13, 2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2512838170769509, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0013511693105101585}, {"id": 246, "seek": 147996, "start": 1499.0, "end": 1507.44, "text": " There's a question on the chat. Okay. Yeah, there's a question on the chat. If I want to put", "tokens": [821, 311, 257, 1168, 322, 264, 5081, 13, 1033, 13, 865, 11, 456, 311, 257, 1168, 322, 264, 5081, 13, 759, 286, 528, 281, 829], "temperature": 0.0, "avg_logprob": -0.2512838170769509, "compression_ratio": 1.6482412060301508, "no_speech_prob": 0.0013511693105101585}, {"id": 247, "seek": 150744, "start": 1507.44, "end": 1512.64, "text": " Catalyst in my simulation, what is the first step? Oh, sorry. If you want to use", "tokens": [9565, 19530, 294, 452, 16575, 11, 437, 307, 264, 700, 1823, 30, 876, 11, 2597, 13, 759, 291, 528, 281, 764], "temperature": 0.0, "avg_logprob": -0.23779840289421803, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006964883068576455}, {"id": 248, "seek": 150744, "start": 1512.64, "end": 1517.72, "text": " Catalyst. In your, yeah. What's the first step? We have some, I think we have some", "tokens": [9565, 19530, 13, 682, 428, 11, 1338, 13, 708, 311, 264, 700, 1823, 30, 492, 362, 512, 11, 286, 519, 321, 362, 512], "temperature": 0.0, "avg_logprob": -0.23779840289421803, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006964883068576455}, {"id": 249, "seek": 150744, "start": 1517.72, "end": 1522.16, "text": " tutorials on example in the code base of ParaView. We have some examples where", "tokens": [17616, 322, 1365, 294, 264, 3089, 3096, 295, 11107, 30203, 13, 492, 362, 512, 5110, 689], "temperature": 0.0, "avg_logprob": -0.23779840289421803, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006964883068576455}, {"id": 250, "seek": 150744, "start": 1522.16, "end": 1527.44, "text": " there are some dummy simulations with just a main, so you can", "tokens": [456, 366, 512, 35064, 35138, 365, 445, 257, 2135, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.23779840289421803, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006964883068576455}, {"id": 251, "seek": 150744, "start": 1527.44, "end": 1534.64, "text": " enter from it to, to see how it is organized. And, and yeah, the first, one", "tokens": [3242, 490, 309, 281, 11, 281, 536, 577, 309, 307, 9983, 13, 400, 11, 293, 1338, 11, 264, 700, 11, 472], "temperature": 0.0, "avg_logprob": -0.23779840289421803, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006964883068576455}, {"id": 252, "seek": 153464, "start": 1534.64, "end": 1541.1200000000001, "text": " first thing is to be able to know what do you want, which data do you want to, to", "tokens": [700, 551, 307, 281, 312, 1075, 281, 458, 437, 360, 291, 528, 11, 597, 1412, 360, 291, 528, 281, 11, 281], "temperature": 0.0, "avg_logprob": -0.20443324609236282, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.00047311914386227727}, {"id": 253, "seek": 153464, "start": 1541.1200000000001, "end": 1547.16, "text": " send through, through Catalyst and where you can access it in your code. And then,", "tokens": [2845, 807, 11, 807, 9565, 19530, 293, 689, 291, 393, 2105, 309, 294, 428, 3089, 13, 400, 550, 11], "temperature": 0.0, "avg_logprob": -0.20443324609236282, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.00047311914386227727}, {"id": 254, "seek": 153464, "start": 1547.16, "end": 1555.0, "text": " it's, and then so you, at this time you, you have located the entry points from", "tokens": [309, 311, 11, 293, 550, 370, 291, 11, 412, 341, 565, 291, 11, 291, 362, 6870, 264, 8729, 2793, 490], "temperature": 0.0, "avg_logprob": -0.20443324609236282, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.00047311914386227727}, {"id": 255, "seek": 153464, "start": 1555.0, "end": 1560.5600000000002, "text": " your simulation code and then you will be able to, to start writing the, the small", "tokens": [428, 16575, 3089, 293, 550, 291, 486, 312, 1075, 281, 11, 281, 722, 3579, 264, 11, 264, 1359], "temperature": 0.0, "avg_logprob": -0.20443324609236282, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.00047311914386227727}, {"id": 256, "seek": 156056, "start": 1560.56, "end": 1567.52, "text": " wrapper you need to wrap your data on the need to the actual API of ParaView.", "tokens": [46906, 291, 643, 281, 7019, 428, 1412, 322, 264, 643, 281, 264, 3539, 9362, 295, 11107, 30203, 13], "temperature": 0.0, "avg_logprob": -0.2228276288067853, "compression_ratio": 1.596938775510204, "no_speech_prob": 0.0007741081644780934}, {"id": 257, "seek": 156056, "start": 1567.52, "end": 1575.9199999999998, "text": " Thanks. Okay. Any other burning questions? Maybe one last, yeah. Last question?", "tokens": [2561, 13, 1033, 13, 2639, 661, 9488, 1651, 30, 2704, 472, 1036, 11, 1338, 13, 5264, 1168, 30], "temperature": 0.0, "avg_logprob": -0.2228276288067853, "compression_ratio": 1.596938775510204, "no_speech_prob": 0.0007741081644780934}, {"id": 258, "seek": 156056, "start": 1576.44, "end": 1585.6399999999999, "text": " Thank you for the talk. A very naive question, because I, is it working? A very", "tokens": [1044, 291, 337, 264, 751, 13, 316, 588, 29052, 1168, 11, 570, 286, 11, 307, 309, 1364, 30, 316, 588], "temperature": 0.0, "avg_logprob": -0.2228276288067853, "compression_ratio": 1.596938775510204, "no_speech_prob": 0.0007741081644780934}, {"id": 259, "seek": 156056, "start": 1585.6399999999999, "end": 1590.04, "text": " naive question because I know almost nothing about, about ParaView. You had", "tokens": [29052, 1168, 570, 286, 458, 1920, 1825, 466, 11, 466, 11107, 30203, 13, 509, 632], "temperature": 0.0, "avg_logprob": -0.2228276288067853, "compression_ratio": 1.596938775510204, "no_speech_prob": 0.0007741081644780934}, {"id": 260, "seek": 159004, "start": 1590.04, "end": 1592.1599999999999, "text": " many components there. One of them was the client that does the", "tokens": [867, 6677, 456, 13, 1485, 295, 552, 390, 264, 6423, 300, 775, 264], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 261, "seek": 159004, "start": 1592.1599999999999, "end": 1597.76, "text": " visualizations. Yeah. Is it, would it be possible at some point in the future to", "tokens": [5056, 14455, 13, 865, 13, 1119, 309, 11, 576, 309, 312, 1944, 412, 512, 935, 294, 264, 2027, 281], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 262, "seek": 159004, "start": 1597.76, "end": 1602.0, "text": " be like a web client where you just log into the website and it just displays", "tokens": [312, 411, 257, 3670, 6423, 689, 291, 445, 3565, 666, 264, 3144, 293, 309, 445, 20119], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 263, "seek": 159004, "start": 1602.0, "end": 1606.84, "text": " everything? Or is it just, due to the architecture is it like super complicated", "tokens": [1203, 30, 1610, 307, 309, 445, 11, 3462, 281, 264, 9482, 307, 309, 411, 1687, 6179], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 264, "seek": 159004, "start": 1606.84, "end": 1615.1599999999999, "text": " to do it that way? We, so the question is, yeah, the question is, are we able to use", "tokens": [281, 360, 309, 300, 636, 30, 492, 11, 370, 264, 1168, 307, 11, 1338, 11, 264, 1168, 307, 11, 366, 321, 1075, 281, 764], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 265, "seek": 159004, "start": 1615.1599999999999, "end": 1620.0, "text": " a web client for ParaView? Just, just for the part that does the visual", "tokens": [257, 3670, 6423, 337, 11107, 30203, 30, 1449, 11, 445, 337, 264, 644, 300, 775, 264, 5056], "temperature": 0.0, "avg_logprob": -0.19228561786042542, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0009876347612589598}, {"id": 266, "seek": 162000, "start": 1620.0, "end": 1624.92, "text": "ization, if that could be like a, we are, we have some web client for ParaView", "tokens": [2144, 11, 498, 300, 727, 312, 411, 257, 11, 321, 366, 11, 321, 362, 512, 3670, 6423, 337, 11107, 30203], "temperature": 0.0, "avg_logprob": -0.2519177672683552, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0004732934175990522}, {"id": 267, "seek": 162000, "start": 1624.92, "end": 1631.56, "text": " already. So we have a framework called Trame, T-R-R-M-E, that's intended to, to", "tokens": [1217, 13, 407, 321, 362, 257, 8388, 1219, 1765, 529, 11, 314, 12, 49, 12, 49, 12, 44, 12, 36, 11, 300, 311, 10226, 281, 11, 281], "temperature": 0.0, "avg_logprob": -0.2519177672683552, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0004732934175990522}, {"id": 268, "seek": 162000, "start": 1631.56, "end": 1636.92, "text": " connect to a ParaView server. And then you build your own front end for", "tokens": [1745, 281, 257, 11107, 30203, 7154, 13, 400, 550, 291, 1322, 428, 1065, 1868, 917, 337], "temperature": 0.0, "avg_logprob": -0.2519177672683552, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0004732934175990522}, {"id": 269, "seek": 162000, "start": 1636.92, "end": 1643.72, "text": " these applications. So basically it's, we don't have a, yeah, you should build your", "tokens": [613, 5821, 13, 407, 1936, 309, 311, 11, 321, 500, 380, 362, 257, 11, 1338, 11, 291, 820, 1322, 428], "temperature": 0.0, "avg_logprob": -0.2519177672683552, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0004732934175990522}, {"id": 270, "seek": 164372, "start": 1643.72, "end": 1650.76, "text": " own. But it can be, okay, I have a server on, I open the, always this data and the", "tokens": [1065, 13, 583, 309, 393, 312, 11, 1392, 11, 286, 362, 257, 7154, 322, 11, 286, 1269, 264, 11, 1009, 341, 1412, 293, 264], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 271, "seek": 164372, "start": 1650.76, "end": 1655.8, "text": " front end is just a 3D round of view. That's already possible quite easily, I", "tokens": [1868, 917, 307, 445, 257, 805, 35, 3098, 295, 1910, 13, 663, 311, 1217, 1944, 1596, 3612, 11, 286], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 272, "seek": 164372, "start": 1655.8, "end": 1660.68, "text": " think, with the Trame framework. And Jupyter Notebooks also, right?", "tokens": [519, 11, 365, 264, 1765, 529, 8388, 13, 400, 22125, 88, 391, 11633, 15170, 611, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 273, "seek": 164372, "start": 1660.68, "end": 1663.2, "text": " Jupyter Notebooks, I think I saw it on the user interface line.", "tokens": [22125, 88, 391, 11633, 15170, 11, 286, 519, 286, 1866, 309, 322, 264, 4195, 9226, 1622, 13], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 274, "seek": 164372, "start": 1663.2, "end": 1669.2, "text": " Yeah. Well, we are, yeah, as we use intensively Python, we also make the", "tokens": [865, 13, 1042, 11, 321, 366, 11, 1338, 11, 382, 321, 764, 18957, 356, 15329, 11, 321, 611, 652, 264], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 275, "seek": 164372, "start": 1669.2, "end": 1673.1200000000001, "text": " step to, to be supported from a Jupyter Notebook and we also have a plugin that", "tokens": [1823, 281, 11, 281, 312, 8104, 490, 257, 22125, 88, 391, 11633, 2939, 293, 321, 611, 362, 257, 23407, 300], "temperature": 0.0, "avg_logprob": -0.2545202717636571, "compression_ratio": 1.6123188405797102, "no_speech_prob": 0.0014227316714823246}, {"id": 276, "seek": 167312, "start": 1673.12, "end": 1678.6, "text": " allows you to control a ParaView GUI. So you can do some stuff in the Notebook.", "tokens": [4045, 291, 281, 1969, 257, 11107, 30203, 17917, 40, 13, 407, 291, 393, 360, 512, 1507, 294, 264, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 277, "seek": 167312, "start": 1678.6, "end": 1682.4799999999998, "text": " And if something goes wrong or you don't understand, you can launch a magic", "tokens": [400, 498, 746, 1709, 2085, 420, 291, 500, 380, 1223, 11, 291, 393, 4025, 257, 5585], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 278, "seek": 167312, "start": 1682.4799999999998, "end": 1685.9599999999998, "text": " command run ParaView that's open the ParaView client with all your Python,", "tokens": [5622, 1190, 11107, 30203, 300, 311, 1269, 264, 11107, 30203, 6423, 365, 439, 428, 15329, 11], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 279, "seek": 167312, "start": 1685.9599999999998, "end": 1690.52, "text": " Python, and you can introspect in the GUI. And then you can go back to your, to", "tokens": [15329, 11, 293, 291, 393, 560, 28713, 294, 264, 17917, 40, 13, 400, 550, 291, 393, 352, 646, 281, 428, 11, 281], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 280, "seek": 167312, "start": 1690.52, "end": 1694.6, "text": " your Notebook. Okay. Thank you very much, Nicholas.", "tokens": [428, 11633, 2939, 13, 1033, 13, 1044, 291, 588, 709, 11, 22924, 13], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 281, "seek": 167312, "start": 1694.6, "end": 1695.6, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.26537434871380144, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00030649174004793167}, {"id": 282, "seek": 169560, "start": 1695.6, "end": 1702.6, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50714], "temperature": 0.0, "avg_logprob": -0.6200001835823059, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.0014456978533416986}], "language": "en"}