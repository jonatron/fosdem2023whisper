{"text": " Okay. Hello, everyone. Welcome to the talk on open telemetry with Grafana. Microphone broke, so I need to do it with this microphone now. Let's see how it goes with typing and live demo. Few words about me, so who am I, why am I here talking about Grafana and open telemetry, so I work at Grafana Labs, I'm an engineering manager, I'm also a manager for our open telemetry squad, and I'm also active in open source, so I'm a member of the Prometheus team where I maintain the Java metrics library. So what are we going to do in this talk in the next 25 minutes or so, so it will almost exclusively be a live demo, so basically the idea is I have a little example application running on my laptop, and it is instrumented with open telemetry, I will show you in a minute what it does and how I instrumented it, and I also have an open source monitoring back and running, right, it consists of three databases, one is Loki, which is a open source logs database, one is Temple, which is an open source trace database, and one is Mimir, which is an open source metrics database, so Mimir is compatible with Prometheus, so I could have shown the exact same demo using Prometheus instead of Mimir, so it doesn't really matter for now. And of course I also have Grafana, I have those databases configured as data sources, and what we are going to do, we are going to start up Grafana, you know, have a look at metrics, have a look at traces, have a look at logs, and basically the idea is that at the end of the talk you kind of have seen all the signals that come out of open telemetry, you know, explore a bit what you can do with this type of data, and so you should have a good overview how open source monitoring with open telemetry looks like, right? So last slide before we jump into the live demo, so this is just a quick overview of what the example application does so that you know what we are going to look at. It's a simple hello world rest service written in Java using Spring Boot, and so basically you can send a request to port 8080 and it will respond with hello world, and in order to make it a bit more interesting, I made it a distributed hello world service, so it doesn't respond directly, but when it receives a request, it reaches out to a greeting service running on port 8081, the greeting service responds with the greeting, which is hello world, and then the response is forwarded to the client, right? And there are random errors to have some error rates as well, so basically a hello world microservice architecture or whatever, right? And in order to instrument this with open telemetry, I use the Java instrumentation agent that's provided by the open telemetry community, that's something you can download on GitHub, and the thing is this thing, you basically attach it to the Java virtual machine at start up time with a special command line parameter, so I didn't modify any source code, I didn't use any SDK or introduce any custom stuff, all we are going to look at in this demo is just data produced by attaching the open telemetry instrumentation to a standard Spring Boot application, right? Cool. So let's get started. As said, I have my data sources configured here, so Prometheus and Mimera are compatible, so it doesn't really matter which one we choose. There are a lot of, so I want to start with metrics, and yeah, so... Yeah? Can we turn the lights down a bit? I don't know. Okay. Maybe the other way around. Okay. I will just continue, come on. So there are lots of metrics that you get from the open telemetry instrumentation, so kind of JVM-related stuff like garbage collection activity and so forth, but the one I want to look at, oh, no, it's getting brighter and brighter. Yeah. Okay. Great. I think there is also a light mode in Grafana. Maybe that would have been a better choice. But no, I'm not going to use light mode. So let's figure out how to do the demo while I have a microphone that I should hold in my hands. Let's just put it here. Okay. Thank you. Cool. So the metric that we are going to look at for the demo, it's a metric named HTTP server duration. This is a metric of type histogram. So histograms have a couple of different numbers attached to them, so there are histogram buckets with the distribution data and so forth, and there's also a count. The count is the most simple one, so we are going to use this in our example. I actually got it two times. I got it once for my greeting service here and once for the hello world application. And if we are just, you know, running this query, maybe take a little bit of a shorter time window here, then we basically see two request counters, right? One is the green line, which is counting the request resulting in HTTP such as 200. So the successful requests, and basically we see that since I started the application on my laptop, I got about a little more than 400 successful requests, and the yellow line is, you know, requests resulting in HTTP status 500, and we got around 50 of them, right? And obviously, raw counter values are not very useful, right? Nobody is interested in how often was my service called since I started the application, and the way, you know, metric monitoring works with Prometheus, as probably most of you know, is that you use the Prometheus query language to get some useful information out of that kind of data, right? And I guess most of you have run some Prometheus queries, but they're still going to show maybe a couple of examples. So for those of you who are not very familiar with that, does this one work again? Hey, nice. It's even better. The lights work, the microphone works. Wow. Now let's hope the demo works. So I'm going to run just a couple of quick, you know, Prometheus queries so that for those of you who are not very familiar with it, so that you get an idea of what it is, right? And the most important function in the Prometheus query language is called the rate function. And what the rate function does, it takes a counter like this and a time interval like five minutes, and then it calculates a per second rate, right? So based on a five minute time interval, we now see that we have about 0.6 requests per second resulting in HTTP status 200, and we have about 0.1 requests per second resulting in HTTP status 500. And this is already quite some useful information, right? So typically you want to know the total load on your system, not buy status code or something. So you basically want to sum these two values up, and obviously there's also a sum function to sum values up, and if you call that, you get the total load on your system, which is just one line now and it's just, you know, around 0.7 requests per second, right? And this is, yeah, this is basically how Prometheus queries work. If you're not familiar with the syntax, there's also kind of a graphical query builder where you can, you know, use a bit drag and drop and get a bit more help and so forth, right? And so eventually, you know, when you got your queries and got your metrics, so what you want to do is you create a metrics dashboard and for monitoring HTTP services, there is, there are a couple of best practices, what type of data you want to visualize on a dashboard for monitoring HTTP services. And the most simple and straightforward thing is to visualize three things. One is the request rate, so for the current load on the system, which is exactly the query that we are seeing here. The next thing you want to see is the error rate, so the percentage of calls that fail. And the third thing is duration. How long does it take, right? And I created a simple example dashboard just to show you how this looks like. So I put the name of the service as a parameter up here so we can reuse the same dashboard for both services. Maybe let's use a 15 minute time window, so here I started the application. The first is the request rate, that's the exact same query that we just saw. Second thing here is the error rate, so we have about, I don't know, around 10% errors in my example application. And then for duration, there are a couple of different ways how to visualize that. So what we see here is basically the raw histogram, right? The histogram buckets. And this representation is actually quite useful because it shows you the shape of the distribution. So what we see here is two spikes, one around 600 milliseconds and one around 1.8 seconds. And this is a typical shape that you would see if your application uses a cache, right? Because then you have a couple of requests that are responded quite quickly. Those are the cache hits. A couple of requests are slow that are the cache misses. And visualizing the shape of the histogram helps you understand kind of the latency behavior of your application, right? The other and most popular way to visualize durations is this one here. These are percentiles. So the green line is the 95th percentile, so it tells us 95% of the calls have been faster than 1.7 seconds and 5% slower than that. The yellow line is the 50th, so half of the calls faster than that, half of the calls slower than that. And this doesn't really tell you the shape of the distribution, but it shows you a development over time, which is useful as well. So if your service becomes slower, those lines will go up, right? And it's also a good indicator if you want to do alerting and so forth. You can define a threshold and say it's above, if it's above a certain threshold, I want to be notified and stuff like that. And there are other more, you know, experimental things like this heat map showing basically development of histograms over time and stuff like that. So it's pretty cool to play with all the different visualizations in Grafana and, you know, see what you can get. So this is a, you know, quick example of a so-called red dashboard, Request Rates, Error Rates duration based on open telemetry data. And the cool thing about this, about it, is that it actually, all that we are seeing here is just based on that single histogram metric HDP server duration. And the fact that this metric is there is not a coincidence. The metric HDP server duration is actually defined in the open telemetry standard as part of the semantic conventions for HDP services. So whenever you monitor an HDP server with open telemetry, then you will find a histogram named HDP server duration. It will have the HDP status as an attribute. It will contain the latencies in milliseconds. That's all part of the standard. So it doesn't matter what programming language your services uses, what framework, whatever. If it's being monitored with open telemetry and it's compatible, you will find that metric and you can create a similar dashboard like that. And this is kind of one of the things that make application monitoring with open telemetry a lot easier than it used to be before these standardization. Cool. So that was a quick look at metrics, but of course we want to look at the other signals as well. So let's switch data sources for now and have a look at traces. So tracing, again, there's a kind of search, like graphical search where you can create your search criteria with drag and drop. There's a relatively new feature which is a query language for traces. So I'm going to use that for now. And one thing you can do is to just search by labels. So I can, for example, say I'm interested in the service name greeting service and then I could basically just open a random trace here. Let's take this as an example. Can I, I need to zoom out a little bit to be able to close the search window here. Okay. So this is how a distributed trace looks like. And if you see it for the first time, it might be a bit hard to understand, but it's actually fairly easy. So you just need like two minutes of introduction and then you will understand traces forever. And to give you that introduction, I actually have one more slide. So just to help you understand what we are seeing here. And the thing is distributed traces consist of spans, right? And spans are time spans. So a span is something that has a point in time where it starts and a point in time where it ends, right? And in open telemetry, there are three different kinds of spans. One are server spans. The second is internal spans and the third is client spans. Okay. So what happens when my Hello World application receives a request? So the first thing that happens if a server receives a request, a server span is created. So that's the first line here. It's started as soon as the request is received. It remains open until the request is responded, right? Then I said in the introduction that I used spring boot for implementing the example application. And the way spring boot works is that it takes the request and passes it to the corresponding spring controller that would handle the request. And open telemetries Java instrumentation agent is nice for Java developers because it just creates internal spans for each spring controller that is involved, right? And that is the second line that we are seeing here. It's basically opened as soon as the spring controller takes over and remains open until the spring controller is done handling the request, which might seem not too useful if I have just a single spring controller anyway, but if you have kind of a larger, you know, application and if you have multiple controllers involved, it gives you quite some interesting insights into what's happening inside your application. Like you would see immediately, like which controller do I spend most time in and so forth, right? And then eventually my Hello Word application reaches out to the greeting service and outgoing requests are represented by client spans. So the client span is basically opened as soon as my HTTP request goes out and remains open until the response is received. And then in the greeting service, the same thing starts again, you know, request is received, which creates a server span and then I have a spring controller as well, which is an internal span and that's the end of my distributed application here. And this is exactly what we are seeing here. And each of those span types has a corresponding metadata attached to it. So if you look at one of the internal spans here, we see the name of the spring controller and the name of the controller method and a couple of JVM-related attributes, whatever. And if we look at an HTTP span, for example, we see, of course, HTTP attributes like the status code, method and so forth, right? So of course, you do not want to just look at random spans. So usually you're looking for something. There are standard attributes in open telemetry that you can use for searching. So we already had the service name greeting service, for example. But the most important or one of the most important attributes is HTTP.status, no,.status code, this one here. And if we, for example, search for spans with HTTP status code 500, then we should find an example of a request that failed. So let's close the search window again. Yes, that's an example of a failed request. You see it with the indicated by those red exclamation marks at the bottom here. So this is where the thing failed, right? So the root cause of the error is the internal span, something in my spring controller in the greeting service. If I look at the metadata attached to that, I actually see that the instrumentation attached the event that caused the error, and this even includes the stack trace. So you can basically immediately navigate to the exact line of code that is the root cause of this error, right? And this is quite cool. So if you have a distributed application and you get an unexpected response from your Hello World application, without distributed tracing, it's pretty hard to find that actually there's an exception in the greeting service that, you know, propagated through your distributed landscape and then eventually caused the unexpected response. And with distributed tracing, finding these kind of things becomes pretty easy because you get all the related calls grouped together, you get the failed ones marked with an exclamation mark, and you can pretty easily navigate to what's the root cause of your error, okay? Cool. So that was a quick look at traces. There are a lot of interesting things about tracing. Maybe one thing I would like to show you, because I find it particularly cool, so if you have all your services instrumented with tracing in your back end, then basically those traces give you metadata about all the network calls happening in your system, and you can do something with that type of data, right? So for example, you can calculate something that we call the service graph. So it looks like this. It's maybe not too impressive if you just have two services calling each other, right? So, but if you imagine, you know, a more larger, you know, dozens or hundreds of services, so it will generate a map of all the services and indicate which service calls which other service, and this is quite useful. For example, if you intend to deploy a breaking change in your greeting service and you want to know who's using the greeting service, what would I break? Then looking at the service graph, you basically get this information right away. Traditionally, if you don't have that, you basically have a PDF with your architecture diagram, and then you look it up there, and also traditionally, there's at least one team that deployed something and forgot to update the diagram, and then you missed that, and there's a service graph that won't happen, right? This is the actual truth. This is based on what's actually happening in your backend, and this is pretty useful in these situations, right? And you can do other things as well, like, you know, have some statistics like the most frequently called endpoint or the endpoint with the most errors and stuff like that. So, that was a quick, quick look at traces. So we covered metrics, we covered traces. One thing I want to show you is that metrics and traces are actually related to each other, right? And so in order to show that, I'm going to go back to our dashboard, because if you, let's take a 15 minute window, then we get a bit more examples. So if you look at the latency data here, you notice these little green dots. These are called exemplars, and this is something that's provided by the auto instrumentation of open telemetry. So whenever it generates latency data, it basically attaches trace IDs of example traces to the latency data, and this is visualized by these little green dots, right? And so you see some examples of particularly fast calls, some examples of particularly slow calls and so forth. And if you, for example, take this dot up here, which is kind of slower than anything else, it's almost two seconds, right? Then you have the trace ID here, and you can navigate to tempo and have a look at the trace and start figuring out why did I have an example of such a slow call in my system, right? And in that case, you would immediately see that most of the time spent in the greeting service. So if you're looking for the performance bottleneck, then this is the most likely thing. Yeah, four minutes, that's fine. Cool. So if I have four minutes, it's high time to jump to logs, the third signal that we didn't look at yet. So let's select Loki, our open source logs database as a data source. So again, there's a query language, there's a graphical query builder and so forth. So let's just open random logs coming from the greeting service. It looks a bit like this. So it's even, I don't know, I didn't even log anything explicitly. I just turned on some whatever spring request logging so that I get some log data. And from time to time, I throw an exception, which is an IO exception to simulate these errors. Looks a bit broken, but that's just because of the resolution that I have here. Yeah, so what you can do, of course, you can do some full text search, for example, can say, I'm interested in these IO exception. And then you would basically get, well, if you spell it correctly, like that, then you would get the list of all IO exceptions, which in my case are just the random errors I'm throwing here. And this query language is actually quite powerful. So you can, this is kind of filtering by a label and filtering by full text search, but you can do totally different things as well. For example, you can have queries that, you know, derive metrics based on log data. There's a function pretty similar to what we have seen in the metrics demo, which is called the rate function. So the rate function, again, takes a time interval and then calculates the per second increase rate. So it basically tells you that we have almost 0.1 of these IO exceptions per second in our log data, which is also kind of useful for information to have. And the last thing to show you, because that's particularly interesting, so it is that these logs and traces and metrics are, again, not independent of each other. They are related to each other. And so if we look at an example here, just let's open a random log line. So what we see here, there's a trace ID. And this is interesting. So how does a trace ID end up in my log line? So this is actually also a feature of the Java instrumentation that's provided by the OpenTelemetry community. So the way logging in general works in Java is that there's a global thing with key value pairs called the log context. And applications can put arbitrary key value pairs into that context. And when you configure your log format, you can define which of those values you want to include in your log data. And if you have this OpenTelemetry agent attached, then as soon as a log line is written in the context of serving an HTTP request, then the corresponding trace ID is put into that log context. And you can configure your log format to include the trace ID in your log data. And that's what I did. And so each of my log lines actually has a trace ID. And so if I see something fancy and I want to know maybe somewhere down my distributed stack something went wrong, I can just query that in tempo, navigate to the corresponding trace, close that here, yeah, and then basically maybe get some information what happened. And then the same navigation works the other way around as well. So of course, there's a little, you know, log button here. So if I see something fancy going on in my greeting service thing here, and maybe the logs have more information, I can click on that, navigate to the logs. And then it basically just generates a query, right? I click on the greeting service with that trace ID. So it's basically just a full text search for that trace ID. And so I will find all my corresponding log lines. In that case, just one line. But if you have a bit better logging, then maybe it would give you some indication what happened there. Okay. So that was a very quick 25 minutes overview of, you know, looking a bit into metrics, looking a bit into tracing, looking a bit into logs. I hope it gave you some impression, you know, what's the type of data that you get out of open telemetry looks like. All of what we did is really, you know, without even modifying the application. I didn't, you know, even start with custom metrics, custom traces and so forth. So but it's already quite some useful data that we get out of that. If you like the demo, if you want to explore it a more, a bit more, want to try it at home, I pushed it on my GitHub and there's a readme telling you how to run it. So you can do that. And yeah, next up, we have a talk that goes a bit more in detail into the tracing part of this. And then after that, we have a talk that goes a bit more into detail how to run open telemetry in Kubernetes. So stay here and thanks for listening. Please remain seated during Q&A. Otherwise, we can't do a real Q&A. So please remain seated. Order any questions. Yes. Hi. Thank you for this. One quick question. You mentioned you just need to add some parameters to the Java virtual machine to run the telemetry. What happens to my application if, for example, the back end of the telemetry is down? Is my application failing or impacted in any way? If the monitoring back end is down. Yes. Say the monitoring is down, but I started my application with these parameters. Is it impacting the application? No. I mean, you won't see metrics, of course, if you're monitoring back end is down, but the application would just continue running. So typically, in like production setups, the applications wouldn't send telemetry data directly to the monitoring back end. But what you usually have is something in the middle. There's alternatives. There's the Grafana agent that you can use for that. There's the open telemetry collector that you can use for that. And it's basically a thing that runs close to the application, takes the telemetry data off the application very quickly, and then, you know, can buffer stuff and process stuff and send it over to the monitoring back end. And that's used for decoupling that a little bit, right? And if you have such an architecture, the application shouldn't be affected at all by that. Two more. Two more. So I really like being able to link from your metrics to traces. But what I'm actually really curious to be able to do, and as far as I know, doesn't exist, or I guess that's my question, is like, is there any thought towards doing this, is being able to go the other direction, where what I'd like to be able to answer is, here's all my trace data, and this node of the trace incremented these counters by this much. So I could ask things like how much network IO or disk IOPS did this complete request do, and where in the tree would that occur? Yeah, that's a good question. I mean, linking from traces to metrics, it's not so straightforward, because I think the things you can do to relate this is to use the service name. So if you have the service name part of your resource attributes of the metrics, and consistently you have the same service name in your trace data, then you can at least, you know, navigate to all traces coming, to all metrics coming from the same service. Maybe you have some more, you know, related attributes, like in whatever instance ID and so forth. But it's not like really a one-to-one relationship, so. That's specific. What request, how much did this request come from the IOPS? Yeah, no, I don't think that's possible. So in this example, you've shown that Grafana World and Prometheus works great with server-side applications. Have you had examples of client-side applications, mobile desktop applications that use Prometheus metrics and then ship their trace, their metrics and traces to the metric backend? Did I hear it correctly? You're asking about starting your traces on the client-side and the web browser and stuff? You have tracing on the server-side, but what about having traces and metrics on the client-side and, for example, for an embedded or mobile application so that you could actually see the trace from when the customer clicked a thing and see the full customer journey? Yeah, that's a great question. That's actually an area where there's currently a lot of research and new projects and so forth. So there is a group called real-user monitoring, RUM, in open telemetry that deal with client-side applications. There's also a project by Grafana. It's called Faro. It's kind of, you know, JavaScript that you can include in your front end, in your HTML page, and then it gives you traces and metrics from in the web browser coming from the web browser. And this is currently a pretty active area, so lots of, you know, movement there. And so there are things to explore. So if you like, check out Faro. It's a nice new project and standardization is also currently being discussed, but it's newer than the rest of what I showed you, right? So it's not as, so there's no, you know, clear standard yet or nothing decided yet. Cool. Okay. Thanks, everyone, again.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.0, "text": " Okay. Hello, everyone. Welcome to the talk on open telemetry with Grafana. Microphone", "tokens": [1033, 13, 2425, 11, 1518, 13, 4027, 281, 264, 751, 322, 1269, 4304, 5537, 627, 365, 8985, 69, 2095, 13, 25642, 4977], "temperature": 0.0, "avg_logprob": -0.2237733288815147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.35705241560935974}, {"id": 1, "seek": 0, "start": 15.0, "end": 19.12, "text": " broke, so I need to do it with this microphone now. Let's see how it goes with typing and", "tokens": [6902, 11, 370, 286, 643, 281, 360, 309, 365, 341, 10952, 586, 13, 961, 311, 536, 577, 309, 1709, 365, 18444, 293], "temperature": 0.0, "avg_logprob": -0.2237733288815147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.35705241560935974}, {"id": 2, "seek": 0, "start": 19.12, "end": 26.84, "text": " live demo. Few words about me, so who am I, why am I here talking about Grafana and open", "tokens": [1621, 10723, 13, 33468, 2283, 466, 385, 11, 370, 567, 669, 286, 11, 983, 669, 286, 510, 1417, 466, 8985, 69, 2095, 293, 1269], "temperature": 0.0, "avg_logprob": -0.2237733288815147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.35705241560935974}, {"id": 3, "seek": 2684, "start": 26.84, "end": 32.72, "text": " telemetry, so I work at Grafana Labs, I'm an engineering manager, I'm also a manager", "tokens": [4304, 5537, 627, 11, 370, 286, 589, 412, 8985, 69, 2095, 40047, 11, 286, 478, 364, 7043, 6598, 11, 286, 478, 611, 257, 6598], "temperature": 0.0, "avg_logprob": -0.18673355038426503, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0003990313853137195}, {"id": 4, "seek": 2684, "start": 32.72, "end": 37.84, "text": " for our open telemetry squad, and I'm also active in open source, so I'm a member of", "tokens": [337, 527, 1269, 4304, 5537, 627, 15310, 11, 293, 286, 478, 611, 4967, 294, 1269, 4009, 11, 370, 286, 478, 257, 4006, 295], "temperature": 0.0, "avg_logprob": -0.18673355038426503, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0003990313853137195}, {"id": 5, "seek": 2684, "start": 37.84, "end": 44.019999999999996, "text": " the Prometheus team where I maintain the Java metrics library. So what are we going to do", "tokens": [264, 2114, 649, 42209, 1469, 689, 286, 6909, 264, 10745, 16367, 6405, 13, 407, 437, 366, 321, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.18673355038426503, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0003990313853137195}, {"id": 6, "seek": 2684, "start": 44.019999999999996, "end": 50.32, "text": " in this talk in the next 25 minutes or so, so it will almost exclusively be a live demo,", "tokens": [294, 341, 751, 294, 264, 958, 3552, 2077, 420, 370, 11, 370, 309, 486, 1920, 20638, 312, 257, 1621, 10723, 11], "temperature": 0.0, "avg_logprob": -0.18673355038426503, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0003990313853137195}, {"id": 7, "seek": 2684, "start": 50.32, "end": 55.480000000000004, "text": " so basically the idea is I have a little example application running on my laptop, and it is", "tokens": [370, 1936, 264, 1558, 307, 286, 362, 257, 707, 1365, 3861, 2614, 322, 452, 10732, 11, 293, 309, 307], "temperature": 0.0, "avg_logprob": -0.18673355038426503, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0003990313853137195}, {"id": 8, "seek": 5548, "start": 55.48, "end": 60.64, "text": " instrumented with open telemetry, I will show you in a minute what it does and how I instrumented", "tokens": [7198, 292, 365, 1269, 4304, 5537, 627, 11, 286, 486, 855, 291, 294, 257, 3456, 437, 309, 775, 293, 577, 286, 7198, 292], "temperature": 0.0, "avg_logprob": -0.1661312833745429, "compression_ratio": 1.8923076923076922, "no_speech_prob": 0.00029883216484449804}, {"id": 9, "seek": 5548, "start": 60.64, "end": 66.56, "text": " it, and I also have an open source monitoring back and running, right, it consists of three", "tokens": [309, 11, 293, 286, 611, 362, 364, 1269, 4009, 11028, 646, 293, 2614, 11, 558, 11, 309, 14689, 295, 1045], "temperature": 0.0, "avg_logprob": -0.1661312833745429, "compression_ratio": 1.8923076923076922, "no_speech_prob": 0.00029883216484449804}, {"id": 10, "seek": 5548, "start": 66.56, "end": 73.92, "text": " databases, one is Loki, which is a open source logs database, one is Temple, which is an", "tokens": [22380, 11, 472, 307, 37940, 11, 597, 307, 257, 1269, 4009, 20820, 8149, 11, 472, 307, 17642, 11, 597, 307, 364], "temperature": 0.0, "avg_logprob": -0.1661312833745429, "compression_ratio": 1.8923076923076922, "no_speech_prob": 0.00029883216484449804}, {"id": 11, "seek": 5548, "start": 73.92, "end": 79.88, "text": " open source trace database, and one is Mimir, which is an open source metrics database, so", "tokens": [1269, 4009, 13508, 8149, 11, 293, 472, 307, 376, 26935, 11, 597, 307, 364, 1269, 4009, 16367, 8149, 11, 370], "temperature": 0.0, "avg_logprob": -0.1661312833745429, "compression_ratio": 1.8923076923076922, "no_speech_prob": 0.00029883216484449804}, {"id": 12, "seek": 7988, "start": 79.88, "end": 85.67999999999999, "text": " Mimir is compatible with Prometheus, so I could have shown the exact same demo using", "tokens": [376, 26935, 307, 18218, 365, 2114, 649, 42209, 11, 370, 286, 727, 362, 4898, 264, 1900, 912, 10723, 1228], "temperature": 0.0, "avg_logprob": -0.09947802439457228, "compression_ratio": 1.7540322580645162, "no_speech_prob": 0.0003227491688448936}, {"id": 13, "seek": 7988, "start": 85.67999999999999, "end": 90.92, "text": " Prometheus instead of Mimir, so it doesn't really matter for now. And of course I also", "tokens": [2114, 649, 42209, 2602, 295, 376, 26935, 11, 370, 309, 1177, 380, 534, 1871, 337, 586, 13, 400, 295, 1164, 286, 611], "temperature": 0.0, "avg_logprob": -0.09947802439457228, "compression_ratio": 1.7540322580645162, "no_speech_prob": 0.0003227491688448936}, {"id": 14, "seek": 7988, "start": 90.92, "end": 95.84, "text": " have Grafana, I have those databases configured as data sources, and what we are going to", "tokens": [362, 8985, 69, 2095, 11, 286, 362, 729, 22380, 30538, 382, 1412, 7139, 11, 293, 437, 321, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.09947802439457228, "compression_ratio": 1.7540322580645162, "no_speech_prob": 0.0003227491688448936}, {"id": 15, "seek": 7988, "start": 95.84, "end": 100.19999999999999, "text": " do, we are going to start up Grafana, you know, have a look at metrics, have a look", "tokens": [360, 11, 321, 366, 516, 281, 722, 493, 8985, 69, 2095, 11, 291, 458, 11, 362, 257, 574, 412, 16367, 11, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.09947802439457228, "compression_ratio": 1.7540322580645162, "no_speech_prob": 0.0003227491688448936}, {"id": 16, "seek": 7988, "start": 100.19999999999999, "end": 105.39999999999999, "text": " at traces, have a look at logs, and basically the idea is that at the end of the talk you", "tokens": [412, 26076, 11, 362, 257, 574, 412, 20820, 11, 293, 1936, 264, 1558, 307, 300, 412, 264, 917, 295, 264, 751, 291], "temperature": 0.0, "avg_logprob": -0.09947802439457228, "compression_ratio": 1.7540322580645162, "no_speech_prob": 0.0003227491688448936}, {"id": 17, "seek": 10540, "start": 105.4, "end": 110.0, "text": " kind of have seen all the signals that come out of open telemetry, you know, explore a", "tokens": [733, 295, 362, 1612, 439, 264, 12354, 300, 808, 484, 295, 1269, 4304, 5537, 627, 11, 291, 458, 11, 6839, 257], "temperature": 0.0, "avg_logprob": -0.1456096822565252, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00019760319264605641}, {"id": 18, "seek": 10540, "start": 110.0, "end": 114.4, "text": " bit what you can do with this type of data, and so you should have a good overview how", "tokens": [857, 437, 291, 393, 360, 365, 341, 2010, 295, 1412, 11, 293, 370, 291, 820, 362, 257, 665, 12492, 577], "temperature": 0.0, "avg_logprob": -0.1456096822565252, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00019760319264605641}, {"id": 19, "seek": 10540, "start": 114.4, "end": 121.4, "text": " open source monitoring with open telemetry looks like, right? So last slide before we", "tokens": [1269, 4009, 11028, 365, 1269, 4304, 5537, 627, 1542, 411, 11, 558, 30, 407, 1036, 4137, 949, 321], "temperature": 0.0, "avg_logprob": -0.1456096822565252, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00019760319264605641}, {"id": 20, "seek": 10540, "start": 121.4, "end": 126.32000000000001, "text": " jump into the live demo, so this is just a quick overview of what the example application", "tokens": [3012, 666, 264, 1621, 10723, 11, 370, 341, 307, 445, 257, 1702, 12492, 295, 437, 264, 1365, 3861], "temperature": 0.0, "avg_logprob": -0.1456096822565252, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00019760319264605641}, {"id": 21, "seek": 10540, "start": 126.32000000000001, "end": 132.4, "text": " does so that you know what we are going to look at. It's a simple hello world rest service", "tokens": [775, 370, 300, 291, 458, 437, 321, 366, 516, 281, 574, 412, 13, 467, 311, 257, 2199, 7751, 1002, 1472, 2643], "temperature": 0.0, "avg_logprob": -0.1456096822565252, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00019760319264605641}, {"id": 22, "seek": 13240, "start": 132.4, "end": 139.6, "text": " written in Java using Spring Boot, and so basically you can send a request to port 8080", "tokens": [3720, 294, 10745, 1228, 14013, 37263, 11, 293, 370, 1936, 291, 393, 2845, 257, 5308, 281, 2436, 4688, 4702], "temperature": 0.0, "avg_logprob": -0.12321939108506688, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.00039760416257195175}, {"id": 23, "seek": 13240, "start": 139.6, "end": 144.56, "text": " and it will respond with hello world, and in order to make it a bit more interesting,", "tokens": [293, 309, 486, 4196, 365, 7751, 1002, 11, 293, 294, 1668, 281, 652, 309, 257, 857, 544, 1880, 11], "temperature": 0.0, "avg_logprob": -0.12321939108506688, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.00039760416257195175}, {"id": 24, "seek": 13240, "start": 144.56, "end": 148.92000000000002, "text": " I made it a distributed hello world service, so it doesn't respond directly, but when it", "tokens": [286, 1027, 309, 257, 12631, 7751, 1002, 2643, 11, 370, 309, 1177, 380, 4196, 3838, 11, 457, 562, 309], "temperature": 0.0, "avg_logprob": -0.12321939108506688, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.00039760416257195175}, {"id": 25, "seek": 13240, "start": 148.92000000000002, "end": 154.6, "text": " receives a request, it reaches out to a greeting service running on port 8081, the greeting", "tokens": [20717, 257, 5308, 11, 309, 14235, 484, 281, 257, 28174, 2643, 2614, 322, 2436, 4688, 32875, 11, 264, 28174], "temperature": 0.0, "avg_logprob": -0.12321939108506688, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.00039760416257195175}, {"id": 26, "seek": 13240, "start": 154.6, "end": 159.08, "text": " service responds with the greeting, which is hello world, and then the response is forwarded", "tokens": [2643, 27331, 365, 264, 28174, 11, 597, 307, 7751, 1002, 11, 293, 550, 264, 4134, 307, 2128, 292], "temperature": 0.0, "avg_logprob": -0.12321939108506688, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.00039760416257195175}, {"id": 27, "seek": 15908, "start": 159.08, "end": 165.20000000000002, "text": " to the client, right? And there are random errors to have some error rates as well, so", "tokens": [281, 264, 6423, 11, 558, 30, 400, 456, 366, 4974, 13603, 281, 362, 512, 6713, 6846, 382, 731, 11, 370], "temperature": 0.0, "avg_logprob": -0.16361478522971826, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0001902874355437234}, {"id": 28, "seek": 15908, "start": 165.20000000000002, "end": 171.32000000000002, "text": " basically a hello world microservice architecture or whatever, right? And in order to instrument", "tokens": [1936, 257, 7751, 1002, 15547, 25006, 9482, 420, 2035, 11, 558, 30, 400, 294, 1668, 281, 7198], "temperature": 0.0, "avg_logprob": -0.16361478522971826, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0001902874355437234}, {"id": 29, "seek": 15908, "start": 171.32000000000002, "end": 177.88000000000002, "text": " this with open telemetry, I use the Java instrumentation agent that's provided by the open telemetry", "tokens": [341, 365, 1269, 4304, 5537, 627, 11, 286, 764, 264, 10745, 7198, 399, 9461, 300, 311, 5649, 538, 264, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.16361478522971826, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0001902874355437234}, {"id": 30, "seek": 15908, "start": 177.88000000000002, "end": 182.76000000000002, "text": " community, that's something you can download on GitHub, and the thing is this thing, you", "tokens": [1768, 11, 300, 311, 746, 291, 393, 5484, 322, 23331, 11, 293, 264, 551, 307, 341, 551, 11, 291], "temperature": 0.0, "avg_logprob": -0.16361478522971826, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0001902874355437234}, {"id": 31, "seek": 15908, "start": 182.76000000000002, "end": 187.68, "text": " basically attach it to the Java virtual machine at start up time with a special command line", "tokens": [1936, 5085, 309, 281, 264, 10745, 6374, 3479, 412, 722, 493, 565, 365, 257, 2121, 5622, 1622], "temperature": 0.0, "avg_logprob": -0.16361478522971826, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0001902874355437234}, {"id": 32, "seek": 18768, "start": 187.68, "end": 193.56, "text": " parameter, so I didn't modify any source code, I didn't use any SDK or introduce any custom", "tokens": [13075, 11, 370, 286, 994, 380, 16927, 604, 4009, 3089, 11, 286, 994, 380, 764, 604, 37135, 420, 5366, 604, 2375], "temperature": 0.0, "avg_logprob": -0.14300730920607044, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.00016136470367200673}, {"id": 33, "seek": 18768, "start": 193.56, "end": 199.68, "text": " stuff, all we are going to look at in this demo is just data produced by attaching the", "tokens": [1507, 11, 439, 321, 366, 516, 281, 574, 412, 294, 341, 10723, 307, 445, 1412, 7126, 538, 39074, 264], "temperature": 0.0, "avg_logprob": -0.14300730920607044, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.00016136470367200673}, {"id": 34, "seek": 18768, "start": 199.68, "end": 207.16, "text": " open telemetry instrumentation to a standard Spring Boot application, right? Cool. So let's", "tokens": [1269, 4304, 5537, 627, 7198, 399, 281, 257, 3832, 14013, 37263, 3861, 11, 558, 30, 8561, 13, 407, 718, 311], "temperature": 0.0, "avg_logprob": -0.14300730920607044, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.00016136470367200673}, {"id": 35, "seek": 18768, "start": 207.16, "end": 213.88, "text": " get started. As said, I have my data sources configured here, so Prometheus and Mimera are", "tokens": [483, 1409, 13, 1018, 848, 11, 286, 362, 452, 1412, 7139, 30538, 510, 11, 370, 2114, 649, 42209, 293, 376, 332, 1663, 366], "temperature": 0.0, "avg_logprob": -0.14300730920607044, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.00016136470367200673}, {"id": 36, "seek": 21388, "start": 213.88, "end": 220.28, "text": " compatible, so it doesn't really matter which one we choose. There are a lot of, so I want", "tokens": [18218, 11, 370, 309, 1177, 380, 534, 1871, 597, 472, 321, 2826, 13, 821, 366, 257, 688, 295, 11, 370, 286, 528], "temperature": 0.0, "avg_logprob": -0.26125102776747483, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.00062420847825706}, {"id": 37, "seek": 21388, "start": 220.28, "end": 234.48, "text": " to start with metrics, and yeah, so... Yeah? Can we turn the lights down a bit? I don't", "tokens": [281, 722, 365, 16367, 11, 293, 1338, 11, 370, 485, 865, 30, 1664, 321, 1261, 264, 5811, 760, 257, 857, 30, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.26125102776747483, "compression_ratio": 1.289855072463768, "no_speech_prob": 0.00062420847825706}, {"id": 38, "seek": 23448, "start": 234.48, "end": 254.88, "text": " know. Okay. Maybe the other way around. Okay. I will just continue, come on. So there are", "tokens": [458, 13, 1033, 13, 2704, 264, 661, 636, 926, 13, 1033, 13, 286, 486, 445, 2354, 11, 808, 322, 13, 407, 456, 366], "temperature": 0.0, "avg_logprob": -0.14912704321054313, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.0001845136284828186}, {"id": 39, "seek": 23448, "start": 254.88, "end": 260.76, "text": " lots of metrics that you get from the open telemetry instrumentation, so kind of JVM-related", "tokens": [3195, 295, 16367, 300, 291, 483, 490, 264, 1269, 4304, 5537, 627, 7198, 399, 11, 370, 733, 295, 508, 53, 44, 12, 12004], "temperature": 0.0, "avg_logprob": -0.14912704321054313, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.0001845136284828186}, {"id": 40, "seek": 26076, "start": 260.76, "end": 267.96, "text": " stuff like garbage collection activity and so forth, but the one I want to look at, oh,", "tokens": [1507, 411, 14150, 5765, 5191, 293, 370, 5220, 11, 457, 264, 472, 286, 528, 281, 574, 412, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.2128557562828064, "compression_ratio": 1.3233082706766917, "no_speech_prob": 0.00010527112317504361}, {"id": 41, "seek": 26076, "start": 267.96, "end": 286.4, "text": " no, it's getting brighter and brighter. Yeah. Okay. Great. I think there is also a light", "tokens": [572, 11, 309, 311, 1242, 19764, 293, 19764, 13, 865, 13, 1033, 13, 3769, 13, 286, 519, 456, 307, 611, 257, 1442], "temperature": 0.0, "avg_logprob": -0.2128557562828064, "compression_ratio": 1.3233082706766917, "no_speech_prob": 0.00010527112317504361}, {"id": 42, "seek": 28640, "start": 286.4, "end": 293.32, "text": " mode in Grafana. Maybe that would have been a better choice. But no, I'm not going to", "tokens": [4391, 294, 8985, 69, 2095, 13, 2704, 300, 576, 362, 668, 257, 1101, 3922, 13, 583, 572, 11, 286, 478, 406, 516, 281], "temperature": 0.0, "avg_logprob": -0.1130950927734375, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.0004440734919626266}, {"id": 43, "seek": 28640, "start": 293.32, "end": 300.47999999999996, "text": " use light mode. So let's figure out how to do the demo while I have a microphone that", "tokens": [764, 1442, 4391, 13, 407, 718, 311, 2573, 484, 577, 281, 360, 264, 10723, 1339, 286, 362, 257, 10952, 300], "temperature": 0.0, "avg_logprob": -0.1130950927734375, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.0004440734919626266}, {"id": 44, "seek": 28640, "start": 300.47999999999996, "end": 314.47999999999996, "text": " I should hold in my hands. Let's just put it here. Okay. Thank you. Cool. So the metric", "tokens": [286, 820, 1797, 294, 452, 2377, 13, 961, 311, 445, 829, 309, 510, 13, 1033, 13, 1044, 291, 13, 8561, 13, 407, 264, 20678], "temperature": 0.0, "avg_logprob": -0.1130950927734375, "compression_ratio": 1.39247311827957, "no_speech_prob": 0.0004440734919626266}, {"id": 45, "seek": 31448, "start": 314.48, "end": 320.0, "text": " that we are going to look at for the demo, it's a metric named HTTP server duration.", "tokens": [300, 321, 366, 516, 281, 574, 412, 337, 264, 10723, 11, 309, 311, 257, 20678, 4926, 33283, 7154, 16365, 13], "temperature": 0.0, "avg_logprob": -0.10960474100198832, "compression_ratio": 1.75, "no_speech_prob": 0.0006117995362728834}, {"id": 46, "seek": 31448, "start": 320.0, "end": 324.88, "text": " This is a metric of type histogram. So histograms have a couple of different numbers attached", "tokens": [639, 307, 257, 20678, 295, 2010, 49816, 13, 407, 49816, 82, 362, 257, 1916, 295, 819, 3547, 8570], "temperature": 0.0, "avg_logprob": -0.10960474100198832, "compression_ratio": 1.75, "no_speech_prob": 0.0006117995362728834}, {"id": 47, "seek": 31448, "start": 324.88, "end": 330.92, "text": " to them, so there are histogram buckets with the distribution data and so forth, and there's", "tokens": [281, 552, 11, 370, 456, 366, 49816, 32191, 365, 264, 7316, 1412, 293, 370, 5220, 11, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.10960474100198832, "compression_ratio": 1.75, "no_speech_prob": 0.0006117995362728834}, {"id": 48, "seek": 31448, "start": 330.92, "end": 337.84000000000003, "text": " also a count. The count is the most simple one, so we are going to use this in our example.", "tokens": [611, 257, 1207, 13, 440, 1207, 307, 264, 881, 2199, 472, 11, 370, 321, 366, 516, 281, 764, 341, 294, 527, 1365, 13], "temperature": 0.0, "avg_logprob": -0.10960474100198832, "compression_ratio": 1.75, "no_speech_prob": 0.0006117995362728834}, {"id": 49, "seek": 31448, "start": 337.84000000000003, "end": 341.8, "text": " I actually got it two times. I got it once for my greeting service here and once for", "tokens": [286, 767, 658, 309, 732, 1413, 13, 286, 658, 309, 1564, 337, 452, 28174, 2643, 510, 293, 1564, 337], "temperature": 0.0, "avg_logprob": -0.10960474100198832, "compression_ratio": 1.75, "no_speech_prob": 0.0006117995362728834}, {"id": 50, "seek": 34180, "start": 341.8, "end": 349.84000000000003, "text": " the hello world application. And if we are just, you know, running this query, maybe", "tokens": [264, 7751, 1002, 3861, 13, 400, 498, 321, 366, 445, 11, 291, 458, 11, 2614, 341, 14581, 11, 1310], "temperature": 0.0, "avg_logprob": -0.17458633276132438, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.00041236914694309235}, {"id": 51, "seek": 34180, "start": 349.84000000000003, "end": 355.32, "text": " take a little bit of a shorter time window here, then we basically see two request counters,", "tokens": [747, 257, 707, 857, 295, 257, 11639, 565, 4910, 510, 11, 550, 321, 1936, 536, 732, 5308, 39338, 11], "temperature": 0.0, "avg_logprob": -0.17458633276132438, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.00041236914694309235}, {"id": 52, "seek": 34180, "start": 355.32, "end": 361.44, "text": " right? One is the green line, which is counting the request resulting in HTTP such as 200.", "tokens": [558, 30, 1485, 307, 264, 3092, 1622, 11, 597, 307, 13251, 264, 5308, 16505, 294, 33283, 1270, 382, 2331, 13], "temperature": 0.0, "avg_logprob": -0.17458633276132438, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.00041236914694309235}, {"id": 53, "seek": 34180, "start": 361.44, "end": 365.96000000000004, "text": " So the successful requests, and basically we see that since I started the application", "tokens": [407, 264, 4406, 12475, 11, 293, 1936, 321, 536, 300, 1670, 286, 1409, 264, 3861], "temperature": 0.0, "avg_logprob": -0.17458633276132438, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.00041236914694309235}, {"id": 54, "seek": 34180, "start": 365.96000000000004, "end": 371.52, "text": " on my laptop, I got about a little more than 400 successful requests, and the yellow line", "tokens": [322, 452, 10732, 11, 286, 658, 466, 257, 707, 544, 813, 8423, 4406, 12475, 11, 293, 264, 5566, 1622], "temperature": 0.0, "avg_logprob": -0.17458633276132438, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.00041236914694309235}, {"id": 55, "seek": 37152, "start": 371.52, "end": 378.4, "text": " is, you know, requests resulting in HTTP status 500, and we got around 50 of them, right?", "tokens": [307, 11, 291, 458, 11, 12475, 16505, 294, 33283, 6558, 5923, 11, 293, 321, 658, 926, 2625, 295, 552, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15644471723954756, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001826559309847653}, {"id": 56, "seek": 37152, "start": 378.4, "end": 384.76, "text": " And obviously, raw counter values are not very useful, right? Nobody is interested in", "tokens": [400, 2745, 11, 8936, 5682, 4190, 366, 406, 588, 4420, 11, 558, 30, 9297, 307, 3102, 294], "temperature": 0.0, "avg_logprob": -0.15644471723954756, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001826559309847653}, {"id": 57, "seek": 37152, "start": 384.76, "end": 390.15999999999997, "text": " how often was my service called since I started the application, and the way, you know, metric", "tokens": [577, 2049, 390, 452, 2643, 1219, 1670, 286, 1409, 264, 3861, 11, 293, 264, 636, 11, 291, 458, 11, 20678], "temperature": 0.0, "avg_logprob": -0.15644471723954756, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001826559309847653}, {"id": 58, "seek": 37152, "start": 390.15999999999997, "end": 396.0, "text": " monitoring works with Prometheus, as probably most of you know, is that you use the Prometheus", "tokens": [11028, 1985, 365, 2114, 649, 42209, 11, 382, 1391, 881, 295, 291, 458, 11, 307, 300, 291, 764, 264, 2114, 649, 42209], "temperature": 0.0, "avg_logprob": -0.15644471723954756, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001826559309847653}, {"id": 59, "seek": 39600, "start": 396.0, "end": 402.28, "text": " query language to get some useful information out of that kind of data, right? And I guess", "tokens": [14581, 2856, 281, 483, 512, 4420, 1589, 484, 295, 300, 733, 295, 1412, 11, 558, 30, 400, 286, 2041], "temperature": 0.0, "avg_logprob": -0.16225009268902718, "compression_ratio": 1.4958677685950412, "no_speech_prob": 0.0006477368879131973}, {"id": 60, "seek": 39600, "start": 402.28, "end": 407.2, "text": " most of you have run some Prometheus queries, but they're still going to show maybe a couple", "tokens": [881, 295, 291, 362, 1190, 512, 2114, 649, 42209, 24109, 11, 457, 436, 434, 920, 516, 281, 855, 1310, 257, 1916], "temperature": 0.0, "avg_logprob": -0.16225009268902718, "compression_ratio": 1.4958677685950412, "no_speech_prob": 0.0006477368879131973}, {"id": 61, "seek": 39600, "start": 407.2, "end": 413.32, "text": " of examples. So for those of you who are not very familiar with that, does this one work", "tokens": [295, 5110, 13, 407, 337, 729, 295, 291, 567, 366, 406, 588, 4963, 365, 300, 11, 775, 341, 472, 589], "temperature": 0.0, "avg_logprob": -0.16225009268902718, "compression_ratio": 1.4958677685950412, "no_speech_prob": 0.0006477368879131973}, {"id": 62, "seek": 39600, "start": 413.32, "end": 420.32, "text": " again? Hey, nice. It's even better. The lights work, the microphone works. Wow. Now let's", "tokens": [797, 30, 1911, 11, 1481, 13, 467, 311, 754, 1101, 13, 440, 5811, 589, 11, 264, 10952, 1985, 13, 3153, 13, 823, 718, 311], "temperature": 0.0, "avg_logprob": -0.16225009268902718, "compression_ratio": 1.4958677685950412, "no_speech_prob": 0.0006477368879131973}, {"id": 63, "seek": 42032, "start": 420.32, "end": 427.76, "text": " hope the demo works. So I'm going to run just a couple of quick, you know, Prometheus queries", "tokens": [1454, 264, 10723, 1985, 13, 407, 286, 478, 516, 281, 1190, 445, 257, 1916, 295, 1702, 11, 291, 458, 11, 2114, 649, 42209, 24109], "temperature": 0.0, "avg_logprob": -0.13695911850248063, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00011025709682144225}, {"id": 64, "seek": 42032, "start": 427.76, "end": 431.32, "text": " so that for those of you who are not very familiar with it, so that you get an idea", "tokens": [370, 300, 337, 729, 295, 291, 567, 366, 406, 588, 4963, 365, 309, 11, 370, 300, 291, 483, 364, 1558], "temperature": 0.0, "avg_logprob": -0.13695911850248063, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00011025709682144225}, {"id": 65, "seek": 42032, "start": 431.32, "end": 436.96, "text": " of what it is, right? And the most important function in the Prometheus query language", "tokens": [295, 437, 309, 307, 11, 558, 30, 400, 264, 881, 1021, 2445, 294, 264, 2114, 649, 42209, 14581, 2856], "temperature": 0.0, "avg_logprob": -0.13695911850248063, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00011025709682144225}, {"id": 66, "seek": 42032, "start": 436.96, "end": 441.4, "text": " is called the rate function. And what the rate function does, it takes a counter like", "tokens": [307, 1219, 264, 3314, 2445, 13, 400, 437, 264, 3314, 2445, 775, 11, 309, 2516, 257, 5682, 411], "temperature": 0.0, "avg_logprob": -0.13695911850248063, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00011025709682144225}, {"id": 67, "seek": 42032, "start": 441.4, "end": 446.71999999999997, "text": " this and a time interval like five minutes, and then it calculates a per second rate,", "tokens": [341, 293, 257, 565, 15035, 411, 1732, 2077, 11, 293, 550, 309, 4322, 1024, 257, 680, 1150, 3314, 11], "temperature": 0.0, "avg_logprob": -0.13695911850248063, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00011025709682144225}, {"id": 68, "seek": 44672, "start": 446.72, "end": 452.92, "text": " right? So based on a five minute time interval, we now see that we have about 0.6 requests", "tokens": [558, 30, 407, 2361, 322, 257, 1732, 3456, 565, 15035, 11, 321, 586, 536, 300, 321, 362, 466, 1958, 13, 21, 12475], "temperature": 0.0, "avg_logprob": -0.12499476538764107, "compression_ratio": 1.6577777777777778, "no_speech_prob": 9.639694326324388e-05}, {"id": 69, "seek": 44672, "start": 452.92, "end": 459.72, "text": " per second resulting in HTTP status 200, and we have about 0.1 requests per second resulting", "tokens": [680, 1150, 16505, 294, 33283, 6558, 2331, 11, 293, 321, 362, 466, 1958, 13, 16, 12475, 680, 1150, 16505], "temperature": 0.0, "avg_logprob": -0.12499476538764107, "compression_ratio": 1.6577777777777778, "no_speech_prob": 9.639694326324388e-05}, {"id": 70, "seek": 44672, "start": 459.72, "end": 466.20000000000005, "text": " in HTTP status 500. And this is already quite some useful information, right? So typically", "tokens": [294, 33283, 6558, 5923, 13, 400, 341, 307, 1217, 1596, 512, 4420, 1589, 11, 558, 30, 407, 5850], "temperature": 0.0, "avg_logprob": -0.12499476538764107, "compression_ratio": 1.6577777777777778, "no_speech_prob": 9.639694326324388e-05}, {"id": 71, "seek": 44672, "start": 466.20000000000005, "end": 472.04, "text": " you want to know the total load on your system, not buy status code or something. So you basically", "tokens": [291, 528, 281, 458, 264, 3217, 3677, 322, 428, 1185, 11, 406, 2256, 6558, 3089, 420, 746, 13, 407, 291, 1936], "temperature": 0.0, "avg_logprob": -0.12499476538764107, "compression_ratio": 1.6577777777777778, "no_speech_prob": 9.639694326324388e-05}, {"id": 72, "seek": 47204, "start": 472.04, "end": 478.24, "text": " want to sum these two values up, and obviously there's also a sum function to sum values", "tokens": [528, 281, 2408, 613, 732, 4190, 493, 11, 293, 2745, 456, 311, 611, 257, 2408, 2445, 281, 2408, 4190], "temperature": 0.0, "avg_logprob": -0.13204484025971228, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.363830470945686e-05}, {"id": 73, "seek": 47204, "start": 478.24, "end": 483.0, "text": " up, and if you call that, you get the total load on your system, which is just one line", "tokens": [493, 11, 293, 498, 291, 818, 300, 11, 291, 483, 264, 3217, 3677, 322, 428, 1185, 11, 597, 307, 445, 472, 1622], "temperature": 0.0, "avg_logprob": -0.13204484025971228, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.363830470945686e-05}, {"id": 74, "seek": 47204, "start": 483.0, "end": 489.68, "text": " now and it's just, you know, around 0.7 requests per second, right? And this is, yeah, this", "tokens": [586, 293, 309, 311, 445, 11, 291, 458, 11, 926, 1958, 13, 22, 12475, 680, 1150, 11, 558, 30, 400, 341, 307, 11, 1338, 11, 341], "temperature": 0.0, "avg_logprob": -0.13204484025971228, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.363830470945686e-05}, {"id": 75, "seek": 47204, "start": 489.68, "end": 494.32000000000005, "text": " is basically how Prometheus queries work. If you're not familiar with the syntax, there's", "tokens": [307, 1936, 577, 2114, 649, 42209, 24109, 589, 13, 759, 291, 434, 406, 4963, 365, 264, 28431, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.13204484025971228, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.363830470945686e-05}, {"id": 76, "seek": 47204, "start": 494.32000000000005, "end": 498.84000000000003, "text": " also kind of a graphical query builder where you can, you know, use a bit drag and drop", "tokens": [611, 733, 295, 257, 35942, 14581, 27377, 689, 291, 393, 11, 291, 458, 11, 764, 257, 857, 5286, 293, 3270], "temperature": 0.0, "avg_logprob": -0.13204484025971228, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.363830470945686e-05}, {"id": 77, "seek": 49884, "start": 498.84, "end": 503.67999999999995, "text": " and get a bit more help and so forth, right? And so eventually, you know, when you got", "tokens": [293, 483, 257, 857, 544, 854, 293, 370, 5220, 11, 558, 30, 400, 370, 4728, 11, 291, 458, 11, 562, 291, 658], "temperature": 0.0, "avg_logprob": -0.15097272621010835, "compression_ratio": 1.810483870967742, "no_speech_prob": 7.00776872690767e-05}, {"id": 78, "seek": 49884, "start": 503.67999999999995, "end": 508.47999999999996, "text": " your queries and got your metrics, so what you want to do is you create a metrics dashboard", "tokens": [428, 24109, 293, 658, 428, 16367, 11, 370, 437, 291, 528, 281, 360, 307, 291, 1884, 257, 16367, 18342], "temperature": 0.0, "avg_logprob": -0.15097272621010835, "compression_ratio": 1.810483870967742, "no_speech_prob": 7.00776872690767e-05}, {"id": 79, "seek": 49884, "start": 508.47999999999996, "end": 514.16, "text": " and for monitoring HTTP services, there is, there are a couple of best practices, what", "tokens": [293, 337, 11028, 33283, 3328, 11, 456, 307, 11, 456, 366, 257, 1916, 295, 1151, 7525, 11, 437], "temperature": 0.0, "avg_logprob": -0.15097272621010835, "compression_ratio": 1.810483870967742, "no_speech_prob": 7.00776872690767e-05}, {"id": 80, "seek": 49884, "start": 514.16, "end": 520.0799999999999, "text": " type of data you want to visualize on a dashboard for monitoring HTTP services. And the most", "tokens": [2010, 295, 1412, 291, 528, 281, 23273, 322, 257, 18342, 337, 11028, 33283, 3328, 13, 400, 264, 881], "temperature": 0.0, "avg_logprob": -0.15097272621010835, "compression_ratio": 1.810483870967742, "no_speech_prob": 7.00776872690767e-05}, {"id": 81, "seek": 49884, "start": 520.0799999999999, "end": 527.72, "text": " simple and straightforward thing is to visualize three things. One is the request rate, so", "tokens": [2199, 293, 15325, 551, 307, 281, 23273, 1045, 721, 13, 1485, 307, 264, 5308, 3314, 11, 370], "temperature": 0.0, "avg_logprob": -0.15097272621010835, "compression_ratio": 1.810483870967742, "no_speech_prob": 7.00776872690767e-05}, {"id": 82, "seek": 52772, "start": 527.72, "end": 532.5600000000001, "text": " for the current load on the system, which is exactly the query that we are seeing here.", "tokens": [337, 264, 2190, 3677, 322, 264, 1185, 11, 597, 307, 2293, 264, 14581, 300, 321, 366, 2577, 510, 13], "temperature": 0.0, "avg_logprob": -0.11813914447749427, "compression_ratio": 1.6380597014925373, "no_speech_prob": 7.53943168092519e-05}, {"id": 83, "seek": 52772, "start": 532.5600000000001, "end": 537.0, "text": " The next thing you want to see is the error rate, so the percentage of calls that fail.", "tokens": [440, 958, 551, 291, 528, 281, 536, 307, 264, 6713, 3314, 11, 370, 264, 9668, 295, 5498, 300, 3061, 13], "temperature": 0.0, "avg_logprob": -0.11813914447749427, "compression_ratio": 1.6380597014925373, "no_speech_prob": 7.53943168092519e-05}, {"id": 84, "seek": 52772, "start": 537.0, "end": 542.72, "text": " And the third thing is duration. How long does it take, right? And I created a simple", "tokens": [400, 264, 2636, 551, 307, 16365, 13, 1012, 938, 775, 309, 747, 11, 558, 30, 400, 286, 2942, 257, 2199], "temperature": 0.0, "avg_logprob": -0.11813914447749427, "compression_ratio": 1.6380597014925373, "no_speech_prob": 7.53943168092519e-05}, {"id": 85, "seek": 52772, "start": 542.72, "end": 548.0400000000001, "text": " example dashboard just to show you how this looks like. So I put the name of the service", "tokens": [1365, 18342, 445, 281, 855, 291, 577, 341, 1542, 411, 13, 407, 286, 829, 264, 1315, 295, 264, 2643], "temperature": 0.0, "avg_logprob": -0.11813914447749427, "compression_ratio": 1.6380597014925373, "no_speech_prob": 7.53943168092519e-05}, {"id": 86, "seek": 52772, "start": 548.0400000000001, "end": 554.76, "text": " as a parameter up here so we can reuse the same dashboard for both services. Maybe let's", "tokens": [382, 257, 13075, 493, 510, 370, 321, 393, 26225, 264, 912, 18342, 337, 1293, 3328, 13, 2704, 718, 311], "temperature": 0.0, "avg_logprob": -0.11813914447749427, "compression_ratio": 1.6380597014925373, "no_speech_prob": 7.53943168092519e-05}, {"id": 87, "seek": 55476, "start": 554.76, "end": 559.6, "text": " use a 15 minute time window, so here I started the application. The first is the request", "tokens": [764, 257, 2119, 3456, 565, 4910, 11, 370, 510, 286, 1409, 264, 3861, 13, 440, 700, 307, 264, 5308], "temperature": 0.0, "avg_logprob": -0.13195149103800455, "compression_ratio": 1.6654135338345866, "no_speech_prob": 0.00011929058382520452}, {"id": 88, "seek": 55476, "start": 559.6, "end": 564.52, "text": " rate, that's the exact same query that we just saw. Second thing here is the error rate,", "tokens": [3314, 11, 300, 311, 264, 1900, 912, 14581, 300, 321, 445, 1866, 13, 5736, 551, 510, 307, 264, 6713, 3314, 11], "temperature": 0.0, "avg_logprob": -0.13195149103800455, "compression_ratio": 1.6654135338345866, "no_speech_prob": 0.00011929058382520452}, {"id": 89, "seek": 55476, "start": 564.52, "end": 570.56, "text": " so we have about, I don't know, around 10% errors in my example application. And then", "tokens": [370, 321, 362, 466, 11, 286, 500, 380, 458, 11, 926, 1266, 4, 13603, 294, 452, 1365, 3861, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.13195149103800455, "compression_ratio": 1.6654135338345866, "no_speech_prob": 0.00011929058382520452}, {"id": 90, "seek": 55476, "start": 570.56, "end": 575.72, "text": " for duration, there are a couple of different ways how to visualize that. So what we see", "tokens": [337, 16365, 11, 456, 366, 257, 1916, 295, 819, 2098, 577, 281, 23273, 300, 13, 407, 437, 321, 536], "temperature": 0.0, "avg_logprob": -0.13195149103800455, "compression_ratio": 1.6654135338345866, "no_speech_prob": 0.00011929058382520452}, {"id": 91, "seek": 55476, "start": 575.72, "end": 581.52, "text": " here is basically the raw histogram, right? The histogram buckets. And this representation", "tokens": [510, 307, 1936, 264, 8936, 49816, 11, 558, 30, 440, 49816, 32191, 13, 400, 341, 10290], "temperature": 0.0, "avg_logprob": -0.13195149103800455, "compression_ratio": 1.6654135338345866, "no_speech_prob": 0.00011929058382520452}, {"id": 92, "seek": 58152, "start": 581.52, "end": 586.56, "text": " is actually quite useful because it shows you the shape of the distribution. So what", "tokens": [307, 767, 1596, 4420, 570, 309, 3110, 291, 264, 3909, 295, 264, 7316, 13, 407, 437], "temperature": 0.0, "avg_logprob": -0.12817497623776927, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.00015076191630214453}, {"id": 93, "seek": 58152, "start": 586.56, "end": 594.1999999999999, "text": " we see here is two spikes, one around 600 milliseconds and one around 1.8 seconds. And", "tokens": [321, 536, 510, 307, 732, 28997, 11, 472, 926, 11849, 34184, 293, 472, 926, 502, 13, 23, 3949, 13, 400], "temperature": 0.0, "avg_logprob": -0.12817497623776927, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.00015076191630214453}, {"id": 94, "seek": 58152, "start": 594.1999999999999, "end": 599.8, "text": " this is a typical shape that you would see if your application uses a cache, right? Because", "tokens": [341, 307, 257, 7476, 3909, 300, 291, 576, 536, 498, 428, 3861, 4960, 257, 19459, 11, 558, 30, 1436], "temperature": 0.0, "avg_logprob": -0.12817497623776927, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.00015076191630214453}, {"id": 95, "seek": 58152, "start": 599.8, "end": 603.76, "text": " then you have a couple of requests that are responded quite quickly. Those are the cache", "tokens": [550, 291, 362, 257, 1916, 295, 12475, 300, 366, 15806, 1596, 2661, 13, 3950, 366, 264, 19459], "temperature": 0.0, "avg_logprob": -0.12817497623776927, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.00015076191630214453}, {"id": 96, "seek": 58152, "start": 603.76, "end": 609.76, "text": " hits. A couple of requests are slow that are the cache misses. And visualizing the shape", "tokens": [8664, 13, 316, 1916, 295, 12475, 366, 2964, 300, 366, 264, 19459, 29394, 13, 400, 5056, 3319, 264, 3909], "temperature": 0.0, "avg_logprob": -0.12817497623776927, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.00015076191630214453}, {"id": 97, "seek": 60976, "start": 609.76, "end": 614.6, "text": " of the histogram helps you understand kind of the latency behavior of your application,", "tokens": [295, 264, 49816, 3665, 291, 1223, 733, 295, 264, 27043, 5223, 295, 428, 3861, 11], "temperature": 0.0, "avg_logprob": -0.15508978216497749, "compression_ratio": 1.7975206611570247, "no_speech_prob": 7.803217158652842e-05}, {"id": 98, "seek": 60976, "start": 614.6, "end": 620.84, "text": " right? The other and most popular way to visualize durations is this one here. These", "tokens": [558, 30, 440, 661, 293, 881, 3743, 636, 281, 23273, 4861, 763, 307, 341, 472, 510, 13, 1981], "temperature": 0.0, "avg_logprob": -0.15508978216497749, "compression_ratio": 1.7975206611570247, "no_speech_prob": 7.803217158652842e-05}, {"id": 99, "seek": 60976, "start": 620.84, "end": 627.64, "text": " are percentiles. So the green line is the 95th percentile, so it tells us 95% of the", "tokens": [366, 3043, 4680, 13, 407, 264, 3092, 1622, 307, 264, 13420, 392, 3043, 794, 11, 370, 309, 5112, 505, 13420, 4, 295, 264], "temperature": 0.0, "avg_logprob": -0.15508978216497749, "compression_ratio": 1.7975206611570247, "no_speech_prob": 7.803217158652842e-05}, {"id": 100, "seek": 60976, "start": 627.64, "end": 633.28, "text": " calls have been faster than 1.7 seconds and 5% slower than that. The yellow line is the", "tokens": [5498, 362, 668, 4663, 813, 502, 13, 22, 3949, 293, 1025, 4, 14009, 813, 300, 13, 440, 5566, 1622, 307, 264], "temperature": 0.0, "avg_logprob": -0.15508978216497749, "compression_ratio": 1.7975206611570247, "no_speech_prob": 7.803217158652842e-05}, {"id": 101, "seek": 60976, "start": 633.28, "end": 638.36, "text": " 50th, so half of the calls faster than that, half of the calls slower than that. And this", "tokens": [2625, 392, 11, 370, 1922, 295, 264, 5498, 4663, 813, 300, 11, 1922, 295, 264, 5498, 14009, 813, 300, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.15508978216497749, "compression_ratio": 1.7975206611570247, "no_speech_prob": 7.803217158652842e-05}, {"id": 102, "seek": 63836, "start": 638.36, "end": 643.48, "text": " doesn't really tell you the shape of the distribution, but it shows you a development over time,", "tokens": [1177, 380, 534, 980, 291, 264, 3909, 295, 264, 7316, 11, 457, 309, 3110, 291, 257, 3250, 670, 565, 11], "temperature": 0.0, "avg_logprob": -0.12395453871342174, "compression_ratio": 1.6545454545454545, "no_speech_prob": 6.85450213495642e-05}, {"id": 103, "seek": 63836, "start": 643.48, "end": 648.4, "text": " which is useful as well. So if your service becomes slower, those lines will go up, right?", "tokens": [597, 307, 4420, 382, 731, 13, 407, 498, 428, 2643, 3643, 14009, 11, 729, 3876, 486, 352, 493, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12395453871342174, "compression_ratio": 1.6545454545454545, "no_speech_prob": 6.85450213495642e-05}, {"id": 104, "seek": 63836, "start": 648.4, "end": 653.48, "text": " And it's also a good indicator if you want to do alerting and so forth. You can define", "tokens": [400, 309, 311, 611, 257, 665, 16961, 498, 291, 528, 281, 360, 419, 27187, 293, 370, 5220, 13, 509, 393, 6964], "temperature": 0.0, "avg_logprob": -0.12395453871342174, "compression_ratio": 1.6545454545454545, "no_speech_prob": 6.85450213495642e-05}, {"id": 105, "seek": 63836, "start": 653.48, "end": 657.92, "text": " a threshold and say it's above, if it's above a certain threshold, I want to be notified", "tokens": [257, 14678, 293, 584, 309, 311, 3673, 11, 498, 309, 311, 3673, 257, 1629, 14678, 11, 286, 528, 281, 312, 18013], "temperature": 0.0, "avg_logprob": -0.12395453871342174, "compression_ratio": 1.6545454545454545, "no_speech_prob": 6.85450213495642e-05}, {"id": 106, "seek": 63836, "start": 657.92, "end": 663.72, "text": " and stuff like that. And there are other more, you know, experimental things like this heat", "tokens": [293, 1507, 411, 300, 13, 400, 456, 366, 661, 544, 11, 291, 458, 11, 17069, 721, 411, 341, 3738], "temperature": 0.0, "avg_logprob": -0.12395453871342174, "compression_ratio": 1.6545454545454545, "no_speech_prob": 6.85450213495642e-05}, {"id": 107, "seek": 66372, "start": 663.72, "end": 669.32, "text": " map showing basically development of histograms over time and stuff like that. So it's pretty", "tokens": [4471, 4099, 1936, 3250, 295, 49816, 82, 670, 565, 293, 1507, 411, 300, 13, 407, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.1894219915072123, "compression_ratio": 1.5446808510638297, "no_speech_prob": 7.572815957246348e-05}, {"id": 108, "seek": 66372, "start": 669.32, "end": 674.44, "text": " cool to play with all the different visualizations in Grafana and, you know, see what you can", "tokens": [1627, 281, 862, 365, 439, 264, 819, 5056, 14455, 294, 8985, 69, 2095, 293, 11, 291, 458, 11, 536, 437, 291, 393], "temperature": 0.0, "avg_logprob": -0.1894219915072123, "compression_ratio": 1.5446808510638297, "no_speech_prob": 7.572815957246348e-05}, {"id": 109, "seek": 66372, "start": 674.44, "end": 681.9200000000001, "text": " get. So this is a, you know, quick example of a so-called red dashboard, Request Rates,", "tokens": [483, 13, 407, 341, 307, 257, 11, 291, 458, 11, 1702, 1365, 295, 257, 370, 12, 11880, 2182, 18342, 11, 1300, 20343, 497, 1024, 11], "temperature": 0.0, "avg_logprob": -0.1894219915072123, "compression_ratio": 1.5446808510638297, "no_speech_prob": 7.572815957246348e-05}, {"id": 110, "seek": 66372, "start": 681.9200000000001, "end": 687.88, "text": " Error Rates duration based on open telemetry data. And the cool thing about this, about", "tokens": [3300, 2874, 497, 1024, 16365, 2361, 322, 1269, 4304, 5537, 627, 1412, 13, 400, 264, 1627, 551, 466, 341, 11, 466], "temperature": 0.0, "avg_logprob": -0.1894219915072123, "compression_ratio": 1.5446808510638297, "no_speech_prob": 7.572815957246348e-05}, {"id": 111, "seek": 68788, "start": 687.88, "end": 694.4, "text": " it, is that it actually, all that we are seeing here is just based on that single histogram", "tokens": [309, 11, 307, 300, 309, 767, 11, 439, 300, 321, 366, 2577, 510, 307, 445, 2361, 322, 300, 2167, 49816], "temperature": 0.0, "avg_logprob": -0.10788721196791705, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.00021171719708945602}, {"id": 112, "seek": 68788, "start": 694.4, "end": 701.72, "text": " metric HDP server duration. And the fact that this metric is there is not a coincidence.", "tokens": [20678, 389, 11373, 7154, 16365, 13, 400, 264, 1186, 300, 341, 20678, 307, 456, 307, 406, 257, 22137, 13], "temperature": 0.0, "avg_logprob": -0.10788721196791705, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.00021171719708945602}, {"id": 113, "seek": 68788, "start": 701.72, "end": 706.76, "text": " The metric HDP server duration is actually defined in the open telemetry standard as", "tokens": [440, 20678, 389, 11373, 7154, 16365, 307, 767, 7642, 294, 264, 1269, 4304, 5537, 627, 3832, 382], "temperature": 0.0, "avg_logprob": -0.10788721196791705, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.00021171719708945602}, {"id": 114, "seek": 68788, "start": 706.76, "end": 715.0, "text": " part of the semantic conventions for HDP services. So whenever you monitor an HDP server with", "tokens": [644, 295, 264, 47982, 33520, 337, 389, 11373, 3328, 13, 407, 5699, 291, 6002, 364, 389, 11373, 7154, 365], "temperature": 0.0, "avg_logprob": -0.10788721196791705, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.00021171719708945602}, {"id": 115, "seek": 71500, "start": 715.0, "end": 721.44, "text": " open telemetry, then you will find a histogram named HDP server duration. It will have the", "tokens": [1269, 4304, 5537, 627, 11, 550, 291, 486, 915, 257, 49816, 4926, 389, 11373, 7154, 16365, 13, 467, 486, 362, 264], "temperature": 0.0, "avg_logprob": -0.11327326739275898, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.0001363797637168318}, {"id": 116, "seek": 71500, "start": 721.44, "end": 727.8, "text": " HDP status as an attribute. It will contain the latencies in milliseconds. That's all", "tokens": [389, 11373, 6558, 382, 364, 19667, 13, 467, 486, 5304, 264, 4465, 6464, 294, 34184, 13, 663, 311, 439], "temperature": 0.0, "avg_logprob": -0.11327326739275898, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.0001363797637168318}, {"id": 117, "seek": 71500, "start": 727.8, "end": 733.8, "text": " part of the standard. So it doesn't matter what programming language your services uses,", "tokens": [644, 295, 264, 3832, 13, 407, 309, 1177, 380, 1871, 437, 9410, 2856, 428, 3328, 4960, 11], "temperature": 0.0, "avg_logprob": -0.11327326739275898, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.0001363797637168318}, {"id": 118, "seek": 71500, "start": 733.8, "end": 739.0, "text": " what framework, whatever. If it's being monitored with open telemetry and it's compatible, you", "tokens": [437, 8388, 11, 2035, 13, 759, 309, 311, 885, 36255, 365, 1269, 4304, 5537, 627, 293, 309, 311, 18218, 11, 291], "temperature": 0.0, "avg_logprob": -0.11327326739275898, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.0001363797637168318}, {"id": 119, "seek": 71500, "start": 739.0, "end": 743.72, "text": " will find that metric and you can create a similar dashboard like that. And this is kind", "tokens": [486, 915, 300, 20678, 293, 291, 393, 1884, 257, 2531, 18342, 411, 300, 13, 400, 341, 307, 733], "temperature": 0.0, "avg_logprob": -0.11327326739275898, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.0001363797637168318}, {"id": 120, "seek": 74372, "start": 743.72, "end": 749.0, "text": " of one of the things that make application monitoring with open telemetry a lot easier", "tokens": [295, 472, 295, 264, 721, 300, 652, 3861, 11028, 365, 1269, 4304, 5537, 627, 257, 688, 3571], "temperature": 0.0, "avg_logprob": -0.12691992455786402, "compression_ratio": 1.5807860262008733, "no_speech_prob": 4.8026919102994725e-05}, {"id": 121, "seek": 74372, "start": 749.0, "end": 757.9200000000001, "text": " than it used to be before these standardization. Cool. So that was a quick look at metrics,", "tokens": [813, 309, 1143, 281, 312, 949, 613, 3832, 2144, 13, 8561, 13, 407, 300, 390, 257, 1702, 574, 412, 16367, 11], "temperature": 0.0, "avg_logprob": -0.12691992455786402, "compression_ratio": 1.5807860262008733, "no_speech_prob": 4.8026919102994725e-05}, {"id": 122, "seek": 74372, "start": 757.9200000000001, "end": 762.6600000000001, "text": " but of course we want to look at the other signals as well. So let's switch data sources", "tokens": [457, 295, 1164, 321, 528, 281, 574, 412, 264, 661, 12354, 382, 731, 13, 407, 718, 311, 3679, 1412, 7139], "temperature": 0.0, "avg_logprob": -0.12691992455786402, "compression_ratio": 1.5807860262008733, "no_speech_prob": 4.8026919102994725e-05}, {"id": 123, "seek": 74372, "start": 762.6600000000001, "end": 772.0400000000001, "text": " for now and have a look at traces. So tracing, again, there's a kind of search, like graphical", "tokens": [337, 586, 293, 362, 257, 574, 412, 26076, 13, 407, 25262, 11, 797, 11, 456, 311, 257, 733, 295, 3164, 11, 411, 35942], "temperature": 0.0, "avg_logprob": -0.12691992455786402, "compression_ratio": 1.5807860262008733, "no_speech_prob": 4.8026919102994725e-05}, {"id": 124, "seek": 77204, "start": 772.04, "end": 777.0, "text": " search where you can create your search criteria with drag and drop. There's a relatively new", "tokens": [3164, 689, 291, 393, 1884, 428, 3164, 11101, 365, 5286, 293, 3270, 13, 821, 311, 257, 7226, 777], "temperature": 0.0, "avg_logprob": -0.14272446467958647, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.00012875825632363558}, {"id": 125, "seek": 77204, "start": 777.0, "end": 783.0, "text": " feature which is a query language for traces. So I'm going to use that for now. And one", "tokens": [4111, 597, 307, 257, 14581, 2856, 337, 26076, 13, 407, 286, 478, 516, 281, 764, 300, 337, 586, 13, 400, 472], "temperature": 0.0, "avg_logprob": -0.14272446467958647, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.00012875825632363558}, {"id": 126, "seek": 77204, "start": 783.0, "end": 788.8399999999999, "text": " thing you can do is to just search by labels. So I can, for example, say I'm interested", "tokens": [551, 291, 393, 360, 307, 281, 445, 3164, 538, 16949, 13, 407, 286, 393, 11, 337, 1365, 11, 584, 286, 478, 3102], "temperature": 0.0, "avg_logprob": -0.14272446467958647, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.00012875825632363558}, {"id": 127, "seek": 77204, "start": 788.8399999999999, "end": 797.0799999999999, "text": " in the service name greeting service and then I could basically just open a random trace", "tokens": [294, 264, 2643, 1315, 28174, 2643, 293, 550, 286, 727, 1936, 445, 1269, 257, 4974, 13508], "temperature": 0.0, "avg_logprob": -0.14272446467958647, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.00012875825632363558}, {"id": 128, "seek": 79708, "start": 797.08, "end": 805.5600000000001, "text": " here. Let's take this as an example. Can I, I need to zoom out a little bit to be able", "tokens": [510, 13, 961, 311, 747, 341, 382, 364, 1365, 13, 1664, 286, 11, 286, 643, 281, 8863, 484, 257, 707, 857, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.11689802395400181, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.00017604786262381822}, {"id": 129, "seek": 79708, "start": 805.5600000000001, "end": 813.44, "text": " to close the search window here. Okay. So this is how a distributed trace looks like.", "tokens": [281, 1998, 264, 3164, 4910, 510, 13, 1033, 13, 407, 341, 307, 577, 257, 12631, 13508, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.11689802395400181, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.00017604786262381822}, {"id": 130, "seek": 79708, "start": 813.44, "end": 818.1600000000001, "text": " And if you see it for the first time, it might be a bit hard to understand, but it's actually", "tokens": [400, 498, 291, 536, 309, 337, 264, 700, 565, 11, 309, 1062, 312, 257, 857, 1152, 281, 1223, 11, 457, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.11689802395400181, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.00017604786262381822}, {"id": 131, "seek": 79708, "start": 818.1600000000001, "end": 822.84, "text": " fairly easy. So you just need like two minutes of introduction and then you will understand", "tokens": [6457, 1858, 13, 407, 291, 445, 643, 411, 732, 2077, 295, 9339, 293, 550, 291, 486, 1223], "temperature": 0.0, "avg_logprob": -0.11689802395400181, "compression_ratio": 1.5497835497835497, "no_speech_prob": 0.00017604786262381822}, {"id": 132, "seek": 82284, "start": 822.84, "end": 828.64, "text": " traces forever. And to give you that introduction, I actually have one more slide. So just to", "tokens": [26076, 5680, 13, 400, 281, 976, 291, 300, 9339, 11, 286, 767, 362, 472, 544, 4137, 13, 407, 445, 281], "temperature": 0.0, "avg_logprob": -0.11971776419823323, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.00011077285307692364}, {"id": 133, "seek": 82284, "start": 828.64, "end": 835.08, "text": " help you understand what we are seeing here. And the thing is distributed traces consist", "tokens": [854, 291, 1223, 437, 321, 366, 2577, 510, 13, 400, 264, 551, 307, 12631, 26076, 4603], "temperature": 0.0, "avg_logprob": -0.11971776419823323, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.00011077285307692364}, {"id": 134, "seek": 82284, "start": 835.08, "end": 839.88, "text": " of spans, right? And spans are time spans. So a span is something that has a point in", "tokens": [295, 44086, 11, 558, 30, 400, 44086, 366, 565, 44086, 13, 407, 257, 16174, 307, 746, 300, 575, 257, 935, 294], "temperature": 0.0, "avg_logprob": -0.11971776419823323, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.00011077285307692364}, {"id": 135, "seek": 82284, "start": 839.88, "end": 845.44, "text": " time where it starts and a point in time where it ends, right? And in open telemetry, there", "tokens": [565, 689, 309, 3719, 293, 257, 935, 294, 565, 689, 309, 5314, 11, 558, 30, 400, 294, 1269, 4304, 5537, 627, 11, 456], "temperature": 0.0, "avg_logprob": -0.11971776419823323, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.00011077285307692364}, {"id": 136, "seek": 82284, "start": 845.44, "end": 851.6, "text": " are three different kinds of spans. One are server spans. The second is internal spans", "tokens": [366, 1045, 819, 3685, 295, 44086, 13, 1485, 366, 7154, 44086, 13, 440, 1150, 307, 6920, 44086], "temperature": 0.0, "avg_logprob": -0.11971776419823323, "compression_ratio": 1.7951807228915662, "no_speech_prob": 0.00011077285307692364}, {"id": 137, "seek": 85160, "start": 851.6, "end": 858.0, "text": " and the third is client spans. Okay. So what happens when my Hello World application receives", "tokens": [293, 264, 2636, 307, 6423, 44086, 13, 1033, 13, 407, 437, 2314, 562, 452, 2425, 3937, 3861, 20717], "temperature": 0.0, "avg_logprob": -0.14331449311355066, "compression_ratio": 1.7264150943396226, "no_speech_prob": 3.8659170968458056e-05}, {"id": 138, "seek": 85160, "start": 858.0, "end": 863.2, "text": " a request? So the first thing that happens if a server receives a request, a server span", "tokens": [257, 5308, 30, 407, 264, 700, 551, 300, 2314, 498, 257, 7154, 20717, 257, 5308, 11, 257, 7154, 16174], "temperature": 0.0, "avg_logprob": -0.14331449311355066, "compression_ratio": 1.7264150943396226, "no_speech_prob": 3.8659170968458056e-05}, {"id": 139, "seek": 85160, "start": 863.2, "end": 868.8000000000001, "text": " is created. So that's the first line here. It's started as soon as the request is received.", "tokens": [307, 2942, 13, 407, 300, 311, 264, 700, 1622, 510, 13, 467, 311, 1409, 382, 2321, 382, 264, 5308, 307, 4613, 13], "temperature": 0.0, "avg_logprob": -0.14331449311355066, "compression_ratio": 1.7264150943396226, "no_speech_prob": 3.8659170968458056e-05}, {"id": 140, "seek": 85160, "start": 868.8000000000001, "end": 876.24, "text": " It remains open until the request is responded, right? Then I said in the introduction that", "tokens": [467, 7023, 1269, 1826, 264, 5308, 307, 15806, 11, 558, 30, 1396, 286, 848, 294, 264, 9339, 300], "temperature": 0.0, "avg_logprob": -0.14331449311355066, "compression_ratio": 1.7264150943396226, "no_speech_prob": 3.8659170968458056e-05}, {"id": 141, "seek": 87624, "start": 876.24, "end": 881.8, "text": " I used spring boot for implementing the example application. And the way spring boot works", "tokens": [286, 1143, 5587, 11450, 337, 18114, 264, 1365, 3861, 13, 400, 264, 636, 5587, 11450, 1985], "temperature": 0.0, "avg_logprob": -0.13612146879497328, "compression_ratio": 1.7848605577689243, "no_speech_prob": 8.810589497443289e-05}, {"id": 142, "seek": 87624, "start": 881.8, "end": 886.48, "text": " is that it takes the request and passes it to the corresponding spring controller that", "tokens": [307, 300, 309, 2516, 264, 5308, 293, 11335, 309, 281, 264, 11760, 5587, 10561, 300], "temperature": 0.0, "avg_logprob": -0.13612146879497328, "compression_ratio": 1.7848605577689243, "no_speech_prob": 8.810589497443289e-05}, {"id": 143, "seek": 87624, "start": 886.48, "end": 893.24, "text": " would handle the request. And open telemetries Java instrumentation agent is nice for Java", "tokens": [576, 4813, 264, 5308, 13, 400, 1269, 4304, 5537, 2244, 10745, 7198, 399, 9461, 307, 1481, 337, 10745], "temperature": 0.0, "avg_logprob": -0.13612146879497328, "compression_ratio": 1.7848605577689243, "no_speech_prob": 8.810589497443289e-05}, {"id": 144, "seek": 87624, "start": 893.24, "end": 897.92, "text": " developers because it just creates internal spans for each spring controller that is involved,", "tokens": [8849, 570, 309, 445, 7829, 6920, 44086, 337, 1184, 5587, 10561, 300, 307, 3288, 11], "temperature": 0.0, "avg_logprob": -0.13612146879497328, "compression_ratio": 1.7848605577689243, "no_speech_prob": 8.810589497443289e-05}, {"id": 145, "seek": 87624, "start": 897.92, "end": 902.6, "text": " right? And that is the second line that we are seeing here. It's basically opened as", "tokens": [558, 30, 400, 300, 307, 264, 1150, 1622, 300, 321, 366, 2577, 510, 13, 467, 311, 1936, 5625, 382], "temperature": 0.0, "avg_logprob": -0.13612146879497328, "compression_ratio": 1.7848605577689243, "no_speech_prob": 8.810589497443289e-05}, {"id": 146, "seek": 90260, "start": 902.6, "end": 907.52, "text": " soon as the spring controller takes over and remains open until the spring controller", "tokens": [2321, 382, 264, 5587, 10561, 2516, 670, 293, 7023, 1269, 1826, 264, 5587, 10561], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 147, "seek": 90260, "start": 907.52, "end": 913.12, "text": " is done handling the request, which might seem not too useful if I have just a single", "tokens": [307, 1096, 13175, 264, 5308, 11, 597, 1062, 1643, 406, 886, 4420, 498, 286, 362, 445, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 148, "seek": 90260, "start": 913.12, "end": 918.0400000000001, "text": " spring controller anyway, but if you have kind of a larger, you know, application and", "tokens": [5587, 10561, 4033, 11, 457, 498, 291, 362, 733, 295, 257, 4833, 11, 291, 458, 11, 3861, 293], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 149, "seek": 90260, "start": 918.0400000000001, "end": 922.2, "text": " if you have multiple controllers involved, it gives you quite some interesting insights", "tokens": [498, 291, 362, 3866, 26903, 3288, 11, 309, 2709, 291, 1596, 512, 1880, 14310], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 150, "seek": 90260, "start": 922.2, "end": 926.44, "text": " into what's happening inside your application. Like you would see immediately, like which", "tokens": [666, 437, 311, 2737, 1854, 428, 3861, 13, 1743, 291, 576, 536, 4258, 11, 411, 597], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 151, "seek": 90260, "start": 926.44, "end": 932.0, "text": " controller do I spend most time in and so forth, right? And then eventually my Hello", "tokens": [10561, 360, 286, 3496, 881, 565, 294, 293, 370, 5220, 11, 558, 30, 400, 550, 4728, 452, 2425], "temperature": 0.0, "avg_logprob": -0.1542020184653146, "compression_ratio": 1.793103448275862, "no_speech_prob": 4.586823706631549e-05}, {"id": 152, "seek": 93200, "start": 932.0, "end": 937.44, "text": " Word application reaches out to the greeting service and outgoing requests are represented", "tokens": [8725, 3861, 14235, 484, 281, 264, 28174, 2643, 293, 41565, 12475, 366, 10379], "temperature": 0.0, "avg_logprob": -0.13130091676617614, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0001343677577096969}, {"id": 153, "seek": 93200, "start": 937.44, "end": 943.36, "text": " by client spans. So the client span is basically opened as soon as my HTTP request goes out", "tokens": [538, 6423, 44086, 13, 407, 264, 6423, 16174, 307, 1936, 5625, 382, 2321, 382, 452, 33283, 5308, 1709, 484], "temperature": 0.0, "avg_logprob": -0.13130091676617614, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0001343677577096969}, {"id": 154, "seek": 93200, "start": 943.36, "end": 947.6, "text": " and remains open until the response is received. And then in the greeting service, the same", "tokens": [293, 7023, 1269, 1826, 264, 4134, 307, 4613, 13, 400, 550, 294, 264, 28174, 2643, 11, 264, 912], "temperature": 0.0, "avg_logprob": -0.13130091676617614, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0001343677577096969}, {"id": 155, "seek": 93200, "start": 947.6, "end": 952.8, "text": " thing starts again, you know, request is received, which creates a server span and then I have", "tokens": [551, 3719, 797, 11, 291, 458, 11, 5308, 307, 4613, 11, 597, 7829, 257, 7154, 16174, 293, 550, 286, 362], "temperature": 0.0, "avg_logprob": -0.13130091676617614, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0001343677577096969}, {"id": 156, "seek": 93200, "start": 952.8, "end": 957.4, "text": " a spring controller as well, which is an internal span and that's the end of my distributed", "tokens": [257, 5587, 10561, 382, 731, 11, 597, 307, 364, 6920, 16174, 293, 300, 311, 264, 917, 295, 452, 12631], "temperature": 0.0, "avg_logprob": -0.13130091676617614, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0001343677577096969}, {"id": 157, "seek": 95740, "start": 957.4, "end": 962.68, "text": " application here. And this is exactly what we are seeing here. And each of those span", "tokens": [3861, 510, 13, 400, 341, 307, 2293, 437, 321, 366, 2577, 510, 13, 400, 1184, 295, 729, 16174], "temperature": 0.0, "avg_logprob": -0.1442028448122357, "compression_ratio": 1.7661290322580645, "no_speech_prob": 0.00023502645490225405}, {"id": 158, "seek": 95740, "start": 962.68, "end": 968.12, "text": " types has a corresponding metadata attached to it. So if you look at one of the internal", "tokens": [3467, 575, 257, 11760, 26603, 8570, 281, 309, 13, 407, 498, 291, 574, 412, 472, 295, 264, 6920], "temperature": 0.0, "avg_logprob": -0.1442028448122357, "compression_ratio": 1.7661290322580645, "no_speech_prob": 0.00023502645490225405}, {"id": 159, "seek": 95740, "start": 968.12, "end": 972.56, "text": " spans here, we see the name of the spring controller and the name of the controller", "tokens": [44086, 510, 11, 321, 536, 264, 1315, 295, 264, 5587, 10561, 293, 264, 1315, 295, 264, 10561], "temperature": 0.0, "avg_logprob": -0.1442028448122357, "compression_ratio": 1.7661290322580645, "no_speech_prob": 0.00023502645490225405}, {"id": 160, "seek": 95740, "start": 972.56, "end": 979.12, "text": " method and a couple of JVM-related attributes, whatever. And if we look at an HTTP span,", "tokens": [3170, 293, 257, 1916, 295, 508, 53, 44, 12, 12004, 17212, 11, 2035, 13, 400, 498, 321, 574, 412, 364, 33283, 16174, 11], "temperature": 0.0, "avg_logprob": -0.1442028448122357, "compression_ratio": 1.7661290322580645, "no_speech_prob": 0.00023502645490225405}, {"id": 161, "seek": 95740, "start": 979.12, "end": 984.36, "text": " for example, we see, of course, HTTP attributes like the status code, method and so forth,", "tokens": [337, 1365, 11, 321, 536, 11, 295, 1164, 11, 33283, 17212, 411, 264, 6558, 3089, 11, 3170, 293, 370, 5220, 11], "temperature": 0.0, "avg_logprob": -0.1442028448122357, "compression_ratio": 1.7661290322580645, "no_speech_prob": 0.00023502645490225405}, {"id": 162, "seek": 98436, "start": 984.36, "end": 992.32, "text": " right? So of course, you do not want to just look at random spans. So usually you're looking", "tokens": [558, 30, 407, 295, 1164, 11, 291, 360, 406, 528, 281, 445, 574, 412, 4974, 44086, 13, 407, 2673, 291, 434, 1237], "temperature": 0.0, "avg_logprob": -0.1600702016249947, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0001508176064817235}, {"id": 163, "seek": 98436, "start": 992.32, "end": 997.5600000000001, "text": " for something. There are standard attributes in open telemetry that you can use for searching.", "tokens": [337, 746, 13, 821, 366, 3832, 17212, 294, 1269, 4304, 5537, 627, 300, 291, 393, 764, 337, 10808, 13], "temperature": 0.0, "avg_logprob": -0.1600702016249947, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0001508176064817235}, {"id": 164, "seek": 98436, "start": 997.5600000000001, "end": 1004.76, "text": " So we already had the service name greeting service, for example. But the most important", "tokens": [407, 321, 1217, 632, 264, 2643, 1315, 28174, 2643, 11, 337, 1365, 13, 583, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.1600702016249947, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0001508176064817235}, {"id": 165, "seek": 98436, "start": 1004.76, "end": 1013.4, "text": " or one of the most important attributes is HTTP.status, no,.status code, this one here.", "tokens": [420, 472, 295, 264, 881, 1021, 17212, 307, 33283, 13, 19435, 301, 11, 572, 11, 2411, 19435, 301, 3089, 11, 341, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.1600702016249947, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0001508176064817235}, {"id": 166, "seek": 101340, "start": 1013.4, "end": 1019.24, "text": " And if we, for example, search for spans with HTTP status code 500, then we should find", "tokens": [400, 498, 321, 11, 337, 1365, 11, 3164, 337, 44086, 365, 33283, 6558, 3089, 5923, 11, 550, 321, 820, 915], "temperature": 0.0, "avg_logprob": -0.07767685181503996, "compression_ratio": 1.6973180076628354, "no_speech_prob": 9.205206151818857e-05}, {"id": 167, "seek": 101340, "start": 1019.24, "end": 1025.04, "text": " an example of a request that failed. So let's close the search window again. Yes, that's", "tokens": [364, 1365, 295, 257, 5308, 300, 7612, 13, 407, 718, 311, 1998, 264, 3164, 4910, 797, 13, 1079, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.07767685181503996, "compression_ratio": 1.6973180076628354, "no_speech_prob": 9.205206151818857e-05}, {"id": 168, "seek": 101340, "start": 1025.04, "end": 1030.3, "text": " an example of a failed request. You see it with the indicated by those red exclamation", "tokens": [364, 1365, 295, 257, 7612, 5308, 13, 509, 536, 309, 365, 264, 16176, 538, 729, 2182, 1624, 43233], "temperature": 0.0, "avg_logprob": -0.07767685181503996, "compression_ratio": 1.6973180076628354, "no_speech_prob": 9.205206151818857e-05}, {"id": 169, "seek": 101340, "start": 1030.3, "end": 1035.96, "text": " marks at the bottom here. So this is where the thing failed, right? So the root cause", "tokens": [10640, 412, 264, 2767, 510, 13, 407, 341, 307, 689, 264, 551, 7612, 11, 558, 30, 407, 264, 5593, 3082], "temperature": 0.0, "avg_logprob": -0.07767685181503996, "compression_ratio": 1.6973180076628354, "no_speech_prob": 9.205206151818857e-05}, {"id": 170, "seek": 101340, "start": 1035.96, "end": 1041.52, "text": " of the error is the internal span, something in my spring controller in the greeting service.", "tokens": [295, 264, 6713, 307, 264, 6920, 16174, 11, 746, 294, 452, 5587, 10561, 294, 264, 28174, 2643, 13], "temperature": 0.0, "avg_logprob": -0.07767685181503996, "compression_ratio": 1.6973180076628354, "no_speech_prob": 9.205206151818857e-05}, {"id": 171, "seek": 104152, "start": 1041.52, "end": 1047.12, "text": " If I look at the metadata attached to that, I actually see that the instrumentation attached", "tokens": [759, 286, 574, 412, 264, 26603, 8570, 281, 300, 11, 286, 767, 536, 300, 264, 7198, 399, 8570], "temperature": 0.0, "avg_logprob": -0.13028933338283263, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.00013033724098931998}, {"id": 172, "seek": 104152, "start": 1047.12, "end": 1052.92, "text": " the event that caused the error, and this even includes the stack trace. So you can", "tokens": [264, 2280, 300, 7008, 264, 6713, 11, 293, 341, 754, 5974, 264, 8630, 13508, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.13028933338283263, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.00013033724098931998}, {"id": 173, "seek": 104152, "start": 1052.92, "end": 1058.28, "text": " basically immediately navigate to the exact line of code that is the root cause of this", "tokens": [1936, 4258, 12350, 281, 264, 1900, 1622, 295, 3089, 300, 307, 264, 5593, 3082, 295, 341], "temperature": 0.0, "avg_logprob": -0.13028933338283263, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.00013033724098931998}, {"id": 174, "seek": 104152, "start": 1058.28, "end": 1063.8799999999999, "text": " error, right? And this is quite cool. So if you have a distributed application and you", "tokens": [6713, 11, 558, 30, 400, 341, 307, 1596, 1627, 13, 407, 498, 291, 362, 257, 12631, 3861, 293, 291], "temperature": 0.0, "avg_logprob": -0.13028933338283263, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.00013033724098931998}, {"id": 175, "seek": 104152, "start": 1063.8799999999999, "end": 1070.16, "text": " get an unexpected response from your Hello World application, without distributed tracing,", "tokens": [483, 364, 13106, 4134, 490, 428, 2425, 3937, 3861, 11, 1553, 12631, 25262, 11], "temperature": 0.0, "avg_logprob": -0.13028933338283263, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.00013033724098931998}, {"id": 176, "seek": 107016, "start": 1070.16, "end": 1075.64, "text": " it's pretty hard to find that actually there's an exception in the greeting service that,", "tokens": [309, 311, 1238, 1152, 281, 915, 300, 767, 456, 311, 364, 11183, 294, 264, 28174, 2643, 300, 11], "temperature": 0.0, "avg_logprob": -0.1598317813873291, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.00014357356121763587}, {"id": 177, "seek": 107016, "start": 1075.64, "end": 1080.8400000000001, "text": " you know, propagated through your distributed landscape and then eventually caused the unexpected", "tokens": [291, 458, 11, 12425, 770, 807, 428, 12631, 9661, 293, 550, 4728, 7008, 264, 13106], "temperature": 0.0, "avg_logprob": -0.1598317813873291, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.00014357356121763587}, {"id": 178, "seek": 107016, "start": 1080.8400000000001, "end": 1086.68, "text": " response. And with distributed tracing, finding these kind of things becomes pretty easy because", "tokens": [4134, 13, 400, 365, 12631, 25262, 11, 5006, 613, 733, 295, 721, 3643, 1238, 1858, 570], "temperature": 0.0, "avg_logprob": -0.1598317813873291, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.00014357356121763587}, {"id": 179, "seek": 107016, "start": 1086.68, "end": 1092.44, "text": " you get all the related calls grouped together, you get the failed ones marked with an exclamation", "tokens": [291, 483, 439, 264, 4077, 5498, 41877, 1214, 11, 291, 483, 264, 7612, 2306, 12658, 365, 364, 1624, 43233], "temperature": 0.0, "avg_logprob": -0.1598317813873291, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.00014357356121763587}, {"id": 180, "seek": 107016, "start": 1092.44, "end": 1098.92, "text": " mark, and you can pretty easily navigate to what's the root cause of your error, okay?", "tokens": [1491, 11, 293, 291, 393, 1238, 3612, 12350, 281, 437, 311, 264, 5593, 3082, 295, 428, 6713, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1598317813873291, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.00014357356121763587}, {"id": 181, "seek": 109892, "start": 1098.92, "end": 1105.1200000000001, "text": " Cool. So that was a quick look at traces. There are a lot of interesting things about", "tokens": [8561, 13, 407, 300, 390, 257, 1702, 574, 412, 26076, 13, 821, 366, 257, 688, 295, 1880, 721, 466], "temperature": 0.0, "avg_logprob": -0.1207116321452613, "compression_ratio": 1.647940074906367, "no_speech_prob": 4.2430827306816354e-05}, {"id": 182, "seek": 109892, "start": 1105.1200000000001, "end": 1110.24, "text": " tracing. Maybe one thing I would like to show you, because I find it particularly cool,", "tokens": [25262, 13, 2704, 472, 551, 286, 576, 411, 281, 855, 291, 11, 570, 286, 915, 309, 4098, 1627, 11], "temperature": 0.0, "avg_logprob": -0.1207116321452613, "compression_ratio": 1.647940074906367, "no_speech_prob": 4.2430827306816354e-05}, {"id": 183, "seek": 109892, "start": 1110.24, "end": 1116.3200000000002, "text": " so if you have all your services instrumented with tracing in your back end, then basically", "tokens": [370, 498, 291, 362, 439, 428, 3328, 7198, 292, 365, 25262, 294, 428, 646, 917, 11, 550, 1936], "temperature": 0.0, "avg_logprob": -0.1207116321452613, "compression_ratio": 1.647940074906367, "no_speech_prob": 4.2430827306816354e-05}, {"id": 184, "seek": 109892, "start": 1116.3200000000002, "end": 1121.8000000000002, "text": " those traces give you metadata about all the network calls happening in your system,", "tokens": [729, 26076, 976, 291, 26603, 466, 439, 264, 3209, 5498, 2737, 294, 428, 1185, 11], "temperature": 0.0, "avg_logprob": -0.1207116321452613, "compression_ratio": 1.647940074906367, "no_speech_prob": 4.2430827306816354e-05}, {"id": 185, "seek": 109892, "start": 1121.8000000000002, "end": 1126.68, "text": " and you can do something with that type of data, right? So for example, you can calculate", "tokens": [293, 291, 393, 360, 746, 365, 300, 2010, 295, 1412, 11, 558, 30, 407, 337, 1365, 11, 291, 393, 8873], "temperature": 0.0, "avg_logprob": -0.1207116321452613, "compression_ratio": 1.647940074906367, "no_speech_prob": 4.2430827306816354e-05}, {"id": 186, "seek": 112668, "start": 1126.68, "end": 1133.3600000000001, "text": " something that we call the service graph. So it looks like this. It's maybe not too impressive", "tokens": [746, 300, 321, 818, 264, 2643, 4295, 13, 407, 309, 1542, 411, 341, 13, 467, 311, 1310, 406, 886, 8992], "temperature": 0.0, "avg_logprob": -0.12152991785067264, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.0001269297208636999}, {"id": 187, "seek": 112668, "start": 1133.3600000000001, "end": 1138.16, "text": " if you just have two services calling each other, right? So, but if you imagine, you", "tokens": [498, 291, 445, 362, 732, 3328, 5141, 1184, 661, 11, 558, 30, 407, 11, 457, 498, 291, 3811, 11, 291], "temperature": 0.0, "avg_logprob": -0.12152991785067264, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.0001269297208636999}, {"id": 188, "seek": 112668, "start": 1138.16, "end": 1142.6000000000001, "text": " know, a more larger, you know, dozens or hundreds of services, so it will generate a map of", "tokens": [458, 11, 257, 544, 4833, 11, 291, 458, 11, 18431, 420, 6779, 295, 3328, 11, 370, 309, 486, 8460, 257, 4471, 295], "temperature": 0.0, "avg_logprob": -0.12152991785067264, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.0001269297208636999}, {"id": 189, "seek": 112668, "start": 1142.6000000000001, "end": 1148.28, "text": " all the services and indicate which service calls which other service, and this is quite", "tokens": [439, 264, 3328, 293, 13330, 597, 2643, 5498, 597, 661, 2643, 11, 293, 341, 307, 1596], "temperature": 0.0, "avg_logprob": -0.12152991785067264, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.0001269297208636999}, {"id": 190, "seek": 112668, "start": 1148.28, "end": 1153.44, "text": " useful. For example, if you intend to deploy a breaking change in your greeting service", "tokens": [4420, 13, 1171, 1365, 11, 498, 291, 19759, 281, 7274, 257, 7697, 1319, 294, 428, 28174, 2643], "temperature": 0.0, "avg_logprob": -0.12152991785067264, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.0001269297208636999}, {"id": 191, "seek": 115344, "start": 1153.44, "end": 1157.68, "text": " and you want to know who's using the greeting service, what would I break? Then looking", "tokens": [293, 291, 528, 281, 458, 567, 311, 1228, 264, 28174, 2643, 11, 437, 576, 286, 1821, 30, 1396, 1237], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 192, "seek": 115344, "start": 1157.68, "end": 1162.52, "text": " at the service graph, you basically get this information right away. Traditionally, if", "tokens": [412, 264, 2643, 4295, 11, 291, 1936, 483, 341, 1589, 558, 1314, 13, 22017, 15899, 11, 498], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 193, "seek": 115344, "start": 1162.52, "end": 1167.1200000000001, "text": " you don't have that, you basically have a PDF with your architecture diagram, and then", "tokens": [291, 500, 380, 362, 300, 11, 291, 1936, 362, 257, 17752, 365, 428, 9482, 10686, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 194, "seek": 115344, "start": 1167.1200000000001, "end": 1172.96, "text": " you look it up there, and also traditionally, there's at least one team that deployed something", "tokens": [291, 574, 309, 493, 456, 11, 293, 611, 19067, 11, 456, 311, 412, 1935, 472, 1469, 300, 17826, 746], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 195, "seek": 115344, "start": 1172.96, "end": 1177.1200000000001, "text": " and forgot to update the diagram, and then you missed that, and there's a service graph", "tokens": [293, 5298, 281, 5623, 264, 10686, 11, 293, 550, 291, 6721, 300, 11, 293, 456, 311, 257, 2643, 4295], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 196, "seek": 115344, "start": 1177.1200000000001, "end": 1181.64, "text": " that won't happen, right? This is the actual truth. This is based on what's actually happening", "tokens": [300, 1582, 380, 1051, 11, 558, 30, 639, 307, 264, 3539, 3494, 13, 639, 307, 2361, 322, 437, 311, 767, 2737], "temperature": 0.0, "avg_logprob": -0.10511310457244633, "compression_ratio": 1.8493150684931507, "no_speech_prob": 0.00010514762834645808}, {"id": 197, "seek": 118164, "start": 1181.64, "end": 1186.88, "text": " in your backend, and this is pretty useful in these situations, right? And you can do", "tokens": [294, 428, 38087, 11, 293, 341, 307, 1238, 4420, 294, 613, 6851, 11, 558, 30, 400, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.12156342577051234, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.00010469558765180409}, {"id": 198, "seek": 118164, "start": 1186.88, "end": 1192.5200000000002, "text": " other things as well, like, you know, have some statistics like the most frequently called", "tokens": [661, 721, 382, 731, 11, 411, 11, 291, 458, 11, 362, 512, 12523, 411, 264, 881, 10374, 1219], "temperature": 0.0, "avg_logprob": -0.12156342577051234, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.00010469558765180409}, {"id": 199, "seek": 118164, "start": 1192.5200000000002, "end": 1199.2800000000002, "text": " endpoint or the endpoint with the most errors and stuff like that. So, that was a quick,", "tokens": [35795, 420, 264, 35795, 365, 264, 881, 13603, 293, 1507, 411, 300, 13, 407, 11, 300, 390, 257, 1702, 11], "temperature": 0.0, "avg_logprob": -0.12156342577051234, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.00010469558765180409}, {"id": 200, "seek": 118164, "start": 1199.2800000000002, "end": 1205.0, "text": " quick look at traces. So we covered metrics, we covered traces. One thing I want to show", "tokens": [1702, 574, 412, 26076, 13, 407, 321, 5343, 16367, 11, 321, 5343, 26076, 13, 1485, 551, 286, 528, 281, 855], "temperature": 0.0, "avg_logprob": -0.12156342577051234, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.00010469558765180409}, {"id": 201, "seek": 118164, "start": 1205.0, "end": 1210.88, "text": " you is that metrics and traces are actually related to each other, right? And so in order", "tokens": [291, 307, 300, 16367, 293, 26076, 366, 767, 4077, 281, 1184, 661, 11, 558, 30, 400, 370, 294, 1668], "temperature": 0.0, "avg_logprob": -0.12156342577051234, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.00010469558765180409}, {"id": 202, "seek": 121088, "start": 1210.88, "end": 1216.5200000000002, "text": " to show that, I'm going to go back to our dashboard, because if you, let's take a 15", "tokens": [281, 855, 300, 11, 286, 478, 516, 281, 352, 646, 281, 527, 18342, 11, 570, 498, 291, 11, 718, 311, 747, 257, 2119], "temperature": 0.0, "avg_logprob": -0.11071800517144605, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00012525411148089916}, {"id": 203, "seek": 121088, "start": 1216.5200000000002, "end": 1222.2, "text": " minute window, then we get a bit more examples. So if you look at the latency data here,", "tokens": [3456, 4910, 11, 550, 321, 483, 257, 857, 544, 5110, 13, 407, 498, 291, 574, 412, 264, 27043, 1412, 510, 11], "temperature": 0.0, "avg_logprob": -0.11071800517144605, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00012525411148089916}, {"id": 204, "seek": 121088, "start": 1222.2, "end": 1227.0, "text": " you notice these little green dots. These are called exemplars, and this is something", "tokens": [291, 3449, 613, 707, 3092, 15026, 13, 1981, 366, 1219, 24112, 685, 11, 293, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.11071800517144605, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00012525411148089916}, {"id": 205, "seek": 121088, "start": 1227.0, "end": 1232.92, "text": " that's provided by the auto instrumentation of open telemetry. So whenever it generates", "tokens": [300, 311, 5649, 538, 264, 8399, 7198, 399, 295, 1269, 4304, 5537, 627, 13, 407, 5699, 309, 23815], "temperature": 0.0, "avg_logprob": -0.11071800517144605, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00012525411148089916}, {"id": 206, "seek": 121088, "start": 1232.92, "end": 1240.48, "text": " latency data, it basically attaches trace IDs of example traces to the latency data,", "tokens": [27043, 1412, 11, 309, 1936, 49404, 13508, 48212, 295, 1365, 26076, 281, 264, 27043, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11071800517144605, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00012525411148089916}, {"id": 207, "seek": 124048, "start": 1240.48, "end": 1244.84, "text": " and this is visualized by these little green dots, right? And so you see some examples", "tokens": [293, 341, 307, 5056, 1602, 538, 613, 707, 3092, 15026, 11, 558, 30, 400, 370, 291, 536, 512, 5110], "temperature": 0.0, "avg_logprob": -0.12353143959401924, "compression_ratio": 1.75, "no_speech_prob": 5.778257764177397e-05}, {"id": 208, "seek": 124048, "start": 1244.84, "end": 1249.44, "text": " of particularly fast calls, some examples of particularly slow calls and so forth.", "tokens": [295, 4098, 2370, 5498, 11, 512, 5110, 295, 4098, 2964, 5498, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12353143959401924, "compression_ratio": 1.75, "no_speech_prob": 5.778257764177397e-05}, {"id": 209, "seek": 124048, "start": 1249.44, "end": 1254.72, "text": " And if you, for example, take this dot up here, which is kind of slower than anything", "tokens": [400, 498, 291, 11, 337, 1365, 11, 747, 341, 5893, 493, 510, 11, 597, 307, 733, 295, 14009, 813, 1340], "temperature": 0.0, "avg_logprob": -0.12353143959401924, "compression_ratio": 1.75, "no_speech_prob": 5.778257764177397e-05}, {"id": 210, "seek": 124048, "start": 1254.72, "end": 1259.88, "text": " else, it's almost two seconds, right? Then you have the trace ID here, and you can navigate", "tokens": [1646, 11, 309, 311, 1920, 732, 3949, 11, 558, 30, 1396, 291, 362, 264, 13508, 7348, 510, 11, 293, 291, 393, 12350], "temperature": 0.0, "avg_logprob": -0.12353143959401924, "compression_ratio": 1.75, "no_speech_prob": 5.778257764177397e-05}, {"id": 211, "seek": 124048, "start": 1259.88, "end": 1265.2, "text": " to tempo and have a look at the trace and start figuring out why did I have an example", "tokens": [281, 8972, 293, 362, 257, 574, 412, 264, 13508, 293, 722, 15213, 484, 983, 630, 286, 362, 364, 1365], "temperature": 0.0, "avg_logprob": -0.12353143959401924, "compression_ratio": 1.75, "no_speech_prob": 5.778257764177397e-05}, {"id": 212, "seek": 126520, "start": 1265.2, "end": 1271.0, "text": " of such a slow call in my system, right? And in that case, you would immediately see that", "tokens": [295, 1270, 257, 2964, 818, 294, 452, 1185, 11, 558, 30, 400, 294, 300, 1389, 11, 291, 576, 4258, 536, 300], "temperature": 0.0, "avg_logprob": -0.12760632378714426, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.00020953880448359996}, {"id": 213, "seek": 126520, "start": 1271.0, "end": 1275.72, "text": " most of the time spent in the greeting service. So if you're looking for the performance bottleneck,", "tokens": [881, 295, 264, 565, 4418, 294, 264, 28174, 2643, 13, 407, 498, 291, 434, 1237, 337, 264, 3389, 44641, 547, 11], "temperature": 0.0, "avg_logprob": -0.12760632378714426, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.00020953880448359996}, {"id": 214, "seek": 126520, "start": 1275.72, "end": 1282.4, "text": " then this is the most likely thing. Yeah, four minutes, that's fine. Cool. So if I have", "tokens": [550, 341, 307, 264, 881, 3700, 551, 13, 865, 11, 1451, 2077, 11, 300, 311, 2489, 13, 8561, 13, 407, 498, 286, 362], "temperature": 0.0, "avg_logprob": -0.12760632378714426, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.00020953880448359996}, {"id": 215, "seek": 126520, "start": 1282.4, "end": 1289.0, "text": " four minutes, it's high time to jump to logs, the third signal that we didn't look at yet.", "tokens": [1451, 2077, 11, 309, 311, 1090, 565, 281, 3012, 281, 20820, 11, 264, 2636, 6358, 300, 321, 994, 380, 574, 412, 1939, 13], "temperature": 0.0, "avg_logprob": -0.12760632378714426, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.00020953880448359996}, {"id": 216, "seek": 128900, "start": 1289.0, "end": 1297.68, "text": " So let's select Loki, our open source logs database as a data source. So again, there's", "tokens": [407, 718, 311, 3048, 37940, 11, 527, 1269, 4009, 20820, 8149, 382, 257, 1412, 4009, 13, 407, 797, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.15424325173361259, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.9637454645126127e-05}, {"id": 217, "seek": 128900, "start": 1297.68, "end": 1302.88, "text": " a query language, there's a graphical query builder and so forth. So let's just open random", "tokens": [257, 14581, 2856, 11, 456, 311, 257, 35942, 14581, 27377, 293, 370, 5220, 13, 407, 718, 311, 445, 1269, 4974], "temperature": 0.0, "avg_logprob": -0.15424325173361259, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.9637454645126127e-05}, {"id": 218, "seek": 128900, "start": 1302.88, "end": 1309.44, "text": " logs coming from the greeting service. It looks a bit like this. So it's even, I don't", "tokens": [20820, 1348, 490, 264, 28174, 2643, 13, 467, 1542, 257, 857, 411, 341, 13, 407, 309, 311, 754, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.15424325173361259, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.9637454645126127e-05}, {"id": 219, "seek": 128900, "start": 1309.44, "end": 1313.8, "text": " know, I didn't even log anything explicitly. I just turned on some whatever spring request", "tokens": [458, 11, 286, 994, 380, 754, 3565, 1340, 20803, 13, 286, 445, 3574, 322, 512, 2035, 5587, 5308], "temperature": 0.0, "avg_logprob": -0.15424325173361259, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.9637454645126127e-05}, {"id": 220, "seek": 128900, "start": 1313.8, "end": 1318.52, "text": " logging so that I get some log data. And from time to time, I throw an exception, which", "tokens": [27991, 370, 300, 286, 483, 512, 3565, 1412, 13, 400, 490, 565, 281, 565, 11, 286, 3507, 364, 11183, 11, 597], "temperature": 0.0, "avg_logprob": -0.15424325173361259, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.9637454645126127e-05}, {"id": 221, "seek": 131852, "start": 1318.52, "end": 1323.8799999999999, "text": " is an IO exception to simulate these errors. Looks a bit broken, but that's just because", "tokens": [307, 364, 39839, 11183, 281, 27817, 613, 13603, 13, 10027, 257, 857, 5463, 11, 457, 300, 311, 445, 570], "temperature": 0.0, "avg_logprob": -0.1608450072152274, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.00020024733385071158}, {"id": 222, "seek": 131852, "start": 1323.8799999999999, "end": 1330.56, "text": " of the resolution that I have here. Yeah, so what you can do, of course, you can do some", "tokens": [295, 264, 8669, 300, 286, 362, 510, 13, 865, 11, 370, 437, 291, 393, 360, 11, 295, 1164, 11, 291, 393, 360, 512], "temperature": 0.0, "avg_logprob": -0.1608450072152274, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.00020024733385071158}, {"id": 223, "seek": 131852, "start": 1330.56, "end": 1339.12, "text": " full text search, for example, can say, I'm interested in these IO exception. And then", "tokens": [1577, 2487, 3164, 11, 337, 1365, 11, 393, 584, 11, 286, 478, 3102, 294, 613, 39839, 11183, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1608450072152274, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.00020024733385071158}, {"id": 224, "seek": 133912, "start": 1339.12, "end": 1348.8, "text": " you would basically get, well, if you spell it correctly, like that, then you would get", "tokens": [291, 576, 1936, 483, 11, 731, 11, 498, 291, 9827, 309, 8944, 11, 411, 300, 11, 550, 291, 576, 483], "temperature": 0.0, "avg_logprob": -0.12789810828442844, "compression_ratio": 1.6066176470588236, "no_speech_prob": 4.3967909732600674e-05}, {"id": 225, "seek": 133912, "start": 1348.8, "end": 1353.08, "text": " the list of all IO exceptions, which in my case are just the random errors I'm throwing", "tokens": [264, 1329, 295, 439, 39839, 22847, 11, 597, 294, 452, 1389, 366, 445, 264, 4974, 13603, 286, 478, 10238], "temperature": 0.0, "avg_logprob": -0.12789810828442844, "compression_ratio": 1.6066176470588236, "no_speech_prob": 4.3967909732600674e-05}, {"id": 226, "seek": 133912, "start": 1353.08, "end": 1357.4399999999998, "text": " here. And this query language is actually quite powerful. So you can, this is kind of", "tokens": [510, 13, 400, 341, 14581, 2856, 307, 767, 1596, 4005, 13, 407, 291, 393, 11, 341, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.12789810828442844, "compression_ratio": 1.6066176470588236, "no_speech_prob": 4.3967909732600674e-05}, {"id": 227, "seek": 133912, "start": 1357.4399999999998, "end": 1361.6, "text": " filtering by a label and filtering by full text search, but you can do totally different", "tokens": [30822, 538, 257, 7645, 293, 30822, 538, 1577, 2487, 3164, 11, 457, 291, 393, 360, 3879, 819], "temperature": 0.0, "avg_logprob": -0.12789810828442844, "compression_ratio": 1.6066176470588236, "no_speech_prob": 4.3967909732600674e-05}, {"id": 228, "seek": 133912, "start": 1361.6, "end": 1366.3999999999999, "text": " things as well. For example, you can have queries that, you know, derive metrics based", "tokens": [721, 382, 731, 13, 1171, 1365, 11, 291, 393, 362, 24109, 300, 11, 291, 458, 11, 28446, 16367, 2361], "temperature": 0.0, "avg_logprob": -0.12789810828442844, "compression_ratio": 1.6066176470588236, "no_speech_prob": 4.3967909732600674e-05}, {"id": 229, "seek": 136640, "start": 1366.4, "end": 1372.2800000000002, "text": " on log data. There's a function pretty similar to what we have seen in the metrics demo,", "tokens": [322, 3565, 1412, 13, 821, 311, 257, 2445, 1238, 2531, 281, 437, 321, 362, 1612, 294, 264, 16367, 10723, 11], "temperature": 0.0, "avg_logprob": -0.09658980095523527, "compression_ratio": 1.6777251184834123, "no_speech_prob": 9.712743485579267e-05}, {"id": 230, "seek": 136640, "start": 1372.2800000000002, "end": 1378.64, "text": " which is called the rate function. So the rate function, again, takes a time interval", "tokens": [597, 307, 1219, 264, 3314, 2445, 13, 407, 264, 3314, 2445, 11, 797, 11, 2516, 257, 565, 15035], "temperature": 0.0, "avg_logprob": -0.09658980095523527, "compression_ratio": 1.6777251184834123, "no_speech_prob": 9.712743485579267e-05}, {"id": 231, "seek": 136640, "start": 1378.64, "end": 1383.68, "text": " and then calculates the per second increase rate. So it basically tells you that we have", "tokens": [293, 550, 4322, 1024, 264, 680, 1150, 3488, 3314, 13, 407, 309, 1936, 5112, 291, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.09658980095523527, "compression_ratio": 1.6777251184834123, "no_speech_prob": 9.712743485579267e-05}, {"id": 232, "seek": 136640, "start": 1383.68, "end": 1391.2800000000002, "text": " almost 0.1 of these IO exceptions per second in our log data, which is also kind of useful", "tokens": [1920, 1958, 13, 16, 295, 613, 39839, 22847, 680, 1150, 294, 527, 3565, 1412, 11, 597, 307, 611, 733, 295, 4420], "temperature": 0.0, "avg_logprob": -0.09658980095523527, "compression_ratio": 1.6777251184834123, "no_speech_prob": 9.712743485579267e-05}, {"id": 233, "seek": 139128, "start": 1391.28, "end": 1398.76, "text": " for information to have. And the last thing to show you, because that's particularly interesting,", "tokens": [337, 1589, 281, 362, 13, 400, 264, 1036, 551, 281, 855, 291, 11, 570, 300, 311, 4098, 1880, 11], "temperature": 0.0, "avg_logprob": -0.1303549421594498, "compression_ratio": 1.6339285714285714, "no_speech_prob": 7.677018584217876e-05}, {"id": 234, "seek": 139128, "start": 1398.76, "end": 1405.96, "text": " so it is that these logs and traces and metrics are, again, not independent of each other.", "tokens": [370, 309, 307, 300, 613, 20820, 293, 26076, 293, 16367, 366, 11, 797, 11, 406, 6695, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.1303549421594498, "compression_ratio": 1.6339285714285714, "no_speech_prob": 7.677018584217876e-05}, {"id": 235, "seek": 139128, "start": 1405.96, "end": 1411.16, "text": " They are related to each other. And so if we look at an example here, just let's open", "tokens": [814, 366, 4077, 281, 1184, 661, 13, 400, 370, 498, 321, 574, 412, 364, 1365, 510, 11, 445, 718, 311, 1269], "temperature": 0.0, "avg_logprob": -0.1303549421594498, "compression_ratio": 1.6339285714285714, "no_speech_prob": 7.677018584217876e-05}, {"id": 236, "seek": 139128, "start": 1411.16, "end": 1417.44, "text": " a random log line. So what we see here, there's a trace ID. And this is interesting. So how", "tokens": [257, 4974, 3565, 1622, 13, 407, 437, 321, 536, 510, 11, 456, 311, 257, 13508, 7348, 13, 400, 341, 307, 1880, 13, 407, 577], "temperature": 0.0, "avg_logprob": -0.1303549421594498, "compression_ratio": 1.6339285714285714, "no_speech_prob": 7.677018584217876e-05}, {"id": 237, "seek": 141744, "start": 1417.44, "end": 1424.68, "text": " does a trace ID end up in my log line? So this is actually also a feature of the Java", "tokens": [775, 257, 13508, 7348, 917, 493, 294, 452, 3565, 1622, 30, 407, 341, 307, 767, 611, 257, 4111, 295, 264, 10745], "temperature": 0.0, "avg_logprob": -0.12063313413549352, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2012277693429496e-05}, {"id": 238, "seek": 141744, "start": 1424.68, "end": 1430.24, "text": " instrumentation that's provided by the OpenTelemetry community. So the way logging in general works", "tokens": [7198, 399, 300, 311, 5649, 538, 264, 7238, 14233, 306, 5537, 627, 1768, 13, 407, 264, 636, 27991, 294, 2674, 1985], "temperature": 0.0, "avg_logprob": -0.12063313413549352, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2012277693429496e-05}, {"id": 239, "seek": 141744, "start": 1430.24, "end": 1436.72, "text": " in Java is that there's a global thing with key value pairs called the log context. And", "tokens": [294, 10745, 307, 300, 456, 311, 257, 4338, 551, 365, 2141, 2158, 15494, 1219, 264, 3565, 4319, 13, 400], "temperature": 0.0, "avg_logprob": -0.12063313413549352, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2012277693429496e-05}, {"id": 240, "seek": 141744, "start": 1436.72, "end": 1441.1200000000001, "text": " applications can put arbitrary key value pairs into that context. And when you configure", "tokens": [5821, 393, 829, 23211, 2141, 2158, 15494, 666, 300, 4319, 13, 400, 562, 291, 22162], "temperature": 0.0, "avg_logprob": -0.12063313413549352, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2012277693429496e-05}, {"id": 241, "seek": 141744, "start": 1441.1200000000001, "end": 1446.6000000000001, "text": " your log format, you can define which of those values you want to include in your log data.", "tokens": [428, 3565, 7877, 11, 291, 393, 6964, 597, 295, 729, 4190, 291, 528, 281, 4090, 294, 428, 3565, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12063313413549352, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2012277693429496e-05}, {"id": 242, "seek": 144660, "start": 1446.6, "end": 1452.56, "text": " And if you have this OpenTelemetry agent attached, then as soon as a log line is written in the", "tokens": [400, 498, 291, 362, 341, 7238, 14233, 306, 5537, 627, 9461, 8570, 11, 550, 382, 2321, 382, 257, 3565, 1622, 307, 3720, 294, 264], "temperature": 0.0, "avg_logprob": -0.08527432690869581, "compression_ratio": 1.699248120300752, "no_speech_prob": 5.5223328672582284e-05}, {"id": 243, "seek": 144660, "start": 1452.56, "end": 1457.8, "text": " context of serving an HTTP request, then the corresponding trace ID is put into that log", "tokens": [4319, 295, 8148, 364, 33283, 5308, 11, 550, 264, 11760, 13508, 7348, 307, 829, 666, 300, 3565], "temperature": 0.0, "avg_logprob": -0.08527432690869581, "compression_ratio": 1.699248120300752, "no_speech_prob": 5.5223328672582284e-05}, {"id": 244, "seek": 144660, "start": 1457.8, "end": 1463.1999999999998, "text": " context. And you can configure your log format to include the trace ID in your log data.", "tokens": [4319, 13, 400, 291, 393, 22162, 428, 3565, 7877, 281, 4090, 264, 13508, 7348, 294, 428, 3565, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08527432690869581, "compression_ratio": 1.699248120300752, "no_speech_prob": 5.5223328672582284e-05}, {"id": 245, "seek": 144660, "start": 1463.1999999999998, "end": 1468.04, "text": " And that's what I did. And so each of my log lines actually has a trace ID. And so if I", "tokens": [400, 300, 311, 437, 286, 630, 13, 400, 370, 1184, 295, 452, 3565, 3876, 767, 575, 257, 13508, 7348, 13, 400, 370, 498, 286], "temperature": 0.0, "avg_logprob": -0.08527432690869581, "compression_ratio": 1.699248120300752, "no_speech_prob": 5.5223328672582284e-05}, {"id": 246, "seek": 144660, "start": 1468.04, "end": 1472.56, "text": " see something fancy and I want to know maybe somewhere down my distributed stack something", "tokens": [536, 746, 10247, 293, 286, 528, 281, 458, 1310, 4079, 760, 452, 12631, 8630, 746], "temperature": 0.0, "avg_logprob": -0.08527432690869581, "compression_ratio": 1.699248120300752, "no_speech_prob": 5.5223328672582284e-05}, {"id": 247, "seek": 147256, "start": 1472.56, "end": 1478.8, "text": " went wrong, I can just query that in tempo, navigate to the corresponding trace, close", "tokens": [1437, 2085, 11, 286, 393, 445, 14581, 300, 294, 8972, 11, 12350, 281, 264, 11760, 13508, 11, 1998], "temperature": 0.0, "avg_logprob": -0.14259815216064453, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.00023367715766653419}, {"id": 248, "seek": 147256, "start": 1478.8, "end": 1483.9199999999998, "text": " that here, yeah, and then basically maybe get some information what happened. And then", "tokens": [300, 510, 11, 1338, 11, 293, 550, 1936, 1310, 483, 512, 1589, 437, 2011, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.14259815216064453, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.00023367715766653419}, {"id": 249, "seek": 147256, "start": 1483.9199999999998, "end": 1488.28, "text": " the same navigation works the other way around as well. So of course, there's a little, you", "tokens": [264, 912, 17346, 1985, 264, 661, 636, 926, 382, 731, 13, 407, 295, 1164, 11, 456, 311, 257, 707, 11, 291], "temperature": 0.0, "avg_logprob": -0.14259815216064453, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.00023367715766653419}, {"id": 250, "seek": 147256, "start": 1488.28, "end": 1493.6799999999998, "text": " know, log button here. So if I see something fancy going on in my greeting service thing", "tokens": [458, 11, 3565, 2960, 510, 13, 407, 498, 286, 536, 746, 10247, 516, 322, 294, 452, 28174, 2643, 551], "temperature": 0.0, "avg_logprob": -0.14259815216064453, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.00023367715766653419}, {"id": 251, "seek": 147256, "start": 1493.6799999999998, "end": 1499.96, "text": " here, and maybe the logs have more information, I can click on that, navigate to the logs.", "tokens": [510, 11, 293, 1310, 264, 20820, 362, 544, 1589, 11, 286, 393, 2052, 322, 300, 11, 12350, 281, 264, 20820, 13], "temperature": 0.0, "avg_logprob": -0.14259815216064453, "compression_ratio": 1.7049808429118773, "no_speech_prob": 0.00023367715766653419}, {"id": 252, "seek": 149996, "start": 1499.96, "end": 1505.08, "text": " And then it basically just generates a query, right? I click on the greeting service with", "tokens": [400, 550, 309, 1936, 445, 23815, 257, 14581, 11, 558, 30, 286, 2052, 322, 264, 28174, 2643, 365], "temperature": 0.0, "avg_logprob": -0.12659578323364257, "compression_ratio": 1.6014492753623188, "no_speech_prob": 7.67839519539848e-05}, {"id": 253, "seek": 149996, "start": 1505.08, "end": 1510.0, "text": " that trace ID. So it's basically just a full text search for that trace ID. And so I will", "tokens": [300, 13508, 7348, 13, 407, 309, 311, 1936, 445, 257, 1577, 2487, 3164, 337, 300, 13508, 7348, 13, 400, 370, 286, 486], "temperature": 0.0, "avg_logprob": -0.12659578323364257, "compression_ratio": 1.6014492753623188, "no_speech_prob": 7.67839519539848e-05}, {"id": 254, "seek": 149996, "start": 1510.0, "end": 1515.28, "text": " find all my corresponding log lines. In that case, just one line. But if you have a bit", "tokens": [915, 439, 452, 11760, 3565, 3876, 13, 682, 300, 1389, 11, 445, 472, 1622, 13, 583, 498, 291, 362, 257, 857], "temperature": 0.0, "avg_logprob": -0.12659578323364257, "compression_ratio": 1.6014492753623188, "no_speech_prob": 7.67839519539848e-05}, {"id": 255, "seek": 149996, "start": 1515.28, "end": 1520.3600000000001, "text": " better logging, then maybe it would give you some indication what happened there. Okay.", "tokens": [1101, 27991, 11, 550, 1310, 309, 576, 976, 291, 512, 18877, 437, 2011, 456, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.12659578323364257, "compression_ratio": 1.6014492753623188, "no_speech_prob": 7.67839519539848e-05}, {"id": 256, "seek": 149996, "start": 1520.3600000000001, "end": 1527.8, "text": " So that was a very quick 25 minutes overview of, you know, looking a bit into metrics,", "tokens": [407, 300, 390, 257, 588, 1702, 3552, 2077, 12492, 295, 11, 291, 458, 11, 1237, 257, 857, 666, 16367, 11], "temperature": 0.0, "avg_logprob": -0.12659578323364257, "compression_ratio": 1.6014492753623188, "no_speech_prob": 7.67839519539848e-05}, {"id": 257, "seek": 152780, "start": 1527.8, "end": 1533.28, "text": " looking a bit into tracing, looking a bit into logs. I hope it gave you some impression,", "tokens": [1237, 257, 857, 666, 25262, 11, 1237, 257, 857, 666, 20820, 13, 286, 1454, 309, 2729, 291, 512, 9995, 11], "temperature": 0.0, "avg_logprob": -0.10642307115637738, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0002607846981845796}, {"id": 258, "seek": 152780, "start": 1533.28, "end": 1538.68, "text": " you know, what's the type of data that you get out of open telemetry looks like. All", "tokens": [291, 458, 11, 437, 311, 264, 2010, 295, 1412, 300, 291, 483, 484, 295, 1269, 4304, 5537, 627, 1542, 411, 13, 1057], "temperature": 0.0, "avg_logprob": -0.10642307115637738, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0002607846981845796}, {"id": 259, "seek": 152780, "start": 1538.68, "end": 1543.36, "text": " of what we did is really, you know, without even modifying the application. I didn't,", "tokens": [295, 437, 321, 630, 307, 534, 11, 291, 458, 11, 1553, 754, 42626, 264, 3861, 13, 286, 994, 380, 11], "temperature": 0.0, "avg_logprob": -0.10642307115637738, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0002607846981845796}, {"id": 260, "seek": 152780, "start": 1543.36, "end": 1548.3999999999999, "text": " you know, even start with custom metrics, custom traces and so forth. So but it's already", "tokens": [291, 458, 11, 754, 722, 365, 2375, 16367, 11, 2375, 26076, 293, 370, 5220, 13, 407, 457, 309, 311, 1217], "temperature": 0.0, "avg_logprob": -0.10642307115637738, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0002607846981845796}, {"id": 261, "seek": 152780, "start": 1548.3999999999999, "end": 1553.68, "text": " quite some useful data that we get out of that. If you like the demo, if you want to", "tokens": [1596, 512, 4420, 1412, 300, 321, 483, 484, 295, 300, 13, 759, 291, 411, 264, 10723, 11, 498, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.10642307115637738, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0002607846981845796}, {"id": 262, "seek": 155368, "start": 1553.68, "end": 1558.8400000000001, "text": " explore it a more, a bit more, want to try it at home, I pushed it on my GitHub and there's", "tokens": [6839, 309, 257, 544, 11, 257, 857, 544, 11, 528, 281, 853, 309, 412, 1280, 11, 286, 9152, 309, 322, 452, 23331, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.18217206434770064, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.00013526022667065263}, {"id": 263, "seek": 155368, "start": 1558.8400000000001, "end": 1566.5600000000002, "text": " a readme telling you how to run it. So you can do that. And yeah, next up, we have a", "tokens": [257, 1401, 1398, 3585, 291, 577, 281, 1190, 309, 13, 407, 291, 393, 360, 300, 13, 400, 1338, 11, 958, 493, 11, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.18217206434770064, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.00013526022667065263}, {"id": 264, "seek": 155368, "start": 1566.5600000000002, "end": 1572.04, "text": " talk that goes a bit more in detail into the tracing part of this. And then after that,", "tokens": [751, 300, 1709, 257, 857, 544, 294, 2607, 666, 264, 25262, 644, 295, 341, 13, 400, 550, 934, 300, 11], "temperature": 0.0, "avg_logprob": -0.18217206434770064, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.00013526022667065263}, {"id": 265, "seek": 155368, "start": 1572.04, "end": 1577.92, "text": " we have a talk that goes a bit more into detail how to run open telemetry in Kubernetes. So", "tokens": [321, 362, 257, 751, 300, 1709, 257, 857, 544, 666, 2607, 577, 281, 1190, 1269, 4304, 5537, 627, 294, 23145, 13, 407], "temperature": 0.0, "avg_logprob": -0.18217206434770064, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.00013526022667065263}, {"id": 266, "seek": 157792, "start": 1577.92, "end": 1588.04, "text": " stay here and thanks for listening. Please remain seated during Q&A. Otherwise, we can't", "tokens": [1754, 510, 293, 3231, 337, 4764, 13, 2555, 6222, 20959, 1830, 1249, 5, 32, 13, 10328, 11, 321, 393, 380], "temperature": 0.0, "avg_logprob": -0.30023756894198333, "compression_ratio": 1.3162393162393162, "no_speech_prob": 0.0007453093421645463}, {"id": 267, "seek": 157792, "start": 1588.04, "end": 1596.6000000000001, "text": " do a real Q&A. So please remain seated. Order any questions. Yes.", "tokens": [360, 257, 957, 1249, 5, 32, 13, 407, 1767, 6222, 20959, 13, 16321, 604, 1651, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.30023756894198333, "compression_ratio": 1.3162393162393162, "no_speech_prob": 0.0007453093421645463}, {"id": 268, "seek": 159660, "start": 1596.6, "end": 1609.12, "text": " Hi. Thank you for this. One quick question. You mentioned you just need to add some parameters", "tokens": [2421, 13, 1044, 291, 337, 341, 13, 1485, 1702, 1168, 13, 509, 2835, 291, 445, 643, 281, 909, 512, 9834], "temperature": 0.0, "avg_logprob": -0.1354158831314302, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.0012106237700209022}, {"id": 269, "seek": 159660, "start": 1609.12, "end": 1616.3999999999999, "text": " to the Java virtual machine to run the telemetry. What happens to my application if, for example,", "tokens": [281, 264, 10745, 6374, 3479, 281, 1190, 264, 4304, 5537, 627, 13, 708, 2314, 281, 452, 3861, 498, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1354158831314302, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.0012106237700209022}, {"id": 270, "seek": 159660, "start": 1616.3999999999999, "end": 1622.56, "text": " the back end of the telemetry is down? Is my application failing or impacted in any way?", "tokens": [264, 646, 917, 295, 264, 4304, 5537, 627, 307, 760, 30, 1119, 452, 3861, 18223, 420, 15653, 294, 604, 636, 30], "temperature": 0.0, "avg_logprob": -0.1354158831314302, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.0012106237700209022}, {"id": 271, "seek": 162256, "start": 1622.56, "end": 1628.24, "text": " If the monitoring back end is down. Yes. Say the monitoring is down, but I started my application", "tokens": [759, 264, 11028, 646, 917, 307, 760, 13, 1079, 13, 6463, 264, 11028, 307, 760, 11, 457, 286, 1409, 452, 3861], "temperature": 0.0, "avg_logprob": -0.1542365955856611, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.0003547726955730468}, {"id": 272, "seek": 162256, "start": 1628.24, "end": 1635.28, "text": " with these parameters. Is it impacting the application? No. I mean, you won't see metrics,", "tokens": [365, 613, 9834, 13, 1119, 309, 29963, 264, 3861, 30, 883, 13, 286, 914, 11, 291, 1582, 380, 536, 16367, 11], "temperature": 0.0, "avg_logprob": -0.1542365955856611, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.0003547726955730468}, {"id": 273, "seek": 162256, "start": 1635.28, "end": 1639.08, "text": " of course, if you're monitoring back end is down, but the application would just continue", "tokens": [295, 1164, 11, 498, 291, 434, 11028, 646, 917, 307, 760, 11, 457, 264, 3861, 576, 445, 2354], "temperature": 0.0, "avg_logprob": -0.1542365955856611, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.0003547726955730468}, {"id": 274, "seek": 162256, "start": 1639.08, "end": 1646.36, "text": " running. So typically, in like production setups, the applications wouldn't send telemetry", "tokens": [2614, 13, 407, 5850, 11, 294, 411, 4265, 46832, 11, 264, 5821, 2759, 380, 2845, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.1542365955856611, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.0003547726955730468}, {"id": 275, "seek": 162256, "start": 1646.36, "end": 1651.1599999999999, "text": " data directly to the monitoring back end. But what you usually have is something in", "tokens": [1412, 3838, 281, 264, 11028, 646, 917, 13, 583, 437, 291, 2673, 362, 307, 746, 294], "temperature": 0.0, "avg_logprob": -0.1542365955856611, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.0003547726955730468}, {"id": 276, "seek": 165116, "start": 1651.16, "end": 1656.44, "text": " the middle. There's alternatives. There's the Grafana agent that you can use for that.", "tokens": [264, 2808, 13, 821, 311, 20478, 13, 821, 311, 264, 8985, 69, 2095, 9461, 300, 291, 393, 764, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.11159036840711321, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.0003287334693595767}, {"id": 277, "seek": 165116, "start": 1656.44, "end": 1659.96, "text": " There's the open telemetry collector that you can use for that. And it's basically a", "tokens": [821, 311, 264, 1269, 4304, 5537, 627, 23960, 300, 291, 393, 764, 337, 300, 13, 400, 309, 311, 1936, 257], "temperature": 0.0, "avg_logprob": -0.11159036840711321, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.0003287334693595767}, {"id": 278, "seek": 165116, "start": 1659.96, "end": 1666.48, "text": " thing that runs close to the application, takes the telemetry data off the application", "tokens": [551, 300, 6676, 1998, 281, 264, 3861, 11, 2516, 264, 4304, 5537, 627, 1412, 766, 264, 3861], "temperature": 0.0, "avg_logprob": -0.11159036840711321, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.0003287334693595767}, {"id": 279, "seek": 165116, "start": 1666.48, "end": 1672.52, "text": " very quickly, and then, you know, can buffer stuff and process stuff and send it over to", "tokens": [588, 2661, 11, 293, 550, 11, 291, 458, 11, 393, 21762, 1507, 293, 1399, 1507, 293, 2845, 309, 670, 281], "temperature": 0.0, "avg_logprob": -0.11159036840711321, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.0003287334693595767}, {"id": 280, "seek": 165116, "start": 1672.52, "end": 1676.92, "text": " the monitoring back end. And that's used for decoupling that a little bit, right? And if", "tokens": [264, 11028, 646, 917, 13, 400, 300, 311, 1143, 337, 979, 263, 11970, 300, 257, 707, 857, 11, 558, 30, 400, 498], "temperature": 0.0, "avg_logprob": -0.11159036840711321, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.0003287334693595767}, {"id": 281, "seek": 167692, "start": 1676.92, "end": 1682.2, "text": " you have such an architecture, the application shouldn't be affected at all by that.", "tokens": [291, 362, 1270, 364, 9482, 11, 264, 3861, 4659, 380, 312, 8028, 412, 439, 538, 300, 13], "temperature": 0.0, "avg_logprob": -0.19547376827317842, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00024353503249585629}, {"id": 282, "seek": 167692, "start": 1682.2, "end": 1685.64, "text": " Two more. Two more.", "tokens": [4453, 544, 13, 4453, 544, 13], "temperature": 0.0, "avg_logprob": -0.19547376827317842, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00024353503249585629}, {"id": 283, "seek": 167692, "start": 1685.64, "end": 1694.2, "text": " So I really like being able to link from your metrics to traces. But what I'm actually", "tokens": [407, 286, 534, 411, 885, 1075, 281, 2113, 490, 428, 16367, 281, 26076, 13, 583, 437, 286, 478, 767], "temperature": 0.0, "avg_logprob": -0.19547376827317842, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00024353503249585629}, {"id": 284, "seek": 167692, "start": 1694.2, "end": 1700.04, "text": " really curious to be able to do, and as far as I know, doesn't exist, or I guess that's", "tokens": [534, 6369, 281, 312, 1075, 281, 360, 11, 293, 382, 1400, 382, 286, 458, 11, 1177, 380, 2514, 11, 420, 286, 2041, 300, 311], "temperature": 0.0, "avg_logprob": -0.19547376827317842, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00024353503249585629}, {"id": 285, "seek": 167692, "start": 1700.04, "end": 1704.0800000000002, "text": " my question, is like, is there any thought towards doing this, is being able to go the", "tokens": [452, 1168, 11, 307, 411, 11, 307, 456, 604, 1194, 3030, 884, 341, 11, 307, 885, 1075, 281, 352, 264], "temperature": 0.0, "avg_logprob": -0.19547376827317842, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00024353503249585629}, {"id": 286, "seek": 170408, "start": 1704.08, "end": 1712.52, "text": " other direction, where what I'd like to be able to answer is, here's all my trace data,", "tokens": [661, 3513, 11, 689, 437, 286, 1116, 411, 281, 312, 1075, 281, 1867, 307, 11, 510, 311, 439, 452, 13508, 1412, 11], "temperature": 0.0, "avg_logprob": -0.2074675496419271, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0003394011873751879}, {"id": 287, "seek": 170408, "start": 1712.52, "end": 1717.1999999999998, "text": " and this node of the trace incremented these counters by this much. So I could ask things", "tokens": [293, 341, 9984, 295, 264, 13508, 1946, 14684, 613, 39338, 538, 341, 709, 13, 407, 286, 727, 1029, 721], "temperature": 0.0, "avg_logprob": -0.2074675496419271, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0003394011873751879}, {"id": 288, "seek": 170408, "start": 1717.1999999999998, "end": 1725.28, "text": " like how much network IO or disk IOPS did this complete request do, and where in the", "tokens": [411, 577, 709, 3209, 39839, 420, 12355, 39839, 6273, 630, 341, 3566, 5308, 360, 11, 293, 689, 294, 264], "temperature": 0.0, "avg_logprob": -0.2074675496419271, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0003394011873751879}, {"id": 289, "seek": 170408, "start": 1725.28, "end": 1727.32, "text": " tree would that occur?", "tokens": [4230, 576, 300, 5160, 30], "temperature": 0.0, "avg_logprob": -0.2074675496419271, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0003394011873751879}, {"id": 290, "seek": 172732, "start": 1727.32, "end": 1737.8, "text": " Yeah, that's a good question. I mean, linking from traces to metrics, it's not so straightforward,", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13, 286, 914, 11, 25775, 490, 26076, 281, 16367, 11, 309, 311, 406, 370, 15325, 11], "temperature": 0.0, "avg_logprob": -0.11546888351440429, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.907228816999123e-05}, {"id": 291, "seek": 172732, "start": 1737.8, "end": 1743.48, "text": " because I think the things you can do to relate this is to use the service name. So if you", "tokens": [570, 286, 519, 264, 721, 291, 393, 360, 281, 10961, 341, 307, 281, 764, 264, 2643, 1315, 13, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.11546888351440429, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.907228816999123e-05}, {"id": 292, "seek": 172732, "start": 1743.48, "end": 1748.8799999999999, "text": " have the service name part of your resource attributes of the metrics, and consistently", "tokens": [362, 264, 2643, 1315, 644, 295, 428, 7684, 17212, 295, 264, 16367, 11, 293, 14961], "temperature": 0.0, "avg_logprob": -0.11546888351440429, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.907228816999123e-05}, {"id": 293, "seek": 172732, "start": 1748.8799999999999, "end": 1753.48, "text": " you have the same service name in your trace data, then you can at least, you know, navigate", "tokens": [291, 362, 264, 912, 2643, 1315, 294, 428, 13508, 1412, 11, 550, 291, 393, 412, 1935, 11, 291, 458, 11, 12350], "temperature": 0.0, "avg_logprob": -0.11546888351440429, "compression_ratio": 1.7370892018779343, "no_speech_prob": 6.907228816999123e-05}, {"id": 294, "seek": 175348, "start": 1753.48, "end": 1759.0, "text": " to all traces coming, to all metrics coming from the same service. Maybe you have some", "tokens": [281, 439, 26076, 1348, 11, 281, 439, 16367, 1348, 490, 264, 912, 2643, 13, 2704, 291, 362, 512], "temperature": 0.0, "avg_logprob": -0.35387949789724044, "compression_ratio": 1.486842105263158, "no_speech_prob": 0.0001640903647057712}, {"id": 295, "seek": 175348, "start": 1759.0, "end": 1764.4, "text": " more, you know, related attributes, like in whatever instance ID and so forth. But it's", "tokens": [544, 11, 291, 458, 11, 4077, 17212, 11, 411, 294, 2035, 5197, 7348, 293, 370, 5220, 13, 583, 309, 311], "temperature": 0.0, "avg_logprob": -0.35387949789724044, "compression_ratio": 1.486842105263158, "no_speech_prob": 0.0001640903647057712}, {"id": 296, "seek": 175348, "start": 1764.4, "end": 1766.96, "text": " not like really a one-to-one relationship, so.", "tokens": [406, 411, 534, 257, 472, 12, 1353, 12, 546, 2480, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.35387949789724044, "compression_ratio": 1.486842105263158, "no_speech_prob": 0.0001640903647057712}, {"id": 297, "seek": 175348, "start": 1766.96, "end": 1772.64, "text": " That's specific. What request, how much did this request come from the IOPS?", "tokens": [663, 311, 2685, 13, 708, 5308, 11, 577, 709, 630, 341, 5308, 808, 490, 264, 39839, 6273, 30], "temperature": 0.0, "avg_logprob": -0.35387949789724044, "compression_ratio": 1.486842105263158, "no_speech_prob": 0.0001640903647057712}, {"id": 298, "seek": 175348, "start": 1772.64, "end": 1776.96, "text": " Yeah, no, I don't think that's possible.", "tokens": [865, 11, 572, 11, 286, 500, 380, 519, 300, 311, 1944, 13], "temperature": 0.0, "avg_logprob": -0.35387949789724044, "compression_ratio": 1.486842105263158, "no_speech_prob": 0.0001640903647057712}, {"id": 299, "seek": 177696, "start": 1776.96, "end": 1783.68, "text": " So in this example, you've shown that Grafana World and Prometheus works great with server-side", "tokens": [407, 294, 341, 1365, 11, 291, 600, 4898, 300, 8985, 69, 2095, 3937, 293, 2114, 649, 42209, 1985, 869, 365, 7154, 12, 1812], "temperature": 0.0, "avg_logprob": -0.3018063254978346, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00016491106362082064}, {"id": 300, "seek": 177696, "start": 1783.68, "end": 1792.04, "text": " applications. Have you had examples of client-side applications, mobile desktop applications that", "tokens": [5821, 13, 3560, 291, 632, 5110, 295, 6423, 12, 1812, 5821, 11, 6013, 14502, 5821, 300], "temperature": 0.0, "avg_logprob": -0.3018063254978346, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00016491106362082064}, {"id": 301, "seek": 177696, "start": 1792.04, "end": 1800.64, "text": " use Prometheus metrics and then ship their trace, their metrics and traces to the metric", "tokens": [764, 2114, 649, 42209, 16367, 293, 550, 5374, 641, 13508, 11, 641, 16367, 293, 26076, 281, 264, 20678], "temperature": 0.0, "avg_logprob": -0.3018063254978346, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00016491106362082064}, {"id": 302, "seek": 177696, "start": 1800.64, "end": 1801.64, "text": " backend?", "tokens": [38087, 30], "temperature": 0.0, "avg_logprob": -0.3018063254978346, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.00016491106362082064}, {"id": 303, "seek": 180164, "start": 1801.64, "end": 1807.96, "text": " Did I hear it correctly? You're asking about starting your traces on the client-side and", "tokens": [2589, 286, 1568, 309, 8944, 30, 509, 434, 3365, 466, 2891, 428, 26076, 322, 264, 6423, 12, 1812, 293], "temperature": 0.0, "avg_logprob": -0.21825746389535758, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00032142348936758935}, {"id": 304, "seek": 180164, "start": 1807.96, "end": 1810.3200000000002, "text": " the web browser and stuff?", "tokens": [264, 3670, 11185, 293, 1507, 30], "temperature": 0.0, "avg_logprob": -0.21825746389535758, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00032142348936758935}, {"id": 305, "seek": 180164, "start": 1810.3200000000002, "end": 1815.48, "text": " You have tracing on the server-side, but what about having traces and metrics on the client-side", "tokens": [509, 362, 25262, 322, 264, 7154, 12, 1812, 11, 457, 437, 466, 1419, 26076, 293, 16367, 322, 264, 6423, 12, 1812], "temperature": 0.0, "avg_logprob": -0.21825746389535758, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00032142348936758935}, {"id": 306, "seek": 180164, "start": 1815.48, "end": 1820.48, "text": " and, for example, for an embedded or mobile application so that you could actually see", "tokens": [293, 11, 337, 1365, 11, 337, 364, 16741, 420, 6013, 3861, 370, 300, 291, 727, 767, 536], "temperature": 0.0, "avg_logprob": -0.21825746389535758, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00032142348936758935}, {"id": 307, "seek": 180164, "start": 1820.48, "end": 1827.3600000000001, "text": " the trace from when the customer clicked a thing and see the full customer journey?", "tokens": [264, 13508, 490, 562, 264, 5474, 23370, 257, 551, 293, 536, 264, 1577, 5474, 4671, 30], "temperature": 0.0, "avg_logprob": -0.21825746389535758, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00032142348936758935}, {"id": 308, "seek": 182736, "start": 1827.36, "end": 1832.6799999999998, "text": " Yeah, that's a great question. That's actually an area where there's currently a lot of research", "tokens": [865, 11, 300, 311, 257, 869, 1168, 13, 663, 311, 767, 364, 1859, 689, 456, 311, 4362, 257, 688, 295, 2132], "temperature": 0.0, "avg_logprob": -0.1855300294251001, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0003170384152326733}, {"id": 309, "seek": 182736, "start": 1832.6799999999998, "end": 1838.36, "text": " and new projects and so forth. So there is a group called real-user monitoring, RUM,", "tokens": [293, 777, 4455, 293, 370, 5220, 13, 407, 456, 307, 257, 1594, 1219, 957, 12, 18088, 11028, 11, 497, 14340, 11], "temperature": 0.0, "avg_logprob": -0.1855300294251001, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0003170384152326733}, {"id": 310, "seek": 182736, "start": 1838.36, "end": 1845.24, "text": " in open telemetry that deal with client-side applications. There's also a project by Grafana.", "tokens": [294, 1269, 4304, 5537, 627, 300, 2028, 365, 6423, 12, 1812, 5821, 13, 821, 311, 611, 257, 1716, 538, 8985, 69, 2095, 13], "temperature": 0.0, "avg_logprob": -0.1855300294251001, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0003170384152326733}, {"id": 311, "seek": 182736, "start": 1845.24, "end": 1850.84, "text": " It's called Faro. It's kind of, you know, JavaScript that you can include in your front", "tokens": [467, 311, 1219, 9067, 78, 13, 467, 311, 733, 295, 11, 291, 458, 11, 15778, 300, 291, 393, 4090, 294, 428, 1868], "temperature": 0.0, "avg_logprob": -0.1855300294251001, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0003170384152326733}, {"id": 312, "seek": 182736, "start": 1850.84, "end": 1857.32, "text": " end, in your HTML page, and then it gives you traces and metrics from in the web browser", "tokens": [917, 11, 294, 428, 17995, 3028, 11, 293, 550, 309, 2709, 291, 26076, 293, 16367, 490, 294, 264, 3670, 11185], "temperature": 0.0, "avg_logprob": -0.1855300294251001, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0003170384152326733}, {"id": 313, "seek": 185732, "start": 1857.32, "end": 1864.28, "text": " coming from the web browser. And this is currently a pretty active area, so lots of, you know,", "tokens": [1348, 490, 264, 3670, 11185, 13, 400, 341, 307, 4362, 257, 1238, 4967, 1859, 11, 370, 3195, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 314, "seek": 185732, "start": 1864.28, "end": 1865.28, "text": " movement there.", "tokens": [3963, 456, 13], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 315, "seek": 185732, "start": 1865.28, "end": 1871.56, "text": " And so there are things to explore. So if you like, check out Faro. It's a nice new project", "tokens": [400, 370, 456, 366, 721, 281, 6839, 13, 407, 498, 291, 411, 11, 1520, 484, 9067, 78, 13, 467, 311, 257, 1481, 777, 1716], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 316, "seek": 185732, "start": 1871.56, "end": 1878.32, "text": " and standardization is also currently being discussed, but it's newer than the rest of", "tokens": [293, 3832, 2144, 307, 611, 4362, 885, 7152, 11, 457, 309, 311, 17628, 813, 264, 1472, 295], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 317, "seek": 185732, "start": 1878.32, "end": 1883.32, "text": " what I showed you, right? So it's not as, so there's no, you know, clear standard yet", "tokens": [437, 286, 4712, 291, 11, 558, 30, 407, 309, 311, 406, 382, 11, 370, 456, 311, 572, 11, 291, 458, 11, 1850, 3832, 1939], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 318, "seek": 185732, "start": 1883.32, "end": 1885.32, "text": " or nothing decided yet.", "tokens": [420, 1825, 3047, 1939, 13], "temperature": 0.0, "avg_logprob": -0.20752493832089486, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.00016535235045012087}, {"id": 319, "seek": 188532, "start": 1885.32, "end": 1888.32, "text": " Cool. Okay. Thanks, everyone, again.", "tokens": [50364, 8561, 13, 1033, 13, 2561, 11, 1518, 11, 797, 13, 50514], "temperature": 0.0, "avg_logprob": -0.39904367006742036, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.0014603391755372286}], "language": "en"}