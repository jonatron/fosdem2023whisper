{"text": " Hello, everyone. Do you hear me well? Thanks, pretty large audience. If I may ask a quick show of hands, who among you have some experience, just any level of experience with machine learning? Okay, cool. Awesome. So, I'll be talking today about how to run testing on machine learning systems. So, there are different keywords, CICD, quality assurance. A few words about us. So, I'm one of the founders of Giscard. We are building a collaborative and open source software platform to precisely ensure the quality of AI models. And I'll be explaining in this presentation a bit how it works. In terms of agenda, I prepared kind of two sections on the why, like why a project on testing machine learning systems is needed, why we personally, I personally decided to work on that problem. Some of the risks and why classical software testing methods don't quite work on AI. And then I'll do some more concrete examples on two important quality criteria that you might want to test for machine learning. One is robustness and the other is furnace. And if we have the time, it's just 30 minutes. I hope that we can do a quick demo of an example use case where we run the full CICD pipeline on a machine learning model. So, to kind of start off easy, I put together a series of memes to explain my personal story of why I came to create a company around, and a project around this machine learning testing thing. So, about 10 years ago, I started in machine learning, statistics, data science, and you know, you had this, you start using the scikit learn API, and you're like, yeah, it's super easy, right? Anybody can be a data scientist, you just dot fit, dot predict, and that's it. You're a data scientist. And probably if you're here today, you're like, yeah, have you tested your model? Yeah, sure. Train test, yeah. Reality, if you've deployed in production, is quite different. So, if you've deployed through production, often you'll have this painful discovery where you have your product manager, business stakeholders to whom you said, look, I worked really hard on the fine tuning and the grid search to get to 85% accuracy, and you push your first version to production, and things don't quite work out. You don't reproduce these good accuracy numbers. So, well, this was me. I hope it's not you. It was one of these, my first experience deploying machine learning through production was on a fraud detection system. So, frauds are notoriously difficult as a use case for machine learning because what you're trying to detect doesn't quite want to be detected. There are people behind it who have a vested interest not to have your machine learning system detect them. So, often in terms of performance, that's at least what I ended up doing a lot of hot fixes in production. It's bad. So, kind of like five years ago, this was my stance on machine learning in production, a very painful grueling experience where you never know when you're going to be a complain, where you're going to be on call to solve something in production. So, that's when I decided to buff up and switch roles to join a software engineering team. I was a data crew back then, so I moved internally from data science to the product team, and here are some of the things to summarize that as someone with a machine learning background, but no real software engineering experience that these were kind of like what I was told, like you must learn the ways of a CI CD, otherwise your project will not come to production. And for context, so I was specifically at that time in charge of creating like an open source layer to federate all the NLP and computer vision APIs that vendors in the cloud provide, and then to do the same for pre-trained NLP and time series models. So, what was difficult in this context is I was not even the one in charge of the models, and the models will be retrained fine-tuned, so the guarantees into the properties of that system as an engineer, that's more difficult. There are some elements in the stack that you don't have control of. So, yeah, this is a bit of a repeat of a previous meme, and I really wanted to say one does not simply ship an ML product without tests. The challenge I had then is that from an engineering management standpoint, I was told, yeah, but you know, it's easy, no engineers, they all write their test cases, so you do machine learning, just write them all, just write all the test cases. So, this was me being kind of a square one. It's like, okay, so you're telling me, I just need to write unit tests, okay, that will not really solve the issue, and that's kind of the beginning of a quest that set me on to build something to solve that gap between, okay, I want to test my models, I need to test my models, and how do, how can I do that? Because clearly, and I'll explain why, unit testing, your model is really not enough. So, a different angle on the Y, I'll try to take a step back and talk about quality in general. I think in this track, we all agree that quality matters, and if you look at AI, it's an industry that's an engineering practice that is far younger than software engineering or civil engineering, and it's just riddled with incidents. I encourage you, if you don't know that resource already, it's an open source database, it's incidentdatabase.ai, and it's a public collection of reports, mostly in the media, of AI models that have not worked properly, and it's a really great work that has been going on for about two years and a half. It's a global initiative, and just in this time, they collected more than 2,000 incidents. Since these are public reports, think of it as the tip of the iceberg, of course. There are a lot of incidents internal to companies that are not necessarily spoken out in the media. The incident database has a very interesting taxonomy of the different types of incidents. It's very multifaceted. I took the liberty to simplify it in three big categories of incidents. One is FX, the other is business economic impact, and the third one is on security. We're talking about really, if they happen at a global company scale, incidents that are very, very severe. In FX, you can have a sexist credit scoring algorithm that exposes the company to lawsuits, to brand image damages, et cetera. These are notoriously hard to identify. In a way, machine learning is precisely about discrimination. It's hard to tell a machine that is learning to discriminate, not to discriminate against certain sensitive groups. I'll speak on some methods that can be used precisely on this problem, but Apple was working with at the time Goldman Sachs on deploying this algorithm and probably some tests and safeguards were unfortunately skipped. It was actually discovered on Twitter that in a simple case, a male loan applicant would get 10x their loan limit compared to his wife. That sparked a huge controversy that probably exposed Apple to some lawsuits. In another area, that is not with sensitive features such as gender. There was a huge catastrophe a year and a half ago that happened to Zillow, a real estate company, where there was a small bias that was overestimating the prices of homes. They decided to put this algorithm live to buy and sell houses. It turned out that this tiny bias, which was left unchecked, was exploited by the real estate agents in the US. Literally, this created a loss of nearly half a billion dollars. Again, maybe if going back to testing, this could have been unseapated and avoided. Now on a more cybersecurity spectrum, there's a lot of good research from cybersecurity labs showing that you can hack, for example, a computer vision system in an autonomous driving context. Here you put a special tape on the road and you can crash a Tesla. We don't quite know if these types of vulnerabilities have been exploited in real life yet, but as AI becomes super ubiquitous, and obviously there are some bad actors out there that might want to hack these systems and introduces a new type of attack vectors. That's also something we need to care about. Both from the practitioners of AI and from a regulatory standpoint, testing just makes sense. Yanlequin, chief AI scientist at META, was actually taking a stance at the beginning of last year on Twitter saying that if you won't trust in a system, you need tests. Also making a slight criticism towards some of the explainability methods, because two years ago, if you've followed that realm, people were saying, oh, you just need explainability and then your problems will go away. Well, that's just part of the answer. Lastly, and this was covered in some of the talks this morning on the big auditorium, there's a growing regulatory requirement to put some checks and balances in place. That also says that you need specifically in case your AI system is high risk, you need to put quality measures in place. The definition of high risk AI systems is pretty large. Obviously, you have anything related to infrastructure, like critical infrastructure, defense, et cetera, but you also have all AI systems that are involved in human resources and public service and financial services, because these are considered, obviously, critical components of society. Now that we kind of agreed that it's an important problem, these are some of the challenges, because if you've encountered some of these issues, you probably looked at some easy solutions, taking some analogies on what you might do to do this. There are three points that make this problem of testing machine learning a bit special, meaning it's still a big work in progress. Point one is that it is really not enough to check the data quality to guarantee the quality of a machine learning system. One of my co-founders doing his PhD proved experimentally, you can run experiments, you can have really clean data in a bad model. So you cannot just say it's an upstream problem, it's technically like systems engineering, you have to take the data, the machine learning model, and the user into context to analyze its properties. Moreover, the errors of the machine learning systems are often caused by pieces of data that did not exist when the model was created, they were clean, but they did not exist. Second point, it's pretty hard just to copy-paste some of the testing methods from software into AI. One is like, yes, you can do some unit tests on machine learning models, but they won't prove much. Because the principle is that it's a transactional system and things are moving quite a lot. So that's a good baseline. If you have a machine learning system and you have some unit tests, that's really like step one. It's better to have that than to have nothing. But you have to embrace the fact that there has got to be a large number of test cases. So you cannot just test on three, five, hundred, even a thousand cases will not be enough. The models themselves are probabilistic. So you have to take into account statistical methods of tests. And lastly, and I think this is specific to, because there has been some systems that were heavily dependent on data, but with AI, AI also came with a fact that you increase the number of data inputs compared to traditional systems. So you very quickly come into issues of, well, it's a combinatorial problem, and it's factually impossible to generate all the combinations. Very simple example of that. How can you test an NLP system? Lastly, like AI touching a lot of different points. If you want to have a complete test coverage, you really need to take into account multiple criteria. So performance of a system, but also robustness, robustness to errors, fairness, privacy, security, reliability. And also, and that's becoming an increasingly important topic with green AI, it's like what is the carbon impact of this AI? Do you really need that many GPUs? Can you make your system a bit more energy efficient? So today I'll focus on the, because I see we have 10 more minutes, I'll focus on two aspects, the robustness and the effects. So I'll start with robustness. Who has read or heard about this paper? Quick show of hands. Okay, one. So who has heard of behavioral testing? Because that's not machine learning specific. Yeah, cool. So Ribeiro three years ago, along with other co-writers of this paper, did I think a fantastic job to see how to adapt behavioral testing, which is a really good practice from software engineering, to the context of machine learning. And specifically wrote something for NLP models. The main problem that this research paper aimed to solve was test case generation. Because really NLP is by a sense a problem, NLP, a natural language processing. So you have an input text, it's just raw text. So you need to test this. But what you can do is to generate test cases that rely on mapping the input and the input changing changes in the text to expectations. I'll give three examples from very, very simple to a bit more complex. One is like the principle of minimum functionality. For example, if you are building a sentiment prediction machine learning system, you could just have a test that says if you have extraordinary in the sentence, you should always predict that the model will say it's a positive message. Now you will probably tell me, yeah, but what about if the user has written it's not extraordinary or absolutely not extraordinary? And that actually brings me to the concept of test template. And the fact that probably for NLP, what you need to do, and this is obviously language specific, is start to have templates where you change the text by, for example, adding negations. And then so you might want to test if your system, if you're adding negation, if you have a certain direction. Because normally if the machine learning model has understood, it should, if it's about sentiment, understand that putting not an extraordinary or not good, you have then synonyms, will not affect the system too much. Or actually either your system, you want it to move to a certain direction or there are cases where you want actually the opposite behavior. You want robustness. So that's called invariance. So for instance, you will want a system that is robust to typos to just changing like a location name, just putting synonyms, et cetera, et cetera. So we've created this diagram to explain it. And it's a really thriving field in the research. There is a lot of research going on these days about testing machine learning systems. And metamorphic testing is one of the leading methods to do that. The principle is, if I take an analogy, is very similar to if you've worked in finance or if you have some friends who work there, the principle of backtesting an investment strategy. You simulate different changes in the market conditions and you see how your strategy, your algorithm behaves, what is the variance of that strategy. This concept applies very well to machine learning. So you need two things. You need one to define a perturbation. So what I was explaining earlier in NLP, perturbation might be adding typos, adding negation. In another context, like let's say it's more in an industrial case, it might be about doubling the values of some sensors or adding noise to an image. And then, pretty simply, you define a test expectation in terms of the metamorphic relation between the output of a machine learning model and the distribution of the output after perturbation. And once you have that, and if you have enough data, then you can actually have, like you can do actual statistical tests, see there's a difference in distribution, et cetera. So I won't have too much time to dive into all the details of this, but we have wrote a technical guide on this topic and you have a link in QR code up there. Next, I'll talk a bit about a really tricky topic, which is AI fairness. And I want to emphasize that it's, at least our recommendation, is not to come at the problem of AI ethics with a closed mind or a top-down definition of this is an ethical system or no, this is an unethical system. My co-founder did his PhD on precisely on this topic and wrote a full paper on this, looking at the philosophical and sociological implications of this. And the gist of it is that, yes, to a certain extent, you can adopt a top-down approach to AI fairness, saying, well, for instance, as an organization, we want to test the fairness on explicitly free, sensitive categories. You can say, well, we want to check for gender balance. We want to check for race balance. That means if the country where you deploy a machine learning allows to collect this data, this is not always the case. But the challenge with these approaches is that, A, you might not have the data to measure this, and B, you may miss out because often when this exercise of defining the quality criteria for fairness and for balance are done, you only have a limited sample. So it's, in taking some sociological analysis, it's really important to have this kind of top-down definition of AI ethics, meet the reality on the ground, and confront the actual users and the makers of the systems to get them to define the definition of ethics, rather than a big organization, if I put a bit of a caricature that says, AI ethics, yeah, we wrote a charter about this. You follow, you read this, you sign, and then, oops, you're ethical. Having said that, so there are some good top-down metrics to adopt that are kind of a baseline, and I'll explain one of them, which is disparate impact. Disparate impact is actually a metric from the human resources management industry from at least 40 years ago, so it's not new. That says, so it's probabilities, but essentially it's about setting a rule of 80%, where you measure the probability of, you define a positive outcome with respect to a given protected population, and you say, well, I want to the proportion of the probability of a positive outcome relative to the probability of a positive outcome in the unprotected context to be above 80%. So, for instance, so if you want to apply that to a, oops, to put more concrete, yeah, so if, say, you're building a model to predict the churn of customers, and you want to check whether your model is biased or not for each class, this formula allows you to really define this metric and write a concrete test case. Right, so I just have three minutes, so I'll highlight what one of the features of our project enables is putting human feedback, so really having an interface where users and not only data scientists can change the parameters, so there's a link to metamorphic testing, and actually give human feedback to a point art where the biases may be, and the benefit of this approach is that it allows for the community to precisely define what they think are the risks. So sadly, we won't have time to do a demo, but this phase, in our project, we call that the inspection phase, and it's about before you test, and this is super important, and again, one of the things where it's different from traditional software testing, before you even test, you need to confront yourself with the data and the model, so that's where actually we think explainability methods really shine, it's because they allow to debug and to identify the zones of risks, and this is precisely what helps once you have qualified feedback to know where you should put your effort in test, so in a nutshell what I'm saying for testing machine learning systems is it's not a matter of creating hundreds of tests, of automating everything, but rather to have a good idea of, from a fairness standpoint and for a performance standpoint, of what are the 10, 15, maybe max 20 tests that you want in your platform. If you want to get started actually on it, this is our GitHub, and if you have a machine learning system to test, we're interested in your feedback.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.12, "text": " Hello, everyone. Do you hear me well? Thanks, pretty large audience. If I may ask a quick", "tokens": [2425, 11, 1518, 13, 1144, 291, 1568, 385, 731, 30, 2561, 11, 1238, 2416, 4034, 13, 759, 286, 815, 1029, 257, 1702], "temperature": 0.0, "avg_logprob": -0.20411219804183298, "compression_ratio": 1.3284671532846715, "no_speech_prob": 0.10358942300081253}, {"id": 1, "seek": 0, "start": 16.12, "end": 22.2, "text": " show of hands, who among you have some experience, just any level of experience with machine", "tokens": [855, 295, 2377, 11, 567, 3654, 291, 362, 512, 1752, 11, 445, 604, 1496, 295, 1752, 365, 3479], "temperature": 0.0, "avg_logprob": -0.20411219804183298, "compression_ratio": 1.3284671532846715, "no_speech_prob": 0.10358942300081253}, {"id": 2, "seek": 2220, "start": 22.2, "end": 32.8, "text": " learning? Okay, cool. Awesome. So, I'll be talking today about how to run testing on", "tokens": [2539, 30, 1033, 11, 1627, 13, 10391, 13, 407, 11, 286, 603, 312, 1417, 965, 466, 577, 281, 1190, 4997, 322], "temperature": 0.0, "avg_logprob": -0.2034806290717974, "compression_ratio": 1.4093264248704662, "no_speech_prob": 0.0003320953110232949}, {"id": 3, "seek": 2220, "start": 32.8, "end": 38.96, "text": " machine learning systems. So, there are different keywords, CICD, quality assurance. A few words", "tokens": [3479, 2539, 3652, 13, 407, 11, 456, 366, 819, 21009, 11, 383, 2532, 35, 11, 3125, 32189, 13, 316, 1326, 2283], "temperature": 0.0, "avg_logprob": -0.2034806290717974, "compression_ratio": 1.4093264248704662, "no_speech_prob": 0.0003320953110232949}, {"id": 4, "seek": 2220, "start": 38.96, "end": 46.480000000000004, "text": " about us. So, I'm one of the founders of Giscard. We are building a collaborative and open", "tokens": [466, 505, 13, 407, 11, 286, 478, 472, 295, 264, 25608, 295, 460, 5606, 515, 13, 492, 366, 2390, 257, 16555, 293, 1269], "temperature": 0.0, "avg_logprob": -0.2034806290717974, "compression_ratio": 1.4093264248704662, "no_speech_prob": 0.0003320953110232949}, {"id": 5, "seek": 4648, "start": 46.48, "end": 52.76, "text": " source software platform to precisely ensure the quality of AI models. And I'll be explaining", "tokens": [4009, 4722, 3663, 281, 13402, 5586, 264, 3125, 295, 7318, 5245, 13, 400, 286, 603, 312, 13468], "temperature": 0.0, "avg_logprob": -0.1679701402962926, "compression_ratio": 1.5665236051502145, "no_speech_prob": 3.566746454453096e-05}, {"id": 6, "seek": 4648, "start": 52.76, "end": 61.08, "text": " in this presentation a bit how it works. In terms of agenda, I prepared kind of two sections", "tokens": [294, 341, 5860, 257, 857, 577, 309, 1985, 13, 682, 2115, 295, 9829, 11, 286, 4927, 733, 295, 732, 10863], "temperature": 0.0, "avg_logprob": -0.1679701402962926, "compression_ratio": 1.5665236051502145, "no_speech_prob": 3.566746454453096e-05}, {"id": 7, "seek": 4648, "start": 61.08, "end": 67.12, "text": " on the why, like why a project on testing machine learning systems is needed, why we", "tokens": [322, 264, 983, 11, 411, 983, 257, 1716, 322, 4997, 3479, 2539, 3652, 307, 2978, 11, 983, 321], "temperature": 0.0, "avg_logprob": -0.1679701402962926, "compression_ratio": 1.5665236051502145, "no_speech_prob": 3.566746454453096e-05}, {"id": 8, "seek": 4648, "start": 67.12, "end": 75.84, "text": " personally, I personally decided to work on that problem. Some of the risks and why classical", "tokens": [5665, 11, 286, 5665, 3047, 281, 589, 322, 300, 1154, 13, 2188, 295, 264, 10888, 293, 983, 13735], "temperature": 0.0, "avg_logprob": -0.1679701402962926, "compression_ratio": 1.5665236051502145, "no_speech_prob": 3.566746454453096e-05}, {"id": 9, "seek": 7584, "start": 75.84, "end": 82.76, "text": " software testing methods don't quite work on AI. And then I'll do some more concrete", "tokens": [4722, 4997, 7150, 500, 380, 1596, 589, 322, 7318, 13, 400, 550, 286, 603, 360, 512, 544, 9859], "temperature": 0.0, "avg_logprob": -0.11962659623887804, "compression_ratio": 1.5274261603375527, "no_speech_prob": 7.746808114461601e-05}, {"id": 10, "seek": 7584, "start": 82.76, "end": 90.84, "text": " examples on two important quality criteria that you might want to test for machine learning.", "tokens": [5110, 322, 732, 1021, 3125, 11101, 300, 291, 1062, 528, 281, 1500, 337, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11962659623887804, "compression_ratio": 1.5274261603375527, "no_speech_prob": 7.746808114461601e-05}, {"id": 11, "seek": 7584, "start": 90.84, "end": 96.44, "text": " One is robustness and the other is furnace. And if we have the time, it's just 30 minutes.", "tokens": [1485, 307, 13956, 1287, 293, 264, 661, 307, 34046, 13, 400, 498, 321, 362, 264, 565, 11, 309, 311, 445, 2217, 2077, 13], "temperature": 0.0, "avg_logprob": -0.11962659623887804, "compression_ratio": 1.5274261603375527, "no_speech_prob": 7.746808114461601e-05}, {"id": 12, "seek": 7584, "start": 96.44, "end": 104.28, "text": " I hope that we can do a quick demo of an example use case where we run the full CICD pipeline", "tokens": [286, 1454, 300, 321, 393, 360, 257, 1702, 10723, 295, 364, 1365, 764, 1389, 689, 321, 1190, 264, 1577, 383, 2532, 35, 15517], "temperature": 0.0, "avg_logprob": -0.11962659623887804, "compression_ratio": 1.5274261603375527, "no_speech_prob": 7.746808114461601e-05}, {"id": 13, "seek": 10428, "start": 104.28, "end": 113.12, "text": " on a machine learning model. So, to kind of start off easy, I put together a series of", "tokens": [322, 257, 3479, 2539, 2316, 13, 407, 11, 281, 733, 295, 722, 766, 1858, 11, 286, 829, 1214, 257, 2638, 295], "temperature": 0.0, "avg_logprob": -0.15742947838523172, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.701891182572581e-05}, {"id": 14, "seek": 10428, "start": 113.12, "end": 123.0, "text": " memes to explain my personal story of why I came to create a company around, and a project", "tokens": [29730, 281, 2903, 452, 2973, 1657, 295, 983, 286, 1361, 281, 1884, 257, 2237, 926, 11, 293, 257, 1716], "temperature": 0.0, "avg_logprob": -0.15742947838523172, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.701891182572581e-05}, {"id": 15, "seek": 10428, "start": 123.0, "end": 129.28, "text": " around this machine learning testing thing. So, about 10 years ago, I started in machine", "tokens": [926, 341, 3479, 2539, 4997, 551, 13, 407, 11, 466, 1266, 924, 2057, 11, 286, 1409, 294, 3479], "temperature": 0.0, "avg_logprob": -0.15742947838523172, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.701891182572581e-05}, {"id": 16, "seek": 12928, "start": 129.28, "end": 136.48, "text": " learning, statistics, data science, and you know, you had this, you start using the scikit", "tokens": [2539, 11, 12523, 11, 1412, 3497, 11, 293, 291, 458, 11, 291, 632, 341, 11, 291, 722, 1228, 264, 2180, 22681], "temperature": 0.0, "avg_logprob": -0.2291692601572169, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.669451557972934e-05}, {"id": 17, "seek": 12928, "start": 136.48, "end": 141.96, "text": " learn API, and you're like, yeah, it's super easy, right? Anybody can be a data scientist,", "tokens": [1466, 9362, 11, 293, 291, 434, 411, 11, 1338, 11, 309, 311, 1687, 1858, 11, 558, 30, 19082, 393, 312, 257, 1412, 12662, 11], "temperature": 0.0, "avg_logprob": -0.2291692601572169, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.669451557972934e-05}, {"id": 18, "seek": 12928, "start": 141.96, "end": 149.96, "text": " you just dot fit, dot predict, and that's it. You're a data scientist. And probably if", "tokens": [291, 445, 5893, 3318, 11, 5893, 6069, 11, 293, 300, 311, 309, 13, 509, 434, 257, 1412, 12662, 13, 400, 1391, 498], "temperature": 0.0, "avg_logprob": -0.2291692601572169, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.669451557972934e-05}, {"id": 19, "seek": 12928, "start": 149.96, "end": 157.88, "text": " you're here today, you're like, yeah, have you tested your model? Yeah, sure. Train test,", "tokens": [291, 434, 510, 965, 11, 291, 434, 411, 11, 1338, 11, 362, 291, 8246, 428, 2316, 30, 865, 11, 988, 13, 28029, 1500, 11], "temperature": 0.0, "avg_logprob": -0.2291692601572169, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.669451557972934e-05}, {"id": 20, "seek": 15788, "start": 157.88, "end": 169.32, "text": " yeah. Reality, if you've deployed in production, is quite different. So, if you've deployed", "tokens": [1338, 13, 33822, 11, 498, 291, 600, 17826, 294, 4265, 11, 307, 1596, 819, 13, 407, 11, 498, 291, 600, 17826], "temperature": 0.0, "avg_logprob": -0.1743842863267468, "compression_ratio": 1.567251461988304, "no_speech_prob": 3.1669962481828406e-05}, {"id": 21, "seek": 15788, "start": 169.32, "end": 178.2, "text": " through production, often you'll have this painful discovery where you have your product", "tokens": [807, 4265, 11, 2049, 291, 603, 362, 341, 11697, 12114, 689, 291, 362, 428, 1674], "temperature": 0.0, "avg_logprob": -0.1743842863267468, "compression_ratio": 1.567251461988304, "no_speech_prob": 3.1669962481828406e-05}, {"id": 22, "seek": 15788, "start": 178.2, "end": 184.16, "text": " manager, business stakeholders to whom you said, look, I worked really hard on the fine", "tokens": [6598, 11, 1606, 17779, 281, 7101, 291, 848, 11, 574, 11, 286, 2732, 534, 1152, 322, 264, 2489], "temperature": 0.0, "avg_logprob": -0.1743842863267468, "compression_ratio": 1.567251461988304, "no_speech_prob": 3.1669962481828406e-05}, {"id": 23, "seek": 18416, "start": 184.16, "end": 192.68, "text": " tuning and the grid search to get to 85% accuracy, and you push your first version to production,", "tokens": [15164, 293, 264, 10748, 3164, 281, 483, 281, 14695, 4, 14170, 11, 293, 291, 2944, 428, 700, 3037, 281, 4265, 11], "temperature": 0.0, "avg_logprob": -0.17271522736885178, "compression_ratio": 1.481081081081081, "no_speech_prob": 2.5825338525464758e-05}, {"id": 24, "seek": 18416, "start": 192.68, "end": 201.16, "text": " and things don't quite work out. You don't reproduce these good accuracy numbers.", "tokens": [293, 721, 500, 380, 1596, 589, 484, 13, 509, 500, 380, 29501, 613, 665, 14170, 3547, 13], "temperature": 0.0, "avg_logprob": -0.17271522736885178, "compression_ratio": 1.481081081081081, "no_speech_prob": 2.5825338525464758e-05}, {"id": 25, "seek": 18416, "start": 201.16, "end": 210.8, "text": " So, well, this was me. I hope it's not you. It was one of these, my first experience deploying", "tokens": [407, 11, 731, 11, 341, 390, 385, 13, 286, 1454, 309, 311, 406, 291, 13, 467, 390, 472, 295, 613, 11, 452, 700, 1752, 34198], "temperature": 0.0, "avg_logprob": -0.17271522736885178, "compression_ratio": 1.481081081081081, "no_speech_prob": 2.5825338525464758e-05}, {"id": 26, "seek": 21080, "start": 210.8, "end": 217.36, "text": " machine learning through production was on a fraud detection system. So, frauds are notoriously", "tokens": [3479, 2539, 807, 4265, 390, 322, 257, 14560, 17784, 1185, 13, 407, 11, 14560, 82, 366, 46772, 8994], "temperature": 0.0, "avg_logprob": -0.12924330911518614, "compression_ratio": 1.7075471698113207, "no_speech_prob": 1.4493867638520896e-05}, {"id": 27, "seek": 21080, "start": 217.36, "end": 222.36, "text": " difficult as a use case for machine learning because what you're trying to detect doesn't", "tokens": [2252, 382, 257, 764, 1389, 337, 3479, 2539, 570, 437, 291, 434, 1382, 281, 5531, 1177, 380], "temperature": 0.0, "avg_logprob": -0.12924330911518614, "compression_ratio": 1.7075471698113207, "no_speech_prob": 1.4493867638520896e-05}, {"id": 28, "seek": 21080, "start": 222.36, "end": 229.44, "text": " quite want to be detected. There are people behind it who have a vested interest not to", "tokens": [1596, 528, 281, 312, 21896, 13, 821, 366, 561, 2261, 309, 567, 362, 257, 49317, 1179, 406, 281], "temperature": 0.0, "avg_logprob": -0.12924330911518614, "compression_ratio": 1.7075471698113207, "no_speech_prob": 1.4493867638520896e-05}, {"id": 29, "seek": 21080, "start": 229.44, "end": 236.56, "text": " have your machine learning system detect them. So, often in terms of performance, that's", "tokens": [362, 428, 3479, 2539, 1185, 5531, 552, 13, 407, 11, 2049, 294, 2115, 295, 3389, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.12924330911518614, "compression_ratio": 1.7075471698113207, "no_speech_prob": 1.4493867638520896e-05}, {"id": 30, "seek": 23656, "start": 236.56, "end": 247.68, "text": " at least what I ended up doing a lot of hot fixes in production. It's bad. So, kind of", "tokens": [412, 1935, 437, 286, 4590, 493, 884, 257, 688, 295, 2368, 32539, 294, 4265, 13, 467, 311, 1578, 13, 407, 11, 733, 295], "temperature": 0.0, "avg_logprob": -0.17474431576936142, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.1186926207738e-05}, {"id": 31, "seek": 23656, "start": 247.68, "end": 256.08, "text": " like five years ago, this was my stance on machine learning in production, a very painful", "tokens": [411, 1732, 924, 2057, 11, 341, 390, 452, 21033, 322, 3479, 2539, 294, 4265, 11, 257, 588, 11697], "temperature": 0.0, "avg_logprob": -0.17474431576936142, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.1186926207738e-05}, {"id": 32, "seek": 23656, "start": 256.08, "end": 263.52, "text": " grueling experience where you never know when you're going to be a complain, where you're", "tokens": [677, 3483, 278, 1752, 689, 291, 1128, 458, 562, 291, 434, 516, 281, 312, 257, 11024, 11, 689, 291, 434], "temperature": 0.0, "avg_logprob": -0.17474431576936142, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.1186926207738e-05}, {"id": 33, "seek": 26352, "start": 263.52, "end": 271.35999999999996, "text": " going to be on call to solve something in production. So, that's when I decided to buff", "tokens": [516, 281, 312, 322, 818, 281, 5039, 746, 294, 4265, 13, 407, 11, 300, 311, 562, 286, 3047, 281, 9204], "temperature": 0.0, "avg_logprob": -0.15966167169458725, "compression_ratio": 1.508108108108108, "no_speech_prob": 1.0285342796123587e-05}, {"id": 34, "seek": 26352, "start": 271.35999999999996, "end": 283.24, "text": " up and switch roles to join a software engineering team. I was a data crew back then, so I moved", "tokens": [493, 293, 3679, 9604, 281, 3917, 257, 4722, 7043, 1469, 13, 286, 390, 257, 1412, 7260, 646, 550, 11, 370, 286, 4259], "temperature": 0.0, "avg_logprob": -0.15966167169458725, "compression_ratio": 1.508108108108108, "no_speech_prob": 1.0285342796123587e-05}, {"id": 35, "seek": 26352, "start": 283.24, "end": 293.24, "text": " internally from data science to the product team, and here are some of the things to summarize", "tokens": [19501, 490, 1412, 3497, 281, 264, 1674, 1469, 11, 293, 510, 366, 512, 295, 264, 721, 281, 20858], "temperature": 0.0, "avg_logprob": -0.15966167169458725, "compression_ratio": 1.508108108108108, "no_speech_prob": 1.0285342796123587e-05}, {"id": 36, "seek": 29324, "start": 293.24, "end": 301.32, "text": " that as someone with a machine learning background, but no real software engineering experience", "tokens": [300, 382, 1580, 365, 257, 3479, 2539, 3678, 11, 457, 572, 957, 4722, 7043, 1752], "temperature": 0.0, "avg_logprob": -0.18282390776134672, "compression_ratio": 1.5081967213114753, "no_speech_prob": 2.6251771487295628e-05}, {"id": 37, "seek": 29324, "start": 301.32, "end": 308.84000000000003, "text": " that these were kind of like what I was told, like you must learn the ways of a CI CD, otherwise", "tokens": [300, 613, 645, 733, 295, 411, 437, 286, 390, 1907, 11, 411, 291, 1633, 1466, 264, 2098, 295, 257, 37777, 6743, 11, 5911], "temperature": 0.0, "avg_logprob": -0.18282390776134672, "compression_ratio": 1.5081967213114753, "no_speech_prob": 2.6251771487295628e-05}, {"id": 38, "seek": 29324, "start": 308.84000000000003, "end": 315.88, "text": " your project will not come to production. And for context, so I was specifically at", "tokens": [428, 1716, 486, 406, 808, 281, 4265, 13, 400, 337, 4319, 11, 370, 286, 390, 4682, 412], "temperature": 0.0, "avg_logprob": -0.18282390776134672, "compression_ratio": 1.5081967213114753, "no_speech_prob": 2.6251771487295628e-05}, {"id": 39, "seek": 31588, "start": 315.88, "end": 325.15999999999997, "text": " that time in charge of creating like an open source layer to federate all the NLP and computer", "tokens": [300, 565, 294, 4602, 295, 4084, 411, 364, 1269, 4009, 4583, 281, 38024, 473, 439, 264, 426, 45196, 293, 3820], "temperature": 0.0, "avg_logprob": -0.08660063062395368, "compression_ratio": 1.481081081081081, "no_speech_prob": 1.5895988326519728e-05}, {"id": 40, "seek": 31588, "start": 325.15999999999997, "end": 333.4, "text": " vision APIs that vendors in the cloud provide, and then to do the same for pre-trained NLP", "tokens": [5201, 21445, 300, 22056, 294, 264, 4588, 2893, 11, 293, 550, 281, 360, 264, 912, 337, 659, 12, 17227, 2001, 426, 45196], "temperature": 0.0, "avg_logprob": -0.08660063062395368, "compression_ratio": 1.481081081081081, "no_speech_prob": 1.5895988326519728e-05}, {"id": 41, "seek": 31588, "start": 333.4, "end": 341.4, "text": " and time series models. So, what was difficult in this context is I was not even the one", "tokens": [293, 565, 2638, 5245, 13, 407, 11, 437, 390, 2252, 294, 341, 4319, 307, 286, 390, 406, 754, 264, 472], "temperature": 0.0, "avg_logprob": -0.08660063062395368, "compression_ratio": 1.481081081081081, "no_speech_prob": 1.5895988326519728e-05}, {"id": 42, "seek": 34140, "start": 341.4, "end": 346.47999999999996, "text": " in charge of the models, and the models will be retrained fine-tuned, so the guarantees", "tokens": [294, 4602, 295, 264, 5245, 11, 293, 264, 5245, 486, 312, 1533, 31774, 2489, 12, 83, 43703, 11, 370, 264, 32567], "temperature": 0.0, "avg_logprob": -0.18851118218408872, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.9510007405187935e-05}, {"id": 43, "seek": 34140, "start": 346.47999999999996, "end": 355.15999999999997, "text": " into the properties of that system as an engineer, that's more difficult. There are some elements", "tokens": [666, 264, 7221, 295, 300, 1185, 382, 364, 11403, 11, 300, 311, 544, 2252, 13, 821, 366, 512, 4959], "temperature": 0.0, "avg_logprob": -0.18851118218408872, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.9510007405187935e-05}, {"id": 44, "seek": 34140, "start": 355.15999999999997, "end": 363.96, "text": " in the stack that you don't have control of. So, yeah, this is a bit of a repeat of a previous", "tokens": [294, 264, 8630, 300, 291, 500, 380, 362, 1969, 295, 13, 407, 11, 1338, 11, 341, 307, 257, 857, 295, 257, 7149, 295, 257, 3894], "temperature": 0.0, "avg_logprob": -0.18851118218408872, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.9510007405187935e-05}, {"id": 45, "seek": 36396, "start": 363.96, "end": 372.52, "text": " meme, and I really wanted to say one does not simply ship an ML product without tests.", "tokens": [21701, 11, 293, 286, 534, 1415, 281, 584, 472, 775, 406, 2935, 5374, 364, 21601, 1674, 1553, 6921, 13], "temperature": 0.0, "avg_logprob": -0.1492760041180779, "compression_ratio": 1.4673913043478262, "no_speech_prob": 7.886699677328579e-06}, {"id": 46, "seek": 36396, "start": 372.52, "end": 381.35999999999996, "text": " The challenge I had then is that from an engineering management standpoint, I was told, yeah, but", "tokens": [440, 3430, 286, 632, 550, 307, 300, 490, 364, 7043, 4592, 15827, 11, 286, 390, 1907, 11, 1338, 11, 457], "temperature": 0.0, "avg_logprob": -0.1492760041180779, "compression_ratio": 1.4673913043478262, "no_speech_prob": 7.886699677328579e-06}, {"id": 47, "seek": 36396, "start": 381.35999999999996, "end": 387.03999999999996, "text": " you know, it's easy, no engineers, they all write their test cases, so you do machine", "tokens": [291, 458, 11, 309, 311, 1858, 11, 572, 11955, 11, 436, 439, 2464, 641, 1500, 3331, 11, 370, 291, 360, 3479], "temperature": 0.0, "avg_logprob": -0.1492760041180779, "compression_ratio": 1.4673913043478262, "no_speech_prob": 7.886699677328579e-06}, {"id": 48, "seek": 38704, "start": 387.04, "end": 394.72, "text": " learning, just write them all, just write all the test cases. So, this was me being", "tokens": [2539, 11, 445, 2464, 552, 439, 11, 445, 2464, 439, 264, 1500, 3331, 13, 407, 11, 341, 390, 385, 885], "temperature": 0.0, "avg_logprob": -0.16341312895429896, "compression_ratio": 1.6682926829268292, "no_speech_prob": 5.173193585505942e-06}, {"id": 49, "seek": 38704, "start": 394.72, "end": 400.76000000000005, "text": " kind of a square one. It's like, okay, so you're telling me, I just need to write unit", "tokens": [733, 295, 257, 3732, 472, 13, 467, 311, 411, 11, 1392, 11, 370, 291, 434, 3585, 385, 11, 286, 445, 643, 281, 2464, 4985], "temperature": 0.0, "avg_logprob": -0.16341312895429896, "compression_ratio": 1.6682926829268292, "no_speech_prob": 5.173193585505942e-06}, {"id": 50, "seek": 38704, "start": 400.76000000000005, "end": 409.0, "text": " tests, okay, that will not really solve the issue, and that's kind of the beginning of", "tokens": [6921, 11, 1392, 11, 300, 486, 406, 534, 5039, 264, 2734, 11, 293, 300, 311, 733, 295, 264, 2863, 295], "temperature": 0.0, "avg_logprob": -0.16341312895429896, "compression_ratio": 1.6682926829268292, "no_speech_prob": 5.173193585505942e-06}, {"id": 51, "seek": 38704, "start": 409.0, "end": 416.24, "text": " a quest that set me on to build something to solve that gap between, okay, I want to", "tokens": [257, 866, 300, 992, 385, 322, 281, 1322, 746, 281, 5039, 300, 7417, 1296, 11, 1392, 11, 286, 528, 281], "temperature": 0.0, "avg_logprob": -0.16341312895429896, "compression_ratio": 1.6682926829268292, "no_speech_prob": 5.173193585505942e-06}, {"id": 52, "seek": 41624, "start": 416.24, "end": 422.8, "text": " test my models, I need to test my models, and how do, how can I do that? Because clearly,", "tokens": [1500, 452, 5245, 11, 286, 643, 281, 1500, 452, 5245, 11, 293, 577, 360, 11, 577, 393, 286, 360, 300, 30, 1436, 4448, 11], "temperature": 0.0, "avg_logprob": -0.14347660064697265, "compression_ratio": 1.4804469273743017, "no_speech_prob": 1.1835661098302808e-05}, {"id": 53, "seek": 41624, "start": 422.8, "end": 432.72, "text": " and I'll explain why, unit testing, your model is really not enough. So, a different angle", "tokens": [293, 286, 603, 2903, 983, 11, 4985, 4997, 11, 428, 2316, 307, 534, 406, 1547, 13, 407, 11, 257, 819, 5802], "temperature": 0.0, "avg_logprob": -0.14347660064697265, "compression_ratio": 1.4804469273743017, "no_speech_prob": 1.1835661098302808e-05}, {"id": 54, "seek": 41624, "start": 432.72, "end": 439.12, "text": " on the Y, I'll try to take a step back and talk about quality in general. I think in", "tokens": [322, 264, 398, 11, 286, 603, 853, 281, 747, 257, 1823, 646, 293, 751, 466, 3125, 294, 2674, 13, 286, 519, 294], "temperature": 0.0, "avg_logprob": -0.14347660064697265, "compression_ratio": 1.4804469273743017, "no_speech_prob": 1.1835661098302808e-05}, {"id": 55, "seek": 43912, "start": 439.12, "end": 449.28000000000003, "text": " this track, we all agree that quality matters, and if you look at AI, it's an industry that's", "tokens": [341, 2837, 11, 321, 439, 3986, 300, 3125, 7001, 11, 293, 498, 291, 574, 412, 7318, 11, 309, 311, 364, 3518, 300, 311], "temperature": 0.0, "avg_logprob": -0.14165172292225398, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.0898726183513645e-06}, {"id": 56, "seek": 43912, "start": 449.28000000000003, "end": 455.92, "text": " an engineering practice that is far younger than software engineering or civil engineering,", "tokens": [364, 7043, 3124, 300, 307, 1400, 7037, 813, 4722, 7043, 420, 5605, 7043, 11], "temperature": 0.0, "avg_logprob": -0.14165172292225398, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.0898726183513645e-06}, {"id": 57, "seek": 43912, "start": 455.92, "end": 462.88, "text": " and it's just riddled with incidents. I encourage you, if you don't know that resource already,", "tokens": [293, 309, 311, 445, 3973, 43130, 365, 21139, 13, 286, 5373, 291, 11, 498, 291, 500, 380, 458, 300, 7684, 1217, 11], "temperature": 0.0, "avg_logprob": -0.14165172292225398, "compression_ratio": 1.5875706214689265, "no_speech_prob": 4.0898726183513645e-06}, {"id": 58, "seek": 46288, "start": 462.88, "end": 472.04, "text": " it's an open source database, it's incidentdatabase.ai, and it's a public collection of reports,", "tokens": [309, 311, 364, 1269, 4009, 8149, 11, 309, 311, 9348, 20367, 455, 651, 13, 1301, 11, 293, 309, 311, 257, 1908, 5765, 295, 7122, 11], "temperature": 0.0, "avg_logprob": -0.13867759704589844, "compression_ratio": 1.5397727272727273, "no_speech_prob": 3.0203313144738786e-05}, {"id": 59, "seek": 46288, "start": 472.04, "end": 479.76, "text": " mostly in the media, of AI models that have not worked properly, and it's a really great", "tokens": [5240, 294, 264, 3021, 11, 295, 7318, 5245, 300, 362, 406, 2732, 6108, 11, 293, 309, 311, 257, 534, 869], "temperature": 0.0, "avg_logprob": -0.13867759704589844, "compression_ratio": 1.5397727272727273, "no_speech_prob": 3.0203313144738786e-05}, {"id": 60, "seek": 46288, "start": 479.76, "end": 487.24, "text": " work that has been going on for about two years and a half. It's a global initiative,", "tokens": [589, 300, 575, 668, 516, 322, 337, 466, 732, 924, 293, 257, 1922, 13, 467, 311, 257, 4338, 11552, 11], "temperature": 0.0, "avg_logprob": -0.13867759704589844, "compression_ratio": 1.5397727272727273, "no_speech_prob": 3.0203313144738786e-05}, {"id": 61, "seek": 48724, "start": 487.24, "end": 495.2, "text": " and just in this time, they collected more than 2,000 incidents. Since these are public", "tokens": [293, 445, 294, 341, 565, 11, 436, 11087, 544, 813, 568, 11, 1360, 21139, 13, 4162, 613, 366, 1908], "temperature": 0.0, "avg_logprob": -0.09351771428034855, "compression_ratio": 1.56, "no_speech_prob": 5.833873001392931e-05}, {"id": 62, "seek": 48724, "start": 495.2, "end": 501.44, "text": " reports, think of it as the tip of the iceberg, of course. There are a lot of incidents internal", "tokens": [7122, 11, 519, 295, 309, 382, 264, 4125, 295, 264, 38880, 11, 295, 1164, 13, 821, 366, 257, 688, 295, 21139, 6920], "temperature": 0.0, "avg_logprob": -0.09351771428034855, "compression_ratio": 1.56, "no_speech_prob": 5.833873001392931e-05}, {"id": 63, "seek": 48724, "start": 501.44, "end": 508.8, "text": " to companies that are not necessarily spoken out in the media. The incident database has", "tokens": [281, 3431, 300, 366, 406, 4725, 10759, 484, 294, 264, 3021, 13, 440, 9348, 8149, 575], "temperature": 0.0, "avg_logprob": -0.09351771428034855, "compression_ratio": 1.56, "no_speech_prob": 5.833873001392931e-05}, {"id": 64, "seek": 50880, "start": 508.8, "end": 517.0, "text": " a very interesting taxonomy of the different types of incidents. It's very multifaceted.", "tokens": [257, 588, 1880, 3366, 23423, 295, 264, 819, 3467, 295, 21139, 13, 467, 311, 588, 39824, 326, 10993, 13], "temperature": 0.0, "avg_logprob": -0.12372363358736038, "compression_ratio": 1.5, "no_speech_prob": 5.0102640670957044e-05}, {"id": 65, "seek": 50880, "start": 517.0, "end": 526.12, "text": " I took the liberty to simplify it in three big categories of incidents. One is FX, the", "tokens": [286, 1890, 264, 22849, 281, 20460, 309, 294, 1045, 955, 10479, 295, 21139, 13, 1485, 307, 37849, 11, 264], "temperature": 0.0, "avg_logprob": -0.12372363358736038, "compression_ratio": 1.5, "no_speech_prob": 5.0102640670957044e-05}, {"id": 66, "seek": 50880, "start": 526.12, "end": 533.16, "text": " other is business economic impact, and the third one is on security. We're talking about", "tokens": [661, 307, 1606, 4836, 2712, 11, 293, 264, 2636, 472, 307, 322, 3825, 13, 492, 434, 1417, 466], "temperature": 0.0, "avg_logprob": -0.12372363358736038, "compression_ratio": 1.5, "no_speech_prob": 5.0102640670957044e-05}, {"id": 67, "seek": 53316, "start": 533.16, "end": 541.36, "text": " really, if they happen at a global company scale, incidents that are very, very severe.", "tokens": [534, 11, 498, 436, 1051, 412, 257, 4338, 2237, 4373, 11, 21139, 300, 366, 588, 11, 588, 8922, 13], "temperature": 0.0, "avg_logprob": -0.14491523013395421, "compression_ratio": 1.4782608695652173, "no_speech_prob": 6.428836786653847e-05}, {"id": 68, "seek": 53316, "start": 541.36, "end": 550.1999999999999, "text": " In FX, you can have a sexist credit scoring algorithm that exposes the company to lawsuits,", "tokens": [682, 37849, 11, 291, 393, 362, 257, 3260, 468, 5397, 22358, 9284, 300, 1278, 4201, 264, 2237, 281, 39493, 11], "temperature": 0.0, "avg_logprob": -0.14491523013395421, "compression_ratio": 1.4782608695652173, "no_speech_prob": 6.428836786653847e-05}, {"id": 69, "seek": 53316, "start": 550.1999999999999, "end": 559.4, "text": " to brand image damages, et cetera. These are notoriously hard to identify. In a way, machine", "tokens": [281, 3360, 3256, 28536, 11, 1030, 11458, 13, 1981, 366, 46772, 8994, 1152, 281, 5876, 13, 682, 257, 636, 11, 3479], "temperature": 0.0, "avg_logprob": -0.14491523013395421, "compression_ratio": 1.4782608695652173, "no_speech_prob": 6.428836786653847e-05}, {"id": 70, "seek": 55940, "start": 559.4, "end": 565.0, "text": " learning is precisely about discrimination. It's hard to tell a machine that is learning", "tokens": [2539, 307, 13402, 466, 15973, 13, 467, 311, 1152, 281, 980, 257, 3479, 300, 307, 2539], "temperature": 0.0, "avg_logprob": -0.10751340263768246, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.54259672248736e-05}, {"id": 71, "seek": 55940, "start": 565.0, "end": 570.1999999999999, "text": " to discriminate, not to discriminate against certain sensitive groups. I'll speak on some", "tokens": [281, 47833, 11, 406, 281, 47833, 1970, 1629, 9477, 3935, 13, 286, 603, 1710, 322, 512], "temperature": 0.0, "avg_logprob": -0.10751340263768246, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.54259672248736e-05}, {"id": 72, "seek": 55940, "start": 570.1999999999999, "end": 575.16, "text": " methods that can be used precisely on this problem, but Apple was working with at the", "tokens": [7150, 300, 393, 312, 1143, 13402, 322, 341, 1154, 11, 457, 6373, 390, 1364, 365, 412, 264], "temperature": 0.0, "avg_logprob": -0.10751340263768246, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.54259672248736e-05}, {"id": 73, "seek": 55940, "start": 575.16, "end": 581.88, "text": " time Goldman Sachs on deploying this algorithm and probably some tests and safeguards were", "tokens": [565, 45378, 25626, 82, 322, 34198, 341, 9284, 293, 1391, 512, 6921, 293, 32358, 84, 2287, 645], "temperature": 0.0, "avg_logprob": -0.10751340263768246, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.54259672248736e-05}, {"id": 74, "seek": 58188, "start": 581.88, "end": 590.8, "text": " unfortunately skipped. It was actually discovered on Twitter that in a simple case, a male loan", "tokens": [7015, 30193, 13, 467, 390, 767, 6941, 322, 5794, 300, 294, 257, 2199, 1389, 11, 257, 7133, 10529], "temperature": 0.0, "avg_logprob": -0.17471754355508773, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00013922201469540596}, {"id": 75, "seek": 58188, "start": 590.8, "end": 600.96, "text": " applicant would get 10x their loan limit compared to his wife. That sparked a huge controversy", "tokens": [30915, 576, 483, 1266, 87, 641, 10529, 4948, 5347, 281, 702, 3836, 13, 663, 39653, 257, 2603, 22976], "temperature": 0.0, "avg_logprob": -0.17471754355508773, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00013922201469540596}, {"id": 76, "seek": 58188, "start": 600.96, "end": 610.0, "text": " that probably exposed Apple to some lawsuits. In another area, that is not with sensitive", "tokens": [300, 1391, 9495, 6373, 281, 512, 39493, 13, 682, 1071, 1859, 11, 300, 307, 406, 365, 9477], "temperature": 0.0, "avg_logprob": -0.17471754355508773, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00013922201469540596}, {"id": 77, "seek": 61000, "start": 610.0, "end": 620.44, "text": " features such as gender. There was a huge catastrophe a year and a half ago that happened", "tokens": [4122, 1270, 382, 7898, 13, 821, 390, 257, 2603, 36043, 257, 1064, 293, 257, 1922, 2057, 300, 2011], "temperature": 0.0, "avg_logprob": -0.15933876849235373, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.00014851127343717963}, {"id": 78, "seek": 61000, "start": 620.44, "end": 630.6, "text": " to Zillow, a real estate company, where there was a small bias that was overestimating the", "tokens": [281, 1176, 373, 305, 11, 257, 957, 9749, 2237, 11, 689, 456, 390, 257, 1359, 12577, 300, 390, 670, 377, 332, 990, 264], "temperature": 0.0, "avg_logprob": -0.15933876849235373, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.00014851127343717963}, {"id": 79, "seek": 63060, "start": 630.6, "end": 640.96, "text": " prices of homes. They decided to put this algorithm live to buy and sell houses. It", "tokens": [7901, 295, 7388, 13, 814, 3047, 281, 829, 341, 9284, 1621, 281, 2256, 293, 3607, 8078, 13, 467], "temperature": 0.0, "avg_logprob": -0.16531322185809796, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.00012866032193414867}, {"id": 80, "seek": 63060, "start": 640.96, "end": 647.6, "text": " turned out that this tiny bias, which was left unchecked, was exploited by the real", "tokens": [3574, 484, 300, 341, 5870, 12577, 11, 597, 390, 1411, 46672, 292, 11, 390, 40918, 538, 264, 957], "temperature": 0.0, "avg_logprob": -0.16531322185809796, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.00012866032193414867}, {"id": 81, "seek": 63060, "start": 647.6, "end": 659.12, "text": " estate agents in the US. Literally, this created a loss of nearly half a billion dollars. Again,", "tokens": [9749, 12554, 294, 264, 2546, 13, 23768, 11, 341, 2942, 257, 4470, 295, 6217, 1922, 257, 5218, 3808, 13, 3764, 11], "temperature": 0.0, "avg_logprob": -0.16531322185809796, "compression_ratio": 1.4426229508196722, "no_speech_prob": 0.00012866032193414867}, {"id": 82, "seek": 65912, "start": 659.12, "end": 666.48, "text": " maybe if going back to testing, this could have been unseapated and avoided. Now on a", "tokens": [1310, 498, 516, 646, 281, 4997, 11, 341, 727, 362, 668, 517, 405, 569, 770, 293, 24890, 13, 823, 322, 257], "temperature": 0.0, "avg_logprob": -0.1989214728860294, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00014919899695087224}, {"id": 83, "seek": 65912, "start": 666.48, "end": 673.08, "text": " more cybersecurity spectrum, there's a lot of good research from cybersecurity labs showing", "tokens": [544, 38765, 11143, 11, 456, 311, 257, 688, 295, 665, 2132, 490, 38765, 20339, 4099], "temperature": 0.0, "avg_logprob": -0.1989214728860294, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00014919899695087224}, {"id": 84, "seek": 65912, "start": 673.08, "end": 680.08, "text": " that you can hack, for example, a computer vision system in an autonomous driving context.", "tokens": [300, 291, 393, 10339, 11, 337, 1365, 11, 257, 3820, 5201, 1185, 294, 364, 23797, 4840, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1989214728860294, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00014919899695087224}, {"id": 85, "seek": 65912, "start": 680.08, "end": 686.6800000000001, "text": " Here you put a special tape on the road and you can crash a Tesla. We don't quite know", "tokens": [1692, 291, 829, 257, 2121, 7314, 322, 264, 3060, 293, 291, 393, 8252, 257, 13666, 13, 492, 500, 380, 1596, 458], "temperature": 0.0, "avg_logprob": -0.1989214728860294, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00014919899695087224}, {"id": 86, "seek": 68668, "start": 686.68, "end": 691.92, "text": " if these types of vulnerabilities have been exploited in real life yet, but as AI becomes", "tokens": [498, 613, 3467, 295, 37633, 362, 668, 40918, 294, 957, 993, 1939, 11, 457, 382, 7318, 3643], "temperature": 0.0, "avg_logprob": -0.16564266591132443, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.00011068359890487045}, {"id": 87, "seek": 68668, "start": 691.92, "end": 696.76, "text": " super ubiquitous, and obviously there are some bad actors out there that might want", "tokens": [1687, 43868, 39831, 11, 293, 2745, 456, 366, 512, 1578, 10037, 484, 456, 300, 1062, 528], "temperature": 0.0, "avg_logprob": -0.16564266591132443, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.00011068359890487045}, {"id": 88, "seek": 68668, "start": 696.76, "end": 703.88, "text": " to hack these systems and introduces a new type of attack vectors. That's also something", "tokens": [281, 10339, 613, 3652, 293, 31472, 257, 777, 2010, 295, 2690, 18875, 13, 663, 311, 611, 746], "temperature": 0.0, "avg_logprob": -0.16564266591132443, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.00011068359890487045}, {"id": 89, "seek": 68668, "start": 703.88, "end": 714.8399999999999, "text": " we need to care about. Both from the practitioners of AI and from a regulatory standpoint, testing", "tokens": [321, 643, 281, 1127, 466, 13, 6767, 490, 264, 25742, 295, 7318, 293, 490, 257, 18260, 15827, 11, 4997], "temperature": 0.0, "avg_logprob": -0.16564266591132443, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.00011068359890487045}, {"id": 90, "seek": 71484, "start": 714.84, "end": 722.6800000000001, "text": " just makes sense. Yanlequin, chief AI scientist at META, was actually taking a stance at the", "tokens": [445, 1669, 2020, 13, 13633, 306, 29360, 11, 9588, 7318, 12662, 412, 376, 4850, 32, 11, 390, 767, 1940, 257, 21033, 412, 264], "temperature": 0.0, "avg_logprob": -0.208524047003852, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0002171949454350397}, {"id": 91, "seek": 71484, "start": 722.6800000000001, "end": 730.32, "text": " beginning of last year on Twitter saying that if you won't trust in a system, you need tests.", "tokens": [2863, 295, 1036, 1064, 322, 5794, 1566, 300, 498, 291, 1582, 380, 3361, 294, 257, 1185, 11, 291, 643, 6921, 13], "temperature": 0.0, "avg_logprob": -0.208524047003852, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0002171949454350397}, {"id": 92, "seek": 71484, "start": 730.32, "end": 735.6, "text": " Also making a slight criticism towards some of the explainability methods, because two", "tokens": [2743, 1455, 257, 4036, 15835, 3030, 512, 295, 264, 2903, 2310, 7150, 11, 570, 732], "temperature": 0.0, "avg_logprob": -0.208524047003852, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0002171949454350397}, {"id": 93, "seek": 71484, "start": 735.6, "end": 740.6800000000001, "text": " years ago, if you've followed that realm, people were saying, oh, you just need explainability", "tokens": [924, 2057, 11, 498, 291, 600, 6263, 300, 15355, 11, 561, 645, 1566, 11, 1954, 11, 291, 445, 643, 2903, 2310], "temperature": 0.0, "avg_logprob": -0.208524047003852, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0002171949454350397}, {"id": 94, "seek": 74068, "start": 740.68, "end": 748.52, "text": " and then your problems will go away. Well, that's just part of the answer. Lastly, and", "tokens": [293, 550, 428, 2740, 486, 352, 1314, 13, 1042, 11, 300, 311, 445, 644, 295, 264, 1867, 13, 18072, 11, 293], "temperature": 0.0, "avg_logprob": -0.15308856964111328, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0001004636287689209}, {"id": 95, "seek": 74068, "start": 748.52, "end": 753.3199999999999, "text": " this was covered in some of the talks this morning on the big auditorium, there's a", "tokens": [341, 390, 5343, 294, 512, 295, 264, 6686, 341, 2446, 322, 264, 955, 33970, 2197, 11, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.15308856964111328, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0001004636287689209}, {"id": 96, "seek": 74068, "start": 753.3199999999999, "end": 760.4, "text": " growing regulatory requirement to put some checks and balances in place. That also says", "tokens": [4194, 18260, 11695, 281, 829, 512, 13834, 293, 33993, 294, 1081, 13, 663, 611, 1619], "temperature": 0.0, "avg_logprob": -0.15308856964111328, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0001004636287689209}, {"id": 97, "seek": 74068, "start": 760.4, "end": 767.0, "text": " that you need specifically in case your AI system is high risk, you need to put quality", "tokens": [300, 291, 643, 4682, 294, 1389, 428, 7318, 1185, 307, 1090, 3148, 11, 291, 643, 281, 829, 3125], "temperature": 0.0, "avg_logprob": -0.15308856964111328, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0001004636287689209}, {"id": 98, "seek": 76700, "start": 767.0, "end": 772.72, "text": " measures in place. The definition of high risk AI systems is pretty large. Obviously,", "tokens": [8000, 294, 1081, 13, 440, 7123, 295, 1090, 3148, 7318, 3652, 307, 1238, 2416, 13, 7580, 11], "temperature": 0.0, "avg_logprob": -0.14922065205044216, "compression_ratio": 1.6164383561643836, "no_speech_prob": 5.641241295961663e-05}, {"id": 99, "seek": 76700, "start": 772.72, "end": 779.24, "text": " you have anything related to infrastructure, like critical infrastructure, defense, et cetera,", "tokens": [291, 362, 1340, 4077, 281, 6896, 11, 411, 4924, 6896, 11, 7654, 11, 1030, 11458, 11], "temperature": 0.0, "avg_logprob": -0.14922065205044216, "compression_ratio": 1.6164383561643836, "no_speech_prob": 5.641241295961663e-05}, {"id": 100, "seek": 76700, "start": 779.24, "end": 786.44, "text": " but you also have all AI systems that are involved in human resources and public service", "tokens": [457, 291, 611, 362, 439, 7318, 3652, 300, 366, 3288, 294, 1952, 3593, 293, 1908, 2643], "temperature": 0.0, "avg_logprob": -0.14922065205044216, "compression_ratio": 1.6164383561643836, "no_speech_prob": 5.641241295961663e-05}, {"id": 101, "seek": 76700, "start": 786.44, "end": 790.88, "text": " and financial services, because these are considered, obviously, critical components", "tokens": [293, 4669, 3328, 11, 570, 613, 366, 4888, 11, 2745, 11, 4924, 6677], "temperature": 0.0, "avg_logprob": -0.14922065205044216, "compression_ratio": 1.6164383561643836, "no_speech_prob": 5.641241295961663e-05}, {"id": 102, "seek": 79088, "start": 790.88, "end": 802.76, "text": " of society. Now that we kind of agreed that it's an important problem, these are some", "tokens": [295, 4086, 13, 823, 300, 321, 733, 295, 9166, 300, 309, 311, 364, 1021, 1154, 11, 613, 366, 512], "temperature": 0.0, "avg_logprob": -0.14823977649211884, "compression_ratio": 1.5142857142857142, "no_speech_prob": 5.4257467127172276e-05}, {"id": 103, "seek": 79088, "start": 802.76, "end": 811.0, "text": " of the challenges, because if you've encountered some of these issues, you probably looked", "tokens": [295, 264, 4759, 11, 570, 498, 291, 600, 20381, 512, 295, 613, 2663, 11, 291, 1391, 2956], "temperature": 0.0, "avg_logprob": -0.14823977649211884, "compression_ratio": 1.5142857142857142, "no_speech_prob": 5.4257467127172276e-05}, {"id": 104, "seek": 79088, "start": 811.0, "end": 819.72, "text": " at some easy solutions, taking some analogies on what you might do to do this. There are", "tokens": [412, 512, 1858, 6547, 11, 1940, 512, 16660, 530, 322, 437, 291, 1062, 360, 281, 360, 341, 13, 821, 366], "temperature": 0.0, "avg_logprob": -0.14823977649211884, "compression_ratio": 1.5142857142857142, "no_speech_prob": 5.4257467127172276e-05}, {"id": 105, "seek": 81972, "start": 819.72, "end": 826.36, "text": " three points that make this problem of testing machine learning a bit special, meaning it's", "tokens": [1045, 2793, 300, 652, 341, 1154, 295, 4997, 3479, 2539, 257, 857, 2121, 11, 3620, 309, 311], "temperature": 0.0, "avg_logprob": -0.13195628654666064, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.5012624216033146e-05}, {"id": 106, "seek": 81972, "start": 826.36, "end": 833.9200000000001, "text": " still a big work in progress. Point one is that it is really not enough to check the", "tokens": [920, 257, 955, 589, 294, 4205, 13, 12387, 472, 307, 300, 309, 307, 534, 406, 1547, 281, 1520, 264], "temperature": 0.0, "avg_logprob": -0.13195628654666064, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.5012624216033146e-05}, {"id": 107, "seek": 81972, "start": 833.9200000000001, "end": 841.6, "text": " data quality to guarantee the quality of a machine learning system. One of my co-founders", "tokens": [1412, 3125, 281, 10815, 264, 3125, 295, 257, 3479, 2539, 1185, 13, 1485, 295, 452, 598, 12, 17493, 433], "temperature": 0.0, "avg_logprob": -0.13195628654666064, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.5012624216033146e-05}, {"id": 108, "seek": 81972, "start": 841.6, "end": 848.36, "text": " doing his PhD proved experimentally, you can run experiments, you can have really clean", "tokens": [884, 702, 14476, 14617, 5120, 379, 11, 291, 393, 1190, 12050, 11, 291, 393, 362, 534, 2541], "temperature": 0.0, "avg_logprob": -0.13195628654666064, "compression_ratio": 1.6619718309859155, "no_speech_prob": 5.5012624216033146e-05}, {"id": 109, "seek": 84836, "start": 848.36, "end": 856.6800000000001, "text": " data in a bad model. So you cannot just say it's an upstream problem, it's technically", "tokens": [1412, 294, 257, 1578, 2316, 13, 407, 291, 2644, 445, 584, 309, 311, 364, 33915, 1154, 11, 309, 311, 12120], "temperature": 0.0, "avg_logprob": -0.20299310068930349, "compression_ratio": 1.5406976744186047, "no_speech_prob": 5.4168842325452715e-05}, {"id": 110, "seek": 84836, "start": 856.6800000000001, "end": 862.44, "text": " like systems engineering, you have to take the data, the machine learning model, and", "tokens": [411, 3652, 7043, 11, 291, 362, 281, 747, 264, 1412, 11, 264, 3479, 2539, 2316, 11, 293], "temperature": 0.0, "avg_logprob": -0.20299310068930349, "compression_ratio": 1.5406976744186047, "no_speech_prob": 5.4168842325452715e-05}, {"id": 111, "seek": 84836, "start": 862.44, "end": 871.0, "text": " the user into context to analyze its properties. Moreover, the errors of the machine learning", "tokens": [264, 4195, 666, 4319, 281, 12477, 1080, 7221, 13, 19838, 11, 264, 13603, 295, 264, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.20299310068930349, "compression_ratio": 1.5406976744186047, "no_speech_prob": 5.4168842325452715e-05}, {"id": 112, "seek": 87100, "start": 871.0, "end": 880.16, "text": " systems are often caused by pieces of data that did not exist when the model was created,", "tokens": [3652, 366, 2049, 7008, 538, 3755, 295, 1412, 300, 630, 406, 2514, 562, 264, 2316, 390, 2942, 11], "temperature": 0.0, "avg_logprob": -0.13603535251341003, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.763311517308466e-05}, {"id": 113, "seek": 87100, "start": 880.16, "end": 891.68, "text": " they were clean, but they did not exist. Second point, it's pretty hard just to copy-paste", "tokens": [436, 645, 2541, 11, 457, 436, 630, 406, 2514, 13, 5736, 935, 11, 309, 311, 1238, 1152, 445, 281, 5055, 12, 79, 9079], "temperature": 0.0, "avg_logprob": -0.13603535251341003, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.763311517308466e-05}, {"id": 114, "seek": 87100, "start": 891.68, "end": 900.2, "text": " some of the testing methods from software into AI. One is like, yes, you can do some", "tokens": [512, 295, 264, 4997, 7150, 490, 4722, 666, 7318, 13, 1485, 307, 411, 11, 2086, 11, 291, 393, 360, 512], "temperature": 0.0, "avg_logprob": -0.13603535251341003, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.763311517308466e-05}, {"id": 115, "seek": 90020, "start": 900.2, "end": 907.72, "text": " unit tests on machine learning models, but they won't prove much. Because the principle", "tokens": [4985, 6921, 322, 3479, 2539, 5245, 11, 457, 436, 1582, 380, 7081, 709, 13, 1436, 264, 8665], "temperature": 0.0, "avg_logprob": -0.15109806918026356, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.839699148666114e-05}, {"id": 116, "seek": 90020, "start": 907.72, "end": 914.1600000000001, "text": " is that it's a transactional system and things are moving quite a lot. So that's a good baseline.", "tokens": [307, 300, 309, 311, 257, 46688, 1966, 1185, 293, 721, 366, 2684, 1596, 257, 688, 13, 407, 300, 311, 257, 665, 20518, 13], "temperature": 0.0, "avg_logprob": -0.15109806918026356, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.839699148666114e-05}, {"id": 117, "seek": 90020, "start": 914.1600000000001, "end": 917.76, "text": " If you have a machine learning system and you have some unit tests, that's really like step", "tokens": [759, 291, 362, 257, 3479, 2539, 1185, 293, 291, 362, 512, 4985, 6921, 11, 300, 311, 534, 411, 1823], "temperature": 0.0, "avg_logprob": -0.15109806918026356, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.839699148666114e-05}, {"id": 118, "seek": 90020, "start": 917.76, "end": 924.5200000000001, "text": " one. It's better to have that than to have nothing. But you have to embrace the fact", "tokens": [472, 13, 467, 311, 1101, 281, 362, 300, 813, 281, 362, 1825, 13, 583, 291, 362, 281, 14038, 264, 1186], "temperature": 0.0, "avg_logprob": -0.15109806918026356, "compression_ratio": 1.7075471698113207, "no_speech_prob": 8.839699148666114e-05}, {"id": 119, "seek": 92452, "start": 924.52, "end": 932.88, "text": " that there has got to be a large number of test cases. So you cannot just test on three,", "tokens": [300, 456, 575, 658, 281, 312, 257, 2416, 1230, 295, 1500, 3331, 13, 407, 291, 2644, 445, 1500, 322, 1045, 11], "temperature": 0.0, "avg_logprob": -0.20620058845071232, "compression_ratio": 1.5695652173913044, "no_speech_prob": 5.4002808610675856e-05}, {"id": 120, "seek": 92452, "start": 932.88, "end": 939.64, "text": " five, hundred, even a thousand cases will not be enough. The models themselves are probabilistic.", "tokens": [1732, 11, 3262, 11, 754, 257, 4714, 3331, 486, 406, 312, 1547, 13, 440, 5245, 2969, 366, 31959, 3142, 13], "temperature": 0.0, "avg_logprob": -0.20620058845071232, "compression_ratio": 1.5695652173913044, "no_speech_prob": 5.4002808610675856e-05}, {"id": 121, "seek": 92452, "start": 939.64, "end": 947.0, "text": " So you have to take into account statistical methods of tests. And lastly, and I think", "tokens": [407, 291, 362, 281, 747, 666, 2696, 22820, 7150, 295, 6921, 13, 400, 16386, 11, 293, 286, 519], "temperature": 0.0, "avg_logprob": -0.20620058845071232, "compression_ratio": 1.5695652173913044, "no_speech_prob": 5.4002808610675856e-05}, {"id": 122, "seek": 92452, "start": 947.0, "end": 953.0799999999999, "text": " this is specific to, because there has been some systems that were heavily dependent on", "tokens": [341, 307, 2685, 281, 11, 570, 456, 575, 668, 512, 3652, 300, 645, 10950, 12334, 322], "temperature": 0.0, "avg_logprob": -0.20620058845071232, "compression_ratio": 1.5695652173913044, "no_speech_prob": 5.4002808610675856e-05}, {"id": 123, "seek": 95308, "start": 953.08, "end": 961.2, "text": " data, but with AI, AI also came with a fact that you increase the number of data inputs", "tokens": [1412, 11, 457, 365, 7318, 11, 7318, 611, 1361, 365, 257, 1186, 300, 291, 3488, 264, 1230, 295, 1412, 15743], "temperature": 0.0, "avg_logprob": -0.17939311265945435, "compression_ratio": 1.4745762711864407, "no_speech_prob": 6.096994911786169e-05}, {"id": 124, "seek": 95308, "start": 961.2, "end": 967.64, "text": " compared to traditional systems. So you very quickly come into issues of, well, it's a", "tokens": [5347, 281, 5164, 3652, 13, 407, 291, 588, 2661, 808, 666, 2663, 295, 11, 731, 11, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.17939311265945435, "compression_ratio": 1.4745762711864407, "no_speech_prob": 6.096994911786169e-05}, {"id": 125, "seek": 95308, "start": 967.64, "end": 975.32, "text": " combinatorial problem, and it's factually impossible to generate all the combinations.", "tokens": [2512, 31927, 831, 1154, 11, 293, 309, 311, 1186, 671, 6243, 281, 8460, 439, 264, 21267, 13], "temperature": 0.0, "avg_logprob": -0.17939311265945435, "compression_ratio": 1.4745762711864407, "no_speech_prob": 6.096994911786169e-05}, {"id": 126, "seek": 97532, "start": 975.32, "end": 985.12, "text": " Very simple example of that. How can you test an NLP system? Lastly, like AI touching a lot", "tokens": [4372, 2199, 1365, 295, 300, 13, 1012, 393, 291, 1500, 364, 426, 45196, 1185, 30, 18072, 11, 411, 7318, 11175, 257, 688], "temperature": 0.0, "avg_logprob": -0.1339480841337745, "compression_ratio": 1.4270833333333333, "no_speech_prob": 5.902112752664834e-05}, {"id": 127, "seek": 97532, "start": 985.12, "end": 990.1600000000001, "text": " of different points. If you want to have a complete test coverage, you really need to", "tokens": [295, 819, 2793, 13, 759, 291, 528, 281, 362, 257, 3566, 1500, 9645, 11, 291, 534, 643, 281], "temperature": 0.0, "avg_logprob": -0.1339480841337745, "compression_ratio": 1.4270833333333333, "no_speech_prob": 5.902112752664834e-05}, {"id": 128, "seek": 97532, "start": 990.1600000000001, "end": 996.5600000000001, "text": " take into account multiple criteria. So performance of a system, but also robustness, robustness", "tokens": [747, 666, 2696, 3866, 11101, 13, 407, 3389, 295, 257, 1185, 11, 457, 611, 13956, 1287, 11, 13956, 1287], "temperature": 0.0, "avg_logprob": -0.1339480841337745, "compression_ratio": 1.4270833333333333, "no_speech_prob": 5.902112752664834e-05}, {"id": 129, "seek": 99656, "start": 996.56, "end": 1005.92, "text": " to errors, fairness, privacy, security, reliability. And also, and that's becoming an increasingly", "tokens": [281, 13603, 11, 29765, 11, 11427, 11, 3825, 11, 24550, 13, 400, 611, 11, 293, 300, 311, 5617, 364, 12980], "temperature": 0.0, "avg_logprob": -0.13122157046669408, "compression_ratio": 1.5120967741935485, "no_speech_prob": 9.650275751482695e-05}, {"id": 130, "seek": 99656, "start": 1005.92, "end": 1011.3199999999999, "text": " important topic with green AI, it's like what is the carbon impact of this AI? Do you really", "tokens": [1021, 4829, 365, 3092, 7318, 11, 309, 311, 411, 437, 307, 264, 5954, 2712, 295, 341, 7318, 30, 1144, 291, 534], "temperature": 0.0, "avg_logprob": -0.13122157046669408, "compression_ratio": 1.5120967741935485, "no_speech_prob": 9.650275751482695e-05}, {"id": 131, "seek": 99656, "start": 1011.3199999999999, "end": 1019.04, "text": " need that many GPUs? Can you make your system a bit more energy efficient? So today I'll", "tokens": [643, 300, 867, 18407, 82, 30, 1664, 291, 652, 428, 1185, 257, 857, 544, 2281, 7148, 30, 407, 965, 286, 603], "temperature": 0.0, "avg_logprob": -0.13122157046669408, "compression_ratio": 1.5120967741935485, "no_speech_prob": 9.650275751482695e-05}, {"id": 132, "seek": 99656, "start": 1019.04, "end": 1024.76, "text": " focus on the, because I see we have 10 more minutes, I'll focus on two aspects, the robustness", "tokens": [1879, 322, 264, 11, 570, 286, 536, 321, 362, 1266, 544, 2077, 11, 286, 603, 1879, 322, 732, 7270, 11, 264, 13956, 1287], "temperature": 0.0, "avg_logprob": -0.13122157046669408, "compression_ratio": 1.5120967741935485, "no_speech_prob": 9.650275751482695e-05}, {"id": 133, "seek": 102476, "start": 1024.76, "end": 1037.0, "text": " and the effects. So I'll start with robustness. Who has read or heard about this paper? Quick", "tokens": [293, 264, 5065, 13, 407, 286, 603, 722, 365, 13956, 1287, 13, 2102, 575, 1401, 420, 2198, 466, 341, 3035, 30, 12101], "temperature": 0.0, "avg_logprob": -0.23296529054641724, "compression_ratio": 1.2805755395683454, "no_speech_prob": 0.0002539586275815964}, {"id": 134, "seek": 102476, "start": 1037.0, "end": 1047.32, "text": " show of hands. Okay, one. So who has heard of behavioral testing? Because that's not", "tokens": [855, 295, 2377, 13, 1033, 11, 472, 13, 407, 567, 575, 2198, 295, 19124, 4997, 30, 1436, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.23296529054641724, "compression_ratio": 1.2805755395683454, "no_speech_prob": 0.0002539586275815964}, {"id": 135, "seek": 104732, "start": 1047.32, "end": 1056.4399999999998, "text": " machine learning specific. Yeah, cool. So Ribeiro three years ago, along with other co-writers", "tokens": [3479, 2539, 2685, 13, 865, 11, 1627, 13, 407, 33668, 650, 5182, 1045, 924, 2057, 11, 2051, 365, 661, 598, 12, 86, 39335], "temperature": 0.0, "avg_logprob": -0.21964432211483226, "compression_ratio": 1.4974093264248705, "no_speech_prob": 9.812261123443022e-05}, {"id": 136, "seek": 104732, "start": 1056.4399999999998, "end": 1066.48, "text": " of this paper, did I think a fantastic job to see how to adapt behavioral testing, which", "tokens": [295, 341, 3035, 11, 630, 286, 519, 257, 5456, 1691, 281, 536, 577, 281, 6231, 19124, 4997, 11, 597], "temperature": 0.0, "avg_logprob": -0.21964432211483226, "compression_ratio": 1.4974093264248705, "no_speech_prob": 9.812261123443022e-05}, {"id": 137, "seek": 104732, "start": 1066.48, "end": 1073.12, "text": " is a really good practice from software engineering, to the context of machine learning. And specifically", "tokens": [307, 257, 534, 665, 3124, 490, 4722, 7043, 11, 281, 264, 4319, 295, 3479, 2539, 13, 400, 4682], "temperature": 0.0, "avg_logprob": -0.21964432211483226, "compression_ratio": 1.4974093264248705, "no_speech_prob": 9.812261123443022e-05}, {"id": 138, "seek": 107312, "start": 1073.12, "end": 1087.7199999999998, "text": " wrote something for NLP models. The main problem that this research paper aimed to solve was", "tokens": [4114, 746, 337, 426, 45196, 5245, 13, 440, 2135, 1154, 300, 341, 2132, 3035, 20540, 281, 5039, 390], "temperature": 0.0, "avg_logprob": -0.21598792605929903, "compression_ratio": 1.318840579710145, "no_speech_prob": 0.00011186068877577782}, {"id": 139, "seek": 107312, "start": 1087.7199999999998, "end": 1094.6799999999998, "text": " test case generation. Because really NLP is by a sense a problem, NLP, a natural language", "tokens": [1500, 1389, 5125, 13, 1436, 534, 426, 45196, 307, 538, 257, 2020, 257, 1154, 11, 426, 45196, 11, 257, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.21598792605929903, "compression_ratio": 1.318840579710145, "no_speech_prob": 0.00011186068877577782}, {"id": 140, "seek": 109468, "start": 1094.68, "end": 1106.64, "text": " processing. So you have an input text, it's just raw text. So you need to test this. But", "tokens": [9007, 13, 407, 291, 362, 364, 4846, 2487, 11, 309, 311, 445, 8936, 2487, 13, 407, 291, 643, 281, 1500, 341, 13, 583], "temperature": 0.0, "avg_logprob": -0.18593144924082655, "compression_ratio": 1.4227642276422765, "no_speech_prob": 3.9137030398705974e-05}, {"id": 141, "seek": 109468, "start": 1106.64, "end": 1119.88, "text": " what you can do is to generate test cases that rely on mapping the input and the input", "tokens": [437, 291, 393, 360, 307, 281, 8460, 1500, 3331, 300, 10687, 322, 18350, 264, 4846, 293, 264, 4846], "temperature": 0.0, "avg_logprob": -0.18593144924082655, "compression_ratio": 1.4227642276422765, "no_speech_prob": 3.9137030398705974e-05}, {"id": 142, "seek": 111988, "start": 1119.88, "end": 1128.64, "text": " changing changes in the text to expectations. I'll give three examples from very, very simple", "tokens": [4473, 2962, 294, 264, 2487, 281, 9843, 13, 286, 603, 976, 1045, 5110, 490, 588, 11, 588, 2199], "temperature": 0.0, "avg_logprob": -0.14797048099705432, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.858701378107071e-05}, {"id": 143, "seek": 111988, "start": 1128.64, "end": 1137.64, "text": " to a bit more complex. One is like the principle of minimum functionality. For example, if", "tokens": [281, 257, 857, 544, 3997, 13, 1485, 307, 411, 264, 8665, 295, 7285, 14980, 13, 1171, 1365, 11, 498], "temperature": 0.0, "avg_logprob": -0.14797048099705432, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.858701378107071e-05}, {"id": 144, "seek": 111988, "start": 1137.64, "end": 1144.2, "text": " you are building a sentiment prediction machine learning system, you could just have a test", "tokens": [291, 366, 2390, 257, 16149, 17630, 3479, 2539, 1185, 11, 291, 727, 445, 362, 257, 1500], "temperature": 0.0, "avg_logprob": -0.14797048099705432, "compression_ratio": 1.4838709677419355, "no_speech_prob": 9.858701378107071e-05}, {"id": 145, "seek": 114420, "start": 1144.2, "end": 1152.96, "text": " that says if you have extraordinary in the sentence, you should always predict that the", "tokens": [300, 1619, 498, 291, 362, 10581, 294, 264, 8174, 11, 291, 820, 1009, 6069, 300, 264], "temperature": 0.0, "avg_logprob": -0.1700244809760422, "compression_ratio": 1.5857988165680474, "no_speech_prob": 3.072220351896249e-05}, {"id": 146, "seek": 114420, "start": 1152.96, "end": 1160.16, "text": " model will say it's a positive message. Now you will probably tell me, yeah, but what", "tokens": [2316, 486, 584, 309, 311, 257, 3353, 3636, 13, 823, 291, 486, 1391, 980, 385, 11, 1338, 11, 457, 437], "temperature": 0.0, "avg_logprob": -0.1700244809760422, "compression_ratio": 1.5857988165680474, "no_speech_prob": 3.072220351896249e-05}, {"id": 147, "seek": 114420, "start": 1160.16, "end": 1170.1200000000001, "text": " about if the user has written it's not extraordinary or absolutely not extraordinary? And that", "tokens": [466, 498, 264, 4195, 575, 3720, 309, 311, 406, 10581, 420, 3122, 406, 10581, 30, 400, 300], "temperature": 0.0, "avg_logprob": -0.1700244809760422, "compression_ratio": 1.5857988165680474, "no_speech_prob": 3.072220351896249e-05}, {"id": 148, "seek": 117012, "start": 1170.12, "end": 1177.9199999999998, "text": " actually brings me to the concept of test template. And the fact that probably for NLP,", "tokens": [767, 5607, 385, 281, 264, 3410, 295, 1500, 12379, 13, 400, 264, 1186, 300, 1391, 337, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.1339502977521232, "compression_ratio": 1.6515837104072397, "no_speech_prob": 8.378708298550919e-05}, {"id": 149, "seek": 117012, "start": 1177.9199999999998, "end": 1182.4799999999998, "text": " what you need to do, and this is obviously language specific, is start to have templates", "tokens": [437, 291, 643, 281, 360, 11, 293, 341, 307, 2745, 2856, 2685, 11, 307, 722, 281, 362, 21165], "temperature": 0.0, "avg_logprob": -0.1339502977521232, "compression_ratio": 1.6515837104072397, "no_speech_prob": 8.378708298550919e-05}, {"id": 150, "seek": 117012, "start": 1182.4799999999998, "end": 1189.8, "text": " where you change the text by, for example, adding negations. And then so you might want", "tokens": [689, 291, 1319, 264, 2487, 538, 11, 337, 1365, 11, 5127, 2485, 763, 13, 400, 550, 370, 291, 1062, 528], "temperature": 0.0, "avg_logprob": -0.1339502977521232, "compression_ratio": 1.6515837104072397, "no_speech_prob": 8.378708298550919e-05}, {"id": 151, "seek": 117012, "start": 1189.8, "end": 1197.08, "text": " to test if your system, if you're adding negation, if you have a certain direction. Because normally", "tokens": [281, 1500, 498, 428, 1185, 11, 498, 291, 434, 5127, 2485, 399, 11, 498, 291, 362, 257, 1629, 3513, 13, 1436, 5646], "temperature": 0.0, "avg_logprob": -0.1339502977521232, "compression_ratio": 1.6515837104072397, "no_speech_prob": 8.378708298550919e-05}, {"id": 152, "seek": 119708, "start": 1197.08, "end": 1203.04, "text": " if the machine learning model has understood, it should, if it's about sentiment, understand", "tokens": [498, 264, 3479, 2539, 2316, 575, 7320, 11, 309, 820, 11, 498, 309, 311, 466, 16149, 11, 1223], "temperature": 0.0, "avg_logprob": -0.14518417630876815, "compression_ratio": 1.6605504587155964, "no_speech_prob": 4.7024008381413296e-05}, {"id": 153, "seek": 119708, "start": 1203.04, "end": 1210.76, "text": " that putting not an extraordinary or not good, you have then synonyms, will not affect the", "tokens": [300, 3372, 406, 364, 10581, 420, 406, 665, 11, 291, 362, 550, 5451, 2526, 2592, 11, 486, 406, 3345, 264], "temperature": 0.0, "avg_logprob": -0.14518417630876815, "compression_ratio": 1.6605504587155964, "no_speech_prob": 4.7024008381413296e-05}, {"id": 154, "seek": 119708, "start": 1210.76, "end": 1220.04, "text": " system too much. Or actually either your system, you want it to move to a certain direction", "tokens": [1185, 886, 709, 13, 1610, 767, 2139, 428, 1185, 11, 291, 528, 309, 281, 1286, 281, 257, 1629, 3513], "temperature": 0.0, "avg_logprob": -0.14518417630876815, "compression_ratio": 1.6605504587155964, "no_speech_prob": 4.7024008381413296e-05}, {"id": 155, "seek": 119708, "start": 1220.04, "end": 1226.12, "text": " or there are cases where you want actually the opposite behavior. You want robustness.", "tokens": [420, 456, 366, 3331, 689, 291, 528, 767, 264, 6182, 5223, 13, 509, 528, 13956, 1287, 13], "temperature": 0.0, "avg_logprob": -0.14518417630876815, "compression_ratio": 1.6605504587155964, "no_speech_prob": 4.7024008381413296e-05}, {"id": 156, "seek": 122612, "start": 1226.12, "end": 1235.12, "text": " So that's called invariance. So for instance, you will want a system that is robust to typos", "tokens": [407, 300, 311, 1219, 33270, 719, 13, 407, 337, 5197, 11, 291, 486, 528, 257, 1185, 300, 307, 13956, 281, 2125, 329], "temperature": 0.0, "avg_logprob": -0.16215781090964734, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.696000440977514e-05}, {"id": 157, "seek": 122612, "start": 1235.12, "end": 1246.12, "text": " to just changing like a location name, just putting synonyms, et cetera, et cetera.", "tokens": [281, 445, 4473, 411, 257, 4914, 1315, 11, 445, 3372, 5451, 2526, 2592, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.16215781090964734, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.696000440977514e-05}, {"id": 158, "seek": 122612, "start": 1246.12, "end": 1254.28, "text": " So we've created this diagram to explain it. And it's a really thriving field in the research.", "tokens": [407, 321, 600, 2942, 341, 10686, 281, 2903, 309, 13, 400, 309, 311, 257, 534, 30643, 2519, 294, 264, 2132, 13], "temperature": 0.0, "avg_logprob": -0.16215781090964734, "compression_ratio": 1.5055555555555555, "no_speech_prob": 8.696000440977514e-05}, {"id": 159, "seek": 125428, "start": 1254.28, "end": 1259.6399999999999, "text": " There is a lot of research going on these days about testing machine learning systems.", "tokens": [821, 307, 257, 688, 295, 2132, 516, 322, 613, 1708, 466, 4997, 3479, 2539, 3652, 13], "temperature": 0.0, "avg_logprob": -0.16817930661714994, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.00011149048077641055}, {"id": 160, "seek": 125428, "start": 1259.6399999999999, "end": 1267.68, "text": " And metamorphic testing is one of the leading methods to do that. The principle is, if I", "tokens": [400, 1131, 50232, 299, 4997, 307, 472, 295, 264, 5775, 7150, 281, 360, 300, 13, 440, 8665, 307, 11, 498, 286], "temperature": 0.0, "avg_logprob": -0.16817930661714994, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.00011149048077641055}, {"id": 161, "seek": 125428, "start": 1267.68, "end": 1274.8799999999999, "text": " take an analogy, is very similar to if you've worked in finance or if you have some friends", "tokens": [747, 364, 21663, 11, 307, 588, 2531, 281, 498, 291, 600, 2732, 294, 10719, 420, 498, 291, 362, 512, 1855], "temperature": 0.0, "avg_logprob": -0.16817930661714994, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.00011149048077641055}, {"id": 162, "seek": 127488, "start": 1274.88, "end": 1284.72, "text": " who work there, the principle of backtesting an investment strategy. You simulate different", "tokens": [567, 589, 456, 11, 264, 8665, 295, 646, 83, 8714, 364, 6078, 5206, 13, 509, 27817, 819], "temperature": 0.0, "avg_logprob": -0.14468941195257778, "compression_ratio": 1.5083798882681565, "no_speech_prob": 5.779186903964728e-05}, {"id": 163, "seek": 127488, "start": 1284.72, "end": 1290.92, "text": " changes in the market conditions and you see how your strategy, your algorithm behaves,", "tokens": [2962, 294, 264, 2142, 4487, 293, 291, 536, 577, 428, 5206, 11, 428, 9284, 36896, 11], "temperature": 0.0, "avg_logprob": -0.14468941195257778, "compression_ratio": 1.5083798882681565, "no_speech_prob": 5.779186903964728e-05}, {"id": 164, "seek": 127488, "start": 1290.92, "end": 1302.96, "text": " what is the variance of that strategy. This concept applies very well to machine learning.", "tokens": [437, 307, 264, 21977, 295, 300, 5206, 13, 639, 3410, 13165, 588, 731, 281, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14468941195257778, "compression_ratio": 1.5083798882681565, "no_speech_prob": 5.779186903964728e-05}, {"id": 165, "seek": 130296, "start": 1302.96, "end": 1315.4, "text": " So you need two things. You need one to define a perturbation. So what I was explaining earlier", "tokens": [407, 291, 643, 732, 721, 13, 509, 643, 472, 281, 6964, 257, 40468, 399, 13, 407, 437, 286, 390, 13468, 3071], "temperature": 0.0, "avg_logprob": -0.1313047136579241, "compression_ratio": 1.5, "no_speech_prob": 4.628115857485682e-05}, {"id": 166, "seek": 130296, "start": 1315.4, "end": 1322.8, "text": " in NLP, perturbation might be adding typos, adding negation. In another context, like", "tokens": [294, 426, 45196, 11, 40468, 399, 1062, 312, 5127, 2125, 329, 11, 5127, 2485, 399, 13, 682, 1071, 4319, 11, 411], "temperature": 0.0, "avg_logprob": -0.1313047136579241, "compression_ratio": 1.5, "no_speech_prob": 4.628115857485682e-05}, {"id": 167, "seek": 130296, "start": 1322.8, "end": 1330.76, "text": " let's say it's more in an industrial case, it might be about doubling the values of some", "tokens": [718, 311, 584, 309, 311, 544, 294, 364, 9987, 1389, 11, 309, 1062, 312, 466, 33651, 264, 4190, 295, 512], "temperature": 0.0, "avg_logprob": -0.1313047136579241, "compression_ratio": 1.5, "no_speech_prob": 4.628115857485682e-05}, {"id": 168, "seek": 133076, "start": 1330.76, "end": 1341.32, "text": " sensors or adding noise to an image. And then, pretty simply, you define a test expectation", "tokens": [14840, 420, 5127, 5658, 281, 364, 3256, 13, 400, 550, 11, 1238, 2935, 11, 291, 6964, 257, 1500, 14334], "temperature": 0.0, "avg_logprob": -0.18236393648035387, "compression_ratio": 1.697674418604651, "no_speech_prob": 6.021424997015856e-05}, {"id": 169, "seek": 133076, "start": 1341.32, "end": 1347.8, "text": " in terms of the metamorphic relation between the output of a machine learning model and", "tokens": [294, 2115, 295, 264, 1131, 50232, 299, 9721, 1296, 264, 5598, 295, 257, 3479, 2539, 2316, 293], "temperature": 0.0, "avg_logprob": -0.18236393648035387, "compression_ratio": 1.697674418604651, "no_speech_prob": 6.021424997015856e-05}, {"id": 170, "seek": 133076, "start": 1347.8, "end": 1354.8, "text": " the distribution of the output after perturbation. And once you have that, and if you have enough", "tokens": [264, 7316, 295, 264, 5598, 934, 40468, 399, 13, 400, 1564, 291, 362, 300, 11, 293, 498, 291, 362, 1547], "temperature": 0.0, "avg_logprob": -0.18236393648035387, "compression_ratio": 1.697674418604651, "no_speech_prob": 6.021424997015856e-05}, {"id": 171, "seek": 133076, "start": 1354.8, "end": 1360.0, "text": " data, then you can actually have, like you can do actual statistical tests, see there's", "tokens": [1412, 11, 550, 291, 393, 767, 362, 11, 411, 291, 393, 360, 3539, 22820, 6921, 11, 536, 456, 311], "temperature": 0.0, "avg_logprob": -0.18236393648035387, "compression_ratio": 1.697674418604651, "no_speech_prob": 6.021424997015856e-05}, {"id": 172, "seek": 136000, "start": 1360.0, "end": 1367.44, "text": " a difference in distribution, et cetera. So I won't have too much time to dive into all", "tokens": [257, 2649, 294, 7316, 11, 1030, 11458, 13, 407, 286, 1582, 380, 362, 886, 709, 565, 281, 9192, 666, 439], "temperature": 0.0, "avg_logprob": -0.1575601963286704, "compression_ratio": 1.543103448275862, "no_speech_prob": 5.33264537807554e-05}, {"id": 173, "seek": 136000, "start": 1367.44, "end": 1374.0, "text": " the details of this, but we have wrote a technical guide on this topic and you have a link in", "tokens": [264, 4365, 295, 341, 11, 457, 321, 362, 4114, 257, 6191, 5934, 322, 341, 4829, 293, 291, 362, 257, 2113, 294], "temperature": 0.0, "avg_logprob": -0.1575601963286704, "compression_ratio": 1.543103448275862, "no_speech_prob": 5.33264537807554e-05}, {"id": 174, "seek": 136000, "start": 1374.0, "end": 1383.68, "text": " QR code up there. Next, I'll talk a bit about a really tricky topic, which is AI fairness.", "tokens": [32784, 3089, 493, 456, 13, 3087, 11, 286, 603, 751, 257, 857, 466, 257, 534, 12414, 4829, 11, 597, 307, 7318, 29765, 13], "temperature": 0.0, "avg_logprob": -0.1575601963286704, "compression_ratio": 1.543103448275862, "no_speech_prob": 5.33264537807554e-05}, {"id": 175, "seek": 136000, "start": 1383.68, "end": 1388.76, "text": " And I want to emphasize that it's, at least our recommendation, is not to come at the", "tokens": [400, 286, 528, 281, 16078, 300, 309, 311, 11, 412, 1935, 527, 11879, 11, 307, 406, 281, 808, 412, 264], "temperature": 0.0, "avg_logprob": -0.1575601963286704, "compression_ratio": 1.543103448275862, "no_speech_prob": 5.33264537807554e-05}, {"id": 176, "seek": 138876, "start": 1388.76, "end": 1398.04, "text": " problem of AI ethics with a closed mind or a top-down definition of this is an ethical", "tokens": [1154, 295, 7318, 19769, 365, 257, 5395, 1575, 420, 257, 1192, 12, 5093, 7123, 295, 341, 307, 364, 18890], "temperature": 0.0, "avg_logprob": -0.15321981545650598, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0001781907631084323}, {"id": 177, "seek": 138876, "start": 1398.04, "end": 1406.64, "text": " system or no, this is an unethical system. My co-founder did his PhD on precisely on", "tokens": [1185, 420, 572, 11, 341, 307, 364, 517, 3293, 804, 1185, 13, 1222, 598, 12, 33348, 630, 702, 14476, 322, 13402, 322], "temperature": 0.0, "avg_logprob": -0.15321981545650598, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0001781907631084323}, {"id": 178, "seek": 138876, "start": 1406.64, "end": 1415.08, "text": " this topic and wrote a full paper on this, looking at the philosophical and sociological", "tokens": [341, 4829, 293, 4114, 257, 1577, 3035, 322, 341, 11, 1237, 412, 264, 25066, 293, 3075, 4383], "temperature": 0.0, "avg_logprob": -0.15321981545650598, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0001781907631084323}, {"id": 179, "seek": 141508, "start": 1415.08, "end": 1425.8799999999999, "text": " implications of this. And the gist of it is that, yes, to a certain extent, you can adopt", "tokens": [16602, 295, 341, 13, 400, 264, 290, 468, 295, 309, 307, 300, 11, 2086, 11, 281, 257, 1629, 8396, 11, 291, 393, 6878], "temperature": 0.0, "avg_logprob": -0.13148513310392138, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00016151103773154318}, {"id": 180, "seek": 141508, "start": 1425.8799999999999, "end": 1433.28, "text": " a top-down approach to AI fairness, saying, well, for instance, as an organization, we", "tokens": [257, 1192, 12, 5093, 3109, 281, 7318, 29765, 11, 1566, 11, 731, 11, 337, 5197, 11, 382, 364, 4475, 11, 321], "temperature": 0.0, "avg_logprob": -0.13148513310392138, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00016151103773154318}, {"id": 181, "seek": 141508, "start": 1433.28, "end": 1441.56, "text": " want to test the fairness on explicitly free, sensitive categories. You can say, well, we", "tokens": [528, 281, 1500, 264, 29765, 322, 20803, 1737, 11, 9477, 10479, 13, 509, 393, 584, 11, 731, 11, 321], "temperature": 0.0, "avg_logprob": -0.13148513310392138, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00016151103773154318}, {"id": 182, "seek": 144156, "start": 1441.56, "end": 1449.6799999999998, "text": " want to check for gender balance. We want to check for race balance. That means if the", "tokens": [528, 281, 1520, 337, 7898, 4772, 13, 492, 528, 281, 1520, 337, 4569, 4772, 13, 663, 1355, 498, 264], "temperature": 0.0, "avg_logprob": -0.14335614717923678, "compression_ratio": 1.5574712643678161, "no_speech_prob": 6.088548616389744e-05}, {"id": 183, "seek": 144156, "start": 1449.6799999999998, "end": 1455.48, "text": " country where you deploy a machine learning allows to collect this data, this is not always", "tokens": [1941, 689, 291, 7274, 257, 3479, 2539, 4045, 281, 2500, 341, 1412, 11, 341, 307, 406, 1009], "temperature": 0.0, "avg_logprob": -0.14335614717923678, "compression_ratio": 1.5574712643678161, "no_speech_prob": 6.088548616389744e-05}, {"id": 184, "seek": 144156, "start": 1455.48, "end": 1466.2, "text": " the case. But the challenge with these approaches is that, A, you might not have the data to", "tokens": [264, 1389, 13, 583, 264, 3430, 365, 613, 11587, 307, 300, 11, 316, 11, 291, 1062, 406, 362, 264, 1412, 281], "temperature": 0.0, "avg_logprob": -0.14335614717923678, "compression_ratio": 1.5574712643678161, "no_speech_prob": 6.088548616389744e-05}, {"id": 185, "seek": 146620, "start": 1466.2, "end": 1473.56, "text": " measure this, and B, you may miss out because often when this exercise of defining the quality", "tokens": [3481, 341, 11, 293, 363, 11, 291, 815, 1713, 484, 570, 2049, 562, 341, 5380, 295, 17827, 264, 3125], "temperature": 0.0, "avg_logprob": -0.1332227678009958, "compression_ratio": 1.483695652173913, "no_speech_prob": 6.568203389178962e-05}, {"id": 186, "seek": 146620, "start": 1473.56, "end": 1485.8, "text": " criteria for fairness and for balance are done, you only have a limited sample. So it's,", "tokens": [11101, 337, 29765, 293, 337, 4772, 366, 1096, 11, 291, 787, 362, 257, 5567, 6889, 13, 407, 309, 311, 11], "temperature": 0.0, "avg_logprob": -0.1332227678009958, "compression_ratio": 1.483695652173913, "no_speech_prob": 6.568203389178962e-05}, {"id": 187, "seek": 146620, "start": 1485.8, "end": 1493.68, "text": " in taking some sociological analysis, it's really important to have this kind of top-down", "tokens": [294, 1940, 512, 3075, 4383, 5215, 11, 309, 311, 534, 1021, 281, 362, 341, 733, 295, 1192, 12, 5093], "temperature": 0.0, "avg_logprob": -0.1332227678009958, "compression_ratio": 1.483695652173913, "no_speech_prob": 6.568203389178962e-05}, {"id": 188, "seek": 149368, "start": 1493.68, "end": 1501.2, "text": " definition of AI ethics, meet the reality on the ground, and confront the actual users", "tokens": [7123, 295, 7318, 19769, 11, 1677, 264, 4103, 322, 264, 2727, 11, 293, 12422, 264, 3539, 5022], "temperature": 0.0, "avg_logprob": -0.1937689463297526, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000245117669692263}, {"id": 189, "seek": 149368, "start": 1501.2, "end": 1508.28, "text": " and the makers of the systems to get them to define the definition of ethics, rather", "tokens": [293, 264, 19323, 295, 264, 3652, 281, 483, 552, 281, 6964, 264, 7123, 295, 19769, 11, 2831], "temperature": 0.0, "avg_logprob": -0.1937689463297526, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000245117669692263}, {"id": 190, "seek": 149368, "start": 1508.28, "end": 1515.68, "text": " than a big organization, if I put a bit of a caricature that says, AI ethics, yeah, we", "tokens": [813, 257, 955, 4475, 11, 498, 286, 829, 257, 857, 295, 257, 45732, 1503, 300, 1619, 11, 7318, 19769, 11, 1338, 11, 321], "temperature": 0.0, "avg_logprob": -0.1937689463297526, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000245117669692263}, {"id": 191, "seek": 149368, "start": 1515.68, "end": 1522.0, "text": " wrote a charter about this. You follow, you read this, you sign, and then, oops, you're", "tokens": [4114, 257, 27472, 466, 341, 13, 509, 1524, 11, 291, 1401, 341, 11, 291, 1465, 11, 293, 550, 11, 34166, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.1937689463297526, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000245117669692263}, {"id": 192, "seek": 152200, "start": 1522.0, "end": 1533.44, "text": " ethical. Having said that, so there are some good top-down metrics to adopt that are kind", "tokens": [18890, 13, 10222, 848, 300, 11, 370, 456, 366, 512, 665, 1192, 12, 5093, 16367, 281, 6878, 300, 366, 733], "temperature": 0.0, "avg_logprob": -0.1407482147216797, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00012388172035571188}, {"id": 193, "seek": 152200, "start": 1533.44, "end": 1540.16, "text": " of a baseline, and I'll explain one of them, which is disparate impact. Disparate impact", "tokens": [295, 257, 20518, 11, 293, 286, 603, 2903, 472, 295, 552, 11, 597, 307, 14548, 473, 2712, 13, 4208, 2181, 473, 2712], "temperature": 0.0, "avg_logprob": -0.1407482147216797, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00012388172035571188}, {"id": 194, "seek": 152200, "start": 1540.16, "end": 1549.04, "text": " is actually a metric from the human resources management industry from at least 40 years", "tokens": [307, 767, 257, 20678, 490, 264, 1952, 3593, 4592, 3518, 490, 412, 1935, 3356, 924], "temperature": 0.0, "avg_logprob": -0.1407482147216797, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00012388172035571188}, {"id": 195, "seek": 154904, "start": 1549.04, "end": 1559.48, "text": " ago, so it's not new. That says, so it's probabilities, but essentially it's about setting a rule", "tokens": [2057, 11, 370, 309, 311, 406, 777, 13, 663, 1619, 11, 370, 309, 311, 33783, 11, 457, 4476, 309, 311, 466, 3287, 257, 4978], "temperature": 0.0, "avg_logprob": -0.17931900853696076, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.00013398837472777814}, {"id": 196, "seek": 154904, "start": 1559.48, "end": 1569.32, "text": " of 80%, where you measure the probability of, you define a positive outcome with respect", "tokens": [295, 4688, 8923, 689, 291, 3481, 264, 8482, 295, 11, 291, 6964, 257, 3353, 9700, 365, 3104], "temperature": 0.0, "avg_logprob": -0.17931900853696076, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.00013398837472777814}, {"id": 197, "seek": 154904, "start": 1569.32, "end": 1578.24, "text": " to a given protected population, and you say, well, I want to the proportion of the probability", "tokens": [281, 257, 2212, 10594, 4415, 11, 293, 291, 584, 11, 731, 11, 286, 528, 281, 264, 16068, 295, 264, 8482], "temperature": 0.0, "avg_logprob": -0.17931900853696076, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.00013398837472777814}, {"id": 198, "seek": 157824, "start": 1578.24, "end": 1586.36, "text": " of a positive outcome relative to the probability of a positive outcome in the unprotected context", "tokens": [295, 257, 3353, 9700, 4972, 281, 264, 8482, 295, 257, 3353, 9700, 294, 264, 517, 33629, 39963, 4319], "temperature": 0.0, "avg_logprob": -0.1848437250876913, "compression_ratio": 1.4645669291338583, "no_speech_prob": 5.126671749167144e-05}, {"id": 199, "seek": 157824, "start": 1586.36, "end": 1600.52, "text": " to be above 80%. So, for instance, so if you want to apply that to a, oops, to put more", "tokens": [281, 312, 3673, 4688, 6856, 407, 11, 337, 5197, 11, 370, 498, 291, 528, 281, 3079, 300, 281, 257, 11, 34166, 11, 281, 829, 544], "temperature": 0.0, "avg_logprob": -0.1848437250876913, "compression_ratio": 1.4645669291338583, "no_speech_prob": 5.126671749167144e-05}, {"id": 200, "seek": 160052, "start": 1600.52, "end": 1608.84, "text": " concrete, yeah, so if, say, you're building a model to predict the churn of customers,", "tokens": [9859, 11, 1338, 11, 370, 498, 11, 584, 11, 291, 434, 2390, 257, 2316, 281, 6069, 264, 417, 925, 295, 4581, 11], "temperature": 0.0, "avg_logprob": -0.19875798296572558, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0001374296407448128}, {"id": 201, "seek": 160052, "start": 1608.84, "end": 1616.32, "text": " and you want to check whether your model is biased or not for each class, this formula", "tokens": [293, 291, 528, 281, 1520, 1968, 428, 2316, 307, 28035, 420, 406, 337, 1184, 1508, 11, 341, 8513], "temperature": 0.0, "avg_logprob": -0.19875798296572558, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0001374296407448128}, {"id": 202, "seek": 160052, "start": 1616.32, "end": 1628.48, "text": " allows you to really define this metric and write a concrete test case. Right, so I just", "tokens": [4045, 291, 281, 534, 6964, 341, 20678, 293, 2464, 257, 9859, 1500, 1389, 13, 1779, 11, 370, 286, 445], "temperature": 0.0, "avg_logprob": -0.19875798296572558, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0001374296407448128}, {"id": 203, "seek": 162848, "start": 1628.48, "end": 1635.6, "text": " have three minutes, so I'll highlight what one of the features of our project enables", "tokens": [362, 1045, 2077, 11, 370, 286, 603, 5078, 437, 472, 295, 264, 4122, 295, 527, 1716, 17077], "temperature": 0.0, "avg_logprob": -0.16454384650713133, "compression_ratio": 1.634703196347032, "no_speech_prob": 8.561224967706949e-05}, {"id": 204, "seek": 162848, "start": 1635.6, "end": 1642.08, "text": " is putting human feedback, so really having an interface where users and not only data", "tokens": [307, 3372, 1952, 5824, 11, 370, 534, 1419, 364, 9226, 689, 5022, 293, 406, 787, 1412], "temperature": 0.0, "avg_logprob": -0.16454384650713133, "compression_ratio": 1.634703196347032, "no_speech_prob": 8.561224967706949e-05}, {"id": 205, "seek": 162848, "start": 1642.08, "end": 1649.0, "text": " scientists can change the parameters, so there's a link to metamorphic testing, and actually", "tokens": [7708, 393, 1319, 264, 9834, 11, 370, 456, 311, 257, 2113, 281, 1131, 50232, 299, 4997, 11, 293, 767], "temperature": 0.0, "avg_logprob": -0.16454384650713133, "compression_ratio": 1.634703196347032, "no_speech_prob": 8.561224967706949e-05}, {"id": 206, "seek": 162848, "start": 1649.0, "end": 1655.04, "text": " give human feedback to a point art where the biases may be, and the benefit of this approach", "tokens": [976, 1952, 5824, 281, 257, 935, 1523, 689, 264, 32152, 815, 312, 11, 293, 264, 5121, 295, 341, 3109], "temperature": 0.0, "avg_logprob": -0.16454384650713133, "compression_ratio": 1.634703196347032, "no_speech_prob": 8.561224967706949e-05}, {"id": 207, "seek": 165504, "start": 1655.04, "end": 1665.96, "text": " is that it allows for the community to precisely define what they think are the risks. So sadly,", "tokens": [307, 300, 309, 4045, 337, 264, 1768, 281, 13402, 6964, 437, 436, 519, 366, 264, 10888, 13, 407, 22023, 11], "temperature": 0.0, "avg_logprob": -0.16106903553009033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.929693336132914e-05}, {"id": 208, "seek": 165504, "start": 1665.96, "end": 1676.24, "text": " we won't have time to do a demo, but this phase, in our project, we call that the inspection", "tokens": [321, 1582, 380, 362, 565, 281, 360, 257, 10723, 11, 457, 341, 5574, 11, 294, 527, 1716, 11, 321, 818, 300, 264, 22085], "temperature": 0.0, "avg_logprob": -0.16106903553009033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.929693336132914e-05}, {"id": 209, "seek": 165504, "start": 1676.24, "end": 1683.24, "text": " phase, and it's about before you test, and this is super important, and again, one of", "tokens": [5574, 11, 293, 309, 311, 466, 949, 291, 1500, 11, 293, 341, 307, 1687, 1021, 11, 293, 797, 11, 472, 295], "temperature": 0.0, "avg_logprob": -0.16106903553009033, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.929693336132914e-05}, {"id": 210, "seek": 168324, "start": 1683.24, "end": 1689.48, "text": " the things where it's different from traditional software testing, before you even test, you", "tokens": [264, 721, 689, 309, 311, 819, 490, 5164, 4722, 4997, 11, 949, 291, 754, 1500, 11, 291], "temperature": 0.0, "avg_logprob": -0.1640030061355745, "compression_ratio": 1.6793893129770991, "no_speech_prob": 7.619846292072907e-05}, {"id": 211, "seek": 168324, "start": 1689.48, "end": 1694.96, "text": " need to confront yourself with the data and the model, so that's where actually we think", "tokens": [643, 281, 12422, 1803, 365, 264, 1412, 293, 264, 2316, 11, 370, 300, 311, 689, 767, 321, 519], "temperature": 0.0, "avg_logprob": -0.1640030061355745, "compression_ratio": 1.6793893129770991, "no_speech_prob": 7.619846292072907e-05}, {"id": 212, "seek": 168324, "start": 1694.96, "end": 1700.68, "text": " explainability methods really shine, it's because they allow to debug and to identify", "tokens": [2903, 2310, 7150, 534, 12207, 11, 309, 311, 570, 436, 2089, 281, 24083, 293, 281, 5876], "temperature": 0.0, "avg_logprob": -0.1640030061355745, "compression_ratio": 1.6793893129770991, "no_speech_prob": 7.619846292072907e-05}, {"id": 213, "seek": 168324, "start": 1700.68, "end": 1706.44, "text": " the zones of risks, and this is precisely what helps once you have qualified feedback", "tokens": [264, 16025, 295, 10888, 11, 293, 341, 307, 13402, 437, 3665, 1564, 291, 362, 15904, 5824], "temperature": 0.0, "avg_logprob": -0.1640030061355745, "compression_ratio": 1.6793893129770991, "no_speech_prob": 7.619846292072907e-05}, {"id": 214, "seek": 168324, "start": 1706.44, "end": 1712.24, "text": " to know where you should put your effort in test, so in a nutshell what I'm saying for", "tokens": [281, 458, 689, 291, 820, 829, 428, 4630, 294, 1500, 11, 370, 294, 257, 37711, 437, 286, 478, 1566, 337], "temperature": 0.0, "avg_logprob": -0.1640030061355745, "compression_ratio": 1.6793893129770991, "no_speech_prob": 7.619846292072907e-05}, {"id": 215, "seek": 171224, "start": 1712.24, "end": 1718.24, "text": " testing machine learning systems is it's not a matter of creating hundreds of tests, of", "tokens": [4997, 3479, 2539, 3652, 307, 309, 311, 406, 257, 1871, 295, 4084, 6779, 295, 6921, 11, 295], "temperature": 0.0, "avg_logprob": -0.17188956623985654, "compression_ratio": 1.5495495495495495, "no_speech_prob": 0.00011671989341266453}, {"id": 216, "seek": 171224, "start": 1718.24, "end": 1723.44, "text": " automating everything, but rather to have a good idea of, from a fairness standpoint", "tokens": [3553, 990, 1203, 11, 457, 2831, 281, 362, 257, 665, 1558, 295, 11, 490, 257, 29765, 15827], "temperature": 0.0, "avg_logprob": -0.17188956623985654, "compression_ratio": 1.5495495495495495, "no_speech_prob": 0.00011671989341266453}, {"id": 217, "seek": 171224, "start": 1723.44, "end": 1731.76, "text": " and for a performance standpoint, of what are the 10, 15, maybe max 20 tests that you", "tokens": [293, 337, 257, 3389, 15827, 11, 295, 437, 366, 264, 1266, 11, 2119, 11, 1310, 11469, 945, 6921, 300, 291], "temperature": 0.0, "avg_logprob": -0.17188956623985654, "compression_ratio": 1.5495495495495495, "no_speech_prob": 0.00011671989341266453}, {"id": 218, "seek": 171224, "start": 1731.76, "end": 1740.48, "text": " want in your platform. If you want to get started actually on it, this is our GitHub,", "tokens": [528, 294, 428, 3663, 13, 759, 291, 528, 281, 483, 1409, 767, 322, 309, 11, 341, 307, 527, 23331, 11], "temperature": 0.0, "avg_logprob": -0.17188956623985654, "compression_ratio": 1.5495495495495495, "no_speech_prob": 0.00011671989341266453}, {"id": 219, "seek": 174048, "start": 1740.48, "end": 1744.56, "text": " and if you have a machine learning system to test, we're interested in your feedback.", "tokens": [50364, 293, 498, 291, 362, 257, 3479, 2539, 1185, 281, 1500, 11, 321, 434, 3102, 294, 428, 5824, 13, 50568], "temperature": 0.0, "avg_logprob": -0.22788940157209123, "compression_ratio": 1.0897435897435896, "no_speech_prob": 0.0001664531882852316}], "language": "en"}