{"text": " All right, everyone, can we get a big welcome to Tuana? Can you hear me? Great. So if anyone was here for the talk before, just a disclaimer, I'm not as good a public speaker. I think I enjoy malware so much, but it's all downhill from there, so just FYI. All right, so I'm going to be talking about building a semantic search application in Python, and specifically we're going to be using an open source framework called Haystack, and that's why I'm here. So a bit about me, I'm a developer advocate at DeepSet, and we maintain Haystack. And yeah, so this is some information about me, but let's just dive right into it. So the agenda I'm going to follow, I'm going to try to keep the NLP stuff quite high level and focus on the how to build bit, but I do have to give a bit of a high level explanation, so I'm going to do a brief history on what we mean by semantic search. Please do not judge me for this example, Kardashian sisters. So let's assume we have a bunch of documents and let's see what would happen if we do some keyword search on it, and let's say we've got the query Kardashian sisters. You might get something a bit like this, which is great, and you can see that there's some clever stuff going on here, sisters maybe associated with siblings and family as well. Keyword search is still very widely used, but this is the type of result you might get from a corpus of documents you might have. But what if that's just not enough? What if I want to be able to ask something like, who is the richest Kardashian sister? How do I make this system understand what I'm trying to get to? So for that, let's have a look at this. There might be some names you've already seen here, especially the last one there. I think everyone and their grandparents have heard of this by now, chat GPT. So these are language models. I'm going to briefly walk through where they get such impressive functionality from. So most of them are based on what we call transformers. What those are doing is what I try to depict at the top here. So imagine that thing in the middle as the language model. And very, very simply put, obviously every model does something a bit different or for slightly different use cases, let's say. Given a piece of text, they will produce some sort of vector representation of that text. They're trained on very vast amounts of text data, and then this is what we get at the end of the day. And this is cool because it's enabled us to do many different things. We can use those vectors to compare them to each other, like dog might be close to cat but far away from teapot, for example. And that's enabled us to do a lot of different things like question answering, summarization, what we call retrieval, so document retrieval. And it's all thanks to these transformers. And a lot of these use cases are often grouped under the term search because actually what's happening in the background is a very clever search algorithm. So question answering and retrieval specifically can be grouped under search. All right, how does this work? And I'm very briefly going to go through what these different types of models do and how they do what they do, and I'm going to talk about the evolution from extractive models to now generative models like chat GPT, for example. The very simple one, and we're going to build our first semantic search application with this type of model, is often referred to as the reader model, simply a question answering model, very specifically an extractive question answering model. The way these work are given a piece of context and query, they're very good at looking through that context and finding, extracting the answer from that context, but it does need that context. Obviously, there are some limitations to these models because they're limited by input length. I can't give it just infinite amounts of data. But we have come up with ways to make that a bit more efficient, and we've introduced models that we often refer to as retriever models, or embedding models. These don't necessarily have to be language models, I'm going to be looking at language models, it could also be based on keyword search that we saw before. But what they do is they act as a sort of filter, so let's say you've got a bunch of documents, let's say you've got thousands and thousands of documents, and the retriever can basically say, hey, I've got this query, and this is the top five, ten most relevant documents that you should look at, and then that means that the reader doesn't have to look through anything. So we actually gain a lot of speed out of this. All right, finally, this is all the hype today, and you'll notice, well, one thing you should notice is you see that the document context, anything like that, I've chopped it off, it's just a query. So these new language models, they don't actually need context. You can give it context, but it doesn't require context. And this is very cool, because they produce human-like answers. What they're trained to do, the task to do, is not extracting answers, it's generating answers. And I just want to point out there are two things here. It doesn't necessarily have to be answers. So I'm going to be looking at an answer generator, but it can just be, you know, prompt it to produce some context, it doesn't necessarily have to be an answer to a question. So we've been seeing this, maybe you've seen some of these scenes lately, so this is chat GPT again on the theme, who is the tallest Kardashian sister, it hasn't just extracted Kendall for me, it said, the tallest Kardashian sister is Kendall Jenner, perfect. But let's see what happens if it's not like a question. This is not my creativity, by the way, but I think it's amazing. Write a poem about Fostam in the style of Markdown, change log, that's what you get. There you go. All right, so these language models are readily available. You might have already heard these names, OpenAI, Kahir. They provide these increasingly large language models. There is a difference when we say language model and large language model, but leave that aside for now, let's not talk about that. There are also many, many, many open source models on Huggingface, and if you don't know what Huggingface is, I think very simply put, I like to refer it to sort of like the GitHub of machine learning. So you can host your open source models and other developers can use them, use them in their projects or even contribute to them. And what's really cool about them, like I said, your search results stop becoming just simple search results, they are human-like answers. So now let's look at how we use these language models for various use cases. For that, I want to talk about Haystack, this is why I'm here. So Haystack is an open source NLP framework built in Python, and what it achieves is basically what this picture is trying to show you. You're free to build your own end-to-end NLP application, and each of those green boxes are a high-level component in Haystack. There are retrievers that we looked at, there are readers that we looked at, we'll look at some different ones as well, and each of these are basically the main class, and you might have different types of readers, different types of retrievers. For example, there could be a reader that is good at looking at paragraphs and extracting answers, but there might be a reader type called table reader that's good at looking at tables and retrieving answers from that. There are integrations with HuggingFace, so that means you can just download a model off of HuggingFace, but also open AI here, obviously you need to provide an API key, but you are free to use those as well. A building in an NLP application isn't just about the search component, you presumably have lots of documents somewhere, maybe the PDFs, maybe the TXDs, so they're components for you to build your indexing pipeline that we call so that you can write your data somewhere in a way that can be used by these language models. Some of those components, we already talked briefly about the reader and the retriever, we're going to be using those. There could be an answer generator, a question generator, we're not going to look at that today, but that's really cool because then you can use those questions to train another model, for example. Summarizer, prompt node, we're going to very briefly look into that, but you get the idea. There's a bunch of components and each of them might have types under them. You can use data connectors, file converters as mentioned, pre-processing your documents in a way that's going to be a bit more useful to the language model, for example, and of course, you need to keep your data somewhere, so you might decide you want to use elastic search or open search, or you might want to use something a bit more vector optimized, and these are all available in the Haystack framework. This is the idea of, I talked about the nodes, but the idea behind building with these nodes is to build your own pipeline. This is just an example. You really don't have to pay attention to the actual names of these components, but to give you an idea. You are free to decide what path your application should take based on a decision. For example, here we have what we call the query classifier, so let's say a user enters a keyword, there's no point in doing fancy embedding search, maybe, so you might route it to keyword search. If the user enters something that's more like a human-formed question, you might say, okay, do some what we call dense retrieval or embedding retrieval. That's just an example. Finally, I'm not going to get into this today at all, but let's say you have a running application, you can just provide it through REST API, and then you're free to query it, upload more files, and index them, and so on. All right, so let's look at how that might look first thing you do is install farm Haystack. If you're curious as to why there is farm at the beginning there, you can talk about this later. It's a bit about the history of the company. Then we just simply initialize two things, the retriever. Here we specifically have the embedding retriever, and notice that I'm giving it the document stall, so the retriever already knows where to look for these documents, and then we define an embedding model. I mentioned that these retrievers could be keyword retrieval, or it could be retrieval based on some embedding representation. Here we're basically saying use this sum model name, so it's just a model, to create the vector representations. Then I'm initializing a reader, and this is a very commonly used, let's say, extract a question answering model. Again, some other model, and these are both off of hugging face, let's imagine. We've got this retriever, and it's connected to a document store, and we've got a reader. How would we build our pipeline? We would first initialize a pipeline, and then the first thing we add is the first node, and we're saying retriever. I'm first adding the retriever, and that input you see, inputs query, is actually a special input in Haystack, and it's usually indicating that this is the entry point. This is the first thing that gets the query, so okay, we've told it, you've got the query. I could leave it here, and this pipeline, if I run it, what it's doing is, given a query, it's just dumping out documents for me. That's what the retriever does, it's just going to return to me the most relevant documents. I want to build a question answering pipeline, so I would maybe add a second node, and I would say now this is the question answering model node, and anything that's the output from the retriever is an input to this node. That's simply it. You could do this, but you could also just use pre-made pipelines. This is a very common one, so we do have a pre-made pipeline for it, and it's just simply called an extractive QA pipeline, and you just tell it what retriever and what reader to use, but the pipeline I built before, that's just a lot more flexible. I'm free to add any more nodes to this, I'm free to extract any nodes from this, so it's just a better way to build your own pipeline. Then simply what I do is I run what now looks like a very random question, but we'll get to it. Then hopefully you have a working system, and you've got an answer. Great. I'm going to build an actual example, so I want to set the scene, and I was very lazy. This is actually the exact example we have in our first tutorial on our website, but let's assume we have a document store somewhere, and it has a bunch of documents, TXT files about Game of Thrones. I'm going to make this document store FIES document store. This is one of the options, so let's assume I've got FIES document store, and of course I want to do question answering, and I want this to be efficient, so we're going to build exactly that pipeline we just saw before, Retriever followed by a reader. Specifically, I'm going to use an embedding Retriever, so these are the ones that can actually look at vector representations and extract the most similar ones, and then we are going to have a reader, simply a question answering node at the end. How would that look? I first initialize my document store. This is basically, I'm not going through the indexing one just now, we'll look at that in a bit, but let's assume the files are already indexed, and they're in that FIES document store, and then I've got a Retriever, I'm telling it where to look, and look at my document store, and I'm using this very specific embedding model of a hugging face. I then tell the Retriever to update all of the embeddings in my document store, so it's basically using that model to create vector representations of all of my TXD files, and then I'm initializing a reader. Same thing that we did before, I'm just using a specific model of a hugging face, this is trained by the company I work for too. Then I do the exact same thing I did before. I'm just creating the pipeline, adding the nodes, and then I run maybe who is the father of ARIA stock, and this is what I might get back as an answer. The thing to notice here, the answers are very eddard, Ned, and that's because it's not generating answers, it's extracting the answer that's already in the context. If you see the first answer below, you'll notice that there's eddard in there, and this pipeline and this model has decided this is the most relevant answer to you, I could have printed out schools, you can get schools, I just haven't here, and then I said give me the top five. The first two, three, I think are correct, so we've got something working, but what if I want to generate human sounding like answers, eddard is pretty okay, I've got the answer, but maybe I want a system, maybe I want to create a chatbot that talks to me. Let's look at how we might do that. This is going to be a bit of a special example, because I'm not going to build a pipeline. The reason for that is, as mentioned before, these generative models don't need context, so I should be able to just use them. We've got this node called the prompt node, and what this does is actually a special node, because you can more fit based on what you want it to do. You might have heard recently this whole terminology around prompt engineering, and that's basically used with models that are able to consume some instruction and act accordingly. By default, our prompt node is basically told, you know, just answer the question, that's all it does, but you could maybe define a template for it, what we call a prompt template, so I could have maybe said, you know, answer the question as a yes or no answer, and it would give me a yes or no answer, but obviously I need to ask it a yes or no question for it to make sense. Anyway, so I'm just using it like this, like the pure form, and I'm using a model from OpenAI, obviously I need to provide an API key, and I'm using this particular one, text of inchy 003. I actually ran these yesterday, so these are the replies I got, and this particular one I ran a few times, so the first time I ran, when is Milosh flying to Frankfurt? By the way, spoiler alert, Milosh is our CEO. So I know who Milosh is, and I know when he's flying to Frankfurt, or when he flew to Frankfurt. And I get an answer, Milosh's flight to Frankfurt is scheduled for August 7th, 2020. This is really convincing sounding, fine, okay, but this one was actually quite impressive, again, if I ran the same exact query with this model, I got, it's not possible to answer this question without more information. This is actually really cool, because clearly this model sometimes can infer that, hey, maybe I need more information to give you an answer, that what we now refer to as hallucination. Maybe you've heard of that term, also these models can hallucinate, they're tasked to generate answers. It's not tasked to generate, you know, actual answers for you, that are truthful. Anyway, let's say, when is Milosh travelling somewhere? I love this answer, when he has the time and money available to do so. And then, I guess, I don't know which one is my favourite, this one, or the next one, who is Milosh? A Greek island. Lovely, okay, but the problem here is, this is very, you know, I could believe this, it's very like, realistic, these answers. So we're going to look at how we can use these large language models for our use cases, and what we're going to do is basically, we're going to do exactly what we did for the extractive QA1, and we're going to use a component that is quite clever, because it's been prompted to say, generate answers based off of these retrieved documents and nothing else. It can sometimes not work well, but there are ways to make it work well, and we won't get into all the creativity behind it, so I'll show you the most basic solution you might get. But this is going to be what we do, it's the same exact pipeline as before, the reader has been replaced by the generator. So I actually have Milosh's ticket to Frankfurt. It was 14th of November, and as a bonus, I thought I'd try, this is my ticket, my euro star ticket, from Amsterdam to London and back. So I've got these, and they are PDFs. And so now I'm going to start defining my new components. So I've got the same files document store, embedding dimensions is not something you should worry about for now, and I'm defining an embedding retriever here. What I'm doing is, again, I'm using a model by OpenAI, so I'm using an API key. So this is the model I'm going to use to create vector representations and then compare it to queries. And this time, I'm not using the front node, I'm using that clever node there, called the OpenAI answer generator. And you might notice it is the exact same model as the one before. We're going to briefly look at indexing, so we've got the PDF text converter and pre-processor. And let's go to the next slide. As mentioned before, there are pre-made pipelines, so I could have just defined generative QA pipeline and told it what generate and retriever to use, but let's look at what it might look like if I were to build it from scratch. And first, you see the indexing pipeline. So if you follow it, you'll notice that it's getting the PDF file and then writing that to a document store, given some pre-processing steps. And I then write my and Niloche's tickets in there. And the querying pipeline is the exact same as the extractive QA pipeline you saw before. All that, the only difference is, the last bit is the answer generator, not the reader. This time, though, it does have some context and it does have some documents. What did I get when I ran the same two questions? I got, who is Milosh? He's not a Greek island. He is the passenger whose travel data is on the passenger itinerary receipt. Now, this is the only information this model knows, so it can't tell me he's my CEO because I haven't uploaded any information about my company. So don't make something up, just tell me what you know. If I run, when is Milosh flying to Frankfurt? I get Milosh is flying to Frankfurt on the correct date and time. And then I had that bonus in there, who is traveling to London. I would get Twana Caelic is traveling to London. Now, if I were to run, let's say, who is, let's say, when is Alfred traveling to Frankfurt? What I haven't shown you here, because I think it goes a bit too deep into building these types of pipelines, for the open AI answer generator, I could actually provide examples and example documents. Just in case I'm worried that it's going to make up something somewhere at a time that this Alfred who doesn't exist is traveling to Frankfurt, I can give it some example saying, hey, if you encounter something like this, just say I don't have the context for it. So I could have just run query pipeline.run when is Alfred traveling to Frankfurt, and it would have told me I have no context for this, so I'm not going to give you the answer. This model that we saw does do that sometimes. The first example we saw, it did say I don't have enough context for this, but not all the time. So this is how you might use it for your own use cases, you might use large language models for your own use cases, and how you might mitigate them hallucinating. So to conclude, extractive question answering models and pipelines are great at retrieving knowledge that already exists in context, however, generative models are really cool because they can generate human-like answers, but combining them with a retrieval augmenter step means that you can use them very specifically for your own use cases. Haystack as I mentioned is fully open source, it's built in Python, and we accept contributions literally welcome, and I would say every release we have a community contribution in there. Thank you very much, and this QR code is our first tutorial, bear in mind it is an extractive one, it's the non-cool one, but it is a good way to start. Thank you very much. Thank you, Luana. We have a few minutes for questions, if you have questions for Luana, we have three minutes for questions, as you can also find her afterwards.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.32, "text": " All right, everyone, can we get a big welcome to Tuana?", "tokens": [50364, 1057, 558, 11, 1518, 11, 393, 321, 483, 257, 955, 2928, 281, 7836, 2095, 30, 51030], "temperature": 0.0, "avg_logprob": -0.43585673549719023, "compression_ratio": 1.25, "no_speech_prob": 0.5068008899688721}, {"id": 1, "seek": 0, "start": 13.32, "end": 15.76, "text": " Can you hear me?", "tokens": [51030, 1664, 291, 1568, 385, 30, 51152], "temperature": 0.0, "avg_logprob": -0.43585673549719023, "compression_ratio": 1.25, "no_speech_prob": 0.5068008899688721}, {"id": 2, "seek": 0, "start": 15.76, "end": 16.76, "text": " Great.", "tokens": [51152, 3769, 13, 51202], "temperature": 0.0, "avg_logprob": -0.43585673549719023, "compression_ratio": 1.25, "no_speech_prob": 0.5068008899688721}, {"id": 3, "seek": 0, "start": 16.76, "end": 27.240000000000002, "text": " So if anyone was here for the talk before, just a disclaimer, I'm not as good a public", "tokens": [51202, 407, 498, 2878, 390, 510, 337, 264, 751, 949, 11, 445, 257, 40896, 11, 286, 478, 406, 382, 665, 257, 1908, 51726], "temperature": 0.0, "avg_logprob": -0.43585673549719023, "compression_ratio": 1.25, "no_speech_prob": 0.5068008899688721}, {"id": 4, "seek": 0, "start": 27.240000000000002, "end": 28.240000000000002, "text": " speaker.", "tokens": [51726, 8145, 13, 51776], "temperature": 0.0, "avg_logprob": -0.43585673549719023, "compression_ratio": 1.25, "no_speech_prob": 0.5068008899688721}, {"id": 5, "seek": 2824, "start": 28.24, "end": 32.44, "text": " I think I enjoy malware so much, but it's all downhill from there, so just FYI.", "tokens": [50364, 286, 519, 286, 2103, 40747, 370, 709, 11, 457, 309, 311, 439, 29929, 490, 456, 11, 370, 445, 42730, 40, 13, 50574], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 6, "seek": 2824, "start": 32.44, "end": 36.36, "text": " All right, so I'm going to be talking about building a semantic search application in", "tokens": [50574, 1057, 558, 11, 370, 286, 478, 516, 281, 312, 1417, 466, 2390, 257, 47982, 3164, 3861, 294, 50770], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 7, "seek": 2824, "start": 36.36, "end": 41.239999999999995, "text": " Python, and specifically we're going to be using an open source framework called Haystack,", "tokens": [50770, 15329, 11, 293, 4682, 321, 434, 516, 281, 312, 1228, 364, 1269, 4009, 8388, 1219, 8721, 372, 501, 11, 51014], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 8, "seek": 2824, "start": 41.239999999999995, "end": 42.239999999999995, "text": " and that's why I'm here.", "tokens": [51014, 293, 300, 311, 983, 286, 478, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 9, "seek": 2824, "start": 42.239999999999995, "end": 47.879999999999995, "text": " So a bit about me, I'm a developer advocate at DeepSet, and we maintain Haystack.", "tokens": [51064, 407, 257, 857, 466, 385, 11, 286, 478, 257, 10754, 14608, 412, 14895, 42718, 11, 293, 321, 6909, 8721, 372, 501, 13, 51346], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 10, "seek": 2824, "start": 47.879999999999995, "end": 51.72, "text": " And yeah, so this is some information about me, but let's just dive right into it.", "tokens": [51346, 400, 1338, 11, 370, 341, 307, 512, 1589, 466, 385, 11, 457, 718, 311, 445, 9192, 558, 666, 309, 13, 51538], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 11, "seek": 2824, "start": 51.72, "end": 56.68, "text": " So the agenda I'm going to follow, I'm going to try to keep the NLP stuff quite high level", "tokens": [51538, 407, 264, 9829, 286, 478, 516, 281, 1524, 11, 286, 478, 516, 281, 853, 281, 1066, 264, 426, 45196, 1507, 1596, 1090, 1496, 51786], "temperature": 0.0, "avg_logprob": -0.21494670763407667, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.28374674916267395}, {"id": 12, "seek": 5668, "start": 56.68, "end": 61.68, "text": " and focus on the how to build bit, but I do have to give a bit of a high level explanation,", "tokens": [50364, 293, 1879, 322, 264, 577, 281, 1322, 857, 11, 457, 286, 360, 362, 281, 976, 257, 857, 295, 257, 1090, 1496, 10835, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 13, "seek": 5668, "start": 61.68, "end": 66.4, "text": " so I'm going to do a brief history on what we mean by semantic search.", "tokens": [50614, 370, 286, 478, 516, 281, 360, 257, 5353, 2503, 322, 437, 321, 914, 538, 47982, 3164, 13, 50850], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 14, "seek": 5668, "start": 66.4, "end": 70.24, "text": " Please do not judge me for this example, Kardashian sisters.", "tokens": [50850, 2555, 360, 406, 6995, 385, 337, 341, 1365, 11, 46044, 11589, 13, 51042], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 15, "seek": 5668, "start": 70.24, "end": 74.52, "text": " So let's assume we have a bunch of documents and let's see what would happen if we do some", "tokens": [51042, 407, 718, 311, 6552, 321, 362, 257, 3840, 295, 8512, 293, 718, 311, 536, 437, 576, 1051, 498, 321, 360, 512, 51256], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 16, "seek": 5668, "start": 74.52, "end": 78.68, "text": " keyword search on it, and let's say we've got the query Kardashian sisters.", "tokens": [51256, 20428, 3164, 322, 309, 11, 293, 718, 311, 584, 321, 600, 658, 264, 14581, 46044, 11589, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 17, "seek": 5668, "start": 78.68, "end": 83.24000000000001, "text": " You might get something a bit like this, which is great, and you can see that there's some", "tokens": [51464, 509, 1062, 483, 746, 257, 857, 411, 341, 11, 597, 307, 869, 11, 293, 291, 393, 536, 300, 456, 311, 512, 51692], "temperature": 0.0, "avg_logprob": -0.1412948303222656, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.009522844105958939}, {"id": 18, "seek": 8324, "start": 83.24, "end": 88.32, "text": " clever stuff going on here, sisters maybe associated with siblings and family as well.", "tokens": [50364, 13494, 1507, 516, 322, 510, 11, 11589, 1310, 6615, 365, 20571, 293, 1605, 382, 731, 13, 50618], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 19, "seek": 8324, "start": 88.32, "end": 92.36, "text": " Keyword search is still very widely used, but this is the type of result you might get", "tokens": [50618, 12759, 7462, 3164, 307, 920, 588, 13371, 1143, 11, 457, 341, 307, 264, 2010, 295, 1874, 291, 1062, 483, 50820], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 20, "seek": 8324, "start": 92.36, "end": 95.56, "text": " from a corpus of documents you might have.", "tokens": [50820, 490, 257, 1181, 31624, 295, 8512, 291, 1062, 362, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 21, "seek": 8324, "start": 95.56, "end": 97.67999999999999, "text": " But what if that's just not enough?", "tokens": [50980, 583, 437, 498, 300, 311, 445, 406, 1547, 30, 51086], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 22, "seek": 8324, "start": 97.67999999999999, "end": 102.91999999999999, "text": " What if I want to be able to ask something like, who is the richest Kardashian sister?", "tokens": [51086, 708, 498, 286, 528, 281, 312, 1075, 281, 1029, 746, 411, 11, 567, 307, 264, 35098, 46044, 4892, 30, 51348], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 23, "seek": 8324, "start": 102.91999999999999, "end": 107.75999999999999, "text": " How do I make this system understand what I'm trying to get to?", "tokens": [51348, 1012, 360, 286, 652, 341, 1185, 1223, 437, 286, 478, 1382, 281, 483, 281, 30, 51590], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 24, "seek": 8324, "start": 107.75999999999999, "end": 109.39999999999999, "text": " So for that, let's have a look at this.", "tokens": [51590, 407, 337, 300, 11, 718, 311, 362, 257, 574, 412, 341, 13, 51672], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 25, "seek": 8324, "start": 109.39999999999999, "end": 113.11999999999999, "text": " There might be some names you've already seen here, especially the last one there.", "tokens": [51672, 821, 1062, 312, 512, 5288, 291, 600, 1217, 1612, 510, 11, 2318, 264, 1036, 472, 456, 13, 51858], "temperature": 0.0, "avg_logprob": -0.1337502091019242, "compression_ratio": 1.623456790123457, "no_speech_prob": 0.0028269279282540083}, {"id": 26, "seek": 11312, "start": 113.12, "end": 117.56, "text": " I think everyone and their grandparents have heard of this by now, chat GPT.", "tokens": [50364, 286, 519, 1518, 293, 641, 21876, 362, 2198, 295, 341, 538, 586, 11, 5081, 26039, 51, 13, 50586], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 27, "seek": 11312, "start": 117.56, "end": 119.24000000000001, "text": " So these are language models.", "tokens": [50586, 407, 613, 366, 2856, 5245, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 28, "seek": 11312, "start": 119.24000000000001, "end": 127.4, "text": " I'm going to briefly walk through where they get such impressive functionality from.", "tokens": [50670, 286, 478, 516, 281, 10515, 1792, 807, 689, 436, 483, 1270, 8992, 14980, 490, 13, 51078], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 29, "seek": 11312, "start": 127.4, "end": 131.76, "text": " So most of them are based on what we call transformers.", "tokens": [51078, 407, 881, 295, 552, 366, 2361, 322, 437, 321, 818, 4088, 433, 13, 51296], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 30, "seek": 11312, "start": 131.76, "end": 135.84, "text": " What those are doing is what I try to depict at the top here.", "tokens": [51296, 708, 729, 366, 884, 307, 437, 286, 853, 281, 31553, 412, 264, 1192, 510, 13, 51500], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 31, "seek": 11312, "start": 135.84, "end": 139.32, "text": " So imagine that thing in the middle as the language model.", "tokens": [51500, 407, 3811, 300, 551, 294, 264, 2808, 382, 264, 2856, 2316, 13, 51674], "temperature": 0.0, "avg_logprob": -0.14631598928700323, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00574510870501399}, {"id": 32, "seek": 13932, "start": 139.32, "end": 144.64, "text": " And very, very simply put, obviously every model does something a bit different or for", "tokens": [50364, 400, 588, 11, 588, 2935, 829, 11, 2745, 633, 2316, 775, 746, 257, 857, 819, 420, 337, 50630], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 33, "seek": 13932, "start": 144.64, "end": 147.79999999999998, "text": " slightly different use cases, let's say.", "tokens": [50630, 4748, 819, 764, 3331, 11, 718, 311, 584, 13, 50788], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 34, "seek": 13932, "start": 147.79999999999998, "end": 153.48, "text": " Given a piece of text, they will produce some sort of vector representation of that text.", "tokens": [50788, 18600, 257, 2522, 295, 2487, 11, 436, 486, 5258, 512, 1333, 295, 8062, 10290, 295, 300, 2487, 13, 51072], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 35, "seek": 13932, "start": 153.48, "end": 157.07999999999998, "text": " They're trained on very vast amounts of text data, and then this is what we get at the", "tokens": [51072, 814, 434, 8895, 322, 588, 8369, 11663, 295, 2487, 1412, 11, 293, 550, 341, 307, 437, 321, 483, 412, 264, 51252], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 36, "seek": 13932, "start": 157.07999999999998, "end": 158.56, "text": " end of the day.", "tokens": [51252, 917, 295, 264, 786, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 37, "seek": 13932, "start": 158.56, "end": 161.68, "text": " And this is cool because it's enabled us to do many different things.", "tokens": [51326, 400, 341, 307, 1627, 570, 309, 311, 15172, 505, 281, 360, 867, 819, 721, 13, 51482], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 38, "seek": 13932, "start": 161.68, "end": 166.35999999999999, "text": " We can use those vectors to compare them to each other, like dog might be close to cat", "tokens": [51482, 492, 393, 764, 729, 18875, 281, 6794, 552, 281, 1184, 661, 11, 411, 3000, 1062, 312, 1998, 281, 3857, 51716], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 39, "seek": 13932, "start": 166.35999999999999, "end": 169.16, "text": " but far away from teapot, for example.", "tokens": [51716, 457, 1400, 1314, 490, 535, 569, 310, 11, 337, 1365, 13, 51856], "temperature": 0.0, "avg_logprob": -0.13436655564741654, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.031002698466181755}, {"id": 40, "seek": 16916, "start": 169.16, "end": 174.12, "text": " And that's enabled us to do a lot of different things like question answering, summarization,", "tokens": [50364, 400, 300, 311, 15172, 505, 281, 360, 257, 688, 295, 819, 721, 411, 1168, 13430, 11, 14611, 2144, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 41, "seek": 16916, "start": 174.12, "end": 177.4, "text": " what we call retrieval, so document retrieval.", "tokens": [50612, 437, 321, 818, 19817, 3337, 11, 370, 4166, 19817, 3337, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 42, "seek": 16916, "start": 177.4, "end": 179.24, "text": " And it's all thanks to these transformers.", "tokens": [50776, 400, 309, 311, 439, 3231, 281, 613, 4088, 433, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 43, "seek": 16916, "start": 179.24, "end": 184.72, "text": " And a lot of these use cases are often grouped under the term search because actually what's", "tokens": [50868, 400, 257, 688, 295, 613, 764, 3331, 366, 2049, 41877, 833, 264, 1433, 3164, 570, 767, 437, 311, 51142], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 44, "seek": 16916, "start": 184.72, "end": 188.6, "text": " happening in the background is a very clever search algorithm.", "tokens": [51142, 2737, 294, 264, 3678, 307, 257, 588, 13494, 3164, 9284, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 45, "seek": 16916, "start": 188.6, "end": 193.0, "text": " So question answering and retrieval specifically can be grouped under search.", "tokens": [51336, 407, 1168, 13430, 293, 19817, 3337, 4682, 393, 312, 41877, 833, 3164, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 46, "seek": 16916, "start": 193.0, "end": 195.68, "text": " All right, how does this work?", "tokens": [51556, 1057, 558, 11, 577, 775, 341, 589, 30, 51690], "temperature": 0.0, "avg_logprob": -0.1848495951238668, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.000349798850947991}, {"id": 47, "seek": 19568, "start": 195.8, "end": 200.56, "text": " And I'm very briefly going to go through what these different types of models do and how", "tokens": [50370, 400, 286, 478, 588, 10515, 516, 281, 352, 807, 437, 613, 819, 3467, 295, 5245, 360, 293, 577, 50608], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 48, "seek": 19568, "start": 200.56, "end": 205.84, "text": " they do what they do, and I'm going to talk about the evolution from extractive models", "tokens": [50608, 436, 360, 437, 436, 360, 11, 293, 286, 478, 516, 281, 751, 466, 264, 9303, 490, 8947, 488, 5245, 50872], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 49, "seek": 19568, "start": 205.84, "end": 209.64000000000001, "text": " to now generative models like chat GPT, for example.", "tokens": [50872, 281, 586, 1337, 1166, 5245, 411, 5081, 26039, 51, 11, 337, 1365, 13, 51062], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 50, "seek": 19568, "start": 209.64000000000001, "end": 213.96, "text": " The very simple one, and we're going to build our first semantic search application with", "tokens": [51062, 440, 588, 2199, 472, 11, 293, 321, 434, 516, 281, 1322, 527, 700, 47982, 3164, 3861, 365, 51278], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 51, "seek": 19568, "start": 213.96, "end": 219.52, "text": " this type of model, is often referred to as the reader model, simply a question answering", "tokens": [51278, 341, 2010, 295, 2316, 11, 307, 2049, 10839, 281, 382, 264, 15149, 2316, 11, 2935, 257, 1168, 13430, 51556], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 52, "seek": 19568, "start": 219.52, "end": 223.56, "text": " model, very specifically an extractive question answering model.", "tokens": [51556, 2316, 11, 588, 4682, 364, 8947, 488, 1168, 13430, 2316, 13, 51758], "temperature": 0.0, "avg_logprob": -0.16560041254216976, "compression_ratio": 1.8153846153846154, "no_speech_prob": 0.005946373101323843}, {"id": 53, "seek": 22356, "start": 223.6, "end": 230.28, "text": " The way these work are given a piece of context and query, they're very good at looking through", "tokens": [50366, 440, 636, 613, 589, 366, 2212, 257, 2522, 295, 4319, 293, 14581, 11, 436, 434, 588, 665, 412, 1237, 807, 50700], "temperature": 0.0, "avg_logprob": -0.1376881891367387, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0014595327666029334}, {"id": 54, "seek": 22356, "start": 230.28, "end": 236.04, "text": " that context and finding, extracting the answer from that context, but it does need that context.", "tokens": [50700, 300, 4319, 293, 5006, 11, 49844, 264, 1867, 490, 300, 4319, 11, 457, 309, 775, 643, 300, 4319, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1376881891367387, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0014595327666029334}, {"id": 55, "seek": 22356, "start": 236.04, "end": 242.08, "text": " Obviously, there are some limitations to these models because they're limited by input length.", "tokens": [50988, 7580, 11, 456, 366, 512, 15705, 281, 613, 5245, 570, 436, 434, 5567, 538, 4846, 4641, 13, 51290], "temperature": 0.0, "avg_logprob": -0.1376881891367387, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0014595327666029334}, {"id": 56, "seek": 22356, "start": 242.08, "end": 245.4, "text": " I can't give it just infinite amounts of data.", "tokens": [51290, 286, 393, 380, 976, 309, 445, 13785, 11663, 295, 1412, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1376881891367387, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0014595327666029334}, {"id": 57, "seek": 22356, "start": 245.4, "end": 250.96, "text": " But we have come up with ways to make that a bit more efficient, and we've introduced", "tokens": [51456, 583, 321, 362, 808, 493, 365, 2098, 281, 652, 300, 257, 857, 544, 7148, 11, 293, 321, 600, 7268, 51734], "temperature": 0.0, "avg_logprob": -0.1376881891367387, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0014595327666029334}, {"id": 58, "seek": 25096, "start": 250.96, "end": 256.44, "text": " models that we often refer to as retriever models, or embedding models.", "tokens": [50364, 5245, 300, 321, 2049, 2864, 281, 382, 19817, 331, 5245, 11, 420, 12240, 3584, 5245, 13, 50638], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 59, "seek": 25096, "start": 256.44, "end": 259.68, "text": " These don't necessarily have to be language models, I'm going to be looking at language", "tokens": [50638, 1981, 500, 380, 4725, 362, 281, 312, 2856, 5245, 11, 286, 478, 516, 281, 312, 1237, 412, 2856, 50800], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 60, "seek": 25096, "start": 259.68, "end": 265.04, "text": " models, it could also be based on keyword search that we saw before.", "tokens": [50800, 5245, 11, 309, 727, 611, 312, 2361, 322, 20428, 3164, 300, 321, 1866, 949, 13, 51068], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 61, "seek": 25096, "start": 265.04, "end": 270.48, "text": " But what they do is they act as a sort of filter, so let's say you've got a bunch of", "tokens": [51068, 583, 437, 436, 360, 307, 436, 605, 382, 257, 1333, 295, 6608, 11, 370, 718, 311, 584, 291, 600, 658, 257, 3840, 295, 51340], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 62, "seek": 25096, "start": 270.48, "end": 274.32, "text": " documents, let's say you've got thousands and thousands of documents, and the retriever", "tokens": [51340, 8512, 11, 718, 311, 584, 291, 600, 658, 5383, 293, 5383, 295, 8512, 11, 293, 264, 19817, 331, 51532], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 63, "seek": 25096, "start": 274.32, "end": 279.36, "text": " can basically say, hey, I've got this query, and this is the top five, ten most relevant", "tokens": [51532, 393, 1936, 584, 11, 4177, 11, 286, 600, 658, 341, 14581, 11, 293, 341, 307, 264, 1192, 1732, 11, 2064, 881, 7340, 51784], "temperature": 0.0, "avg_logprob": -0.13751008775499132, "compression_ratio": 1.856060606060606, "no_speech_prob": 0.00103137141559273}, {"id": 64, "seek": 27936, "start": 279.36, "end": 282.6, "text": " documents that you should look at, and then that means that the reader doesn't have to", "tokens": [50364, 8512, 300, 291, 820, 574, 412, 11, 293, 550, 300, 1355, 300, 264, 15149, 1177, 380, 362, 281, 50526], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 65, "seek": 27936, "start": 282.6, "end": 284.08000000000004, "text": " look through anything.", "tokens": [50526, 574, 807, 1340, 13, 50600], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 66, "seek": 27936, "start": 284.08000000000004, "end": 286.96000000000004, "text": " So we actually gain a lot of speed out of this.", "tokens": [50600, 407, 321, 767, 6052, 257, 688, 295, 3073, 484, 295, 341, 13, 50744], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 67, "seek": 27936, "start": 286.96000000000004, "end": 292.84000000000003, "text": " All right, finally, this is all the hype today, and you'll notice, well, one thing you should", "tokens": [50744, 1057, 558, 11, 2721, 11, 341, 307, 439, 264, 24144, 965, 11, 293, 291, 603, 3449, 11, 731, 11, 472, 551, 291, 820, 51038], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 68, "seek": 27936, "start": 292.84000000000003, "end": 298.68, "text": " notice is you see that the document context, anything like that, I've chopped it off, it's", "tokens": [51038, 3449, 307, 291, 536, 300, 264, 4166, 4319, 11, 1340, 411, 300, 11, 286, 600, 16497, 309, 766, 11, 309, 311, 51330], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 69, "seek": 27936, "start": 298.68, "end": 300.08000000000004, "text": " just a query.", "tokens": [51330, 445, 257, 14581, 13, 51400], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 70, "seek": 27936, "start": 300.08000000000004, "end": 303.84000000000003, "text": " So these new language models, they don't actually need context.", "tokens": [51400, 407, 613, 777, 2856, 5245, 11, 436, 500, 380, 767, 643, 4319, 13, 51588], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 71, "seek": 27936, "start": 303.84000000000003, "end": 307.48, "text": " You can give it context, but it doesn't require context.", "tokens": [51588, 509, 393, 976, 309, 4319, 11, 457, 309, 1177, 380, 3651, 4319, 13, 51770], "temperature": 0.0, "avg_logprob": -0.19308199201311385, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0006713049951940775}, {"id": 72, "seek": 30748, "start": 307.48, "end": 311.0, "text": " And this is very cool, because they produce human-like answers.", "tokens": [50364, 400, 341, 307, 588, 1627, 11, 570, 436, 5258, 1952, 12, 4092, 6338, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 73, "seek": 30748, "start": 311.0, "end": 317.52000000000004, "text": " What they're trained to do, the task to do, is not extracting answers, it's generating", "tokens": [50540, 708, 436, 434, 8895, 281, 360, 11, 264, 5633, 281, 360, 11, 307, 406, 49844, 6338, 11, 309, 311, 17746, 50866], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 74, "seek": 30748, "start": 317.52000000000004, "end": 319.36, "text": " answers.", "tokens": [50866, 6338, 13, 50958], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 75, "seek": 30748, "start": 319.36, "end": 322.68, "text": " And I just want to point out there are two things here.", "tokens": [50958, 400, 286, 445, 528, 281, 935, 484, 456, 366, 732, 721, 510, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 76, "seek": 30748, "start": 322.68, "end": 325.28000000000003, "text": " It doesn't necessarily have to be answers.", "tokens": [51124, 467, 1177, 380, 4725, 362, 281, 312, 6338, 13, 51254], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 77, "seek": 30748, "start": 325.28000000000003, "end": 330.48, "text": " So I'm going to be looking at an answer generator, but it can just be, you know, prompt it to", "tokens": [51254, 407, 286, 478, 516, 281, 312, 1237, 412, 364, 1867, 19265, 11, 457, 309, 393, 445, 312, 11, 291, 458, 11, 12391, 309, 281, 51514], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 78, "seek": 30748, "start": 330.48, "end": 335.88, "text": " produce some context, it doesn't necessarily have to be an answer to a question.", "tokens": [51514, 5258, 512, 4319, 11, 309, 1177, 380, 4725, 362, 281, 312, 364, 1867, 281, 257, 1168, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1469911824101987, "compression_ratio": 1.8041666666666667, "no_speech_prob": 0.004633298609405756}, {"id": 79, "seek": 33588, "start": 335.88, "end": 341.24, "text": " So we've been seeing this, maybe you've seen some of these scenes lately, so this is chat", "tokens": [50364, 407, 321, 600, 668, 2577, 341, 11, 1310, 291, 600, 1612, 512, 295, 613, 8026, 12881, 11, 370, 341, 307, 5081, 50632], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 80, "seek": 33588, "start": 341.24, "end": 346.56, "text": " GPT again on the theme, who is the tallest Kardashian sister, it hasn't just extracted", "tokens": [50632, 26039, 51, 797, 322, 264, 6314, 11, 567, 307, 264, 42075, 46044, 4892, 11, 309, 6132, 380, 445, 34086, 50898], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 81, "seek": 33588, "start": 346.56, "end": 351.4, "text": " Kendall for me, it said, the tallest Kardashian sister is Kendall Jenner, perfect.", "tokens": [50898, 38794, 337, 385, 11, 309, 848, 11, 264, 42075, 46044, 4892, 307, 38794, 9228, 1193, 11, 2176, 13, 51140], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 82, "seek": 33588, "start": 351.4, "end": 354.8, "text": " But let's see what happens if it's not like a question.", "tokens": [51140, 583, 718, 311, 536, 437, 2314, 498, 309, 311, 406, 411, 257, 1168, 13, 51310], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 83, "seek": 33588, "start": 354.8, "end": 358.48, "text": " This is not my creativity, by the way, but I think it's amazing.", "tokens": [51310, 639, 307, 406, 452, 12915, 11, 538, 264, 636, 11, 457, 286, 519, 309, 311, 2243, 13, 51494], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 84, "seek": 33588, "start": 358.48, "end": 363.2, "text": " Write a poem about Fostam in the style of Markdown, change log, that's what you get.", "tokens": [51494, 23499, 257, 13065, 466, 479, 555, 335, 294, 264, 3758, 295, 3934, 5093, 11, 1319, 3565, 11, 300, 311, 437, 291, 483, 13, 51730], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 85, "seek": 33588, "start": 363.2, "end": 364.2, "text": " There you go.", "tokens": [51730, 821, 291, 352, 13, 51780], "temperature": 0.0, "avg_logprob": -0.245782707676743, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.004810239188373089}, {"id": 86, "seek": 36420, "start": 364.2, "end": 368.24, "text": " All right, so these language models are readily available.", "tokens": [50364, 1057, 558, 11, 370, 613, 2856, 5245, 366, 26336, 2435, 13, 50566], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 87, "seek": 36420, "start": 368.24, "end": 371.47999999999996, "text": " You might have already heard these names, OpenAI, Kahir.", "tokens": [50566, 509, 1062, 362, 1217, 2198, 613, 5288, 11, 7238, 48698, 11, 39444, 347, 13, 50728], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 88, "seek": 36420, "start": 371.47999999999996, "end": 375.32, "text": " They provide these increasingly large language models.", "tokens": [50728, 814, 2893, 613, 12980, 2416, 2856, 5245, 13, 50920], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 89, "seek": 36420, "start": 375.32, "end": 379.28, "text": " There is a difference when we say language model and large language model, but leave", "tokens": [50920, 821, 307, 257, 2649, 562, 321, 584, 2856, 2316, 293, 2416, 2856, 2316, 11, 457, 1856, 51118], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 90, "seek": 36420, "start": 379.28, "end": 382.24, "text": " that aside for now, let's not talk about that.", "tokens": [51118, 300, 7359, 337, 586, 11, 718, 311, 406, 751, 466, 300, 13, 51266], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 91, "seek": 36420, "start": 382.24, "end": 386.56, "text": " There are also many, many, many open source models on Huggingface, and if you don't know", "tokens": [51266, 821, 366, 611, 867, 11, 867, 11, 867, 1269, 4009, 5245, 322, 46892, 3249, 2868, 11, 293, 498, 291, 500, 380, 458, 51482], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 92, "seek": 36420, "start": 386.56, "end": 391.48, "text": " what Huggingface is, I think very simply put, I like to refer it to sort of like the GitHub", "tokens": [51482, 437, 46892, 3249, 2868, 307, 11, 286, 519, 588, 2935, 829, 11, 286, 411, 281, 2864, 309, 281, 1333, 295, 411, 264, 23331, 51728], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 93, "seek": 36420, "start": 391.48, "end": 392.48, "text": " of machine learning.", "tokens": [51728, 295, 3479, 2539, 13, 51778], "temperature": 0.0, "avg_logprob": -0.21148990094661713, "compression_ratio": 1.7379310344827585, "no_speech_prob": 0.004464557860046625}, {"id": 94, "seek": 39248, "start": 392.64000000000004, "end": 396.96000000000004, "text": " So you can host your open source models and other developers can use them, use them in", "tokens": [50372, 407, 291, 393, 3975, 428, 1269, 4009, 5245, 293, 661, 8849, 393, 764, 552, 11, 764, 552, 294, 50588], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 95, "seek": 39248, "start": 396.96000000000004, "end": 400.36, "text": " their projects or even contribute to them.", "tokens": [50588, 641, 4455, 420, 754, 10586, 281, 552, 13, 50758], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 96, "seek": 39248, "start": 400.36, "end": 405.40000000000003, "text": " And what's really cool about them, like I said, your search results stop becoming just", "tokens": [50758, 400, 437, 311, 534, 1627, 466, 552, 11, 411, 286, 848, 11, 428, 3164, 3542, 1590, 5617, 445, 51010], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 97, "seek": 39248, "start": 405.40000000000003, "end": 409.32, "text": " simple search results, they are human-like answers.", "tokens": [51010, 2199, 3164, 3542, 11, 436, 366, 1952, 12, 4092, 6338, 13, 51206], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 98, "seek": 39248, "start": 409.32, "end": 415.44, "text": " So now let's look at how we use these language models for various use cases.", "tokens": [51206, 407, 586, 718, 311, 574, 412, 577, 321, 764, 613, 2856, 5245, 337, 3683, 764, 3331, 13, 51512], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 99, "seek": 39248, "start": 415.44, "end": 419.12, "text": " For that, I want to talk about Haystack, this is why I'm here.", "tokens": [51512, 1171, 300, 11, 286, 528, 281, 751, 466, 8721, 372, 501, 11, 341, 307, 983, 286, 478, 510, 13, 51696], "temperature": 0.0, "avg_logprob": -0.16641226268949963, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.004620260093361139}, {"id": 100, "seek": 41912, "start": 419.12, "end": 426.36, "text": " So Haystack is an open source NLP framework built in Python, and what it achieves is basically", "tokens": [50364, 407, 8721, 372, 501, 307, 364, 1269, 4009, 426, 45196, 8388, 3094, 294, 15329, 11, 293, 437, 309, 3538, 977, 307, 1936, 50726], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 101, "seek": 41912, "start": 426.36, "end": 429.16, "text": " what this picture is trying to show you.", "tokens": [50726, 437, 341, 3036, 307, 1382, 281, 855, 291, 13, 50866], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 102, "seek": 41912, "start": 429.16, "end": 434.8, "text": " You're free to build your own end-to-end NLP application, and each of those green boxes", "tokens": [50866, 509, 434, 1737, 281, 1322, 428, 1065, 917, 12, 1353, 12, 521, 426, 45196, 3861, 11, 293, 1184, 295, 729, 3092, 9002, 51148], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 103, "seek": 41912, "start": 434.8, "end": 437.56, "text": " are a high-level component in Haystack.", "tokens": [51148, 366, 257, 1090, 12, 12418, 6542, 294, 8721, 372, 501, 13, 51286], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 104, "seek": 41912, "start": 437.56, "end": 440.92, "text": " There are retrievers that we looked at, there are readers that we looked at, we'll look", "tokens": [51286, 821, 366, 19817, 840, 300, 321, 2956, 412, 11, 456, 366, 17147, 300, 321, 2956, 412, 11, 321, 603, 574, 51454], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 105, "seek": 41912, "start": 440.92, "end": 445.4, "text": " at some different ones as well, and each of these are basically the main class, and you", "tokens": [51454, 412, 512, 819, 2306, 382, 731, 11, 293, 1184, 295, 613, 366, 1936, 264, 2135, 1508, 11, 293, 291, 51678], "temperature": 0.0, "avg_logprob": -0.14080706213274571, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.0038609520997852087}, {"id": 106, "seek": 44540, "start": 445.4, "end": 449.32, "text": " might have different types of readers, different types of retrievers.", "tokens": [50364, 1062, 362, 819, 3467, 295, 17147, 11, 819, 3467, 295, 19817, 840, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 107, "seek": 44540, "start": 449.32, "end": 454.0, "text": " For example, there could be a reader that is good at looking at paragraphs and extracting", "tokens": [50560, 1171, 1365, 11, 456, 727, 312, 257, 15149, 300, 307, 665, 412, 1237, 412, 48910, 293, 49844, 50794], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 108, "seek": 44540, "start": 454.0, "end": 458.03999999999996, "text": " answers, but there might be a reader type called table reader that's good at looking", "tokens": [50794, 6338, 11, 457, 456, 1062, 312, 257, 15149, 2010, 1219, 3199, 15149, 300, 311, 665, 412, 1237, 50996], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 109, "seek": 44540, "start": 458.03999999999996, "end": 461.23999999999995, "text": " at tables and retrieving answers from that.", "tokens": [50996, 412, 8020, 293, 19817, 798, 6338, 490, 300, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 110, "seek": 44540, "start": 461.23999999999995, "end": 465.23999999999995, "text": " There are integrations with HuggingFace, so that means you can just download a model off", "tokens": [51156, 821, 366, 3572, 763, 365, 46892, 3249, 37, 617, 11, 370, 300, 1355, 291, 393, 445, 5484, 257, 2316, 766, 51356], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 111, "seek": 44540, "start": 465.23999999999995, "end": 470.79999999999995, "text": " of HuggingFace, but also open AI here, obviously you need to provide an API key, but you are", "tokens": [51356, 295, 46892, 3249, 37, 617, 11, 457, 611, 1269, 7318, 510, 11, 2745, 291, 643, 281, 2893, 364, 9362, 2141, 11, 457, 291, 366, 51634], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 112, "seek": 44540, "start": 470.79999999999995, "end": 474.12, "text": " free to use those as well.", "tokens": [51634, 1737, 281, 764, 729, 382, 731, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1906050395190231, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.0014544801088050008}, {"id": 113, "seek": 47412, "start": 474.84000000000003, "end": 480.64, "text": " A building in an NLP application isn't just about the search component, you presumably", "tokens": [50400, 316, 2390, 294, 364, 426, 45196, 3861, 1943, 380, 445, 466, 264, 3164, 6542, 11, 291, 26742, 50690], "temperature": 0.0, "avg_logprob": -0.19706936075229836, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.0016758088022470474}, {"id": 114, "seek": 47412, "start": 480.64, "end": 486.24, "text": " have lots of documents somewhere, maybe the PDFs, maybe the TXDs, so they're components", "tokens": [50690, 362, 3195, 295, 8512, 4079, 11, 1310, 264, 17752, 82, 11, 1310, 264, 314, 55, 35, 82, 11, 370, 436, 434, 6677, 50970], "temperature": 0.0, "avg_logprob": -0.19706936075229836, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.0016758088022470474}, {"id": 115, "seek": 47412, "start": 486.24, "end": 492.68, "text": " for you to build your indexing pipeline that we call so that you can write your data somewhere", "tokens": [50970, 337, 291, 281, 1322, 428, 8186, 278, 15517, 300, 321, 818, 370, 300, 291, 393, 2464, 428, 1412, 4079, 51292], "temperature": 0.0, "avg_logprob": -0.19706936075229836, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.0016758088022470474}, {"id": 116, "seek": 47412, "start": 492.68, "end": 496.84000000000003, "text": " in a way that can be used by these language models.", "tokens": [51292, 294, 257, 636, 300, 393, 312, 1143, 538, 613, 2856, 5245, 13, 51500], "temperature": 0.0, "avg_logprob": -0.19706936075229836, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.0016758088022470474}, {"id": 117, "seek": 47412, "start": 496.84000000000003, "end": 501.08, "text": " Some of those components, we already talked briefly about the reader and the retriever,", "tokens": [51500, 2188, 295, 729, 6677, 11, 321, 1217, 2825, 10515, 466, 264, 15149, 293, 264, 19817, 331, 11, 51712], "temperature": 0.0, "avg_logprob": -0.19706936075229836, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.0016758088022470474}, {"id": 118, "seek": 50108, "start": 501.12, "end": 502.47999999999996, "text": " we're going to be using those.", "tokens": [50366, 321, 434, 516, 281, 312, 1228, 729, 13, 50434], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 119, "seek": 50108, "start": 502.47999999999996, "end": 505.84, "text": " There could be an answer generator, a question generator, we're not going to look at that", "tokens": [50434, 821, 727, 312, 364, 1867, 19265, 11, 257, 1168, 19265, 11, 321, 434, 406, 516, 281, 574, 412, 300, 50602], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 120, "seek": 50108, "start": 505.84, "end": 509.47999999999996, "text": " today, but that's really cool because then you can use those questions to train another", "tokens": [50602, 965, 11, 457, 300, 311, 534, 1627, 570, 550, 291, 393, 764, 729, 1651, 281, 3847, 1071, 50784], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 121, "seek": 50108, "start": 509.47999999999996, "end": 511.52, "text": " model, for example.", "tokens": [50784, 2316, 11, 337, 1365, 13, 50886], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 122, "seek": 50108, "start": 511.52, "end": 515.68, "text": " Summarizer, prompt node, we're going to very briefly look into that, but you get the idea.", "tokens": [50886, 8626, 6209, 6545, 11, 12391, 9984, 11, 321, 434, 516, 281, 588, 10515, 574, 666, 300, 11, 457, 291, 483, 264, 1558, 13, 51094], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 123, "seek": 50108, "start": 515.68, "end": 521.0, "text": " There's a bunch of components and each of them might have types under them.", "tokens": [51094, 821, 311, 257, 3840, 295, 6677, 293, 1184, 295, 552, 1062, 362, 3467, 833, 552, 13, 51360], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 124, "seek": 50108, "start": 521.0, "end": 526.12, "text": " You can use data connectors, file converters as mentioned, pre-processing your documents", "tokens": [51360, 509, 393, 764, 1412, 31865, 11, 3991, 9652, 1559, 382, 2835, 11, 659, 12, 41075, 278, 428, 8512, 51616], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 125, "seek": 50108, "start": 526.12, "end": 530.48, "text": " in a way that's going to be a bit more useful to the language model, for example, and of", "tokens": [51616, 294, 257, 636, 300, 311, 516, 281, 312, 257, 857, 544, 4420, 281, 264, 2856, 2316, 11, 337, 1365, 11, 293, 295, 51834], "temperature": 0.0, "avg_logprob": -0.15143538174563892, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00298151932656765}, {"id": 126, "seek": 53048, "start": 530.52, "end": 535.32, "text": " course, you need to keep your data somewhere, so you might decide you want to use elastic", "tokens": [50366, 1164, 11, 291, 643, 281, 1066, 428, 1412, 4079, 11, 370, 291, 1062, 4536, 291, 528, 281, 764, 17115, 50606], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 127, "seek": 53048, "start": 535.32, "end": 539.8000000000001, "text": " search or open search, or you might want to use something a bit more vector optimized,", "tokens": [50606, 3164, 420, 1269, 3164, 11, 420, 291, 1062, 528, 281, 764, 746, 257, 857, 544, 8062, 26941, 11, 50830], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 128, "seek": 53048, "start": 539.8000000000001, "end": 545.5600000000001, "text": " and these are all available in the Haystack framework.", "tokens": [50830, 293, 613, 366, 439, 2435, 294, 264, 8721, 372, 501, 8388, 13, 51118], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 129, "seek": 53048, "start": 545.5600000000001, "end": 550.64, "text": " This is the idea of, I talked about the nodes, but the idea behind building with these nodes", "tokens": [51118, 639, 307, 264, 1558, 295, 11, 286, 2825, 466, 264, 13891, 11, 457, 264, 1558, 2261, 2390, 365, 613, 13891, 51372], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 130, "seek": 53048, "start": 550.64, "end": 552.6, "text": " is to build your own pipeline.", "tokens": [51372, 307, 281, 1322, 428, 1065, 15517, 13, 51470], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 131, "seek": 53048, "start": 552.6, "end": 553.76, "text": " This is just an example.", "tokens": [51470, 639, 307, 445, 364, 1365, 13, 51528], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 132, "seek": 53048, "start": 553.76, "end": 557.08, "text": " You really don't have to pay attention to the actual names of these components, but to", "tokens": [51528, 509, 534, 500, 380, 362, 281, 1689, 3202, 281, 264, 3539, 5288, 295, 613, 6677, 11, 457, 281, 51694], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 133, "seek": 53048, "start": 557.08, "end": 558.6800000000001, "text": " give you an idea.", "tokens": [51694, 976, 291, 364, 1558, 13, 51774], "temperature": 0.0, "avg_logprob": -0.15901334498955952, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0001862620993051678}, {"id": 134, "seek": 55868, "start": 558.7199999999999, "end": 564.64, "text": " You are free to decide what path your application should take based on a decision.", "tokens": [50366, 509, 366, 1737, 281, 4536, 437, 3100, 428, 3861, 820, 747, 2361, 322, 257, 3537, 13, 50662], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 135, "seek": 55868, "start": 564.64, "end": 569.16, "text": " For example, here we have what we call the query classifier, so let's say a user enters", "tokens": [50662, 1171, 1365, 11, 510, 321, 362, 437, 321, 818, 264, 14581, 1508, 9902, 11, 370, 718, 311, 584, 257, 4195, 18780, 50888], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 136, "seek": 55868, "start": 569.16, "end": 573.76, "text": " a keyword, there's no point in doing fancy embedding search, maybe, so you might route", "tokens": [50888, 257, 20428, 11, 456, 311, 572, 935, 294, 884, 10247, 12240, 3584, 3164, 11, 1310, 11, 370, 291, 1062, 7955, 51118], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 137, "seek": 55868, "start": 573.76, "end": 575.88, "text": " it to keyword search.", "tokens": [51118, 309, 281, 20428, 3164, 13, 51224], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 138, "seek": 55868, "start": 575.88, "end": 580.8399999999999, "text": " If the user enters something that's more like a human-formed question, you might say, okay,", "tokens": [51224, 759, 264, 4195, 18780, 746, 300, 311, 544, 411, 257, 1952, 12, 22892, 1168, 11, 291, 1062, 584, 11, 1392, 11, 51472], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 139, "seek": 55868, "start": 580.8399999999999, "end": 584.7199999999999, "text": " do some what we call dense retrieval or embedding retrieval.", "tokens": [51472, 360, 512, 437, 321, 818, 18011, 19817, 3337, 420, 12240, 3584, 19817, 3337, 13, 51666], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 140, "seek": 55868, "start": 584.7199999999999, "end": 586.4799999999999, "text": " That's just an example.", "tokens": [51666, 663, 311, 445, 364, 1365, 13, 51754], "temperature": 0.0, "avg_logprob": -0.18577657427106584, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0009148959652520716}, {"id": 141, "seek": 58648, "start": 586.84, "end": 591.0, "text": " Finally, I'm not going to get into this today at all, but let's say you have a running application,", "tokens": [50382, 6288, 11, 286, 478, 406, 516, 281, 483, 666, 341, 965, 412, 439, 11, 457, 718, 311, 584, 291, 362, 257, 2614, 3861, 11, 50590], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 142, "seek": 58648, "start": 591.0, "end": 595.52, "text": " you can just provide it through REST API, and then you're free to query it, upload more", "tokens": [50590, 291, 393, 445, 2893, 309, 807, 497, 14497, 9362, 11, 293, 550, 291, 434, 1737, 281, 14581, 309, 11, 6580, 544, 50816], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 143, "seek": 58648, "start": 595.52, "end": 598.64, "text": " files, and index them, and so on.", "tokens": [50816, 7098, 11, 293, 8186, 552, 11, 293, 370, 322, 13, 50972], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 144, "seek": 58648, "start": 598.64, "end": 604.4, "text": " All right, so let's look at how that might look first thing you do is install farm Haystack.", "tokens": [50972, 1057, 558, 11, 370, 718, 311, 574, 412, 577, 300, 1062, 574, 700, 551, 291, 360, 307, 3625, 5421, 8721, 372, 501, 13, 51260], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 145, "seek": 58648, "start": 604.4, "end": 608.08, "text": " If you're curious as to why there is farm at the beginning there, you can talk about", "tokens": [51260, 759, 291, 434, 6369, 382, 281, 983, 456, 307, 5421, 412, 264, 2863, 456, 11, 291, 393, 751, 466, 51444], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 146, "seek": 58648, "start": 608.08, "end": 609.08, "text": " this later.", "tokens": [51444, 341, 1780, 13, 51494], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 147, "seek": 58648, "start": 609.08, "end": 612.12, "text": " It's a bit about the history of the company.", "tokens": [51494, 467, 311, 257, 857, 466, 264, 2503, 295, 264, 2237, 13, 51646], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 148, "seek": 58648, "start": 612.12, "end": 616.24, "text": " Then we just simply initialize two things, the retriever.", "tokens": [51646, 1396, 321, 445, 2935, 5883, 1125, 732, 721, 11, 264, 19817, 331, 13, 51852], "temperature": 0.0, "avg_logprob": -0.19327823209090972, "compression_ratio": 1.6580645161290322, "no_speech_prob": 0.00035676039988175035}, {"id": 149, "seek": 61624, "start": 616.24, "end": 620.92, "text": " Here we specifically have the embedding retriever, and notice that I'm giving it the document", "tokens": [50364, 1692, 321, 4682, 362, 264, 12240, 3584, 19817, 331, 11, 293, 3449, 300, 286, 478, 2902, 309, 264, 4166, 50598], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 150, "seek": 61624, "start": 620.92, "end": 626.24, "text": " stall, so the retriever already knows where to look for these documents, and then we define", "tokens": [50598, 19633, 11, 370, 264, 19817, 331, 1217, 3255, 689, 281, 574, 337, 613, 8512, 11, 293, 550, 321, 6964, 50864], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 151, "seek": 61624, "start": 626.24, "end": 627.6, "text": " an embedding model.", "tokens": [50864, 364, 12240, 3584, 2316, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 152, "seek": 61624, "start": 627.6, "end": 632.2, "text": " I mentioned that these retrievers could be keyword retrieval, or it could be retrieval", "tokens": [50932, 286, 2835, 300, 613, 19817, 840, 727, 312, 20428, 19817, 3337, 11, 420, 309, 727, 312, 19817, 3337, 51162], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 153, "seek": 61624, "start": 632.2, "end": 634.92, "text": " based on some embedding representation.", "tokens": [51162, 2361, 322, 512, 12240, 3584, 10290, 13, 51298], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 154, "seek": 61624, "start": 634.92, "end": 641.2, "text": " Here we're basically saying use this sum model name, so it's just a model, to create the", "tokens": [51298, 1692, 321, 434, 1936, 1566, 764, 341, 2408, 2316, 1315, 11, 370, 309, 311, 445, 257, 2316, 11, 281, 1884, 264, 51612], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 155, "seek": 61624, "start": 641.2, "end": 643.32, "text": " vector representations.", "tokens": [51612, 8062, 33358, 13, 51718], "temperature": 0.0, "avg_logprob": -0.1811428599887424, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0013942408841103315}, {"id": 156, "seek": 64332, "start": 643.32, "end": 649.6400000000001, "text": " Then I'm initializing a reader, and this is a very commonly used, let's say, extract", "tokens": [50364, 1396, 286, 478, 5883, 3319, 257, 15149, 11, 293, 341, 307, 257, 588, 12719, 1143, 11, 718, 311, 584, 11, 8947, 50680], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 157, "seek": 64332, "start": 649.6400000000001, "end": 650.8000000000001, "text": " a question answering model.", "tokens": [50680, 257, 1168, 13430, 2316, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 158, "seek": 64332, "start": 650.8000000000001, "end": 655.5600000000001, "text": " Again, some other model, and these are both off of hugging face, let's imagine.", "tokens": [50738, 3764, 11, 512, 661, 2316, 11, 293, 613, 366, 1293, 766, 295, 41706, 1851, 11, 718, 311, 3811, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 159, "seek": 64332, "start": 655.5600000000001, "end": 661.08, "text": " We've got this retriever, and it's connected to a document store, and we've got a reader.", "tokens": [50976, 492, 600, 658, 341, 19817, 331, 11, 293, 309, 311, 4582, 281, 257, 4166, 3531, 11, 293, 321, 600, 658, 257, 15149, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 160, "seek": 64332, "start": 661.08, "end": 663.88, "text": " How would we build our pipeline?", "tokens": [51252, 1012, 576, 321, 1322, 527, 15517, 30, 51392], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 161, "seek": 64332, "start": 663.88, "end": 668.5600000000001, "text": " We would first initialize a pipeline, and then the first thing we add is the first node,", "tokens": [51392, 492, 576, 700, 5883, 1125, 257, 15517, 11, 293, 550, 264, 700, 551, 321, 909, 307, 264, 700, 9984, 11, 51626], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 162, "seek": 64332, "start": 668.5600000000001, "end": 669.72, "text": " and we're saying retriever.", "tokens": [51626, 293, 321, 434, 1566, 19817, 331, 13, 51684], "temperature": 0.0, "avg_logprob": -0.1940059338585805, "compression_ratio": 1.7560975609756098, "no_speech_prob": 0.0019476987654343247}, {"id": 163, "seek": 66972, "start": 669.76, "end": 674.64, "text": " I'm first adding the retriever, and that input you see, inputs query, is actually a special", "tokens": [50366, 286, 478, 700, 5127, 264, 19817, 331, 11, 293, 300, 4846, 291, 536, 11, 15743, 14581, 11, 307, 767, 257, 2121, 50610], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 164, "seek": 66972, "start": 674.64, "end": 679.8000000000001, "text": " input in Haystack, and it's usually indicating that this is the entry point.", "tokens": [50610, 4846, 294, 8721, 372, 501, 11, 293, 309, 311, 2673, 25604, 300, 341, 307, 264, 8729, 935, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 165, "seek": 66972, "start": 679.8000000000001, "end": 684.48, "text": " This is the first thing that gets the query, so okay, we've told it, you've got the query.", "tokens": [50868, 639, 307, 264, 700, 551, 300, 2170, 264, 14581, 11, 370, 1392, 11, 321, 600, 1907, 309, 11, 291, 600, 658, 264, 14581, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 166, "seek": 66972, "start": 684.48, "end": 689.88, "text": " I could leave it here, and this pipeline, if I run it, what it's doing is, given a query,", "tokens": [51102, 286, 727, 1856, 309, 510, 11, 293, 341, 15517, 11, 498, 286, 1190, 309, 11, 437, 309, 311, 884, 307, 11, 2212, 257, 14581, 11, 51372], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 167, "seek": 66972, "start": 689.88, "end": 691.8000000000001, "text": " it's just dumping out documents for me.", "tokens": [51372, 309, 311, 445, 42224, 484, 8512, 337, 385, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 168, "seek": 66972, "start": 691.8000000000001, "end": 695.96, "text": " That's what the retriever does, it's just going to return to me the most relevant documents.", "tokens": [51468, 663, 311, 437, 264, 19817, 331, 775, 11, 309, 311, 445, 516, 281, 2736, 281, 385, 264, 881, 7340, 8512, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1508536194310044, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0011259519960731268}, {"id": 169, "seek": 69596, "start": 695.96, "end": 701.5600000000001, "text": " I want to build a question answering pipeline, so I would maybe add a second node, and I", "tokens": [50364, 286, 528, 281, 1322, 257, 1168, 13430, 15517, 11, 370, 286, 576, 1310, 909, 257, 1150, 9984, 11, 293, 286, 50644], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 170, "seek": 69596, "start": 701.5600000000001, "end": 706.5600000000001, "text": " would say now this is the question answering model node, and anything that's the output", "tokens": [50644, 576, 584, 586, 341, 307, 264, 1168, 13430, 2316, 9984, 11, 293, 1340, 300, 311, 264, 5598, 50894], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 171, "seek": 69596, "start": 706.5600000000001, "end": 710.0, "text": " from the retriever is an input to this node.", "tokens": [50894, 490, 264, 19817, 331, 307, 364, 4846, 281, 341, 9984, 13, 51066], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 172, "seek": 69596, "start": 710.0, "end": 711.0, "text": " That's simply it.", "tokens": [51066, 663, 311, 2935, 309, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 173, "seek": 69596, "start": 711.0, "end": 717.84, "text": " You could do this, but you could also just use pre-made pipelines.", "tokens": [51116, 509, 727, 360, 341, 11, 457, 291, 727, 611, 445, 764, 659, 12, 10341, 40168, 13, 51458], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 174, "seek": 69596, "start": 717.84, "end": 720.96, "text": " This is a very common one, so we do have a pre-made pipeline for it, and it's just simply", "tokens": [51458, 639, 307, 257, 588, 2689, 472, 11, 370, 321, 360, 362, 257, 659, 12, 10341, 15517, 337, 309, 11, 293, 309, 311, 445, 2935, 51614], "temperature": 0.0, "avg_logprob": -0.1278621727061049, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.0013835771242156625}, {"id": 175, "seek": 72096, "start": 720.96, "end": 725.12, "text": " called an extractive QA pipeline, and you just tell it what retriever and what reader", "tokens": [50364, 1219, 364, 8947, 488, 1249, 32, 15517, 11, 293, 291, 445, 980, 309, 437, 19817, 331, 293, 437, 15149, 50572], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 176, "seek": 72096, "start": 725.12, "end": 730.48, "text": " to use, but the pipeline I built before, that's just a lot more flexible.", "tokens": [50572, 281, 764, 11, 457, 264, 15517, 286, 3094, 949, 11, 300, 311, 445, 257, 688, 544, 11358, 13, 50840], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 177, "seek": 72096, "start": 730.48, "end": 736.44, "text": " I'm free to add any more nodes to this, I'm free to extract any nodes from this, so it's", "tokens": [50840, 286, 478, 1737, 281, 909, 604, 544, 13891, 281, 341, 11, 286, 478, 1737, 281, 8947, 604, 13891, 490, 341, 11, 370, 309, 311, 51138], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 178, "seek": 72096, "start": 736.44, "end": 740.76, "text": " just a better way to build your own pipeline.", "tokens": [51138, 445, 257, 1101, 636, 281, 1322, 428, 1065, 15517, 13, 51354], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 179, "seek": 72096, "start": 740.76, "end": 745.96, "text": " Then simply what I do is I run what now looks like a very random question, but we'll get", "tokens": [51354, 1396, 2935, 437, 286, 360, 307, 286, 1190, 437, 586, 1542, 411, 257, 588, 4974, 1168, 11, 457, 321, 603, 483, 51614], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 180, "seek": 72096, "start": 745.96, "end": 746.96, "text": " to it.", "tokens": [51614, 281, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12415383715148366, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.033574800938367844}, {"id": 181, "seek": 74696, "start": 746.96, "end": 750.6800000000001, "text": " Then hopefully you have a working system, and you've got an answer.", "tokens": [50364, 1396, 4696, 291, 362, 257, 1364, 1185, 11, 293, 291, 600, 658, 364, 1867, 13, 50550], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 182, "seek": 74696, "start": 750.6800000000001, "end": 751.6800000000001, "text": " Great.", "tokens": [50550, 3769, 13, 50600], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 183, "seek": 74696, "start": 751.6800000000001, "end": 756.6, "text": " I'm going to build an actual example, so I want to set the scene, and I was very lazy.", "tokens": [50600, 286, 478, 516, 281, 1322, 364, 3539, 1365, 11, 370, 286, 528, 281, 992, 264, 4145, 11, 293, 286, 390, 588, 14847, 13, 50846], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 184, "seek": 74696, "start": 756.6, "end": 761.12, "text": " This is actually the exact example we have in our first tutorial on our website, but", "tokens": [50846, 639, 307, 767, 264, 1900, 1365, 321, 362, 294, 527, 700, 7073, 322, 527, 3144, 11, 457, 51072], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 185, "seek": 74696, "start": 761.12, "end": 767.12, "text": " let's assume we have a document store somewhere, and it has a bunch of documents, TXT files", "tokens": [51072, 718, 311, 6552, 321, 362, 257, 4166, 3531, 4079, 11, 293, 309, 575, 257, 3840, 295, 8512, 11, 314, 20542, 7098, 51372], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 186, "seek": 74696, "start": 767.12, "end": 768.96, "text": " about Game of Thrones.", "tokens": [51372, 466, 7522, 295, 31659, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 187, "seek": 74696, "start": 768.96, "end": 772.24, "text": " I'm going to make this document store FIES document store.", "tokens": [51464, 286, 478, 516, 281, 652, 341, 4166, 3531, 479, 40, 2358, 4166, 3531, 13, 51628], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 188, "seek": 74696, "start": 772.24, "end": 776.4000000000001, "text": " This is one of the options, so let's assume I've got FIES document store, and of course", "tokens": [51628, 639, 307, 472, 295, 264, 3956, 11, 370, 718, 311, 6552, 286, 600, 658, 479, 40, 2358, 4166, 3531, 11, 293, 295, 1164, 51836], "temperature": 0.0, "avg_logprob": -0.19994127577629642, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.005334262736141682}, {"id": 189, "seek": 77640, "start": 776.4399999999999, "end": 779.88, "text": " I want to do question answering, and I want this to be efficient, so we're going to build", "tokens": [50366, 286, 528, 281, 360, 1168, 13430, 11, 293, 286, 528, 341, 281, 312, 7148, 11, 370, 321, 434, 516, 281, 1322, 50538], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 190, "seek": 77640, "start": 779.88, "end": 784.0799999999999, "text": " exactly that pipeline we just saw before, Retriever followed by a reader.", "tokens": [50538, 2293, 300, 15517, 321, 445, 1866, 949, 11, 11495, 5469, 331, 6263, 538, 257, 15149, 13, 50748], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 191, "seek": 77640, "start": 784.0799999999999, "end": 788.24, "text": " Specifically, I'm going to use an embedding Retriever, so these are the ones that can", "tokens": [50748, 26058, 11, 286, 478, 516, 281, 764, 364, 12240, 3584, 11495, 5469, 331, 11, 370, 613, 366, 264, 2306, 300, 393, 50956], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 192, "seek": 77640, "start": 788.24, "end": 793.4, "text": " actually look at vector representations and extract the most similar ones, and then we", "tokens": [50956, 767, 574, 412, 8062, 33358, 293, 8947, 264, 881, 2531, 2306, 11, 293, 550, 321, 51214], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 193, "seek": 77640, "start": 793.4, "end": 798.56, "text": " are going to have a reader, simply a question answering node at the end.", "tokens": [51214, 366, 516, 281, 362, 257, 15149, 11, 2935, 257, 1168, 13430, 9984, 412, 264, 917, 13, 51472], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 194, "seek": 77640, "start": 798.56, "end": 799.88, "text": " How would that look?", "tokens": [51472, 1012, 576, 300, 574, 30, 51538], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 195, "seek": 77640, "start": 799.88, "end": 802.76, "text": " I first initialize my document store.", "tokens": [51538, 286, 700, 5883, 1125, 452, 4166, 3531, 13, 51682], "temperature": 0.0, "avg_logprob": -0.16692903486348815, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.000436113536125049}, {"id": 196, "seek": 80276, "start": 802.8, "end": 806.48, "text": " This is basically, I'm not going through the indexing one just now, we'll look at that", "tokens": [50366, 639, 307, 1936, 11, 286, 478, 406, 516, 807, 264, 8186, 278, 472, 445, 586, 11, 321, 603, 574, 412, 300, 50550], "temperature": 0.0, "avg_logprob": -0.1507831605012752, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.00039629722596146166}, {"id": 197, "seek": 80276, "start": 806.48, "end": 811.12, "text": " in a bit, but let's assume the files are already indexed, and they're in that FIES document", "tokens": [50550, 294, 257, 857, 11, 457, 718, 311, 6552, 264, 7098, 366, 1217, 8186, 292, 11, 293, 436, 434, 294, 300, 479, 40, 2358, 4166, 50782], "temperature": 0.0, "avg_logprob": -0.1507831605012752, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.00039629722596146166}, {"id": 198, "seek": 80276, "start": 811.12, "end": 815.64, "text": " store, and then I've got a Retriever, I'm telling it where to look, and look at my document", "tokens": [50782, 3531, 11, 293, 550, 286, 600, 658, 257, 11495, 5469, 331, 11, 286, 478, 3585, 309, 689, 281, 574, 11, 293, 574, 412, 452, 4166, 51008], "temperature": 0.0, "avg_logprob": -0.1507831605012752, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.00039629722596146166}, {"id": 199, "seek": 80276, "start": 815.64, "end": 820.8, "text": " store, and I'm using this very specific embedding model of a hugging face.", "tokens": [51008, 3531, 11, 293, 286, 478, 1228, 341, 588, 2685, 12240, 3584, 2316, 295, 257, 41706, 1851, 13, 51266], "temperature": 0.0, "avg_logprob": -0.1507831605012752, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.00039629722596146166}, {"id": 200, "seek": 80276, "start": 820.8, "end": 827.48, "text": " I then tell the Retriever to update all of the embeddings in my document store, so it's", "tokens": [51266, 286, 550, 980, 264, 11495, 5469, 331, 281, 5623, 439, 295, 264, 12240, 29432, 294, 452, 4166, 3531, 11, 370, 309, 311, 51600], "temperature": 0.0, "avg_logprob": -0.1507831605012752, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.00039629722596146166}, {"id": 201, "seek": 82748, "start": 827.52, "end": 833.52, "text": " basically using that model to create vector representations of all of my TXD files, and", "tokens": [50366, 1936, 1228, 300, 2316, 281, 1884, 8062, 33358, 295, 439, 295, 452, 314, 55, 35, 7098, 11, 293, 50666], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 202, "seek": 82748, "start": 833.52, "end": 835.44, "text": " then I'm initializing a reader.", "tokens": [50666, 550, 286, 478, 5883, 3319, 257, 15149, 13, 50762], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 203, "seek": 82748, "start": 835.44, "end": 841.04, "text": " Same thing that we did before, I'm just using a specific model of a hugging face, this is", "tokens": [50762, 10635, 551, 300, 321, 630, 949, 11, 286, 478, 445, 1228, 257, 2685, 2316, 295, 257, 41706, 1851, 11, 341, 307, 51042], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 204, "seek": 82748, "start": 841.04, "end": 844.48, "text": " trained by the company I work for too.", "tokens": [51042, 8895, 538, 264, 2237, 286, 589, 337, 886, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 205, "seek": 82748, "start": 844.48, "end": 846.4, "text": " Then I do the exact same thing I did before.", "tokens": [51214, 1396, 286, 360, 264, 1900, 912, 551, 286, 630, 949, 13, 51310], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 206, "seek": 82748, "start": 846.4, "end": 851.24, "text": " I'm just creating the pipeline, adding the nodes, and then I run maybe who is the father", "tokens": [51310, 286, 478, 445, 4084, 264, 15517, 11, 5127, 264, 13891, 11, 293, 550, 286, 1190, 1310, 567, 307, 264, 3086, 51552], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 207, "seek": 82748, "start": 851.24, "end": 856.76, "text": " of ARIA stock, and this is what I might get back as an answer.", "tokens": [51552, 295, 316, 41125, 4127, 11, 293, 341, 307, 437, 286, 1062, 483, 646, 382, 364, 1867, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16446364720662435, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.0015476191183552146}, {"id": 208, "seek": 85676, "start": 856.8, "end": 861.0, "text": " The thing to notice here, the answers are very eddard, Ned, and that's because it's", "tokens": [50366, 440, 551, 281, 3449, 510, 11, 264, 6338, 366, 588, 1257, 67, 515, 11, 31355, 11, 293, 300, 311, 570, 309, 311, 50576], "temperature": 0.0, "avg_logprob": -0.18002503436544667, "compression_ratio": 1.7975708502024292, "no_speech_prob": 0.0034689605236053467}, {"id": 209, "seek": 85676, "start": 861.0, "end": 865.6, "text": " not generating answers, it's extracting the answer that's already in the context.", "tokens": [50576, 406, 17746, 6338, 11, 309, 311, 49844, 264, 1867, 300, 311, 1217, 294, 264, 4319, 13, 50806], "temperature": 0.0, "avg_logprob": -0.18002503436544667, "compression_ratio": 1.7975708502024292, "no_speech_prob": 0.0034689605236053467}, {"id": 210, "seek": 85676, "start": 865.6, "end": 871.6, "text": " If you see the first answer below, you'll notice that there's eddard in there, and this pipeline", "tokens": [50806, 759, 291, 536, 264, 700, 1867, 2507, 11, 291, 603, 3449, 300, 456, 311, 1257, 67, 515, 294, 456, 11, 293, 341, 15517, 51106], "temperature": 0.0, "avg_logprob": -0.18002503436544667, "compression_ratio": 1.7975708502024292, "no_speech_prob": 0.0034689605236053467}, {"id": 211, "seek": 85676, "start": 871.6, "end": 875.92, "text": " and this model has decided this is the most relevant answer to you, I could have printed", "tokens": [51106, 293, 341, 2316, 575, 3047, 341, 307, 264, 881, 7340, 1867, 281, 291, 11, 286, 727, 362, 13567, 51322], "temperature": 0.0, "avg_logprob": -0.18002503436544667, "compression_ratio": 1.7975708502024292, "no_speech_prob": 0.0034689605236053467}, {"id": 212, "seek": 85676, "start": 875.92, "end": 882.16, "text": " out schools, you can get schools, I just haven't here, and then I said give me the top five.", "tokens": [51322, 484, 4656, 11, 291, 393, 483, 4656, 11, 286, 445, 2378, 380, 510, 11, 293, 550, 286, 848, 976, 385, 264, 1192, 1732, 13, 51634], "temperature": 0.0, "avg_logprob": -0.18002503436544667, "compression_ratio": 1.7975708502024292, "no_speech_prob": 0.0034689605236053467}, {"id": 213, "seek": 88216, "start": 882.1999999999999, "end": 887.9599999999999, "text": " The first two, three, I think are correct, so we've got something working, but what if", "tokens": [50366, 440, 700, 732, 11, 1045, 11, 286, 519, 366, 3006, 11, 370, 321, 600, 658, 746, 1364, 11, 457, 437, 498, 50654], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 214, "seek": 88216, "start": 887.9599999999999, "end": 893.28, "text": " I want to generate human sounding like answers, eddard is pretty okay, I've got the answer,", "tokens": [50654, 286, 528, 281, 8460, 1952, 24931, 411, 6338, 11, 1257, 67, 515, 307, 1238, 1392, 11, 286, 600, 658, 264, 1867, 11, 50920], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 215, "seek": 88216, "start": 893.28, "end": 898.68, "text": " but maybe I want a system, maybe I want to create a chatbot that talks to me.", "tokens": [50920, 457, 1310, 286, 528, 257, 1185, 11, 1310, 286, 528, 281, 1884, 257, 5081, 18870, 300, 6686, 281, 385, 13, 51190], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 216, "seek": 88216, "start": 898.68, "end": 902.4399999999999, "text": " Let's look at how we might do that.", "tokens": [51190, 961, 311, 574, 412, 577, 321, 1062, 360, 300, 13, 51378], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 217, "seek": 88216, "start": 902.4399999999999, "end": 907.04, "text": " This is going to be a bit of a special example, because I'm not going to build a pipeline.", "tokens": [51378, 639, 307, 516, 281, 312, 257, 857, 295, 257, 2121, 1365, 11, 570, 286, 478, 406, 516, 281, 1322, 257, 15517, 13, 51608], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 218, "seek": 88216, "start": 907.04, "end": 911.4, "text": " The reason for that is, as mentioned before, these generative models don't need context,", "tokens": [51608, 440, 1778, 337, 300, 307, 11, 382, 2835, 949, 11, 613, 1337, 1166, 5245, 500, 380, 643, 4319, 11, 51826], "temperature": 0.0, "avg_logprob": -0.18658697605133057, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.0007835865253582597}, {"id": 219, "seek": 91140, "start": 911.4399999999999, "end": 913.88, "text": " so I should be able to just use them.", "tokens": [50366, 370, 286, 820, 312, 1075, 281, 445, 764, 552, 13, 50488], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 220, "seek": 91140, "start": 913.88, "end": 919.9599999999999, "text": " We've got this node called the prompt node, and what this does is actually a special node,", "tokens": [50488, 492, 600, 658, 341, 9984, 1219, 264, 12391, 9984, 11, 293, 437, 341, 775, 307, 767, 257, 2121, 9984, 11, 50792], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 221, "seek": 91140, "start": 919.9599999999999, "end": 924.64, "text": " because you can more fit based on what you want it to do.", "tokens": [50792, 570, 291, 393, 544, 3318, 2361, 322, 437, 291, 528, 309, 281, 360, 13, 51026], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 222, "seek": 91140, "start": 924.64, "end": 929.68, "text": " You might have heard recently this whole terminology around prompt engineering, and that's basically", "tokens": [51026, 509, 1062, 362, 2198, 3938, 341, 1379, 27575, 926, 12391, 7043, 11, 293, 300, 311, 1936, 51278], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 223, "seek": 91140, "start": 929.68, "end": 935.0, "text": " used with models that are able to consume some instruction and act accordingly.", "tokens": [51278, 1143, 365, 5245, 300, 366, 1075, 281, 14732, 512, 10951, 293, 605, 19717, 13, 51544], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 224, "seek": 91140, "start": 935.0, "end": 939.92, "text": " By default, our prompt node is basically told, you know, just answer the question, that's", "tokens": [51544, 3146, 7576, 11, 527, 12391, 9984, 307, 1936, 1907, 11, 291, 458, 11, 445, 1867, 264, 1168, 11, 300, 311, 51790], "temperature": 0.0, "avg_logprob": -0.15583532827871818, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.0015158243477344513}, {"id": 225, "seek": 93992, "start": 939.9599999999999, "end": 945.24, "text": " all it does, but you could maybe define a template for it, what we call a prompt template, so", "tokens": [50366, 439, 309, 775, 11, 457, 291, 727, 1310, 6964, 257, 12379, 337, 309, 11, 437, 321, 818, 257, 12391, 12379, 11, 370, 50630], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 226, "seek": 93992, "start": 945.24, "end": 950.68, "text": " I could have maybe said, you know, answer the question as a yes or no answer, and it", "tokens": [50630, 286, 727, 362, 1310, 848, 11, 291, 458, 11, 1867, 264, 1168, 382, 257, 2086, 420, 572, 1867, 11, 293, 309, 50902], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 227, "seek": 93992, "start": 950.68, "end": 954.12, "text": " would give me a yes or no answer, but obviously I need to ask it a yes or no question for", "tokens": [50902, 576, 976, 385, 257, 2086, 420, 572, 1867, 11, 457, 2745, 286, 643, 281, 1029, 309, 257, 2086, 420, 572, 1168, 337, 51074], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 228, "seek": 93992, "start": 954.12, "end": 955.12, "text": " it to make sense.", "tokens": [51074, 309, 281, 652, 2020, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 229, "seek": 93992, "start": 955.12, "end": 960.36, "text": " Anyway, so I'm just using it like this, like the pure form, and I'm using a model from", "tokens": [51124, 5684, 11, 370, 286, 478, 445, 1228, 309, 411, 341, 11, 411, 264, 6075, 1254, 11, 293, 286, 478, 1228, 257, 2316, 490, 51386], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 230, "seek": 93992, "start": 960.36, "end": 965.5999999999999, "text": " OpenAI, obviously I need to provide an API key, and I'm using this particular one, text", "tokens": [51386, 7238, 48698, 11, 2745, 286, 643, 281, 2893, 364, 9362, 2141, 11, 293, 286, 478, 1228, 341, 1729, 472, 11, 2487, 51648], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 231, "seek": 93992, "start": 965.5999999999999, "end": 967.7199999999999, "text": " of inchy 003.", "tokens": [51648, 295, 7227, 88, 7143, 18, 13, 51754], "temperature": 0.0, "avg_logprob": -0.1846331428079044, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0014586878241971135}, {"id": 232, "seek": 96772, "start": 967.72, "end": 971.88, "text": " I actually ran these yesterday, so these are the replies I got, and this particular one", "tokens": [50364, 286, 767, 5872, 613, 5186, 11, 370, 613, 366, 264, 42289, 286, 658, 11, 293, 341, 1729, 472, 50572], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 233, "seek": 96772, "start": 971.88, "end": 976.44, "text": " I ran a few times, so the first time I ran, when is Milosh flying to Frankfurt?", "tokens": [50572, 286, 5872, 257, 1326, 1413, 11, 370, 264, 700, 565, 286, 5872, 11, 562, 307, 7036, 3019, 7137, 281, 36530, 30, 50800], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 234, "seek": 96772, "start": 976.44, "end": 980.36, "text": " By the way, spoiler alert, Milosh is our CEO.", "tokens": [50800, 3146, 264, 636, 11, 26927, 9615, 11, 7036, 3019, 307, 527, 9282, 13, 50996], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 235, "seek": 96772, "start": 980.36, "end": 985.1600000000001, "text": " So I know who Milosh is, and I know when he's flying to Frankfurt, or when he flew to Frankfurt.", "tokens": [50996, 407, 286, 458, 567, 7036, 3019, 307, 11, 293, 286, 458, 562, 415, 311, 7137, 281, 36530, 11, 420, 562, 415, 15728, 281, 36530, 13, 51236], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 236, "seek": 96772, "start": 985.1600000000001, "end": 991.08, "text": " And I get an answer, Milosh's flight to Frankfurt is scheduled for August 7th, 2020.", "tokens": [51236, 400, 286, 483, 364, 1867, 11, 7036, 3019, 311, 7018, 281, 36530, 307, 15678, 337, 6897, 1614, 392, 11, 4808, 13, 51532], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 237, "seek": 96772, "start": 991.08, "end": 997.6800000000001, "text": " This is really convincing sounding, fine, okay, but this one was actually quite impressive,", "tokens": [51532, 639, 307, 534, 24823, 24931, 11, 2489, 11, 1392, 11, 457, 341, 472, 390, 767, 1596, 8992, 11, 51862], "temperature": 0.0, "avg_logprob": -0.15809232271634616, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.0009736987994983792}, {"id": 238, "seek": 99768, "start": 997.68, "end": 1004.28, "text": " again, if I ran the same exact query with this model, I got, it's not possible to answer", "tokens": [50364, 797, 11, 498, 286, 5872, 264, 912, 1900, 14581, 365, 341, 2316, 11, 286, 658, 11, 309, 311, 406, 1944, 281, 1867, 50694], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 239, "seek": 99768, "start": 1004.28, "end": 1005.88, "text": " this question without more information.", "tokens": [50694, 341, 1168, 1553, 544, 1589, 13, 50774], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 240, "seek": 99768, "start": 1005.88, "end": 1011.88, "text": " This is actually really cool, because clearly this model sometimes can infer that, hey, maybe", "tokens": [50774, 639, 307, 767, 534, 1627, 11, 570, 4448, 341, 2316, 2171, 393, 13596, 300, 11, 4177, 11, 1310, 51074], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 241, "seek": 99768, "start": 1011.88, "end": 1017.8, "text": " I need more information to give you an answer, that what we now refer to as hallucination.", "tokens": [51074, 286, 643, 544, 1589, 281, 976, 291, 364, 1867, 11, 300, 437, 321, 586, 2864, 281, 382, 35212, 2486, 13, 51370], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 242, "seek": 99768, "start": 1017.8, "end": 1021.3599999999999, "text": " Maybe you've heard of that term, also these models can hallucinate, they're tasked to", "tokens": [51370, 2704, 291, 600, 2198, 295, 300, 1433, 11, 611, 613, 5245, 393, 35212, 13923, 11, 436, 434, 38621, 281, 51548], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 243, "seek": 99768, "start": 1021.3599999999999, "end": 1022.3599999999999, "text": " generate answers.", "tokens": [51548, 8460, 6338, 13, 51598], "temperature": 0.0, "avg_logprob": -0.19116818671133004, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.006992756854742765}, {"id": 244, "seek": 102236, "start": 1022.92, "end": 1028.72, "text": " It's not tasked to generate, you know, actual answers for you, that are truthful.", "tokens": [50392, 467, 311, 406, 38621, 281, 8460, 11, 291, 458, 11, 3539, 6338, 337, 291, 11, 300, 366, 44669, 13, 50682], "temperature": 0.0, "avg_logprob": -0.23775344297110315, "compression_ratio": 1.5, "no_speech_prob": 0.019451895728707314}, {"id": 245, "seek": 102236, "start": 1028.72, "end": 1031.84, "text": " Anyway, let's say, when is Milosh travelling somewhere?", "tokens": [50682, 5684, 11, 718, 311, 584, 11, 562, 307, 7036, 3019, 20515, 4079, 30, 50838], "temperature": 0.0, "avg_logprob": -0.23775344297110315, "compression_ratio": 1.5, "no_speech_prob": 0.019451895728707314}, {"id": 246, "seek": 102236, "start": 1031.84, "end": 1040.64, "text": " I love this answer, when he has the time and money available to do so.", "tokens": [50838, 286, 959, 341, 1867, 11, 562, 415, 575, 264, 565, 293, 1460, 2435, 281, 360, 370, 13, 51278], "temperature": 0.0, "avg_logprob": -0.23775344297110315, "compression_ratio": 1.5, "no_speech_prob": 0.019451895728707314}, {"id": 247, "seek": 102236, "start": 1040.64, "end": 1045.56, "text": " And then, I guess, I don't know which one is my favourite, this one, or the next one,", "tokens": [51278, 400, 550, 11, 286, 2041, 11, 286, 500, 380, 458, 597, 472, 307, 452, 10696, 11, 341, 472, 11, 420, 264, 958, 472, 11, 51524], "temperature": 0.0, "avg_logprob": -0.23775344297110315, "compression_ratio": 1.5, "no_speech_prob": 0.019451895728707314}, {"id": 248, "seek": 104556, "start": 1045.56, "end": 1048.56, "text": " who is Milosh?", "tokens": [50364, 567, 307, 7036, 3019, 30, 50514], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 249, "seek": 104556, "start": 1048.56, "end": 1051.56, "text": " A Greek island.", "tokens": [50514, 316, 10281, 6077, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 250, "seek": 104556, "start": 1051.56, "end": 1058.28, "text": " Lovely, okay, but the problem here is, this is very, you know, I could believe this, it's", "tokens": [50664, 33925, 11, 1392, 11, 457, 264, 1154, 510, 307, 11, 341, 307, 588, 11, 291, 458, 11, 286, 727, 1697, 341, 11, 309, 311, 51000], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 251, "seek": 104556, "start": 1058.28, "end": 1061.84, "text": " very like, realistic, these answers.", "tokens": [51000, 588, 411, 11, 12465, 11, 613, 6338, 13, 51178], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 252, "seek": 104556, "start": 1061.84, "end": 1067.84, "text": " So we're going to look at how we can use these large language models for our use cases, and", "tokens": [51178, 407, 321, 434, 516, 281, 574, 412, 577, 321, 393, 764, 613, 2416, 2856, 5245, 337, 527, 764, 3331, 11, 293, 51478], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 253, "seek": 104556, "start": 1067.84, "end": 1071.12, "text": " what we're going to do is basically, we're going to do exactly what we did for the extractive", "tokens": [51478, 437, 321, 434, 516, 281, 360, 307, 1936, 11, 321, 434, 516, 281, 360, 2293, 437, 321, 630, 337, 264, 8947, 488, 51642], "temperature": 0.0, "avg_logprob": -0.22678139744972697, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.016994228586554527}, {"id": 254, "seek": 107112, "start": 1071.1599999999999, "end": 1076.4799999999998, "text": " QA1, and we're going to use a component that is quite clever, because it's been prompted", "tokens": [50366, 1249, 32, 16, 11, 293, 321, 434, 516, 281, 764, 257, 6542, 300, 307, 1596, 13494, 11, 570, 309, 311, 668, 31042, 50632], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 255, "seek": 107112, "start": 1076.4799999999998, "end": 1083.52, "text": " to say, generate answers based off of these retrieved documents and nothing else.", "tokens": [50632, 281, 584, 11, 8460, 6338, 2361, 766, 295, 613, 19817, 937, 8512, 293, 1825, 1646, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 256, "seek": 107112, "start": 1083.52, "end": 1088.9199999999998, "text": " It can sometimes not work well, but there are ways to make it work well, and we won't", "tokens": [50984, 467, 393, 2171, 406, 589, 731, 11, 457, 456, 366, 2098, 281, 652, 309, 589, 731, 11, 293, 321, 1582, 380, 51254], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 257, "seek": 107112, "start": 1088.9199999999998, "end": 1094.04, "text": " get into all the creativity behind it, so I'll show you the most basic solution you", "tokens": [51254, 483, 666, 439, 264, 12915, 2261, 309, 11, 370, 286, 603, 855, 291, 264, 881, 3875, 3827, 291, 51510], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 258, "seek": 107112, "start": 1094.04, "end": 1095.04, "text": " might get.", "tokens": [51510, 1062, 483, 13, 51560], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 259, "seek": 107112, "start": 1095.04, "end": 1099.84, "text": " But this is going to be what we do, it's the same exact pipeline as before, the reader", "tokens": [51560, 583, 341, 307, 516, 281, 312, 437, 321, 360, 11, 309, 311, 264, 912, 1900, 15517, 382, 949, 11, 264, 15149, 51800], "temperature": 0.0, "avg_logprob": -0.1373824990313986, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.01968178153038025}, {"id": 260, "seek": 109984, "start": 1099.8799999999999, "end": 1102.52, "text": " has been replaced by the generator.", "tokens": [50366, 575, 668, 10772, 538, 264, 19265, 13, 50498], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 261, "seek": 109984, "start": 1102.52, "end": 1106.6399999999999, "text": " So I actually have Milosh's ticket to Frankfurt.", "tokens": [50498, 407, 286, 767, 362, 7036, 3019, 311, 10550, 281, 36530, 13, 50704], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 262, "seek": 109984, "start": 1106.6399999999999, "end": 1113.04, "text": " It was 14th of November, and as a bonus, I thought I'd try, this is my ticket, my euro", "tokens": [50704, 467, 390, 3499, 392, 295, 7674, 11, 293, 382, 257, 10882, 11, 286, 1194, 286, 1116, 853, 11, 341, 307, 452, 10550, 11, 452, 14206, 51024], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 263, "seek": 109984, "start": 1113.04, "end": 1118.04, "text": " star ticket, from Amsterdam to London and back.", "tokens": [51024, 3543, 10550, 11, 490, 28291, 281, 7042, 293, 646, 13, 51274], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 264, "seek": 109984, "start": 1118.04, "end": 1121.76, "text": " So I've got these, and they are PDFs.", "tokens": [51274, 407, 286, 600, 658, 613, 11, 293, 436, 366, 17752, 82, 13, 51460], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 265, "seek": 109984, "start": 1121.76, "end": 1126.76, "text": " And so now I'm going to start defining my new components.", "tokens": [51460, 400, 370, 586, 286, 478, 516, 281, 722, 17827, 452, 777, 6677, 13, 51710], "temperature": 0.0, "avg_logprob": -0.19912871070530103, "compression_ratio": 1.4318181818181819, "no_speech_prob": 0.0018639371264725924}, {"id": 266, "seek": 112676, "start": 1126.76, "end": 1131.84, "text": " So I've got the same files document store, embedding dimensions is not something you", "tokens": [50364, 407, 286, 600, 658, 264, 912, 7098, 4166, 3531, 11, 12240, 3584, 12819, 307, 406, 746, 291, 50618], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 267, "seek": 112676, "start": 1131.84, "end": 1136.2, "text": " should worry about for now, and I'm defining an embedding retriever here.", "tokens": [50618, 820, 3292, 466, 337, 586, 11, 293, 286, 478, 17827, 364, 12240, 3584, 19817, 331, 510, 13, 50836], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 268, "seek": 112676, "start": 1136.2, "end": 1142.04, "text": " What I'm doing is, again, I'm using a model by OpenAI, so I'm using an API key.", "tokens": [50836, 708, 286, 478, 884, 307, 11, 797, 11, 286, 478, 1228, 257, 2316, 538, 7238, 48698, 11, 370, 286, 478, 1228, 364, 9362, 2141, 13, 51128], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 269, "seek": 112676, "start": 1142.04, "end": 1147.24, "text": " So this is the model I'm going to use to create vector representations and then compare it", "tokens": [51128, 407, 341, 307, 264, 2316, 286, 478, 516, 281, 764, 281, 1884, 8062, 33358, 293, 550, 6794, 309, 51388], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 270, "seek": 112676, "start": 1147.24, "end": 1149.12, "text": " to queries.", "tokens": [51388, 281, 24109, 13, 51482], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 271, "seek": 112676, "start": 1149.12, "end": 1153.68, "text": " And this time, I'm not using the front node, I'm using that clever node there, called the", "tokens": [51482, 400, 341, 565, 11, 286, 478, 406, 1228, 264, 1868, 9984, 11, 286, 478, 1228, 300, 13494, 9984, 456, 11, 1219, 264, 51710], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 272, "seek": 112676, "start": 1153.68, "end": 1155.6, "text": " OpenAI answer generator.", "tokens": [51710, 7238, 48698, 1867, 19265, 13, 51806], "temperature": 0.0, "avg_logprob": -0.18042001491639673, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.002735980786383152}, {"id": 273, "seek": 115560, "start": 1155.6, "end": 1161.1999999999998, "text": " And you might notice it is the exact same model as the one before.", "tokens": [50364, 400, 291, 1062, 3449, 309, 307, 264, 1900, 912, 2316, 382, 264, 472, 949, 13, 50644], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 274, "seek": 115560, "start": 1161.1999999999998, "end": 1166.76, "text": " We're going to briefly look at indexing, so we've got the PDF text converter and pre-processor.", "tokens": [50644, 492, 434, 516, 281, 10515, 574, 412, 8186, 278, 11, 370, 321, 600, 658, 264, 17752, 2487, 33905, 293, 659, 12, 4318, 25432, 13, 50922], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 275, "seek": 115560, "start": 1166.76, "end": 1168.84, "text": " And let's go to the next slide.", "tokens": [50922, 400, 718, 311, 352, 281, 264, 958, 4137, 13, 51026], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 276, "seek": 115560, "start": 1168.84, "end": 1173.56, "text": " As mentioned before, there are pre-made pipelines, so I could have just defined generative QA", "tokens": [51026, 1018, 2835, 949, 11, 456, 366, 659, 12, 10341, 40168, 11, 370, 286, 727, 362, 445, 7642, 1337, 1166, 1249, 32, 51262], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 277, "seek": 115560, "start": 1173.56, "end": 1177.36, "text": " pipeline and told it what generate and retriever to use, but let's look at what it might look", "tokens": [51262, 15517, 293, 1907, 309, 437, 8460, 293, 19817, 331, 281, 764, 11, 457, 718, 311, 574, 412, 437, 309, 1062, 574, 51452], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 278, "seek": 115560, "start": 1177.36, "end": 1180.56, "text": " like if I were to build it from scratch.", "tokens": [51452, 411, 498, 286, 645, 281, 1322, 309, 490, 8459, 13, 51612], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 279, "seek": 115560, "start": 1180.56, "end": 1183.52, "text": " And first, you see the indexing pipeline.", "tokens": [51612, 400, 700, 11, 291, 536, 264, 8186, 278, 15517, 13, 51760], "temperature": 0.0, "avg_logprob": -0.15670099712553479, "compression_ratio": 1.6726618705035972, "no_speech_prob": 0.0006291007739491761}, {"id": 280, "seek": 118352, "start": 1183.52, "end": 1188.52, "text": " So if you follow it, you'll notice that it's getting the PDF file and then writing that", "tokens": [50364, 407, 498, 291, 1524, 309, 11, 291, 603, 3449, 300, 309, 311, 1242, 264, 17752, 3991, 293, 550, 3579, 300, 50614], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 281, "seek": 118352, "start": 1188.52, "end": 1191.28, "text": " to a document store, given some pre-processing steps.", "tokens": [50614, 281, 257, 4166, 3531, 11, 2212, 512, 659, 12, 41075, 278, 4439, 13, 50752], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 282, "seek": 118352, "start": 1191.28, "end": 1194.96, "text": " And I then write my and Niloche's tickets in there.", "tokens": [50752, 400, 286, 550, 2464, 452, 293, 426, 10720, 1876, 311, 12628, 294, 456, 13, 50936], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 283, "seek": 118352, "start": 1194.96, "end": 1199.92, "text": " And the querying pipeline is the exact same as the extractive QA pipeline you saw before.", "tokens": [50936, 400, 264, 7083, 1840, 15517, 307, 264, 1900, 912, 382, 264, 8947, 488, 1249, 32, 15517, 291, 1866, 949, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 284, "seek": 118352, "start": 1199.92, "end": 1205.04, "text": " All that, the only difference is, the last bit is the answer generator, not the reader.", "tokens": [51184, 1057, 300, 11, 264, 787, 2649, 307, 11, 264, 1036, 857, 307, 264, 1867, 19265, 11, 406, 264, 15149, 13, 51440], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 285, "seek": 118352, "start": 1205.04, "end": 1210.24, "text": " This time, though, it does have some context and it does have some documents.", "tokens": [51440, 639, 565, 11, 1673, 11, 309, 775, 362, 512, 4319, 293, 309, 775, 362, 512, 8512, 13, 51700], "temperature": 0.0, "avg_logprob": -0.15249840687897245, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0001414383004885167}, {"id": 286, "seek": 121024, "start": 1210.24, "end": 1214.52, "text": " What did I get when I ran the same two questions?", "tokens": [50364, 708, 630, 286, 483, 562, 286, 5872, 264, 912, 732, 1651, 30, 50578], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 287, "seek": 121024, "start": 1214.52, "end": 1216.92, "text": " I got, who is Milosh?", "tokens": [50578, 286, 658, 11, 567, 307, 7036, 3019, 30, 50698], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 288, "seek": 121024, "start": 1216.92, "end": 1218.6, "text": " He's not a Greek island.", "tokens": [50698, 634, 311, 406, 257, 10281, 6077, 13, 50782], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 289, "seek": 121024, "start": 1218.6, "end": 1223.44, "text": " He is the passenger whose travel data is on the passenger itinerary receipt.", "tokens": [50782, 634, 307, 264, 18707, 6104, 3147, 1412, 307, 322, 264, 18707, 309, 4564, 822, 33882, 13, 51024], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 290, "seek": 121024, "start": 1223.44, "end": 1227.68, "text": " Now, this is the only information this model knows, so it can't tell me he's my CEO because", "tokens": [51024, 823, 11, 341, 307, 264, 787, 1589, 341, 2316, 3255, 11, 370, 309, 393, 380, 980, 385, 415, 311, 452, 9282, 570, 51236], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 291, "seek": 121024, "start": 1227.68, "end": 1231.36, "text": " I haven't uploaded any information about my company.", "tokens": [51236, 286, 2378, 380, 17135, 604, 1589, 466, 452, 2237, 13, 51420], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 292, "seek": 121024, "start": 1231.36, "end": 1235.76, "text": " So don't make something up, just tell me what you know.", "tokens": [51420, 407, 500, 380, 652, 746, 493, 11, 445, 980, 385, 437, 291, 458, 13, 51640], "temperature": 0.0, "avg_logprob": -0.17252705891927084, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.018633106723427773}, {"id": 293, "seek": 123576, "start": 1235.84, "end": 1239.24, "text": " If I run, when is Milosh flying to Frankfurt?", "tokens": [50368, 759, 286, 1190, 11, 562, 307, 7036, 3019, 7137, 281, 36530, 30, 50538], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 294, "seek": 123576, "start": 1239.24, "end": 1244.04, "text": " I get Milosh is flying to Frankfurt on the correct date and time.", "tokens": [50538, 286, 483, 7036, 3019, 307, 7137, 281, 36530, 322, 264, 3006, 4002, 293, 565, 13, 50778], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 295, "seek": 123576, "start": 1244.04, "end": 1248.48, "text": " And then I had that bonus in there, who is traveling to London.", "tokens": [50778, 400, 550, 286, 632, 300, 10882, 294, 456, 11, 567, 307, 9712, 281, 7042, 13, 51000], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 296, "seek": 123576, "start": 1248.48, "end": 1251.84, "text": " I would get Twana Caelic is traveling to London.", "tokens": [51000, 286, 576, 483, 2574, 2095, 383, 4300, 299, 307, 9712, 281, 7042, 13, 51168], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 297, "seek": 123576, "start": 1251.84, "end": 1261.52, "text": " Now, if I were to run, let's say, who is, let's say, when is Alfred traveling to Frankfurt?", "tokens": [51168, 823, 11, 498, 286, 645, 281, 1190, 11, 718, 311, 584, 11, 567, 307, 11, 718, 311, 584, 11, 562, 307, 28327, 9712, 281, 36530, 30, 51652], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 298, "seek": 123576, "start": 1261.52, "end": 1265.32, "text": " What I haven't shown you here, because I think it goes a bit too deep into building these", "tokens": [51652, 708, 286, 2378, 380, 4898, 291, 510, 11, 570, 286, 519, 309, 1709, 257, 857, 886, 2452, 666, 2390, 613, 51842], "temperature": 0.0, "avg_logprob": -0.20650271540102752, "compression_ratio": 1.7964601769911503, "no_speech_prob": 0.020638179033994675}, {"id": 299, "seek": 126532, "start": 1265.36, "end": 1274.04, "text": " types of pipelines, for the open AI answer generator, I could actually provide examples", "tokens": [50366, 3467, 295, 40168, 11, 337, 264, 1269, 7318, 1867, 19265, 11, 286, 727, 767, 2893, 5110, 50800], "temperature": 0.0, "avg_logprob": -0.21576620737711588, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0008208374492824078}, {"id": 300, "seek": 126532, "start": 1274.04, "end": 1276.24, "text": " and example documents.", "tokens": [50800, 293, 1365, 8512, 13, 50910], "temperature": 0.0, "avg_logprob": -0.21576620737711588, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0008208374492824078}, {"id": 301, "seek": 126532, "start": 1276.24, "end": 1280.9199999999998, "text": " Just in case I'm worried that it's going to make up something somewhere at a time that", "tokens": [50910, 1449, 294, 1389, 286, 478, 5804, 300, 309, 311, 516, 281, 652, 493, 746, 4079, 412, 257, 565, 300, 51144], "temperature": 0.0, "avg_logprob": -0.21576620737711588, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0008208374492824078}, {"id": 302, "seek": 126532, "start": 1280.9199999999998, "end": 1285.8799999999999, "text": " this Alfred who doesn't exist is traveling to Frankfurt, I can give it some example saying,", "tokens": [51144, 341, 28327, 567, 1177, 380, 2514, 307, 9712, 281, 36530, 11, 286, 393, 976, 309, 512, 1365, 1566, 11, 51392], "temperature": 0.0, "avg_logprob": -0.21576620737711588, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0008208374492824078}, {"id": 303, "seek": 126532, "start": 1285.8799999999999, "end": 1291.1599999999999, "text": " hey, if you encounter something like this, just say I don't have the context for it.", "tokens": [51392, 4177, 11, 498, 291, 8593, 746, 411, 341, 11, 445, 584, 286, 500, 380, 362, 264, 4319, 337, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.21576620737711588, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0008208374492824078}, {"id": 304, "seek": 129116, "start": 1291.2, "end": 1296.3200000000002, "text": " So I could have just run query pipeline.run when is Alfred traveling to Frankfurt, and", "tokens": [50366, 407, 286, 727, 362, 445, 1190, 14581, 15517, 13, 12997, 562, 307, 28327, 9712, 281, 36530, 11, 293, 50622], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 305, "seek": 129116, "start": 1296.3200000000002, "end": 1301.0800000000002, "text": " it would have told me I have no context for this, so I'm not going to give you the answer.", "tokens": [50622, 309, 576, 362, 1907, 385, 286, 362, 572, 4319, 337, 341, 11, 370, 286, 478, 406, 516, 281, 976, 291, 264, 1867, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 306, "seek": 129116, "start": 1301.0800000000002, "end": 1304.1200000000001, "text": " This model that we saw does do that sometimes.", "tokens": [50860, 639, 2316, 300, 321, 1866, 775, 360, 300, 2171, 13, 51012], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 307, "seek": 129116, "start": 1304.1200000000001, "end": 1309.4, "text": " The first example we saw, it did say I don't have enough context for this, but not all", "tokens": [51012, 440, 700, 1365, 321, 1866, 11, 309, 630, 584, 286, 500, 380, 362, 1547, 4319, 337, 341, 11, 457, 406, 439, 51276], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 308, "seek": 129116, "start": 1309.4, "end": 1310.4, "text": " the time.", "tokens": [51276, 264, 565, 13, 51326], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 309, "seek": 129116, "start": 1310.4, "end": 1314.3600000000001, "text": " So this is how you might use it for your own use cases, you might use large language models", "tokens": [51326, 407, 341, 307, 577, 291, 1062, 764, 309, 337, 428, 1065, 764, 3331, 11, 291, 1062, 764, 2416, 2856, 5245, 51524], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 310, "seek": 129116, "start": 1314.3600000000001, "end": 1319.64, "text": " for your own use cases, and how you might mitigate them hallucinating.", "tokens": [51524, 337, 428, 1065, 764, 3331, 11, 293, 577, 291, 1062, 27336, 552, 35212, 8205, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15481378161717976, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0004737294220831245}, {"id": 311, "seek": 131964, "start": 1319.68, "end": 1324.64, "text": " So to conclude, extractive question answering models and pipelines are great at retrieving", "tokens": [50366, 407, 281, 16886, 11, 8947, 488, 1168, 13430, 5245, 293, 40168, 366, 869, 412, 19817, 798, 50614], "temperature": 0.0, "avg_logprob": -0.19104064862752698, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.0013268071925267577}, {"id": 312, "seek": 131964, "start": 1324.64, "end": 1329.44, "text": " knowledge that already exists in context, however, generative models are really cool", "tokens": [50614, 3601, 300, 1217, 8198, 294, 4319, 11, 4461, 11, 1337, 1166, 5245, 366, 534, 1627, 50854], "temperature": 0.0, "avg_logprob": -0.19104064862752698, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.0013268071925267577}, {"id": 313, "seek": 131964, "start": 1329.44, "end": 1334.92, "text": " because they can generate human-like answers, but combining them with a retrieval augmenter", "tokens": [50854, 570, 436, 393, 8460, 1952, 12, 4092, 6338, 11, 457, 21928, 552, 365, 257, 19817, 3337, 29919, 260, 51128], "temperature": 0.0, "avg_logprob": -0.19104064862752698, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.0013268071925267577}, {"id": 314, "seek": 131964, "start": 1334.92, "end": 1340.48, "text": " step means that you can use them very specifically for your own use cases.", "tokens": [51128, 1823, 1355, 300, 291, 393, 764, 552, 588, 4682, 337, 428, 1065, 764, 3331, 13, 51406], "temperature": 0.0, "avg_logprob": -0.19104064862752698, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.0013268071925267577}, {"id": 315, "seek": 131964, "start": 1340.48, "end": 1346.2800000000002, "text": " Haystack as I mentioned is fully open source, it's built in Python, and we accept contributions", "tokens": [51406, 8721, 372, 501, 382, 286, 2835, 307, 4498, 1269, 4009, 11, 309, 311, 3094, 294, 15329, 11, 293, 321, 3241, 15725, 51696], "temperature": 0.0, "avg_logprob": -0.19104064862752698, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.0013268071925267577}, {"id": 316, "seek": 134628, "start": 1347.0, "end": 1352.44, "text": " literally welcome, and I would say every release we have a community contribution in there.", "tokens": [50400, 3736, 2928, 11, 293, 286, 576, 584, 633, 4374, 321, 362, 257, 1768, 13150, 294, 456, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2550427685045216, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.028156321495771408}, {"id": 317, "seek": 134628, "start": 1352.44, "end": 1357.84, "text": " Thank you very much, and this QR code is our first tutorial, bear in mind it is an extractive", "tokens": [50672, 1044, 291, 588, 709, 11, 293, 341, 32784, 3089, 307, 527, 700, 7073, 11, 6155, 294, 1575, 309, 307, 364, 8947, 488, 50942], "temperature": 0.0, "avg_logprob": -0.2550427685045216, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.028156321495771408}, {"id": 318, "seek": 134628, "start": 1357.84, "end": 1362.04, "text": " one, it's the non-cool one, but it is a good way to start.", "tokens": [50942, 472, 11, 309, 311, 264, 2107, 12, 28155, 472, 11, 457, 309, 307, 257, 665, 636, 281, 722, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2550427685045216, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.028156321495771408}, {"id": 319, "seek": 134628, "start": 1362.04, "end": 1363.04, "text": " Thank you very much.", "tokens": [51152, 1044, 291, 588, 709, 13, 51202], "temperature": 0.0, "avg_logprob": -0.2550427685045216, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.028156321495771408}, {"id": 320, "seek": 136304, "start": 1363.04, "end": 1370.04, "text": " Thank you, Luana.", "tokens": [50364, 1044, 291, 11, 5047, 2095, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3200959265232086, "compression_ratio": 1.2823529411764707, "no_speech_prob": 0.09655674546957016}, {"id": 321, "seek": 136304, "start": 1386.24, "end": 1390.6399999999999, "text": " We have a few minutes for questions, if you have questions for Luana, we have three minutes", "tokens": [51524, 492, 362, 257, 1326, 2077, 337, 1651, 11, 498, 291, 362, 1651, 337, 5047, 2095, 11, 321, 362, 1045, 2077, 51744], "temperature": 0.0, "avg_logprob": -0.3200959265232086, "compression_ratio": 1.2823529411764707, "no_speech_prob": 0.09655674546957016}, {"id": 322, "seek": 139064, "start": 1390.64, "end": 1393.8000000000002, "text": " for questions, as you can also find her afterwards.", "tokens": [50366, 337, 1651, 11, 382, 291, 393, 611, 915, 720, 10543, 13, 50522], "temperature": 0.0, "avg_logprob": -0.317664418901716, "compression_ratio": 0.8793103448275862, "no_speech_prob": 0.23430714011192322}], "language": "en"}