{"text": " Hello everybody, I will talk about Go Evil, which is a project, a personal project, which allows you to do one-liners in Go, so you just type your Go code and call it with Go Evil, and you can simply write a name or word from the command line. So this is like magic. I will show you a bit under the hood how it works. The word project is about 300 lines of code, not more. Here is an example, you can call Go Evil and tell it to take the code from the STD in, but here is how it works under the hood. From your Go code, from the command line, Go Evil generates a full Go program, and so the dash E allows to print that Go program that has been generated. It is sometimes useful when you want to debug the syntax around that you make, and then the Go import equal allows to stop using Go import, because here you see that in that code, there is no import of the FMT package, but it is introduced by the Go imports, which is called by Go Evil. So I am announcing today that Go Evil has been, 1.0 has been released just a few hours ago, and the new feature of Go Evil 1 is that Go modules are supported, and with Go module, you get locked versions for your dependency code from Go Evil. So this allows to submit to share your one-liners with other people, because the previous code that I showed was depending on the dependency to be installed in Go pass. And so that's it. Try it, use it, report bugs, and I'm available for question later. Thank you. It's weirdly enough, not the first open source project to be released when people are in the dev room. If this is your slide, you can come up now. Hello, everyone. My name is Keegan. I'm a staff software engineer element, and I've been spending the past year debugging why Go servers are slow. So hands up, who's made a crud application before? Create read, update, delete. That's basically everyone in this room, which is what I thought. Who's tried to speed up their server before? This is a slow request, 3.6 seconds. Fewer people, but still a fair number of people. Cool. Who's used PPROF before? So flame graphs. It's great. Who's used runtime trace before? Not that many people. Okay. Who's struggled to figure out what was going on when you're using this? Right. Okay. Great. This talk is for you. So the first thing you need to really is use spans to make these traces readable. Very easy. If you've ever used Jager spans before, they're basically the same sort of thing. So you can create a new task, and then you get a new context. You pass the context through to new functions. You can create regions from those, and you end up getting something that looks a bit like the stuff on the bottom there. You can also add log lines for some contextual information. That'll appear on the UI, which we'll get to in a moment. And the crash course in using runtime trace is you make a trace in the same way that you'd make a CPU profile with PPROF, except you hit a different endpoint, but you also tell it how long you want to trace for, and then you use gotool trace to open that trace. You don't use the gotool PPROF, confusingly, and you'll get something like the bottom over here, which is quite a lot of scary words and links, and you have no idea which thing to click. The only thing you care about is the user-defined tasks. If you click on that, you'll see something a bit like this. The only thing you care about is this GoRoutine view, and if you click on that, you can profile basically everything. So, for example, here's a bit of a request, which is slow because of garbage collection, and if you click on any one of those Gs at the bottom, which are highlighted with the red circle, you'll see stack traces that mention GC. Also, the blue bar in the middle there says GC, so spoiler. Other thing, if you have slow SQL queries, you can find that as well because if you click on any of these things, you'll see stack traces, and those stack traces refer to any point where the GoRoutine yields away for network IO or syscalls or things like that. So, you can clearly see, oh, it's doing something with SQL, and it's just doing the same thing for SQL for not particularly long here, only 20 mils, but still, it takes a long time. You can do the same thing for profiling functions, if functions are being slow, so you may, this is calling the same function over and over and over again, which it probably shouldn't be doing in this particular scenario, but again, it depends on your actual code as to whether or not this is the right thing for it to do. Sometimes that is normal behavior, in this case, it's definitely not normal behavior. So, the TLDR is you should probably use runtime trace next time and not CPU profiles. So, for me, I've sped up requests that were taking 3.6 seconds to 96 milliseconds for the same request, and they're bottlenecks from various different things, so from garbage collection to poor database queries and poor computational complexity on certain algorithms, and some of these things will only be visible if you use runtime trace. So, flame graphs don't help you for debugging slow SQL queries, but runtime trace will do. Thank you very much. Thank you. If this GitHub repo is yours, come to the stage. And you've got 10 seconds to switch laptops. 10. No. And it works, which is a miracle for Linux. Hi. I actually didn't create a slide, and this will be the fastest lightning talk in my life. Basically, I just wanted to talk about the JSON package and the issue what we faced with, and a lot of people faced with it. Basically, it's the... Have you ever used struct with omitempty? Then, basically, this is where the issue come in, and that is an open issue here, which trying to fix this, but it's basically abandoned, and it's a pretty big issue because it's created in 2015, and there is nearly 200 comments under that. And basically, I just wanted to make an attention on this ticket, because if someone fixing this ticket, that means that, basically, you can do something like what I show you in this code. So it's really hard with point. Yeah. Probably use this package, the encoding JSON. I have a struct here, which is here. Thank you. Thank you. So this is basically, I introduced a new struct, which is basically a new string, or something like that, and here I added omitempty. In this case, I implemented the E0 method here, which says if it's not valid, then it's basically a 0, so I wanted to remove it from the JSON. But if I run the actual code, please run it. Live demo is in a lightning talk. You're brave. Yes, live coding. You see that it's basically here inside the JSON, however, I wanted to basically an empty JSON. And there is another implementation with exactly the same code, but I just created a pumpkin seed JSON, which is exactly the copy of the built-in JSON. The only difference here that the issue what I mentioned is basically suggesting an implementation that the omitempty section of the built-in JSON should check for the E0 method, whether it's existing in the struct or not. And if I run this one, it's basically doing what it should do. And basically that's it. So this is something what I think should be implemented in Go and this ticket with this number is basically showing actual implementations for that. Right now, most of them are not declined but not processed. So I think if anyone has a good idea how to implement it in Go, then basically it would be nice to put into this ticket. There are also, this is the actual change request in the code language what the guy made and I just copied his code. Yeah. One disclaimer, the pumpkin seed JSON package, you shouldn't use in production. And that's it. Thank you. If this is your slide, come to the stage. All right. Hello. My name is Michiel. I created Mox. I've been working on this for quite some time. I started using it two weeks ago, released it earlier this week. It's a meal server. So I'm curious, is anyone here running their own meal servers around the main? One, two persons? Wow. Okay. Three, room for improvement. So let's go right ahead. This is the tagline. It's a modern, full-featured open source secure meal server for low maintenance self-hosted email. So let's break it down. It's modern because it supports all the latest meal standards and there have been added quite a few over the years. It is full-featured in the sense that it aims to do everything at once, meaning all the relevant email standards. So you just need this one thing. You don't need a whole bunch of components to make a working system. So just really to make it easier. It's MIT licensed. It is secure, meaning it supports all the latest security things about email like TLS, et cetera. And of course, a bit of secure coding and low maintenance. So you actually started using it because I hear many people are moving all their email to the cloud, some big providers because it's too hard apparently to run a meal server. So it's for your self-hosted email. Email is one of the oldest decentralized messaging protocols, but we're making it more centralized by moving everything to the few big providers. So Mox is an attempt to make it so easy that you will all start using it. So a bunch of features, a list of acronyms. IMAP, so you can access your mail, SNTP, so you can send mail. Nowadays, if you want to send mail, you need to configure SPF, DKIM, DMARC. Does anyone know what that means? Yeah, see, that's good. Automatic TLS, so you don't have to worry about any certificate stuff. So it's like the caddy for email. TLS reporting, MTA, STS, that's one of the latest additions to secure email. There's a reputation-based junk filter in there, so if you receive messages from people and you don't like those messages and you mark them as junk, the next time those people send mail, it's rejected. So new senders don't have any reputation. You can look at the content, so there's a content-based abyeasing spam filter, so in there. Internationalized email, so you can have smileys in your domain names, that's what you want. And auto-configuration, so you get your thunderbird, and setup is just instant. No need to worry about all the port numbers, et cetera. It just works. So getting started, of course, now you're all convinced you want to use this. Luckily, there's a quick start. You just set up a Linux machine, probably, get your email address for your domain, and you get a configuration file that's all, that has this all configured. You just can start it right after. Not only does it make a configuration file, also print some commands and all the DNS records that you need to create, so you don't have to think. You can just copy, paste, and be happy. Then the code, 40K lines of implementation, 10K lines of tests, quite some test coverage. There's integration tests, fuzzing tests. It's all pure Go, no C Go, just go install, cross compile, all the good stuff that you get from Go. The implementation is heavily cross-referenced with the RFCs, so both ways. You can go from code to the RFC and back from the RFC to the places in the code where it's used. So this is supposed to help with maintenance, so it's implementing all these protocols, and it gets a bit overwhelming to understand all of that. So if you would code it once, you cannot go back to the specification and back to the implementation. You don't know what's going to, so how you, how to fix bugs, et cetera. Let's move. Oh, wow, quick. So what's next? I just released it. I'm looking for feedback. Please use it and tell me if it works for you or why it does not work for you. So I aim to make it very simple, so if you find something that's not simple, let me know. Of course, if you find bugs, let me know. And this is where you can find it. All right. Thank you. If this is your slide deck, you can come to the stage. If this is nobody's slide deck, I'll just skip it. Something with Postgres. If this is your 404 page which you sent to me, please also come to talk to me. So yeah, also the speaker is not found. That's the thing with last minute talks. Then I had one backup speaker. You can come to the stage. And the gophers are also falling down. They are tired. Understands me too, me too. Yes, I have HDMI. I also use USB-C. Let me just close this down for you. That's 4G clicker. Okay. So thank you, first of all. So I want to tell a go-of-story and why we use Go to have to implement this idea of fluid pull requests. Before starting with that, I need to talk a little bit about pull requests. So for that, I brought Robin and Kat with me. So Robin wants to contribute to a project that Kat is a maintainer. And what everyone does or at least they try to, they open a branch, they create what they have to do. And then at the end, it comes a time when it needs to merge into main. And then when Kat comes in and says, wait a minute, we need to review those changes. So this kind of methodology is important for critical contributions from interested parties. And it's well-known as open source projects, especially with the name of pull requests. We also use it inside our own companies. But it's well-known at the open source community. And it's quite popular. As you can see, in 2021, we got a lot of pull requests. And the process goes like you do whatever you want to do. Then the CI triggers, you get the review, you get some feedback, and then you have to apply the feedback. And we enter a loop here until someone decides that it's good to go and we get our approval. Then it goes to merge and that everyone is happy. And the problem here is that Robin goes through this process every time, regardless of the type of change it is. And we are unavailable with the fact that Robin and Kat have been contributing and working with each other for some time. So this idea that all pull requests are the same can be actually improved. For instance, this scenario where Robin is just trying to do some configuration change, why do we need a pull request? Maybe we can just go directly to main without a review. Another scenario where Robin just gets an API with some documentation or some warnings. Let's imagine why can it go to main and then we can do a review afterwards. And then when it comes to critical changes, then when we want to stop the process and say, okay, this is critical, we need to have a very good review here. And maybe instead of just asking one guy, we can ask two people for them to get their own approval. So this idea of pull requests is that all that I just said could be defined in rules. And we can apply those rules into our own process and minimize the time. That's where we came with the review pad, which is done on go and it's full open source. And that's where we can define all these ideas of what are the rules for our team. So here's how we could work with this terminology. Behind this is go, of course, then it can, for instance, if my changes are all on markdown files, I want to merge my pull request right away. So no review. If, for instance, my author actually is considered a new joiner, a new joiner could be someone that didn't do 10 PRs, like Spotify does, I want to assign a reviewer from my tech leads. And then, for instance, if I want to get some compliance, make sure that my pull request is an issue. I can confirm that and make sure that the user gets notified as soon as possible in order to iterate on that. And then we can do some more incredible things. I want you to look at the line at the top where we have an annotation saying that it's critical, saying that every time someone changes that function, that function is critical. If the function is critical, if my code touches a function that has this annotation, then I want to trigger my pull request review that is for critical changes, like I want to assign a label, I want to send someone from the tech list to review it, and I want to notify join, which is the tech architecture. Okay, we had a talk this morning about reducing cognitive load from Federic, and I want to show how we could do that with this terminology. So here's how we could look into line of sign and make sure that if someone uses a lot of tabs, so it means that we have a lot of loops between each other, if and else, we can actually send a warning to the user. For instance, our error validation, making sure that they don't use string contains for errors or equals, but they use error is. And last one, the mysterious Boolean, making sure that no more than one Boolean is used in the function signature, that's pretty much it, how we could use to make our lives easier on pull requests. Thank you all. Thank you. The last lightning talk of the day is from me again. What do we want to talk about today? Well, two subjects, what is naming God? No, I want to talk to you first of all a big thank you again to everyone. First of all, to all speakers who came here today to give an amazing talk, standing with a lot of stress to say things. I also want to thank Eva again for helping me out. I also want to thank the two FOSDEM engineers in the back who made our audio video work all day. I want to thank the people from FOSDEM who brought me food today. I also want to thank everybody at FOSDEM. And I also want to thank all the volunteers. I think they are left right now. Who helped us with video, even what they couldn't solve today. Thank you very much. Thank you all for coming, by the way. Thank you for staying so late. Thank you. And now my second subject. Which is that Go is a garbage collected language. And you know you can trigger the garbage collection by doing runtime.gc. So when the time is 19 o'clock, I want you all to do runtime.gc and grab some waste you see around it and put it in any of our bins. But I think Eva wants to say something. Yes. Thank you. Thank everyone that has been here to help you. But without you this wasn't possible. So a big thank you to Marcia. And thank you for coming. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.52, "text": " Hello everybody, I will talk about Go Evil, which is a project, a personal project, which", "tokens": [2425, 2201, 11, 286, 486, 751, 466, 1037, 20528, 11, 597, 307, 257, 1716, 11, 257, 2973, 1716, 11, 597], "temperature": 0.0, "avg_logprob": -0.38880464982013313, "compression_ratio": 1.359375, "no_speech_prob": 0.18752996623516083}, {"id": 1, "seek": 0, "start": 15.52, "end": 27.88, "text": " allows you to do one-liners in Go, so you just type your Go code and call it with Go", "tokens": [4045, 291, 281, 360, 472, 12, 5045, 433, 294, 1037, 11, 370, 291, 445, 2010, 428, 1037, 3089, 293, 818, 309, 365, 1037], "temperature": 0.0, "avg_logprob": -0.38880464982013313, "compression_ratio": 1.359375, "no_speech_prob": 0.18752996623516083}, {"id": 2, "seek": 2788, "start": 27.88, "end": 38.92, "text": " Evil, and you can simply write a name or word from the command line. So this is like magic.", "tokens": [20528, 11, 293, 291, 393, 2935, 2464, 257, 1315, 420, 1349, 490, 264, 5622, 1622, 13, 407, 341, 307, 411, 5585, 13], "temperature": 0.0, "avg_logprob": -0.2811078651278627, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.001242484082467854}, {"id": 3, "seek": 2788, "start": 38.92, "end": 51.16, "text": " I will show you a bit under the hood how it works. The word project is about 300 lines of code,", "tokens": [286, 486, 855, 291, 257, 857, 833, 264, 13376, 577, 309, 1985, 13, 440, 1349, 1716, 307, 466, 6641, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.2811078651278627, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.001242484082467854}, {"id": 4, "seek": 5116, "start": 51.16, "end": 64.12, "text": " not more. Here is an example, you can call Go Evil and tell it to take the code from the", "tokens": [406, 544, 13, 1692, 307, 364, 1365, 11, 291, 393, 818, 1037, 20528, 293, 980, 309, 281, 747, 264, 3089, 490, 264], "temperature": 0.0, "avg_logprob": -0.3078926893380972, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.001833018148317933}, {"id": 5, "seek": 6412, "start": 64.12, "end": 79.04, "text": " STD in, but here is how it works under the hood. From your Go code, from the command line, Go Evil", "tokens": [4904, 35, 294, 11, 457, 510, 307, 577, 309, 1985, 833, 264, 13376, 13, 3358, 428, 1037, 3089, 11, 490, 264, 5622, 1622, 11, 1037, 20528], "temperature": 0.0, "avg_logprob": -0.25573336161099947, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.002477404195815325}, {"id": 6, "seek": 6412, "start": 79.04, "end": 92.72, "text": " generates a full Go program, and so the dash E allows to print that Go program that has been", "tokens": [23815, 257, 1577, 1037, 1461, 11, 293, 370, 264, 8240, 462, 4045, 281, 4482, 300, 1037, 1461, 300, 575, 668], "temperature": 0.0, "avg_logprob": -0.25573336161099947, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.002477404195815325}, {"id": 7, "seek": 9272, "start": 92.72, "end": 101.03999999999999, "text": " generated. It is sometimes useful when you want to debug the syntax around that you make, and then", "tokens": [10833, 13, 467, 307, 2171, 4420, 562, 291, 528, 281, 24083, 264, 28431, 926, 300, 291, 652, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.25915386365807574, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0009079452720470726}, {"id": 8, "seek": 9272, "start": 101.03999999999999, "end": 112.2, "text": " the Go import equal allows to stop using Go import, because here you see that in that code,", "tokens": [264, 1037, 974, 2681, 4045, 281, 1590, 1228, 1037, 974, 11, 570, 510, 291, 536, 300, 294, 300, 3089, 11], "temperature": 0.0, "avg_logprob": -0.25915386365807574, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0009079452720470726}, {"id": 9, "seek": 11220, "start": 112.2, "end": 121.96000000000001, "text": " there is no import of the FMT package, but it is introduced by the Go imports, which is called", "tokens": [456, 307, 572, 974, 295, 264, 29614, 51, 7372, 11, 457, 309, 307, 7268, 538, 264, 1037, 41596, 11, 597, 307, 1219], "temperature": 0.0, "avg_logprob": -0.17033872237572303, "compression_ratio": 1.364963503649635, "no_speech_prob": 0.0013113470049574971}, {"id": 10, "seek": 11220, "start": 121.96000000000001, "end": 133.04, "text": " by Go Evil. So I am announcing today that Go Evil has been, 1.0 has been released just a few", "tokens": [538, 1037, 20528, 13, 407, 286, 669, 28706, 965, 300, 1037, 20528, 575, 668, 11, 502, 13, 15, 575, 668, 4736, 445, 257, 1326], "temperature": 0.0, "avg_logprob": -0.17033872237572303, "compression_ratio": 1.364963503649635, "no_speech_prob": 0.0013113470049574971}, {"id": 11, "seek": 13304, "start": 133.04, "end": 143.95999999999998, "text": " hours ago, and the new feature of Go Evil 1 is that Go modules are supported, and with Go module,", "tokens": [2496, 2057, 11, 293, 264, 777, 4111, 295, 1037, 20528, 502, 307, 300, 1037, 16679, 366, 8104, 11, 293, 365, 1037, 10088, 11], "temperature": 0.0, "avg_logprob": -0.15364797910054526, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.0007209363975562155}, {"id": 12, "seek": 13304, "start": 143.95999999999998, "end": 159.28, "text": " you get locked versions for your dependency code from Go Evil. So this allows to submit to share", "tokens": [291, 483, 9376, 9606, 337, 428, 33621, 3089, 490, 1037, 20528, 13, 407, 341, 4045, 281, 10315, 281, 2073], "temperature": 0.0, "avg_logprob": -0.15364797910054526, "compression_ratio": 1.375886524822695, "no_speech_prob": 0.0007209363975562155}, {"id": 13, "seek": 15928, "start": 159.28, "end": 167.88, "text": " your one-liners with other people, because the previous code that I showed was depending on", "tokens": [428, 472, 12, 5045, 433, 365, 661, 561, 11, 570, 264, 3894, 3089, 300, 286, 4712, 390, 5413, 322], "temperature": 0.0, "avg_logprob": -0.21986681101273517, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.001555306022055447}, {"id": 14, "seek": 15928, "start": 167.88, "end": 178.48, "text": " the dependency to be installed in Go pass. And so that's it. Try it, use it, report bugs,", "tokens": [264, 33621, 281, 312, 8899, 294, 1037, 1320, 13, 400, 370, 300, 311, 309, 13, 6526, 309, 11, 764, 309, 11, 2275, 15120, 11], "temperature": 0.0, "avg_logprob": -0.21986681101273517, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.001555306022055447}, {"id": 15, "seek": 17848, "start": 178.48, "end": 192.76, "text": " and I'm available for question later. Thank you. It's weirdly enough, not the first open source", "tokens": [293, 286, 478, 2435, 337, 1168, 1780, 13, 1044, 291, 13, 467, 311, 48931, 1547, 11, 406, 264, 700, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.2316128412882487, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0020174128003418446}, {"id": 16, "seek": 17848, "start": 192.76, "end": 198.95999999999998, "text": " project to be released when people are in the dev room. If this is your slide, you can come up now.", "tokens": [1716, 281, 312, 4736, 562, 561, 366, 294, 264, 1905, 1808, 13, 759, 341, 307, 428, 4137, 11, 291, 393, 808, 493, 586, 13], "temperature": 0.0, "avg_logprob": -0.2316128412882487, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0020174128003418446}, {"id": 17, "seek": 19896, "start": 198.96, "end": 213.0, "text": " Hello, everyone. My name is Keegan. I'm a staff software engineer element, and I've been spending", "tokens": [2425, 11, 1518, 13, 1222, 1315, 307, 3189, 43118, 13, 286, 478, 257, 3525, 4722, 11403, 4478, 11, 293, 286, 600, 668, 6434], "temperature": 0.0, "avg_logprob": -0.21175328572591146, "compression_ratio": 1.4195121951219511, "no_speech_prob": 0.00041275742114521563}, {"id": 18, "seek": 19896, "start": 213.0, "end": 220.76000000000002, "text": " the past year debugging why Go servers are slow. So hands up, who's made a crud application before?", "tokens": [264, 1791, 1064, 45592, 983, 1037, 15909, 366, 2964, 13, 407, 2377, 493, 11, 567, 311, 1027, 257, 941, 532, 3861, 949, 30], "temperature": 0.0, "avg_logprob": -0.21175328572591146, "compression_ratio": 1.4195121951219511, "no_speech_prob": 0.00041275742114521563}, {"id": 19, "seek": 19896, "start": 220.76000000000002, "end": 224.16, "text": " Create read, update, delete. That's basically everyone in this room, which is what I thought.", "tokens": [20248, 1401, 11, 5623, 11, 12097, 13, 663, 311, 1936, 1518, 294, 341, 1808, 11, 597, 307, 437, 286, 1194, 13], "temperature": 0.0, "avg_logprob": -0.21175328572591146, "compression_ratio": 1.4195121951219511, "no_speech_prob": 0.00041275742114521563}, {"id": 20, "seek": 22416, "start": 224.16, "end": 231.16, "text": " Who's tried to speed up their server before? This is a slow request, 3.6 seconds. Fewer people,", "tokens": [2102, 311, 3031, 281, 3073, 493, 641, 7154, 949, 30, 639, 307, 257, 2964, 5308, 11, 805, 13, 21, 3949, 13, 33468, 260, 561, 11], "temperature": 0.0, "avg_logprob": -0.21555442453544832, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0004359031154308468}, {"id": 21, "seek": 22416, "start": 231.16, "end": 237.51999999999998, "text": " but still a fair number of people. Cool. Who's used PPROF before? So flame graphs. It's great.", "tokens": [457, 920, 257, 3143, 1230, 295, 561, 13, 8561, 13, 2102, 311, 1143, 37369, 7142, 37, 949, 30, 407, 13287, 24877, 13, 467, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.21555442453544832, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0004359031154308468}, {"id": 22, "seek": 22416, "start": 237.51999999999998, "end": 246.96, "text": " Who's used runtime trace before? Not that many people. Okay. Who's struggled to figure out what", "tokens": [2102, 311, 1143, 34474, 13508, 949, 30, 1726, 300, 867, 561, 13, 1033, 13, 2102, 311, 19023, 281, 2573, 484, 437], "temperature": 0.0, "avg_logprob": -0.21555442453544832, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0004359031154308468}, {"id": 23, "seek": 22416, "start": 246.96, "end": 254.07999999999998, "text": " was going on when you're using this? Right. Okay. Great. This talk is for you. So the first thing", "tokens": [390, 516, 322, 562, 291, 434, 1228, 341, 30, 1779, 13, 1033, 13, 3769, 13, 639, 751, 307, 337, 291, 13, 407, 264, 700, 551], "temperature": 0.0, "avg_logprob": -0.21555442453544832, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0004359031154308468}, {"id": 24, "seek": 25408, "start": 254.08, "end": 260.24, "text": " you need to really is use spans to make these traces readable. Very easy. If you've ever used", "tokens": [291, 643, 281, 534, 307, 764, 44086, 281, 652, 613, 26076, 49857, 13, 4372, 1858, 13, 759, 291, 600, 1562, 1143], "temperature": 0.0, "avg_logprob": -0.15500703894573709, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.00040138568147085607}, {"id": 25, "seek": 25408, "start": 260.24, "end": 264.44, "text": " Jager spans before, they're basically the same sort of thing. So you can create a new task,", "tokens": [508, 3557, 44086, 949, 11, 436, 434, 1936, 264, 912, 1333, 295, 551, 13, 407, 291, 393, 1884, 257, 777, 5633, 11], "temperature": 0.0, "avg_logprob": -0.15500703894573709, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.00040138568147085607}, {"id": 26, "seek": 25408, "start": 264.44, "end": 268.72, "text": " and then you get a new context. You pass the context through to new functions. You can create", "tokens": [293, 550, 291, 483, 257, 777, 4319, 13, 509, 1320, 264, 4319, 807, 281, 777, 6828, 13, 509, 393, 1884], "temperature": 0.0, "avg_logprob": -0.15500703894573709, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.00040138568147085607}, {"id": 27, "seek": 25408, "start": 268.72, "end": 272.04, "text": " regions from those, and you end up getting something that looks a bit like the stuff on the", "tokens": [10682, 490, 729, 11, 293, 291, 917, 493, 1242, 746, 300, 1542, 257, 857, 411, 264, 1507, 322, 264], "temperature": 0.0, "avg_logprob": -0.15500703894573709, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.00040138568147085607}, {"id": 28, "seek": 25408, "start": 272.04, "end": 279.36, "text": " bottom there. You can also add log lines for some contextual information. That'll appear on the UI,", "tokens": [2767, 456, 13, 509, 393, 611, 909, 3565, 3876, 337, 512, 35526, 1589, 13, 663, 603, 4204, 322, 264, 15682, 11], "temperature": 0.0, "avg_logprob": -0.15500703894573709, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.00040138568147085607}, {"id": 29, "seek": 27936, "start": 279.36, "end": 285.92, "text": " which we'll get to in a moment. And the crash course in using runtime trace is you make a trace", "tokens": [597, 321, 603, 483, 281, 294, 257, 1623, 13, 400, 264, 8252, 1164, 294, 1228, 34474, 13508, 307, 291, 652, 257, 13508], "temperature": 0.0, "avg_logprob": -0.12571106851100922, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.00014965275477152318}, {"id": 30, "seek": 27936, "start": 285.92, "end": 290.52000000000004, "text": " in the same way that you'd make a CPU profile with PPROF, except you hit a different endpoint,", "tokens": [294, 264, 912, 636, 300, 291, 1116, 652, 257, 13199, 7964, 365, 37369, 7142, 37, 11, 3993, 291, 2045, 257, 819, 35795, 11], "temperature": 0.0, "avg_logprob": -0.12571106851100922, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.00014965275477152318}, {"id": 31, "seek": 27936, "start": 290.52000000000004, "end": 295.92, "text": " but you also tell it how long you want to trace for, and then you use gotool trace to open that", "tokens": [457, 291, 611, 980, 309, 577, 938, 291, 528, 281, 13508, 337, 11, 293, 550, 291, 764, 658, 1092, 13508, 281, 1269, 300], "temperature": 0.0, "avg_logprob": -0.12571106851100922, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.00014965275477152318}, {"id": 32, "seek": 27936, "start": 295.92, "end": 301.28000000000003, "text": " trace. You don't use the gotool PPROF, confusingly, and you'll get something like the bottom over", "tokens": [13508, 13, 509, 500, 380, 764, 264, 658, 1092, 37369, 7142, 37, 11, 13181, 356, 11, 293, 291, 603, 483, 746, 411, 264, 2767, 670], "temperature": 0.0, "avg_logprob": -0.12571106851100922, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.00014965275477152318}, {"id": 33, "seek": 27936, "start": 301.28000000000003, "end": 307.0, "text": " here, which is quite a lot of scary words and links, and you have no idea which thing to click.", "tokens": [510, 11, 597, 307, 1596, 257, 688, 295, 6958, 2283, 293, 6123, 11, 293, 291, 362, 572, 1558, 597, 551, 281, 2052, 13], "temperature": 0.0, "avg_logprob": -0.12571106851100922, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.00014965275477152318}, {"id": 34, "seek": 30700, "start": 307.0, "end": 312.72, "text": " The only thing you care about is the user-defined tasks. If you click on that, you'll see something", "tokens": [440, 787, 551, 291, 1127, 466, 307, 264, 4195, 12, 37716, 9608, 13, 759, 291, 2052, 322, 300, 11, 291, 603, 536, 746], "temperature": 0.0, "avg_logprob": -0.12467537241533769, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.000317721685860306}, {"id": 35, "seek": 30700, "start": 312.72, "end": 317.28, "text": " a bit like this. The only thing you care about is this GoRoutine view, and if you click on that,", "tokens": [257, 857, 411, 341, 13, 440, 787, 551, 291, 1127, 466, 307, 341, 1037, 49, 45075, 1910, 11, 293, 498, 291, 2052, 322, 300, 11], "temperature": 0.0, "avg_logprob": -0.12467537241533769, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.000317721685860306}, {"id": 36, "seek": 30700, "start": 317.28, "end": 321.92, "text": " you can profile basically everything. So, for example, here's a bit of a request,", "tokens": [291, 393, 7964, 1936, 1203, 13, 407, 11, 337, 1365, 11, 510, 311, 257, 857, 295, 257, 5308, 11], "temperature": 0.0, "avg_logprob": -0.12467537241533769, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.000317721685860306}, {"id": 37, "seek": 30700, "start": 321.92, "end": 326.24, "text": " which is slow because of garbage collection, and if you click on any one of those Gs at the", "tokens": [597, 307, 2964, 570, 295, 14150, 5765, 11, 293, 498, 291, 2052, 322, 604, 472, 295, 729, 460, 82, 412, 264], "temperature": 0.0, "avg_logprob": -0.12467537241533769, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.000317721685860306}, {"id": 38, "seek": 30700, "start": 326.24, "end": 331.6, "text": " bottom, which are highlighted with the red circle, you'll see stack traces that mention GC. Also,", "tokens": [2767, 11, 597, 366, 17173, 365, 264, 2182, 6329, 11, 291, 603, 536, 8630, 26076, 300, 2152, 29435, 13, 2743, 11], "temperature": 0.0, "avg_logprob": -0.12467537241533769, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.000317721685860306}, {"id": 39, "seek": 33160, "start": 331.6, "end": 338.44, "text": " the blue bar in the middle there says GC, so spoiler. Other thing, if you have slow SQL queries,", "tokens": [264, 3344, 2159, 294, 264, 2808, 456, 1619, 29435, 11, 370, 26927, 13, 5358, 551, 11, 498, 291, 362, 2964, 19200, 24109, 11], "temperature": 0.0, "avg_logprob": -0.14194809221753887, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.00020550593035295606}, {"id": 40, "seek": 33160, "start": 338.44, "end": 344.64000000000004, "text": " you can find that as well because if you click on any of these things, you'll see stack traces,", "tokens": [291, 393, 915, 300, 382, 731, 570, 498, 291, 2052, 322, 604, 295, 613, 721, 11, 291, 603, 536, 8630, 26076, 11], "temperature": 0.0, "avg_logprob": -0.14194809221753887, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.00020550593035295606}, {"id": 41, "seek": 33160, "start": 344.64000000000004, "end": 352.6, "text": " and those stack traces refer to any point where the GoRoutine yields away for network IO or", "tokens": [293, 729, 8630, 26076, 2864, 281, 604, 935, 689, 264, 1037, 49, 45075, 32168, 1314, 337, 3209, 39839, 420], "temperature": 0.0, "avg_logprob": -0.14194809221753887, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.00020550593035295606}, {"id": 42, "seek": 33160, "start": 352.6, "end": 357.28000000000003, "text": " syscalls or things like that. So, you can clearly see, oh, it's doing something with SQL, and it's", "tokens": [262, 749, 66, 39655, 420, 721, 411, 300, 13, 407, 11, 291, 393, 4448, 536, 11, 1954, 11, 309, 311, 884, 746, 365, 19200, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.14194809221753887, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.00020550593035295606}, {"id": 43, "seek": 35728, "start": 357.28, "end": 362.84, "text": " just doing the same thing for SQL for not particularly long here, only 20 mils, but still,", "tokens": [445, 884, 264, 912, 551, 337, 19200, 337, 406, 4098, 938, 510, 11, 787, 945, 1962, 82, 11, 457, 920, 11], "temperature": 0.0, "avg_logprob": -0.14035846068795804, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.00011305260704830289}, {"id": 44, "seek": 35728, "start": 362.84, "end": 369.2, "text": " it takes a long time. You can do the same thing for profiling functions, if functions are being", "tokens": [309, 2516, 257, 938, 565, 13, 509, 393, 360, 264, 912, 551, 337, 1740, 4883, 6828, 11, 498, 6828, 366, 885], "temperature": 0.0, "avg_logprob": -0.14035846068795804, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.00011305260704830289}, {"id": 45, "seek": 35728, "start": 369.2, "end": 374.59999999999997, "text": " slow, so you may, this is calling the same function over and over and over again, which it", "tokens": [2964, 11, 370, 291, 815, 11, 341, 307, 5141, 264, 912, 2445, 670, 293, 670, 293, 670, 797, 11, 597, 309], "temperature": 0.0, "avg_logprob": -0.14035846068795804, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.00011305260704830289}, {"id": 46, "seek": 35728, "start": 374.59999999999997, "end": 380.44, "text": " probably shouldn't be doing in this particular scenario, but again, it depends on your actual", "tokens": [1391, 4659, 380, 312, 884, 294, 341, 1729, 9005, 11, 457, 797, 11, 309, 5946, 322, 428, 3539], "temperature": 0.0, "avg_logprob": -0.14035846068795804, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.00011305260704830289}, {"id": 47, "seek": 35728, "start": 380.44, "end": 384.59999999999997, "text": " code as to whether or not this is the right thing for it to do. Sometimes that is normal", "tokens": [3089, 382, 281, 1968, 420, 406, 341, 307, 264, 558, 551, 337, 309, 281, 360, 13, 4803, 300, 307, 2710], "temperature": 0.0, "avg_logprob": -0.14035846068795804, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.00011305260704830289}, {"id": 48, "seek": 38460, "start": 384.6, "end": 389.48, "text": " behavior, in this case, it's definitely not normal behavior. So, the TLDR is you should", "tokens": [5223, 11, 294, 341, 1389, 11, 309, 311, 2138, 406, 2710, 5223, 13, 407, 11, 264, 40277, 9301, 307, 291, 820], "temperature": 0.0, "avg_logprob": -0.13857413700648716, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00031123391818255186}, {"id": 49, "seek": 38460, "start": 389.48, "end": 396.96000000000004, "text": " probably use runtime trace next time and not CPU profiles. So, for me, I've sped up requests", "tokens": [1391, 764, 34474, 13508, 958, 565, 293, 406, 13199, 23693, 13, 407, 11, 337, 385, 11, 286, 600, 637, 292, 493, 12475], "temperature": 0.0, "avg_logprob": -0.13857413700648716, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00031123391818255186}, {"id": 50, "seek": 38460, "start": 396.96000000000004, "end": 402.12, "text": " that were taking 3.6 seconds to 96 milliseconds for the same request, and they're bottlenecks", "tokens": [300, 645, 1940, 805, 13, 21, 3949, 281, 24124, 34184, 337, 264, 912, 5308, 11, 293, 436, 434, 44641, 2761], "temperature": 0.0, "avg_logprob": -0.13857413700648716, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00031123391818255186}, {"id": 51, "seek": 38460, "start": 402.12, "end": 407.28000000000003, "text": " from various different things, so from garbage collection to poor database queries and poor", "tokens": [490, 3683, 819, 721, 11, 370, 490, 14150, 5765, 281, 4716, 8149, 24109, 293, 4716], "temperature": 0.0, "avg_logprob": -0.13857413700648716, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00031123391818255186}, {"id": 52, "seek": 38460, "start": 407.28000000000003, "end": 411.48, "text": " computational complexity on certain algorithms, and some of these things will only be visible", "tokens": [28270, 14024, 322, 1629, 14642, 11, 293, 512, 295, 613, 721, 486, 787, 312, 8974], "temperature": 0.0, "avg_logprob": -0.13857413700648716, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00031123391818255186}, {"id": 53, "seek": 41148, "start": 411.48, "end": 416.88, "text": " if you use runtime trace. So, flame graphs don't help you for debugging slow SQL queries,", "tokens": [498, 291, 764, 34474, 13508, 13, 407, 11, 13287, 24877, 500, 380, 854, 291, 337, 45592, 2964, 19200, 24109, 11], "temperature": 0.0, "avg_logprob": -0.24109816942058626, "compression_ratio": 1.3865030674846626, "no_speech_prob": 0.00025124105741269886}, {"id": 54, "seek": 41148, "start": 416.88, "end": 428.04, "text": " but runtime trace will do. Thank you very much.", "tokens": [457, 34474, 13508, 486, 360, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.24109816942058626, "compression_ratio": 1.3865030674846626, "no_speech_prob": 0.00025124105741269886}, {"id": 55, "seek": 41148, "start": 428.04, "end": 440.20000000000005, "text": " Thank you. If this GitHub repo is yours, come to the stage. And you've got 10 seconds to", "tokens": [1044, 291, 13, 759, 341, 23331, 49040, 307, 6342, 11, 808, 281, 264, 3233, 13, 400, 291, 600, 658, 1266, 3949, 281], "temperature": 0.0, "avg_logprob": -0.24109816942058626, "compression_ratio": 1.3865030674846626, "no_speech_prob": 0.00025124105741269886}, {"id": 56, "seek": 44020, "start": 440.2, "end": 457.64, "text": " switch laptops. 10. No. And it works, which is a miracle for Linux.", "tokens": [3679, 27642, 13, 1266, 13, 883, 13, 400, 309, 1985, 11, 597, 307, 257, 14660, 337, 18734, 13], "temperature": 0.0, "avg_logprob": -0.42048961466008966, "compression_ratio": 0.9178082191780822, "no_speech_prob": 0.0020363989751785994}, {"id": 57, "seek": 45764, "start": 457.64, "end": 479.8, "text": " Hi. I actually didn't create a slide, and this will be the fastest lightning talk in", "tokens": [2421, 13, 286, 767, 994, 380, 1884, 257, 4137, 11, 293, 341, 486, 312, 264, 14573, 16589, 751, 294], "temperature": 0.0, "avg_logprob": -0.23998513332633084, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.0025468021631240845}, {"id": 58, "seek": 45764, "start": 479.8, "end": 485.76, "text": " my life. Basically, I just wanted to talk about the JSON package and the issue what", "tokens": [452, 993, 13, 8537, 11, 286, 445, 1415, 281, 751, 466, 264, 31828, 7372, 293, 264, 2734, 437], "temperature": 0.0, "avg_logprob": -0.23998513332633084, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.0025468021631240845}, {"id": 59, "seek": 48576, "start": 485.76, "end": 498.28, "text": " we faced with, and a lot of people faced with it. Basically, it's the... Have you ever used", "tokens": [321, 11446, 365, 11, 293, 257, 688, 295, 561, 11446, 365, 309, 13, 8537, 11, 309, 311, 264, 485, 3560, 291, 1562, 1143], "temperature": 0.0, "avg_logprob": -0.22762577350323016, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.0009561433107592165}, {"id": 60, "seek": 48576, "start": 498.28, "end": 506.4, "text": " struct with omitempty? Then, basically, this is where the issue come in, and that is an", "tokens": [6594, 365, 3406, 270, 4543, 88, 30, 1396, 11, 1936, 11, 341, 307, 689, 264, 2734, 808, 294, 11, 293, 300, 307, 364], "temperature": 0.0, "avg_logprob": -0.22762577350323016, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.0009561433107592165}, {"id": 61, "seek": 50640, "start": 506.4, "end": 515.4, "text": " open issue here, which trying to fix this, but it's basically abandoned, and it's a pretty", "tokens": [1269, 2734, 510, 11, 597, 1382, 281, 3191, 341, 11, 457, 309, 311, 1936, 13732, 11, 293, 309, 311, 257, 1238], "temperature": 0.0, "avg_logprob": -0.17870613978459285, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0014936210354790092}, {"id": 62, "seek": 50640, "start": 515.4, "end": 526.84, "text": " big issue because it's created in 2015, and there is nearly 200 comments under that. And", "tokens": [955, 2734, 570, 309, 311, 2942, 294, 7546, 11, 293, 456, 307, 6217, 2331, 3053, 833, 300, 13, 400], "temperature": 0.0, "avg_logprob": -0.17870613978459285, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0014936210354790092}, {"id": 63, "seek": 50640, "start": 526.84, "end": 531.64, "text": " basically, I just wanted to make an attention on this ticket, because if someone fixing", "tokens": [1936, 11, 286, 445, 1415, 281, 652, 364, 3202, 322, 341, 10550, 11, 570, 498, 1580, 19442], "temperature": 0.0, "avg_logprob": -0.17870613978459285, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0014936210354790092}, {"id": 64, "seek": 53164, "start": 531.64, "end": 539.12, "text": " this ticket, that means that, basically, you can do something like what I show you in this", "tokens": [341, 10550, 11, 300, 1355, 300, 11, 1936, 11, 291, 393, 360, 746, 411, 437, 286, 855, 291, 294, 341], "temperature": 0.0, "avg_logprob": -0.3166588113663044, "compression_ratio": 1.2949640287769784, "no_speech_prob": 0.0010771924862638116}, {"id": 65, "seek": 53164, "start": 539.12, "end": 559.6, "text": " code. So it's really hard with point. Yeah. Probably use this package, the encoding JSON.", "tokens": [3089, 13, 407, 309, 311, 534, 1152, 365, 935, 13, 865, 13, 9210, 764, 341, 7372, 11, 264, 43430, 31828, 13], "temperature": 0.0, "avg_logprob": -0.3166588113663044, "compression_ratio": 1.2949640287769784, "no_speech_prob": 0.0010771924862638116}, {"id": 66, "seek": 55960, "start": 559.6, "end": 570.6, "text": " I have a struct here, which is here. Thank you. Thank you. So this is basically, I introduced", "tokens": [286, 362, 257, 6594, 510, 11, 597, 307, 510, 13, 1044, 291, 13, 1044, 291, 13, 407, 341, 307, 1936, 11, 286, 7268], "temperature": 0.0, "avg_logprob": -0.22020064728169503, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.001140480162575841}, {"id": 67, "seek": 55960, "start": 570.6, "end": 577.9200000000001, "text": " a new struct, which is basically a new string, or something like that, and here I added omitempty.", "tokens": [257, 777, 6594, 11, 597, 307, 1936, 257, 777, 6798, 11, 420, 746, 411, 300, 11, 293, 510, 286, 3869, 3406, 270, 4543, 88, 13], "temperature": 0.0, "avg_logprob": -0.22020064728169503, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.001140480162575841}, {"id": 68, "seek": 55960, "start": 577.9200000000001, "end": 584.88, "text": " In this case, I implemented the E0 method here, which says if it's not valid, then it's", "tokens": [682, 341, 1389, 11, 286, 12270, 264, 462, 15, 3170, 510, 11, 597, 1619, 498, 309, 311, 406, 7363, 11, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.22020064728169503, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.001140480162575841}, {"id": 69, "seek": 58488, "start": 584.88, "end": 594.04, "text": " basically a 0, so I wanted to remove it from the JSON. But if I run the actual code, please", "tokens": [1936, 257, 1958, 11, 370, 286, 1415, 281, 4159, 309, 490, 264, 31828, 13, 583, 498, 286, 1190, 264, 3539, 3089, 11, 1767], "temperature": 0.0, "avg_logprob": -0.24302128645089957, "compression_ratio": 1.2805755395683454, "no_speech_prob": 0.0003118592721875757}, {"id": 70, "seek": 58488, "start": 594.04, "end": 606.2, "text": " run it. Live demo is in a lightning talk. You're brave. Yes, live coding. You see that", "tokens": [1190, 309, 13, 10385, 10723, 307, 294, 257, 16589, 751, 13, 509, 434, 12653, 13, 1079, 11, 1621, 17720, 13, 509, 536, 300], "temperature": 0.0, "avg_logprob": -0.24302128645089957, "compression_ratio": 1.2805755395683454, "no_speech_prob": 0.0003118592721875757}, {"id": 71, "seek": 60620, "start": 606.2, "end": 618.12, "text": " it's basically here inside the JSON, however, I wanted to basically an empty JSON. And there", "tokens": [309, 311, 1936, 510, 1854, 264, 31828, 11, 4461, 11, 286, 1415, 281, 1936, 364, 6707, 31828, 13, 400, 456], "temperature": 0.0, "avg_logprob": -0.1528699434720553, "compression_ratio": 1.5054945054945055, "no_speech_prob": 8.79429280757904e-05}, {"id": 72, "seek": 60620, "start": 618.12, "end": 625.6400000000001, "text": " is another implementation with exactly the same code, but I just created a pumpkin seed", "tokens": [307, 1071, 11420, 365, 2293, 264, 912, 3089, 11, 457, 286, 445, 2942, 257, 17537, 8871], "temperature": 0.0, "avg_logprob": -0.1528699434720553, "compression_ratio": 1.5054945054945055, "no_speech_prob": 8.79429280757904e-05}, {"id": 73, "seek": 60620, "start": 625.6400000000001, "end": 633.88, "text": " JSON, which is exactly the copy of the built-in JSON. The only difference here that the issue", "tokens": [31828, 11, 597, 307, 2293, 264, 5055, 295, 264, 3094, 12, 259, 31828, 13, 440, 787, 2649, 510, 300, 264, 2734], "temperature": 0.0, "avg_logprob": -0.1528699434720553, "compression_ratio": 1.5054945054945055, "no_speech_prob": 8.79429280757904e-05}, {"id": 74, "seek": 63388, "start": 633.88, "end": 639.96, "text": " what I mentioned is basically suggesting an implementation that the omitempty section", "tokens": [437, 286, 2835, 307, 1936, 18094, 364, 11420, 300, 264, 3406, 270, 4543, 88, 3541], "temperature": 0.0, "avg_logprob": -0.13603049605640014, "compression_ratio": 1.52, "no_speech_prob": 0.000194388790987432}, {"id": 75, "seek": 63388, "start": 639.96, "end": 645.92, "text": " of the built-in JSON should check for the E0 method, whether it's existing in the struct", "tokens": [295, 264, 3094, 12, 259, 31828, 820, 1520, 337, 264, 462, 15, 3170, 11, 1968, 309, 311, 6741, 294, 264, 6594], "temperature": 0.0, "avg_logprob": -0.13603049605640014, "compression_ratio": 1.52, "no_speech_prob": 0.000194388790987432}, {"id": 76, "seek": 63388, "start": 645.92, "end": 662.88, "text": " or not. And if I run this one, it's basically doing what it should do. And basically that's", "tokens": [420, 406, 13, 400, 498, 286, 1190, 341, 472, 11, 309, 311, 1936, 884, 437, 309, 820, 360, 13, 400, 1936, 300, 311], "temperature": 0.0, "avg_logprob": -0.13603049605640014, "compression_ratio": 1.52, "no_speech_prob": 0.000194388790987432}, {"id": 77, "seek": 66288, "start": 662.88, "end": 676.2, "text": " it. So this is something what I think should be implemented in Go and this ticket with", "tokens": [309, 13, 407, 341, 307, 746, 437, 286, 519, 820, 312, 12270, 294, 1037, 293, 341, 10550, 365], "temperature": 0.0, "avg_logprob": -0.17951249319409568, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.0004925528191961348}, {"id": 78, "seek": 66288, "start": 676.2, "end": 685.4, "text": " this number is basically showing actual implementations for that. Right now, most of them are not", "tokens": [341, 1230, 307, 1936, 4099, 3539, 4445, 763, 337, 300, 13, 1779, 586, 11, 881, 295, 552, 366, 406], "temperature": 0.0, "avg_logprob": -0.17951249319409568, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.0004925528191961348}, {"id": 79, "seek": 66288, "start": 685.4, "end": 691.8, "text": " declined but not processed. So I think if anyone has a good idea how to implement it", "tokens": [29213, 457, 406, 18846, 13, 407, 286, 519, 498, 2878, 575, 257, 665, 1558, 577, 281, 4445, 309], "temperature": 0.0, "avg_logprob": -0.17951249319409568, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.0004925528191961348}, {"id": 80, "seek": 69180, "start": 691.8, "end": 699.28, "text": " in Go, then basically it would be nice to put into this ticket. There are also, this", "tokens": [294, 1037, 11, 550, 1936, 309, 576, 312, 1481, 281, 829, 666, 341, 10550, 13, 821, 366, 611, 11, 341], "temperature": 0.0, "avg_logprob": -0.24157779161320175, "compression_ratio": 1.3492063492063493, "no_speech_prob": 0.000340470316587016}, {"id": 81, "seek": 69180, "start": 699.28, "end": 707.0799999999999, "text": " is the actual change request in the code language what the guy made and I just copied", "tokens": [307, 264, 3539, 1319, 5308, 294, 264, 3089, 2856, 437, 264, 2146, 1027, 293, 286, 445, 25365], "temperature": 0.0, "avg_logprob": -0.24157779161320175, "compression_ratio": 1.3492063492063493, "no_speech_prob": 0.000340470316587016}, {"id": 82, "seek": 70708, "start": 707.08, "end": 721.84, "text": " his code. Yeah. One disclaimer, the pumpkin seed JSON package, you shouldn't use in production.", "tokens": [702, 3089, 13, 865, 13, 1485, 40896, 11, 264, 17537, 8871, 31828, 7372, 11, 291, 4659, 380, 764, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.30229833602905276, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.0011107773752883077}, {"id": 83, "seek": 72184, "start": 721.84, "end": 744.2, "text": " And that's it. Thank you. If this is your slide, come to the stage.", "tokens": [400, 300, 311, 309, 13, 1044, 291, 13, 759, 341, 307, 428, 4137, 11, 808, 281, 264, 3233, 13], "temperature": 0.0, "avg_logprob": -0.26910537222157355, "compression_ratio": 0.9710144927536232, "no_speech_prob": 0.0021176913287490606}, {"id": 84, "seek": 74420, "start": 744.2, "end": 751.88, "text": " All right. Hello. My name is Michiel. I created Mox. I've been working on this for quite some", "tokens": [1057, 558, 13, 2425, 13, 1222, 1315, 307, 3392, 1187, 13, 286, 2942, 3335, 87, 13, 286, 600, 668, 1364, 322, 341, 337, 1596, 512], "temperature": 0.0, "avg_logprob": -0.2930838521321615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.002445266116410494}, {"id": 85, "seek": 74420, "start": 751.88, "end": 758.2, "text": " time. I started using it two weeks ago, released it earlier this week. It's a meal server.", "tokens": [565, 13, 286, 1409, 1228, 309, 732, 3259, 2057, 11, 4736, 309, 3071, 341, 1243, 13, 467, 311, 257, 6791, 7154, 13], "temperature": 0.0, "avg_logprob": -0.2930838521321615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.002445266116410494}, {"id": 86, "seek": 74420, "start": 758.2, "end": 763.32, "text": " So I'm curious, is anyone here running their own meal servers around the main? One, two", "tokens": [407, 286, 478, 6369, 11, 307, 2878, 510, 2614, 641, 1065, 6791, 15909, 926, 264, 2135, 30, 1485, 11, 732], "temperature": 0.0, "avg_logprob": -0.2930838521321615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.002445266116410494}, {"id": 87, "seek": 76332, "start": 763.32, "end": 774.4000000000001, "text": " persons? Wow. Okay. Three, room for improvement. So let's go right ahead. This is the tagline.", "tokens": [14453, 30, 3153, 13, 1033, 13, 6244, 11, 1808, 337, 10444, 13, 407, 718, 311, 352, 558, 2286, 13, 639, 307, 264, 6162, 1889, 13], "temperature": 0.0, "avg_logprob": -0.20088739106149384, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0008388273417949677}, {"id": 88, "seek": 76332, "start": 774.4000000000001, "end": 778.36, "text": " It's a modern, full-featured open source secure meal server for low maintenance self-hosted", "tokens": [467, 311, 257, 4363, 11, 1577, 12, 2106, 1503, 67, 1269, 4009, 7144, 6791, 7154, 337, 2295, 11258, 2698, 12, 6037, 292], "temperature": 0.0, "avg_logprob": -0.20088739106149384, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0008388273417949677}, {"id": 89, "seek": 76332, "start": 778.36, "end": 784.24, "text": " email. So let's break it down. It's modern because it supports all the latest meal standards", "tokens": [3796, 13, 407, 718, 311, 1821, 309, 760, 13, 467, 311, 4363, 570, 309, 9346, 439, 264, 6792, 6791, 7787], "temperature": 0.0, "avg_logprob": -0.20088739106149384, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0008388273417949677}, {"id": 90, "seek": 76332, "start": 784.24, "end": 789.72, "text": " and there have been added quite a few over the years. It is full-featured in the sense", "tokens": [293, 456, 362, 668, 3869, 1596, 257, 1326, 670, 264, 924, 13, 467, 307, 1577, 12, 2106, 1503, 67, 294, 264, 2020], "temperature": 0.0, "avg_logprob": -0.20088739106149384, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0008388273417949677}, {"id": 91, "seek": 78972, "start": 789.72, "end": 794.88, "text": " that it aims to do everything at once, meaning all the relevant email standards. So you just", "tokens": [300, 309, 24683, 281, 360, 1203, 412, 1564, 11, 3620, 439, 264, 7340, 3796, 7787, 13, 407, 291, 445], "temperature": 0.0, "avg_logprob": -0.14397090452688713, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.000939875200856477}, {"id": 92, "seek": 78972, "start": 794.88, "end": 798.64, "text": " need this one thing. You don't need a whole bunch of components to make a working system.", "tokens": [643, 341, 472, 551, 13, 509, 500, 380, 643, 257, 1379, 3840, 295, 6677, 281, 652, 257, 1364, 1185, 13], "temperature": 0.0, "avg_logprob": -0.14397090452688713, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.000939875200856477}, {"id": 93, "seek": 78972, "start": 798.64, "end": 803.6800000000001, "text": " So just really to make it easier. It's MIT licensed. It is secure, meaning it supports", "tokens": [407, 445, 534, 281, 652, 309, 3571, 13, 467, 311, 13100, 25225, 13, 467, 307, 7144, 11, 3620, 309, 9346], "temperature": 0.0, "avg_logprob": -0.14397090452688713, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.000939875200856477}, {"id": 94, "seek": 78972, "start": 803.6800000000001, "end": 808.88, "text": " all the latest security things about email like TLS, et cetera. And of course, a bit", "tokens": [439, 264, 6792, 3825, 721, 466, 3796, 411, 314, 19198, 11, 1030, 11458, 13, 400, 295, 1164, 11, 257, 857], "temperature": 0.0, "avg_logprob": -0.14397090452688713, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.000939875200856477}, {"id": 95, "seek": 78972, "start": 808.88, "end": 814.0400000000001, "text": " of secure coding and low maintenance. So you actually started using it because I hear many", "tokens": [295, 7144, 17720, 293, 2295, 11258, 13, 407, 291, 767, 1409, 1228, 309, 570, 286, 1568, 867], "temperature": 0.0, "avg_logprob": -0.14397090452688713, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.000939875200856477}, {"id": 96, "seek": 81404, "start": 814.04, "end": 820.12, "text": " people are moving all their email to the cloud, some big providers because it's too hard apparently", "tokens": [561, 366, 2684, 439, 641, 3796, 281, 264, 4588, 11, 512, 955, 11330, 570, 309, 311, 886, 1152, 7970], "temperature": 0.0, "avg_logprob": -0.1472100582751599, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0004721829027403146}, {"id": 97, "seek": 81404, "start": 820.12, "end": 828.24, "text": " to run a meal server. So it's for your self-hosted email. Email is one of the oldest decentralized", "tokens": [281, 1190, 257, 6791, 7154, 13, 407, 309, 311, 337, 428, 2698, 12, 6037, 292, 3796, 13, 49482, 307, 472, 295, 264, 14026, 32870], "temperature": 0.0, "avg_logprob": -0.1472100582751599, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0004721829027403146}, {"id": 98, "seek": 81404, "start": 828.24, "end": 833.5999999999999, "text": " messaging protocols, but we're making it more centralized by moving everything to the few", "tokens": [21812, 20618, 11, 457, 321, 434, 1455, 309, 544, 32395, 538, 2684, 1203, 281, 264, 1326], "temperature": 0.0, "avg_logprob": -0.1472100582751599, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0004721829027403146}, {"id": 99, "seek": 81404, "start": 833.5999999999999, "end": 841.52, "text": " big providers. So Mox is an attempt to make it so easy that you will all start using it.", "tokens": [955, 11330, 13, 407, 3335, 87, 307, 364, 5217, 281, 652, 309, 370, 1858, 300, 291, 486, 439, 722, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.1472100582751599, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0004721829027403146}, {"id": 100, "seek": 84152, "start": 841.52, "end": 846.76, "text": " So a bunch of features, a list of acronyms. IMAP, so you can access your mail, SNTP, so", "tokens": [407, 257, 3840, 295, 4122, 11, 257, 1329, 295, 31713, 88, 2592, 13, 21463, 4715, 11, 370, 291, 393, 2105, 428, 10071, 11, 13955, 16804, 11, 370], "temperature": 0.0, "avg_logprob": -0.23950397267061121, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.000492592342197895}, {"id": 101, "seek": 84152, "start": 846.76, "end": 851.64, "text": " you can send mail. Nowadays, if you want to send mail, you need to configure SPF, DKIM,", "tokens": [291, 393, 2845, 10071, 13, 28908, 11, 498, 291, 528, 281, 2845, 10071, 11, 291, 643, 281, 22162, 8420, 37, 11, 31934, 6324, 11], "temperature": 0.0, "avg_logprob": -0.23950397267061121, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.000492592342197895}, {"id": 102, "seek": 84152, "start": 851.64, "end": 858.16, "text": " DMARC. Does anyone know what that means? Yeah, see, that's good. Automatic TLS, so you don't", "tokens": [15322, 1899, 34, 13, 4402, 2878, 458, 437, 300, 1355, 30, 865, 11, 536, 11, 300, 311, 665, 13, 6049, 13143, 314, 19198, 11, 370, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.23950397267061121, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.000492592342197895}, {"id": 103, "seek": 84152, "start": 858.16, "end": 863.0, "text": " have to worry about any certificate stuff. So it's like the caddy for email. TLS reporting,", "tokens": [362, 281, 3292, 466, 604, 15953, 1507, 13, 407, 309, 311, 411, 264, 12209, 3173, 337, 3796, 13, 314, 19198, 10031, 11], "temperature": 0.0, "avg_logprob": -0.23950397267061121, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.000492592342197895}, {"id": 104, "seek": 84152, "start": 863.0, "end": 869.52, "text": " MTA, STS, that's one of the latest additions to secure email. There's a reputation-based", "tokens": [376, 8241, 11, 4904, 50, 11, 300, 311, 472, 295, 264, 6792, 35113, 281, 7144, 3796, 13, 821, 311, 257, 13061, 12, 6032], "temperature": 0.0, "avg_logprob": -0.23950397267061121, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.000492592342197895}, {"id": 105, "seek": 86952, "start": 869.52, "end": 874.48, "text": " junk filter in there, so if you receive messages from people and you don't like those messages", "tokens": [19109, 6608, 294, 456, 11, 370, 498, 291, 4774, 7897, 490, 561, 293, 291, 500, 380, 411, 729, 7897], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 106, "seek": 86952, "start": 874.48, "end": 879.76, "text": " and you mark them as junk, the next time those people send mail, it's rejected. So new senders", "tokens": [293, 291, 1491, 552, 382, 19109, 11, 264, 958, 565, 729, 561, 2845, 10071, 11, 309, 311, 15749, 13, 407, 777, 2845, 433], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 107, "seek": 86952, "start": 879.76, "end": 883.64, "text": " don't have any reputation. You can look at the content, so there's a content-based abyeasing", "tokens": [500, 380, 362, 604, 13061, 13, 509, 393, 574, 412, 264, 2701, 11, 370, 456, 311, 257, 2701, 12, 6032, 410, 1200, 3349], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 108, "seek": 86952, "start": 883.64, "end": 887.52, "text": " spam filter, so in there. Internationalized email, so you can have smileys in your domain", "tokens": [24028, 6608, 11, 370, 294, 456, 13, 9157, 1602, 3796, 11, 370, 291, 393, 362, 7563, 749, 294, 428, 9274], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 109, "seek": 86952, "start": 887.52, "end": 892.36, "text": " names, that's what you want. And auto-configuration, so you get your thunderbird, and setup is", "tokens": [5288, 11, 300, 311, 437, 291, 528, 13, 400, 8399, 12, 1671, 20646, 8167, 11, 370, 291, 483, 428, 19898, 18080, 11, 293, 8657, 307], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 110, "seek": 86952, "start": 892.36, "end": 897.8, "text": " just instant. No need to worry about all the port numbers, et cetera. It just works. So", "tokens": [445, 9836, 13, 883, 643, 281, 3292, 466, 439, 264, 2436, 3547, 11, 1030, 11458, 13, 467, 445, 1985, 13, 407], "temperature": 0.0, "avg_logprob": -0.22884860860890355, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.001448091701604426}, {"id": 111, "seek": 89780, "start": 897.8, "end": 901.4799999999999, "text": " getting started, of course, now you're all convinced you want to use this. Luckily, there's", "tokens": [1242, 1409, 11, 295, 1164, 11, 586, 291, 434, 439, 12561, 291, 528, 281, 764, 341, 13, 19726, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 112, "seek": 89780, "start": 901.4799999999999, "end": 907.7199999999999, "text": " a quick start. You just set up a Linux machine, probably, get your email address for your", "tokens": [257, 1702, 722, 13, 509, 445, 992, 493, 257, 18734, 3479, 11, 1391, 11, 483, 428, 3796, 2985, 337, 428], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 113, "seek": 89780, "start": 907.7199999999999, "end": 911.52, "text": " domain, and you get a configuration file that's all, that has this all configured. You just", "tokens": [9274, 11, 293, 291, 483, 257, 11694, 3991, 300, 311, 439, 11, 300, 575, 341, 439, 30538, 13, 509, 445], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 114, "seek": 89780, "start": 911.52, "end": 916.04, "text": " can start it right after. Not only does it make a configuration file, also print some", "tokens": [393, 722, 309, 558, 934, 13, 1726, 787, 775, 309, 652, 257, 11694, 3991, 11, 611, 4482, 512], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 115, "seek": 89780, "start": 916.04, "end": 919.64, "text": " commands and all the DNS records that you need to create, so you don't have to think.", "tokens": [16901, 293, 439, 264, 35153, 7724, 300, 291, 643, 281, 1884, 11, 370, 291, 500, 380, 362, 281, 519, 13], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 116, "seek": 89780, "start": 919.64, "end": 926.4399999999999, "text": " You can just copy, paste, and be happy. Then the code, 40K lines of implementation, 10K", "tokens": [509, 393, 445, 5055, 11, 9163, 11, 293, 312, 2055, 13, 1396, 264, 3089, 11, 3356, 42, 3876, 295, 11420, 11, 1266, 42], "temperature": 0.0, "avg_logprob": -0.24659395217895508, "compression_ratio": 1.670846394984326, "no_speech_prob": 0.0007657592068426311}, {"id": 117, "seek": 92644, "start": 926.44, "end": 931.4000000000001, "text": " lines of tests, quite some test coverage. There's integration tests, fuzzing tests.", "tokens": [3876, 295, 6921, 11, 1596, 512, 1500, 9645, 13, 821, 311, 10980, 6921, 11, 283, 3334, 8781, 6921, 13], "temperature": 0.0, "avg_logprob": -0.2221873919169108, "compression_ratio": 1.669172932330827, "no_speech_prob": 0.00034898711601272225}, {"id": 118, "seek": 92644, "start": 931.4000000000001, "end": 936.24, "text": " It's all pure Go, no C Go, just go install, cross compile, all the good stuff that you", "tokens": [467, 311, 439, 6075, 1037, 11, 572, 383, 1037, 11, 445, 352, 3625, 11, 3278, 31413, 11, 439, 264, 665, 1507, 300, 291], "temperature": 0.0, "avg_logprob": -0.2221873919169108, "compression_ratio": 1.669172932330827, "no_speech_prob": 0.00034898711601272225}, {"id": 119, "seek": 92644, "start": 936.24, "end": 943.84, "text": " get from Go. The implementation is heavily cross-referenced with the RFCs, so both ways.", "tokens": [483, 490, 1037, 13, 440, 11420, 307, 10950, 3278, 12, 265, 612, 14672, 365, 264, 497, 18671, 82, 11, 370, 1293, 2098, 13], "temperature": 0.0, "avg_logprob": -0.2221873919169108, "compression_ratio": 1.669172932330827, "no_speech_prob": 0.00034898711601272225}, {"id": 120, "seek": 92644, "start": 943.84, "end": 948.12, "text": " You can go from code to the RFC and back from the RFC to the places in the code where it's", "tokens": [509, 393, 352, 490, 3089, 281, 264, 497, 18671, 293, 646, 490, 264, 497, 18671, 281, 264, 3190, 294, 264, 3089, 689, 309, 311], "temperature": 0.0, "avg_logprob": -0.2221873919169108, "compression_ratio": 1.669172932330827, "no_speech_prob": 0.00034898711601272225}, {"id": 121, "seek": 92644, "start": 948.12, "end": 952.2800000000001, "text": " used. So this is supposed to help with maintenance, so it's implementing all these protocols,", "tokens": [1143, 13, 407, 341, 307, 3442, 281, 854, 365, 11258, 11, 370, 309, 311, 18114, 439, 613, 20618, 11], "temperature": 0.0, "avg_logprob": -0.2221873919169108, "compression_ratio": 1.669172932330827, "no_speech_prob": 0.00034898711601272225}, {"id": 122, "seek": 95228, "start": 952.28, "end": 958.8, "text": " and it gets a bit overwhelming to understand all of that. So if you would code it once,", "tokens": [293, 309, 2170, 257, 857, 13373, 281, 1223, 439, 295, 300, 13, 407, 498, 291, 576, 3089, 309, 1564, 11], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 123, "seek": 95228, "start": 958.8, "end": 963.68, "text": " you cannot go back to the specification and back to the implementation. You don't know", "tokens": [291, 2644, 352, 646, 281, 264, 31256, 293, 646, 281, 264, 11420, 13, 509, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 124, "seek": 95228, "start": 963.68, "end": 969.28, "text": " what's going to, so how you, how to fix bugs, et cetera. Let's move. Oh, wow, quick.", "tokens": [437, 311, 516, 281, 11, 370, 577, 291, 11, 577, 281, 3191, 15120, 11, 1030, 11458, 13, 961, 311, 1286, 13, 876, 11, 6076, 11, 1702, 13], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 125, "seek": 95228, "start": 969.28, "end": 973.72, "text": " So what's next? I just released it. I'm looking for feedback. Please use it and tell me if", "tokens": [407, 437, 311, 958, 30, 286, 445, 4736, 309, 13, 286, 478, 1237, 337, 5824, 13, 2555, 764, 309, 293, 980, 385, 498], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 126, "seek": 95228, "start": 973.72, "end": 978.0, "text": " it works for you or why it does not work for you. So I aim to make it very simple, so if", "tokens": [309, 1985, 337, 291, 420, 983, 309, 775, 406, 589, 337, 291, 13, 407, 286, 5939, 281, 652, 309, 588, 2199, 11, 370, 498], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 127, "seek": 95228, "start": 978.0, "end": 981.56, "text": " you find something that's not simple, let me know. Of course, if you find bugs, let", "tokens": [291, 915, 746, 300, 311, 406, 2199, 11, 718, 385, 458, 13, 2720, 1164, 11, 498, 291, 915, 15120, 11, 718], "temperature": 0.0, "avg_logprob": -0.1778200619841275, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.00021002696303185076}, {"id": 128, "seek": 98156, "start": 981.56, "end": 988.56, "text": " me know. And this is where you can find it. All right.", "tokens": [385, 458, 13, 400, 341, 307, 689, 291, 393, 915, 309, 13, 1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21991404365090764, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0018008277984336019}, {"id": 129, "seek": 98156, "start": 988.56, "end": 999.1999999999999, "text": " Thank you. If this is your slide deck, you can come to the stage. If this is nobody's", "tokens": [1044, 291, 13, 759, 341, 307, 428, 4137, 9341, 11, 291, 393, 808, 281, 264, 3233, 13, 759, 341, 307, 5079, 311], "temperature": 0.0, "avg_logprob": -0.21991404365090764, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0018008277984336019}, {"id": 130, "seek": 98156, "start": 999.1999999999999, "end": 1009.04, "text": " slide deck, I'll just skip it. Something with Postgres. If this is your 404 page which you", "tokens": [4137, 9341, 11, 286, 603, 445, 10023, 309, 13, 6595, 365, 10223, 45189, 13, 759, 341, 307, 428, 3356, 19, 3028, 597, 291], "temperature": 0.0, "avg_logprob": -0.21991404365090764, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0018008277984336019}, {"id": 131, "seek": 100904, "start": 1009.04, "end": 1016.68, "text": " sent to me, please also come to talk to me. So yeah, also the speaker is not found. That's", "tokens": [2279, 281, 385, 11, 1767, 611, 808, 281, 751, 281, 385, 13, 407, 1338, 11, 611, 264, 8145, 307, 406, 1352, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.24914298119483055, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0011612479574978352}, {"id": 132, "seek": 100904, "start": 1016.68, "end": 1021.7199999999999, "text": " the thing with last minute talks. Then I had one backup speaker. You can come to the stage.", "tokens": [264, 551, 365, 1036, 3456, 6686, 13, 1396, 286, 632, 472, 14807, 8145, 13, 509, 393, 808, 281, 264, 3233, 13], "temperature": 0.0, "avg_logprob": -0.24914298119483055, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0011612479574978352}, {"id": 133, "seek": 100904, "start": 1021.7199999999999, "end": 1030.72, "text": " And the gophers are also falling down. They are tired. Understands me too, me too. Yes,", "tokens": [400, 264, 352, 950, 433, 366, 611, 7440, 760, 13, 814, 366, 5868, 13, 26093, 82, 385, 886, 11, 385, 886, 13, 1079, 11], "temperature": 0.0, "avg_logprob": -0.24914298119483055, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0011612479574978352}, {"id": 134, "seek": 103072, "start": 1030.72, "end": 1055.72, "text": " I have HDMI. I also use USB-C. Let me just close this down for you. That's 4G clicker.", "tokens": [286, 362, 30811, 13, 286, 611, 764, 10109, 12, 34, 13, 961, 385, 445, 1998, 341, 760, 337, 291, 13, 663, 311, 1017, 38, 2052, 260, 13], "temperature": 0.0, "avg_logprob": -0.5524884500811177, "compression_ratio": 0.9662921348314607, "no_speech_prob": 0.007615178357809782}, {"id": 135, "seek": 105572, "start": 1055.72, "end": 1080.0, "text": " Okay. So thank you, first of all. So I want to tell a go-of-story and why we use Go to", "tokens": [1033, 13, 407, 1309, 291, 11, 700, 295, 439, 13, 407, 286, 528, 281, 980, 257, 352, 12, 2670, 12, 24513, 293, 983, 321, 764, 1037, 281], "temperature": 0.0, "avg_logprob": -0.32185203798355594, "compression_ratio": 1.048780487804878, "no_speech_prob": 0.00528764771297574}, {"id": 136, "seek": 108000, "start": 1080.0, "end": 1085.84, "text": " have to implement this idea of fluid pull requests. Before starting with that, I need", "tokens": [362, 281, 4445, 341, 1558, 295, 9113, 2235, 12475, 13, 4546, 2891, 365, 300, 11, 286, 643], "temperature": 0.0, "avg_logprob": -0.2834953580583845, "compression_ratio": 1.752, "no_speech_prob": 0.004373001400381327}, {"id": 137, "seek": 108000, "start": 1085.84, "end": 1091.92, "text": " to talk a little bit about pull requests. So for that, I brought Robin and Kat with me.", "tokens": [281, 751, 257, 707, 857, 466, 2235, 12475, 13, 407, 337, 300, 11, 286, 3038, 16533, 293, 8365, 365, 385, 13], "temperature": 0.0, "avg_logprob": -0.2834953580583845, "compression_ratio": 1.752, "no_speech_prob": 0.004373001400381327}, {"id": 138, "seek": 108000, "start": 1091.92, "end": 1099.0, "text": " So Robin wants to contribute to a project that Kat is a maintainer. And what everyone", "tokens": [407, 16533, 2738, 281, 10586, 281, 257, 1716, 300, 8365, 307, 257, 6909, 260, 13, 400, 437, 1518], "temperature": 0.0, "avg_logprob": -0.2834953580583845, "compression_ratio": 1.752, "no_speech_prob": 0.004373001400381327}, {"id": 139, "seek": 108000, "start": 1099.0, "end": 1104.36, "text": " does or at least they try to, they open a branch, they create what they have to do. And", "tokens": [775, 420, 412, 1935, 436, 853, 281, 11, 436, 1269, 257, 9819, 11, 436, 1884, 437, 436, 362, 281, 360, 13, 400], "temperature": 0.0, "avg_logprob": -0.2834953580583845, "compression_ratio": 1.752, "no_speech_prob": 0.004373001400381327}, {"id": 140, "seek": 108000, "start": 1104.36, "end": 1108.56, "text": " then at the end, it comes a time when it needs to merge into main. And then when Kat comes", "tokens": [550, 412, 264, 917, 11, 309, 1487, 257, 565, 562, 309, 2203, 281, 22183, 666, 2135, 13, 400, 550, 562, 8365, 1487], "temperature": 0.0, "avg_logprob": -0.2834953580583845, "compression_ratio": 1.752, "no_speech_prob": 0.004373001400381327}, {"id": 141, "seek": 110856, "start": 1108.56, "end": 1114.9199999999998, "text": " in and says, wait a minute, we need to review those changes. So this kind of methodology", "tokens": [294, 293, 1619, 11, 1699, 257, 3456, 11, 321, 643, 281, 3131, 729, 2962, 13, 407, 341, 733, 295, 24850], "temperature": 0.0, "avg_logprob": -0.20118248128445348, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0017380033386871219}, {"id": 142, "seek": 110856, "start": 1114.9199999999998, "end": 1120.2, "text": " is important for critical contributions from interested parties. And it's well-known as", "tokens": [307, 1021, 337, 4924, 15725, 490, 3102, 8265, 13, 400, 309, 311, 731, 12, 6861, 382], "temperature": 0.0, "avg_logprob": -0.20118248128445348, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0017380033386871219}, {"id": 143, "seek": 110856, "start": 1120.2, "end": 1125.36, "text": " open source projects, especially with the name of pull requests. We also use it inside", "tokens": [1269, 4009, 4455, 11, 2318, 365, 264, 1315, 295, 2235, 12475, 13, 492, 611, 764, 309, 1854], "temperature": 0.0, "avg_logprob": -0.20118248128445348, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0017380033386871219}, {"id": 144, "seek": 110856, "start": 1125.36, "end": 1132.28, "text": " our own companies. But it's well-known at the open source community. And it's quite", "tokens": [527, 1065, 3431, 13, 583, 309, 311, 731, 12, 6861, 412, 264, 1269, 4009, 1768, 13, 400, 309, 311, 1596], "temperature": 0.0, "avg_logprob": -0.20118248128445348, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0017380033386871219}, {"id": 145, "seek": 110856, "start": 1132.28, "end": 1138.24, "text": " popular. As you can see, in 2021, we got a lot of pull requests. And the process goes", "tokens": [3743, 13, 1018, 291, 393, 536, 11, 294, 7201, 11, 321, 658, 257, 688, 295, 2235, 12475, 13, 400, 264, 1399, 1709], "temperature": 0.0, "avg_logprob": -0.20118248128445348, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0017380033386871219}, {"id": 146, "seek": 113824, "start": 1138.24, "end": 1142.34, "text": " like you do whatever you want to do. Then the CI triggers, you get the review, you get", "tokens": [411, 291, 360, 2035, 291, 528, 281, 360, 13, 1396, 264, 37777, 22827, 11, 291, 483, 264, 3131, 11, 291, 483], "temperature": 0.0, "avg_logprob": -0.16202714902545334, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0007930703577585518}, {"id": 147, "seek": 113824, "start": 1142.34, "end": 1146.88, "text": " some feedback, and then you have to apply the feedback. And we enter a loop here until", "tokens": [512, 5824, 11, 293, 550, 291, 362, 281, 3079, 264, 5824, 13, 400, 321, 3242, 257, 6367, 510, 1826], "temperature": 0.0, "avg_logprob": -0.16202714902545334, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0007930703577585518}, {"id": 148, "seek": 113824, "start": 1146.88, "end": 1151.88, "text": " someone decides that it's good to go and we get our approval. Then it goes to merge and", "tokens": [1580, 14898, 300, 309, 311, 665, 281, 352, 293, 321, 483, 527, 13317, 13, 1396, 309, 1709, 281, 22183, 293], "temperature": 0.0, "avg_logprob": -0.16202714902545334, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0007930703577585518}, {"id": 149, "seek": 113824, "start": 1151.88, "end": 1157.6, "text": " that everyone is happy. And the problem here is that Robin goes through this process every", "tokens": [300, 1518, 307, 2055, 13, 400, 264, 1154, 510, 307, 300, 16533, 1709, 807, 341, 1399, 633], "temperature": 0.0, "avg_logprob": -0.16202714902545334, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0007930703577585518}, {"id": 150, "seek": 113824, "start": 1157.6, "end": 1165.1200000000001, "text": " time, regardless of the type of change it is. And we are unavailable with the fact that", "tokens": [565, 11, 10060, 295, 264, 2010, 295, 1319, 309, 307, 13, 400, 321, 366, 36541, 32699, 365, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.16202714902545334, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.0007930703577585518}, {"id": 151, "seek": 116512, "start": 1165.12, "end": 1172.12, "text": " Robin and Kat have been contributing and working with each other for some time. So this idea", "tokens": [16533, 293, 8365, 362, 668, 19270, 293, 1364, 365, 1184, 661, 337, 512, 565, 13, 407, 341, 1558], "temperature": 0.0, "avg_logprob": -0.175670411851671, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0011963569559156895}, {"id": 152, "seek": 116512, "start": 1172.12, "end": 1177.0, "text": " that all pull requests are the same can be actually improved. For instance, this scenario", "tokens": [300, 439, 2235, 12475, 366, 264, 912, 393, 312, 767, 9689, 13, 1171, 5197, 11, 341, 9005], "temperature": 0.0, "avg_logprob": -0.175670411851671, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0011963569559156895}, {"id": 153, "seek": 116512, "start": 1177.0, "end": 1181.6, "text": " where Robin is just trying to do some configuration change, why do we need a pull request? Maybe", "tokens": [689, 16533, 307, 445, 1382, 281, 360, 512, 11694, 1319, 11, 983, 360, 321, 643, 257, 2235, 5308, 30, 2704], "temperature": 0.0, "avg_logprob": -0.175670411851671, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0011963569559156895}, {"id": 154, "seek": 116512, "start": 1181.6, "end": 1188.2399999999998, "text": " we can just go directly to main without a review. Another scenario where Robin just", "tokens": [321, 393, 445, 352, 3838, 281, 2135, 1553, 257, 3131, 13, 3996, 9005, 689, 16533, 445], "temperature": 0.0, "avg_logprob": -0.175670411851671, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0011963569559156895}, {"id": 155, "seek": 118824, "start": 1188.24, "end": 1196.04, "text": " gets an API with some documentation or some warnings. Let's imagine why can it go to main", "tokens": [2170, 364, 9362, 365, 512, 14333, 420, 512, 30009, 13, 961, 311, 3811, 983, 393, 309, 352, 281, 2135], "temperature": 0.0, "avg_logprob": -0.21638818220658737, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0010414057178422809}, {"id": 156, "seek": 118824, "start": 1196.04, "end": 1201.1200000000001, "text": " and then we can do a review afterwards. And then when it comes to critical changes, then", "tokens": [293, 550, 321, 393, 360, 257, 3131, 10543, 13, 400, 550, 562, 309, 1487, 281, 4924, 2962, 11, 550], "temperature": 0.0, "avg_logprob": -0.21638818220658737, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0010414057178422809}, {"id": 157, "seek": 118824, "start": 1201.1200000000001, "end": 1204.48, "text": " when we want to stop the process and say, okay, this is critical, we need to have a", "tokens": [562, 321, 528, 281, 1590, 264, 1399, 293, 584, 11, 1392, 11, 341, 307, 4924, 11, 321, 643, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.21638818220658737, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0010414057178422809}, {"id": 158, "seek": 118824, "start": 1204.48, "end": 1209.68, "text": " very good review here. And maybe instead of just asking one guy, we can ask two people", "tokens": [588, 665, 3131, 510, 13, 400, 1310, 2602, 295, 445, 3365, 472, 2146, 11, 321, 393, 1029, 732, 561], "temperature": 0.0, "avg_logprob": -0.21638818220658737, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0010414057178422809}, {"id": 159, "seek": 118824, "start": 1209.68, "end": 1215.28, "text": " for them to get their own approval. So this idea of pull requests is that all that I just", "tokens": [337, 552, 281, 483, 641, 1065, 13317, 13, 407, 341, 1558, 295, 2235, 12475, 307, 300, 439, 300, 286, 445], "temperature": 0.0, "avg_logprob": -0.21638818220658737, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0010414057178422809}, {"id": 160, "seek": 121528, "start": 1215.28, "end": 1222.56, "text": " said could be defined in rules. And we can apply those rules into our own process and", "tokens": [848, 727, 312, 7642, 294, 4474, 13, 400, 321, 393, 3079, 729, 4474, 666, 527, 1065, 1399, 293], "temperature": 0.0, "avg_logprob": -0.20830619469117584, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.0025282215792685747}, {"id": 161, "seek": 121528, "start": 1222.56, "end": 1228.3999999999999, "text": " minimize the time. That's where we came with the review pad, which is done on go and it's", "tokens": [17522, 264, 565, 13, 663, 311, 689, 321, 1361, 365, 264, 3131, 6887, 11, 597, 307, 1096, 322, 352, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.20830619469117584, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.0025282215792685747}, {"id": 162, "seek": 121528, "start": 1228.3999999999999, "end": 1232.8799999999999, "text": " full open source. And that's where we can define all these ideas of what are the rules", "tokens": [1577, 1269, 4009, 13, 400, 300, 311, 689, 321, 393, 6964, 439, 613, 3487, 295, 437, 366, 264, 4474], "temperature": 0.0, "avg_logprob": -0.20830619469117584, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.0025282215792685747}, {"id": 163, "seek": 121528, "start": 1232.8799999999999, "end": 1241.6399999999999, "text": " for our team. So here's how we could work with this terminology. Behind this is go,", "tokens": [337, 527, 1469, 13, 407, 510, 311, 577, 321, 727, 589, 365, 341, 27575, 13, 20475, 341, 307, 352, 11], "temperature": 0.0, "avg_logprob": -0.20830619469117584, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.0025282215792685747}, {"id": 164, "seek": 124164, "start": 1241.64, "end": 1247.88, "text": " of course, then it can, for instance, if my changes are all on markdown files, I want", "tokens": [295, 1164, 11, 550, 309, 393, 11, 337, 5197, 11, 498, 452, 2962, 366, 439, 322, 1491, 5093, 7098, 11, 286, 528], "temperature": 0.0, "avg_logprob": -0.1905436660304214, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.001512071117758751}, {"id": 165, "seek": 124164, "start": 1247.88, "end": 1252.8400000000001, "text": " to merge my pull request right away. So no review. If, for instance, my author actually", "tokens": [281, 22183, 452, 2235, 5308, 558, 1314, 13, 407, 572, 3131, 13, 759, 11, 337, 5197, 11, 452, 3793, 767], "temperature": 0.0, "avg_logprob": -0.1905436660304214, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.001512071117758751}, {"id": 166, "seek": 124164, "start": 1252.8400000000001, "end": 1260.3200000000002, "text": " is considered a new joiner, a new joiner could be someone that didn't do 10 PRs, like Spotify", "tokens": [307, 4888, 257, 777, 3917, 260, 11, 257, 777, 3917, 260, 727, 312, 1580, 300, 994, 380, 360, 1266, 11568, 82, 11, 411, 29036], "temperature": 0.0, "avg_logprob": -0.1905436660304214, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.001512071117758751}, {"id": 167, "seek": 124164, "start": 1260.3200000000002, "end": 1267.1200000000001, "text": " does, I want to assign a reviewer from my tech leads. And then, for instance, if I want", "tokens": [775, 11, 286, 528, 281, 6269, 257, 3131, 260, 490, 452, 7553, 6689, 13, 400, 550, 11, 337, 5197, 11, 498, 286, 528], "temperature": 0.0, "avg_logprob": -0.1905436660304214, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.001512071117758751}, {"id": 168, "seek": 126712, "start": 1267.12, "end": 1272.36, "text": " to get some compliance, make sure that my pull request is an issue. I can confirm that and", "tokens": [281, 483, 512, 15882, 11, 652, 988, 300, 452, 2235, 5308, 307, 364, 2734, 13, 286, 393, 9064, 300, 293], "temperature": 0.0, "avg_logprob": -0.14556319758577166, "compression_ratio": 1.8112449799196788, "no_speech_prob": 0.0010418028105050325}, {"id": 169, "seek": 126712, "start": 1272.36, "end": 1279.0, "text": " make sure that the user gets notified as soon as possible in order to iterate on that. And", "tokens": [652, 988, 300, 264, 4195, 2170, 18013, 382, 2321, 382, 1944, 294, 1668, 281, 44497, 322, 300, 13, 400], "temperature": 0.0, "avg_logprob": -0.14556319758577166, "compression_ratio": 1.8112449799196788, "no_speech_prob": 0.0010418028105050325}, {"id": 170, "seek": 126712, "start": 1279.0, "end": 1285.6399999999999, "text": " then we can do some more incredible things. I want you to look at the line at the top", "tokens": [550, 321, 393, 360, 512, 544, 4651, 721, 13, 286, 528, 291, 281, 574, 412, 264, 1622, 412, 264, 1192], "temperature": 0.0, "avg_logprob": -0.14556319758577166, "compression_ratio": 1.8112449799196788, "no_speech_prob": 0.0010418028105050325}, {"id": 171, "seek": 126712, "start": 1285.6399999999999, "end": 1289.7199999999998, "text": " where we have an annotation saying that it's critical, saying that every time someone changes", "tokens": [689, 321, 362, 364, 48654, 1566, 300, 309, 311, 4924, 11, 1566, 300, 633, 565, 1580, 2962], "temperature": 0.0, "avg_logprob": -0.14556319758577166, "compression_ratio": 1.8112449799196788, "no_speech_prob": 0.0010418028105050325}, {"id": 172, "seek": 126712, "start": 1289.7199999999998, "end": 1295.1599999999999, "text": " that function, that function is critical. If the function is critical, if my code touches", "tokens": [300, 2445, 11, 300, 2445, 307, 4924, 13, 759, 264, 2445, 307, 4924, 11, 498, 452, 3089, 17431], "temperature": 0.0, "avg_logprob": -0.14556319758577166, "compression_ratio": 1.8112449799196788, "no_speech_prob": 0.0010418028105050325}, {"id": 173, "seek": 129516, "start": 1295.16, "end": 1300.24, "text": " a function that has this annotation, then I want to trigger my pull request review that", "tokens": [257, 2445, 300, 575, 341, 48654, 11, 550, 286, 528, 281, 7875, 452, 2235, 5308, 3131, 300], "temperature": 0.0, "avg_logprob": -0.2023970402708841, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0006278605433180928}, {"id": 174, "seek": 129516, "start": 1300.24, "end": 1304.96, "text": " is for critical changes, like I want to assign a label, I want to send someone from the tech", "tokens": [307, 337, 4924, 2962, 11, 411, 286, 528, 281, 6269, 257, 7645, 11, 286, 528, 281, 2845, 1580, 490, 264, 7553], "temperature": 0.0, "avg_logprob": -0.2023970402708841, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0006278605433180928}, {"id": 175, "seek": 129516, "start": 1304.96, "end": 1310.28, "text": " list to review it, and I want to notify join, which is the tech architecture. Okay, we", "tokens": [1329, 281, 3131, 309, 11, 293, 286, 528, 281, 36560, 3917, 11, 597, 307, 264, 7553, 9482, 13, 1033, 11, 321], "temperature": 0.0, "avg_logprob": -0.2023970402708841, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0006278605433180928}, {"id": 176, "seek": 129516, "start": 1310.28, "end": 1316.0, "text": " had a talk this morning about reducing cognitive load from Federic, and I want to show how", "tokens": [632, 257, 751, 341, 2446, 466, 12245, 15605, 3677, 490, 45545, 299, 11, 293, 286, 528, 281, 855, 577], "temperature": 0.0, "avg_logprob": -0.2023970402708841, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0006278605433180928}, {"id": 177, "seek": 129516, "start": 1316.0, "end": 1321.44, "text": " we could do that with this terminology. So here's how we could look into line of sign", "tokens": [321, 727, 360, 300, 365, 341, 27575, 13, 407, 510, 311, 577, 321, 727, 574, 666, 1622, 295, 1465], "temperature": 0.0, "avg_logprob": -0.2023970402708841, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0006278605433180928}, {"id": 178, "seek": 132144, "start": 1321.44, "end": 1327.76, "text": " and make sure that if someone uses a lot of tabs, so it means that we have a lot of loops", "tokens": [293, 652, 988, 300, 498, 1580, 4960, 257, 688, 295, 20743, 11, 370, 309, 1355, 300, 321, 362, 257, 688, 295, 16121], "temperature": 0.0, "avg_logprob": -0.16900015390047463, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0009675269247964025}, {"id": 179, "seek": 132144, "start": 1327.76, "end": 1333.6000000000001, "text": " between each other, if and else, we can actually send a warning to the user. For instance,", "tokens": [1296, 1184, 661, 11, 498, 293, 1646, 11, 321, 393, 767, 2845, 257, 9164, 281, 264, 4195, 13, 1171, 5197, 11], "temperature": 0.0, "avg_logprob": -0.16900015390047463, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0009675269247964025}, {"id": 180, "seek": 132144, "start": 1333.6000000000001, "end": 1338.4, "text": " our error validation, making sure that they don't use string contains for errors or equals,", "tokens": [527, 6713, 24071, 11, 1455, 988, 300, 436, 500, 380, 764, 6798, 8306, 337, 13603, 420, 6915, 11], "temperature": 0.0, "avg_logprob": -0.16900015390047463, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0009675269247964025}, {"id": 181, "seek": 132144, "start": 1338.4, "end": 1345.4, "text": " but they use error is. And last one, the mysterious Boolean, making sure that no more than one", "tokens": [457, 436, 764, 6713, 307, 13, 400, 1036, 472, 11, 264, 13831, 23351, 28499, 11, 1455, 988, 300, 572, 544, 813, 472], "temperature": 0.0, "avg_logprob": -0.16900015390047463, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0009675269247964025}, {"id": 182, "seek": 134540, "start": 1345.4, "end": 1352.2, "text": " Boolean is used in the function signature, that's pretty much it, how we could use to", "tokens": [23351, 28499, 307, 1143, 294, 264, 2445, 13397, 11, 300, 311, 1238, 709, 309, 11, 577, 321, 727, 764, 281], "temperature": 0.0, "avg_logprob": -0.23393174971657238, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0014937574742361903}, {"id": 183, "seek": 134540, "start": 1352.2, "end": 1359.96, "text": " make our lives easier on pull requests. Thank you all.", "tokens": [652, 527, 2909, 3571, 322, 2235, 12475, 13, 1044, 291, 439, 13], "temperature": 0.0, "avg_logprob": -0.23393174971657238, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0014937574742361903}, {"id": 184, "seek": 134540, "start": 1359.96, "end": 1367.8000000000002, "text": " Thank you. The last lightning talk of the day is from me again. What do we want to talk", "tokens": [1044, 291, 13, 440, 1036, 16589, 751, 295, 264, 786, 307, 490, 385, 797, 13, 708, 360, 321, 528, 281, 751], "temperature": 0.0, "avg_logprob": -0.23393174971657238, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0014937574742361903}, {"id": 185, "seek": 134540, "start": 1367.8000000000002, "end": 1373.52, "text": " about today? Well, two subjects, what is naming God? No, I want to talk to you first of all", "tokens": [466, 965, 30, 1042, 11, 732, 13066, 11, 437, 307, 25290, 1265, 30, 883, 11, 286, 528, 281, 751, 281, 291, 700, 295, 439], "temperature": 0.0, "avg_logprob": -0.23393174971657238, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.0014937574742361903}, {"id": 186, "seek": 137352, "start": 1373.52, "end": 1379.4, "text": " a big thank you again to everyone. First of all, to all speakers who came here today to", "tokens": [257, 955, 1309, 291, 797, 281, 1518, 13, 2386, 295, 439, 11, 281, 439, 9518, 567, 1361, 510, 965, 281], "temperature": 0.0, "avg_logprob": -0.16261462962373774, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0034038645680993795}, {"id": 187, "seek": 137352, "start": 1379.4, "end": 1385.28, "text": " give an amazing talk, standing with a lot of stress to say things. I also want to thank", "tokens": [976, 364, 2243, 751, 11, 4877, 365, 257, 688, 295, 4244, 281, 584, 721, 13, 286, 611, 528, 281, 1309], "temperature": 0.0, "avg_logprob": -0.16261462962373774, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0034038645680993795}, {"id": 188, "seek": 137352, "start": 1385.28, "end": 1391.84, "text": " Eva again for helping me out. I also want to thank the two FOSDEM engineers in the back", "tokens": [29377, 797, 337, 4315, 385, 484, 13, 286, 611, 528, 281, 1309, 264, 732, 479, 4367, 35, 6683, 11955, 294, 264, 646], "temperature": 0.0, "avg_logprob": -0.16261462962373774, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0034038645680993795}, {"id": 189, "seek": 137352, "start": 1391.84, "end": 1400.4, "text": " who made our audio video work all day. I want to thank the people from FOSDEM who brought", "tokens": [567, 1027, 527, 6278, 960, 589, 439, 786, 13, 286, 528, 281, 1309, 264, 561, 490, 479, 4367, 35, 6683, 567, 3038], "temperature": 0.0, "avg_logprob": -0.16261462962373774, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0034038645680993795}, {"id": 190, "seek": 140040, "start": 1400.4, "end": 1408.5600000000002, "text": " me food today. I also want to thank everybody at FOSDEM. And I also want to thank all the", "tokens": [385, 1755, 965, 13, 286, 611, 528, 281, 1309, 2201, 412, 479, 4367, 35, 6683, 13, 400, 286, 611, 528, 281, 1309, 439, 264], "temperature": 0.0, "avg_logprob": -0.23041274096514727, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0031904405914247036}, {"id": 191, "seek": 140040, "start": 1408.5600000000002, "end": 1420.3200000000002, "text": " volunteers. I think they are left right now. Who helped us with video, even what they couldn't", "tokens": [14352, 13, 286, 519, 436, 366, 1411, 558, 586, 13, 2102, 4254, 505, 365, 960, 11, 754, 437, 436, 2809, 380], "temperature": 0.0, "avg_logprob": -0.23041274096514727, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0031904405914247036}, {"id": 192, "seek": 140040, "start": 1420.3200000000002, "end": 1425.92, "text": " solve today. Thank you very much. Thank you all for coming, by the way. Thank you for", "tokens": [5039, 965, 13, 1044, 291, 588, 709, 13, 1044, 291, 439, 337, 1348, 11, 538, 264, 636, 13, 1044, 291, 337], "temperature": 0.0, "avg_logprob": -0.23041274096514727, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0031904405914247036}, {"id": 193, "seek": 142592, "start": 1425.92, "end": 1435.92, "text": " staying so late. Thank you. And now my second subject. Which is that Go is a garbage collected", "tokens": [7939, 370, 3469, 13, 1044, 291, 13, 400, 586, 452, 1150, 3983, 13, 3013, 307, 300, 1037, 307, 257, 14150, 11087], "temperature": 0.0, "avg_logprob": -0.15895961486187177, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0016367507632821798}, {"id": 194, "seek": 142592, "start": 1435.92, "end": 1441.88, "text": " language. And you know you can trigger the garbage collection by doing runtime.gc. So", "tokens": [2856, 13, 400, 291, 458, 291, 393, 7875, 264, 14150, 5765, 538, 884, 34474, 13, 70, 66, 13, 407], "temperature": 0.0, "avg_logprob": -0.15895961486187177, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0016367507632821798}, {"id": 195, "seek": 142592, "start": 1441.88, "end": 1449.24, "text": " when the time is 19 o'clock, I want you all to do runtime.gc and grab some waste you see", "tokens": [562, 264, 565, 307, 1294, 277, 6, 9023, 11, 286, 528, 291, 439, 281, 360, 34474, 13, 70, 66, 293, 4444, 512, 5964, 291, 536], "temperature": 0.0, "avg_logprob": -0.15895961486187177, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0016367507632821798}, {"id": 196, "seek": 142592, "start": 1449.24, "end": 1454.72, "text": " around it and put it in any of our bins. But I think Eva wants to say something. Yes.", "tokens": [926, 309, 293, 829, 309, 294, 604, 295, 527, 41275, 13, 583, 286, 519, 29377, 2738, 281, 584, 746, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.15895961486187177, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0016367507632821798}, {"id": 197, "seek": 145472, "start": 1454.72, "end": 1460.32, "text": " Thank you. Thank everyone that has been here to help you. But without you this wasn't possible.", "tokens": [1044, 291, 13, 1044, 1518, 300, 575, 668, 510, 281, 854, 291, 13, 583, 1553, 291, 341, 2067, 380, 1944, 13], "temperature": 0.0, "avg_logprob": -0.25181735080221423, "compression_ratio": 1.35, "no_speech_prob": 0.00695132277905941}, {"id": 198, "seek": 145472, "start": 1460.32, "end": 1471.2, "text": " So a big thank you to Marcia. And thank you for coming.", "tokens": [407, 257, 955, 1309, 291, 281, 2039, 2755, 13, 400, 1309, 291, 337, 1348, 13], "temperature": 0.0, "avg_logprob": -0.25181735080221423, "compression_ratio": 1.35, "no_speech_prob": 0.00695132277905941}, {"id": 199, "seek": 147120, "start": 1471.2, "end": 1488.24, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51216], "temperature": 0.0, "avg_logprob": -0.8807039260864258, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0006237231427803636}], "language": "en"}