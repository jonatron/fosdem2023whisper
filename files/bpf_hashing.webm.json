{"text": " All right, so let's get started again. The next talk is by Anton about optimizing BPF hash map and friends. Hello, I'm Anton, working at Datapart team at Isovalent and yeah, this is talk about how simple it is to optimize BPF hash map. So about a year ago, like a little less, Andreina Kricker proposed, suggested to try new hash functions for BPF hash map and BPF tech trace map. And in Selume we use, and in Tatragon we use hashes extensively, so for us it's a big deal and I decided to give it a try. So I will briefly provide some like benchmarking how to one-on-one and then we will take a look at different hash functions and then we will see benchmarks for hash maps which utilize old hash functions and new hash functions. So first thing to do when your benchmark is to try to reduce noise because modern CPUs do everything to ruin your benchmarking. They will run on different frequency, hyperthreading will get in and in the best case you benchmark inside kernel because you can disable preemption and interrupts. So benchmark, we take, if you want to benchmark some function, we first measure some kind of time source or clock source and then we execute our function, maybe in a loop and then we measure time once again and then we divide the number of observations by the number of loops and get our result. So in some cases we can't execute our function in a loop. For example, if it's not like an abstract hash function, it's just, if it's just part of kernel then n is obviously equal to 1 and we need to take an account the time it takes to get time and one obvious way to do this is to benchmark an empty loop and this give us roughly how long the get time function works and typically people benchmark with something like rdtc instruction and if you don't reduce noise then rdtc instruction is pretty unstable. So here my CPU obviously runs on two different frequencies and if my function which I want to benchmark runs 30, 40 cycles then deviation in this case is bigger than function itself so I can't rely on it. So if we disable like reduce noise as I said then results become way more stable so here it's like maybe it looks scary but it's actually like 37 plus minus one cycle so even for very small functions this makes benchmarking more reliable but in this form rdtc doesn't work because it's not a serializing instruction which means that if you insert your code here then it can be executed in the middle of code and even after your code so and it can differ from execution to execution and so we need some way to serialize it and luckily there are no ways to do this so just serialize it and there is a white paper written maybe ten years ago about titanium and it's still valid with some changes from architecture to architecture but yeah we can do this. In this case benchmarking like the offset takes a little bit more like it's not 37 cycles anymore it's 70 cycles but again it's pretty stable and we can use this number to offset our measurements and in fact I did such benchmarking when I don't run in a loop it's then switch it to like more dumb benchmarks when you do loops over bpf maps because it's harder to port things and if you want someone else to try these benchmarks they will have to patch their kernel and this is not simple. So let's take a look at several hash functions of interest so jhash is currently used hash function in the bpf it's Bob Jenkins hash and it's probably was developed some 30 years ago so spooky hash is another it's a newer version it's not not a newer version it's a newer hash from Bob Jenkins and then there goes xxhashes xxh32n64 it's a previous generation of these functions they are available in kernel and we can try them as well and xxh3 is like the state of art hash function. So if we just take a look at this plot the orange line which goes there is jhash which is currently used the green line which looks to be winning here is like the previous generation xxh64 the blue line this is the newest generation xxh3 and while it looks here that it doesn't perform as well as xxh64 for small keys it does outperform it and for bpf hash maps we primarily interested in using it for small keys like I don't know like actually I never use it like huge keys and in any case this like xxh3 works faster than Jenkins hash and later I will show that it can be actually run even more faster but one interesting thing is that the spooky hash it actually like it it performs pretty bad for small keys because it has a lot of like setup which it does in any case but later it starts outperform like every hash function I was interested when it does it so it does it at about key size of 9000 it's cool but it's not the key size of interest for us. So if we take a look at xxh3 and jhash we can see that the blue line xxh3 it actually outperforms like jhash for all key sizes and there is also this green line it's jhash2 it's optimized version of jhash which can be used if your key size is multiple of 4 and it's actually used in bloom filters but for some reason not in hash maps. So if we take closer look at small key sizes we see that yes xxh3 outperforms jhash so for me it's enough reason to try to benchmark maps with it and let's take a look now at BPF maps which use hash functions so first one is stack trace map and then hash map in bloom filters. So stack trace was actually the main reason for Andrey to propose xxh3 because what it does it takes a stack trace and then it hashes it and creates a map of IDs which refers to this traces and if there are hash collisions then old stack traces are lost and we get incorrect picture about the system and stack traces is not too random so if your hash function is not very good if it doesn't have very good like avalanche properties then it will create more collisions for less random data and xxh3 behaves way better than jhash for avalanche properties and this is like one of reasons and the main reason to use it for stack trace. The other reason as a benchmark that's also like for stack trace it also runs about twice faster for typical key sizes because typical key size it's like 8 bytes per stack depth and this is typically like 60, 80, 100 bytes so xxh3 is like is a very good candidate here so for hash benchmarks I was primarily focused on lookups because this is what we do the most and this is the thing which like is easy to measure compared to like more complex pictures so there are some links to benchmarks I used and scripts to actually execute benchmark and plot it because like for every change I had to draw some like 100 or 150 pictures for different key sizes for different fullness of hash maps so it's impossible to do otherwise like and I had a few pictures here so if we just use xxh3 then it looks like it like the new map which is orange it outperforms the original map which is blue and here is lookup speed in cycles vertical and horizontal axis is a key size so the bigger the key the better the more gain from using the new hash function but if we take like a bigger map I see that xxh3 as it is degrades for key size 4 and this is already like for me it's a blocker I can't like propose a change which degrades existing applications so then I went to a different architecture micro architecture and here I saw that it degrades for different key size like here it degrades for key size 12 and if you if we take like a bigger map it degrades for like key size 24 and then I thought how to fix this because it's if it works for bigger keys then maybe I can utilize this and I did the same thing as bloom bloom filter currently does so bloom filter executes jh2 for key sizes divisible by 4 and it uses jhash in other cases so I did the same utilize jhash 2 for key sizes of which are divisible by 4 but for small ones like it's this keyln divided by 4 keyln 32 it's actually computed it's just keyln divided by 4 but it's computed during hash initialization and we can decide for which key sizes we do this and with this hash function I finally see that it doesn't degrade anywhere so this is like 10k 100k and 100% full which is like the worst case and if we take another slice this is 100k 100k map with key size 8 and on the left side it's almost empty on the right side it's 100% full and the bigger key size the bigger gain for particular key size so for key size 64 new like map with new hash function runs about 50% faster and for key size 128 it runs almost twice faster and bloom filters as I mentioned they use the jhash 2 for keys divisible by 4 so I don't expect any gain for keys divisible by 4 at least for small keys and it looks like this so this is like an extreme case of bloom filter with 9 hashes but and I just did it so it reproduces the plot of hash function here and yes for small keys it is the same and for bigger keys we have a gain and here is the key size 240 where xh3 function originally utilizes vector instructions and we can't use obviously vector instructions in BPF maps and for key size 240 it's like it is expected to start using vector instructions but there is also scalar implementation which works faster than jhash but it degrades at this point so and another thing to mention that old hash functions jhash xxh64 they were designed and optimized it with O2 option in mind so if we switch to O3 then they will behave the same but xxh3 actually runs like 50-60% faster so it actually performs way better with O3 so I just jump like here so and I know that like O3 is no go for kernel like there were several attempts to introduce it and the reason was that there are no candidates which benefit from this O3 but this one is a particular candidate like hash if we could use O3 for hash map because not only for xxh3 because it should be inline then in this case we would be able to get rid of this composite hash which mixes hashes and just use it as is so yeah as I said for stuck trace map it definitely makes sense to use it so there is both benefit in speed small one because stuck trace map the bottleneck for speed is not the hash but the bottleneck for hash collisions is the hash and for hash map it's a question maybe someone would advise me on what to do with O3 and after I run like benchmarks on slightly bigger number of architectures then I think this is also like a good candidate to use in the hash map so here are some links for benchmarks and paper which I use it for those who will be reading this and thank you all right thanks a lot any questions you for the O3 thing can you only compile maybe the like the hash map file with O3 yeah yeah if it is like currently it is disabled like for every file in kernel for custom build we can enable it but like generally we just pass O2 everywhere if it's possible just to compile BPF maps with O3 this will solve the thing yes it's it's not such a big change so you don't have to compile the whole kernel with O3 yeah yeah yeah it's local code okay any other questions no then thanks a lot you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.2, "text": " All right, so let's get started again.", "tokens": [1057, 558, 11, 370, 718, 311, 483, 1409, 797, 13], "temperature": 0.0, "avg_logprob": -0.4204527537027995, "compression_ratio": 1.2972972972972974, "no_speech_prob": 0.538371205329895}, {"id": 1, "seek": 0, "start": 16.2, "end": 22.04, "text": " The next talk is by Anton about optimizing BPF hash map and friends.", "tokens": [440, 958, 751, 307, 538, 15291, 466, 40425, 40533, 37, 22019, 4471, 293, 1855, 13], "temperature": 0.0, "avg_logprob": -0.4204527537027995, "compression_ratio": 1.2972972972972974, "no_speech_prob": 0.538371205329895}, {"id": 2, "seek": 0, "start": 22.04, "end": 29.96, "text": " Hello, I'm Anton, working at Datapart team at Isovalent and yeah, this is talk about", "tokens": [2425, 11, 286, 478, 15291, 11, 1364, 412, 9315, 569, 446, 1469, 412, 286, 539, 3337, 317, 293, 1338, 11, 341, 307, 751, 466], "temperature": 0.0, "avg_logprob": -0.4204527537027995, "compression_ratio": 1.2972972972972974, "no_speech_prob": 0.538371205329895}, {"id": 3, "seek": 2996, "start": 29.96, "end": 35.92, "text": " how simple it is to optimize BPF hash map.", "tokens": [577, 2199, 309, 307, 281, 19719, 40533, 37, 22019, 4471, 13], "temperature": 0.0, "avg_logprob": -0.36728809497974535, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.0007514897733926773}, {"id": 4, "seek": 2996, "start": 35.92, "end": 44.68, "text": " So about a year ago, like a little less, Andreina Kricker proposed, suggested to try new hash", "tokens": [407, 466, 257, 1064, 2057, 11, 411, 257, 707, 1570, 11, 20667, 1426, 6332, 33804, 10348, 11, 10945, 281, 853, 777, 22019], "temperature": 0.0, "avg_logprob": -0.36728809497974535, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.0007514897733926773}, {"id": 5, "seek": 2996, "start": 44.68, "end": 51.92, "text": " functions for BPF hash map and BPF tech trace map.", "tokens": [6828, 337, 40533, 37, 22019, 4471, 293, 40533, 37, 7553, 13508, 4471, 13], "temperature": 0.0, "avg_logprob": -0.36728809497974535, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.0007514897733926773}, {"id": 6, "seek": 5192, "start": 51.92, "end": 60.160000000000004, "text": " And in Selume we use, and in Tatragon we use hashes extensively, so for us it's a big", "tokens": [400, 294, 10736, 2540, 321, 764, 11, 293, 294, 19645, 25997, 321, 764, 575, 8076, 32636, 11, 370, 337, 505, 309, 311, 257, 955], "temperature": 0.0, "avg_logprob": -0.29357656836509705, "compression_ratio": 1.3831168831168832, "no_speech_prob": 0.0004695600364357233}, {"id": 7, "seek": 5192, "start": 60.160000000000004, "end": 64.12, "text": " deal and I decided to give it a try.", "tokens": [2028, 293, 286, 3047, 281, 976, 309, 257, 853, 13], "temperature": 0.0, "avg_logprob": -0.29357656836509705, "compression_ratio": 1.3831168831168832, "no_speech_prob": 0.0004695600364357233}, {"id": 8, "seek": 5192, "start": 64.12, "end": 72.36, "text": " So I will briefly provide some like benchmarking how to one-on-one and then we will take a", "tokens": [407, 286, 486, 10515, 2893, 512, 411, 18927, 278, 577, 281, 472, 12, 266, 12, 546, 293, 550, 321, 486, 747, 257], "temperature": 0.0, "avg_logprob": -0.29357656836509705, "compression_ratio": 1.3831168831168832, "no_speech_prob": 0.0004695600364357233}, {"id": 9, "seek": 7236, "start": 72.36, "end": 82.6, "text": " look at different hash functions and then we will see benchmarks for hash maps which utilize", "tokens": [574, 412, 819, 22019, 6828, 293, 550, 321, 486, 536, 43751, 337, 22019, 11317, 597, 16117], "temperature": 0.0, "avg_logprob": -0.157961908976237, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.00019199623784516007}, {"id": 10, "seek": 7236, "start": 82.6, "end": 84.96, "text": " old hash functions and new hash functions.", "tokens": [1331, 22019, 6828, 293, 777, 22019, 6828, 13], "temperature": 0.0, "avg_logprob": -0.157961908976237, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.00019199623784516007}, {"id": 11, "seek": 7236, "start": 84.96, "end": 94.0, "text": " So first thing to do when your benchmark is to try to reduce noise because modern CPUs", "tokens": [407, 700, 551, 281, 360, 562, 428, 18927, 307, 281, 853, 281, 5407, 5658, 570, 4363, 13199, 82], "temperature": 0.0, "avg_logprob": -0.157961908976237, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.00019199623784516007}, {"id": 12, "seek": 7236, "start": 94.0, "end": 96.4, "text": " do everything to ruin your benchmarking.", "tokens": [360, 1203, 281, 15514, 428, 18927, 278, 13], "temperature": 0.0, "avg_logprob": -0.157961908976237, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.00019199623784516007}, {"id": 13, "seek": 9640, "start": 96.4, "end": 106.12, "text": " They will run on different frequency, hyperthreading will get in and in the best case you benchmark", "tokens": [814, 486, 1190, 322, 819, 7893, 11, 9848, 392, 35908, 486, 483, 294, 293, 294, 264, 1151, 1389, 291, 18927], "temperature": 0.0, "avg_logprob": -0.16462519199033326, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.00014852614549454302}, {"id": 14, "seek": 9640, "start": 106.12, "end": 110.16000000000001, "text": " inside kernel because you can disable preemption and interrupts.", "tokens": [1854, 28256, 570, 291, 393, 28362, 659, 26033, 293, 12729, 82, 13], "temperature": 0.0, "avg_logprob": -0.16462519199033326, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.00014852614549454302}, {"id": 15, "seek": 9640, "start": 110.16000000000001, "end": 118.48, "text": " So benchmark, we take, if you want to benchmark some function, we first measure some kind", "tokens": [407, 18927, 11, 321, 747, 11, 498, 291, 528, 281, 18927, 512, 2445, 11, 321, 700, 3481, 512, 733], "temperature": 0.0, "avg_logprob": -0.16462519199033326, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.00014852614549454302}, {"id": 16, "seek": 9640, "start": 118.48, "end": 123.56, "text": " of time source or clock source and then we execute our function, maybe in a loop and", "tokens": [295, 565, 4009, 420, 7830, 4009, 293, 550, 321, 14483, 527, 2445, 11, 1310, 294, 257, 6367, 293], "temperature": 0.0, "avg_logprob": -0.16462519199033326, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.00014852614549454302}, {"id": 17, "seek": 12356, "start": 123.56, "end": 132.08, "text": " then we measure time once again and then we divide the number of observations by the number", "tokens": [550, 321, 3481, 565, 1564, 797, 293, 550, 321, 9845, 264, 1230, 295, 18163, 538, 264, 1230], "temperature": 0.0, "avg_logprob": -0.13047074235003928, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0001763322070473805}, {"id": 18, "seek": 12356, "start": 132.08, "end": 135.16, "text": " of loops and get our result.", "tokens": [295, 16121, 293, 483, 527, 1874, 13], "temperature": 0.0, "avg_logprob": -0.13047074235003928, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0001763322070473805}, {"id": 19, "seek": 12356, "start": 135.16, "end": 139.24, "text": " So in some cases we can't execute our function in a loop.", "tokens": [407, 294, 512, 3331, 321, 393, 380, 14483, 527, 2445, 294, 257, 6367, 13], "temperature": 0.0, "avg_logprob": -0.13047074235003928, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0001763322070473805}, {"id": 20, "seek": 12356, "start": 139.24, "end": 143.56, "text": " For example, if it's not like an abstract hash function, it's just, if it's just part", "tokens": [1171, 1365, 11, 498, 309, 311, 406, 411, 364, 12649, 22019, 2445, 11, 309, 311, 445, 11, 498, 309, 311, 445, 644], "temperature": 0.0, "avg_logprob": -0.13047074235003928, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0001763322070473805}, {"id": 21, "seek": 12356, "start": 143.56, "end": 150.44, "text": " of kernel then n is obviously equal to 1 and we need to take an account the time it takes", "tokens": [295, 28256, 550, 297, 307, 2745, 2681, 281, 502, 293, 321, 643, 281, 747, 364, 2696, 264, 565, 309, 2516], "temperature": 0.0, "avg_logprob": -0.13047074235003928, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0001763322070473805}, {"id": 22, "seek": 15044, "start": 150.44, "end": 160.76, "text": " to get time and one obvious way to do this is to benchmark an empty loop and this give", "tokens": [281, 483, 565, 293, 472, 6322, 636, 281, 360, 341, 307, 281, 18927, 364, 6707, 6367, 293, 341, 976], "temperature": 0.0, "avg_logprob": -0.16065292358398436, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.00015386340965051204}, {"id": 23, "seek": 15044, "start": 160.76, "end": 168.04, "text": " us roughly how long the get time function works and typically people benchmark with", "tokens": [505, 9810, 577, 938, 264, 483, 565, 2445, 1985, 293, 5850, 561, 18927, 365], "temperature": 0.0, "avg_logprob": -0.16065292358398436, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.00015386340965051204}, {"id": 24, "seek": 15044, "start": 168.04, "end": 174.44, "text": " something like rdtc instruction and if you don't reduce noise then rdtc instruction", "tokens": [746, 411, 367, 39488, 66, 10951, 293, 498, 291, 500, 380, 5407, 5658, 550, 367, 39488, 66, 10951], "temperature": 0.0, "avg_logprob": -0.16065292358398436, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.00015386340965051204}, {"id": 25, "seek": 15044, "start": 174.44, "end": 176.4, "text": " is pretty unstable.", "tokens": [307, 1238, 23742, 13], "temperature": 0.0, "avg_logprob": -0.16065292358398436, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.00015386340965051204}, {"id": 26, "seek": 17640, "start": 176.4, "end": 183.88, "text": " So here my CPU obviously runs on two different frequencies and if my function which I want", "tokens": [407, 510, 452, 13199, 2745, 6676, 322, 732, 819, 20250, 293, 498, 452, 2445, 597, 286, 528], "temperature": 0.0, "avg_logprob": -0.18927820205688475, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.00011554628144949675}, {"id": 27, "seek": 17640, "start": 183.88, "end": 198.08, "text": " to benchmark runs 30, 40 cycles then deviation in this case is bigger than function itself", "tokens": [281, 18927, 6676, 2217, 11, 3356, 17796, 550, 25163, 294, 341, 1389, 307, 3801, 813, 2445, 2564], "temperature": 0.0, "avg_logprob": -0.18927820205688475, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.00011554628144949675}, {"id": 28, "seek": 17640, "start": 198.08, "end": 199.12, "text": " so I can't rely on it.", "tokens": [370, 286, 393, 380, 10687, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.18927820205688475, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.00011554628144949675}, {"id": 29, "seek": 19912, "start": 199.12, "end": 206.68, "text": " So if we disable like reduce noise as I said then results become way more stable so here", "tokens": [407, 498, 321, 28362, 411, 5407, 5658, 382, 286, 848, 550, 3542, 1813, 636, 544, 8351, 370, 510], "temperature": 0.0, "avg_logprob": -0.13327629225594656, "compression_ratio": 1.6188340807174888, "no_speech_prob": 6.033157842466608e-05}, {"id": 30, "seek": 19912, "start": 206.68, "end": 213.52, "text": " it's like maybe it looks scary but it's actually like 37 plus minus one cycle so even for very", "tokens": [309, 311, 411, 1310, 309, 1542, 6958, 457, 309, 311, 767, 411, 13435, 1804, 3175, 472, 6586, 370, 754, 337, 588], "temperature": 0.0, "avg_logprob": -0.13327629225594656, "compression_ratio": 1.6188340807174888, "no_speech_prob": 6.033157842466608e-05}, {"id": 31, "seek": 19912, "start": 213.52, "end": 221.84, "text": " small functions this makes benchmarking more reliable but in this form rdtc doesn't work", "tokens": [1359, 6828, 341, 1669, 18927, 278, 544, 12924, 457, 294, 341, 1254, 367, 39488, 66, 1177, 380, 589], "temperature": 0.0, "avg_logprob": -0.13327629225594656, "compression_ratio": 1.6188340807174888, "no_speech_prob": 6.033157842466608e-05}, {"id": 32, "seek": 19912, "start": 221.84, "end": 227.28, "text": " because it's not a serializing instruction which means that if you insert your code here", "tokens": [570, 309, 311, 406, 257, 17436, 3319, 10951, 597, 1355, 300, 498, 291, 8969, 428, 3089, 510], "temperature": 0.0, "avg_logprob": -0.13327629225594656, "compression_ratio": 1.6188340807174888, "no_speech_prob": 6.033157842466608e-05}, {"id": 33, "seek": 22728, "start": 227.28, "end": 235.04, "text": " then it can be executed in the middle of code and even after your code so and it can differ", "tokens": [550, 309, 393, 312, 17577, 294, 264, 2808, 295, 3089, 293, 754, 934, 428, 3089, 370, 293, 309, 393, 743], "temperature": 0.0, "avg_logprob": -0.15139914133462562, "compression_ratio": 1.8109452736318408, "no_speech_prob": 8.077231905190274e-05}, {"id": 34, "seek": 22728, "start": 235.04, "end": 243.6, "text": " from execution to execution and so we need some way to serialize it and luckily there", "tokens": [490, 15058, 281, 15058, 293, 370, 321, 643, 512, 636, 281, 17436, 1125, 309, 293, 22880, 456], "temperature": 0.0, "avg_logprob": -0.15139914133462562, "compression_ratio": 1.8109452736318408, "no_speech_prob": 8.077231905190274e-05}, {"id": 35, "seek": 22728, "start": 243.6, "end": 250.04, "text": " are no ways to do this so just serialize it and there is a white paper written maybe ten", "tokens": [366, 572, 2098, 281, 360, 341, 370, 445, 17436, 1125, 309, 293, 456, 307, 257, 2418, 3035, 3720, 1310, 2064], "temperature": 0.0, "avg_logprob": -0.15139914133462562, "compression_ratio": 1.8109452736318408, "no_speech_prob": 8.077231905190274e-05}, {"id": 36, "seek": 22728, "start": 250.04, "end": 256.96, "text": " years ago about titanium and it's still valid with some changes from architecture to architecture", "tokens": [924, 2057, 466, 35289, 293, 309, 311, 920, 7363, 365, 512, 2962, 490, 9482, 281, 9482], "temperature": 0.0, "avg_logprob": -0.15139914133462562, "compression_ratio": 1.8109452736318408, "no_speech_prob": 8.077231905190274e-05}, {"id": 37, "seek": 25696, "start": 256.96, "end": 259.82, "text": " but yeah we can do this.", "tokens": [457, 1338, 321, 393, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.12353168834339488, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001214059375342913}, {"id": 38, "seek": 25696, "start": 259.82, "end": 267.44, "text": " In this case benchmarking like the offset takes a little bit more like it's not 37 cycles", "tokens": [682, 341, 1389, 18927, 278, 411, 264, 18687, 2516, 257, 707, 857, 544, 411, 309, 311, 406, 13435, 17796], "temperature": 0.0, "avg_logprob": -0.12353168834339488, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001214059375342913}, {"id": 39, "seek": 25696, "start": 267.44, "end": 274.2, "text": " anymore it's 70 cycles but again it's pretty stable and we can use this number to offset", "tokens": [3602, 309, 311, 5285, 17796, 457, 797, 309, 311, 1238, 8351, 293, 321, 393, 764, 341, 1230, 281, 18687], "temperature": 0.0, "avg_logprob": -0.12353168834339488, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001214059375342913}, {"id": 40, "seek": 25696, "start": 274.2, "end": 281.32, "text": " our measurements and in fact I did such benchmarking when I don't run in a loop it's then switch", "tokens": [527, 15383, 293, 294, 1186, 286, 630, 1270, 18927, 278, 562, 286, 500, 380, 1190, 294, 257, 6367, 309, 311, 550, 3679], "temperature": 0.0, "avg_logprob": -0.12353168834339488, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001214059375342913}, {"id": 41, "seek": 28132, "start": 281.32, "end": 288.2, "text": " it to like more dumb benchmarks when you do loops over bpf maps because it's harder to", "tokens": [309, 281, 411, 544, 10316, 43751, 562, 291, 360, 16121, 670, 272, 25302, 11317, 570, 309, 311, 6081, 281], "temperature": 0.0, "avg_logprob": -0.14560949480211413, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.0002446237194817513}, {"id": 42, "seek": 28132, "start": 288.2, "end": 294.76, "text": " port things and if you want someone else to try these benchmarks they will have to patch", "tokens": [2436, 721, 293, 498, 291, 528, 1580, 1646, 281, 853, 613, 43751, 436, 486, 362, 281, 9972], "temperature": 0.0, "avg_logprob": -0.14560949480211413, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.0002446237194817513}, {"id": 43, "seek": 28132, "start": 294.76, "end": 297.96, "text": " their kernel and this is not simple.", "tokens": [641, 28256, 293, 341, 307, 406, 2199, 13], "temperature": 0.0, "avg_logprob": -0.14560949480211413, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.0002446237194817513}, {"id": 44, "seek": 28132, "start": 297.96, "end": 306.6, "text": " So let's take a look at several hash functions of interest so jhash is currently used hash", "tokens": [407, 718, 311, 747, 257, 574, 412, 2940, 22019, 6828, 295, 1179, 370, 361, 71, 1299, 307, 4362, 1143, 22019], "temperature": 0.0, "avg_logprob": -0.14560949480211413, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.0002446237194817513}, {"id": 45, "seek": 30660, "start": 306.6, "end": 314.92, "text": " function in the bpf it's Bob Jenkins hash and it's probably was developed some 30 years", "tokens": [2445, 294, 264, 272, 25302, 309, 311, 6085, 41273, 22019, 293, 309, 311, 1391, 390, 4743, 512, 2217, 924], "temperature": 0.0, "avg_logprob": -0.2457404931386312, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.00014782664948143065}, {"id": 46, "seek": 30660, "start": 314.92, "end": 321.6, "text": " ago so spooky hash is another it's a newer version it's not not a newer version it's", "tokens": [2057, 370, 30510, 22019, 307, 1071, 309, 311, 257, 17628, 3037, 309, 311, 406, 406, 257, 17628, 3037, 309, 311], "temperature": 0.0, "avg_logprob": -0.2457404931386312, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.00014782664948143065}, {"id": 47, "seek": 30660, "start": 321.6, "end": 332.72, "text": " a newer hash from Bob Jenkins and then there goes xxhashes xxh32n64 it's a previous generation", "tokens": [257, 17628, 22019, 490, 6085, 41273, 293, 550, 456, 1709, 2031, 87, 71, 12808, 2031, 87, 71, 11440, 77, 19395, 309, 311, 257, 3894, 5125], "temperature": 0.0, "avg_logprob": -0.2457404931386312, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.00014782664948143065}, {"id": 48, "seek": 33272, "start": 332.72, "end": 341.88000000000005, "text": " of these functions they are available in kernel and we can try them as well and xxh3 is like", "tokens": [295, 613, 6828, 436, 366, 2435, 294, 28256, 293, 321, 393, 853, 552, 382, 731, 293, 2031, 87, 71, 18, 307, 411], "temperature": 0.0, "avg_logprob": -0.13400974521389256, "compression_ratio": 1.6684782608695652, "no_speech_prob": 8.404418622376397e-05}, {"id": 49, "seek": 33272, "start": 341.88000000000005, "end": 345.16, "text": " the state of art hash function.", "tokens": [264, 1785, 295, 1523, 22019, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13400974521389256, "compression_ratio": 1.6684782608695652, "no_speech_prob": 8.404418622376397e-05}, {"id": 50, "seek": 33272, "start": 345.16, "end": 353.48, "text": " So if we just take a look at this plot the orange line which goes there is jhash which", "tokens": [407, 498, 321, 445, 747, 257, 574, 412, 341, 7542, 264, 7671, 1622, 597, 1709, 456, 307, 361, 71, 1299, 597], "temperature": 0.0, "avg_logprob": -0.13400974521389256, "compression_ratio": 1.6684782608695652, "no_speech_prob": 8.404418622376397e-05}, {"id": 51, "seek": 33272, "start": 353.48, "end": 360.88000000000005, "text": " is currently used the green line which looks to be winning here is like the previous generation", "tokens": [307, 4362, 1143, 264, 3092, 1622, 597, 1542, 281, 312, 8224, 510, 307, 411, 264, 3894, 5125], "temperature": 0.0, "avg_logprob": -0.13400974521389256, "compression_ratio": 1.6684782608695652, "no_speech_prob": 8.404418622376397e-05}, {"id": 52, "seek": 36088, "start": 360.88, "end": 373.04, "text": " xxh64 the blue line this is the newest generation xxh3 and while it looks here that it doesn't", "tokens": [2031, 87, 71, 19395, 264, 3344, 1622, 341, 307, 264, 17569, 5125, 2031, 87, 71, 18, 293, 1339, 309, 1542, 510, 300, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.11982720869558829, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.00020984398724976927}, {"id": 53, "seek": 36088, "start": 373.04, "end": 384.56, "text": " perform as well as xxh64 for small keys it does outperform it and for bpf hash maps we", "tokens": [2042, 382, 731, 382, 2031, 87, 71, 19395, 337, 1359, 9317, 309, 775, 484, 26765, 309, 293, 337, 272, 25302, 22019, 11317, 321], "temperature": 0.0, "avg_logprob": -0.11982720869558829, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.00020984398724976927}, {"id": 54, "seek": 38456, "start": 384.56, "end": 391.28000000000003, "text": " primarily interested in using it for small keys like I don't know like actually I never", "tokens": [10029, 3102, 294, 1228, 309, 337, 1359, 9317, 411, 286, 500, 380, 458, 411, 767, 286, 1128], "temperature": 0.0, "avg_logprob": -0.12383771524196718, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.00024022891011554748}, {"id": 55, "seek": 38456, "start": 391.28000000000003, "end": 402.04, "text": " use it like huge keys and in any case this like xxh3 works faster than Jenkins hash and", "tokens": [764, 309, 411, 2603, 9317, 293, 294, 604, 1389, 341, 411, 2031, 87, 71, 18, 1985, 4663, 813, 41273, 22019, 293], "temperature": 0.0, "avg_logprob": -0.12383771524196718, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.00024022891011554748}, {"id": 56, "seek": 38456, "start": 402.04, "end": 406.84000000000003, "text": " later I will show that it can be actually run even more faster but one interesting thing", "tokens": [1780, 286, 486, 855, 300, 309, 393, 312, 767, 1190, 754, 544, 4663, 457, 472, 1880, 551], "temperature": 0.0, "avg_logprob": -0.12383771524196718, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.00024022891011554748}, {"id": 57, "seek": 38456, "start": 406.84000000000003, "end": 413.12, "text": " is that the spooky hash it actually like it it performs pretty bad for small keys because", "tokens": [307, 300, 264, 30510, 22019, 309, 767, 411, 309, 309, 26213, 1238, 1578, 337, 1359, 9317, 570], "temperature": 0.0, "avg_logprob": -0.12383771524196718, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.00024022891011554748}, {"id": 58, "seek": 41312, "start": 413.12, "end": 418.64, "text": " it has a lot of like setup which it does in any case but later it starts outperform like", "tokens": [309, 575, 257, 688, 295, 411, 8657, 597, 309, 775, 294, 604, 1389, 457, 1780, 309, 3719, 484, 26765, 411], "temperature": 0.0, "avg_logprob": -0.12053978696782539, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.371407728991471e-05}, {"id": 59, "seek": 41312, "start": 418.64, "end": 423.64, "text": " every hash function I was interested when it does it so it does it at about key size", "tokens": [633, 22019, 2445, 286, 390, 3102, 562, 309, 775, 309, 370, 309, 775, 309, 412, 466, 2141, 2744], "temperature": 0.0, "avg_logprob": -0.12053978696782539, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.371407728991471e-05}, {"id": 60, "seek": 41312, "start": 423.64, "end": 432.0, "text": " of 9000 it's cool but it's not the key size of interest for us.", "tokens": [295, 1722, 1360, 309, 311, 1627, 457, 309, 311, 406, 264, 2141, 2744, 295, 1179, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.12053978696782539, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.371407728991471e-05}, {"id": 61, "seek": 41312, "start": 432.0, "end": 442.2, "text": " So if we take a look at xxh3 and jhash we can see that the blue line xxh3 it actually", "tokens": [407, 498, 321, 747, 257, 574, 412, 2031, 87, 71, 18, 293, 361, 71, 1299, 321, 393, 536, 300, 264, 3344, 1622, 2031, 87, 71, 18, 309, 767], "temperature": 0.0, "avg_logprob": -0.12053978696782539, "compression_ratio": 1.6069651741293531, "no_speech_prob": 3.371407728991471e-05}, {"id": 62, "seek": 44220, "start": 442.2, "end": 453.08, "text": " outperforms like jhash for all key sizes and there is also this green line it's jhash2", "tokens": [484, 26765, 82, 411, 361, 71, 1299, 337, 439, 2141, 11602, 293, 456, 307, 611, 341, 3092, 1622, 309, 311, 361, 71, 1299, 17], "temperature": 0.0, "avg_logprob": -0.15976010198178497, "compression_ratio": 1.55, "no_speech_prob": 5.551018693950027e-05}, {"id": 63, "seek": 44220, "start": 453.08, "end": 460.32, "text": " it's optimized version of jhash which can be used if your key size is multiple of 4", "tokens": [309, 311, 26941, 3037, 295, 361, 71, 1299, 597, 393, 312, 1143, 498, 428, 2141, 2744, 307, 3866, 295, 1017], "temperature": 0.0, "avg_logprob": -0.15976010198178497, "compression_ratio": 1.55, "no_speech_prob": 5.551018693950027e-05}, {"id": 64, "seek": 44220, "start": 460.32, "end": 465.36, "text": " and it's actually used in bloom filters but for some reason not in hash maps.", "tokens": [293, 309, 311, 767, 1143, 294, 26899, 15995, 457, 337, 512, 1778, 406, 294, 22019, 11317, 13], "temperature": 0.0, "avg_logprob": -0.15976010198178497, "compression_ratio": 1.55, "no_speech_prob": 5.551018693950027e-05}, {"id": 65, "seek": 46536, "start": 465.36, "end": 475.92, "text": " So if we take closer look at small key sizes we see that yes xxh3 outperforms jhash so", "tokens": [407, 498, 321, 747, 4966, 574, 412, 1359, 2141, 11602, 321, 536, 300, 2086, 2031, 87, 71, 18, 484, 26765, 82, 361, 71, 1299, 370], "temperature": 0.0, "avg_logprob": -0.14361069752619818, "compression_ratio": 1.3488372093023255, "no_speech_prob": 5.8661738876253366e-05}, {"id": 66, "seek": 46536, "start": 475.92, "end": 485.36, "text": " for me it's enough reason to try to benchmark maps with it and let's take a look now at", "tokens": [337, 385, 309, 311, 1547, 1778, 281, 853, 281, 18927, 11317, 365, 309, 293, 718, 311, 747, 257, 574, 586, 412], "temperature": 0.0, "avg_logprob": -0.14361069752619818, "compression_ratio": 1.3488372093023255, "no_speech_prob": 5.8661738876253366e-05}, {"id": 67, "seek": 48536, "start": 485.36, "end": 495.96000000000004, "text": " BPF maps which use hash functions so first one is stack trace map and then hash map in", "tokens": [40533, 37, 11317, 597, 764, 22019, 6828, 370, 700, 472, 307, 8630, 13508, 4471, 293, 550, 22019, 4471, 294], "temperature": 0.0, "avg_logprob": -0.23269292753036708, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.00011587983317440376}, {"id": 68, "seek": 48536, "start": 495.96000000000004, "end": 496.96000000000004, "text": " bloom filters.", "tokens": [26899, 15995, 13], "temperature": 0.0, "avg_logprob": -0.23269292753036708, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.00011587983317440376}, {"id": 69, "seek": 48536, "start": 496.96000000000004, "end": 505.28000000000003, "text": " So stack trace was actually the main reason for Andrey to propose xxh3 because what it", "tokens": [407, 8630, 13508, 390, 767, 264, 2135, 1778, 337, 400, 7950, 281, 17421, 2031, 87, 71, 18, 570, 437, 309], "temperature": 0.0, "avg_logprob": -0.23269292753036708, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.00011587983317440376}, {"id": 70, "seek": 48536, "start": 505.28000000000003, "end": 514.16, "text": " does it takes a stack trace and then it hashes it and creates a map of IDs which refers to", "tokens": [775, 309, 2516, 257, 8630, 13508, 293, 550, 309, 575, 8076, 309, 293, 7829, 257, 4471, 295, 48212, 597, 14942, 281], "temperature": 0.0, "avg_logprob": -0.23269292753036708, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.00011587983317440376}, {"id": 71, "seek": 51416, "start": 514.16, "end": 522.0799999999999, "text": " this traces and if there are hash collisions then old stack traces are lost and we get", "tokens": [341, 26076, 293, 498, 456, 366, 22019, 46537, 550, 1331, 8630, 26076, 366, 2731, 293, 321, 483], "temperature": 0.0, "avg_logprob": -0.161789063484438, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0004316986305639148}, {"id": 72, "seek": 51416, "start": 522.0799999999999, "end": 531.88, "text": " incorrect picture about the system and stack traces is not too random so if your hash function", "tokens": [18424, 3036, 466, 264, 1185, 293, 8630, 26076, 307, 406, 886, 4974, 370, 498, 428, 22019, 2445], "temperature": 0.0, "avg_logprob": -0.161789063484438, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0004316986305639148}, {"id": 73, "seek": 51416, "start": 531.88, "end": 539.28, "text": " is not very good if it doesn't have very good like avalanche properties then it will create", "tokens": [307, 406, 588, 665, 498, 309, 1177, 380, 362, 588, 665, 411, 1305, 14163, 1876, 7221, 550, 309, 486, 1884], "temperature": 0.0, "avg_logprob": -0.161789063484438, "compression_ratio": 1.6748466257668713, "no_speech_prob": 0.0004316986305639148}, {"id": 74, "seek": 53928, "start": 539.28, "end": 549.04, "text": " more collisions for less random data and xxh3 behaves way better than jhash for avalanche", "tokens": [544, 46537, 337, 1570, 4974, 1412, 293, 2031, 87, 71, 18, 36896, 636, 1101, 813, 361, 71, 1299, 337, 1305, 14163, 1876], "temperature": 0.0, "avg_logprob": -0.15382876325009473, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.00013123253302183002}, {"id": 75, "seek": 53928, "start": 549.04, "end": 557.8399999999999, "text": " properties and this is like one of reasons and the main reason to use it for stack trace.", "tokens": [7221, 293, 341, 307, 411, 472, 295, 4112, 293, 264, 2135, 1778, 281, 764, 309, 337, 8630, 13508, 13], "temperature": 0.0, "avg_logprob": -0.15382876325009473, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.00013123253302183002}, {"id": 76, "seek": 53928, "start": 557.8399999999999, "end": 563.16, "text": " The other reason as a benchmark that's also like for stack trace it also runs about twice", "tokens": [440, 661, 1778, 382, 257, 18927, 300, 311, 611, 411, 337, 8630, 13508, 309, 611, 6676, 466, 6091], "temperature": 0.0, "avg_logprob": -0.15382876325009473, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.00013123253302183002}, {"id": 77, "seek": 56316, "start": 563.16, "end": 570.0799999999999, "text": " faster for typical key sizes because typical key size it's like 8 bytes per stack depth", "tokens": [4663, 337, 7476, 2141, 11602, 570, 7476, 2141, 2744, 309, 311, 411, 1649, 36088, 680, 8630, 7161], "temperature": 0.0, "avg_logprob": -0.15361622198304134, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0009168469114229083}, {"id": 78, "seek": 56316, "start": 570.0799999999999, "end": 584.8399999999999, "text": " and this is typically like 60, 80, 100 bytes so xxh3 is like is a very good candidate here", "tokens": [293, 341, 307, 5850, 411, 4060, 11, 4688, 11, 2319, 36088, 370, 2031, 87, 71, 18, 307, 411, 307, 257, 588, 665, 11532, 510], "temperature": 0.0, "avg_logprob": -0.15361622198304134, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0009168469114229083}, {"id": 79, "seek": 56316, "start": 584.8399999999999, "end": 590.68, "text": " so for hash benchmarks I was primarily focused on lookups because this is what we do the", "tokens": [370, 337, 22019, 43751, 286, 390, 10029, 5178, 322, 574, 7528, 570, 341, 307, 437, 321, 360, 264], "temperature": 0.0, "avg_logprob": -0.15361622198304134, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0009168469114229083}, {"id": 80, "seek": 59068, "start": 590.68, "end": 600.4799999999999, "text": " most and this is the thing which like is easy to measure compared to like more complex pictures", "tokens": [881, 293, 341, 307, 264, 551, 597, 411, 307, 1858, 281, 3481, 5347, 281, 411, 544, 3997, 5242], "temperature": 0.0, "avg_logprob": -0.15314719615838465, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.00019072122813668102}, {"id": 81, "seek": 59068, "start": 600.4799999999999, "end": 605.1999999999999, "text": " so there are some links to benchmarks I used and scripts to actually execute benchmark", "tokens": [370, 456, 366, 512, 6123, 281, 43751, 286, 1143, 293, 23294, 281, 767, 14483, 18927], "temperature": 0.0, "avg_logprob": -0.15314719615838465, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.00019072122813668102}, {"id": 82, "seek": 59068, "start": 605.1999999999999, "end": 613.68, "text": " and plot it because like for every change I had to draw some like 100 or 150 pictures", "tokens": [293, 7542, 309, 570, 411, 337, 633, 1319, 286, 632, 281, 2642, 512, 411, 2319, 420, 8451, 5242], "temperature": 0.0, "avg_logprob": -0.15314719615838465, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.00019072122813668102}, {"id": 83, "seek": 59068, "start": 613.68, "end": 618.76, "text": " for different key sizes for different fullness of hash maps so it's impossible to do otherwise", "tokens": [337, 819, 2141, 11602, 337, 819, 45262, 295, 22019, 11317, 370, 309, 311, 6243, 281, 360, 5911], "temperature": 0.0, "avg_logprob": -0.15314719615838465, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.00019072122813668102}, {"id": 84, "seek": 61876, "start": 618.76, "end": 628.76, "text": " like and I had a few pictures here so if we just use xxh3 then it looks like it like the", "tokens": [411, 293, 286, 632, 257, 1326, 5242, 510, 370, 498, 321, 445, 764, 2031, 87, 71, 18, 550, 309, 1542, 411, 309, 411, 264], "temperature": 0.0, "avg_logprob": -0.12066054344177246, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.00013428089732769877}, {"id": 85, "seek": 61876, "start": 628.76, "end": 638.08, "text": " new map which is orange it outperforms the original map which is blue and here is lookup", "tokens": [777, 4471, 597, 307, 7671, 309, 484, 26765, 82, 264, 3380, 4471, 597, 307, 3344, 293, 510, 307, 574, 1010], "temperature": 0.0, "avg_logprob": -0.12066054344177246, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.00013428089732769877}, {"id": 86, "seek": 63808, "start": 638.08, "end": 648.96, "text": " speed in cycles vertical and horizontal axis is a key size so the bigger the key the better", "tokens": [3073, 294, 17796, 9429, 293, 12750, 10298, 307, 257, 2141, 2744, 370, 264, 3801, 264, 2141, 264, 1101], "temperature": 0.0, "avg_logprob": -0.13556712781879263, "compression_ratio": 1.5257142857142858, "no_speech_prob": 9.259575017495081e-05}, {"id": 87, "seek": 63808, "start": 648.96, "end": 655.36, "text": " the more gain from using the new hash function but if we take like a bigger map I see that", "tokens": [264, 544, 6052, 490, 1228, 264, 777, 22019, 2445, 457, 498, 321, 747, 411, 257, 3801, 4471, 286, 536, 300], "temperature": 0.0, "avg_logprob": -0.13556712781879263, "compression_ratio": 1.5257142857142858, "no_speech_prob": 9.259575017495081e-05}, {"id": 88, "seek": 63808, "start": 655.36, "end": 663.32, "text": " xxh3 as it is degrades for key size 4 and this is already like for me it's a blocker", "tokens": [2031, 87, 71, 18, 382, 309, 307, 368, 22626, 337, 2141, 2744, 1017, 293, 341, 307, 1217, 411, 337, 385, 309, 311, 257, 3461, 260], "temperature": 0.0, "avg_logprob": -0.13556712781879263, "compression_ratio": 1.5257142857142858, "no_speech_prob": 9.259575017495081e-05}, {"id": 89, "seek": 66332, "start": 663.32, "end": 672.0400000000001, "text": " I can't like propose a change which degrades existing applications so then I went to a", "tokens": [286, 393, 380, 411, 17421, 257, 1319, 597, 368, 22626, 6741, 5821, 370, 550, 286, 1437, 281, 257], "temperature": 0.0, "avg_logprob": -0.1574075261100394, "compression_ratio": 1.6624203821656052, "no_speech_prob": 4.767965947394259e-05}, {"id": 90, "seek": 66332, "start": 672.0400000000001, "end": 680.12, "text": " different architecture micro architecture and here I saw that it degrades for different", "tokens": [819, 9482, 4532, 9482, 293, 510, 286, 1866, 300, 309, 368, 22626, 337, 819], "temperature": 0.0, "avg_logprob": -0.1574075261100394, "compression_ratio": 1.6624203821656052, "no_speech_prob": 4.767965947394259e-05}, {"id": 91, "seek": 66332, "start": 680.12, "end": 687.12, "text": " key size like here it degrades for key size 12 and if you if we take like a bigger map", "tokens": [2141, 2744, 411, 510, 309, 368, 22626, 337, 2141, 2744, 2272, 293, 498, 291, 498, 321, 747, 411, 257, 3801, 4471], "temperature": 0.0, "avg_logprob": -0.1574075261100394, "compression_ratio": 1.6624203821656052, "no_speech_prob": 4.767965947394259e-05}, {"id": 92, "seek": 68712, "start": 687.12, "end": 697.96, "text": " it degrades for like key size 24 and then I thought how to fix this because it's if", "tokens": [309, 368, 22626, 337, 411, 2141, 2744, 4022, 293, 550, 286, 1194, 577, 281, 3191, 341, 570, 309, 311, 498], "temperature": 0.0, "avg_logprob": -0.17301635465760162, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00030739654903300107}, {"id": 93, "seek": 68712, "start": 697.96, "end": 704.28, "text": " it works for bigger keys then maybe I can utilize this and I did the same thing as bloom bloom", "tokens": [309, 1985, 337, 3801, 9317, 550, 1310, 286, 393, 16117, 341, 293, 286, 630, 264, 912, 551, 382, 26899, 26899], "temperature": 0.0, "avg_logprob": -0.17301635465760162, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00030739654903300107}, {"id": 94, "seek": 68712, "start": 704.28, "end": 711.44, "text": " filter currently does so bloom filter executes jh2 for key sizes divisible by 4 and it uses", "tokens": [6608, 4362, 775, 370, 26899, 6608, 4454, 1819, 361, 71, 17, 337, 2141, 11602, 25974, 964, 538, 1017, 293, 309, 4960], "temperature": 0.0, "avg_logprob": -0.17301635465760162, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00030739654903300107}, {"id": 95, "seek": 71144, "start": 711.44, "end": 719.12, "text": " jhash in other cases so I did the same utilize jhash 2 for key sizes of which are divisible", "tokens": [361, 71, 1299, 294, 661, 3331, 370, 286, 630, 264, 912, 16117, 361, 71, 1299, 568, 337, 2141, 11602, 295, 597, 366, 25974, 964], "temperature": 0.0, "avg_logprob": -0.19738919394356863, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0001911162253236398}, {"id": 96, "seek": 71144, "start": 719.12, "end": 730.84, "text": " by 4 but for small ones like it's this keyln divided by 4 keyln 32 it's actually computed", "tokens": [538, 1017, 457, 337, 1359, 2306, 411, 309, 311, 341, 2141, 75, 77, 6666, 538, 1017, 2141, 75, 77, 8858, 309, 311, 767, 40610], "temperature": 0.0, "avg_logprob": -0.19738919394356863, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0001911162253236398}, {"id": 97, "seek": 71144, "start": 730.84, "end": 737.8800000000001, "text": " it's just keyln divided by 4 but it's computed during hash initialization and we can decide", "tokens": [309, 311, 445, 2141, 75, 77, 6666, 538, 1017, 457, 309, 311, 40610, 1830, 22019, 5883, 2144, 293, 321, 393, 4536], "temperature": 0.0, "avg_logprob": -0.19738919394356863, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0001911162253236398}, {"id": 98, "seek": 73788, "start": 737.88, "end": 743.8, "text": " for which key sizes we do this and with this hash function I finally see that it doesn't", "tokens": [337, 597, 2141, 11602, 321, 360, 341, 293, 365, 341, 22019, 2445, 286, 2721, 536, 300, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.12383766174316406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00014091216144151986}, {"id": 99, "seek": 73788, "start": 743.8, "end": 753.08, "text": " degrade anywhere so this is like 10k 100k and 100% full which is like the worst case", "tokens": [368, 8692, 4992, 370, 341, 307, 411, 1266, 74, 2319, 74, 293, 2319, 4, 1577, 597, 307, 411, 264, 5855, 1389], "temperature": 0.0, "avg_logprob": -0.12383766174316406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00014091216144151986}, {"id": 100, "seek": 73788, "start": 753.08, "end": 762.4, "text": " and if we take another slice this is 100k 100k map with key size 8 and on the left side", "tokens": [293, 498, 321, 747, 1071, 13153, 341, 307, 2319, 74, 2319, 74, 4471, 365, 2141, 2744, 1649, 293, 322, 264, 1411, 1252], "temperature": 0.0, "avg_logprob": -0.12383766174316406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00014091216144151986}, {"id": 101, "seek": 76240, "start": 762.4, "end": 770.3199999999999, "text": " it's almost empty on the right side it's 100% full and the bigger key size the bigger gain", "tokens": [309, 311, 1920, 6707, 322, 264, 558, 1252, 309, 311, 2319, 4, 1577, 293, 264, 3801, 2141, 2744, 264, 3801, 6052], "temperature": 0.0, "avg_logprob": -0.0980750069473729, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00027800389216281474}, {"id": 102, "seek": 76240, "start": 770.3199999999999, "end": 778.4, "text": " for particular key size so for key size 64 new like map with new hash function runs about", "tokens": [337, 1729, 2141, 2744, 370, 337, 2141, 2744, 12145, 777, 411, 4471, 365, 777, 22019, 2445, 6676, 466], "temperature": 0.0, "avg_logprob": -0.0980750069473729, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00027800389216281474}, {"id": 103, "seek": 76240, "start": 778.4, "end": 787.04, "text": " 50% faster and for key size 128 it runs almost twice faster and bloom filters as I mentioned", "tokens": [2625, 4, 4663, 293, 337, 2141, 2744, 29810, 309, 6676, 1920, 6091, 4663, 293, 26899, 15995, 382, 286, 2835], "temperature": 0.0, "avg_logprob": -0.0980750069473729, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00027800389216281474}, {"id": 104, "seek": 78704, "start": 787.04, "end": 797.16, "text": " they use the jhash 2 for keys divisible by 4 so I don't expect any gain for keys divisible", "tokens": [436, 764, 264, 361, 71, 1299, 568, 337, 9317, 25974, 964, 538, 1017, 370, 286, 500, 380, 2066, 604, 6052, 337, 9317, 25974, 964], "temperature": 0.0, "avg_logprob": -0.09504173226552466, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.0003230123547837138}, {"id": 105, "seek": 78704, "start": 797.16, "end": 802.7199999999999, "text": " by 4 at least for small keys and it looks like this so this is like an extreme case", "tokens": [538, 1017, 412, 1935, 337, 1359, 9317, 293, 309, 1542, 411, 341, 370, 341, 307, 411, 364, 8084, 1389], "temperature": 0.0, "avg_logprob": -0.09504173226552466, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.0003230123547837138}, {"id": 106, "seek": 78704, "start": 802.7199999999999, "end": 811.88, "text": " of bloom filter with 9 hashes but and I just did it so it reproduces the plot of hash function", "tokens": [295, 26899, 6608, 365, 1722, 575, 8076, 457, 293, 286, 445, 630, 309, 370, 309, 11408, 887, 264, 7542, 295, 22019, 2445], "temperature": 0.0, "avg_logprob": -0.09504173226552466, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.0003230123547837138}, {"id": 107, "seek": 81188, "start": 811.88, "end": 819.64, "text": " here and yes for small keys it is the same and for bigger keys we have a gain and here", "tokens": [510, 293, 2086, 337, 1359, 9317, 309, 307, 264, 912, 293, 337, 3801, 9317, 321, 362, 257, 6052, 293, 510], "temperature": 0.0, "avg_logprob": -0.16568049517544833, "compression_ratio": 1.6265060240963856, "no_speech_prob": 5.3662177379010245e-05}, {"id": 108, "seek": 81188, "start": 819.64, "end": 830.8, "text": " is the key size 240 where xh3 function originally utilizes vector instructions and we can't use", "tokens": [307, 264, 2141, 2744, 26837, 689, 2031, 71, 18, 2445, 7993, 4976, 5660, 8062, 9415, 293, 321, 393, 380, 764], "temperature": 0.0, "avg_logprob": -0.16568049517544833, "compression_ratio": 1.6265060240963856, "no_speech_prob": 5.3662177379010245e-05}, {"id": 109, "seek": 81188, "start": 830.8, "end": 837.08, "text": " obviously vector instructions in BPF maps and for key size 240 it's like it is expected", "tokens": [2745, 8062, 9415, 294, 40533, 37, 11317, 293, 337, 2141, 2744, 26837, 309, 311, 411, 309, 307, 5176], "temperature": 0.0, "avg_logprob": -0.16568049517544833, "compression_ratio": 1.6265060240963856, "no_speech_prob": 5.3662177379010245e-05}, {"id": 110, "seek": 83708, "start": 837.08, "end": 843.4000000000001, "text": " to start using vector instructions but there is also scalar implementation which works", "tokens": [281, 722, 1228, 8062, 9415, 457, 456, 307, 611, 39684, 11420, 597, 1985], "temperature": 0.0, "avg_logprob": -0.13459827786400205, "compression_ratio": 1.5348837209302326, "no_speech_prob": 6.39321660855785e-05}, {"id": 111, "seek": 83708, "start": 843.4000000000001, "end": 854.44, "text": " faster than jhash but it degrades at this point so and another thing to mention that", "tokens": [4663, 813, 361, 71, 1299, 457, 309, 368, 22626, 412, 341, 935, 370, 293, 1071, 551, 281, 2152, 300], "temperature": 0.0, "avg_logprob": -0.13459827786400205, "compression_ratio": 1.5348837209302326, "no_speech_prob": 6.39321660855785e-05}, {"id": 112, "seek": 83708, "start": 854.44, "end": 865.88, "text": " old hash functions jhash xxh64 they were designed and optimized it with O2 option in mind so", "tokens": [1331, 22019, 6828, 361, 71, 1299, 2031, 87, 71, 19395, 436, 645, 4761, 293, 26941, 309, 365, 422, 17, 3614, 294, 1575, 370], "temperature": 0.0, "avg_logprob": -0.13459827786400205, "compression_ratio": 1.5348837209302326, "no_speech_prob": 6.39321660855785e-05}, {"id": 113, "seek": 86588, "start": 865.88, "end": 876.96, "text": " if we switch to O3 then they will behave the same but xxh3 actually runs like 50-60% faster", "tokens": [498, 321, 3679, 281, 422, 18, 550, 436, 486, 15158, 264, 912, 457, 2031, 87, 71, 18, 767, 6676, 411, 2625, 12, 4550, 4, 4663], "temperature": 0.0, "avg_logprob": -0.19130224340102253, "compression_ratio": 1.3088235294117647, "no_speech_prob": 0.0003641693328972906}, {"id": 114, "seek": 86588, "start": 876.96, "end": 888.72, "text": " so it actually performs way better with O3 so I just jump like here so and I know that", "tokens": [370, 309, 767, 26213, 636, 1101, 365, 422, 18, 370, 286, 445, 3012, 411, 510, 370, 293, 286, 458, 300], "temperature": 0.0, "avg_logprob": -0.19130224340102253, "compression_ratio": 1.3088235294117647, "no_speech_prob": 0.0003641693328972906}, {"id": 115, "seek": 88872, "start": 888.72, "end": 896.08, "text": " like O3 is no go for kernel like there were several attempts to introduce it and the reason", "tokens": [411, 422, 18, 307, 572, 352, 337, 28256, 411, 456, 645, 2940, 15257, 281, 5366, 309, 293, 264, 1778], "temperature": 0.0, "avg_logprob": -0.1419313604181463, "compression_ratio": 1.5739644970414202, "no_speech_prob": 8.27177136670798e-05}, {"id": 116, "seek": 88872, "start": 896.08, "end": 903.72, "text": " was that there are no candidates which benefit from this O3 but this one is a particular", "tokens": [390, 300, 456, 366, 572, 11255, 597, 5121, 490, 341, 422, 18, 457, 341, 472, 307, 257, 1729], "temperature": 0.0, "avg_logprob": -0.1419313604181463, "compression_ratio": 1.5739644970414202, "no_speech_prob": 8.27177136670798e-05}, {"id": 117, "seek": 88872, "start": 903.72, "end": 915.1600000000001, "text": " candidate like hash if we could use O3 for hash map because not only for xxh3 because", "tokens": [11532, 411, 22019, 498, 321, 727, 764, 422, 18, 337, 22019, 4471, 570, 406, 787, 337, 2031, 87, 71, 18, 570], "temperature": 0.0, "avg_logprob": -0.1419313604181463, "compression_ratio": 1.5739644970414202, "no_speech_prob": 8.27177136670798e-05}, {"id": 118, "seek": 91516, "start": 915.16, "end": 926.24, "text": " it should be inline then in this case we would be able to get rid of this composite hash", "tokens": [309, 820, 312, 294, 1889, 550, 294, 341, 1389, 321, 576, 312, 1075, 281, 483, 3973, 295, 341, 25557, 22019], "temperature": 0.0, "avg_logprob": -0.16374880202273104, "compression_ratio": 1.448, "no_speech_prob": 0.0002051257761195302}, {"id": 119, "seek": 91516, "start": 926.24, "end": 937.04, "text": " which mixes hashes and just use it as is so yeah as I said for stuck trace map it definitely", "tokens": [597, 37121, 575, 8076, 293, 445, 764, 309, 382, 307, 370, 1338, 382, 286, 848, 337, 5541, 13508, 4471, 309, 2138], "temperature": 0.0, "avg_logprob": -0.16374880202273104, "compression_ratio": 1.448, "no_speech_prob": 0.0002051257761195302}, {"id": 120, "seek": 93704, "start": 937.04, "end": 949.0799999999999, "text": " makes sense to use it so there is both benefit in speed small one because stuck trace map", "tokens": [1669, 2020, 281, 764, 309, 370, 456, 307, 1293, 5121, 294, 3073, 1359, 472, 570, 5541, 13508, 4471], "temperature": 0.0, "avg_logprob": -0.14704539283873544, "compression_ratio": 1.6751592356687899, "no_speech_prob": 8.52412122185342e-05}, {"id": 121, "seek": 93704, "start": 949.0799999999999, "end": 954.9599999999999, "text": " the bottleneck for speed is not the hash but the bottleneck for hash collisions is the", "tokens": [264, 44641, 547, 337, 3073, 307, 406, 264, 22019, 457, 264, 44641, 547, 337, 22019, 46537, 307, 264], "temperature": 0.0, "avg_logprob": -0.14704539283873544, "compression_ratio": 1.6751592356687899, "no_speech_prob": 8.52412122185342e-05}, {"id": 122, "seek": 93704, "start": 954.9599999999999, "end": 963.24, "text": " hash and for hash map it's a question maybe someone would advise me on what to do with", "tokens": [22019, 293, 337, 22019, 4471, 309, 311, 257, 1168, 1310, 1580, 576, 18312, 385, 322, 437, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.14704539283873544, "compression_ratio": 1.6751592356687899, "no_speech_prob": 8.52412122185342e-05}, {"id": 123, "seek": 96324, "start": 963.24, "end": 974.6800000000001, "text": " O3 and after I run like benchmarks on slightly bigger number of architectures then I think", "tokens": [422, 18, 293, 934, 286, 1190, 411, 43751, 322, 4748, 3801, 1230, 295, 6331, 1303, 550, 286, 519], "temperature": 0.0, "avg_logprob": -0.19395096071304813, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.380129293072969e-05}, {"id": 124, "seek": 96324, "start": 974.6800000000001, "end": 981.4, "text": " this is also like a good candidate to use in the hash map so here are some links for", "tokens": [341, 307, 611, 411, 257, 665, 11532, 281, 764, 294, 264, 22019, 4471, 370, 510, 366, 512, 6123, 337], "temperature": 0.0, "avg_logprob": -0.19395096071304813, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.380129293072969e-05}, {"id": 125, "seek": 96324, "start": 981.4, "end": 986.84, "text": " benchmarks and paper which I use it for those who will be reading this and thank you", "tokens": [43751, 293, 3035, 597, 286, 764, 309, 337, 729, 567, 486, 312, 3760, 341, 293, 1309, 291], "temperature": 0.0, "avg_logprob": -0.19395096071304813, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.380129293072969e-05}, {"id": 126, "seek": 98684, "start": 986.84, "end": 1004.84, "text": " all right thanks a lot any questions you for the O3 thing can you only compile maybe the", "tokens": [439, 558, 3231, 257, 688, 604, 1651, 291, 337, 264, 422, 18, 551, 393, 291, 787, 31413, 1310, 264], "temperature": 0.0, "avg_logprob": -0.2097022750160911, "compression_ratio": 1.4416666666666667, "no_speech_prob": 0.0005803873646073043}, {"id": 127, "seek": 98684, "start": 1004.84, "end": 1011.12, "text": " like the hash map file with O3 yeah yeah if it is like currently it is disabled like", "tokens": [411, 264, 22019, 4471, 3991, 365, 422, 18, 1338, 1338, 498, 309, 307, 411, 4362, 309, 307, 15191, 411], "temperature": 0.0, "avg_logprob": -0.2097022750160911, "compression_ratio": 1.4416666666666667, "no_speech_prob": 0.0005803873646073043}, {"id": 128, "seek": 101112, "start": 1011.12, "end": 1017.52, "text": " for every file in kernel for custom build we can enable it but like generally we just", "tokens": [337, 633, 3991, 294, 28256, 337, 2375, 1322, 321, 393, 9528, 309, 457, 411, 5101, 321, 445], "temperature": 0.0, "avg_logprob": -0.211304394166861, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0003907324862666428}, {"id": 129, "seek": 101112, "start": 1017.52, "end": 1023.88, "text": " pass O2 everywhere if it's possible just to compile BPF maps with O3 this will solve the", "tokens": [1320, 422, 17, 5315, 498, 309, 311, 1944, 445, 281, 31413, 40533, 37, 11317, 365, 422, 18, 341, 486, 5039, 264], "temperature": 0.0, "avg_logprob": -0.211304394166861, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0003907324862666428}, {"id": 130, "seek": 101112, "start": 1023.88, "end": 1028.44, "text": " thing yes it's it's not such a big change so you don't have to compile the whole kernel", "tokens": [551, 2086, 309, 311, 309, 311, 406, 1270, 257, 955, 1319, 370, 291, 500, 380, 362, 281, 31413, 264, 1379, 28256], "temperature": 0.0, "avg_logprob": -0.211304394166861, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0003907324862666428}, {"id": 131, "seek": 102844, "start": 1028.44, "end": 1044.56, "text": " with O3 yeah yeah yeah it's local code okay any other questions no then thanks a lot", "tokens": [50364, 365, 422, 18, 1338, 1338, 1338, 309, 311, 2654, 3089, 1392, 604, 661, 1651, 572, 550, 3231, 257, 688, 51170], "temperature": 0.0, "avg_logprob": -0.2833093079653653, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.0017024241387844086}, {"id": 132, "seek": 105844, "start": 1058.44, "end": 1060.5, "text": " you", "tokens": [50364, 291, 50467], "temperature": 0.0, "avg_logprob": -0.9088994264602661, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9472287893295288}], "language": "en"}