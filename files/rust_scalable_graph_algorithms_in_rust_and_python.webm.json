{"text": " Paul, we're going to talk about scalable graph algorithms in Rust. Thank you. Hello. I am Martin. I'm here with Paul. And we talk about scalable graph algorithms in Rust and some Python. Who are we? We're both engineers at a company called Neo4j, and the J stands not for R, like Rust, unfortunately. But we will talk about a little bit what we do in our day job, which is we work on a product which is called Neo4j Graph Data Science, so Neo4j is a graph database. Maybe you heard of it. It's written in Java. And the two of us, we work on a project called Graph Data Science, which is essentially a plug-in for the Neo4j Graph Database. And it provides a collection of graph and machine learning algorithms that we deploy to our customers, and they use it for many different things, but like the top three applications of these things are fraud detection, recommendation, and identity resolution. And we have customers with up to 10 billion nodes and 65 billion relationship graphs that they compute our algorithms on. And you can find out more about the product and the source code is available online and in the documentation, of course, if you follow those things. Last week, we released version 2.3 of the Graph Data Science product, and what you can see here is basically all the graph and machine learning algorithm that we provide to our customers or users, ordered by some category, and some of them you probably know from university like Dijkstra path searching algorithms or connect components algorithms. Okay, but we are in the Rust step room, so why do we talk about Java? So first of all, so Paul and I, we discovered Rust like two or three years ago, and we started building like smaller tools, libraries, fell in love with the language, and we were curious about how we can actually do what we do at work, like implementing those parallel algorithms in Java, how good would they perform if we would do the same thing in Rust, and also make a bit more use of what Rust has to offer. And we had a quick look around. There is only one graph library that is, I think, very popular, which is called PetGraph, which is focusing on a network X replacement, and it focused on like single-threaded execution of graph algorithms, basically textbook implementations. So we thought we want to go like the step further and do like the parallel implementations of the graph algorithms. So that's how we started the project. So we started in May 21. First of all, it was an experiment, basically, or a hobby project by the two of us, where we tried to figure out like what's the maximum performance we can get out of this implementation. And then over time, yeah, we got more interest into the project, and we added some more implementations of different algorithms, like it's not a lot, but you will see that later. And we added some Python support that we will talk about in a demo. We added an arrow server so that you can use this thing in a network, for example, and everything is available on GitHub and MAT licensed. The project itself contains or consists of five grades. Today we will talk about three of those. The one at the bottom is the graph builder, which is essentially the abstraction or data structure that represents the in-memory graph that we use to compute on. It's a CSR, compressed sparse row representation. And it also has a builder API, hence the name, to allow users of this library to construct graphs, either programmatically or from files. On top of that, we have the actual graph grade. And yes, the name was free on grades, I owe it wasn't actually free, but the owner wanted to give it away anyway, so we took it, lucky us. So yeah, this contains some algorithms, and then we have graph mate, which are our Python bindings on top of the graph and the graph builder grades. The servers, the arrow server, and the app is the CLI application that we won't talk about today. So let's start with the graph builder. The graph builder is basically an API for building directed and undirected so-called property graphs. What you can see on the right-hand side is a undirected graph consisting of five nodes, zero to four, which are connected by edges. And they have no direction, hence the graph is undirected. How do we construct such a graph? So what we show here is the Rust API. So the main entry point that you can see is the graph builder, which is this thing here. And the graph builder is just authenticated, and you give it, you can call the edges function, which takes, in this example, like we take an array of tuples where the first value is the source node and the second value is the target node to describe essentially the graph on the right-hand side. We call build, and what we want to construct here is basically controlled by the type that we assigned to the G variable, which is an undirected CSR graph. So it's very similar to collect, where you can specify I want to collect into a vec or a string or something like that. Basically we use a type system here, which is very nice or expressive in comparison to Java to basically define what the output of this function call is. And we have this undirected CSR graph, which is a concrete implementation of a graph. And the first, the type parameter we use here, U64, is basically the type for the node IDs. And then once you have this constructed, you have several methods available, like the degree, which is the number of edges that are connected to a node. So the degree of node one is free because it's connected to zero, two, and three. And you can get the neighbors of a specific node as an iterator. And in this particular implementation, you can also turn this iterator into a slice, which means you basically have zero copy if you want to access the neighborhood of a node, which is very useful if you want to implement performant graph algorithms. Now we want to turn this into a directed graph, which means now our edges have these little arrows at the end, which means an edge has always a source node, or a source node where it starts and an end node where it ends. The only thing that we need to change here is basically the return type or the type of our G variable, which is now a directed graph. And we have additional methods. We have the out degree, because now we have to differentiate between the outgoing edges and the incoming edges, and the same for the out neighbors and the in neighbors. What we can also do is, since we are talking about property graphs, which means we have properties on nodes and also on edges, we can add node values as another builder method or builder function. Again an array, which is node count minus one length, where you basically provide the node values that you want to add to your nodes. It gives you an additional function called node value to access the value. Similar for edge values, now you do basically a triple, where the third value is the relationship value. For example, here like the 0.1 for the edge between zero and zero and one, which is this one. All right. And we get another method down here, it's the out neighbors with values, which in addition to the target ID of this edge also gives you the relationship where the edge weight. For convenience, we have this so-called GDL string that you can provide, GDL stands for graph definition language, which is another crate that we wrote. It's basically a subset of the Cypher query language, that is the main query language of Neo4j, which allows you to declaratively describe the graph on the right-hand side using sqr syntax. So basically this in parenthesis n zero is node zero and this kind of JSON style map here is the property map, like P1 for example, to describe node zero and an edge is expressed by node zero where you can refer to a variable that you declared earlier and connect it to another one called n1, which is this edge description and you can also provide properties to this edge. It's basically used for testing or for building like small POCs to play around, it's much simpler than using the edges methods and so on. But it's basically the same graph that we constructed before. So as you can see, you can create those graphs programmatically if you use like the edges method and so on. The construction is also parallelized under the hood using rayon. But the main use case is usually from reading graphs from files and we have a graph input trade that you can implement. We provide three implementations. The most common one, especially if you want to start playing around, is using an edge list, which is basically a text file where in each row you have a source and a target and an optional value and graph 500 is a benchmark or a benchmark description specification for HPC graph algorithm benchmarks and they also provide a data generator and we basically can read the binary file format that this generator produces. Like I said, everything as part of graph creation is parallelized using rayon and we will see this in the demo in action. The next grade I want to just mention briefly is the graph grade, which contains the parallel graph algorithm implementations. At the moment, it's these four, which we implemented in the first place to compare them to our Java implementations. So for example, PageRank, it's an algorithm to give like a population, popularity value to a node. It basically tells like if you traverse the graph randomly, like how likely is it that you end up with that node, so it's kind of a popularity metric. Connected components, Paul will talk a little bit about that in the demo. Again, also the graph algorithms are parallelized using rayon and if you want to see more or just open an issue or PR. Just a quick Rust API where how we call this algorithm, so in the graph pre-loot, we also provide like all the algorithm methods, for example, PageRank as you can see in the middle. The first thing we do here, we have a GDL graph again. It's the graph on the right-hand side without any properties. We create a directed graph with a specific layout, which Paul will talk about and then we call the PageRank method using that graph as a reference. You can specify some parameters in the PageRank config, which are not that important right now and the result are the scores, which are the values assigned to each node after the computation is done. And that's basically it. Okay, over to Paul with GraphMate. Yeah. Hi, I'm Paul and I want to talk about GraphMate, which are our Python bindings over this set of crates that Martin just talked about. And we just had a wonderful talk about Python APIs on top of Rust and this is in the same spirit. So we want to expose a Python API for Rust implementations and we don't want to deal with all the shortcomings in Python in terms of like proper parallelism and memory management. We also integrate with NumPy and Pandas, which are de facto standard libraries in anything Python. It's very alpha, so it works and you can install it for a pip. It's available on PyPy and I just want to run through a demo, which is a notebook. And there we go. So first we, I think I need to clip this on once again. Okay, we configure some logging so we can see some outputs and we import typical Python prelude. We import our crates and as well as NumPy and Pandas. And in this demo, we are loading a Graph 500 graph in particular scale 42 and Graph 500 describes it as you have your scale number, two to the power of the scale number of nodes and 16 times as many relationships and this ends up in, so we load that file. We also load a direct graph and it takes a few seconds and at the end we will get a graph that says we have about 16 million, almost 17 million nodes and about 260 million relationships. And we are loading a direct graph, which means we have two sets of logging outputs that look very similar because we do it once for the outgoing, once for the incoming direction. And we also use the duplicated layout, which will merge parallel edges between the same source and target node pair. It will de-duplicate them and we only represent one of them in the graph. And with that we can run PageRank. So PageRank is a method on the graph object that we get and PageRank is an iterative algorithm. It runs in a number of iterations and when it finds that the result is good enough, it will stop and we can now access some metadata about the run. So we see we ran eight iterations in about 1.3 seconds, but now we also want to access the actual scores, the PageRank scores. In the other slide that Martin showed, we store the scores in a WEC of F32 and we don't want to copy that WEC into Python land, into the Python heap, convert the floating point numbers into Python numbers. So we are interfacing with the C API from NumPy and we return an array view that points to that WEC. So when you call scores, there's no copying involved. We return you a NumPy array that directly accesses the data and there's nothing to be copied in the Python heap or anywhere else. And you can use that array, it's a proper NumPy array and you can use that for example to put it into a partner's data frame and then do some calculations based on that. The numbers here don't really mean anything in particular, it's just for demonstration. And the next algorithm you want to run is WCC, which stands for weekly connected components. So it basically identifies components within a graph. Every node that is connected together is one component and we run that, it takes about 200 milliseconds, we're still running on the same graph. And similar to PageRank, we can access the data here and the data is an array where for every position in that array for that node ID, it's the component ID, so every node that is together in the same component will have the same component ID in that array. And we can use a pandas method here, drop duplicates, which will give us all the unique components IDs so we can identify the number of components here. And so we see, we are down from 16 million something something nodes down to almost eight million unique components. And yeah, that is WCC and for the last thing we want to count the total number of triangles in the graph. A triangle is defined as a connection between three nodes from A to B to Z back to A. And for that, first we need to convert the graph into an undirected graph. There's a method there. And it'll take a little while, a few seconds, because it's creating a new graph. We have to basically merge those two out and in lists together, we produce a new graph and since it's a new API, a new type in Rust, we also return it as a new graph. Which means if we, if we're low on memory, we can delete references to the old graph, we don't need that anymore. There's a particular optimization for triangle counting that makes it not be super slow, which we call make degree audit. I don't really want to go into details what it's actually doing, but it's, let me just run triangle counting here. It makes it so that triangle count finishes within a minute, not within five minutes. And that optimization only takes like one and a half seconds. At the bottom, you can see the H-top output, so you can see that it's actually using all the cores and proper parallelism without any typical Python shenanigans that you need to do to avoid the GIL and so on. And we don't have to watch it finish. We can go back to the presentation. This is a summary of the demonstration that we just went through. We don't need to look at it anymore. Once in our repository, we have three variations of the demonstration. We have the first one in Python that we just showed. We have another one using the Rust API and the third one using the arrow server that Martin mentioned, where there's a Python client, but it's not using the library directly. It's using arrow and arrow flight to communicate with the server and doing it remotely. I mean, if you're interested in those demos, you can follow those QR code links at the end. And I think by now, triangle counting should be done. So we took about a minute and it found 10 billion triangles. If I count it correctly, it seems, yeah, it seems it way. So that is for the demonstration. Now we can look back a little bit and talk about the lessons learned, particularly for us coming from the JVM world. And so using Rust as a Java developer, first of all, the way the Rust paradigms require us to think differently about the code and allow us to think differently about the code. Things like using the type system to define whether or not we have indirect or undirect the graph. And this is, of course, very nice and very refreshing coming from a Java world. But also, we have a better mechanical sympathy for what happens. We don't have to think about this JVM black box where things go through before they touch the hardware. Ecosystem cargo, Rust analyzer is very, very nice. But also, of course, there are some downsides to it. We don't have that experience of just clicking a fancy button in the IDE to run a debug or a profiler. We have to actually learn different tools and do things the proper way, I guess. Yeah, but what about performance? We talked about what we want to do in a performance way. And for every algorithm that we have implemented, we are faster and less memory-intensive than the Java implementations. It's not just about that. It's also predictable behavior. No latency spikes, no allocation rates, no cheat compiler that does things in the back. And just quickly showing what we want to do from the future, of course, expanding all the things. And if you want to play around with it, feel welcome and open issues. There's also a longer version of this talk because I'm already out of time, so thank you. We don't have time for all of this.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.04, "text": " Paul, we're going to talk about scalable graph algorithms in Rust.", "tokens": [4552, 11, 321, 434, 516, 281, 751, 466, 38481, 4295, 14642, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 1, "seek": 0, "start": 16.04, "end": 17.04, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 2, "seek": 0, "start": 17.04, "end": 18.04, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 3, "seek": 0, "start": 18.04, "end": 19.04, "text": " I am Martin.", "tokens": [286, 669, 9184, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 4, "seek": 0, "start": 19.04, "end": 20.04, "text": " I'm here with Paul.", "tokens": [286, 478, 510, 365, 4552, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 5, "seek": 0, "start": 20.04, "end": 24.8, "text": " And we talk about scalable graph algorithms in Rust and some Python.", "tokens": [400, 321, 751, 466, 38481, 4295, 14642, 294, 34952, 293, 512, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 6, "seek": 0, "start": 24.8, "end": 25.8, "text": " Who are we?", "tokens": [2102, 366, 321, 30], "temperature": 0.0, "avg_logprob": -0.2276622095415669, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.11406328529119492}, {"id": 7, "seek": 2580, "start": 25.8, "end": 33.32, "text": " We're both engineers at a company called Neo4j, and the J stands not for R, like Rust, unfortunately.", "tokens": [492, 434, 1293, 11955, 412, 257, 2237, 1219, 24458, 19, 73, 11, 293, 264, 508, 7382, 406, 337, 497, 11, 411, 34952, 11, 7015, 13], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 8, "seek": 2580, "start": 33.32, "end": 38.8, "text": " But we will talk about a little bit what we do in our day job, which is we work on a product", "tokens": [583, 321, 486, 751, 466, 257, 707, 857, 437, 321, 360, 294, 527, 786, 1691, 11, 597, 307, 321, 589, 322, 257, 1674], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 9, "seek": 2580, "start": 38.8, "end": 42.8, "text": " which is called Neo4j Graph Data Science, so Neo4j is a graph database.", "tokens": [597, 307, 1219, 24458, 19, 73, 21884, 11888, 8976, 11, 370, 24458, 19, 73, 307, 257, 4295, 8149, 13], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 10, "seek": 2580, "start": 42.8, "end": 43.92, "text": " Maybe you heard of it.", "tokens": [2704, 291, 2198, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 11, "seek": 2580, "start": 43.92, "end": 46.28, "text": " It's written in Java.", "tokens": [467, 311, 3720, 294, 10745, 13], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 12, "seek": 2580, "start": 46.28, "end": 51.040000000000006, "text": " And the two of us, we work on a project called Graph Data Science, which is essentially a", "tokens": [400, 264, 732, 295, 505, 11, 321, 589, 322, 257, 1716, 1219, 21884, 11888, 8976, 11, 597, 307, 4476, 257], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 13, "seek": 2580, "start": 51.040000000000006, "end": 53.6, "text": " plug-in for the Neo4j Graph Database.", "tokens": [5452, 12, 259, 337, 264, 24458, 19, 73, 21884, 40461, 651, 13], "temperature": 0.0, "avg_logprob": -0.20482078311950203, "compression_ratio": 1.71484375, "no_speech_prob": 0.0002504125004634261}, {"id": 14, "seek": 5360, "start": 53.6, "end": 58.36, "text": " And it provides a collection of graph and machine learning algorithms that we deploy", "tokens": [400, 309, 6417, 257, 5765, 295, 4295, 293, 3479, 2539, 14642, 300, 321, 7274], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 15, "seek": 5360, "start": 58.36, "end": 62.480000000000004, "text": " to our customers, and they use it for many different things, but like the top three", "tokens": [281, 527, 4581, 11, 293, 436, 764, 309, 337, 867, 819, 721, 11, 457, 411, 264, 1192, 1045], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 16, "seek": 5360, "start": 62.480000000000004, "end": 68.68, "text": " applications of these things are fraud detection, recommendation, and identity resolution.", "tokens": [5821, 295, 613, 721, 366, 14560, 17784, 11, 11879, 11, 293, 6575, 8669, 13], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 17, "seek": 5360, "start": 68.68, "end": 74.52000000000001, "text": " And we have customers with up to 10 billion nodes and 65 billion relationship graphs that", "tokens": [400, 321, 362, 4581, 365, 493, 281, 1266, 5218, 13891, 293, 11624, 5218, 2480, 24877, 300], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 18, "seek": 5360, "start": 74.52000000000001, "end": 77.48, "text": " they compute our algorithms on.", "tokens": [436, 14722, 527, 14642, 322, 13], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 19, "seek": 5360, "start": 77.48, "end": 82.12, "text": " And you can find out more about the product and the source code is available online and", "tokens": [400, 291, 393, 915, 484, 544, 466, 264, 1674, 293, 264, 4009, 3089, 307, 2435, 2950, 293], "temperature": 0.0, "avg_logprob": -0.1507209720033588, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00022516933677252382}, {"id": 20, "seek": 8212, "start": 82.12, "end": 86.80000000000001, "text": " in the documentation, of course, if you follow those things.", "tokens": [294, 264, 14333, 11, 295, 1164, 11, 498, 291, 1524, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.18538944537823016, "compression_ratio": 1.59375, "no_speech_prob": 5.460054671857506e-05}, {"id": 21, "seek": 8212, "start": 86.80000000000001, "end": 92.0, "text": " Last week, we released version 2.3 of the Graph Data Science product, and what you can", "tokens": [5264, 1243, 11, 321, 4736, 3037, 568, 13, 18, 295, 264, 21884, 11888, 8976, 1674, 11, 293, 437, 291, 393], "temperature": 0.0, "avg_logprob": -0.18538944537823016, "compression_ratio": 1.59375, "no_speech_prob": 5.460054671857506e-05}, {"id": 22, "seek": 8212, "start": 92.0, "end": 97.4, "text": " see here is basically all the graph and machine learning algorithm that we provide to our", "tokens": [536, 510, 307, 1936, 439, 264, 4295, 293, 3479, 2539, 9284, 300, 321, 2893, 281, 527], "temperature": 0.0, "avg_logprob": -0.18538944537823016, "compression_ratio": 1.59375, "no_speech_prob": 5.460054671857506e-05}, {"id": 23, "seek": 8212, "start": 97.4, "end": 104.0, "text": " customers or users, ordered by some category, and some of them you probably know from university", "tokens": [4581, 420, 5022, 11, 8866, 538, 512, 7719, 11, 293, 512, 295, 552, 291, 1391, 458, 490, 5454], "temperature": 0.0, "avg_logprob": -0.18538944537823016, "compression_ratio": 1.59375, "no_speech_prob": 5.460054671857506e-05}, {"id": 24, "seek": 8212, "start": 104.0, "end": 110.2, "text": " like Dijkstra path searching algorithms or connect components algorithms.", "tokens": [411, 413, 6940, 19639, 3100, 10808, 14642, 420, 1745, 6677, 14642, 13], "temperature": 0.0, "avg_logprob": -0.18538944537823016, "compression_ratio": 1.59375, "no_speech_prob": 5.460054671857506e-05}, {"id": 25, "seek": 11020, "start": 110.2, "end": 116.72, "text": " Okay, but we are in the Rust step room, so why do we talk about Java?", "tokens": [1033, 11, 457, 321, 366, 294, 264, 34952, 1823, 1808, 11, 370, 983, 360, 321, 751, 466, 10745, 30], "temperature": 0.0, "avg_logprob": -0.15114331026689723, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00030286790570244193}, {"id": 26, "seek": 11020, "start": 116.72, "end": 121.48, "text": " So first of all, so Paul and I, we discovered Rust like two or three years ago, and we started", "tokens": [407, 700, 295, 439, 11, 370, 4552, 293, 286, 11, 321, 6941, 34952, 411, 732, 420, 1045, 924, 2057, 11, 293, 321, 1409], "temperature": 0.0, "avg_logprob": -0.15114331026689723, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00030286790570244193}, {"id": 27, "seek": 11020, "start": 121.48, "end": 127.72, "text": " building like smaller tools, libraries, fell in love with the language, and we were curious", "tokens": [2390, 411, 4356, 3873, 11, 15148, 11, 5696, 294, 959, 365, 264, 2856, 11, 293, 321, 645, 6369], "temperature": 0.0, "avg_logprob": -0.15114331026689723, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00030286790570244193}, {"id": 28, "seek": 11020, "start": 127.72, "end": 132.64000000000001, "text": " about how we can actually do what we do at work, like implementing those parallel algorithms", "tokens": [466, 577, 321, 393, 767, 360, 437, 321, 360, 412, 589, 11, 411, 18114, 729, 8952, 14642], "temperature": 0.0, "avg_logprob": -0.15114331026689723, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00030286790570244193}, {"id": 29, "seek": 11020, "start": 132.64000000000001, "end": 137.2, "text": " in Java, how good would they perform if we would do the same thing in Rust, and also", "tokens": [294, 10745, 11, 577, 665, 576, 436, 2042, 498, 321, 576, 360, 264, 912, 551, 294, 34952, 11, 293, 611], "temperature": 0.0, "avg_logprob": -0.15114331026689723, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00030286790570244193}, {"id": 30, "seek": 13720, "start": 137.2, "end": 141.04, "text": " make a bit more use of what Rust has to offer.", "tokens": [652, 257, 857, 544, 764, 295, 437, 34952, 575, 281, 2626, 13], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 31, "seek": 13720, "start": 141.04, "end": 142.04, "text": " And we had a quick look around.", "tokens": [400, 321, 632, 257, 1702, 574, 926, 13], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 32, "seek": 13720, "start": 142.04, "end": 146.88, "text": " There is only one graph library that is, I think, very popular, which is called PetGraph,", "tokens": [821, 307, 787, 472, 4295, 6405, 300, 307, 11, 286, 519, 11, 588, 3743, 11, 597, 307, 1219, 10472, 38, 2662, 11], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 33, "seek": 13720, "start": 146.88, "end": 152.56, "text": " which is focusing on a network X replacement, and it focused on like single-threaded execution", "tokens": [597, 307, 8416, 322, 257, 3209, 1783, 14419, 11, 293, 309, 5178, 322, 411, 2167, 12, 392, 2538, 292, 15058], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 34, "seek": 13720, "start": 152.56, "end": 155.88, "text": " of graph algorithms, basically textbook implementations.", "tokens": [295, 4295, 14642, 11, 1936, 25591, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 35, "seek": 13720, "start": 155.88, "end": 159.51999999999998, "text": " So we thought we want to go like the step further and do like the parallel implementations", "tokens": [407, 321, 1194, 321, 528, 281, 352, 411, 264, 1823, 3052, 293, 360, 411, 264, 8952, 4445, 763], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 36, "seek": 13720, "start": 159.51999999999998, "end": 164.23999999999998, "text": " of the graph algorithms.", "tokens": [295, 264, 4295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.17387707450173118, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.693705366458744e-05}, {"id": 37, "seek": 16424, "start": 164.24, "end": 167.44, "text": " So that's how we started the project.", "tokens": [407, 300, 311, 577, 321, 1409, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 38, "seek": 16424, "start": 167.44, "end": 169.56, "text": " So we started in May 21.", "tokens": [407, 321, 1409, 294, 1891, 5080, 13], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 39, "seek": 16424, "start": 169.56, "end": 173.64000000000001, "text": " First of all, it was an experiment, basically, or a hobby project by the two of us, where", "tokens": [2386, 295, 439, 11, 309, 390, 364, 5120, 11, 1936, 11, 420, 257, 18240, 1716, 538, 264, 732, 295, 505, 11, 689], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 40, "seek": 16424, "start": 173.64000000000001, "end": 178.68, "text": " we tried to figure out like what's the maximum performance we can get out of this implementation.", "tokens": [321, 3031, 281, 2573, 484, 411, 437, 311, 264, 6674, 3389, 321, 393, 483, 484, 295, 341, 11420, 13], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 41, "seek": 16424, "start": 178.68, "end": 184.60000000000002, "text": " And then over time, yeah, we got more interest into the project, and we added some more implementations", "tokens": [400, 550, 670, 565, 11, 1338, 11, 321, 658, 544, 1179, 666, 264, 1716, 11, 293, 321, 3869, 512, 544, 4445, 763], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 42, "seek": 16424, "start": 184.60000000000002, "end": 188.76000000000002, "text": " of different algorithms, like it's not a lot, but you will see that later.", "tokens": [295, 819, 14642, 11, 411, 309, 311, 406, 257, 688, 11, 457, 291, 486, 536, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 43, "seek": 16424, "start": 188.76000000000002, "end": 193.12, "text": " And we added some Python support that we will talk about in a demo.", "tokens": [400, 321, 3869, 512, 15329, 1406, 300, 321, 486, 751, 466, 294, 257, 10723, 13], "temperature": 0.0, "avg_logprob": -0.128384068608284, "compression_ratio": 1.702054794520548, "no_speech_prob": 9.87512685242109e-05}, {"id": 44, "seek": 19312, "start": 193.12, "end": 197.44, "text": " We added an arrow server so that you can use this thing in a network, for example, and", "tokens": [492, 3869, 364, 11610, 7154, 370, 300, 291, 393, 764, 341, 551, 294, 257, 3209, 11, 337, 1365, 11, 293], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 45, "seek": 19312, "start": 197.44, "end": 204.84, "text": " everything is available on GitHub and MAT licensed.", "tokens": [1203, 307, 2435, 322, 23331, 293, 5904, 25225, 13], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 46, "seek": 19312, "start": 204.84, "end": 207.84, "text": " The project itself contains or consists of five grades.", "tokens": [440, 1716, 2564, 8306, 420, 14689, 295, 1732, 18041, 13], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 47, "seek": 19312, "start": 207.84, "end": 209.84, "text": " Today we will talk about three of those.", "tokens": [2692, 321, 486, 751, 466, 1045, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 48, "seek": 19312, "start": 209.84, "end": 214.04000000000002, "text": " The one at the bottom is the graph builder, which is essentially the abstraction or data", "tokens": [440, 472, 412, 264, 2767, 307, 264, 4295, 27377, 11, 597, 307, 4476, 264, 37765, 420, 1412], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 49, "seek": 19312, "start": 214.04000000000002, "end": 217.72, "text": " structure that represents the in-memory graph that we use to compute on.", "tokens": [3877, 300, 8855, 264, 294, 12, 17886, 827, 4295, 300, 321, 764, 281, 14722, 322, 13], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 50, "seek": 19312, "start": 217.72, "end": 220.92000000000002, "text": " It's a CSR, compressed sparse row representation.", "tokens": [467, 311, 257, 9460, 49, 11, 30353, 637, 11668, 5386, 10290, 13], "temperature": 0.0, "avg_logprob": -0.18395236654019137, "compression_ratio": 1.5907473309608542, "no_speech_prob": 4.8196317948168144e-05}, {"id": 51, "seek": 22092, "start": 220.92, "end": 226.35999999999999, "text": " And it also has a builder API, hence the name, to allow users of this library to construct", "tokens": [400, 309, 611, 575, 257, 27377, 9362, 11, 16678, 264, 1315, 11, 281, 2089, 5022, 295, 341, 6405, 281, 7690], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 52, "seek": 22092, "start": 226.35999999999999, "end": 229.6, "text": " graphs, either programmatically or from files.", "tokens": [24877, 11, 2139, 37648, 5030, 420, 490, 7098, 13], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 53, "seek": 22092, "start": 229.6, "end": 233.0, "text": " On top of that, we have the actual graph grade.", "tokens": [1282, 1192, 295, 300, 11, 321, 362, 264, 3539, 4295, 7204, 13], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 54, "seek": 22092, "start": 233.0, "end": 237.79999999999998, "text": " And yes, the name was free on grades, I owe it wasn't actually free, but the owner wanted", "tokens": [400, 2086, 11, 264, 1315, 390, 1737, 322, 18041, 11, 286, 16655, 309, 2067, 380, 767, 1737, 11, 457, 264, 7289, 1415], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 55, "seek": 22092, "start": 237.79999999999998, "end": 243.0, "text": " to give it away anyway, so we took it, lucky us.", "tokens": [281, 976, 309, 1314, 4033, 11, 370, 321, 1890, 309, 11, 6356, 505, 13], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 56, "seek": 22092, "start": 243.0, "end": 246.79999999999998, "text": " So yeah, this contains some algorithms, and then we have graph mate, which are our Python", "tokens": [407, 1338, 11, 341, 8306, 512, 14642, 11, 293, 550, 321, 362, 4295, 11709, 11, 597, 366, 527, 15329], "temperature": 0.0, "avg_logprob": -0.17459980357776989, "compression_ratio": 1.5984555984555984, "no_speech_prob": 6.190108979353681e-05}, {"id": 57, "seek": 24680, "start": 246.8, "end": 251.84, "text": " bindings on top of the graph and the graph builder grades.", "tokens": [14786, 1109, 322, 1192, 295, 264, 4295, 293, 264, 4295, 27377, 18041, 13], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 58, "seek": 24680, "start": 251.84, "end": 255.16000000000003, "text": " The servers, the arrow server, and the app is the CLI application that we won't talk", "tokens": [440, 15909, 11, 264, 11610, 7154, 11, 293, 264, 724, 307, 264, 12855, 40, 3861, 300, 321, 1582, 380, 751], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 59, "seek": 24680, "start": 255.16000000000003, "end": 257.32, "text": " about today.", "tokens": [466, 965, 13], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 60, "seek": 24680, "start": 257.32, "end": 259.2, "text": " So let's start with the graph builder.", "tokens": [407, 718, 311, 722, 365, 264, 4295, 27377, 13], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 61, "seek": 24680, "start": 259.2, "end": 262.96000000000004, "text": " The graph builder is basically an API for building directed and undirected so-called", "tokens": [440, 4295, 27377, 307, 1936, 364, 9362, 337, 2390, 12898, 293, 674, 11890, 292, 370, 12, 11880], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 62, "seek": 24680, "start": 262.96000000000004, "end": 265.52, "text": " property graphs.", "tokens": [4707, 24877, 13], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 63, "seek": 24680, "start": 265.52, "end": 269.88, "text": " What you can see on the right-hand side is a undirected graph consisting of five nodes,", "tokens": [708, 291, 393, 536, 322, 264, 558, 12, 5543, 1252, 307, 257, 674, 11890, 292, 4295, 33921, 295, 1732, 13891, 11], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 64, "seek": 24680, "start": 269.88, "end": 273.12, "text": " zero to four, which are connected by edges.", "tokens": [4018, 281, 1451, 11, 597, 366, 4582, 538, 8819, 13], "temperature": 0.0, "avg_logprob": -0.152604621753358, "compression_ratio": 1.716, "no_speech_prob": 8.057763625402004e-05}, {"id": 65, "seek": 27312, "start": 273.12, "end": 276.88, "text": " And they have no direction, hence the graph is undirected.", "tokens": [400, 436, 362, 572, 3513, 11, 16678, 264, 4295, 307, 674, 11890, 292, 13], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 66, "seek": 27312, "start": 276.88, "end": 278.44, "text": " How do we construct such a graph?", "tokens": [1012, 360, 321, 7690, 1270, 257, 4295, 30], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 67, "seek": 27312, "start": 278.44, "end": 281.12, "text": " So what we show here is the Rust API.", "tokens": [407, 437, 321, 855, 510, 307, 264, 34952, 9362, 13], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 68, "seek": 27312, "start": 281.12, "end": 287.52, "text": " So the main entry point that you can see is the graph builder, which is this thing here.", "tokens": [407, 264, 2135, 8729, 935, 300, 291, 393, 536, 307, 264, 4295, 27377, 11, 597, 307, 341, 551, 510, 13], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 69, "seek": 27312, "start": 287.52, "end": 292.4, "text": " And the graph builder is just authenticated, and you give it, you can call the edges function,", "tokens": [400, 264, 4295, 27377, 307, 445, 9214, 3587, 11, 293, 291, 976, 309, 11, 291, 393, 818, 264, 8819, 2445, 11], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 70, "seek": 27312, "start": 292.4, "end": 296.8, "text": " which takes, in this example, like we take an array of tuples where the first value is", "tokens": [597, 2516, 11, 294, 341, 1365, 11, 411, 321, 747, 364, 10225, 295, 2604, 2622, 689, 264, 700, 2158, 307], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 71, "seek": 27312, "start": 296.8, "end": 300.64, "text": " the source node and the second value is the target node to describe essentially the graph", "tokens": [264, 4009, 9984, 293, 264, 1150, 2158, 307, 264, 3779, 9984, 281, 6786, 4476, 264, 4295], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 72, "seek": 27312, "start": 300.64, "end": 302.92, "text": " on the right-hand side.", "tokens": [322, 264, 558, 12, 5543, 1252, 13], "temperature": 0.0, "avg_logprob": -0.12371187067743558, "compression_ratio": 1.7944250871080138, "no_speech_prob": 5.727542156819254e-05}, {"id": 73, "seek": 30292, "start": 302.92, "end": 309.36, "text": " We call build, and what we want to construct here is basically controlled by the type that", "tokens": [492, 818, 1322, 11, 293, 437, 321, 528, 281, 7690, 510, 307, 1936, 10164, 538, 264, 2010, 300], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 74, "seek": 30292, "start": 309.36, "end": 314.08000000000004, "text": " we assigned to the G variable, which is an undirected CSR graph.", "tokens": [321, 13279, 281, 264, 460, 7006, 11, 597, 307, 364, 674, 11890, 292, 9460, 49, 4295, 13], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 75, "seek": 30292, "start": 314.08000000000004, "end": 318.44, "text": " So it's very similar to collect, where you can specify I want to collect into a vec or", "tokens": [407, 309, 311, 588, 2531, 281, 2500, 11, 689, 291, 393, 16500, 286, 528, 281, 2500, 666, 257, 42021, 420], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 76, "seek": 30292, "start": 318.44, "end": 320.40000000000003, "text": " a string or something like that.", "tokens": [257, 6798, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 77, "seek": 30292, "start": 320.40000000000003, "end": 325.88, "text": " Basically we use a type system here, which is very nice or expressive in comparison to", "tokens": [8537, 321, 764, 257, 2010, 1185, 510, 11, 597, 307, 588, 1481, 420, 40189, 294, 9660, 281], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 78, "seek": 30292, "start": 325.88, "end": 331.32, "text": " Java to basically define what the output of this function call is.", "tokens": [10745, 281, 1936, 6964, 437, 264, 5598, 295, 341, 2445, 818, 307, 13], "temperature": 0.0, "avg_logprob": -0.13248390521643297, "compression_ratio": 1.6374045801526718, "no_speech_prob": 7.58899623178877e-05}, {"id": 79, "seek": 33132, "start": 331.32, "end": 335.56, "text": " And we have this undirected CSR graph, which is a concrete implementation of a graph.", "tokens": [400, 321, 362, 341, 674, 11890, 292, 9460, 49, 4295, 11, 597, 307, 257, 9859, 11420, 295, 257, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 80, "seek": 33132, "start": 335.56, "end": 343.88, "text": " And the first, the type parameter we use here, U64, is basically the type for the node IDs.", "tokens": [400, 264, 700, 11, 264, 2010, 13075, 321, 764, 510, 11, 624, 19395, 11, 307, 1936, 264, 2010, 337, 264, 9984, 48212, 13], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 81, "seek": 33132, "start": 343.88, "end": 347.64, "text": " And then once you have this constructed, you have several methods available, like the degree,", "tokens": [400, 550, 1564, 291, 362, 341, 17083, 11, 291, 362, 2940, 7150, 2435, 11, 411, 264, 4314, 11], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 82, "seek": 33132, "start": 347.64, "end": 350.24, "text": " which is the number of edges that are connected to a node.", "tokens": [597, 307, 264, 1230, 295, 8819, 300, 366, 4582, 281, 257, 9984, 13], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 83, "seek": 33132, "start": 350.24, "end": 355.0, "text": " So the degree of node one is free because it's connected to zero, two, and three.", "tokens": [407, 264, 4314, 295, 9984, 472, 307, 1737, 570, 309, 311, 4582, 281, 4018, 11, 732, 11, 293, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 84, "seek": 33132, "start": 355.0, "end": 359.2, "text": " And you can get the neighbors of a specific node as an iterator.", "tokens": [400, 291, 393, 483, 264, 12512, 295, 257, 2685, 9984, 382, 364, 17138, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1187563562780861, "compression_ratio": 1.7345454545454546, "no_speech_prob": 7.353322871495038e-05}, {"id": 85, "seek": 35920, "start": 359.2, "end": 363.44, "text": " And in this particular implementation, you can also turn this iterator into a slice,", "tokens": [400, 294, 341, 1729, 11420, 11, 291, 393, 611, 1261, 341, 17138, 1639, 666, 257, 13153, 11], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 86, "seek": 35920, "start": 363.44, "end": 368.24, "text": " which means you basically have zero copy if you want to access the neighborhood of a node,", "tokens": [597, 1355, 291, 1936, 362, 4018, 5055, 498, 291, 528, 281, 2105, 264, 7630, 295, 257, 9984, 11], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 87, "seek": 35920, "start": 368.24, "end": 374.15999999999997, "text": " which is very useful if you want to implement performant graph algorithms.", "tokens": [597, 307, 588, 4420, 498, 291, 528, 281, 4445, 2042, 394, 4295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 88, "seek": 35920, "start": 374.15999999999997, "end": 377.84, "text": " Now we want to turn this into a directed graph, which means now our edges have these", "tokens": [823, 321, 528, 281, 1261, 341, 666, 257, 12898, 4295, 11, 597, 1355, 586, 527, 8819, 362, 613], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 89, "seek": 35920, "start": 377.84, "end": 382.59999999999997, "text": " little arrows at the end, which means an edge has always a source node, or a source node", "tokens": [707, 19669, 412, 264, 917, 11, 597, 1355, 364, 4691, 575, 1009, 257, 4009, 9984, 11, 420, 257, 4009, 9984], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 90, "seek": 35920, "start": 382.59999999999997, "end": 385.76, "text": " where it starts and an end node where it ends.", "tokens": [689, 309, 3719, 293, 364, 917, 9984, 689, 309, 5314, 13], "temperature": 0.0, "avg_logprob": -0.10742499147142683, "compression_ratio": 1.884, "no_speech_prob": 8.590393554186448e-05}, {"id": 91, "seek": 38576, "start": 385.76, "end": 389.4, "text": " The only thing that we need to change here is basically the return type or the type of", "tokens": [440, 787, 551, 300, 321, 643, 281, 1319, 510, 307, 1936, 264, 2736, 2010, 420, 264, 2010, 295], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 92, "seek": 38576, "start": 389.4, "end": 392.92, "text": " our G variable, which is now a directed graph.", "tokens": [527, 460, 7006, 11, 597, 307, 586, 257, 12898, 4295, 13], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 93, "seek": 38576, "start": 392.92, "end": 395.28, "text": " And we have additional methods.", "tokens": [400, 321, 362, 4497, 7150, 13], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 94, "seek": 38576, "start": 395.28, "end": 398.8, "text": " We have the out degree, because now we have to differentiate between the outgoing edges", "tokens": [492, 362, 264, 484, 4314, 11, 570, 586, 321, 362, 281, 23203, 1296, 264, 41565, 8819], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 95, "seek": 38576, "start": 398.8, "end": 404.84, "text": " and the incoming edges, and the same for the out neighbors and the in neighbors.", "tokens": [293, 264, 22341, 8819, 11, 293, 264, 912, 337, 264, 484, 12512, 293, 264, 294, 12512, 13], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 96, "seek": 38576, "start": 404.84, "end": 408.64, "text": " What we can also do is, since we are talking about property graphs, which means we have", "tokens": [708, 321, 393, 611, 360, 307, 11, 1670, 321, 366, 1417, 466, 4707, 24877, 11, 597, 1355, 321, 362], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 97, "seek": 38576, "start": 408.64, "end": 414.44, "text": " properties on nodes and also on edges, we can add node values as another builder method", "tokens": [7221, 322, 13891, 293, 611, 322, 8819, 11, 321, 393, 909, 9984, 4190, 382, 1071, 27377, 3170], "temperature": 0.0, "avg_logprob": -0.11693254311879477, "compression_ratio": 1.8411552346570397, "no_speech_prob": 8.599009015597403e-05}, {"id": 98, "seek": 41444, "start": 414.44, "end": 416.0, "text": " or builder function.", "tokens": [420, 27377, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 99, "seek": 41444, "start": 416.0, "end": 422.12, "text": " Again an array, which is node count minus one length, where you basically provide the", "tokens": [3764, 364, 10225, 11, 597, 307, 9984, 1207, 3175, 472, 4641, 11, 689, 291, 1936, 2893, 264], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 100, "seek": 41444, "start": 422.12, "end": 425.8, "text": " node values that you want to add to your nodes.", "tokens": [9984, 4190, 300, 291, 528, 281, 909, 281, 428, 13891, 13], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 101, "seek": 41444, "start": 425.8, "end": 431.15999999999997, "text": " It gives you an additional function called node value to access the value.", "tokens": [467, 2709, 291, 364, 4497, 2445, 1219, 9984, 2158, 281, 2105, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 102, "seek": 41444, "start": 431.15999999999997, "end": 439.12, "text": " Similar for edge values, now you do basically a triple, where the third value is the relationship", "tokens": [10905, 337, 4691, 4190, 11, 586, 291, 360, 1936, 257, 15508, 11, 689, 264, 2636, 2158, 307, 264, 2480], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 103, "seek": 41444, "start": 439.12, "end": 440.12, "text": " value.", "tokens": [2158, 13], "temperature": 0.0, "avg_logprob": -0.13042156784622758, "compression_ratio": 1.6954314720812182, "no_speech_prob": 0.00013915947056375444}, {"id": 104, "seek": 44012, "start": 440.12, "end": 447.2, "text": " For example, here like the 0.1 for the edge between zero and zero and one, which is this", "tokens": [1171, 1365, 11, 510, 411, 264, 1958, 13, 16, 337, 264, 4691, 1296, 4018, 293, 4018, 293, 472, 11, 597, 307, 341], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 105, "seek": 44012, "start": 447.2, "end": 448.2, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 106, "seek": 44012, "start": 448.2, "end": 449.36, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 107, "seek": 44012, "start": 449.36, "end": 452.76, "text": " And we get another method down here, it's the out neighbors with values, which in addition", "tokens": [400, 321, 483, 1071, 3170, 760, 510, 11, 309, 311, 264, 484, 12512, 365, 4190, 11, 597, 294, 4500], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 108, "seek": 44012, "start": 452.76, "end": 459.64, "text": " to the target ID of this edge also gives you the relationship where the edge weight.", "tokens": [281, 264, 3779, 7348, 295, 341, 4691, 611, 2709, 291, 264, 2480, 689, 264, 4691, 3364, 13], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 109, "seek": 44012, "start": 459.64, "end": 466.32, "text": " For convenience, we have this so-called GDL string that you can provide, GDL stands for", "tokens": [1171, 19283, 11, 321, 362, 341, 370, 12, 11880, 460, 35, 43, 6798, 300, 291, 393, 2893, 11, 460, 35, 43, 7382, 337], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 110, "seek": 44012, "start": 466.32, "end": 469.68, "text": " graph definition language, which is another crate that we wrote.", "tokens": [4295, 7123, 2856, 11, 597, 307, 1071, 42426, 300, 321, 4114, 13], "temperature": 0.0, "avg_logprob": -0.21318015717623526, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0001605245197424665}, {"id": 111, "seek": 46968, "start": 469.68, "end": 473.24, "text": " It's basically a subset of the Cypher query language, that is the main query language", "tokens": [467, 311, 1936, 257, 25993, 295, 264, 10295, 79, 511, 14581, 2856, 11, 300, 307, 264, 2135, 14581, 2856], "temperature": 0.0, "avg_logprob": -0.23145562410354614, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0001307169732172042}, {"id": 112, "seek": 46968, "start": 473.24, "end": 478.68, "text": " of Neo4j, which allows you to declaratively describe the graph on the right-hand side", "tokens": [295, 24458, 19, 73, 11, 597, 4045, 291, 281, 16694, 19020, 6786, 264, 4295, 322, 264, 558, 12, 5543, 1252], "temperature": 0.0, "avg_logprob": -0.23145562410354614, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0001307169732172042}, {"id": 113, "seek": 46968, "start": 478.68, "end": 480.40000000000003, "text": " using sqr syntax.", "tokens": [1228, 262, 80, 81, 28431, 13], "temperature": 0.0, "avg_logprob": -0.23145562410354614, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0001307169732172042}, {"id": 114, "seek": 46968, "start": 480.40000000000003, "end": 488.24, "text": " So basically this in parenthesis n zero is node zero and this kind of JSON style map", "tokens": [407, 1936, 341, 294, 23350, 9374, 297, 4018, 307, 9984, 4018, 293, 341, 733, 295, 31828, 3758, 4471], "temperature": 0.0, "avg_logprob": -0.23145562410354614, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0001307169732172042}, {"id": 115, "seek": 46968, "start": 488.24, "end": 494.32, "text": " here is the property map, like P1 for example, to describe node zero and an edge is expressed", "tokens": [510, 307, 264, 4707, 4471, 11, 411, 430, 16, 337, 1365, 11, 281, 6786, 9984, 4018, 293, 364, 4691, 307, 12675], "temperature": 0.0, "avg_logprob": -0.23145562410354614, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0001307169732172042}, {"id": 116, "seek": 49432, "start": 494.32, "end": 500.56, "text": " by node zero where you can refer to a variable that you declared earlier and connect it to", "tokens": [538, 9984, 4018, 689, 291, 393, 2864, 281, 257, 7006, 300, 291, 15489, 3071, 293, 1745, 309, 281], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 117, "seek": 49432, "start": 500.56, "end": 506.2, "text": " another one called n1, which is this edge description and you can also provide properties", "tokens": [1071, 472, 1219, 297, 16, 11, 597, 307, 341, 4691, 3855, 293, 291, 393, 611, 2893, 7221], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 118, "seek": 49432, "start": 506.2, "end": 508.0, "text": " to this edge.", "tokens": [281, 341, 4691, 13], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 119, "seek": 49432, "start": 508.0, "end": 514.0, "text": " It's basically used for testing or for building like small POCs to play around, it's much", "tokens": [467, 311, 1936, 1143, 337, 4997, 420, 337, 2390, 411, 1359, 22299, 33290, 281, 862, 926, 11, 309, 311, 709], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 120, "seek": 49432, "start": 514.0, "end": 517.4399999999999, "text": " simpler than using the edges methods and so on.", "tokens": [18587, 813, 1228, 264, 8819, 7150, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 121, "seek": 49432, "start": 517.4399999999999, "end": 521.08, "text": " But it's basically the same graph that we constructed before.", "tokens": [583, 309, 311, 1936, 264, 912, 4295, 300, 321, 17083, 949, 13], "temperature": 0.0, "avg_logprob": -0.12673252507259972, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.00010351106175221503}, {"id": 122, "seek": 52108, "start": 521.08, "end": 525.64, "text": " So as you can see, you can create those graphs programmatically if you use like the edges", "tokens": [407, 382, 291, 393, 536, 11, 291, 393, 1884, 729, 24877, 37648, 5030, 498, 291, 764, 411, 264, 8819], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 123, "seek": 52108, "start": 525.64, "end": 526.64, "text": " method and so on.", "tokens": [3170, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 124, "seek": 52108, "start": 526.64, "end": 530.2, "text": " The construction is also parallelized under the hood using rayon.", "tokens": [440, 6435, 307, 611, 8952, 1602, 833, 264, 13376, 1228, 18592, 266, 13], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 125, "seek": 52108, "start": 530.2, "end": 535.08, "text": " But the main use case is usually from reading graphs from files and we have a graph input", "tokens": [583, 264, 2135, 764, 1389, 307, 2673, 490, 3760, 24877, 490, 7098, 293, 321, 362, 257, 4295, 4846], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 126, "seek": 52108, "start": 535.08, "end": 536.96, "text": " trade that you can implement.", "tokens": [4923, 300, 291, 393, 4445, 13], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 127, "seek": 52108, "start": 536.96, "end": 538.8000000000001, "text": " We provide three implementations.", "tokens": [492, 2893, 1045, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 128, "seek": 52108, "start": 538.8000000000001, "end": 542.2, "text": " The most common one, especially if you want to start playing around, is using an edge", "tokens": [440, 881, 2689, 472, 11, 2318, 498, 291, 528, 281, 722, 2433, 926, 11, 307, 1228, 364, 4691], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 129, "seek": 52108, "start": 542.2, "end": 546.36, "text": " list, which is basically a text file where in each row you have a source and a target", "tokens": [1329, 11, 597, 307, 1936, 257, 2487, 3991, 689, 294, 1184, 5386, 291, 362, 257, 4009, 293, 257, 3779], "temperature": 0.0, "avg_logprob": -0.1558990478515625, "compression_ratio": 1.7695035460992907, "no_speech_prob": 4.599677413352765e-05}, {"id": 130, "seek": 54636, "start": 546.36, "end": 554.04, "text": " and an optional value and graph 500 is a benchmark or a benchmark description specification", "tokens": [293, 364, 17312, 2158, 293, 4295, 5923, 307, 257, 18927, 420, 257, 18927, 3855, 31256], "temperature": 0.0, "avg_logprob": -0.10997675671989535, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00011915191862499341}, {"id": 131, "seek": 54636, "start": 554.04, "end": 559.72, "text": " for HPC graph algorithm benchmarks and they also provide a data generator and we basically", "tokens": [337, 12557, 34, 4295, 9284, 43751, 293, 436, 611, 2893, 257, 1412, 19265, 293, 321, 1936], "temperature": 0.0, "avg_logprob": -0.10997675671989535, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00011915191862499341}, {"id": 132, "seek": 54636, "start": 559.72, "end": 564.88, "text": " can read the binary file format that this generator produces.", "tokens": [393, 1401, 264, 17434, 3991, 7877, 300, 341, 19265, 14725, 13], "temperature": 0.0, "avg_logprob": -0.10997675671989535, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00011915191862499341}, {"id": 133, "seek": 54636, "start": 564.88, "end": 569.36, "text": " Like I said, everything as part of graph creation is parallelized using rayon and we will see", "tokens": [1743, 286, 848, 11, 1203, 382, 644, 295, 4295, 8016, 307, 8952, 1602, 1228, 18592, 266, 293, 321, 486, 536], "temperature": 0.0, "avg_logprob": -0.10997675671989535, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00011915191862499341}, {"id": 134, "seek": 54636, "start": 569.36, "end": 571.64, "text": " this in the demo in action.", "tokens": [341, 294, 264, 10723, 294, 3069, 13], "temperature": 0.0, "avg_logprob": -0.10997675671989535, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00011915191862499341}, {"id": 135, "seek": 57164, "start": 571.64, "end": 577.64, "text": " The next grade I want to just mention briefly is the graph grade, which contains the parallel", "tokens": [440, 958, 7204, 286, 528, 281, 445, 2152, 10515, 307, 264, 4295, 7204, 11, 597, 8306, 264, 8952], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 136, "seek": 57164, "start": 577.64, "end": 579.68, "text": " graph algorithm implementations.", "tokens": [4295, 9284, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 137, "seek": 57164, "start": 579.68, "end": 584.84, "text": " At the moment, it's these four, which we implemented in the first place to compare them to our", "tokens": [1711, 264, 1623, 11, 309, 311, 613, 1451, 11, 597, 321, 12270, 294, 264, 700, 1081, 281, 6794, 552, 281, 527], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 138, "seek": 57164, "start": 584.84, "end": 586.56, "text": " Java implementations.", "tokens": [10745, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 139, "seek": 57164, "start": 586.56, "end": 593.0, "text": " So for example, PageRank, it's an algorithm to give like a population, popularity value", "tokens": [407, 337, 1365, 11, 21217, 49, 657, 11, 309, 311, 364, 9284, 281, 976, 411, 257, 4415, 11, 19301, 2158], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 140, "seek": 57164, "start": 593.0, "end": 594.0, "text": " to a node.", "tokens": [281, 257, 9984, 13], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 141, "seek": 57164, "start": 594.0, "end": 598.08, "text": " It basically tells like if you traverse the graph randomly, like how likely is it that", "tokens": [467, 1936, 5112, 411, 498, 291, 45674, 264, 4295, 16979, 11, 411, 577, 3700, 307, 309, 300], "temperature": 0.0, "avg_logprob": -0.16249356951032365, "compression_ratio": 1.736842105263158, "no_speech_prob": 7.810253009665757e-05}, {"id": 142, "seek": 59808, "start": 598.08, "end": 602.88, "text": " you end up with that node, so it's kind of a popularity metric.", "tokens": [291, 917, 493, 365, 300, 9984, 11, 370, 309, 311, 733, 295, 257, 19301, 20678, 13], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 143, "seek": 59808, "start": 602.88, "end": 606.44, "text": " Connected components, Paul will talk a little bit about that in the demo.", "tokens": [11653, 292, 6677, 11, 4552, 486, 751, 257, 707, 857, 466, 300, 294, 264, 10723, 13], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 144, "seek": 59808, "start": 606.44, "end": 611.5200000000001, "text": " Again, also the graph algorithms are parallelized using rayon and if you want to see more or", "tokens": [3764, 11, 611, 264, 4295, 14642, 366, 8952, 1602, 1228, 18592, 266, 293, 498, 291, 528, 281, 536, 544, 420], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 145, "seek": 59808, "start": 611.5200000000001, "end": 614.96, "text": " just open an issue or PR.", "tokens": [445, 1269, 364, 2734, 420, 11568, 13], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 146, "seek": 59808, "start": 614.96, "end": 622.0, "text": " Just a quick Rust API where how we call this algorithm, so in the graph pre-loot, we also", "tokens": [1449, 257, 1702, 34952, 9362, 689, 577, 321, 818, 341, 9284, 11, 370, 294, 264, 4295, 659, 12, 752, 310, 11, 321, 611], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 147, "seek": 59808, "start": 622.0, "end": 626.88, "text": " provide like all the algorithm methods, for example, PageRank as you can see in the middle.", "tokens": [2893, 411, 439, 264, 9284, 7150, 11, 337, 1365, 11, 21217, 49, 657, 382, 291, 393, 536, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.18750551011827257, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.82036004238762e-05}, {"id": 148, "seek": 62688, "start": 626.88, "end": 630.88, "text": " The first thing we do here, we have a GDL graph again.", "tokens": [440, 700, 551, 321, 360, 510, 11, 321, 362, 257, 460, 35, 43, 4295, 797, 13], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 149, "seek": 62688, "start": 630.88, "end": 633.6, "text": " It's the graph on the right-hand side without any properties.", "tokens": [467, 311, 264, 4295, 322, 264, 558, 12, 5543, 1252, 1553, 604, 7221, 13], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 150, "seek": 62688, "start": 633.6, "end": 639.12, "text": " We create a directed graph with a specific layout, which Paul will talk about and then", "tokens": [492, 1884, 257, 12898, 4295, 365, 257, 2685, 13333, 11, 597, 4552, 486, 751, 466, 293, 550], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 151, "seek": 62688, "start": 639.12, "end": 642.52, "text": " we call the PageRank method using that graph as a reference.", "tokens": [321, 818, 264, 21217, 49, 657, 3170, 1228, 300, 4295, 382, 257, 6408, 13], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 152, "seek": 62688, "start": 642.52, "end": 646.16, "text": " You can specify some parameters in the PageRank config, which are not that important right", "tokens": [509, 393, 16500, 512, 9834, 294, 264, 21217, 49, 657, 6662, 11, 597, 366, 406, 300, 1021, 558], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 153, "seek": 62688, "start": 646.16, "end": 651.2, "text": " now and the result are the scores, which are the values assigned to each node after the", "tokens": [586, 293, 264, 1874, 366, 264, 13444, 11, 597, 366, 264, 4190, 13279, 281, 1184, 9984, 934, 264], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 154, "seek": 62688, "start": 651.2, "end": 653.8, "text": " computation is done.", "tokens": [24903, 307, 1096, 13], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 155, "seek": 62688, "start": 653.8, "end": 655.28, "text": " And that's basically it.", "tokens": [400, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.15546554565429688, "compression_ratio": 1.6804123711340206, "no_speech_prob": 5.0544007535791025e-05}, {"id": 156, "seek": 65528, "start": 655.28, "end": 660.28, "text": " Okay, over to Paul with GraphMate.", "tokens": [1033, 11, 670, 281, 4552, 365, 21884, 44, 473, 13], "temperature": 0.0, "avg_logprob": -0.2696312247932731, "compression_ratio": 1.5, "no_speech_prob": 0.0003066880162805319}, {"id": 157, "seek": 65528, "start": 660.28, "end": 663.28, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2696312247932731, "compression_ratio": 1.5, "no_speech_prob": 0.0003066880162805319}, {"id": 158, "seek": 65528, "start": 663.28, "end": 675.88, "text": " Hi, I'm Paul and I want to talk about GraphMate, which are our Python bindings over this set", "tokens": [2421, 11, 286, 478, 4552, 293, 286, 528, 281, 751, 466, 21884, 44, 473, 11, 597, 366, 527, 15329, 14786, 1109, 670, 341, 992], "temperature": 0.0, "avg_logprob": -0.2696312247932731, "compression_ratio": 1.5, "no_speech_prob": 0.0003066880162805319}, {"id": 159, "seek": 65528, "start": 675.88, "end": 679.64, "text": " of crates that Martin just talked about.", "tokens": [295, 941, 1024, 300, 9184, 445, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.2696312247932731, "compression_ratio": 1.5, "no_speech_prob": 0.0003066880162805319}, {"id": 160, "seek": 65528, "start": 679.64, "end": 684.4, "text": " And we just had a wonderful talk about Python APIs on top of Rust and this is in the same", "tokens": [400, 321, 445, 632, 257, 3715, 751, 466, 15329, 21445, 322, 1192, 295, 34952, 293, 341, 307, 294, 264, 912], "temperature": 0.0, "avg_logprob": -0.2696312247932731, "compression_ratio": 1.5, "no_speech_prob": 0.0003066880162805319}, {"id": 161, "seek": 68440, "start": 684.4, "end": 685.78, "text": " spirit.", "tokens": [3797, 13], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 162, "seek": 68440, "start": 685.78, "end": 689.68, "text": " So we want to expose a Python API for Rust implementations and we don't want to deal", "tokens": [407, 321, 528, 281, 19219, 257, 15329, 9362, 337, 34952, 4445, 763, 293, 321, 500, 380, 528, 281, 2028], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 163, "seek": 68440, "start": 689.68, "end": 697.88, "text": " with all the shortcomings in Python in terms of like proper parallelism and memory management.", "tokens": [365, 439, 264, 2099, 49886, 294, 15329, 294, 2115, 295, 411, 2296, 8952, 1434, 293, 4675, 4592, 13], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 164, "seek": 68440, "start": 697.88, "end": 705.04, "text": " We also integrate with NumPy and Pandas, which are de facto standard libraries in anything", "tokens": [492, 611, 13365, 365, 22592, 47, 88, 293, 16995, 296, 11, 597, 366, 368, 42225, 3832, 15148, 294, 1340], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 165, "seek": 68440, "start": 705.04, "end": 707.0799999999999, "text": " Python.", "tokens": [15329, 13], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 166, "seek": 68440, "start": 707.0799999999999, "end": 714.0799999999999, "text": " It's very alpha, so it works and you can install it for a pip.", "tokens": [467, 311, 588, 8961, 11, 370, 309, 1985, 293, 291, 393, 3625, 309, 337, 257, 8489, 13], "temperature": 0.0, "avg_logprob": -0.2089354756114247, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0001279608259210363}, {"id": 167, "seek": 71408, "start": 714.08, "end": 724.0400000000001, "text": " It's available on PyPy and I just want to run through a demo, which is a notebook.", "tokens": [467, 311, 2435, 322, 9953, 47, 88, 293, 286, 445, 528, 281, 1190, 807, 257, 10723, 11, 597, 307, 257, 21060, 13], "temperature": 0.0, "avg_logprob": -0.3130995941162109, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.0002530984638724476}, {"id": 168, "seek": 71408, "start": 724.0400000000001, "end": 726.9200000000001, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.3130995941162109, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.0002530984638724476}, {"id": 169, "seek": 71408, "start": 726.9200000000001, "end": 732.32, "text": " So first we, I think I need to clip this on once again.", "tokens": [407, 700, 321, 11, 286, 519, 286, 643, 281, 7353, 341, 322, 1564, 797, 13], "temperature": 0.0, "avg_logprob": -0.3130995941162109, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.0002530984638724476}, {"id": 170, "seek": 73232, "start": 732.32, "end": 749.08, "text": " Okay, we configure some logging so we can see some outputs and we import typical Python", "tokens": [1033, 11, 321, 22162, 512, 27991, 370, 321, 393, 536, 512, 23930, 293, 321, 974, 7476, 15329], "temperature": 0.0, "avg_logprob": -0.26578072767991284, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.0001672476646490395}, {"id": 171, "seek": 73232, "start": 749.08, "end": 750.08, "text": " prelude.", "tokens": [659, 32334, 13], "temperature": 0.0, "avg_logprob": -0.26578072767991284, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.0001672476646490395}, {"id": 172, "seek": 73232, "start": 750.08, "end": 755.2800000000001, "text": " We import our crates and as well as NumPy and Pandas.", "tokens": [492, 974, 527, 941, 1024, 293, 382, 731, 382, 22592, 47, 88, 293, 16995, 296, 13], "temperature": 0.0, "avg_logprob": -0.26578072767991284, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.0001672476646490395}, {"id": 173, "seek": 73232, "start": 755.2800000000001, "end": 762.24, "text": " And in this demo, we are loading a Graph 500 graph in particular scale 42 and Graph 500", "tokens": [400, 294, 341, 10723, 11, 321, 366, 15114, 257, 21884, 5923, 4295, 294, 1729, 4373, 14034, 293, 21884, 5923], "temperature": 0.0, "avg_logprob": -0.26578072767991284, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.0001672476646490395}, {"id": 174, "seek": 76224, "start": 762.24, "end": 768.24, "text": " describes it as you have your scale number, two to the power of the scale number of nodes", "tokens": [15626, 309, 382, 291, 362, 428, 4373, 1230, 11, 732, 281, 264, 1347, 295, 264, 4373, 1230, 295, 13891], "temperature": 0.0, "avg_logprob": -0.2073219446035532, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.0001566820137668401}, {"id": 175, "seek": 76224, "start": 768.24, "end": 777.2, "text": " and 16 times as many relationships and this ends up in, so we load that file.", "tokens": [293, 3165, 1413, 382, 867, 6159, 293, 341, 5314, 493, 294, 11, 370, 321, 3677, 300, 3991, 13], "temperature": 0.0, "avg_logprob": -0.2073219446035532, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.0001566820137668401}, {"id": 176, "seek": 76224, "start": 777.2, "end": 782.44, "text": " We also load a direct graph and it takes a few seconds and at the end we will get a", "tokens": [492, 611, 3677, 257, 2047, 4295, 293, 309, 2516, 257, 1326, 3949, 293, 412, 264, 917, 321, 486, 483, 257], "temperature": 0.0, "avg_logprob": -0.2073219446035532, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.0001566820137668401}, {"id": 177, "seek": 78244, "start": 782.44, "end": 793.12, "text": " graph that says we have about 16 million, almost 17 million nodes and about 260 million", "tokens": [4295, 300, 1619, 321, 362, 466, 3165, 2459, 11, 1920, 3282, 2459, 13891, 293, 466, 44624, 2459], "temperature": 0.0, "avg_logprob": -0.16459005019244025, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00021199921320658177}, {"id": 178, "seek": 78244, "start": 793.12, "end": 794.12, "text": " relationships.", "tokens": [6159, 13], "temperature": 0.0, "avg_logprob": -0.16459005019244025, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00021199921320658177}, {"id": 179, "seek": 78244, "start": 794.12, "end": 799.0, "text": " And we are loading a direct graph, which means we have two sets of logging outputs that look", "tokens": [400, 321, 366, 15114, 257, 2047, 4295, 11, 597, 1355, 321, 362, 732, 6352, 295, 27991, 23930, 300, 574], "temperature": 0.0, "avg_logprob": -0.16459005019244025, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00021199921320658177}, {"id": 180, "seek": 78244, "start": 799.0, "end": 803.8000000000001, "text": " very similar because we do it once for the outgoing, once for the incoming direction.", "tokens": [588, 2531, 570, 321, 360, 309, 1564, 337, 264, 41565, 11, 1564, 337, 264, 22341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.16459005019244025, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00021199921320658177}, {"id": 181, "seek": 78244, "start": 803.8000000000001, "end": 811.84, "text": " And we also use the duplicated layout, which will merge parallel edges between the same", "tokens": [400, 321, 611, 764, 264, 1581, 564, 3587, 13333, 11, 597, 486, 22183, 8952, 8819, 1296, 264, 912], "temperature": 0.0, "avg_logprob": -0.16459005019244025, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00021199921320658177}, {"id": 182, "seek": 81184, "start": 811.84, "end": 813.4, "text": " source and target node pair.", "tokens": [4009, 293, 3779, 9984, 6119, 13], "temperature": 0.0, "avg_logprob": -0.16695692274305557, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00021867998293600976}, {"id": 183, "seek": 81184, "start": 813.4, "end": 818.2800000000001, "text": " It will de-duplicate them and we only represent one of them in the graph.", "tokens": [467, 486, 368, 12, 769, 4770, 473, 552, 293, 321, 787, 2906, 472, 295, 552, 294, 264, 4295, 13], "temperature": 0.0, "avg_logprob": -0.16695692274305557, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00021867998293600976}, {"id": 184, "seek": 81184, "start": 818.2800000000001, "end": 821.8000000000001, "text": " And with that we can run PageRank.", "tokens": [400, 365, 300, 321, 393, 1190, 21217, 49, 657, 13], "temperature": 0.0, "avg_logprob": -0.16695692274305557, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00021867998293600976}, {"id": 185, "seek": 81184, "start": 821.8000000000001, "end": 830.12, "text": " So PageRank is a method on the graph object that we get and PageRank is an iterative algorithm.", "tokens": [407, 21217, 49, 657, 307, 257, 3170, 322, 264, 4295, 2657, 300, 321, 483, 293, 21217, 49, 657, 307, 364, 17138, 1166, 9284, 13], "temperature": 0.0, "avg_logprob": -0.16695692274305557, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00021867998293600976}, {"id": 186, "seek": 81184, "start": 830.12, "end": 835.5600000000001, "text": " It runs in a number of iterations and when it finds that the result is good enough, it", "tokens": [467, 6676, 294, 257, 1230, 295, 36540, 293, 562, 309, 10704, 300, 264, 1874, 307, 665, 1547, 11, 309], "temperature": 0.0, "avg_logprob": -0.16695692274305557, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.00021867998293600976}, {"id": 187, "seek": 83556, "start": 835.56, "end": 842.1999999999999, "text": " will stop and we can now access some metadata about the run.", "tokens": [486, 1590, 293, 321, 393, 586, 2105, 512, 26603, 466, 264, 1190, 13], "temperature": 0.0, "avg_logprob": -0.1600825932561135, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00017006811685860157}, {"id": 188, "seek": 83556, "start": 842.1999999999999, "end": 847.4799999999999, "text": " So we see we ran eight iterations in about 1.3 seconds, but now we also want to access", "tokens": [407, 321, 536, 321, 5872, 3180, 36540, 294, 466, 502, 13, 18, 3949, 11, 457, 586, 321, 611, 528, 281, 2105], "temperature": 0.0, "avg_logprob": -0.1600825932561135, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00017006811685860157}, {"id": 189, "seek": 83556, "start": 847.4799999999999, "end": 850.64, "text": " the actual scores, the PageRank scores.", "tokens": [264, 3539, 13444, 11, 264, 21217, 49, 657, 13444, 13], "temperature": 0.0, "avg_logprob": -0.1600825932561135, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00017006811685860157}, {"id": 190, "seek": 83556, "start": 850.64, "end": 858.4, "text": " In the other slide that Martin showed, we store the scores in a WEC of F32 and we don't", "tokens": [682, 264, 661, 4137, 300, 9184, 4712, 11, 321, 3531, 264, 13444, 294, 257, 343, 8140, 295, 479, 11440, 293, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1600825932561135, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00017006811685860157}, {"id": 191, "seek": 83556, "start": 858.4, "end": 862.92, "text": " want to copy that WEC into Python land, into the Python heap, convert the floating point", "tokens": [528, 281, 5055, 300, 343, 8140, 666, 15329, 2117, 11, 666, 264, 15329, 33591, 11, 7620, 264, 12607, 935], "temperature": 0.0, "avg_logprob": -0.1600825932561135, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00017006811685860157}, {"id": 192, "seek": 86292, "start": 862.92, "end": 867.12, "text": " numbers into Python numbers.", "tokens": [3547, 666, 15329, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 193, "seek": 86292, "start": 867.12, "end": 874.64, "text": " So we are interfacing with the C API from NumPy and we return an array view that points", "tokens": [407, 321, 366, 14510, 5615, 365, 264, 383, 9362, 490, 22592, 47, 88, 293, 321, 2736, 364, 10225, 1910, 300, 2793], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 194, "seek": 86292, "start": 874.64, "end": 877.92, "text": " to that WEC.", "tokens": [281, 300, 343, 8140, 13], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 195, "seek": 86292, "start": 877.92, "end": 881.1999999999999, "text": " So when you call scores, there's no copying involved.", "tokens": [407, 562, 291, 818, 13444, 11, 456, 311, 572, 27976, 3288, 13], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 196, "seek": 86292, "start": 881.1999999999999, "end": 886.56, "text": " We return you a NumPy array that directly accesses the data and there's nothing to be", "tokens": [492, 2736, 291, 257, 22592, 47, 88, 10225, 300, 3838, 2105, 279, 264, 1412, 293, 456, 311, 1825, 281, 312], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 197, "seek": 86292, "start": 886.56, "end": 889.4399999999999, "text": " copied in the Python heap or anywhere else.", "tokens": [25365, 294, 264, 15329, 33591, 420, 4992, 1646, 13], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 198, "seek": 86292, "start": 889.4399999999999, "end": 892.88, "text": " And you can use that array, it's a proper NumPy array and you can use that for example", "tokens": [400, 291, 393, 764, 300, 10225, 11, 309, 311, 257, 2296, 22592, 47, 88, 10225, 293, 291, 393, 764, 300, 337, 1365], "temperature": 0.0, "avg_logprob": -0.15363414070822975, "compression_ratio": 1.702127659574468, "no_speech_prob": 7.685866876272485e-05}, {"id": 199, "seek": 89288, "start": 892.88, "end": 901.96, "text": " to put it into a partner's data frame and then do some calculations based on that.", "tokens": [281, 829, 309, 666, 257, 4975, 311, 1412, 3920, 293, 550, 360, 512, 20448, 2361, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.25196726028233357, "compression_ratio": 1.488262910798122, "no_speech_prob": 0.0002037970843957737}, {"id": 200, "seek": 89288, "start": 901.96, "end": 906.24, "text": " The numbers here don't really mean anything in particular, it's just for demonstration.", "tokens": [440, 3547, 510, 500, 380, 534, 914, 1340, 294, 1729, 11, 309, 311, 445, 337, 16520, 13], "temperature": 0.0, "avg_logprob": -0.25196726028233357, "compression_ratio": 1.488262910798122, "no_speech_prob": 0.0002037970843957737}, {"id": 201, "seek": 89288, "start": 906.24, "end": 914.36, "text": " And the next algorithm you want to run is WCC, which stands for weekly connected components.", "tokens": [400, 264, 958, 9284, 291, 528, 281, 1190, 307, 343, 11717, 11, 597, 7382, 337, 12460, 4582, 6677, 13], "temperature": 0.0, "avg_logprob": -0.25196726028233357, "compression_ratio": 1.488262910798122, "no_speech_prob": 0.0002037970843957737}, {"id": 202, "seek": 89288, "start": 914.36, "end": 919.32, "text": " So it basically identifies components within a graph.", "tokens": [407, 309, 1936, 34597, 6677, 1951, 257, 4295, 13], "temperature": 0.0, "avg_logprob": -0.25196726028233357, "compression_ratio": 1.488262910798122, "no_speech_prob": 0.0002037970843957737}, {"id": 203, "seek": 91932, "start": 919.32, "end": 925.6, "text": " Every node that is connected together is one component and we run that, it takes about", "tokens": [2048, 9984, 300, 307, 4582, 1214, 307, 472, 6542, 293, 321, 1190, 300, 11, 309, 2516, 466], "temperature": 0.0, "avg_logprob": -0.15913408629748285, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.00010969803406624123}, {"id": 204, "seek": 91932, "start": 925.6, "end": 929.6400000000001, "text": " 200 milliseconds, we're still running on the same graph.", "tokens": [2331, 34184, 11, 321, 434, 920, 2614, 322, 264, 912, 4295, 13], "temperature": 0.0, "avg_logprob": -0.15913408629748285, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.00010969803406624123}, {"id": 205, "seek": 91932, "start": 929.6400000000001, "end": 935.88, "text": " And similar to PageRank, we can access the data here and the data is an array where for", "tokens": [400, 2531, 281, 21217, 49, 657, 11, 321, 393, 2105, 264, 1412, 510, 293, 264, 1412, 307, 364, 10225, 689, 337], "temperature": 0.0, "avg_logprob": -0.15913408629748285, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.00010969803406624123}, {"id": 206, "seek": 91932, "start": 935.88, "end": 941.24, "text": " every position in that array for that node ID, it's the component ID, so every node", "tokens": [633, 2535, 294, 300, 10225, 337, 300, 9984, 7348, 11, 309, 311, 264, 6542, 7348, 11, 370, 633, 9984], "temperature": 0.0, "avg_logprob": -0.15913408629748285, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.00010969803406624123}, {"id": 207, "seek": 91932, "start": 941.24, "end": 945.8000000000001, "text": " that is together in the same component will have the same component ID in that array.", "tokens": [300, 307, 1214, 294, 264, 912, 6542, 486, 362, 264, 912, 6542, 7348, 294, 300, 10225, 13], "temperature": 0.0, "avg_logprob": -0.15913408629748285, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.00010969803406624123}, {"id": 208, "seek": 94580, "start": 945.8, "end": 952.64, "text": " And we can use a pandas method here, drop duplicates, which will give us all the unique", "tokens": [400, 321, 393, 764, 257, 4565, 296, 3170, 510, 11, 3270, 17154, 1024, 11, 597, 486, 976, 505, 439, 264, 3845], "temperature": 0.0, "avg_logprob": -0.21876051209189676, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.00018041783187072724}, {"id": 209, "seek": 94580, "start": 952.64, "end": 958.16, "text": " components IDs so we can identify the number of components here.", "tokens": [6677, 48212, 370, 321, 393, 5876, 264, 1230, 295, 6677, 510, 13], "temperature": 0.0, "avg_logprob": -0.21876051209189676, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.00018041783187072724}, {"id": 210, "seek": 94580, "start": 958.16, "end": 965.4, "text": " And so we see, we are down from 16 million something something nodes down to almost eight", "tokens": [400, 370, 321, 536, 11, 321, 366, 760, 490, 3165, 2459, 746, 746, 13891, 760, 281, 1920, 3180], "temperature": 0.0, "avg_logprob": -0.21876051209189676, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.00018041783187072724}, {"id": 211, "seek": 94580, "start": 965.4, "end": 968.7199999999999, "text": " million unique components.", "tokens": [2459, 3845, 6677, 13], "temperature": 0.0, "avg_logprob": -0.21876051209189676, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.00018041783187072724}, {"id": 212, "seek": 94580, "start": 968.7199999999999, "end": 975.3599999999999, "text": " And yeah, that is WCC and for the last thing we want to count the total number of triangles", "tokens": [400, 1338, 11, 300, 307, 343, 11717, 293, 337, 264, 1036, 551, 321, 528, 281, 1207, 264, 3217, 1230, 295, 29896], "temperature": 0.0, "avg_logprob": -0.21876051209189676, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.00018041783187072724}, {"id": 213, "seek": 97536, "start": 975.36, "end": 976.36, "text": " in the graph.", "tokens": [294, 264, 4295, 13], "temperature": 0.0, "avg_logprob": -0.14864397048950195, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.00029516505310311913}, {"id": 214, "seek": 97536, "start": 976.36, "end": 982.84, "text": " A triangle is defined as a connection between three nodes from A to B to Z back to A.", "tokens": [316, 13369, 307, 7642, 382, 257, 4984, 1296, 1045, 13891, 490, 316, 281, 363, 281, 1176, 646, 281, 316, 13], "temperature": 0.0, "avg_logprob": -0.14864397048950195, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.00029516505310311913}, {"id": 215, "seek": 97536, "start": 982.84, "end": 987.76, "text": " And for that, first we need to convert the graph into an undirected graph.", "tokens": [400, 337, 300, 11, 700, 321, 643, 281, 7620, 264, 4295, 666, 364, 674, 11890, 292, 4295, 13], "temperature": 0.0, "avg_logprob": -0.14864397048950195, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.00029516505310311913}, {"id": 216, "seek": 97536, "start": 987.76, "end": 991.6, "text": " There's a method there.", "tokens": [821, 311, 257, 3170, 456, 13], "temperature": 0.0, "avg_logprob": -0.14864397048950195, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.00029516505310311913}, {"id": 217, "seek": 97536, "start": 991.6, "end": 998.84, "text": " And it'll take a little while, a few seconds, because it's creating a new graph.", "tokens": [400, 309, 603, 747, 257, 707, 1339, 11, 257, 1326, 3949, 11, 570, 309, 311, 4084, 257, 777, 4295, 13], "temperature": 0.0, "avg_logprob": -0.14864397048950195, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.00029516505310311913}, {"id": 218, "seek": 99884, "start": 998.84, "end": 1006.08, "text": " We have to basically merge those two out and in lists together, we produce a new graph", "tokens": [492, 362, 281, 1936, 22183, 729, 732, 484, 293, 294, 14511, 1214, 11, 321, 5258, 257, 777, 4295], "temperature": 0.0, "avg_logprob": -0.18765771841701073, "compression_ratio": 1.5135135135135136, "no_speech_prob": 7.213577919173986e-05}, {"id": 219, "seek": 99884, "start": 1006.08, "end": 1012.36, "text": " and since it's a new API, a new type in Rust, we also return it as a new graph.", "tokens": [293, 1670, 309, 311, 257, 777, 9362, 11, 257, 777, 2010, 294, 34952, 11, 321, 611, 2736, 309, 382, 257, 777, 4295, 13], "temperature": 0.0, "avg_logprob": -0.18765771841701073, "compression_ratio": 1.5135135135135136, "no_speech_prob": 7.213577919173986e-05}, {"id": 220, "seek": 99884, "start": 1012.36, "end": 1016.8000000000001, "text": " Which means if we, if we're low on memory, we can delete references to the old graph,", "tokens": [3013, 1355, 498, 321, 11, 498, 321, 434, 2295, 322, 4675, 11, 321, 393, 12097, 15400, 281, 264, 1331, 4295, 11], "temperature": 0.0, "avg_logprob": -0.18765771841701073, "compression_ratio": 1.5135135135135136, "no_speech_prob": 7.213577919173986e-05}, {"id": 221, "seek": 99884, "start": 1016.8000000000001, "end": 1020.72, "text": " we don't need that anymore.", "tokens": [321, 500, 380, 643, 300, 3602, 13], "temperature": 0.0, "avg_logprob": -0.18765771841701073, "compression_ratio": 1.5135135135135136, "no_speech_prob": 7.213577919173986e-05}, {"id": 222, "seek": 102072, "start": 1020.72, "end": 1029.48, "text": " There's a particular optimization for triangle counting that makes it not be super slow,", "tokens": [821, 311, 257, 1729, 19618, 337, 13369, 13251, 300, 1669, 309, 406, 312, 1687, 2964, 11], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 223, "seek": 102072, "start": 1029.48, "end": 1032.0, "text": " which we call make degree audit.", "tokens": [597, 321, 818, 652, 4314, 17748, 13], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 224, "seek": 102072, "start": 1032.0, "end": 1038.72, "text": " I don't really want to go into details what it's actually doing, but it's, let me just", "tokens": [286, 500, 380, 534, 528, 281, 352, 666, 4365, 437, 309, 311, 767, 884, 11, 457, 309, 311, 11, 718, 385, 445], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 225, "seek": 102072, "start": 1038.72, "end": 1042.04, "text": " run triangle counting here.", "tokens": [1190, 13369, 13251, 510, 13], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 226, "seek": 102072, "start": 1042.04, "end": 1046.4, "text": " It makes it so that triangle count finishes within a minute, not within five minutes.", "tokens": [467, 1669, 309, 370, 300, 13369, 1207, 23615, 1951, 257, 3456, 11, 406, 1951, 1732, 2077, 13], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 227, "seek": 102072, "start": 1046.4, "end": 1049.48, "text": " And that optimization only takes like one and a half seconds.", "tokens": [400, 300, 19618, 787, 2516, 411, 472, 293, 257, 1922, 3949, 13], "temperature": 0.0, "avg_logprob": -0.17718635066863028, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.00021889508934691548}, {"id": 228, "seek": 104948, "start": 1049.48, "end": 1054.96, "text": " At the bottom, you can see the H-top output, so you can see that it's actually using all", "tokens": [1711, 264, 2767, 11, 291, 393, 536, 264, 389, 12, 19337, 5598, 11, 370, 291, 393, 536, 300, 309, 311, 767, 1228, 439], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 229, "seek": 104948, "start": 1054.96, "end": 1061.28, "text": " the cores and proper parallelism without any typical Python shenanigans that you need", "tokens": [264, 24826, 293, 2296, 8952, 1434, 1553, 604, 7476, 15329, 402, 45008, 49088, 300, 291, 643], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 230, "seek": 104948, "start": 1061.28, "end": 1065.3600000000001, "text": " to do to avoid the GIL and so on.", "tokens": [281, 360, 281, 5042, 264, 460, 4620, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 231, "seek": 104948, "start": 1065.3600000000001, "end": 1069.52, "text": " And we don't have to watch it finish.", "tokens": [400, 321, 500, 380, 362, 281, 1159, 309, 2413, 13], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 232, "seek": 104948, "start": 1069.52, "end": 1072.6, "text": " We can go back to the presentation.", "tokens": [492, 393, 352, 646, 281, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 233, "seek": 104948, "start": 1072.6, "end": 1076.24, "text": " This is a summary of the demonstration that we just went through.", "tokens": [639, 307, 257, 12691, 295, 264, 16520, 300, 321, 445, 1437, 807, 13], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 234, "seek": 104948, "start": 1076.24, "end": 1079.3600000000001, "text": " We don't need to look at it anymore.", "tokens": [492, 500, 380, 643, 281, 574, 412, 309, 3602, 13], "temperature": 0.0, "avg_logprob": -0.17896174493237077, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.0004211415653117001}, {"id": 235, "seek": 107936, "start": 1079.36, "end": 1084.8799999999999, "text": " Once in our repository, we have three variations of the demonstration.", "tokens": [3443, 294, 527, 25841, 11, 321, 362, 1045, 17840, 295, 264, 16520, 13], "temperature": 0.0, "avg_logprob": -0.17498378753662108, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00013051160203758627}, {"id": 236, "seek": 107936, "start": 1084.8799999999999, "end": 1087.6, "text": " We have the first one in Python that we just showed.", "tokens": [492, 362, 264, 700, 472, 294, 15329, 300, 321, 445, 4712, 13], "temperature": 0.0, "avg_logprob": -0.17498378753662108, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00013051160203758627}, {"id": 237, "seek": 107936, "start": 1087.6, "end": 1092.84, "text": " We have another one using the Rust API and the third one using the arrow server that", "tokens": [492, 362, 1071, 472, 1228, 264, 34952, 9362, 293, 264, 2636, 472, 1228, 264, 11610, 7154, 300], "temperature": 0.0, "avg_logprob": -0.17498378753662108, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00013051160203758627}, {"id": 238, "seek": 107936, "start": 1092.84, "end": 1098.7199999999998, "text": " Martin mentioned, where there's a Python client, but it's not using the library directly.", "tokens": [9184, 2835, 11, 689, 456, 311, 257, 15329, 6423, 11, 457, 309, 311, 406, 1228, 264, 6405, 3838, 13], "temperature": 0.0, "avg_logprob": -0.17498378753662108, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00013051160203758627}, {"id": 239, "seek": 107936, "start": 1098.7199999999998, "end": 1104.36, "text": " It's using arrow and arrow flight to communicate with the server and doing it remotely.", "tokens": [467, 311, 1228, 11610, 293, 11610, 7018, 281, 7890, 365, 264, 7154, 293, 884, 309, 20824, 13], "temperature": 0.0, "avg_logprob": -0.17498378753662108, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00013051160203758627}, {"id": 240, "seek": 110436, "start": 1104.36, "end": 1109.7199999999998, "text": " I mean, if you're interested in those demos, you can follow those QR code links at the", "tokens": [286, 914, 11, 498, 291, 434, 3102, 294, 729, 33788, 11, 291, 393, 1524, 729, 32784, 3089, 6123, 412, 264], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 241, "seek": 110436, "start": 1109.7199999999998, "end": 1110.7199999999998, "text": " end.", "tokens": [917, 13], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 242, "seek": 110436, "start": 1110.7199999999998, "end": 1114.08, "text": " And I think by now, triangle counting should be done.", "tokens": [400, 286, 519, 538, 586, 11, 13369, 13251, 820, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 243, "seek": 110436, "start": 1114.08, "end": 1117.8, "text": " So we took about a minute and it found 10 billion triangles.", "tokens": [407, 321, 1890, 466, 257, 3456, 293, 309, 1352, 1266, 5218, 29896, 13], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 244, "seek": 110436, "start": 1117.8, "end": 1125.36, "text": " If I count it correctly, it seems, yeah, it seems it way.", "tokens": [759, 286, 1207, 309, 8944, 11, 309, 2544, 11, 1338, 11, 309, 2544, 309, 636, 13], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 245, "seek": 110436, "start": 1125.36, "end": 1129.56, "text": " So that is for the demonstration.", "tokens": [407, 300, 307, 337, 264, 16520, 13], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 246, "seek": 110436, "start": 1129.56, "end": 1134.04, "text": " Now we can look back a little bit and talk about the lessons learned, particularly for", "tokens": [823, 321, 393, 574, 646, 257, 707, 857, 293, 751, 466, 264, 8820, 3264, 11, 4098, 337], "temperature": 0.0, "avg_logprob": -0.2126789278197057, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.0002928143658209592}, {"id": 247, "seek": 113404, "start": 1134.04, "end": 1137.56, "text": " us coming from the JVM world.", "tokens": [505, 1348, 490, 264, 508, 53, 44, 1002, 13], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 248, "seek": 113404, "start": 1137.56, "end": 1146.08, "text": " And so using Rust as a Java developer, first of all, the way the Rust paradigms require", "tokens": [400, 370, 1228, 34952, 382, 257, 10745, 10754, 11, 700, 295, 439, 11, 264, 636, 264, 34952, 13480, 328, 2592, 3651], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 249, "seek": 113404, "start": 1146.08, "end": 1150.32, "text": " us to think differently about the code and allow us to think differently about the code.", "tokens": [505, 281, 519, 7614, 466, 264, 3089, 293, 2089, 505, 281, 519, 7614, 466, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 250, "seek": 113404, "start": 1150.32, "end": 1154.96, "text": " Things like using the type system to define whether or not we have indirect or undirect", "tokens": [9514, 411, 1228, 264, 2010, 1185, 281, 6964, 1968, 420, 406, 321, 362, 19523, 420, 517, 44868], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 251, "seek": 113404, "start": 1154.96, "end": 1156.48, "text": " the graph.", "tokens": [264, 4295, 13], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 252, "seek": 113404, "start": 1156.48, "end": 1162.3999999999999, "text": " And this is, of course, very nice and very refreshing coming from a Java world.", "tokens": [400, 341, 307, 11, 295, 1164, 11, 588, 1481, 293, 588, 19772, 1348, 490, 257, 10745, 1002, 13], "temperature": 0.0, "avg_logprob": -0.17825949312460543, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00044315707054920495}, {"id": 253, "seek": 116240, "start": 1162.4, "end": 1166.0800000000002, "text": " But also, we have a better mechanical sympathy for what happens.", "tokens": [583, 611, 11, 321, 362, 257, 1101, 12070, 33240, 337, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 254, "seek": 116240, "start": 1166.0800000000002, "end": 1170.8000000000002, "text": " We don't have to think about this JVM black box where things go through before they touch", "tokens": [492, 500, 380, 362, 281, 519, 466, 341, 508, 53, 44, 2211, 2424, 689, 721, 352, 807, 949, 436, 2557], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 255, "seek": 116240, "start": 1170.8000000000002, "end": 1174.3600000000001, "text": " the hardware.", "tokens": [264, 8837, 13], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 256, "seek": 116240, "start": 1174.3600000000001, "end": 1180.0800000000002, "text": " Ecosystem cargo, Rust analyzer is very, very nice.", "tokens": [462, 6877, 9321, 19449, 11, 34952, 6459, 4527, 307, 588, 11, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 257, "seek": 116240, "start": 1180.0800000000002, "end": 1183.4, "text": " But also, of course, there are some downsides to it.", "tokens": [583, 611, 11, 295, 1164, 11, 456, 366, 512, 21554, 1875, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 258, "seek": 116240, "start": 1183.4, "end": 1188.4, "text": " We don't have that experience of just clicking a fancy button in the IDE to run a debug or", "tokens": [492, 500, 380, 362, 300, 1752, 295, 445, 9697, 257, 10247, 2960, 294, 264, 40930, 281, 1190, 257, 24083, 420], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 259, "seek": 116240, "start": 1188.4, "end": 1189.4, "text": " a profiler.", "tokens": [257, 1740, 5441, 13], "temperature": 0.0, "avg_logprob": -0.15682574418874887, "compression_ratio": 1.5120967741935485, "no_speech_prob": 0.00021757266949862242}, {"id": 260, "seek": 118940, "start": 1189.4, "end": 1193.92, "text": " We have to actually learn different tools and do things the proper way, I guess.", "tokens": [492, 362, 281, 767, 1466, 819, 3873, 293, 360, 721, 264, 2296, 636, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 261, "seek": 118940, "start": 1193.92, "end": 1196.48, "text": " Yeah, but what about performance?", "tokens": [865, 11, 457, 437, 466, 3389, 30], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 262, "seek": 118940, "start": 1196.48, "end": 1199.3600000000001, "text": " We talked about what we want to do in a performance way.", "tokens": [492, 2825, 466, 437, 321, 528, 281, 360, 294, 257, 3389, 636, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 263, "seek": 118940, "start": 1199.3600000000001, "end": 1205.2800000000002, "text": " And for every algorithm that we have implemented, we are faster and less memory-intensive than", "tokens": [400, 337, 633, 9284, 300, 321, 362, 12270, 11, 321, 366, 4663, 293, 1570, 4675, 12, 686, 2953, 813], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 264, "seek": 118940, "start": 1205.2800000000002, "end": 1207.0400000000002, "text": " the Java implementations.", "tokens": [264, 10745, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 265, "seek": 118940, "start": 1207.0400000000002, "end": 1208.0400000000002, "text": " It's not just about that.", "tokens": [467, 311, 406, 445, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 266, "seek": 118940, "start": 1208.0400000000002, "end": 1211.0800000000002, "text": " It's also predictable behavior.", "tokens": [467, 311, 611, 27737, 5223, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 267, "seek": 118940, "start": 1211.0800000000002, "end": 1217.92, "text": " No latency spikes, no allocation rates, no cheat compiler that does things in the back.", "tokens": [883, 27043, 28997, 11, 572, 27599, 6846, 11, 572, 17470, 31958, 300, 775, 721, 294, 264, 646, 13], "temperature": 0.0, "avg_logprob": -0.18399911360307172, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.00020374920859467238}, {"id": 268, "seek": 121792, "start": 1217.92, "end": 1223.16, "text": " And just quickly showing what we want to do from the future, of course, expanding all", "tokens": [400, 445, 2661, 4099, 437, 321, 528, 281, 360, 490, 264, 2027, 11, 295, 1164, 11, 14702, 439], "temperature": 0.0, "avg_logprob": -0.21025705941115755, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.00043125791125930846}, {"id": 269, "seek": 121792, "start": 1223.16, "end": 1224.16, "text": " the things.", "tokens": [264, 721, 13], "temperature": 0.0, "avg_logprob": -0.21025705941115755, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.00043125791125930846}, {"id": 270, "seek": 121792, "start": 1224.16, "end": 1231.04, "text": " And if you want to play around with it, feel welcome and open issues.", "tokens": [400, 498, 291, 528, 281, 862, 926, 365, 309, 11, 841, 2928, 293, 1269, 2663, 13], "temperature": 0.0, "avg_logprob": -0.21025705941115755, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.00043125791125930846}, {"id": 271, "seek": 121792, "start": 1231.04, "end": 1235.48, "text": " There's also a longer version of this talk because I'm already out of time, so thank", "tokens": [821, 311, 611, 257, 2854, 3037, 295, 341, 751, 570, 286, 478, 1217, 484, 295, 565, 11, 370, 1309], "temperature": 0.0, "avg_logprob": -0.21025705941115755, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.00043125791125930846}, {"id": 272, "seek": 121792, "start": 1235.48, "end": 1236.48, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.21025705941115755, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.00043125791125930846}, {"id": 273, "seek": 123648, "start": 1236.48, "end": 1251.72, "text": " We don't have time for all of this.", "tokens": [50364, 492, 500, 380, 362, 565, 337, 439, 295, 341, 13, 51126], "temperature": 0.0, "avg_logprob": -0.7576583715585562, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.0003508488880470395}], "language": "en"}