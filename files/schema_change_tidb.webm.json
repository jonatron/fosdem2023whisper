{"text": " I'm Mattias, I work at PingCap. We are doing a distributed SQL database called the TIDB. It's MySQL compatible, so for the clients it just looks the same, and I do have a short talk about online skimages at scale in TIDB. Similar to MySQL, a distributed database is slightly different. MySQL does a metadata lock, so it basically needs to stop the world, no transaction can go through the metadata lock just to change the metadata. That means that it's a short lock when you do any kind of DDL, but it also means that when you're doing replication, this metadata lock actually stops replication a bit, so if it's not an instant DDL, you would start getting replication delay when the DDL goes through. So a distributed database is of course different. From the client perspective, you should just see a normal database. You should just expect it to be transactional, it should be acid compliant query with your normal SQL queries. For the user, you shouldn't see any changes, but of course underneath it's distributed on multiple nodes, et cetera. So if you take ad index as an example of a DDL, we can't do the synchronous stop the world scenario with the MDL example in MySQL, so that's something that we need to solve. During that, we do need to copy and create all the index entries for creating a new index while normal traffic comes in. So in the beginning, MySQL did more or less stop the full table and copied everything over and then it released it again. Nowadays, they are much better on the online and only keeping the metadata lock. But that's something we need to do better in a distributed database. So the proposed solution is to version the schema, so every change you're doing to a schema or a table, you do that as a specific version. You need to allow sessions to use either the most up-to-date version to the current version of the schema or a previous version. So then you can do transitions in between these states or versions. And we need to guarantee that the states between the previous version and the current version are compatible. So that basically means we need to create some kind of states from the before or the start states to the public state where it's usable. And I think it's easiest to go backwards to actually see what kind of states are needed. So here the VN, it's the current version. And we start by the public, it's the end state, everyone sees the index. So selects goes there, insert updates, everything goes there. The previous version, we can actually remove the selects, but it still needs to do all the updates, insert some deletes there. And as you see, regardless if you're, so the time here can be a bit confusing. So the transactions are of course using the real time, current time, but it might see a different version of the schema because we can't require all transactions to constantly check for the new schema and stop the world for that. Let's then move and say we are in this write only state. Before state for that, then of course we cannot do selects. Do we need to do inserts before to make them compatible? Well, we don't actually serve the reads. So we do not need to do the inserts in this state. Backfill will help with it. And then of course comes the question, how would backfill handle it? So for backfill to handle this correctly, we actually need to have another state between public and the write only state. And as you see, statewide for transactions, it doesn't really change anything, but it gives time for doing this backfilling because when we enter the write reorganization state, then we know that the state before, it's the write only. So all changes will be double write. That means that updates can be a bit tricky because we say we're not doing insert, but how do we handle updates? So we need to go a bit deeper to see how that's handled. In the add index example, we do have the table data that's public. So everyone should be able to read directly from the table without the index. So let's say we have at time zero, a session that sees this new write only state. And it does an insert. It inserts into the table, and it updates the index. So you can find the row through the index. Then later on, another session comes in, but that session has not yet transitioned to this write only state. So it's in the state before, and it wants to update this. So it goes to the table and updates the row. That's public, so that's what it needs to do. But then how about the index? We don't actually need to insert into the index here because that will be handled by the backfill in the write organization state. But the trick here is that we actually need to remove the old entry as a part of the update. So update actually means that we need to propagate the deletes into this new index object, but we do not need to do the inserts. So we need to propagate updates as delete only. And that also makes it easy to handle the delete, so we do need to handle deletes in the new index. That also gives a name for the state, so delete only state. When you're reading this, it's inspired by a paper from Google about online asynchronous schema changes in F1, so on top of Spanner. Then it takes some time before you understand exactly why you do need a delete state. But this is the reason, so we'll be able to move through the different stages. I'm not inserting the new row in the index or the new entry in the index. Does that not mean that nothing else in the system can use it because you have to wait for the backfill to complete? Yeah, so you don't read from the index until it goes public. It should complete, okay, so you have to wait for it and it doesn't mess around the way. It just delays that. You could have done it at the same time while you're all deleting. So since if you would insert it, then it would more or less be overwritten by the reorg phase anyway because the reorg needs to read from a snapshot and take all that data. So a snapshot taken somewhere when everything were on the right only. So it would just be overhead of doing the insert. It wouldn't actually mess up anything. It would still be correct, but it would just be unnecessary. And then if we move on from the delete only state, the previous version can actually be the start state because as long as deletes are done, the previous version does not need to do anything that really states. So there we have the different states that it needs to transition through for keeping transactions running without being blocked. So here we do have the full part of the asynchronous DDL in online, that's done online in a distributed database. Do you support distributed transactions and if you do, what transactions in XA prepare state? So we do not support XA transactions right now, but of course if you're connected to different SQL nodes, it looks just like it is a master or a primary wherever. So full read and write in however you connect. So transaction is a bit slightly different. You cannot have transactions spanning more than two versions. So you need to either wait or you need to block, stop and fail transactions that are too long-running. Okay. And these versions, you have like several versions associated with a single or nice game of change. Yes. Yes. So a single DDL goes through multiple stages. And currently I'm actually working with partitioning and for alter table reorganize partition where you take one set of partitions into a new set, then there's another thing. So during the reorganized phase when you're copying data, you do select from the old one then you go to public, so you select from this one, which means that if someone is actually on the right reorganization state, then they will select from that that's not updated in this one. So you need to add an additional state between the right reorganization and the public state just for moving the select. So it's a double right while moving the reads. And all this is done in tidy B and I'm not sure how many is familiar with tidy B. Okay. Good. Then let's do a quick introduction to this tidy B is mainly architecture around three different components. You have PD which stands for placement driver. It creates the timestamps for transaction handling and it knows about the data locations. So it knows where the date on which node the data are. Then we have an SQL layer that is stateless. So it's very easy to spin up or scale in the different number of nodes. Here we have re-implemented the MySQL protocol. So this is actually written in Go. And all of it is in Apache 2 license. So we do not share any code from MySQL or Maria. It's completely new since 2015 when the project started. And then we have a storage layer. The base storage layer is a Thai KV, so it's a distributed key value store. We even have people that run stats as a distributed key value store and don't bother about the SQL part. So that's what you can do as well. And then we do also have an additional, an optional way of storing the data in what we call Thai Flash. That's a column store. So by connecting it here you can actually do analytics like aggregations and so on on the same data within the same transaction even. And the optimizer here would choose what is the fastest way. What has the lowest cost for executing the query. So you don't have any ETL or anything like that in between. It's very easy to just add. You're doing all the tables and set the Thai Flash replica equals one or two or if you add more than one, then you also get the MPP, so massive parallel processing part of it. We do have an, you can run Spark on it as well. And let's just go down slightly deeper on how we actually store the data. So we take all this data and split it into ranges about 100 megabytes and each such range is stored in three, or yeah it's configurable, let's say three copies in the Thai KV storage nodes and each such region is forming a raft group. So that's how it keeps the HA and the high availability. Thai KV is using ROXDB as lower level storage. So it's an LSM tree, yeah it's similar as MIROX in Percona or MariaDB. So it's not B-tree based. Through this raft protocol, that's how we also can connect the column store. So that's how we also have it, so you can run it in the same transaction and even if you have a join, maybe it's faster to execute parts of it through an index in the row store and then do some of the table scans and aggregation in Thai Flash in the column store. And this is optional, but this is not, this is the base. You always need to have the row store and you can have this as an option. There's a lot of tooling that works. So first of all, I would say that the data migration, so it's easy to have a ThaiDB cluster to read the binary logs or just set it up for dumping an upstream MySQL instance or even several instances into the same cluster so you can combine all the data back. We have the backup and restore, very good dump story. I think that even works with MySQL. You have the tool for do a diff between the different instances, change data capture that can go to either another ThaiDB cluster or MySQL instance, go through Kafka as well if you want. Try up, that's a way for managing and deploying ThaiDB and all components you want. You can even use it as a playground to start it in your laptop. It will download the binaries and start everything, including monitoring everything. So it's very easy to just try out. We have an operator if you want to run it in Kubernetes as well in the cloud. So we even have it as a cloud service, you can do anything from on-prem up to a cloud service in many different ways. And we also have Lightning, which is an optimized import tool, and that's what I will actually use in the next slide soon. A year ago, we started a project because we heard and compared the ad index performance in ThaiDB cluster versus, for example, Cassandra or Aurora. And at that time, we were basically three times slower because we haven't optimized that it was just stable proven and it worked, but it was not fast. And that's especially when you're doing proof of concept or loading the data, that's where it's really beneficial to speed it up. And the way it worked, it would just do data copying through small transaction batches more or less. So that also creates a lot of overhead with transaction handling, et cetera. That's not actually needed when you're doing a backfill because during backfill process, during the data, it doesn't actually need to be transactional. And it's only a single node that does this, a single TIDB node that orchestrates it. I'm not going to go deep into this, it basically just shows how you're creating a command in one ThaiDB node and it goes into a table, a ThaiDB owner will do it, go through the different steps and do the data migrations and data copying. So what we did first was create a feature with this feature flag. It uses this lightning the import tool technology. It's completely built in in ThaiDB cluster, so it's not the external tool. But it reads the data and then it creates these SSD files for ingestion in RocksDB. So it's very efficient load and it has very low impact on the storage side. It just moves these files into the storage and enables them and takes them into the RocksDB levels. The result was around three times speed up and of course a lot less impact on normal load in the cluster. So even if you have a highly loaded cluster, you can do this almost without impacting it. And then we did a bit of analysis of where we could improve even more and there was things like the scheduling could be improved just to shorten the time. Instead of reading directly from the key value store, we could use these co-operators, co-processors for removing columns that's not needed, for example, for doing optimized scans, etc. We disconnected the read and write dependencies so they could run in parallel in asynchronous and a lot of other small optimization. And that created yet another three to five times speed up. So all in all, during the last year, we had done 10 times improvement in speed while we're still only using a single TIDB node and now we're three times faster than the baseline of the other implementations in Cockroach and Aurora that we have compared with. And there's a bit more to do, so we're currently looking into how we even can distribute this instead of running it on a single TIDB node and also being able to auto-tune the priority. So if you have load that goes a bit up and a bit down, so the DDL work can adjust to that. And that is, if you depend on a single TIDB node, if that breaks for any reason, then your basis is going back to the previous stage, is it? Yeah, so we have a state state, so we go back a little bit, a little bit, but you don't need to redo the whole feeling of the index or anything like that. And yeah, it's all on GitHub. If you're interested a bit in how it actually works, I would recommend go to OSS Insights. I would say it's a demo site. It runs TIDB in the background, and it's a simple web UI, quite nice UI on top. But it has all of the events from GitHub, so currently it's 5.5 billion records, and we store it in a single table. It's a bit other things there as well. And you can compare for your own GitHub ID or your own project, your own repository, compare it, and so on, and check some different frameworks, et cetera. It's quite cool, actually. Tie-up is very useful if you want to try it on your own laptop or in your own data center. Of course, you can go to TIDB cloud as well, but I didn't mention that here because that's our commercial offer. Something else that we have that is not directly connected, it's chaos mesh. So if you have a system on Kubernetes and you want to see how it handles different errors, you can use that for injecting errors. That's something that we used for stabilizing and testing out the TIDB cluster. Then I think I'm out of time. Perfect timing, so you have time to answer questions? Yeah. Yeah? First of all, I'm very interested in how do you organize the htap transitioning. I mean, you have both storages, and I miss the way you move the data from row into column or format. I believe you do double copy. You have double copies of the data itself. So we always have the copy here, and the raft leader of the group is always here. So you do have raft leader and raft follower in the Thai KV, and then we extended the raft protocol. So we have learner states here, so they can never become leaders. So that's how we do. So this is a must, and this is optional. What about the optimizer model? How do you calculate the cost-based approach to understand which storage format you use? And it's also the influence of the volcano optimizer model, so that's how you more or less pipeline the different things and can move parts of the pipeline into an MPP framework that handles the column store. And I wonder if this model and the optimizer are dispersed across the multiple partitions of the TIDB operator, or it's in single? So the optimizer, that's in the TIDB project, in the TIDB repository. So the SQL node, and when it executes, it's pushed down this co-processor request and also through the MPP framework for pushing down query fragments or the co-processor request into either TIDB or a Thai KV or two Thai flash. So for example, if you're doing a join where one part of the table can be resolved fast by an index lookup, then it will go here for that part of the table. And for another table, it might be a big table scan or aggregation that will be faster here. So then it actually can combine that. But do you, your cost-based model is based on some assumptions about the cost of these corporations, right? I'm not sure. I don't know the details deep enough for answer that. And last question, how do you test the compatibility of my SQL client protocol between your implementation and because it's a big question. Yeah, yeah. So we don't have any own connectors or anything like that. We just relying on MySQL connectors or MariaDB connectors. And that's what we're using when we're testing. So you basically have the test use that tries different kind of queries and after they pass it, you understand that they are somehow equal. Yeah. And of course, there are differences. But I would say the compatibility with the MySQL dialect, it's very, very high. But of course, like management commands for replication doesn't work because we don't do replication. We have internal replication or we use change data capture for transfer to another cluster. Thank you. Last question. What that clash does when there is high rate of single cell updates, like how it handles this, like rewriting the code files or keeping it separate? It's a derivative of click house. So it caches the changes and then it updates it partially or rewrites the whole. Can this kind of get clicked behind after TKV because it takes more time? It can. But if it's behind, then it will more or less fall back here. You have some tweaking options. You can even do it as optimizer hints that you want to use either engine, for example, etc. Thank you. If there are more questions, I'm sure Mattias will be able to answer. Yeah, I'm here. Even Daniel is here as well. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.28, "text": " I'm Mattias, I work at PingCap.", "tokens": [286, 478, 7397, 4609, 11, 286, 589, 412, 33645, 46671, 13], "temperature": 0.0, "avg_logprob": -0.34440568564594654, "compression_ratio": 1.3313253012048192, "no_speech_prob": 0.07698113471269608}, {"id": 1, "seek": 0, "start": 8.28, "end": 11.76, "text": " We are doing a distributed SQL database called the TIDB.", "tokens": [492, 366, 884, 257, 12631, 19200, 8149, 1219, 264, 314, 2777, 33, 13], "temperature": 0.0, "avg_logprob": -0.34440568564594654, "compression_ratio": 1.3313253012048192, "no_speech_prob": 0.07698113471269608}, {"id": 2, "seek": 0, "start": 11.76, "end": 17.92, "text": " It's MySQL compatible, so for the clients it just looks the same, and I do have a short", "tokens": [467, 311, 1222, 39934, 18218, 11, 370, 337, 264, 6982, 309, 445, 1542, 264, 912, 11, 293, 286, 360, 362, 257, 2099], "temperature": 0.0, "avg_logprob": -0.34440568564594654, "compression_ratio": 1.3313253012048192, "no_speech_prob": 0.07698113471269608}, {"id": 3, "seek": 0, "start": 17.92, "end": 25.8, "text": " talk about online skimages at scale in TIDB.", "tokens": [751, 466, 2950, 1110, 332, 1660, 412, 4373, 294, 314, 2777, 33, 13], "temperature": 0.0, "avg_logprob": -0.34440568564594654, "compression_ratio": 1.3313253012048192, "no_speech_prob": 0.07698113471269608}, {"id": 4, "seek": 2580, "start": 25.8, "end": 31.44, "text": " Similar to MySQL, a distributed database is slightly different.", "tokens": [10905, 281, 1222, 39934, 11, 257, 12631, 8149, 307, 4748, 819, 13], "temperature": 0.0, "avg_logprob": -0.18599074819813605, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.209750138921663e-05}, {"id": 5, "seek": 2580, "start": 31.44, "end": 36.84, "text": " MySQL does a metadata lock, so it basically needs to stop the world, no transaction can", "tokens": [1222, 39934, 775, 257, 26603, 4017, 11, 370, 309, 1936, 2203, 281, 1590, 264, 1002, 11, 572, 14425, 393], "temperature": 0.0, "avg_logprob": -0.18599074819813605, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.209750138921663e-05}, {"id": 6, "seek": 2580, "start": 36.84, "end": 42.72, "text": " go through the metadata lock just to change the metadata.", "tokens": [352, 807, 264, 26603, 4017, 445, 281, 1319, 264, 26603, 13], "temperature": 0.0, "avg_logprob": -0.18599074819813605, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.209750138921663e-05}, {"id": 7, "seek": 2580, "start": 42.72, "end": 48.88, "text": " That means that it's a short lock when you do any kind of DDL, but it also means that", "tokens": [663, 1355, 300, 309, 311, 257, 2099, 4017, 562, 291, 360, 604, 733, 295, 30778, 43, 11, 457, 309, 611, 1355, 300], "temperature": 0.0, "avg_logprob": -0.18599074819813605, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.209750138921663e-05}, {"id": 8, "seek": 2580, "start": 48.88, "end": 54.64, "text": " when you're doing replication, this metadata lock actually stops replication a bit, so", "tokens": [562, 291, 434, 884, 39911, 11, 341, 26603, 4017, 767, 10094, 39911, 257, 857, 11, 370], "temperature": 0.0, "avg_logprob": -0.18599074819813605, "compression_ratio": 1.6977777777777778, "no_speech_prob": 3.209750138921663e-05}, {"id": 9, "seek": 5464, "start": 54.64, "end": 60.160000000000004, "text": " if it's not an instant DDL, you would start getting replication delay when the DDL goes", "tokens": [498, 309, 311, 406, 364, 9836, 30778, 43, 11, 291, 576, 722, 1242, 39911, 8577, 562, 264, 30778, 43, 1709], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 10, "seek": 5464, "start": 60.160000000000004, "end": 63.68, "text": " through.", "tokens": [807, 13], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 11, "seek": 5464, "start": 63.68, "end": 67.32, "text": " So a distributed database is of course different.", "tokens": [407, 257, 12631, 8149, 307, 295, 1164, 819, 13], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 12, "seek": 5464, "start": 67.32, "end": 71.44, "text": " From the client perspective, you should just see a normal database.", "tokens": [3358, 264, 6423, 4585, 11, 291, 820, 445, 536, 257, 2710, 8149, 13], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 13, "seek": 5464, "start": 71.44, "end": 77.68, "text": " You should just expect it to be transactional, it should be acid compliant query with your", "tokens": [509, 820, 445, 2066, 309, 281, 312, 46688, 1966, 11, 309, 820, 312, 8258, 36248, 14581, 365, 428], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 14, "seek": 5464, "start": 77.68, "end": 79.52, "text": " normal SQL queries.", "tokens": [2710, 19200, 24109, 13], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 15, "seek": 5464, "start": 79.52, "end": 83.88, "text": " For the user, you shouldn't see any changes, but of course underneath it's distributed", "tokens": [1171, 264, 4195, 11, 291, 4659, 380, 536, 604, 2962, 11, 457, 295, 1164, 7223, 309, 311, 12631], "temperature": 0.0, "avg_logprob": -0.18561872482299804, "compression_ratio": 1.6816326530612244, "no_speech_prob": 1.778333717084024e-05}, {"id": 16, "seek": 8388, "start": 83.88, "end": 87.08, "text": " on multiple nodes, et cetera.", "tokens": [322, 3866, 13891, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1536382039388021, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.570236968807876e-05}, {"id": 17, "seek": 8388, "start": 87.08, "end": 92.88, "text": " So if you take ad index as an example of a DDL, we can't do the synchronous stop the", "tokens": [407, 498, 291, 747, 614, 8186, 382, 364, 1365, 295, 257, 30778, 43, 11, 321, 393, 380, 360, 264, 44743, 1590, 264], "temperature": 0.0, "avg_logprob": -0.1536382039388021, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.570236968807876e-05}, {"id": 18, "seek": 8388, "start": 92.88, "end": 101.08, "text": " world scenario with the MDL example in MySQL, so that's something that we need to solve.", "tokens": [1002, 9005, 365, 264, 22521, 43, 1365, 294, 1222, 39934, 11, 370, 300, 311, 746, 300, 321, 643, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.1536382039388021, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.570236968807876e-05}, {"id": 19, "seek": 8388, "start": 101.08, "end": 106.28, "text": " During that, we do need to copy and create all the index entries for creating a new index", "tokens": [6842, 300, 11, 321, 360, 643, 281, 5055, 293, 1884, 439, 264, 8186, 23041, 337, 4084, 257, 777, 8186], "temperature": 0.0, "avg_logprob": -0.1536382039388021, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.570236968807876e-05}, {"id": 20, "seek": 8388, "start": 106.28, "end": 109.36, "text": " while normal traffic comes in.", "tokens": [1339, 2710, 6419, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.1536382039388021, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.570236968807876e-05}, {"id": 21, "seek": 10936, "start": 109.36, "end": 115.76, "text": " So in the beginning, MySQL did more or less stop the full table and copied everything over", "tokens": [407, 294, 264, 2863, 11, 1222, 39934, 630, 544, 420, 1570, 1590, 264, 1577, 3199, 293, 25365, 1203, 670], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 22, "seek": 10936, "start": 115.76, "end": 117.28, "text": " and then it released it again.", "tokens": [293, 550, 309, 4736, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 23, "seek": 10936, "start": 117.28, "end": 122.2, "text": " Nowadays, they are much better on the online and only keeping the metadata lock.", "tokens": [28908, 11, 436, 366, 709, 1101, 322, 264, 2950, 293, 787, 5145, 264, 26603, 4017, 13], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 24, "seek": 10936, "start": 122.2, "end": 127.88, "text": " But that's something we need to do better in a distributed database.", "tokens": [583, 300, 311, 746, 321, 643, 281, 360, 1101, 294, 257, 12631, 8149, 13], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 25, "seek": 10936, "start": 127.88, "end": 132.96, "text": " So the proposed solution is to version the schema, so every change you're doing to a", "tokens": [407, 264, 10348, 3827, 307, 281, 3037, 264, 34078, 11, 370, 633, 1319, 291, 434, 884, 281, 257], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 26, "seek": 10936, "start": 132.96, "end": 137.76, "text": " schema or a table, you do that as a specific version.", "tokens": [34078, 420, 257, 3199, 11, 291, 360, 300, 382, 257, 2685, 3037, 13], "temperature": 0.0, "avg_logprob": -0.11991327115804842, "compression_ratio": 1.64, "no_speech_prob": 2.9266360797919333e-05}, {"id": 27, "seek": 13776, "start": 137.76, "end": 143.76, "text": " You need to allow sessions to use either the most up-to-date version to the current version", "tokens": [509, 643, 281, 2089, 11081, 281, 764, 2139, 264, 881, 493, 12, 1353, 12, 17393, 3037, 281, 264, 2190, 3037], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 28, "seek": 13776, "start": 143.76, "end": 146.04, "text": " of the schema or a previous version.", "tokens": [295, 264, 34078, 420, 257, 3894, 3037, 13], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 29, "seek": 13776, "start": 146.04, "end": 151.23999999999998, "text": " So then you can do transitions in between these states or versions.", "tokens": [407, 550, 291, 393, 360, 23767, 294, 1296, 613, 4368, 420, 9606, 13], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 30, "seek": 13776, "start": 151.23999999999998, "end": 157.76, "text": " And we need to guarantee that the states between the previous version and the current version", "tokens": [400, 321, 643, 281, 10815, 300, 264, 4368, 1296, 264, 3894, 3037, 293, 264, 2190, 3037], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 31, "seek": 13776, "start": 157.76, "end": 160.04, "text": " are compatible.", "tokens": [366, 18218, 13], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 32, "seek": 13776, "start": 160.04, "end": 164.95999999999998, "text": " So that basically means we need to create some kind of states from the before or the", "tokens": [407, 300, 1936, 1355, 321, 643, 281, 1884, 512, 733, 295, 4368, 490, 264, 949, 420, 264], "temperature": 0.0, "avg_logprob": -0.10330342722463083, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.00013630239118356258}, {"id": 33, "seek": 16496, "start": 164.96, "end": 172.56, "text": " start states to the public state where it's usable.", "tokens": [722, 4368, 281, 264, 1908, 1785, 689, 309, 311, 29975, 13], "temperature": 0.0, "avg_logprob": -0.14273595248951632, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00013596487406175584}, {"id": 34, "seek": 16496, "start": 172.56, "end": 177.64000000000001, "text": " And I think it's easiest to go backwards to actually see what kind of states are needed.", "tokens": [400, 286, 519, 309, 311, 12889, 281, 352, 12204, 281, 767, 536, 437, 733, 295, 4368, 366, 2978, 13], "temperature": 0.0, "avg_logprob": -0.14273595248951632, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00013596487406175584}, {"id": 35, "seek": 16496, "start": 177.64000000000001, "end": 181.96, "text": " So here the VN, it's the current version.", "tokens": [407, 510, 264, 691, 45, 11, 309, 311, 264, 2190, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14273595248951632, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00013596487406175584}, {"id": 36, "seek": 16496, "start": 181.96, "end": 187.16, "text": " And we start by the public, it's the end state, everyone sees the index.", "tokens": [400, 321, 722, 538, 264, 1908, 11, 309, 311, 264, 917, 1785, 11, 1518, 8194, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.14273595248951632, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00013596487406175584}, {"id": 37, "seek": 16496, "start": 187.16, "end": 192.48000000000002, "text": " So selects goes there, insert updates, everything goes there.", "tokens": [407, 3048, 82, 1709, 456, 11, 8969, 9205, 11, 1203, 1709, 456, 13], "temperature": 0.0, "avg_logprob": -0.14273595248951632, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00013596487406175584}, {"id": 38, "seek": 19248, "start": 192.48, "end": 197.79999999999998, "text": " The previous version, we can actually remove the selects, but it still needs to do all", "tokens": [440, 3894, 3037, 11, 321, 393, 767, 4159, 264, 3048, 82, 11, 457, 309, 920, 2203, 281, 360, 439], "temperature": 0.0, "avg_logprob": -0.1477831768733199, "compression_ratio": 1.6637554585152838, "no_speech_prob": 7.494505553040653e-05}, {"id": 39, "seek": 19248, "start": 197.79999999999998, "end": 202.23999999999998, "text": " the updates, insert some deletes there.", "tokens": [264, 9205, 11, 8969, 512, 1103, 37996, 456, 13], "temperature": 0.0, "avg_logprob": -0.1477831768733199, "compression_ratio": 1.6637554585152838, "no_speech_prob": 7.494505553040653e-05}, {"id": 40, "seek": 19248, "start": 202.23999999999998, "end": 208.35999999999999, "text": " And as you see, regardless if you're, so the time here can be a bit confusing.", "tokens": [400, 382, 291, 536, 11, 10060, 498, 291, 434, 11, 370, 264, 565, 510, 393, 312, 257, 857, 13181, 13], "temperature": 0.0, "avg_logprob": -0.1477831768733199, "compression_ratio": 1.6637554585152838, "no_speech_prob": 7.494505553040653e-05}, {"id": 41, "seek": 19248, "start": 208.35999999999999, "end": 214.32, "text": " So the transactions are of course using the real time, current time, but it might see", "tokens": [407, 264, 16856, 366, 295, 1164, 1228, 264, 957, 565, 11, 2190, 565, 11, 457, 309, 1062, 536], "temperature": 0.0, "avg_logprob": -0.1477831768733199, "compression_ratio": 1.6637554585152838, "no_speech_prob": 7.494505553040653e-05}, {"id": 42, "seek": 19248, "start": 214.32, "end": 222.44, "text": " a different version of the schema because we can't require all transactions to constantly", "tokens": [257, 819, 3037, 295, 264, 34078, 570, 321, 393, 380, 3651, 439, 16856, 281, 6460], "temperature": 0.0, "avg_logprob": -0.1477831768733199, "compression_ratio": 1.6637554585152838, "no_speech_prob": 7.494505553040653e-05}, {"id": 43, "seek": 22244, "start": 222.44, "end": 229.48, "text": " check for the new schema and stop the world for that.", "tokens": [1520, 337, 264, 777, 34078, 293, 1590, 264, 1002, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 44, "seek": 22244, "start": 229.48, "end": 233.07999999999998, "text": " Let's then move and say we are in this write only state.", "tokens": [961, 311, 550, 1286, 293, 584, 321, 366, 294, 341, 2464, 787, 1785, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 45, "seek": 22244, "start": 233.07999999999998, "end": 237.92, "text": " Before state for that, then of course we cannot do selects.", "tokens": [4546, 1785, 337, 300, 11, 550, 295, 1164, 321, 2644, 360, 3048, 82, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 46, "seek": 22244, "start": 237.92, "end": 241.6, "text": " Do we need to do inserts before to make them compatible?", "tokens": [1144, 321, 643, 281, 360, 49163, 949, 281, 652, 552, 18218, 30], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 47, "seek": 22244, "start": 241.6, "end": 245.48, "text": " Well, we don't actually serve the reads.", "tokens": [1042, 11, 321, 500, 380, 767, 4596, 264, 15700, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 48, "seek": 22244, "start": 245.48, "end": 250.2, "text": " So we do not need to do the inserts in this state.", "tokens": [407, 321, 360, 406, 643, 281, 360, 264, 49163, 294, 341, 1785, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 49, "seek": 22244, "start": 250.2, "end": 252.0, "text": " Backfill will help with it.", "tokens": [5833, 31072, 486, 854, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.18348131374436982, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.00033235823502764106}, {"id": 50, "seek": 25200, "start": 252.0, "end": 256.8, "text": " And then of course comes the question, how would backfill handle it?", "tokens": [400, 550, 295, 1164, 1487, 264, 1168, 11, 577, 576, 646, 31072, 4813, 309, 30], "temperature": 0.0, "avg_logprob": -0.11974182348141725, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0001292713131988421}, {"id": 51, "seek": 25200, "start": 256.8, "end": 262.24, "text": " So for backfill to handle this correctly, we actually need to have another state between", "tokens": [407, 337, 646, 31072, 281, 4813, 341, 8944, 11, 321, 767, 643, 281, 362, 1071, 1785, 1296], "temperature": 0.0, "avg_logprob": -0.11974182348141725, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0001292713131988421}, {"id": 52, "seek": 25200, "start": 262.24, "end": 266.08, "text": " public and the write only state.", "tokens": [1908, 293, 264, 2464, 787, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11974182348141725, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0001292713131988421}, {"id": 53, "seek": 25200, "start": 266.08, "end": 272.56, "text": " And as you see, statewide for transactions, it doesn't really change anything, but it", "tokens": [400, 382, 291, 536, 11, 34487, 337, 16856, 11, 309, 1177, 380, 534, 1319, 1340, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.11974182348141725, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0001292713131988421}, {"id": 54, "seek": 25200, "start": 272.56, "end": 279.08, "text": " gives time for doing this backfilling because when we enter the write reorganization state,", "tokens": [2709, 565, 337, 884, 341, 646, 69, 7345, 570, 562, 321, 3242, 264, 2464, 41203, 2144, 1785, 11], "temperature": 0.0, "avg_logprob": -0.11974182348141725, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0001292713131988421}, {"id": 55, "seek": 27908, "start": 279.08, "end": 282.52, "text": " then we know that the state before, it's the write only.", "tokens": [550, 321, 458, 300, 264, 1785, 949, 11, 309, 311, 264, 2464, 787, 13], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 56, "seek": 27908, "start": 282.52, "end": 288.96, "text": " So all changes will be double write.", "tokens": [407, 439, 2962, 486, 312, 3834, 2464, 13], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 57, "seek": 27908, "start": 288.96, "end": 294.24, "text": " That means that updates can be a bit tricky because we say we're not doing insert, but", "tokens": [663, 1355, 300, 9205, 393, 312, 257, 857, 12414, 570, 321, 584, 321, 434, 406, 884, 8969, 11, 457], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 58, "seek": 27908, "start": 294.24, "end": 296.15999999999997, "text": " how do we handle updates?", "tokens": [577, 360, 321, 4813, 9205, 30], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 59, "seek": 27908, "start": 296.15999999999997, "end": 303.15999999999997, "text": " So we need to go a bit deeper to see how that's handled.", "tokens": [407, 321, 643, 281, 352, 257, 857, 7731, 281, 536, 577, 300, 311, 18033, 13], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 60, "seek": 27908, "start": 303.15999999999997, "end": 308.0, "text": " In the add index example, we do have the table data that's public.", "tokens": [682, 264, 909, 8186, 1365, 11, 321, 360, 362, 264, 3199, 1412, 300, 311, 1908, 13], "temperature": 0.0, "avg_logprob": -0.1349501713462498, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00011423390242271125}, {"id": 61, "seek": 30800, "start": 308.0, "end": 313.52, "text": " So everyone should be able to read directly from the table without the index.", "tokens": [407, 1518, 820, 312, 1075, 281, 1401, 3838, 490, 264, 3199, 1553, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.12775648752848306, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.00017435372865293175}, {"id": 62, "seek": 30800, "start": 313.52, "end": 320.88, "text": " So let's say we have at time zero, a session that sees this new write only state.", "tokens": [407, 718, 311, 584, 321, 362, 412, 565, 4018, 11, 257, 5481, 300, 8194, 341, 777, 2464, 787, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12775648752848306, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.00017435372865293175}, {"id": 63, "seek": 30800, "start": 320.88, "end": 322.28, "text": " And it does an insert.", "tokens": [400, 309, 775, 364, 8969, 13], "temperature": 0.0, "avg_logprob": -0.12775648752848306, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.00017435372865293175}, {"id": 64, "seek": 30800, "start": 322.28, "end": 329.08, "text": " It inserts into the table, and it updates the index.", "tokens": [467, 49163, 666, 264, 3199, 11, 293, 309, 9205, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.12775648752848306, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.00017435372865293175}, {"id": 65, "seek": 30800, "start": 329.08, "end": 334.16, "text": " So you can find the row through the index.", "tokens": [407, 291, 393, 915, 264, 5386, 807, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.12775648752848306, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.00017435372865293175}, {"id": 66, "seek": 33416, "start": 334.16, "end": 341.72, "text": " Then later on, another session comes in, but that session has not yet transitioned to this", "tokens": [1396, 1780, 322, 11, 1071, 5481, 1487, 294, 11, 457, 300, 5481, 575, 406, 1939, 47346, 281, 341], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 67, "seek": 33416, "start": 341.72, "end": 342.72, "text": " write only state.", "tokens": [2464, 787, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 68, "seek": 33416, "start": 342.72, "end": 348.24, "text": " So it's in the state before, and it wants to update this.", "tokens": [407, 309, 311, 294, 264, 1785, 949, 11, 293, 309, 2738, 281, 5623, 341, 13], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 69, "seek": 33416, "start": 348.24, "end": 351.04, "text": " So it goes to the table and updates the row.", "tokens": [407, 309, 1709, 281, 264, 3199, 293, 9205, 264, 5386, 13], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 70, "seek": 33416, "start": 351.04, "end": 354.24, "text": " That's public, so that's what it needs to do.", "tokens": [663, 311, 1908, 11, 370, 300, 311, 437, 309, 2203, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 71, "seek": 33416, "start": 354.24, "end": 357.24, "text": " But then how about the index?", "tokens": [583, 550, 577, 466, 264, 8186, 30], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 72, "seek": 33416, "start": 357.24, "end": 362.8, "text": " We don't actually need to insert into the index here because that will be handled by", "tokens": [492, 500, 380, 767, 643, 281, 8969, 666, 264, 8186, 510, 570, 300, 486, 312, 18033, 538], "temperature": 0.0, "avg_logprob": -0.12570101671879835, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00021183706121519208}, {"id": 73, "seek": 36280, "start": 362.8, "end": 366.36, "text": " the backfill in the write organization state.", "tokens": [264, 646, 31072, 294, 264, 2464, 4475, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12476562261581421, "compression_ratio": 1.7307692307692308, "no_speech_prob": 7.577942596981302e-05}, {"id": 74, "seek": 36280, "start": 366.36, "end": 375.0, "text": " But the trick here is that we actually need to remove the old entry as a part of the update.", "tokens": [583, 264, 4282, 510, 307, 300, 321, 767, 643, 281, 4159, 264, 1331, 8729, 382, 257, 644, 295, 264, 5623, 13], "temperature": 0.0, "avg_logprob": -0.12476562261581421, "compression_ratio": 1.7307692307692308, "no_speech_prob": 7.577942596981302e-05}, {"id": 75, "seek": 36280, "start": 375.0, "end": 381.8, "text": " So update actually means that we need to propagate the deletes into this new index object, but", "tokens": [407, 5623, 767, 1355, 300, 321, 643, 281, 48256, 264, 1103, 37996, 666, 341, 777, 8186, 2657, 11, 457], "temperature": 0.0, "avg_logprob": -0.12476562261581421, "compression_ratio": 1.7307692307692308, "no_speech_prob": 7.577942596981302e-05}, {"id": 76, "seek": 36280, "start": 381.8, "end": 387.22, "text": " we do not need to do the inserts.", "tokens": [321, 360, 406, 643, 281, 360, 264, 49163, 13], "temperature": 0.0, "avg_logprob": -0.12476562261581421, "compression_ratio": 1.7307692307692308, "no_speech_prob": 7.577942596981302e-05}, {"id": 77, "seek": 36280, "start": 387.22, "end": 391.36, "text": " So we need to propagate updates as delete only.", "tokens": [407, 321, 643, 281, 48256, 9205, 382, 12097, 787, 13], "temperature": 0.0, "avg_logprob": -0.12476562261581421, "compression_ratio": 1.7307692307692308, "no_speech_prob": 7.577942596981302e-05}, {"id": 78, "seek": 39136, "start": 391.36, "end": 395.96000000000004, "text": " And that also makes it easy to handle the delete, so we do need to handle deletes in", "tokens": [400, 300, 611, 1669, 309, 1858, 281, 4813, 264, 12097, 11, 370, 321, 360, 643, 281, 4813, 1103, 37996, 294], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 79, "seek": 39136, "start": 395.96000000000004, "end": 398.0, "text": " the new index.", "tokens": [264, 777, 8186, 13], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 80, "seek": 39136, "start": 398.0, "end": 402.08000000000004, "text": " That also gives a name for the state, so delete only state.", "tokens": [663, 611, 2709, 257, 1315, 337, 264, 1785, 11, 370, 12097, 787, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 81, "seek": 39136, "start": 402.08000000000004, "end": 410.24, "text": " When you're reading this, it's inspired by a paper from Google about online asynchronous", "tokens": [1133, 291, 434, 3760, 341, 11, 309, 311, 7547, 538, 257, 3035, 490, 3329, 466, 2950, 49174], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 82, "seek": 39136, "start": 410.24, "end": 414.16, "text": " schema changes in F1, so on top of Spanner.", "tokens": [34078, 2962, 294, 479, 16, 11, 370, 322, 1192, 295, 1738, 9805, 13], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 83, "seek": 39136, "start": 414.16, "end": 419.44, "text": " Then it takes some time before you understand exactly why you do need a delete state.", "tokens": [1396, 309, 2516, 512, 565, 949, 291, 1223, 2293, 983, 291, 360, 643, 257, 12097, 1785, 13], "temperature": 0.0, "avg_logprob": -0.1240857249558574, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00022371947125066072}, {"id": 84, "seek": 41944, "start": 419.44, "end": 424.92, "text": " But this is the reason, so we'll be able to move through the different stages.", "tokens": [583, 341, 307, 264, 1778, 11, 370, 321, 603, 312, 1075, 281, 1286, 807, 264, 819, 10232, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 85, "seek": 41944, "start": 424.92, "end": 430.64, "text": " I'm not inserting the new row in the index or the new entry in the index.", "tokens": [286, 478, 406, 46567, 264, 777, 5386, 294, 264, 8186, 420, 264, 777, 8729, 294, 264, 8186, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 86, "seek": 41944, "start": 430.64, "end": 434.48, "text": " Does that not mean that nothing else in the system can use it because you have to wait", "tokens": [4402, 300, 406, 914, 300, 1825, 1646, 294, 264, 1185, 393, 764, 309, 570, 291, 362, 281, 1699], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 87, "seek": 41944, "start": 434.48, "end": 435.96, "text": " for the backfill to complete?", "tokens": [337, 264, 646, 31072, 281, 3566, 30], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 88, "seek": 41944, "start": 435.96, "end": 439.24, "text": " Yeah, so you don't read from the index until it goes public.", "tokens": [865, 11, 370, 291, 500, 380, 1401, 490, 264, 8186, 1826, 309, 1709, 1908, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 89, "seek": 41944, "start": 439.24, "end": 442.28, "text": " It should complete, okay, so you have to wait for it and it doesn't mess around the way.", "tokens": [467, 820, 3566, 11, 1392, 11, 370, 291, 362, 281, 1699, 337, 309, 293, 309, 1177, 380, 2082, 926, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 90, "seek": 41944, "start": 442.28, "end": 443.28, "text": " It just delays that.", "tokens": [467, 445, 28610, 300, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 91, "seek": 41944, "start": 443.28, "end": 445.92, "text": " You could have done it at the same time while you're all deleting.", "tokens": [509, 727, 362, 1096, 309, 412, 264, 912, 565, 1339, 291, 434, 439, 48946, 13], "temperature": 0.0, "avg_logprob": -0.32625442392685833, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00030266051180660725}, {"id": 92, "seek": 44592, "start": 445.92, "end": 451.64000000000004, "text": " So since if you would insert it, then it would more or less be overwritten by the reorg", "tokens": [407, 1670, 498, 291, 576, 8969, 309, 11, 550, 309, 576, 544, 420, 1570, 312, 670, 26859, 538, 264, 319, 4646], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 93, "seek": 44592, "start": 451.64000000000004, "end": 458.48, "text": " phase anyway because the reorg needs to read from a snapshot and take all that data.", "tokens": [5574, 4033, 570, 264, 319, 4646, 2203, 281, 1401, 490, 257, 30163, 293, 747, 439, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 94, "seek": 44592, "start": 458.48, "end": 464.64, "text": " So a snapshot taken somewhere when everything were on the right only.", "tokens": [407, 257, 30163, 2726, 4079, 562, 1203, 645, 322, 264, 558, 787, 13], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 95, "seek": 44592, "start": 464.64, "end": 466.6, "text": " So it would just be overhead of doing the insert.", "tokens": [407, 309, 576, 445, 312, 19922, 295, 884, 264, 8969, 13], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 96, "seek": 44592, "start": 466.6, "end": 468.6, "text": " It wouldn't actually mess up anything.", "tokens": [467, 2759, 380, 767, 2082, 493, 1340, 13], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 97, "seek": 44592, "start": 468.6, "end": 475.40000000000003, "text": " It would still be correct, but it would just be unnecessary.", "tokens": [467, 576, 920, 312, 3006, 11, 457, 309, 576, 445, 312, 19350, 13], "temperature": 0.0, "avg_logprob": -0.19773747969646843, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.00019917383906431496}, {"id": 98, "seek": 47540, "start": 475.4, "end": 479.79999999999995, "text": " And then if we move on from the delete only state, the previous version can actually be", "tokens": [400, 550, 498, 321, 1286, 322, 490, 264, 12097, 787, 1785, 11, 264, 3894, 3037, 393, 767, 312], "temperature": 0.0, "avg_logprob": -0.11716207705046001, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0002662799379322678}, {"id": 99, "seek": 47540, "start": 479.79999999999995, "end": 486.96, "text": " the start state because as long as deletes are done, the previous version does not need", "tokens": [264, 722, 1785, 570, 382, 938, 382, 1103, 37996, 366, 1096, 11, 264, 3894, 3037, 775, 406, 643], "temperature": 0.0, "avg_logprob": -0.11716207705046001, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0002662799379322678}, {"id": 100, "seek": 47540, "start": 486.96, "end": 489.32, "text": " to do anything that really states.", "tokens": [281, 360, 1340, 300, 534, 4368, 13], "temperature": 0.0, "avg_logprob": -0.11716207705046001, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0002662799379322678}, {"id": 101, "seek": 47540, "start": 489.32, "end": 494.64, "text": " So there we have the different states that it needs to transition through for keeping", "tokens": [407, 456, 321, 362, 264, 819, 4368, 300, 309, 2203, 281, 6034, 807, 337, 5145], "temperature": 0.0, "avg_logprob": -0.11716207705046001, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0002662799379322678}, {"id": 102, "seek": 47540, "start": 494.64, "end": 498.71999999999997, "text": " transactions running without being blocked.", "tokens": [16856, 2614, 1553, 885, 15470, 13], "temperature": 0.0, "avg_logprob": -0.11716207705046001, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0002662799379322678}, {"id": 103, "seek": 49872, "start": 498.72, "end": 508.24, "text": " So here we do have the full part of the asynchronous DDL in online, that's done online in a distributed", "tokens": [407, 510, 321, 360, 362, 264, 1577, 644, 295, 264, 49174, 30778, 43, 294, 2950, 11, 300, 311, 1096, 2950, 294, 257, 12631], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 104, "seek": 49872, "start": 508.24, "end": 509.24, "text": " database.", "tokens": [8149, 13], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 105, "seek": 49872, "start": 509.24, "end": 515.6, "text": " Do you support distributed transactions and if you do, what transactions in XA prepare", "tokens": [1144, 291, 1406, 12631, 16856, 293, 498, 291, 360, 11, 437, 16856, 294, 1783, 32, 5940], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 106, "seek": 49872, "start": 515.6, "end": 516.6, "text": " state?", "tokens": [1785, 30], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 107, "seek": 49872, "start": 516.6, "end": 522.44, "text": " So we do not support XA transactions right now, but of course if you're connected to", "tokens": [407, 321, 360, 406, 1406, 1783, 32, 16856, 558, 586, 11, 457, 295, 1164, 498, 291, 434, 4582, 281], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 108, "seek": 49872, "start": 522.44, "end": 528.6800000000001, "text": " different SQL nodes, it looks just like it is a master or a primary wherever.", "tokens": [819, 19200, 13891, 11, 309, 1542, 445, 411, 309, 307, 257, 4505, 420, 257, 6194, 8660, 13], "temperature": 0.0, "avg_logprob": -0.23240356035129997, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0005196419078856707}, {"id": 109, "seek": 52868, "start": 528.68, "end": 534.3599999999999, "text": " So full read and write in however you connect.", "tokens": [407, 1577, 1401, 293, 2464, 294, 4461, 291, 1745, 13], "temperature": 0.0, "avg_logprob": -0.27039257685343426, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0005899770185351372}, {"id": 110, "seek": 52868, "start": 534.3599999999999, "end": 548.76, "text": " So transaction is a bit slightly different.", "tokens": [407, 14425, 307, 257, 857, 4748, 819, 13], "temperature": 0.0, "avg_logprob": -0.27039257685343426, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0005899770185351372}, {"id": 111, "seek": 52868, "start": 548.76, "end": 553.52, "text": " You cannot have transactions spanning more than two versions.", "tokens": [509, 2644, 362, 16856, 47626, 544, 813, 732, 9606, 13], "temperature": 0.0, "avg_logprob": -0.27039257685343426, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0005899770185351372}, {"id": 112, "seek": 55352, "start": 553.52, "end": 562.8, "text": " So you need to either wait or you need to block, stop and fail transactions that are", "tokens": [407, 291, 643, 281, 2139, 1699, 420, 291, 643, 281, 3461, 11, 1590, 293, 3061, 16856, 300, 366], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 113, "seek": 55352, "start": 562.8, "end": 563.8, "text": " too long-running.", "tokens": [886, 938, 12, 45482, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 114, "seek": 55352, "start": 563.8, "end": 564.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 115, "seek": 55352, "start": 564.8, "end": 570.12, "text": " And these versions, you have like several versions associated with a single or nice", "tokens": [400, 613, 9606, 11, 291, 362, 411, 2940, 9606, 6615, 365, 257, 2167, 420, 1481], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 116, "seek": 55352, "start": 570.12, "end": 571.12, "text": " game of change.", "tokens": [1216, 295, 1319, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 117, "seek": 55352, "start": 571.12, "end": 572.12, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 118, "seek": 55352, "start": 572.12, "end": 573.12, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 119, "seek": 55352, "start": 573.12, "end": 581.72, "text": " So a single DDL goes through multiple stages.", "tokens": [407, 257, 2167, 30778, 43, 1709, 807, 3866, 10232, 13], "temperature": 0.0, "avg_logprob": -0.31471784491288035, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.0002357021876377985}, {"id": 120, "seek": 58172, "start": 581.72, "end": 589.6, "text": " And currently I'm actually working with partitioning and for alter table reorganize partition where", "tokens": [400, 4362, 286, 478, 767, 1364, 365, 24808, 278, 293, 337, 11337, 3199, 41203, 1125, 24808, 689], "temperature": 0.0, "avg_logprob": -0.20065975189208984, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.0002746226091403514}, {"id": 121, "seek": 58172, "start": 589.6, "end": 596.32, "text": " you take one set of partitions into a new set, then there's another thing.", "tokens": [291, 747, 472, 992, 295, 644, 2451, 666, 257, 777, 992, 11, 550, 456, 311, 1071, 551, 13], "temperature": 0.0, "avg_logprob": -0.20065975189208984, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.0002746226091403514}, {"id": 122, "seek": 58172, "start": 596.32, "end": 606.32, "text": " So during the reorganized phase when you're copying data, you do select from the old one", "tokens": [407, 1830, 264, 41203, 1602, 5574, 562, 291, 434, 27976, 1412, 11, 291, 360, 3048, 490, 264, 1331, 472], "temperature": 0.0, "avg_logprob": -0.20065975189208984, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.0002746226091403514}, {"id": 123, "seek": 60632, "start": 606.32, "end": 612.48, "text": " then you go to public, so you select from this one, which means that if someone is actually", "tokens": [550, 291, 352, 281, 1908, 11, 370, 291, 3048, 490, 341, 472, 11, 597, 1355, 300, 498, 1580, 307, 767], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 124, "seek": 60632, "start": 612.48, "end": 618.48, "text": " on the right reorganization state, then they will select from that that's not updated in", "tokens": [322, 264, 558, 41203, 2144, 1785, 11, 550, 436, 486, 3048, 490, 300, 300, 311, 406, 10588, 294], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 125, "seek": 60632, "start": 618.48, "end": 619.48, "text": " this one.", "tokens": [341, 472, 13], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 126, "seek": 60632, "start": 619.48, "end": 627.44, "text": " So you need to add an additional state between the right reorganization and the public state", "tokens": [407, 291, 643, 281, 909, 364, 4497, 1785, 1296, 264, 558, 41203, 2144, 293, 264, 1908, 1785], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 127, "seek": 60632, "start": 627.44, "end": 629.24, "text": " just for moving the select.", "tokens": [445, 337, 2684, 264, 3048, 13], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 128, "seek": 60632, "start": 629.24, "end": 635.8800000000001, "text": " So it's a double right while moving the reads.", "tokens": [407, 309, 311, 257, 3834, 558, 1339, 2684, 264, 15700, 13], "temperature": 0.0, "avg_logprob": -0.10020976120166564, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00024530736845918}, {"id": 129, "seek": 63588, "start": 635.88, "end": 642.56, "text": " And all this is done in tidy B and I'm not sure how many is familiar with tidy B.", "tokens": [400, 439, 341, 307, 1096, 294, 34646, 363, 293, 286, 478, 406, 988, 577, 867, 307, 4963, 365, 34646, 363, 13], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 130, "seek": 63588, "start": 642.56, "end": 643.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 131, "seek": 63588, "start": 643.56, "end": 644.56, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 132, "seek": 63588, "start": 644.56, "end": 653.0, "text": " Then let's do a quick introduction to this tidy B is mainly architecture around three", "tokens": [1396, 718, 311, 360, 257, 1702, 9339, 281, 341, 34646, 363, 307, 8704, 9482, 926, 1045], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 133, "seek": 63588, "start": 653.0, "end": 654.52, "text": " different components.", "tokens": [819, 6677, 13], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 134, "seek": 63588, "start": 654.52, "end": 657.32, "text": " You have PD which stands for placement driver.", "tokens": [509, 362, 10464, 597, 7382, 337, 17257, 6787, 13], "temperature": 0.0, "avg_logprob": -0.21263727501257143, "compression_ratio": 1.3854748603351956, "no_speech_prob": 0.00018364173592999578}, {"id": 135, "seek": 65732, "start": 657.32, "end": 666.32, "text": " It creates the timestamps for transaction handling and it knows about the data locations.", "tokens": [467, 7829, 264, 49108, 23150, 337, 14425, 13175, 293, 309, 3255, 466, 264, 1412, 9253, 13], "temperature": 0.0, "avg_logprob": -0.13883689554726206, "compression_ratio": 1.5606060606060606, "no_speech_prob": 3.857443880406208e-05}, {"id": 136, "seek": 65732, "start": 666.32, "end": 670.7600000000001, "text": " So it knows where the date on which node the data are.", "tokens": [407, 309, 3255, 689, 264, 4002, 322, 597, 9984, 264, 1412, 366, 13], "temperature": 0.0, "avg_logprob": -0.13883689554726206, "compression_ratio": 1.5606060606060606, "no_speech_prob": 3.857443880406208e-05}, {"id": 137, "seek": 65732, "start": 670.7600000000001, "end": 675.5600000000001, "text": " Then we have an SQL layer that is stateless.", "tokens": [1396, 321, 362, 364, 19200, 4583, 300, 307, 2219, 4272, 13], "temperature": 0.0, "avg_logprob": -0.13883689554726206, "compression_ratio": 1.5606060606060606, "no_speech_prob": 3.857443880406208e-05}, {"id": 138, "seek": 65732, "start": 675.5600000000001, "end": 681.72, "text": " So it's very easy to spin up or scale in the different number of nodes.", "tokens": [407, 309, 311, 588, 1858, 281, 6060, 493, 420, 4373, 294, 264, 819, 1230, 295, 13891, 13], "temperature": 0.0, "avg_logprob": -0.13883689554726206, "compression_ratio": 1.5606060606060606, "no_speech_prob": 3.857443880406208e-05}, {"id": 139, "seek": 65732, "start": 681.72, "end": 685.32, "text": " Here we have re-implemented the MySQL protocol.", "tokens": [1692, 321, 362, 319, 12, 332, 781, 14684, 264, 1222, 39934, 10336, 13], "temperature": 0.0, "avg_logprob": -0.13883689554726206, "compression_ratio": 1.5606060606060606, "no_speech_prob": 3.857443880406208e-05}, {"id": 140, "seek": 68532, "start": 685.32, "end": 688.72, "text": " So this is actually written in Go.", "tokens": [407, 341, 307, 767, 3720, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 141, "seek": 68532, "start": 688.72, "end": 691.0400000000001, "text": " And all of it is in Apache 2 license.", "tokens": [400, 439, 295, 309, 307, 294, 46597, 568, 10476, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 142, "seek": 68532, "start": 691.0400000000001, "end": 694.36, "text": " So we do not share any code from MySQL or Maria.", "tokens": [407, 321, 360, 406, 2073, 604, 3089, 490, 1222, 39934, 420, 12734, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 143, "seek": 68532, "start": 694.36, "end": 700.84, "text": " It's completely new since 2015 when the project started.", "tokens": [467, 311, 2584, 777, 1670, 7546, 562, 264, 1716, 1409, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 144, "seek": 68532, "start": 700.84, "end": 703.6800000000001, "text": " And then we have a storage layer.", "tokens": [400, 550, 321, 362, 257, 6725, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 145, "seek": 68532, "start": 703.6800000000001, "end": 710.2800000000001, "text": " The base storage layer is a Thai KV, so it's a distributed key value store.", "tokens": [440, 3096, 6725, 4583, 307, 257, 19254, 591, 53, 11, 370, 309, 311, 257, 12631, 2141, 2158, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 146, "seek": 68532, "start": 710.2800000000001, "end": 714.8800000000001, "text": " We even have people that run stats as a distributed key value store and don't bother about the", "tokens": [492, 754, 362, 561, 300, 1190, 18152, 382, 257, 12631, 2141, 2158, 3531, 293, 500, 380, 8677, 466, 264], "temperature": 0.0, "avg_logprob": -0.1908724858210637, "compression_ratio": 1.5761316872427984, "no_speech_prob": 5.119146590004675e-05}, {"id": 147, "seek": 71488, "start": 714.88, "end": 716.84, "text": " SQL part.", "tokens": [19200, 644, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 148, "seek": 71488, "start": 716.84, "end": 719.04, "text": " So that's what you can do as well.", "tokens": [407, 300, 311, 437, 291, 393, 360, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 149, "seek": 71488, "start": 719.04, "end": 725.4, "text": " And then we do also have an additional, an optional way of storing the data in what we", "tokens": [400, 550, 321, 360, 611, 362, 364, 4497, 11, 364, 17312, 636, 295, 26085, 264, 1412, 294, 437, 321], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 150, "seek": 71488, "start": 725.4, "end": 727.2, "text": " call Thai Flash.", "tokens": [818, 19254, 20232, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 151, "seek": 71488, "start": 727.2, "end": 728.8, "text": " That's a column store.", "tokens": [663, 311, 257, 7738, 3531, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 152, "seek": 71488, "start": 728.8, "end": 736.2, "text": " So by connecting it here you can actually do analytics like aggregations and so on on", "tokens": [407, 538, 11015, 309, 510, 291, 393, 767, 360, 15370, 411, 16743, 763, 293, 370, 322, 322], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 153, "seek": 71488, "start": 736.2, "end": 740.16, "text": " the same data within the same transaction even.", "tokens": [264, 912, 1412, 1951, 264, 912, 14425, 754, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 154, "seek": 71488, "start": 740.16, "end": 744.12, "text": " And the optimizer here would choose what is the fastest way.", "tokens": [400, 264, 5028, 6545, 510, 576, 2826, 437, 307, 264, 14573, 636, 13], "temperature": 0.0, "avg_logprob": -0.15545369157887468, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.00033499562414363027}, {"id": 155, "seek": 74412, "start": 744.12, "end": 748.8, "text": " What has the lowest cost for executing the query.", "tokens": [708, 575, 264, 12437, 2063, 337, 32368, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1849499085370232, "compression_ratio": 1.455813953488372, "no_speech_prob": 5.0705471949186176e-05}, {"id": 156, "seek": 74412, "start": 748.8, "end": 753.2, "text": " So you don't have any ETL or anything like that in between.", "tokens": [407, 291, 500, 380, 362, 604, 36953, 43, 420, 1340, 411, 300, 294, 1296, 13], "temperature": 0.0, "avg_logprob": -0.1849499085370232, "compression_ratio": 1.455813953488372, "no_speech_prob": 5.0705471949186176e-05}, {"id": 157, "seek": 74412, "start": 753.2, "end": 754.52, "text": " It's very easy to just add.", "tokens": [467, 311, 588, 1858, 281, 445, 909, 13], "temperature": 0.0, "avg_logprob": -0.1849499085370232, "compression_ratio": 1.455813953488372, "no_speech_prob": 5.0705471949186176e-05}, {"id": 158, "seek": 74412, "start": 754.52, "end": 760.48, "text": " You're doing all the tables and set the Thai Flash replica equals one or two or if you", "tokens": [509, 434, 884, 439, 264, 8020, 293, 992, 264, 19254, 20232, 35456, 6915, 472, 420, 732, 420, 498, 291], "temperature": 0.0, "avg_logprob": -0.1849499085370232, "compression_ratio": 1.455813953488372, "no_speech_prob": 5.0705471949186176e-05}, {"id": 159, "seek": 74412, "start": 760.48, "end": 769.6, "text": " add more than one, then you also get the MPP, so massive parallel processing part of it.", "tokens": [909, 544, 813, 472, 11, 550, 291, 611, 483, 264, 14146, 47, 11, 370, 5994, 8952, 9007, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1849499085370232, "compression_ratio": 1.455813953488372, "no_speech_prob": 5.0705471949186176e-05}, {"id": 160, "seek": 76960, "start": 769.6, "end": 775.8000000000001, "text": " We do have an, you can run Spark on it as well.", "tokens": [492, 360, 362, 364, 11, 291, 393, 1190, 23424, 322, 309, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19040389177275868, "compression_ratio": 1.4901960784313726, "no_speech_prob": 7.99694680608809e-05}, {"id": 161, "seek": 76960, "start": 775.8000000000001, "end": 781.9200000000001, "text": " And let's just go down slightly deeper on how we actually store the data.", "tokens": [400, 718, 311, 445, 352, 760, 4748, 7731, 322, 577, 321, 767, 3531, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19040389177275868, "compression_ratio": 1.4901960784313726, "no_speech_prob": 7.99694680608809e-05}, {"id": 162, "seek": 76960, "start": 781.9200000000001, "end": 792.0400000000001, "text": " So we take all this data and split it into ranges about 100 megabytes and each such range", "tokens": [407, 321, 747, 439, 341, 1412, 293, 7472, 309, 666, 22526, 466, 2319, 10816, 24538, 293, 1184, 1270, 3613], "temperature": 0.0, "avg_logprob": -0.19040389177275868, "compression_ratio": 1.4901960784313726, "no_speech_prob": 7.99694680608809e-05}, {"id": 163, "seek": 76960, "start": 792.0400000000001, "end": 798.84, "text": " is stored in three, or yeah it's configurable, let's say three copies in the Thai KV storage", "tokens": [307, 12187, 294, 1045, 11, 420, 1338, 309, 311, 22192, 712, 11, 718, 311, 584, 1045, 14341, 294, 264, 19254, 591, 53, 6725], "temperature": 0.0, "avg_logprob": -0.19040389177275868, "compression_ratio": 1.4901960784313726, "no_speech_prob": 7.99694680608809e-05}, {"id": 164, "seek": 79884, "start": 798.84, "end": 804.0, "text": " nodes and each such region is forming a raft group.", "tokens": [13891, 293, 1184, 1270, 4458, 307, 15745, 257, 43863, 1594, 13], "temperature": 0.0, "avg_logprob": -0.23739748115999154, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.00011202169844182208}, {"id": 165, "seek": 79884, "start": 804.0, "end": 811.36, "text": " So that's how it keeps the HA and the high availability.", "tokens": [407, 300, 311, 577, 309, 5965, 264, 11979, 293, 264, 1090, 17945, 13], "temperature": 0.0, "avg_logprob": -0.23739748115999154, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.00011202169844182208}, {"id": 166, "seek": 79884, "start": 811.36, "end": 815.72, "text": " Thai KV is using ROXDB as lower level storage.", "tokens": [19254, 591, 53, 307, 1228, 9025, 55, 27735, 382, 3126, 1496, 6725, 13], "temperature": 0.0, "avg_logprob": -0.23739748115999154, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.00011202169844182208}, {"id": 167, "seek": 79884, "start": 815.72, "end": 824.76, "text": " So it's an LSM tree, yeah it's similar as MIROX in Percona or MariaDB.", "tokens": [407, 309, 311, 364, 36657, 44, 4230, 11, 1338, 309, 311, 2531, 382, 13696, 7142, 55, 294, 3026, 1671, 64, 420, 12734, 27735, 13], "temperature": 0.0, "avg_logprob": -0.23739748115999154, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.00011202169844182208}, {"id": 168, "seek": 79884, "start": 824.76, "end": 828.72, "text": " So it's not B-tree based.", "tokens": [407, 309, 311, 406, 363, 12, 83, 701, 2361, 13], "temperature": 0.0, "avg_logprob": -0.23739748115999154, "compression_ratio": 1.3695652173913044, "no_speech_prob": 0.00011202169844182208}, {"id": 169, "seek": 82872, "start": 828.72, "end": 835.52, "text": " Through this raft protocol, that's how we also can connect the column store.", "tokens": [8927, 341, 43863, 10336, 11, 300, 311, 577, 321, 611, 393, 1745, 264, 7738, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1179746353265011, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.447102350648493e-05}, {"id": 170, "seek": 82872, "start": 835.52, "end": 841.96, "text": " So that's how we also have it, so you can run it in the same transaction and even if", "tokens": [407, 300, 311, 577, 321, 611, 362, 309, 11, 370, 291, 393, 1190, 309, 294, 264, 912, 14425, 293, 754, 498], "temperature": 0.0, "avg_logprob": -0.1179746353265011, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.447102350648493e-05}, {"id": 171, "seek": 82872, "start": 841.96, "end": 850.48, "text": " you have a join, maybe it's faster to execute parts of it through an index in the row store", "tokens": [291, 362, 257, 3917, 11, 1310, 309, 311, 4663, 281, 14483, 3166, 295, 309, 807, 364, 8186, 294, 264, 5386, 3531], "temperature": 0.0, "avg_logprob": -0.1179746353265011, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.447102350648493e-05}, {"id": 172, "seek": 85048, "start": 850.48, "end": 863.32, "text": " and then do some of the table scans and aggregation in Thai Flash in the column store.", "tokens": [293, 550, 360, 512, 295, 264, 3199, 35116, 293, 16743, 399, 294, 19254, 20232, 294, 264, 7738, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1339092118399484, "compression_ratio": 1.5393939393939393, "no_speech_prob": 3.6596564314095303e-05}, {"id": 173, "seek": 85048, "start": 863.32, "end": 866.5600000000001, "text": " And this is optional, but this is not, this is the base.", "tokens": [400, 341, 307, 17312, 11, 457, 341, 307, 406, 11, 341, 307, 264, 3096, 13], "temperature": 0.0, "avg_logprob": -0.1339092118399484, "compression_ratio": 1.5393939393939393, "no_speech_prob": 3.6596564314095303e-05}, {"id": 174, "seek": 85048, "start": 866.5600000000001, "end": 876.32, "text": " You always need to have the row store and you can have this as an option.", "tokens": [509, 1009, 643, 281, 362, 264, 5386, 3531, 293, 291, 393, 362, 341, 382, 364, 3614, 13], "temperature": 0.0, "avg_logprob": -0.1339092118399484, "compression_ratio": 1.5393939393939393, "no_speech_prob": 3.6596564314095303e-05}, {"id": 175, "seek": 85048, "start": 876.32, "end": 878.28, "text": " There's a lot of tooling that works.", "tokens": [821, 311, 257, 688, 295, 46593, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1339092118399484, "compression_ratio": 1.5393939393939393, "no_speech_prob": 3.6596564314095303e-05}, {"id": 176, "seek": 87828, "start": 878.28, "end": 885.68, "text": " So first of all, I would say that the data migration, so it's easy to have a ThaiDB cluster", "tokens": [407, 700, 295, 439, 11, 286, 576, 584, 300, 264, 1412, 17011, 11, 370, 309, 311, 1858, 281, 362, 257, 19254, 27735, 13630], "temperature": 0.0, "avg_logprob": -0.1580795094936709, "compression_ratio": 1.5, "no_speech_prob": 7.067900151014328e-05}, {"id": 177, "seek": 87828, "start": 885.68, "end": 893.88, "text": " to read the binary logs or just set it up for dumping an upstream MySQL instance or", "tokens": [281, 1401, 264, 17434, 20820, 420, 445, 992, 309, 493, 337, 42224, 364, 33915, 1222, 39934, 5197, 420], "temperature": 0.0, "avg_logprob": -0.1580795094936709, "compression_ratio": 1.5, "no_speech_prob": 7.067900151014328e-05}, {"id": 178, "seek": 87828, "start": 893.88, "end": 901.8399999999999, "text": " even several instances into the same cluster so you can combine all the data back.", "tokens": [754, 2940, 14519, 666, 264, 912, 13630, 370, 291, 393, 10432, 439, 264, 1412, 646, 13], "temperature": 0.0, "avg_logprob": -0.1580795094936709, "compression_ratio": 1.5, "no_speech_prob": 7.067900151014328e-05}, {"id": 179, "seek": 87828, "start": 901.8399999999999, "end": 906.88, "text": " We have the backup and restore, very good dump story.", "tokens": [492, 362, 264, 14807, 293, 15227, 11, 588, 665, 11430, 1657, 13], "temperature": 0.0, "avg_logprob": -0.1580795094936709, "compression_ratio": 1.5, "no_speech_prob": 7.067900151014328e-05}, {"id": 180, "seek": 90688, "start": 906.88, "end": 911.84, "text": " I think that even works with MySQL.", "tokens": [286, 519, 300, 754, 1985, 365, 1222, 39934, 13], "temperature": 0.0, "avg_logprob": -0.19395282350737472, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00011386464757379144}, {"id": 181, "seek": 90688, "start": 911.84, "end": 918.68, "text": " You have the tool for do a diff between the different instances, change data capture", "tokens": [509, 362, 264, 2290, 337, 360, 257, 7593, 1296, 264, 819, 14519, 11, 1319, 1412, 7983], "temperature": 0.0, "avg_logprob": -0.19395282350737472, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00011386464757379144}, {"id": 182, "seek": 90688, "start": 918.68, "end": 926.84, "text": " that can go to either another ThaiDB cluster or MySQL instance, go through Kafka as well", "tokens": [300, 393, 352, 281, 2139, 1071, 19254, 27735, 13630, 420, 1222, 39934, 5197, 11, 352, 807, 47064, 382, 731], "temperature": 0.0, "avg_logprob": -0.19395282350737472, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00011386464757379144}, {"id": 183, "seek": 90688, "start": 926.84, "end": 930.24, "text": " if you want.", "tokens": [498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.19395282350737472, "compression_ratio": 1.3788819875776397, "no_speech_prob": 0.00011386464757379144}, {"id": 184, "seek": 93024, "start": 930.24, "end": 938.52, "text": " Try up, that's a way for managing and deploying ThaiDB and all components you want.", "tokens": [6526, 493, 11, 300, 311, 257, 636, 337, 11642, 293, 34198, 19254, 27735, 293, 439, 6677, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.14163752035661178, "compression_ratio": 1.5401785714285714, "no_speech_prob": 3.370024569449015e-05}, {"id": 185, "seek": 93024, "start": 938.52, "end": 942.12, "text": " You can even use it as a playground to start it in your laptop.", "tokens": [509, 393, 754, 764, 309, 382, 257, 24646, 281, 722, 309, 294, 428, 10732, 13], "temperature": 0.0, "avg_logprob": -0.14163752035661178, "compression_ratio": 1.5401785714285714, "no_speech_prob": 3.370024569449015e-05}, {"id": 186, "seek": 93024, "start": 942.12, "end": 945.48, "text": " It will download the binaries and start everything, including monitoring everything.", "tokens": [467, 486, 5484, 264, 5171, 4889, 293, 722, 1203, 11, 3009, 11028, 1203, 13], "temperature": 0.0, "avg_logprob": -0.14163752035661178, "compression_ratio": 1.5401785714285714, "no_speech_prob": 3.370024569449015e-05}, {"id": 187, "seek": 93024, "start": 945.48, "end": 950.24, "text": " So it's very easy to just try out.", "tokens": [407, 309, 311, 588, 1858, 281, 445, 853, 484, 13], "temperature": 0.0, "avg_logprob": -0.14163752035661178, "compression_ratio": 1.5401785714285714, "no_speech_prob": 3.370024569449015e-05}, {"id": 188, "seek": 93024, "start": 950.24, "end": 956.08, "text": " We have an operator if you want to run it in Kubernetes as well in the cloud.", "tokens": [492, 362, 364, 12973, 498, 291, 528, 281, 1190, 309, 294, 23145, 382, 731, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.14163752035661178, "compression_ratio": 1.5401785714285714, "no_speech_prob": 3.370024569449015e-05}, {"id": 189, "seek": 95608, "start": 956.08, "end": 962.6800000000001, "text": " So we even have it as a cloud service, you can do anything from on-prem up to a cloud", "tokens": [407, 321, 754, 362, 309, 382, 257, 4588, 2643, 11, 291, 393, 360, 1340, 490, 322, 12, 29403, 493, 281, 257, 4588], "temperature": 0.0, "avg_logprob": -0.19862862995692662, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.00018772657494992018}, {"id": 190, "seek": 95608, "start": 962.6800000000001, "end": 965.8000000000001, "text": " service in many different ways.", "tokens": [2643, 294, 867, 819, 2098, 13], "temperature": 0.0, "avg_logprob": -0.19862862995692662, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.00018772657494992018}, {"id": 191, "seek": 95608, "start": 965.8000000000001, "end": 972.0, "text": " And we also have Lightning, which is an optimized import tool, and that's what I will actually", "tokens": [400, 321, 611, 362, 28848, 11, 597, 307, 364, 26941, 974, 2290, 11, 293, 300, 311, 437, 286, 486, 767], "temperature": 0.0, "avg_logprob": -0.19862862995692662, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.00018772657494992018}, {"id": 192, "seek": 95608, "start": 972.0, "end": 977.5600000000001, "text": " use in the next slide soon.", "tokens": [764, 294, 264, 958, 4137, 2321, 13], "temperature": 0.0, "avg_logprob": -0.19862862995692662, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.00018772657494992018}, {"id": 193, "seek": 95608, "start": 977.5600000000001, "end": 984.4000000000001, "text": " A year ago, we started a project because we heard and compared the ad index performance", "tokens": [316, 1064, 2057, 11, 321, 1409, 257, 1716, 570, 321, 2198, 293, 5347, 264, 614, 8186, 3389], "temperature": 0.0, "avg_logprob": -0.19862862995692662, "compression_ratio": 1.5255813953488373, "no_speech_prob": 0.00018772657494992018}, {"id": 194, "seek": 98440, "start": 984.4, "end": 990.8, "text": " in ThaiDB cluster versus, for example, Cassandra or Aurora.", "tokens": [294, 19254, 27735, 13630, 5717, 11, 337, 1365, 11, 18208, 18401, 420, 40663, 13], "temperature": 0.0, "avg_logprob": -0.17171161651611327, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.00012808921746909618}, {"id": 195, "seek": 98440, "start": 990.8, "end": 997.1999999999999, "text": " And at that time, we were basically three times slower because we haven't optimized", "tokens": [400, 412, 300, 565, 11, 321, 645, 1936, 1045, 1413, 14009, 570, 321, 2378, 380, 26941], "temperature": 0.0, "avg_logprob": -0.17171161651611327, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.00012808921746909618}, {"id": 196, "seek": 98440, "start": 997.1999999999999, "end": 1003.9599999999999, "text": " that it was just stable proven and it worked, but it was not fast.", "tokens": [300, 309, 390, 445, 8351, 12785, 293, 309, 2732, 11, 457, 309, 390, 406, 2370, 13], "temperature": 0.0, "avg_logprob": -0.17171161651611327, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.00012808921746909618}, {"id": 197, "seek": 98440, "start": 1003.9599999999999, "end": 1010.0, "text": " And that's especially when you're doing proof of concept or loading the data, that's where", "tokens": [400, 300, 311, 2318, 562, 291, 434, 884, 8177, 295, 3410, 420, 15114, 264, 1412, 11, 300, 311, 689], "temperature": 0.0, "avg_logprob": -0.17171161651611327, "compression_ratio": 1.5125628140703518, "no_speech_prob": 0.00012808921746909618}, {"id": 198, "seek": 101000, "start": 1010.0, "end": 1016.56, "text": " it's really beneficial to speed it up.", "tokens": [309, 311, 534, 14072, 281, 3073, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.10451568603515625, "compression_ratio": 1.5024630541871922, "no_speech_prob": 5.9076264733448625e-05}, {"id": 199, "seek": 101000, "start": 1016.56, "end": 1022.2, "text": " And the way it worked, it would just do data copying through small transaction batches", "tokens": [400, 264, 636, 309, 2732, 11, 309, 576, 445, 360, 1412, 27976, 807, 1359, 14425, 15245, 279], "temperature": 0.0, "avg_logprob": -0.10451568603515625, "compression_ratio": 1.5024630541871922, "no_speech_prob": 5.9076264733448625e-05}, {"id": 200, "seek": 101000, "start": 1022.2, "end": 1023.2, "text": " more or less.", "tokens": [544, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.10451568603515625, "compression_ratio": 1.5024630541871922, "no_speech_prob": 5.9076264733448625e-05}, {"id": 201, "seek": 101000, "start": 1023.2, "end": 1028.8, "text": " So that also creates a lot of overhead with transaction handling, et cetera.", "tokens": [407, 300, 611, 7829, 257, 688, 295, 19922, 365, 14425, 13175, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.10451568603515625, "compression_ratio": 1.5024630541871922, "no_speech_prob": 5.9076264733448625e-05}, {"id": 202, "seek": 101000, "start": 1028.8, "end": 1037.64, "text": " That's not actually needed when you're doing a backfill because during backfill process,", "tokens": [663, 311, 406, 767, 2978, 562, 291, 434, 884, 257, 646, 31072, 570, 1830, 646, 31072, 1399, 11], "temperature": 0.0, "avg_logprob": -0.10451568603515625, "compression_ratio": 1.5024630541871922, "no_speech_prob": 5.9076264733448625e-05}, {"id": 203, "seek": 103764, "start": 1037.64, "end": 1042.92, "text": " during the data, it doesn't actually need to be transactional.", "tokens": [1830, 264, 1412, 11, 309, 1177, 380, 767, 643, 281, 312, 46688, 1966, 13], "temperature": 0.0, "avg_logprob": -0.15089938345919834, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0001493371237302199}, {"id": 204, "seek": 103764, "start": 1042.92, "end": 1048.88, "text": " And it's only a single node that does this, a single TIDB node that orchestrates it.", "tokens": [400, 309, 311, 787, 257, 2167, 9984, 300, 775, 341, 11, 257, 2167, 28819, 27735, 9984, 300, 14161, 12507, 309, 13], "temperature": 0.0, "avg_logprob": -0.15089938345919834, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0001493371237302199}, {"id": 205, "seek": 103764, "start": 1048.88, "end": 1055.3600000000001, "text": " I'm not going to go deep into this, it basically just shows how you're creating a command in", "tokens": [286, 478, 406, 516, 281, 352, 2452, 666, 341, 11, 309, 1936, 445, 3110, 577, 291, 434, 4084, 257, 5622, 294], "temperature": 0.0, "avg_logprob": -0.15089938345919834, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0001493371237302199}, {"id": 206, "seek": 103764, "start": 1055.3600000000001, "end": 1060.68, "text": " one ThaiDB node and it goes into a table, a ThaiDB owner will do it, go through the different", "tokens": [472, 19254, 27735, 9984, 293, 309, 1709, 666, 257, 3199, 11, 257, 19254, 27735, 7289, 486, 360, 309, 11, 352, 807, 264, 819], "temperature": 0.0, "avg_logprob": -0.15089938345919834, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0001493371237302199}, {"id": 207, "seek": 106068, "start": 1060.68, "end": 1068.4, "text": " steps and do the data migrations and data copying.", "tokens": [4439, 293, 360, 264, 1412, 6186, 12154, 293, 1412, 27976, 13], "temperature": 0.0, "avg_logprob": -0.15268645967755998, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.00016150162264239043}, {"id": 208, "seek": 106068, "start": 1068.4, "end": 1076.5600000000002, "text": " So what we did first was create a feature with this feature flag.", "tokens": [407, 437, 321, 630, 700, 390, 1884, 257, 4111, 365, 341, 4111, 7166, 13], "temperature": 0.0, "avg_logprob": -0.15268645967755998, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.00016150162264239043}, {"id": 209, "seek": 106068, "start": 1076.5600000000002, "end": 1080.1200000000001, "text": " It uses this lightning the import tool technology.", "tokens": [467, 4960, 341, 16589, 264, 974, 2290, 2899, 13], "temperature": 0.0, "avg_logprob": -0.15268645967755998, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.00016150162264239043}, {"id": 210, "seek": 106068, "start": 1080.1200000000001, "end": 1084.28, "text": " It's completely built in in ThaiDB cluster, so it's not the external tool.", "tokens": [467, 311, 2584, 3094, 294, 294, 19254, 27735, 13630, 11, 370, 309, 311, 406, 264, 8320, 2290, 13], "temperature": 0.0, "avg_logprob": -0.15268645967755998, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.00016150162264239043}, {"id": 211, "seek": 106068, "start": 1084.28, "end": 1090.64, "text": " But it reads the data and then it creates these SSD files for ingestion in RocksDB.", "tokens": [583, 309, 15700, 264, 1412, 293, 550, 309, 7829, 613, 30262, 7098, 337, 3957, 31342, 294, 6922, 82, 27735, 13], "temperature": 0.0, "avg_logprob": -0.15268645967755998, "compression_ratio": 1.5523809523809524, "no_speech_prob": 0.00016150162264239043}, {"id": 212, "seek": 109064, "start": 1090.64, "end": 1097.3200000000002, "text": " So it's very efficient load and it has very low impact on the storage side.", "tokens": [407, 309, 311, 588, 7148, 3677, 293, 309, 575, 588, 2295, 2712, 322, 264, 6725, 1252, 13], "temperature": 0.0, "avg_logprob": -0.13269654909769693, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.00022645022545475513}, {"id": 213, "seek": 109064, "start": 1097.3200000000002, "end": 1102.72, "text": " It just moves these files into the storage and enables them and takes them into the", "tokens": [467, 445, 6067, 613, 7098, 666, 264, 6725, 293, 17077, 552, 293, 2516, 552, 666, 264], "temperature": 0.0, "avg_logprob": -0.13269654909769693, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.00022645022545475513}, {"id": 214, "seek": 109064, "start": 1102.72, "end": 1105.76, "text": " RocksDB levels.", "tokens": [6922, 82, 27735, 4358, 13], "temperature": 0.0, "avg_logprob": -0.13269654909769693, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.00022645022545475513}, {"id": 215, "seek": 109064, "start": 1105.76, "end": 1112.16, "text": " The result was around three times speed up and of course a lot less impact on normal", "tokens": [440, 1874, 390, 926, 1045, 1413, 3073, 493, 293, 295, 1164, 257, 688, 1570, 2712, 322, 2710], "temperature": 0.0, "avg_logprob": -0.13269654909769693, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.00022645022545475513}, {"id": 216, "seek": 109064, "start": 1112.16, "end": 1113.3600000000001, "text": " load in the cluster.", "tokens": [3677, 294, 264, 13630, 13], "temperature": 0.0, "avg_logprob": -0.13269654909769693, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.00022645022545475513}, {"id": 217, "seek": 111336, "start": 1113.36, "end": 1123.7199999999998, "text": " So even if you have a highly loaded cluster, you can do this almost without impacting it.", "tokens": [407, 754, 498, 291, 362, 257, 5405, 13210, 13630, 11, 291, 393, 360, 341, 1920, 1553, 29963, 309, 13], "temperature": 0.0, "avg_logprob": -0.10225226139200144, "compression_ratio": 1.4969325153374233, "no_speech_prob": 2.528074946894776e-05}, {"id": 218, "seek": 111336, "start": 1123.7199999999998, "end": 1132.28, "text": " And then we did a bit of analysis of where we could improve even more and there was things", "tokens": [400, 550, 321, 630, 257, 857, 295, 5215, 295, 689, 321, 727, 3470, 754, 544, 293, 456, 390, 721], "temperature": 0.0, "avg_logprob": -0.10225226139200144, "compression_ratio": 1.4969325153374233, "no_speech_prob": 2.528074946894776e-05}, {"id": 219, "seek": 111336, "start": 1132.28, "end": 1138.04, "text": " like the scheduling could be improved just to shorten the time.", "tokens": [411, 264, 29055, 727, 312, 9689, 445, 281, 39632, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.10225226139200144, "compression_ratio": 1.4969325153374233, "no_speech_prob": 2.528074946894776e-05}, {"id": 220, "seek": 113804, "start": 1138.04, "end": 1145.52, "text": " Instead of reading directly from the key value store, we could use these co-operators, co-processors", "tokens": [7156, 295, 3760, 3838, 490, 264, 2141, 2158, 3531, 11, 321, 727, 764, 613, 598, 12, 7192, 3391, 11, 598, 12, 41075, 830], "temperature": 0.0, "avg_logprob": -0.16070628810573268, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0001663351577008143}, {"id": 221, "seek": 113804, "start": 1145.52, "end": 1157.8, "text": " for removing columns that's not needed, for example, for doing optimized scans, etc.", "tokens": [337, 12720, 13766, 300, 311, 406, 2978, 11, 337, 1365, 11, 337, 884, 26941, 35116, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.16070628810573268, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0001663351577008143}, {"id": 222, "seek": 113804, "start": 1157.8, "end": 1165.04, "text": " We disconnected the read and write dependencies so they could run in parallel in asynchronous", "tokens": [492, 29426, 264, 1401, 293, 2464, 36606, 370, 436, 727, 1190, 294, 8952, 294, 49174], "temperature": 0.0, "avg_logprob": -0.16070628810573268, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0001663351577008143}, {"id": 223, "seek": 113804, "start": 1165.04, "end": 1167.36, "text": " and a lot of other small optimization.", "tokens": [293, 257, 688, 295, 661, 1359, 19618, 13], "temperature": 0.0, "avg_logprob": -0.16070628810573268, "compression_ratio": 1.551219512195122, "no_speech_prob": 0.0001663351577008143}, {"id": 224, "seek": 116736, "start": 1167.36, "end": 1173.1999999999998, "text": " And that created yet another three to five times speed up.", "tokens": [400, 300, 2942, 1939, 1071, 1045, 281, 1732, 1413, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.1766940434773763, "compression_ratio": 1.4658385093167703, "no_speech_prob": 7.823085616109893e-05}, {"id": 225, "seek": 116736, "start": 1173.1999999999998, "end": 1180.7199999999998, "text": " So all in all, during the last year, we had done 10 times improvement in speed while we're", "tokens": [407, 439, 294, 439, 11, 1830, 264, 1036, 1064, 11, 321, 632, 1096, 1266, 1413, 10444, 294, 3073, 1339, 321, 434], "temperature": 0.0, "avg_logprob": -0.1766940434773763, "compression_ratio": 1.4658385093167703, "no_speech_prob": 7.823085616109893e-05}, {"id": 226, "seek": 116736, "start": 1180.7199999999998, "end": 1188.8, "text": " still only using a single TIDB node and now we're three times faster than the baseline", "tokens": [920, 787, 1228, 257, 2167, 314, 2777, 33, 9984, 293, 586, 321, 434, 1045, 1413, 4663, 813, 264, 20518], "temperature": 0.0, "avg_logprob": -0.1766940434773763, "compression_ratio": 1.4658385093167703, "no_speech_prob": 7.823085616109893e-05}, {"id": 227, "seek": 118880, "start": 1188.8, "end": 1198.32, "text": " of the other implementations in Cockroach and Aurora that we have compared with.", "tokens": [295, 264, 661, 4445, 763, 294, 39410, 340, 608, 293, 40663, 300, 321, 362, 5347, 365, 13], "temperature": 0.0, "avg_logprob": -0.1326107102997449, "compression_ratio": 1.5344827586206897, "no_speech_prob": 6.349480827338994e-05}, {"id": 228, "seek": 118880, "start": 1198.32, "end": 1203.32, "text": " And there's a bit more to do, so we're currently looking into how we even can distribute this", "tokens": [400, 456, 311, 257, 857, 544, 281, 360, 11, 370, 321, 434, 4362, 1237, 666, 577, 321, 754, 393, 20594, 341], "temperature": 0.0, "avg_logprob": -0.1326107102997449, "compression_ratio": 1.5344827586206897, "no_speech_prob": 6.349480827338994e-05}, {"id": 229, "seek": 118880, "start": 1203.32, "end": 1211.36, "text": " instead of running it on a single TIDB node and also being able to auto-tune the priority.", "tokens": [2602, 295, 2614, 309, 322, 257, 2167, 314, 2777, 33, 9984, 293, 611, 885, 1075, 281, 8399, 12, 83, 2613, 264, 9365, 13], "temperature": 0.0, "avg_logprob": -0.1326107102997449, "compression_ratio": 1.5344827586206897, "no_speech_prob": 6.349480827338994e-05}, {"id": 230, "seek": 118880, "start": 1211.36, "end": 1216.6, "text": " So if you have load that goes a bit up and a bit down, so the DDL work can adjust to", "tokens": [407, 498, 291, 362, 3677, 300, 1709, 257, 857, 493, 293, 257, 857, 760, 11, 370, 264, 30778, 43, 589, 393, 4369, 281], "temperature": 0.0, "avg_logprob": -0.1326107102997449, "compression_ratio": 1.5344827586206897, "no_speech_prob": 6.349480827338994e-05}, {"id": 231, "seek": 118880, "start": 1216.6, "end": 1217.6, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.1326107102997449, "compression_ratio": 1.5344827586206897, "no_speech_prob": 6.349480827338994e-05}, {"id": 232, "seek": 121760, "start": 1217.6, "end": 1221.32, "text": " And that is, if you depend on a single TIDB node, if that breaks for any reason, then", "tokens": [400, 300, 307, 11, 498, 291, 5672, 322, 257, 2167, 314, 2777, 33, 9984, 11, 498, 300, 9857, 337, 604, 1778, 11, 550], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 233, "seek": 121760, "start": 1221.32, "end": 1223.7199999999998, "text": " your basis is going back to the previous stage, is it?", "tokens": [428, 5143, 307, 516, 646, 281, 264, 3894, 3233, 11, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 234, "seek": 121760, "start": 1223.7199999999998, "end": 1230.48, "text": " Yeah, so we have a state state, so we go back a little bit, a little bit, but you don't", "tokens": [865, 11, 370, 321, 362, 257, 1785, 1785, 11, 370, 321, 352, 646, 257, 707, 857, 11, 257, 707, 857, 11, 457, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 235, "seek": 121760, "start": 1230.48, "end": 1237.52, "text": " need to redo the whole feeling of the index or anything like that.", "tokens": [643, 281, 29956, 264, 1379, 2633, 295, 264, 8186, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 236, "seek": 121760, "start": 1237.52, "end": 1241.48, "text": " And yeah, it's all on GitHub.", "tokens": [400, 1338, 11, 309, 311, 439, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 237, "seek": 121760, "start": 1241.48, "end": 1246.9599999999998, "text": " If you're interested a bit in how it actually works, I would recommend go to OSS Insights.", "tokens": [759, 291, 434, 3102, 257, 857, 294, 577, 309, 767, 1985, 11, 286, 576, 2748, 352, 281, 12731, 50, 9442, 5761, 13], "temperature": 0.0, "avg_logprob": -0.2631566842397054, "compression_ratio": 1.6, "no_speech_prob": 0.00031498371390625834}, {"id": 238, "seek": 124696, "start": 1246.96, "end": 1249.24, "text": " I would say it's a demo site.", "tokens": [286, 576, 584, 309, 311, 257, 10723, 3621, 13], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 239, "seek": 124696, "start": 1249.24, "end": 1257.76, "text": " It runs TIDB in the background, and it's a simple web UI, quite nice UI on top.", "tokens": [467, 6676, 314, 2777, 33, 294, 264, 3678, 11, 293, 309, 311, 257, 2199, 3670, 15682, 11, 1596, 1481, 15682, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 240, "seek": 124696, "start": 1257.76, "end": 1263.28, "text": " But it has all of the events from GitHub, so currently it's 5.5 billion records, and", "tokens": [583, 309, 575, 439, 295, 264, 3931, 490, 23331, 11, 370, 4362, 309, 311, 1025, 13, 20, 5218, 7724, 11, 293], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 241, "seek": 124696, "start": 1263.28, "end": 1265.92, "text": " we store it in a single table.", "tokens": [321, 3531, 309, 294, 257, 2167, 3199, 13], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 242, "seek": 124696, "start": 1265.92, "end": 1268.32, "text": " It's a bit other things there as well.", "tokens": [467, 311, 257, 857, 661, 721, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 243, "seek": 124696, "start": 1268.32, "end": 1274.72, "text": " And you can compare for your own GitHub ID or your own project, your own repository,", "tokens": [400, 291, 393, 6794, 337, 428, 1065, 23331, 7348, 420, 428, 1065, 1716, 11, 428, 1065, 25841, 11], "temperature": 0.0, "avg_logprob": -0.14746557624594678, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.00042492771171964705}, {"id": 244, "seek": 127472, "start": 1274.72, "end": 1279.24, "text": " compare it, and so on, and check some different frameworks, et cetera.", "tokens": [6794, 309, 11, 293, 370, 322, 11, 293, 1520, 512, 819, 29834, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 245, "seek": 127472, "start": 1279.24, "end": 1282.08, "text": " It's quite cool, actually.", "tokens": [467, 311, 1596, 1627, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 246, "seek": 127472, "start": 1282.08, "end": 1288.04, "text": " Tie-up is very useful if you want to try it on your own laptop or in your own data center.", "tokens": [36804, 12, 1010, 307, 588, 4420, 498, 291, 528, 281, 853, 309, 322, 428, 1065, 10732, 420, 294, 428, 1065, 1412, 3056, 13], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 247, "seek": 127472, "start": 1288.04, "end": 1293.44, "text": " Of course, you can go to TIDB cloud as well, but I didn't mention that here because that's", "tokens": [2720, 1164, 11, 291, 393, 352, 281, 314, 2777, 33, 4588, 382, 731, 11, 457, 286, 994, 380, 2152, 300, 510, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 248, "seek": 127472, "start": 1293.44, "end": 1298.1200000000001, "text": " our commercial offer.", "tokens": [527, 6841, 2626, 13], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 249, "seek": 127472, "start": 1298.1200000000001, "end": 1302.76, "text": " Something else that we have that is not directly connected, it's chaos mesh.", "tokens": [6595, 1646, 300, 321, 362, 300, 307, 406, 3838, 4582, 11, 309, 311, 14158, 17407, 13], "temperature": 0.0, "avg_logprob": -0.16610974531907302, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.000140894902870059}, {"id": 250, "seek": 130276, "start": 1302.76, "end": 1308.8799999999999, "text": " So if you have a system on Kubernetes and you want to see how it handles different errors,", "tokens": [407, 498, 291, 362, 257, 1185, 322, 23145, 293, 291, 528, 281, 536, 577, 309, 18722, 819, 13603, 11], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 251, "seek": 130276, "start": 1308.8799999999999, "end": 1311.6, "text": " you can use that for injecting errors.", "tokens": [291, 393, 764, 300, 337, 10711, 278, 13603, 13], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 252, "seek": 130276, "start": 1311.6, "end": 1319.24, "text": " That's something that we used for stabilizing and testing out the TIDB cluster.", "tokens": [663, 311, 746, 300, 321, 1143, 337, 11652, 3319, 293, 4997, 484, 264, 314, 2777, 33, 13630, 13], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 253, "seek": 130276, "start": 1319.24, "end": 1321.44, "text": " Then I think I'm out of time.", "tokens": [1396, 286, 519, 286, 478, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 254, "seek": 130276, "start": 1321.44, "end": 1325.6, "text": " Perfect timing, so you have time to answer questions?", "tokens": [10246, 10822, 11, 370, 291, 362, 565, 281, 1867, 1651, 30], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 255, "seek": 130276, "start": 1325.6, "end": 1326.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 256, "seek": 130276, "start": 1326.6, "end": 1327.6, "text": " Yeah?", "tokens": [865, 30], "temperature": 0.0, "avg_logprob": -0.19368794906971065, "compression_ratio": 1.4950980392156863, "no_speech_prob": 0.00019678051467053592}, {"id": 257, "seek": 132760, "start": 1327.6, "end": 1337.6, "text": " First of all, I'm very interested in how do you organize the htap transitioning.", "tokens": [2386, 295, 439, 11, 286, 478, 588, 3102, 294, 577, 360, 291, 13859, 264, 276, 83, 569, 33777, 13], "temperature": 0.0, "avg_logprob": -0.44388659795125324, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.001595002831891179}, {"id": 258, "seek": 132760, "start": 1337.6, "end": 1348.6, "text": " I mean, you have both storages, and I miss the way you move the data from row into column", "tokens": [286, 914, 11, 291, 362, 1293, 5967, 1660, 11, 293, 286, 1713, 264, 636, 291, 1286, 264, 1412, 490, 5386, 666, 7738], "temperature": 0.0, "avg_logprob": -0.44388659795125324, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.001595002831891179}, {"id": 259, "seek": 132760, "start": 1348.6, "end": 1349.6, "text": " or format.", "tokens": [420, 7877, 13], "temperature": 0.0, "avg_logprob": -0.44388659795125324, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.001595002831891179}, {"id": 260, "seek": 132760, "start": 1349.6, "end": 1350.6, "text": " I believe you do double copy.", "tokens": [286, 1697, 291, 360, 3834, 5055, 13], "temperature": 0.0, "avg_logprob": -0.44388659795125324, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.001595002831891179}, {"id": 261, "seek": 132760, "start": 1350.6, "end": 1352.7199999999998, "text": " You have double copies of the data itself.", "tokens": [509, 362, 3834, 14341, 295, 264, 1412, 2564, 13], "temperature": 0.0, "avg_logprob": -0.44388659795125324, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.001595002831891179}, {"id": 262, "seek": 135272, "start": 1352.72, "end": 1363.84, "text": " So we always have the copy here, and the raft leader of the group is always here.", "tokens": [407, 321, 1009, 362, 264, 5055, 510, 11, 293, 264, 43863, 5263, 295, 264, 1594, 307, 1009, 510, 13], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 263, "seek": 135272, "start": 1363.84, "end": 1369.8, "text": " So you do have raft leader and raft follower in the Thai KV, and then we extended the raft", "tokens": [407, 291, 360, 362, 43863, 5263, 293, 43863, 35413, 294, 264, 19254, 591, 53, 11, 293, 550, 321, 10913, 264, 43863], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 264, "seek": 135272, "start": 1369.8, "end": 1370.8, "text": " protocol.", "tokens": [10336, 13], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 265, "seek": 135272, "start": 1370.8, "end": 1375.48, "text": " So we have learner states here, so they can never become leaders.", "tokens": [407, 321, 362, 33347, 4368, 510, 11, 370, 436, 393, 1128, 1813, 3523, 13], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 266, "seek": 135272, "start": 1375.48, "end": 1376.76, "text": " So that's how we do.", "tokens": [407, 300, 311, 577, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 267, "seek": 135272, "start": 1376.76, "end": 1379.2, "text": " So this is a must, and this is optional.", "tokens": [407, 341, 307, 257, 1633, 11, 293, 341, 307, 17312, 13], "temperature": 0.0, "avg_logprob": -0.2715897126631303, "compression_ratio": 1.7318435754189945, "no_speech_prob": 0.00026298974989913404}, {"id": 268, "seek": 137920, "start": 1379.2, "end": 1386.2, "text": " What about the optimizer model?", "tokens": [708, 466, 264, 5028, 6545, 2316, 30], "temperature": 0.0, "avg_logprob": -0.32421031365027797, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.0008101687417365611}, {"id": 269, "seek": 137920, "start": 1386.2, "end": 1402.64, "text": " How do you calculate the cost-based approach to understand which storage format you use?", "tokens": [1012, 360, 291, 8873, 264, 2063, 12, 6032, 3109, 281, 1223, 597, 6725, 7877, 291, 764, 30], "temperature": 0.0, "avg_logprob": -0.32421031365027797, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.0008101687417365611}, {"id": 270, "seek": 137920, "start": 1402.64, "end": 1407.32, "text": " And it's also the influence of the volcano optimizer model, so that's how you more or", "tokens": [400, 309, 311, 611, 264, 6503, 295, 264, 21979, 5028, 6545, 2316, 11, 370, 300, 311, 577, 291, 544, 420], "temperature": 0.0, "avg_logprob": -0.32421031365027797, "compression_ratio": 1.4206896551724137, "no_speech_prob": 0.0008101687417365611}, {"id": 271, "seek": 140732, "start": 1407.32, "end": 1414.0, "text": " less pipeline the different things and can move parts of the pipeline into an MPP framework", "tokens": [1570, 15517, 264, 819, 721, 293, 393, 1286, 3166, 295, 264, 15517, 666, 364, 14146, 47, 8388], "temperature": 0.0, "avg_logprob": -0.3389983288077421, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0001720322179608047}, {"id": 272, "seek": 140732, "start": 1414.0, "end": 1417.0, "text": " that handles the column store.", "tokens": [300, 18722, 264, 7738, 3531, 13], "temperature": 0.0, "avg_logprob": -0.3389983288077421, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0001720322179608047}, {"id": 273, "seek": 140732, "start": 1417.0, "end": 1425.28, "text": " And I wonder if this model and the optimizer are dispersed across the multiple partitions", "tokens": [400, 286, 2441, 498, 341, 2316, 293, 264, 5028, 6545, 366, 48059, 2108, 264, 3866, 644, 2451], "temperature": 0.0, "avg_logprob": -0.3389983288077421, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0001720322179608047}, {"id": 274, "seek": 140732, "start": 1425.28, "end": 1430.8799999999999, "text": " of the TIDB operator, or it's in single?", "tokens": [295, 264, 314, 2777, 33, 12973, 11, 420, 309, 311, 294, 2167, 30], "temperature": 0.0, "avg_logprob": -0.3389983288077421, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0001720322179608047}, {"id": 275, "seek": 140732, "start": 1430.8799999999999, "end": 1435.8799999999999, "text": " So the optimizer, that's in the TIDB project, in the TIDB repository.", "tokens": [407, 264, 5028, 6545, 11, 300, 311, 294, 264, 314, 2777, 33, 1716, 11, 294, 264, 314, 2777, 33, 25841, 13], "temperature": 0.0, "avg_logprob": -0.3389983288077421, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0001720322179608047}, {"id": 276, "seek": 143588, "start": 1435.88, "end": 1442.96, "text": " So the SQL node, and when it executes, it's pushed down this co-processor request and", "tokens": [407, 264, 19200, 9984, 11, 293, 562, 309, 4454, 1819, 11, 309, 311, 9152, 760, 341, 598, 12, 4318, 25432, 5308, 293], "temperature": 0.0, "avg_logprob": -0.1407126819386202, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.0003297298972029239}, {"id": 277, "seek": 143588, "start": 1442.96, "end": 1450.88, "text": " also through the MPP framework for pushing down query fragments or the co-processor request", "tokens": [611, 807, 264, 14146, 47, 8388, 337, 7380, 760, 14581, 29197, 420, 264, 598, 12, 4318, 25432, 5308], "temperature": 0.0, "avg_logprob": -0.1407126819386202, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.0003297298972029239}, {"id": 278, "seek": 143588, "start": 1450.88, "end": 1456.0400000000002, "text": " into either TIDB or a Thai KV or two Thai flash.", "tokens": [666, 2139, 314, 2777, 33, 420, 257, 19254, 591, 53, 420, 732, 19254, 7319, 13], "temperature": 0.0, "avg_logprob": -0.1407126819386202, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.0003297298972029239}, {"id": 279, "seek": 143588, "start": 1456.0400000000002, "end": 1462.2, "text": " So for example, if you're doing a join where one part of the table can be resolved fast", "tokens": [407, 337, 1365, 11, 498, 291, 434, 884, 257, 3917, 689, 472, 644, 295, 264, 3199, 393, 312, 20772, 2370], "temperature": 0.0, "avg_logprob": -0.1407126819386202, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.0003297298972029239}, {"id": 280, "seek": 146220, "start": 1462.2, "end": 1467.0, "text": " by an index lookup, then it will go here for that part of the table.", "tokens": [538, 364, 8186, 574, 1010, 11, 550, 309, 486, 352, 510, 337, 300, 644, 295, 264, 3199, 13], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 281, "seek": 146220, "start": 1467.0, "end": 1473.32, "text": " And for another table, it might be a big table scan or aggregation that will be faster here.", "tokens": [400, 337, 1071, 3199, 11, 309, 1062, 312, 257, 955, 3199, 11049, 420, 16743, 399, 300, 486, 312, 4663, 510, 13], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 282, "seek": 146220, "start": 1473.32, "end": 1475.8, "text": " So then it actually can combine that.", "tokens": [407, 550, 309, 767, 393, 10432, 300, 13], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 283, "seek": 146220, "start": 1475.8, "end": 1482.88, "text": " But do you, your cost-based model is based on some assumptions about the cost of these", "tokens": [583, 360, 291, 11, 428, 2063, 12, 6032, 2316, 307, 2361, 322, 512, 17695, 466, 264, 2063, 295, 613], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 284, "seek": 146220, "start": 1482.88, "end": 1484.72, "text": " corporations, right?", "tokens": [17676, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 285, "seek": 146220, "start": 1484.72, "end": 1485.88, "text": " I'm not sure.", "tokens": [286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 286, "seek": 146220, "start": 1485.88, "end": 1490.6000000000001, "text": " I don't know the details deep enough for answer that.", "tokens": [286, 500, 380, 458, 264, 4365, 2452, 1547, 337, 1867, 300, 13], "temperature": 0.0, "avg_logprob": -0.24570548418656135, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0002905168803408742}, {"id": 287, "seek": 149060, "start": 1490.6, "end": 1500.12, "text": " And last question, how do you test the compatibility of my SQL client protocol between your implementation", "tokens": [400, 1036, 1168, 11, 577, 360, 291, 1500, 264, 34237, 295, 452, 19200, 6423, 10336, 1296, 428, 11420], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 288, "seek": 149060, "start": 1500.12, "end": 1503.1599999999999, "text": " and because it's a big question.", "tokens": [293, 570, 309, 311, 257, 955, 1168, 13], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 289, "seek": 149060, "start": 1503.1599999999999, "end": 1504.1599999999999, "text": " Yeah, yeah.", "tokens": [865, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 290, "seek": 149060, "start": 1504.1599999999999, "end": 1509.32, "text": " So we don't have any own connectors or anything like that.", "tokens": [407, 321, 500, 380, 362, 604, 1065, 31865, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 291, "seek": 149060, "start": 1509.32, "end": 1514.28, "text": " We just relying on MySQL connectors or MariaDB connectors.", "tokens": [492, 445, 24140, 322, 1222, 39934, 31865, 420, 12734, 27735, 31865, 13], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 292, "seek": 149060, "start": 1514.28, "end": 1517.8799999999999, "text": " And that's what we're using when we're testing.", "tokens": [400, 300, 311, 437, 321, 434, 1228, 562, 321, 434, 4997, 13], "temperature": 0.0, "avg_logprob": -0.27495287671501256, "compression_ratio": 1.4952830188679245, "no_speech_prob": 0.000648253015242517}, {"id": 293, "seek": 151788, "start": 1517.88, "end": 1524.16, "text": " So you basically have the test use that tries different kind of queries and after they pass", "tokens": [407, 291, 1936, 362, 264, 1500, 764, 300, 9898, 819, 733, 295, 24109, 293, 934, 436, 1320], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 294, "seek": 151788, "start": 1524.16, "end": 1527.7600000000002, "text": " it, you understand that they are somehow equal.", "tokens": [309, 11, 291, 1223, 300, 436, 366, 6063, 2681, 13], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 295, "seek": 151788, "start": 1527.7600000000002, "end": 1528.7600000000002, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 296, "seek": 151788, "start": 1528.7600000000002, "end": 1531.68, "text": " And of course, there are differences.", "tokens": [400, 295, 1164, 11, 456, 366, 7300, 13], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 297, "seek": 151788, "start": 1531.68, "end": 1536.96, "text": " But I would say the compatibility with the MySQL dialect, it's very, very high.", "tokens": [583, 286, 576, 584, 264, 34237, 365, 264, 1222, 39934, 24652, 11, 309, 311, 588, 11, 588, 1090, 13], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 298, "seek": 151788, "start": 1536.96, "end": 1541.92, "text": " But of course, like management commands for replication doesn't work because we don't", "tokens": [583, 295, 1164, 11, 411, 4592, 16901, 337, 39911, 1177, 380, 589, 570, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 299, "seek": 151788, "start": 1541.92, "end": 1543.96, "text": " do replication.", "tokens": [360, 39911, 13], "temperature": 0.0, "avg_logprob": -0.23839017847082117, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0005904850549995899}, {"id": 300, "seek": 154396, "start": 1543.96, "end": 1550.1200000000001, "text": " We have internal replication or we use change data capture for transfer to another cluster.", "tokens": [492, 362, 6920, 39911, 420, 321, 764, 1319, 1412, 7983, 337, 5003, 281, 1071, 13630, 13], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 301, "seek": 154396, "start": 1550.1200000000001, "end": 1552.1200000000001, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 302, "seek": 154396, "start": 1552.1200000000001, "end": 1554.1200000000001, "text": " Last question.", "tokens": [5264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 303, "seek": 154396, "start": 1554.1200000000001, "end": 1561.56, "text": " What that clash does when there is high rate of single cell updates, like how it handles", "tokens": [708, 300, 36508, 775, 562, 456, 307, 1090, 3314, 295, 2167, 2815, 9205, 11, 411, 577, 309, 18722], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 304, "seek": 154396, "start": 1561.56, "end": 1566.16, "text": " this, like rewriting the code files or keeping it separate?", "tokens": [341, 11, 411, 319, 19868, 264, 3089, 7098, 420, 5145, 309, 4994, 30], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 305, "seek": 154396, "start": 1566.16, "end": 1569.76, "text": " It's a derivative of click house.", "tokens": [467, 311, 257, 13760, 295, 2052, 1782, 13], "temperature": 0.0, "avg_logprob": -0.469856923421224, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.0005529082845896482}, {"id": 306, "seek": 156976, "start": 1569.76, "end": 1576.8799999999999, "text": " So it caches the changes and then it updates it partially or rewrites the whole.", "tokens": [407, 309, 269, 13272, 264, 2962, 293, 550, 309, 9205, 309, 18886, 420, 319, 86, 30931, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 307, "seek": 156976, "start": 1576.8799999999999, "end": 1580.92, "text": " Can this kind of get clicked behind after TKV because it takes more time?", "tokens": [1664, 341, 733, 295, 483, 23370, 2261, 934, 314, 42, 53, 570, 309, 2516, 544, 565, 30], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 308, "seek": 156976, "start": 1580.92, "end": 1581.92, "text": " It can.", "tokens": [467, 393, 13], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 309, "seek": 156976, "start": 1581.92, "end": 1588.48, "text": " But if it's behind, then it will more or less fall back here.", "tokens": [583, 498, 309, 311, 2261, 11, 550, 309, 486, 544, 420, 1570, 2100, 646, 510, 13], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 310, "seek": 156976, "start": 1588.48, "end": 1589.96, "text": " You have some tweaking options.", "tokens": [509, 362, 512, 6986, 2456, 3956, 13], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 311, "seek": 156976, "start": 1589.96, "end": 1598.0, "text": " You can even do it as optimizer hints that you want to use either engine, for example,", "tokens": [509, 393, 754, 360, 309, 382, 5028, 6545, 27271, 300, 291, 528, 281, 764, 2139, 2848, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 312, "seek": 156976, "start": 1598.0, "end": 1599.0, "text": " etc.", "tokens": [5183, 13], "temperature": 0.0, "avg_logprob": -0.25307130813598633, "compression_ratio": 1.519650655021834, "no_speech_prob": 0.0004986036219634116}, {"id": 313, "seek": 159900, "start": 1599.0, "end": 1600.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3826038654033954, "compression_ratio": 1.3008849557522124, "no_speech_prob": 0.001697832252830267}, {"id": 314, "seek": 159900, "start": 1600.0, "end": 1602.6, "text": " If there are more questions, I'm sure Mattias will be able to answer.", "tokens": [759, 456, 366, 544, 1651, 11, 286, 478, 988, 7397, 4609, 486, 312, 1075, 281, 1867, 13], "temperature": 0.0, "avg_logprob": -0.3826038654033954, "compression_ratio": 1.3008849557522124, "no_speech_prob": 0.001697832252830267}, {"id": 315, "seek": 159900, "start": 1602.6, "end": 1603.6, "text": " Yeah, I'm here.", "tokens": [865, 11, 286, 478, 510, 13], "temperature": 0.0, "avg_logprob": -0.3826038654033954, "compression_ratio": 1.3008849557522124, "no_speech_prob": 0.001697832252830267}, {"id": 316, "seek": 159900, "start": 1603.6, "end": 1606.6, "text": " Even Daniel is here as well.", "tokens": [2754, 8033, 307, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3826038654033954, "compression_ratio": 1.3008849557522124, "no_speech_prob": 0.001697832252830267}, {"id": 317, "seek": 159900, "start": 1606.6, "end": 1607.6, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3826038654033954, "compression_ratio": 1.3008849557522124, "no_speech_prob": 0.001697832252830267}, {"id": 318, "seek": 160760, "start": 1607.6, "end": 1629.24, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51446], "temperature": 0.2, "avg_logprob": -0.9887194633483887, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0006897632265463471}], "language": "en"}