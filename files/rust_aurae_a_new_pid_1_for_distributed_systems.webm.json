{"text": " Check, one, two, hello. Hello. Hello. Hi. Where's Malte? Hi. Hi. Nice to meet you. Okay. Sorry. Just like one of my hacker friends that has been working with me on the project. I've actually never met him in person, so nice to meet you. Anyway, today we're going to be talking about Aura or Aode, however you want to pronounce it, is fine, which we're temporarily calling a distributed systems runtime, and that's the name that has caused the least amount of friction over the past few months. Okay. So, my least favorite slide, my slide about me, so I'm an engineer, I work at GitHub, I helpkeepgithub.com online, sorry about the Shaw thing last week. Yeah. So, I keep a lot of systems online, some of you may or may not use them, all of you hopefully have good opinions of them, and then if you want to follow me on the Fediverse, there's where you can follow me. So I'll do overview and context, so if you want to go to the GitHub repo, you can grab a photo of this or just remember it, the link to the slides are there right now, I just forced push to main like two seconds ago, so you can go and you can see the slides and there's like links to everything there that I'll be going over today. So if you want to grab that, go ahead and grab that. Okay. So, we're going to start off, I'll do a little bit of context, I'll answer the question, what is Aura, what does it do, and then we'll spend the last two thirds of the presentation talking about Rust and why we decided to use Rust for the project and some reports about how it's going so far and some of my experience as well. Okay. So, just show of hands, who here has heard of Aura before? Oh, God. Okay. Well, thank you for following my project, that makes me very happy but also a little terrified. So anyway, Aura, it's an open source Rust project and it's aimed at simplifying node management at scale. And so, when I talk about it, I usually say it's basically a generic execution engine for containers, VMs, and processes. The really quick pitch that I'll give on Aura is all of these things, containers, VMs, hypervisors, and basic process management is all that I do at GitHub and all that I have done in my career for the past 10 years. And I have used a plethora of tools to do this and I was tired of learning and managing all these different tools and so I hope that this will be the last tool I ever have to work on in my career. So I wrote a thesis about the project and I'm trying hard to continually reevaluate this thesis and basically it says that by bringing some deliberate runtime controls to a node, we can unlock a new generation of higher order distributed systems. And what I mean by that is, in my experience, a lot of the things we do on a node are organic and grew over the past 30 years or so. And this is more of a deliberate set of what do we need in the enterprise and what do we need at a bare minimum on the node. And I think that if we get that right, we're actually going to have a much more interesting conversations in the coming decades. So I also believe that simplifying the execution stack will foster and secure observable systems while reducing complexity and risk. And complexity, if you have ever ran Kubernetes, is the name of the game. Cool. So I'll be talking about these things called nodes today. So node is a keyword. And when I say node, pretty much always in life, but very specifically in this talk, what I mean is a single compute unit in a set. So this would be one or more computers that we're trying to group together and manage as a set of computers. So when we do one thing to a node, the sort of assumption here is you want to go and do this twice or three times or 10,000 times sometimes or so on. So when we say node, I want you to think of a set of computers or an array of computers. OK. So what does Aura do? So the thesis here is this is going to be a central control for every runtime process on a node. So whether you're running PID 1 or a container or a virtual machine, the hope is that all of this can be funneled through the Aura binary at runtime, and Aura will have the ability to not only manage it, but also observe it and control it and start it and stop it. And who knows? Maybe even one day debug it if I'm very lucky. It runs as a minimal in its system. So this is important. A lot of folks want to compare Aura to system D. And the more I think about it, the more I think that I really believe Aura and system D have different goals. Aura doesn't really want to become a desktop manager. In fact, it kind of wants to be the opposite of that. It wants to be as lightweight and as minimal as possible. In a perfect world, there would be no user space on an Aura system because we wouldn't actually want users touching a single computer. Remember, we're managing sets of computers. And so the hope here is that we can make this as lightweight as possible. Additionally, we want this thing to have a remote API. So the idea of a single person sitting at a desk and operating on a single node is kind of irrelevant here. So everything that we do on the node, whether it's scheduling another process like a bash shell or it's scheduling a container, should all come through this remote API. And we're going to learn more about this API in Rust specifically later on in the talk. Also it runs on Linux. Right now it's tightly coupled to the Linux kernel. So what doesn't it do? So it doesn't do generic desktop support. So that's just completely out of scope. I don't want to deal with your Bluetooth drivers. I don't want to deal with your sound drivers. I don't want to manage your desktop interface. I don't care. In a perfect world, this hooks up to network and that's about the most advanced user interface we're going to have to one of these nodes in a set. Additionally, higher order scheduling is out of scope. So when we talk about enterprise management, whether it's some sort of orchestration system like Kubernetes or not, a lot of those discussions very quickly go into the scheduling discussion. There was a really good article, I think it was yesterday or the day before on Hacker News that came out of fly.io about their orchestrator experience with Nomad. I see somebody shaking their head. Yeah, you read the article. It was a great article. And maybe we can find a link to it and put it in the video or something for folks. But that conversation was very much about how do we make scheduling decisions with available resources today. And that is pretty much all I do at my day job at GitHub and that's all I've been doing managing Kubernetes for the past five or six years. And so while I'm very interested in having that conversation, my hope is that by simplifying the node, we can make those scheduling conversations easier in the future. And what I mean by that is that we will have less to say about what we actually do on a node and we can effectively make nodes boring. So it doesn't run on Darwin and it doesn't run on Windows. Like I said, we're tightly coupled to the Linux kernel, which if you haven't pieced it together yet, is why Rust is very exciting for the project. Okay, so again in summary, where did Aura come from? It came with challenges with complexity at scale, so we just want the node to be boring. And it became, there was this desire to simplify and secure the stack. So I do deeply believe that with simple systems come secure systems. Every hack that I've been a part of in the industry has usually started with some sort of disparate and unknown fragmented attack surface that somebody's been able to exploit and do some sort of lateral movement once they're into the system. So if we can simplify that and we can just make the conversation involve less moving pieces, my hope is that we can actually secure the stack. I also want there to be a stronger node API. So who here has ever debugged the KubeLit API before? Who here even knows what this is? Okay, so we have a handful of people. So the KubeLit is a Kubernetes version of we're going to go run an agent on a node. It does have an API, last I checked it was undocumented and it was tightly coupled with the Kubernetes control plane. We hope to break that. We hope to just have a generic API that you could use to run a single process remotely or you could schedule millions of processes remotely and we want that to be a very strong and thoughtful API. One of the big lessons of running large distributed systems at scale is that the bigger you get, the less trust that you can have in the people working on your systems. So as I've grown either like my small mastodon server that's grown into a medium sized mastodon server or even dealing with thousands of nodes at scale. One of the lessons that I've noticed is that all workloads tend to this untrusted banality. So the bigger you get, the less you can trust a single workload. And even if these workloads are on the same team as you, you really want to start looking at them as an isolation zone that you don't want to trust too much from the centralized control plane perspective. So we started off ORA with a few guiding principles. Number one, I want it to be boring. So we're targeting a single binary. We want this binary to be polymorphic in nature. Who here is familiar with Busybox? Great. Yeah, Busybox. It's a good binary in my opinion. I really like what it does. There's a switch on R0 and it basically behaves like however you call it. So we're trying to get some similar functionality into the ORA binary as well. And we also want this thing to be lightweight and have a very strong scope and be as low risk as possible. Additionally, we wanted this thing to be attainable. We wanted to play nice with others. So I knew that I wanted this to fit in neatly with Kubernetes. I knew I wanted this to fit in neatly with Linux. And I knew I wanted pretty much everyone in this room to feel realistically like they could be running this thing on their laptops one day as the project grows. And so in order to do that, the API was going to be the majority of what we were talking about as we began developing the project. And ultimately, I wanted it to be functional. I don't want it to subserve the needs of a corporation. I don't want it to serve the needs of a higher order control plane. I literally just want a standard library for executing processes in containers and VMs at scale. What we do with that is out of scope. I just want it to work first and foremost. So ultimately, I want boring systems. And if you see in the background here, there's all of these like subtle distributed system propaganda notes that you can go look at if you want to look at the slides later. So ultimately, I wanted the thing to be safe. So when we're looking at tenant security, one of the questions I ask is, how do we make it easy to do the right thing? And I think that comes from the underlying infrastructure. And in our case, Aura is the underlying infrastructure. And we intended to build a very strong project here that would unlock this sort of safe paradigm that we could give a team a binary, and they would be able to run their applications on top of it. And we wouldn't really have to worry about anybody sneaking out of their container or accessing any parts of the systems. We didn't want them to access. So tenant security is a strong motivator for this as well. OK. So about six months ago on Twitch, which I do a Twitch stream. You should maybe follow me if you want to learn more about the project. But I started to write this paper. And it was mostly as some bro in chat was like, yo, why don't you just go rebuild system D? And I was just like, maybe I will. And so anyway, I ended up writing this paper. And so, well, here we are. And so the paper really grew. And it started to answer a bunch of questions about, why should we go write it and go? No, no, no. We should go write it in C, because C is going to be the most common language that will interface neatly with the kernel, and we can do EVPF probes and so on. No, no, no, no. We should go write it in Rust. You can go look, there's a Google doc, and it's just got all these comments of people from all over the internet, all over the industry, arguing about what we should do. And eventually, we settled on, we want a lightweight node, Damon, and thus became the Aura runtime project. OK. So this is where we shift from the conceptual, what is Aura? How did we get here? What problems does it solve? And we start to get a little deeper into the code. So when we originally started the project, we started writing it in Go, the Go programming language. And there's two kind of predecessor projects that later turned into Aura, which is written in Rust. This first one that we call Aura Legacy, which up until about five, well, I guess 15 minutes ago now, but right before I walked into the room, this was a private GitHub repo, and I've gone ahead and actually opened it up. So if you want to go see the original code in Go, there's some really interesting things in there. We did some libp2p bit torrent style routing between nodes, where you can build a nest of nodes and things. But you can really see where this runtime, Damon, started and some of the original concepts that we were tinkering around with. Ultimately, though, we ran into a lot of the same problems that I ran into in Kubernetes, which was I needed to start recreating these objects, and I needed to start reading some config, whether that be YAML, JSON, or something similar, and then marshal that onto a struct in memory, and then go and do arbitrary things with that, in our case, schedule a pod. And one of the things that was kind of outstanding in the back of my mind was, what about access to libc? I knew as soon as we started scheduling containers and DMs, we absolutely were going to need native access to libc. Additionally, there's this project called NAML, which is basically Turing Complete Kubernetes Config, it's written in Go, and it just uses the Go SDK, and that was yet another way of sort of validating this idea of we need to start making our system stronger and building stronger interfaces for teams to manage different parts of the stack. So those are the two sort of precursors to the Aura runtime as it exists today. That of course, writing it in Go came with some challenges. The big one here is obviously native access to libc. We were going to be creating C groups against the Linux kernel. We definitely wanted to use the clone3 system call, and the container runtimes of today had some assumptions about how we were going to be executing the clone3 system call that, of course, I had to disagree with because, hi, have you met me? I have to disagree with everything. And we also wanted to implement some ptrace functionality as well. So obviously, Go was going to give us some challenges here when it came to using CGo, so Rust became very exciting and definitely got a lot of attention very quickly as we were writing the Go side of things. We also wanted ebpf for networking. I personally want it for security and maybe for some other interesting service mesh ideas, but I do think that having ebpf for networking as a non-negotiable, we're definitely going to want to simplify what Kubernetes refers to as kubeproxy that we can now invent our own name and hopefully simplify that layer, but I digress. We also wanted some access to native virtualization library, so all the KBM stuff is written in C. And if you go look at the Firecracker code base, that is also written in Rust that vendors the KBM bindings. And so we knew we would want to access these three components, and all three of these are going to be problematic with Go. Update as of about an hour ago, I went to the state of the Go room across the hall here. Did anybody else go to the Go talk this morning? Yeah, we got three or four hands up here, so this kind of pissed me off. Go has unwrapped now as of 1.2.0, and they also freaking have.clone. And I was just like, bro, get off our keywords, this is totally like, this is our thing. So anyway, it's really exciting to see Go taking these concepts a little more seriously, and if you've ever written Rust before, who here has written unwrap in Rust? Put your hands down, we're not supposed to do that, I don't know what we're supposed to use now, I just get so much shit on my Twitch stream every time I write unwrap, but yes, we do have unwrap and clone in Go now, which is just a strong indicator that we're likely doing something right with Rust. So anyway, I made the decision to move to Rust, and I didn't know very much about Rust when I made the decision, and I literally just started out the main function and said, we'll figure it out as we go, and I ordered the Rust book and just jumped in and started to write code with the hope of accessing kernel constructs and C groups and EBPF probes. So what could possibly go wrong here? Okay, so how are we doing on time, by the way, we're 15 minutes in, okay, cool. So Rust to help us solve the YAML problem, I suspect we're all familiar with feeding YAML to machines, we've all done this before at some point in our lifetime, okay. So this is a thing I do a lot working in large distributed systems, and I work with people who do this a lot, and if we do it so much, we've tried to get really good at doing it, and that I think, that's an interesting discussion. So in my opinion, so warning, Chris Nova opinions here, in my opinion, all config ultimately is going to drift towards Turing completion. So I see this C++ templates, anybody, anybody C++ templates, okay, Helm charts, customizing Kubernetes, any of the templating rendering languages that you see in web dev and front end work, there's all kinds of interesting Python libraries that will allow you to interpolate your config and so on. In my opinion, a good balance is kind of something like bash that is Turing complete, but it just comes with some strong guarantees. And so I knew very quickly that I didn't want to be feeding YAML to Aura. I definitely didn't want to recreate this idea of we're going to have to manage a thousand pieces of YAML because we have a thousand different nodes. So I wanted to explore more about what are some options that we have here, so we're not just feeding YAML to machines anymore. So thus became this really interesting project of mine, we'll see if this pans out, which is this binary called AuraScript. So AuraScript is a, it's a Rust binary, we have it compiling with Muzzle today, and embeds all of the connection logic for a single machine. And so we'll talk more about the semantics of AuraScript in a second. But ultimately what you need to understand to kind of get the initial motivation here is that this aims to be an alternative to managing YAML at scale. So I found this really fascinating type script runtime called Dino. Have folks heard of Dino before? Can I swear in here? I, FN, love Dino. I'm sorry, I really like this project. If you want a good example of like, hey, I just want to see a really successful Rust project that has a really strong community, I would encourage you to just go look at the Dino project. I think their code is beautiful, I think what it does is beautiful, I think the way that they manage the project is beautiful, it's just a really good quality project and it solves a problem for us with Aura. And so Dino is basically, it's a runtime for type script and it's written in Rust. And the way the project is set up, that you can go and you can add your own custom interpreted logic and you can build fancy things into the binary and you can do things with the type script interpretation at runtime, which is precisely what we needed to do with Aura. So here is the model now. So instead of feeding YAML to a single node, we now have this higher order set of libraries that we can statically compile into a binary and we can interpret it directly on a machine. So in order for you to interface with an Aura node or a set of nodes, all you need is one binary, mtls config and then whatever type script you want to write. And this is an alternative to like any of the Nomad command line tools or the Mesos command line tools or the Kubernetes, kubectl, kubectl command line tool. And now you can just write it all directly in type script. So this is actually a concrete example of what would be, what system D would call a unit file, what Kubernetes would call a manifest and what Aura just calls a freaking type script file because we don't have fancy names for our stuff yet. So you can see here at the top, we basically contact the Aura standard library. We get a new client and then we can allocate this thing called a cell. A cell is basically an abstraction for a C group. We cordon off a section of the system and we say like we want to use a certain percentage of the available CPUs on a node and I want it to only let processes run. In this case for 0.4 seconds and then we'll use the kernel to just kill the process if it runs longer than that. And so the first thing we would do is we would allocate that which is an improvement over Kubernetes as it exists today because we can allocate resources before we actually start anything in that area and then we can go ahead and actually start whatever we want. And so you can see I simplified the example just for today but it's just, it's remote command injection as a service. So this whole talk was just basically like how to go and run a bash command in on a server. And so now you can express your commands and similar primitives that you would see in other run times directly in TypeScript. The interesting thing here is TypeScript is just natively more expressive than a lot of the Amble things that we see today. In this case we can actually do math but I'm sure you can imagine you can do other things as well. You can access logic, loops, if statements, there's if branching and so on. And so we were able to actually solve some of these like templatey rendering style problems by just doing things natively in a well known and easy to understand language such as TypeScript. So patterns started to emerge. So Rust gave us the ability to generate the TypeScript binary with all of the magic behind the scenes MTLS security config that we wanted. And so now the conversation was a little more like this which is how do I manage a small set of TypeScript and it's much more flexible and you can start to actually express things the way that we used to and just express things statically and then you can have all of your Turing complete logical components below and you can mix and match these however you want. So in addition to addressing the YAML problem with Dino and TypeScript, Rust also helped us to solve the sidecar problem and by us, I mean this is our hope as we operate our mastodon servers and our various other ridiculous side projects that we operate both in my basement and in a colo in Germany. So talking about sidecars, who here knows what a sidecar is, show of hands, okay most folks do. Okay, so a sidecar that is always available with the same features as a host. So this is going to sound a little bit weird and the slide is going to look a little bit weird but just bear with me as we kind of like unpack what's actually going on here. What we want that I don't think we're talking about is that sentence. I actually think what we want is we want a sidecar to sit along our applications that does literally the exact same things we have to do on a given host whenever we're managing these workloads at scale. As I began looking into writing sidecars at the host level, I began drilling deeper and deeper into the C programming language as I was writing this in Rust and just made the connection that memory safety was going to be key because we're going to be running these demons right alongside of your workload. And so unpacking the need to do this really helps you understand why we shifted over to Rust. So again, another Chris Nova opinion, any sufficiently mature infrastructure service will evolve into a sidecar. So if you have done any sort of structured logging, in my opinion, if you will continue to build structured logging and you'll continue to ship logs, that will eventually turn into a sidecar that you're going to want to go run beside your app so you have this transparent logging experience. You can rinse and repeat that paradigm for pretty much anything, secrets, authentication data, and so on. And so I started to see these patterns kind of surface. And very specifically, I started to look at how would I solve these with Rust? And as it turns out, the Rust ecosystem had a plethora of pleasant surprises for me as I started to explore what putting some of these features into a binary would look like. Logging was boring because we could just use Tokyo Streams, Auth N and Auth Z was boring because all I had to do was just use the Rust-derived primitives to just start applying Auth Z to each of our units in the source code. Identity was boring because I didn't even get to fight with open SSL anymore. We just had to use Rust TLS and that was easy. And so the network was also easy because we had native access to Linux and Lib C so we could just very boringly schedule a Linux device and we got a Linux device and it was pretty straightforward. So we were able to create this at the node and now my question was how do we bring this into the workload level at scale? And I think this is where most of the conversations you start talking about things like Istio and service meshes and structured logging and so forth. And I actually think that we can simplify that conversation too. And so what we were able to do with Aura is we just spawned the root daemon and use that as the new PID one in any of our nested isolation zones. And when I say spawn, I very directly mean like we literally read the byte code from the kernel and we build an image at runtime with the byte for byte, the same byte code that's running on the host and then we can just go and execute whatever we want against the same API as the original host runs and all of this is memory safe. So I can put this right next to your application in the same namespaces running in a container or running in a virtual machine and there's a relatively low risk of any sort of binary exploitation at scale. So here's a model of what that looks like. So on the left big side here we have the Aura host daemon and on the right we have the three types of isolation zones that you can run with the daemon. You have a cell sandbox which is effectively a C group, a pod sandbox which is a group of containers running in unique Linux namespaces and a virtual machine which is effectively a container with a kernel and some virtualization technology. All of this is possible with Rust natively and all of this was made possible by spawning the binary and creating these nested isolation zones at runtime. Additionally Rust was able to help solve the untrusted workload problem because of the memory safety and that Rust offers and because of this really interesting model that we have right here. So this is a zoomed in model that might look familiar if you've ever done any container escapes before and in this model basically what we're saying is we're replacing any sort of like pause or initialization sequence in an isolation zone with the same daemon we run on the host. So I think the Rust binary for Aura right now is about 40 megabytes and we can just copy that into a container and run that alongside your application. So it's a relatively small application, runtime that will sit right alongside of your app so managing memory from MTLS and RID. So as I'm writing Rust one of the things I notice is I start paying attention to memory management more every time I try to clone something or the freaking borrow checker yells at me that kind of like is a small like grim reminder of my roots as a C developer. This is an interesting takeaway the only memory that we need to share that multiple parts of the system have access to in this entire model whether we're creating containers or VMs is the shared MTLS config. So this is the only bit of shared memory that we really have to manage and Rust very clearly called that out and to be a candidate I don't think I would be able to as be as comfortable with this model if I was doing this in something like go. So Rust was able to help us solve the maintainability problem so somebody say Rust macros. So we have a really brilliant guy future highway who helps us work on the project and future highway is our resident macro guy. Does everybody here have a macro guy in your team? Because you should. He has made things a lot simpler for us. So one of the things we struggled with go in Kubernetes specifically was like how do we generate objects with unique logic. Rust macros were a solution to this for us. So if you've ever looked at the Kubernetes code base you can see we've created these things called CRDs that started out as third party resources and we've built this entire bespoke API machinery system that basically is a glorified macro system that allows us to generate go in the project. So we're allowed to use Rust macros now and it's a very simple model in the code base. We basically have a combinatorics problem where we're able to map the different primitives to the different logical systems that are unique to us and we can generate our source code as needed. And so our source code ends up looking like this which I think we've successfully achieved boring for a low level run time. This is a fairly straightforward call and then we can be confident that the code it generates is unique to the project and encapsulates all of our concerns as maintainers. So really the whole conversation now is just the proto conversation. Everything can be generated by Rust macros. The whole project really is pretty much on autogen at this point. You can just go introduce a new field in the API and then you can spit out a new client, it'll plumb itself into the run time, it'll plumb itself into the AuraScript library and everything is given to us for free just because of macros in Rust. And so this is our code path and the way that we're able to take advantage of macros. We do a lot of manual work, we fight with the borough checker, we make some improvements and then we get done and we encapsulate it into a macro and we can simplify our code path by just replacing all of that with a macro after we've been done. And so this is the Aura project as it exists today, which again I'm very stoked to say that this is a very boring exercise. So a quick update and then I'll be done with my talk here. There's a few components, all of which are written in Rust here. Number one, the AuraD daemon is the main static binary that's written in Rust and compiled with Muzzle. So we can ship that without any of the shared objects on the host directly into an isolation zone. AER is a completely generated from Proto Client. So this is exciting, we can actually call a GRPC API directly from the client, we don't have to do any of the run time plumbing. So if we add a bool to the Proto file, we get dash dash bool directly in the client compiled for free without typing a single line of code. So this is a very exciting primitive for us, so we can just begin to have API conversations and not necessarily care about the internals of the program anymore. AuraScript is completely generated and we have this exciting project down here, which is AE, which is an alternative command line client written in Go. So ultimately the lesson here is Rust was able to help us solve the boring problem. We have a very complicated, very obscure piece of technology that is you don't really have to do much to work on it anymore. Most of it's on autopilot at this point and most of the conversations are very philosophical in nature and not necessarily about how to implement things in the software. So takeaways about the project, Aura is completely stateless, so you can restart a node and it's basically empty until you push config to it, which means all of our systems are declarative like NixOS now and you can just pass things like TypeScript or JSON to them and it makes it easy to manage things like containers. Next we have some to-dos for the project and I would encourage you all to get involved and if you want to see a demo of all this, I'll be out here in the hallway after the talk and you can come and you can track me down and I'm happy to give you a demo. So anyway, I think we have a few minutes for questions and five minutes for questions, so I'll take questions and if you want to get involved, here's how to get involved and I'm Chris Nova, please clap. You mentioned the size of the binary being, does it work? You mentioned the size of the binary being 40 megabytes, is that with size optimization or no? Sorry, say that again? Is the size of the binary at 40 megabytes with size optimization applied already or no? No, that's completely unoptimized, that is like just straight out of the compiler without any aftermarket tuning. Amazing talk, quick question. So if I want to have just enough Linux to like pixie boot into this thing, like do you guys have any templates because it feels like a shame to run it on something like RHEL, like I just need like enough of Linux to just pixie boot into that? Yeah, so the question is basically can we pixie boot this and then you mentioned RHEL. Where we're going, we don't need Red Hat, so I guess what I would say is in theory all you need to run is static Linux kernel and Aura and a network connection and some MTLS config, and so everything else at that point, all of your packages, your services, your daemons are passed to it via the API. Hi, you mentioned that you use a lot of macros. I've also run into problems where, you know, you have a combinatorial explosion of templates in C++ speak or something like that. What are your thoughts on generics for generating some of this rather than macros in order to be a bit more type safe, I suppose? Personally, I got a little drunk with generics, I'm not going to lie. When I first moved over from Go, because I was just so excited about it, the reason I like macros is because we can add logic to them. So we have, like to give you an example, we have containers and we have VMs. So we'll have a section of the macro dedicated just to VMs that manages the kernel. And that's irrelevant to the container systems in the project because containers run on the host kernel. And so we can embed those small branches directly into the macro code so that macros generate slightly different outputs based off of the inputs that are given to them. So for Aura, when you're dealing with similar systems of code that have small nuances like we are, macros really, in my opinion, are the way to go. Did I answer your question? Looks like. A simple question, so can I actually give the configuration instead of like Aura script or TypeScript just in Rust? Yeah, of course. So we have this Rust client here, it's basically a Rust SDK. And then we have a tool called AER, which takes it a step further and it's automatically generated with macros. And it's a compiled binary that you can just use from the command line. So you can just type commands directly into it and it will run against the server on the back end. Do you think code is Rust? Yeah, there's also an SDK. So you could write your own Rust code and it's GRPC. So you could generate, you could write it in Go or in WeDo and you could write it in Python or Ruby or realistically anything, any client you want. Hi. I was wondering when you talk about the remote API, have you considered a future direction to make this a unicolonel? A unicolonel. Yeah. Yeah. I have a slide for this. So I added like a bunch of like FAQ slides to the end because I knew that we were going to get all these good questions. The answer is it depends, hold on, let's see if I can't find it. You guys get to see. There it is. It depends. What does unicolonel mean to you? I think the most minimal system we could do would be a Linux kernel as it exists today, like good old fashioned stock Linux giant make file to hold nine yards. And then the ORID daemon and that would be the minimal system. Anything else you would need to pass to it at runtime? I think we have time for about one more question. So you said it doesn't do any higher order scheduling. I guess I'm kind of curious what, if you want to do things like resilience or steering or if the job dies, bring something back up, what are people typically using with Aura? So Aura is still very new. I think that my hope for the project is kind of like the same hope I had with my book, solve the lower layer first and then that is going to open the door for higher order conversations in the future. My hope is that there's a whole ecosystem of schedulers. You change your scheduler, you change your socks, well maybe not that often, but the point would be that that's very specific to the needs of the current organization that's working on it. And I would hope that we can still use the Kubernetes scheduler or the Nomad scheduler to schedule jobs on Aura. I know there's also some machine learning folks who have some data resiliency problems that are interested in Aura right now and plan on using some weird global mesh that will do a peer-to-peer network around the world, kind of like BitTorrent, and then they intend to use Aura for that. So I think there's some opportunities there. The project itself won't ever have an opinion on a scheduler. Maybe I personally will start another project to do that in the future or something, but this is the scope for now. So that's all the time we have. Okay. Can we hear it again?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Check, one, two, hello.", "tokens": [6881, 11, 472, 11, 732, 11, 7751, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 1, "seek": 0, "start": 7.0, "end": 8.0, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 2, "seek": 0, "start": 8.0, "end": 9.0, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 3, "seek": 0, "start": 9.0, "end": 10.0, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 4, "seek": 0, "start": 10.0, "end": 11.0, "text": " Where's Malte?", "tokens": [2305, 311, 5746, 975, 30], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 5, "seek": 0, "start": 11.0, "end": 12.0, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 6, "seek": 0, "start": 12.0, "end": 13.0, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 7, "seek": 0, "start": 13.0, "end": 14.0, "text": " Nice to meet you.", "tokens": [5490, 281, 1677, 291, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 8, "seek": 0, "start": 14.0, "end": 15.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 9, "seek": 0, "start": 15.0, "end": 16.0, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 10, "seek": 0, "start": 16.0, "end": 18.0, "text": " Just like one of my hacker friends that has been working with me on the project.", "tokens": [1449, 411, 472, 295, 452, 38155, 1855, 300, 575, 668, 1364, 365, 385, 322, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 11, "seek": 0, "start": 18.0, "end": 22.0, "text": " I've actually never met him in person, so nice to meet you.", "tokens": [286, 600, 767, 1128, 1131, 796, 294, 954, 11, 370, 1481, 281, 1677, 291, 13], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 12, "seek": 0, "start": 22.0, "end": 26.68, "text": " Anyway, today we're going to be talking about Aura or Aode, however you want to pronounce", "tokens": [5684, 11, 965, 321, 434, 516, 281, 312, 1417, 466, 316, 2991, 420, 316, 1429, 11, 4461, 291, 528, 281, 19567], "temperature": 0.0, "avg_logprob": -0.2728364657511753, "compression_ratio": 1.4885844748858448, "no_speech_prob": 0.0824866071343422}, {"id": 13, "seek": 2668, "start": 26.68, "end": 31.68, "text": " it, is fine, which we're temporarily calling a distributed systems runtime, and that's", "tokens": [309, 11, 307, 2489, 11, 597, 321, 434, 23750, 5141, 257, 12631, 3652, 34474, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 14, "seek": 2668, "start": 31.68, "end": 36.68, "text": " the name that has caused the least amount of friction over the past few months.", "tokens": [264, 1315, 300, 575, 7008, 264, 1935, 2372, 295, 17710, 670, 264, 1791, 1326, 2493, 13], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 15, "seek": 2668, "start": 36.68, "end": 37.68, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 16, "seek": 2668, "start": 37.68, "end": 45.239999999999995, "text": " So, my least favorite slide, my slide about me, so I'm an engineer, I work at GitHub,", "tokens": [407, 11, 452, 1935, 2954, 4137, 11, 452, 4137, 466, 385, 11, 370, 286, 478, 364, 11403, 11, 286, 589, 412, 23331, 11], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 17, "seek": 2668, "start": 45.239999999999995, "end": 50.68, "text": " I helpkeepgithub.com online, sorry about the Shaw thing last week.", "tokens": [286, 854, 16055, 70, 355, 836, 13, 1112, 2950, 11, 2597, 466, 264, 27132, 551, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 18, "seek": 2668, "start": 50.68, "end": 51.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25050987735871344, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.1710480875044595e-06}, {"id": 19, "seek": 5168, "start": 51.68, "end": 58.0, "text": " So, I keep a lot of systems online, some of you may or may not use them, all of you hopefully", "tokens": [407, 11, 286, 1066, 257, 688, 295, 3652, 2950, 11, 512, 295, 291, 815, 420, 815, 406, 764, 552, 11, 439, 295, 291, 4696], "temperature": 0.0, "avg_logprob": -0.148752509423022, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.220893515594071e-06}, {"id": 20, "seek": 5168, "start": 58.0, "end": 61.64, "text": " have good opinions of them, and then if you want to follow me on the Fediverse, there's", "tokens": [362, 665, 11819, 295, 552, 11, 293, 550, 498, 291, 528, 281, 1524, 385, 322, 264, 7772, 5376, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.148752509423022, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.220893515594071e-06}, {"id": 21, "seek": 5168, "start": 61.64, "end": 64.48, "text": " where you can follow me.", "tokens": [689, 291, 393, 1524, 385, 13], "temperature": 0.0, "avg_logprob": -0.148752509423022, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.220893515594071e-06}, {"id": 22, "seek": 5168, "start": 64.48, "end": 72.12, "text": " So I'll do overview and context, so if you want to go to the GitHub repo, you can grab", "tokens": [407, 286, 603, 360, 12492, 293, 4319, 11, 370, 498, 291, 528, 281, 352, 281, 264, 23331, 49040, 11, 291, 393, 4444], "temperature": 0.0, "avg_logprob": -0.148752509423022, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.220893515594071e-06}, {"id": 23, "seek": 5168, "start": 72.12, "end": 76.03999999999999, "text": " a photo of this or just remember it, the link to the slides are there right now, I just", "tokens": [257, 5052, 295, 341, 420, 445, 1604, 309, 11, 264, 2113, 281, 264, 9788, 366, 456, 558, 586, 11, 286, 445], "temperature": 0.0, "avg_logprob": -0.148752509423022, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.220893515594071e-06}, {"id": 24, "seek": 7604, "start": 76.04, "end": 82.28, "text": " forced push to main like two seconds ago, so you can go and you can see the slides and", "tokens": [7579, 2944, 281, 2135, 411, 732, 3949, 2057, 11, 370, 291, 393, 352, 293, 291, 393, 536, 264, 9788, 293], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 25, "seek": 7604, "start": 82.28, "end": 84.68, "text": " there's like links to everything there that I'll be going over today.", "tokens": [456, 311, 411, 6123, 281, 1203, 456, 300, 286, 603, 312, 516, 670, 965, 13], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 26, "seek": 7604, "start": 84.68, "end": 87.04, "text": " So if you want to grab that, go ahead and grab that.", "tokens": [407, 498, 291, 528, 281, 4444, 300, 11, 352, 2286, 293, 4444, 300, 13], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 27, "seek": 7604, "start": 87.04, "end": 88.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 28, "seek": 7604, "start": 88.04, "end": 91.4, "text": " So, we're going to start off, I'll do a little bit of context, I'll answer the question,", "tokens": [407, 11, 321, 434, 516, 281, 722, 766, 11, 286, 603, 360, 257, 707, 857, 295, 4319, 11, 286, 603, 1867, 264, 1168, 11], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 29, "seek": 7604, "start": 91.4, "end": 97.72, "text": " what is Aura, what does it do, and then we'll spend the last two thirds of the presentation", "tokens": [437, 307, 316, 2991, 11, 437, 775, 309, 360, 11, 293, 550, 321, 603, 3496, 264, 1036, 732, 34552, 295, 264, 5860], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 30, "seek": 7604, "start": 97.72, "end": 104.12, "text": " talking about Rust and why we decided to use Rust for the project and some reports about", "tokens": [1417, 466, 34952, 293, 983, 321, 3047, 281, 764, 34952, 337, 264, 1716, 293, 512, 7122, 466], "temperature": 0.0, "avg_logprob": -0.15357164236215445, "compression_ratio": 1.7509025270758123, "no_speech_prob": 1.3005101209273562e-05}, {"id": 31, "seek": 10412, "start": 104.12, "end": 107.56, "text": " how it's going so far and some of my experience as well.", "tokens": [577, 309, 311, 516, 370, 1400, 293, 512, 295, 452, 1752, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 32, "seek": 10412, "start": 107.56, "end": 108.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 33, "seek": 10412, "start": 108.56, "end": 111.76, "text": " So, just show of hands, who here has heard of Aura before?", "tokens": [407, 11, 445, 855, 295, 2377, 11, 567, 510, 575, 2198, 295, 316, 2991, 949, 30], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 34, "seek": 10412, "start": 111.76, "end": 112.76, "text": " Oh, God.", "tokens": [876, 11, 1265, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 35, "seek": 10412, "start": 112.76, "end": 113.76, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 36, "seek": 10412, "start": 113.76, "end": 122.16, "text": " Well, thank you for following my project, that makes me very happy but also a little", "tokens": [1042, 11, 1309, 291, 337, 3480, 452, 1716, 11, 300, 1669, 385, 588, 2055, 457, 611, 257, 707], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 37, "seek": 10412, "start": 122.16, "end": 124.16, "text": " terrified.", "tokens": [23051, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 38, "seek": 10412, "start": 124.16, "end": 128.48000000000002, "text": " So anyway, Aura, it's an open source Rust project and it's aimed at simplifying node", "tokens": [407, 4033, 11, 316, 2991, 11, 309, 311, 364, 1269, 4009, 34952, 1716, 293, 309, 311, 20540, 412, 6883, 5489, 9984], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 39, "seek": 10412, "start": 128.48000000000002, "end": 131.0, "text": " management at scale.", "tokens": [4592, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.21545936065970114, "compression_ratio": 1.4824561403508771, "no_speech_prob": 2.8127647055953275e-06}, {"id": 40, "seek": 13100, "start": 131.0, "end": 134.76, "text": " And so, when I talk about it, I usually say it's basically a generic execution engine", "tokens": [400, 370, 11, 562, 286, 751, 466, 309, 11, 286, 2673, 584, 309, 311, 1936, 257, 19577, 15058, 2848], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 41, "seek": 13100, "start": 134.76, "end": 138.68, "text": " for containers, VMs, and processes.", "tokens": [337, 17089, 11, 18038, 82, 11, 293, 7555, 13], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 42, "seek": 13100, "start": 138.68, "end": 144.64, "text": " The really quick pitch that I'll give on Aura is all of these things, containers, VMs,", "tokens": [440, 534, 1702, 7293, 300, 286, 603, 976, 322, 316, 2991, 307, 439, 295, 613, 721, 11, 17089, 11, 18038, 82, 11], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 43, "seek": 13100, "start": 144.64, "end": 150.08, "text": " hypervisors, and basic process management is all that I do at GitHub and all that I", "tokens": [9848, 4938, 830, 11, 293, 3875, 1399, 4592, 307, 439, 300, 286, 360, 412, 23331, 293, 439, 300, 286], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 44, "seek": 13100, "start": 150.08, "end": 152.12, "text": " have done in my career for the past 10 years.", "tokens": [362, 1096, 294, 452, 3988, 337, 264, 1791, 1266, 924, 13], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 45, "seek": 13100, "start": 152.12, "end": 157.28, "text": " And I have used a plethora of tools to do this and I was tired of learning and managing", "tokens": [400, 286, 362, 1143, 257, 499, 302, 7013, 295, 3873, 281, 360, 341, 293, 286, 390, 5868, 295, 2539, 293, 11642], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 46, "seek": 13100, "start": 157.28, "end": 160.8, "text": " all these different tools and so I hope that this will be the last tool I ever have to", "tokens": [439, 613, 819, 3873, 293, 370, 286, 1454, 300, 341, 486, 312, 264, 1036, 2290, 286, 1562, 362, 281], "temperature": 0.0, "avg_logprob": -0.12552830752204447, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.336769390851259e-06}, {"id": 47, "seek": 16080, "start": 160.8, "end": 165.4, "text": " work on in my career.", "tokens": [589, 322, 294, 452, 3988, 13], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 48, "seek": 16080, "start": 165.4, "end": 171.20000000000002, "text": " So I wrote a thesis about the project and I'm trying hard to continually reevaluate this", "tokens": [407, 286, 4114, 257, 22288, 466, 264, 1716, 293, 286, 478, 1382, 1152, 281, 22277, 43060, 3337, 10107, 341], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 49, "seek": 16080, "start": 171.20000000000002, "end": 177.0, "text": " thesis and basically it says that by bringing some deliberate runtime controls to a node,", "tokens": [22288, 293, 1936, 309, 1619, 300, 538, 5062, 512, 30515, 34474, 9003, 281, 257, 9984, 11], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 50, "seek": 16080, "start": 177.0, "end": 180.64000000000001, "text": " we can unlock a new generation of higher order distributed systems.", "tokens": [321, 393, 11634, 257, 777, 5125, 295, 2946, 1668, 12631, 3652, 13], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 51, "seek": 16080, "start": 180.64000000000001, "end": 185.48000000000002, "text": " And what I mean by that is, in my experience, a lot of the things we do on a node are organic", "tokens": [400, 437, 286, 914, 538, 300, 307, 11, 294, 452, 1752, 11, 257, 688, 295, 264, 721, 321, 360, 322, 257, 9984, 366, 10220], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 52, "seek": 16080, "start": 185.48000000000002, "end": 188.84, "text": " and grew over the past 30 years or so.", "tokens": [293, 6109, 670, 264, 1791, 2217, 924, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.13127137174700745, "compression_ratio": 1.56640625, "no_speech_prob": 7.5261432357365265e-06}, {"id": 53, "seek": 18884, "start": 188.84, "end": 193.28, "text": " And this is more of a deliberate set of what do we need in the enterprise and what do we", "tokens": [400, 341, 307, 544, 295, 257, 30515, 992, 295, 437, 360, 321, 643, 294, 264, 14132, 293, 437, 360, 321], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 54, "seek": 18884, "start": 193.28, "end": 195.08, "text": " need at a bare minimum on the node.", "tokens": [643, 412, 257, 6949, 7285, 322, 264, 9984, 13], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 55, "seek": 18884, "start": 195.08, "end": 198.08, "text": " And I think that if we get that right, we're actually going to have a much more interesting", "tokens": [400, 286, 519, 300, 498, 321, 483, 300, 558, 11, 321, 434, 767, 516, 281, 362, 257, 709, 544, 1880], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 56, "seek": 18884, "start": 198.08, "end": 201.12, "text": " conversations in the coming decades.", "tokens": [7315, 294, 264, 1348, 7878, 13], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 57, "seek": 18884, "start": 201.12, "end": 206.84, "text": " So I also believe that simplifying the execution stack will foster and secure observable systems", "tokens": [407, 286, 611, 1697, 300, 6883, 5489, 264, 15058, 8630, 486, 17114, 293, 7144, 9951, 712, 3652], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 58, "seek": 18884, "start": 206.84, "end": 209.72, "text": " while reducing complexity and risk.", "tokens": [1339, 12245, 14024, 293, 3148, 13], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 59, "seek": 18884, "start": 209.72, "end": 214.68, "text": " And complexity, if you have ever ran Kubernetes, is the name of the game.", "tokens": [400, 14024, 11, 498, 291, 362, 1562, 5872, 23145, 11, 307, 264, 1315, 295, 264, 1216, 13], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 60, "seek": 18884, "start": 214.68, "end": 215.68, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.12222602678381879, "compression_ratio": 1.6702508960573477, "no_speech_prob": 4.9363607104169205e-06}, {"id": 61, "seek": 21568, "start": 215.68, "end": 218.92000000000002, "text": " So I'll be talking about these things called nodes today.", "tokens": [407, 286, 603, 312, 1417, 466, 613, 721, 1219, 13891, 965, 13], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 62, "seek": 21568, "start": 218.92000000000002, "end": 220.88, "text": " So node is a keyword.", "tokens": [407, 9984, 307, 257, 20428, 13], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 63, "seek": 21568, "start": 220.88, "end": 226.20000000000002, "text": " And when I say node, pretty much always in life, but very specifically in this talk,", "tokens": [400, 562, 286, 584, 9984, 11, 1238, 709, 1009, 294, 993, 11, 457, 588, 4682, 294, 341, 751, 11], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 64, "seek": 21568, "start": 226.20000000000002, "end": 230.24, "text": " what I mean is a single compute unit in a set.", "tokens": [437, 286, 914, 307, 257, 2167, 14722, 4985, 294, 257, 992, 13], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 65, "seek": 21568, "start": 230.24, "end": 235.20000000000002, "text": " So this would be one or more computers that we're trying to group together and manage", "tokens": [407, 341, 576, 312, 472, 420, 544, 10807, 300, 321, 434, 1382, 281, 1594, 1214, 293, 3067], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 66, "seek": 21568, "start": 235.20000000000002, "end": 236.36, "text": " as a set of computers.", "tokens": [382, 257, 992, 295, 10807, 13], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 67, "seek": 21568, "start": 236.36, "end": 241.08, "text": " So when we do one thing to a node, the sort of assumption here is you want to go and", "tokens": [407, 562, 321, 360, 472, 551, 281, 257, 9984, 11, 264, 1333, 295, 15302, 510, 307, 291, 528, 281, 352, 293], "temperature": 0.0, "avg_logprob": -0.10256056829330025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.8618093135009985e-06}, {"id": 68, "seek": 24108, "start": 241.08, "end": 247.28, "text": " do this twice or three times or 10,000 times sometimes or so on.", "tokens": [360, 341, 6091, 420, 1045, 1413, 420, 1266, 11, 1360, 1413, 2171, 420, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 69, "seek": 24108, "start": 247.28, "end": 252.52, "text": " So when we say node, I want you to think of a set of computers or an array of computers.", "tokens": [407, 562, 321, 584, 9984, 11, 286, 528, 291, 281, 519, 295, 257, 992, 295, 10807, 420, 364, 10225, 295, 10807, 13], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 70, "seek": 24108, "start": 252.52, "end": 253.72000000000003, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 71, "seek": 24108, "start": 253.72000000000003, "end": 257.68, "text": " So what does Aura do?", "tokens": [407, 437, 775, 316, 2991, 360, 30], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 72, "seek": 24108, "start": 257.68, "end": 261.76, "text": " So the thesis here is this is going to be a central control for every runtime process", "tokens": [407, 264, 22288, 510, 307, 341, 307, 516, 281, 312, 257, 5777, 1969, 337, 633, 34474, 1399], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 73, "seek": 24108, "start": 261.76, "end": 262.76, "text": " on a node.", "tokens": [322, 257, 9984, 13], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 74, "seek": 24108, "start": 262.76, "end": 268.64, "text": " So whether you're running PID 1 or a container or a virtual machine, the hope is that all", "tokens": [407, 1968, 291, 434, 2614, 430, 2777, 502, 420, 257, 10129, 420, 257, 6374, 3479, 11, 264, 1454, 307, 300, 439], "temperature": 0.0, "avg_logprob": -0.13283592405773345, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.8570782433234854e-06}, {"id": 75, "seek": 26864, "start": 268.64, "end": 274.2, "text": " of this can be funneled through the Aura binary at runtime, and Aura will have the ability", "tokens": [295, 341, 393, 312, 24515, 292, 807, 264, 316, 2991, 17434, 412, 34474, 11, 293, 316, 2991, 486, 362, 264, 3485], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 76, "seek": 26864, "start": 274.2, "end": 279.68, "text": " to not only manage it, but also observe it and control it and start it and stop it.", "tokens": [281, 406, 787, 3067, 309, 11, 457, 611, 11441, 309, 293, 1969, 309, 293, 722, 309, 293, 1590, 309, 13], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 77, "seek": 26864, "start": 279.68, "end": 280.68, "text": " And who knows?", "tokens": [400, 567, 3255, 30], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 78, "seek": 26864, "start": 280.68, "end": 284.44, "text": " Maybe even one day debug it if I'm very lucky.", "tokens": [2704, 754, 472, 786, 24083, 309, 498, 286, 478, 588, 6356, 13], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 79, "seek": 26864, "start": 284.44, "end": 286.52, "text": " It runs as a minimal in its system.", "tokens": [467, 6676, 382, 257, 13206, 294, 1080, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 80, "seek": 26864, "start": 286.52, "end": 288.08, "text": " So this is important.", "tokens": [407, 341, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 81, "seek": 26864, "start": 288.08, "end": 292.12, "text": " A lot of folks want to compare Aura to system D. And the more I think about it, the more", "tokens": [316, 688, 295, 4024, 528, 281, 6794, 316, 2991, 281, 1185, 413, 13, 400, 264, 544, 286, 519, 466, 309, 11, 264, 544], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 82, "seek": 26864, "start": 292.12, "end": 296.91999999999996, "text": " I think that I really believe Aura and system D have different goals.", "tokens": [286, 519, 300, 286, 534, 1697, 316, 2991, 293, 1185, 413, 362, 819, 5493, 13], "temperature": 0.0, "avg_logprob": -0.12116070244255966, "compression_ratio": 1.6654411764705883, "no_speech_prob": 1.5687382983742282e-05}, {"id": 83, "seek": 29692, "start": 296.92, "end": 299.36, "text": " Aura doesn't really want to become a desktop manager.", "tokens": [316, 2991, 1177, 380, 534, 528, 281, 1813, 257, 14502, 6598, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 84, "seek": 29692, "start": 299.36, "end": 301.84000000000003, "text": " In fact, it kind of wants to be the opposite of that.", "tokens": [682, 1186, 11, 309, 733, 295, 2738, 281, 312, 264, 6182, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 85, "seek": 29692, "start": 301.84000000000003, "end": 305.56, "text": " It wants to be as lightweight and as minimal as possible.", "tokens": [467, 2738, 281, 312, 382, 22052, 293, 382, 13206, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 86, "seek": 29692, "start": 305.56, "end": 310.44, "text": " In a perfect world, there would be no user space on an Aura system because we wouldn't", "tokens": [682, 257, 2176, 1002, 11, 456, 576, 312, 572, 4195, 1901, 322, 364, 316, 2991, 1185, 570, 321, 2759, 380], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 87, "seek": 29692, "start": 310.44, "end": 313.44, "text": " actually want users touching a single computer.", "tokens": [767, 528, 5022, 11175, 257, 2167, 3820, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 88, "seek": 29692, "start": 313.44, "end": 316.68, "text": " Remember, we're managing sets of computers.", "tokens": [5459, 11, 321, 434, 11642, 6352, 295, 10807, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 89, "seek": 29692, "start": 316.68, "end": 321.52000000000004, "text": " And so the hope here is that we can make this as lightweight as possible.", "tokens": [400, 370, 264, 1454, 510, 307, 300, 321, 393, 652, 341, 382, 22052, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 90, "seek": 29692, "start": 321.52000000000004, "end": 325.20000000000005, "text": " Additionally, we want this thing to have a remote API.", "tokens": [19927, 11, 321, 528, 341, 551, 281, 362, 257, 8607, 9362, 13], "temperature": 0.0, "avg_logprob": -0.12774085210374564, "compression_ratio": 1.7518518518518518, "no_speech_prob": 3.041188620045432e-06}, {"id": 91, "seek": 32520, "start": 325.2, "end": 330.76, "text": " So the idea of a single person sitting at a desk and operating on a single node is kind", "tokens": [407, 264, 1558, 295, 257, 2167, 954, 3798, 412, 257, 10026, 293, 7447, 322, 257, 2167, 9984, 307, 733], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 92, "seek": 32520, "start": 330.76, "end": 331.88, "text": " of irrelevant here.", "tokens": [295, 28682, 510, 13], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 93, "seek": 32520, "start": 331.88, "end": 336.56, "text": " So everything that we do on the node, whether it's scheduling another process like a bash", "tokens": [407, 1203, 300, 321, 360, 322, 264, 9984, 11, 1968, 309, 311, 29055, 1071, 1399, 411, 257, 46183], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 94, "seek": 32520, "start": 336.56, "end": 341.8, "text": " shell or it's scheduling a container, should all come through this remote API.", "tokens": [8720, 420, 309, 311, 29055, 257, 10129, 11, 820, 439, 808, 807, 341, 8607, 9362, 13], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 95, "seek": 32520, "start": 341.8, "end": 348.08, "text": " And we're going to learn more about this API in Rust specifically later on in the talk.", "tokens": [400, 321, 434, 516, 281, 1466, 544, 466, 341, 9362, 294, 34952, 4682, 1780, 322, 294, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 96, "seek": 32520, "start": 348.08, "end": 349.59999999999997, "text": " Also it runs on Linux.", "tokens": [2743, 309, 6676, 322, 18734, 13], "temperature": 0.0, "avg_logprob": -0.12346777319908142, "compression_ratio": 1.6125, "no_speech_prob": 6.5393010117986705e-06}, {"id": 97, "seek": 34960, "start": 349.6, "end": 355.48, "text": " Right now it's tightly coupled to the Linux kernel.", "tokens": [1779, 586, 309, 311, 21952, 29482, 281, 264, 18734, 28256, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 98, "seek": 34960, "start": 355.48, "end": 357.68, "text": " So what doesn't it do?", "tokens": [407, 437, 1177, 380, 309, 360, 30], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 99, "seek": 34960, "start": 357.68, "end": 359.76000000000005, "text": " So it doesn't do generic desktop support.", "tokens": [407, 309, 1177, 380, 360, 19577, 14502, 1406, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 100, "seek": 34960, "start": 359.76000000000005, "end": 361.08000000000004, "text": " So that's just completely out of scope.", "tokens": [407, 300, 311, 445, 2584, 484, 295, 11923, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 101, "seek": 34960, "start": 361.08000000000004, "end": 363.68, "text": " I don't want to deal with your Bluetooth drivers.", "tokens": [286, 500, 380, 528, 281, 2028, 365, 428, 20286, 11590, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 102, "seek": 34960, "start": 363.68, "end": 365.76000000000005, "text": " I don't want to deal with your sound drivers.", "tokens": [286, 500, 380, 528, 281, 2028, 365, 428, 1626, 11590, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 103, "seek": 34960, "start": 365.76000000000005, "end": 368.52000000000004, "text": " I don't want to manage your desktop interface.", "tokens": [286, 500, 380, 528, 281, 3067, 428, 14502, 9226, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 104, "seek": 34960, "start": 368.52000000000004, "end": 370.08000000000004, "text": " I don't care.", "tokens": [286, 500, 380, 1127, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 105, "seek": 34960, "start": 370.08000000000004, "end": 373.88, "text": " In a perfect world, this hooks up to network and that's about the most advanced user interface", "tokens": [682, 257, 2176, 1002, 11, 341, 26485, 493, 281, 3209, 293, 300, 311, 466, 264, 881, 7339, 4195, 9226], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 106, "seek": 34960, "start": 373.88, "end": 377.36, "text": " we're going to have to one of these nodes in a set.", "tokens": [321, 434, 516, 281, 362, 281, 472, 295, 613, 13891, 294, 257, 992, 13], "temperature": 0.0, "avg_logprob": -0.12674275040626526, "compression_ratio": 1.796875, "no_speech_prob": 2.521397391319624e-06}, {"id": 107, "seek": 37736, "start": 377.36, "end": 380.76, "text": " Additionally, higher order scheduling is out of scope.", "tokens": [19927, 11, 2946, 1668, 29055, 307, 484, 295, 11923, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 108, "seek": 37736, "start": 380.76, "end": 385.68, "text": " So when we talk about enterprise management, whether it's some sort of orchestration system", "tokens": [407, 562, 321, 751, 466, 14132, 4592, 11, 1968, 309, 311, 512, 1333, 295, 14161, 2405, 1185], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 109, "seek": 37736, "start": 385.68, "end": 391.44, "text": " like Kubernetes or not, a lot of those discussions very quickly go into the scheduling discussion.", "tokens": [411, 23145, 420, 406, 11, 257, 688, 295, 729, 11088, 588, 2661, 352, 666, 264, 29055, 5017, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 110, "seek": 37736, "start": 391.44, "end": 395.28000000000003, "text": " There was a really good article, I think it was yesterday or the day before on Hacker", "tokens": [821, 390, 257, 534, 665, 7222, 11, 286, 519, 309, 390, 5186, 420, 264, 786, 949, 322, 389, 23599], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 111, "seek": 37736, "start": 395.28000000000003, "end": 399.88, "text": " News that came out of fly.io about their orchestrator experience with Nomad.", "tokens": [7987, 300, 1361, 484, 295, 3603, 13, 1004, 466, 641, 14161, 19802, 1752, 365, 31272, 345, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 112, "seek": 37736, "start": 399.88, "end": 401.16, "text": " I see somebody shaking their head.", "tokens": [286, 536, 2618, 15415, 641, 1378, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 113, "seek": 37736, "start": 401.16, "end": 402.96000000000004, "text": " Yeah, you read the article.", "tokens": [865, 11, 291, 1401, 264, 7222, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 114, "seek": 37736, "start": 402.96000000000004, "end": 404.56, "text": " It was a great article.", "tokens": [467, 390, 257, 869, 7222, 13], "temperature": 0.0, "avg_logprob": -0.1254814692905971, "compression_ratio": 1.6779661016949152, "no_speech_prob": 8.138487828546204e-06}, {"id": 115, "seek": 40456, "start": 404.56, "end": 409.44, "text": " And maybe we can find a link to it and put it in the video or something for folks.", "tokens": [400, 1310, 321, 393, 915, 257, 2113, 281, 309, 293, 829, 309, 294, 264, 960, 420, 746, 337, 4024, 13], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 116, "seek": 40456, "start": 409.44, "end": 413.96, "text": " But that conversation was very much about how do we make scheduling decisions with available", "tokens": [583, 300, 3761, 390, 588, 709, 466, 577, 360, 321, 652, 29055, 5327, 365, 2435], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 117, "seek": 40456, "start": 413.96, "end": 415.68, "text": " resources today.", "tokens": [3593, 965, 13], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 118, "seek": 40456, "start": 415.68, "end": 420.36, "text": " And that is pretty much all I do at my day job at GitHub and that's all I've been doing", "tokens": [400, 300, 307, 1238, 709, 439, 286, 360, 412, 452, 786, 1691, 412, 23331, 293, 300, 311, 439, 286, 600, 668, 884], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 119, "seek": 40456, "start": 420.36, "end": 423.88, "text": " managing Kubernetes for the past five or six years.", "tokens": [11642, 23145, 337, 264, 1791, 1732, 420, 2309, 924, 13], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 120, "seek": 40456, "start": 423.88, "end": 429.48, "text": " And so while I'm very interested in having that conversation, my hope is that by simplifying", "tokens": [400, 370, 1339, 286, 478, 588, 3102, 294, 1419, 300, 3761, 11, 452, 1454, 307, 300, 538, 6883, 5489], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 121, "seek": 40456, "start": 429.48, "end": 434.24, "text": " the node, we can make those scheduling conversations easier in the future.", "tokens": [264, 9984, 11, 321, 393, 652, 729, 29055, 7315, 3571, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.08847231824858849, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.6696227248758078e-05}, {"id": 122, "seek": 43424, "start": 434.24, "end": 438.72, "text": " And what I mean by that is that we will have less to say about what we actually do on a", "tokens": [400, 437, 286, 914, 538, 300, 307, 300, 321, 486, 362, 1570, 281, 584, 466, 437, 321, 767, 360, 322, 257], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 123, "seek": 43424, "start": 438.72, "end": 442.76, "text": " node and we can effectively make nodes boring.", "tokens": [9984, 293, 321, 393, 8659, 652, 13891, 9989, 13], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 124, "seek": 43424, "start": 442.76, "end": 445.64, "text": " So it doesn't run on Darwin and it doesn't run on Windows.", "tokens": [407, 309, 1177, 380, 1190, 322, 30233, 293, 309, 1177, 380, 1190, 322, 8591, 13], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 125, "seek": 43424, "start": 445.64, "end": 448.96000000000004, "text": " Like I said, we're tightly coupled to the Linux kernel, which if you haven't pieced", "tokens": [1743, 286, 848, 11, 321, 434, 21952, 29482, 281, 264, 18734, 28256, 11, 597, 498, 291, 2378, 380, 1730, 1232], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 126, "seek": 43424, "start": 448.96000000000004, "end": 454.88, "text": " it together yet, is why Rust is very exciting for the project.", "tokens": [309, 1214, 1939, 11, 307, 983, 34952, 307, 588, 4670, 337, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 127, "seek": 43424, "start": 454.88, "end": 458.88, "text": " Okay, so again in summary, where did Aura come from?", "tokens": [1033, 11, 370, 797, 294, 12691, 11, 689, 630, 316, 2991, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 128, "seek": 43424, "start": 458.88, "end": 463.28000000000003, "text": " It came with challenges with complexity at scale, so we just want the node to be boring.", "tokens": [467, 1361, 365, 4759, 365, 14024, 412, 4373, 11, 370, 321, 445, 528, 264, 9984, 281, 312, 9989, 13], "temperature": 0.0, "avg_logprob": -0.16643305122852325, "compression_ratio": 1.601328903654485, "no_speech_prob": 3.904764071194222e-06}, {"id": 129, "seek": 46328, "start": 463.28, "end": 469.28, "text": " And it became, there was this desire to simplify and secure the stack.", "tokens": [400, 309, 3062, 11, 456, 390, 341, 7516, 281, 20460, 293, 7144, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 130, "seek": 46328, "start": 469.28, "end": 474.08, "text": " So I do deeply believe that with simple systems come secure systems.", "tokens": [407, 286, 360, 8760, 1697, 300, 365, 2199, 3652, 808, 7144, 3652, 13], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 131, "seek": 46328, "start": 474.08, "end": 478.91999999999996, "text": " Every hack that I've been a part of in the industry has usually started with some sort", "tokens": [2048, 10339, 300, 286, 600, 668, 257, 644, 295, 294, 264, 3518, 575, 2673, 1409, 365, 512, 1333], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 132, "seek": 46328, "start": 478.91999999999996, "end": 484.32, "text": " of disparate and unknown fragmented attack surface that somebody's been able to exploit", "tokens": [295, 14548, 473, 293, 9841, 9241, 14684, 2690, 3753, 300, 2618, 311, 668, 1075, 281, 25924], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 133, "seek": 46328, "start": 484.32, "end": 488.11999999999995, "text": " and do some sort of lateral movement once they're into the system.", "tokens": [293, 360, 512, 1333, 295, 25128, 3963, 1564, 436, 434, 666, 264, 1185, 13], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 134, "seek": 46328, "start": 488.11999999999995, "end": 492.64, "text": " So if we can simplify that and we can just make the conversation involve less moving", "tokens": [407, 498, 321, 393, 20460, 300, 293, 321, 393, 445, 652, 264, 3761, 9494, 1570, 2684], "temperature": 0.0, "avg_logprob": -0.08650861596161465, "compression_ratio": 1.7388059701492538, "no_speech_prob": 1.1296643606328871e-05}, {"id": 135, "seek": 49264, "start": 492.64, "end": 496.84, "text": " pieces, my hope is that we can actually secure the stack.", "tokens": [3755, 11, 452, 1454, 307, 300, 321, 393, 767, 7144, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 136, "seek": 49264, "start": 496.84, "end": 499.15999999999997, "text": " I also want there to be a stronger node API.", "tokens": [286, 611, 528, 456, 281, 312, 257, 7249, 9984, 9362, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 137, "seek": 49264, "start": 499.15999999999997, "end": 504.68, "text": " So who here has ever debugged the KubeLit API before?", "tokens": [407, 567, 510, 575, 1562, 24083, 3004, 264, 591, 1977, 43, 270, 9362, 949, 30], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 138, "seek": 49264, "start": 504.68, "end": 506.15999999999997, "text": " Who here even knows what this is?", "tokens": [2102, 510, 754, 3255, 437, 341, 307, 30], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 139, "seek": 49264, "start": 506.15999999999997, "end": 508.76, "text": " Okay, so we have a handful of people.", "tokens": [1033, 11, 370, 321, 362, 257, 16458, 295, 561, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 140, "seek": 49264, "start": 508.76, "end": 514.3199999999999, "text": " So the KubeLit is a Kubernetes version of we're going to go run an agent on a node.", "tokens": [407, 264, 591, 1977, 43, 270, 307, 257, 23145, 3037, 295, 321, 434, 516, 281, 352, 1190, 364, 9461, 322, 257, 9984, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 141, "seek": 49264, "start": 514.3199999999999, "end": 519.2, "text": " It does have an API, last I checked it was undocumented and it was tightly coupled with", "tokens": [467, 775, 362, 364, 9362, 11, 1036, 286, 10033, 309, 390, 40472, 293, 309, 390, 21952, 29482, 365], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 142, "seek": 49264, "start": 519.2, "end": 521.28, "text": " the Kubernetes control plane.", "tokens": [264, 23145, 1969, 5720, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 143, "seek": 49264, "start": 521.28, "end": 522.28, "text": " We hope to break that.", "tokens": [492, 1454, 281, 1821, 300, 13], "temperature": 0.0, "avg_logprob": -0.17565947540046634, "compression_ratio": 1.629496402877698, "no_speech_prob": 4.859879481955431e-06}, {"id": 144, "seek": 52228, "start": 522.28, "end": 527.16, "text": " We hope to just have a generic API that you could use to run a single process remotely", "tokens": [492, 1454, 281, 445, 362, 257, 19577, 9362, 300, 291, 727, 764, 281, 1190, 257, 2167, 1399, 20824], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 145, "seek": 52228, "start": 527.16, "end": 531.52, "text": " or you could schedule millions of processes remotely and we want that to be a very strong", "tokens": [420, 291, 727, 7567, 6803, 295, 7555, 20824, 293, 321, 528, 300, 281, 312, 257, 588, 2068], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 146, "seek": 52228, "start": 531.52, "end": 537.24, "text": " and thoughtful API.", "tokens": [293, 21566, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 147, "seek": 52228, "start": 537.24, "end": 543.12, "text": " One of the big lessons of running large distributed systems at scale is that the bigger you get,", "tokens": [1485, 295, 264, 955, 8820, 295, 2614, 2416, 12631, 3652, 412, 4373, 307, 300, 264, 3801, 291, 483, 11], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 148, "seek": 52228, "start": 543.12, "end": 547.0, "text": " the less trust that you can have in the people working on your systems.", "tokens": [264, 1570, 3361, 300, 291, 393, 362, 294, 264, 561, 1364, 322, 428, 3652, 13], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 149, "seek": 52228, "start": 547.0, "end": 552.04, "text": " So as I've grown either like my small mastodon server that's grown into a medium sized mastodon", "tokens": [407, 382, 286, 600, 7709, 2139, 411, 452, 1359, 27055, 378, 266, 7154, 300, 311, 7709, 666, 257, 6399, 20004, 27055, 378, 266], "temperature": 0.0, "avg_logprob": -0.1017426924272017, "compression_ratio": 1.7137546468401488, "no_speech_prob": 2.6421682832733495e-06}, {"id": 150, "seek": 55204, "start": 552.04, "end": 556.04, "text": " server or even dealing with thousands of nodes at scale.", "tokens": [7154, 420, 754, 6260, 365, 5383, 295, 13891, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 151, "seek": 55204, "start": 556.04, "end": 562.1999999999999, "text": " One of the lessons that I've noticed is that all workloads tend to this untrusted banality.", "tokens": [1485, 295, 264, 8820, 300, 286, 600, 5694, 307, 300, 439, 32452, 3928, 281, 341, 1701, 81, 6589, 5643, 1860, 13], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 152, "seek": 55204, "start": 562.1999999999999, "end": 566.56, "text": " So the bigger you get, the less you can trust a single workload.", "tokens": [407, 264, 3801, 291, 483, 11, 264, 1570, 291, 393, 3361, 257, 2167, 20139, 13], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 153, "seek": 55204, "start": 566.56, "end": 569.8399999999999, "text": " And even if these workloads are on the same team as you, you really want to start looking", "tokens": [400, 754, 498, 613, 32452, 366, 322, 264, 912, 1469, 382, 291, 11, 291, 534, 528, 281, 722, 1237], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 154, "seek": 55204, "start": 569.8399999999999, "end": 576.28, "text": " at them as an isolation zone that you don't want to trust too much from the centralized", "tokens": [412, 552, 382, 364, 16001, 6668, 300, 291, 500, 380, 528, 281, 3361, 886, 709, 490, 264, 32395], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 155, "seek": 55204, "start": 576.28, "end": 580.92, "text": " control plane perspective.", "tokens": [1969, 5720, 4585, 13], "temperature": 0.0, "avg_logprob": -0.07916359807930741, "compression_ratio": 1.672, "no_speech_prob": 2.6422148948768154e-06}, {"id": 156, "seek": 58092, "start": 580.92, "end": 583.68, "text": " So we started off ORA with a few guiding principles.", "tokens": [407, 321, 1409, 766, 422, 3750, 365, 257, 1326, 25061, 9156, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 157, "seek": 58092, "start": 583.68, "end": 585.68, "text": " Number one, I want it to be boring.", "tokens": [5118, 472, 11, 286, 528, 309, 281, 312, 9989, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 158, "seek": 58092, "start": 585.68, "end": 587.76, "text": " So we're targeting a single binary.", "tokens": [407, 321, 434, 17918, 257, 2167, 17434, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 159, "seek": 58092, "start": 587.76, "end": 590.4, "text": " We want this binary to be polymorphic in nature.", "tokens": [492, 528, 341, 17434, 281, 312, 6754, 76, 18191, 299, 294, 3687, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 160, "seek": 58092, "start": 590.4, "end": 593.4399999999999, "text": " Who here is familiar with Busybox?", "tokens": [2102, 510, 307, 4963, 365, 8006, 88, 4995, 30], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 161, "seek": 58092, "start": 593.4399999999999, "end": 594.4399999999999, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 162, "seek": 58092, "start": 594.4399999999999, "end": 595.4399999999999, "text": " Yeah, Busybox.", "tokens": [865, 11, 8006, 88, 4995, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 163, "seek": 58092, "start": 595.4399999999999, "end": 597.0, "text": " It's a good binary in my opinion.", "tokens": [467, 311, 257, 665, 17434, 294, 452, 4800, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 164, "seek": 58092, "start": 597.0, "end": 598.0, "text": " I really like what it does.", "tokens": [286, 534, 411, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 165, "seek": 58092, "start": 598.0, "end": 602.1999999999999, "text": " There's a switch on R0 and it basically behaves like however you call it.", "tokens": [821, 311, 257, 3679, 322, 497, 15, 293, 309, 1936, 36896, 411, 4461, 291, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 166, "seek": 58092, "start": 602.1999999999999, "end": 607.92, "text": " So we're trying to get some similar functionality into the ORA binary as well.", "tokens": [407, 321, 434, 1382, 281, 483, 512, 2531, 14980, 666, 264, 422, 3750, 17434, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17324291058440708, "compression_ratio": 1.5724381625441697, "no_speech_prob": 2.85691385215614e-06}, {"id": 167, "seek": 60792, "start": 607.92, "end": 611.76, "text": " And we also want this thing to be lightweight and have a very strong scope and be as low", "tokens": [400, 321, 611, 528, 341, 551, 281, 312, 22052, 293, 362, 257, 588, 2068, 11923, 293, 312, 382, 2295], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 168, "seek": 60792, "start": 611.76, "end": 613.4399999999999, "text": " risk as possible.", "tokens": [3148, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 169, "seek": 60792, "start": 613.4399999999999, "end": 616.1999999999999, "text": " Additionally, we wanted this thing to be attainable.", "tokens": [19927, 11, 321, 1415, 341, 551, 281, 312, 23766, 712, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 170, "seek": 60792, "start": 616.1999999999999, "end": 617.64, "text": " We wanted to play nice with others.", "tokens": [492, 1415, 281, 862, 1481, 365, 2357, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 171, "seek": 60792, "start": 617.64, "end": 620.8, "text": " So I knew that I wanted this to fit in neatly with Kubernetes.", "tokens": [407, 286, 2586, 300, 286, 1415, 341, 281, 3318, 294, 36634, 365, 23145, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 172, "seek": 60792, "start": 620.8, "end": 623.0799999999999, "text": " I knew I wanted this to fit in neatly with Linux.", "tokens": [286, 2586, 286, 1415, 341, 281, 3318, 294, 36634, 365, 18734, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 173, "seek": 60792, "start": 623.0799999999999, "end": 627.4799999999999, "text": " And I knew I wanted pretty much everyone in this room to feel realistically like they", "tokens": [400, 286, 2586, 286, 1415, 1238, 709, 1518, 294, 341, 1808, 281, 841, 40734, 411, 436], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 174, "seek": 60792, "start": 627.4799999999999, "end": 632.28, "text": " could be running this thing on their laptops one day as the project grows.", "tokens": [727, 312, 2614, 341, 551, 322, 641, 27642, 472, 786, 382, 264, 1716, 13156, 13], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 175, "seek": 60792, "start": 632.28, "end": 636.24, "text": " And so in order to do that, the API was going to be the majority of what we were talking", "tokens": [400, 370, 294, 1668, 281, 360, 300, 11, 264, 9362, 390, 516, 281, 312, 264, 6286, 295, 437, 321, 645, 1417], "temperature": 0.0, "avg_logprob": -0.08472909246172224, "compression_ratio": 1.885135135135135, "no_speech_prob": 5.013508143747458e-06}, {"id": 176, "seek": 63624, "start": 636.24, "end": 639.72, "text": " about as we began developing the project.", "tokens": [466, 382, 321, 4283, 6416, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 177, "seek": 63624, "start": 639.72, "end": 641.32, "text": " And ultimately, I wanted it to be functional.", "tokens": [400, 6284, 11, 286, 1415, 309, 281, 312, 11745, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 178, "seek": 63624, "start": 641.32, "end": 644.28, "text": " I don't want it to subserve the needs of a corporation.", "tokens": [286, 500, 380, 528, 309, 281, 2090, 3768, 264, 2203, 295, 257, 22197, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 179, "seek": 63624, "start": 644.28, "end": 647.16, "text": " I don't want it to serve the needs of a higher order control plane.", "tokens": [286, 500, 380, 528, 309, 281, 4596, 264, 2203, 295, 257, 2946, 1668, 1969, 5720, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 180, "seek": 63624, "start": 647.16, "end": 652.8, "text": " I literally just want a standard library for executing processes in containers and VMs", "tokens": [286, 3736, 445, 528, 257, 3832, 6405, 337, 32368, 7555, 294, 17089, 293, 18038, 82], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 181, "seek": 63624, "start": 652.8, "end": 653.8, "text": " at scale.", "tokens": [412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 182, "seek": 63624, "start": 653.8, "end": 655.24, "text": " What we do with that is out of scope.", "tokens": [708, 321, 360, 365, 300, 307, 484, 295, 11923, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 183, "seek": 63624, "start": 655.24, "end": 658.92, "text": " I just want it to work first and foremost.", "tokens": [286, 445, 528, 309, 281, 589, 700, 293, 18864, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 184, "seek": 63624, "start": 658.92, "end": 660.96, "text": " So ultimately, I want boring systems.", "tokens": [407, 6284, 11, 286, 528, 9989, 3652, 13], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 185, "seek": 63624, "start": 660.96, "end": 665.84, "text": " And if you see in the background here, there's all of these like subtle distributed system", "tokens": [400, 498, 291, 536, 294, 264, 3678, 510, 11, 456, 311, 439, 295, 613, 411, 13743, 12631, 1185], "temperature": 0.0, "avg_logprob": -0.11180620762839247, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.165810590464389e-05}, {"id": 186, "seek": 66584, "start": 665.84, "end": 673.64, "text": " propaganda notes that you can go look at if you want to look at the slides later.", "tokens": [22968, 5570, 300, 291, 393, 352, 574, 412, 498, 291, 528, 281, 574, 412, 264, 9788, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 187, "seek": 66584, "start": 673.64, "end": 676.2, "text": " So ultimately, I wanted the thing to be safe.", "tokens": [407, 6284, 11, 286, 1415, 264, 551, 281, 312, 3273, 13], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 188, "seek": 66584, "start": 676.2, "end": 680.6, "text": " So when we're looking at tenant security, one of the questions I ask is, how do we make", "tokens": [407, 562, 321, 434, 1237, 412, 31000, 3825, 11, 472, 295, 264, 1651, 286, 1029, 307, 11, 577, 360, 321, 652], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 189, "seek": 66584, "start": 680.6, "end": 682.32, "text": " it easy to do the right thing?", "tokens": [309, 1858, 281, 360, 264, 558, 551, 30], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 190, "seek": 66584, "start": 682.32, "end": 685.36, "text": " And I think that comes from the underlying infrastructure.", "tokens": [400, 286, 519, 300, 1487, 490, 264, 14217, 6896, 13], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 191, "seek": 66584, "start": 685.36, "end": 688.84, "text": " And in our case, Aura is the underlying infrastructure.", "tokens": [400, 294, 527, 1389, 11, 316, 2991, 307, 264, 14217, 6896, 13], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 192, "seek": 66584, "start": 688.84, "end": 695.6, "text": " And we intended to build a very strong project here that would unlock this sort of safe paradigm", "tokens": [400, 321, 10226, 281, 1322, 257, 588, 2068, 1716, 510, 300, 576, 11634, 341, 1333, 295, 3273, 24709], "temperature": 0.0, "avg_logprob": -0.08057890440288343, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.7103767428779975e-06}, {"id": 193, "seek": 69560, "start": 695.6, "end": 699.8000000000001, "text": " that we could give a team a binary, and they would be able to run their applications on", "tokens": [300, 321, 727, 976, 257, 1469, 257, 17434, 11, 293, 436, 576, 312, 1075, 281, 1190, 641, 5821, 322], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 194, "seek": 69560, "start": 699.8000000000001, "end": 700.8000000000001, "text": " top of it.", "tokens": [1192, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 195, "seek": 69560, "start": 700.8000000000001, "end": 704.6, "text": " And we wouldn't really have to worry about anybody sneaking out of their container or", "tokens": [400, 321, 2759, 380, 534, 362, 281, 3292, 466, 4472, 48525, 484, 295, 641, 10129, 420], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 196, "seek": 69560, "start": 704.6, "end": 706.64, "text": " accessing any parts of the systems.", "tokens": [26440, 604, 3166, 295, 264, 3652, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 197, "seek": 69560, "start": 706.64, "end": 708.64, "text": " We didn't want them to access.", "tokens": [492, 994, 380, 528, 552, 281, 2105, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 198, "seek": 69560, "start": 708.64, "end": 715.16, "text": " So tenant security is a strong motivator for this as well.", "tokens": [407, 31000, 3825, 307, 257, 2068, 5426, 1639, 337, 341, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 199, "seek": 69560, "start": 715.16, "end": 716.16, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 200, "seek": 69560, "start": 716.16, "end": 721.48, "text": " So about six months ago on Twitch, which I do a Twitch stream.", "tokens": [407, 466, 2309, 2493, 2057, 322, 22222, 11, 597, 286, 360, 257, 22222, 4309, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 201, "seek": 69560, "start": 721.48, "end": 724.6800000000001, "text": " You should maybe follow me if you want to learn more about the project.", "tokens": [509, 820, 1310, 1524, 385, 498, 291, 528, 281, 1466, 544, 466, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.14501821694253875, "compression_ratio": 1.6268115942028984, "no_speech_prob": 2.99388489111152e-06}, {"id": 202, "seek": 72468, "start": 724.68, "end": 726.1999999999999, "text": " But I started to write this paper.", "tokens": [583, 286, 1409, 281, 2464, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 203, "seek": 72468, "start": 726.1999999999999, "end": 730.7199999999999, "text": " And it was mostly as some bro in chat was like, yo, why don't you just go rebuild system", "tokens": [400, 309, 390, 5240, 382, 512, 2006, 294, 5081, 390, 411, 11, 5290, 11, 983, 500, 380, 291, 445, 352, 16877, 1185], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 204, "seek": 72468, "start": 730.7199999999999, "end": 731.7199999999999, "text": " D?", "tokens": [413, 30], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 205, "seek": 72468, "start": 731.7199999999999, "end": 733.12, "text": " And I was just like, maybe I will.", "tokens": [400, 286, 390, 445, 411, 11, 1310, 286, 486, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 206, "seek": 72468, "start": 733.12, "end": 735.3599999999999, "text": " And so anyway, I ended up writing this paper.", "tokens": [400, 370, 4033, 11, 286, 4590, 493, 3579, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 207, "seek": 72468, "start": 735.3599999999999, "end": 736.9599999999999, "text": " And so, well, here we are.", "tokens": [400, 370, 11, 731, 11, 510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 208, "seek": 72468, "start": 736.9599999999999, "end": 739.1999999999999, "text": " And so the paper really grew.", "tokens": [400, 370, 264, 3035, 534, 6109, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 209, "seek": 72468, "start": 739.1999999999999, "end": 742.52, "text": " And it started to answer a bunch of questions about, why should we go write it and go?", "tokens": [400, 309, 1409, 281, 1867, 257, 3840, 295, 1651, 466, 11, 983, 820, 321, 352, 2464, 309, 293, 352, 30], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 210, "seek": 72468, "start": 742.52, "end": 743.52, "text": " No, no, no.", "tokens": [883, 11, 572, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 211, "seek": 72468, "start": 743.52, "end": 748.0, "text": " We should go write it in C, because C is going to be the most common language that will interface", "tokens": [492, 820, 352, 2464, 309, 294, 383, 11, 570, 383, 307, 516, 281, 312, 264, 881, 2689, 2856, 300, 486, 9226], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 212, "seek": 72468, "start": 748.0, "end": 750.8, "text": " neatly with the kernel, and we can do EVPF probes and so on.", "tokens": [36634, 365, 264, 28256, 11, 293, 321, 393, 360, 15733, 47, 37, 1239, 279, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 213, "seek": 72468, "start": 750.8, "end": 751.8, "text": " No, no, no, no.", "tokens": [883, 11, 572, 11, 572, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 214, "seek": 72468, "start": 751.8, "end": 752.8, "text": " We should go write it in Rust.", "tokens": [492, 820, 352, 2464, 309, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.13992028825738456, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0001214444637298584}, {"id": 215, "seek": 75280, "start": 752.8, "end": 755.8399999999999, "text": " You can go look, there's a Google doc, and it's just got all these comments of people", "tokens": [509, 393, 352, 574, 11, 456, 311, 257, 3329, 3211, 11, 293, 309, 311, 445, 658, 439, 613, 3053, 295, 561], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 216, "seek": 75280, "start": 755.8399999999999, "end": 760.0, "text": " from all over the internet, all over the industry, arguing about what we should do.", "tokens": [490, 439, 670, 264, 4705, 11, 439, 670, 264, 3518, 11, 19697, 466, 437, 321, 820, 360, 13], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 217, "seek": 75280, "start": 760.0, "end": 766.4399999999999, "text": " And eventually, we settled on, we want a lightweight node, Damon, and thus became the Aura runtime", "tokens": [400, 4728, 11, 321, 14819, 322, 11, 321, 528, 257, 22052, 9984, 11, 47197, 11, 293, 8807, 3062, 264, 316, 2991, 34474], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 218, "seek": 75280, "start": 766.4399999999999, "end": 768.4399999999999, "text": " project.", "tokens": [1716, 13], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 219, "seek": 75280, "start": 768.4399999999999, "end": 769.88, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 220, "seek": 75280, "start": 769.88, "end": 773.92, "text": " So this is where we shift from the conceptual, what is Aura?", "tokens": [407, 341, 307, 689, 321, 5513, 490, 264, 24106, 11, 437, 307, 316, 2991, 30], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 221, "seek": 75280, "start": 773.92, "end": 774.92, "text": " How did we get here?", "tokens": [1012, 630, 321, 483, 510, 30], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 222, "seek": 75280, "start": 774.92, "end": 776.3199999999999, "text": " What problems does it solve?", "tokens": [708, 2740, 775, 309, 5039, 30], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 223, "seek": 75280, "start": 776.3199999999999, "end": 780.3199999999999, "text": " And we start to get a little deeper into the code.", "tokens": [400, 321, 722, 281, 483, 257, 707, 7731, 666, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18106708218974452, "compression_ratio": 1.6167883211678833, "no_speech_prob": 5.33768798050005e-06}, {"id": 224, "seek": 78032, "start": 780.32, "end": 784.88, "text": " So when we originally started the project, we started writing it in Go, the Go programming", "tokens": [407, 562, 321, 7993, 1409, 264, 1716, 11, 321, 1409, 3579, 309, 294, 1037, 11, 264, 1037, 9410], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 225, "seek": 78032, "start": 784.88, "end": 785.88, "text": " language.", "tokens": [2856, 13], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 226, "seek": 78032, "start": 785.88, "end": 791.12, "text": " And there's two kind of predecessor projects that later turned into Aura, which is written", "tokens": [400, 456, 311, 732, 733, 295, 34991, 4455, 300, 1780, 3574, 666, 316, 2991, 11, 597, 307, 3720], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 227, "seek": 78032, "start": 791.12, "end": 792.6, "text": " in Rust.", "tokens": [294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 228, "seek": 78032, "start": 792.6, "end": 798.0400000000001, "text": " This first one that we call Aura Legacy, which up until about five, well, I guess 15 minutes", "tokens": [639, 700, 472, 300, 321, 818, 316, 2991, 42838, 11, 597, 493, 1826, 466, 1732, 11, 731, 11, 286, 2041, 2119, 2077], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 229, "seek": 78032, "start": 798.0400000000001, "end": 803.36, "text": " ago now, but right before I walked into the room, this was a private GitHub repo, and", "tokens": [2057, 586, 11, 457, 558, 949, 286, 7628, 666, 264, 1808, 11, 341, 390, 257, 4551, 23331, 49040, 11, 293], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 230, "seek": 78032, "start": 803.36, "end": 805.48, "text": " I've gone ahead and actually opened it up.", "tokens": [286, 600, 2780, 2286, 293, 767, 5625, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 231, "seek": 78032, "start": 805.48, "end": 810.2800000000001, "text": " So if you want to go see the original code in Go, there's some really interesting things", "tokens": [407, 498, 291, 528, 281, 352, 536, 264, 3380, 3089, 294, 1037, 11, 456, 311, 512, 534, 1880, 721], "temperature": 0.0, "avg_logprob": -0.1165706047644982, "compression_ratio": 1.6537216828478964, "no_speech_prob": 1.1118261681986041e-05}, {"id": 232, "seek": 81028, "start": 810.28, "end": 811.28, "text": " in there.", "tokens": [294, 456, 13], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 233, "seek": 81028, "start": 811.28, "end": 817.28, "text": " We did some libp2p bit torrent style routing between nodes, where you can build a nest of", "tokens": [492, 630, 512, 22854, 79, 17, 79, 857, 3930, 1753, 3758, 32722, 1296, 13891, 11, 689, 291, 393, 1322, 257, 15646, 295], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 234, "seek": 81028, "start": 817.28, "end": 818.28, "text": " nodes and things.", "tokens": [13891, 293, 721, 13], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 235, "seek": 81028, "start": 818.28, "end": 823.28, "text": " But you can really see where this runtime, Damon, started and some of the original concepts", "tokens": [583, 291, 393, 534, 536, 689, 341, 34474, 11, 47197, 11, 1409, 293, 512, 295, 264, 3380, 10392], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 236, "seek": 81028, "start": 823.28, "end": 827.28, "text": " that we were tinkering around with.", "tokens": [300, 321, 645, 256, 475, 1794, 926, 365, 13], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 237, "seek": 81028, "start": 827.28, "end": 831.4, "text": " Ultimately, though, we ran into a lot of the same problems that I ran into in Kubernetes,", "tokens": [23921, 11, 1673, 11, 321, 5872, 666, 257, 688, 295, 264, 912, 2740, 300, 286, 5872, 666, 294, 23145, 11], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 238, "seek": 81028, "start": 831.4, "end": 836.9599999999999, "text": " which was I needed to start recreating these objects, and I needed to start reading some", "tokens": [597, 390, 286, 2978, 281, 722, 850, 44613, 613, 6565, 11, 293, 286, 2978, 281, 722, 3760, 512], "temperature": 0.0, "avg_logprob": -0.24421650279651988, "compression_ratio": 1.696, "no_speech_prob": 1.2409182090777904e-05}, {"id": 239, "seek": 83696, "start": 836.96, "end": 842.2800000000001, "text": " config, whether that be YAML, JSON, or something similar, and then marshal that onto a struct", "tokens": [6662, 11, 1968, 300, 312, 398, 2865, 43, 11, 31828, 11, 420, 746, 2531, 11, 293, 550, 30517, 4947, 300, 3911, 257, 6594], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 240, "seek": 83696, "start": 842.2800000000001, "end": 848.36, "text": " in memory, and then go and do arbitrary things with that, in our case, schedule a pod.", "tokens": [294, 4675, 11, 293, 550, 352, 293, 360, 23211, 721, 365, 300, 11, 294, 527, 1389, 11, 7567, 257, 2497, 13], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 241, "seek": 83696, "start": 848.36, "end": 851.88, "text": " And one of the things that was kind of outstanding in the back of my mind was, what about access", "tokens": [400, 472, 295, 264, 721, 300, 390, 733, 295, 14485, 294, 264, 646, 295, 452, 1575, 390, 11, 437, 466, 2105], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 242, "seek": 83696, "start": 851.88, "end": 852.88, "text": " to libc?", "tokens": [281, 22854, 66, 30], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 243, "seek": 83696, "start": 852.88, "end": 856.9200000000001, "text": " I knew as soon as we started scheduling containers and DMs, we absolutely were going to need", "tokens": [286, 2586, 382, 2321, 382, 321, 1409, 29055, 17089, 293, 15322, 82, 11, 321, 3122, 645, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 244, "seek": 83696, "start": 856.9200000000001, "end": 858.96, "text": " native access to libc.", "tokens": [8470, 2105, 281, 22854, 66, 13], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 245, "seek": 83696, "start": 858.96, "end": 863.6, "text": " Additionally, there's this project called NAML, which is basically Turing Complete Kubernetes", "tokens": [19927, 11, 456, 311, 341, 1716, 1219, 426, 2865, 43, 11, 597, 307, 1936, 314, 1345, 34687, 23145], "temperature": 0.0, "avg_logprob": -0.16487082839012146, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.093287199997576e-06}, {"id": 246, "seek": 86360, "start": 863.6, "end": 870.32, "text": " Config, it's written in Go, and it just uses the Go SDK, and that was yet another way of", "tokens": [44151, 11, 309, 311, 3720, 294, 1037, 11, 293, 309, 445, 4960, 264, 1037, 37135, 11, 293, 300, 390, 1939, 1071, 636, 295], "temperature": 0.0, "avg_logprob": -0.12096505106231313, "compression_ratio": 1.5571428571428572, "no_speech_prob": 8.529192200512625e-06}, {"id": 247, "seek": 86360, "start": 870.32, "end": 875.0400000000001, "text": " sort of validating this idea of we need to start making our system stronger and building", "tokens": [1333, 295, 7363, 990, 341, 1558, 295, 321, 643, 281, 722, 1455, 527, 1185, 7249, 293, 2390], "temperature": 0.0, "avg_logprob": -0.12096505106231313, "compression_ratio": 1.5571428571428572, "no_speech_prob": 8.529192200512625e-06}, {"id": 248, "seek": 86360, "start": 875.0400000000001, "end": 879.6800000000001, "text": " stronger interfaces for teams to manage different parts of the stack.", "tokens": [7249, 28416, 337, 5491, 281, 3067, 819, 3166, 295, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.12096505106231313, "compression_ratio": 1.5571428571428572, "no_speech_prob": 8.529192200512625e-06}, {"id": 249, "seek": 86360, "start": 879.6800000000001, "end": 890.0, "text": " So those are the two sort of precursors to the Aura runtime as it exists today.", "tokens": [407, 729, 366, 264, 732, 1333, 295, 41736, 830, 281, 264, 316, 2991, 34474, 382, 309, 8198, 965, 13], "temperature": 0.0, "avg_logprob": -0.12096505106231313, "compression_ratio": 1.5571428571428572, "no_speech_prob": 8.529192200512625e-06}, {"id": 250, "seek": 89000, "start": 890.0, "end": 894.28, "text": " That of course, writing it in Go came with some challenges.", "tokens": [663, 295, 1164, 11, 3579, 309, 294, 1037, 1361, 365, 512, 4759, 13], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 251, "seek": 89000, "start": 894.28, "end": 898.56, "text": " The big one here is obviously native access to libc.", "tokens": [440, 955, 472, 510, 307, 2745, 8470, 2105, 281, 22854, 66, 13], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 252, "seek": 89000, "start": 898.56, "end": 902.28, "text": " We were going to be creating C groups against the Linux kernel.", "tokens": [492, 645, 516, 281, 312, 4084, 383, 3935, 1970, 264, 18734, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 253, "seek": 89000, "start": 902.28, "end": 907.52, "text": " We definitely wanted to use the clone3 system call, and the container runtimes of today", "tokens": [492, 2138, 1415, 281, 764, 264, 26506, 18, 1185, 818, 11, 293, 264, 10129, 49435, 1532, 295, 965], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 254, "seek": 89000, "start": 907.52, "end": 912.4, "text": " had some assumptions about how we were going to be executing the clone3 system call that,", "tokens": [632, 512, 17695, 466, 577, 321, 645, 516, 281, 312, 32368, 264, 26506, 18, 1185, 818, 300, 11], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 255, "seek": 89000, "start": 912.4, "end": 916.52, "text": " of course, I had to disagree with because, hi, have you met me?", "tokens": [295, 1164, 11, 286, 632, 281, 14091, 365, 570, 11, 4879, 11, 362, 291, 1131, 385, 30], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 256, "seek": 89000, "start": 916.52, "end": 919.28, "text": " I have to disagree with everything.", "tokens": [286, 362, 281, 14091, 365, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1372295346176415, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.1122517207695637e-05}, {"id": 257, "seek": 91928, "start": 919.28, "end": 923.1999999999999, "text": " And we also wanted to implement some ptrace functionality as well.", "tokens": [400, 321, 611, 1415, 281, 4445, 512, 280, 6903, 617, 14980, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 258, "seek": 91928, "start": 923.1999999999999, "end": 929.1999999999999, "text": " So obviously, Go was going to give us some challenges here when it came to using CGo,", "tokens": [407, 2745, 11, 1037, 390, 516, 281, 976, 505, 512, 4759, 510, 562, 309, 1361, 281, 1228, 383, 12104, 11], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 259, "seek": 91928, "start": 929.1999999999999, "end": 935.6, "text": " so Rust became very exciting and definitely got a lot of attention very quickly as we", "tokens": [370, 34952, 3062, 588, 4670, 293, 2138, 658, 257, 688, 295, 3202, 588, 2661, 382, 321], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 260, "seek": 91928, "start": 935.6, "end": 938.9599999999999, "text": " were writing the Go side of things.", "tokens": [645, 3579, 264, 1037, 1252, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 261, "seek": 91928, "start": 938.9599999999999, "end": 941.16, "text": " We also wanted ebpf for networking.", "tokens": [492, 611, 1415, 308, 65, 25302, 337, 17985, 13], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 262, "seek": 91928, "start": 941.16, "end": 946.92, "text": " I personally want it for security and maybe for some other interesting service mesh ideas,", "tokens": [286, 5665, 528, 309, 337, 3825, 293, 1310, 337, 512, 661, 1880, 2643, 17407, 3487, 11], "temperature": 0.0, "avg_logprob": -0.15188571104069346, "compression_ratio": 1.5912698412698412, "no_speech_prob": 9.970959581551142e-06}, {"id": 263, "seek": 94692, "start": 946.92, "end": 951.92, "text": " but I do think that having ebpf for networking as a non-negotiable, we're definitely going", "tokens": [457, 286, 360, 519, 300, 1419, 308, 65, 25302, 337, 17985, 382, 257, 2107, 12, 28561, 8206, 712, 11, 321, 434, 2138, 516], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 264, "seek": 94692, "start": 951.92, "end": 958.52, "text": " to want to simplify what Kubernetes refers to as kubeproxy that we can now invent our", "tokens": [281, 528, 281, 20460, 437, 23145, 14942, 281, 382, 350, 836, 595, 340, 12876, 300, 321, 393, 586, 7962, 527], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 265, "seek": 94692, "start": 958.52, "end": 962.3199999999999, "text": " own name and hopefully simplify that layer, but I digress.", "tokens": [1065, 1315, 293, 4696, 20460, 300, 4583, 11, 457, 286, 2528, 735, 13], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 266, "seek": 94692, "start": 962.3199999999999, "end": 966.88, "text": " We also wanted some access to native virtualization library, so all the KBM stuff is written in", "tokens": [492, 611, 1415, 512, 2105, 281, 8470, 6374, 2144, 6405, 11, 370, 439, 264, 591, 18345, 1507, 307, 3720, 294], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 267, "seek": 94692, "start": 966.88, "end": 971.8, "text": " C. And if you go look at the Firecracker code base, that is also written in Rust that vendors", "tokens": [383, 13, 400, 498, 291, 352, 574, 412, 264, 7652, 10757, 23599, 3089, 3096, 11, 300, 307, 611, 3720, 294, 34952, 300, 22056], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 268, "seek": 94692, "start": 971.8, "end": 973.52, "text": " the KBM bindings.", "tokens": [264, 591, 18345, 14786, 1109, 13], "temperature": 0.0, "avg_logprob": -0.14305797544848017, "compression_ratio": 1.5765124555160142, "no_speech_prob": 8.66331083670957e-06}, {"id": 269, "seek": 97352, "start": 973.52, "end": 977.6, "text": " And so we knew we would want to access these three components, and all three of these are", "tokens": [400, 370, 321, 2586, 321, 576, 528, 281, 2105, 613, 1045, 6677, 11, 293, 439, 1045, 295, 613, 366], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 270, "seek": 97352, "start": 977.6, "end": 981.72, "text": " going to be problematic with Go.", "tokens": [516, 281, 312, 19011, 365, 1037, 13], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 271, "seek": 97352, "start": 981.72, "end": 988.72, "text": " Update as of about an hour ago, I went to the state of the Go room across the hall here.", "tokens": [28923, 382, 295, 466, 364, 1773, 2057, 11, 286, 1437, 281, 264, 1785, 295, 264, 1037, 1808, 2108, 264, 6500, 510, 13], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 272, "seek": 97352, "start": 988.72, "end": 991.4399999999999, "text": " Did anybody else go to the Go talk this morning?", "tokens": [2589, 4472, 1646, 352, 281, 264, 1037, 751, 341, 2446, 30], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 273, "seek": 97352, "start": 991.4399999999999, "end": 995.56, "text": " Yeah, we got three or four hands up here, so this kind of pissed me off.", "tokens": [865, 11, 321, 658, 1045, 420, 1451, 2377, 493, 510, 11, 370, 341, 733, 295, 23795, 385, 766, 13], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 274, "seek": 97352, "start": 995.56, "end": 1003.48, "text": " Go has unwrapped now as of 1.2.0, and they also freaking have.clone.", "tokens": [1037, 575, 14853, 424, 3320, 586, 382, 295, 502, 13, 17, 13, 15, 11, 293, 436, 611, 14612, 362, 2411, 3474, 546, 13], "temperature": 0.0, "avg_logprob": -0.16821816485861074, "compression_ratio": 1.5889328063241106, "no_speech_prob": 4.494371751206927e-06}, {"id": 275, "seek": 100348, "start": 1003.48, "end": 1010.12, "text": " And I was just like, bro, get off our keywords, this is totally like, this is our thing.", "tokens": [400, 286, 390, 445, 411, 11, 2006, 11, 483, 766, 527, 21009, 11, 341, 307, 3879, 411, 11, 341, 307, 527, 551, 13], "temperature": 0.0, "avg_logprob": -0.17989906511808695, "compression_ratio": 1.6075471698113208, "no_speech_prob": 2.3180809876066633e-05}, {"id": 276, "seek": 100348, "start": 1010.12, "end": 1015.9200000000001, "text": " So anyway, it's really exciting to see Go taking these concepts a little more seriously,", "tokens": [407, 4033, 11, 309, 311, 534, 4670, 281, 536, 1037, 1940, 613, 10392, 257, 707, 544, 6638, 11], "temperature": 0.0, "avg_logprob": -0.17989906511808695, "compression_ratio": 1.6075471698113208, "no_speech_prob": 2.3180809876066633e-05}, {"id": 277, "seek": 100348, "start": 1015.9200000000001, "end": 1021.32, "text": " and if you've ever written Rust before, who here has written unwrap in Rust?", "tokens": [293, 498, 291, 600, 1562, 3720, 34952, 949, 11, 567, 510, 575, 3720, 14853, 4007, 294, 34952, 30], "temperature": 0.0, "avg_logprob": -0.17989906511808695, "compression_ratio": 1.6075471698113208, "no_speech_prob": 2.3180809876066633e-05}, {"id": 278, "seek": 100348, "start": 1021.32, "end": 1025.52, "text": " Put your hands down, we're not supposed to do that, I don't know what we're supposed", "tokens": [4935, 428, 2377, 760, 11, 321, 434, 406, 3442, 281, 360, 300, 11, 286, 500, 380, 458, 437, 321, 434, 3442], "temperature": 0.0, "avg_logprob": -0.17989906511808695, "compression_ratio": 1.6075471698113208, "no_speech_prob": 2.3180809876066633e-05}, {"id": 279, "seek": 100348, "start": 1025.52, "end": 1029.92, "text": " to use now, I just get so much shit on my Twitch stream every time I write unwrap, but", "tokens": [281, 764, 586, 11, 286, 445, 483, 370, 709, 4611, 322, 452, 22222, 4309, 633, 565, 286, 2464, 14853, 4007, 11, 457], "temperature": 0.0, "avg_logprob": -0.17989906511808695, "compression_ratio": 1.6075471698113208, "no_speech_prob": 2.3180809876066633e-05}, {"id": 280, "seek": 102992, "start": 1029.92, "end": 1037.5600000000002, "text": " yes, we do have unwrap and clone in Go now, which is just a strong indicator that we're", "tokens": [2086, 11, 321, 360, 362, 14853, 4007, 293, 26506, 294, 1037, 586, 11, 597, 307, 445, 257, 2068, 16961, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 281, "seek": 102992, "start": 1037.5600000000002, "end": 1039.92, "text": " likely doing something right with Rust.", "tokens": [3700, 884, 746, 558, 365, 34952, 13], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 282, "seek": 102992, "start": 1039.92, "end": 1043.88, "text": " So anyway, I made the decision to move to Rust, and I didn't know very much about Rust", "tokens": [407, 4033, 11, 286, 1027, 264, 3537, 281, 1286, 281, 34952, 11, 293, 286, 994, 380, 458, 588, 709, 466, 34952], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 283, "seek": 102992, "start": 1043.88, "end": 1047.64, "text": " when I made the decision, and I literally just started out the main function and said,", "tokens": [562, 286, 1027, 264, 3537, 11, 293, 286, 3736, 445, 1409, 484, 264, 2135, 2445, 293, 848, 11], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 284, "seek": 102992, "start": 1047.64, "end": 1052.68, "text": " we'll figure it out as we go, and I ordered the Rust book and just jumped in and started", "tokens": [321, 603, 2573, 309, 484, 382, 321, 352, 11, 293, 286, 8866, 264, 34952, 1446, 293, 445, 13864, 294, 293, 1409], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 285, "seek": 102992, "start": 1052.68, "end": 1059.68, "text": " to write code with the hope of accessing kernel constructs and C groups and EBPF probes.", "tokens": [281, 2464, 3089, 365, 264, 1454, 295, 26440, 28256, 7690, 82, 293, 383, 3935, 293, 50148, 47, 37, 1239, 279, 13], "temperature": 0.0, "avg_logprob": -0.119329406369117, "compression_ratio": 1.6689895470383276, "no_speech_prob": 4.092467406735523e-06}, {"id": 286, "seek": 105968, "start": 1059.68, "end": 1062.76, "text": " So what could possibly go wrong here?", "tokens": [407, 437, 727, 6264, 352, 2085, 510, 30], "temperature": 0.0, "avg_logprob": -0.16609562465122768, "compression_ratio": 1.5182186234817814, "no_speech_prob": 3.3400949632778065e-06}, {"id": 287, "seek": 105968, "start": 1062.76, "end": 1069.52, "text": " Okay, so how are we doing on time, by the way, we're 15 minutes in, okay, cool.", "tokens": [1033, 11, 370, 577, 366, 321, 884, 322, 565, 11, 538, 264, 636, 11, 321, 434, 2119, 2077, 294, 11, 1392, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.16609562465122768, "compression_ratio": 1.5182186234817814, "no_speech_prob": 3.3400949632778065e-06}, {"id": 288, "seek": 105968, "start": 1069.52, "end": 1076.24, "text": " So Rust to help us solve the YAML problem, I suspect we're all familiar with feeding", "tokens": [407, 34952, 281, 854, 505, 5039, 264, 398, 2865, 43, 1154, 11, 286, 9091, 321, 434, 439, 4963, 365, 12919], "temperature": 0.0, "avg_logprob": -0.16609562465122768, "compression_ratio": 1.5182186234817814, "no_speech_prob": 3.3400949632778065e-06}, {"id": 289, "seek": 105968, "start": 1076.24, "end": 1081.64, "text": " YAML to machines, we've all done this before at some point in our lifetime, okay.", "tokens": [398, 2865, 43, 281, 8379, 11, 321, 600, 439, 1096, 341, 949, 412, 512, 935, 294, 527, 11364, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.16609562465122768, "compression_ratio": 1.5182186234817814, "no_speech_prob": 3.3400949632778065e-06}, {"id": 290, "seek": 105968, "start": 1081.64, "end": 1085.24, "text": " So this is a thing I do a lot working in large distributed systems, and I work with people", "tokens": [407, 341, 307, 257, 551, 286, 360, 257, 688, 1364, 294, 2416, 12631, 3652, 11, 293, 286, 589, 365, 561], "temperature": 0.0, "avg_logprob": -0.16609562465122768, "compression_ratio": 1.5182186234817814, "no_speech_prob": 3.3400949632778065e-06}, {"id": 291, "seek": 108524, "start": 1085.24, "end": 1091.0, "text": " who do this a lot, and if we do it so much, we've tried to get really good at doing it,", "tokens": [567, 360, 341, 257, 688, 11, 293, 498, 321, 360, 309, 370, 709, 11, 321, 600, 3031, 281, 483, 534, 665, 412, 884, 309, 11], "temperature": 0.0, "avg_logprob": -0.1981463623046875, "compression_ratio": 1.6473214285714286, "no_speech_prob": 6.239898084459128e-06}, {"id": 292, "seek": 108524, "start": 1091.0, "end": 1093.92, "text": " and that I think, that's an interesting discussion.", "tokens": [293, 300, 286, 519, 11, 300, 311, 364, 1880, 5017, 13], "temperature": 0.0, "avg_logprob": -0.1981463623046875, "compression_ratio": 1.6473214285714286, "no_speech_prob": 6.239898084459128e-06}, {"id": 293, "seek": 108524, "start": 1093.92, "end": 1101.08, "text": " So in my opinion, so warning, Chris Nova opinions here, in my opinion, all config ultimately", "tokens": [407, 294, 452, 4800, 11, 370, 9164, 11, 6688, 27031, 11819, 510, 11, 294, 452, 4800, 11, 439, 6662, 6284], "temperature": 0.0, "avg_logprob": -0.1981463623046875, "compression_ratio": 1.6473214285714286, "no_speech_prob": 6.239898084459128e-06}, {"id": 294, "seek": 108524, "start": 1101.08, "end": 1104.52, "text": " is going to drift towards Turing completion.", "tokens": [307, 516, 281, 19699, 3030, 314, 1345, 19372, 13], "temperature": 0.0, "avg_logprob": -0.1981463623046875, "compression_ratio": 1.6473214285714286, "no_speech_prob": 6.239898084459128e-06}, {"id": 295, "seek": 108524, "start": 1104.52, "end": 1113.2, "text": " So I see this C++ templates, anybody, anybody C++ templates, okay, Helm charts, customizing", "tokens": [407, 286, 536, 341, 383, 25472, 21165, 11, 4472, 11, 4472, 383, 25472, 21165, 11, 1392, 11, 6128, 76, 17767, 11, 2375, 3319], "temperature": 0.0, "avg_logprob": -0.1981463623046875, "compression_ratio": 1.6473214285714286, "no_speech_prob": 6.239898084459128e-06}, {"id": 296, "seek": 111320, "start": 1113.2, "end": 1119.2, "text": " Kubernetes, any of the templating rendering languages that you see in web dev and front", "tokens": [23145, 11, 604, 295, 264, 9100, 990, 22407, 8650, 300, 291, 536, 294, 3670, 1905, 293, 1868], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 297, "seek": 111320, "start": 1119.2, "end": 1124.32, "text": " end work, there's all kinds of interesting Python libraries that will allow you to interpolate", "tokens": [917, 589, 11, 456, 311, 439, 3685, 295, 1880, 15329, 15148, 300, 486, 2089, 291, 281, 44902, 473], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 298, "seek": 111320, "start": 1124.32, "end": 1126.64, "text": " your config and so on.", "tokens": [428, 6662, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 299, "seek": 111320, "start": 1126.64, "end": 1131.96, "text": " In my opinion, a good balance is kind of something like bash that is Turing complete, but it", "tokens": [682, 452, 4800, 11, 257, 665, 4772, 307, 733, 295, 746, 411, 46183, 300, 307, 314, 1345, 3566, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 300, "seek": 111320, "start": 1131.96, "end": 1134.2, "text": " just comes with some strong guarantees.", "tokens": [445, 1487, 365, 512, 2068, 32567, 13], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 301, "seek": 111320, "start": 1134.2, "end": 1138.1200000000001, "text": " And so I knew very quickly that I didn't want to be feeding YAML to Aura.", "tokens": [400, 370, 286, 2586, 588, 2661, 300, 286, 994, 380, 528, 281, 312, 12919, 398, 2865, 43, 281, 316, 2991, 13], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 302, "seek": 111320, "start": 1138.1200000000001, "end": 1142.68, "text": " I definitely didn't want to recreate this idea of we're going to have to manage a thousand", "tokens": [286, 2138, 994, 380, 528, 281, 25833, 341, 1558, 295, 321, 434, 516, 281, 362, 281, 3067, 257, 4714], "temperature": 0.0, "avg_logprob": -0.13651412963867188, "compression_ratio": 1.6437908496732025, "no_speech_prob": 1.7499429304734804e-05}, {"id": 303, "seek": 114268, "start": 1142.68, "end": 1146.48, "text": " pieces of YAML because we have a thousand different nodes.", "tokens": [3755, 295, 398, 2865, 43, 570, 321, 362, 257, 4714, 819, 13891, 13], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 304, "seek": 114268, "start": 1146.48, "end": 1150.72, "text": " So I wanted to explore more about what are some options that we have here, so we're not", "tokens": [407, 286, 1415, 281, 6839, 544, 466, 437, 366, 512, 3956, 300, 321, 362, 510, 11, 370, 321, 434, 406], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 305, "seek": 114268, "start": 1150.72, "end": 1154.0800000000002, "text": " just feeding YAML to machines anymore.", "tokens": [445, 12919, 398, 2865, 43, 281, 8379, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 306, "seek": 114268, "start": 1154.0800000000002, "end": 1160.04, "text": " So thus became this really interesting project of mine, we'll see if this pans out, which", "tokens": [407, 8807, 3062, 341, 534, 1880, 1716, 295, 3892, 11, 321, 603, 536, 498, 341, 32905, 484, 11, 597], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 307, "seek": 114268, "start": 1160.04, "end": 1162.76, "text": " is this binary called AuraScript.", "tokens": [307, 341, 17434, 1219, 316, 2991, 14237, 13], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 308, "seek": 114268, "start": 1162.76, "end": 1168.72, "text": " So AuraScript is a, it's a Rust binary, we have it compiling with Muzzle today, and embeds", "tokens": [407, 316, 2991, 14237, 307, 257, 11, 309, 311, 257, 34952, 17434, 11, 321, 362, 309, 715, 4883, 365, 376, 16740, 306, 965, 11, 293, 12240, 82], "temperature": 0.0, "avg_logprob": -0.1298527024009011, "compression_ratio": 1.5810276679841897, "no_speech_prob": 3.446075425017625e-06}, {"id": 309, "seek": 116872, "start": 1168.72, "end": 1173.44, "text": " all of the connection logic for a single machine.", "tokens": [439, 295, 264, 4984, 9952, 337, 257, 2167, 3479, 13], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 310, "seek": 116872, "start": 1173.44, "end": 1177.56, "text": " And so we'll talk more about the semantics of AuraScript in a second.", "tokens": [400, 370, 321, 603, 751, 544, 466, 264, 4361, 45298, 295, 316, 2991, 14237, 294, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 311, "seek": 116872, "start": 1177.56, "end": 1182.24, "text": " But ultimately what you need to understand to kind of get the initial motivation here", "tokens": [583, 6284, 437, 291, 643, 281, 1223, 281, 733, 295, 483, 264, 5883, 12335, 510], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 312, "seek": 116872, "start": 1182.24, "end": 1189.64, "text": " is that this aims to be an alternative to managing YAML at scale.", "tokens": [307, 300, 341, 24683, 281, 312, 364, 8535, 281, 11642, 398, 2865, 43, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 313, "seek": 116872, "start": 1189.64, "end": 1195.28, "text": " So I found this really fascinating type script runtime called Dino.", "tokens": [407, 286, 1352, 341, 534, 10343, 2010, 5755, 34474, 1219, 413, 2982, 13], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 314, "seek": 116872, "start": 1195.28, "end": 1197.44, "text": " Have folks heard of Dino before?", "tokens": [3560, 4024, 2198, 295, 413, 2982, 949, 30], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 315, "seek": 116872, "start": 1197.44, "end": 1198.44, "text": " Can I swear in here?", "tokens": [1664, 286, 11902, 294, 510, 30], "temperature": 0.0, "avg_logprob": -0.12349980485205557, "compression_ratio": 1.5291828793774318, "no_speech_prob": 2.0577390387188643e-06}, {"id": 316, "seek": 119844, "start": 1198.44, "end": 1200.96, "text": " I, FN, love Dino.", "tokens": [286, 11, 479, 45, 11, 959, 413, 2982, 13], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 317, "seek": 119844, "start": 1200.96, "end": 1205.4, "text": " I'm sorry, I really like this project.", "tokens": [286, 478, 2597, 11, 286, 534, 411, 341, 1716, 13], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 318, "seek": 119844, "start": 1205.4, "end": 1209.3600000000001, "text": " If you want a good example of like, hey, I just want to see a really successful Rust", "tokens": [759, 291, 528, 257, 665, 1365, 295, 411, 11, 4177, 11, 286, 445, 528, 281, 536, 257, 534, 4406, 34952], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 319, "seek": 119844, "start": 1209.3600000000001, "end": 1214.3600000000001, "text": " project that has a really strong community, I would encourage you to just go look at the", "tokens": [1716, 300, 575, 257, 534, 2068, 1768, 11, 286, 576, 5373, 291, 281, 445, 352, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 320, "seek": 119844, "start": 1214.3600000000001, "end": 1215.3600000000001, "text": " Dino project.", "tokens": [413, 2982, 1716, 13], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 321, "seek": 119844, "start": 1215.3600000000001, "end": 1219.24, "text": " I think their code is beautiful, I think what it does is beautiful, I think the way that", "tokens": [286, 519, 641, 3089, 307, 2238, 11, 286, 519, 437, 309, 775, 307, 2238, 11, 286, 519, 264, 636, 300], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 322, "seek": 119844, "start": 1219.24, "end": 1223.1200000000001, "text": " they manage the project is beautiful, it's just a really good quality project and it", "tokens": [436, 3067, 264, 1716, 307, 2238, 11, 309, 311, 445, 257, 534, 665, 3125, 1716, 293, 309], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 323, "seek": 119844, "start": 1223.1200000000001, "end": 1225.64, "text": " solves a problem for us with Aura.", "tokens": [39890, 257, 1154, 337, 505, 365, 316, 2991, 13], "temperature": 0.0, "avg_logprob": -0.14825213623046876, "compression_ratio": 1.7976190476190477, "no_speech_prob": 4.196868758299388e-05}, {"id": 324, "seek": 122564, "start": 1225.64, "end": 1230.8000000000002, "text": " And so Dino is basically, it's a runtime for type script and it's written in Rust.", "tokens": [400, 370, 413, 2982, 307, 1936, 11, 309, 311, 257, 34474, 337, 2010, 5755, 293, 309, 311, 3720, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.12217201789220174, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.3496294286596822e-06}, {"id": 325, "seek": 122564, "start": 1230.8000000000002, "end": 1235.88, "text": " And the way the project is set up, that you can go and you can add your own custom interpreted", "tokens": [400, 264, 636, 264, 1716, 307, 992, 493, 11, 300, 291, 393, 352, 293, 291, 393, 909, 428, 1065, 2375, 26749], "temperature": 0.0, "avg_logprob": -0.12217201789220174, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.3496294286596822e-06}, {"id": 326, "seek": 122564, "start": 1235.88, "end": 1240.0, "text": " logic and you can build fancy things into the binary and you can do things with the", "tokens": [9952, 293, 291, 393, 1322, 10247, 721, 666, 264, 17434, 293, 291, 393, 360, 721, 365, 264], "temperature": 0.0, "avg_logprob": -0.12217201789220174, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.3496294286596822e-06}, {"id": 327, "seek": 122564, "start": 1240.0, "end": 1247.8400000000001, "text": " type script interpretation at runtime, which is precisely what we needed to do with Aura.", "tokens": [2010, 5755, 14174, 412, 34474, 11, 597, 307, 13402, 437, 321, 2978, 281, 360, 365, 316, 2991, 13], "temperature": 0.0, "avg_logprob": -0.12217201789220174, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.3496294286596822e-06}, {"id": 328, "seek": 122564, "start": 1247.8400000000001, "end": 1249.8000000000002, "text": " So here is the model now.", "tokens": [407, 510, 307, 264, 2316, 586, 13], "temperature": 0.0, "avg_logprob": -0.12217201789220174, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.3496294286596822e-06}, {"id": 329, "seek": 124980, "start": 1249.8, "end": 1255.96, "text": " So instead of feeding YAML to a single node, we now have this higher order set of libraries", "tokens": [407, 2602, 295, 12919, 398, 2865, 43, 281, 257, 2167, 9984, 11, 321, 586, 362, 341, 2946, 1668, 992, 295, 15148], "temperature": 0.0, "avg_logprob": -0.11192203868519177, "compression_ratio": 1.6197718631178708, "no_speech_prob": 5.254822553979466e-06}, {"id": 330, "seek": 124980, "start": 1255.96, "end": 1262.32, "text": " that we can statically compile into a binary and we can interpret it directly on a machine.", "tokens": [300, 321, 393, 2219, 984, 31413, 666, 257, 17434, 293, 321, 393, 7302, 309, 3838, 322, 257, 3479, 13], "temperature": 0.0, "avg_logprob": -0.11192203868519177, "compression_ratio": 1.6197718631178708, "no_speech_prob": 5.254822553979466e-06}, {"id": 331, "seek": 124980, "start": 1262.32, "end": 1268.08, "text": " So in order for you to interface with an Aura node or a set of nodes, all you need is one", "tokens": [407, 294, 1668, 337, 291, 281, 9226, 365, 364, 316, 2991, 9984, 420, 257, 992, 295, 13891, 11, 439, 291, 643, 307, 472], "temperature": 0.0, "avg_logprob": -0.11192203868519177, "compression_ratio": 1.6197718631178708, "no_speech_prob": 5.254822553979466e-06}, {"id": 332, "seek": 124980, "start": 1268.08, "end": 1273.24, "text": " binary, mtls config and then whatever type script you want to write.", "tokens": [17434, 11, 275, 83, 11784, 6662, 293, 550, 2035, 2010, 5755, 291, 528, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.11192203868519177, "compression_ratio": 1.6197718631178708, "no_speech_prob": 5.254822553979466e-06}, {"id": 333, "seek": 124980, "start": 1273.24, "end": 1277.6399999999999, "text": " And this is an alternative to like any of the Nomad command line tools or the Mesos", "tokens": [400, 341, 307, 364, 8535, 281, 411, 604, 295, 264, 31272, 345, 5622, 1622, 3873, 420, 264, 17485, 329], "temperature": 0.0, "avg_logprob": -0.11192203868519177, "compression_ratio": 1.6197718631178708, "no_speech_prob": 5.254822553979466e-06}, {"id": 334, "seek": 127764, "start": 1277.64, "end": 1282.16, "text": " command line tools or the Kubernetes, kubectl, kubectl command line tool.", "tokens": [5622, 1622, 3873, 420, 264, 23145, 11, 350, 836, 557, 75, 11, 350, 836, 557, 75, 5622, 1622, 2290, 13], "temperature": 0.0, "avg_logprob": -0.16287018625359787, "compression_ratio": 1.6863636363636363, "no_speech_prob": 8.397303645324428e-06}, {"id": 335, "seek": 127764, "start": 1282.16, "end": 1286.44, "text": " And now you can just write it all directly in type script.", "tokens": [400, 586, 291, 393, 445, 2464, 309, 439, 3838, 294, 2010, 5755, 13], "temperature": 0.0, "avg_logprob": -0.16287018625359787, "compression_ratio": 1.6863636363636363, "no_speech_prob": 8.397303645324428e-06}, {"id": 336, "seek": 127764, "start": 1286.44, "end": 1293.24, "text": " So this is actually a concrete example of what would be, what system D would call a", "tokens": [407, 341, 307, 767, 257, 9859, 1365, 295, 437, 576, 312, 11, 437, 1185, 413, 576, 818, 257], "temperature": 0.0, "avg_logprob": -0.16287018625359787, "compression_ratio": 1.6863636363636363, "no_speech_prob": 8.397303645324428e-06}, {"id": 337, "seek": 127764, "start": 1293.24, "end": 1299.88, "text": " unit file, what Kubernetes would call a manifest and what Aura just calls a freaking type script", "tokens": [4985, 3991, 11, 437, 23145, 576, 818, 257, 10067, 293, 437, 316, 2991, 445, 5498, 257, 14612, 2010, 5755], "temperature": 0.0, "avg_logprob": -0.16287018625359787, "compression_ratio": 1.6863636363636363, "no_speech_prob": 8.397303645324428e-06}, {"id": 338, "seek": 127764, "start": 1299.88, "end": 1304.0400000000002, "text": " file because we don't have fancy names for our stuff yet.", "tokens": [3991, 570, 321, 500, 380, 362, 10247, 5288, 337, 527, 1507, 1939, 13], "temperature": 0.0, "avg_logprob": -0.16287018625359787, "compression_ratio": 1.6863636363636363, "no_speech_prob": 8.397303645324428e-06}, {"id": 339, "seek": 130404, "start": 1304.04, "end": 1309.32, "text": " So you can see here at the top, we basically contact the Aura standard library.", "tokens": [407, 291, 393, 536, 510, 412, 264, 1192, 11, 321, 1936, 3385, 264, 316, 2991, 3832, 6405, 13], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 340, "seek": 130404, "start": 1309.32, "end": 1314.8799999999999, "text": " We get a new client and then we can allocate this thing called a cell.", "tokens": [492, 483, 257, 777, 6423, 293, 550, 321, 393, 35713, 341, 551, 1219, 257, 2815, 13], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 341, "seek": 130404, "start": 1314.8799999999999, "end": 1318.18, "text": " A cell is basically an abstraction for a C group.", "tokens": [316, 2815, 307, 1936, 364, 37765, 337, 257, 383, 1594, 13], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 342, "seek": 130404, "start": 1318.18, "end": 1322.28, "text": " We cordon off a section of the system and we say like we want to use a certain percentage", "tokens": [492, 12250, 266, 766, 257, 3541, 295, 264, 1185, 293, 321, 584, 411, 321, 528, 281, 764, 257, 1629, 9668], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 343, "seek": 130404, "start": 1322.28, "end": 1327.8799999999999, "text": " of the available CPUs on a node and I want it to only let processes run.", "tokens": [295, 264, 2435, 13199, 82, 322, 257, 9984, 293, 286, 528, 309, 281, 787, 718, 7555, 1190, 13], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 344, "seek": 130404, "start": 1327.8799999999999, "end": 1331.76, "text": " In this case for 0.4 seconds and then we'll use the kernel to just kill the process if", "tokens": [682, 341, 1389, 337, 1958, 13, 19, 3949, 293, 550, 321, 603, 764, 264, 28256, 281, 445, 1961, 264, 1399, 498], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 345, "seek": 130404, "start": 1331.76, "end": 1333.92, "text": " it runs longer than that.", "tokens": [309, 6676, 2854, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.12467665142483181, "compression_ratio": 1.6701754385964913, "no_speech_prob": 2.224976242359844e-06}, {"id": 346, "seek": 133392, "start": 1333.92, "end": 1338.2, "text": " And so the first thing we would do is we would allocate that which is an improvement over", "tokens": [400, 370, 264, 700, 551, 321, 576, 360, 307, 321, 576, 35713, 300, 597, 307, 364, 10444, 670], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 347, "seek": 133392, "start": 1338.2, "end": 1342.92, "text": " Kubernetes as it exists today because we can allocate resources before we actually start", "tokens": [23145, 382, 309, 8198, 965, 570, 321, 393, 35713, 3593, 949, 321, 767, 722], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 348, "seek": 133392, "start": 1342.92, "end": 1348.48, "text": " anything in that area and then we can go ahead and actually start whatever we want.", "tokens": [1340, 294, 300, 1859, 293, 550, 321, 393, 352, 2286, 293, 767, 722, 2035, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 349, "seek": 133392, "start": 1348.48, "end": 1353.24, "text": " And so you can see I simplified the example just for today but it's just, it's remote", "tokens": [400, 370, 291, 393, 536, 286, 26335, 264, 1365, 445, 337, 965, 457, 309, 311, 445, 11, 309, 311, 8607], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 350, "seek": 133392, "start": 1353.24, "end": 1354.88, "text": " command injection as a service.", "tokens": [5622, 22873, 382, 257, 2643, 13], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 351, "seek": 133392, "start": 1354.88, "end": 1362.3200000000002, "text": " So this whole talk was just basically like how to go and run a bash command in on a server.", "tokens": [407, 341, 1379, 751, 390, 445, 1936, 411, 577, 281, 352, 293, 1190, 257, 46183, 5622, 294, 322, 257, 7154, 13], "temperature": 0.0, "avg_logprob": -0.10393112356012518, "compression_ratio": 1.754646840148699, "no_speech_prob": 2.6419886580697494e-06}, {"id": 352, "seek": 136232, "start": 1362.32, "end": 1366.8, "text": " And so now you can express your commands and similar primitives that you would see in other", "tokens": [400, 370, 586, 291, 393, 5109, 428, 16901, 293, 2531, 2886, 38970, 300, 291, 576, 536, 294, 661], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 353, "seek": 136232, "start": 1366.8, "end": 1371.04, "text": " run times directly in TypeScript.", "tokens": [1190, 1413, 3838, 294, 15576, 14237, 13], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 354, "seek": 136232, "start": 1371.04, "end": 1377.8, "text": " The interesting thing here is TypeScript is just natively more expressive than a lot of", "tokens": [440, 1880, 551, 510, 307, 15576, 14237, 307, 445, 8470, 356, 544, 40189, 813, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 355, "seek": 136232, "start": 1377.8, "end": 1379.8799999999999, "text": " the Amble things that we see today.", "tokens": [264, 2012, 638, 721, 300, 321, 536, 965, 13], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 356, "seek": 136232, "start": 1379.8799999999999, "end": 1384.36, "text": " In this case we can actually do math but I'm sure you can imagine you can do other things", "tokens": [682, 341, 1389, 321, 393, 767, 360, 5221, 457, 286, 478, 988, 291, 393, 3811, 291, 393, 360, 661, 721], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 357, "seek": 136232, "start": 1384.36, "end": 1385.36, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 358, "seek": 136232, "start": 1385.36, "end": 1390.3999999999999, "text": " You can access logic, loops, if statements, there's if branching and so on.", "tokens": [509, 393, 2105, 9952, 11, 16121, 11, 498, 12363, 11, 456, 311, 498, 9819, 278, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.137178036051059, "compression_ratio": 1.6892430278884463, "no_speech_prob": 2.2956296561460476e-06}, {"id": 359, "seek": 139040, "start": 1390.4, "end": 1394.96, "text": " And so we were able to actually solve some of these like templatey rendering style problems", "tokens": [400, 370, 321, 645, 1075, 281, 767, 5039, 512, 295, 613, 411, 12379, 88, 22407, 3758, 2740], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 360, "seek": 139040, "start": 1394.96, "end": 1402.16, "text": " by just doing things natively in a well known and easy to understand language such as TypeScript.", "tokens": [538, 445, 884, 721, 8470, 356, 294, 257, 731, 2570, 293, 1858, 281, 1223, 2856, 1270, 382, 15576, 14237, 13], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 361, "seek": 139040, "start": 1402.16, "end": 1404.3200000000002, "text": " So patterns started to emerge.", "tokens": [407, 8294, 1409, 281, 21511, 13], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 362, "seek": 139040, "start": 1404.3200000000002, "end": 1410.6000000000001, "text": " So Rust gave us the ability to generate the TypeScript binary with all of the magic behind", "tokens": [407, 34952, 2729, 505, 264, 3485, 281, 8460, 264, 15576, 14237, 17434, 365, 439, 295, 264, 5585, 2261], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 363, "seek": 139040, "start": 1410.6000000000001, "end": 1413.8000000000002, "text": " the scenes MTLS security config that we wanted.", "tokens": [264, 8026, 37333, 19198, 3825, 6662, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 364, "seek": 139040, "start": 1413.8000000000002, "end": 1418.4, "text": " And so now the conversation was a little more like this which is how do I manage a small", "tokens": [400, 370, 586, 264, 3761, 390, 257, 707, 544, 411, 341, 597, 307, 577, 360, 286, 3067, 257, 1359], "temperature": 0.0, "avg_logprob": -0.12589450065906233, "compression_ratio": 1.6231884057971016, "no_speech_prob": 5.337408310879255e-06}, {"id": 365, "seek": 141840, "start": 1418.4, "end": 1424.16, "text": " set of TypeScript and it's much more flexible and you can start to actually express things", "tokens": [992, 295, 15576, 14237, 293, 309, 311, 709, 544, 11358, 293, 291, 393, 722, 281, 767, 5109, 721], "temperature": 0.0, "avg_logprob": -0.14231303719913257, "compression_ratio": 1.635135135135135, "no_speech_prob": 7.765874215692747e-06}, {"id": 366, "seek": 141840, "start": 1424.16, "end": 1428.24, "text": " the way that we used to and just express things statically and then you can have all of your", "tokens": [264, 636, 300, 321, 1143, 281, 293, 445, 5109, 721, 2219, 984, 293, 550, 291, 393, 362, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.14231303719913257, "compression_ratio": 1.635135135135135, "no_speech_prob": 7.765874215692747e-06}, {"id": 367, "seek": 141840, "start": 1428.24, "end": 1436.92, "text": " Turing complete logical components below and you can mix and match these however you want.", "tokens": [314, 1345, 3566, 14978, 6677, 2507, 293, 291, 393, 2890, 293, 2995, 613, 4461, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.14231303719913257, "compression_ratio": 1.635135135135135, "no_speech_prob": 7.765874215692747e-06}, {"id": 368, "seek": 141840, "start": 1436.92, "end": 1446.24, "text": " So in addition to addressing the YAML problem with Dino and TypeScript, Rust also helped", "tokens": [407, 294, 4500, 281, 14329, 264, 398, 2865, 43, 1154, 365, 413, 2982, 293, 15576, 14237, 11, 34952, 611, 4254], "temperature": 0.0, "avg_logprob": -0.14231303719913257, "compression_ratio": 1.635135135135135, "no_speech_prob": 7.765874215692747e-06}, {"id": 369, "seek": 144624, "start": 1446.24, "end": 1452.32, "text": " us to solve the sidecar problem and by us, I mean this is our hope as we operate our", "tokens": [505, 281, 5039, 264, 1252, 6166, 1154, 293, 538, 505, 11, 286, 914, 341, 307, 527, 1454, 382, 321, 9651, 527], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 370, "seek": 144624, "start": 1452.32, "end": 1458.1200000000001, "text": " mastodon servers and our various other ridiculous side projects that we operate both in my basement", "tokens": [27055, 378, 266, 15909, 293, 527, 3683, 661, 11083, 1252, 4455, 300, 321, 9651, 1293, 294, 452, 16893], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 371, "seek": 144624, "start": 1458.1200000000001, "end": 1460.6, "text": " and in a colo in Germany.", "tokens": [293, 294, 257, 1173, 78, 294, 7244, 13], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 372, "seek": 144624, "start": 1460.6, "end": 1466.1200000000001, "text": " So talking about sidecars, who here knows what a sidecar is, show of hands, okay most", "tokens": [407, 1417, 466, 1252, 66, 685, 11, 567, 510, 3255, 437, 257, 1252, 6166, 307, 11, 855, 295, 2377, 11, 1392, 881], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 373, "seek": 144624, "start": 1466.1200000000001, "end": 1467.1200000000001, "text": " folks do.", "tokens": [4024, 360, 13], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 374, "seek": 144624, "start": 1467.1200000000001, "end": 1472.2, "text": " Okay, so a sidecar that is always available with the same features as a host.", "tokens": [1033, 11, 370, 257, 1252, 6166, 300, 307, 1009, 2435, 365, 264, 912, 4122, 382, 257, 3975, 13], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 375, "seek": 144624, "start": 1472.2, "end": 1475.84, "text": " So this is going to sound a little bit weird and the slide is going to look a little bit", "tokens": [407, 341, 307, 516, 281, 1626, 257, 707, 857, 3657, 293, 264, 4137, 307, 516, 281, 574, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17807180919344462, "compression_ratio": 1.758364312267658, "no_speech_prob": 1.1657277354970574e-05}, {"id": 376, "seek": 147584, "start": 1475.84, "end": 1481.24, "text": " weird but just bear with me as we kind of like unpack what's actually going on here.", "tokens": [3657, 457, 445, 6155, 365, 385, 382, 321, 733, 295, 411, 26699, 437, 311, 767, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 377, "seek": 147584, "start": 1481.24, "end": 1485.84, "text": " What we want that I don't think we're talking about is that sentence.", "tokens": [708, 321, 528, 300, 286, 500, 380, 519, 321, 434, 1417, 466, 307, 300, 8174, 13], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 378, "seek": 147584, "start": 1485.84, "end": 1491.1599999999999, "text": " I actually think what we want is we want a sidecar to sit along our applications that", "tokens": [286, 767, 519, 437, 321, 528, 307, 321, 528, 257, 1252, 6166, 281, 1394, 2051, 527, 5821, 300], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 379, "seek": 147584, "start": 1491.1599999999999, "end": 1497.28, "text": " does literally the exact same things we have to do on a given host whenever we're managing", "tokens": [775, 3736, 264, 1900, 912, 721, 321, 362, 281, 360, 322, 257, 2212, 3975, 5699, 321, 434, 11642], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 380, "seek": 147584, "start": 1497.28, "end": 1500.36, "text": " these workloads at scale.", "tokens": [613, 32452, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 381, "seek": 147584, "start": 1500.36, "end": 1505.52, "text": " As I began looking into writing sidecars at the host level, I began drilling deeper and", "tokens": [1018, 286, 4283, 1237, 666, 3579, 1252, 66, 685, 412, 264, 3975, 1496, 11, 286, 4283, 26290, 7731, 293], "temperature": 0.0, "avg_logprob": -0.10452127894130323, "compression_ratio": 1.7315175097276265, "no_speech_prob": 1.3209555618232116e-05}, {"id": 382, "seek": 150552, "start": 1505.52, "end": 1509.92, "text": " deeper into the C programming language as I was writing this in Rust and just made the", "tokens": [7731, 666, 264, 383, 9410, 2856, 382, 286, 390, 3579, 341, 294, 34952, 293, 445, 1027, 264], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 383, "seek": 150552, "start": 1509.92, "end": 1514.52, "text": " connection that memory safety was going to be key because we're going to be running these", "tokens": [4984, 300, 4675, 4514, 390, 516, 281, 312, 2141, 570, 321, 434, 516, 281, 312, 2614, 613], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 384, "seek": 150552, "start": 1514.52, "end": 1518.76, "text": " demons right alongside of your workload.", "tokens": [19733, 558, 12385, 295, 428, 20139, 13], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 385, "seek": 150552, "start": 1518.76, "end": 1525.12, "text": " And so unpacking the need to do this really helps you understand why we shifted over to", "tokens": [400, 370, 26699, 278, 264, 643, 281, 360, 341, 534, 3665, 291, 1223, 983, 321, 18892, 670, 281], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 386, "seek": 150552, "start": 1525.12, "end": 1526.32, "text": " Rust.", "tokens": [34952, 13], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 387, "seek": 150552, "start": 1526.32, "end": 1533.04, "text": " So again, another Chris Nova opinion, any sufficiently mature infrastructure service", "tokens": [407, 797, 11, 1071, 6688, 27031, 4800, 11, 604, 31868, 14442, 6896, 2643], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 388, "seek": 150552, "start": 1533.04, "end": 1535.24, "text": " will evolve into a sidecar.", "tokens": [486, 16693, 666, 257, 1252, 6166, 13], "temperature": 0.0, "avg_logprob": -0.12355157517895256, "compression_ratio": 1.5880149812734083, "no_speech_prob": 8.266245458798949e-06}, {"id": 389, "seek": 153524, "start": 1535.24, "end": 1539.68, "text": " So if you have done any sort of structured logging, in my opinion, if you will continue", "tokens": [407, 498, 291, 362, 1096, 604, 1333, 295, 18519, 27991, 11, 294, 452, 4800, 11, 498, 291, 486, 2354], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 390, "seek": 153524, "start": 1539.68, "end": 1544.68, "text": " to build structured logging and you'll continue to ship logs, that will eventually turn into", "tokens": [281, 1322, 18519, 27991, 293, 291, 603, 2354, 281, 5374, 20820, 11, 300, 486, 4728, 1261, 666], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 391, "seek": 153524, "start": 1544.68, "end": 1548.16, "text": " a sidecar that you're going to want to go run beside your app so you have this transparent", "tokens": [257, 1252, 6166, 300, 291, 434, 516, 281, 528, 281, 352, 1190, 15726, 428, 724, 370, 291, 362, 341, 12737], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 392, "seek": 153524, "start": 1548.16, "end": 1549.6, "text": " logging experience.", "tokens": [27991, 1752, 13], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 393, "seek": 153524, "start": 1549.6, "end": 1553.72, "text": " You can rinse and repeat that paradigm for pretty much anything, secrets, authentication", "tokens": [509, 393, 27026, 293, 7149, 300, 24709, 337, 1238, 709, 1340, 11, 14093, 11, 26643], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 394, "seek": 153524, "start": 1553.72, "end": 1555.48, "text": " data, and so on.", "tokens": [1412, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 395, "seek": 153524, "start": 1555.48, "end": 1559.64, "text": " And so I started to see these patterns kind of surface.", "tokens": [400, 370, 286, 1409, 281, 536, 613, 8294, 733, 295, 3753, 13], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 396, "seek": 153524, "start": 1559.64, "end": 1564.72, "text": " And very specifically, I started to look at how would I solve these with Rust?", "tokens": [400, 588, 4682, 11, 286, 1409, 281, 574, 412, 577, 576, 286, 5039, 613, 365, 34952, 30], "temperature": 0.0, "avg_logprob": -0.098048825902263, "compression_ratio": 1.738562091503268, "no_speech_prob": 3.6117367017141078e-06}, {"id": 397, "seek": 156472, "start": 1564.72, "end": 1570.32, "text": " And as it turns out, the Rust ecosystem had a plethora of pleasant surprises for me as", "tokens": [400, 382, 309, 4523, 484, 11, 264, 34952, 11311, 632, 257, 499, 302, 7013, 295, 16232, 22655, 337, 385, 382], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 398, "seek": 156472, "start": 1570.32, "end": 1576.92, "text": " I started to explore what putting some of these features into a binary would look like.", "tokens": [286, 1409, 281, 6839, 437, 3372, 512, 295, 613, 4122, 666, 257, 17434, 576, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 399, "seek": 156472, "start": 1576.92, "end": 1581.92, "text": " Logging was boring because we could just use Tokyo Streams, Auth N and Auth Z was boring", "tokens": [10824, 3249, 390, 9989, 570, 321, 727, 445, 764, 15147, 24904, 82, 11, 40231, 426, 293, 40231, 1176, 390, 9989], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 400, "seek": 156472, "start": 1581.92, "end": 1587.8, "text": " because all I had to do was just use the Rust-derived primitives to just start applying Auth Z to", "tokens": [570, 439, 286, 632, 281, 360, 390, 445, 764, 264, 34952, 12, 1068, 3194, 2886, 38970, 281, 445, 722, 9275, 40231, 1176, 281], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 401, "seek": 156472, "start": 1587.8, "end": 1590.48, "text": " each of our units in the source code.", "tokens": [1184, 295, 527, 6815, 294, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 402, "seek": 156472, "start": 1590.48, "end": 1593.56, "text": " Identity was boring because I didn't even get to fight with open SSL anymore.", "tokens": [25905, 507, 390, 9989, 570, 286, 994, 380, 754, 483, 281, 2092, 365, 1269, 12238, 43, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1300990916480703, "compression_ratio": 1.685512367491166, "no_speech_prob": 4.331221498432569e-05}, {"id": 403, "seek": 159356, "start": 1593.56, "end": 1596.6799999999998, "text": " We just had to use Rust TLS and that was easy.", "tokens": [492, 445, 632, 281, 764, 34952, 314, 19198, 293, 300, 390, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 404, "seek": 159356, "start": 1596.6799999999998, "end": 1601.12, "text": " And so the network was also easy because we had native access to Linux and Lib C so we", "tokens": [400, 370, 264, 3209, 390, 611, 1858, 570, 321, 632, 8470, 2105, 281, 18734, 293, 15834, 383, 370, 321], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 405, "seek": 159356, "start": 1601.12, "end": 1606.76, "text": " could just very boringly schedule a Linux device and we got a Linux device and it was", "tokens": [727, 445, 588, 9989, 356, 7567, 257, 18734, 4302, 293, 321, 658, 257, 18734, 4302, 293, 309, 390], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 406, "seek": 159356, "start": 1606.76, "end": 1609.04, "text": " pretty straightforward.", "tokens": [1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 407, "seek": 159356, "start": 1609.04, "end": 1616.44, "text": " So we were able to create this at the node and now my question was how do we bring this", "tokens": [407, 321, 645, 1075, 281, 1884, 341, 412, 264, 9984, 293, 586, 452, 1168, 390, 577, 360, 321, 1565, 341], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 408, "seek": 159356, "start": 1616.44, "end": 1619.3999999999999, "text": " into the workload level at scale?", "tokens": [666, 264, 20139, 1496, 412, 4373, 30], "temperature": 0.0, "avg_logprob": -0.15708211127747881, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.3374492381408345e-06}, {"id": 409, "seek": 161940, "start": 1619.4, "end": 1624.96, "text": " And I think this is where most of the conversations you start talking about things like Istio and", "tokens": [400, 286, 519, 341, 307, 689, 881, 295, 264, 7315, 291, 722, 1417, 466, 721, 411, 12810, 1004, 293], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 410, "seek": 161940, "start": 1624.96, "end": 1628.8000000000002, "text": " service meshes and structured logging and so forth.", "tokens": [2643, 3813, 8076, 293, 18519, 27991, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 411, "seek": 161940, "start": 1628.8000000000002, "end": 1633.2800000000002, "text": " And I actually think that we can simplify that conversation too.", "tokens": [400, 286, 767, 519, 300, 321, 393, 20460, 300, 3761, 886, 13], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 412, "seek": 161940, "start": 1633.2800000000002, "end": 1639.0, "text": " And so what we were able to do with Aura is we just spawned the root daemon and use that", "tokens": [400, 370, 437, 321, 645, 1075, 281, 360, 365, 316, 2991, 307, 321, 445, 17088, 292, 264, 5593, 1120, 36228, 293, 764, 300], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 413, "seek": 161940, "start": 1639.0, "end": 1643.0400000000002, "text": " as the new PID one in any of our nested isolation zones.", "tokens": [382, 264, 777, 430, 2777, 472, 294, 604, 295, 527, 15646, 292, 16001, 16025, 13], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 414, "seek": 161940, "start": 1643.0400000000002, "end": 1647.52, "text": " And when I say spawn, I very directly mean like we literally read the byte code from", "tokens": [400, 562, 286, 584, 17088, 11, 286, 588, 3838, 914, 411, 321, 3736, 1401, 264, 40846, 3089, 490], "temperature": 0.0, "avg_logprob": -0.12593136177406655, "compression_ratio": 1.6856060606060606, "no_speech_prob": 9.078992661670782e-06}, {"id": 415, "seek": 164752, "start": 1647.52, "end": 1652.8, "text": " the kernel and we build an image at runtime with the byte for byte, the same byte code", "tokens": [264, 28256, 293, 321, 1322, 364, 3256, 412, 34474, 365, 264, 40846, 337, 40846, 11, 264, 912, 40846, 3089], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 416, "seek": 164752, "start": 1652.8, "end": 1657.96, "text": " that's running on the host and then we can just go and execute whatever we want against", "tokens": [300, 311, 2614, 322, 264, 3975, 293, 550, 321, 393, 445, 352, 293, 14483, 2035, 321, 528, 1970], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 417, "seek": 164752, "start": 1657.96, "end": 1663.08, "text": " the same API as the original host runs and all of this is memory safe.", "tokens": [264, 912, 9362, 382, 264, 3380, 3975, 6676, 293, 439, 295, 341, 307, 4675, 3273, 13], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 418, "seek": 164752, "start": 1663.08, "end": 1668.32, "text": " So I can put this right next to your application in the same namespaces running in a container", "tokens": [407, 286, 393, 829, 341, 558, 958, 281, 428, 3861, 294, 264, 912, 5288, 79, 2116, 2614, 294, 257, 10129], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 419, "seek": 164752, "start": 1668.32, "end": 1673.24, "text": " or running in a virtual machine and there's a relatively low risk of any sort of binary", "tokens": [420, 2614, 294, 257, 6374, 3479, 293, 456, 311, 257, 7226, 2295, 3148, 295, 604, 1333, 295, 17434], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 420, "seek": 164752, "start": 1673.24, "end": 1677.28, "text": " exploitation at scale.", "tokens": [33122, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.12129065312376809, "compression_ratio": 1.7083333333333333, "no_speech_prob": 4.222375991957961e-06}, {"id": 421, "seek": 167728, "start": 1677.28, "end": 1679.72, "text": " So here's a model of what that looks like.", "tokens": [407, 510, 311, 257, 2316, 295, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 422, "seek": 167728, "start": 1679.72, "end": 1685.8799999999999, "text": " So on the left big side here we have the Aura host daemon and on the right we have the three", "tokens": [407, 322, 264, 1411, 955, 1252, 510, 321, 362, 264, 316, 2991, 3975, 1120, 36228, 293, 322, 264, 558, 321, 362, 264, 1045], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 423, "seek": 167728, "start": 1685.8799999999999, "end": 1688.72, "text": " types of isolation zones that you can run with the daemon.", "tokens": [3467, 295, 16001, 16025, 300, 291, 393, 1190, 365, 264, 1120, 36228, 13], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 424, "seek": 167728, "start": 1688.72, "end": 1694.16, "text": " You have a cell sandbox which is effectively a C group, a pod sandbox which is a group of", "tokens": [509, 362, 257, 2815, 42115, 597, 307, 8659, 257, 383, 1594, 11, 257, 2497, 42115, 597, 307, 257, 1594, 295], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 425, "seek": 167728, "start": 1694.16, "end": 1700.56, "text": " containers running in unique Linux namespaces and a virtual machine which is effectively", "tokens": [17089, 2614, 294, 3845, 18734, 5288, 79, 2116, 293, 257, 6374, 3479, 597, 307, 8659], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 426, "seek": 167728, "start": 1700.56, "end": 1704.56, "text": " a container with a kernel and some virtualization technology.", "tokens": [257, 10129, 365, 257, 28256, 293, 512, 6374, 2144, 2899, 13], "temperature": 0.0, "avg_logprob": -0.1041373315258561, "compression_ratio": 1.7540322580645162, "no_speech_prob": 2.812697175613721e-06}, {"id": 427, "seek": 170456, "start": 1704.56, "end": 1710.44, "text": " All of this is possible with Rust natively and all of this was made possible by spawning", "tokens": [1057, 295, 341, 307, 1944, 365, 34952, 8470, 356, 293, 439, 295, 341, 390, 1027, 1944, 538, 637, 35880], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 428, "seek": 170456, "start": 1710.44, "end": 1717.1599999999999, "text": " the binary and creating these nested isolation zones at runtime.", "tokens": [264, 17434, 293, 4084, 613, 15646, 292, 16001, 16025, 412, 34474, 13], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 429, "seek": 170456, "start": 1717.1599999999999, "end": 1720.6799999999998, "text": " Additionally Rust was able to help solve the untrusted workload problem because of the", "tokens": [19927, 34952, 390, 1075, 281, 854, 5039, 264, 1701, 81, 6589, 20139, 1154, 570, 295, 264], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 430, "seek": 170456, "start": 1720.6799999999998, "end": 1727.56, "text": " memory safety and that Rust offers and because of this really interesting model that we have", "tokens": [4675, 4514, 293, 300, 34952, 7736, 293, 570, 295, 341, 534, 1880, 2316, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 431, "seek": 170456, "start": 1727.56, "end": 1729.2, "text": " right here.", "tokens": [558, 510, 13], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 432, "seek": 170456, "start": 1729.2, "end": 1733.48, "text": " So this is a zoomed in model that might look familiar if you've ever done any container", "tokens": [407, 341, 307, 257, 8863, 292, 294, 2316, 300, 1062, 574, 4963, 498, 291, 600, 1562, 1096, 604, 10129], "temperature": 0.0, "avg_logprob": -0.10039151316941386, "compression_ratio": 1.7389558232931728, "no_speech_prob": 5.681958100467455e-06}, {"id": 433, "seek": 173348, "start": 1733.48, "end": 1739.48, "text": " escapes before and in this model basically what we're saying is we're replacing any sort", "tokens": [43769, 949, 293, 294, 341, 2316, 1936, 437, 321, 434, 1566, 307, 321, 434, 19139, 604, 1333], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 434, "seek": 173348, "start": 1739.48, "end": 1744.84, "text": " of like pause or initialization sequence in an isolation zone with the same daemon we", "tokens": [295, 411, 10465, 420, 5883, 2144, 8310, 294, 364, 16001, 6668, 365, 264, 912, 1120, 36228, 321], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 435, "seek": 173348, "start": 1744.84, "end": 1746.72, "text": " run on the host.", "tokens": [1190, 322, 264, 3975, 13], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 436, "seek": 173348, "start": 1746.72, "end": 1750.96, "text": " So I think the Rust binary for Aura right now is about 40 megabytes and we can just", "tokens": [407, 286, 519, 264, 34952, 17434, 337, 316, 2991, 558, 586, 307, 466, 3356, 10816, 24538, 293, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 437, "seek": 173348, "start": 1750.96, "end": 1754.44, "text": " copy that into a container and run that alongside your application.", "tokens": [5055, 300, 666, 257, 10129, 293, 1190, 300, 12385, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 438, "seek": 173348, "start": 1754.44, "end": 1762.68, "text": " So it's a relatively small application, runtime that will sit right alongside of your app", "tokens": [407, 309, 311, 257, 7226, 1359, 3861, 11, 34474, 300, 486, 1394, 558, 12385, 295, 428, 724], "temperature": 0.0, "avg_logprob": -0.12247653101004806, "compression_ratio": 1.6463878326996197, "no_speech_prob": 1.1841724699479528e-05}, {"id": 439, "seek": 176268, "start": 1762.68, "end": 1767.8400000000001, "text": " so managing memory from MTLS and RID.", "tokens": [370, 11642, 4675, 490, 37333, 19198, 293, 497, 2777, 13], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 440, "seek": 176268, "start": 1767.8400000000001, "end": 1773.04, "text": " So as I'm writing Rust one of the things I notice is I start paying attention to memory", "tokens": [407, 382, 286, 478, 3579, 34952, 472, 295, 264, 721, 286, 3449, 307, 286, 722, 6229, 3202, 281, 4675], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 441, "seek": 176268, "start": 1773.04, "end": 1777.76, "text": " management more every time I try to clone something or the freaking borrow checker yells", "tokens": [4592, 544, 633, 565, 286, 853, 281, 26506, 746, 420, 264, 14612, 11172, 1520, 260, 48543], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 442, "seek": 176268, "start": 1777.76, "end": 1783.6000000000001, "text": " at me that kind of like is a small like grim reminder of my roots as a C developer.", "tokens": [412, 385, 300, 733, 295, 411, 307, 257, 1359, 411, 36010, 13548, 295, 452, 10669, 382, 257, 383, 10754, 13], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 443, "seek": 176268, "start": 1783.6000000000001, "end": 1788.6000000000001, "text": " This is an interesting takeaway the only memory that we need to share that multiple parts", "tokens": [639, 307, 364, 1880, 30681, 264, 787, 4675, 300, 321, 643, 281, 2073, 300, 3866, 3166], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 444, "seek": 176268, "start": 1788.6000000000001, "end": 1792.64, "text": " of the system have access to in this entire model whether we're creating containers or", "tokens": [295, 264, 1185, 362, 2105, 281, 294, 341, 2302, 2316, 1968, 321, 434, 4084, 17089, 420], "temperature": 0.0, "avg_logprob": -0.17062932950956328, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.681434686266584e-06}, {"id": 445, "seek": 179264, "start": 1792.64, "end": 1796.0800000000002, "text": " VMs is the shared MTLS config.", "tokens": [18038, 82, 307, 264, 5507, 37333, 19198, 6662, 13], "temperature": 0.0, "avg_logprob": -0.1480530051774876, "compression_ratio": 1.625, "no_speech_prob": 2.4057412701949943e-06}, {"id": 446, "seek": 179264, "start": 1796.0800000000002, "end": 1801.64, "text": " So this is the only bit of shared memory that we really have to manage and Rust very clearly", "tokens": [407, 341, 307, 264, 787, 857, 295, 5507, 4675, 300, 321, 534, 362, 281, 3067, 293, 34952, 588, 4448], "temperature": 0.0, "avg_logprob": -0.1480530051774876, "compression_ratio": 1.625, "no_speech_prob": 2.4057412701949943e-06}, {"id": 447, "seek": 179264, "start": 1801.64, "end": 1805.88, "text": " called that out and to be a candidate I don't think I would be able to as be as comfortable", "tokens": [1219, 300, 484, 293, 281, 312, 257, 11532, 286, 500, 380, 519, 286, 576, 312, 1075, 281, 382, 312, 382, 4619], "temperature": 0.0, "avg_logprob": -0.1480530051774876, "compression_ratio": 1.625, "no_speech_prob": 2.4057412701949943e-06}, {"id": 448, "seek": 179264, "start": 1805.88, "end": 1810.48, "text": " with this model if I was doing this in something like go.", "tokens": [365, 341, 2316, 498, 286, 390, 884, 341, 294, 746, 411, 352, 13], "temperature": 0.0, "avg_logprob": -0.1480530051774876, "compression_ratio": 1.625, "no_speech_prob": 2.4057412701949943e-06}, {"id": 449, "seek": 179264, "start": 1810.48, "end": 1817.72, "text": " So Rust was able to help us solve the maintainability problem so somebody say Rust macros.", "tokens": [407, 34952, 390, 1075, 281, 854, 505, 5039, 264, 6909, 2310, 1154, 370, 2618, 584, 34952, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.1480530051774876, "compression_ratio": 1.625, "no_speech_prob": 2.4057412701949943e-06}, {"id": 450, "seek": 181772, "start": 1817.72, "end": 1824.64, "text": " So we have a really brilliant guy future highway who helps us work on the project and future", "tokens": [407, 321, 362, 257, 534, 10248, 2146, 2027, 17205, 567, 3665, 505, 589, 322, 264, 1716, 293, 2027], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 451, "seek": 181772, "start": 1824.64, "end": 1826.68, "text": " highway is our resident macro guy.", "tokens": [17205, 307, 527, 10832, 18887, 2146, 13], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 452, "seek": 181772, "start": 1826.68, "end": 1830.2, "text": " Does everybody here have a macro guy in your team?", "tokens": [4402, 2201, 510, 362, 257, 18887, 2146, 294, 428, 1469, 30], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 453, "seek": 181772, "start": 1830.2, "end": 1831.84, "text": " Because you should.", "tokens": [1436, 291, 820, 13], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 454, "seek": 181772, "start": 1831.84, "end": 1834.8, "text": " He has made things a lot simpler for us.", "tokens": [634, 575, 1027, 721, 257, 688, 18587, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 455, "seek": 181772, "start": 1834.8, "end": 1838.72, "text": " So one of the things we struggled with go in Kubernetes specifically was like how do", "tokens": [407, 472, 295, 264, 721, 321, 19023, 365, 352, 294, 23145, 4682, 390, 411, 577, 360], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 456, "seek": 181772, "start": 1838.72, "end": 1842.2, "text": " we generate objects with unique logic.", "tokens": [321, 8460, 6565, 365, 3845, 9952, 13], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 457, "seek": 181772, "start": 1842.2, "end": 1844.32, "text": " Rust macros were a solution to this for us.", "tokens": [34952, 7912, 2635, 645, 257, 3827, 281, 341, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 458, "seek": 181772, "start": 1844.32, "end": 1847.3600000000001, "text": " So if you've ever looked at the Kubernetes code base you can see we've created these", "tokens": [407, 498, 291, 600, 1562, 2956, 412, 264, 23145, 3089, 3096, 291, 393, 536, 321, 600, 2942, 613], "temperature": 0.0, "avg_logprob": -0.1656465217715404, "compression_ratio": 1.696551724137931, "no_speech_prob": 3.9037613532855175e-06}, {"id": 459, "seek": 184736, "start": 1847.36, "end": 1852.1599999999999, "text": " things called CRDs that started out as third party resources and we've built this entire", "tokens": [721, 1219, 14123, 35, 82, 300, 1409, 484, 382, 2636, 3595, 3593, 293, 321, 600, 3094, 341, 2302], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 460, "seek": 184736, "start": 1852.1599999999999, "end": 1858.36, "text": " bespoke API machinery system that basically is a glorified macro system that allows us", "tokens": [4097, 48776, 9362, 27302, 1185, 300, 1936, 307, 257, 26623, 2587, 18887, 1185, 300, 4045, 505], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 461, "seek": 184736, "start": 1858.36, "end": 1861.76, "text": " to generate go in the project.", "tokens": [281, 8460, 352, 294, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 462, "seek": 184736, "start": 1861.76, "end": 1866.6799999999998, "text": " So we're allowed to use Rust macros now and it's a very simple model in the code base.", "tokens": [407, 321, 434, 4350, 281, 764, 34952, 7912, 2635, 586, 293, 309, 311, 257, 588, 2199, 2316, 294, 264, 3089, 3096, 13], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 463, "seek": 184736, "start": 1866.6799999999998, "end": 1870.76, "text": " We basically have a combinatorics problem where we're able to map the different primitives", "tokens": [492, 1936, 362, 257, 2512, 31927, 1167, 1154, 689, 321, 434, 1075, 281, 4471, 264, 819, 2886, 38970], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 464, "seek": 184736, "start": 1870.76, "end": 1875.76, "text": " to the different logical systems that are unique to us and we can generate our source", "tokens": [281, 264, 819, 14978, 3652, 300, 366, 3845, 281, 505, 293, 321, 393, 8460, 527, 4009], "temperature": 0.0, "avg_logprob": -0.11012113416517103, "compression_ratio": 1.7472118959107807, "no_speech_prob": 7.526869922003243e-06}, {"id": 465, "seek": 187576, "start": 1875.76, "end": 1879.72, "text": " code as needed.", "tokens": [3089, 382, 2978, 13], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 466, "seek": 187576, "start": 1879.72, "end": 1885.76, "text": " And so our source code ends up looking like this which I think we've successfully achieved", "tokens": [400, 370, 527, 4009, 3089, 5314, 493, 1237, 411, 341, 597, 286, 519, 321, 600, 10727, 11042], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 467, "seek": 187576, "start": 1885.76, "end": 1888.66, "text": " boring for a low level run time.", "tokens": [9989, 337, 257, 2295, 1496, 1190, 565, 13], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 468, "seek": 187576, "start": 1888.66, "end": 1892.8799999999999, "text": " This is a fairly straightforward call and then we can be confident that the code it", "tokens": [639, 307, 257, 6457, 15325, 818, 293, 550, 321, 393, 312, 6679, 300, 264, 3089, 309], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 469, "seek": 187576, "start": 1892.8799999999999, "end": 1899.56, "text": " generates is unique to the project and encapsulates all of our concerns as maintainers.", "tokens": [23815, 307, 3845, 281, 264, 1716, 293, 38745, 26192, 439, 295, 527, 7389, 382, 6909, 433, 13], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 470, "seek": 187576, "start": 1899.56, "end": 1904.12, "text": " So really the whole conversation now is just the proto conversation.", "tokens": [407, 534, 264, 1379, 3761, 586, 307, 445, 264, 47896, 3761, 13], "temperature": 0.0, "avg_logprob": -0.10686044259504839, "compression_ratio": 1.6033755274261603, "no_speech_prob": 2.993926955241477e-06}, {"id": 471, "seek": 190412, "start": 1904.12, "end": 1905.7199999999998, "text": " Everything can be generated by Rust macros.", "tokens": [5471, 393, 312, 10833, 538, 34952, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 472, "seek": 190412, "start": 1905.7199999999998, "end": 1909.84, "text": " The whole project really is pretty much on autogen at this point.", "tokens": [440, 1379, 1716, 534, 307, 1238, 709, 322, 1476, 8799, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 473, "seek": 190412, "start": 1909.84, "end": 1914.36, "text": " You can just go introduce a new field in the API and then you can spit out a new client,", "tokens": [509, 393, 445, 352, 5366, 257, 777, 2519, 294, 264, 9362, 293, 550, 291, 393, 22127, 484, 257, 777, 6423, 11], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 474, "seek": 190412, "start": 1914.36, "end": 1918.1599999999999, "text": " it'll plumb itself into the run time, it'll plumb itself into the AuraScript library and", "tokens": [309, 603, 499, 2860, 2564, 666, 264, 1190, 565, 11, 309, 603, 499, 2860, 2564, 666, 264, 316, 2991, 14237, 6405, 293], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 475, "seek": 190412, "start": 1918.1599999999999, "end": 1923.8, "text": " everything is given to us for free just because of macros in Rust.", "tokens": [1203, 307, 2212, 281, 505, 337, 1737, 445, 570, 295, 7912, 2635, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 476, "seek": 190412, "start": 1923.8, "end": 1928.84, "text": " And so this is our code path and the way that we're able to take advantage of macros.", "tokens": [400, 370, 341, 307, 527, 3089, 3100, 293, 264, 636, 300, 321, 434, 1075, 281, 747, 5002, 295, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 477, "seek": 190412, "start": 1928.84, "end": 1932.84, "text": " We do a lot of manual work, we fight with the borough checker, we make some improvements", "tokens": [492, 360, 257, 688, 295, 9688, 589, 11, 321, 2092, 365, 264, 14828, 581, 1520, 260, 11, 321, 652, 512, 13797], "temperature": 0.0, "avg_logprob": -0.1580215152219045, "compression_ratio": 1.6900958466453675, "no_speech_prob": 1.804820385586936e-05}, {"id": 478, "seek": 193284, "start": 1932.84, "end": 1937.0, "text": " and then we get done and we encapsulate it into a macro and we can simplify our code", "tokens": [293, 550, 321, 483, 1096, 293, 321, 38745, 5256, 309, 666, 257, 18887, 293, 321, 393, 20460, 527, 3089], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 479, "seek": 193284, "start": 1937.0, "end": 1941.6, "text": " path by just replacing all of that with a macro after we've been done.", "tokens": [3100, 538, 445, 19139, 439, 295, 300, 365, 257, 18887, 934, 321, 600, 668, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 480, "seek": 193284, "start": 1941.6, "end": 1947.1599999999999, "text": " And so this is the Aura project as it exists today, which again I'm very stoked to say", "tokens": [400, 370, 341, 307, 264, 316, 2991, 1716, 382, 309, 8198, 965, 11, 597, 797, 286, 478, 588, 49145, 281, 584], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 481, "seek": 193284, "start": 1947.1599999999999, "end": 1951.32, "text": " that this is a very boring exercise.", "tokens": [300, 341, 307, 257, 588, 9989, 5380, 13], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 482, "seek": 193284, "start": 1951.32, "end": 1955.1999999999998, "text": " So a quick update and then I'll be done with my talk here.", "tokens": [407, 257, 1702, 5623, 293, 550, 286, 603, 312, 1096, 365, 452, 751, 510, 13], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 483, "seek": 193284, "start": 1955.1999999999998, "end": 1959.76, "text": " There's a few components, all of which are written in Rust here.", "tokens": [821, 311, 257, 1326, 6677, 11, 439, 295, 597, 366, 3720, 294, 34952, 510, 13], "temperature": 0.0, "avg_logprob": -0.11260404410185637, "compression_ratio": 1.6055776892430278, "no_speech_prob": 5.5927644098119345e-06}, {"id": 484, "seek": 195976, "start": 1959.76, "end": 1963.52, "text": " Number one, the AuraD daemon is the main static binary that's written in Rust and compiled", "tokens": [5118, 472, 11, 264, 316, 2991, 35, 1120, 36228, 307, 264, 2135, 13437, 17434, 300, 311, 3720, 294, 34952, 293, 36548], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 485, "seek": 195976, "start": 1963.52, "end": 1964.76, "text": " with Muzzle.", "tokens": [365, 376, 16740, 306, 13], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 486, "seek": 195976, "start": 1964.76, "end": 1968.92, "text": " So we can ship that without any of the shared objects on the host directly into an isolation", "tokens": [407, 321, 393, 5374, 300, 1553, 604, 295, 264, 5507, 6565, 322, 264, 3975, 3838, 666, 364, 16001], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 487, "seek": 195976, "start": 1968.92, "end": 1969.92, "text": " zone.", "tokens": [6668, 13], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 488, "seek": 195976, "start": 1969.92, "end": 1973.72, "text": " AER is a completely generated from Proto Client.", "tokens": [316, 1598, 307, 257, 2584, 10833, 490, 2114, 6738, 2033, 1196, 13], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 489, "seek": 195976, "start": 1973.72, "end": 1979.04, "text": " So this is exciting, we can actually call a GRPC API directly from the client, we don't", "tokens": [407, 341, 307, 4670, 11, 321, 393, 767, 818, 257, 10903, 12986, 9362, 3838, 490, 264, 6423, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 490, "seek": 195976, "start": 1979.04, "end": 1981.8799999999999, "text": " have to do any of the run time plumbing.", "tokens": [362, 281, 360, 604, 295, 264, 1190, 565, 39993, 13], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 491, "seek": 195976, "start": 1981.8799999999999, "end": 1987.72, "text": " So if we add a bool to the Proto file, we get dash dash bool directly in the client", "tokens": [407, 498, 321, 909, 257, 748, 401, 281, 264, 2114, 6738, 3991, 11, 321, 483, 8240, 8240, 748, 401, 3838, 294, 264, 6423], "temperature": 0.0, "avg_logprob": -0.16056571373572717, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.959798156458419e-06}, {"id": 492, "seek": 198772, "start": 1987.72, "end": 1991.08, "text": " compiled for free without typing a single line of code.", "tokens": [36548, 337, 1737, 1553, 18444, 257, 2167, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 493, "seek": 198772, "start": 1991.08, "end": 1995.48, "text": " So this is a very exciting primitive for us, so we can just begin to have API conversations", "tokens": [407, 341, 307, 257, 588, 4670, 28540, 337, 505, 11, 370, 321, 393, 445, 1841, 281, 362, 9362, 7315], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 494, "seek": 198772, "start": 1995.48, "end": 1999.72, "text": " and not necessarily care about the internals of the program anymore.", "tokens": [293, 406, 4725, 1127, 466, 264, 2154, 1124, 295, 264, 1461, 3602, 13], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 495, "seek": 198772, "start": 1999.72, "end": 2004.48, "text": " AuraScript is completely generated and we have this exciting project down here, which", "tokens": [316, 2991, 14237, 307, 2584, 10833, 293, 321, 362, 341, 4670, 1716, 760, 510, 11, 597], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 496, "seek": 198772, "start": 2004.48, "end": 2010.72, "text": " is AE, which is an alternative command line client written in Go.", "tokens": [307, 32207, 11, 597, 307, 364, 8535, 5622, 1622, 6423, 3720, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 497, "seek": 198772, "start": 2010.72, "end": 2015.48, "text": " So ultimately the lesson here is Rust was able to help us solve the boring problem.", "tokens": [407, 6284, 264, 6898, 510, 307, 34952, 390, 1075, 281, 854, 505, 5039, 264, 9989, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10684935863201435, "compression_ratio": 1.6317689530685922, "no_speech_prob": 8.266469194495585e-06}, {"id": 498, "seek": 201548, "start": 2015.48, "end": 2019.84, "text": " We have a very complicated, very obscure piece of technology that is you don't really have", "tokens": [492, 362, 257, 588, 6179, 11, 588, 34443, 2522, 295, 2899, 300, 307, 291, 500, 380, 534, 362], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 499, "seek": 201548, "start": 2019.84, "end": 2022.84, "text": " to do much to work on it anymore.", "tokens": [281, 360, 709, 281, 589, 322, 309, 3602, 13], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 500, "seek": 201548, "start": 2022.84, "end": 2027.4, "text": " Most of it's on autopilot at this point and most of the conversations are very philosophical", "tokens": [4534, 295, 309, 311, 322, 31090, 31516, 412, 341, 935, 293, 881, 295, 264, 7315, 366, 588, 25066], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 501, "seek": 201548, "start": 2027.4, "end": 2032.84, "text": " in nature and not necessarily about how to implement things in the software.", "tokens": [294, 3687, 293, 406, 4725, 466, 577, 281, 4445, 721, 294, 264, 4722, 13], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 502, "seek": 201548, "start": 2032.84, "end": 2037.96, "text": " So takeaways about the project, Aura is completely stateless, so you can restart a node and it's", "tokens": [407, 45584, 466, 264, 1716, 11, 316, 2991, 307, 2584, 2219, 4272, 11, 370, 291, 393, 21022, 257, 9984, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 503, "seek": 201548, "start": 2037.96, "end": 2042.32, "text": " basically empty until you push config to it, which means all of our systems are declarative", "tokens": [1936, 6707, 1826, 291, 2944, 6662, 281, 309, 11, 597, 1355, 439, 295, 527, 3652, 366, 16694, 1166], "temperature": 0.0, "avg_logprob": -0.10337145138630825, "compression_ratio": 1.654109589041096, "no_speech_prob": 3.3926344258361496e-06}, {"id": 504, "seek": 204232, "start": 2042.32, "end": 2047.72, "text": " like NixOS now and you can just pass things like TypeScript or JSON to them and it makes", "tokens": [411, 426, 970, 4367, 586, 293, 291, 393, 445, 1320, 721, 411, 15576, 14237, 420, 31828, 281, 552, 293, 309, 1669], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 505, "seek": 204232, "start": 2047.72, "end": 2052.0, "text": " it easy to manage things like containers.", "tokens": [309, 1858, 281, 3067, 721, 411, 17089, 13], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 506, "seek": 204232, "start": 2052.0, "end": 2056.36, "text": " Next we have some to-dos for the project and I would encourage you all to get involved", "tokens": [3087, 321, 362, 512, 281, 12, 33749, 337, 264, 1716, 293, 286, 576, 5373, 291, 439, 281, 483, 3288], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 507, "seek": 204232, "start": 2056.36, "end": 2061.84, "text": " and if you want to see a demo of all this, I'll be out here in the hallway after the", "tokens": [293, 498, 291, 528, 281, 536, 257, 10723, 295, 439, 341, 11, 286, 603, 312, 484, 510, 294, 264, 23903, 934, 264], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 508, "seek": 204232, "start": 2061.84, "end": 2067.7599999999998, "text": " talk and you can come and you can track me down and I'm happy to give you a demo.", "tokens": [751, 293, 291, 393, 808, 293, 291, 393, 2837, 385, 760, 293, 286, 478, 2055, 281, 976, 291, 257, 10723, 13], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 509, "seek": 204232, "start": 2067.7599999999998, "end": 2071.7999999999997, "text": " So anyway, I think we have a few minutes for questions and five minutes for questions,", "tokens": [407, 4033, 11, 286, 519, 321, 362, 257, 1326, 2077, 337, 1651, 293, 1732, 2077, 337, 1651, 11], "temperature": 0.0, "avg_logprob": -0.11318308357300798, "compression_ratio": 1.7444444444444445, "no_speech_prob": 1.6698830222594552e-05}, {"id": 510, "seek": 207180, "start": 2071.8, "end": 2077.5600000000004, "text": " so I'll take questions and if you want to get involved, here's how to get involved and", "tokens": [370, 286, 603, 747, 1651, 293, 498, 291, 528, 281, 483, 3288, 11, 510, 311, 577, 281, 483, 3288, 293], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 511, "seek": 207180, "start": 2077.5600000000004, "end": 2089.76, "text": " I'm Chris Nova, please clap.", "tokens": [286, 478, 6688, 27031, 11, 1767, 20760, 13], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 512, "seek": 207180, "start": 2089.76, "end": 2094.0, "text": " You mentioned the size of the binary being, does it work?", "tokens": [509, 2835, 264, 2744, 295, 264, 17434, 885, 11, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 513, "seek": 207180, "start": 2094.0, "end": 2098.2400000000002, "text": " You mentioned the size of the binary being 40 megabytes, is that with size optimization", "tokens": [509, 2835, 264, 2744, 295, 264, 17434, 885, 3356, 10816, 24538, 11, 307, 300, 365, 2744, 19618], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 514, "seek": 207180, "start": 2098.2400000000002, "end": 2099.2400000000002, "text": " or no?", "tokens": [420, 572, 30], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 515, "seek": 207180, "start": 2099.2400000000002, "end": 2100.6800000000003, "text": " Sorry, say that again?", "tokens": [4919, 11, 584, 300, 797, 30], "temperature": 0.0, "avg_logprob": -0.16047903932171104, "compression_ratio": 1.6166666666666667, "no_speech_prob": 4.7398829337907955e-05}, {"id": 516, "seek": 210068, "start": 2100.68, "end": 2104.9199999999996, "text": " Is the size of the binary at 40 megabytes with size optimization applied already or", "tokens": [1119, 264, 2744, 295, 264, 17434, 412, 3356, 10816, 24538, 365, 2744, 19618, 6456, 1217, 420], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 517, "seek": 210068, "start": 2104.9199999999996, "end": 2105.9199999999996, "text": " no?", "tokens": [572, 30], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 518, "seek": 210068, "start": 2105.9199999999996, "end": 2109.52, "text": " No, that's completely unoptimized, that is like just straight out of the compiler without", "tokens": [883, 11, 300, 311, 2584, 517, 5747, 332, 1602, 11, 300, 307, 411, 445, 2997, 484, 295, 264, 31958, 1553], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 519, "seek": 210068, "start": 2109.52, "end": 2115.3199999999997, "text": " any aftermarket tuning.", "tokens": [604, 934, 16414, 15164, 13], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 520, "seek": 210068, "start": 2115.3199999999997, "end": 2117.24, "text": " Amazing talk, quick question.", "tokens": [14165, 751, 11, 1702, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 521, "seek": 210068, "start": 2117.24, "end": 2121.3999999999996, "text": " So if I want to have just enough Linux to like pixie boot into this thing, like do", "tokens": [407, 498, 286, 528, 281, 362, 445, 1547, 18734, 281, 411, 11273, 414, 11450, 666, 341, 551, 11, 411, 360], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 522, "seek": 210068, "start": 2121.3999999999996, "end": 2125.08, "text": " you guys have any templates because it feels like a shame to run it on something like", "tokens": [291, 1074, 362, 604, 21165, 570, 309, 3417, 411, 257, 10069, 281, 1190, 309, 322, 746, 411], "temperature": 0.0, "avg_logprob": -0.20706834980085784, "compression_ratio": 1.5810276679841897, "no_speech_prob": 0.00045040977420285344}, {"id": 523, "seek": 212508, "start": 2125.08, "end": 2130.7999999999997, "text": " RHEL, like I just need like enough of Linux to just pixie boot into that?", "tokens": [50209, 3158, 11, 411, 286, 445, 643, 411, 1547, 295, 18734, 281, 445, 11273, 414, 11450, 666, 300, 30], "temperature": 0.0, "avg_logprob": -0.21191072463989258, "compression_ratio": 1.5924528301886793, "no_speech_prob": 0.00024494322133250535}, {"id": 524, "seek": 212508, "start": 2130.7999999999997, "end": 2135.48, "text": " Yeah, so the question is basically can we pixie boot this and then you mentioned RHEL.", "tokens": [865, 11, 370, 264, 1168, 307, 1936, 393, 321, 11273, 414, 11450, 341, 293, 550, 291, 2835, 50209, 3158, 13], "temperature": 0.0, "avg_logprob": -0.21191072463989258, "compression_ratio": 1.5924528301886793, "no_speech_prob": 0.00024494322133250535}, {"id": 525, "seek": 212508, "start": 2135.48, "end": 2141.7999999999997, "text": " Where we're going, we don't need Red Hat, so I guess what I would say is in theory all", "tokens": [2305, 321, 434, 516, 11, 321, 500, 380, 643, 4477, 15867, 11, 370, 286, 2041, 437, 286, 576, 584, 307, 294, 5261, 439], "temperature": 0.0, "avg_logprob": -0.21191072463989258, "compression_ratio": 1.5924528301886793, "no_speech_prob": 0.00024494322133250535}, {"id": 526, "seek": 212508, "start": 2141.7999999999997, "end": 2147.16, "text": " you need to run is static Linux kernel and Aura and a network connection and some MTLS", "tokens": [291, 643, 281, 1190, 307, 13437, 18734, 28256, 293, 316, 2991, 293, 257, 3209, 4984, 293, 512, 37333, 19198], "temperature": 0.0, "avg_logprob": -0.21191072463989258, "compression_ratio": 1.5924528301886793, "no_speech_prob": 0.00024494322133250535}, {"id": 527, "seek": 212508, "start": 2147.16, "end": 2151.7599999999998, "text": " config, and so everything else at that point, all of your packages, your services, your", "tokens": [6662, 11, 293, 370, 1203, 1646, 412, 300, 935, 11, 439, 295, 428, 17401, 11, 428, 3328, 11, 428], "temperature": 0.0, "avg_logprob": -0.21191072463989258, "compression_ratio": 1.5924528301886793, "no_speech_prob": 0.00024494322133250535}, {"id": 528, "seek": 215176, "start": 2151.76, "end": 2157.8, "text": " daemons are passed to it via the API.", "tokens": [1120, 443, 892, 366, 4678, 281, 309, 5766, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.17885303497314453, "compression_ratio": 1.5147058823529411, "no_speech_prob": 3.762557389563881e-05}, {"id": 529, "seek": 215176, "start": 2157.8, "end": 2166.92, "text": " Hi, you mentioned that you use a lot of macros.", "tokens": [2421, 11, 291, 2835, 300, 291, 764, 257, 688, 295, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.17885303497314453, "compression_ratio": 1.5147058823529411, "no_speech_prob": 3.762557389563881e-05}, {"id": 530, "seek": 215176, "start": 2166.92, "end": 2172.1600000000003, "text": " I've also run into problems where, you know, you have a combinatorial explosion of templates", "tokens": [286, 600, 611, 1190, 666, 2740, 689, 11, 291, 458, 11, 291, 362, 257, 2512, 31927, 831, 15673, 295, 21165], "temperature": 0.0, "avg_logprob": -0.17885303497314453, "compression_ratio": 1.5147058823529411, "no_speech_prob": 3.762557389563881e-05}, {"id": 531, "seek": 215176, "start": 2172.1600000000003, "end": 2177.0, "text": " in C++ speak or something like that.", "tokens": [294, 383, 25472, 1710, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17885303497314453, "compression_ratio": 1.5147058823529411, "no_speech_prob": 3.762557389563881e-05}, {"id": 532, "seek": 215176, "start": 2177.0, "end": 2180.96, "text": " What are your thoughts on generics for generating some of this rather than macros in order to", "tokens": [708, 366, 428, 4598, 322, 1337, 1167, 337, 17746, 512, 295, 341, 2831, 813, 7912, 2635, 294, 1668, 281], "temperature": 0.0, "avg_logprob": -0.17885303497314453, "compression_ratio": 1.5147058823529411, "no_speech_prob": 3.762557389563881e-05}, {"id": 533, "seek": 218096, "start": 2180.96, "end": 2183.88, "text": " be a bit more type safe, I suppose?", "tokens": [312, 257, 857, 544, 2010, 3273, 11, 286, 7297, 30], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 534, "seek": 218096, "start": 2183.88, "end": 2189.04, "text": " Personally, I got a little drunk with generics, I'm not going to lie.", "tokens": [21079, 11, 286, 658, 257, 707, 11192, 365, 1337, 1167, 11, 286, 478, 406, 516, 281, 4544, 13], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 535, "seek": 218096, "start": 2189.04, "end": 2193.6, "text": " When I first moved over from Go, because I was just so excited about it, the reason", "tokens": [1133, 286, 700, 4259, 670, 490, 1037, 11, 570, 286, 390, 445, 370, 2919, 466, 309, 11, 264, 1778], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 536, "seek": 218096, "start": 2193.6, "end": 2196.36, "text": " I like macros is because we can add logic to them.", "tokens": [286, 411, 7912, 2635, 307, 570, 321, 393, 909, 9952, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 537, "seek": 218096, "start": 2196.36, "end": 2200.2400000000002, "text": " So we have, like to give you an example, we have containers and we have VMs.", "tokens": [407, 321, 362, 11, 411, 281, 976, 291, 364, 1365, 11, 321, 362, 17089, 293, 321, 362, 18038, 82, 13], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 538, "seek": 218096, "start": 2200.2400000000002, "end": 2205.2400000000002, "text": " So we'll have a section of the macro dedicated just to VMs that manages the kernel.", "tokens": [407, 321, 603, 362, 257, 3541, 295, 264, 18887, 8374, 445, 281, 18038, 82, 300, 22489, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 539, "seek": 218096, "start": 2205.2400000000002, "end": 2209.56, "text": " And that's irrelevant to the container systems in the project because containers run on the", "tokens": [400, 300, 311, 28682, 281, 264, 10129, 3652, 294, 264, 1716, 570, 17089, 1190, 322, 264], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 540, "seek": 218096, "start": 2209.56, "end": 2210.88, "text": " host kernel.", "tokens": [3975, 28256, 13], "temperature": 0.0, "avg_logprob": -0.145754477557014, "compression_ratio": 1.6866666666666668, "no_speech_prob": 2.3917122234706767e-05}, {"id": 541, "seek": 221088, "start": 2210.88, "end": 2216.0, "text": " And so we can embed those small branches directly into the macro code so that macros generate", "tokens": [400, 370, 321, 393, 12240, 729, 1359, 14770, 3838, 666, 264, 18887, 3089, 370, 300, 7912, 2635, 8460], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 542, "seek": 221088, "start": 2216.0, "end": 2220.48, "text": " slightly different outputs based off of the inputs that are given to them.", "tokens": [4748, 819, 23930, 2361, 766, 295, 264, 15743, 300, 366, 2212, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 543, "seek": 221088, "start": 2220.48, "end": 2225.92, "text": " So for Aura, when you're dealing with similar systems of code that have small nuances like", "tokens": [407, 337, 316, 2991, 11, 562, 291, 434, 6260, 365, 2531, 3652, 295, 3089, 300, 362, 1359, 38775, 411], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 544, "seek": 221088, "start": 2225.92, "end": 2230.36, "text": " we are, macros really, in my opinion, are the way to go.", "tokens": [321, 366, 11, 7912, 2635, 534, 11, 294, 452, 4800, 11, 366, 264, 636, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 545, "seek": 221088, "start": 2230.36, "end": 2232.52, "text": " Did I answer your question?", "tokens": [2589, 286, 1867, 428, 1168, 30], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 546, "seek": 221088, "start": 2232.52, "end": 2237.12, "text": " Looks like.", "tokens": [10027, 411, 13], "temperature": 0.0, "avg_logprob": -0.2173521230508993, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.00013724659220315516}, {"id": 547, "seek": 223712, "start": 2237.12, "end": 2243.56, "text": " A simple question, so can I actually give the configuration instead of like Aura script", "tokens": [316, 2199, 1168, 11, 370, 393, 286, 767, 976, 264, 11694, 2602, 295, 411, 316, 2991, 5755], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 548, "seek": 223712, "start": 2243.56, "end": 2246.12, "text": " or TypeScript just in Rust?", "tokens": [420, 15576, 14237, 445, 294, 34952, 30], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 549, "seek": 223712, "start": 2246.12, "end": 2248.04, "text": " Yeah, of course.", "tokens": [865, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 550, "seek": 223712, "start": 2248.04, "end": 2254.08, "text": " So we have this Rust client here, it's basically a Rust SDK.", "tokens": [407, 321, 362, 341, 34952, 6423, 510, 11, 309, 311, 1936, 257, 34952, 37135, 13], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 551, "seek": 223712, "start": 2254.08, "end": 2259.08, "text": " And then we have a tool called AER, which takes it a step further and it's automatically", "tokens": [400, 550, 321, 362, 257, 2290, 1219, 316, 1598, 11, 597, 2516, 309, 257, 1823, 3052, 293, 309, 311, 6772], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 552, "seek": 223712, "start": 2259.08, "end": 2260.6, "text": " generated with macros.", "tokens": [10833, 365, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 553, "seek": 223712, "start": 2260.6, "end": 2264.7999999999997, "text": " And it's a compiled binary that you can just use from the command line.", "tokens": [400, 309, 311, 257, 36548, 17434, 300, 291, 393, 445, 764, 490, 264, 5622, 1622, 13], "temperature": 0.0, "avg_logprob": -0.23289716361772897, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.00019659445388242602}, {"id": 554, "seek": 226480, "start": 2264.8, "end": 2270.52, "text": " So you can just type commands directly into it and it will run against the server on the", "tokens": [407, 291, 393, 445, 2010, 16901, 3838, 666, 309, 293, 309, 486, 1190, 1970, 264, 7154, 322, 264], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 555, "seek": 226480, "start": 2270.52, "end": 2271.52, "text": " back end.", "tokens": [646, 917, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 556, "seek": 226480, "start": 2271.52, "end": 2272.52, "text": " Do you think code is Rust?", "tokens": [1144, 291, 519, 3089, 307, 34952, 30], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 557, "seek": 226480, "start": 2272.52, "end": 2273.52, "text": " Yeah, there's also an SDK.", "tokens": [865, 11, 456, 311, 611, 364, 37135, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 558, "seek": 226480, "start": 2273.52, "end": 2275.5600000000004, "text": " So you could write your own Rust code and it's GRPC.", "tokens": [407, 291, 727, 2464, 428, 1065, 34952, 3089, 293, 309, 311, 10903, 12986, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 559, "seek": 226480, "start": 2275.5600000000004, "end": 2280.52, "text": " So you could generate, you could write it in Go or in WeDo and you could write it in Python", "tokens": [407, 291, 727, 8460, 11, 291, 727, 2464, 309, 294, 1037, 420, 294, 492, 7653, 293, 291, 727, 2464, 309, 294, 15329], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 560, "seek": 226480, "start": 2280.52, "end": 2284.28, "text": " or Ruby or realistically anything, any client you want.", "tokens": [420, 19907, 420, 40734, 1340, 11, 604, 6423, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 561, "seek": 226480, "start": 2284.28, "end": 2285.28, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 562, "seek": 226480, "start": 2285.28, "end": 2290.6000000000004, "text": " I was wondering when you talk about the remote API, have you considered a future direction", "tokens": [286, 390, 6359, 562, 291, 751, 466, 264, 8607, 9362, 11, 362, 291, 4888, 257, 2027, 3513], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 563, "seek": 226480, "start": 2290.6000000000004, "end": 2292.6000000000004, "text": " to make this a unicolonel?", "tokens": [281, 652, 341, 257, 517, 299, 38780, 338, 30], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 564, "seek": 226480, "start": 2292.6000000000004, "end": 2293.6000000000004, "text": " A unicolonel.", "tokens": [316, 517, 299, 38780, 338, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 565, "seek": 226480, "start": 2293.6000000000004, "end": 2294.6000000000004, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23661706858667833, "compression_ratio": 1.71875, "no_speech_prob": 2.2116326363175176e-05}, {"id": 566, "seek": 229460, "start": 2294.6, "end": 2295.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 567, "seek": 229460, "start": 2295.6, "end": 2296.6, "text": " I have a slide for this.", "tokens": [286, 362, 257, 4137, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 568, "seek": 229460, "start": 2296.6, "end": 2300.24, "text": " So I added like a bunch of like FAQ slides to the end because I knew that we were going", "tokens": [407, 286, 3869, 411, 257, 3840, 295, 411, 19894, 48, 9788, 281, 264, 917, 570, 286, 2586, 300, 321, 645, 516], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 569, "seek": 229460, "start": 2300.24, "end": 2302.68, "text": " to get all these good questions.", "tokens": [281, 483, 439, 613, 665, 1651, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 570, "seek": 229460, "start": 2302.68, "end": 2306.6, "text": " The answer is it depends, hold on, let's see if I can't find it.", "tokens": [440, 1867, 307, 309, 5946, 11, 1797, 322, 11, 718, 311, 536, 498, 286, 393, 380, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 571, "seek": 229460, "start": 2306.6, "end": 2307.6, "text": " You guys get to see.", "tokens": [509, 1074, 483, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 572, "seek": 229460, "start": 2307.6, "end": 2310.2799999999997, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 573, "seek": 229460, "start": 2310.2799999999997, "end": 2311.2799999999997, "text": " It depends.", "tokens": [467, 5946, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 574, "seek": 229460, "start": 2311.2799999999997, "end": 2312.44, "text": " What does unicolonel mean to you?", "tokens": [708, 775, 517, 299, 38780, 338, 914, 281, 291, 30], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 575, "seek": 229460, "start": 2312.44, "end": 2316.24, "text": " I think the most minimal system we could do would be a Linux kernel as it exists today,", "tokens": [286, 519, 264, 881, 13206, 1185, 321, 727, 360, 576, 312, 257, 18734, 28256, 382, 309, 8198, 965, 11], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 576, "seek": 229460, "start": 2316.24, "end": 2320.36, "text": " like good old fashioned stock Linux giant make file to hold nine yards.", "tokens": [411, 665, 1331, 40646, 4127, 18734, 7410, 652, 3991, 281, 1797, 4949, 18685, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 577, "seek": 229460, "start": 2320.36, "end": 2323.7999999999997, "text": " And then the ORID daemon and that would be the minimal system.", "tokens": [400, 550, 264, 19654, 2777, 1120, 36228, 293, 300, 576, 312, 264, 13206, 1185, 13], "temperature": 0.0, "avg_logprob": -0.18794588325849546, "compression_ratio": 1.6424050632911393, "no_speech_prob": 3.071090395678766e-05}, {"id": 578, "seek": 232380, "start": 2323.8, "end": 2326.4, "text": " Anything else you would need to pass to it at runtime?", "tokens": [11998, 1646, 291, 576, 643, 281, 1320, 281, 309, 412, 34474, 30], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 579, "seek": 232380, "start": 2326.4, "end": 2335.2400000000002, "text": " I think we have time for about one more question.", "tokens": [286, 519, 321, 362, 565, 337, 466, 472, 544, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 580, "seek": 232380, "start": 2335.2400000000002, "end": 2337.4, "text": " So you said it doesn't do any higher order scheduling.", "tokens": [407, 291, 848, 309, 1177, 380, 360, 604, 2946, 1668, 29055, 13], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 581, "seek": 232380, "start": 2337.4, "end": 2341.76, "text": " I guess I'm kind of curious what, if you want to do things like resilience or steering", "tokens": [286, 2041, 286, 478, 733, 295, 6369, 437, 11, 498, 291, 528, 281, 360, 721, 411, 19980, 420, 14823], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 582, "seek": 232380, "start": 2341.76, "end": 2347.0, "text": " or if the job dies, bring something back up, what are people typically using with Aura?", "tokens": [420, 498, 264, 1691, 2714, 11, 1565, 746, 646, 493, 11, 437, 366, 561, 5850, 1228, 365, 316, 2991, 30], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 583, "seek": 232380, "start": 2347.0, "end": 2349.04, "text": " So Aura is still very new.", "tokens": [407, 316, 2991, 307, 920, 588, 777, 13], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 584, "seek": 232380, "start": 2349.04, "end": 2353.0, "text": " I think that my hope for the project is kind of like the same hope I had with my book,", "tokens": [286, 519, 300, 452, 1454, 337, 264, 1716, 307, 733, 295, 411, 264, 912, 1454, 286, 632, 365, 452, 1446, 11], "temperature": 0.0, "avg_logprob": -0.1849473905162651, "compression_ratio": 1.6290909090909091, "no_speech_prob": 1.8053247913485393e-05}, {"id": 585, "seek": 235300, "start": 2353.0, "end": 2358.8, "text": " solve the lower layer first and then that is going to open the door for higher order", "tokens": [5039, 264, 3126, 4583, 700, 293, 550, 300, 307, 516, 281, 1269, 264, 2853, 337, 2946, 1668], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 586, "seek": 235300, "start": 2358.8, "end": 2360.48, "text": " conversations in the future.", "tokens": [7315, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 587, "seek": 235300, "start": 2360.48, "end": 2363.84, "text": " My hope is that there's a whole ecosystem of schedulers.", "tokens": [1222, 1454, 307, 300, 456, 311, 257, 1379, 11311, 295, 12000, 433, 13], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 588, "seek": 235300, "start": 2363.84, "end": 2369.64, "text": " You change your scheduler, you change your socks, well maybe not that often, but the", "tokens": [509, 1319, 428, 12000, 260, 11, 291, 1319, 428, 17564, 11, 731, 1310, 406, 300, 2049, 11, 457, 264], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 589, "seek": 235300, "start": 2369.64, "end": 2373.84, "text": " point would be that that's very specific to the needs of the current organization that's", "tokens": [935, 576, 312, 300, 300, 311, 588, 2685, 281, 264, 2203, 295, 264, 2190, 4475, 300, 311], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 590, "seek": 235300, "start": 2373.84, "end": 2375.04, "text": " working on it.", "tokens": [1364, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 591, "seek": 235300, "start": 2375.04, "end": 2379.48, "text": " And I would hope that we can still use the Kubernetes scheduler or the Nomad scheduler", "tokens": [400, 286, 576, 1454, 300, 321, 393, 920, 764, 264, 23145, 12000, 260, 420, 264, 31272, 345, 12000, 260], "temperature": 0.0, "avg_logprob": -0.16662738106467506, "compression_ratio": 1.7088122605363985, "no_speech_prob": 3.943243427784182e-05}, {"id": 592, "seek": 237948, "start": 2379.48, "end": 2382.96, "text": " to schedule jobs on Aura.", "tokens": [281, 7567, 4782, 322, 316, 2991, 13], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 593, "seek": 237948, "start": 2382.96, "end": 2387.52, "text": " I know there's also some machine learning folks who have some data resiliency problems", "tokens": [286, 458, 456, 311, 611, 512, 3479, 2539, 4024, 567, 362, 512, 1412, 48712, 2740], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 594, "seek": 237948, "start": 2387.52, "end": 2394.76, "text": " that are interested in Aura right now and plan on using some weird global mesh that will", "tokens": [300, 366, 3102, 294, 316, 2991, 558, 586, 293, 1393, 322, 1228, 512, 3657, 4338, 17407, 300, 486], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 595, "seek": 237948, "start": 2394.76, "end": 2399.32, "text": " do a peer-to-peer network around the world, kind of like BitTorrent, and then they intend", "tokens": [360, 257, 15108, 12, 1353, 12, 494, 260, 3209, 926, 264, 1002, 11, 733, 295, 411, 9101, 51, 284, 1753, 11, 293, 550, 436, 19759], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 596, "seek": 237948, "start": 2399.32, "end": 2400.32, "text": " to use Aura for that.", "tokens": [281, 764, 316, 2991, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 597, "seek": 237948, "start": 2400.32, "end": 2402.64, "text": " So I think there's some opportunities there.", "tokens": [407, 286, 519, 456, 311, 512, 4786, 456, 13], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 598, "seek": 237948, "start": 2402.64, "end": 2406.6, "text": " The project itself won't ever have an opinion on a scheduler.", "tokens": [440, 1716, 2564, 1582, 380, 1562, 362, 364, 4800, 322, 257, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.1270307764276728, "compression_ratio": 1.6091954022988506, "no_speech_prob": 1.5203282600850798e-05}, {"id": 599, "seek": 240660, "start": 2406.6, "end": 2410.2, "text": " Maybe I personally will start another project to do that in the future or something, but", "tokens": [2704, 286, 5665, 486, 722, 1071, 1716, 281, 360, 300, 294, 264, 2027, 420, 746, 11, 457], "temperature": 0.0, "avg_logprob": -0.3557286996107835, "compression_ratio": 1.2681159420289856, "no_speech_prob": 2.837817373801954e-05}, {"id": 600, "seek": 240660, "start": 2410.2, "end": 2412.2, "text": " this is the scope for now.", "tokens": [341, 307, 264, 11923, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.3557286996107835, "compression_ratio": 1.2681159420289856, "no_speech_prob": 2.837817373801954e-05}, {"id": 601, "seek": 240660, "start": 2412.2, "end": 2414.6, "text": " So that's all the time we have.", "tokens": [407, 300, 311, 439, 264, 565, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.3557286996107835, "compression_ratio": 1.2681159420289856, "no_speech_prob": 2.837817373801954e-05}, {"id": 602, "seek": 240660, "start": 2414.6, "end": 2415.6, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3557286996107835, "compression_ratio": 1.2681159420289856, "no_speech_prob": 2.837817373801954e-05}, {"id": 603, "seek": 241560, "start": 2415.6, "end": 2439.6, "text": " Can we hear it again?", "tokens": [1664, 321, 1568, 309, 797, 30], "temperature": 0.0, "avg_logprob": -0.6099475860595703, "compression_ratio": 0.7241379310344828, "no_speech_prob": 7.431721314787865e-05}], "language": "en"}