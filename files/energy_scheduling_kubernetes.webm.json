{"text": " Hello, everyone. Today we are going to talk about how you can achieve sustainability in computing, how you can do energy efficient placement of Kubernetes workload. My name is Parul Singh and I work as a senior software engineer at Red Hat. With me, we have Guy Liu. He is a software engineer intern and today we are presenting this presentation together. So we are part of CNCF and we are taking community-based initiatives on environment sustainability. If you want to check our proposal, you can follow the link. We also have done a few projects. Again, using community-based approach, the first one of them is carbon aware scaling with KEDA. We did this with Microsoft and we investigated how you can use electricity and carbon intensity to make workload scaling decisions. Another one that we've been working with IBM Research is Clever. That is container, label, energy efficient, VP, a recommender for Kubernetes. And if you want to check out both of these projects, you can just follow the QR. So the agenda is very simple. We'll give a brief background of the things, how they are at the moment and then we're going to introduce a sustainability stack which consists of two projects, the Kepler and the Model Server and then we will have a demo. So here we have an interesting quote that sums up the motivation of our sustainability stack and the problem it seeks to solve. So according to Gardner, in 2021 an ACM technology brief estimated that the information and communication technology sector contributed between 1.8% and 3.9% of global carbon emissions, which is astonishingly more than the CO2 emission contributions of both Germany and Italy combined. The significant carbon footprint and significant energy consumption of the tech industry begs the following questions. How can we measure energy consumption quickly and indirectly? How can we measure energy consumption of workloads? And how can we then attribute power on shared resources to processes, containers and pods? So with these issues in mind, we introduced a cloud-native sustainability stack which seeks to address these questions and problems. Perule will first start by discussing the Kepler project and then I will discuss the Kepler Model Server project. Let's talk about the energy consumption attribution methodology used by the Kepler. What Kepler is based on the principle that power consumption is attributed to the resource usage by process containers and pods. For example, let's say you have a pod that consumed 10% of CPU, that means it attributed to 10% of CPU power consumption. Similar if you have like five containers and they total contributed to 50% of CPU usage, that means they attributed to 50% of CPU power consumption. It is so on and so forth for other resources like memory and GPU etc. And this we based this principle based on the studies and we have attached the link to the paper. If you're interested you can check that out. So Kepler is a Kubernetes-based efficient power level exporter and it uses software counters to measure power consumption by hardware resources and exports them as Prometheus metrics. Kepler does three things. The first is reporting. It reports per pod level energy consumption including resources like CPU, GPU and RAM and it supports bare metal as well as PM. So you can measure your workloads energy consumption even on AWS or Azure etc. And it supports Prometheus for exporting the metrics and you can see the dashboards using Grafana. It's very important that Kepler has low energy footprint because what we're trying to do is measure. So we don't want to have Kepler consuming a lot of power itself. So we used EBPF to probe the counters and this considerably reduced the computational resource used by Kepler. And at last we support ML models to estimate energy consumption when you don't have a power meter and Kai will talk more about it in the Kepler model server portion but we use ML models to predict the energy consumption when inherent power meter is not available. The second part of the sustainability stack is the Kepler model server. So by default Kepler will use a supported power measurement tool or meter to measure node-related energy metrics like CPU core, DRAM and then they uses this to estimate pod level energy metrics. But what happens when Kepler does not have access to a supported power meter? This is where the Kepler model server steps in to provide trained models that use software counters and performance metrics to predict relevant energy metrics. The tech stack of the Kepler model server also includes TensorFlow Keras, Psychic, Flask and Prometheus. So let's take a look at some of the models the Kepler model server has implemented. For example, we have a linear regression model that predicts node level CPU core energy consumption with the following categorical and normalized numerical software counters and performance metrics. This model also supports incremental learning, incremental training on new batches of data to improve the model's performance on a cluster. The second example also provides a linear regression model capable of online learning but it instead predicts node level DRAM energy consumption with the following software counters and performance metrics. So let's take a look at how the model server fits in Kepler as a whole. So the first part is training our models on a variety of training workloads where Kepler can export node energy metrics and performance metrics because a power meter is present. In this case Kepler retrieves these node energy metrics from agents which are then collected and exported as Prometheus metrics. The model server scrapes these Prometheus metrics, sets up training, testing and validation data sets and then trains, evaluates and saves the model with the new data. The second part is now exporting these trained models to Kepler for prediction whenever a power meter is not provided. The Kepler model server can export the model itself as an archive to Kepler and this is done with flash grouts. The model server can also export the model's weights directly using flash grouts and or Prometheus metrics. In the future we will also like to export the model weights using the open telemetry metrics API. Now that we have talked about sustainability stack let's see how you can do carbon intensity aware scheduling. So the use case that we are trying to solve is can you put a check or can you control the carbon intensity of your workload. For example is it possible to fuel your workloads using renewable energy like solar power or wind power when available and switch to fossil fuel when the renewable energy is not at disposal. So the use case premise is based on multi-node cluster where you have nodes in different geographical zones and the workloads that we will be talking about is long-running patch or machine learning workloads that that keeps on retraining algorithm or any long-running patch workloads that are not affected by rescheduling of that runs long enough that have an impact to considerable impact on carbon intensity and they don't they're not affected if you reschedule them on different nodes. So our demo setup is based on OpenShift cluster and for monitoring we're using Prometheus. We would be using features like chains, toleration and node selectors to orchestrate where the workload is going to run on which node and we will have a carbon intensity forecaster that will forecast the carbon intensity of nodes and for this demo we are only considering two Rs step that that means that a carbon intensity forecaster would predict what is the carbon intensity for the next two hours. So let's first describe the carbon intensity forecaster. The forecaster has access to an exporter which scrapes time series carbon intensity data from numerous public APIs like electricity map or national grid and it then exports this data as Prometheus metrics. The forecaster will then scrape the Prometheus metrics from the exporter and update its models for each of the node with new time series data. In this demo we will have three nodes so the forecaster will have individual models for each of the three nodes which are in different zones of course. The carbon forecaster will then provide a prediction of the carbon intensity of the desired region a few hours in advance. Note that the carbon intensity forecaster and exporter are extendable interfaces this means the forecaster can implement many different types of time series forecasting models and the exporter can scrape from many different carbon data APIs. So now that we have a carbon intensity forecaster external applications like the Cron job will forecast the potential carbon intensity sometime into the future for each of the three nodes. The Cron job does this by making an HTTP request to the carbon forecaster using the get slash forecasted CIN point and each of the three nodes are then periodically assigned node labels depending on the carbon intensity. Red stands for a relatively high carbon intensity yellow stands for a medium carbon intensity and green stands for a relatively low carbon intensity. So in this example note 1 is for forecasted two hours in the future to have the highest carbon intensity so it is labeled red. Node 2 is forecasted two hours in the future to have a medium carbon intensity, so it is labeled yellow and note 3 is forecasted two hours in the future to have the lowest carbon intensity so it is labeled green. Now that you have assigned labels to the node, it's on the pod to declare its intention that what kind of node it prefers and also what kind of node it does not prefer at all. So for example in the pod yaml you specify node selector carbon intensity as green that means it prefers nodes that have labels as carbon intensity green and you also have to add as a tolerations where you have to say that you don't have the toleration effect no execute means that this pod does not have toleration to run on nodes that have been tainted as red. So if the scheduler will try to schedule this pod on node 1 that has label and taint potas red, this pod would evict within 5 seconds. So that's what the toleration second is for. So let's see how this this looks like. So you have node 1 and there was that has label that had label green now it's turning to red that means its carbon intensity is increasing. So we will taint the node and we will apply the taint as carbon intensity red and no execute. So as soon as this taint is applied the pod is evicted from node 1 and it's assigned to node 2 which has the carbon intensity is changing from red to green and it has been tainted. So tainting the nodes ensures that pods are evicted by the nodes if pods have no tolerations for taint. So this is like the whole picture we have a carbon intensity exporter that queries the various public API to gather the carbon intensity data and it exports them as Prometheus metrics. Now the node label and why is a cronchial what it does it queries a carbon intensity forecaster and it queries in head of time what is going to be the carbon intensity of the various nodes and it patches the labels and taints based on the forecasted carbon intensity. So let's get to the demo. First I'm going to show you how you can install a Kepler operator on an OpenShift environment. The first the release that we have right now is V1 alpha 1 and it has a prerequisite that it needs C group V2 and it follows Kepler 0.4 release and it deploys Kepler both on Kubernetes and OpenShift. So when you're deploying it on OpenShift it also reconfigure your OpenShift nodes by applying a machine config and SCC and right now Kepler uses local linear regression estimator in Kepler main container with offline trained models but in the next release we are planning to provide end-to-end learning pipeline where it can train the model as well as use the model. So if you're interested in a code you can follow us on GitHub repository and so let's get to the demo. To deploy the operator go inside the Kepler operator project and run the make deploy that will create all the manifest and install the operator in the namespace Kepler operator system. So now I'm just going to go into the Kepler operator system the namespace and I'm going to apply let's see if the operator has been yeah so you can see that the operator is running now I'm going to apply the CRD and wait for the Kepler instances to get started. So as you can see the Kepler instances are running and they are each of them is up and running and they are each of them are running on each of the nodes as a demon set pod so that's why you see so many of them and now I'm going to deploy Grafana give it a second yes so Grafana is deployed now to enable user workload monitoring I'm going to apply the config map and that ensures that the Prometheus and the user workload monitoring namespace is capturing the Prometheus metrics so let's see if the pods are up and running in the OpenShift user monitoring project as you can see that all the pods are running so now to see the metrics I'm just going to the Grafana URL just sign in and because we applied the Grafana operator so the default Kepler dashboard should be available give it a second it will load yeah so now you can see the energy reporting from Kepler you can see the carbon footprint you can see the power consumption in namespaces total power consumption pod process power consumption and total power consumption by namespace so that's the default Grafana dashboard so now that we have seen how you can install and play around with your Kepler operator it's time to see how you can also do carbon intensity aware scheduling so for that I have a cluster already ready so you can see that there are six nodes on this cluster and for my this demo I'm not going to run anything on the master node so I'm only going to do things on the worker node so I have applied the cron job and I'm just waiting it to become active all right so that job has been scheduled let's wait for you to get completed as you can see that the crown job has been completed so let's see what gains and what labels it has applied to the three nodes so to see the labels I'm going to use the same script that I have written okay so you can see that the node 2 2 3 has got the label green node 2 2 2 has got the label red and node 1 2 6 has the label yellow so anytime that we are going to schedule a carbon intensive aware pod or workload it should favor 2 2 3 which has carbon intensity as green let's also check what gains has been assigned and if they match the labels so you can see that the node 1 2 a 6 and 1 2 3 which has carbon intensity as green and yellow have no gains while the node 2 2 2 which has the carbon intensity as red as you can see over here has the taint applied now I am going to test it out if this works as expected by applying or by scheduling a long-running workload before I do that I just want to watch all the pods in the namespace so right now there's no pod so so I have applied this pod and it has it has no tolerations for node that has tainted red and it favors a node or it wants a node that has the label green so over here you can see that the CITS pod the pod that I just ran had some issue or had some problem in finding the right node that's because the default scheduler was trying to sign it on a node that didn't have the right label or didn't have the right taint so it took some took a while so let's verify where this pod is running so you can see that it has been scheduled on the go it was scheduled on two to three but right now it's running on 126 and 126 is a node that has common intensity as yellow so that's that's completely okay the time it was scheduled on either the green node or on the yellow node which is okay as long as it's not scheduled on the red node so that would be all thank you for watching the demo we would like to share a few lessons that we learned while working on this project the first is that finding the zone cover intensity data is not simple some points are missing and not all of them are free we also need to support multiple and complex query types for example right now we are just querying what is the current or the average cover intensity in zone XYZ but we need to have more complicated queries like which zone has the lowest cover intensity and we are also thinking of contributing the work that we have done with the cover intensity forecasting and integrating it with green software foundation carbon away SDK which is another open source community that has been working on sustainability and green software so the road ahead for us looks like we are thinking of extending the multi node logic to multi cluster and we're exploring how you can do that using kcp and we are also thinking of integrating carbon intensity awareness in Kubernetes plugins existing plugins for example the trimaran target load packing is a scheduler plugin by in the Kubernetes sake and we're thinking of integrating the profile with the carbon intensity awareness and also thinking of how you can tune trimaran further for energy efficiency so that was all if you are more interested in learning about the principle of that capra is based on you can follow the link and check out a project we have attached the GitHub repo for the project as well as the model server and thank you so much and any questions you okay do you want to take that question do you see the question okay so yeah sir um sorry I'm just trying to see the questions I have to switch back and forth okay sir um sorry I'm just trying to see the questions I somebody asked how do we split the energy for the pod oh um yeah I think I can answer that um this was done on Kepler I believe and I was developed by somebody else but essentially there are two ways for like the model server we also have recently have like models that'll use the performance metrics and then the software counters to directly try and predict pod energy um when it that that's one option and then second option in Kepler is typically um once it generates the energy it'll then try and attribute it I believe to each of the pods and I think that's based on um is it based on cpu utilization proof I don't know yeah what we do is we monitor the cpu utilization although whatever the cpu instruction or the process is going on and then we use cgroup id to kind of like attribute what how that energy is related to which pod because we take the cgroup id and we translate that which particular process or container it's related to so that's how we gather the metrics so the important thing to note over here is Kepler uses the models to estimate or predict the energy consumption and these models are already trained they already have they are already being published so Kepler uses these models to predict pod energy level consumption on scenarios where you're not running on bare metal on those cases we don't have the access to the inbuilt power meter so in those scenarios we estimate or we predict what is going to be the energy consumption so another question is how what is the credibility of the uh uh greenness uh that data is as good as the data published by the public api for example we have electricity map in us and national grid in europe and uh that is one of a one of a problem as well that the the greenness or the accuracy of the carbon intensity is as good as the data that's being published by the public api we cannot control that okay i should probably note that we will also aim for any data that's from the government so i think national grid is uh straight from is from the uk government so i think that's pretty reliable and we will always make sure that the data that we use is from reliable sources", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.48, "text": " Hello, everyone. Today we are going to talk about how you can achieve sustainability in", "tokens": [2425, 11, 1518, 13, 2692, 321, 366, 516, 281, 751, 466, 577, 291, 393, 4584, 16360, 294], "temperature": 0.0, "avg_logprob": -0.24345163755779026, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.17690393328666687}, {"id": 1, "seek": 0, "start": 10.48, "end": 16.080000000000002, "text": " computing, how you can do energy efficient placement of Kubernetes workload. My name", "tokens": [15866, 11, 577, 291, 393, 360, 2281, 7148, 17257, 295, 23145, 20139, 13, 1222, 1315], "temperature": 0.0, "avg_logprob": -0.24345163755779026, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.17690393328666687}, {"id": 2, "seek": 0, "start": 16.080000000000002, "end": 20.92, "text": " is Parul Singh and I work as a senior software engineer at Red Hat. With me, we have Guy", "tokens": [307, 3457, 425, 27529, 293, 286, 589, 382, 257, 7965, 4722, 11403, 412, 4477, 15867, 13, 2022, 385, 11, 321, 362, 14690], "temperature": 0.0, "avg_logprob": -0.24345163755779026, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.17690393328666687}, {"id": 3, "seek": 0, "start": 20.92, "end": 28.92, "text": " Liu. He is a software engineer intern and today we are presenting this presentation", "tokens": [18056, 13, 634, 307, 257, 4722, 11403, 2154, 293, 965, 321, 366, 15578, 341, 5860], "temperature": 0.0, "avg_logprob": -0.24345163755779026, "compression_ratio": 1.5825688073394495, "no_speech_prob": 0.17690393328666687}, {"id": 4, "seek": 2892, "start": 28.92, "end": 35.32, "text": " together. So we are part of CNCF and we are taking community-based initiatives on", "tokens": [1214, 13, 407, 321, 366, 644, 295, 14589, 34, 37, 293, 321, 366, 1940, 1768, 12, 6032, 16194, 322], "temperature": 0.0, "avg_logprob": -0.17788710492722531, "compression_ratio": 1.5737051792828685, "no_speech_prob": 8.210660598706454e-05}, {"id": 5, "seek": 2892, "start": 35.32, "end": 40.72, "text": " environment sustainability. If you want to check our proposal, you can follow the", "tokens": [2823, 16360, 13, 759, 291, 528, 281, 1520, 527, 11494, 11, 291, 393, 1524, 264], "temperature": 0.0, "avg_logprob": -0.17788710492722531, "compression_ratio": 1.5737051792828685, "no_speech_prob": 8.210660598706454e-05}, {"id": 6, "seek": 2892, "start": 40.72, "end": 48.72, "text": " link. We also have done a few projects. Again, using community-based approach, the", "tokens": [2113, 13, 492, 611, 362, 1096, 257, 1326, 4455, 13, 3764, 11, 1228, 1768, 12, 6032, 3109, 11, 264], "temperature": 0.0, "avg_logprob": -0.17788710492722531, "compression_ratio": 1.5737051792828685, "no_speech_prob": 8.210660598706454e-05}, {"id": 7, "seek": 2892, "start": 48.72, "end": 52.88, "text": " first one of them is carbon aware scaling with KEDA. We did this with", "tokens": [700, 472, 295, 552, 307, 5954, 3650, 21589, 365, 591, 4731, 32, 13, 492, 630, 341, 365], "temperature": 0.0, "avg_logprob": -0.17788710492722531, "compression_ratio": 1.5737051792828685, "no_speech_prob": 8.210660598706454e-05}, {"id": 8, "seek": 2892, "start": 52.88, "end": 58.52, "text": " Microsoft and we investigated how you can use electricity and carbon intensity", "tokens": [8116, 293, 321, 30070, 577, 291, 393, 764, 10356, 293, 5954, 13749], "temperature": 0.0, "avg_logprob": -0.17788710492722531, "compression_ratio": 1.5737051792828685, "no_speech_prob": 8.210660598706454e-05}, {"id": 9, "seek": 5852, "start": 58.52, "end": 62.720000000000006, "text": " to make workload scaling decisions. Another one that we've been working with", "tokens": [281, 652, 20139, 21589, 5327, 13, 3996, 472, 300, 321, 600, 668, 1364, 365], "temperature": 0.0, "avg_logprob": -0.22391327305843955, "compression_ratio": 1.4861660079051384, "no_speech_prob": 1.5682709999964572e-05}, {"id": 10, "seek": 5852, "start": 62.720000000000006, "end": 68.8, "text": " IBM Research is Clever. That is container, label, energy efficient, VP, a", "tokens": [23487, 10303, 307, 8834, 331, 13, 663, 307, 10129, 11, 7645, 11, 2281, 7148, 11, 35812, 11, 257], "temperature": 0.0, "avg_logprob": -0.22391327305843955, "compression_ratio": 1.4861660079051384, "no_speech_prob": 1.5682709999964572e-05}, {"id": 11, "seek": 5852, "start": 68.8, "end": 73.56, "text": " recommender for Kubernetes. And if you want to check out both of these", "tokens": [2748, 260, 337, 23145, 13, 400, 498, 291, 528, 281, 1520, 484, 1293, 295, 613], "temperature": 0.0, "avg_logprob": -0.22391327305843955, "compression_ratio": 1.4861660079051384, "no_speech_prob": 1.5682709999964572e-05}, {"id": 12, "seek": 5852, "start": 73.56, "end": 80.2, "text": " projects, you can just follow the QR. So the agenda is very simple. We'll give a", "tokens": [4455, 11, 291, 393, 445, 1524, 264, 32784, 13, 407, 264, 9829, 307, 588, 2199, 13, 492, 603, 976, 257], "temperature": 0.0, "avg_logprob": -0.22391327305843955, "compression_ratio": 1.4861660079051384, "no_speech_prob": 1.5682709999964572e-05}, {"id": 13, "seek": 5852, "start": 80.2, "end": 86.60000000000001, "text": " brief background of the things, how they are at the moment and then we're", "tokens": [5353, 3678, 295, 264, 721, 11, 577, 436, 366, 412, 264, 1623, 293, 550, 321, 434], "temperature": 0.0, "avg_logprob": -0.22391327305843955, "compression_ratio": 1.4861660079051384, "no_speech_prob": 1.5682709999964572e-05}, {"id": 14, "seek": 8660, "start": 86.6, "end": 92.91999999999999, "text": " going to introduce a sustainability stack which consists of two projects, the", "tokens": [516, 281, 5366, 257, 16360, 8630, 597, 14689, 295, 732, 4455, 11, 264], "temperature": 0.0, "avg_logprob": -0.14770126342773438, "compression_ratio": 1.645021645021645, "no_speech_prob": 2.3536778826382942e-05}, {"id": 15, "seek": 8660, "start": 92.91999999999999, "end": 98.39999999999999, "text": " Kepler and the Model Server and then we will have a demo. So here we have an", "tokens": [3189, 22732, 293, 264, 17105, 25684, 293, 550, 321, 486, 362, 257, 10723, 13, 407, 510, 321, 362, 364], "temperature": 0.0, "avg_logprob": -0.14770126342773438, "compression_ratio": 1.645021645021645, "no_speech_prob": 2.3536778826382942e-05}, {"id": 16, "seek": 8660, "start": 98.39999999999999, "end": 102.16, "text": " interesting quote that sums up the motivation of our sustainability stack", "tokens": [1880, 6513, 300, 34499, 493, 264, 12335, 295, 527, 16360, 8630], "temperature": 0.0, "avg_logprob": -0.14770126342773438, "compression_ratio": 1.645021645021645, "no_speech_prob": 2.3536778826382942e-05}, {"id": 17, "seek": 8660, "start": 102.16, "end": 108.16, "text": " and the problem it seeks to solve. So according to Gardner, in 2021 an ACM", "tokens": [293, 264, 1154, 309, 28840, 281, 5039, 13, 407, 4650, 281, 12882, 1193, 11, 294, 7201, 364, 8157, 44], "temperature": 0.0, "avg_logprob": -0.14770126342773438, "compression_ratio": 1.645021645021645, "no_speech_prob": 2.3536778826382942e-05}, {"id": 18, "seek": 8660, "start": 108.16, "end": 112.44, "text": " technology brief estimated that the information and communication technology", "tokens": [2899, 5353, 14109, 300, 264, 1589, 293, 6101, 2899], "temperature": 0.0, "avg_logprob": -0.14770126342773438, "compression_ratio": 1.645021645021645, "no_speech_prob": 2.3536778826382942e-05}, {"id": 19, "seek": 11244, "start": 112.44, "end": 119.39999999999999, "text": " sector contributed between 1.8% and 3.9% of global carbon emissions, which is", "tokens": [6977, 18434, 1296, 502, 13, 23, 4, 293, 805, 13, 24, 4, 295, 4338, 5954, 14607, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1413520336151123, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012719465303234756}, {"id": 20, "seek": 11244, "start": 119.39999999999999, "end": 124.2, "text": " astonishingly more than the CO2 emission contributions of both Germany and", "tokens": [35264, 356, 544, 813, 264, 3002, 17, 29513, 15725, 295, 1293, 7244, 293], "temperature": 0.0, "avg_logprob": -0.1413520336151123, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012719465303234756}, {"id": 21, "seek": 11244, "start": 124.2, "end": 129.76, "text": " Italy combined. The significant carbon footprint and significant energy", "tokens": [10705, 9354, 13, 440, 4776, 5954, 24222, 293, 4776, 2281], "temperature": 0.0, "avg_logprob": -0.1413520336151123, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012719465303234756}, {"id": 22, "seek": 11244, "start": 129.76, "end": 135.2, "text": " consumption of the tech industry begs the following questions. How can we measure", "tokens": [12126, 295, 264, 7553, 3518, 4612, 82, 264, 3480, 1651, 13, 1012, 393, 321, 3481], "temperature": 0.0, "avg_logprob": -0.1413520336151123, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012719465303234756}, {"id": 23, "seek": 11244, "start": 135.2, "end": 139.44, "text": " energy consumption quickly and indirectly? How can we measure energy", "tokens": [2281, 12126, 2661, 293, 37779, 30, 1012, 393, 321, 3481, 2281], "temperature": 0.0, "avg_logprob": -0.1413520336151123, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00012719465303234756}, {"id": 24, "seek": 13944, "start": 139.44, "end": 143.92, "text": " consumption of workloads? And how can we then attribute power on shared", "tokens": [12126, 295, 32452, 30, 400, 577, 393, 321, 550, 19667, 1347, 322, 5507], "temperature": 0.0, "avg_logprob": -0.1515780289967855, "compression_ratio": 1.5648535564853556, "no_speech_prob": 4.002719288109802e-05}, {"id": 25, "seek": 13944, "start": 143.92, "end": 150.68, "text": " resources to processes, containers and pods? So with these issues in mind, we", "tokens": [3593, 281, 7555, 11, 17089, 293, 31925, 30, 407, 365, 613, 2663, 294, 1575, 11, 321], "temperature": 0.0, "avg_logprob": -0.1515780289967855, "compression_ratio": 1.5648535564853556, "no_speech_prob": 4.002719288109802e-05}, {"id": 26, "seek": 13944, "start": 150.68, "end": 154.72, "text": " introduced a cloud-native sustainability stack which seeks to address these", "tokens": [7268, 257, 4588, 12, 77, 1166, 16360, 8630, 597, 28840, 281, 2985, 613], "temperature": 0.0, "avg_logprob": -0.1515780289967855, "compression_ratio": 1.5648535564853556, "no_speech_prob": 4.002719288109802e-05}, {"id": 27, "seek": 13944, "start": 154.72, "end": 158.92, "text": " questions and problems. Perule will first start by discussing the Kepler", "tokens": [1651, 293, 2740, 13, 3026, 2271, 486, 700, 722, 538, 10850, 264, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.1515780289967855, "compression_ratio": 1.5648535564853556, "no_speech_prob": 4.002719288109802e-05}, {"id": 28, "seek": 13944, "start": 158.92, "end": 163.88, "text": " project and then I will discuss the Kepler Model Server project. Let's talk", "tokens": [1716, 293, 550, 286, 486, 2248, 264, 3189, 22732, 17105, 25684, 1716, 13, 961, 311, 751], "temperature": 0.0, "avg_logprob": -0.1515780289967855, "compression_ratio": 1.5648535564853556, "no_speech_prob": 4.002719288109802e-05}, {"id": 29, "seek": 16388, "start": 163.88, "end": 170.76, "text": " about the energy consumption attribution methodology used by the Kepler. What", "tokens": [466, 264, 2281, 12126, 9080, 1448, 24850, 1143, 538, 264, 3189, 22732, 13, 708], "temperature": 0.0, "avg_logprob": -0.13428583912465764, "compression_ratio": 1.738532110091743, "no_speech_prob": 1.891516149044037e-05}, {"id": 30, "seek": 16388, "start": 170.76, "end": 175.92, "text": " Kepler is based on the principle that power consumption is attributed to the", "tokens": [3189, 22732, 307, 2361, 322, 264, 8665, 300, 1347, 12126, 307, 30976, 281, 264], "temperature": 0.0, "avg_logprob": -0.13428583912465764, "compression_ratio": 1.738532110091743, "no_speech_prob": 1.891516149044037e-05}, {"id": 31, "seek": 16388, "start": 175.92, "end": 180.51999999999998, "text": " resource usage by process containers and pods. For example, let's say you have a", "tokens": [7684, 14924, 538, 1399, 17089, 293, 31925, 13, 1171, 1365, 11, 718, 311, 584, 291, 362, 257], "temperature": 0.0, "avg_logprob": -0.13428583912465764, "compression_ratio": 1.738532110091743, "no_speech_prob": 1.891516149044037e-05}, {"id": 32, "seek": 16388, "start": 180.51999999999998, "end": 186.0, "text": " pod that consumed 10% of CPU, that means it attributed to 10% of CPU power", "tokens": [2497, 300, 21226, 1266, 4, 295, 13199, 11, 300, 1355, 309, 30976, 281, 1266, 4, 295, 13199, 1347], "temperature": 0.0, "avg_logprob": -0.13428583912465764, "compression_ratio": 1.738532110091743, "no_speech_prob": 1.891516149044037e-05}, {"id": 33, "seek": 16388, "start": 186.0, "end": 190.56, "text": " consumption. Similar if you have like five containers and they total", "tokens": [12126, 13, 10905, 498, 291, 362, 411, 1732, 17089, 293, 436, 3217], "temperature": 0.0, "avg_logprob": -0.13428583912465764, "compression_ratio": 1.738532110091743, "no_speech_prob": 1.891516149044037e-05}, {"id": 34, "seek": 19056, "start": 190.56, "end": 196.84, "text": " contributed to 50% of CPU usage, that means they attributed to 50% of CPU power", "tokens": [18434, 281, 2625, 4, 295, 13199, 14924, 11, 300, 1355, 436, 30976, 281, 2625, 4, 295, 13199, 1347], "temperature": 0.0, "avg_logprob": -0.245917139174063, "compression_ratio": 1.5144230769230769, "no_speech_prob": 2.3539374524261802e-05}, {"id": 35, "seek": 19056, "start": 196.84, "end": 206.88, "text": " consumption. It is so on and so forth for other resources like memory and GPU", "tokens": [12126, 13, 467, 307, 370, 322, 293, 370, 5220, 337, 661, 3593, 411, 4675, 293, 18407], "temperature": 0.0, "avg_logprob": -0.245917139174063, "compression_ratio": 1.5144230769230769, "no_speech_prob": 2.3539374524261802e-05}, {"id": 36, "seek": 19056, "start": 206.88, "end": 212.72, "text": " etc. And this we based this principle based on the studies and we have", "tokens": [5183, 13, 400, 341, 321, 2361, 341, 8665, 2361, 322, 264, 5313, 293, 321, 362], "temperature": 0.0, "avg_logprob": -0.245917139174063, "compression_ratio": 1.5144230769230769, "no_speech_prob": 2.3539374524261802e-05}, {"id": 37, "seek": 19056, "start": 212.72, "end": 220.28, "text": " attached the link to the paper. If you're interested you can check that out. So Kepler", "tokens": [8570, 264, 2113, 281, 264, 3035, 13, 759, 291, 434, 3102, 291, 393, 1520, 300, 484, 13, 407, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.245917139174063, "compression_ratio": 1.5144230769230769, "no_speech_prob": 2.3539374524261802e-05}, {"id": 38, "seek": 22028, "start": 220.28, "end": 226.84, "text": " is a Kubernetes-based efficient power level exporter and it uses software", "tokens": [307, 257, 23145, 12, 6032, 7148, 1347, 1496, 1278, 6122, 293, 309, 4960, 4722], "temperature": 0.0, "avg_logprob": -0.16065981247845817, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.0229131880332716e-05}, {"id": 39, "seek": 22028, "start": 226.84, "end": 231.2, "text": " counters to measure power consumption by hardware resources and exports them as", "tokens": [39338, 281, 3481, 1347, 12126, 538, 8837, 3593, 293, 31428, 552, 382], "temperature": 0.0, "avg_logprob": -0.16065981247845817, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.0229131880332716e-05}, {"id": 40, "seek": 22028, "start": 231.2, "end": 236.72, "text": " Prometheus metrics. Kepler does three things. The first is reporting. It", "tokens": [2114, 649, 42209, 16367, 13, 3189, 22732, 775, 1045, 721, 13, 440, 700, 307, 10031, 13, 467], "temperature": 0.0, "avg_logprob": -0.16065981247845817, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.0229131880332716e-05}, {"id": 41, "seek": 22028, "start": 236.72, "end": 243.08, "text": " reports per pod level energy consumption including resources like CPU, GPU and RAM", "tokens": [7122, 680, 2497, 1496, 2281, 12126, 3009, 3593, 411, 13199, 11, 18407, 293, 14561], "temperature": 0.0, "avg_logprob": -0.16065981247845817, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.0229131880332716e-05}, {"id": 42, "seek": 22028, "start": 243.08, "end": 249.2, "text": " and it supports bare metal as well as PM. So you can measure your workloads", "tokens": [293, 309, 9346, 6949, 5760, 382, 731, 382, 12499, 13, 407, 291, 393, 3481, 428, 32452], "temperature": 0.0, "avg_logprob": -0.16065981247845817, "compression_ratio": 1.5975103734439835, "no_speech_prob": 3.0229131880332716e-05}, {"id": 43, "seek": 24920, "start": 249.2, "end": 258.76, "text": " energy consumption even on AWS or Azure etc. And it supports Prometheus for", "tokens": [2281, 12126, 754, 322, 17650, 420, 11969, 5183, 13, 400, 309, 9346, 2114, 649, 42209, 337], "temperature": 0.0, "avg_logprob": -0.13640592037103114, "compression_ratio": 1.4855769230769231, "no_speech_prob": 1.0286344149790239e-05}, {"id": 44, "seek": 24920, "start": 258.76, "end": 266.71999999999997, "text": " exporting the metrics and you can see the dashboards using Grafana. It's very", "tokens": [44686, 264, 16367, 293, 291, 393, 536, 264, 8240, 17228, 1228, 8985, 69, 2095, 13, 467, 311, 588], "temperature": 0.0, "avg_logprob": -0.13640592037103114, "compression_ratio": 1.4855769230769231, "no_speech_prob": 1.0286344149790239e-05}, {"id": 45, "seek": 24920, "start": 266.71999999999997, "end": 271.64, "text": " important that Kepler has low energy footprint because what we're trying to", "tokens": [1021, 300, 3189, 22732, 575, 2295, 2281, 24222, 570, 437, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.13640592037103114, "compression_ratio": 1.4855769230769231, "no_speech_prob": 1.0286344149790239e-05}, {"id": 46, "seek": 24920, "start": 271.64, "end": 278.24, "text": " do is measure. So we don't want to have Kepler consuming a lot of power itself.", "tokens": [360, 307, 3481, 13, 407, 321, 500, 380, 528, 281, 362, 3189, 22732, 19867, 257, 688, 295, 1347, 2564, 13], "temperature": 0.0, "avg_logprob": -0.13640592037103114, "compression_ratio": 1.4855769230769231, "no_speech_prob": 1.0286344149790239e-05}, {"id": 47, "seek": 27824, "start": 278.24, "end": 284.40000000000003, "text": " So we used EBPF to probe the counters and this considerably reduced the", "tokens": [407, 321, 1143, 50148, 47, 37, 281, 22715, 264, 39338, 293, 341, 31308, 9212, 264], "temperature": 0.0, "avg_logprob": -0.12234153210277289, "compression_ratio": 1.5255102040816326, "no_speech_prob": 1.1840369552373886e-05}, {"id": 48, "seek": 27824, "start": 284.40000000000003, "end": 290.68, "text": " computational resource used by Kepler. And at last we support ML models to", "tokens": [28270, 7684, 1143, 538, 3189, 22732, 13, 400, 412, 1036, 321, 1406, 21601, 5245, 281], "temperature": 0.0, "avg_logprob": -0.12234153210277289, "compression_ratio": 1.5255102040816326, "no_speech_prob": 1.1840369552373886e-05}, {"id": 49, "seek": 27824, "start": 290.68, "end": 297.0, "text": " estimate energy consumption when you don't have a power meter and Kai will", "tokens": [12539, 2281, 12126, 562, 291, 500, 380, 362, 257, 1347, 9255, 293, 20753, 486], "temperature": 0.0, "avg_logprob": -0.12234153210277289, "compression_ratio": 1.5255102040816326, "no_speech_prob": 1.1840369552373886e-05}, {"id": 50, "seek": 27824, "start": 297.0, "end": 306.04, "text": " talk more about it in the Kepler model server portion but we use ML models to", "tokens": [751, 544, 466, 309, 294, 264, 3189, 22732, 2316, 7154, 8044, 457, 321, 764, 21601, 5245, 281], "temperature": 0.0, "avg_logprob": -0.12234153210277289, "compression_ratio": 1.5255102040816326, "no_speech_prob": 1.1840369552373886e-05}, {"id": 51, "seek": 30604, "start": 306.04, "end": 313.08000000000004, "text": " predict the energy consumption when inherent power meter is not available.", "tokens": [6069, 264, 2281, 12126, 562, 26387, 1347, 9255, 307, 406, 2435, 13], "temperature": 0.0, "avg_logprob": -0.1426772381885942, "compression_ratio": 1.5948275862068966, "no_speech_prob": 1.4063028174859937e-05}, {"id": 52, "seek": 30604, "start": 313.08000000000004, "end": 318.6, "text": " The second part of the sustainability stack is the Kepler model server. So by", "tokens": [440, 1150, 644, 295, 264, 16360, 8630, 307, 264, 3189, 22732, 2316, 7154, 13, 407, 538], "temperature": 0.0, "avg_logprob": -0.1426772381885942, "compression_ratio": 1.5948275862068966, "no_speech_prob": 1.4063028174859937e-05}, {"id": 53, "seek": 30604, "start": 318.6, "end": 323.08000000000004, "text": " default Kepler will use a supported power measurement tool or meter to measure", "tokens": [7576, 3189, 22732, 486, 764, 257, 8104, 1347, 13160, 2290, 420, 9255, 281, 3481], "temperature": 0.0, "avg_logprob": -0.1426772381885942, "compression_ratio": 1.5948275862068966, "no_speech_prob": 1.4063028174859937e-05}, {"id": 54, "seek": 30604, "start": 323.08000000000004, "end": 328.40000000000003, "text": " node-related energy metrics like CPU core, DRAM and then they", "tokens": [9984, 12, 12004, 2281, 16367, 411, 13199, 4965, 11, 12118, 2865, 293, 550, 436], "temperature": 0.0, "avg_logprob": -0.1426772381885942, "compression_ratio": 1.5948275862068966, "no_speech_prob": 1.4063028174859937e-05}, {"id": 55, "seek": 30604, "start": 328.40000000000003, "end": 334.0, "text": " uses this to estimate pod level energy metrics. But what happens when Kepler", "tokens": [4960, 341, 281, 12539, 2497, 1496, 2281, 16367, 13, 583, 437, 2314, 562, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.1426772381885942, "compression_ratio": 1.5948275862068966, "no_speech_prob": 1.4063028174859937e-05}, {"id": 56, "seek": 33400, "start": 334.0, "end": 338.6, "text": " does not have access to a supported power meter? This is where the Kepler model", "tokens": [775, 406, 362, 2105, 281, 257, 8104, 1347, 9255, 30, 639, 307, 689, 264, 3189, 22732, 2316], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 57, "seek": 33400, "start": 338.6, "end": 342.56, "text": " server steps in to provide trained models that use software counters and", "tokens": [7154, 4439, 294, 281, 2893, 8895, 5245, 300, 764, 4722, 39338, 293], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 58, "seek": 33400, "start": 342.56, "end": 347.64, "text": " performance metrics to predict relevant energy metrics. The tech stack of the", "tokens": [3389, 16367, 281, 6069, 7340, 2281, 16367, 13, 440, 7553, 8630, 295, 264], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 59, "seek": 33400, "start": 347.64, "end": 352.52, "text": " Kepler model server also includes TensorFlow Keras, Psychic, Flask and", "tokens": [3189, 22732, 2316, 7154, 611, 5974, 37624, 591, 6985, 11, 17303, 299, 11, 3235, 3863, 293], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 60, "seek": 33400, "start": 352.52, "end": 358.48, "text": " Prometheus. So let's take a look at some of the models the Kepler model server has", "tokens": [2114, 649, 42209, 13, 407, 718, 311, 747, 257, 574, 412, 512, 295, 264, 5245, 264, 3189, 22732, 2316, 7154, 575], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 61, "seek": 33400, "start": 358.48, "end": 363.24, "text": " implemented. For example, we have a linear regression model that predicts node", "tokens": [12270, 13, 1171, 1365, 11, 321, 362, 257, 8213, 24590, 2316, 300, 6069, 82, 9984], "temperature": 0.0, "avg_logprob": -0.14406262503729927, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.0144294467172585e-05}, {"id": 62, "seek": 36324, "start": 363.24, "end": 368.24, "text": " level CPU core energy consumption with the following categorical and normalized", "tokens": [1496, 13199, 4965, 2281, 12126, 365, 264, 3480, 19250, 804, 293, 48704], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 63, "seek": 36324, "start": 368.24, "end": 372.52, "text": " numerical software counters and performance metrics. This model also", "tokens": [29054, 4722, 39338, 293, 3389, 16367, 13, 639, 2316, 611], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 64, "seek": 36324, "start": 372.52, "end": 377.36, "text": " supports incremental learning, incremental training on new batches of data to", "tokens": [9346, 35759, 2539, 11, 35759, 3097, 322, 777, 15245, 279, 295, 1412, 281], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 65, "seek": 36324, "start": 377.36, "end": 382.68, "text": " improve the model's performance on a cluster. The second example also provides", "tokens": [3470, 264, 2316, 311, 3389, 322, 257, 13630, 13, 440, 1150, 1365, 611, 6417], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 66, "seek": 36324, "start": 382.68, "end": 386.64, "text": " a linear regression model capable of online learning but it instead predicts", "tokens": [257, 8213, 24590, 2316, 8189, 295, 2950, 2539, 457, 309, 2602, 6069, 82], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 67, "seek": 36324, "start": 386.64, "end": 391.32, "text": " node level DRAM energy consumption with the following software counters and", "tokens": [9984, 1496, 12118, 2865, 2281, 12126, 365, 264, 3480, 4722, 39338, 293], "temperature": 0.0, "avg_logprob": -0.14262246001850476, "compression_ratio": 1.7821011673151752, "no_speech_prob": 1.520469413662795e-05}, {"id": 68, "seek": 39132, "start": 391.32, "end": 397.52, "text": " performance metrics. So let's take a look at how the model server fits in Kepler", "tokens": [3389, 16367, 13, 407, 718, 311, 747, 257, 574, 412, 577, 264, 2316, 7154, 9001, 294, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.09428816163138057, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.9831848476314917e-05}, {"id": 69, "seek": 39132, "start": 397.52, "end": 402.48, "text": " as a whole. So the first part is training our models on a variety of training", "tokens": [382, 257, 1379, 13, 407, 264, 700, 644, 307, 3097, 527, 5245, 322, 257, 5673, 295, 3097], "temperature": 0.0, "avg_logprob": -0.09428816163138057, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.9831848476314917e-05}, {"id": 70, "seek": 39132, "start": 402.48, "end": 406.96, "text": " workloads where Kepler can export node energy metrics and performance metrics", "tokens": [32452, 689, 3189, 22732, 393, 10725, 9984, 2281, 16367, 293, 3389, 16367], "temperature": 0.0, "avg_logprob": -0.09428816163138057, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.9831848476314917e-05}, {"id": 71, "seek": 39132, "start": 406.96, "end": 412.52, "text": " because a power meter is present. In this case Kepler retrieves these node energy", "tokens": [570, 257, 1347, 9255, 307, 1974, 13, 682, 341, 1389, 3189, 22732, 19817, 977, 613, 9984, 2281], "temperature": 0.0, "avg_logprob": -0.09428816163138057, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.9831848476314917e-05}, {"id": 72, "seek": 39132, "start": 412.52, "end": 417.03999999999996, "text": " metrics from agents which are then collected and exported as Prometheus", "tokens": [16367, 490, 12554, 597, 366, 550, 11087, 293, 42055, 382, 2114, 649, 42209], "temperature": 0.0, "avg_logprob": -0.09428816163138057, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.9831848476314917e-05}, {"id": 73, "seek": 41704, "start": 417.04, "end": 423.72, "text": " metrics. The model server scrapes these Prometheus metrics, sets up training,", "tokens": [16367, 13, 440, 2316, 7154, 23138, 279, 613, 2114, 649, 42209, 16367, 11, 6352, 493, 3097, 11], "temperature": 0.0, "avg_logprob": -0.10891382590584132, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2122423615655862e-05}, {"id": 74, "seek": 41704, "start": 423.72, "end": 429.08000000000004, "text": " testing and validation data sets and then trains, evaluates and saves the model", "tokens": [4997, 293, 24071, 1412, 6352, 293, 550, 16329, 11, 6133, 1024, 293, 19155, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10891382590584132, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2122423615655862e-05}, {"id": 75, "seek": 41704, "start": 429.08000000000004, "end": 434.92, "text": " with the new data. The second part is now exporting these trained models to Kepler", "tokens": [365, 264, 777, 1412, 13, 440, 1150, 644, 307, 586, 44686, 613, 8895, 5245, 281, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.10891382590584132, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2122423615655862e-05}, {"id": 76, "seek": 41704, "start": 434.92, "end": 440.04, "text": " for prediction whenever a power meter is not provided. The Kepler model server", "tokens": [337, 17630, 5699, 257, 1347, 9255, 307, 406, 5649, 13, 440, 3189, 22732, 2316, 7154], "temperature": 0.0, "avg_logprob": -0.10891382590584132, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2122423615655862e-05}, {"id": 77, "seek": 41704, "start": 440.04, "end": 443.96000000000004, "text": " can export the model itself as an archive to Kepler and this is done with", "tokens": [393, 10725, 264, 2316, 2564, 382, 364, 23507, 281, 3189, 22732, 293, 341, 307, 1096, 365], "temperature": 0.0, "avg_logprob": -0.10891382590584132, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2122423615655862e-05}, {"id": 78, "seek": 44396, "start": 443.96, "end": 448.96, "text": " flash grouts. The model server can also export the model's weights directly", "tokens": [7319, 677, 7711, 13, 440, 2316, 7154, 393, 611, 10725, 264, 2316, 311, 17443, 3838], "temperature": 0.0, "avg_logprob": -0.14242559998900026, "compression_ratio": 1.6637554585152838, "no_speech_prob": 2.178021168219857e-05}, {"id": 79, "seek": 44396, "start": 448.96, "end": 454.15999999999997, "text": " using flash grouts and or Prometheus metrics. In the future we will also like", "tokens": [1228, 7319, 677, 7711, 293, 420, 2114, 649, 42209, 16367, 13, 682, 264, 2027, 321, 486, 611, 411], "temperature": 0.0, "avg_logprob": -0.14242559998900026, "compression_ratio": 1.6637554585152838, "no_speech_prob": 2.178021168219857e-05}, {"id": 80, "seek": 44396, "start": 454.15999999999997, "end": 460.76, "text": " to export the model weights using the open telemetry metrics API. Now that we", "tokens": [281, 10725, 264, 2316, 17443, 1228, 264, 1269, 4304, 5537, 627, 16367, 9362, 13, 823, 300, 321], "temperature": 0.0, "avg_logprob": -0.14242559998900026, "compression_ratio": 1.6637554585152838, "no_speech_prob": 2.178021168219857e-05}, {"id": 81, "seek": 44396, "start": 460.76, "end": 464.44, "text": " have talked about sustainability stack let's see how you can do carbon", "tokens": [362, 2825, 466, 16360, 8630, 718, 311, 536, 577, 291, 393, 360, 5954], "temperature": 0.0, "avg_logprob": -0.14242559998900026, "compression_ratio": 1.6637554585152838, "no_speech_prob": 2.178021168219857e-05}, {"id": 82, "seek": 44396, "start": 464.44, "end": 470.91999999999996, "text": " intensity aware scheduling. So the use case that we are trying to solve is can", "tokens": [13749, 3650, 29055, 13, 407, 264, 764, 1389, 300, 321, 366, 1382, 281, 5039, 307, 393], "temperature": 0.0, "avg_logprob": -0.14242559998900026, "compression_ratio": 1.6637554585152838, "no_speech_prob": 2.178021168219857e-05}, {"id": 83, "seek": 47092, "start": 470.92, "end": 475.36, "text": " you put a check or can you control the carbon intensity of your workload. For", "tokens": [291, 829, 257, 1520, 420, 393, 291, 1969, 264, 5954, 13749, 295, 428, 20139, 13, 1171], "temperature": 0.0, "avg_logprob": -0.13505081903366817, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.221477759827394e-05}, {"id": 84, "seek": 47092, "start": 475.36, "end": 480.76, "text": " example is it possible to fuel your workloads using renewable energy like", "tokens": [1365, 307, 309, 1944, 281, 6616, 428, 32452, 1228, 20938, 2281, 411], "temperature": 0.0, "avg_logprob": -0.13505081903366817, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.221477759827394e-05}, {"id": 85, "seek": 47092, "start": 480.76, "end": 486.28000000000003, "text": " solar power or wind power when available and switch to fossil fuel when the", "tokens": [7936, 1347, 420, 2468, 1347, 562, 2435, 293, 3679, 281, 18737, 6616, 562, 264], "temperature": 0.0, "avg_logprob": -0.13505081903366817, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.221477759827394e-05}, {"id": 86, "seek": 47092, "start": 486.28000000000003, "end": 492.64, "text": " renewable energy is not at disposal. So the use case premise is based on", "tokens": [20938, 2281, 307, 406, 412, 26400, 13, 407, 264, 764, 1389, 22045, 307, 2361, 322], "temperature": 0.0, "avg_logprob": -0.13505081903366817, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.221477759827394e-05}, {"id": 87, "seek": 47092, "start": 492.64, "end": 497.84000000000003, "text": " multi-node cluster where you have nodes in different geographical zones and the", "tokens": [4825, 12, 77, 1429, 13630, 689, 291, 362, 13891, 294, 819, 39872, 16025, 293, 264], "temperature": 0.0, "avg_logprob": -0.13505081903366817, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.221477759827394e-05}, {"id": 88, "seek": 49784, "start": 497.84, "end": 502.71999999999997, "text": " workloads that we will be talking about is long-running patch or machine", "tokens": [32452, 300, 321, 486, 312, 1417, 466, 307, 938, 12, 45482, 9972, 420, 3479], "temperature": 0.0, "avg_logprob": -0.1705773382475882, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.3547205273644067e-05}, {"id": 89, "seek": 49784, "start": 502.71999999999997, "end": 509.2, "text": " learning workloads that that keeps on retraining algorithm or any long-running", "tokens": [2539, 32452, 300, 300, 5965, 322, 49356, 1760, 9284, 420, 604, 938, 12, 45482], "temperature": 0.0, "avg_logprob": -0.1705773382475882, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.3547205273644067e-05}, {"id": 90, "seek": 49784, "start": 509.2, "end": 517.6, "text": " patch workloads that are not affected by rescheduling of that runs long enough", "tokens": [9972, 32452, 300, 366, 406, 8028, 538, 725, 19318, 425, 278, 295, 300, 6676, 938, 1547], "temperature": 0.0, "avg_logprob": -0.1705773382475882, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.3547205273644067e-05}, {"id": 91, "seek": 49784, "start": 517.6, "end": 521.72, "text": " that have an impact to considerable impact on carbon intensity and they", "tokens": [300, 362, 364, 2712, 281, 24167, 2712, 322, 5954, 13749, 293, 436], "temperature": 0.0, "avg_logprob": -0.1705773382475882, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.3547205273644067e-05}, {"id": 92, "seek": 52172, "start": 521.72, "end": 529.0400000000001, "text": " don't they're not affected if you reschedule them on different nodes. So our", "tokens": [500, 380, 436, 434, 406, 8028, 498, 291, 725, 19318, 2271, 552, 322, 819, 13891, 13, 407, 527], "temperature": 0.0, "avg_logprob": -0.18867909011020456, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.799710485618562e-06}, {"id": 93, "seek": 52172, "start": 529.0400000000001, "end": 533.5600000000001, "text": " demo setup is based on OpenShift cluster and for monitoring we're using Prometheus.", "tokens": [10723, 8657, 307, 2361, 322, 7238, 7774, 2008, 13630, 293, 337, 11028, 321, 434, 1228, 2114, 649, 42209, 13], "temperature": 0.0, "avg_logprob": -0.18867909011020456, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.799710485618562e-06}, {"id": 94, "seek": 52172, "start": 533.5600000000001, "end": 539.3000000000001, "text": " We would be using features like chains, toleration and node selectors to", "tokens": [492, 576, 312, 1228, 4122, 411, 12626, 11, 11125, 399, 293, 9984, 3048, 830, 281], "temperature": 0.0, "avg_logprob": -0.18867909011020456, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.799710485618562e-06}, {"id": 95, "seek": 52172, "start": 539.3000000000001, "end": 544.88, "text": " orchestrate where the workload is going to run on which node and we will have a", "tokens": [14161, 4404, 689, 264, 20139, 307, 516, 281, 1190, 322, 597, 9984, 293, 321, 486, 362, 257], "temperature": 0.0, "avg_logprob": -0.18867909011020456, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.799710485618562e-06}, {"id": 96, "seek": 52172, "start": 544.88, "end": 549.48, "text": " carbon intensity forecaster that will forecast the carbon intensity of nodes", "tokens": [5954, 13749, 2091, 42640, 300, 486, 14330, 264, 5954, 13749, 295, 13891], "temperature": 0.0, "avg_logprob": -0.18867909011020456, "compression_ratio": 1.6455696202531647, "no_speech_prob": 8.799710485618562e-06}, {"id": 97, "seek": 54948, "start": 549.48, "end": 554.36, "text": " and for this demo we are only considering two Rs step that that means", "tokens": [293, 337, 341, 10723, 321, 366, 787, 8079, 732, 21643, 1823, 300, 300, 1355], "temperature": 0.0, "avg_logprob": -0.16163765091493906, "compression_ratio": 1.7557603686635945, "no_speech_prob": 2.97678434435511e-05}, {"id": 98, "seek": 54948, "start": 554.36, "end": 558.36, "text": " that a carbon intensity forecaster would predict what is the carbon intensity", "tokens": [300, 257, 5954, 13749, 2091, 42640, 576, 6069, 437, 307, 264, 5954, 13749], "temperature": 0.0, "avg_logprob": -0.16163765091493906, "compression_ratio": 1.7557603686635945, "no_speech_prob": 2.97678434435511e-05}, {"id": 99, "seek": 54948, "start": 558.36, "end": 563.4, "text": " for the next two hours. So let's first describe the carbon intensity forecaster.", "tokens": [337, 264, 958, 732, 2496, 13, 407, 718, 311, 700, 6786, 264, 5954, 13749, 2091, 42640, 13], "temperature": 0.0, "avg_logprob": -0.16163765091493906, "compression_ratio": 1.7557603686635945, "no_speech_prob": 2.97678434435511e-05}, {"id": 100, "seek": 54948, "start": 563.4, "end": 568.96, "text": " The forecaster has access to an exporter which scrapes time series carbon", "tokens": [440, 2091, 42640, 575, 2105, 281, 364, 1278, 6122, 597, 23138, 279, 565, 2638, 5954], "temperature": 0.0, "avg_logprob": -0.16163765091493906, "compression_ratio": 1.7557603686635945, "no_speech_prob": 2.97678434435511e-05}, {"id": 101, "seek": 54948, "start": 568.96, "end": 575.0, "text": " intensity data from numerous public APIs like electricity map or national grid", "tokens": [13749, 1412, 490, 12546, 1908, 21445, 411, 10356, 4471, 420, 4048, 10748], "temperature": 0.0, "avg_logprob": -0.16163765091493906, "compression_ratio": 1.7557603686635945, "no_speech_prob": 2.97678434435511e-05}, {"id": 102, "seek": 57500, "start": 575.0, "end": 580.28, "text": " and it then exports this data as Prometheus metrics. The forecaster will", "tokens": [293, 309, 550, 31428, 341, 1412, 382, 2114, 649, 42209, 16367, 13, 440, 2091, 42640, 486], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 103, "seek": 57500, "start": 580.28, "end": 584.92, "text": " then scrape the Prometheus metrics from the exporter and update its models for", "tokens": [550, 32827, 264, 2114, 649, 42209, 16367, 490, 264, 1278, 6122, 293, 5623, 1080, 5245, 337], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 104, "seek": 57500, "start": 584.92, "end": 589.24, "text": " each of the node with new time series data. In this demo we will have three", "tokens": [1184, 295, 264, 9984, 365, 777, 565, 2638, 1412, 13, 682, 341, 10723, 321, 486, 362, 1045], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 105, "seek": 57500, "start": 589.24, "end": 593.16, "text": " nodes so the forecaster will have individual models for each of the three", "tokens": [13891, 370, 264, 2091, 42640, 486, 362, 2609, 5245, 337, 1184, 295, 264, 1045], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 106, "seek": 57500, "start": 593.16, "end": 597.2, "text": " nodes which are in different zones of course. The carbon forecaster will then", "tokens": [13891, 597, 366, 294, 819, 16025, 295, 1164, 13, 440, 5954, 2091, 42640, 486, 550], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 107, "seek": 57500, "start": 597.2, "end": 601.24, "text": " provide a prediction of the carbon intensity of the desired region a few", "tokens": [2893, 257, 17630, 295, 264, 5954, 13749, 295, 264, 14721, 4458, 257, 1326], "temperature": 0.0, "avg_logprob": -0.07457889375232515, "compression_ratio": 2.0, "no_speech_prob": 3.762069172807969e-05}, {"id": 108, "seek": 60124, "start": 601.24, "end": 606.24, "text": " hours in advance. Note that the carbon intensity forecaster and exporter are", "tokens": [2496, 294, 7295, 13, 11633, 300, 264, 5954, 13749, 2091, 42640, 293, 1278, 6122, 366], "temperature": 0.0, "avg_logprob": -0.1436170384853701, "compression_ratio": 1.7616822429906542, "no_speech_prob": 6.009296339470893e-05}, {"id": 109, "seek": 60124, "start": 606.24, "end": 610.6, "text": " extendable interfaces this means the forecaster can implement many different", "tokens": [10101, 712, 28416, 341, 1355, 264, 2091, 42640, 393, 4445, 867, 819], "temperature": 0.0, "avg_logprob": -0.1436170384853701, "compression_ratio": 1.7616822429906542, "no_speech_prob": 6.009296339470893e-05}, {"id": 110, "seek": 60124, "start": 610.6, "end": 614.52, "text": " types of time series forecasting models and the exporter can scrape from many", "tokens": [3467, 295, 565, 2638, 44331, 5245, 293, 264, 1278, 6122, 393, 32827, 490, 867], "temperature": 0.0, "avg_logprob": -0.1436170384853701, "compression_ratio": 1.7616822429906542, "no_speech_prob": 6.009296339470893e-05}, {"id": 111, "seek": 60124, "start": 614.52, "end": 621.6800000000001, "text": " different carbon data APIs. So now that we have a carbon intensity forecaster", "tokens": [819, 5954, 1412, 21445, 13, 407, 586, 300, 321, 362, 257, 5954, 13749, 2091, 42640], "temperature": 0.0, "avg_logprob": -0.1436170384853701, "compression_ratio": 1.7616822429906542, "no_speech_prob": 6.009296339470893e-05}, {"id": 112, "seek": 60124, "start": 621.6800000000001, "end": 626.52, "text": " external applications like the Cron job will forecast the potential", "tokens": [8320, 5821, 411, 264, 383, 2044, 1691, 486, 14330, 264, 3995], "temperature": 0.0, "avg_logprob": -0.1436170384853701, "compression_ratio": 1.7616822429906542, "no_speech_prob": 6.009296339470893e-05}, {"id": 113, "seek": 62652, "start": 626.52, "end": 631.92, "text": " carbon intensity sometime into the future for each of the three nodes. The", "tokens": [5954, 13749, 15053, 666, 264, 2027, 337, 1184, 295, 264, 1045, 13891, 13, 440], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 114, "seek": 62652, "start": 631.92, "end": 636.84, "text": " Cron job does this by making an HTTP request to the carbon forecaster using", "tokens": [383, 2044, 1691, 775, 341, 538, 1455, 364, 33283, 5308, 281, 264, 5954, 2091, 42640, 1228], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 115, "seek": 62652, "start": 636.84, "end": 642.96, "text": " the get slash forecasted CIN point and each of the three nodes are then", "tokens": [264, 483, 17330, 14330, 292, 383, 1464, 935, 293, 1184, 295, 264, 1045, 13891, 366, 550], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 116, "seek": 62652, "start": 642.96, "end": 647.68, "text": " periodically assigned node labels depending on the carbon intensity. Red", "tokens": [38916, 13279, 9984, 16949, 5413, 322, 264, 5954, 13749, 13, 4477], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 117, "seek": 62652, "start": 647.68, "end": 651.56, "text": " stands for a relatively high carbon intensity yellow stands for a medium", "tokens": [7382, 337, 257, 7226, 1090, 5954, 13749, 5566, 7382, 337, 257, 6399], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 118, "seek": 62652, "start": 651.56, "end": 656.0, "text": " carbon intensity and green stands for a relatively low carbon intensity. So in", "tokens": [5954, 13749, 293, 3092, 7382, 337, 257, 7226, 2295, 5954, 13749, 13, 407, 294], "temperature": 0.0, "avg_logprob": -0.12000539130771283, "compression_ratio": 1.9605263157894737, "no_speech_prob": 4.398394230520353e-05}, {"id": 119, "seek": 65600, "start": 656.0, "end": 660.62, "text": " this example note 1 is for forecasted two hours in the future to have the", "tokens": [341, 1365, 3637, 502, 307, 337, 14330, 292, 732, 2496, 294, 264, 2027, 281, 362, 264], "temperature": 1.0, "avg_logprob": -0.2977892557779948, "compression_ratio": 2.294478527607362, "no_speech_prob": 2.0458999642869458e-05}, {"id": 120, "seek": 65600, "start": 660.62, "end": 667.24, "text": " highest carbon intensity so it is labeled red. Node 2 is forecasted", "tokens": [6343, 5954, 13749, 370, 309, 307, 21335, 2182, 13, 38640, 568, 307, 14330, 292], "temperature": 1.0, "avg_logprob": -0.2977892557779948, "compression_ratio": 2.294478527607362, "no_speech_prob": 2.0458999642869458e-05}, {"id": 121, "seek": 65600, "start": 667.24, "end": 670.88, "text": " two hours in the future to have a medium carbon intensity, so it is labeled", "tokens": [732, 2496, 294, 264, 2027, 281, 362, 257, 6399, 5954, 13749, 11, 370, 309, 307, 21335], "temperature": 1.0, "avg_logprob": -0.2977892557779948, "compression_ratio": 2.294478527607362, "no_speech_prob": 2.0458999642869458e-05}, {"id": 122, "seek": 65600, "start": 670.88, "end": 675.88, "text": " yellow and note 3 is forecasted two hours in the future to have the lowest", "tokens": [5566, 293, 3637, 805, 307, 14330, 292, 732, 2496, 294, 264, 2027, 281, 362, 264, 12437], "temperature": 1.0, "avg_logprob": -0.2977892557779948, "compression_ratio": 2.294478527607362, "no_speech_prob": 2.0458999642869458e-05}, {"id": 123, "seek": 65600, "start": 675.88, "end": 682.6, "text": " carbon intensity so it is labeled green. Now that you have assigned labels to the", "tokens": [5954, 13749, 370, 309, 307, 21335, 3092, 13, 823, 300, 291, 362, 13279, 16949, 281, 264], "temperature": 1.0, "avg_logprob": -0.2977892557779948, "compression_ratio": 2.294478527607362, "no_speech_prob": 2.0458999642869458e-05}, {"id": 124, "seek": 68260, "start": 682.6, "end": 688.84, "text": " node, it's on the pod to declare its intention that what kind of node it", "tokens": [9984, 11, 309, 311, 322, 264, 2497, 281, 19710, 1080, 7789, 300, 437, 733, 295, 9984, 309], "temperature": 0.0, "avg_logprob": -0.20018654365044136, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.0367400124669075}, {"id": 125, "seek": 68260, "start": 688.84, "end": 698.16, "text": " prefers and also what kind of node it does not prefer at all. So for example in", "tokens": [44334, 293, 611, 437, 733, 295, 9984, 309, 775, 406, 4382, 412, 439, 13, 407, 337, 1365, 294], "temperature": 0.0, "avg_logprob": -0.20018654365044136, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.0367400124669075}, {"id": 126, "seek": 68260, "start": 698.16, "end": 703.36, "text": " the pod yaml you specify node selector carbon intensity as green that means it", "tokens": [264, 2497, 288, 335, 75, 291, 16500, 9984, 23264, 1672, 5954, 13749, 382, 3092, 300, 1355, 309], "temperature": 0.0, "avg_logprob": -0.20018654365044136, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.0367400124669075}, {"id": 127, "seek": 68260, "start": 703.36, "end": 709.76, "text": " prefers nodes that have labels as carbon intensity green and you also have to add", "tokens": [44334, 13891, 300, 362, 16949, 382, 5954, 13749, 3092, 293, 291, 611, 362, 281, 909], "temperature": 0.0, "avg_logprob": -0.20018654365044136, "compression_ratio": 1.7784090909090908, "no_speech_prob": 0.0367400124669075}, {"id": 128, "seek": 70976, "start": 709.76, "end": 717.72, "text": " as a tolerations where you have to say that you don't have the toleration", "tokens": [382, 257, 11125, 763, 689, 291, 362, 281, 584, 300, 291, 500, 380, 362, 264, 11125, 399], "temperature": 0.0, "avg_logprob": -0.2338982866956042, "compression_ratio": 1.8, "no_speech_prob": 4.538085704552941e-05}, {"id": 129, "seek": 70976, "start": 717.72, "end": 722.96, "text": " effect no execute means that this pod does not have toleration to", "tokens": [1802, 572, 14483, 1355, 300, 341, 2497, 775, 406, 362, 11125, 399, 281], "temperature": 0.0, "avg_logprob": -0.2338982866956042, "compression_ratio": 1.8, "no_speech_prob": 4.538085704552941e-05}, {"id": 130, "seek": 70976, "start": 722.96, "end": 732.48, "text": " run on nodes that have been tainted as red. So if the scheduler will try to", "tokens": [1190, 322, 13891, 300, 362, 668, 256, 26278, 382, 2182, 13, 407, 498, 264, 12000, 260, 486, 853, 281], "temperature": 0.0, "avg_logprob": -0.2338982866956042, "compression_ratio": 1.8, "no_speech_prob": 4.538085704552941e-05}, {"id": 131, "seek": 70976, "start": 732.48, "end": 737.88, "text": " schedule this pod on node 1 that has label and taint potas red, this pod", "tokens": [7567, 341, 2497, 322, 9984, 502, 300, 575, 7645, 293, 256, 5114, 1847, 296, 2182, 11, 341, 2497], "temperature": 0.0, "avg_logprob": -0.2338982866956042, "compression_ratio": 1.8, "no_speech_prob": 4.538085704552941e-05}, {"id": 132, "seek": 73788, "start": 737.88, "end": 747.8, "text": " would evict within 5 seconds. So that's what the toleration second is for. So", "tokens": [576, 1073, 985, 1951, 1025, 3949, 13, 407, 300, 311, 437, 264, 11125, 399, 1150, 307, 337, 13, 407], "temperature": 0.0, "avg_logprob": -0.19618612454261308, "compression_ratio": 1.6329787234042554, "no_speech_prob": 1.0449807632539887e-05}, {"id": 133, "seek": 73788, "start": 747.8, "end": 756.04, "text": " let's see how this this looks like. So you have node 1 and there was that has", "tokens": [718, 311, 536, 577, 341, 341, 1542, 411, 13, 407, 291, 362, 9984, 502, 293, 456, 390, 300, 575], "temperature": 0.0, "avg_logprob": -0.19618612454261308, "compression_ratio": 1.6329787234042554, "no_speech_prob": 1.0449807632539887e-05}, {"id": 134, "seek": 73788, "start": 756.04, "end": 761.0, "text": " label that had label green now it's turning to red that means its carbon", "tokens": [7645, 300, 632, 7645, 3092, 586, 309, 311, 6246, 281, 2182, 300, 1355, 1080, 5954], "temperature": 0.0, "avg_logprob": -0.19618612454261308, "compression_ratio": 1.6329787234042554, "no_speech_prob": 1.0449807632539887e-05}, {"id": 135, "seek": 73788, "start": 761.0, "end": 766.76, "text": " intensity is increasing. So we will taint the node and we will apply the taint", "tokens": [13749, 307, 5662, 13, 407, 321, 486, 256, 5114, 264, 9984, 293, 321, 486, 3079, 264, 256, 5114], "temperature": 0.0, "avg_logprob": -0.19618612454261308, "compression_ratio": 1.6329787234042554, "no_speech_prob": 1.0449807632539887e-05}, {"id": 136, "seek": 76676, "start": 766.76, "end": 773.4, "text": " as carbon intensity red and no execute. So as soon as this taint is applied the", "tokens": [382, 5954, 13749, 2182, 293, 572, 14483, 13, 407, 382, 2321, 382, 341, 256, 5114, 307, 6456, 264], "temperature": 0.0, "avg_logprob": -0.09911003819218388, "compression_ratio": 1.7486033519553073, "no_speech_prob": 2.4297540221596137e-05}, {"id": 137, "seek": 76676, "start": 773.4, "end": 779.3199999999999, "text": " pod is evicted from node 1 and it's assigned to node 2 which has the carbon", "tokens": [2497, 307, 1073, 11254, 490, 9984, 502, 293, 309, 311, 13279, 281, 9984, 568, 597, 575, 264, 5954], "temperature": 0.0, "avg_logprob": -0.09911003819218388, "compression_ratio": 1.7486033519553073, "no_speech_prob": 2.4297540221596137e-05}, {"id": 138, "seek": 76676, "start": 779.3199999999999, "end": 784.56, "text": " intensity is changing from red to green and it has been tainted. So tainting the", "tokens": [13749, 307, 4473, 490, 2182, 281, 3092, 293, 309, 575, 668, 256, 26278, 13, 407, 256, 491, 783, 264], "temperature": 0.0, "avg_logprob": -0.09911003819218388, "compression_ratio": 1.7486033519553073, "no_speech_prob": 2.4297540221596137e-05}, {"id": 139, "seek": 76676, "start": 784.56, "end": 789.28, "text": " nodes ensures that pods are evicted by the nodes if pods have no tolerations", "tokens": [13891, 28111, 300, 31925, 366, 1073, 11254, 538, 264, 13891, 498, 31925, 362, 572, 11125, 763], "temperature": 0.0, "avg_logprob": -0.09911003819218388, "compression_ratio": 1.7486033519553073, "no_speech_prob": 2.4297540221596137e-05}, {"id": 140, "seek": 78928, "start": 789.28, "end": 796.8399999999999, "text": " for taint. So this is like the whole picture we have a carbon intensity", "tokens": [337, 256, 5114, 13, 407, 341, 307, 411, 264, 1379, 3036, 321, 362, 257, 5954, 13749], "temperature": 0.0, "avg_logprob": -0.23026384989420573, "compression_ratio": 1.6721311475409837, "no_speech_prob": 9.665054676588625e-06}, {"id": 141, "seek": 78928, "start": 796.8399999999999, "end": 801.8399999999999, "text": " exporter that queries the various public API to gather the carbon intensity", "tokens": [1278, 6122, 300, 24109, 264, 3683, 1908, 9362, 281, 5448, 264, 5954, 13749], "temperature": 0.0, "avg_logprob": -0.23026384989420573, "compression_ratio": 1.6721311475409837, "no_speech_prob": 9.665054676588625e-06}, {"id": 142, "seek": 78928, "start": 801.8399999999999, "end": 811.6, "text": " data and it exports them as Prometheus metrics. Now the node label and why is", "tokens": [1412, 293, 309, 31428, 552, 382, 2114, 649, 42209, 16367, 13, 823, 264, 9984, 7645, 293, 983, 307], "temperature": 0.0, "avg_logprob": -0.23026384989420573, "compression_ratio": 1.6721311475409837, "no_speech_prob": 9.665054676588625e-06}, {"id": 143, "seek": 78928, "start": 811.6, "end": 817.24, "text": " a cronchial what it does it queries a carbon intensity forecaster and it queries", "tokens": [257, 941, 266, 339, 831, 437, 309, 775, 309, 24109, 257, 5954, 13749, 2091, 42640, 293, 309, 24109], "temperature": 0.0, "avg_logprob": -0.23026384989420573, "compression_ratio": 1.6721311475409837, "no_speech_prob": 9.665054676588625e-06}, {"id": 144, "seek": 81724, "start": 817.24, "end": 821.36, "text": " in head of time what is going to be the carbon intensity of the various nodes and", "tokens": [294, 1378, 295, 565, 437, 307, 516, 281, 312, 264, 5954, 13749, 295, 264, 3683, 13891, 293], "temperature": 0.0, "avg_logprob": -0.16729463315477558, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.8406389901647344e-05}, {"id": 145, "seek": 81724, "start": 821.36, "end": 828.48, "text": " it patches the labels and taints based on the forecasted carbon intensity. So", "tokens": [309, 26531, 264, 16949, 293, 256, 19594, 2361, 322, 264, 14330, 292, 5954, 13749, 13, 407], "temperature": 0.0, "avg_logprob": -0.16729463315477558, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.8406389901647344e-05}, {"id": 146, "seek": 81724, "start": 828.48, "end": 835.08, "text": " let's get to the demo. First I'm going to show you how you can install a Kepler", "tokens": [718, 311, 483, 281, 264, 10723, 13, 2386, 286, 478, 516, 281, 855, 291, 577, 291, 393, 3625, 257, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.16729463315477558, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.8406389901647344e-05}, {"id": 147, "seek": 81724, "start": 835.08, "end": 839.5600000000001, "text": " operator on an OpenShift environment. The first the release that we have right", "tokens": [12973, 322, 364, 7238, 7774, 2008, 2823, 13, 440, 700, 264, 4374, 300, 321, 362, 558], "temperature": 0.0, "avg_logprob": -0.16729463315477558, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.8406389901647344e-05}, {"id": 148, "seek": 81724, "start": 839.5600000000001, "end": 845.72, "text": " now is V1 alpha 1 and it has a prerequisite that it needs C group V2 and", "tokens": [586, 307, 691, 16, 8961, 502, 293, 309, 575, 257, 38333, 34152, 300, 309, 2203, 383, 1594, 691, 17, 293], "temperature": 0.0, "avg_logprob": -0.16729463315477558, "compression_ratio": 1.6024590163934427, "no_speech_prob": 2.8406389901647344e-05}, {"id": 149, "seek": 84572, "start": 845.72, "end": 852.44, "text": " it follows Kepler 0.4 release and it deploys Kepler both on Kubernetes and", "tokens": [309, 10002, 3189, 22732, 1958, 13, 19, 4374, 293, 309, 368, 49522, 3189, 22732, 1293, 322, 23145, 293], "temperature": 0.0, "avg_logprob": -0.15230302321605194, "compression_ratio": 1.6063829787234043, "no_speech_prob": 3.480396480881609e-05}, {"id": 150, "seek": 84572, "start": 852.44, "end": 857.0, "text": " OpenShift. So when you're deploying it on OpenShift it also reconfigure your", "tokens": [7238, 7774, 2008, 13, 407, 562, 291, 434, 34198, 309, 322, 7238, 7774, 2008, 309, 611, 9993, 20646, 540, 428], "temperature": 0.0, "avg_logprob": -0.15230302321605194, "compression_ratio": 1.6063829787234043, "no_speech_prob": 3.480396480881609e-05}, {"id": 151, "seek": 84572, "start": 857.0, "end": 865.28, "text": " OpenShift nodes by applying a machine config and SCC and right now Kepler", "tokens": [7238, 7774, 2008, 13891, 538, 9275, 257, 3479, 6662, 293, 9028, 34, 293, 558, 586, 3189, 22732], "temperature": 0.0, "avg_logprob": -0.15230302321605194, "compression_ratio": 1.6063829787234043, "no_speech_prob": 3.480396480881609e-05}, {"id": 152, "seek": 84572, "start": 865.28, "end": 871.36, "text": " uses local linear regression estimator in Kepler main container with offline", "tokens": [4960, 2654, 8213, 24590, 8017, 1639, 294, 3189, 22732, 2135, 10129, 365, 21857], "temperature": 0.0, "avg_logprob": -0.15230302321605194, "compression_ratio": 1.6063829787234043, "no_speech_prob": 3.480396480881609e-05}, {"id": 153, "seek": 87136, "start": 871.36, "end": 877.04, "text": " trained models but in the next release we are planning to provide end-to-end", "tokens": [8895, 5245, 457, 294, 264, 958, 4374, 321, 366, 5038, 281, 2893, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.11920595169067383, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.0239780244301073e-05}, {"id": 154, "seek": 87136, "start": 877.04, "end": 884.88, "text": " learning pipeline where it can train the model as well as use the model. So if", "tokens": [2539, 15517, 689, 309, 393, 3847, 264, 2316, 382, 731, 382, 764, 264, 2316, 13, 407, 498], "temperature": 0.0, "avg_logprob": -0.11920595169067383, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.0239780244301073e-05}, {"id": 155, "seek": 87136, "start": 884.88, "end": 891.2, "text": " you're interested in a code you can follow us on GitHub repository and so", "tokens": [291, 434, 3102, 294, 257, 3089, 291, 393, 1524, 505, 322, 23331, 25841, 293, 370], "temperature": 0.0, "avg_logprob": -0.11920595169067383, "compression_ratio": 1.4870129870129871, "no_speech_prob": 3.0239780244301073e-05}, {"id": 156, "seek": 89120, "start": 891.2, "end": 903.6400000000001, "text": " let's get to the demo. To deploy the operator go inside the Kepler operator", "tokens": [718, 311, 483, 281, 264, 10723, 13, 1407, 7274, 264, 12973, 352, 1854, 264, 3189, 22732, 12973], "temperature": 0.0, "avg_logprob": -0.1539982727595738, "compression_ratio": 1.6357142857142857, "no_speech_prob": 6.642458629357861e-06}, {"id": 157, "seek": 89120, "start": 903.6400000000001, "end": 911.9200000000001, "text": " project and run the make deploy that will create all the manifest and install", "tokens": [1716, 293, 1190, 264, 652, 7274, 300, 486, 1884, 439, 264, 10067, 293, 3625], "temperature": 0.0, "avg_logprob": -0.1539982727595738, "compression_ratio": 1.6357142857142857, "no_speech_prob": 6.642458629357861e-06}, {"id": 158, "seek": 89120, "start": 911.9200000000001, "end": 917.1600000000001, "text": " the operator in the namespace Kepler operator system. So now I'm just going", "tokens": [264, 12973, 294, 264, 5288, 17940, 3189, 22732, 12973, 1185, 13, 407, 586, 286, 478, 445, 516], "temperature": 0.0, "avg_logprob": -0.1539982727595738, "compression_ratio": 1.6357142857142857, "no_speech_prob": 6.642458629357861e-06}, {"id": 159, "seek": 91716, "start": 917.16, "end": 924.4399999999999, "text": " to go into the Kepler operator system the namespace and I'm going to apply", "tokens": [281, 352, 666, 264, 3189, 22732, 12973, 1185, 264, 5288, 17940, 293, 286, 478, 516, 281, 3079], "temperature": 0.0, "avg_logprob": -0.1382954955101013, "compression_ratio": 1.855421686746988, "no_speech_prob": 3.480390296317637e-05}, {"id": 160, "seek": 91716, "start": 924.4399999999999, "end": 929.92, "text": " let's see if the operator has been yeah so you can see that the operator is", "tokens": [718, 311, 536, 498, 264, 12973, 575, 668, 1338, 370, 291, 393, 536, 300, 264, 12973, 307], "temperature": 0.0, "avg_logprob": -0.1382954955101013, "compression_ratio": 1.855421686746988, "no_speech_prob": 3.480390296317637e-05}, {"id": 161, "seek": 91716, "start": 929.92, "end": 936.48, "text": " running now I'm going to apply the CRD and wait for the Kepler instances to get", "tokens": [2614, 586, 286, 478, 516, 281, 3079, 264, 14123, 35, 293, 1699, 337, 264, 3189, 22732, 14519, 281, 483], "temperature": 0.0, "avg_logprob": -0.1382954955101013, "compression_ratio": 1.855421686746988, "no_speech_prob": 3.480390296317637e-05}, {"id": 162, "seek": 91716, "start": 936.48, "end": 944.52, "text": " started. So as you can see the Kepler instances are running and they are each", "tokens": [1409, 13, 407, 382, 291, 393, 536, 264, 3189, 22732, 14519, 366, 2614, 293, 436, 366, 1184], "temperature": 0.0, "avg_logprob": -0.1382954955101013, "compression_ratio": 1.855421686746988, "no_speech_prob": 3.480390296317637e-05}, {"id": 163, "seek": 94452, "start": 944.52, "end": 949.24, "text": " of them is up and running and they are each of them are running on each of the", "tokens": [295, 552, 307, 493, 293, 2614, 293, 436, 366, 1184, 295, 552, 366, 2614, 322, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.14225490806029015, "compression_ratio": 1.8300970873786409, "no_speech_prob": 2.7104022592538968e-05}, {"id": 164, "seek": 94452, "start": 949.24, "end": 955.3199999999999, "text": " nodes as a demon set pod so that's why you see so many of them and now I'm", "tokens": [13891, 382, 257, 14283, 992, 2497, 370, 300, 311, 983, 291, 536, 370, 867, 295, 552, 293, 586, 286, 478], "temperature": 0.0, "avg_logprob": -0.14225490806029015, "compression_ratio": 1.8300970873786409, "no_speech_prob": 2.7104022592538968e-05}, {"id": 165, "seek": 94452, "start": 955.3199999999999, "end": 962.0, "text": " going to deploy Grafana give it a second yes so Grafana is deployed now to", "tokens": [516, 281, 7274, 8985, 69, 2095, 976, 309, 257, 1150, 2086, 370, 8985, 69, 2095, 307, 17826, 586, 281], "temperature": 0.0, "avg_logprob": -0.14225490806029015, "compression_ratio": 1.8300970873786409, "no_speech_prob": 2.7104022592538968e-05}, {"id": 166, "seek": 94452, "start": 962.0, "end": 966.4399999999999, "text": " enable user workload monitoring I'm going to apply the config map and that", "tokens": [9528, 4195, 20139, 11028, 286, 478, 516, 281, 3079, 264, 6662, 4471, 293, 300], "temperature": 0.0, "avg_logprob": -0.14225490806029015, "compression_ratio": 1.8300970873786409, "no_speech_prob": 2.7104022592538968e-05}, {"id": 167, "seek": 94452, "start": 966.4399999999999, "end": 971.24, "text": " ensures that the Prometheus and the user workload monitoring namespace is", "tokens": [28111, 300, 264, 2114, 649, 42209, 293, 264, 4195, 20139, 11028, 5288, 17940, 307], "temperature": 0.0, "avg_logprob": -0.14225490806029015, "compression_ratio": 1.8300970873786409, "no_speech_prob": 2.7104022592538968e-05}, {"id": 168, "seek": 97124, "start": 971.24, "end": 979.6800000000001, "text": " capturing the Prometheus metrics so let's see if the pods are up and running in", "tokens": [23384, 264, 2114, 649, 42209, 16367, 370, 718, 311, 536, 498, 264, 31925, 366, 493, 293, 2614, 294], "temperature": 0.0, "avg_logprob": -0.14518790948586385, "compression_ratio": 1.5364238410596027, "no_speech_prob": 4.069145870744251e-05}, {"id": 169, "seek": 97124, "start": 979.6800000000001, "end": 985.5600000000001, "text": " the OpenShift user monitoring project as you can see that all the pods are", "tokens": [264, 7238, 7774, 2008, 4195, 11028, 1716, 382, 291, 393, 536, 300, 439, 264, 31925, 366], "temperature": 0.0, "avg_logprob": -0.14518790948586385, "compression_ratio": 1.5364238410596027, "no_speech_prob": 4.069145870744251e-05}, {"id": 170, "seek": 97124, "start": 985.5600000000001, "end": 995.32, "text": " running so now to see the metrics I'm just going to the Grafana URL just sign", "tokens": [2614, 370, 586, 281, 536, 264, 16367, 286, 478, 445, 516, 281, 264, 8985, 69, 2095, 12905, 445, 1465], "temperature": 0.0, "avg_logprob": -0.14518790948586385, "compression_ratio": 1.5364238410596027, "no_speech_prob": 4.069145870744251e-05}, {"id": 171, "seek": 99532, "start": 995.32, "end": 1002.6400000000001, "text": " in and because we applied the Grafana operator so the default Kepler dashboard", "tokens": [294, 293, 570, 321, 6456, 264, 8985, 69, 2095, 12973, 370, 264, 7576, 3189, 22732, 18342], "temperature": 0.0, "avg_logprob": -0.1471011025565011, "compression_ratio": 1.7206703910614525, "no_speech_prob": 2.7531803425517865e-05}, {"id": 172, "seek": 99532, "start": 1002.6400000000001, "end": 1008.88, "text": " should be available give it a second it will load yeah so now you can see the", "tokens": [820, 312, 2435, 976, 309, 257, 1150, 309, 486, 3677, 1338, 370, 586, 291, 393, 536, 264], "temperature": 0.0, "avg_logprob": -0.1471011025565011, "compression_ratio": 1.7206703910614525, "no_speech_prob": 2.7531803425517865e-05}, {"id": 173, "seek": 99532, "start": 1008.88, "end": 1013.32, "text": " energy reporting from Kepler you can see the carbon footprint you can see the", "tokens": [2281, 10031, 490, 3189, 22732, 291, 393, 536, 264, 5954, 24222, 291, 393, 536, 264], "temperature": 0.0, "avg_logprob": -0.1471011025565011, "compression_ratio": 1.7206703910614525, "no_speech_prob": 2.7531803425517865e-05}, {"id": 174, "seek": 99532, "start": 1013.32, "end": 1019.5600000000001, "text": " power consumption in namespaces total power consumption pod process power", "tokens": [1347, 12126, 294, 5288, 79, 2116, 3217, 1347, 12126, 2497, 1399, 1347], "temperature": 0.0, "avg_logprob": -0.1471011025565011, "compression_ratio": 1.7206703910614525, "no_speech_prob": 2.7531803425517865e-05}, {"id": 175, "seek": 101956, "start": 1019.56, "end": 1025.24, "text": " consumption and total power consumption by namespace so that's the default", "tokens": [12126, 293, 3217, 1347, 12126, 538, 5288, 17940, 370, 300, 311, 264, 7576], "temperature": 0.0, "avg_logprob": -0.13315865691279022, "compression_ratio": 1.625668449197861, "no_speech_prob": 1.4969579751777928e-05}, {"id": 176, "seek": 101956, "start": 1025.24, "end": 1031.6799999999998, "text": " Grafana dashboard so now that we have seen how you can install and play around", "tokens": [8985, 69, 2095, 18342, 370, 586, 300, 321, 362, 1612, 577, 291, 393, 3625, 293, 862, 926], "temperature": 0.0, "avg_logprob": -0.13315865691279022, "compression_ratio": 1.625668449197861, "no_speech_prob": 1.4969579751777928e-05}, {"id": 177, "seek": 101956, "start": 1031.6799999999998, "end": 1037.56, "text": " with your Kepler operator it's time to see how you can also do carbon", "tokens": [365, 428, 3189, 22732, 12973, 309, 311, 565, 281, 536, 577, 291, 393, 611, 360, 5954], "temperature": 0.0, "avg_logprob": -0.13315865691279022, "compression_ratio": 1.625668449197861, "no_speech_prob": 1.4969579751777928e-05}, {"id": 178, "seek": 101956, "start": 1037.56, "end": 1049.28, "text": " intensity aware scheduling so for that I have a cluster already ready so you can", "tokens": [13749, 3650, 29055, 370, 337, 300, 286, 362, 257, 13630, 1217, 1919, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.13315865691279022, "compression_ratio": 1.625668449197861, "no_speech_prob": 1.4969579751777928e-05}, {"id": 179, "seek": 104928, "start": 1049.28, "end": 1056.36, "text": " see that there are six nodes on this cluster and for my this demo I'm not", "tokens": [536, 300, 456, 366, 2309, 13891, 322, 341, 13630, 293, 337, 452, 341, 10723, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.12268252312382565, "compression_ratio": 1.702247191011236, "no_speech_prob": 2.7106319976155646e-05}, {"id": 180, "seek": 104928, "start": 1056.36, "end": 1063.32, "text": " going to run anything on the master node so I'm only going to do things on the", "tokens": [516, 281, 1190, 1340, 322, 264, 4505, 9984, 370, 286, 478, 787, 516, 281, 360, 721, 322, 264], "temperature": 0.0, "avg_logprob": -0.12268252312382565, "compression_ratio": 1.702247191011236, "no_speech_prob": 2.7106319976155646e-05}, {"id": 181, "seek": 104928, "start": 1063.32, "end": 1070.44, "text": " worker node so I have applied the cron job and I'm just waiting it to become", "tokens": [11346, 9984, 370, 286, 362, 6456, 264, 941, 266, 1691, 293, 286, 478, 445, 3806, 309, 281, 1813], "temperature": 0.0, "avg_logprob": -0.12268252312382565, "compression_ratio": 1.702247191011236, "no_speech_prob": 2.7106319976155646e-05}, {"id": 182, "seek": 104928, "start": 1070.44, "end": 1076.8, "text": " active all right so that job has been scheduled let's wait for you to get", "tokens": [4967, 439, 558, 370, 300, 1691, 575, 668, 15678, 718, 311, 1699, 337, 291, 281, 483], "temperature": 0.0, "avg_logprob": -0.12268252312382565, "compression_ratio": 1.702247191011236, "no_speech_prob": 2.7106319976155646e-05}, {"id": 183, "seek": 107680, "start": 1076.8, "end": 1086.12, "text": " completed as you can see that the crown job has been completed so let's see what", "tokens": [7365, 382, 291, 393, 536, 300, 264, 11841, 1691, 575, 668, 7365, 370, 718, 311, 536, 437], "temperature": 0.0, "avg_logprob": -0.142175627536461, "compression_ratio": 1.6712328767123288, "no_speech_prob": 7.966932753333822e-05}, {"id": 184, "seek": 107680, "start": 1086.12, "end": 1096.6399999999999, "text": " gains and what labels it has applied to the three nodes so to see the labels I'm", "tokens": [16823, 293, 437, 16949, 309, 575, 6456, 281, 264, 1045, 13891, 370, 281, 536, 264, 16949, 286, 478], "temperature": 0.0, "avg_logprob": -0.142175627536461, "compression_ratio": 1.6712328767123288, "no_speech_prob": 7.966932753333822e-05}, {"id": 185, "seek": 107680, "start": 1096.6399999999999, "end": 1105.04, "text": " going to use the same script that I have written okay so you can see that the node", "tokens": [516, 281, 764, 264, 912, 5755, 300, 286, 362, 3720, 1392, 370, 291, 393, 536, 300, 264, 9984], "temperature": 0.0, "avg_logprob": -0.142175627536461, "compression_ratio": 1.6712328767123288, "no_speech_prob": 7.966932753333822e-05}, {"id": 186, "seek": 110504, "start": 1105.04, "end": 1115.76, "text": " 2 2 3 has got the label green node 2 2 2 has got the label red and node 1 2 6 has", "tokens": [568, 568, 805, 575, 658, 264, 7645, 3092, 9984, 568, 568, 568, 575, 658, 264, 7645, 2182, 293, 9984, 502, 568, 1386, 575], "temperature": 0.0, "avg_logprob": -0.18243015789594808, "compression_ratio": 1.63013698630137, "no_speech_prob": 4.7573506890330464e-05}, {"id": 187, "seek": 110504, "start": 1115.76, "end": 1124.04, "text": " the label yellow so anytime that we are going to schedule a carbon intensive", "tokens": [264, 7645, 5566, 370, 13038, 300, 321, 366, 516, 281, 7567, 257, 5954, 18957], "temperature": 0.0, "avg_logprob": -0.18243015789594808, "compression_ratio": 1.63013698630137, "no_speech_prob": 4.7573506890330464e-05}, {"id": 188, "seek": 110504, "start": 1124.04, "end": 1130.3999999999999, "text": " aware pod or workload it should favor 2 2 3 which has carbon intensity as green", "tokens": [3650, 2497, 420, 20139, 309, 820, 2294, 568, 568, 805, 597, 575, 5954, 13749, 382, 3092], "temperature": 0.0, "avg_logprob": -0.18243015789594808, "compression_ratio": 1.63013698630137, "no_speech_prob": 4.7573506890330464e-05}, {"id": 189, "seek": 113040, "start": 1130.4, "end": 1138.2, "text": " let's also check what gains has been assigned and if they match the labels so", "tokens": [718, 311, 611, 1520, 437, 16823, 575, 668, 13279, 293, 498, 436, 2995, 264, 16949, 370], "temperature": 0.0, "avg_logprob": -0.1768726599021036, "compression_ratio": 1.6344827586206896, "no_speech_prob": 4.469296254683286e-05}, {"id": 190, "seek": 113040, "start": 1138.2, "end": 1147.24, "text": " you can see that the node 1 2 a 6 and 1 2 3 which has carbon intensity as green", "tokens": [291, 393, 536, 300, 264, 9984, 502, 568, 257, 1386, 293, 502, 568, 805, 597, 575, 5954, 13749, 382, 3092], "temperature": 0.0, "avg_logprob": -0.1768726599021036, "compression_ratio": 1.6344827586206896, "no_speech_prob": 4.469296254683286e-05}, {"id": 191, "seek": 113040, "start": 1147.24, "end": 1155.68, "text": " and yellow have no gains while the node 2 2 2 which has the carbon intensity as", "tokens": [293, 5566, 362, 572, 16823, 1339, 264, 9984, 568, 568, 568, 597, 575, 264, 5954, 13749, 382], "temperature": 0.0, "avg_logprob": -0.1768726599021036, "compression_ratio": 1.6344827586206896, "no_speech_prob": 4.469296254683286e-05}, {"id": 192, "seek": 115568, "start": 1155.68, "end": 1162.0, "text": " red as you can see over here has the taint applied now I am going to test it", "tokens": [2182, 382, 291, 393, 536, 670, 510, 575, 264, 256, 5114, 6456, 586, 286, 669, 516, 281, 1500, 309], "temperature": 0.0, "avg_logprob": -0.14557560284932455, "compression_ratio": 1.4935064935064934, "no_speech_prob": 5.4758616897743195e-05}, {"id": 193, "seek": 115568, "start": 1162.0, "end": 1168.48, "text": " out if this works as expected by applying or by scheduling a long-running", "tokens": [484, 498, 341, 1985, 382, 5176, 538, 9275, 420, 538, 29055, 257, 938, 12, 45482], "temperature": 0.0, "avg_logprob": -0.14557560284932455, "compression_ratio": 1.4935064935064934, "no_speech_prob": 5.4758616897743195e-05}, {"id": 194, "seek": 115568, "start": 1168.48, "end": 1177.0, "text": " workload before I do that I just want to watch all the pods in the namespace so", "tokens": [20139, 949, 286, 360, 300, 286, 445, 528, 281, 1159, 439, 264, 31925, 294, 264, 5288, 17940, 370], "temperature": 0.0, "avg_logprob": -0.14557560284932455, "compression_ratio": 1.4935064935064934, "no_speech_prob": 5.4758616897743195e-05}, {"id": 195, "seek": 117700, "start": 1177.0, "end": 1188.4, "text": " right now there's no pod so so I have applied this pod and it has it has no", "tokens": [558, 586, 456, 311, 572, 2497, 370, 370, 286, 362, 6456, 341, 2497, 293, 309, 575, 309, 575, 572], "temperature": 0.0, "avg_logprob": -0.24119562916941456, "compression_ratio": 1.6829268292682926, "no_speech_prob": 9.080275049200282e-06}, {"id": 196, "seek": 117700, "start": 1188.4, "end": 1194.82, "text": " tolerations for node that has tainted red and it favors a node or it wants a", "tokens": [11125, 763, 337, 9984, 300, 575, 256, 26278, 2182, 293, 309, 40554, 257, 9984, 420, 309, 2738, 257], "temperature": 0.0, "avg_logprob": -0.24119562916941456, "compression_ratio": 1.6829268292682926, "no_speech_prob": 9.080275049200282e-06}, {"id": 197, "seek": 117700, "start": 1194.82, "end": 1200.08, "text": " node that has the label green so over here you can see that the CITS pod the", "tokens": [9984, 300, 575, 264, 7645, 3092, 370, 670, 510, 291, 393, 536, 300, 264, 383, 3927, 50, 2497, 264], "temperature": 0.0, "avg_logprob": -0.24119562916941456, "compression_ratio": 1.6829268292682926, "no_speech_prob": 9.080275049200282e-06}, {"id": 198, "seek": 117700, "start": 1200.08, "end": 1205.48, "text": " pod that I just ran had some issue or had some", "tokens": [2497, 300, 286, 445, 5872, 632, 512, 2734, 420, 632, 512], "temperature": 0.0, "avg_logprob": -0.24119562916941456, "compression_ratio": 1.6829268292682926, "no_speech_prob": 9.080275049200282e-06}, {"id": 199, "seek": 120548, "start": 1205.48, "end": 1209.72, "text": " problem in finding the right node that's because the default scheduler was", "tokens": [1154, 294, 5006, 264, 558, 9984, 300, 311, 570, 264, 7576, 12000, 260, 390], "temperature": 0.0, "avg_logprob": -0.1315754376924955, "compression_ratio": 1.8070175438596492, "no_speech_prob": 2.1780946553917602e-05}, {"id": 200, "seek": 120548, "start": 1209.72, "end": 1213.72, "text": " trying to sign it on a node that didn't have the right label or didn't have the", "tokens": [1382, 281, 1465, 309, 322, 257, 9984, 300, 994, 380, 362, 264, 558, 7645, 420, 994, 380, 362, 264], "temperature": 0.0, "avg_logprob": -0.1315754376924955, "compression_ratio": 1.8070175438596492, "no_speech_prob": 2.1780946553917602e-05}, {"id": 201, "seek": 120548, "start": 1213.72, "end": 1220.1200000000001, "text": " right taint so it took some took a while so let's verify where this pod is", "tokens": [558, 256, 5114, 370, 309, 1890, 512, 1890, 257, 1339, 370, 718, 311, 16888, 689, 341, 2497, 307], "temperature": 0.0, "avg_logprob": -0.1315754376924955, "compression_ratio": 1.8070175438596492, "no_speech_prob": 2.1780946553917602e-05}, {"id": 202, "seek": 120548, "start": 1220.1200000000001, "end": 1228.16, "text": " running so you can see that it has been scheduled on the go it was scheduled on", "tokens": [2614, 370, 291, 393, 536, 300, 309, 575, 668, 15678, 322, 264, 352, 309, 390, 15678, 322], "temperature": 0.0, "avg_logprob": -0.1315754376924955, "compression_ratio": 1.8070175438596492, "no_speech_prob": 2.1780946553917602e-05}, {"id": 203, "seek": 122816, "start": 1228.16, "end": 1236.68, "text": " two to three but right now it's running on 126 and 126 is a node that has", "tokens": [732, 281, 1045, 457, 558, 586, 309, 311, 2614, 322, 2272, 21, 293, 2272, 21, 307, 257, 9984, 300, 575], "temperature": 0.0, "avg_logprob": -0.15156010736393022, "compression_ratio": 1.684782608695652, "no_speech_prob": 2.429905543976929e-05}, {"id": 204, "seek": 122816, "start": 1236.68, "end": 1242.2, "text": " common intensity as yellow so that's that's completely okay the time it was", "tokens": [2689, 13749, 382, 5566, 370, 300, 311, 300, 311, 2584, 1392, 264, 565, 309, 390], "temperature": 0.0, "avg_logprob": -0.15156010736393022, "compression_ratio": 1.684782608695652, "no_speech_prob": 2.429905543976929e-05}, {"id": 205, "seek": 122816, "start": 1242.2, "end": 1246.52, "text": " scheduled on either the green node or on the yellow node which is okay as long as", "tokens": [15678, 322, 2139, 264, 3092, 9984, 420, 322, 264, 5566, 9984, 597, 307, 1392, 382, 938, 382], "temperature": 0.0, "avg_logprob": -0.15156010736393022, "compression_ratio": 1.684782608695652, "no_speech_prob": 2.429905543976929e-05}, {"id": 206, "seek": 122816, "start": 1246.52, "end": 1253.0400000000002, "text": " it's not scheduled on the red node so that would be all thank you for watching", "tokens": [309, 311, 406, 15678, 322, 264, 2182, 9984, 370, 300, 576, 312, 439, 1309, 291, 337, 1976], "temperature": 0.0, "avg_logprob": -0.15156010736393022, "compression_ratio": 1.684782608695652, "no_speech_prob": 2.429905543976929e-05}, {"id": 207, "seek": 125304, "start": 1253.04, "end": 1258.68, "text": " the demo we would like to share a few lessons that we learned while working", "tokens": [264, 10723, 321, 576, 411, 281, 2073, 257, 1326, 8820, 300, 321, 3264, 1339, 1364], "temperature": 0.0, "avg_logprob": -0.1285205490287693, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.649225568049587e-05}, {"id": 208, "seek": 125304, "start": 1258.68, "end": 1264.68, "text": " on this project the first is that finding the zone cover intensity data is", "tokens": [322, 341, 1716, 264, 700, 307, 300, 5006, 264, 6668, 2060, 13749, 1412, 307], "temperature": 0.0, "avg_logprob": -0.1285205490287693, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.649225568049587e-05}, {"id": 209, "seek": 125304, "start": 1264.68, "end": 1271.6399999999999, "text": " not simple some points are missing and not all of them are free we also need to", "tokens": [406, 2199, 512, 2793, 366, 5361, 293, 406, 439, 295, 552, 366, 1737, 321, 611, 643, 281], "temperature": 0.0, "avg_logprob": -0.1285205490287693, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.649225568049587e-05}, {"id": 210, "seek": 125304, "start": 1271.6399999999999, "end": 1276.76, "text": " support multiple and complex query types for example right now we are just", "tokens": [1406, 3866, 293, 3997, 14581, 3467, 337, 1365, 558, 586, 321, 366, 445], "temperature": 0.0, "avg_logprob": -0.1285205490287693, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.649225568049587e-05}, {"id": 211, "seek": 125304, "start": 1276.76, "end": 1282.48, "text": " querying what is the current or the average cover intensity in zone XYZ but", "tokens": [7083, 1840, 437, 307, 264, 2190, 420, 264, 4274, 2060, 13749, 294, 6668, 48826, 57, 457], "temperature": 0.0, "avg_logprob": -0.1285205490287693, "compression_ratio": 1.7162162162162162, "no_speech_prob": 5.649225568049587e-05}, {"id": 212, "seek": 128248, "start": 1282.48, "end": 1286.68, "text": " we need to have more complicated queries like which zone has the lowest", "tokens": [321, 643, 281, 362, 544, 6179, 24109, 411, 597, 6668, 575, 264, 12437], "temperature": 0.0, "avg_logprob": -0.14361940489874947, "compression_ratio": 1.7536231884057971, "no_speech_prob": 2.3535496438853443e-05}, {"id": 213, "seek": 128248, "start": 1286.68, "end": 1292.32, "text": " cover intensity and we are also thinking of contributing the work that we have", "tokens": [2060, 13749, 293, 321, 366, 611, 1953, 295, 19270, 264, 589, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.14361940489874947, "compression_ratio": 1.7536231884057971, "no_speech_prob": 2.3535496438853443e-05}, {"id": 214, "seek": 128248, "start": 1292.32, "end": 1296.3600000000001, "text": " done with the cover intensity forecasting and integrating it with", "tokens": [1096, 365, 264, 2060, 13749, 44331, 293, 26889, 309, 365], "temperature": 0.0, "avg_logprob": -0.14361940489874947, "compression_ratio": 1.7536231884057971, "no_speech_prob": 2.3535496438853443e-05}, {"id": 215, "seek": 128248, "start": 1296.3600000000001, "end": 1302.3600000000001, "text": " green software foundation carbon away SDK which is another open source", "tokens": [3092, 4722, 7030, 5954, 1314, 37135, 597, 307, 1071, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.14361940489874947, "compression_ratio": 1.7536231884057971, "no_speech_prob": 2.3535496438853443e-05}, {"id": 216, "seek": 128248, "start": 1302.3600000000001, "end": 1309.3600000000001, "text": " community that has been working on sustainability and green software so the", "tokens": [1768, 300, 575, 668, 1364, 322, 16360, 293, 3092, 4722, 370, 264], "temperature": 0.0, "avg_logprob": -0.14361940489874947, "compression_ratio": 1.7536231884057971, "no_speech_prob": 2.3535496438853443e-05}, {"id": 217, "seek": 130936, "start": 1309.36, "end": 1315.36, "text": " road ahead for us looks like we are thinking of extending the multi node", "tokens": [3060, 2286, 337, 505, 1542, 411, 321, 366, 1953, 295, 24360, 264, 4825, 9984], "temperature": 0.0, "avg_logprob": -0.201247344782323, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.428566040180158e-05}, {"id": 218, "seek": 130936, "start": 1315.36, "end": 1319.9599999999998, "text": " logic to multi cluster and we're exploring how you can do that using kcp", "tokens": [9952, 281, 4825, 13630, 293, 321, 434, 12736, 577, 291, 393, 360, 300, 1228, 350, 66, 79], "temperature": 0.0, "avg_logprob": -0.201247344782323, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.428566040180158e-05}, {"id": 219, "seek": 130936, "start": 1319.9599999999998, "end": 1325.3999999999999, "text": " and we are also thinking of integrating carbon intensity awareness in", "tokens": [293, 321, 366, 611, 1953, 295, 26889, 5954, 13749, 8888, 294], "temperature": 0.0, "avg_logprob": -0.201247344782323, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.428566040180158e-05}, {"id": 220, "seek": 130936, "start": 1325.3999999999999, "end": 1332.1599999999999, "text": " Kubernetes plugins existing plugins for example the trimaran target load", "tokens": [23145, 33759, 6741, 33759, 337, 1365, 264, 10445, 17142, 3779, 3677], "temperature": 0.0, "avg_logprob": -0.201247344782323, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.428566040180158e-05}, {"id": 221, "seek": 130936, "start": 1332.1599999999999, "end": 1337.8, "text": " packing is a scheduler plugin by in the Kubernetes sake and we're thinking of", "tokens": [20815, 307, 257, 12000, 260, 23407, 538, 294, 264, 23145, 9717, 293, 321, 434, 1953, 295], "temperature": 0.0, "avg_logprob": -0.201247344782323, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.428566040180158e-05}, {"id": 222, "seek": 133780, "start": 1337.8, "end": 1343.96, "text": " integrating the profile with the carbon intensity awareness and also thinking", "tokens": [26889, 264, 7964, 365, 264, 5954, 13749, 8888, 293, 611, 1953], "temperature": 0.0, "avg_logprob": -0.1731142926571974, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.1681884138379246e-05}, {"id": 223, "seek": 133780, "start": 1343.96, "end": 1352.32, "text": " of how you can tune trimaran further for energy efficiency so that was all if", "tokens": [295, 577, 291, 393, 10864, 10445, 17142, 3052, 337, 2281, 10493, 370, 300, 390, 439, 498], "temperature": 0.0, "avg_logprob": -0.1731142926571974, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.1681884138379246e-05}, {"id": 224, "seek": 133780, "start": 1352.32, "end": 1358.48, "text": " you are more interested in learning about the principle of that capra is", "tokens": [291, 366, 544, 3102, 294, 2539, 466, 264, 8665, 295, 300, 1410, 424, 307], "temperature": 0.0, "avg_logprob": -0.1731142926571974, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.1681884138379246e-05}, {"id": 225, "seek": 133780, "start": 1358.48, "end": 1364.6399999999999, "text": " based on you can follow the link and check out a project we have attached the", "tokens": [2361, 322, 291, 393, 1524, 264, 2113, 293, 1520, 484, 257, 1716, 321, 362, 8570, 264], "temperature": 0.0, "avg_logprob": -0.1731142926571974, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.1681884138379246e-05}, {"id": 226, "seek": 136464, "start": 1364.64, "end": 1372.44, "text": " GitHub repo for the project as well as the model server and thank you so much", "tokens": [23331, 49040, 337, 264, 1716, 382, 731, 382, 264, 2316, 7154, 293, 1309, 291, 370, 709], "temperature": 0.0, "avg_logprob": -0.3071090380350749, "compression_ratio": 1.1728395061728396, "no_speech_prob": 0.0003144379297737032}, {"id": 227, "seek": 137244, "start": 1372.44, "end": 1397.76, "text": " and any questions", "tokens": [50364, 293, 604, 1651, 51630], "temperature": 0.0, "avg_logprob": -0.5632929007212321, "compression_ratio": 0.68, "no_speech_prob": 0.0005821540835313499}, {"id": 228, "seek": 140244, "start": 1402.44, "end": 1404.5, "text": " you", "tokens": [50364, 291, 50467], "temperature": 0.0, "avg_logprob": -0.8508306741714478, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.7896295785903931}, {"id": 229, "seek": 143244, "start": 1432.44, "end": 1462.16, "text": " okay do you want to take that question", "tokens": [1392, 360, 291, 528, 281, 747, 300, 1168], "temperature": 0.6000000000000001, "avg_logprob": -0.8148223559061686, "compression_ratio": 0.8444444444444444, "no_speech_prob": 0.37484103441238403}, {"id": 230, "seek": 146216, "start": 1462.16, "end": 1481.1200000000001, "text": " do you see the question okay so yeah sir um sorry I'm just trying to see the", "tokens": [360, 291, 536, 264, 1168, 1392, 370, 1338, 4735, 1105, 2597, 286, 478, 445, 1382, 281, 536, 264], "temperature": 0.0, "avg_logprob": -0.38786333257501776, "compression_ratio": 1.0704225352112675, "no_speech_prob": 0.18849433958530426}, {"id": 231, "seek": 148112, "start": 1481.12, "end": 1492.9599999999998, "text": " questions I have to switch back and forth okay sir um sorry I'm just trying to see the questions I", "tokens": [1651, 286, 362, 281, 3679, 646, 293, 5220, 1392, 4735, 1105, 2597, 286, 478, 445, 1382, 281, 536, 264, 1651, 286], "temperature": 0.0, "avg_logprob": -0.4147122192382813, "compression_ratio": 1.180722891566265, "no_speech_prob": 0.0017709680832922459}, {"id": 232, "seek": 149296, "start": 1492.96, "end": 1522.16, "text": " somebody asked how do we split the", "tokens": [50364, 2618, 2351, 577, 360, 321, 7472, 264, 51824], "temperature": 0.0, "avg_logprob": -0.4125668048858643, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.012595871463418007}, {"id": 233, "seek": 152296, "start": 1523.52, "end": 1525.04, "text": " energy for the pod", "tokens": [2281, 337, 264, 2497], "temperature": 0.0, "avg_logprob": -0.18703895348768967, "compression_ratio": 1.4825174825174825, "no_speech_prob": 0.12839312851428986}, {"id": 234, "seek": 152296, "start": 1531.04, "end": 1538.56, "text": " oh um yeah I think I can answer that um this was done on Kepler I believe and I was developed", "tokens": [1954, 1105, 1338, 286, 519, 286, 393, 1867, 300, 1105, 341, 390, 1096, 322, 3189, 22732, 286, 1697, 293, 286, 390, 4743], "temperature": 0.0, "avg_logprob": -0.18703895348768967, "compression_ratio": 1.4825174825174825, "no_speech_prob": 0.12839312851428986}, {"id": 235, "seek": 152296, "start": 1538.56, "end": 1546.16, "text": " by somebody else but essentially there are two ways for like the model server we also have recently", "tokens": [538, 2618, 1646, 457, 4476, 456, 366, 732, 2098, 337, 411, 264, 2316, 7154, 321, 611, 362, 3938], "temperature": 0.0, "avg_logprob": -0.18703895348768967, "compression_ratio": 1.4825174825174825, "no_speech_prob": 0.12839312851428986}, {"id": 236, "seek": 154616, "start": 1546.16, "end": 1554.72, "text": " have like models that'll use the performance metrics and then the software counters to directly", "tokens": [362, 411, 5245, 300, 603, 764, 264, 3389, 16367, 293, 550, 264, 4722, 39338, 281, 3838], "temperature": 0.0, "avg_logprob": -0.0993710444523738, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.759501487365924e-05}, {"id": 237, "seek": 154616, "start": 1554.72, "end": 1561.6000000000001, "text": " try and predict pod energy um when it that that's one option and then second option in Kepler is", "tokens": [853, 293, 6069, 2497, 2281, 1105, 562, 309, 300, 300, 311, 472, 3614, 293, 550, 1150, 3614, 294, 3189, 22732, 307], "temperature": 0.0, "avg_logprob": -0.0993710444523738, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.759501487365924e-05}, {"id": 238, "seek": 154616, "start": 1561.6000000000001, "end": 1569.92, "text": " typically um once it generates the energy it'll then try and attribute it I believe to each of the", "tokens": [5850, 1105, 1564, 309, 23815, 264, 2281, 309, 603, 550, 853, 293, 19667, 309, 286, 1697, 281, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.0993710444523738, "compression_ratio": 1.6256983240223464, "no_speech_prob": 3.759501487365924e-05}, {"id": 239, "seek": 156992, "start": 1569.92, "end": 1579.44, "text": " pods and I think that's based on um is it based on cpu utilization proof I don't know yeah what we", "tokens": [31925, 293, 286, 519, 300, 311, 2361, 322, 1105, 307, 309, 2361, 322, 269, 34859, 37074, 8177, 286, 500, 380, 458, 1338, 437, 321], "temperature": 0.0, "avg_logprob": -0.15341655186244418, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.718369306530803e-05}, {"id": 240, "seek": 156992, "start": 1579.44, "end": 1586.24, "text": " do is we monitor the cpu utilization although whatever the cpu instruction or the process", "tokens": [360, 307, 321, 6002, 264, 269, 34859, 37074, 4878, 2035, 264, 269, 34859, 10951, 420, 264, 1399], "temperature": 0.0, "avg_logprob": -0.15341655186244418, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.718369306530803e-05}, {"id": 241, "seek": 156992, "start": 1586.24, "end": 1593.8400000000001, "text": " is going on and then we use cgroup id to kind of like attribute what how that energy is related", "tokens": [307, 516, 322, 293, 550, 321, 764, 269, 17377, 4496, 281, 733, 295, 411, 19667, 437, 577, 300, 2281, 307, 4077], "temperature": 0.0, "avg_logprob": -0.15341655186244418, "compression_ratio": 1.6511627906976745, "no_speech_prob": 7.718369306530803e-05}, {"id": 242, "seek": 159384, "start": 1593.84, "end": 1601.9199999999998, "text": " to which pod because we take the cgroup id and we translate that which particular process or", "tokens": [281, 597, 2497, 570, 321, 747, 264, 269, 17377, 4496, 293, 321, 13799, 300, 597, 1729, 1399, 420], "temperature": 0.0, "avg_logprob": -0.055974475921146453, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.2410516319505405e-05}, {"id": 243, "seek": 159384, "start": 1601.9199999999998, "end": 1608.8799999999999, "text": " container it's related to so that's how we gather the metrics so the important thing to note over", "tokens": [10129, 309, 311, 4077, 281, 370, 300, 311, 577, 321, 5448, 264, 16367, 370, 264, 1021, 551, 281, 3637, 670], "temperature": 0.0, "avg_logprob": -0.055974475921146453, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.2410516319505405e-05}, {"id": 244, "seek": 159384, "start": 1608.8799999999999, "end": 1616.6399999999999, "text": " here is Kepler uses the models to estimate or predict the energy consumption and these models", "tokens": [510, 307, 3189, 22732, 4960, 264, 5245, 281, 12539, 420, 6069, 264, 2281, 12126, 293, 613, 5245], "temperature": 0.0, "avg_logprob": -0.055974475921146453, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.2410516319505405e-05}, {"id": 245, "seek": 161664, "start": 1616.64, "end": 1624.72, "text": " are already trained they already have they are already being published so Kepler uses these", "tokens": [366, 1217, 8895, 436, 1217, 362, 436, 366, 1217, 885, 6572, 370, 3189, 22732, 4960, 613], "temperature": 0.0, "avg_logprob": -0.07979590183979757, "compression_ratio": 1.7755102040816326, "no_speech_prob": 2.4058988401520764e-06}, {"id": 246, "seek": 161664, "start": 1624.72, "end": 1631.0400000000002, "text": " models to predict pod energy level consumption on scenarios where you're not running on bare metal", "tokens": [5245, 281, 6069, 2497, 2281, 1496, 12126, 322, 15077, 689, 291, 434, 406, 2614, 322, 6949, 5760], "temperature": 0.0, "avg_logprob": -0.07979590183979757, "compression_ratio": 1.7755102040816326, "no_speech_prob": 2.4058988401520764e-06}, {"id": 247, "seek": 161664, "start": 1631.6000000000001, "end": 1637.6000000000001, "text": " on those cases we don't have the access to the inbuilt power meter so in those scenarios we", "tokens": [322, 729, 3331, 321, 500, 380, 362, 264, 2105, 281, 264, 294, 23018, 1347, 9255, 370, 294, 729, 15077, 321], "temperature": 0.0, "avg_logprob": -0.07979590183979757, "compression_ratio": 1.7755102040816326, "no_speech_prob": 2.4058988401520764e-06}, {"id": 248, "seek": 163760, "start": 1637.6, "end": 1653.52, "text": " estimate or we predict what is going to be the energy consumption", "tokens": [12539, 420, 321, 6069, 437, 307, 516, 281, 312, 264, 2281, 12126], "temperature": 0.0, "avg_logprob": -0.19191616979138604, "compression_ratio": 1.3225806451612903, "no_speech_prob": 6.641856089117937e-06}, {"id": 249, "seek": 163760, "start": 1656.9599999999998, "end": 1661.4399999999998, "text": " so another question is how what is the credibility of the", "tokens": [370, 1071, 1168, 307, 577, 437, 307, 264, 28852, 295, 264], "temperature": 0.0, "avg_logprob": -0.19191616979138604, "compression_ratio": 1.3225806451612903, "no_speech_prob": 6.641856089117937e-06}, {"id": 250, "seek": 166144, "start": 1661.44, "end": 1669.76, "text": " uh uh greenness uh that data is as good as the data published by the public api for example", "tokens": [2232, 2232, 3092, 1287, 2232, 300, 1412, 307, 382, 665, 382, 264, 1412, 6572, 538, 264, 1908, 1882, 72, 337, 1365], "temperature": 0.0, "avg_logprob": -0.11269405526174626, "compression_ratio": 1.763975155279503, "no_speech_prob": 3.218514029867947e-05}, {"id": 251, "seek": 166144, "start": 1671.2, "end": 1678.88, "text": " we have electricity map in us and national grid in europe and uh that is one of a one of a problem", "tokens": [321, 362, 10356, 4471, 294, 505, 293, 4048, 10748, 294, 27207, 293, 2232, 300, 307, 472, 295, 257, 472, 295, 257, 1154], "temperature": 0.0, "avg_logprob": -0.11269405526174626, "compression_ratio": 1.763975155279503, "no_speech_prob": 3.218514029867947e-05}, {"id": 252, "seek": 166144, "start": 1678.88, "end": 1684.3200000000002, "text": " as well that the the greenness or the accuracy of the carbon intensity is as good as the data", "tokens": [382, 731, 300, 264, 264, 3092, 1287, 420, 264, 14170, 295, 264, 5954, 13749, 307, 382, 665, 382, 264, 1412], "temperature": 0.0, "avg_logprob": -0.11269405526174626, "compression_ratio": 1.763975155279503, "no_speech_prob": 3.218514029867947e-05}, {"id": 253, "seek": 168432, "start": 1684.32, "end": 1692.72, "text": " that's being published by the public api we cannot control that", "tokens": [300, 311, 885, 6572, 538, 264, 1908, 1882, 72, 321, 2644, 1969, 300], "temperature": 0.0, "avg_logprob": -0.15733238908111072, "compression_ratio": 1.6987179487179487, "no_speech_prob": 2.667527769517619e-05}, {"id": 254, "seek": 168432, "start": 1700.08, "end": 1706.1599999999999, "text": " okay i should probably note that we will also aim for any data that's from the government so i think", "tokens": [1392, 741, 820, 1391, 3637, 300, 321, 486, 611, 5939, 337, 604, 1412, 300, 311, 490, 264, 2463, 370, 741, 519], "temperature": 0.0, "avg_logprob": -0.15733238908111072, "compression_ratio": 1.6987179487179487, "no_speech_prob": 2.667527769517619e-05}, {"id": 255, "seek": 170616, "start": 1706.16, "end": 1715.1200000000001, "text": " national grid is uh straight from is from the uk government so i think that's pretty reliable and", "tokens": [4048, 10748, 307, 2232, 2997, 490, 307, 490, 264, 26769, 2463, 370, 741, 519, 300, 311, 1238, 12924, 293], "temperature": 0.0, "avg_logprob": -0.1584767806224334, "compression_ratio": 1.4786324786324787, "no_speech_prob": 4.001249180873856e-05}, {"id": 256, "seek": 171512, "start": 1715.12, "end": 1739.04, "text": " we will always make sure that the data that we use is from reliable sources", "tokens": [50364, 321, 486, 1009, 652, 988, 300, 264, 1412, 300, 321, 764, 307, 490, 12924, 7139, 51560], "temperature": 0.0, "avg_logprob": -0.19564252429538304, "compression_ratio": 1.1029411764705883, "no_speech_prob": 1.2791740118700545e-05}], "language": "en"}