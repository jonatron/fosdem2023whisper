{"text": " All right, hey everyone, my name is Owen, this is Kavi, we're going to be talking about Loki today, this is a project that's very near and dear to my heart, been working on for a while, but I believe it's actually the second FOSDEM top on the subject. First one right here by Tom 2017 or 2018, so we'll cover some of the differences since then and let's jump in. So first of all we're going to get a few things from this talk, one how Loki works a little bit differently than a lot of things that came before it, and what some of those trade-offs look like and why I think that's an advantage, and then you can also learn some of the tips and tricks that we've learned building cloud-native distributed systems that need to be up all the time, and maybe you can incorporate some of that into your designs later. So we are both engineers at Grafana Labs. We work primarily on Loki, the open source project, but then also do a bunch to make sure it runs as part of our SAS, I'm Owen and this is Kavi again. And you can find the project here, this is our GitHub repo. We are operators and implementers, so we both build and operate and run the software. I use Loki every day to debug Loki, and that's kind of a labor of love, but it comes because we get paged a lot, well not a lot, I actually shouldn't say that on it, you know. But we do get paged and so we are very empathetic to that fact. This is actually the last time I was here in Belgium, this is in Ghent on top of the Gravenstein, this is not stage, I actually did get paged here. And so this is actually using Loki to figure out what was wrong with our hosted version of Loki itself. But this is my first time here at Fozdom, and it's been a lot of fun so far, so thanks for having me. So this was actually coined by a friend and colleague Ed Welch, Loki is a time series database, but for strings, this is effectively how Loki works at the end of the day. And so the first off we'll jump into figuring out what exactly a time series database is. So yeah, what exactly is time series database, right? So if you think from the like a normal database, all you always see like a key and value, right? And in time series database, surprise, surprise, you have timestamp. So what you see is like for every unique identifier, you have array of records, or tuple, what are you going to call. So on each record, we'll have a value and a timestamp attached to it, right? So in this example, as you see, so for this identifier, you have a value v0, a timestamp t0, and value v1, a timestamp t1, so on and so forth, right? So this is the mental model, we want you to like, yeah, I mean, keep in mind throughout the talk, so that you understand some of the decisions, why we made the way we are. So to see this in action, right? So this is how it looks like in Prometheus, and this is how it looks like in Loki. So what is identified here is a unique combination of this, what we call label set, right? Here it's app equal to NGNX, cluster equal to US central zero. So this is like a unique identifier, which has this list of records. And as you can see, the only difference between Prometheus and Loki here is the type of the value, right? So in the record, you see timestamp, comma, value, and the value is floor 64 in Prometheus, which is 34.5, you see here. And in Loki, it's a string. It's a log line you ingest into Loki. So that's what, when we say Loki is a time series for strings, that's what we mean. So yeah, it's a log line there. So we definitely take or steal, or however you want to put it, a lot from Prometheus itself. And this is actually what this looks like in terms of how we index and store data. We're going to talk a lot about indexing in this talk, or particularly the lack of indexing. So Loki just basically takes in index and metadata, but doesn't index all content at all, which tends to be the vast majority of all this data. So here we're looking at the set of Prometheus-style label values and the timestamp. All that is kind of controlled and used for routing purposes of queries. And then the log contents themselves, which, contrary to the size of the underlying graphs on the screen, the lines, the log contents are much, much larger. So that's the biggest piece here. So this allows us to kind of figure out and use our expertise for the systems that we run and use these labels, which tend to be topological, so the source of your contents. The application equals API, the cluster, the environment, that sort of thing. And then slice down to the time period that you care about, and we can use our own expertise as operators to figure out where we should look. And so as you can see here, this maybe is a pretty broad section corresponding to about a terabyte of data over that period, but you can mix and match these things. And so this goes to a fraction of that down to about 10 gigabytes by just looking at the sets of applications that we actually care about rather than all of the applications and replicas deployed in that cluster. So to give you a bit of a taste of what we mean when we say Loki is performant and Loki executes code faster, so this is what we mean. This is the metric we took from one of our internal cluster. So what you're basically seeing here is this particular Loki cell at peak, it's processing like 50 terabytes per day, right? And what you see in the UI, it's a Grafana UI, by the way, where you're running some lock yield, which is a Kodi language we use to get visibility of your logs that you ingest right into Loki. We'll talk about lock yield a bit later. So yeah, this is specifically a metric Kodi. This is, like, basically you are trying to figure out the metrics on the fly just from your logs without any instrumentation, right? So this particular query is processed like 10 terabytes of data in 12 seconds, which is almost like one terabytes per second throughput. So that's what we mean when we say, yeah, Loki is faster performant. Yeah, so my favorite piece here is that these are actually constructed from logs themselves are not Prometheus metrics or anything, we log query metadata for every query that comes into Loki and we're kind of extracting the part of the log line that talks about how fast it processes data and then accumulating it over a bunch of subqueries to get this final value and then graphing that over time. So let's step back now and I like to think about how Loki was designed kind of in response to what came before it, you know, if we look really far back, we remember using tail and grep on like individual log files and that's still in use a ton today, but it doesn't work so well when you start to need, you know, hundreds or thousands of machines and maybe they're ephemeral, right? So we had to kind of build new strategies for handling this and a bunch of things have come up in the past, you know, 10, 15 years. And sometimes it leads us into the next point where we've accumulated all of this complexity and sometimes we've really missed that experience and I like this tweet because it's just incredibly emblematic of that experience that sometimes I just really wish I did have grep on top of, you know, all of the log aggregation and on top of the underlying complexity and scale that we've accumulated over the past couple decades. Yeah, so broadly speaking, like, so that's one of the goal of Loki, at least on the query side, to have to take the same experience you have before, like with just grep and tail that you're confident with, can we bring the same experience in this modern cloud native distributor systems era, right, where you have your logs, like speeding out from different machines from different applications, yeah, similar setup, right? So like I mentioned before, Loki has this language called lockium, and that's how you query your logs back to get some visibility, and this is heavily inspired from Prometheus. So people who are familiar with Prometheus may already get, could I get a grasp here? So this particular query what you see at the top is basically saying, like, give all the logs from this particular namespace, let's say Loki day of 005, and then give me all the logs that matches only the error in the log line, right? So as you can see, the experience is like a pipe equal to error. So you can still combine multiple pipes here. So that's the kind of like experience we're talking about, right, so, yeah. So doesn't mean you have to, you can only like grep for only specific pattern, you can also grep for specific IDs, like use case can be like your order ID or trace ID you want to find in the logs. So you can also do some kind of regress match here, and also, like I said, you can also like put multiple pipelines here, right, you can do under R, doesn't matter. So it's basically like, first you choose which logs to look for, like a routing, what Owen was saying, and then you can do all your piping, so, to mix and match. So that's the idea we're talking about. So this query is a bit different, as you'd have seen, like compared to previous two examples, which we called as a lock query, and this is a metric query. So in the lock query, the output you see after when your query is executed, you will see, you're going to see like list of logs that matches the particular pattern, right? So here, it's a metric. So if you see what this query does, it's similar to the last one, but we added two different things, right, here, the rate and the sum by. So what this means is, without doing any instrumentation, like the logs are coming as it is, so you can really find your error per second rate of all your application aggregated by the container, which means, so, doesn't matter like how many applications running in this namespace, so you can aggregate by container just from this metric query. So yeah, that's the idea we are trying to, like, the experience we want to, like, with a lock here. Yeah. This is probably my favorite part about Loki of all the things, right, the ability to extract information at query time ultimately means that you can be reactive instead of proactive. You don't have to, you know, figure out a schema and make sure that you're recording something in a precise way before the bad thing happens, right, because a lot of the time it's the unknown unknowns that get us. And so this, the ability to extract this structure, you know, and particularly to do it from logs, really allows you to figure out when things go wrong rather than having to re-instrument before you can get that information. So next up, we're going to talk a little bit less about the experience querying it and more about how it's constructed. So this is the kind of design choices that we make. So particularly individually being able to scale different parts of the system, particularly tuning your kind of preference for cost versus latency. On the read path, Loki runs, is intended to run across commodity hardware, and our only real dependency is object storage. So we store everything cheaply in a generally managed solution, although you can, you know, run your own object storage or use file system backends if you prefer. So this is how Loki ingestion path architecture looks at the high level. So when you send logs to Loki, and this is what it goes through. So the key takeaway here is, like Owen said, the only external dependency you see here is the object store. So everything else is a component of Loki. So of course we have like different deployment models. So usually all these components can be put together in a single binary. So if you're new to Loki, you are starting just to play with it at maybe like a small scale. So you just run it as a single binary, as a single process, and you send all the logs to a single process. And the only, you just put the bucket name and you're done. So all your logs are getting ingested into Loki, just fine, right? So that's what when we say like, yeah, like less dependency in running Loki. Yeah, in this case, it's an ingestion path. And so one of the things about not indexing the log contents themselves means that it's really easy to ingest data from a bunch of different sources. So maybe you work at an organization, and one team writes in one language, one team writes in another language. They have no standardization over the formats that they're using, the schemas of the logs, if they're using structured logging. So you can have one team in JSON, one team who just pulls in NGINX logs, and then another team that uses something like log format. And that's all OK, because Loki doesn't really care. We just index the source of where this came from, along with the timing information. So you can, each individual team, in that sense, can extract that information when they query it and choose what and how that they actually do care about in their logs. Yes, speaking of query path, right? So this is high-level architecture on the query side. So again, these are all like Loki components, except like a two dependency here. One is object storage, of course. The other one is like optional, which is like a cache. So again, the same pattern applied here, right? So you can combine all these Loki components into a single binary, and you can just run it as a single process. And all you need to do is just point to the persistent object store, and then cache for the performance. And that's it. So you're good to go on the read path, right? So again, if you want to run in a highly available fashion, let's say you hit some scale and you want to tweak some things. So this is how particularly we run in our internal like SAS. So here you have more control in a way, say you can tweak individual component, you can scale individual component, and yeah, that's the idea. So again, the key thing here is the simple external dependencies. So yeah, and it's really powerful to be able to develop on a single binary running all these subcomponents or things that eventually run in microservices in a single process, and then being able to scale that out depending on your tolerance for scale and running HA or data replication, that sort of thing. So because the index in Loki is very minimal, it's just this routing information or the data topology, really. It allows this to be much, much smaller than you would otherwise think. So I actually pulled this from one of the clusters that we run internally. It's close to 40 terabytes a day and just shy of 140 megabytes of index. So it's much, much smaller. And this gives us a lot of benefits when we talk about some of the next stuff. Yeah, so if I were you, probably I'd be asking this question, okay, folks, you're talking a lot about tiny indexes, right? How on earth do you make the query faster? So also this thought comes from the idea like either you are like from an academic background or like a practitioner who is building a distributed systems, it's always been taught like if you want to make something faster, you index it, right? So here we are sharing our experience like where if you want to index everything to make everything faster, so at some point in scale, your index is going to be much larger than the actual data, right? And in our experience, like handling huge index creates much more problems at scale. So that's a whole idea here, right? So let's understand like how low key makes the query faster with the tiny index, right? And this is how. So don't let the image scare you. So let's understand piece by piece here. So at the top, you see the query that comes in, right? So for the sake of discussion, let's say this query is asking for the data for 1 hour period, right? Time period. So the query path architecture what you saw before, what the first thing it does is it takes this query, the huge query, and it try to make it like a sub-query by time split. So in this case, it splits this 1 hour query into 4, 15 minutes query. We call it a sub-query, right? And the trick is it doesn't stop here. So low key index is designed in such a way so that it can look into the index and say, hey, this many data, like this many bytes, it needs to touch. So we can dynamically decide how many worker pool you need to process that query. So that's where this performance comes from. So for example, like in this case, let's say this 15 minute sub-query is touching, I don't know, like 10 gigabytes of data. So and you can plan accordingly like how many worker pools I can schedule this query into, right? So let's think about this for a while, right? So now the key takeaway here is you get to have a control over your cost versus performance. So you start with low key and you hit some limit, right? And at some scale, you're going to hit the limit. And you can increase the performance just by adding more query pool, like more query workers. So yeah, that's like a key control we want to have with the low key. So yeah, that's how we make query faster. So the next thing we're going to talk about is retention. We get asked this a lot, particularly wanting to store things like application logs for some period of time. You know, we use 30 days as kind of our standard, but it could be whatever your use case is and then things like audit logs for much longer periods of time. This is pretty easily tunable in low key. But the important part here is we were talking about the index size earlier. As we don't index the log contents themselves, retention is very, very easy and cost efficient to do because all of our data is stored in object storage. And so if we extract that kind of earlier index sizing slide out to what it would look like in a year, this is roughly what we get. And so again, we just use this index for routing information, which means that all of the data is effectively live. There's no like rehydrating or, you know, hot and cold storage tiers. Everything is served out of object storage generally all the time. Now you can, there's some nuance there. You can put like caches in a couple of places and that sort of thing. But the idea that you can use object storage as your primary back end is very powerful, especially when you consider cost over long periods of time. All right. So yeah, we've been saying low key has been built for the operators and for dels, right? And when it comes to operation, yeah, if you've been run any database or any distributed systems at scale, so you always want to keep on like top of the latest release of whatever the product you're running, right, to get the latest optimization, latest features, so on and so forth. And the other use case is like sometimes you want to migrate your data, right, from one persistent store to another one. In this case, maybe one GCS bucket to another one, even across the cloud provider, right? You sometimes you find like maybe S3 is better. Like I can go with S3. I can change from GCS to S3. So with all these use cases, can we do all these operations with zero downtime? So can we do that? So that's something like we can do in low key. We have been doing it many times. So to give you a complete example here, and this is one of my favorite part of low key config, like we call it as a period config. So what you're seeing here is we have something called like schema version. So whenever we change anything with index or any new feature comes in, and if it's like we use different index format or something, we change this version. Like in this case, you see V11 to V12, right? And so what you can do is, let's say you want to start using this new feature from Jan 2023, right? All you need to do is go and put the version V12 from the start date and you're done. So low key can understand this and it can work with both different schema version. So this is how you can like upgrade your schema and production like lively without downtime. So this is one example. The other one is like the migration, right? Like I talked before. So you may want to move your data within the cloud provider or across the cloud provider. For example, it's the same thing here. So from 2023 of Jan, I need to store all my new data into like S3 instead of GCS, right? So you go back and you change this one config and you're done. So low key again understands this. So when the query comes in, it checks whether which data it's asking for, like which time range it's asking for, and it can go and fix the data accordingly, right? And yeah, again, without downtime. So it also works really well with the retention. Let's say if you have 30 day retention and yeah, after 30 days, you don't care. So all your new data is stored to the new bucket. So yeah, this is FOSDOM all about the community. So we launched low key at 2019 as open source and we have like active community going on. So these are some of the ways you can reach us. We have a public Slack and we have a community forum. And every month we also have a low key community call. We alternate between US and EU time zone. So yeah, come say hi. Yeah, we are happy to, and if any of the things which we talked about excites you, come talk to us. We'll be like more than happy to have a new contributor to the project. So yeah. Yeah, we should have asked this probably in the beginning, but is there anyone out there using Prometheus? Yeah, a couple. All right, a lot. Yeah, good. What about any low key users? Okay, not bad. Thanks. That makes me really happy. Anyone run into hard configuration problems with low key? Yeah. Oh, no hands. That's weird. No, I'm just kidding. Yeah, we got some work to do on. Yeah, so things to take away from the talk. Key is meant to largely function like a distributed grep that you can also kind of pull metrics and analytics out of. It's low footprint, storing things in object storage and not relying on schemas. And we really target kind of easy operations with that as well as low cost. And then please come join the community. Come talk to us. We're going to be hanging out for probably at least 10, 15 minutes if you have any questions. We'd love to hear from you. All right, thank you. Yeah, I can share. Any questions? Hey, great talk. I was wondering with the amount of query sharding you're doing, how are you kind of synchronizing the result in the end? Or is it like synchronized in the UI at the end? Or like, do you need to sort things because that might be very expensive? All right, so I had a little difficulty hearing that, but I think the question was, how do we synchronize sharding in the query engine? Is that in the end when you show it in the UI? When we show it in the UI, it happens a layer down in Loki itself. So we've already merged everything. I think the real answer to that is probably a lot longer than I can give in this question, but talk to me. It's one of my favorite things to talk about. Thanks. Great talk. This was mentioned several times that the main power of Loki is that it indexes only labels basically, and it doesn't index actual log messages. But to make log messages searchable, you want to have many labels in this case, and many labels often lead to cardinality explosions, how you deal with it. Is there any good recommendation? How many labels I should have? What are trade-offs here? Yeah, so the way that I think about this is you probably want to index where the log came from less than what's in it, right? So we definitely added the ability to kind of like, especially in PromTail configs, which is the agent that we write, the ability to like add extra things in there, and people have been really clever about what they put in. Unfortunately, that can also be somewhat of a foot gun at times. So I'd say index the things that correspond to your topology, right? Where the logs come from. So environment, application, cluster, that sort of thing. And less things that have to do with the contents of them themselves. There's probably a somewhat longer answer there, too, and then we've also been doing some recent work to make some of this less of a user concern, particularly about distribution of individual log stream throughput. So if you have one application which is logging like 10 megabytes a second, that can be really harmful on Loki in some ways. And so people got clever around splitting that out into different streams. But in the future, we're going to be doing a lot of that automatically and transparently behind Loki's API. Just to add one thing on top of what Owen said, like in log yield, which we didn't show here. So if you have something in your log lying itself that you want to treat it as a label, you can do it on the fly with the log yield query language itself. So we have some like a parser. If it's a log form, if it's a JSON parser, you can use it like labels, the things in your log line itself. Hi. Thank you for the great talk. I have many questions, but I'll ask one. Do you have any tips in terms of scaling query and caching layers? Like from my experience, usually they're very overutilized, but when people start querying, you get all the crashes. Do you have any code and ratios or anything? Yeah. So this is one of the things like how we expose configurations around query parallelism and controls. I wish I had a do over on this a few years back, because there's a couple different configurations. Largely around for every individual tenant, how many subqueries you want to be allowed to be processed per query at a time, and then there's also things like concurrency for each of your query components, right? How many go routines should you associate with or should you devote to running queries independently? But yeah, I got some work to do to make that easily digestible, if that's fair. So this year at work, I end up kind of giving to Loki for 10 minutes about 20 gigabytes of log from a 500 machine. It was just a one-time experiment that they ran from time to time. And we started using Loki. We optimized it as intended. So using index query was great. At a certain point, my colleagues come to me and say, we want the log string for each individual machine in a file. And I started asking Loki for this, and he took ages to extract this information. Now I hope you're going to tell me, this is not the use case for Loki. You did very well to use RC's log and just pushing things in a file. But if you have another answer, I'll be glad. We didn't catch the question fully, but my understanding is you're asking, like, can the log yield find logs coming from a single machine? Is that right? Am I getting it right? So you're talking about, I guess, some kind of federation, maybe? So why do you want to store it in the file in a single machine? Because they wanted to run analysis on a specific stream of log from a specific machine. So technically, you can store log files in the file system. So instead of using object storage, technically, you can use a file system. That's completely possible. But we encourage to use object storage when it comes to scale, because that's how it works. Yeah, is the question behind the question there changing where you actually store it so that you can then run your own processing tools on top? Yeah? Yeah? Okay. Yeah, that's actually a relatively common ask. It's not something that we support at the moment. We kind of have our own format that we store in object storage or whatnot. We do have some tooling, one's called chunk inspect, which allows you to kind of iterate through all of the chunks, which could be from a particular stream or log file. But it's not incredibly batteries included at the moment, if that makes sense. Hello, I have the use case that I store some telemetry data with my logs sometimes, like a metric or sometimes, which I don't want to be indexed, but I also don't want to encode in the actual log message because it's already structured data. Is it possible to have fields that are not indexed or data that are not indexed? It's funny that you asked that. Yeah, so there's kind of a growing question around non-index metadata like that, right? That's not actually stored in the index itself, but like includes some degree of structure. I know we see this as kind of a current need, particularly for like hotel formats, so it's something that we're looking into right now, actually. So thanks a lot, everyone. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.64, "text": " All right, hey everyone, my name is Owen, this is Kavi, we're going to be talking about", "tokens": [1057, 558, 11, 4177, 1518, 11, 452, 1315, 307, 32867, 11, 341, 307, 591, 18442, 11, 321, 434, 516, 281, 312, 1417, 466], "temperature": 0.0, "avg_logprob": -0.2446700680640436, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1407945305109024}, {"id": 1, "seek": 0, "start": 14.64, "end": 18.68, "text": " Loki today, this is a project that's very near and dear to my heart, been working on", "tokens": [37940, 965, 11, 341, 307, 257, 1716, 300, 311, 588, 2651, 293, 6875, 281, 452, 1917, 11, 668, 1364, 322], "temperature": 0.0, "avg_logprob": -0.2446700680640436, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1407945305109024}, {"id": 2, "seek": 0, "start": 18.68, "end": 23.44, "text": " for a while, but I believe it's actually the second FOSDEM top on the subject.", "tokens": [337, 257, 1339, 11, 457, 286, 1697, 309, 311, 767, 264, 1150, 479, 4367, 35, 6683, 1192, 322, 264, 3983, 13], "temperature": 0.0, "avg_logprob": -0.2446700680640436, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1407945305109024}, {"id": 3, "seek": 0, "start": 23.44, "end": 29.240000000000002, "text": " First one right here by Tom 2017 or 2018, so we'll cover some of the differences since", "tokens": [2386, 472, 558, 510, 538, 5041, 6591, 420, 6096, 11, 370, 321, 603, 2060, 512, 295, 264, 7300, 1670], "temperature": 0.0, "avg_logprob": -0.2446700680640436, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1407945305109024}, {"id": 4, "seek": 2924, "start": 29.24, "end": 32.64, "text": " then and let's jump in.", "tokens": [550, 293, 718, 311, 3012, 294, 13], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 5, "seek": 2924, "start": 32.64, "end": 38.32, "text": " So first of all we're going to get a few things from this talk, one how Loki works a little", "tokens": [407, 700, 295, 439, 321, 434, 516, 281, 483, 257, 1326, 721, 490, 341, 751, 11, 472, 577, 37940, 1985, 257, 707], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 6, "seek": 2924, "start": 38.32, "end": 42.5, "text": " bit differently than a lot of things that came before it, and what some of those trade-offs", "tokens": [857, 7614, 813, 257, 688, 295, 721, 300, 1361, 949, 309, 11, 293, 437, 512, 295, 729, 4923, 12, 19231], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 7, "seek": 2924, "start": 42.5, "end": 47.28, "text": " look like and why I think that's an advantage, and then you can also learn some of the tips", "tokens": [574, 411, 293, 983, 286, 519, 300, 311, 364, 5002, 11, 293, 550, 291, 393, 611, 1466, 512, 295, 264, 6082], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 8, "seek": 2924, "start": 47.28, "end": 52.92, "text": " and tricks that we've learned building cloud-native distributed systems that need to be up all", "tokens": [293, 11733, 300, 321, 600, 3264, 2390, 4588, 12, 77, 1166, 12631, 3652, 300, 643, 281, 312, 493, 439], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 9, "seek": 2924, "start": 52.92, "end": 58.36, "text": " the time, and maybe you can incorporate some of that into your designs later.", "tokens": [264, 565, 11, 293, 1310, 291, 393, 16091, 512, 295, 300, 666, 428, 11347, 1780, 13], "temperature": 0.0, "avg_logprob": -0.11581230163574219, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.123785012983717e-05}, {"id": 10, "seek": 5836, "start": 58.36, "end": 61.76, "text": " So we are both engineers at Grafana Labs.", "tokens": [407, 321, 366, 1293, 11955, 412, 8985, 69, 2095, 40047, 13], "temperature": 0.0, "avg_logprob": -0.15301398820774528, "compression_ratio": 1.5089285714285714, "no_speech_prob": 1.3594948541140184e-05}, {"id": 11, "seek": 5836, "start": 61.76, "end": 66.4, "text": " We work primarily on Loki, the open source project, but then also do a bunch to make", "tokens": [492, 589, 10029, 322, 37940, 11, 264, 1269, 4009, 1716, 11, 457, 550, 611, 360, 257, 3840, 281, 652], "temperature": 0.0, "avg_logprob": -0.15301398820774528, "compression_ratio": 1.5089285714285714, "no_speech_prob": 1.3594948541140184e-05}, {"id": 12, "seek": 5836, "start": 66.4, "end": 73.12, "text": " sure it runs as part of our SAS, I'm Owen and this is Kavi again.", "tokens": [988, 309, 6676, 382, 644, 295, 527, 33441, 11, 286, 478, 32867, 293, 341, 307, 591, 18442, 797, 13], "temperature": 0.0, "avg_logprob": -0.15301398820774528, "compression_ratio": 1.5089285714285714, "no_speech_prob": 1.3594948541140184e-05}, {"id": 13, "seek": 5836, "start": 73.12, "end": 78.28, "text": " And you can find the project here, this is our GitHub repo.", "tokens": [400, 291, 393, 915, 264, 1716, 510, 11, 341, 307, 527, 23331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.15301398820774528, "compression_ratio": 1.5089285714285714, "no_speech_prob": 1.3594948541140184e-05}, {"id": 14, "seek": 5836, "start": 78.28, "end": 83.03999999999999, "text": " We are operators and implementers, so we both build and operate and run the software.", "tokens": [492, 366, 19077, 293, 4445, 433, 11, 370, 321, 1293, 1322, 293, 9651, 293, 1190, 264, 4722, 13], "temperature": 0.0, "avg_logprob": -0.15301398820774528, "compression_ratio": 1.5089285714285714, "no_speech_prob": 1.3594948541140184e-05}, {"id": 15, "seek": 8304, "start": 83.04, "end": 92.16000000000001, "text": " I use Loki every day to debug Loki, and that's kind of a labor of love, but it comes because", "tokens": [286, 764, 37940, 633, 786, 281, 24083, 37940, 11, 293, 300, 311, 733, 295, 257, 5938, 295, 959, 11, 457, 309, 1487, 570], "temperature": 0.0, "avg_logprob": -0.1871506741172389, "compression_ratio": 1.5502392344497609, "no_speech_prob": 8.531042840331793e-05}, {"id": 16, "seek": 8304, "start": 92.16000000000001, "end": 97.16000000000001, "text": " we get paged a lot, well not a lot, I actually shouldn't say that on it, you know.", "tokens": [321, 483, 280, 2980, 257, 688, 11, 731, 406, 257, 688, 11, 286, 767, 4659, 380, 584, 300, 322, 309, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1871506741172389, "compression_ratio": 1.5502392344497609, "no_speech_prob": 8.531042840331793e-05}, {"id": 17, "seek": 8304, "start": 97.16000000000001, "end": 102.2, "text": " But we do get paged and so we are very empathetic to that fact.", "tokens": [583, 321, 360, 483, 280, 2980, 293, 370, 321, 366, 588, 27155, 3532, 281, 300, 1186, 13], "temperature": 0.0, "avg_logprob": -0.1871506741172389, "compression_ratio": 1.5502392344497609, "no_speech_prob": 8.531042840331793e-05}, {"id": 18, "seek": 8304, "start": 102.2, "end": 106.24000000000001, "text": " This is actually the last time I was here in Belgium, this is in Ghent on top of the", "tokens": [639, 307, 767, 264, 1036, 565, 286, 390, 510, 294, 28094, 11, 341, 307, 294, 20321, 317, 322, 1192, 295, 264], "temperature": 0.0, "avg_logprob": -0.1871506741172389, "compression_ratio": 1.5502392344497609, "no_speech_prob": 8.531042840331793e-05}, {"id": 19, "seek": 10624, "start": 106.24, "end": 113.11999999999999, "text": " Gravenstein, this is not stage, I actually did get paged here.", "tokens": [8985, 553, 9089, 11, 341, 307, 406, 3233, 11, 286, 767, 630, 483, 280, 2980, 510, 13], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 20, "seek": 10624, "start": 113.11999999999999, "end": 118.47999999999999, "text": " And so this is actually using Loki to figure out what was wrong with our hosted version", "tokens": [400, 370, 341, 307, 767, 1228, 37940, 281, 2573, 484, 437, 390, 2085, 365, 527, 19204, 3037], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 21, "seek": 10624, "start": 118.47999999999999, "end": 121.16, "text": " of Loki itself.", "tokens": [295, 37940, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 22, "seek": 10624, "start": 121.16, "end": 125.64, "text": " But this is my first time here at Fozdom, and it's been a lot of fun so far, so thanks", "tokens": [583, 341, 307, 452, 700, 565, 510, 412, 8564, 89, 4121, 11, 293, 309, 311, 668, 257, 688, 295, 1019, 370, 1400, 11, 370, 3231], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 23, "seek": 10624, "start": 125.64, "end": 129.92, "text": " for having me.", "tokens": [337, 1419, 385, 13], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 24, "seek": 10624, "start": 129.92, "end": 134.44, "text": " So this was actually coined by a friend and colleague Ed Welch, Loki is a time series", "tokens": [407, 341, 390, 767, 45222, 538, 257, 1277, 293, 13532, 3977, 3778, 339, 11, 37940, 307, 257, 565, 2638], "temperature": 0.0, "avg_logprob": -0.1849313735961914, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.5374720280524343e-05}, {"id": 25, "seek": 13444, "start": 134.44, "end": 139.76, "text": " database, but for strings, this is effectively how Loki works at the end of the day.", "tokens": [8149, 11, 457, 337, 13985, 11, 341, 307, 8659, 577, 37940, 1985, 412, 264, 917, 295, 264, 786, 13], "temperature": 0.0, "avg_logprob": -0.217277889251709, "compression_ratio": 1.7828054298642535, "no_speech_prob": 6.994221621425822e-05}, {"id": 26, "seek": 13444, "start": 139.76, "end": 146.44, "text": " And so the first off we'll jump into figuring out what exactly a time series database is.", "tokens": [400, 370, 264, 700, 766, 321, 603, 3012, 666, 15213, 484, 437, 2293, 257, 565, 2638, 8149, 307, 13], "temperature": 0.0, "avg_logprob": -0.217277889251709, "compression_ratio": 1.7828054298642535, "no_speech_prob": 6.994221621425822e-05}, {"id": 27, "seek": 13444, "start": 146.44, "end": 149.76, "text": " So yeah, what exactly is time series database, right?", "tokens": [407, 1338, 11, 437, 2293, 307, 565, 2638, 8149, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.217277889251709, "compression_ratio": 1.7828054298642535, "no_speech_prob": 6.994221621425822e-05}, {"id": 28, "seek": 13444, "start": 149.76, "end": 155.96, "text": " So if you think from the like a normal database, all you always see like a key and value, right?", "tokens": [407, 498, 291, 519, 490, 264, 411, 257, 2710, 8149, 11, 439, 291, 1009, 536, 411, 257, 2141, 293, 2158, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.217277889251709, "compression_ratio": 1.7828054298642535, "no_speech_prob": 6.994221621425822e-05}, {"id": 29, "seek": 13444, "start": 155.96, "end": 159.2, "text": " And in time series database, surprise, surprise, you have timestamp.", "tokens": [400, 294, 565, 2638, 8149, 11, 6365, 11, 6365, 11, 291, 362, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.217277889251709, "compression_ratio": 1.7828054298642535, "no_speech_prob": 6.994221621425822e-05}, {"id": 30, "seek": 15920, "start": 159.2, "end": 165.76, "text": " So what you see is like for every unique identifier, you have array of records, or tuple, what", "tokens": [407, 437, 291, 536, 307, 411, 337, 633, 3845, 45690, 11, 291, 362, 10225, 295, 7724, 11, 420, 2604, 781, 11, 437], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 31, "seek": 15920, "start": 165.76, "end": 166.92, "text": " are you going to call.", "tokens": [366, 291, 516, 281, 818, 13], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 32, "seek": 15920, "start": 166.92, "end": 171.51999999999998, "text": " So on each record, we'll have a value and a timestamp attached to it, right?", "tokens": [407, 322, 1184, 2136, 11, 321, 603, 362, 257, 2158, 293, 257, 49108, 1215, 8570, 281, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 33, "seek": 15920, "start": 171.51999999999998, "end": 177.32, "text": " So in this example, as you see, so for this identifier, you have a value v0, a timestamp", "tokens": [407, 294, 341, 1365, 11, 382, 291, 536, 11, 370, 337, 341, 45690, 11, 291, 362, 257, 2158, 371, 15, 11, 257, 49108, 1215], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 34, "seek": 15920, "start": 177.32, "end": 181.6, "text": " t0, and value v1, a timestamp t1, so on and so forth, right?", "tokens": [256, 15, 11, 293, 2158, 371, 16, 11, 257, 49108, 1215, 256, 16, 11, 370, 322, 293, 370, 5220, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 35, "seek": 15920, "start": 181.6, "end": 186.92, "text": " So this is the mental model, we want you to like, yeah, I mean, keep in mind throughout", "tokens": [407, 341, 307, 264, 4973, 2316, 11, 321, 528, 291, 281, 411, 11, 1338, 11, 286, 914, 11, 1066, 294, 1575, 3710], "temperature": 0.0, "avg_logprob": -0.18323563795823317, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0001017171234707348}, {"id": 36, "seek": 18692, "start": 186.92, "end": 191.55999999999997, "text": " the talk, so that you understand some of the decisions, why we made the way we are.", "tokens": [264, 751, 11, 370, 300, 291, 1223, 512, 295, 264, 5327, 11, 983, 321, 1027, 264, 636, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 37, "seek": 18692, "start": 191.55999999999997, "end": 194.11999999999998, "text": " So to see this in action, right?", "tokens": [407, 281, 536, 341, 294, 3069, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 38, "seek": 18692, "start": 194.11999999999998, "end": 197.79999999999998, "text": " So this is how it looks like in Prometheus, and this is how it looks like in Loki.", "tokens": [407, 341, 307, 577, 309, 1542, 411, 294, 2114, 649, 42209, 11, 293, 341, 307, 577, 309, 1542, 411, 294, 37940, 13], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 39, "seek": 18692, "start": 197.79999999999998, "end": 205.51999999999998, "text": " So what is identified here is a unique combination of this, what we call label set, right?", "tokens": [407, 437, 307, 9234, 510, 307, 257, 3845, 6562, 295, 341, 11, 437, 321, 818, 7645, 992, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 40, "seek": 18692, "start": 205.51999999999998, "end": 209.64, "text": " Here it's app equal to NGNX, cluster equal to US central zero.", "tokens": [1692, 309, 311, 724, 2681, 281, 426, 38, 45, 55, 11, 13630, 2681, 281, 2546, 5777, 4018, 13], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 41, "seek": 18692, "start": 209.64, "end": 213.88, "text": " So this is like a unique identifier, which has this list of records.", "tokens": [407, 341, 307, 411, 257, 3845, 45690, 11, 597, 575, 341, 1329, 295, 7724, 13], "temperature": 0.0, "avg_logprob": -0.13410251423463984, "compression_ratio": 1.743801652892562, "no_speech_prob": 5.9110538131790236e-05}, {"id": 42, "seek": 21388, "start": 213.88, "end": 219.51999999999998, "text": " And as you can see, the only difference between Prometheus and Loki here is the type of the", "tokens": [400, 382, 291, 393, 536, 11, 264, 787, 2649, 1296, 2114, 649, 42209, 293, 37940, 510, 307, 264, 2010, 295, 264], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 43, "seek": 21388, "start": 219.51999999999998, "end": 220.88, "text": " value, right?", "tokens": [2158, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 44, "seek": 21388, "start": 220.88, "end": 226.4, "text": " So in the record, you see timestamp, comma, value, and the value is floor 64 in Prometheus,", "tokens": [407, 294, 264, 2136, 11, 291, 536, 49108, 1215, 11, 22117, 11, 2158, 11, 293, 264, 2158, 307, 4123, 12145, 294, 2114, 649, 42209, 11], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 45, "seek": 21388, "start": 226.4, "end": 229.2, "text": " which is 34.5, you see here.", "tokens": [597, 307, 12790, 13, 20, 11, 291, 536, 510, 13], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 46, "seek": 21388, "start": 229.2, "end": 230.96, "text": " And in Loki, it's a string.", "tokens": [400, 294, 37940, 11, 309, 311, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 47, "seek": 21388, "start": 230.96, "end": 233.6, "text": " It's a log line you ingest into Loki.", "tokens": [467, 311, 257, 3565, 1622, 291, 3957, 377, 666, 37940, 13], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 48, "seek": 21388, "start": 233.6, "end": 239.12, "text": " So that's what, when we say Loki is a time series for strings, that's what we mean.", "tokens": [407, 300, 311, 437, 11, 562, 321, 584, 37940, 307, 257, 565, 2638, 337, 13985, 11, 300, 311, 437, 321, 914, 13], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 49, "seek": 21388, "start": 239.12, "end": 243.72, "text": " So yeah, it's a log line there.", "tokens": [407, 1338, 11, 309, 311, 257, 3565, 1622, 456, 13], "temperature": 0.0, "avg_logprob": -0.15466191218449518, "compression_ratio": 1.7215189873417722, "no_speech_prob": 9.568270616000518e-05}, {"id": 50, "seek": 24372, "start": 243.72, "end": 249.88, "text": " So we definitely take or steal, or however you want to put it, a lot from Prometheus itself.", "tokens": [407, 321, 2138, 747, 420, 11009, 11, 420, 4461, 291, 528, 281, 829, 309, 11, 257, 688, 490, 2114, 649, 42209, 2564, 13], "temperature": 0.0, "avg_logprob": -0.13177403863870873, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.93291327074985e-06}, {"id": 51, "seek": 24372, "start": 249.88, "end": 255.96, "text": " And this is actually what this looks like in terms of how we index and store data.", "tokens": [400, 341, 307, 767, 437, 341, 1542, 411, 294, 2115, 295, 577, 321, 8186, 293, 3531, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13177403863870873, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.93291327074985e-06}, {"id": 52, "seek": 24372, "start": 255.96, "end": 260.32, "text": " We're going to talk a lot about indexing in this talk, or particularly the lack of indexing.", "tokens": [492, 434, 516, 281, 751, 257, 688, 466, 8186, 278, 294, 341, 751, 11, 420, 4098, 264, 5011, 295, 8186, 278, 13], "temperature": 0.0, "avg_logprob": -0.13177403863870873, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.93291327074985e-06}, {"id": 53, "seek": 24372, "start": 260.32, "end": 266.16, "text": " So Loki just basically takes in index and metadata, but doesn't index all content at", "tokens": [407, 37940, 445, 1936, 2516, 294, 8186, 293, 26603, 11, 457, 1177, 380, 8186, 439, 2701, 412], "temperature": 0.0, "avg_logprob": -0.13177403863870873, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.93291327074985e-06}, {"id": 54, "seek": 24372, "start": 266.16, "end": 269.72, "text": " all, which tends to be the vast majority of all this data.", "tokens": [439, 11, 597, 12258, 281, 312, 264, 8369, 6286, 295, 439, 341, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13177403863870873, "compression_ratio": 1.6349206349206349, "no_speech_prob": 4.93291327074985e-06}, {"id": 55, "seek": 26972, "start": 269.72, "end": 276.16, "text": " So here we're looking at the set of Prometheus-style label values and the timestamp.", "tokens": [407, 510, 321, 434, 1237, 412, 264, 992, 295, 2114, 649, 42209, 12, 15014, 7645, 4190, 293, 264, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 56, "seek": 26972, "start": 276.16, "end": 280.52000000000004, "text": " All that is kind of controlled and used for routing purposes of queries.", "tokens": [1057, 300, 307, 733, 295, 10164, 293, 1143, 337, 32722, 9932, 295, 24109, 13], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 57, "seek": 26972, "start": 280.52000000000004, "end": 286.12, "text": " And then the log contents themselves, which, contrary to the size of the underlying graphs", "tokens": [400, 550, 264, 3565, 15768, 2969, 11, 597, 11, 19506, 281, 264, 2744, 295, 264, 14217, 24877], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 58, "seek": 26972, "start": 286.12, "end": 290.0, "text": " on the screen, the lines, the log contents are much, much larger.", "tokens": [322, 264, 2568, 11, 264, 3876, 11, 264, 3565, 15768, 366, 709, 11, 709, 4833, 13], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 59, "seek": 26972, "start": 290.0, "end": 293.24, "text": " So that's the biggest piece here.", "tokens": [407, 300, 311, 264, 3880, 2522, 510, 13], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 60, "seek": 26972, "start": 293.24, "end": 296.84000000000003, "text": " So this allows us to kind of figure out and use our expertise for the systems that we", "tokens": [407, 341, 4045, 505, 281, 733, 295, 2573, 484, 293, 764, 527, 11769, 337, 264, 3652, 300, 321], "temperature": 0.0, "avg_logprob": -0.11649584770202637, "compression_ratio": 1.7019607843137254, "no_speech_prob": 3.7811469155712985e-06}, {"id": 61, "seek": 29684, "start": 296.84, "end": 303.35999999999996, "text": " run and use these labels, which tend to be topological, so the source of your contents.", "tokens": [1190, 293, 764, 613, 16949, 11, 597, 3928, 281, 312, 1192, 4383, 11, 370, 264, 4009, 295, 428, 15768, 13], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 62, "seek": 29684, "start": 303.35999999999996, "end": 308.91999999999996, "text": " The application equals API, the cluster, the environment, that sort of thing.", "tokens": [440, 3861, 6915, 9362, 11, 264, 13630, 11, 264, 2823, 11, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 63, "seek": 29684, "start": 308.91999999999996, "end": 312.28, "text": " And then slice down to the time period that you care about, and we can use our own expertise", "tokens": [400, 550, 13153, 760, 281, 264, 565, 2896, 300, 291, 1127, 466, 11, 293, 321, 393, 764, 527, 1065, 11769], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 64, "seek": 29684, "start": 312.28, "end": 316.32, "text": " as operators to figure out where we should look.", "tokens": [382, 19077, 281, 2573, 484, 689, 321, 820, 574, 13], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 65, "seek": 29684, "start": 316.32, "end": 321.32, "text": " And so as you can see here, this maybe is a pretty broad section corresponding to about", "tokens": [400, 370, 382, 291, 393, 536, 510, 11, 341, 1310, 307, 257, 1238, 4152, 3541, 11760, 281, 466], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 66, "seek": 29684, "start": 321.32, "end": 326.28, "text": " a terabyte of data over that period, but you can mix and match these things.", "tokens": [257, 1796, 34529, 295, 1412, 670, 300, 2896, 11, 457, 291, 393, 2890, 293, 2995, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.12593400889429554, "compression_ratio": 1.6797153024911031, "no_speech_prob": 1.0943660527118482e-05}, {"id": 67, "seek": 32628, "start": 326.28, "end": 331.44, "text": " And so this goes to a fraction of that down to about 10 gigabytes by just looking at the", "tokens": [400, 370, 341, 1709, 281, 257, 14135, 295, 300, 760, 281, 466, 1266, 42741, 538, 445, 1237, 412, 264], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 68, "seek": 32628, "start": 331.44, "end": 335.71999999999997, "text": " sets of applications that we actually care about rather than all of the applications", "tokens": [6352, 295, 5821, 300, 321, 767, 1127, 466, 2831, 813, 439, 295, 264, 5821], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 69, "seek": 32628, "start": 335.71999999999997, "end": 340.35999999999996, "text": " and replicas deployed in that cluster.", "tokens": [293, 3248, 9150, 17826, 294, 300, 13630, 13], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 70, "seek": 32628, "start": 340.35999999999996, "end": 346.71999999999997, "text": " So to give you a bit of a taste of what we mean when we say Loki is performant and Loki", "tokens": [407, 281, 976, 291, 257, 857, 295, 257, 3939, 295, 437, 321, 914, 562, 321, 584, 37940, 307, 2042, 394, 293, 37940], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 71, "seek": 32628, "start": 346.71999999999997, "end": 349.76, "text": " executes code faster, so this is what we mean.", "tokens": [4454, 1819, 3089, 4663, 11, 370, 341, 307, 437, 321, 914, 13], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 72, "seek": 32628, "start": 349.76, "end": 353.15999999999997, "text": " This is the metric we took from one of our internal cluster.", "tokens": [639, 307, 264, 20678, 321, 1890, 490, 472, 295, 527, 6920, 13630, 13], "temperature": 0.0, "avg_logprob": -0.1344307637682148, "compression_ratio": 1.7215189873417722, "no_speech_prob": 4.421884568728274e-06}, {"id": 73, "seek": 35316, "start": 353.16, "end": 359.84000000000003, "text": " So what you're basically seeing here is this particular Loki cell at peak, it's processing", "tokens": [407, 437, 291, 434, 1936, 2577, 510, 307, 341, 1729, 37940, 2815, 412, 10651, 11, 309, 311, 9007], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 74, "seek": 35316, "start": 359.84000000000003, "end": 362.40000000000003, "text": " like 50 terabytes per day, right?", "tokens": [411, 2625, 1796, 24538, 680, 786, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 75, "seek": 35316, "start": 362.40000000000003, "end": 366.24, "text": " And what you see in the UI, it's a Grafana UI, by the way, where you're running some", "tokens": [400, 437, 291, 536, 294, 264, 15682, 11, 309, 311, 257, 8985, 69, 2095, 15682, 11, 538, 264, 636, 11, 689, 291, 434, 2614, 512], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 76, "seek": 35316, "start": 366.24, "end": 372.28000000000003, "text": " lock yield, which is a Kodi language we use to get visibility of your logs that you ingest", "tokens": [4017, 11257, 11, 597, 307, 257, 591, 30727, 2856, 321, 764, 281, 483, 19883, 295, 428, 20820, 300, 291, 3957, 377], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 77, "seek": 35316, "start": 372.28000000000003, "end": 373.28000000000003, "text": " right into Loki.", "tokens": [558, 666, 37940, 13], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 78, "seek": 35316, "start": 373.28000000000003, "end": 375.20000000000005, "text": " We'll talk about lock yield a bit later.", "tokens": [492, 603, 751, 466, 4017, 11257, 257, 857, 1780, 13], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 79, "seek": 35316, "start": 375.20000000000005, "end": 378.16, "text": " So yeah, this is specifically a metric Kodi.", "tokens": [407, 1338, 11, 341, 307, 4682, 257, 20678, 591, 30727, 13], "temperature": 0.0, "avg_logprob": -0.21580520429109273, "compression_ratio": 1.5866141732283465, "no_speech_prob": 2.8781043511116877e-05}, {"id": 80, "seek": 37816, "start": 378.16, "end": 384.88000000000005, "text": " This is, like, basically you are trying to figure out the metrics on the fly just from", "tokens": [639, 307, 11, 411, 11, 1936, 291, 366, 1382, 281, 2573, 484, 264, 16367, 322, 264, 3603, 445, 490], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 81, "seek": 37816, "start": 384.88000000000005, "end": 387.12, "text": " your logs without any instrumentation, right?", "tokens": [428, 20820, 1553, 604, 7198, 399, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 82, "seek": 37816, "start": 387.12, "end": 392.36, "text": " So this particular query is processed like 10 terabytes of data in 12 seconds, which", "tokens": [407, 341, 1729, 14581, 307, 18846, 411, 1266, 1796, 24538, 295, 1412, 294, 2272, 3949, 11, 597], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 83, "seek": 37816, "start": 392.36, "end": 395.12, "text": " is almost like one terabytes per second throughput.", "tokens": [307, 1920, 411, 472, 1796, 24538, 680, 1150, 44629, 13], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 84, "seek": 37816, "start": 395.12, "end": 400.48, "text": " So that's what we mean when we say, yeah, Loki is faster performant.", "tokens": [407, 300, 311, 437, 321, 914, 562, 321, 584, 11, 1338, 11, 37940, 307, 4663, 2042, 394, 13], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 85, "seek": 37816, "start": 400.48, "end": 404.44000000000005, "text": " Yeah, so my favorite piece here is that these are actually constructed from logs themselves", "tokens": [865, 11, 370, 452, 2954, 2522, 510, 307, 300, 613, 366, 767, 17083, 490, 20820, 2969], "temperature": 0.0, "avg_logprob": -0.175411798421619, "compression_ratio": 1.5867158671586716, "no_speech_prob": 1.1650337910396047e-05}, {"id": 86, "seek": 40444, "start": 404.44, "end": 408.48, "text": " are not Prometheus metrics or anything, we log query metadata for every query that comes", "tokens": [366, 406, 2114, 649, 42209, 16367, 420, 1340, 11, 321, 3565, 14581, 26603, 337, 633, 14581, 300, 1487], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 87, "seek": 40444, "start": 408.48, "end": 412.64, "text": " into Loki and we're kind of extracting the part of the log line that talks about how", "tokens": [666, 37940, 293, 321, 434, 733, 295, 49844, 264, 644, 295, 264, 3565, 1622, 300, 6686, 466, 577], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 88, "seek": 40444, "start": 412.64, "end": 416.64, "text": " fast it processes data and then accumulating it over a bunch of subqueries to get this", "tokens": [2370, 309, 7555, 1412, 293, 550, 12989, 12162, 309, 670, 257, 3840, 295, 1422, 358, 21659, 281, 483, 341], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 89, "seek": 40444, "start": 416.64, "end": 421.15999999999997, "text": " final value and then graphing that over time.", "tokens": [2572, 2158, 293, 550, 1295, 79, 571, 300, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 90, "seek": 40444, "start": 421.15999999999997, "end": 426.6, "text": " So let's step back now and I like to think about how Loki was designed kind of in response", "tokens": [407, 718, 311, 1823, 646, 586, 293, 286, 411, 281, 519, 466, 577, 37940, 390, 4761, 733, 295, 294, 4134], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 91, "seek": 40444, "start": 426.6, "end": 431.72, "text": " to what came before it, you know, if we look really far back, we remember using tail and", "tokens": [281, 437, 1361, 949, 309, 11, 291, 458, 11, 498, 321, 574, 534, 1400, 646, 11, 321, 1604, 1228, 6838, 293], "temperature": 0.0, "avg_logprob": -0.14113008483382297, "compression_ratio": 1.6875, "no_speech_prob": 8.134494237310719e-06}, {"id": 92, "seek": 43172, "start": 431.72, "end": 437.44000000000005, "text": " grep on like individual log files and that's still in use a ton today, but it doesn't work", "tokens": [6066, 79, 322, 411, 2609, 3565, 7098, 293, 300, 311, 920, 294, 764, 257, 2952, 965, 11, 457, 309, 1177, 380, 589], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 93, "seek": 43172, "start": 437.44000000000005, "end": 443.72, "text": " so well when you start to need, you know, hundreds or thousands of machines and maybe", "tokens": [370, 731, 562, 291, 722, 281, 643, 11, 291, 458, 11, 6779, 420, 5383, 295, 8379, 293, 1310], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 94, "seek": 43172, "start": 443.72, "end": 445.16, "text": " they're ephemeral, right?", "tokens": [436, 434, 308, 41245, 2790, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 95, "seek": 43172, "start": 445.16, "end": 450.08000000000004, "text": " So we had to kind of build new strategies for handling this and a bunch of things have", "tokens": [407, 321, 632, 281, 733, 295, 1322, 777, 9029, 337, 13175, 341, 293, 257, 3840, 295, 721, 362], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 96, "seek": 43172, "start": 450.08000000000004, "end": 453.44000000000005, "text": " come up in the past, you know, 10, 15 years.", "tokens": [808, 493, 294, 264, 1791, 11, 291, 458, 11, 1266, 11, 2119, 924, 13], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 97, "seek": 43172, "start": 453.44000000000005, "end": 458.68, "text": " And sometimes it leads us into the next point where we've accumulated all of this complexity", "tokens": [400, 2171, 309, 6689, 505, 666, 264, 958, 935, 689, 321, 600, 31346, 439, 295, 341, 14024], "temperature": 0.0, "avg_logprob": -0.1635649741232932, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.299125506193377e-05}, {"id": 98, "seek": 45868, "start": 458.68, "end": 463.28000000000003, "text": " and sometimes we've really missed that experience and I like this tweet because it's just incredibly", "tokens": [293, 2171, 321, 600, 534, 6721, 300, 1752, 293, 286, 411, 341, 15258, 570, 309, 311, 445, 6252], "temperature": 0.0, "avg_logprob": -0.171788489464486, "compression_ratio": 1.6706349206349207, "no_speech_prob": 3.7514862924581394e-05}, {"id": 99, "seek": 45868, "start": 463.28000000000003, "end": 469.88, "text": " emblematic of that experience that sometimes I just really wish I did have grep on top", "tokens": [35949, 2399, 295, 300, 1752, 300, 2171, 286, 445, 534, 3172, 286, 630, 362, 6066, 79, 322, 1192], "temperature": 0.0, "avg_logprob": -0.171788489464486, "compression_ratio": 1.6706349206349207, "no_speech_prob": 3.7514862924581394e-05}, {"id": 100, "seek": 45868, "start": 469.88, "end": 474.48, "text": " of, you know, all of the log aggregation and on top of the underlying complexity and scale", "tokens": [295, 11, 291, 458, 11, 439, 295, 264, 3565, 16743, 399, 293, 322, 1192, 295, 264, 14217, 14024, 293, 4373], "temperature": 0.0, "avg_logprob": -0.171788489464486, "compression_ratio": 1.6706349206349207, "no_speech_prob": 3.7514862924581394e-05}, {"id": 101, "seek": 45868, "start": 474.48, "end": 478.32, "text": " that we've accumulated over the past couple decades.", "tokens": [300, 321, 600, 31346, 670, 264, 1791, 1916, 7878, 13], "temperature": 0.0, "avg_logprob": -0.171788489464486, "compression_ratio": 1.6706349206349207, "no_speech_prob": 3.7514862924581394e-05}, {"id": 102, "seek": 45868, "start": 478.32, "end": 485.52, "text": " Yeah, so broadly speaking, like, so that's one of the goal of Loki, at least on the query", "tokens": [865, 11, 370, 19511, 4124, 11, 411, 11, 370, 300, 311, 472, 295, 264, 3387, 295, 37940, 11, 412, 1935, 322, 264, 14581], "temperature": 0.0, "avg_logprob": -0.171788489464486, "compression_ratio": 1.6706349206349207, "no_speech_prob": 3.7514862924581394e-05}, {"id": 103, "seek": 48552, "start": 485.52, "end": 490.52, "text": " side, to have to take the same experience you have before, like with just grep and tail", "tokens": [1252, 11, 281, 362, 281, 747, 264, 912, 1752, 291, 362, 949, 11, 411, 365, 445, 6066, 79, 293, 6838], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 104, "seek": 48552, "start": 490.52, "end": 494.59999999999997, "text": " that you're confident with, can we bring the same experience in this modern cloud native", "tokens": [300, 291, 434, 6679, 365, 11, 393, 321, 1565, 264, 912, 1752, 294, 341, 4363, 4588, 8470], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 105, "seek": 48552, "start": 494.59999999999997, "end": 499.15999999999997, "text": " distributor systems era, right, where you have your logs, like speeding out from different", "tokens": [49192, 3652, 4249, 11, 558, 11, 689, 291, 362, 428, 20820, 11, 411, 35593, 484, 490, 819], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 106, "seek": 48552, "start": 499.15999999999997, "end": 503.47999999999996, "text": " machines from different applications, yeah, similar setup, right?", "tokens": [8379, 490, 819, 5821, 11, 1338, 11, 2531, 8657, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 107, "seek": 48552, "start": 503.47999999999996, "end": 510.44, "text": " So like I mentioned before, Loki has this language called lockium, and that's how you", "tokens": [407, 411, 286, 2835, 949, 11, 37940, 575, 341, 2856, 1219, 4017, 2197, 11, 293, 300, 311, 577, 291], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 108, "seek": 48552, "start": 510.44, "end": 515.3199999999999, "text": " query your logs back to get some visibility, and this is heavily inspired from Prometheus.", "tokens": [14581, 428, 20820, 646, 281, 483, 512, 19883, 11, 293, 341, 307, 10950, 7547, 490, 2114, 649, 42209, 13], "temperature": 0.0, "avg_logprob": -0.19748914039741128, "compression_ratio": 1.728813559322034, "no_speech_prob": 9.55883733695373e-05}, {"id": 109, "seek": 51532, "start": 515.32, "end": 520.7600000000001, "text": " So people who are familiar with Prometheus may already get, could I get a grasp here?", "tokens": [407, 561, 567, 366, 4963, 365, 2114, 649, 42209, 815, 1217, 483, 11, 727, 286, 483, 257, 21743, 510, 30], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 110, "seek": 51532, "start": 520.7600000000001, "end": 527.1600000000001, "text": " So this particular query what you see at the top is basically saying, like, give all the", "tokens": [407, 341, 1729, 14581, 437, 291, 536, 412, 264, 1192, 307, 1936, 1566, 11, 411, 11, 976, 439, 264], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 111, "seek": 51532, "start": 527.1600000000001, "end": 532.84, "text": " logs from this particular namespace, let's say Loki day of 005, and then give me all", "tokens": [20820, 490, 341, 1729, 5288, 17940, 11, 718, 311, 584, 37940, 786, 295, 7143, 20, 11, 293, 550, 976, 385, 439], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 112, "seek": 51532, "start": 532.84, "end": 536.0, "text": " the logs that matches only the error in the log line, right?", "tokens": [264, 20820, 300, 10676, 787, 264, 6713, 294, 264, 3565, 1622, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 113, "seek": 51532, "start": 536.0, "end": 540.36, "text": " So as you can see, the experience is like a pipe equal to error.", "tokens": [407, 382, 291, 393, 536, 11, 264, 1752, 307, 411, 257, 11240, 2681, 281, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 114, "seek": 51532, "start": 540.36, "end": 543.2800000000001, "text": " So you can still combine multiple pipes here.", "tokens": [407, 291, 393, 920, 10432, 3866, 21882, 510, 13], "temperature": 0.0, "avg_logprob": -0.1401484582276471, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.972771057509817e-05}, {"id": 115, "seek": 54328, "start": 543.28, "end": 547.48, "text": " So that's the kind of like experience we're talking about, right, so, yeah.", "tokens": [407, 300, 311, 264, 733, 295, 411, 1752, 321, 434, 1417, 466, 11, 558, 11, 370, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 116, "seek": 54328, "start": 547.48, "end": 552.8399999999999, "text": " So doesn't mean you have to, you can only like grep for only specific pattern, you can", "tokens": [407, 1177, 380, 914, 291, 362, 281, 11, 291, 393, 787, 411, 6066, 79, 337, 787, 2685, 5102, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 117, "seek": 54328, "start": 552.8399999999999, "end": 558.36, "text": " also grep for specific IDs, like use case can be like your order ID or trace ID you", "tokens": [611, 6066, 79, 337, 2685, 48212, 11, 411, 764, 1389, 393, 312, 411, 428, 1668, 7348, 420, 13508, 7348, 291], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 118, "seek": 54328, "start": 558.36, "end": 559.8, "text": " want to find in the logs.", "tokens": [528, 281, 915, 294, 264, 20820, 13], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 119, "seek": 54328, "start": 559.8, "end": 563.6, "text": " So you can also do some kind of regress match here, and also, like I said, you can also", "tokens": [407, 291, 393, 611, 360, 512, 733, 295, 1121, 735, 2995, 510, 11, 293, 611, 11, 411, 286, 848, 11, 291, 393, 611], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 120, "seek": 54328, "start": 563.6, "end": 568.52, "text": " like put multiple pipelines here, right, you can do under R, doesn't matter.", "tokens": [411, 829, 3866, 40168, 510, 11, 558, 11, 291, 393, 360, 833, 497, 11, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.2170379982619989, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00010036514140665531}, {"id": 121, "seek": 56852, "start": 568.52, "end": 573.36, "text": " So it's basically like, first you choose which logs to look for, like a routing, what Owen", "tokens": [407, 309, 311, 1936, 411, 11, 700, 291, 2826, 597, 20820, 281, 574, 337, 11, 411, 257, 32722, 11, 437, 32867], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 122, "seek": 56852, "start": 573.36, "end": 577.4, "text": " was saying, and then you can do all your piping, so, to mix and match.", "tokens": [390, 1566, 11, 293, 550, 291, 393, 360, 439, 428, 35204, 11, 370, 11, 281, 2890, 293, 2995, 13], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 123, "seek": 56852, "start": 577.4, "end": 579.84, "text": " So that's the idea we're talking about.", "tokens": [407, 300, 311, 264, 1558, 321, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 124, "seek": 56852, "start": 579.84, "end": 584.84, "text": " So this query is a bit different, as you'd have seen, like compared to previous two examples,", "tokens": [407, 341, 14581, 307, 257, 857, 819, 11, 382, 291, 1116, 362, 1612, 11, 411, 5347, 281, 3894, 732, 5110, 11], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 125, "seek": 56852, "start": 584.84, "end": 588.1999999999999, "text": " which we called as a lock query, and this is a metric query.", "tokens": [597, 321, 1219, 382, 257, 4017, 14581, 11, 293, 341, 307, 257, 20678, 14581, 13], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 126, "seek": 56852, "start": 588.1999999999999, "end": 593.1999999999999, "text": " So in the lock query, the output you see after when your query is executed, you will see,", "tokens": [407, 294, 264, 4017, 14581, 11, 264, 5598, 291, 536, 934, 562, 428, 14581, 307, 17577, 11, 291, 486, 536, 11], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 127, "seek": 56852, "start": 593.1999999999999, "end": 597.0, "text": " you're going to see like list of logs that matches the particular pattern, right?", "tokens": [291, 434, 516, 281, 536, 411, 1329, 295, 20820, 300, 10676, 264, 1729, 5102, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19003468922206335, "compression_ratio": 1.7541528239202657, "no_speech_prob": 3.263588951085694e-05}, {"id": 128, "seek": 59700, "start": 597.0, "end": 598.64, "text": " So here, it's a metric.", "tokens": [407, 510, 11, 309, 311, 257, 20678, 13], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 129, "seek": 59700, "start": 598.64, "end": 603.96, "text": " So if you see what this query does, it's similar to the last one, but we added two different", "tokens": [407, 498, 291, 536, 437, 341, 14581, 775, 11, 309, 311, 2531, 281, 264, 1036, 472, 11, 457, 321, 3869, 732, 819], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 130, "seek": 59700, "start": 603.96, "end": 606.54, "text": " things, right, here, the rate and the sum by.", "tokens": [721, 11, 558, 11, 510, 11, 264, 3314, 293, 264, 2408, 538, 13], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 131, "seek": 59700, "start": 606.54, "end": 613.44, "text": " So what this means is, without doing any instrumentation, like the logs are coming as it is, so you can", "tokens": [407, 437, 341, 1355, 307, 11, 1553, 884, 604, 7198, 399, 11, 411, 264, 20820, 366, 1348, 382, 309, 307, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 132, "seek": 59700, "start": 613.44, "end": 619.08, "text": " really find your error per second rate of all your application aggregated by the container,", "tokens": [534, 915, 428, 6713, 680, 1150, 3314, 295, 439, 428, 3861, 16743, 770, 538, 264, 10129, 11], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 133, "seek": 59700, "start": 619.08, "end": 622.88, "text": " which means, so, doesn't matter like how many applications running in this namespace, so", "tokens": [597, 1355, 11, 370, 11, 1177, 380, 1871, 411, 577, 867, 5821, 2614, 294, 341, 5288, 17940, 11, 370], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 134, "seek": 59700, "start": 622.88, "end": 626.16, "text": " you can aggregate by container just from this metric query.", "tokens": [291, 393, 26118, 538, 10129, 445, 490, 341, 20678, 14581, 13], "temperature": 0.0, "avg_logprob": -0.12176039769099309, "compression_ratio": 1.8107142857142857, "no_speech_prob": 3.019911855517421e-05}, {"id": 135, "seek": 62616, "start": 626.16, "end": 633.04, "text": " So yeah, that's the idea we are trying to, like, the experience we want to, like, with", "tokens": [407, 1338, 11, 300, 311, 264, 1558, 321, 366, 1382, 281, 11, 411, 11, 264, 1752, 321, 528, 281, 11, 411, 11, 365], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 136, "seek": 62616, "start": 633.04, "end": 634.04, "text": " a lock here.", "tokens": [257, 4017, 510, 13], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 137, "seek": 62616, "start": 634.04, "end": 635.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 138, "seek": 62616, "start": 635.04, "end": 638.8399999999999, "text": " This is probably my favorite part about Loki of all the things, right, the ability to extract", "tokens": [639, 307, 1391, 452, 2954, 644, 466, 37940, 295, 439, 264, 721, 11, 558, 11, 264, 3485, 281, 8947], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 139, "seek": 62616, "start": 638.8399999999999, "end": 643.48, "text": " information at query time ultimately means that you can be reactive instead of proactive.", "tokens": [1589, 412, 14581, 565, 6284, 1355, 300, 291, 393, 312, 28897, 2602, 295, 28028, 13], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 140, "seek": 62616, "start": 643.48, "end": 647.0799999999999, "text": " You don't have to, you know, figure out a schema and make sure that you're recording", "tokens": [509, 500, 380, 362, 281, 11, 291, 458, 11, 2573, 484, 257, 34078, 293, 652, 988, 300, 291, 434, 6613], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 141, "seek": 62616, "start": 647.0799999999999, "end": 651.6, "text": " something in a precise way before the bad thing happens, right, because a lot of the", "tokens": [746, 294, 257, 13600, 636, 949, 264, 1578, 551, 2314, 11, 558, 11, 570, 257, 688, 295, 264], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 142, "seek": 62616, "start": 651.6, "end": 653.72, "text": " time it's the unknown unknowns that get us.", "tokens": [565, 309, 311, 264, 9841, 46048, 300, 483, 505, 13], "temperature": 0.0, "avg_logprob": -0.19741810939108678, "compression_ratio": 1.6993243243243243, "no_speech_prob": 3.0172051992849447e-05}, {"id": 143, "seek": 65372, "start": 653.72, "end": 660.08, "text": " And so this, the ability to extract this structure, you know, and particularly to do it from logs,", "tokens": [400, 370, 341, 11, 264, 3485, 281, 8947, 341, 3877, 11, 291, 458, 11, 293, 4098, 281, 360, 309, 490, 20820, 11], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 144, "seek": 65372, "start": 660.08, "end": 664.36, "text": " really allows you to figure out when things go wrong rather than having to re-instrument", "tokens": [534, 4045, 291, 281, 2573, 484, 562, 721, 352, 2085, 2831, 813, 1419, 281, 319, 12, 50024, 2206], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 145, "seek": 65372, "start": 664.36, "end": 669.24, "text": " before you can get that information.", "tokens": [949, 291, 393, 483, 300, 1589, 13], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 146, "seek": 65372, "start": 669.24, "end": 673.52, "text": " So next up, we're going to talk a little bit less about the experience querying it and", "tokens": [407, 958, 493, 11, 321, 434, 516, 281, 751, 257, 707, 857, 1570, 466, 264, 1752, 7083, 1840, 309, 293], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 147, "seek": 65372, "start": 673.52, "end": 675.5600000000001, "text": " more about how it's constructed.", "tokens": [544, 466, 577, 309, 311, 17083, 13], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 148, "seek": 65372, "start": 675.5600000000001, "end": 679.12, "text": " So this is the kind of design choices that we make.", "tokens": [407, 341, 307, 264, 733, 295, 1715, 7994, 300, 321, 652, 13], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 149, "seek": 65372, "start": 679.12, "end": 683.2, "text": " So particularly individually being able to scale different parts of the system, particularly", "tokens": [407, 4098, 16652, 885, 1075, 281, 4373, 819, 3166, 295, 264, 1185, 11, 4098], "temperature": 0.0, "avg_logprob": -0.153926027232203, "compression_ratio": 1.7097902097902098, "no_speech_prob": 9.960655916074757e-06}, {"id": 150, "seek": 68320, "start": 683.2, "end": 689.9200000000001, "text": " tuning your kind of preference for cost versus latency.", "tokens": [15164, 428, 733, 295, 17502, 337, 2063, 5717, 27043, 13], "temperature": 0.0, "avg_logprob": -0.17507567229094328, "compression_ratio": 1.5570776255707763, "no_speech_prob": 2.7080493964604102e-05}, {"id": 151, "seek": 68320, "start": 689.9200000000001, "end": 696.48, "text": " On the read path, Loki runs, is intended to run across commodity hardware, and our only", "tokens": [1282, 264, 1401, 3100, 11, 37940, 6676, 11, 307, 10226, 281, 1190, 2108, 29125, 8837, 11, 293, 527, 787], "temperature": 0.0, "avg_logprob": -0.17507567229094328, "compression_ratio": 1.5570776255707763, "no_speech_prob": 2.7080493964604102e-05}, {"id": 152, "seek": 68320, "start": 696.48, "end": 698.08, "text": " real dependency is object storage.", "tokens": [957, 33621, 307, 2657, 6725, 13], "temperature": 0.0, "avg_logprob": -0.17507567229094328, "compression_ratio": 1.5570776255707763, "no_speech_prob": 2.7080493964604102e-05}, {"id": 153, "seek": 68320, "start": 698.08, "end": 702.5600000000001, "text": " So we store everything cheaply in a generally managed solution, although you can, you know,", "tokens": [407, 321, 3531, 1203, 7084, 356, 294, 257, 5101, 6453, 3827, 11, 4878, 291, 393, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17507567229094328, "compression_ratio": 1.5570776255707763, "no_speech_prob": 2.7080493964604102e-05}, {"id": 154, "seek": 68320, "start": 702.5600000000001, "end": 711.08, "text": " run your own object storage or use file system backends if you prefer.", "tokens": [1190, 428, 1065, 2657, 6725, 420, 764, 3991, 1185, 646, 2581, 498, 291, 4382, 13], "temperature": 0.0, "avg_logprob": -0.17507567229094328, "compression_ratio": 1.5570776255707763, "no_speech_prob": 2.7080493964604102e-05}, {"id": 155, "seek": 71108, "start": 711.08, "end": 716.24, "text": " So this is how Loki ingestion path architecture looks at the high level.", "tokens": [407, 341, 307, 577, 37940, 3957, 31342, 3100, 9482, 1542, 412, 264, 1090, 1496, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 156, "seek": 71108, "start": 716.24, "end": 719.44, "text": " So when you send logs to Loki, and this is what it goes through.", "tokens": [407, 562, 291, 2845, 20820, 281, 37940, 11, 293, 341, 307, 437, 309, 1709, 807, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 157, "seek": 71108, "start": 719.44, "end": 725.9200000000001, "text": " So the key takeaway here is, like Owen said, the only external dependency you see here", "tokens": [407, 264, 2141, 30681, 510, 307, 11, 411, 32867, 848, 11, 264, 787, 8320, 33621, 291, 536, 510], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 158, "seek": 71108, "start": 725.9200000000001, "end": 727.24, "text": " is the object store.", "tokens": [307, 264, 2657, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 159, "seek": 71108, "start": 727.24, "end": 729.6, "text": " So everything else is a component of Loki.", "tokens": [407, 1203, 1646, 307, 257, 6542, 295, 37940, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 160, "seek": 71108, "start": 729.6, "end": 732.12, "text": " So of course we have like different deployment models.", "tokens": [407, 295, 1164, 321, 362, 411, 819, 19317, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 161, "seek": 71108, "start": 732.12, "end": 736.2, "text": " So usually all these components can be put together in a single binary.", "tokens": [407, 2673, 439, 613, 6677, 393, 312, 829, 1214, 294, 257, 2167, 17434, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 162, "seek": 71108, "start": 736.2, "end": 739.6800000000001, "text": " So if you're new to Loki, you are starting just to play with it at maybe like a small", "tokens": [407, 498, 291, 434, 777, 281, 37940, 11, 291, 366, 2891, 445, 281, 862, 365, 309, 412, 1310, 411, 257, 1359], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 163, "seek": 71108, "start": 739.6800000000001, "end": 740.6800000000001, "text": " scale.", "tokens": [4373, 13], "temperature": 0.0, "avg_logprob": -0.1231345690213717, "compression_ratio": 1.6877076411960132, "no_speech_prob": 2.3488979422836564e-05}, {"id": 164, "seek": 74068, "start": 740.68, "end": 744.12, "text": " So you just run it as a single binary, as a single process, and you send all the logs", "tokens": [407, 291, 445, 1190, 309, 382, 257, 2167, 17434, 11, 382, 257, 2167, 1399, 11, 293, 291, 2845, 439, 264, 20820], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 165, "seek": 74068, "start": 744.12, "end": 745.4799999999999, "text": " to a single process.", "tokens": [281, 257, 2167, 1399, 13], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 166, "seek": 74068, "start": 745.4799999999999, "end": 748.0, "text": " And the only, you just put the bucket name and you're done.", "tokens": [400, 264, 787, 11, 291, 445, 829, 264, 13058, 1315, 293, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 167, "seek": 74068, "start": 748.0, "end": 751.12, "text": " So all your logs are getting ingested into Loki, just fine, right?", "tokens": [407, 439, 428, 20820, 366, 1242, 3957, 21885, 666, 37940, 11, 445, 2489, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 168, "seek": 74068, "start": 751.12, "end": 756.0799999999999, "text": " So that's what when we say like, yeah, like less dependency in running Loki.", "tokens": [407, 300, 311, 437, 562, 321, 584, 411, 11, 1338, 11, 411, 1570, 33621, 294, 2614, 37940, 13], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 169, "seek": 74068, "start": 756.0799999999999, "end": 761.4, "text": " Yeah, in this case, it's an ingestion path.", "tokens": [865, 11, 294, 341, 1389, 11, 309, 311, 364, 3957, 31342, 3100, 13], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 170, "seek": 74068, "start": 761.4, "end": 766.12, "text": " And so one of the things about not indexing the log contents themselves means that it's", "tokens": [400, 370, 472, 295, 264, 721, 466, 406, 8186, 278, 264, 3565, 15768, 2969, 1355, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 171, "seek": 74068, "start": 766.12, "end": 769.3599999999999, "text": " really easy to ingest data from a bunch of different sources.", "tokens": [534, 1858, 281, 3957, 377, 1412, 490, 257, 3840, 295, 819, 7139, 13], "temperature": 0.0, "avg_logprob": -0.17221925554484346, "compression_ratio": 1.7439446366782008, "no_speech_prob": 3.066718636546284e-05}, {"id": 172, "seek": 76936, "start": 769.36, "end": 772.72, "text": " So maybe you work at an organization, and one team writes in one language, one team", "tokens": [407, 1310, 291, 589, 412, 364, 4475, 11, 293, 472, 1469, 13657, 294, 472, 2856, 11, 472, 1469], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 173, "seek": 76936, "start": 772.72, "end": 773.72, "text": " writes in another language.", "tokens": [13657, 294, 1071, 2856, 13], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 174, "seek": 76936, "start": 773.72, "end": 780.4, "text": " They have no standardization over the formats that they're using, the schemas of the logs,", "tokens": [814, 362, 572, 3832, 2144, 670, 264, 25879, 300, 436, 434, 1228, 11, 264, 22627, 296, 295, 264, 20820, 11], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 175, "seek": 76936, "start": 780.4, "end": 781.4, "text": " if they're using structured logging.", "tokens": [498, 436, 434, 1228, 18519, 27991, 13], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 176, "seek": 76936, "start": 781.4, "end": 787.6, "text": " So you can have one team in JSON, one team who just pulls in NGINX logs, and then another", "tokens": [407, 291, 393, 362, 472, 1469, 294, 31828, 11, 472, 1469, 567, 445, 16982, 294, 426, 38, 1464, 55, 20820, 11, 293, 550, 1071], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 177, "seek": 76936, "start": 787.6, "end": 789.16, "text": " team that uses something like log format.", "tokens": [1469, 300, 4960, 746, 411, 3565, 7877, 13], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 178, "seek": 76936, "start": 789.16, "end": 791.92, "text": " And that's all OK, because Loki doesn't really care.", "tokens": [400, 300, 311, 439, 2264, 11, 570, 37940, 1177, 380, 534, 1127, 13], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 179, "seek": 76936, "start": 791.92, "end": 796.8000000000001, "text": " We just index the source of where this came from, along with the timing information.", "tokens": [492, 445, 8186, 264, 4009, 295, 689, 341, 1361, 490, 11, 2051, 365, 264, 10822, 1589, 13], "temperature": 0.0, "avg_logprob": -0.15244569044846754, "compression_ratio": 1.7312925170068028, "no_speech_prob": 1.1648516192508396e-05}, {"id": 180, "seek": 79680, "start": 796.8, "end": 800.56, "text": " So you can, each individual team, in that sense, can extract that information when they", "tokens": [407, 291, 393, 11, 1184, 2609, 1469, 11, 294, 300, 2020, 11, 393, 8947, 300, 1589, 562, 436], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 181, "seek": 79680, "start": 800.56, "end": 806.04, "text": " query it and choose what and how that they actually do care about in their logs.", "tokens": [14581, 309, 293, 2826, 437, 293, 577, 300, 436, 767, 360, 1127, 466, 294, 641, 20820, 13], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 182, "seek": 79680, "start": 806.04, "end": 810.5999999999999, "text": " Yes, speaking of query path, right?", "tokens": [1079, 11, 4124, 295, 14581, 3100, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 183, "seek": 79680, "start": 810.5999999999999, "end": 813.68, "text": " So this is high-level architecture on the query side.", "tokens": [407, 341, 307, 1090, 12, 12418, 9482, 322, 264, 14581, 1252, 13], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 184, "seek": 79680, "start": 813.68, "end": 819.9599999999999, "text": " So again, these are all like Loki components, except like a two dependency here.", "tokens": [407, 797, 11, 613, 366, 439, 411, 37940, 6677, 11, 3993, 411, 257, 732, 33621, 510, 13], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 185, "seek": 79680, "start": 819.9599999999999, "end": 822.0, "text": " One is object storage, of course.", "tokens": [1485, 307, 2657, 6725, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 186, "seek": 79680, "start": 822.0, "end": 824.5999999999999, "text": " The other one is like optional, which is like a cache.", "tokens": [440, 661, 472, 307, 411, 17312, 11, 597, 307, 411, 257, 19459, 13], "temperature": 0.0, "avg_logprob": -0.15839874961159445, "compression_ratio": 1.6090225563909775, "no_speech_prob": 1.7764097719918936e-05}, {"id": 187, "seek": 82460, "start": 824.6, "end": 828.12, "text": " So again, the same pattern applied here, right?", "tokens": [407, 797, 11, 264, 912, 5102, 6456, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 188, "seek": 82460, "start": 828.12, "end": 832.0, "text": " So you can combine all these Loki components into a single binary, and you can just run", "tokens": [407, 291, 393, 10432, 439, 613, 37940, 6677, 666, 257, 2167, 17434, 11, 293, 291, 393, 445, 1190], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 189, "seek": 82460, "start": 832.0, "end": 833.5600000000001, "text": " it as a single process.", "tokens": [309, 382, 257, 2167, 1399, 13], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 190, "seek": 82460, "start": 833.5600000000001, "end": 837.6800000000001, "text": " And all you need to do is just point to the persistent object store, and then cache for", "tokens": [400, 439, 291, 643, 281, 360, 307, 445, 935, 281, 264, 24315, 2657, 3531, 11, 293, 550, 19459, 337], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 191, "seek": 82460, "start": 837.6800000000001, "end": 838.6800000000001, "text": " the performance.", "tokens": [264, 3389, 13], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 192, "seek": 82460, "start": 838.6800000000001, "end": 839.6800000000001, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 193, "seek": 82460, "start": 839.6800000000001, "end": 841.52, "text": " So you're good to go on the read path, right?", "tokens": [407, 291, 434, 665, 281, 352, 322, 264, 1401, 3100, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 194, "seek": 82460, "start": 841.52, "end": 847.4, "text": " So again, if you want to run in a highly available fashion, let's say you hit some scale and", "tokens": [407, 797, 11, 498, 291, 528, 281, 1190, 294, 257, 5405, 2435, 6700, 11, 718, 311, 584, 291, 2045, 512, 4373, 293], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 195, "seek": 82460, "start": 847.4, "end": 849.44, "text": " you want to tweak some things.", "tokens": [291, 528, 281, 29879, 512, 721, 13], "temperature": 0.0, "avg_logprob": -0.13775968551635742, "compression_ratio": 1.726923076923077, "no_speech_prob": 4.1893403249559924e-05}, {"id": 196, "seek": 84944, "start": 849.44, "end": 854.7600000000001, "text": " So this is how particularly we run in our internal like SAS.", "tokens": [407, 341, 307, 577, 4098, 321, 1190, 294, 527, 6920, 411, 33441, 13], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 197, "seek": 84944, "start": 854.7600000000001, "end": 859.9200000000001, "text": " So here you have more control in a way, say you can tweak individual component, you can", "tokens": [407, 510, 291, 362, 544, 1969, 294, 257, 636, 11, 584, 291, 393, 29879, 2609, 6542, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 198, "seek": 84944, "start": 859.9200000000001, "end": 863.2, "text": " scale individual component, and yeah, that's the idea.", "tokens": [4373, 2609, 6542, 11, 293, 1338, 11, 300, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 199, "seek": 84944, "start": 863.2, "end": 867.9200000000001, "text": " So again, the key thing here is the simple external dependencies.", "tokens": [407, 797, 11, 264, 2141, 551, 510, 307, 264, 2199, 8320, 36606, 13], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 200, "seek": 84944, "start": 867.9200000000001, "end": 872.2, "text": " So yeah, and it's really powerful to be able to develop on a single binary running all", "tokens": [407, 1338, 11, 293, 309, 311, 534, 4005, 281, 312, 1075, 281, 1499, 322, 257, 2167, 17434, 2614, 439], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 201, "seek": 84944, "start": 872.2, "end": 877.72, "text": " these subcomponents or things that eventually run in microservices in a single process, and", "tokens": [613, 1422, 21541, 40496, 420, 721, 300, 4728, 1190, 294, 15547, 47480, 294, 257, 2167, 1399, 11, 293], "temperature": 0.0, "avg_logprob": -0.1719318672462746, "compression_ratio": 1.7568627450980392, "no_speech_prob": 1.0936176295217592e-05}, {"id": 202, "seek": 87772, "start": 877.72, "end": 883.84, "text": " then being able to scale that out depending on your tolerance for scale and running HA", "tokens": [550, 885, 1075, 281, 4373, 300, 484, 5413, 322, 428, 23368, 337, 4373, 293, 2614, 11979], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 203, "seek": 87772, "start": 883.84, "end": 887.5600000000001, "text": " or data replication, that sort of thing.", "tokens": [420, 1412, 39911, 11, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 204, "seek": 87772, "start": 887.5600000000001, "end": 893.44, "text": " So because the index in Loki is very minimal, it's just this routing information or the", "tokens": [407, 570, 264, 8186, 294, 37940, 307, 588, 13206, 11, 309, 311, 445, 341, 32722, 1589, 420, 264], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 205, "seek": 87772, "start": 893.44, "end": 895.64, "text": " data topology, really.", "tokens": [1412, 1192, 1793, 11, 534, 13], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 206, "seek": 87772, "start": 895.64, "end": 898.52, "text": " It allows this to be much, much smaller than you would otherwise think.", "tokens": [467, 4045, 341, 281, 312, 709, 11, 709, 4356, 813, 291, 576, 5911, 519, 13], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 207, "seek": 87772, "start": 898.52, "end": 902.1600000000001, "text": " So I actually pulled this from one of the clusters that we run internally.", "tokens": [407, 286, 767, 7373, 341, 490, 472, 295, 264, 23313, 300, 321, 1190, 19501, 13], "temperature": 0.0, "avg_logprob": -0.1442687229443622, "compression_ratio": 1.610878661087866, "no_speech_prob": 8.526303645339794e-06}, {"id": 208, "seek": 90216, "start": 902.16, "end": 909.0799999999999, "text": " It's close to 40 terabytes a day and just shy of 140 megabytes of index.", "tokens": [467, 311, 1998, 281, 3356, 1796, 24538, 257, 786, 293, 445, 12685, 295, 21548, 10816, 24538, 295, 8186, 13], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 209, "seek": 90216, "start": 909.0799999999999, "end": 910.8399999999999, "text": " So it's much, much smaller.", "tokens": [407, 309, 311, 709, 11, 709, 4356, 13], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 210, "seek": 90216, "start": 910.8399999999999, "end": 915.16, "text": " And this gives us a lot of benefits when we talk about some of the next stuff.", "tokens": [400, 341, 2709, 505, 257, 688, 295, 5311, 562, 321, 751, 466, 512, 295, 264, 958, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 211, "seek": 90216, "start": 915.16, "end": 920.92, "text": " Yeah, so if I were you, probably I'd be asking this question, okay, folks, you're talking", "tokens": [865, 11, 370, 498, 286, 645, 291, 11, 1391, 286, 1116, 312, 3365, 341, 1168, 11, 1392, 11, 4024, 11, 291, 434, 1417], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 212, "seek": 90216, "start": 920.92, "end": 923.12, "text": " a lot about tiny indexes, right?", "tokens": [257, 688, 466, 5870, 8186, 279, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 213, "seek": 90216, "start": 923.12, "end": 925.1999999999999, "text": " How on earth do you make the query faster?", "tokens": [1012, 322, 4120, 360, 291, 652, 264, 14581, 4663, 30], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 214, "seek": 90216, "start": 925.1999999999999, "end": 931.4, "text": " So also this thought comes from the idea like either you are like from an academic background", "tokens": [407, 611, 341, 1194, 1487, 490, 264, 1558, 411, 2139, 291, 366, 411, 490, 364, 7778, 3678], "temperature": 0.0, "avg_logprob": -0.1690518856048584, "compression_ratio": 1.579136690647482, "no_speech_prob": 1.565976344863884e-05}, {"id": 215, "seek": 93140, "start": 931.4, "end": 935.84, "text": " or like a practitioner who is building a distributed systems, it's always been taught like if you", "tokens": [420, 411, 257, 32125, 567, 307, 2390, 257, 12631, 3652, 11, 309, 311, 1009, 668, 5928, 411, 498, 291], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 216, "seek": 93140, "start": 935.84, "end": 939.04, "text": " want to make something faster, you index it, right?", "tokens": [528, 281, 652, 746, 4663, 11, 291, 8186, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 217, "seek": 93140, "start": 939.04, "end": 944.92, "text": " So here we are sharing our experience like where if you want to index everything to make", "tokens": [407, 510, 321, 366, 5414, 527, 1752, 411, 689, 498, 291, 528, 281, 8186, 1203, 281, 652], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 218, "seek": 93140, "start": 944.92, "end": 949.56, "text": " everything faster, so at some point in scale, your index is going to be much larger than", "tokens": [1203, 4663, 11, 370, 412, 512, 935, 294, 4373, 11, 428, 8186, 307, 516, 281, 312, 709, 4833, 813], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 219, "seek": 93140, "start": 949.56, "end": 951.4399999999999, "text": " the actual data, right?", "tokens": [264, 3539, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 220, "seek": 93140, "start": 951.4399999999999, "end": 957.28, "text": " And in our experience, like handling huge index creates much more problems at scale.", "tokens": [400, 294, 527, 1752, 11, 411, 13175, 2603, 8186, 7829, 709, 544, 2740, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 221, "seek": 93140, "start": 957.28, "end": 959.52, "text": " So that's a whole idea here, right?", "tokens": [407, 300, 311, 257, 1379, 1558, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.138243990359099, "compression_ratio": 1.8153846153846154, "no_speech_prob": 1.7758007743395865e-05}, {"id": 222, "seek": 95952, "start": 959.52, "end": 966.52, "text": " So let's understand like how low key makes the query faster with the tiny index, right?", "tokens": [407, 718, 311, 1223, 411, 577, 2295, 2141, 1669, 264, 14581, 4663, 365, 264, 5870, 8186, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 223, "seek": 95952, "start": 966.52, "end": 968.0, "text": " And this is how.", "tokens": [400, 341, 307, 577, 13], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 224, "seek": 95952, "start": 968.0, "end": 970.72, "text": " So don't let the image scare you.", "tokens": [407, 500, 380, 718, 264, 3256, 17185, 291, 13], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 225, "seek": 95952, "start": 970.72, "end": 973.0799999999999, "text": " So let's understand piece by piece here.", "tokens": [407, 718, 311, 1223, 2522, 538, 2522, 510, 13], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 226, "seek": 95952, "start": 973.0799999999999, "end": 976.56, "text": " So at the top, you see the query that comes in, right?", "tokens": [407, 412, 264, 1192, 11, 291, 536, 264, 14581, 300, 1487, 294, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 227, "seek": 95952, "start": 976.56, "end": 982.76, "text": " So for the sake of discussion, let's say this query is asking for the data for 1 hour period,", "tokens": [407, 337, 264, 9717, 295, 5017, 11, 718, 311, 584, 341, 14581, 307, 3365, 337, 264, 1412, 337, 502, 1773, 2896, 11], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 228, "seek": 95952, "start": 982.76, "end": 983.76, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 229, "seek": 95952, "start": 983.76, "end": 984.76, "text": " Time period.", "tokens": [6161, 2896, 13], "temperature": 0.0, "avg_logprob": -0.17476248273662492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.502873212506529e-05}, {"id": 230, "seek": 98476, "start": 984.76, "end": 989.96, "text": " So the query path architecture what you saw before, what the first thing it does is it", "tokens": [407, 264, 14581, 3100, 9482, 437, 291, 1866, 949, 11, 437, 264, 700, 551, 309, 775, 307, 309], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 231, "seek": 98476, "start": 989.96, "end": 995.64, "text": " takes this query, the huge query, and it try to make it like a sub-query by time split.", "tokens": [2516, 341, 14581, 11, 264, 2603, 14581, 11, 293, 309, 853, 281, 652, 309, 411, 257, 1422, 12, 358, 2109, 538, 565, 7472, 13], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 232, "seek": 98476, "start": 995.64, "end": 1001.04, "text": " So in this case, it splits this 1 hour query into 4, 15 minutes query.", "tokens": [407, 294, 341, 1389, 11, 309, 37741, 341, 502, 1773, 14581, 666, 1017, 11, 2119, 2077, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 233, "seek": 98476, "start": 1001.04, "end": 1002.92, "text": " We call it a sub-query, right?", "tokens": [492, 818, 309, 257, 1422, 12, 358, 2109, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 234, "seek": 98476, "start": 1002.92, "end": 1005.16, "text": " And the trick is it doesn't stop here.", "tokens": [400, 264, 4282, 307, 309, 1177, 380, 1590, 510, 13], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 235, "seek": 98476, "start": 1005.16, "end": 1011.3199999999999, "text": " So low key index is designed in such a way so that it can look into the index and say,", "tokens": [407, 2295, 2141, 8186, 307, 4761, 294, 1270, 257, 636, 370, 300, 309, 393, 574, 666, 264, 8186, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.1563383464155526, "compression_ratio": 1.675, "no_speech_prob": 2.4237369871116243e-05}, {"id": 236, "seek": 101132, "start": 1011.32, "end": 1015.44, "text": " hey, this many data, like this many bytes, it needs to touch.", "tokens": [4177, 11, 341, 867, 1412, 11, 411, 341, 867, 36088, 11, 309, 2203, 281, 2557, 13], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 237, "seek": 101132, "start": 1015.44, "end": 1020.6, "text": " So we can dynamically decide how many worker pool you need to process that query.", "tokens": [407, 321, 393, 43492, 4536, 577, 867, 11346, 7005, 291, 643, 281, 1399, 300, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 238, "seek": 101132, "start": 1020.6, "end": 1023.1600000000001, "text": " So that's where this performance comes from.", "tokens": [407, 300, 311, 689, 341, 3389, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 239, "seek": 101132, "start": 1023.1600000000001, "end": 1028.96, "text": " So for example, like in this case, let's say this 15 minute sub-query is touching, I don't", "tokens": [407, 337, 1365, 11, 411, 294, 341, 1389, 11, 718, 311, 584, 341, 2119, 3456, 1422, 12, 358, 2109, 307, 11175, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 240, "seek": 101132, "start": 1028.96, "end": 1031.1200000000001, "text": " know, like 10 gigabytes of data.", "tokens": [458, 11, 411, 1266, 42741, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 241, "seek": 101132, "start": 1031.1200000000001, "end": 1037.04, "text": " So and you can plan accordingly like how many worker pools I can schedule this query into,", "tokens": [407, 293, 291, 393, 1393, 19717, 411, 577, 867, 11346, 28688, 286, 393, 7567, 341, 14581, 666, 11], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 242, "seek": 101132, "start": 1037.04, "end": 1038.04, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 243, "seek": 101132, "start": 1038.04, "end": 1040.2, "text": " So let's think about this for a while, right?", "tokens": [407, 718, 311, 519, 466, 341, 337, 257, 1339, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1487678558595719, "compression_ratio": 1.7404580152671756, "no_speech_prob": 5.903182318434119e-05}, {"id": 244, "seek": 104020, "start": 1040.2, "end": 1048.16, "text": " So now the key takeaway here is you get to have a control over your cost versus performance.", "tokens": [407, 586, 264, 2141, 30681, 510, 307, 291, 483, 281, 362, 257, 1969, 670, 428, 2063, 5717, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 245, "seek": 104020, "start": 1048.16, "end": 1052.1200000000001, "text": " So you start with low key and you hit some limit, right?", "tokens": [407, 291, 722, 365, 2295, 2141, 293, 291, 2045, 512, 4948, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 246, "seek": 104020, "start": 1052.1200000000001, "end": 1054.56, "text": " And at some scale, you're going to hit the limit.", "tokens": [400, 412, 512, 4373, 11, 291, 434, 516, 281, 2045, 264, 4948, 13], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 247, "seek": 104020, "start": 1054.56, "end": 1059.72, "text": " And you can increase the performance just by adding more query pool, like more query", "tokens": [400, 291, 393, 3488, 264, 3389, 445, 538, 5127, 544, 14581, 7005, 11, 411, 544, 14581], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 248, "seek": 104020, "start": 1059.72, "end": 1060.72, "text": " workers.", "tokens": [5600, 13], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 249, "seek": 104020, "start": 1060.72, "end": 1064.44, "text": " So yeah, that's like a key control we want to have with the low key.", "tokens": [407, 1338, 11, 300, 311, 411, 257, 2141, 1969, 321, 528, 281, 362, 365, 264, 2295, 2141, 13], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 250, "seek": 104020, "start": 1064.44, "end": 1067.92, "text": " So yeah, that's how we make query faster.", "tokens": [407, 1338, 11, 300, 311, 577, 321, 652, 14581, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1728882745865288, "compression_ratio": 1.7719298245614035, "no_speech_prob": 9.709570440463722e-05}, {"id": 251, "seek": 106792, "start": 1067.92, "end": 1072.4, "text": " So the next thing we're going to talk about is retention.", "tokens": [407, 264, 958, 551, 321, 434, 516, 281, 751, 466, 307, 22871, 13], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 252, "seek": 106792, "start": 1072.4, "end": 1077.96, "text": " We get asked this a lot, particularly wanting to store things like application logs for", "tokens": [492, 483, 2351, 341, 257, 688, 11, 4098, 7935, 281, 3531, 721, 411, 3861, 20820, 337], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 253, "seek": 106792, "start": 1077.96, "end": 1078.96, "text": " some period of time.", "tokens": [512, 2896, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 254, "seek": 106792, "start": 1078.96, "end": 1082.6000000000001, "text": " You know, we use 30 days as kind of our standard, but it could be whatever your use case is", "tokens": [509, 458, 11, 321, 764, 2217, 1708, 382, 733, 295, 527, 3832, 11, 457, 309, 727, 312, 2035, 428, 764, 1389, 307], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 255, "seek": 106792, "start": 1082.6000000000001, "end": 1086.48, "text": " and then things like audit logs for much longer periods of time.", "tokens": [293, 550, 721, 411, 17748, 20820, 337, 709, 2854, 13804, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 256, "seek": 106792, "start": 1086.48, "end": 1089.48, "text": " This is pretty easily tunable in low key.", "tokens": [639, 307, 1238, 3612, 4267, 712, 294, 2295, 2141, 13], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 257, "seek": 106792, "start": 1089.48, "end": 1094.0, "text": " But the important part here is we were talking about the index size earlier.", "tokens": [583, 264, 1021, 644, 510, 307, 321, 645, 1417, 466, 264, 8186, 2744, 3071, 13], "temperature": 0.0, "avg_logprob": -0.11744806116277522, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8330849343328737e-05}, {"id": 258, "seek": 109400, "start": 1094.0, "end": 1101.36, "text": " As we don't index the log contents themselves, retention is very, very easy and cost efficient", "tokens": [1018, 321, 500, 380, 8186, 264, 3565, 15768, 2969, 11, 22871, 307, 588, 11, 588, 1858, 293, 2063, 7148], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 259, "seek": 109400, "start": 1101.36, "end": 1105.4, "text": " to do because all of our data is stored in object storage.", "tokens": [281, 360, 570, 439, 295, 527, 1412, 307, 12187, 294, 2657, 6725, 13], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 260, "seek": 109400, "start": 1105.4, "end": 1109.16, "text": " And so if we extract that kind of earlier index sizing slide out to what it would look", "tokens": [400, 370, 498, 321, 8947, 300, 733, 295, 3071, 8186, 45435, 4137, 484, 281, 437, 309, 576, 574], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 261, "seek": 109400, "start": 1109.16, "end": 1113.12, "text": " like in a year, this is roughly what we get.", "tokens": [411, 294, 257, 1064, 11, 341, 307, 9810, 437, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 262, "seek": 109400, "start": 1113.12, "end": 1117.68, "text": " And so again, we just use this index for routing information, which means that all of the data", "tokens": [400, 370, 797, 11, 321, 445, 764, 341, 8186, 337, 32722, 1589, 11, 597, 1355, 300, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 263, "seek": 109400, "start": 1117.68, "end": 1118.92, "text": " is effectively live.", "tokens": [307, 8659, 1621, 13], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 264, "seek": 109400, "start": 1118.92, "end": 1123.16, "text": " There's no like rehydrating or, you know, hot and cold storage tiers.", "tokens": [821, 311, 572, 411, 319, 21591, 8754, 420, 11, 291, 458, 11, 2368, 293, 3554, 6725, 40563, 13], "temperature": 0.0, "avg_logprob": -0.1061021327972412, "compression_ratio": 1.7065217391304348, "no_speech_prob": 3.965643372794148e-06}, {"id": 265, "seek": 112316, "start": 1123.16, "end": 1126.0400000000002, "text": " Everything is served out of object storage generally all the time.", "tokens": [5471, 307, 7584, 484, 295, 2657, 6725, 5101, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 266, "seek": 112316, "start": 1126.0400000000002, "end": 1127.68, "text": " Now you can, there's some nuance there.", "tokens": [823, 291, 393, 11, 456, 311, 512, 42625, 456, 13], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 267, "seek": 112316, "start": 1127.68, "end": 1130.76, "text": " You can put like caches in a couple of places and that sort of thing.", "tokens": [509, 393, 829, 411, 269, 13272, 294, 257, 1916, 295, 3190, 293, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 268, "seek": 112316, "start": 1130.76, "end": 1136.4, "text": " But the idea that you can use object storage as your primary back end is very powerful,", "tokens": [583, 264, 1558, 300, 291, 393, 764, 2657, 6725, 382, 428, 6194, 646, 917, 307, 588, 4005, 11], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 269, "seek": 112316, "start": 1136.4, "end": 1141.44, "text": " especially when you consider cost over long periods of time.", "tokens": [2318, 562, 291, 1949, 2063, 670, 938, 13804, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 270, "seek": 112316, "start": 1141.44, "end": 1144.0800000000002, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 271, "seek": 112316, "start": 1144.0800000000002, "end": 1149.2, "text": " So yeah, we've been saying low key has been built for the operators and for dels, right?", "tokens": [407, 1338, 11, 321, 600, 668, 1566, 2295, 2141, 575, 668, 3094, 337, 264, 19077, 293, 337, 1103, 82, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2443000548476473, "compression_ratio": 1.6346153846153846, "no_speech_prob": 2.6662157324608415e-05}, {"id": 272, "seek": 114920, "start": 1149.2, "end": 1154.2, "text": " And when it comes to operation, yeah, if you've been run any database or any distributed", "tokens": [400, 562, 309, 1487, 281, 6916, 11, 1338, 11, 498, 291, 600, 668, 1190, 604, 8149, 420, 604, 12631], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 273, "seek": 114920, "start": 1154.2, "end": 1159.64, "text": " systems at scale, so you always want to keep on like top of the latest release of whatever", "tokens": [3652, 412, 4373, 11, 370, 291, 1009, 528, 281, 1066, 322, 411, 1192, 295, 264, 6792, 4374, 295, 2035], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 274, "seek": 114920, "start": 1159.64, "end": 1163.0800000000002, "text": " the product you're running, right, to get the latest optimization, latest features,", "tokens": [264, 1674, 291, 434, 2614, 11, 558, 11, 281, 483, 264, 6792, 19618, 11, 6792, 4122, 11], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 275, "seek": 114920, "start": 1163.0800000000002, "end": 1164.48, "text": " so on and so forth.", "tokens": [370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 276, "seek": 114920, "start": 1164.48, "end": 1169.28, "text": " And the other use case is like sometimes you want to migrate your data, right, from one", "tokens": [400, 264, 661, 764, 1389, 307, 411, 2171, 291, 528, 281, 31821, 428, 1412, 11, 558, 11, 490, 472], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 277, "seek": 114920, "start": 1169.28, "end": 1170.48, "text": " persistent store to another one.", "tokens": [24315, 3531, 281, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 278, "seek": 114920, "start": 1170.48, "end": 1175.04, "text": " In this case, maybe one GCS bucket to another one, even across the cloud provider, right?", "tokens": [682, 341, 1389, 11, 1310, 472, 460, 26283, 13058, 281, 1071, 472, 11, 754, 2108, 264, 4588, 12398, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 279, "seek": 114920, "start": 1175.04, "end": 1177.44, "text": " You sometimes you find like maybe S3 is better.", "tokens": [509, 2171, 291, 915, 411, 1310, 318, 18, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 280, "seek": 114920, "start": 1177.44, "end": 1178.8, "text": " Like I can go with S3.", "tokens": [1743, 286, 393, 352, 365, 318, 18, 13], "temperature": 0.0, "avg_logprob": -0.17439663246886372, "compression_ratio": 1.7331288343558282, "no_speech_prob": 4.3892865505767986e-05}, {"id": 281, "seek": 117880, "start": 1178.8, "end": 1180.6, "text": " I can change from GCS to S3.", "tokens": [286, 393, 1319, 490, 460, 26283, 281, 318, 18, 13], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 282, "seek": 117880, "start": 1180.6, "end": 1185.68, "text": " So with all these use cases, can we do all these operations with zero downtime?", "tokens": [407, 365, 439, 613, 764, 3331, 11, 393, 321, 360, 439, 613, 7705, 365, 4018, 49648, 30], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 283, "seek": 117880, "start": 1185.68, "end": 1186.8, "text": " So can we do that?", "tokens": [407, 393, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 284, "seek": 117880, "start": 1186.8, "end": 1189.24, "text": " So that's something like we can do in low key.", "tokens": [407, 300, 311, 746, 411, 321, 393, 360, 294, 2295, 2141, 13], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 285, "seek": 117880, "start": 1189.24, "end": 1190.6, "text": " We have been doing it many times.", "tokens": [492, 362, 668, 884, 309, 867, 1413, 13], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 286, "seek": 117880, "start": 1190.6, "end": 1195.0, "text": " So to give you a complete example here, and this is one of my favorite part of low key", "tokens": [407, 281, 976, 291, 257, 3566, 1365, 510, 11, 293, 341, 307, 472, 295, 452, 2954, 644, 295, 2295, 2141], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 287, "seek": 117880, "start": 1195.0, "end": 1197.72, "text": " config, like we call it as a period config.", "tokens": [6662, 11, 411, 321, 818, 309, 382, 257, 2896, 6662, 13], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 288, "seek": 117880, "start": 1197.72, "end": 1202.48, "text": " So what you're seeing here is we have something called like schema version.", "tokens": [407, 437, 291, 434, 2577, 510, 307, 321, 362, 746, 1219, 411, 34078, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 289, "seek": 117880, "start": 1202.48, "end": 1206.6, "text": " So whenever we change anything with index or any new feature comes in, and if it's like", "tokens": [407, 5699, 321, 1319, 1340, 365, 8186, 420, 604, 777, 4111, 1487, 294, 11, 293, 498, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.14796457428863083, "compression_ratio": 1.7526132404181185, "no_speech_prob": 2.8385305995470844e-05}, {"id": 290, "seek": 120660, "start": 1206.6, "end": 1210.52, "text": " we use different index format or something, we change this version.", "tokens": [321, 764, 819, 8186, 7877, 420, 746, 11, 321, 1319, 341, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 291, "seek": 120660, "start": 1210.52, "end": 1214.7199999999998, "text": " Like in this case, you see V11 to V12, right?", "tokens": [1743, 294, 341, 1389, 11, 291, 536, 691, 5348, 281, 691, 4762, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 292, "seek": 120660, "start": 1214.7199999999998, "end": 1219.3999999999999, "text": " And so what you can do is, let's say you want to start using this new feature from", "tokens": [400, 370, 437, 291, 393, 360, 307, 11, 718, 311, 584, 291, 528, 281, 722, 1228, 341, 777, 4111, 490], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 293, "seek": 120660, "start": 1219.3999999999999, "end": 1221.3999999999999, "text": " Jan 2023, right?", "tokens": [4956, 44377, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 294, "seek": 120660, "start": 1221.3999999999999, "end": 1225.6799999999998, "text": " All you need to do is go and put the version V12 from the start date and you're done.", "tokens": [1057, 291, 643, 281, 360, 307, 352, 293, 829, 264, 3037, 691, 4762, 490, 264, 722, 4002, 293, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 295, "seek": 120660, "start": 1225.6799999999998, "end": 1231.6399999999999, "text": " So low key can understand this and it can work with both different schema version.", "tokens": [407, 2295, 2141, 393, 1223, 341, 293, 309, 393, 589, 365, 1293, 819, 34078, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14304268246605284, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.59232313814573e-05}, {"id": 296, "seek": 123164, "start": 1231.64, "end": 1238.6000000000001, "text": " So this is how you can like upgrade your schema and production like lively without downtime.", "tokens": [407, 341, 307, 577, 291, 393, 411, 11484, 428, 34078, 293, 4265, 411, 30866, 1553, 49648, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 297, "seek": 123164, "start": 1238.6000000000001, "end": 1241.16, "text": " So this is one example.", "tokens": [407, 341, 307, 472, 1365, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 298, "seek": 123164, "start": 1241.16, "end": 1243.4, "text": " The other one is like the migration, right?", "tokens": [440, 661, 472, 307, 411, 264, 17011, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 299, "seek": 123164, "start": 1243.4, "end": 1244.6000000000001, "text": " Like I talked before.", "tokens": [1743, 286, 2825, 949, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 300, "seek": 123164, "start": 1244.6000000000001, "end": 1250.76, "text": " So you may want to move your data within the cloud provider or across the cloud provider.", "tokens": [407, 291, 815, 528, 281, 1286, 428, 1412, 1951, 264, 4588, 12398, 420, 2108, 264, 4588, 12398, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 301, "seek": 123164, "start": 1250.76, "end": 1252.48, "text": " For example, it's the same thing here.", "tokens": [1171, 1365, 11, 309, 311, 264, 912, 551, 510, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 302, "seek": 123164, "start": 1252.48, "end": 1258.76, "text": " So from 2023 of Jan, I need to store all my new data into like S3 instead of GCS, right?", "tokens": [407, 490, 44377, 295, 4956, 11, 286, 643, 281, 3531, 439, 452, 777, 1412, 666, 411, 318, 18, 2602, 295, 460, 26283, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 303, "seek": 123164, "start": 1258.76, "end": 1261.5600000000002, "text": " So you go back and you change this one config and you're done.", "tokens": [407, 291, 352, 646, 293, 291, 1319, 341, 472, 6662, 293, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.12309021334494313, "compression_ratio": 1.6836363636363636, "no_speech_prob": 7.348611688939855e-05}, {"id": 304, "seek": 126156, "start": 1261.56, "end": 1263.04, "text": " So low key again understands this.", "tokens": [407, 2295, 2141, 797, 15146, 341, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 305, "seek": 126156, "start": 1263.04, "end": 1267.8, "text": " So when the query comes in, it checks whether which data it's asking for, like which time", "tokens": [407, 562, 264, 14581, 1487, 294, 11, 309, 13834, 1968, 597, 1412, 309, 311, 3365, 337, 11, 411, 597, 565], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 306, "seek": 126156, "start": 1267.8, "end": 1272.04, "text": " range it's asking for, and it can go and fix the data accordingly, right?", "tokens": [3613, 309, 311, 3365, 337, 11, 293, 309, 393, 352, 293, 3191, 264, 1412, 19717, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 307, "seek": 126156, "start": 1272.04, "end": 1273.84, "text": " And yeah, again, without downtime.", "tokens": [400, 1338, 11, 797, 11, 1553, 49648, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 308, "seek": 126156, "start": 1273.84, "end": 1276.3999999999999, "text": " So it also works really well with the retention.", "tokens": [407, 309, 611, 1985, 534, 731, 365, 264, 22871, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 309, "seek": 126156, "start": 1276.3999999999999, "end": 1280.8, "text": " Let's say if you have 30 day retention and yeah, after 30 days, you don't care.", "tokens": [961, 311, 584, 498, 291, 362, 2217, 786, 22871, 293, 1338, 11, 934, 2217, 1708, 11, 291, 500, 380, 1127, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 310, "seek": 126156, "start": 1280.8, "end": 1286.08, "text": " So all your new data is stored to the new bucket.", "tokens": [407, 439, 428, 777, 1412, 307, 12187, 281, 264, 777, 13058, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 311, "seek": 126156, "start": 1286.08, "end": 1288.84, "text": " So yeah, this is FOSDOM all about the community.", "tokens": [407, 1338, 11, 341, 307, 479, 4367, 35, 5251, 439, 466, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.18339848518371582, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.00015994040586519986}, {"id": 312, "seek": 128884, "start": 1288.84, "end": 1294.9599999999998, "text": " So we launched low key at 2019 as open source and we have like active community going on.", "tokens": [407, 321, 8730, 2295, 2141, 412, 6071, 382, 1269, 4009, 293, 321, 362, 411, 4967, 1768, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 313, "seek": 128884, "start": 1294.9599999999998, "end": 1297.56, "text": " So these are some of the ways you can reach us.", "tokens": [407, 613, 366, 512, 295, 264, 2098, 291, 393, 2524, 505, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 314, "seek": 128884, "start": 1297.56, "end": 1301.56, "text": " We have a public Slack and we have a community forum.", "tokens": [492, 362, 257, 1908, 37211, 293, 321, 362, 257, 1768, 17542, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 315, "seek": 128884, "start": 1301.56, "end": 1303.9199999999998, "text": " And every month we also have a low key community call.", "tokens": [400, 633, 1618, 321, 611, 362, 257, 2295, 2141, 1768, 818, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 316, "seek": 128884, "start": 1303.9199999999998, "end": 1306.1599999999999, "text": " We alternate between US and EU time zone.", "tokens": [492, 18873, 1296, 2546, 293, 10887, 565, 6668, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 317, "seek": 128884, "start": 1306.1599999999999, "end": 1307.6799999999998, "text": " So yeah, come say hi.", "tokens": [407, 1338, 11, 808, 584, 4879, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 318, "seek": 128884, "start": 1307.6799999999998, "end": 1312.56, "text": " Yeah, we are happy to, and if any of the things which we talked about excites you, come talk", "tokens": [865, 11, 321, 366, 2055, 281, 11, 293, 498, 604, 295, 264, 721, 597, 321, 2825, 466, 1624, 3324, 291, 11, 808, 751], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 319, "seek": 128884, "start": 1312.56, "end": 1313.56, "text": " to us.", "tokens": [281, 505, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 320, "seek": 128884, "start": 1313.56, "end": 1316.6, "text": " We'll be like more than happy to have a new contributor to the project.", "tokens": [492, 603, 312, 411, 544, 813, 2055, 281, 362, 257, 777, 42859, 281, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 321, "seek": 128884, "start": 1316.6, "end": 1317.6, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2124969786491947, "compression_ratio": 1.7228070175438597, "no_speech_prob": 0.00012211708235554397}, {"id": 322, "seek": 131760, "start": 1317.6, "end": 1320.76, "text": " Yeah, we should have asked this probably in the beginning, but is there anyone out there", "tokens": [865, 11, 321, 820, 362, 2351, 341, 1391, 294, 264, 2863, 11, 457, 307, 456, 2878, 484, 456], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 323, "seek": 131760, "start": 1320.76, "end": 1321.76, "text": " using Prometheus?", "tokens": [1228, 2114, 649, 42209, 30], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 324, "seek": 131760, "start": 1321.76, "end": 1322.76, "text": " Yeah, a couple.", "tokens": [865, 11, 257, 1916, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 325, "seek": 131760, "start": 1322.76, "end": 1323.76, "text": " All right, a lot.", "tokens": [1057, 558, 11, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 326, "seek": 131760, "start": 1323.76, "end": 1324.76, "text": " Yeah, good.", "tokens": [865, 11, 665, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 327, "seek": 131760, "start": 1324.76, "end": 1326.76, "text": " What about any low key users?", "tokens": [708, 466, 604, 2295, 2141, 5022, 30], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 328, "seek": 131760, "start": 1326.76, "end": 1328.76, "text": " Okay, not bad.", "tokens": [1033, 11, 406, 1578, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 329, "seek": 131760, "start": 1328.76, "end": 1329.76, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 330, "seek": 131760, "start": 1329.76, "end": 1331.32, "text": " That makes me really happy.", "tokens": [663, 1669, 385, 534, 2055, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 331, "seek": 131760, "start": 1331.32, "end": 1334.6399999999999, "text": " Anyone run into hard configuration problems with low key?", "tokens": [14643, 1190, 666, 1152, 11694, 2740, 365, 2295, 2141, 30], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 332, "seek": 131760, "start": 1334.6399999999999, "end": 1335.6399999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 333, "seek": 131760, "start": 1335.6399999999999, "end": 1337.1599999999999, "text": " Oh, no hands.", "tokens": [876, 11, 572, 2377, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 334, "seek": 131760, "start": 1337.1599999999999, "end": 1338.1599999999999, "text": " That's weird.", "tokens": [663, 311, 3657, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 335, "seek": 131760, "start": 1338.1599999999999, "end": 1339.1599999999999, "text": " No, I'm just kidding.", "tokens": [883, 11, 286, 478, 445, 9287, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 336, "seek": 131760, "start": 1339.1599999999999, "end": 1341.1599999999999, "text": " Yeah, we got some work to do on.", "tokens": [865, 11, 321, 658, 512, 589, 281, 360, 322, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 337, "seek": 131760, "start": 1341.1599999999999, "end": 1344.7199999999998, "text": " Yeah, so things to take away from the talk.", "tokens": [865, 11, 370, 721, 281, 747, 1314, 490, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.2384441997988004, "compression_ratio": 1.593984962406015, "no_speech_prob": 8.743363287067041e-05}, {"id": 338, "seek": 134472, "start": 1344.72, "end": 1349.08, "text": " Key is meant to largely function like a distributed grep that you can also kind of pull metrics", "tokens": [12759, 307, 4140, 281, 11611, 2445, 411, 257, 12631, 6066, 79, 300, 291, 393, 611, 733, 295, 2235, 16367], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 339, "seek": 134472, "start": 1349.08, "end": 1350.64, "text": " and analytics out of.", "tokens": [293, 15370, 484, 295, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 340, "seek": 134472, "start": 1350.64, "end": 1355.8, "text": " It's low footprint, storing things in object storage and not relying on schemas.", "tokens": [467, 311, 2295, 24222, 11, 26085, 721, 294, 2657, 6725, 293, 406, 24140, 322, 22627, 296, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 341, "seek": 134472, "start": 1355.8, "end": 1359.56, "text": " And we really target kind of easy operations with that as well as low cost.", "tokens": [400, 321, 534, 3779, 733, 295, 1858, 7705, 365, 300, 382, 731, 382, 2295, 2063, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 342, "seek": 134472, "start": 1359.56, "end": 1361.48, "text": " And then please come join the community.", "tokens": [400, 550, 1767, 808, 3917, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 343, "seek": 134472, "start": 1361.48, "end": 1362.48, "text": " Come talk to us.", "tokens": [2492, 751, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 344, "seek": 134472, "start": 1362.48, "end": 1366.28, "text": " We're going to be hanging out for probably at least 10, 15 minutes if you have any questions.", "tokens": [492, 434, 516, 281, 312, 8345, 484, 337, 1391, 412, 1935, 1266, 11, 2119, 2077, 498, 291, 362, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 345, "seek": 134472, "start": 1366.28, "end": 1368.28, "text": " We'd love to hear from you.", "tokens": [492, 1116, 959, 281, 1568, 490, 291, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 346, "seek": 134472, "start": 1368.28, "end": 1371.28, "text": " All right, thank you.", "tokens": [1057, 558, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.19728890991210937, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.92619315587217e-05}, {"id": 347, "seek": 137128, "start": 1371.28, "end": 1379.36, "text": " Yeah, I can share.", "tokens": [865, 11, 286, 393, 2073, 13], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 348, "seek": 137128, "start": 1379.36, "end": 1380.36, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 349, "seek": 137128, "start": 1380.36, "end": 1386.68, "text": " Hey, great talk.", "tokens": [1911, 11, 869, 751, 13], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 350, "seek": 137128, "start": 1386.68, "end": 1391.6399999999999, "text": " I was wondering with the amount of query sharding you're doing, how are you kind of synchronizing", "tokens": [286, 390, 6359, 365, 264, 2372, 295, 14581, 402, 515, 278, 291, 434, 884, 11, 577, 366, 291, 733, 295, 19331, 3319], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 351, "seek": 137128, "start": 1391.6399999999999, "end": 1392.84, "text": " the result in the end?", "tokens": [264, 1874, 294, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 352, "seek": 137128, "start": 1392.84, "end": 1395.8799999999999, "text": " Or is it like synchronized in the UI at the end?", "tokens": [1610, 307, 309, 411, 19331, 1602, 294, 264, 15682, 412, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 353, "seek": 137128, "start": 1395.8799999999999, "end": 1399.08, "text": " Or like, do you need to sort things because that might be very expensive?", "tokens": [1610, 411, 11, 360, 291, 643, 281, 1333, 721, 570, 300, 1062, 312, 588, 5124, 30], "temperature": 0.0, "avg_logprob": -0.28741678698309536, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.00017912940529640764}, {"id": 354, "seek": 139908, "start": 1399.08, "end": 1403.12, "text": " All right, so I had a little difficulty hearing that, but I think the question was, how do", "tokens": [1057, 558, 11, 370, 286, 632, 257, 707, 10360, 4763, 300, 11, 457, 286, 519, 264, 1168, 390, 11, 577, 360], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 355, "seek": 139908, "start": 1403.12, "end": 1406.48, "text": " we synchronize sharding in the query engine?", "tokens": [321, 19331, 1125, 402, 515, 278, 294, 264, 14581, 2848, 30], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 356, "seek": 139908, "start": 1406.48, "end": 1413.12, "text": " Is that in the end when you show it in the UI?", "tokens": [1119, 300, 294, 264, 917, 562, 291, 855, 309, 294, 264, 15682, 30], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 357, "seek": 139908, "start": 1413.12, "end": 1417.4399999999998, "text": " When we show it in the UI, it happens a layer down in Loki itself.", "tokens": [1133, 321, 855, 309, 294, 264, 15682, 11, 309, 2314, 257, 4583, 760, 294, 37940, 2564, 13], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 358, "seek": 139908, "start": 1417.4399999999998, "end": 1420.1599999999999, "text": " So we've already merged everything.", "tokens": [407, 321, 600, 1217, 36427, 1203, 13], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 359, "seek": 139908, "start": 1420.1599999999999, "end": 1424.8799999999999, "text": " I think the real answer to that is probably a lot longer than I can give in this question,", "tokens": [286, 519, 264, 957, 1867, 281, 300, 307, 1391, 257, 688, 2854, 813, 286, 393, 976, 294, 341, 1168, 11], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 360, "seek": 139908, "start": 1424.8799999999999, "end": 1425.8799999999999, "text": " but talk to me.", "tokens": [457, 751, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.24108030145818538, "compression_ratio": 1.6401673640167365, "no_speech_prob": 8.603729656897485e-05}, {"id": 361, "seek": 142588, "start": 1425.88, "end": 1430.4, "text": " It's one of my favorite things to talk about.", "tokens": [467, 311, 472, 295, 452, 2954, 721, 281, 751, 466, 13], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 362, "seek": 142588, "start": 1430.4, "end": 1431.4, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 363, "seek": 142588, "start": 1431.4, "end": 1432.9, "text": " Great talk.", "tokens": [3769, 751, 13], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 364, "seek": 142588, "start": 1432.9, "end": 1438.6000000000001, "text": " This was mentioned several times that the main power of Loki is that it indexes only", "tokens": [639, 390, 2835, 2940, 1413, 300, 264, 2135, 1347, 295, 37940, 307, 300, 309, 8186, 279, 787], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 365, "seek": 142588, "start": 1438.6000000000001, "end": 1444.16, "text": " labels basically, and it doesn't index actual log messages.", "tokens": [16949, 1936, 11, 293, 309, 1177, 380, 8186, 3539, 3565, 7897, 13], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 366, "seek": 142588, "start": 1444.16, "end": 1449.88, "text": " But to make log messages searchable, you want to have many labels in this case, and many", "tokens": [583, 281, 652, 3565, 7897, 3164, 712, 11, 291, 528, 281, 362, 867, 16949, 294, 341, 1389, 11, 293, 867], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 367, "seek": 142588, "start": 1449.88, "end": 1454.0800000000002, "text": " labels often lead to cardinality explosions, how you deal with it.", "tokens": [16949, 2049, 1477, 281, 2920, 259, 1860, 36872, 11, 577, 291, 2028, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.16420767704645792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0011381839867681265}, {"id": 368, "seek": 145408, "start": 1454.08, "end": 1456.0, "text": " Is there any good recommendation?", "tokens": [1119, 456, 604, 665, 11879, 30], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 369, "seek": 145408, "start": 1456.0, "end": 1458.24, "text": " How many labels I should have?", "tokens": [1012, 867, 16949, 286, 820, 362, 30], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 370, "seek": 145408, "start": 1458.24, "end": 1459.24, "text": " What are trade-offs here?", "tokens": [708, 366, 4923, 12, 19231, 510, 30], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 371, "seek": 145408, "start": 1459.24, "end": 1465.3999999999999, "text": " Yeah, so the way that I think about this is you probably want to index where the log", "tokens": [865, 11, 370, 264, 636, 300, 286, 519, 466, 341, 307, 291, 1391, 528, 281, 8186, 689, 264, 3565], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 372, "seek": 145408, "start": 1465.3999999999999, "end": 1468.28, "text": " came from less than what's in it, right?", "tokens": [1361, 490, 1570, 813, 437, 311, 294, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 373, "seek": 145408, "start": 1468.28, "end": 1471.56, "text": " So we definitely added the ability to kind of like, especially in PromTail configs, which", "tokens": [407, 321, 2138, 3869, 264, 3485, 281, 733, 295, 411, 11, 2318, 294, 15833, 51, 864, 6662, 82, 11, 597], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 374, "seek": 145408, "start": 1471.56, "end": 1476.72, "text": " is the agent that we write, the ability to like add extra things in there, and people", "tokens": [307, 264, 9461, 300, 321, 2464, 11, 264, 3485, 281, 411, 909, 2857, 721, 294, 456, 11, 293, 561], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 375, "seek": 145408, "start": 1476.72, "end": 1478.32, "text": " have been really clever about what they put in.", "tokens": [362, 668, 534, 13494, 466, 437, 436, 829, 294, 13], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 376, "seek": 145408, "start": 1478.32, "end": 1482.12, "text": " Unfortunately, that can also be somewhat of a foot gun at times.", "tokens": [8590, 11, 300, 393, 611, 312, 8344, 295, 257, 2671, 3874, 412, 1413, 13], "temperature": 0.0, "avg_logprob": -0.18368419847990336, "compression_ratio": 1.6185897435897436, "no_speech_prob": 7.355896377703175e-05}, {"id": 377, "seek": 148212, "start": 1482.12, "end": 1487.08, "text": " So I'd say index the things that correspond to your topology, right?", "tokens": [407, 286, 1116, 584, 8186, 264, 721, 300, 6805, 281, 428, 1192, 1793, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 378, "seek": 148212, "start": 1487.08, "end": 1488.36, "text": " Where the logs come from.", "tokens": [2305, 264, 20820, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 379, "seek": 148212, "start": 1488.36, "end": 1491.1999999999998, "text": " So environment, application, cluster, that sort of thing.", "tokens": [407, 2823, 11, 3861, 11, 13630, 11, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 380, "seek": 148212, "start": 1491.1999999999998, "end": 1494.6799999999998, "text": " And less things that have to do with the contents of them themselves.", "tokens": [400, 1570, 721, 300, 362, 281, 360, 365, 264, 15768, 295, 552, 2969, 13], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 381, "seek": 148212, "start": 1494.6799999999998, "end": 1499.0, "text": " There's probably a somewhat longer answer there, too, and then we've also been doing", "tokens": [821, 311, 1391, 257, 8344, 2854, 1867, 456, 11, 886, 11, 293, 550, 321, 600, 611, 668, 884], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 382, "seek": 148212, "start": 1499.0, "end": 1505.84, "text": " some recent work to make some of this less of a user concern, particularly about distribution", "tokens": [512, 5162, 589, 281, 652, 512, 295, 341, 1570, 295, 257, 4195, 3136, 11, 4098, 466, 7316], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 383, "seek": 148212, "start": 1505.84, "end": 1507.8799999999999, "text": " of individual log stream throughput.", "tokens": [295, 2609, 3565, 4309, 44629, 13], "temperature": 0.0, "avg_logprob": -0.13342491331554596, "compression_ratio": 1.6102941176470589, "no_speech_prob": 7.88356101111276e-06}, {"id": 384, "seek": 150788, "start": 1507.88, "end": 1513.68, "text": " So if you have one application which is logging like 10 megabytes a second, that can be really", "tokens": [407, 498, 291, 362, 472, 3861, 597, 307, 27991, 411, 1266, 10816, 24538, 257, 1150, 11, 300, 393, 312, 534], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 385, "seek": 150788, "start": 1513.68, "end": 1515.88, "text": " harmful on Loki in some ways.", "tokens": [19727, 322, 37940, 294, 512, 2098, 13], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 386, "seek": 150788, "start": 1515.88, "end": 1519.48, "text": " And so people got clever around splitting that out into different streams.", "tokens": [400, 370, 561, 658, 13494, 926, 30348, 300, 484, 666, 819, 15842, 13], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 387, "seek": 150788, "start": 1519.48, "end": 1522.8400000000001, "text": " But in the future, we're going to be doing a lot of that automatically and transparently", "tokens": [583, 294, 264, 2027, 11, 321, 434, 516, 281, 312, 884, 257, 688, 295, 300, 6772, 293, 7132, 6420], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 388, "seek": 150788, "start": 1522.8400000000001, "end": 1526.48, "text": " behind Loki's API.", "tokens": [2261, 37940, 311, 9362, 13], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 389, "seek": 150788, "start": 1526.48, "end": 1531.16, "text": " Just to add one thing on top of what Owen said, like in log yield, which we didn't show here.", "tokens": [1449, 281, 909, 472, 551, 322, 1192, 295, 437, 32867, 848, 11, 411, 294, 3565, 11257, 11, 597, 321, 994, 380, 855, 510, 13], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 390, "seek": 150788, "start": 1531.16, "end": 1536.2, "text": " So if you have something in your log lying itself that you want to treat it as a label,", "tokens": [407, 498, 291, 362, 746, 294, 428, 3565, 8493, 2564, 300, 291, 528, 281, 2387, 309, 382, 257, 7645, 11], "temperature": 0.0, "avg_logprob": -0.13745860130556167, "compression_ratio": 1.63, "no_speech_prob": 1.182473533845041e-05}, {"id": 391, "seek": 153620, "start": 1536.2, "end": 1539.96, "text": " you can do it on the fly with the log yield query language itself.", "tokens": [291, 393, 360, 309, 322, 264, 3603, 365, 264, 3565, 11257, 14581, 2856, 2564, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 392, "seek": 153620, "start": 1539.96, "end": 1542.64, "text": " So we have some like a parser.", "tokens": [407, 321, 362, 512, 411, 257, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 393, "seek": 153620, "start": 1542.64, "end": 1549.68, "text": " If it's a log form, if it's a JSON parser, you can use it like labels, the things in", "tokens": [759, 309, 311, 257, 3565, 1254, 11, 498, 309, 311, 257, 31828, 21156, 260, 11, 291, 393, 764, 309, 411, 16949, 11, 264, 721, 294], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 394, "seek": 153620, "start": 1549.68, "end": 1551.68, "text": " your log line itself.", "tokens": [428, 3565, 1622, 2564, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 395, "seek": 153620, "start": 1551.68, "end": 1553.68, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 396, "seek": 153620, "start": 1553.68, "end": 1554.68, "text": " Thank you for the great talk.", "tokens": [1044, 291, 337, 264, 869, 751, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 397, "seek": 153620, "start": 1554.68, "end": 1558.32, "text": " I have many questions, but I'll ask one.", "tokens": [286, 362, 867, 1651, 11, 457, 286, 603, 1029, 472, 13], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 398, "seek": 153620, "start": 1558.32, "end": 1563.72, "text": " Do you have any tips in terms of scaling query and caching layers?", "tokens": [1144, 291, 362, 604, 6082, 294, 2115, 295, 21589, 14581, 293, 269, 2834, 7914, 30], "temperature": 0.0, "avg_logprob": -0.18587342378135038, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.00043614747119136155}, {"id": 399, "seek": 156372, "start": 1563.72, "end": 1567.84, "text": " Like from my experience, usually they're very overutilized, but when people start querying,", "tokens": [1743, 490, 452, 1752, 11, 2673, 436, 434, 588, 670, 20835, 1602, 11, 457, 562, 561, 722, 7083, 1840, 11], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 400, "seek": 156372, "start": 1567.84, "end": 1568.84, "text": " you get all the crashes.", "tokens": [291, 483, 439, 264, 28642, 13], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 401, "seek": 156372, "start": 1568.84, "end": 1571.24, "text": " Do you have any code and ratios or anything?", "tokens": [1144, 291, 362, 604, 3089, 293, 32435, 420, 1340, 30], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 402, "seek": 156372, "start": 1571.24, "end": 1572.24, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 403, "seek": 156372, "start": 1572.24, "end": 1576.96, "text": " So this is one of the things like how we expose configurations around query parallelism and", "tokens": [407, 341, 307, 472, 295, 264, 721, 411, 577, 321, 19219, 31493, 926, 14581, 8952, 1434, 293], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 404, "seek": 156372, "start": 1576.96, "end": 1578.28, "text": " controls.", "tokens": [9003, 13], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 405, "seek": 156372, "start": 1578.28, "end": 1584.64, "text": " I wish I had a do over on this a few years back, because there's a couple different configurations.", "tokens": [286, 3172, 286, 632, 257, 360, 670, 322, 341, 257, 1326, 924, 646, 11, 570, 456, 311, 257, 1916, 819, 31493, 13], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 406, "seek": 156372, "start": 1584.64, "end": 1589.88, "text": " Largely around for every individual tenant, how many subqueries you want to be allowed", "tokens": [11569, 70, 736, 926, 337, 633, 2609, 31000, 11, 577, 867, 1422, 358, 21659, 291, 528, 281, 312, 4350], "temperature": 0.0, "avg_logprob": -0.22472904468404836, "compression_ratio": 1.6170212765957446, "no_speech_prob": 9.134146966971457e-05}, {"id": 407, "seek": 158988, "start": 1589.88, "end": 1595.6000000000001, "text": " to be processed per query at a time, and then there's also things like concurrency for each", "tokens": [281, 312, 18846, 680, 14581, 412, 257, 565, 11, 293, 550, 456, 311, 611, 721, 411, 23702, 10457, 337, 1184], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 408, "seek": 158988, "start": 1595.6000000000001, "end": 1597.16, "text": " of your query components, right?", "tokens": [295, 428, 14581, 6677, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 409, "seek": 158988, "start": 1597.16, "end": 1601.3200000000002, "text": " How many go routines should you associate with or should you devote to running queries", "tokens": [1012, 867, 352, 33827, 820, 291, 14644, 365, 420, 820, 291, 23184, 281, 2614, 24109], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 410, "seek": 158988, "start": 1601.3200000000002, "end": 1603.8400000000001, "text": " independently?", "tokens": [21761, 30], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 411, "seek": 158988, "start": 1603.8400000000001, "end": 1612.2, "text": " But yeah, I got some work to do to make that easily digestible, if that's fair.", "tokens": [583, 1338, 11, 286, 658, 512, 589, 281, 360, 281, 652, 300, 3612, 13884, 964, 11, 498, 300, 311, 3143, 13], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 412, "seek": 158988, "start": 1612.2, "end": 1619.0400000000002, "text": " So this year at work, I end up kind of giving to Loki for 10 minutes about 20 gigabytes", "tokens": [407, 341, 1064, 412, 589, 11, 286, 917, 493, 733, 295, 2902, 281, 37940, 337, 1266, 2077, 466, 945, 42741], "temperature": 0.0, "avg_logprob": -0.1909301064231179, "compression_ratio": 1.5330739299610896, "no_speech_prob": 9.21237951843068e-05}, {"id": 413, "seek": 161904, "start": 1619.04, "end": 1621.6, "text": " of log from a 500 machine.", "tokens": [295, 3565, 490, 257, 5923, 3479, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 414, "seek": 161904, "start": 1621.6, "end": 1626.28, "text": " It was just a one-time experiment that they ran from time to time.", "tokens": [467, 390, 445, 257, 472, 12, 3766, 5120, 300, 436, 5872, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 415, "seek": 161904, "start": 1626.28, "end": 1628.04, "text": " And we started using Loki.", "tokens": [400, 321, 1409, 1228, 37940, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 416, "seek": 161904, "start": 1628.04, "end": 1630.6399999999999, "text": " We optimized it as intended.", "tokens": [492, 26941, 309, 382, 10226, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 417, "seek": 161904, "start": 1630.6399999999999, "end": 1632.84, "text": " So using index query was great.", "tokens": [407, 1228, 8186, 14581, 390, 869, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 418, "seek": 161904, "start": 1632.84, "end": 1638.96, "text": " At a certain point, my colleagues come to me and say, we want the log string for each", "tokens": [1711, 257, 1629, 935, 11, 452, 7734, 808, 281, 385, 293, 584, 11, 321, 528, 264, 3565, 6798, 337, 1184], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 419, "seek": 161904, "start": 1638.96, "end": 1641.6399999999999, "text": " individual machine in a file.", "tokens": [2609, 3479, 294, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 420, "seek": 161904, "start": 1641.6399999999999, "end": 1647.2, "text": " And I started asking Loki for this, and he took ages to extract this information.", "tokens": [400, 286, 1409, 3365, 37940, 337, 341, 11, 293, 415, 1890, 12357, 281, 8947, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.18276923837013614, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.00013552300515584648}, {"id": 421, "seek": 164720, "start": 1647.2, "end": 1650.8, "text": " Now I hope you're going to tell me, this is not the use case for Loki.", "tokens": [823, 286, 1454, 291, 434, 516, 281, 980, 385, 11, 341, 307, 406, 264, 764, 1389, 337, 37940, 13], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 422, "seek": 164720, "start": 1650.8, "end": 1655.52, "text": " You did very well to use RC's log and just pushing things in a file.", "tokens": [509, 630, 588, 731, 281, 764, 28987, 311, 3565, 293, 445, 7380, 721, 294, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 423, "seek": 164720, "start": 1655.52, "end": 1659.72, "text": " But if you have another answer, I'll be glad.", "tokens": [583, 498, 291, 362, 1071, 1867, 11, 286, 603, 312, 5404, 13], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 424, "seek": 164720, "start": 1659.72, "end": 1667.6000000000001, "text": " We didn't catch the question fully, but my understanding is you're asking, like, can", "tokens": [492, 994, 380, 3745, 264, 1168, 4498, 11, 457, 452, 3701, 307, 291, 434, 3365, 11, 411, 11, 393], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 425, "seek": 164720, "start": 1667.6000000000001, "end": 1671.28, "text": " the log yield find logs coming from a single machine?", "tokens": [264, 3565, 11257, 915, 20820, 1348, 490, 257, 2167, 3479, 30], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 426, "seek": 164720, "start": 1671.28, "end": 1672.28, "text": " Is that right?", "tokens": [1119, 300, 558, 30], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 427, "seek": 164720, "start": 1672.28, "end": 1673.28, "text": " Am I getting it right?", "tokens": [2012, 286, 1242, 309, 558, 30], "temperature": 0.0, "avg_logprob": -0.259222544156588, "compression_ratio": 1.4836065573770492, "no_speech_prob": 0.0005061920383013785}, {"id": 428, "seek": 167328, "start": 1673.28, "end": 1686.48, "text": " So you're talking about, I guess, some kind of federation, maybe?", "tokens": [407, 291, 434, 1417, 466, 11, 286, 2041, 11, 512, 733, 295, 4636, 5053, 11, 1310, 30], "temperature": 0.0, "avg_logprob": -0.22915989078887522, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.000168684491654858}, {"id": 429, "seek": 167328, "start": 1686.48, "end": 1689.44, "text": " So why do you want to store it in the file in a single machine?", "tokens": [407, 983, 360, 291, 528, 281, 3531, 309, 294, 264, 3991, 294, 257, 2167, 3479, 30], "temperature": 0.0, "avg_logprob": -0.22915989078887522, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.000168684491654858}, {"id": 430, "seek": 167328, "start": 1689.44, "end": 1697.96, "text": " Because they wanted to run analysis on a specific stream of log from a specific machine.", "tokens": [1436, 436, 1415, 281, 1190, 5215, 322, 257, 2685, 4309, 295, 3565, 490, 257, 2685, 3479, 13], "temperature": 0.0, "avg_logprob": -0.22915989078887522, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.000168684491654858}, {"id": 431, "seek": 167328, "start": 1697.96, "end": 1701.28, "text": " So technically, you can store log files in the file system.", "tokens": [407, 12120, 11, 291, 393, 3531, 3565, 7098, 294, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.22915989078887522, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.000168684491654858}, {"id": 432, "seek": 170128, "start": 1701.28, "end": 1706.96, "text": " So instead of using object storage, technically, you can use a file system.", "tokens": [407, 2602, 295, 1228, 2657, 6725, 11, 12120, 11, 291, 393, 764, 257, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 433, "seek": 170128, "start": 1706.96, "end": 1708.12, "text": " That's completely possible.", "tokens": [663, 311, 2584, 1944, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 434, "seek": 170128, "start": 1708.12, "end": 1713.8, "text": " But we encourage to use object storage when it comes to scale, because that's how it works.", "tokens": [583, 321, 5373, 281, 764, 2657, 6725, 562, 309, 1487, 281, 4373, 11, 570, 300, 311, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 435, "seek": 170128, "start": 1713.8, "end": 1720.12, "text": " Yeah, is the question behind the question there changing where you actually store it", "tokens": [865, 11, 307, 264, 1168, 2261, 264, 1168, 456, 4473, 689, 291, 767, 3531, 309], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 436, "seek": 170128, "start": 1720.12, "end": 1723.6399999999999, "text": " so that you can then run your own processing tools on top?", "tokens": [370, 300, 291, 393, 550, 1190, 428, 1065, 9007, 3873, 322, 1192, 30], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 437, "seek": 170128, "start": 1723.6399999999999, "end": 1724.6399999999999, "text": " Yeah?", "tokens": [865, 30], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 438, "seek": 170128, "start": 1724.6399999999999, "end": 1725.6399999999999, "text": " Yeah?", "tokens": [865, 30], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 439, "seek": 170128, "start": 1725.6399999999999, "end": 1726.6399999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 440, "seek": 170128, "start": 1726.6399999999999, "end": 1728.68, "text": " Yeah, that's actually a relatively common ask.", "tokens": [865, 11, 300, 311, 767, 257, 7226, 2689, 1029, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 441, "seek": 170128, "start": 1728.68, "end": 1730.28, "text": " It's not something that we support at the moment.", "tokens": [467, 311, 406, 746, 300, 321, 1406, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.17897321410098319, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.680758764967322e-05}, {"id": 442, "seek": 173028, "start": 1730.28, "end": 1733.84, "text": " We kind of have our own format that we store in object storage or whatnot.", "tokens": [492, 733, 295, 362, 527, 1065, 7877, 300, 321, 3531, 294, 2657, 6725, 420, 25882, 13], "temperature": 0.0, "avg_logprob": -0.16957335897011333, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.00015461012662854046}, {"id": 443, "seek": 173028, "start": 1733.84, "end": 1737.56, "text": " We do have some tooling, one's called chunk inspect, which allows you to kind of iterate", "tokens": [492, 360, 362, 512, 46593, 11, 472, 311, 1219, 16635, 15018, 11, 597, 4045, 291, 281, 733, 295, 44497], "temperature": 0.0, "avg_logprob": -0.16957335897011333, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.00015461012662854046}, {"id": 444, "seek": 173028, "start": 1737.56, "end": 1741.8, "text": " through all of the chunks, which could be from a particular stream or log file.", "tokens": [807, 439, 295, 264, 24004, 11, 597, 727, 312, 490, 257, 1729, 4309, 420, 3565, 3991, 13], "temperature": 0.0, "avg_logprob": -0.16957335897011333, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.00015461012662854046}, {"id": 445, "seek": 173028, "start": 1741.8, "end": 1747.28, "text": " But it's not incredibly batteries included at the moment, if that makes sense.", "tokens": [583, 309, 311, 406, 6252, 13070, 5556, 412, 264, 1623, 11, 498, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.16957335897011333, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.00015461012662854046}, {"id": 446, "seek": 173028, "start": 1747.28, "end": 1757.32, "text": " Hello, I have the use case that I store some telemetry data with my logs sometimes, like", "tokens": [2425, 11, 286, 362, 264, 764, 1389, 300, 286, 3531, 512, 4304, 5537, 627, 1412, 365, 452, 20820, 2171, 11, 411], "temperature": 0.0, "avg_logprob": -0.16957335897011333, "compression_ratio": 1.6181102362204725, "no_speech_prob": 0.00015461012662854046}, {"id": 447, "seek": 175732, "start": 1757.32, "end": 1763.32, "text": " a metric or sometimes, which I don't want to be indexed, but I also don't want to encode", "tokens": [257, 20678, 420, 2171, 11, 597, 286, 500, 380, 528, 281, 312, 8186, 292, 11, 457, 286, 611, 500, 380, 528, 281, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.15675083796183267, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.875932452501729e-05}, {"id": 448, "seek": 175732, "start": 1763.32, "end": 1768.2, "text": " in the actual log message because it's already structured data.", "tokens": [294, 264, 3539, 3565, 3636, 570, 309, 311, 1217, 18519, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15675083796183267, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.875932452501729e-05}, {"id": 449, "seek": 175732, "start": 1768.2, "end": 1775.3999999999999, "text": " Is it possible to have fields that are not indexed or data that are not indexed?", "tokens": [1119, 309, 1944, 281, 362, 7909, 300, 366, 406, 8186, 292, 420, 1412, 300, 366, 406, 8186, 292, 30], "temperature": 0.0, "avg_logprob": -0.15675083796183267, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.875932452501729e-05}, {"id": 450, "seek": 175732, "start": 1775.3999999999999, "end": 1776.8, "text": " It's funny that you asked that.", "tokens": [467, 311, 4074, 300, 291, 2351, 300, 13], "temperature": 0.0, "avg_logprob": -0.15675083796183267, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.875932452501729e-05}, {"id": 451, "seek": 175732, "start": 1776.8, "end": 1783.6799999999998, "text": " Yeah, so there's kind of a growing question around non-index metadata like that, right?", "tokens": [865, 11, 370, 456, 311, 733, 295, 257, 4194, 1168, 926, 2107, 12, 471, 3121, 26603, 411, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15675083796183267, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.875932452501729e-05}, {"id": 452, "seek": 178368, "start": 1783.68, "end": 1788.1200000000001, "text": " That's not actually stored in the index itself, but like includes some degree of structure.", "tokens": [663, 311, 406, 767, 12187, 294, 264, 8186, 2564, 11, 457, 411, 5974, 512, 4314, 295, 3877, 13], "temperature": 0.0, "avg_logprob": -0.2464411151942922, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.2391528545995243e-05}, {"id": 453, "seek": 178368, "start": 1788.1200000000001, "end": 1794.72, "text": " I know we see this as kind of a current need, particularly for like hotel formats, so it's", "tokens": [286, 458, 321, 536, 341, 382, 733, 295, 257, 2190, 643, 11, 4098, 337, 411, 7622, 25879, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.2464411151942922, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.2391528545995243e-05}, {"id": 454, "seek": 178368, "start": 1794.72, "end": 1799.5600000000002, "text": " something that we're looking into right now, actually.", "tokens": [746, 300, 321, 434, 1237, 666, 558, 586, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.2464411151942922, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.2391528545995243e-05}, {"id": 455, "seek": 178368, "start": 1799.5600000000002, "end": 1808.96, "text": " So thanks a lot, everyone.", "tokens": [407, 3231, 257, 688, 11, 1518, 13], "temperature": 0.0, "avg_logprob": -0.2464411151942922, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.2391528545995243e-05}, {"id": 456, "seek": 180896, "start": 1808.96, "end": 1817.28, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50780], "temperature": 0.0, "avg_logprob": -0.6939675807952881, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0015864296583458781}], "language": "en"}