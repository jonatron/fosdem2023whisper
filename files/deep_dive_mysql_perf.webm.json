{"text": " Now, we already spoke here a little bit about developers and especially the front-end developers. One purpose of this talk for me is to really sort of this kind of a bridge, the gap, which I often see between the people who really have a database as a center of their at least professional life. Any people who are writing an application and database just of them like a thing. It's like a toilet. You do your business and you move on with your life. Something like that. For those people, the database is typically like a black box. There is this black box and what I want is I connect to the service point which is provided to me. I connect it quickly. I run my queries and that's all I care about and all that kind of change buffer combaya. Never heard about it. What about queries? What would you as a developer think about queries? Well, these are actually pretty simple things. When you connect it to a service point, you are queries. You want them to complete if no errors. You want them to provide you correct result set because if you wouldn't, we could alter over my school tables to black hole and get a fantastic performance. No errors too. And also you want them to make sure they complete in that response time what your application expects. I think that is a very important thing to understand. If you look at from the individual developer standpoint, like Ryan application, hey, performance response time for my queries is all I care about. And how that's internal database kitchen works, somebody else's problem. Now if you think about the response time from the database point of view, that is often seen like, well, I see that response time for a query is in average or whatever distribution. We'll talk about that later. But that is different from what business cares about. If you think about the business point of view, you think about, well, do my user have outstanding experience in terms of performance with all the application interactions? That means like a search should work and place an order should work and whatever. And the database is important part of that, of course, but not is their complete part. What is interesting in this case is what as database engineers, we often talk about those kind of different events, kind of like a bad performance and the downtime. And say, well, you know, no, no, we weren't down, it just was taken 15 minutes to run my very basic query. Well, from user standpoint, the bad performance, very bad performance isn't distinguishable from downtime. Because A, we don't have parents, then even if people are very patient, then the browser or some other timeout will happen and nobody gives a shit about that query which may still continue to run. Another thing to understand about query performance is you do not want to focus on the averages. I like this kind of one saying, but there was one sleeve demand to try to cross the river in the average one meter deep. That is same applies to the query. If your average query time is X, that means pretty much nothing. You need to understand more about that. And I like in this case to look at their percentiles and even more to make sure you can look at a specific distribution of your query response time. If you have that, that gives you a lot more insight. Now one thing to understand about the percentile, you may be looking and saying, well, great. My queries have this decent 99 percentile, but that does not mean on a business side what 99 percent of your users have a good or acceptable experience. Why is that? Well, because guess what? The single user interaction can correspond to a lot of queries sequentially, which all add up and typically through their joining user has a number of those interactions. So I would say even 99 percentile that may all well, depending on your application, only correspond to like 50 percent of user session. So if you really see the complicated large environments, they are really focused on either relatively short SLA or rather high percentiles. Another thing that I would encourage to pay attention to is errors. And make sure you are measuring response time for those as well, because errors actually can be offered two kinds, fast errors and slow errors. In certain cases, let's say if your table doesn't exist, you may be like, get the response time straight away, and if you put all your error queries and actually normal queries in the same bucket, you may say, oh my gosh, my response times are doing kind of so well. But on the other hand, if your query is, for example, error is a lock weight time out, then that is a slow error. It actually will have a higher response time than the normal cases. That is why I always suggest to make sure we measure response time for normal queries and for queries with problems differently. Another thing which is very important is looking at response time over time, because traffic changes, a lot of things are going on in the system and just saying, hey, I'll have a response time of x over some long period of time, it's not very helpful. Also what you would see in many cases, you still start those like a small performance problems, maybe like SLA violations, which are if unfixed, they convert in the downtime. For example, in my SQL world, you may say, well, I have forgotten this kind of runaway query, and my history accumulates. It will slowly increase and increase your response time. If you measure that over time and say, well, something is not trending in the right directions, you probably can fix it before that will be seen as a downtime by your users. If you are not, then not so much. This is example what we have here, what you often may see something like this as well, where all the queries have like a spike in the response time, which you often may correspond to something external happening in the environment. I think here is what is very interesting, especially for us running in the cloud, we only have limited observability to environment. If there is some shit going on on the Amazon backend, they're not going to tell us that. Oh, you know what, we had, let's say, some free hard drives failed, which back our EBS and we had to some rebalance, yada, yada. The other question I would ask is where we want to measure response time from a queries. In my opinion, both application view and database you are in the combinations are very helpful because the application can see real thing. If your network, for example, is adding some latency or whatever, and you will see that from application, not so much in the database, because it's only sees from, hey, I got response to, then it's sent the data back. But the database view allows you often to see a lot more inside about what have been going on inside, where from application side, we often can just capture the query time, maybe some very basic additional parameters. So what we spoke from our business view, right? Well, we already said what that all users have outstanding performance experience of all the application interactions, right? Let's now try to break it down a little bit more, right, to what that may mean. In this case, I want to introduce this little project or flag from you. This is SQL Commenter project by Google, right, I mean, which is pretty cool in terms of what it allows to pass you the metadata, right, which you understand as developer all the way to SQL query. They implemented that support from a number of frameworks, right, and it's also supported in their Google Cloud monitoring environment, right? And I wouldn't very much see that developed more, right, and for at least kind of us come to some sort of shared standards between the databases, right, to wherever, how we can augment query information with sort of like a tags, values, right, which users care about. So what are possibilities which can be quite helpful in this regard? Well, finding, for example, who is our actual user tenant, who is query, corresponds, right, because we often may have, you know, different performance issues, right, finding the application, like some sort of like a subset of application functionality where many of them may be hitting the application, right, version information, maybe information about like an, their engineer of a team who is responsible. I often see DBAs or SRAs team having problem, like, oh, I see this nasty query which was shipped yesterday, I know because, shipped today because I know it wasn't very yesterday, right, but now having to find out who a hell introduced that stupid query, maybe problematic in a large environment. Now a lot of focus, and I think the core of the query-based observability may be about the query. But the query, I mean, obviously like a query with sort of like it's, which are same except different parameters, and that is very helpful because, well, obviously they have a different complexity, different expected SLA, and so on and so forth. The next way also to break things down for me would be to look at the schema or database, and why is that interesting? How? I just noticed right now what it's been cut a bit, you see, well, anyway, life is life. I'm just not going to be lucky in this room, right, yes, yeah, but, well, we can blame our windows, right, on this conference we can and should blame windows. Okay, well, why schema and database are also good because often we would separate in the multi-tenant applications different tenants by schema, right, and in that case that gives us a good profiling for performance of their different schemas, right, like we can see here in the example with PMM tool. Another thing what I found is very helpful to find a way to separate the data by different tables, right, in many cases you want to say, hey, you know what, how a query is hitting given table is affected, especially if it did some change which relates to the table. Hey, you know what, I changed the indexing on this table, let me see how all the queries hitting this table is impacted, very helpful because there may be some surprising differences. Database users, that is another thing which is quite helpful because that often allows us to identify the service application, right, if you're following good security practices you would not let all your applications, right, just use one username, you know, not a good idea, right, and also find human troublemakers, right, which are doing, having direct access, right, and so many times you'll find somebody, you know, running the query, right, and say, okay, well, yeah, it's slow but wherever I'll go for lunch, you know, I have time, well, you may have time but your database may not, right, so we also, like here's example how we provide that. I also mentioned database hosts and indexes in many instances, in many cases that is very helpful because even if you may think, oh, my different database instance should perform the same, well, world is a messy place and world in the cloud is even messy place, right, they may not exactly have the same performance due to, you know, some strange configuration differently, having a bad day, right, or even maybe having a different load, right, and that is a good to be able to break it down, right, when you see some of your queries are not performing very well. I would also look at the same stuff from a web server or application server instance because, again, if you have, like, maybe like a hundred nodes, you deploy the same application, you may think, hey, we're all going to perform the same, hitting the database, well, that is not always the case, right, they have seen changes from people saying one FM is misconfigured or for some reason cannot connect the cache, so it's, you know, hitting, you know, ten times more queries, right, on the database than it should be, or the application rollout didn't go well, where UV eliminated nasty query on 99 of application instance but not some others, right, it's a very good to actually be able to validate that because what you would see or, like, again, from a DBA standpoint, you know, developers, sysadmins, storage people, they are going to tell you shit, right, but they are going to lie, right, they are going to lie, right, maybe not intentionally, maybe because of their ignorance and limitation of their tool but as a DBA, a city or something, you want to point them out to their shit and say, look, I have evidence, right, evidence is good, right, so clients costs, custom tags is very helpful if you can extend, that is what we spoke about, the SQL commenters, something else which I find very helpful which we cannot quite easily get with MySQL but being able to separate the query by the query plans, right, often you may have a query which looks the same but it may take different execution plans, right, and often that may be correlated to its performance. In certain cases, it is totally fine, right, very different situations, sometimes MySQL optimizer may get a little bit, you know, crazy just and has that optimizer plan drift for no good reason which may not be very easy to catch, right, and will be helpful to do. What I also would like to highlight is when you find the specific query and say, hey, this query has nasty performance, right, we often want to understand where that query response time comes from, right, and that is some of their things, right, where it can come from, certain of them are relatively easy to find out, right, certain are not very well, right, for example, wherever query has waited on available CPU, right, because system was already saturated, well, you can't really see on per query basics, right, you can only see those things, well, my CPU was kind of like a super packed, right, on a period of time. Okay, here are a couple of other things to consider when you're looking at the queries. One, you want to really look at separately the bad queries, right, versus victims, because sometimes you will see, oh, queries are getting slower, but it's not because of FAM, it's about some other nasty queries, right, maybe that is your Java developer who thought, well, you know, to solve my problems, I will just launch with 200 threads, right, and make sure I am good, but everything else is kind of slowed down, right, and that's maybe tricky. One thing is what you should not forget the currently running queries. In many cases, like if you look in performance schema queries by dash address, that gives you what happened in the past, but believe me, if you start, you know, 50 instances of some very bad query, which continues to run, well, that may be the reason of your problem, not the past, right, and to connect to that, I think it is less problem in my skill right now, right, if you're using query timeouts, which is a very good practice, right, because if you say, hey, you know what, for all my interactive queries, by default, I set the timeout of, let's say, 15 seconds, then you should not care too much about your past queries because, well, you know what, everything gets killed after 15 minutes. Also, 50 seconds, right, you should not ignore the stuff which is invisible from a query standpoint, right, databases do a lot of shit in the background, you may also do things or your operation teams like, well, backups or provisioning another node for cloning, right, for the clouds or wherever your VM system may need to do something in the background, it may not be directly visible, but that can impact the query performance, right, so sometimes, well, when you observe a query impact and you can't really see what is causing that, it's possible. I also would encourage to avoid what I would call like a biased something. I see people sometimes would say, hey, you know what, we will set long query time to one second and only look at the queries which are more than one second in length, well, you may be only focusing on the outliers, right, and missing the possibility to optimize other queries, right, or actually even focusing on the queries which provide, which are responsible for providing that bad experience, right, for your users. Okay, we find another thing like a last minute I have or something, I wanted to say, hey, what I would like to see from my skill to do better, who is Kenny, no Kenny? Yes, he's always hiding, right, he probably wanted to get another sandwich, damn it. Okay, so here are some things that I would like to see. One is better support of prepared statements, right, and right now it's kind of, you know, not done in the same way, right, which is, I think, is a problem, right. Now I would say consider grouping data by time in certain cases, right now you get like all the statements in one table, right, and you have a lot of statement variety, that table tends to overflow, right, which is not really helpful, right, and if you have to kind of reset your queries all the time, that is not very, you know, good practice in my opinion. Provide list of tables query touches, right, that is very helpful because, well, my skill parser already knows it, right, it knows tables query touches, but it's very hard to parse it out from a query, especially if you consider views, right. I don't know by looking at the query alone, wherever something is a table or a view, right, so, in this case. Information about plan ID, right, I would like to see for the query, right, some sort of plan hash or something, so I know then query is using something like that, and also what I would call like a top weight summary, right. Right now we have information about the weights in my skill performance query and about query, but I cannot see and say, oh, that query was slow because it spent XYZ amount of weight on something or whatever, right, or at least kind of like some small class of queries, right, I don't think that's convenient. Well, with that, that's all I had to say, hope that will help you to avoid tuning your indexes by, by the credit card, and yes, oh, I have a time for questions, you told me like, Peter, five minutes, oh, to answer, I have a time for questions, yes, any questions, no, oh, yeah, what's the difference or advantages of this SQL commenter thing compared to what open tracing standards people start tracing the whole thing, what's the difference of SQL commenter? Well, what I would say in this case, yes, I mean, there is obviously open tracing framework, right, which you can use, this gets specifically to the database and specifically in every query, right, if you look at the open tracing framework, I think, you know, getting every query, right, maybe a lot of, a lot of volume out there, right, and again, I also think, well, the good thing if also SQL commenter, right, is what that does it automatically, if you will, right, that does not require you to take an extra integration. Okay, anybody else, yeah, I mean, it works with MariaDB as well, yes, well, there are not practices, there are no good practices, right, like you can, there is a lot of optimizer hints you can use, right, so you can actually force the query to go like this particular stuff, right, but that also prevents optimizer choosing different plan if better plan becomes available. Yeah. Never use forced index, always use ignore index, okay, well, then thank you, folks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.44, "text": " Now, we already spoke here a little bit about developers and especially the front-end developers.", "tokens": [823, 11, 321, 1217, 7179, 510, 257, 707, 857, 466, 8849, 293, 2318, 264, 1868, 12, 521, 8849, 13], "temperature": 0.0, "avg_logprob": -0.287849255462191, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.21550306677818298}, {"id": 1, "seek": 0, "start": 17.44, "end": 22.56, "text": " One purpose of this talk for me is to really sort of this kind of a bridge, the gap, which", "tokens": [1485, 4334, 295, 341, 751, 337, 385, 307, 281, 534, 1333, 295, 341, 733, 295, 257, 7283, 11, 264, 7417, 11, 597], "temperature": 0.0, "avg_logprob": -0.287849255462191, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.21550306677818298}, {"id": 2, "seek": 0, "start": 22.56, "end": 28.6, "text": " I often see between the people who really have a database as a center of their at least", "tokens": [286, 2049, 536, 1296, 264, 561, 567, 534, 362, 257, 8149, 382, 257, 3056, 295, 641, 412, 1935], "temperature": 0.0, "avg_logprob": -0.287849255462191, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.21550306677818298}, {"id": 3, "seek": 2860, "start": 28.6, "end": 31.28, "text": " professional life.", "tokens": [4843, 993, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 4, "seek": 2860, "start": 31.28, "end": 37.6, "text": " Any people who are writing an application and database just of them like a thing.", "tokens": [2639, 561, 567, 366, 3579, 364, 3861, 293, 8149, 445, 295, 552, 411, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 5, "seek": 2860, "start": 37.6, "end": 38.6, "text": " It's like a toilet.", "tokens": [467, 311, 411, 257, 11137, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 6, "seek": 2860, "start": 38.6, "end": 43.120000000000005, "text": " You do your business and you move on with your life.", "tokens": [509, 360, 428, 1606, 293, 291, 1286, 322, 365, 428, 993, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 7, "seek": 2860, "start": 43.120000000000005, "end": 44.120000000000005, "text": " Something like that.", "tokens": [6595, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 8, "seek": 2860, "start": 44.120000000000005, "end": 49.8, "text": " For those people, the database is typically like a black box.", "tokens": [1171, 729, 561, 11, 264, 8149, 307, 5850, 411, 257, 2211, 2424, 13], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 9, "seek": 2860, "start": 49.8, "end": 58.0, "text": " There is this black box and what I want is I connect to the service point which is provided", "tokens": [821, 307, 341, 2211, 2424, 293, 437, 286, 528, 307, 286, 1745, 281, 264, 2643, 935, 597, 307, 5649], "temperature": 0.0, "avg_logprob": -0.26375449105594934, "compression_ratio": 1.5963302752293578, "no_speech_prob": 9.030865476233885e-05}, {"id": 10, "seek": 5800, "start": 58.0, "end": 59.0, "text": " to me.", "tokens": [281, 385, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 11, "seek": 5800, "start": 59.0, "end": 60.0, "text": " I connect it quickly.", "tokens": [286, 1745, 309, 2661, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 12, "seek": 5800, "start": 60.0, "end": 69.24, "text": " I run my queries and that's all I care about and all that kind of change buffer combaya.", "tokens": [286, 1190, 452, 24109, 293, 300, 311, 439, 286, 1127, 466, 293, 439, 300, 733, 295, 1319, 21762, 2512, 4427, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 13, "seek": 5800, "start": 69.24, "end": 71.72, "text": " Never heard about it.", "tokens": [7344, 2198, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 14, "seek": 5800, "start": 71.72, "end": 74.03999999999999, "text": " What about queries?", "tokens": [708, 466, 24109, 30], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 15, "seek": 5800, "start": 74.03999999999999, "end": 77.12, "text": " What would you as a developer think about queries?", "tokens": [708, 576, 291, 382, 257, 10754, 519, 466, 24109, 30], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 16, "seek": 5800, "start": 77.12, "end": 80.4, "text": " Well, these are actually pretty simple things.", "tokens": [1042, 11, 613, 366, 767, 1238, 2199, 721, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 17, "seek": 5800, "start": 80.4, "end": 82.36, "text": " When you connect it to a service point, you are queries.", "tokens": [1133, 291, 1745, 309, 281, 257, 2643, 935, 11, 291, 366, 24109, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 18, "seek": 5800, "start": 82.36, "end": 85.16, "text": " You want them to complete if no errors.", "tokens": [509, 528, 552, 281, 3566, 498, 572, 13603, 13], "temperature": 0.0, "avg_logprob": -0.21882619761457348, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.09880584711209e-05}, {"id": 19, "seek": 8516, "start": 85.16, "end": 91.24, "text": " You want them to provide you correct result set because if you wouldn't, we could alter", "tokens": [509, 528, 552, 281, 2893, 291, 3006, 1874, 992, 570, 498, 291, 2759, 380, 11, 321, 727, 11337], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 20, "seek": 8516, "start": 91.24, "end": 95.75999999999999, "text": " over my school tables to black hole and get a fantastic performance.", "tokens": [670, 452, 1395, 8020, 281, 2211, 5458, 293, 483, 257, 5456, 3389, 13], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 21, "seek": 8516, "start": 95.75999999999999, "end": 98.32, "text": " No errors too.", "tokens": [883, 13603, 886, 13], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 22, "seek": 8516, "start": 98.32, "end": 104.12, "text": " And also you want them to make sure they complete in that response time what your application", "tokens": [400, 611, 291, 528, 552, 281, 652, 988, 436, 3566, 294, 300, 4134, 565, 437, 428, 3861], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 23, "seek": 8516, "start": 104.12, "end": 105.12, "text": " expects.", "tokens": [33280, 13], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 24, "seek": 8516, "start": 105.12, "end": 108.32, "text": " I think that is a very important thing to understand.", "tokens": [286, 519, 300, 307, 257, 588, 1021, 551, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 25, "seek": 8516, "start": 108.32, "end": 114.28, "text": " If you look at from the individual developer standpoint, like Ryan application, hey, performance", "tokens": [759, 291, 574, 412, 490, 264, 2609, 10754, 15827, 11, 411, 9116, 3861, 11, 4177, 11, 3389], "temperature": 0.0, "avg_logprob": -0.255164418901716, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.00010680297418730333}, {"id": 26, "seek": 11428, "start": 114.28, "end": 118.8, "text": " response time for my queries is all I care about.", "tokens": [4134, 565, 337, 452, 24109, 307, 439, 286, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.2650449566724824, "compression_ratio": 1.5933014354066986, "no_speech_prob": 7.161276880651712e-05}, {"id": 27, "seek": 11428, "start": 118.8, "end": 126.16, "text": " And how that's internal database kitchen works, somebody else's problem.", "tokens": [400, 577, 300, 311, 6920, 8149, 6525, 1985, 11, 2618, 1646, 311, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2650449566724824, "compression_ratio": 1.5933014354066986, "no_speech_prob": 7.161276880651712e-05}, {"id": 28, "seek": 11428, "start": 126.16, "end": 133.04, "text": " Now if you think about the response time from the database point of view, that is often", "tokens": [823, 498, 291, 519, 466, 264, 4134, 565, 490, 264, 8149, 935, 295, 1910, 11, 300, 307, 2049], "temperature": 0.0, "avg_logprob": -0.2650449566724824, "compression_ratio": 1.5933014354066986, "no_speech_prob": 7.161276880651712e-05}, {"id": 29, "seek": 11428, "start": 133.04, "end": 140.56, "text": " seen like, well, I see that response time for a query is in average or whatever distribution.", "tokens": [1612, 411, 11, 731, 11, 286, 536, 300, 4134, 565, 337, 257, 14581, 307, 294, 4274, 420, 2035, 7316, 13], "temperature": 0.0, "avg_logprob": -0.2650449566724824, "compression_ratio": 1.5933014354066986, "no_speech_prob": 7.161276880651712e-05}, {"id": 30, "seek": 11428, "start": 140.56, "end": 142.4, "text": " We'll talk about that later.", "tokens": [492, 603, 751, 466, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.2650449566724824, "compression_ratio": 1.5933014354066986, "no_speech_prob": 7.161276880651712e-05}, {"id": 31, "seek": 14240, "start": 142.4, "end": 147.96, "text": " But that is different from what business cares about.", "tokens": [583, 300, 307, 819, 490, 437, 1606, 12310, 466, 13], "temperature": 0.0, "avg_logprob": -0.14433616466736526, "compression_ratio": 1.6554621848739495, "no_speech_prob": 2.5775863832677715e-05}, {"id": 32, "seek": 14240, "start": 147.96, "end": 154.0, "text": " If you think about the business point of view, you think about, well, do my user have outstanding", "tokens": [759, 291, 519, 466, 264, 1606, 935, 295, 1910, 11, 291, 519, 466, 11, 731, 11, 360, 452, 4195, 362, 14485], "temperature": 0.0, "avg_logprob": -0.14433616466736526, "compression_ratio": 1.6554621848739495, "no_speech_prob": 2.5775863832677715e-05}, {"id": 33, "seek": 14240, "start": 154.0, "end": 159.76, "text": " experience in terms of performance with all the application interactions?", "tokens": [1752, 294, 2115, 295, 3389, 365, 439, 264, 3861, 13280, 30], "temperature": 0.0, "avg_logprob": -0.14433616466736526, "compression_ratio": 1.6554621848739495, "no_speech_prob": 2.5775863832677715e-05}, {"id": 34, "seek": 14240, "start": 159.76, "end": 165.52, "text": " That means like a search should work and place an order should work and whatever.", "tokens": [663, 1355, 411, 257, 3164, 820, 589, 293, 1081, 364, 1668, 820, 589, 293, 2035, 13], "temperature": 0.0, "avg_logprob": -0.14433616466736526, "compression_ratio": 1.6554621848739495, "no_speech_prob": 2.5775863832677715e-05}, {"id": 35, "seek": 14240, "start": 165.52, "end": 172.08, "text": " And the database is important part of that, of course, but not is their complete part.", "tokens": [400, 264, 8149, 307, 1021, 644, 295, 300, 11, 295, 1164, 11, 457, 406, 307, 641, 3566, 644, 13], "temperature": 0.0, "avg_logprob": -0.14433616466736526, "compression_ratio": 1.6554621848739495, "no_speech_prob": 2.5775863832677715e-05}, {"id": 36, "seek": 17208, "start": 172.08, "end": 177.0, "text": " What is interesting in this case is what as database engineers, we often talk about those", "tokens": [708, 307, 1880, 294, 341, 1389, 307, 437, 382, 8149, 11955, 11, 321, 2049, 751, 466, 729], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 37, "seek": 17208, "start": 177.0, "end": 182.84, "text": " kind of different events, kind of like a bad performance and the downtime.", "tokens": [733, 295, 819, 3931, 11, 733, 295, 411, 257, 1578, 3389, 293, 264, 49648, 13], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 38, "seek": 17208, "start": 182.84, "end": 187.60000000000002, "text": " And say, well, you know, no, no, we weren't down, it just was taken 15 minutes to run", "tokens": [400, 584, 11, 731, 11, 291, 458, 11, 572, 11, 572, 11, 321, 4999, 380, 760, 11, 309, 445, 390, 2726, 2119, 2077, 281, 1190], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 39, "seek": 17208, "start": 187.60000000000002, "end": 190.60000000000002, "text": " my very basic query.", "tokens": [452, 588, 3875, 14581, 13], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 40, "seek": 17208, "start": 190.60000000000002, "end": 197.72000000000003, "text": " Well, from user standpoint, the bad performance, very bad performance isn't distinguishable", "tokens": [1042, 11, 490, 4195, 15827, 11, 264, 1578, 3389, 11, 588, 1578, 3389, 1943, 380, 20206, 712], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 41, "seek": 17208, "start": 197.72000000000003, "end": 199.28, "text": " from downtime.", "tokens": [490, 49648, 13], "temperature": 0.0, "avg_logprob": -0.20936880509058634, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.1148068450856954e-05}, {"id": 42, "seek": 19928, "start": 199.28, "end": 203.72, "text": " Because A, we don't have parents, then even if people are very patient, then the browser", "tokens": [1436, 316, 11, 321, 500, 380, 362, 3152, 11, 550, 754, 498, 561, 366, 588, 4537, 11, 550, 264, 11185], "temperature": 0.0, "avg_logprob": -0.20880367991688487, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00011877678480232134}, {"id": 43, "seek": 19928, "start": 203.72, "end": 208.0, "text": " or some other timeout will happen and nobody gives a shit about that query which may still", "tokens": [420, 512, 661, 565, 346, 486, 1051, 293, 5079, 2709, 257, 4611, 466, 300, 14581, 597, 815, 920], "temperature": 0.0, "avg_logprob": -0.20880367991688487, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00011877678480232134}, {"id": 44, "seek": 19928, "start": 208.0, "end": 211.64, "text": " continue to run.", "tokens": [2354, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.20880367991688487, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00011877678480232134}, {"id": 45, "seek": 19928, "start": 211.64, "end": 220.28, "text": " Another thing to understand about query performance is you do not want to focus on the averages.", "tokens": [3996, 551, 281, 1223, 466, 14581, 3389, 307, 291, 360, 406, 528, 281, 1879, 322, 264, 42257, 13], "temperature": 0.0, "avg_logprob": -0.20880367991688487, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00011877678480232134}, {"id": 46, "seek": 19928, "start": 220.28, "end": 225.72, "text": " I like this kind of one saying, but there was one sleeve demand to try to cross the", "tokens": [286, 411, 341, 733, 295, 472, 1566, 11, 457, 456, 390, 472, 21138, 4733, 281, 853, 281, 3278, 264], "temperature": 0.0, "avg_logprob": -0.20880367991688487, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.00011877678480232134}, {"id": 47, "seek": 22572, "start": 225.72, "end": 231.32, "text": " river in the average one meter deep.", "tokens": [6810, 294, 264, 4274, 472, 9255, 2452, 13], "temperature": 0.0, "avg_logprob": -0.14952648861307494, "compression_ratio": 1.5027932960893855, "no_speech_prob": 4.126952262595296e-05}, {"id": 48, "seek": 22572, "start": 231.32, "end": 233.76, "text": " That is same applies to the query.", "tokens": [663, 307, 912, 13165, 281, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.14952648861307494, "compression_ratio": 1.5027932960893855, "no_speech_prob": 4.126952262595296e-05}, {"id": 49, "seek": 22572, "start": 233.76, "end": 239.32, "text": " If your average query time is X, that means pretty much nothing.", "tokens": [759, 428, 4274, 14581, 565, 307, 1783, 11, 300, 1355, 1238, 709, 1825, 13], "temperature": 0.0, "avg_logprob": -0.14952648861307494, "compression_ratio": 1.5027932960893855, "no_speech_prob": 4.126952262595296e-05}, {"id": 50, "seek": 22572, "start": 239.32, "end": 245.24, "text": " You need to understand more about that.", "tokens": [509, 643, 281, 1223, 544, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.14952648861307494, "compression_ratio": 1.5027932960893855, "no_speech_prob": 4.126952262595296e-05}, {"id": 51, "seek": 22572, "start": 245.24, "end": 254.32, "text": " And I like in this case to look at their percentiles and even more to make sure you can look", "tokens": [400, 286, 411, 294, 341, 1389, 281, 574, 412, 641, 3043, 4680, 293, 754, 544, 281, 652, 988, 291, 393, 574], "temperature": 0.0, "avg_logprob": -0.14952648861307494, "compression_ratio": 1.5027932960893855, "no_speech_prob": 4.126952262595296e-05}, {"id": 52, "seek": 25432, "start": 254.32, "end": 261.76, "text": " at a specific distribution of your query response time.", "tokens": [412, 257, 2685, 7316, 295, 428, 14581, 4134, 565, 13], "temperature": 0.0, "avg_logprob": -0.20847241083780924, "compression_ratio": 1.5265957446808511, "no_speech_prob": 6.847345503047109e-05}, {"id": 53, "seek": 25432, "start": 261.76, "end": 265.92, "text": " If you have that, that gives you a lot more insight.", "tokens": [759, 291, 362, 300, 11, 300, 2709, 291, 257, 688, 544, 11269, 13], "temperature": 0.0, "avg_logprob": -0.20847241083780924, "compression_ratio": 1.5265957446808511, "no_speech_prob": 6.847345503047109e-05}, {"id": 54, "seek": 25432, "start": 265.92, "end": 272.12, "text": " Now one thing to understand about the percentile, you may be looking and saying, well, great.", "tokens": [823, 472, 551, 281, 1223, 466, 264, 3043, 794, 11, 291, 815, 312, 1237, 293, 1566, 11, 731, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.20847241083780924, "compression_ratio": 1.5265957446808511, "no_speech_prob": 6.847345503047109e-05}, {"id": 55, "seek": 25432, "start": 272.12, "end": 281.0, "text": " My queries have this decent 99 percentile, but that does not mean on a business side", "tokens": [1222, 24109, 362, 341, 8681, 11803, 3043, 794, 11, 457, 300, 775, 406, 914, 322, 257, 1606, 1252], "temperature": 0.0, "avg_logprob": -0.20847241083780924, "compression_ratio": 1.5265957446808511, "no_speech_prob": 6.847345503047109e-05}, {"id": 56, "seek": 28100, "start": 281.0, "end": 285.32, "text": " what 99 percent of your users have a good or acceptable experience.", "tokens": [437, 11803, 3043, 295, 428, 5022, 362, 257, 665, 420, 15513, 1752, 13], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 57, "seek": 28100, "start": 285.32, "end": 286.32, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 58, "seek": 28100, "start": 286.32, "end": 287.32, "text": " Well, because guess what?", "tokens": [1042, 11, 570, 2041, 437, 30], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 59, "seek": 28100, "start": 287.32, "end": 293.76, "text": " The single user interaction can correspond to a lot of queries sequentially, which all", "tokens": [440, 2167, 4195, 9285, 393, 6805, 281, 257, 688, 295, 24109, 5123, 3137, 11, 597, 439], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 60, "seek": 28100, "start": 293.76, "end": 300.72, "text": " add up and typically through their joining user has a number of those interactions.", "tokens": [909, 493, 293, 5850, 807, 641, 5549, 4195, 575, 257, 1230, 295, 729, 13280, 13], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 61, "seek": 28100, "start": 300.72, "end": 308.72, "text": " So I would say even 99 percentile that may all well, depending on your application, only", "tokens": [407, 286, 576, 584, 754, 11803, 3043, 794, 300, 815, 439, 731, 11, 5413, 322, 428, 3861, 11, 787], "temperature": 0.0, "avg_logprob": -0.20329363592739763, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00013005170330870897}, {"id": 62, "seek": 30872, "start": 308.72, "end": 312.64000000000004, "text": " correspond to like 50 percent of user session.", "tokens": [6805, 281, 411, 2625, 3043, 295, 4195, 5481, 13], "temperature": 0.0, "avg_logprob": -0.22680839998968716, "compression_ratio": 1.4431818181818181, "no_speech_prob": 0.00020309409592300653}, {"id": 63, "seek": 30872, "start": 312.64000000000004, "end": 321.84000000000003, "text": " So if you really see the complicated large environments, they are really focused on either", "tokens": [407, 498, 291, 534, 536, 264, 6179, 2416, 12388, 11, 436, 366, 534, 5178, 322, 2139], "temperature": 0.0, "avg_logprob": -0.22680839998968716, "compression_ratio": 1.4431818181818181, "no_speech_prob": 0.00020309409592300653}, {"id": 64, "seek": 30872, "start": 321.84000000000003, "end": 328.64000000000004, "text": " relatively short SLA or rather high percentiles.", "tokens": [7226, 2099, 318, 11435, 420, 2831, 1090, 3043, 4680, 13], "temperature": 0.0, "avg_logprob": -0.22680839998968716, "compression_ratio": 1.4431818181818181, "no_speech_prob": 0.00020309409592300653}, {"id": 65, "seek": 30872, "start": 328.64000000000004, "end": 334.04, "text": " Another thing that I would encourage to pay attention to is errors.", "tokens": [3996, 551, 300, 286, 576, 5373, 281, 1689, 3202, 281, 307, 13603, 13], "temperature": 0.0, "avg_logprob": -0.22680839998968716, "compression_ratio": 1.4431818181818181, "no_speech_prob": 0.00020309409592300653}, {"id": 66, "seek": 33404, "start": 334.04, "end": 340.52000000000004, "text": " And make sure you are measuring response time for those as well, because errors actually", "tokens": [400, 652, 988, 291, 366, 13389, 4134, 565, 337, 729, 382, 731, 11, 570, 13603, 767], "temperature": 0.0, "avg_logprob": -0.13498119316478768, "compression_ratio": 1.7446808510638299, "no_speech_prob": 6.269002915360034e-05}, {"id": 67, "seek": 33404, "start": 340.52000000000004, "end": 344.32, "text": " can be offered two kinds, fast errors and slow errors.", "tokens": [393, 312, 8059, 732, 3685, 11, 2370, 13603, 293, 2964, 13603, 13], "temperature": 0.0, "avg_logprob": -0.13498119316478768, "compression_ratio": 1.7446808510638299, "no_speech_prob": 6.269002915360034e-05}, {"id": 68, "seek": 33404, "start": 344.32, "end": 348.96000000000004, "text": " In certain cases, let's say if your table doesn't exist, you may be like, get the response", "tokens": [682, 1629, 3331, 11, 718, 311, 584, 498, 428, 3199, 1177, 380, 2514, 11, 291, 815, 312, 411, 11, 483, 264, 4134], "temperature": 0.0, "avg_logprob": -0.13498119316478768, "compression_ratio": 1.7446808510638299, "no_speech_prob": 6.269002915360034e-05}, {"id": 69, "seek": 33404, "start": 348.96000000000004, "end": 355.8, "text": " time straight away, and if you put all your error queries and actually normal queries", "tokens": [565, 2997, 1314, 11, 293, 498, 291, 829, 439, 428, 6713, 24109, 293, 767, 2710, 24109], "temperature": 0.0, "avg_logprob": -0.13498119316478768, "compression_ratio": 1.7446808510638299, "no_speech_prob": 6.269002915360034e-05}, {"id": 70, "seek": 33404, "start": 355.8, "end": 362.0, "text": " in the same bucket, you may say, oh my gosh, my response times are doing kind of so well.", "tokens": [294, 264, 912, 13058, 11, 291, 815, 584, 11, 1954, 452, 6502, 11, 452, 4134, 1413, 366, 884, 733, 295, 370, 731, 13], "temperature": 0.0, "avg_logprob": -0.13498119316478768, "compression_ratio": 1.7446808510638299, "no_speech_prob": 6.269002915360034e-05}, {"id": 71, "seek": 36200, "start": 362.0, "end": 367.44, "text": " But on the other hand, if your query is, for example, error is a lock weight time out,", "tokens": [583, 322, 264, 661, 1011, 11, 498, 428, 14581, 307, 11, 337, 1365, 11, 6713, 307, 257, 4017, 3364, 565, 484, 11], "temperature": 0.0, "avg_logprob": -0.14650908383456143, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.267613854201045e-05}, {"id": 72, "seek": 36200, "start": 367.44, "end": 369.56, "text": " then that is a slow error.", "tokens": [550, 300, 307, 257, 2964, 6713, 13], "temperature": 0.0, "avg_logprob": -0.14650908383456143, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.267613854201045e-05}, {"id": 73, "seek": 36200, "start": 369.56, "end": 374.84, "text": " It actually will have a higher response time than the normal cases.", "tokens": [467, 767, 486, 362, 257, 2946, 4134, 565, 813, 264, 2710, 3331, 13], "temperature": 0.0, "avg_logprob": -0.14650908383456143, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.267613854201045e-05}, {"id": 74, "seek": 36200, "start": 374.84, "end": 381.96, "text": " That is why I always suggest to make sure we measure response time for normal queries", "tokens": [663, 307, 983, 286, 1009, 3402, 281, 652, 988, 321, 3481, 4134, 565, 337, 2710, 24109], "temperature": 0.0, "avg_logprob": -0.14650908383456143, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.267613854201045e-05}, {"id": 75, "seek": 36200, "start": 381.96, "end": 385.92, "text": " and for queries with problems differently.", "tokens": [293, 337, 24109, 365, 2740, 7614, 13], "temperature": 0.0, "avg_logprob": -0.14650908383456143, "compression_ratio": 1.5816326530612246, "no_speech_prob": 2.267613854201045e-05}, {"id": 76, "seek": 38592, "start": 385.92, "end": 395.08000000000004, "text": " Another thing which is very important is looking at response time over time, because traffic", "tokens": [3996, 551, 597, 307, 588, 1021, 307, 1237, 412, 4134, 565, 670, 565, 11, 570, 6419], "temperature": 0.0, "avg_logprob": -0.1800939862321063, "compression_ratio": 1.5821596244131455, "no_speech_prob": 2.2549216737388633e-05}, {"id": 77, "seek": 38592, "start": 395.08000000000004, "end": 400.20000000000005, "text": " changes, a lot of things are going on in the system and just saying, hey, I'll have a response", "tokens": [2962, 11, 257, 688, 295, 721, 366, 516, 322, 294, 264, 1185, 293, 445, 1566, 11, 4177, 11, 286, 603, 362, 257, 4134], "temperature": 0.0, "avg_logprob": -0.1800939862321063, "compression_ratio": 1.5821596244131455, "no_speech_prob": 2.2549216737388633e-05}, {"id": 78, "seek": 38592, "start": 400.20000000000005, "end": 404.44, "text": " time of x over some long period of time, it's not very helpful.", "tokens": [565, 295, 2031, 670, 512, 938, 2896, 295, 565, 11, 309, 311, 406, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1800939862321063, "compression_ratio": 1.5821596244131455, "no_speech_prob": 2.2549216737388633e-05}, {"id": 79, "seek": 38592, "start": 404.44, "end": 411.52000000000004, "text": " Also what you would see in many cases, you still start those like a small performance", "tokens": [2743, 437, 291, 576, 536, 294, 867, 3331, 11, 291, 920, 722, 729, 411, 257, 1359, 3389], "temperature": 0.0, "avg_logprob": -0.1800939862321063, "compression_ratio": 1.5821596244131455, "no_speech_prob": 2.2549216737388633e-05}, {"id": 80, "seek": 41152, "start": 411.52, "end": 421.52, "text": " problems, maybe like SLA violations, which are if unfixed, they convert in the downtime.", "tokens": [2740, 11, 1310, 411, 318, 11435, 30405, 11, 597, 366, 498, 3971, 40303, 11, 436, 7620, 294, 264, 49648, 13], "temperature": 0.0, "avg_logprob": -0.17737177143926205, "compression_ratio": 1.5446808510638297, "no_speech_prob": 5.6651730119483545e-05}, {"id": 81, "seek": 41152, "start": 421.52, "end": 427.32, "text": " For example, in my SQL world, you may say, well, I have forgotten this kind of runaway", "tokens": [1171, 1365, 11, 294, 452, 19200, 1002, 11, 291, 815, 584, 11, 731, 11, 286, 362, 11832, 341, 733, 295, 1190, 10318], "temperature": 0.0, "avg_logprob": -0.17737177143926205, "compression_ratio": 1.5446808510638297, "no_speech_prob": 5.6651730119483545e-05}, {"id": 82, "seek": 41152, "start": 427.32, "end": 430.79999999999995, "text": " query, and my history accumulates.", "tokens": [14581, 11, 293, 452, 2503, 12989, 26192, 13], "temperature": 0.0, "avg_logprob": -0.17737177143926205, "compression_ratio": 1.5446808510638297, "no_speech_prob": 5.6651730119483545e-05}, {"id": 83, "seek": 41152, "start": 430.79999999999995, "end": 434.71999999999997, "text": " It will slowly increase and increase your response time.", "tokens": [467, 486, 5692, 3488, 293, 3488, 428, 4134, 565, 13], "temperature": 0.0, "avg_logprob": -0.17737177143926205, "compression_ratio": 1.5446808510638297, "no_speech_prob": 5.6651730119483545e-05}, {"id": 84, "seek": 41152, "start": 434.71999999999997, "end": 439.15999999999997, "text": " If you measure that over time and say, well, something is not trending in the right directions,", "tokens": [759, 291, 3481, 300, 670, 565, 293, 584, 11, 731, 11, 746, 307, 406, 28692, 294, 264, 558, 11095, 11], "temperature": 0.0, "avg_logprob": -0.17737177143926205, "compression_ratio": 1.5446808510638297, "no_speech_prob": 5.6651730119483545e-05}, {"id": 85, "seek": 43916, "start": 439.16, "end": 446.20000000000005, "text": " you probably can fix it before that will be seen as a downtime by your users.", "tokens": [291, 1391, 393, 3191, 309, 949, 300, 486, 312, 1612, 382, 257, 49648, 538, 428, 5022, 13], "temperature": 0.0, "avg_logprob": -0.20149265016828263, "compression_ratio": 1.6411483253588517, "no_speech_prob": 2.9032160455244593e-05}, {"id": 86, "seek": 43916, "start": 446.20000000000005, "end": 453.20000000000005, "text": " If you are not, then not so much.", "tokens": [759, 291, 366, 406, 11, 550, 406, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.20149265016828263, "compression_ratio": 1.6411483253588517, "no_speech_prob": 2.9032160455244593e-05}, {"id": 87, "seek": 43916, "start": 453.20000000000005, "end": 459.76000000000005, "text": " This is example what we have here, what you often may see something like this as well,", "tokens": [639, 307, 1365, 437, 321, 362, 510, 11, 437, 291, 2049, 815, 536, 746, 411, 341, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.20149265016828263, "compression_ratio": 1.6411483253588517, "no_speech_prob": 2.9032160455244593e-05}, {"id": 88, "seek": 43916, "start": 459.76000000000005, "end": 464.76000000000005, "text": " where all the queries have like a spike in the response time, which you often may correspond", "tokens": [689, 439, 264, 24109, 362, 411, 257, 21053, 294, 264, 4134, 565, 11, 597, 291, 2049, 815, 6805], "temperature": 0.0, "avg_logprob": -0.20149265016828263, "compression_ratio": 1.6411483253588517, "no_speech_prob": 2.9032160455244593e-05}, {"id": 89, "seek": 43916, "start": 464.76000000000005, "end": 468.28000000000003, "text": " to something external happening in the environment.", "tokens": [281, 746, 8320, 2737, 294, 264, 2823, 13], "temperature": 0.0, "avg_logprob": -0.20149265016828263, "compression_ratio": 1.6411483253588517, "no_speech_prob": 2.9032160455244593e-05}, {"id": 90, "seek": 46828, "start": 468.28, "end": 472.4, "text": " I think here is what is very interesting, especially for us running in the cloud, we", "tokens": [286, 519, 510, 307, 437, 307, 588, 1880, 11, 2318, 337, 505, 2614, 294, 264, 4588, 11, 321], "temperature": 0.0, "avg_logprob": -0.26197333188401056, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.906065052840859e-05}, {"id": 91, "seek": 46828, "start": 472.4, "end": 476.47999999999996, "text": " only have limited observability to environment.", "tokens": [787, 362, 5567, 9951, 2310, 281, 2823, 13], "temperature": 0.0, "avg_logprob": -0.26197333188401056, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.906065052840859e-05}, {"id": 92, "seek": 46828, "start": 476.47999999999996, "end": 480.79999999999995, "text": " If there is some shit going on on the Amazon backend, they're not going to tell us that.", "tokens": [759, 456, 307, 512, 4611, 516, 322, 322, 264, 6795, 38087, 11, 436, 434, 406, 516, 281, 980, 505, 300, 13], "temperature": 0.0, "avg_logprob": -0.26197333188401056, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.906065052840859e-05}, {"id": 93, "seek": 46828, "start": 480.79999999999995, "end": 486.67999999999995, "text": " Oh, you know what, we had, let's say, some free hard drives failed, which back our EBS", "tokens": [876, 11, 291, 458, 437, 11, 321, 632, 11, 718, 311, 584, 11, 512, 1737, 1152, 11754, 7612, 11, 597, 646, 527, 462, 8176], "temperature": 0.0, "avg_logprob": -0.26197333188401056, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.906065052840859e-05}, {"id": 94, "seek": 46828, "start": 486.67999999999995, "end": 494.28, "text": " and we had to some rebalance, yada, yada.", "tokens": [293, 321, 632, 281, 512, 319, 29215, 11, 288, 1538, 11, 288, 1538, 13], "temperature": 0.0, "avg_logprob": -0.26197333188401056, "compression_ratio": 1.5486725663716814, "no_speech_prob": 5.906065052840859e-05}, {"id": 95, "seek": 49428, "start": 494.28, "end": 500.32, "text": " The other question I would ask is where we want to measure response time from a queries.", "tokens": [440, 661, 1168, 286, 576, 1029, 307, 689, 321, 528, 281, 3481, 4134, 565, 490, 257, 24109, 13], "temperature": 0.0, "avg_logprob": -0.22345093886057535, "compression_ratio": 1.6775510204081632, "no_speech_prob": 2.3322108972934075e-05}, {"id": 96, "seek": 49428, "start": 500.32, "end": 506.44, "text": " In my opinion, both application view and database you are in the combinations are very helpful", "tokens": [682, 452, 4800, 11, 1293, 3861, 1910, 293, 8149, 291, 366, 294, 264, 21267, 366, 588, 4961], "temperature": 0.0, "avg_logprob": -0.22345093886057535, "compression_ratio": 1.6775510204081632, "no_speech_prob": 2.3322108972934075e-05}, {"id": 97, "seek": 49428, "start": 506.44, "end": 510.08, "text": " because the application can see real thing.", "tokens": [570, 264, 3861, 393, 536, 957, 551, 13], "temperature": 0.0, "avg_logprob": -0.22345093886057535, "compression_ratio": 1.6775510204081632, "no_speech_prob": 2.3322108972934075e-05}, {"id": 98, "seek": 49428, "start": 510.08, "end": 515.0799999999999, "text": " If your network, for example, is adding some latency or whatever, and you will see that", "tokens": [759, 428, 3209, 11, 337, 1365, 11, 307, 5127, 512, 27043, 420, 2035, 11, 293, 291, 486, 536, 300], "temperature": 0.0, "avg_logprob": -0.22345093886057535, "compression_ratio": 1.6775510204081632, "no_speech_prob": 2.3322108972934075e-05}, {"id": 99, "seek": 49428, "start": 515.0799999999999, "end": 520.9599999999999, "text": " from application, not so much in the database, because it's only sees from, hey, I got response", "tokens": [490, 3861, 11, 406, 370, 709, 294, 264, 8149, 11, 570, 309, 311, 787, 8194, 490, 11, 4177, 11, 286, 658, 4134], "temperature": 0.0, "avg_logprob": -0.22345093886057535, "compression_ratio": 1.6775510204081632, "no_speech_prob": 2.3322108972934075e-05}, {"id": 100, "seek": 52096, "start": 520.96, "end": 526.48, "text": " to, then it's sent the data back.", "tokens": [281, 11, 550, 309, 311, 2279, 264, 1412, 646, 13], "temperature": 0.0, "avg_logprob": -0.236917724609375, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001374641724396497}, {"id": 101, "seek": 52096, "start": 526.48, "end": 531.96, "text": " But the database view allows you often to see a lot more inside about what have been", "tokens": [583, 264, 8149, 1910, 4045, 291, 2049, 281, 536, 257, 688, 544, 1854, 466, 437, 362, 668], "temperature": 0.0, "avg_logprob": -0.236917724609375, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001374641724396497}, {"id": 102, "seek": 52096, "start": 531.96, "end": 540.0, "text": " going on inside, where from application side, we often can just capture the query time,", "tokens": [516, 322, 1854, 11, 689, 490, 3861, 1252, 11, 321, 2049, 393, 445, 7983, 264, 14581, 565, 11], "temperature": 0.0, "avg_logprob": -0.236917724609375, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001374641724396497}, {"id": 103, "seek": 52096, "start": 540.0, "end": 544.5600000000001, "text": " maybe some very basic additional parameters.", "tokens": [1310, 512, 588, 3875, 4497, 9834, 13], "temperature": 0.0, "avg_logprob": -0.236917724609375, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001374641724396497}, {"id": 104, "seek": 52096, "start": 544.5600000000001, "end": 548.4000000000001, "text": " So what we spoke from our business view, right?", "tokens": [407, 437, 321, 7179, 490, 527, 1606, 1910, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.236917724609375, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.0001374641724396497}, {"id": 105, "seek": 54840, "start": 548.4, "end": 553.8, "text": " Well, we already said what that all users have outstanding performance experience of", "tokens": [1042, 11, 321, 1217, 848, 437, 300, 439, 5022, 362, 14485, 3389, 1752, 295], "temperature": 0.0, "avg_logprob": -0.19822364725092406, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.00020960201800335199}, {"id": 106, "seek": 54840, "start": 553.8, "end": 556.12, "text": " all the application interactions, right?", "tokens": [439, 264, 3861, 13280, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19822364725092406, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.00020960201800335199}, {"id": 107, "seek": 54840, "start": 556.12, "end": 563.16, "text": " Let's now try to break it down a little bit more, right, to what that may mean.", "tokens": [961, 311, 586, 853, 281, 1821, 309, 760, 257, 707, 857, 544, 11, 558, 11, 281, 437, 300, 815, 914, 13], "temperature": 0.0, "avg_logprob": -0.19822364725092406, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.00020960201800335199}, {"id": 108, "seek": 54840, "start": 563.16, "end": 568.16, "text": " In this case, I want to introduce this little project or flag from you.", "tokens": [682, 341, 1389, 11, 286, 528, 281, 5366, 341, 707, 1716, 420, 7166, 490, 291, 13], "temperature": 0.0, "avg_logprob": -0.19822364725092406, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.00020960201800335199}, {"id": 109, "seek": 54840, "start": 568.16, "end": 575.76, "text": " This is SQL Commenter project by Google, right, I mean, which is pretty cool in terms of what", "tokens": [639, 307, 19200, 2432, 518, 260, 1716, 538, 3329, 11, 558, 11, 286, 914, 11, 597, 307, 1238, 1627, 294, 2115, 295, 437], "temperature": 0.0, "avg_logprob": -0.19822364725092406, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.00020960201800335199}, {"id": 110, "seek": 57576, "start": 575.76, "end": 581.68, "text": " it allows to pass you the metadata, right, which you understand as developer all the way", "tokens": [309, 4045, 281, 1320, 291, 264, 26603, 11, 558, 11, 597, 291, 1223, 382, 10754, 439, 264, 636], "temperature": 0.0, "avg_logprob": -0.1564637525582019, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.00013034924631938338}, {"id": 111, "seek": 57576, "start": 581.68, "end": 583.2, "text": " to SQL query.", "tokens": [281, 19200, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1564637525582019, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.00013034924631938338}, {"id": 112, "seek": 57576, "start": 583.2, "end": 591.2, "text": " They implemented that support from a number of frameworks, right, and it's also supported", "tokens": [814, 12270, 300, 1406, 490, 257, 1230, 295, 29834, 11, 558, 11, 293, 309, 311, 611, 8104], "temperature": 0.0, "avg_logprob": -0.1564637525582019, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.00013034924631938338}, {"id": 113, "seek": 57576, "start": 591.2, "end": 596.16, "text": " in their Google Cloud monitoring environment, right?", "tokens": [294, 641, 3329, 8061, 11028, 2823, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1564637525582019, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.00013034924631938338}, {"id": 114, "seek": 57576, "start": 596.16, "end": 601.76, "text": " And I wouldn't very much see that developed more, right, and for at least kind of us come", "tokens": [400, 286, 2759, 380, 588, 709, 536, 300, 4743, 544, 11, 558, 11, 293, 337, 412, 1935, 733, 295, 505, 808], "temperature": 0.0, "avg_logprob": -0.1564637525582019, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.00013034924631938338}, {"id": 115, "seek": 60176, "start": 601.76, "end": 607.72, "text": " to some sort of shared standards between the databases, right, to wherever, how we can augment", "tokens": [281, 512, 1333, 295, 5507, 7787, 1296, 264, 22380, 11, 558, 11, 281, 8660, 11, 577, 321, 393, 29919], "temperature": 0.0, "avg_logprob": -0.20683256009729897, "compression_ratio": 1.5841121495327102, "no_speech_prob": 9.892575326375663e-05}, {"id": 116, "seek": 60176, "start": 607.72, "end": 616.04, "text": " query information with sort of like a tags, values, right, which users care about.", "tokens": [14581, 1589, 365, 1333, 295, 411, 257, 18632, 11, 4190, 11, 558, 11, 597, 5022, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.20683256009729897, "compression_ratio": 1.5841121495327102, "no_speech_prob": 9.892575326375663e-05}, {"id": 117, "seek": 60176, "start": 616.04, "end": 619.6, "text": " So what are possibilities which can be quite helpful in this regard?", "tokens": [407, 437, 366, 12178, 597, 393, 312, 1596, 4961, 294, 341, 3843, 30], "temperature": 0.0, "avg_logprob": -0.20683256009729897, "compression_ratio": 1.5841121495327102, "no_speech_prob": 9.892575326375663e-05}, {"id": 118, "seek": 60176, "start": 619.6, "end": 625.96, "text": " Well, finding, for example, who is our actual user tenant, who is query, corresponds, right,", "tokens": [1042, 11, 5006, 11, 337, 1365, 11, 567, 307, 527, 3539, 4195, 31000, 11, 567, 307, 14581, 11, 23249, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.20683256009729897, "compression_ratio": 1.5841121495327102, "no_speech_prob": 9.892575326375663e-05}, {"id": 119, "seek": 62596, "start": 625.96, "end": 633.2, "text": " because we often may have, you know, different performance issues, right, finding the application,", "tokens": [570, 321, 2049, 815, 362, 11, 291, 458, 11, 819, 3389, 2663, 11, 558, 11, 5006, 264, 3861, 11], "temperature": 0.0, "avg_logprob": -0.25569291909535724, "compression_ratio": 1.6875, "no_speech_prob": 7.485876267310232e-05}, {"id": 120, "seek": 62596, "start": 633.2, "end": 637.96, "text": " like some sort of like a subset of application functionality where many of them may be hitting", "tokens": [411, 512, 1333, 295, 411, 257, 25993, 295, 3861, 14980, 689, 867, 295, 552, 815, 312, 8850], "temperature": 0.0, "avg_logprob": -0.25569291909535724, "compression_ratio": 1.6875, "no_speech_prob": 7.485876267310232e-05}, {"id": 121, "seek": 62596, "start": 637.96, "end": 644.8000000000001, "text": " the application, right, version information, maybe information about like an, their engineer", "tokens": [264, 3861, 11, 558, 11, 3037, 1589, 11, 1310, 1589, 466, 411, 364, 11, 641, 11403], "temperature": 0.0, "avg_logprob": -0.25569291909535724, "compression_ratio": 1.6875, "no_speech_prob": 7.485876267310232e-05}, {"id": 122, "seek": 62596, "start": 644.8000000000001, "end": 646.84, "text": " of a team who is responsible.", "tokens": [295, 257, 1469, 567, 307, 6250, 13], "temperature": 0.0, "avg_logprob": -0.25569291909535724, "compression_ratio": 1.6875, "no_speech_prob": 7.485876267310232e-05}, {"id": 123, "seek": 62596, "start": 646.84, "end": 652.64, "text": " I often see DBAs or SRAs team having problem, like, oh, I see this nasty query which was", "tokens": [286, 2049, 536, 413, 9295, 82, 420, 318, 3750, 82, 1469, 1419, 1154, 11, 411, 11, 1954, 11, 286, 536, 341, 17923, 14581, 597, 390], "temperature": 0.0, "avg_logprob": -0.25569291909535724, "compression_ratio": 1.6875, "no_speech_prob": 7.485876267310232e-05}, {"id": 124, "seek": 65264, "start": 652.64, "end": 657.92, "text": " shipped yesterday, I know because, shipped today because I know it wasn't very yesterday,", "tokens": [25312, 5186, 11, 286, 458, 570, 11, 25312, 965, 570, 286, 458, 309, 2067, 380, 588, 5186, 11], "temperature": 0.0, "avg_logprob": -0.21656249405501724, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.282771285739727e-05}, {"id": 125, "seek": 65264, "start": 657.92, "end": 664.36, "text": " right, but now having to find out who a hell introduced that stupid query, maybe problematic", "tokens": [558, 11, 457, 586, 1419, 281, 915, 484, 567, 257, 4921, 7268, 300, 6631, 14581, 11, 1310, 19011], "temperature": 0.0, "avg_logprob": -0.21656249405501724, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.282771285739727e-05}, {"id": 126, "seek": 65264, "start": 664.36, "end": 667.48, "text": " in a large environment.", "tokens": [294, 257, 2416, 2823, 13], "temperature": 0.0, "avg_logprob": -0.21656249405501724, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.282771285739727e-05}, {"id": 127, "seek": 65264, "start": 667.48, "end": 673.1999999999999, "text": " Now a lot of focus, and I think the core of the query-based observability may be about", "tokens": [823, 257, 688, 295, 1879, 11, 293, 286, 519, 264, 4965, 295, 264, 14581, 12, 6032, 9951, 2310, 815, 312, 466], "temperature": 0.0, "avg_logprob": -0.21656249405501724, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.282771285739727e-05}, {"id": 128, "seek": 65264, "start": 673.1999999999999, "end": 674.1999999999999, "text": " the query.", "tokens": [264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.21656249405501724, "compression_ratio": 1.5510204081632653, "no_speech_prob": 3.282771285739727e-05}, {"id": 129, "seek": 67420, "start": 674.2, "end": 683.6400000000001, "text": " But the query, I mean, obviously like a query with sort of like it's, which are same except", "tokens": [583, 264, 14581, 11, 286, 914, 11, 2745, 411, 257, 14581, 365, 1333, 295, 411, 309, 311, 11, 597, 366, 912, 3993], "temperature": 0.0, "avg_logprob": -0.20707695742687546, "compression_ratio": 1.5654205607476634, "no_speech_prob": 8.444890409009531e-05}, {"id": 130, "seek": 67420, "start": 683.6400000000001, "end": 688.5200000000001, "text": " different parameters, and that is very helpful because, well, obviously they have a different", "tokens": [819, 9834, 11, 293, 300, 307, 588, 4961, 570, 11, 731, 11, 2745, 436, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.20707695742687546, "compression_ratio": 1.5654205607476634, "no_speech_prob": 8.444890409009531e-05}, {"id": 131, "seek": 67420, "start": 688.5200000000001, "end": 693.5200000000001, "text": " complexity, different expected SLA, and so on and so forth.", "tokens": [14024, 11, 819, 5176, 318, 11435, 11, 293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.20707695742687546, "compression_ratio": 1.5654205607476634, "no_speech_prob": 8.444890409009531e-05}, {"id": 132, "seek": 67420, "start": 693.5200000000001, "end": 703.2, "text": " The next way also to break things down for me would be to look at the schema or database,", "tokens": [440, 958, 636, 611, 281, 1821, 721, 760, 337, 385, 576, 312, 281, 574, 412, 264, 34078, 420, 8149, 11], "temperature": 0.0, "avg_logprob": -0.20707695742687546, "compression_ratio": 1.5654205607476634, "no_speech_prob": 8.444890409009531e-05}, {"id": 133, "seek": 70320, "start": 703.2, "end": 705.88, "text": " and why is that interesting?", "tokens": [293, 983, 307, 300, 1880, 30], "temperature": 0.0, "avg_logprob": -0.33779546192714144, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00046051901881583035}, {"id": 134, "seek": 70320, "start": 705.88, "end": 706.88, "text": " How?", "tokens": [1012, 30], "temperature": 0.0, "avg_logprob": -0.33779546192714144, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00046051901881583035}, {"id": 135, "seek": 70320, "start": 706.88, "end": 716.88, "text": " I just noticed right now what it's been cut a bit, you see, well, anyway, life is life.", "tokens": [286, 445, 5694, 558, 586, 437, 309, 311, 668, 1723, 257, 857, 11, 291, 536, 11, 731, 11, 4033, 11, 993, 307, 993, 13], "temperature": 0.0, "avg_logprob": -0.33779546192714144, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00046051901881583035}, {"id": 136, "seek": 70320, "start": 716.88, "end": 724.48, "text": " I'm just not going to be lucky in this room, right, yes, yeah, but, well, we can blame", "tokens": [286, 478, 445, 406, 516, 281, 312, 6356, 294, 341, 1808, 11, 558, 11, 2086, 11, 1338, 11, 457, 11, 731, 11, 321, 393, 10127], "temperature": 0.0, "avg_logprob": -0.33779546192714144, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00046051901881583035}, {"id": 137, "seek": 70320, "start": 724.48, "end": 729.6800000000001, "text": " our windows, right, on this conference we can and should blame windows.", "tokens": [527, 9309, 11, 558, 11, 322, 341, 7586, 321, 393, 293, 820, 10127, 9309, 13], "temperature": 0.0, "avg_logprob": -0.33779546192714144, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00046051901881583035}, {"id": 138, "seek": 72968, "start": 729.68, "end": 739.52, "text": " Okay, well, why schema and database are also good because often we would separate in the", "tokens": [1033, 11, 731, 11, 983, 34078, 293, 8149, 366, 611, 665, 570, 2049, 321, 576, 4994, 294, 264], "temperature": 0.0, "avg_logprob": -0.20100001990795135, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0002487690362613648}, {"id": 139, "seek": 72968, "start": 739.52, "end": 747.5999999999999, "text": " multi-tenant applications different tenants by schema, right, and in that case that gives", "tokens": [4825, 12, 1147, 394, 5821, 819, 31216, 538, 34078, 11, 558, 11, 293, 294, 300, 1389, 300, 2709], "temperature": 0.0, "avg_logprob": -0.20100001990795135, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0002487690362613648}, {"id": 140, "seek": 72968, "start": 747.5999999999999, "end": 755.28, "text": " us a good profiling for performance of their different schemas, right, like we can see here", "tokens": [505, 257, 665, 1740, 4883, 337, 3389, 295, 641, 819, 22627, 296, 11, 558, 11, 411, 321, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.20100001990795135, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0002487690362613648}, {"id": 141, "seek": 75528, "start": 755.28, "end": 760.76, "text": " in the example with PMM tool.", "tokens": [294, 264, 1365, 365, 12499, 44, 2290, 13], "temperature": 0.0, "avg_logprob": -0.21167652130126954, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.476878090528771e-05}, {"id": 142, "seek": 75528, "start": 760.76, "end": 765.68, "text": " Another thing what I found is very helpful to find a way to separate the data by different", "tokens": [3996, 551, 437, 286, 1352, 307, 588, 4961, 281, 915, 257, 636, 281, 4994, 264, 1412, 538, 819], "temperature": 0.0, "avg_logprob": -0.21167652130126954, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.476878090528771e-05}, {"id": 143, "seek": 75528, "start": 765.68, "end": 771.92, "text": " tables, right, in many cases you want to say, hey, you know what, how a query is hitting", "tokens": [8020, 11, 558, 11, 294, 867, 3331, 291, 528, 281, 584, 11, 4177, 11, 291, 458, 437, 11, 577, 257, 14581, 307, 8850], "temperature": 0.0, "avg_logprob": -0.21167652130126954, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.476878090528771e-05}, {"id": 144, "seek": 75528, "start": 771.92, "end": 777.52, "text": " given table is affected, especially if it did some change which relates to the table.", "tokens": [2212, 3199, 307, 8028, 11, 2318, 498, 309, 630, 512, 1319, 597, 16155, 281, 264, 3199, 13], "temperature": 0.0, "avg_logprob": -0.21167652130126954, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.476878090528771e-05}, {"id": 145, "seek": 75528, "start": 777.52, "end": 783.6, "text": " Hey, you know what, I changed the indexing on this table, let me see how all the queries", "tokens": [1911, 11, 291, 458, 437, 11, 286, 3105, 264, 8186, 278, 322, 341, 3199, 11, 718, 385, 536, 577, 439, 264, 24109], "temperature": 0.0, "avg_logprob": -0.21167652130126954, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.476878090528771e-05}, {"id": 146, "seek": 78360, "start": 783.6, "end": 792.6, "text": " hitting this table is impacted, very helpful because there may be some surprising differences.", "tokens": [8850, 341, 3199, 307, 15653, 11, 588, 4961, 570, 456, 815, 312, 512, 8830, 7300, 13], "temperature": 0.0, "avg_logprob": -0.13345775121374975, "compression_ratio": 1.6697247706422018, "no_speech_prob": 1.2584528121806215e-05}, {"id": 147, "seek": 78360, "start": 792.6, "end": 797.44, "text": " Database users, that is another thing which is quite helpful because that often allows", "tokens": [40461, 651, 5022, 11, 300, 307, 1071, 551, 597, 307, 1596, 4961, 570, 300, 2049, 4045], "temperature": 0.0, "avg_logprob": -0.13345775121374975, "compression_ratio": 1.6697247706422018, "no_speech_prob": 1.2584528121806215e-05}, {"id": 148, "seek": 78360, "start": 797.44, "end": 801.96, "text": " us to identify the service application, right, if you're following good security practices", "tokens": [505, 281, 5876, 264, 2643, 3861, 11, 558, 11, 498, 291, 434, 3480, 665, 3825, 7525], "temperature": 0.0, "avg_logprob": -0.13345775121374975, "compression_ratio": 1.6697247706422018, "no_speech_prob": 1.2584528121806215e-05}, {"id": 149, "seek": 78360, "start": 801.96, "end": 808.08, "text": " you would not let all your applications, right, just use one username, you know, not a good", "tokens": [291, 576, 406, 718, 439, 428, 5821, 11, 558, 11, 445, 764, 472, 30351, 11, 291, 458, 11, 406, 257, 665], "temperature": 0.0, "avg_logprob": -0.13345775121374975, "compression_ratio": 1.6697247706422018, "no_speech_prob": 1.2584528121806215e-05}, {"id": 150, "seek": 80808, "start": 808.08, "end": 816.1600000000001, "text": " idea, right, and also find human troublemakers, right, which are doing, having direct access,", "tokens": [1558, 11, 558, 11, 293, 611, 915, 1952, 3455, 1113, 19552, 11, 558, 11, 597, 366, 884, 11, 1419, 2047, 2105, 11], "temperature": 0.0, "avg_logprob": -0.19529408099604587, "compression_ratio": 1.7320574162679425, "no_speech_prob": 9.631515422370285e-05}, {"id": 151, "seek": 80808, "start": 816.1600000000001, "end": 821.48, "text": " right, and so many times you'll find somebody, you know, running the query, right, and say,", "tokens": [558, 11, 293, 370, 867, 1413, 291, 603, 915, 2618, 11, 291, 458, 11, 2614, 264, 14581, 11, 558, 11, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.19529408099604587, "compression_ratio": 1.7320574162679425, "no_speech_prob": 9.631515422370285e-05}, {"id": 152, "seek": 80808, "start": 821.48, "end": 826.9200000000001, "text": " okay, well, yeah, it's slow but wherever I'll go for lunch, you know, I have time, well,", "tokens": [1392, 11, 731, 11, 1338, 11, 309, 311, 2964, 457, 8660, 286, 603, 352, 337, 6349, 11, 291, 458, 11, 286, 362, 565, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.19529408099604587, "compression_ratio": 1.7320574162679425, "no_speech_prob": 9.631515422370285e-05}, {"id": 153, "seek": 80808, "start": 826.9200000000001, "end": 835.1600000000001, "text": " you may have time but your database may not, right, so we also, like here's example how", "tokens": [291, 815, 362, 565, 457, 428, 8149, 815, 406, 11, 558, 11, 370, 321, 611, 11, 411, 510, 311, 1365, 577], "temperature": 0.0, "avg_logprob": -0.19529408099604587, "compression_ratio": 1.7320574162679425, "no_speech_prob": 9.631515422370285e-05}, {"id": 154, "seek": 83516, "start": 835.16, "end": 844.8, "text": " we provide that. I also mentioned database hosts and indexes in many instances, in many", "tokens": [321, 2893, 300, 13, 286, 611, 2835, 8149, 21573, 293, 8186, 279, 294, 867, 14519, 11, 294, 867], "temperature": 0.0, "avg_logprob": -0.13500793681425208, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.614083344582468e-05}, {"id": 155, "seek": 83516, "start": 844.8, "end": 850.0799999999999, "text": " cases that is very helpful because even if you may think, oh, my different database instance", "tokens": [3331, 300, 307, 588, 4961, 570, 754, 498, 291, 815, 519, 11, 1954, 11, 452, 819, 8149, 5197], "temperature": 0.0, "avg_logprob": -0.13500793681425208, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.614083344582468e-05}, {"id": 156, "seek": 83516, "start": 850.0799999999999, "end": 855.8, "text": " should perform the same, well, world is a messy place and world in the cloud is even", "tokens": [820, 2042, 264, 912, 11, 731, 11, 1002, 307, 257, 16191, 1081, 293, 1002, 294, 264, 4588, 307, 754], "temperature": 0.0, "avg_logprob": -0.13500793681425208, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.614083344582468e-05}, {"id": 157, "seek": 83516, "start": 855.8, "end": 862.8399999999999, "text": " messy place, right, they may not exactly have the same performance due to, you know, some", "tokens": [16191, 1081, 11, 558, 11, 436, 815, 406, 2293, 362, 264, 912, 3389, 3462, 281, 11, 291, 458, 11, 512], "temperature": 0.0, "avg_logprob": -0.13500793681425208, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.614083344582468e-05}, {"id": 158, "seek": 86284, "start": 862.84, "end": 868.0400000000001, "text": " strange configuration differently, having a bad day, right, or even maybe having a different", "tokens": [5861, 11694, 7614, 11, 1419, 257, 1578, 786, 11, 558, 11, 420, 754, 1310, 1419, 257, 819], "temperature": 0.0, "avg_logprob": -0.16150359420089033, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.746829341049306e-05}, {"id": 159, "seek": 86284, "start": 868.0400000000001, "end": 872.9200000000001, "text": " load, right, and that is a good to be able to break it down, right, when you see some", "tokens": [3677, 11, 558, 11, 293, 300, 307, 257, 665, 281, 312, 1075, 281, 1821, 309, 760, 11, 558, 11, 562, 291, 536, 512], "temperature": 0.0, "avg_logprob": -0.16150359420089033, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.746829341049306e-05}, {"id": 160, "seek": 86284, "start": 872.9200000000001, "end": 880.08, "text": " of your queries are not performing very well. I would also look at the same stuff from a", "tokens": [295, 428, 24109, 366, 406, 10205, 588, 731, 13, 286, 576, 611, 574, 412, 264, 912, 1507, 490, 257], "temperature": 0.0, "avg_logprob": -0.16150359420089033, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.746829341049306e-05}, {"id": 161, "seek": 86284, "start": 880.08, "end": 884.6800000000001, "text": " web server or application server instance because, again, if you have, like, maybe like", "tokens": [3670, 7154, 420, 3861, 7154, 5197, 570, 11, 797, 11, 498, 291, 362, 11, 411, 11, 1310, 411], "temperature": 0.0, "avg_logprob": -0.16150359420089033, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.746829341049306e-05}, {"id": 162, "seek": 86284, "start": 884.6800000000001, "end": 890.8000000000001, "text": " a hundred nodes, you deploy the same application, you may think, hey, we're all going to perform", "tokens": [257, 3262, 13891, 11, 291, 7274, 264, 912, 3861, 11, 291, 815, 519, 11, 4177, 11, 321, 434, 439, 516, 281, 2042], "temperature": 0.0, "avg_logprob": -0.16150359420089033, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.746829341049306e-05}, {"id": 163, "seek": 89080, "start": 890.8, "end": 895.16, "text": " the same, hitting the database, well, that is not always the case, right, they have seen", "tokens": [264, 912, 11, 8850, 264, 8149, 11, 731, 11, 300, 307, 406, 1009, 264, 1389, 11, 558, 11, 436, 362, 1612], "temperature": 0.0, "avg_logprob": -0.19003528831279384, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.970314749807585e-06}, {"id": 164, "seek": 89080, "start": 895.16, "end": 900.8399999999999, "text": " changes from people saying one FM is misconfigured or for some reason cannot connect the cache,", "tokens": [2962, 490, 561, 1566, 472, 29614, 307, 27631, 20646, 3831, 420, 337, 512, 1778, 2644, 1745, 264, 19459, 11], "temperature": 0.0, "avg_logprob": -0.19003528831279384, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.970314749807585e-06}, {"id": 165, "seek": 89080, "start": 900.8399999999999, "end": 904.68, "text": " so it's, you know, hitting, you know, ten times more queries, right, on the database", "tokens": [370, 309, 311, 11, 291, 458, 11, 8850, 11, 291, 458, 11, 2064, 1413, 544, 24109, 11, 558, 11, 322, 264, 8149], "temperature": 0.0, "avg_logprob": -0.19003528831279384, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.970314749807585e-06}, {"id": 166, "seek": 89080, "start": 904.68, "end": 911.7199999999999, "text": " than it should be, or the application rollout didn't go well, where UV eliminated nasty", "tokens": [813, 309, 820, 312, 11, 420, 264, 3861, 3373, 346, 994, 380, 352, 731, 11, 689, 17887, 20308, 17923], "temperature": 0.0, "avg_logprob": -0.19003528831279384, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.970314749807585e-06}, {"id": 167, "seek": 89080, "start": 911.7199999999999, "end": 917.04, "text": " query on 99 of application instance but not some others, right, it's a very good to actually", "tokens": [14581, 322, 11803, 295, 3861, 5197, 457, 406, 512, 2357, 11, 558, 11, 309, 311, 257, 588, 665, 281, 767], "temperature": 0.0, "avg_logprob": -0.19003528831279384, "compression_ratio": 1.7241379310344827, "no_speech_prob": 9.970314749807585e-06}, {"id": 168, "seek": 91704, "start": 917.04, "end": 923.28, "text": " be able to validate that because what you would see or, like, again, from a DBA standpoint,", "tokens": [312, 1075, 281, 29562, 300, 570, 437, 291, 576, 536, 420, 11, 411, 11, 797, 11, 490, 257, 413, 9295, 15827, 11], "temperature": 0.0, "avg_logprob": -0.20651737524538624, "compression_ratio": 1.7681159420289856, "no_speech_prob": 2.912491800088901e-05}, {"id": 169, "seek": 91704, "start": 923.28, "end": 928.92, "text": " you know, developers, sysadmins, storage people, they are going to tell you shit, right, but", "tokens": [291, 458, 11, 8849, 11, 262, 749, 345, 76, 1292, 11, 6725, 561, 11, 436, 366, 516, 281, 980, 291, 4611, 11, 558, 11, 457], "temperature": 0.0, "avg_logprob": -0.20651737524538624, "compression_ratio": 1.7681159420289856, "no_speech_prob": 2.912491800088901e-05}, {"id": 170, "seek": 91704, "start": 928.92, "end": 935.24, "text": " they are going to lie, right, they are going to lie, right, maybe not intentionally, maybe", "tokens": [436, 366, 516, 281, 4544, 11, 558, 11, 436, 366, 516, 281, 4544, 11, 558, 11, 1310, 406, 22062, 11, 1310], "temperature": 0.0, "avg_logprob": -0.20651737524538624, "compression_ratio": 1.7681159420289856, "no_speech_prob": 2.912491800088901e-05}, {"id": 171, "seek": 91704, "start": 935.24, "end": 940.4399999999999, "text": " because of their ignorance and limitation of their tool but as a DBA, a city or something,", "tokens": [570, 295, 641, 25390, 293, 27432, 295, 641, 2290, 457, 382, 257, 413, 9295, 11, 257, 2307, 420, 746, 11], "temperature": 0.0, "avg_logprob": -0.20651737524538624, "compression_ratio": 1.7681159420289856, "no_speech_prob": 2.912491800088901e-05}, {"id": 172, "seek": 94044, "start": 940.44, "end": 947.5600000000001, "text": " you want to point them out to their shit and say, look, I have evidence, right, evidence", "tokens": [291, 528, 281, 935, 552, 484, 281, 641, 4611, 293, 584, 11, 574, 11, 286, 362, 4467, 11, 558, 11, 4467], "temperature": 0.0, "avg_logprob": -0.1899240221296038, "compression_ratio": 1.5449438202247192, "no_speech_prob": 7.168071897467598e-05}, {"id": 173, "seek": 94044, "start": 947.5600000000001, "end": 955.5200000000001, "text": " is good, right, so clients costs, custom tags is very helpful if you can extend, that is", "tokens": [307, 665, 11, 558, 11, 370, 6982, 5497, 11, 2375, 18632, 307, 588, 4961, 498, 291, 393, 10101, 11, 300, 307], "temperature": 0.0, "avg_logprob": -0.1899240221296038, "compression_ratio": 1.5449438202247192, "no_speech_prob": 7.168071897467598e-05}, {"id": 174, "seek": 94044, "start": 955.5200000000001, "end": 965.9200000000001, "text": " what we spoke about, the SQL commenters, something else which I find very helpful which we cannot", "tokens": [437, 321, 7179, 466, 11, 264, 19200, 2871, 433, 11, 746, 1646, 597, 286, 915, 588, 4961, 597, 321, 2644], "temperature": 0.0, "avg_logprob": -0.1899240221296038, "compression_ratio": 1.5449438202247192, "no_speech_prob": 7.168071897467598e-05}, {"id": 175, "seek": 96592, "start": 965.92, "end": 972.16, "text": " quite easily get with MySQL but being able to separate the query by the query plans, right,", "tokens": [1596, 3612, 483, 365, 1222, 39934, 457, 885, 1075, 281, 4994, 264, 14581, 538, 264, 14581, 5482, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.15711462020874023, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.85878884117119e-05}, {"id": 176, "seek": 96592, "start": 972.16, "end": 977.56, "text": " often you may have a query which looks the same but it may take different execution", "tokens": [2049, 291, 815, 362, 257, 14581, 597, 1542, 264, 912, 457, 309, 815, 747, 819, 15058], "temperature": 0.0, "avg_logprob": -0.15711462020874023, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.85878884117119e-05}, {"id": 177, "seek": 96592, "start": 977.56, "end": 983.12, "text": " plans, right, and often that may be correlated to its performance.", "tokens": [5482, 11, 558, 11, 293, 2049, 300, 815, 312, 38574, 281, 1080, 3389, 13], "temperature": 0.0, "avg_logprob": -0.15711462020874023, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.85878884117119e-05}, {"id": 178, "seek": 96592, "start": 983.12, "end": 988.92, "text": " In certain cases, it is totally fine, right, very different situations, sometimes MySQL", "tokens": [682, 1629, 3331, 11, 309, 307, 3879, 2489, 11, 558, 11, 588, 819, 6851, 11, 2171, 1222, 39934], "temperature": 0.0, "avg_logprob": -0.15711462020874023, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.85878884117119e-05}, {"id": 179, "seek": 96592, "start": 988.92, "end": 995.1999999999999, "text": " optimizer may get a little bit, you know, crazy just and has that optimizer plan drift", "tokens": [5028, 6545, 815, 483, 257, 707, 857, 11, 291, 458, 11, 3219, 445, 293, 575, 300, 5028, 6545, 1393, 19699], "temperature": 0.0, "avg_logprob": -0.15711462020874023, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.85878884117119e-05}, {"id": 180, "seek": 99520, "start": 995.2, "end": 1002.6400000000001, "text": " for no good reason which may not be very easy to catch, right, and will be helpful to do.", "tokens": [337, 572, 665, 1778, 597, 815, 406, 312, 588, 1858, 281, 3745, 11, 558, 11, 293, 486, 312, 4961, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1435938648793889, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.885709444759414e-05}, {"id": 181, "seek": 99520, "start": 1002.6400000000001, "end": 1009.96, "text": " What I also would like to highlight is when you find the specific query and say, hey,", "tokens": [708, 286, 611, 576, 411, 281, 5078, 307, 562, 291, 915, 264, 2685, 14581, 293, 584, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.1435938648793889, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.885709444759414e-05}, {"id": 182, "seek": 99520, "start": 1009.96, "end": 1016.6800000000001, "text": " this query has nasty performance, right, we often want to understand where that query", "tokens": [341, 14581, 575, 17923, 3389, 11, 558, 11, 321, 2049, 528, 281, 1223, 689, 300, 14581], "temperature": 0.0, "avg_logprob": -0.1435938648793889, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.885709444759414e-05}, {"id": 183, "seek": 99520, "start": 1016.6800000000001, "end": 1022.6400000000001, "text": " response time comes from, right, and that is some of their things, right, where it can", "tokens": [4134, 565, 1487, 490, 11, 558, 11, 293, 300, 307, 512, 295, 641, 721, 11, 558, 11, 689, 309, 393], "temperature": 0.0, "avg_logprob": -0.1435938648793889, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.885709444759414e-05}, {"id": 184, "seek": 102264, "start": 1022.64, "end": 1031.76, "text": " come from, certain of them are relatively easy to find out, right, certain are not very", "tokens": [808, 490, 11, 1629, 295, 552, 366, 7226, 1858, 281, 915, 484, 11, 558, 11, 1629, 366, 406, 588], "temperature": 0.0, "avg_logprob": -0.19340804250616778, "compression_ratio": 1.7075471698113207, "no_speech_prob": 5.5639975471422076e-05}, {"id": 185, "seek": 102264, "start": 1031.76, "end": 1037.72, "text": " well, right, for example, wherever query has waited on available CPU, right, because system", "tokens": [731, 11, 558, 11, 337, 1365, 11, 8660, 14581, 575, 15240, 322, 2435, 13199, 11, 558, 11, 570, 1185], "temperature": 0.0, "avg_logprob": -0.19340804250616778, "compression_ratio": 1.7075471698113207, "no_speech_prob": 5.5639975471422076e-05}, {"id": 186, "seek": 102264, "start": 1037.72, "end": 1042.08, "text": " was already saturated, well, you can't really see on per query basics, right, you can only", "tokens": [390, 1217, 25408, 11, 731, 11, 291, 393, 380, 534, 536, 322, 680, 14581, 14688, 11, 558, 11, 291, 393, 787], "temperature": 0.0, "avg_logprob": -0.19340804250616778, "compression_ratio": 1.7075471698113207, "no_speech_prob": 5.5639975471422076e-05}, {"id": 187, "seek": 102264, "start": 1042.08, "end": 1047.6, "text": " see those things, well, my CPU was kind of like a super packed, right, on a period of", "tokens": [536, 729, 721, 11, 731, 11, 452, 13199, 390, 733, 295, 411, 257, 1687, 13265, 11, 558, 11, 322, 257, 2896, 295], "temperature": 0.0, "avg_logprob": -0.19340804250616778, "compression_ratio": 1.7075471698113207, "no_speech_prob": 5.5639975471422076e-05}, {"id": 188, "seek": 102264, "start": 1047.6, "end": 1048.6, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.19340804250616778, "compression_ratio": 1.7075471698113207, "no_speech_prob": 5.5639975471422076e-05}, {"id": 189, "seek": 104860, "start": 1048.6, "end": 1055.1599999999999, "text": " Okay, here are a couple of other things to consider when you're looking at the queries.", "tokens": [1033, 11, 510, 366, 257, 1916, 295, 661, 721, 281, 1949, 562, 291, 434, 1237, 412, 264, 24109, 13], "temperature": 0.0, "avg_logprob": -0.15549784766303168, "compression_ratio": 1.6289592760180995, "no_speech_prob": 6.593616853933781e-05}, {"id": 190, "seek": 104860, "start": 1055.1599999999999, "end": 1060.4399999999998, "text": " One, you want to really look at separately the bad queries, right, versus victims, because", "tokens": [1485, 11, 291, 528, 281, 534, 574, 412, 14759, 264, 1578, 24109, 11, 558, 11, 5717, 11448, 11, 570], "temperature": 0.0, "avg_logprob": -0.15549784766303168, "compression_ratio": 1.6289592760180995, "no_speech_prob": 6.593616853933781e-05}, {"id": 191, "seek": 104860, "start": 1060.4399999999998, "end": 1065.4399999999998, "text": " sometimes you will see, oh, queries are getting slower, but it's not because of FAM, it's", "tokens": [2171, 291, 486, 536, 11, 1954, 11, 24109, 366, 1242, 14009, 11, 457, 309, 311, 406, 570, 295, 479, 2865, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.15549784766303168, "compression_ratio": 1.6289592760180995, "no_speech_prob": 6.593616853933781e-05}, {"id": 192, "seek": 104860, "start": 1065.4399999999998, "end": 1071.48, "text": " about some other nasty queries, right, maybe that is your Java developer who thought, well,", "tokens": [466, 512, 661, 17923, 24109, 11, 558, 11, 1310, 300, 307, 428, 10745, 10754, 567, 1194, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.15549784766303168, "compression_ratio": 1.6289592760180995, "no_speech_prob": 6.593616853933781e-05}, {"id": 193, "seek": 107148, "start": 1071.48, "end": 1081.28, "text": " you know, to solve my problems, I will just launch with 200 threads, right, and make sure", "tokens": [291, 458, 11, 281, 5039, 452, 2740, 11, 286, 486, 445, 4025, 365, 2331, 19314, 11, 558, 11, 293, 652, 988], "temperature": 0.0, "avg_logprob": -0.18239691171301417, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.00013958899944555014}, {"id": 194, "seek": 107148, "start": 1081.28, "end": 1089.56, "text": " I am good, but everything else is kind of slowed down, right, and that's maybe tricky.", "tokens": [286, 669, 665, 11, 457, 1203, 1646, 307, 733, 295, 32057, 760, 11, 558, 11, 293, 300, 311, 1310, 12414, 13], "temperature": 0.0, "avg_logprob": -0.18239691171301417, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.00013958899944555014}, {"id": 195, "seek": 107148, "start": 1089.56, "end": 1093.24, "text": " One thing is what you should not forget the currently running queries.", "tokens": [1485, 551, 307, 437, 291, 820, 406, 2870, 264, 4362, 2614, 24109, 13], "temperature": 0.0, "avg_logprob": -0.18239691171301417, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.00013958899944555014}, {"id": 196, "seek": 107148, "start": 1093.24, "end": 1097.6, "text": " In many cases, like if you look in performance schema queries by dash address, that gives", "tokens": [682, 867, 3331, 11, 411, 498, 291, 574, 294, 3389, 34078, 24109, 538, 8240, 2985, 11, 300, 2709], "temperature": 0.0, "avg_logprob": -0.18239691171301417, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.00013958899944555014}, {"id": 197, "seek": 109760, "start": 1097.6, "end": 1102.7199999999998, "text": " you what happened in the past, but believe me, if you start, you know, 50 instances", "tokens": [291, 437, 2011, 294, 264, 1791, 11, 457, 1697, 385, 11, 498, 291, 722, 11, 291, 458, 11, 2625, 14519], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 198, "seek": 109760, "start": 1102.7199999999998, "end": 1108.9199999999998, "text": " of some very bad query, which continues to run, well, that may be the reason of your", "tokens": [295, 512, 588, 1578, 14581, 11, 597, 6515, 281, 1190, 11, 731, 11, 300, 815, 312, 264, 1778, 295, 428], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 199, "seek": 109760, "start": 1108.9199999999998, "end": 1113.6399999999999, "text": " problem, not the past, right, and to connect to that, I think it is less problem in my", "tokens": [1154, 11, 406, 264, 1791, 11, 558, 11, 293, 281, 1745, 281, 300, 11, 286, 519, 309, 307, 1570, 1154, 294, 452], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 200, "seek": 109760, "start": 1113.6399999999999, "end": 1118.76, "text": " skill right now, right, if you're using query timeouts, which is a very good practice, right,", "tokens": [5389, 558, 586, 11, 558, 11, 498, 291, 434, 1228, 14581, 565, 7711, 11, 597, 307, 257, 588, 665, 3124, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 201, "seek": 109760, "start": 1118.76, "end": 1122.08, "text": " because if you say, hey, you know what, for all my interactive queries, by default, I", "tokens": [570, 498, 291, 584, 11, 4177, 11, 291, 458, 437, 11, 337, 439, 452, 15141, 24109, 11, 538, 7576, 11, 286], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 202, "seek": 109760, "start": 1122.08, "end": 1127.12, "text": " set the timeout of, let's say, 15 seconds, then you should not care too much about your", "tokens": [992, 264, 565, 346, 295, 11, 718, 311, 584, 11, 2119, 3949, 11, 550, 291, 820, 406, 1127, 886, 709, 466, 428], "temperature": 0.0, "avg_logprob": -0.15704385999222875, "compression_ratio": 1.7728813559322034, "no_speech_prob": 6.12783624092117e-05}, {"id": 203, "seek": 112712, "start": 1127.12, "end": 1133.9599999999998, "text": " past queries because, well, you know what, everything gets killed after 15 minutes.", "tokens": [1791, 24109, 570, 11, 731, 11, 291, 458, 437, 11, 1203, 2170, 4652, 934, 2119, 2077, 13], "temperature": 0.0, "avg_logprob": -0.20873850654153264, "compression_ratio": 1.5758928571428572, "no_speech_prob": 5.625223275274038e-05}, {"id": 204, "seek": 112712, "start": 1133.9599999999998, "end": 1138.32, "text": " Also, 50 seconds, right, you should not ignore the stuff which is invisible from a query", "tokens": [2743, 11, 2625, 3949, 11, 558, 11, 291, 820, 406, 11200, 264, 1507, 597, 307, 14603, 490, 257, 14581], "temperature": 0.0, "avg_logprob": -0.20873850654153264, "compression_ratio": 1.5758928571428572, "no_speech_prob": 5.625223275274038e-05}, {"id": 205, "seek": 112712, "start": 1138.32, "end": 1146.12, "text": " standpoint, right, databases do a lot of shit in the background, you may also do things", "tokens": [15827, 11, 558, 11, 22380, 360, 257, 688, 295, 4611, 294, 264, 3678, 11, 291, 815, 611, 360, 721], "temperature": 0.0, "avg_logprob": -0.20873850654153264, "compression_ratio": 1.5758928571428572, "no_speech_prob": 5.625223275274038e-05}, {"id": 206, "seek": 112712, "start": 1146.12, "end": 1153.36, "text": " or your operation teams like, well, backups or provisioning another node for cloning, right,", "tokens": [420, 428, 6916, 5491, 411, 11, 731, 11, 50160, 420, 17225, 278, 1071, 9984, 337, 596, 16638, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.20873850654153264, "compression_ratio": 1.5758928571428572, "no_speech_prob": 5.625223275274038e-05}, {"id": 207, "seek": 115336, "start": 1153.36, "end": 1158.8799999999999, "text": " for the clouds or wherever your VM system may need to do something in the background,", "tokens": [337, 264, 12193, 420, 8660, 428, 18038, 1185, 815, 643, 281, 360, 746, 294, 264, 3678, 11], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 208, "seek": 115336, "start": 1158.8799999999999, "end": 1163.56, "text": " it may not be directly visible, but that can impact the query performance, right, so sometimes,", "tokens": [309, 815, 406, 312, 3838, 8974, 11, 457, 300, 393, 2712, 264, 14581, 3389, 11, 558, 11, 370, 2171, 11], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 209, "seek": 115336, "start": 1163.56, "end": 1169.8799999999999, "text": " well, when you observe a query impact and you can't really see what is causing that,", "tokens": [731, 11, 562, 291, 11441, 257, 14581, 2712, 293, 291, 393, 380, 534, 536, 437, 307, 9853, 300, 11], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 210, "seek": 115336, "start": 1169.8799999999999, "end": 1171.4799999999998, "text": " it's possible.", "tokens": [309, 311, 1944, 13], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 211, "seek": 115336, "start": 1171.4799999999998, "end": 1176.76, "text": " I also would encourage to avoid what I would call like a biased something.", "tokens": [286, 611, 576, 5373, 281, 5042, 437, 286, 576, 818, 411, 257, 28035, 746, 13], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 212, "seek": 115336, "start": 1176.76, "end": 1180.6399999999999, "text": " I see people sometimes would say, hey, you know what, we will set long query time to", "tokens": [286, 536, 561, 2171, 576, 584, 11, 4177, 11, 291, 458, 437, 11, 321, 486, 992, 938, 14581, 565, 281], "temperature": 0.0, "avg_logprob": -0.16560220280918506, "compression_ratio": 1.683206106870229, "no_speech_prob": 3.238489807699807e-05}, {"id": 213, "seek": 118064, "start": 1180.64, "end": 1185.0400000000002, "text": " one second and only look at the queries which are more than one second in length, well,", "tokens": [472, 1150, 293, 787, 574, 412, 264, 24109, 597, 366, 544, 813, 472, 1150, 294, 4641, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.18724084829355214, "compression_ratio": 1.7486910994764397, "no_speech_prob": 0.00010883256618399173}, {"id": 214, "seek": 118064, "start": 1185.0400000000002, "end": 1190.8400000000001, "text": " you may be only focusing on the outliers, right, and missing the possibility to optimize", "tokens": [291, 815, 312, 787, 8416, 322, 264, 484, 23646, 11, 558, 11, 293, 5361, 264, 7959, 281, 19719], "temperature": 0.0, "avg_logprob": -0.18724084829355214, "compression_ratio": 1.7486910994764397, "no_speech_prob": 0.00010883256618399173}, {"id": 215, "seek": 118064, "start": 1190.8400000000001, "end": 1199.0800000000002, "text": " other queries, right, or actually even focusing on the queries which provide, which are responsible", "tokens": [661, 24109, 11, 558, 11, 420, 767, 754, 8416, 322, 264, 24109, 597, 2893, 11, 597, 366, 6250], "temperature": 0.0, "avg_logprob": -0.18724084829355214, "compression_ratio": 1.7486910994764397, "no_speech_prob": 0.00010883256618399173}, {"id": 216, "seek": 118064, "start": 1199.0800000000002, "end": 1204.24, "text": " for providing that bad experience, right, for your users.", "tokens": [337, 6530, 300, 1578, 1752, 11, 558, 11, 337, 428, 5022, 13], "temperature": 0.0, "avg_logprob": -0.18724084829355214, "compression_ratio": 1.7486910994764397, "no_speech_prob": 0.00010883256618399173}, {"id": 217, "seek": 120424, "start": 1204.24, "end": 1211.84, "text": " Okay, we find another thing like a last minute I have or something, I wanted to say, hey,", "tokens": [1033, 11, 321, 915, 1071, 551, 411, 257, 1036, 3456, 286, 362, 420, 746, 11, 286, 1415, 281, 584, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.28076444670211437, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.00022219252423383296}, {"id": 218, "seek": 120424, "start": 1211.84, "end": 1218.72, "text": " what I would like to see from my skill to do better, who is Kenny, no Kenny?", "tokens": [437, 286, 576, 411, 281, 536, 490, 452, 5389, 281, 360, 1101, 11, 567, 307, 33681, 11, 572, 33681, 30], "temperature": 0.0, "avg_logprob": -0.28076444670211437, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.00022219252423383296}, {"id": 219, "seek": 120424, "start": 1218.72, "end": 1226.24, "text": " Yes, he's always hiding, right, he probably wanted to get another sandwich, damn it.", "tokens": [1079, 11, 415, 311, 1009, 10596, 11, 558, 11, 415, 1391, 1415, 281, 483, 1071, 11141, 11, 8151, 309, 13], "temperature": 0.0, "avg_logprob": -0.28076444670211437, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.00022219252423383296}, {"id": 220, "seek": 120424, "start": 1226.24, "end": 1230.1200000000001, "text": " Okay, so here are some things that I would like to see.", "tokens": [1033, 11, 370, 510, 366, 512, 721, 300, 286, 576, 411, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.28076444670211437, "compression_ratio": 1.6157894736842104, "no_speech_prob": 0.00022219252423383296}, {"id": 221, "seek": 123012, "start": 1230.12, "end": 1237.6, "text": " One is better support of prepared statements, right, and right now it's kind of, you know,", "tokens": [1485, 307, 1101, 1406, 295, 4927, 12363, 11, 558, 11, 293, 558, 586, 309, 311, 733, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2146553939647889, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00015549098316114396}, {"id": 222, "seek": 123012, "start": 1237.6, "end": 1245.32, "text": " not done in the same way, right, which is, I think, is a problem, right.", "tokens": [406, 1096, 294, 264, 912, 636, 11, 558, 11, 597, 307, 11, 286, 519, 11, 307, 257, 1154, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2146553939647889, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00015549098316114396}, {"id": 223, "seek": 123012, "start": 1245.32, "end": 1250.2399999999998, "text": " Now I would say consider grouping data by time in certain cases, right now you get like", "tokens": [823, 286, 576, 584, 1949, 40149, 1412, 538, 565, 294, 1629, 3331, 11, 558, 586, 291, 483, 411], "temperature": 0.0, "avg_logprob": -0.2146553939647889, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00015549098316114396}, {"id": 224, "seek": 123012, "start": 1250.2399999999998, "end": 1256.04, "text": " all the statements in one table, right, and you have a lot of statement variety, that", "tokens": [439, 264, 12363, 294, 472, 3199, 11, 558, 11, 293, 291, 362, 257, 688, 295, 5629, 5673, 11, 300], "temperature": 0.0, "avg_logprob": -0.2146553939647889, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00015549098316114396}, {"id": 225, "seek": 125604, "start": 1256.04, "end": 1262.3999999999999, "text": " table tends to overflow, right, which is not really helpful, right, and if you have to", "tokens": [3199, 12258, 281, 37772, 11, 558, 11, 597, 307, 406, 534, 4961, 11, 558, 11, 293, 498, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.13182936112085977, "compression_ratio": 1.7548076923076923, "no_speech_prob": 8.681607869220898e-05}, {"id": 226, "seek": 125604, "start": 1262.3999999999999, "end": 1270.8799999999999, "text": " kind of reset your queries all the time, that is not very, you know, good practice in my", "tokens": [733, 295, 14322, 428, 24109, 439, 264, 565, 11, 300, 307, 406, 588, 11, 291, 458, 11, 665, 3124, 294, 452], "temperature": 0.0, "avg_logprob": -0.13182936112085977, "compression_ratio": 1.7548076923076923, "no_speech_prob": 8.681607869220898e-05}, {"id": 227, "seek": 125604, "start": 1270.8799999999999, "end": 1272.48, "text": " opinion.", "tokens": [4800, 13], "temperature": 0.0, "avg_logprob": -0.13182936112085977, "compression_ratio": 1.7548076923076923, "no_speech_prob": 8.681607869220898e-05}, {"id": 228, "seek": 125604, "start": 1272.48, "end": 1279.3999999999999, "text": " Provide list of tables query touches, right, that is very helpful because, well, my skill", "tokens": [15685, 482, 1329, 295, 8020, 14581, 17431, 11, 558, 11, 300, 307, 588, 4961, 570, 11, 731, 11, 452, 5389], "temperature": 0.0, "avg_logprob": -0.13182936112085977, "compression_ratio": 1.7548076923076923, "no_speech_prob": 8.681607869220898e-05}, {"id": 229, "seek": 125604, "start": 1279.3999999999999, "end": 1284.96, "text": " parser already knows it, right, it knows tables query touches, but it's very hard to parse", "tokens": [21156, 260, 1217, 3255, 309, 11, 558, 11, 309, 3255, 8020, 14581, 17431, 11, 457, 309, 311, 588, 1152, 281, 48377], "temperature": 0.0, "avg_logprob": -0.13182936112085977, "compression_ratio": 1.7548076923076923, "no_speech_prob": 8.681607869220898e-05}, {"id": 230, "seek": 128496, "start": 1284.96, "end": 1290.16, "text": " it out from a query, especially if you consider views, right.", "tokens": [309, 484, 490, 257, 14581, 11, 2318, 498, 291, 1949, 6809, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 231, "seek": 128496, "start": 1290.16, "end": 1295.64, "text": " I don't know by looking at the query alone, wherever something is a table or a view, right,", "tokens": [286, 500, 380, 458, 538, 1237, 412, 264, 14581, 3312, 11, 8660, 746, 307, 257, 3199, 420, 257, 1910, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 232, "seek": 128496, "start": 1295.64, "end": 1297.4, "text": " so, in this case.", "tokens": [370, 11, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 233, "seek": 128496, "start": 1297.4, "end": 1303.04, "text": " Information about plan ID, right, I would like to see for the query, right, some sort", "tokens": [15357, 466, 1393, 7348, 11, 558, 11, 286, 576, 411, 281, 536, 337, 264, 14581, 11, 558, 11, 512, 1333], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 234, "seek": 128496, "start": 1303.04, "end": 1309.72, "text": " of plan hash or something, so I know then query is using something like that, and also", "tokens": [295, 1393, 22019, 420, 746, 11, 370, 286, 458, 550, 14581, 307, 1228, 746, 411, 300, 11, 293, 611], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 235, "seek": 128496, "start": 1309.72, "end": 1312.48, "text": " what I would call like a top weight summary, right.", "tokens": [437, 286, 576, 818, 411, 257, 1192, 3364, 12691, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.15091309591988536, "compression_ratio": 1.736842105263158, "no_speech_prob": 4.95183739985805e-05}, {"id": 236, "seek": 131248, "start": 1312.48, "end": 1318.08, "text": " Right now we have information about the weights in my skill performance query and about query,", "tokens": [1779, 586, 321, 362, 1589, 466, 264, 17443, 294, 452, 5389, 3389, 14581, 293, 466, 14581, 11], "temperature": 0.0, "avg_logprob": -0.1793806812550762, "compression_ratio": 1.65, "no_speech_prob": 6.57256314298138e-05}, {"id": 237, "seek": 131248, "start": 1318.08, "end": 1325.52, "text": " but I cannot see and say, oh, that query was slow because it spent XYZ amount of weight", "tokens": [457, 286, 2644, 536, 293, 584, 11, 1954, 11, 300, 14581, 390, 2964, 570, 309, 4418, 48826, 57, 2372, 295, 3364], "temperature": 0.0, "avg_logprob": -0.1793806812550762, "compression_ratio": 1.65, "no_speech_prob": 6.57256314298138e-05}, {"id": 238, "seek": 131248, "start": 1325.52, "end": 1331.32, "text": " on something or whatever, right, or at least kind of like some small class of queries,", "tokens": [322, 746, 420, 2035, 11, 558, 11, 420, 412, 1935, 733, 295, 411, 512, 1359, 1508, 295, 24109, 11], "temperature": 0.0, "avg_logprob": -0.1793806812550762, "compression_ratio": 1.65, "no_speech_prob": 6.57256314298138e-05}, {"id": 239, "seek": 131248, "start": 1331.32, "end": 1334.04, "text": " right, I don't think that's convenient.", "tokens": [558, 11, 286, 500, 380, 519, 300, 311, 10851, 13], "temperature": 0.0, "avg_logprob": -0.1793806812550762, "compression_ratio": 1.65, "no_speech_prob": 6.57256314298138e-05}, {"id": 240, "seek": 131248, "start": 1334.04, "end": 1342.16, "text": " Well, with that, that's all I had to say, hope that will help you to avoid tuning your", "tokens": [1042, 11, 365, 300, 11, 300, 311, 439, 286, 632, 281, 584, 11, 1454, 300, 486, 854, 291, 281, 5042, 15164, 428], "temperature": 0.0, "avg_logprob": -0.1793806812550762, "compression_ratio": 1.65, "no_speech_prob": 6.57256314298138e-05}, {"id": 241, "seek": 134216, "start": 1342.16, "end": 1352.5600000000002, "text": " indexes by, by the credit card, and yes, oh, I have a time for questions, you told me", "tokens": [8186, 279, 538, 11, 538, 264, 5397, 2920, 11, 293, 2086, 11, 1954, 11, 286, 362, 257, 565, 337, 1651, 11, 291, 1907, 385], "temperature": 0.0, "avg_logprob": -0.3803614331530286, "compression_ratio": 1.5847953216374269, "no_speech_prob": 0.00013752611994277686}, {"id": 242, "seek": 134216, "start": 1352.5600000000002, "end": 1359.76, "text": " like, Peter, five minutes, oh, to answer, I have a time for questions, yes, any questions,", "tokens": [411, 11, 6508, 11, 1732, 2077, 11, 1954, 11, 281, 1867, 11, 286, 362, 257, 565, 337, 1651, 11, 2086, 11, 604, 1651, 11], "temperature": 0.0, "avg_logprob": -0.3803614331530286, "compression_ratio": 1.5847953216374269, "no_speech_prob": 0.00013752611994277686}, {"id": 243, "seek": 134216, "start": 1359.76, "end": 1369.0800000000002, "text": " no, oh, yeah, what's the difference or advantages of this SQL commenter thing compared to what", "tokens": [572, 11, 1954, 11, 1338, 11, 437, 311, 264, 2649, 420, 14906, 295, 341, 19200, 2871, 260, 551, 5347, 281, 437], "temperature": 0.0, "avg_logprob": -0.3803614331530286, "compression_ratio": 1.5847953216374269, "no_speech_prob": 0.00013752611994277686}, {"id": 244, "seek": 136908, "start": 1369.08, "end": 1374.12, "text": " open tracing standards people start tracing the whole thing, what's the difference of", "tokens": [1269, 25262, 7787, 561, 722, 25262, 264, 1379, 551, 11, 437, 311, 264, 2649, 295], "temperature": 0.0, "avg_logprob": -0.36505126953125, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.000904663116671145}, {"id": 245, "seek": 136908, "start": 1374.12, "end": 1375.12, "text": " SQL commenter?", "tokens": [19200, 2871, 260, 30], "temperature": 0.0, "avg_logprob": -0.36505126953125, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.000904663116671145}, {"id": 246, "seek": 136908, "start": 1375.12, "end": 1381.32, "text": " Well, what I would say in this case, yes, I mean, there is obviously open tracing framework,", "tokens": [1042, 11, 437, 286, 576, 584, 294, 341, 1389, 11, 2086, 11, 286, 914, 11, 456, 307, 2745, 1269, 25262, 8388, 11], "temperature": 0.0, "avg_logprob": -0.36505126953125, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.000904663116671145}, {"id": 247, "seek": 136908, "start": 1381.32, "end": 1392.76, "text": " right, which you can use, this gets specifically to the database and specifically in every query,", "tokens": [558, 11, 597, 291, 393, 764, 11, 341, 2170, 4682, 281, 264, 8149, 293, 4682, 294, 633, 14581, 11], "temperature": 0.0, "avg_logprob": -0.36505126953125, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.000904663116671145}, {"id": 248, "seek": 139276, "start": 1392.76, "end": 1400.28, "text": " right, if you look at the open tracing framework, I think, you know, getting every query, right,", "tokens": [558, 11, 498, 291, 574, 412, 264, 1269, 25262, 8388, 11, 286, 519, 11, 291, 458, 11, 1242, 633, 14581, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.2197771177187071, "compression_ratio": 1.67, "no_speech_prob": 4.52889762527775e-05}, {"id": 249, "seek": 139276, "start": 1400.28, "end": 1408.48, "text": " maybe a lot of, a lot of volume out there, right, and again, I also think, well, the", "tokens": [1310, 257, 688, 295, 11, 257, 688, 295, 5523, 484, 456, 11, 558, 11, 293, 797, 11, 286, 611, 519, 11, 731, 11, 264], "temperature": 0.0, "avg_logprob": -0.2197771177187071, "compression_ratio": 1.67, "no_speech_prob": 4.52889762527775e-05}, {"id": 250, "seek": 139276, "start": 1408.48, "end": 1414.04, "text": " good thing if also SQL commenter, right, is what that does it automatically, if you will,", "tokens": [665, 551, 498, 611, 19200, 2871, 260, 11, 558, 11, 307, 437, 300, 775, 309, 6772, 11, 498, 291, 486, 11], "temperature": 0.0, "avg_logprob": -0.2197771177187071, "compression_ratio": 1.67, "no_speech_prob": 4.52889762527775e-05}, {"id": 251, "seek": 139276, "start": 1414.04, "end": 1419.28, "text": " right, that does not require you to take an extra integration.", "tokens": [558, 11, 300, 775, 406, 3651, 291, 281, 747, 364, 2857, 10980, 13], "temperature": 0.0, "avg_logprob": -0.2197771177187071, "compression_ratio": 1.67, "no_speech_prob": 4.52889762527775e-05}, {"id": 252, "seek": 141928, "start": 1419.28, "end": 1449.08, "text": " Okay, anybody else, yeah, I mean, it works with MariaDB as well, yes, well, there are", "tokens": [1033, 11, 4472, 1646, 11, 1338, 11, 286, 914, 11, 309, 1985, 365, 12734, 27735, 382, 731, 11, 2086, 11, 731, 11, 456, 366], "temperature": 0.0, "avg_logprob": -0.3808901309967041, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.000936258933506906}, {"id": 253, "seek": 144908, "start": 1449.08, "end": 1457.1599999999999, "text": " not practices, there are no good practices, right, like you can, there is a lot of optimizer", "tokens": [406, 7525, 11, 456, 366, 572, 665, 7525, 11, 558, 11, 411, 291, 393, 11, 456, 307, 257, 688, 295, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.22102600819355733, "compression_ratio": 1.6497175141242937, "no_speech_prob": 2.245012547064107e-05}, {"id": 254, "seek": 144908, "start": 1457.1599999999999, "end": 1461.6, "text": " hints you can use, right, so you can actually force the query to go like this particular", "tokens": [27271, 291, 393, 764, 11, 558, 11, 370, 291, 393, 767, 3464, 264, 14581, 281, 352, 411, 341, 1729], "temperature": 0.0, "avg_logprob": -0.22102600819355733, "compression_ratio": 1.6497175141242937, "no_speech_prob": 2.245012547064107e-05}, {"id": 255, "seek": 144908, "start": 1461.6, "end": 1467.8799999999999, "text": " stuff, right, but that also prevents optimizer choosing different plan if better plan becomes", "tokens": [1507, 11, 558, 11, 457, 300, 611, 22367, 5028, 6545, 10875, 819, 1393, 498, 1101, 1393, 3643], "temperature": 0.0, "avg_logprob": -0.22102600819355733, "compression_ratio": 1.6497175141242937, "no_speech_prob": 2.245012547064107e-05}, {"id": 256, "seek": 144908, "start": 1467.8799999999999, "end": 1468.8799999999999, "text": " available.", "tokens": [2435, 13], "temperature": 0.0, "avg_logprob": -0.22102600819355733, "compression_ratio": 1.6497175141242937, "no_speech_prob": 2.245012547064107e-05}, {"id": 257, "seek": 144908, "start": 1468.8799999999999, "end": 1469.8799999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22102600819355733, "compression_ratio": 1.6497175141242937, "no_speech_prob": 2.245012547064107e-05}, {"id": 258, "seek": 146988, "start": 1469.88, "end": 1486.3600000000001, "text": " Never use forced index, always use ignore index, okay, well, then thank you, folks.", "tokens": [50364, 7344, 764, 7579, 8186, 11, 1009, 764, 11200, 8186, 11, 1392, 11, 731, 11, 550, 1309, 291, 11, 4024, 13, 51188], "temperature": 0.0, "avg_logprob": -0.5214441548223081, "compression_ratio": 1.0506329113924051, "no_speech_prob": 0.0035167192108929157}], "language": "en"}