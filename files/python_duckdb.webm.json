{"text": " Let's welcome Pedro Holanda for his talk on DuckDB and a magnificent snake duck. Yeah, you guys can be surprised at anything you can find as a rubber duck these days, you know. All right, so I'm Pedro Holanda. I am one of the main contributors of the DuckDB projects, which is an open source database system, and also I'm the COO of DuckDB Labs. And today I'm going to be talking a little bit about how DuckDB can bring analytical SQL power directly into your Python show. So to give you a little bit of an idea of how the stocks look like, I'm going to start with what is DuckDB. So I'm here talking about one more database system. I want to motivate you guys that we actually needed to do one more database system. The other ones didn't solve the problems we had. And then I'm going to go over the main characteristics of DuckDB, so what actually makes it special. Then I'm going to go over DuckDB in the Python land, so how DuckDB integrates in the Python ecosystem. I'm going to do a little demo. The basic idea is that we're going to use the infamous New York City taxi data sets, and we're going to try to do some estimation of fair costs, and we're going to use DuckDB partners and PySpark just to see a couple of the differences of the things I'm going to be talking over. And then some summary of the talk. So what is DuckDB? Well, DuckDB was actually born at CWI, which is the research center of mathematics and computer science in the Netherlands. And what we actually had there is that a lot of the projects, the PhD student projects, the master projects, they are very data sciencey. So usually you have a data science problem, and you want to throw a database measurement system at the data science problem because you're handling data. So initially we were like, OK, we can probably use a database server, use a database connection, and then just transfer the data from the relational database to your Python terminal, for example, like where your analytical tools are. And it turns out that's quite a bad idea, because you are transferring a lot of data. So that's pretty costly. And then you're like, OK, this is really not solving our problem, can we draw inspiration from somewhere else? And then, of course, there are SQLites, the most famous database out there, at least the most used one. And it has quite a nice property, which is being an embedded database system. Being an embedded database system, it means it can run inside your Python process. So you can eliminate this data transfer cost. SQLite comes with one design decision that is a transactional database, so it's actually super optimized for small updates, but it's not really optimized for analytics. So we kind of wanted to do SQLites in terms of being easy to use and eliminating this data transfer cost, but focusing on analytical queries. And that's kind of how the database was born, and that's also why we frame it as a SQLite for analytics. It also has a very simple installation. So if you think about Python, you just do a bit so, and you're good. This is embedded, there's no server management. So let's say you just want to, I don't know, query a pre-kit file, two lines of code you can write, query it. Like, there's no starting of server, there's no schema creation, the schema is inferred from the object, so it's very easy, very fast. And we also really focus on this fast transfer between analytical languages and their tools, like in Python and R, to DuckDB. DuckDB is currently in pre-release, I think the last version we released was 0.6, 0.7 is coming up soon. I need the web pages like a little bit more details about all the things that are in each release. All right, so I'm going to go over some of the main characteristics of DuckDB, particularly like the color data storage, a little bit about compression. I'm going to talk about vectorized execution, so these are all like core database stuff. Actually talking about vectorized execution engine, it's going to be difficult because Professor Bonk is here and he actually created that, so I'll try to do it correctly. A little bit about end-to-end core optimization, parallelism and beyond-memory execution. So color data storage, well, there's basically two ways that you can do it. One is a raw store, a scone store, as an example of raw store, we have SQLites, and the whole thing about the whole idea is they're storing your data consecutively in memory per row. So that basically means that if you want to fetch an individual row, that's very cheap because it's continuous in memory, however, you always have to fetch all the columns. So analytical queries, usually you have very white tables, but you just want to really get a couple of these columns. So what if you only want to use a field? So in this example, what if you just are interested in the price of a product, but not the stores as sold, right? In a column store, you actually have your layout that the data of the column is consecutively in memory. So if you want to access just a couple columns, you can actually have immense savings on this KIO and memory bandwidth. So that's why this type of format is really optimized for analytics. So to give you a more concrete example, let's say that we have a one terabyte table with 100 columns. For simplicity, let's say all the columns have the same size, and we just require five columns of the table in our analytical query. So in a raw store, let's SQLites, reading this whole table, if you have a disk with around 100 megabytes per second, it will take you three hours. If you were using a column store model, which is what Pandas inductively does, for example, using these five columns from disk, it takes you eight minutes. So there's a huge improvement by just setting up the correct storage formats for your workload. Compression. Well, I'm not going to go into a lot of detail about the compression algorithms that we implement in Duck2B, but what I can tell you is because of having a column store, you're going to have your data from your column continuously in memory, which gives you a very good advantage to compressing it, because usually the data from the same column is somewhat similar. So you can apply cool things like RLE, FSST and CHIMP for floating point numbers, FSST for strings. So you can start applying all these algorithms and really decrease the size of your data. So in this table here, we actually have, I think this is from one year ago, one year and a half. 0.2.8 from Duck2B. We had no compression at that point, and then a year and a half later, we actually managed to implement all these things, which got us five times better compression, Y19, for example, 3.18 better compression in the taxi data set that I'm going to be using later. And why is compression so important? Well, if we go back again to the same example, where we were reading our five columns, and it was costing us to read them from disk eight minutes because of the storage formats, if we compress these columns, we suddenly don't have to read 50 gigabytes anymore, right? You read less. And then of course, you apply like the best case from what I showed you from the last table, five times, there are increases that cost you one point, one minute and 40 seconds. So execution, well, there's three ways of doing a query execution. There's actually one more, but it's not in the slides. So our SQLites use the top-of-the-time processing, which means that you process one row at a time. Pandas uses column-of-the-time processing, which means that you process one column at a time. And DuckDB uses kind of like a mix of the both, which is a technique developed by Peter, the vectorized processing where you process batches of a column at a time. So basically, the top-of-the-time model from SQLites, it was optimized for a time where computers didn't have a lot of memory. There was low memory to be used because you only need to really keep one row in memory throughout your whole query plan. So the memory was expensive, that's what it could do, but this comes with a high CPU overhead per tuple because you're constantly resetting your caches, you don't have any cache-conscious algorithm running that piece of data up to the production of your query results. If you go to the column-of-the-time, which is what Pandas uses, this already brings like better CPU utilization, it allows for SIMD. But it comes with the cost of materializing large intermediates in memory. It basically means that you need the whole column in memory at that point to process for that operator. And of course, the intermediates can be gigabytes each, so that's pretty problematic when the data sizes are large. And that's why you see, for example, that Pandas, if your data doesn't fit in memory, what does it happen? It crashes. And then if you go to the vectorized query processing, it's actually optimized for CPU cache locality, you can do SIMD instructions, pipelining, and the whole idea is that your intermediates are actually going to fit here in L1 cache. So basically you're going to be paying this latency of one nanosecond to be accessing your data throughout your query plan instead of paying the latency of a main memory, which is also the case of a column database, which is 100 nanoseconds. It seems like a small difference, but when you're constantly executing this, this really becomes a bottleneck. And to end-score optimizations, of course, something that we have inducted to be, so we have stuff like expression rewriting, join ordering, subquery flattening, filtering, projection pushdown, which is a bit more simple, but it's extremely important and brings a huge difference in the cost of the query. So here's an example of a projection pushdown. Say you have a table with five columns, A, B, C, D, E, and you want to run a query, that's pretty small, but the query is like a selects minimum from column A, where there's a filtering column A saying the column A is bigger than zero and you group by B. So the whole point of this query is that you're only using two columns of the table, right? So what the ductdb optimizer will do is like, okay, in this scanner, I know I don't need all the columns, I just need N and B, and you just don't have to read the other ones. If you do the same one in pandas, for example, you can apply your filter, and then you have the filter, the group by the aggregator, but at the time you're doing this filter, you're still filtering all the other columns you're not going to be using your query. Of course, you can manually make this optimization, but it's pretty nice that the database system can do that for you. Of course, the ductdb also has automatic parallelism and beyond-memory execution, so ductdb has parallel versions of most of its operators, I think all of our scanners, including with insertion order preservation of parallelize now, aggregations, joins, pandas, for example, only supports single-threaded execution. We all have pretty good laptops these days, right? So it's a shame if you cannot really take advantage of parallelism. And ductdb, again, supports the execution of data that does not fit in memory. It's kind of the never give up, never surrender approach, it's like, we're going to execute this query. We try to always have graceful degradation, also, that it just doesn't suddenly crash in performance. And the whole goal is really to never crash and always execute the query. All right, so a little bit about ductdb in the Python lens. Basically we have an API, it's a dbapi to.ocompliant, so, far too much what SQLite has, for example, you can create a connection and can start executing queries, but we also wanted to have something similar to the data frame API that still could, people that can't come from pandas, for example, could still have something familiar to work on. So here in this example, you can also create a connection. You can create this relation, which kind of looks like a data frame, you just point it to a table, you can do a show to inspect what the table is inside, and you can apply, for example, these chaining operators, right, like a filter, a projection. So in the end, this is all lazily executed, and this also allows you to take advantage of the optimizer of ductdb, even if you do the chaining operations. Of course, I talked to you about memory transfer, so we were very careful as well into being very integrated with this, very common libraries in Python. So with pandas and pyarrow, for example, what we actually do is that in the end, for pandas, the columns are usually not pyarrows, which turns out to be RC vectors, which turns out that's also kind of what we use. So with a little bit of makeup in the metadata, we can just directly read them, and they're all in the same process, right? So we have access to that piece of memory, which in the end means that you can actually access the data from pandas in ductdb without paying any transfer costs, at least constant transfer costs just for doing the metadata makeup, let's say. And there's the same thing with pyarrow. We also have what we call zero copy, so we can read error objects and output error objects without any extra costs. With NumPy, we also support SQLCAMY, and in IBIS, they're actually the default back-end from them, I think, since six months ago. A little bit of usage, so as you can see, this is our PyPy download counts. The Python library is actually our most downloaded API. We have APIs for all sorts of languages, and you can see that in the last month, we had like 900,000 downloads, so there are a lot of people there trying out and using ductdb in their Python scripts. So now it's the demo time, let me get this, all right, this looks like you can see. So this is just installing ductdb PySpark and getting our yellow trip data dataset, our executor, this, our database, just importing the stuff we're going to be using, and here is just like getting a connection from ductdb, creating a relation that's just, okay, we're going to, as a parquet file, ductdb can be parquet files, and then you can just print to inspect what's out there, right, so if we run this, we can see like, okay, these are the columns we have, we have vendor ID, we have pick up dates, time, passenger counts, you have the types of the columns, you can also have a little result preview, just have an idea of what it looks like, so I think this dataset has about like 20 columns, maybe, and there's just information about the taxi rides in New York in 2016, and then you can also, for example, run a simple query here, I'm just doing like accounts to know how many tuples are there, and we have about 10 million tuples on this dataset. All right, so this function here is just to do a little bit of benchmarking, coming from academia, we do have to do something that's kind of fair, I guess, so I run just five times and take the median time of everything, and then this is actually where then we start, so we start off with data frame, so Pundas can also read parquet files, and the whole thing about ductdb again is that it's not here as a replacement for Pundas, this is not run by itself, but something that can work together with Pundas, so the cool thing is that we can, again, read and output data frames without any extra cost, so let's say that in the query here, we're just getting the passenger counts, then the average tip amount of trips that had a short distance, right, and we group by passengers, by the number of passengers, so what we want to know is for short trips, does the amount of tip matters by the number of passengers in that ride, and what you can see here is that you can, again, read from the data frame, that's what we're doing, and we just have to use the data frame name in our SQL query, and if you call.df from the query results, you also output in your data frame, and it's pretty cool because the data frames have these plots bars, they have plotting capabilities that ductdb doesn't have, and you can get easily a very nice chart, so you see here, apparently, there's some dirty data because before getting in tips, when they don't have anyone in their rides, I'm not sure what that is, but apparently like if you have more people, seven to nine, maybe like the more expensive cars, you get a higher tip, and you can do the same thing in pandas, of course, right, like in pandas you don't have SQL, you're going to have to do, to use their own language, to do the group by, the average, and you can directly use the plots, and the whole point here is to show the different execution time, like, now we're waiting, okay, so took a second, and ductdb took 0.2, so this is like a 5x, right, to 0.25, so like 4x, and you also have to consider that we're using like a, not a very beefy machine, right, this is a co-lib machine, imagine if you had more cars, this difference would also be bigger, and then I added spark for fun, so actually spark can also read data frames, but it crashes out of memory in my co-lib machine, so I had to give up on this, and read directly from par K files, but it does output it as a data frame, I think we're going to have to wait a little bit, but as me it's best, so of course spark is not designed for small data sets, but turns out there are a lot of use cases where you use these small data sets, as you're going, it's warming up a little bit, it's good for the winter, it produces some energy, I think, alright, okay, so it took two seconds, 2.2 seconds, the actual execution, and that's already like, what, more than two times what Pandas was, so, yeah, anyway, for the demo of course, I showed you something that's fairly simple, can you do actually very complicated things, maybe not very complicated, but more complicated, so here I'm not really going to go over the query, but the whole idea is that we can just, for example, use ductDB to run linear regression, so can we predict, can we estimate the fare with the trip distance, and turns out you can just calculate the alpha and beta with not such a crazy query, and then you can again export it to Pandas, and you have a very nice figure there, so you can really combine these two to get the best out of both, alright, that was the demo, summary, oh that's my last slide, good, so yeah, ductDB is an embedded database system, again it's completely open source, it's under the MIT license, since it came from academia, this is something that we're always worried about, it's to also give it back to everyone, because it was usually funded by taxpayers money, so everyone can use it, 100% of what we do is actually open source, there's nothing that's closed source, it's designed for analytical queries, so data analysis, data science, has binding for many languages, so of course I'm at the Python dev room, I'm talking about Python, but we have our Java, turns out that Java is like one of our most downloaded APIs, so I guess that's an interesting sign, Java scripts, and a bunch of others, it has very tight integrations with the Python ecosystem, again the whole idea is that you eliminate transfer costs, implements the database in relation to APIs, the relation to API again is this more data frame like, and has full SQL support, so anything you can imagine like window functions or what not, you can just express them using duck to be, and that's it, thank you very much for paying attention, happy to answer questions. Thank you Petron, so we have five minutes for your questions. You mentioned, thanks for the great presentation, you mentioned beyond memory execution, and kind of that it tries not to degrade as much, can you shine a little bit more light on kind of what happens under the hood, and how much degradation happens? Of course, I think that's, there's only the ordering operator that actually does that, we have Lawrence that's doing his PhD, so there's a lot of operators that need to research to be developed, that's more of a goal than something that actually happens now, but the whole goal is that you really don't have this sudden spike in the future, but there's research going on, in the future there will be more to be shared for sure. Thank you very much for the talk, and it's very exciting to see such a tool, such a powerful tool, I'm working usually with data warehouses, and I saw on the website that you do not recommend using this with data warehouses, I would like to know why. So of course, there's no one solution for our problems, there are cases that the warehouses are very good fits, it turns out that for data science for example, which is kind of what we preach the most, they're usually not good because then you fall back to the senior data outside your database system, like you're not really going to be running your Python codes inside the system, you can do that for UDS for example, but they are messy, they're a bit nasty, so you want really to have it embedded in your Python process, so you completely eliminate data transfer costs, because usually what you do is like, okay, I have a table, 10 columns, I'm going over 4 columns, but I'm really reading huge chunks of it, so that's a bottleneck we try to eliminate. How do you handle updates? Although we are in the analytical database system, we do do updates, so Mark, I don't know where he is, but he's there, he developed MVCC algorithm for OLAP, so we have the same asset transactional capabilities that you would expect from a transactional database, of course, if you have a transactional workload, you should still go for Postgrease or SQLize or a database that handles this type of transactions, but Mark developed like a full-on algorithm to handle updates completely, yeah. How do you compare to Vertica? How do you compare to Vertica? I have a good question, I think in terms of analytical queries, TPCH, probably similar performance, but then again, the whole point is that if you go again for the Python process, the data transfer costs will take most of the time there, and then it's really catered for this type of scenario, the embedded scenario. We have one minute left for one more question. Yeah, I actually have a rappel somewhere for a bunch of examples as well, I'm very happy to share it. I don't know where I'll post it. Ah, the false then thing, I guess. All right. All right. Thank you a lot, Pedro. Thanks a lot. Thank you very much. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.48, "text": " Let's welcome Pedro Holanda for his talk on DuckDB and a magnificent snake duck.", "tokens": [961, 311, 2928, 26662, 11086, 5575, 337, 702, 751, 322, 29266, 27735, 293, 257, 23690, 12650, 12482, 13], "temperature": 0.0, "avg_logprob": -0.2807515462239583, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.1594683974981308}, {"id": 1, "seek": 0, "start": 13.48, "end": 19.72, "text": " Yeah, you guys can be surprised at anything you can find as a rubber duck these days,", "tokens": [865, 11, 291, 1074, 393, 312, 6100, 412, 1340, 291, 393, 915, 382, 257, 11593, 12482, 613, 1708, 11], "temperature": 0.0, "avg_logprob": -0.2807515462239583, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.1594683974981308}, {"id": 2, "seek": 0, "start": 19.72, "end": 20.72, "text": " you know.", "tokens": [291, 458, 13], "temperature": 0.0, "avg_logprob": -0.2807515462239583, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.1594683974981308}, {"id": 3, "seek": 0, "start": 20.72, "end": 22.92, "text": " All right, so I'm Pedro Holanda.", "tokens": [1057, 558, 11, 370, 286, 478, 26662, 11086, 5575, 13], "temperature": 0.0, "avg_logprob": -0.2807515462239583, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.1594683974981308}, {"id": 4, "seek": 0, "start": 22.92, "end": 28.080000000000002, "text": " I am one of the main contributors of the DuckDB projects, which is an open source database", "tokens": [286, 669, 472, 295, 264, 2135, 45627, 295, 264, 29266, 27735, 4455, 11, 597, 307, 364, 1269, 4009, 8149], "temperature": 0.0, "avg_logprob": -0.2807515462239583, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.1594683974981308}, {"id": 5, "seek": 2808, "start": 28.08, "end": 32.68, "text": " system, and also I'm the COO of DuckDB Labs.", "tokens": [1185, 11, 293, 611, 286, 478, 264, 3002, 46, 295, 29266, 27735, 40047, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 6, "seek": 2808, "start": 32.68, "end": 36.64, "text": " And today I'm going to be talking a little bit about how DuckDB can bring analytical", "tokens": [400, 965, 286, 478, 516, 281, 312, 1417, 257, 707, 857, 466, 577, 29266, 27735, 393, 1565, 29579], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 7, "seek": 2808, "start": 36.64, "end": 40.48, "text": " SQL power directly into your Python show.", "tokens": [19200, 1347, 3838, 666, 428, 15329, 855, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 8, "seek": 2808, "start": 40.48, "end": 44.92, "text": " So to give you a little bit of an idea of how the stocks look like, I'm going to start", "tokens": [407, 281, 976, 291, 257, 707, 857, 295, 364, 1558, 295, 577, 264, 12966, 574, 411, 11, 286, 478, 516, 281, 722], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 9, "seek": 2808, "start": 44.92, "end": 46.64, "text": " with what is DuckDB.", "tokens": [365, 437, 307, 29266, 27735, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 10, "seek": 2808, "start": 46.64, "end": 50.12, "text": " So I'm here talking about one more database system.", "tokens": [407, 286, 478, 510, 1417, 466, 472, 544, 8149, 1185, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 11, "seek": 2808, "start": 50.12, "end": 54.36, "text": " I want to motivate you guys that we actually needed to do one more database system.", "tokens": [286, 528, 281, 28497, 291, 1074, 300, 321, 767, 2978, 281, 360, 472, 544, 8149, 1185, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 12, "seek": 2808, "start": 54.36, "end": 57.56, "text": " The other ones didn't solve the problems we had.", "tokens": [440, 661, 2306, 994, 380, 5039, 264, 2740, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.15768283081054688, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00019907619571313262}, {"id": 13, "seek": 5756, "start": 57.56, "end": 63.760000000000005, "text": " And then I'm going to go over the main characteristics of DuckDB, so what actually makes it special.", "tokens": [400, 550, 286, 478, 516, 281, 352, 670, 264, 2135, 10891, 295, 29266, 27735, 11, 370, 437, 767, 1669, 309, 2121, 13], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 14, "seek": 5756, "start": 63.760000000000005, "end": 68.24000000000001, "text": " Then I'm going to go over DuckDB in the Python land, so how DuckDB integrates in the Python", "tokens": [1396, 286, 478, 516, 281, 352, 670, 29266, 27735, 294, 264, 15329, 2117, 11, 370, 577, 29266, 27735, 3572, 1024, 294, 264, 15329], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 15, "seek": 5756, "start": 68.24000000000001, "end": 69.24000000000001, "text": " ecosystem.", "tokens": [11311, 13], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 16, "seek": 5756, "start": 69.24000000000001, "end": 72.76, "text": " I'm going to do a little demo.", "tokens": [286, 478, 516, 281, 360, 257, 707, 10723, 13], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 17, "seek": 5756, "start": 72.76, "end": 78.16, "text": " The basic idea is that we're going to use the infamous New York City taxi data sets,", "tokens": [440, 3875, 1558, 307, 300, 321, 434, 516, 281, 764, 264, 30769, 1873, 3609, 4392, 18984, 1412, 6352, 11], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 18, "seek": 5756, "start": 78.16, "end": 84.36, "text": " and we're going to try to do some estimation of fair costs, and we're going to use DuckDB", "tokens": [293, 321, 434, 516, 281, 853, 281, 360, 512, 35701, 295, 3143, 5497, 11, 293, 321, 434, 516, 281, 764, 29266, 27735], "temperature": 0.0, "avg_logprob": -0.17682403701919694, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.0338887124089524e-05}, {"id": 19, "seek": 8436, "start": 84.36, "end": 88.36, "text": " partners and PySpark just to see a couple of the differences of the things I'm going", "tokens": [4462, 293, 9953, 14014, 809, 445, 281, 536, 257, 1916, 295, 264, 7300, 295, 264, 721, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 20, "seek": 8436, "start": 88.36, "end": 90.12, "text": " to be talking over.", "tokens": [281, 312, 1417, 670, 13], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 21, "seek": 8436, "start": 90.12, "end": 92.44, "text": " And then some summary of the talk.", "tokens": [400, 550, 512, 12691, 295, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 22, "seek": 8436, "start": 92.44, "end": 93.44, "text": " So what is DuckDB?", "tokens": [407, 437, 307, 29266, 27735, 30], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 23, "seek": 8436, "start": 93.44, "end": 99.28, "text": " Well, DuckDB was actually born at CWI, which is the research center of mathematics and", "tokens": [1042, 11, 29266, 27735, 390, 767, 4232, 412, 383, 54, 40, 11, 597, 307, 264, 2132, 3056, 295, 18666, 293], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 24, "seek": 8436, "start": 99.28, "end": 101.76, "text": " computer science in the Netherlands.", "tokens": [3820, 3497, 294, 264, 20873, 13], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 25, "seek": 8436, "start": 101.76, "end": 105.76, "text": " And what we actually had there is that a lot of the projects, the PhD student projects,", "tokens": [400, 437, 321, 767, 632, 456, 307, 300, 257, 688, 295, 264, 4455, 11, 264, 14476, 3107, 4455, 11], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 26, "seek": 8436, "start": 105.76, "end": 108.84, "text": " the master projects, they are very data sciencey.", "tokens": [264, 4505, 4455, 11, 436, 366, 588, 1412, 3497, 88, 13], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 27, "seek": 8436, "start": 108.84, "end": 112.4, "text": " So usually you have a data science problem, and you want to throw a database measurement", "tokens": [407, 2673, 291, 362, 257, 1412, 3497, 1154, 11, 293, 291, 528, 281, 3507, 257, 8149, 13160], "temperature": 0.0, "avg_logprob": -0.1943928522008066, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.00010030217526946217}, {"id": 28, "seek": 11240, "start": 112.4, "end": 116.16000000000001, "text": " system at the data science problem because you're handling data.", "tokens": [1185, 412, 264, 1412, 3497, 1154, 570, 291, 434, 13175, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 29, "seek": 11240, "start": 116.16000000000001, "end": 122.08000000000001, "text": " So initially we were like, OK, we can probably use a database server, use a database connection,", "tokens": [407, 9105, 321, 645, 411, 11, 2264, 11, 321, 393, 1391, 764, 257, 8149, 7154, 11, 764, 257, 8149, 4984, 11], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 30, "seek": 11240, "start": 122.08000000000001, "end": 127.80000000000001, "text": " and then just transfer the data from the relational database to your Python terminal, for example,", "tokens": [293, 550, 445, 5003, 264, 1412, 490, 264, 38444, 8149, 281, 428, 15329, 14709, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 31, "seek": 11240, "start": 127.80000000000001, "end": 130.36, "text": " like where your analytical tools are.", "tokens": [411, 689, 428, 29579, 3873, 366, 13], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 32, "seek": 11240, "start": 130.36, "end": 134.92000000000002, "text": " And it turns out that's quite a bad idea, because you are transferring a lot of data.", "tokens": [400, 309, 4523, 484, 300, 311, 1596, 257, 1578, 1558, 11, 570, 291, 366, 31437, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 33, "seek": 11240, "start": 134.92000000000002, "end": 136.88, "text": " So that's pretty costly.", "tokens": [407, 300, 311, 1238, 28328, 13], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 34, "seek": 11240, "start": 136.88, "end": 140.76, "text": " And then you're like, OK, this is really not solving our problem, can we draw inspiration", "tokens": [400, 550, 291, 434, 411, 11, 2264, 11, 341, 307, 534, 406, 12606, 527, 1154, 11, 393, 321, 2642, 10249], "temperature": 0.0, "avg_logprob": -0.15368062655131023, "compression_ratio": 1.7885304659498207, "no_speech_prob": 3.9989081415114924e-05}, {"id": 35, "seek": 14076, "start": 140.76, "end": 142.2, "text": " from somewhere else?", "tokens": [490, 4079, 1646, 30], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 36, "seek": 14076, "start": 142.2, "end": 145.79999999999998, "text": " And then, of course, there are SQLites, the most famous database out there, at least the", "tokens": [400, 550, 11, 295, 1164, 11, 456, 366, 19200, 3324, 11, 264, 881, 4618, 8149, 484, 456, 11, 412, 1935, 264], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 37, "seek": 14076, "start": 145.79999999999998, "end": 148.16, "text": " most used one.", "tokens": [881, 1143, 472, 13], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 38, "seek": 14076, "start": 148.16, "end": 153.6, "text": " And it has quite a nice property, which is being an embedded database system.", "tokens": [400, 309, 575, 1596, 257, 1481, 4707, 11, 597, 307, 885, 364, 16741, 8149, 1185, 13], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 39, "seek": 14076, "start": 153.6, "end": 157.35999999999999, "text": " Being an embedded database system, it means it can run inside your Python process.", "tokens": [8891, 364, 16741, 8149, 1185, 11, 309, 1355, 309, 393, 1190, 1854, 428, 15329, 1399, 13], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 40, "seek": 14076, "start": 157.35999999999999, "end": 161.68, "text": " So you can eliminate this data transfer cost.", "tokens": [407, 291, 393, 13819, 341, 1412, 5003, 2063, 13], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 41, "seek": 14076, "start": 161.68, "end": 166.88, "text": " SQLite comes with one design decision that is a transactional database, so it's actually", "tokens": [19200, 642, 1487, 365, 472, 1715, 3537, 300, 307, 257, 46688, 1966, 8149, 11, 370, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.19349831801194411, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.412908987840638e-05}, {"id": 42, "seek": 16688, "start": 166.88, "end": 172.72, "text": " super optimized for small updates, but it's not really optimized for analytics.", "tokens": [1687, 26941, 337, 1359, 9205, 11, 457, 309, 311, 406, 534, 26941, 337, 15370, 13], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 43, "seek": 16688, "start": 172.72, "end": 177.79999999999998, "text": " So we kind of wanted to do SQLites in terms of being easy to use and eliminating this", "tokens": [407, 321, 733, 295, 1415, 281, 360, 19200, 3324, 294, 2115, 295, 885, 1858, 281, 764, 293, 31203, 341], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 44, "seek": 16688, "start": 177.79999999999998, "end": 183.16, "text": " data transfer cost, but focusing on analytical queries.", "tokens": [1412, 5003, 2063, 11, 457, 8416, 322, 29579, 24109, 13], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 45, "seek": 16688, "start": 183.16, "end": 187.48, "text": " And that's kind of how the database was born, and that's also why we frame it as a SQLite", "tokens": [400, 300, 311, 733, 295, 577, 264, 8149, 390, 4232, 11, 293, 300, 311, 611, 983, 321, 3920, 309, 382, 257, 19200, 642], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 46, "seek": 16688, "start": 187.48, "end": 189.56, "text": " for analytics.", "tokens": [337, 15370, 13], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 47, "seek": 16688, "start": 189.56, "end": 191.24, "text": " It also has a very simple installation.", "tokens": [467, 611, 575, 257, 588, 2199, 13260, 13], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 48, "seek": 16688, "start": 191.24, "end": 194.76, "text": " So if you think about Python, you just do a bit so, and you're good.", "tokens": [407, 498, 291, 519, 466, 15329, 11, 291, 445, 360, 257, 857, 370, 11, 293, 291, 434, 665, 13], "temperature": 0.0, "avg_logprob": -0.19051727362438642, "compression_ratio": 1.6795366795366795, "no_speech_prob": 1.5919726138236e-05}, {"id": 49, "seek": 19476, "start": 194.76, "end": 196.88, "text": " This is embedded, there's no server management.", "tokens": [639, 307, 16741, 11, 456, 311, 572, 7154, 4592, 13], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 50, "seek": 19476, "start": 196.88, "end": 201.12, "text": " So let's say you just want to, I don't know, query a pre-kit file, two lines of code you", "tokens": [407, 718, 311, 584, 291, 445, 528, 281, 11, 286, 500, 380, 458, 11, 14581, 257, 659, 12, 22681, 3991, 11, 732, 3876, 295, 3089, 291], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 51, "seek": 19476, "start": 201.12, "end": 202.12, "text": " can write, query it.", "tokens": [393, 2464, 11, 14581, 309, 13], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 52, "seek": 19476, "start": 202.12, "end": 208.12, "text": " Like, there's no starting of server, there's no schema creation, the schema is inferred", "tokens": [1743, 11, 456, 311, 572, 2891, 295, 7154, 11, 456, 311, 572, 34078, 8016, 11, 264, 34078, 307, 13596, 986], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 53, "seek": 19476, "start": 208.12, "end": 211.23999999999998, "text": " from the object, so it's very easy, very fast.", "tokens": [490, 264, 2657, 11, 370, 309, 311, 588, 1858, 11, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 54, "seek": 19476, "start": 211.23999999999998, "end": 216.32, "text": " And we also really focus on this fast transfer between analytical languages and their tools,", "tokens": [400, 321, 611, 534, 1879, 322, 341, 2370, 5003, 1296, 29579, 8650, 293, 641, 3873, 11], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 55, "seek": 19476, "start": 216.32, "end": 218.84, "text": " like in Python and R, to DuckDB.", "tokens": [411, 294, 15329, 293, 497, 11, 281, 29266, 27735, 13], "temperature": 0.0, "avg_logprob": -0.2529990041357839, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.00014326923701446503}, {"id": 56, "seek": 21884, "start": 218.84, "end": 225.6, "text": " DuckDB is currently in pre-release, I think the last version we released was 0.6, 0.7", "tokens": [29266, 27735, 307, 4362, 294, 659, 12, 265, 1122, 11, 286, 519, 264, 1036, 3037, 321, 4736, 390, 1958, 13, 21, 11, 1958, 13, 22], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 57, "seek": 21884, "start": 225.6, "end": 226.6, "text": " is coming up soon.", "tokens": [307, 1348, 493, 2321, 13], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 58, "seek": 21884, "start": 226.6, "end": 232.04, "text": " I need the web pages like a little bit more details about all the things that are in each", "tokens": [286, 643, 264, 3670, 7183, 411, 257, 707, 857, 544, 4365, 466, 439, 264, 721, 300, 366, 294, 1184], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 59, "seek": 21884, "start": 232.04, "end": 233.04, "text": " release.", "tokens": [4374, 13], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 60, "seek": 21884, "start": 233.04, "end": 239.2, "text": " All right, so I'm going to go over some of the main characteristics of DuckDB, particularly", "tokens": [1057, 558, 11, 370, 286, 478, 516, 281, 352, 670, 512, 295, 264, 2135, 10891, 295, 29266, 27735, 11, 4098], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 61, "seek": 21884, "start": 239.2, "end": 243.4, "text": " like the color data storage, a little bit about compression.", "tokens": [411, 264, 2017, 1412, 6725, 11, 257, 707, 857, 466, 19355, 13], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 62, "seek": 21884, "start": 243.4, "end": 247.72, "text": " I'm going to talk about vectorized execution, so these are all like core database stuff.", "tokens": [286, 478, 516, 281, 751, 466, 8062, 1602, 15058, 11, 370, 613, 366, 439, 411, 4965, 8149, 1507, 13], "temperature": 0.0, "avg_logprob": -0.23593129950054623, "compression_ratio": 1.654275092936803, "no_speech_prob": 6.382822175510228e-05}, {"id": 63, "seek": 24772, "start": 247.72, "end": 252.84, "text": " Actually talking about vectorized execution engine, it's going to be difficult because", "tokens": [5135, 1417, 466, 8062, 1602, 15058, 2848, 11, 309, 311, 516, 281, 312, 2252, 570], "temperature": 0.0, "avg_logprob": -0.24074452263968332, "compression_ratio": 1.5787545787545787, "no_speech_prob": 0.00016926713578868657}, {"id": 64, "seek": 24772, "start": 252.84, "end": 258.72, "text": " Professor Bonk is here and he actually created that, so I'll try to do it correctly.", "tokens": [8419, 7368, 74, 307, 510, 293, 415, 767, 2942, 300, 11, 370, 286, 603, 853, 281, 360, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.24074452263968332, "compression_ratio": 1.5787545787545787, "no_speech_prob": 0.00016926713578868657}, {"id": 65, "seek": 24772, "start": 258.72, "end": 265.16, "text": " A little bit about end-to-end core optimization, parallelism and beyond-memory execution.", "tokens": [316, 707, 857, 466, 917, 12, 1353, 12, 521, 4965, 19618, 11, 8952, 1434, 293, 4399, 12, 17886, 827, 15058, 13], "temperature": 0.0, "avg_logprob": -0.24074452263968332, "compression_ratio": 1.5787545787545787, "no_speech_prob": 0.00016926713578868657}, {"id": 66, "seek": 24772, "start": 265.16, "end": 270.0, "text": " So color data storage, well, there's basically two ways that you can do it.", "tokens": [407, 2017, 1412, 6725, 11, 731, 11, 456, 311, 1936, 732, 2098, 300, 291, 393, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.24074452263968332, "compression_ratio": 1.5787545787545787, "no_speech_prob": 0.00016926713578868657}, {"id": 67, "seek": 24772, "start": 270.0, "end": 275.16, "text": " One is a raw store, a scone store, as an example of raw store, we have SQLites, and the whole", "tokens": [1485, 307, 257, 8936, 3531, 11, 257, 795, 546, 3531, 11, 382, 364, 1365, 295, 8936, 3531, 11, 321, 362, 19200, 3324, 11, 293, 264, 1379], "temperature": 0.0, "avg_logprob": -0.24074452263968332, "compression_ratio": 1.5787545787545787, "no_speech_prob": 0.00016926713578868657}, {"id": 68, "seek": 27516, "start": 275.16, "end": 282.92, "text": " thing about the whole idea is they're storing your data consecutively in memory per row.", "tokens": [551, 466, 264, 1379, 1558, 307, 436, 434, 26085, 428, 1412, 27154, 3413, 294, 4675, 680, 5386, 13], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 69, "seek": 27516, "start": 282.92, "end": 286.6, "text": " So that basically means that if you want to fetch an individual row, that's very cheap", "tokens": [407, 300, 1936, 1355, 300, 498, 291, 528, 281, 23673, 364, 2609, 5386, 11, 300, 311, 588, 7084], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 70, "seek": 27516, "start": 286.6, "end": 292.20000000000005, "text": " because it's continuous in memory, however, you always have to fetch all the columns.", "tokens": [570, 309, 311, 10957, 294, 4675, 11, 4461, 11, 291, 1009, 362, 281, 23673, 439, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 71, "seek": 27516, "start": 292.20000000000005, "end": 297.36, "text": " So analytical queries, usually you have very white tables, but you just want to really", "tokens": [407, 29579, 24109, 11, 2673, 291, 362, 588, 2418, 8020, 11, 457, 291, 445, 528, 281, 534], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 72, "seek": 27516, "start": 297.36, "end": 300.24, "text": " get a couple of these columns.", "tokens": [483, 257, 1916, 295, 613, 13766, 13], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 73, "seek": 27516, "start": 300.24, "end": 302.04, "text": " So what if you only want to use a field?", "tokens": [407, 437, 498, 291, 787, 528, 281, 764, 257, 2519, 30], "temperature": 0.0, "avg_logprob": -0.18113002962279087, "compression_ratio": 1.728395061728395, "no_speech_prob": 4.250181882525794e-05}, {"id": 74, "seek": 30204, "start": 302.04, "end": 308.16, "text": " So in this example, what if you just are interested in the price of a product, but not the stores", "tokens": [407, 294, 341, 1365, 11, 437, 498, 291, 445, 366, 3102, 294, 264, 3218, 295, 257, 1674, 11, 457, 406, 264, 9512], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 75, "seek": 30204, "start": 308.16, "end": 311.48, "text": " as sold, right?", "tokens": [382, 3718, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 76, "seek": 30204, "start": 311.48, "end": 319.24, "text": " In a column store, you actually have your layout that the data of the column is consecutively", "tokens": [682, 257, 7738, 3531, 11, 291, 767, 362, 428, 13333, 300, 264, 1412, 295, 264, 7738, 307, 27154, 3413], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 77, "seek": 30204, "start": 319.24, "end": 320.24, "text": " in memory.", "tokens": [294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 78, "seek": 30204, "start": 320.24, "end": 324.40000000000003, "text": " So if you want to access just a couple columns, you can actually have immense savings on this", "tokens": [407, 498, 291, 528, 281, 2105, 445, 257, 1916, 13766, 11, 291, 393, 767, 362, 22920, 13454, 322, 341], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 79, "seek": 30204, "start": 324.40000000000003, "end": 327.28000000000003, "text": " KIO and memory bandwidth.", "tokens": [591, 15167, 293, 4675, 23647, 13], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 80, "seek": 30204, "start": 327.28000000000003, "end": 331.36, "text": " So that's why this type of format is really optimized for analytics.", "tokens": [407, 300, 311, 983, 341, 2010, 295, 7877, 307, 534, 26941, 337, 15370, 13], "temperature": 0.0, "avg_logprob": -0.16403095538799578, "compression_ratio": 1.6748971193415638, "no_speech_prob": 2.169729668821674e-05}, {"id": 81, "seek": 33136, "start": 331.36, "end": 335.88, "text": " So to give you a more concrete example, let's say that we have a one terabyte table with", "tokens": [407, 281, 976, 291, 257, 544, 9859, 1365, 11, 718, 311, 584, 300, 321, 362, 257, 472, 1796, 34529, 3199, 365], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 82, "seek": 33136, "start": 335.88, "end": 338.04, "text": " 100 columns.", "tokens": [2319, 13766, 13], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 83, "seek": 33136, "start": 338.04, "end": 341.88, "text": " For simplicity, let's say all the columns have the same size, and we just require five", "tokens": [1171, 25632, 11, 718, 311, 584, 439, 264, 13766, 362, 264, 912, 2744, 11, 293, 321, 445, 3651, 1732], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 84, "seek": 33136, "start": 341.88, "end": 344.56, "text": " columns of the table in our analytical query.", "tokens": [13766, 295, 264, 3199, 294, 527, 29579, 14581, 13], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 85, "seek": 33136, "start": 344.56, "end": 349.52000000000004, "text": " So in a raw store, let's SQLites, reading this whole table, if you have a disk with", "tokens": [407, 294, 257, 8936, 3531, 11, 718, 311, 19200, 3324, 11, 3760, 341, 1379, 3199, 11, 498, 291, 362, 257, 12355, 365], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 86, "seek": 33136, "start": 349.52000000000004, "end": 352.6, "text": " around 100 megabytes per second, it will take you three hours.", "tokens": [926, 2319, 10816, 24538, 680, 1150, 11, 309, 486, 747, 291, 1045, 2496, 13], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 87, "seek": 33136, "start": 352.6, "end": 357.16, "text": " If you were using a column store model, which is what Pandas inductively does, for example,", "tokens": [759, 291, 645, 1228, 257, 7738, 3531, 2316, 11, 597, 307, 437, 16995, 296, 31612, 3413, 775, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.20269447326660156, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.8855955204344355e-05}, {"id": 88, "seek": 35716, "start": 357.16, "end": 361.52000000000004, "text": " using these five columns from disk, it takes you eight minutes.", "tokens": [1228, 613, 1732, 13766, 490, 12355, 11, 309, 2516, 291, 3180, 2077, 13], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 89, "seek": 35716, "start": 361.52000000000004, "end": 369.40000000000003, "text": " So there's a huge improvement by just setting up the correct storage formats for your workload.", "tokens": [407, 456, 311, 257, 2603, 10444, 538, 445, 3287, 493, 264, 3006, 6725, 25879, 337, 428, 20139, 13], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 90, "seek": 35716, "start": 369.40000000000003, "end": 370.40000000000003, "text": " Compression.", "tokens": [6620, 2775, 13], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 91, "seek": 35716, "start": 370.40000000000003, "end": 373.68, "text": " Well, I'm not going to go into a lot of detail about the compression algorithms that we implement", "tokens": [1042, 11, 286, 478, 406, 516, 281, 352, 666, 257, 688, 295, 2607, 466, 264, 19355, 14642, 300, 321, 4445], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 92, "seek": 35716, "start": 373.68, "end": 379.76000000000005, "text": " in Duck2B, but what I can tell you is because of having a column store, you're going to", "tokens": [294, 29266, 17, 33, 11, 457, 437, 286, 393, 980, 291, 307, 570, 295, 1419, 257, 7738, 3531, 11, 291, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 93, "seek": 35716, "start": 379.76000000000005, "end": 385.88, "text": " have your data from your column continuously in memory, which gives you a very good advantage", "tokens": [362, 428, 1412, 490, 428, 7738, 15684, 294, 4675, 11, 597, 2709, 291, 257, 588, 665, 5002], "temperature": 0.0, "avg_logprob": -0.2206125436005769, "compression_ratio": 1.6258992805755397, "no_speech_prob": 6.483556353487074e-05}, {"id": 94, "seek": 38588, "start": 385.88, "end": 391.12, "text": " to compressing it, because usually the data from the same column is somewhat similar.", "tokens": [281, 14778, 278, 309, 11, 570, 2673, 264, 1412, 490, 264, 912, 7738, 307, 8344, 2531, 13], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 95, "seek": 38588, "start": 391.12, "end": 400.96, "text": " So you can apply cool things like RLE, FSST and CHIMP for floating point numbers, FSST", "tokens": [407, 291, 393, 3079, 1627, 721, 411, 497, 2634, 11, 41138, 6840, 293, 5995, 6324, 47, 337, 12607, 935, 3547, 11, 41138, 6840], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 96, "seek": 38588, "start": 400.96, "end": 401.96, "text": " for strings.", "tokens": [337, 13985, 13], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 97, "seek": 38588, "start": 401.96, "end": 405.76, "text": " So you can start applying all these algorithms and really decrease the size of your data.", "tokens": [407, 291, 393, 722, 9275, 439, 613, 14642, 293, 534, 11514, 264, 2744, 295, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 98, "seek": 38588, "start": 405.76, "end": 411.48, "text": " So in this table here, we actually have, I think this is from one year ago, one year", "tokens": [407, 294, 341, 3199, 510, 11, 321, 767, 362, 11, 286, 519, 341, 307, 490, 472, 1064, 2057, 11, 472, 1064], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 99, "seek": 38588, "start": 411.48, "end": 413.48, "text": " and a half.", "tokens": [293, 257, 1922, 13], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 100, "seek": 38588, "start": 413.48, "end": 415.52, "text": " 0.2.8 from Duck2B.", "tokens": [1958, 13, 17, 13, 23, 490, 29266, 17, 33, 13], "temperature": 0.0, "avg_logprob": -0.21660521223738388, "compression_ratio": 1.570281124497992, "no_speech_prob": 2.974605195049662e-05}, {"id": 101, "seek": 41552, "start": 415.52, "end": 419.79999999999995, "text": " We had no compression at that point, and then a year and a half later, we actually managed", "tokens": [492, 632, 572, 19355, 412, 300, 935, 11, 293, 550, 257, 1064, 293, 257, 1922, 1780, 11, 321, 767, 6453], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 102, "seek": 41552, "start": 419.79999999999995, "end": 425.71999999999997, "text": " to implement all these things, which got us five times better compression, Y19, for example,", "tokens": [281, 4445, 439, 613, 721, 11, 597, 658, 505, 1732, 1413, 1101, 19355, 11, 398, 3405, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 103, "seek": 41552, "start": 425.71999999999997, "end": 432.59999999999997, "text": " 3.18 better compression in the taxi data set that I'm going to be using later.", "tokens": [805, 13, 6494, 1101, 19355, 294, 264, 18984, 1412, 992, 300, 286, 478, 516, 281, 312, 1228, 1780, 13], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 104, "seek": 41552, "start": 432.59999999999997, "end": 434.0, "text": " And why is compression so important?", "tokens": [400, 983, 307, 19355, 370, 1021, 30], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 105, "seek": 41552, "start": 434.0, "end": 438.96, "text": " Well, if we go back again to the same example, where we were reading our five columns, and", "tokens": [1042, 11, 498, 321, 352, 646, 797, 281, 264, 912, 1365, 11, 689, 321, 645, 3760, 527, 1732, 13766, 11, 293], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 106, "seek": 41552, "start": 438.96, "end": 443.84, "text": " it was costing us to read them from disk eight minutes because of the storage formats, if", "tokens": [309, 390, 37917, 505, 281, 1401, 552, 490, 12355, 3180, 2077, 570, 295, 264, 6725, 25879, 11, 498], "temperature": 0.0, "avg_logprob": -0.20495078142951517, "compression_ratio": 1.708185053380783, "no_speech_prob": 4.97507062391378e-05}, {"id": 107, "seek": 44384, "start": 443.84, "end": 449.0, "text": " we compress these columns, we suddenly don't have to read 50 gigabytes anymore, right?", "tokens": [321, 14778, 613, 13766, 11, 321, 5800, 500, 380, 362, 281, 1401, 2625, 42741, 3602, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 108, "seek": 44384, "start": 449.0, "end": 450.11999999999995, "text": " You read less.", "tokens": [509, 1401, 1570, 13], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 109, "seek": 44384, "start": 450.11999999999995, "end": 453.44, "text": " And then of course, you apply like the best case from what I showed you from the last", "tokens": [400, 550, 295, 1164, 11, 291, 3079, 411, 264, 1151, 1389, 490, 437, 286, 4712, 291, 490, 264, 1036], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 110, "seek": 44384, "start": 453.44, "end": 460.55999999999995, "text": " table, five times, there are increases that cost you one point, one minute and 40 seconds.", "tokens": [3199, 11, 1732, 1413, 11, 456, 366, 8637, 300, 2063, 291, 472, 935, 11, 472, 3456, 293, 3356, 3949, 13], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 111, "seek": 44384, "start": 460.55999999999995, "end": 467.28, "text": " So execution, well, there's three ways of doing a query execution.", "tokens": [407, 15058, 11, 731, 11, 456, 311, 1045, 2098, 295, 884, 257, 14581, 15058, 13], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 112, "seek": 44384, "start": 467.28, "end": 469.67999999999995, "text": " There's actually one more, but it's not in the slides.", "tokens": [821, 311, 767, 472, 544, 11, 457, 309, 311, 406, 294, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.23861448581402117, "compression_ratio": 1.556420233463035, "no_speech_prob": 5.5404343584086746e-05}, {"id": 113, "seek": 46968, "start": 469.68, "end": 474.08, "text": " So our SQLites use the top-of-the-time processing, which means that you process one row at a", "tokens": [407, 527, 19200, 3324, 764, 264, 1192, 12, 2670, 12, 3322, 12, 3766, 9007, 11, 597, 1355, 300, 291, 1399, 472, 5386, 412, 257], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 114, "seek": 46968, "start": 474.08, "end": 475.08, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 115, "seek": 46968, "start": 475.08, "end": 478.68, "text": " Pandas uses column-of-the-time processing, which means that you process one column at", "tokens": [16995, 296, 4960, 7738, 12, 2670, 12, 3322, 12, 3766, 9007, 11, 597, 1355, 300, 291, 1399, 472, 7738, 412], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 116, "seek": 46968, "start": 478.68, "end": 479.68, "text": " a time.", "tokens": [257, 565, 13], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 117, "seek": 46968, "start": 479.68, "end": 485.12, "text": " And DuckDB uses kind of like a mix of the both, which is a technique developed by Peter, the", "tokens": [400, 29266, 27735, 4960, 733, 295, 411, 257, 2890, 295, 264, 1293, 11, 597, 307, 257, 6532, 4743, 538, 6508, 11, 264], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 118, "seek": 46968, "start": 485.12, "end": 489.72, "text": " vectorized processing where you process batches of a column at a time.", "tokens": [8062, 1602, 9007, 689, 291, 1399, 15245, 279, 295, 257, 7738, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 119, "seek": 46968, "start": 489.72, "end": 493.8, "text": " So basically, the top-of-the-time model from SQLites, it was optimized for a time where", "tokens": [407, 1936, 11, 264, 1192, 12, 2670, 12, 3322, 12, 3766, 2316, 490, 19200, 3324, 11, 309, 390, 26941, 337, 257, 565, 689], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 120, "seek": 46968, "start": 493.8, "end": 495.72, "text": " computers didn't have a lot of memory.", "tokens": [10807, 994, 380, 362, 257, 688, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.2158053342033835, "compression_ratio": 1.932, "no_speech_prob": 8.242205512942746e-05}, {"id": 121, "seek": 49572, "start": 495.72, "end": 500.76000000000005, "text": " There was low memory to be used because you only need to really keep one row in memory", "tokens": [821, 390, 2295, 4675, 281, 312, 1143, 570, 291, 787, 643, 281, 534, 1066, 472, 5386, 294, 4675], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 122, "seek": 49572, "start": 500.76000000000005, "end": 503.84000000000003, "text": " throughout your whole query plan.", "tokens": [3710, 428, 1379, 14581, 1393, 13], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 123, "seek": 49572, "start": 503.84000000000003, "end": 508.56, "text": " So the memory was expensive, that's what it could do, but this comes with a high CPU overhead", "tokens": [407, 264, 4675, 390, 5124, 11, 300, 311, 437, 309, 727, 360, 11, 457, 341, 1487, 365, 257, 1090, 13199, 19922], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 124, "seek": 49572, "start": 508.56, "end": 514.4, "text": " per tuple because you're constantly resetting your caches, you don't have any cache-conscious", "tokens": [680, 2604, 781, 570, 291, 434, 6460, 14322, 783, 428, 269, 13272, 11, 291, 500, 380, 362, 604, 19459, 12, 19877], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 125, "seek": 49572, "start": 514.4, "end": 520.0, "text": " algorithm running that piece of data up to the production of your query results.", "tokens": [9284, 2614, 300, 2522, 295, 1412, 493, 281, 264, 4265, 295, 428, 14581, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 126, "seek": 49572, "start": 520.0, "end": 523.5600000000001, "text": " If you go to the column-of-the-time, which is what Pandas uses, this already brings like", "tokens": [759, 291, 352, 281, 264, 7738, 12, 2670, 12, 3322, 12, 3766, 11, 597, 307, 437, 16995, 296, 4960, 11, 341, 1217, 5607, 411], "temperature": 0.0, "avg_logprob": -0.17687281440286076, "compression_ratio": 1.6313993174061434, "no_speech_prob": 2.423789737804327e-05}, {"id": 127, "seek": 52356, "start": 523.56, "end": 527.3199999999999, "text": " better CPU utilization, it allows for SIMD.", "tokens": [1101, 13199, 37074, 11, 309, 4045, 337, 24738, 35, 13], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 128, "seek": 52356, "start": 527.3199999999999, "end": 532.04, "text": " But it comes with the cost of materializing large intermediates in memory.", "tokens": [583, 309, 1487, 365, 264, 2063, 295, 2527, 3319, 2416, 15184, 1024, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 129, "seek": 52356, "start": 532.04, "end": 536.04, "text": " It basically means that you need the whole column in memory at that point to process", "tokens": [467, 1936, 1355, 300, 291, 643, 264, 1379, 7738, 294, 4675, 412, 300, 935, 281, 1399], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 130, "seek": 52356, "start": 536.04, "end": 537.3599999999999, "text": " for that operator.", "tokens": [337, 300, 12973, 13], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 131, "seek": 52356, "start": 537.3599999999999, "end": 541.76, "text": " And of course, the intermediates can be gigabytes each, so that's pretty problematic when the", "tokens": [400, 295, 1164, 11, 264, 15184, 1024, 393, 312, 42741, 1184, 11, 370, 300, 311, 1238, 19011, 562, 264], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 132, "seek": 52356, "start": 541.76, "end": 543.0799999999999, "text": " data sizes are large.", "tokens": [1412, 11602, 366, 2416, 13], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 133, "seek": 52356, "start": 543.0799999999999, "end": 546.76, "text": " And that's why you see, for example, that Pandas, if your data doesn't fit in memory,", "tokens": [400, 300, 311, 983, 291, 536, 11, 337, 1365, 11, 300, 16995, 296, 11, 498, 428, 1412, 1177, 380, 3318, 294, 4675, 11], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 134, "seek": 52356, "start": 546.76, "end": 547.76, "text": " what does it happen?", "tokens": [437, 775, 309, 1051, 30], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 135, "seek": 52356, "start": 547.76, "end": 550.4399999999999, "text": " It crashes.", "tokens": [467, 28642, 13], "temperature": 0.0, "avg_logprob": -0.14039950370788573, "compression_ratio": 1.6379928315412187, "no_speech_prob": 2.5421966711292043e-05}, {"id": 136, "seek": 55044, "start": 550.44, "end": 555.6400000000001, "text": " And then if you go to the vectorized query processing, it's actually optimized for CPU", "tokens": [400, 550, 498, 291, 352, 281, 264, 8062, 1602, 14581, 9007, 11, 309, 311, 767, 26941, 337, 13199], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 137, "seek": 55044, "start": 555.6400000000001, "end": 560.2, "text": " cache locality, you can do SIMD instructions, pipelining, and the whole idea is that your", "tokens": [19459, 1628, 1860, 11, 291, 393, 360, 24738, 35, 9415, 11, 8489, 338, 1760, 11, 293, 264, 1379, 1558, 307, 300, 428], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 138, "seek": 55044, "start": 560.2, "end": 564.96, "text": " intermediates are actually going to fit here in L1 cache.", "tokens": [15184, 1024, 366, 767, 516, 281, 3318, 510, 294, 441, 16, 19459, 13], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 139, "seek": 55044, "start": 564.96, "end": 568.48, "text": " So basically you're going to be paying this latency of one nanosecond to be accessing", "tokens": [407, 1936, 291, 434, 516, 281, 312, 6229, 341, 27043, 295, 472, 14067, 541, 18882, 281, 312, 26440], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 140, "seek": 55044, "start": 568.48, "end": 572.8800000000001, "text": " your data throughout your query plan instead of paying the latency of a main memory, which", "tokens": [428, 1412, 3710, 428, 14581, 1393, 2602, 295, 6229, 264, 27043, 295, 257, 2135, 4675, 11, 597], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 141, "seek": 55044, "start": 572.8800000000001, "end": 577.2800000000001, "text": " is also the case of a column database, which is 100 nanoseconds.", "tokens": [307, 611, 264, 1389, 295, 257, 7738, 8149, 11, 597, 307, 2319, 14067, 541, 28750, 13], "temperature": 0.0, "avg_logprob": -0.15362670057910985, "compression_ratio": 1.7060931899641576, "no_speech_prob": 2.9441825972753577e-05}, {"id": 142, "seek": 57728, "start": 577.28, "end": 581.8399999999999, "text": " It seems like a small difference, but when you're constantly executing this, this really", "tokens": [467, 2544, 411, 257, 1359, 2649, 11, 457, 562, 291, 434, 6460, 32368, 341, 11, 341, 534], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 143, "seek": 57728, "start": 581.8399999999999, "end": 586.3199999999999, "text": " becomes a bottleneck.", "tokens": [3643, 257, 44641, 547, 13], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 144, "seek": 57728, "start": 586.3199999999999, "end": 589.4, "text": " And to end-score optimizations, of course, something that we have inducted to be, so", "tokens": [400, 281, 917, 12, 4417, 418, 5028, 14455, 11, 295, 1164, 11, 746, 300, 321, 362, 31612, 292, 281, 312, 11, 370], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 145, "seek": 57728, "start": 589.4, "end": 594.24, "text": " we have stuff like expression rewriting, join ordering, subquery flattening, filtering,", "tokens": [321, 362, 1507, 411, 6114, 319, 19868, 11, 3917, 21739, 11, 1422, 358, 2109, 24183, 278, 11, 30822, 11], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 146, "seek": 57728, "start": 594.24, "end": 600.24, "text": " projection pushdown, which is a bit more simple, but it's extremely important and brings a", "tokens": [22743, 2944, 5093, 11, 597, 307, 257, 857, 544, 2199, 11, 457, 309, 311, 4664, 1021, 293, 5607, 257], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 147, "seek": 57728, "start": 600.24, "end": 602.4, "text": " huge difference in the cost of the query.", "tokens": [2603, 2649, 294, 264, 2063, 295, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 148, "seek": 57728, "start": 602.4, "end": 605.76, "text": " So here's an example of a projection pushdown.", "tokens": [407, 510, 311, 364, 1365, 295, 257, 22743, 2944, 5093, 13], "temperature": 0.0, "avg_logprob": -0.2011851779485153, "compression_ratio": 1.6775362318840579, "no_speech_prob": 3.9738861232763156e-05}, {"id": 149, "seek": 60576, "start": 605.76, "end": 610.4, "text": " Say you have a table with five columns, A, B, C, D, E, and you want to run a query, that's", "tokens": [6463, 291, 362, 257, 3199, 365, 1732, 13766, 11, 316, 11, 363, 11, 383, 11, 413, 11, 462, 11, 293, 291, 528, 281, 1190, 257, 14581, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 150, "seek": 60576, "start": 610.4, "end": 616.92, "text": " pretty small, but the query is like a selects minimum from column A, where there's a filtering", "tokens": [1238, 1359, 11, 457, 264, 14581, 307, 411, 257, 3048, 82, 7285, 490, 7738, 316, 11, 689, 456, 311, 257, 30822], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 151, "seek": 60576, "start": 616.92, "end": 619.92, "text": " column A saying the column A is bigger than zero and you group by B.", "tokens": [7738, 316, 1566, 264, 7738, 316, 307, 3801, 813, 4018, 293, 291, 1594, 538, 363, 13], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 152, "seek": 60576, "start": 619.92, "end": 624.8, "text": " So the whole point of this query is that you're only using two columns of the table, right?", "tokens": [407, 264, 1379, 935, 295, 341, 14581, 307, 300, 291, 434, 787, 1228, 732, 13766, 295, 264, 3199, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 153, "seek": 60576, "start": 624.8, "end": 629.24, "text": " So what the ductdb optimizer will do is like, okay, in this scanner, I know I don't need", "tokens": [407, 437, 264, 25954, 67, 65, 5028, 6545, 486, 360, 307, 411, 11, 1392, 11, 294, 341, 30211, 11, 286, 458, 286, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 154, "seek": 60576, "start": 629.24, "end": 634.3199999999999, "text": " all the columns, I just need N and B, and you just don't have to read the other ones.", "tokens": [439, 264, 13766, 11, 286, 445, 643, 426, 293, 363, 11, 293, 291, 445, 500, 380, 362, 281, 1401, 264, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.18333692358644216, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0002058471436612308}, {"id": 155, "seek": 63432, "start": 634.32, "end": 638.6800000000001, "text": " If you do the same one in pandas, for example, you can apply your filter, and then you have", "tokens": [759, 291, 360, 264, 912, 472, 294, 4565, 296, 11, 337, 1365, 11, 291, 393, 3079, 428, 6608, 11, 293, 550, 291, 362], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 156, "seek": 63432, "start": 638.6800000000001, "end": 642.6, "text": " the filter, the group by the aggregator, but at the time you're doing this filter, you're", "tokens": [264, 6608, 11, 264, 1594, 538, 264, 16743, 1639, 11, 457, 412, 264, 565, 291, 434, 884, 341, 6608, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 157, "seek": 63432, "start": 642.6, "end": 645.88, "text": " still filtering all the other columns you're not going to be using your query.", "tokens": [920, 30822, 439, 264, 661, 13766, 291, 434, 406, 516, 281, 312, 1228, 428, 14581, 13], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 158, "seek": 63432, "start": 645.88, "end": 651.12, "text": " Of course, you can manually make this optimization, but it's pretty nice that the database system", "tokens": [2720, 1164, 11, 291, 393, 16945, 652, 341, 19618, 11, 457, 309, 311, 1238, 1481, 300, 264, 8149, 1185], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 159, "seek": 63432, "start": 651.12, "end": 654.1600000000001, "text": " can do that for you.", "tokens": [393, 360, 300, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 160, "seek": 63432, "start": 654.1600000000001, "end": 659.44, "text": " Of course, the ductdb also has automatic parallelism and beyond-memory execution, so ductdb has", "tokens": [2720, 1164, 11, 264, 25954, 67, 65, 611, 575, 12509, 8952, 1434, 293, 4399, 12, 17886, 827, 15058, 11, 370, 25954, 67, 65, 575], "temperature": 0.0, "avg_logprob": -0.16708287885112147, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.9998787542572245e-05}, {"id": 161, "seek": 65944, "start": 659.44, "end": 665.5600000000001, "text": " parallel versions of most of its operators, I think all of our scanners, including with", "tokens": [8952, 9606, 295, 881, 295, 1080, 19077, 11, 286, 519, 439, 295, 527, 795, 25792, 11, 3009, 365], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 162, "seek": 65944, "start": 665.5600000000001, "end": 671.6, "text": " insertion order preservation of parallelize now, aggregations, joins, pandas, for example,", "tokens": [8969, 313, 1668, 27257, 295, 8952, 1125, 586, 11, 16743, 763, 11, 24397, 11, 4565, 296, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 163, "seek": 65944, "start": 671.6, "end": 674.6800000000001, "text": " only supports single-threaded execution.", "tokens": [787, 9346, 2167, 12, 392, 2538, 292, 15058, 13], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 164, "seek": 65944, "start": 674.6800000000001, "end": 677.24, "text": " We all have pretty good laptops these days, right?", "tokens": [492, 439, 362, 1238, 665, 27642, 613, 1708, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 165, "seek": 65944, "start": 677.24, "end": 681.6400000000001, "text": " So it's a shame if you cannot really take advantage of parallelism.", "tokens": [407, 309, 311, 257, 10069, 498, 291, 2644, 534, 747, 5002, 295, 8952, 1434, 13], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 166, "seek": 65944, "start": 681.6400000000001, "end": 686.4000000000001, "text": " And ductdb, again, supports the execution of data that does not fit in memory.", "tokens": [400, 25954, 67, 65, 11, 797, 11, 9346, 264, 15058, 295, 1412, 300, 775, 406, 3318, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.22892610082086526, "compression_ratio": 1.61003861003861, "no_speech_prob": 6.900434527778998e-05}, {"id": 167, "seek": 68640, "start": 686.4, "end": 690.24, "text": " It's kind of the never give up, never surrender approach, it's like, we're going to execute", "tokens": [467, 311, 733, 295, 264, 1128, 976, 493, 11, 1128, 22185, 3109, 11, 309, 311, 411, 11, 321, 434, 516, 281, 14483], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 168, "seek": 68640, "start": 690.24, "end": 692.12, "text": " this query.", "tokens": [341, 14581, 13], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 169, "seek": 68640, "start": 692.12, "end": 695.9599999999999, "text": " We try to always have graceful degradation, also, that it just doesn't suddenly crash", "tokens": [492, 853, 281, 1009, 362, 10042, 906, 40519, 11, 611, 11, 300, 309, 445, 1177, 380, 5800, 8252], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 170, "seek": 68640, "start": 695.9599999999999, "end": 696.9599999999999, "text": " in performance.", "tokens": [294, 3389, 13], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 171, "seek": 68640, "start": 696.9599999999999, "end": 701.64, "text": " And the whole goal is really to never crash and always execute the query.", "tokens": [400, 264, 1379, 3387, 307, 534, 281, 1128, 8252, 293, 1009, 14483, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 172, "seek": 68640, "start": 701.64, "end": 706.9599999999999, "text": " All right, so a little bit about ductdb in the Python lens.", "tokens": [1057, 558, 11, 370, 257, 707, 857, 466, 25954, 67, 65, 294, 264, 15329, 6765, 13], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 173, "seek": 68640, "start": 706.9599999999999, "end": 715.16, "text": " Basically we have an API, it's a dbapi to.ocompliant, so, far too much what SQLite has, for example,", "tokens": [8537, 321, 362, 364, 9362, 11, 309, 311, 257, 274, 65, 35891, 281, 2411, 905, 298, 564, 5798, 11, 370, 11, 1400, 886, 709, 437, 19200, 642, 575, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.25154341125488283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.000167195452377e-05}, {"id": 174, "seek": 71516, "start": 715.16, "end": 719.92, "text": " you can create a connection and can start executing queries, but we also wanted to have", "tokens": [291, 393, 1884, 257, 4984, 293, 393, 722, 32368, 24109, 11, 457, 321, 611, 1415, 281, 362], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 175, "seek": 71516, "start": 719.92, "end": 725.56, "text": " something similar to the data frame API that still could, people that can't come from pandas,", "tokens": [746, 2531, 281, 264, 1412, 3920, 9362, 300, 920, 727, 11, 561, 300, 393, 380, 808, 490, 4565, 296, 11], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 176, "seek": 71516, "start": 725.56, "end": 728.3199999999999, "text": " for example, could still have something familiar to work on.", "tokens": [337, 1365, 11, 727, 920, 362, 746, 4963, 281, 589, 322, 13], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 177, "seek": 71516, "start": 728.3199999999999, "end": 731.4, "text": " So here in this example, you can also create a connection.", "tokens": [407, 510, 294, 341, 1365, 11, 291, 393, 611, 1884, 257, 4984, 13], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 178, "seek": 71516, "start": 731.4, "end": 734.9599999999999, "text": " You can create this relation, which kind of looks like a data frame, you just point it", "tokens": [509, 393, 1884, 341, 9721, 11, 597, 733, 295, 1542, 411, 257, 1412, 3920, 11, 291, 445, 935, 309], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 179, "seek": 71516, "start": 734.9599999999999, "end": 739.8399999999999, "text": " to a table, you can do a show to inspect what the table is inside, and you can apply, for", "tokens": [281, 257, 3199, 11, 291, 393, 360, 257, 855, 281, 15018, 437, 264, 3199, 307, 1854, 11, 293, 291, 393, 3079, 11, 337], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 180, "seek": 71516, "start": 739.8399999999999, "end": 744.48, "text": " example, these chaining operators, right, like a filter, a projection.", "tokens": [1365, 11, 613, 417, 3686, 19077, 11, 558, 11, 411, 257, 6608, 11, 257, 22743, 13], "temperature": 0.0, "avg_logprob": -0.21291253146003275, "compression_ratio": 1.8610169491525423, "no_speech_prob": 4.12650297221262e-05}, {"id": 181, "seek": 74448, "start": 744.48, "end": 751.4, "text": " So in the end, this is all lazily executed, and this also allows you to take advantage", "tokens": [407, 294, 264, 917, 11, 341, 307, 439, 19320, 953, 17577, 11, 293, 341, 611, 4045, 291, 281, 747, 5002], "temperature": 0.0, "avg_logprob": -0.21214731433723547, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.93463399936445e-05}, {"id": 182, "seek": 74448, "start": 751.4, "end": 756.76, "text": " of the optimizer of ductdb, even if you do the chaining operations.", "tokens": [295, 264, 5028, 6545, 295, 25954, 67, 65, 11, 754, 498, 291, 360, 264, 417, 3686, 7705, 13], "temperature": 0.0, "avg_logprob": -0.21214731433723547, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.93463399936445e-05}, {"id": 183, "seek": 74448, "start": 756.76, "end": 765.6, "text": " Of course, I talked to you about memory transfer, so we were very careful as well into being", "tokens": [2720, 1164, 11, 286, 2825, 281, 291, 466, 4675, 5003, 11, 370, 321, 645, 588, 5026, 382, 731, 666, 885], "temperature": 0.0, "avg_logprob": -0.21214731433723547, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.93463399936445e-05}, {"id": 184, "seek": 74448, "start": 765.6, "end": 769.5600000000001, "text": " very integrated with this, very common libraries in Python.", "tokens": [588, 10919, 365, 341, 11, 588, 2689, 15148, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.21214731433723547, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.93463399936445e-05}, {"id": 185, "seek": 76956, "start": 769.56, "end": 777.04, "text": " So with pandas and pyarrow, for example, what we actually do is that in the end, for pandas,", "tokens": [407, 365, 4565, 296, 293, 10664, 289, 1892, 11, 337, 1365, 11, 437, 321, 767, 360, 307, 300, 294, 264, 917, 11, 337, 4565, 296, 11], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 186, "seek": 76956, "start": 777.04, "end": 782.5999999999999, "text": " the columns are usually not pyarrows, which turns out to be RC vectors, which turns out", "tokens": [264, 13766, 366, 2673, 406, 10664, 289, 28251, 11, 597, 4523, 484, 281, 312, 28987, 18875, 11, 597, 4523, 484], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 187, "seek": 76956, "start": 782.5999999999999, "end": 784.28, "text": " that's also kind of what we use.", "tokens": [300, 311, 611, 733, 295, 437, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 188, "seek": 76956, "start": 784.28, "end": 789.16, "text": " So with a little bit of makeup in the metadata, we can just directly read them, and they're", "tokens": [407, 365, 257, 707, 857, 295, 6567, 294, 264, 26603, 11, 321, 393, 445, 3838, 1401, 552, 11, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 189, "seek": 76956, "start": 789.16, "end": 790.7199999999999, "text": " all in the same process, right?", "tokens": [439, 294, 264, 912, 1399, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 190, "seek": 76956, "start": 790.7199999999999, "end": 796.56, "text": " So we have access to that piece of memory, which in the end means that you can actually", "tokens": [407, 321, 362, 2105, 281, 300, 2522, 295, 4675, 11, 597, 294, 264, 917, 1355, 300, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.16514810741457164, "compression_ratio": 1.7708333333333333, "no_speech_prob": 3.0509947464452125e-05}, {"id": 191, "seek": 79656, "start": 796.56, "end": 801.7199999999999, "text": " access the data from pandas in ductdb without paying any transfer costs, at least constant", "tokens": [2105, 264, 1412, 490, 4565, 296, 294, 25954, 67, 65, 1553, 6229, 604, 5003, 5497, 11, 412, 1935, 5754], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 192, "seek": 79656, "start": 801.7199999999999, "end": 806.4, "text": " transfer costs just for doing the metadata makeup, let's say.", "tokens": [5003, 5497, 445, 337, 884, 264, 26603, 6567, 11, 718, 311, 584, 13], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 193, "seek": 79656, "start": 806.4, "end": 808.1199999999999, "text": " And there's the same thing with pyarrow.", "tokens": [400, 456, 311, 264, 912, 551, 365, 10664, 289, 1892, 13], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 194, "seek": 79656, "start": 808.1199999999999, "end": 814.04, "text": " We also have what we call zero copy, so we can read error objects and output error objects", "tokens": [492, 611, 362, 437, 321, 818, 4018, 5055, 11, 370, 321, 393, 1401, 6713, 6565, 293, 5598, 6713, 6565], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 195, "seek": 79656, "start": 814.04, "end": 816.88, "text": " without any extra costs.", "tokens": [1553, 604, 2857, 5497, 13], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 196, "seek": 79656, "start": 816.88, "end": 822.1999999999999, "text": " With NumPy, we also support SQLCAMY, and in IBIS, they're actually the default back-end", "tokens": [2022, 22592, 47, 88, 11, 321, 611, 1406, 19200, 34, 2865, 56, 11, 293, 294, 40385, 2343, 11, 436, 434, 767, 264, 7576, 646, 12, 521], "temperature": 0.0, "avg_logprob": -0.21557510233371058, "compression_ratio": 1.562992125984252, "no_speech_prob": 0.0001675363164395094}, {"id": 197, "seek": 82220, "start": 822.2, "end": 828.36, "text": " from them, I think, since six months ago.", "tokens": [490, 552, 11, 286, 519, 11, 1670, 2309, 2493, 2057, 13], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 198, "seek": 82220, "start": 828.36, "end": 835.6800000000001, "text": " A little bit of usage, so as you can see, this is our PyPy download counts.", "tokens": [316, 707, 857, 295, 14924, 11, 370, 382, 291, 393, 536, 11, 341, 307, 527, 9953, 47, 88, 5484, 14893, 13], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 199, "seek": 82220, "start": 835.6800000000001, "end": 838.4000000000001, "text": " The Python library is actually our most downloaded API.", "tokens": [440, 15329, 6405, 307, 767, 527, 881, 21748, 9362, 13], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 200, "seek": 82220, "start": 838.4000000000001, "end": 844.12, "text": " We have APIs for all sorts of languages, and you can see that in the last month, we had", "tokens": [492, 362, 21445, 337, 439, 7527, 295, 8650, 11, 293, 291, 393, 536, 300, 294, 264, 1036, 1618, 11, 321, 632], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 201, "seek": 82220, "start": 844.12, "end": 849.6800000000001, "text": " like 900,000 downloads, so there are a lot of people there trying out and using ductdb", "tokens": [411, 22016, 11, 1360, 36553, 11, 370, 456, 366, 257, 688, 295, 561, 456, 1382, 484, 293, 1228, 25954, 67, 65], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 202, "seek": 82220, "start": 849.6800000000001, "end": 852.1600000000001, "text": " in their Python scripts.", "tokens": [294, 641, 15329, 23294, 13], "temperature": 0.0, "avg_logprob": -0.18978374444165277, "compression_ratio": 1.5872340425531914, "no_speech_prob": 7.802151230862364e-05}, {"id": 203, "seek": 85216, "start": 852.16, "end": 863.04, "text": " So now it's the demo time, let me get this, all right, this looks like you can see.", "tokens": [407, 586, 309, 311, 264, 10723, 565, 11, 718, 385, 483, 341, 11, 439, 558, 11, 341, 1542, 411, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.3528175354003906, "compression_ratio": 1.6682464454976302, "no_speech_prob": 3.5829947591992095e-05}, {"id": 204, "seek": 85216, "start": 863.04, "end": 869.28, "text": " So this is just installing ductdb PySpark and getting our yellow trip data dataset,", "tokens": [407, 341, 307, 445, 20762, 25954, 67, 65, 9953, 14014, 809, 293, 1242, 527, 5566, 4931, 1412, 28872, 11], "temperature": 0.0, "avg_logprob": -0.3528175354003906, "compression_ratio": 1.6682464454976302, "no_speech_prob": 3.5829947591992095e-05}, {"id": 205, "seek": 85216, "start": 869.28, "end": 875.4, "text": " our executor, this, our database, just importing the stuff we're going to be using, and here", "tokens": [527, 7568, 284, 11, 341, 11, 527, 8149, 11, 445, 43866, 264, 1507, 321, 434, 516, 281, 312, 1228, 11, 293, 510], "temperature": 0.0, "avg_logprob": -0.3528175354003906, "compression_ratio": 1.6682464454976302, "no_speech_prob": 3.5829947591992095e-05}, {"id": 206, "seek": 85216, "start": 875.4, "end": 881.0, "text": " is just like getting a connection from ductdb, creating a relation that's just, okay, we're", "tokens": [307, 445, 411, 1242, 257, 4984, 490, 25954, 67, 65, 11, 4084, 257, 9721, 300, 311, 445, 11, 1392, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.3528175354003906, "compression_ratio": 1.6682464454976302, "no_speech_prob": 3.5829947591992095e-05}, {"id": 207, "seek": 88100, "start": 881.0, "end": 885.36, "text": " going to, as a parquet file, ductdb can be parquet files, and then you can just print", "tokens": [516, 281, 11, 382, 257, 971, 19343, 3991, 11, 25954, 67, 65, 393, 312, 971, 19343, 7098, 11, 293, 550, 291, 393, 445, 4482], "temperature": 0.0, "avg_logprob": -0.18514628449747386, "compression_ratio": 1.7341269841269842, "no_speech_prob": 1.92144052562071e-05}, {"id": 208, "seek": 88100, "start": 885.36, "end": 890.12, "text": " to inspect what's out there, right, so if we run this, we can see like, okay, these", "tokens": [281, 15018, 437, 311, 484, 456, 11, 558, 11, 370, 498, 321, 1190, 341, 11, 321, 393, 536, 411, 11, 1392, 11, 613], "temperature": 0.0, "avg_logprob": -0.18514628449747386, "compression_ratio": 1.7341269841269842, "no_speech_prob": 1.92144052562071e-05}, {"id": 209, "seek": 88100, "start": 890.12, "end": 894.88, "text": " are the columns we have, we have vendor ID, we have pick up dates, time, passenger counts,", "tokens": [366, 264, 13766, 321, 362, 11, 321, 362, 24321, 7348, 11, 321, 362, 1888, 493, 11691, 11, 565, 11, 18707, 14893, 11], "temperature": 0.0, "avg_logprob": -0.18514628449747386, "compression_ratio": 1.7341269841269842, "no_speech_prob": 1.92144052562071e-05}, {"id": 210, "seek": 88100, "start": 894.88, "end": 899.4, "text": " you have the types of the columns, you can also have a little result preview, just have", "tokens": [291, 362, 264, 3467, 295, 264, 13766, 11, 291, 393, 611, 362, 257, 707, 1874, 14281, 11, 445, 362], "temperature": 0.0, "avg_logprob": -0.18514628449747386, "compression_ratio": 1.7341269841269842, "no_speech_prob": 1.92144052562071e-05}, {"id": 211, "seek": 88100, "start": 899.4, "end": 905.0, "text": " an idea of what it looks like, so I think this dataset has about like 20 columns, maybe,", "tokens": [364, 1558, 295, 437, 309, 1542, 411, 11, 370, 286, 519, 341, 28872, 575, 466, 411, 945, 13766, 11, 1310, 11], "temperature": 0.0, "avg_logprob": -0.18514628449747386, "compression_ratio": 1.7341269841269842, "no_speech_prob": 1.92144052562071e-05}, {"id": 212, "seek": 90500, "start": 905.0, "end": 912.56, "text": " and there's just information about the taxi rides in New York in 2016, and then you can", "tokens": [293, 456, 311, 445, 1589, 466, 264, 18984, 20773, 294, 1873, 3609, 294, 6549, 11, 293, 550, 291, 393], "temperature": 0.0, "avg_logprob": -0.1603026175284171, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.661243524926249e-05}, {"id": 213, "seek": 90500, "start": 912.56, "end": 917.88, "text": " also, for example, run a simple query here, I'm just doing like accounts to know how many", "tokens": [611, 11, 337, 1365, 11, 1190, 257, 2199, 14581, 510, 11, 286, 478, 445, 884, 411, 9402, 281, 458, 577, 867], "temperature": 0.0, "avg_logprob": -0.1603026175284171, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.661243524926249e-05}, {"id": 214, "seek": 90500, "start": 917.88, "end": 922.72, "text": " tuples are there, and we have about 10 million tuples on this dataset.", "tokens": [2604, 2622, 366, 456, 11, 293, 321, 362, 466, 1266, 2459, 2604, 2622, 322, 341, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1603026175284171, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.661243524926249e-05}, {"id": 215, "seek": 90500, "start": 922.72, "end": 926.68, "text": " All right, so this function here is just to do a little bit of benchmarking, coming from", "tokens": [1057, 558, 11, 370, 341, 2445, 510, 307, 445, 281, 360, 257, 707, 857, 295, 18927, 278, 11, 1348, 490], "temperature": 0.0, "avg_logprob": -0.1603026175284171, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.661243524926249e-05}, {"id": 216, "seek": 90500, "start": 926.68, "end": 932.44, "text": " academia, we do have to do something that's kind of fair, I guess, so I run just five", "tokens": [28937, 11, 321, 360, 362, 281, 360, 746, 300, 311, 733, 295, 3143, 11, 286, 2041, 11, 370, 286, 1190, 445, 1732], "temperature": 0.0, "avg_logprob": -0.1603026175284171, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.661243524926249e-05}, {"id": 217, "seek": 93244, "start": 932.44, "end": 937.1600000000001, "text": " times and take the median time of everything, and then this is actually where then we start,", "tokens": [1413, 293, 747, 264, 26779, 565, 295, 1203, 11, 293, 550, 341, 307, 767, 689, 550, 321, 722, 11], "temperature": 0.0, "avg_logprob": -0.17442604591106547, "compression_ratio": 1.790983606557377, "no_speech_prob": 2.8826450943597592e-05}, {"id": 218, "seek": 93244, "start": 937.1600000000001, "end": 941.84, "text": " so we start off with data frame, so Pundas can also read parquet files, and the whole", "tokens": [370, 321, 722, 766, 365, 1412, 3920, 11, 370, 430, 997, 296, 393, 611, 1401, 971, 19343, 7098, 11, 293, 264, 1379], "temperature": 0.0, "avg_logprob": -0.17442604591106547, "compression_ratio": 1.790983606557377, "no_speech_prob": 2.8826450943597592e-05}, {"id": 219, "seek": 93244, "start": 941.84, "end": 946.5200000000001, "text": " thing about ductdb again is that it's not here as a replacement for Pundas, this is", "tokens": [551, 466, 25954, 67, 65, 797, 307, 300, 309, 311, 406, 510, 382, 257, 14419, 337, 430, 997, 296, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.17442604591106547, "compression_ratio": 1.790983606557377, "no_speech_prob": 2.8826450943597592e-05}, {"id": 220, "seek": 93244, "start": 946.5200000000001, "end": 951.44, "text": " not run by itself, but something that can work together with Pundas, so the cool thing", "tokens": [406, 1190, 538, 2564, 11, 457, 746, 300, 393, 589, 1214, 365, 430, 997, 296, 11, 370, 264, 1627, 551], "temperature": 0.0, "avg_logprob": -0.17442604591106547, "compression_ratio": 1.790983606557377, "no_speech_prob": 2.8826450943597592e-05}, {"id": 221, "seek": 93244, "start": 951.44, "end": 957.32, "text": " is that we can, again, read and output data frames without any extra cost, so let's say", "tokens": [307, 300, 321, 393, 11, 797, 11, 1401, 293, 5598, 1412, 12083, 1553, 604, 2857, 2063, 11, 370, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.17442604591106547, "compression_ratio": 1.790983606557377, "no_speech_prob": 2.8826450943597592e-05}, {"id": 222, "seek": 95732, "start": 957.32, "end": 962.44, "text": " that in the query here, we're just getting the passenger counts, then the average tip", "tokens": [300, 294, 264, 14581, 510, 11, 321, 434, 445, 1242, 264, 18707, 14893, 11, 550, 264, 4274, 4125], "temperature": 0.0, "avg_logprob": -0.19111498012099154, "compression_ratio": 1.8457446808510638, "no_speech_prob": 7.318806456169114e-05}, {"id": 223, "seek": 95732, "start": 962.44, "end": 969.9200000000001, "text": " amount of trips that had a short distance, right, and we group by passengers, by the", "tokens": [2372, 295, 16051, 300, 632, 257, 2099, 4560, 11, 558, 11, 293, 321, 1594, 538, 18436, 11, 538, 264], "temperature": 0.0, "avg_logprob": -0.19111498012099154, "compression_ratio": 1.8457446808510638, "no_speech_prob": 7.318806456169114e-05}, {"id": 224, "seek": 95732, "start": 969.9200000000001, "end": 976.6800000000001, "text": " number of passengers, so what we want to know is for short trips, does the amount of tip", "tokens": [1230, 295, 18436, 11, 370, 437, 321, 528, 281, 458, 307, 337, 2099, 16051, 11, 775, 264, 2372, 295, 4125], "temperature": 0.0, "avg_logprob": -0.19111498012099154, "compression_ratio": 1.8457446808510638, "no_speech_prob": 7.318806456169114e-05}, {"id": 225, "seek": 95732, "start": 976.6800000000001, "end": 984.2800000000001, "text": " matters by the number of passengers in that ride, and what you can see here is that you", "tokens": [7001, 538, 264, 1230, 295, 18436, 294, 300, 5077, 11, 293, 437, 291, 393, 536, 510, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.19111498012099154, "compression_ratio": 1.8457446808510638, "no_speech_prob": 7.318806456169114e-05}, {"id": 226, "seek": 98428, "start": 984.28, "end": 989.12, "text": " can, again, read from the data frame, that's what we're doing, and we just have to use", "tokens": [393, 11, 797, 11, 1401, 490, 264, 1412, 3920, 11, 300, 311, 437, 321, 434, 884, 11, 293, 321, 445, 362, 281, 764], "temperature": 0.0, "avg_logprob": -0.15533070815236946, "compression_ratio": 1.766798418972332, "no_speech_prob": 4.6051147364778444e-05}, {"id": 227, "seek": 98428, "start": 989.12, "end": 995.12, "text": " the data frame name in our SQL query, and if you call.df from the query results, you", "tokens": [264, 1412, 3920, 1315, 294, 527, 19200, 14581, 11, 293, 498, 291, 818, 2411, 45953, 490, 264, 14581, 3542, 11, 291], "temperature": 0.0, "avg_logprob": -0.15533070815236946, "compression_ratio": 1.766798418972332, "no_speech_prob": 4.6051147364778444e-05}, {"id": 228, "seek": 98428, "start": 995.12, "end": 1000.36, "text": " also output in your data frame, and it's pretty cool because the data frames have these plots", "tokens": [611, 5598, 294, 428, 1412, 3920, 11, 293, 309, 311, 1238, 1627, 570, 264, 1412, 12083, 362, 613, 28609], "temperature": 0.0, "avg_logprob": -0.15533070815236946, "compression_ratio": 1.766798418972332, "no_speech_prob": 4.6051147364778444e-05}, {"id": 229, "seek": 98428, "start": 1000.36, "end": 1005.24, "text": " bars, they have plotting capabilities that ductdb doesn't have, and you can get easily", "tokens": [10228, 11, 436, 362, 41178, 10862, 300, 25954, 67, 65, 1177, 380, 362, 11, 293, 291, 393, 483, 3612], "temperature": 0.0, "avg_logprob": -0.15533070815236946, "compression_ratio": 1.766798418972332, "no_speech_prob": 4.6051147364778444e-05}, {"id": 230, "seek": 98428, "start": 1005.24, "end": 1012.12, "text": " a very nice chart, so you see here, apparently, there's some dirty data because before getting", "tokens": [257, 588, 1481, 6927, 11, 370, 291, 536, 510, 11, 7970, 11, 456, 311, 512, 9360, 1412, 570, 949, 1242], "temperature": 0.0, "avg_logprob": -0.15533070815236946, "compression_ratio": 1.766798418972332, "no_speech_prob": 4.6051147364778444e-05}, {"id": 231, "seek": 101212, "start": 1012.12, "end": 1017.08, "text": " in tips, when they don't have anyone in their rides, I'm not sure what that is, but apparently", "tokens": [294, 6082, 11, 562, 436, 500, 380, 362, 2878, 294, 641, 20773, 11, 286, 478, 406, 988, 437, 300, 307, 11, 457, 7970], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 232, "seek": 101212, "start": 1017.08, "end": 1021.5600000000001, "text": " like if you have more people, seven to nine, maybe like the more expensive cars, you get", "tokens": [411, 498, 291, 362, 544, 561, 11, 3407, 281, 4949, 11, 1310, 411, 264, 544, 5124, 5163, 11, 291, 483], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 233, "seek": 101212, "start": 1021.5600000000001, "end": 1026.12, "text": " a higher tip, and you can do the same thing in pandas, of course, right, like in pandas", "tokens": [257, 2946, 4125, 11, 293, 291, 393, 360, 264, 912, 551, 294, 4565, 296, 11, 295, 1164, 11, 558, 11, 411, 294, 4565, 296], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 234, "seek": 101212, "start": 1026.12, "end": 1029.16, "text": " you don't have SQL, you're going to have to do, to use their own language, to do the", "tokens": [291, 500, 380, 362, 19200, 11, 291, 434, 516, 281, 362, 281, 360, 11, 281, 764, 641, 1065, 2856, 11, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 235, "seek": 101212, "start": 1029.16, "end": 1034.48, "text": " group by, the average, and you can directly use the plots, and the whole point here is", "tokens": [1594, 538, 11, 264, 4274, 11, 293, 291, 393, 3838, 764, 264, 28609, 11, 293, 264, 1379, 935, 510, 307], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 236, "seek": 101212, "start": 1034.48, "end": 1041.8, "text": " to show the different execution time, like, now we're waiting, okay, so took a second,", "tokens": [281, 855, 264, 819, 15058, 565, 11, 411, 11, 586, 321, 434, 3806, 11, 1392, 11, 370, 1890, 257, 1150, 11], "temperature": 0.0, "avg_logprob": -0.1892034727951576, "compression_ratio": 1.760797342192691, "no_speech_prob": 0.0001845575898187235}, {"id": 237, "seek": 104180, "start": 1041.8, "end": 1048.9199999999998, "text": " and ductdb took 0.2, so this is like a 5x, right, to 0.25, so like 4x, and you also have", "tokens": [293, 25954, 67, 65, 1890, 1958, 13, 17, 11, 370, 341, 307, 411, 257, 1025, 87, 11, 558, 11, 281, 1958, 13, 6074, 11, 370, 411, 1017, 87, 11, 293, 291, 611, 362], "temperature": 0.0, "avg_logprob": -0.14906528384186502, "compression_ratio": 1.6964980544747081, "no_speech_prob": 7.336068665608764e-05}, {"id": 238, "seek": 104180, "start": 1048.9199999999998, "end": 1052.56, "text": " to consider that we're using like a, not a very beefy machine, right, this is a co-lib", "tokens": [281, 1949, 300, 321, 434, 1228, 411, 257, 11, 406, 257, 588, 9256, 88, 3479, 11, 558, 11, 341, 307, 257, 598, 12, 38270], "temperature": 0.0, "avg_logprob": -0.14906528384186502, "compression_ratio": 1.6964980544747081, "no_speech_prob": 7.336068665608764e-05}, {"id": 239, "seek": 104180, "start": 1052.56, "end": 1057.8799999999999, "text": " machine, imagine if you had more cars, this difference would also be bigger, and then", "tokens": [3479, 11, 3811, 498, 291, 632, 544, 5163, 11, 341, 2649, 576, 611, 312, 3801, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.14906528384186502, "compression_ratio": 1.6964980544747081, "no_speech_prob": 7.336068665608764e-05}, {"id": 240, "seek": 104180, "start": 1057.8799999999999, "end": 1064.3999999999999, "text": " I added spark for fun, so actually spark can also read data frames, but it crashes out", "tokens": [286, 3869, 9908, 337, 1019, 11, 370, 767, 9908, 393, 611, 1401, 1412, 12083, 11, 457, 309, 28642, 484], "temperature": 0.0, "avg_logprob": -0.14906528384186502, "compression_ratio": 1.6964980544747081, "no_speech_prob": 7.336068665608764e-05}, {"id": 241, "seek": 104180, "start": 1064.3999999999999, "end": 1069.04, "text": " of memory in my co-lib machine, so I had to give up on this, and read directly from par", "tokens": [295, 4675, 294, 452, 598, 12, 38270, 3479, 11, 370, 286, 632, 281, 976, 493, 322, 341, 11, 293, 1401, 3838, 490, 971], "temperature": 0.0, "avg_logprob": -0.14906528384186502, "compression_ratio": 1.6964980544747081, "no_speech_prob": 7.336068665608764e-05}, {"id": 242, "seek": 106904, "start": 1069.04, "end": 1076.04, "text": " K files, but it does output it as a data frame, I think we're going to have to wait a little", "tokens": [591, 7098, 11, 457, 309, 775, 5598, 309, 382, 257, 1412, 3920, 11, 286, 519, 321, 434, 516, 281, 362, 281, 1699, 257, 707], "temperature": 0.0, "avg_logprob": -0.21861249522158974, "compression_ratio": 1.6130952380952381, "no_speech_prob": 4.532845559879206e-05}, {"id": 243, "seek": 106904, "start": 1076.04, "end": 1085.3999999999999, "text": " bit, but as me it's best, so of course spark is not designed for small data sets, but turns", "tokens": [857, 11, 457, 382, 385, 309, 311, 1151, 11, 370, 295, 1164, 9908, 307, 406, 4761, 337, 1359, 1412, 6352, 11, 457, 4523], "temperature": 0.0, "avg_logprob": -0.21861249522158974, "compression_ratio": 1.6130952380952381, "no_speech_prob": 4.532845559879206e-05}, {"id": 244, "seek": 106904, "start": 1085.3999999999999, "end": 1090.0, "text": " out there are a lot of use cases where you use these small data sets, as you're going,", "tokens": [484, 456, 366, 257, 688, 295, 764, 3331, 689, 291, 764, 613, 1359, 1412, 6352, 11, 382, 291, 434, 516, 11], "temperature": 0.0, "avg_logprob": -0.21861249522158974, "compression_ratio": 1.6130952380952381, "no_speech_prob": 4.532845559879206e-05}, {"id": 245, "seek": 109000, "start": 1090.0, "end": 1099.56, "text": " it's warming up a little bit, it's good for the winter, it produces some energy, I think,", "tokens": [309, 311, 17983, 493, 257, 707, 857, 11, 309, 311, 665, 337, 264, 6355, 11, 309, 14725, 512, 2281, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.24367452922620272, "compression_ratio": 1.497142857142857, "no_speech_prob": 1.38023588078795e-05}, {"id": 246, "seek": 109000, "start": 1099.56, "end": 1108.12, "text": " alright, okay, so it took two seconds, 2.2 seconds, the actual execution, and that's", "tokens": [5845, 11, 1392, 11, 370, 309, 1890, 732, 3949, 11, 568, 13, 17, 3949, 11, 264, 3539, 15058, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.24367452922620272, "compression_ratio": 1.497142857142857, "no_speech_prob": 1.38023588078795e-05}, {"id": 247, "seek": 109000, "start": 1108.12, "end": 1116.08, "text": " already like, what, more than two times what Pandas was, so, yeah, anyway, for the demo", "tokens": [1217, 411, 11, 437, 11, 544, 813, 732, 1413, 437, 16995, 296, 390, 11, 370, 11, 1338, 11, 4033, 11, 337, 264, 10723], "temperature": 0.0, "avg_logprob": -0.24367452922620272, "compression_ratio": 1.497142857142857, "no_speech_prob": 1.38023588078795e-05}, {"id": 248, "seek": 111608, "start": 1116.08, "end": 1120.84, "text": " of course, I showed you something that's fairly simple, can you do actually very complicated", "tokens": [295, 1164, 11, 286, 4712, 291, 746, 300, 311, 6457, 2199, 11, 393, 291, 360, 767, 588, 6179], "temperature": 0.0, "avg_logprob": -0.188431661063378, "compression_ratio": 1.7568627450980392, "no_speech_prob": 3.8154972571646795e-05}, {"id": 249, "seek": 111608, "start": 1120.84, "end": 1124.8, "text": " things, maybe not very complicated, but more complicated, so here I'm not really going to", "tokens": [721, 11, 1310, 406, 588, 6179, 11, 457, 544, 6179, 11, 370, 510, 286, 478, 406, 534, 516, 281], "temperature": 0.0, "avg_logprob": -0.188431661063378, "compression_ratio": 1.7568627450980392, "no_speech_prob": 3.8154972571646795e-05}, {"id": 250, "seek": 111608, "start": 1124.8, "end": 1129.8, "text": " go over the query, but the whole idea is that we can just, for example, use ductDB to run", "tokens": [352, 670, 264, 14581, 11, 457, 264, 1379, 1558, 307, 300, 321, 393, 445, 11, 337, 1365, 11, 764, 25954, 27735, 281, 1190], "temperature": 0.0, "avg_logprob": -0.188431661063378, "compression_ratio": 1.7568627450980392, "no_speech_prob": 3.8154972571646795e-05}, {"id": 251, "seek": 111608, "start": 1129.8, "end": 1138.6, "text": " linear regression, so can we predict, can we estimate the fare with the trip distance,", "tokens": [8213, 24590, 11, 370, 393, 321, 6069, 11, 393, 321, 12539, 264, 11994, 365, 264, 4931, 4560, 11], "temperature": 0.0, "avg_logprob": -0.188431661063378, "compression_ratio": 1.7568627450980392, "no_speech_prob": 3.8154972571646795e-05}, {"id": 252, "seek": 111608, "start": 1138.6, "end": 1144.48, "text": " and turns out you can just calculate the alpha and beta with not such a crazy query, and", "tokens": [293, 4523, 484, 291, 393, 445, 8873, 264, 8961, 293, 9861, 365, 406, 1270, 257, 3219, 14581, 11, 293], "temperature": 0.0, "avg_logprob": -0.188431661063378, "compression_ratio": 1.7568627450980392, "no_speech_prob": 3.8154972571646795e-05}, {"id": 253, "seek": 114448, "start": 1144.48, "end": 1148.8, "text": " then you can again export it to Pandas, and you have a very nice figure there, so you", "tokens": [550, 291, 393, 797, 10725, 309, 281, 16995, 296, 11, 293, 291, 362, 257, 588, 1481, 2573, 456, 11, 370, 291], "temperature": 0.0, "avg_logprob": -0.1848919967125202, "compression_ratio": 1.6035714285714286, "no_speech_prob": 4.305556649342179e-05}, {"id": 254, "seek": 114448, "start": 1148.8, "end": 1158.88, "text": " can really combine these two to get the best out of both, alright, that was the demo, summary,", "tokens": [393, 534, 10432, 613, 732, 281, 483, 264, 1151, 484, 295, 1293, 11, 5845, 11, 300, 390, 264, 10723, 11, 12691, 11], "temperature": 0.0, "avg_logprob": -0.1848919967125202, "compression_ratio": 1.6035714285714286, "no_speech_prob": 4.305556649342179e-05}, {"id": 255, "seek": 114448, "start": 1158.88, "end": 1164.76, "text": " oh that's my last slide, good, so yeah, ductDB is an embedded database system, again it's", "tokens": [1954, 300, 311, 452, 1036, 4137, 11, 665, 11, 370, 1338, 11, 25954, 27735, 307, 364, 16741, 8149, 1185, 11, 797, 309, 311], "temperature": 0.0, "avg_logprob": -0.1848919967125202, "compression_ratio": 1.6035714285714286, "no_speech_prob": 4.305556649342179e-05}, {"id": 256, "seek": 114448, "start": 1164.76, "end": 1169.6, "text": " completely open source, it's under the MIT license, since it came from academia, this", "tokens": [2584, 1269, 4009, 11, 309, 311, 833, 264, 13100, 10476, 11, 1670, 309, 1361, 490, 28937, 11, 341], "temperature": 0.0, "avg_logprob": -0.1848919967125202, "compression_ratio": 1.6035714285714286, "no_speech_prob": 4.305556649342179e-05}, {"id": 257, "seek": 114448, "start": 1169.6, "end": 1174.08, "text": " is something that we're always worried about, it's to also give it back to everyone, because", "tokens": [307, 746, 300, 321, 434, 1009, 5804, 466, 11, 309, 311, 281, 611, 976, 309, 646, 281, 1518, 11, 570], "temperature": 0.0, "avg_logprob": -0.1848919967125202, "compression_ratio": 1.6035714285714286, "no_speech_prob": 4.305556649342179e-05}, {"id": 258, "seek": 117408, "start": 1174.08, "end": 1179.6, "text": " it was usually funded by taxpayers money, so everyone can use it, 100% of what we do", "tokens": [309, 390, 2673, 14385, 538, 38205, 1460, 11, 370, 1518, 393, 764, 309, 11, 2319, 4, 295, 437, 321, 360], "temperature": 0.0, "avg_logprob": -0.14513541556693413, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.7840309485327452e-05}, {"id": 259, "seek": 117408, "start": 1179.6, "end": 1185.32, "text": " is actually open source, there's nothing that's closed source, it's designed for analytical", "tokens": [307, 767, 1269, 4009, 11, 456, 311, 1825, 300, 311, 5395, 4009, 11, 309, 311, 4761, 337, 29579], "temperature": 0.0, "avg_logprob": -0.14513541556693413, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.7840309485327452e-05}, {"id": 260, "seek": 117408, "start": 1185.32, "end": 1192.1599999999999, "text": " queries, so data analysis, data science, has binding for many languages, so of course", "tokens": [24109, 11, 370, 1412, 5215, 11, 1412, 3497, 11, 575, 17359, 337, 867, 8650, 11, 370, 295, 1164], "temperature": 0.0, "avg_logprob": -0.14513541556693413, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.7840309485327452e-05}, {"id": 261, "seek": 117408, "start": 1192.1599999999999, "end": 1197.76, "text": " I'm at the Python dev room, I'm talking about Python, but we have our Java, turns out that", "tokens": [286, 478, 412, 264, 15329, 1905, 1808, 11, 286, 478, 1417, 466, 15329, 11, 457, 321, 362, 527, 10745, 11, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.14513541556693413, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.7840309485327452e-05}, {"id": 262, "seek": 117408, "start": 1197.76, "end": 1203.8, "text": " Java is like one of our most downloaded APIs, so I guess that's an interesting sign, Java", "tokens": [10745, 307, 411, 472, 295, 527, 881, 21748, 21445, 11, 370, 286, 2041, 300, 311, 364, 1880, 1465, 11, 10745], "temperature": 0.0, "avg_logprob": -0.14513541556693413, "compression_ratio": 1.6407407407407408, "no_speech_prob": 2.7840309485327452e-05}, {"id": 263, "seek": 120380, "start": 1203.8, "end": 1209.28, "text": " scripts, and a bunch of others, it has very tight integrations with the Python ecosystem,", "tokens": [23294, 11, 293, 257, 3840, 295, 2357, 11, 309, 575, 588, 4524, 3572, 763, 365, 264, 15329, 11311, 11], "temperature": 0.0, "avg_logprob": -0.20493716728396533, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.00011869015725096688}, {"id": 264, "seek": 120380, "start": 1209.28, "end": 1215.32, "text": " again the whole idea is that you eliminate transfer costs, implements the database in", "tokens": [797, 264, 1379, 1558, 307, 300, 291, 13819, 5003, 5497, 11, 704, 17988, 264, 8149, 294], "temperature": 0.0, "avg_logprob": -0.20493716728396533, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.00011869015725096688}, {"id": 265, "seek": 120380, "start": 1215.32, "end": 1220.52, "text": " relation to APIs, the relation to API again is this more data frame like, and has full", "tokens": [9721, 281, 21445, 11, 264, 9721, 281, 9362, 797, 307, 341, 544, 1412, 3920, 411, 11, 293, 575, 1577], "temperature": 0.0, "avg_logprob": -0.20493716728396533, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.00011869015725096688}, {"id": 266, "seek": 120380, "start": 1220.52, "end": 1226.6399999999999, "text": " SQL support, so anything you can imagine like window functions or what not, you can just", "tokens": [19200, 1406, 11, 370, 1340, 291, 393, 3811, 411, 4910, 6828, 420, 437, 406, 11, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.20493716728396533, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.00011869015725096688}, {"id": 267, "seek": 122664, "start": 1226.64, "end": 1236.2800000000002, "text": " express them using duck to be, and that's it, thank you very much for paying attention,", "tokens": [5109, 552, 1228, 12482, 281, 312, 11, 293, 300, 311, 309, 11, 1309, 291, 588, 709, 337, 6229, 3202, 11], "temperature": 0.0, "avg_logprob": -0.4364205637285786, "compression_ratio": 1.2127659574468086, "no_speech_prob": 0.0005774425808340311}, {"id": 268, "seek": 122664, "start": 1236.2800000000002, "end": 1252.68, "text": " happy to answer questions.", "tokens": [2055, 281, 1867, 1651, 13], "temperature": 0.0, "avg_logprob": -0.4364205637285786, "compression_ratio": 1.2127659574468086, "no_speech_prob": 0.0005774425808340311}, {"id": 269, "seek": 125268, "start": 1252.68, "end": 1262.92, "text": " Thank you Petron, so we have five minutes for your questions.", "tokens": [1044, 291, 10472, 2044, 11, 370, 321, 362, 1732, 2077, 337, 428, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2852007548014323, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.006082952953875065}, {"id": 270, "seek": 125268, "start": 1262.92, "end": 1269.8, "text": " You mentioned, thanks for the great presentation, you mentioned beyond memory execution, and", "tokens": [509, 2835, 11, 3231, 337, 264, 869, 5860, 11, 291, 2835, 4399, 4675, 15058, 11, 293], "temperature": 0.0, "avg_logprob": -0.2852007548014323, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.006082952953875065}, {"id": 271, "seek": 125268, "start": 1269.8, "end": 1274.96, "text": " kind of that it tries not to degrade as much, can you shine a little bit more light on", "tokens": [733, 295, 300, 309, 9898, 406, 281, 368, 8692, 382, 709, 11, 393, 291, 12207, 257, 707, 857, 544, 1442, 322], "temperature": 0.0, "avg_logprob": -0.2852007548014323, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.006082952953875065}, {"id": 272, "seek": 125268, "start": 1274.96, "end": 1280.96, "text": " kind of what happens under the hood, and how much degradation happens?", "tokens": [733, 295, 437, 2314, 833, 264, 13376, 11, 293, 577, 709, 40519, 2314, 30], "temperature": 0.0, "avg_logprob": -0.2852007548014323, "compression_ratio": 1.6082474226804124, "no_speech_prob": 0.006082952953875065}, {"id": 273, "seek": 128096, "start": 1280.96, "end": 1286.0, "text": " Of course, I think that's, there's only the ordering operator that actually does that,", "tokens": [2720, 1164, 11, 286, 519, 300, 311, 11, 456, 311, 787, 264, 21739, 12973, 300, 767, 775, 300, 11], "temperature": 0.0, "avg_logprob": -0.18687610192732376, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0007139791850931942}, {"id": 274, "seek": 128096, "start": 1286.0, "end": 1291.96, "text": " we have Lawrence that's doing his PhD, so there's a lot of operators that need to research", "tokens": [321, 362, 22787, 300, 311, 884, 702, 14476, 11, 370, 456, 311, 257, 688, 295, 19077, 300, 643, 281, 2132], "temperature": 0.0, "avg_logprob": -0.18687610192732376, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0007139791850931942}, {"id": 275, "seek": 128096, "start": 1291.96, "end": 1296.24, "text": " to be developed, that's more of a goal than something that actually happens now, but the", "tokens": [281, 312, 4743, 11, 300, 311, 544, 295, 257, 3387, 813, 746, 300, 767, 2314, 586, 11, 457, 264], "temperature": 0.0, "avg_logprob": -0.18687610192732376, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0007139791850931942}, {"id": 276, "seek": 128096, "start": 1296.24, "end": 1301.52, "text": " whole goal is that you really don't have this sudden spike in the future, but there's research", "tokens": [1379, 3387, 307, 300, 291, 534, 500, 380, 362, 341, 3990, 21053, 294, 264, 2027, 11, 457, 456, 311, 2132], "temperature": 0.0, "avg_logprob": -0.18687610192732376, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0007139791850931942}, {"id": 277, "seek": 130152, "start": 1301.52, "end": 1317.28, "text": " going on, in the future there will be more to be shared for sure.", "tokens": [516, 322, 11, 294, 264, 2027, 456, 486, 312, 544, 281, 312, 5507, 337, 988, 13], "temperature": 0.0, "avg_logprob": -0.14339612512027516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 0.0017162896692752838}, {"id": 278, "seek": 130152, "start": 1317.28, "end": 1321.92, "text": " Thank you very much for the talk, and it's very exciting to see such a tool, such a powerful", "tokens": [1044, 291, 588, 709, 337, 264, 751, 11, 293, 309, 311, 588, 4670, 281, 536, 1270, 257, 2290, 11, 1270, 257, 4005], "temperature": 0.0, "avg_logprob": -0.14339612512027516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 0.0017162896692752838}, {"id": 279, "seek": 130152, "start": 1321.92, "end": 1329.96, "text": " tool, I'm working usually with data warehouses, and I saw on the website that you do not recommend", "tokens": [2290, 11, 286, 478, 1364, 2673, 365, 1412, 17464, 29578, 11, 293, 286, 1866, 322, 264, 3144, 300, 291, 360, 406, 2748], "temperature": 0.0, "avg_logprob": -0.14339612512027516, "compression_ratio": 1.5029239766081872, "no_speech_prob": 0.0017162896692752838}, {"id": 280, "seek": 132996, "start": 1329.96, "end": 1335.32, "text": " using this with data warehouses, I would like to know why.", "tokens": [1228, 341, 365, 1412, 17464, 29578, 11, 286, 576, 411, 281, 458, 983, 13], "temperature": 0.0, "avg_logprob": -0.1709875730004641, "compression_ratio": 1.664, "no_speech_prob": 0.00017115419905167073}, {"id": 281, "seek": 132996, "start": 1335.32, "end": 1341.24, "text": " So of course, there's no one solution for our problems, there are cases that the warehouses", "tokens": [407, 295, 1164, 11, 456, 311, 572, 472, 3827, 337, 527, 2740, 11, 456, 366, 3331, 300, 264, 17464, 29578], "temperature": 0.0, "avg_logprob": -0.1709875730004641, "compression_ratio": 1.664, "no_speech_prob": 0.00017115419905167073}, {"id": 282, "seek": 132996, "start": 1341.24, "end": 1346.8, "text": " are very good fits, it turns out that for data science for example, which is kind of", "tokens": [366, 588, 665, 9001, 11, 309, 4523, 484, 300, 337, 1412, 3497, 337, 1365, 11, 597, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.1709875730004641, "compression_ratio": 1.664, "no_speech_prob": 0.00017115419905167073}, {"id": 283, "seek": 132996, "start": 1346.8, "end": 1352.0, "text": " what we preach the most, they're usually not good because then you fall back to the senior", "tokens": [437, 321, 21552, 264, 881, 11, 436, 434, 2673, 406, 665, 570, 550, 291, 2100, 646, 281, 264, 7965], "temperature": 0.0, "avg_logprob": -0.1709875730004641, "compression_ratio": 1.664, "no_speech_prob": 0.00017115419905167073}, {"id": 284, "seek": 132996, "start": 1352.0, "end": 1356.28, "text": " data outside your database system, like you're not really going to be running your Python", "tokens": [1412, 2380, 428, 8149, 1185, 11, 411, 291, 434, 406, 534, 516, 281, 312, 2614, 428, 15329], "temperature": 0.0, "avg_logprob": -0.1709875730004641, "compression_ratio": 1.664, "no_speech_prob": 0.00017115419905167073}, {"id": 285, "seek": 135628, "start": 1356.28, "end": 1360.8799999999999, "text": " codes inside the system, you can do that for UDS for example, but they are messy, they're", "tokens": [14211, 1854, 264, 1185, 11, 291, 393, 360, 300, 337, 624, 11844, 337, 1365, 11, 457, 436, 366, 16191, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.2571371532621838, "compression_ratio": 1.588, "no_speech_prob": 0.00026729831006377935}, {"id": 286, "seek": 135628, "start": 1360.8799999999999, "end": 1366.04, "text": " a bit nasty, so you want really to have it embedded in your Python process, so you completely", "tokens": [257, 857, 17923, 11, 370, 291, 528, 534, 281, 362, 309, 16741, 294, 428, 15329, 1399, 11, 370, 291, 2584], "temperature": 0.0, "avg_logprob": -0.2571371532621838, "compression_ratio": 1.588, "no_speech_prob": 0.00026729831006377935}, {"id": 287, "seek": 135628, "start": 1366.04, "end": 1370.3999999999999, "text": " eliminate data transfer costs, because usually what you do is like, okay, I have a table,", "tokens": [13819, 1412, 5003, 5497, 11, 570, 2673, 437, 291, 360, 307, 411, 11, 1392, 11, 286, 362, 257, 3199, 11], "temperature": 0.0, "avg_logprob": -0.2571371532621838, "compression_ratio": 1.588, "no_speech_prob": 0.00026729831006377935}, {"id": 288, "seek": 135628, "start": 1370.3999999999999, "end": 1377.3999999999999, "text": " 10 columns, I'm going over 4 columns, but I'm really reading huge chunks of it, so that's", "tokens": [1266, 13766, 11, 286, 478, 516, 670, 1017, 13766, 11, 457, 286, 478, 534, 3760, 2603, 24004, 295, 309, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.2571371532621838, "compression_ratio": 1.588, "no_speech_prob": 0.00026729831006377935}, {"id": 289, "seek": 135628, "start": 1377.3999999999999, "end": 1379.12, "text": " a bottleneck we try to eliminate.", "tokens": [257, 44641, 547, 321, 853, 281, 13819, 13], "temperature": 0.0, "avg_logprob": -0.2571371532621838, "compression_ratio": 1.588, "no_speech_prob": 0.00026729831006377935}, {"id": 290, "seek": 137912, "start": 1379.12, "end": 1386.6, "text": " How do you handle updates?", "tokens": [1012, 360, 291, 4813, 9205, 30], "temperature": 0.0, "avg_logprob": -0.23838324160189242, "compression_ratio": 1.45, "no_speech_prob": 0.00036499713314697146}, {"id": 291, "seek": 137912, "start": 1386.6, "end": 1392.4399999999998, "text": " Although we are in the analytical database system, we do do updates, so Mark, I don't", "tokens": [5780, 321, 366, 294, 264, 29579, 8149, 1185, 11, 321, 360, 360, 9205, 11, 370, 3934, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.23838324160189242, "compression_ratio": 1.45, "no_speech_prob": 0.00036499713314697146}, {"id": 292, "seek": 137912, "start": 1392.4399999999998, "end": 1400.1599999999999, "text": " know where he is, but he's there, he developed MVCC algorithm for OLAP, so we have the same", "tokens": [458, 689, 415, 307, 11, 457, 415, 311, 456, 11, 415, 4743, 17663, 11717, 9284, 337, 39191, 4715, 11, 370, 321, 362, 264, 912], "temperature": 0.0, "avg_logprob": -0.23838324160189242, "compression_ratio": 1.45, "no_speech_prob": 0.00036499713314697146}, {"id": 293, "seek": 137912, "start": 1400.1599999999999, "end": 1404.2399999999998, "text": " asset transactional capabilities that you would expect from a transactional database,", "tokens": [11999, 46688, 1966, 10862, 300, 291, 576, 2066, 490, 257, 46688, 1966, 8149, 11], "temperature": 0.0, "avg_logprob": -0.23838324160189242, "compression_ratio": 1.45, "no_speech_prob": 0.00036499713314697146}, {"id": 294, "seek": 140424, "start": 1404.24, "end": 1410.84, "text": " of course, if you have a transactional workload, you should still go for Postgrease or SQLize", "tokens": [295, 1164, 11, 498, 291, 362, 257, 46688, 1966, 20139, 11, 291, 820, 920, 352, 337, 10223, 33248, 651, 420, 19200, 1125], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 295, "seek": 140424, "start": 1410.84, "end": 1417.08, "text": " or a database that handles this type of transactions, but Mark developed like a full-on algorithm", "tokens": [420, 257, 8149, 300, 18722, 341, 2010, 295, 16856, 11, 457, 3934, 4743, 411, 257, 1577, 12, 266, 9284], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 296, "seek": 140424, "start": 1417.08, "end": 1420.76, "text": " to handle updates completely, yeah.", "tokens": [281, 4813, 9205, 2584, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 297, "seek": 140424, "start": 1420.76, "end": 1423.2, "text": " How do you compare to Vertica?", "tokens": [1012, 360, 291, 6794, 281, 21044, 2262, 30], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 298, "seek": 140424, "start": 1423.2, "end": 1425.4, "text": " How do you compare to Vertica?", "tokens": [1012, 360, 291, 6794, 281, 21044, 2262, 30], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 299, "seek": 140424, "start": 1425.4, "end": 1431.8, "text": " I have a good question, I think in terms of analytical queries, TPCH, probably similar", "tokens": [286, 362, 257, 665, 1168, 11, 286, 519, 294, 2115, 295, 29579, 24109, 11, 44462, 5462, 11, 1391, 2531], "temperature": 0.0, "avg_logprob": -0.36105087122966334, "compression_ratio": 1.5932203389830508, "no_speech_prob": 8.900147076928988e-05}, {"id": 300, "seek": 143180, "start": 1431.8, "end": 1438.28, "text": " performance, but then again, the whole point is that if you go again for the Python process,", "tokens": [3389, 11, 457, 550, 797, 11, 264, 1379, 935, 307, 300, 498, 291, 352, 797, 337, 264, 15329, 1399, 11], "temperature": 0.0, "avg_logprob": -0.19117159083269644, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0011293904390186071}, {"id": 301, "seek": 143180, "start": 1438.28, "end": 1444.48, "text": " the data transfer costs will take most of the time there, and then it's really catered", "tokens": [264, 1412, 5003, 5497, 486, 747, 881, 295, 264, 565, 456, 11, 293, 550, 309, 311, 534, 21557, 292], "temperature": 0.0, "avg_logprob": -0.19117159083269644, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0011293904390186071}, {"id": 302, "seek": 143180, "start": 1444.48, "end": 1451.32, "text": " for this type of scenario, the embedded scenario.", "tokens": [337, 341, 2010, 295, 9005, 11, 264, 16741, 9005, 13], "temperature": 0.0, "avg_logprob": -0.19117159083269644, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0011293904390186071}, {"id": 303, "seek": 143180, "start": 1451.32, "end": 1455.28, "text": " We have one minute left for one more question.", "tokens": [492, 362, 472, 3456, 1411, 337, 472, 544, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19117159083269644, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0011293904390186071}, {"id": 304, "seek": 145528, "start": 1455.28, "end": 1462.28, "text": " Yeah, I actually have a rappel somewhere for a bunch of examples as well, I'm very happy", "tokens": [865, 11, 286, 767, 362, 257, 8125, 338, 4079, 337, 257, 3840, 295, 5110, 382, 731, 11, 286, 478, 588, 2055], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 305, "seek": 145528, "start": 1462.28, "end": 1463.28, "text": " to share it.", "tokens": [281, 2073, 309, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 306, "seek": 145528, "start": 1463.28, "end": 1464.28, "text": " I don't know where I'll post it.", "tokens": [286, 500, 380, 458, 689, 286, 603, 2183, 309, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 307, "seek": 145528, "start": 1464.28, "end": 1465.28, "text": " Ah, the false then thing, I guess.", "tokens": [2438, 11, 264, 7908, 550, 551, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 308, "seek": 145528, "start": 1465.28, "end": 1466.28, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 309, "seek": 145528, "start": 1466.28, "end": 1467.28, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 310, "seek": 145528, "start": 1467.28, "end": 1468.28, "text": " Thank you a lot, Pedro.", "tokens": [1044, 291, 257, 688, 11, 26662, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 311, "seek": 145528, "start": 1468.28, "end": 1469.28, "text": " Thanks a lot.", "tokens": [2561, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 312, "seek": 145528, "start": 1469.28, "end": 1470.28, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.33900438506027747, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.009794431738555431}, {"id": 313, "seek": 147028, "start": 1470.28, "end": 1486.28, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 51164], "temperature": 0.0, "avg_logprob": -0.8680188655853271, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.0006191131542436779}], "language": "en"}