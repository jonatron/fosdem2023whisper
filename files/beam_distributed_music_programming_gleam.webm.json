{"text": " So, now we have Haley Thompson and we're going to talk about the distributed music programming with Gleam, Beam and the Web Audio API. Give it up. Okay, so, hello everyone, yeah, today I'm going to be talking about a little web app I've been making using Beam, Gleam and the Web Audio API. Just before I get into that, maybe a little bit about who I am. My name is Haley. I'm a front-end Elm developer, actually, so I don't really do any back-end stuff. I'm totally new to Beam, Erlang and Elixir. I've been doing Elm professionally, almost exclusively, for about three years now and kind of personally, for four or maybe five, and also a PhD student. I'm writing up my thesis at the moment on programming language design and particularly how it relates to sound and music computing. And finally, I am a Gleam community person. If you've ever dropped into the Gleam Discord, you've probably seen me spending way too much of my own time there. So distributed audio, what the heck am I talking about? What am I going to be making? This nondescript-looking box is called a mono, and one of the things it can be is a step sequencer. And so what that means is each of these buttons represents a note that can be played, and the columns are steps in time, and the rows are different notes, different frequencies. And what I'd like to make is one of these in software, and I want to supercharge that basically by making it networked and collaborative. So we want everyone to be working on the same instrument, you know, on different computers over the web. The way I structured this talk, I'm not going to be going into too many technical details about Gleam or the app itself. If you were here earlier this morning, Harry's talk would have done a really good job of introducing you to Gleam, and if you missed that, the language docs are a much better start than what I could give you. So instead, I'm first going to go over some of the languages I could have chosen and didn't, and then briefly explain why I picked Gleam. And then I'm going to give you a very, very abridged tour of the codebase by basically building the thing from the ground up. So why not your favorite language? Why not JavaScript? Well, I've been doing Elm, as I said, for three, four, five years now. I've been in this great statically-typed, pure, functional fantasy land, and the idea of going back to a mutable, dynamically-typed, object-oriented thing terrifies me. I just don't want to do that at all. So okay, why not Elm then? If I'm so used to it, why would I not use that? Well, I actually maintain a package for doing Web Audio things in Elm, but if you've ever used Elm before, you probably know it has a rather interesting take on foreign function interfaces and interrupt with JavaScript, and I just don't want to deal with that for this particular project. And then it also leaves the question open on what to choose for the back end, and really add, like, just one language for the entire stack. And finally, why not Elixir? Well, I don't know it for a start. As I understand, I'm still going to need to use a lot of JavaScript for the audio side of things, even if I use something like LiveView. And I'm a bit of a type nerd, so the dynamic typing kind of puts me off a bit. For me, I think Gleam conveniently addresses all of these things. So I get to use the same language across the entire stack. Gleam targets both Arlang and JavaScript, and I get to share types across the stack as well. So my audio code and my messaging and stuff, this can all be well typed across kind of the network boundary. It also got a really good interop story. The FFI in Gleam is very simple, very, very easy to use. And so if I need to dip into JavaScript or Arlang or Elixir, that can be quite easy. And also, it's a very simple language. So for someone like me that's very new to back end programming, this is a great kind of soft introduction to Beam and OTP and that sort of thing. Well, I didn't go to that slide, but that's the slide I just did. The first thing I want to do is make some sounds. And to do that, we need to have a bit of an understanding of the web audio API. And so a super, super quick primer on that is it's a lowish level browser API for making sounds on the web. You create audio nodes, so they might be sound sources like an oscillator or some signal processing like a filter or a delay, and you connect those into a graph in JavaScript. But all the signal processing happens in native code that we don't write and we don't control. So this is just a very brief example of what that looks like in JavaScript. I don't know about any of you, but to me, this is really, really clunky. We create a bunch of nodes, then we set a bunch of properties, then we have to remember to connect them up, and then we have to remember to start some of them, and then at the end hopefully we get some sound. Instead, what I'd like to do is get a really nice declarative API for this, something that we might be used to for doing like view code. And for that, I'm going to model that with these two types in Glean. So we have a node type with a filled T, which stands for type, and so that says whether it's an oscillator or a delay or a filter. And we have a list of parameters that we want set on that node, and then a list of connections. And then we end up with something like this. So this is the same audio graph that we just saw with a, in my opinion, a much, much nicer API. You kind of get implicit connections based on how nested things are, kind of like a DOM tray or HTML or something. What I'd need to do then is write a little bit of JavaScript to turn those Glean values into some Web Audio code, and we're not going to go into any detail on that here. It took me about 50 lines of JavaScript to do that, and that is the only not Glean code that I wrote in this whole app. So assuming that all works, the next thing we want to do is render something onto a page. For that, we're going to use a framework that I made called Luster. I've said maybe like 50 times now that I'm a big Elm fan, and so Luster takes a lot of the ideas from Elm, particularly its ModelView update or the Elm architecture, and it basically applies it on top of React. So we actually have a wrapper for React, and we can use React components and all that sort of thing with this nice kind of unidirectional stake flow. So we start off with a model, and this is what we're going to derive both user interface and audio code from. And so here, I don't have the type up on the screen, but where we've got rows, a row has the note, so the frequency to play, and then an array of steps that either indicate whether it's on or off, and we take that model and we render it into something. Now Gleam doesn't have macros, it doesn't have a templating engine, or really anything like JSX or anything like that. What we have is just functions. So here, we're calling element.dev, and we're setting a class on it, and then inside we're rendering a button, and we have this message, this update step message, and basically that's going to be fired whenever the button is clicked on, and that goes through the runtime into our update function. We change some rows, update some program state, and the cycle continues. So the state changes, our UI changes, more interactions, blah, blah, blah. If all goes well, we end up with something that looks like this. And what we have here is just a simple client web app. This is the sequence that I've been talking about. This only runs on the client, so anyone that loads this up is going to get their own thing. And so far, we haven't spoken about back-end, so I'm assuming you're serving this on GitHub pages or your own server or whatever. So what we want to do next is serve this with some Gleam code, and to do that, we're going to use two more packages. One is called GLSEN. This is a fairly low-level package that sets up a supervisor and manages a pool of connections that can manage things like TCP connections and sockets and this sort of thing. And on top of that, another package called mist, which is a web server written in Gleam that provides a kind of dead simple HTTP server that you can then configure to accept web socket connections or do SSL connections, these sorts of things. So far, I've been heavily abridging the code. This is pretty much all you need to start serving some static files using mist and GLSEN. The magic kind of happens just in this very simple serve static asset function, which takes a path. Ideally we'd do some finalization on the path, but I've left that out to be brief. Read the file if the file exists. We just respond and we make sure we set the right headers, and that's it. Now we can host our little web app statically with more Gleam code. The final piece of the puzzle then is client server communication. How do we make this distributed? How do we have everyone connected to the same instance? So for that, we need to set up web sockets and mist makes this dead simple as well. You just set up an upgrade handler on any particular path that you want here. It's just the web socket path, and that code looks like this. You set up some event listeners on when the socket opens or closes, and then also how you want to handle messages. On WS message here, essentially just Jason decodes the message into something well typed and sends that off to our app's main process. On the front end, we need to hook up web sockets as well. There's a package for that called LusterWebSocket. This isn't made by me. Someone else has very gratefully made this. For that, we just need to call WS.init in our app's init function, and that will set up everything that we need, so it will do all the plumbing into the runtime to make sure the events are dispatched and end up in our update function. So here, we pass in this WebSocket message constructor, and then whenever we get an event on the WebSocket that goes into our update function, we can change our state, do whatever we need to do, and that will affect the app and renders and so on. Now, I mess, that is the wrong text, but oh well. I mentioned earlier that one of the great things about DREAM is that we can share types across the front and the back end. And so, what we can start to do is have to type messages between client and server. So here, we have a to back end message type, so this is what the clients will send to the back end to ask it to update some state change. So for example, start the sequence, stop it, toggle a step on or off, update some parameters, and then we'd handle that in our apps main update function on the back end. So here, we're updating some shared state, and this is the state that is shared across all clients, and then we're broadcasting that state back to clients. And we do that with a to front end message, and so this is the same kind of idea in reverse. This will tell the client to update a particular part of its model. That looks like this. Again, we decode the JSON that we're getting from the web socket, and then we can just branch off of that, and this would be called in our update function. And so what we end up is this really neat, tidy kind of loop where the server sends a message to the client with some state to render, then user interaction happens, an event is emitted from there, and instead of updating the state locally, we send a message back to the back end, that updates the state on the back end, and then that state is broadcast back to the clients, and we have the same kind of event loop that we had just on the client, but now across the network. Now I've waffled on for a bit, I think it would be cool to maybe see a demo. I'm not sure we can get the sound. I'm going to check the sound of the video guys, let's try to do what you want to do. What would you like me to do? I'll try to play audio, and I will see if I can. Yeah, we are trying to play audio with the mini jack. I can just play out the speaker, it's fine. It's not a very big room. The mini jack audio is not coming off. Okay, well while they're dealing with that, I'll just explain what's happening, I think it's kind of clear. So we have two clients open here. Okay, that's important, no problem. Maybe it was me that was having no sound. If it was muted, maybe it was the plug in there, let's try. No. Okay, cool. It wasn't user error, it was okay. So we have two instances going on here, for some reason that one isn't going, there we go. So I can change the parameters on this side, you can see they're reflected on the other, add steps or whatever. Yes, and so this is all totally networked, conceptually you could run this on the web and have, I mean this is just running locally but I would have hoped that people could open up here. So just a recap, we've got a full stack GLEAM app, we have an ATP server on the back end, we have a React app on the front end, both written in pure GLEAM, both sharing types, and we have this live view style of communication, but specifically or kind of crucially, this communication is well typed and so we know all the messages that we're supposed to be handling on both the front end and the back end. And this is just a quick kind of look at how many lines of code we're in this code base, and so you can see 85 lines of JavaScript was all that was needed and everything else is pure GLEAM. Which I think is pretty cool, it's pretty exciting that you can do that today. So yeah, thank you for listening. Thank you, are there any questions, yep. Thank you for sharing, maybe it was apparent from your presentation but I just wanted to check how are the different clients synchronized. Yeah, okay, so let me go back. We had this model and when I introduced that each client had their own model and so basically the server has its own version of this now and it's broadcasting, every time the sequence resets, it broadcasts the entire model to make sure everything stays in sync and then whenever one client changes something it broadcasts a message to tell the client to update their local version. So it depends on how the client gets this new information and that's more or less okay enough for synchronization. Yeah it seems to be kind of fine, I guess if one person is in Australia and one is over here there's going to be some noticeable ping but then you wouldn't be stupid enough to do that. Thank you. So I don't know much about the Gleam front end stuff, what was necessary to write in JavaScript that you couldn't write in Gleam? Yeah, the JavaScript is just the part that actually renders the Web Audio stuff. So that's the APIs that are available in Gleam? Well so Gleam doesn't really have any browser API bindings at the moment, I could have FFI'd the whole thing and probably taken a bit more into Gleam but for that particular bit I've done that JavaScript myself quite a few times and so it was just quicker to just keep that little bit in JavaScript. Thank you, thanks. Any other question? In the beginning you presented an API for connecting audio nodes by using nesting, my question is how would that work with more complex graphs that have forks and merges or feedbacks? So you're talking about this, right? Yeah, actually presented a kind of Striptown version of the actual API and there we have like keyed nodes so you can assign like an ID to a node and then there's like Reth nodes as well so you can refer to other nodes in the graph outside of the tree and so that way you can keep this kind of tree-like structure but jump out and refer to anything you want and have loops or whatever. And so actually that's what's happening in this app so we've got that delay that's going on in the background and that's the feedback loop and then it's going, yeah, does that make sense? Cool. Any other question? Hello, sorry, I didn't see the full presentation, I arrived in the middle and maybe I will ask something that you already shared but I would like to know if can we apply this environment for live coding, improvise the performance, it's mainly dedicated for building clients and applications? Yeah, I think you could totally transfer these ideas to live coding or performance, I mean ultimately it just comes down to sending messages right and so here we're sending like user interaction events but you could do conceptually the same thing with code snippets or some other kind of data transfer, yeah. Any other question? Hi, Redsorg, I was wondering you said it was compatible with React and so will it be compatible with other frameworks like Vue or the future? Yeah, at the moment it's just React but it's been on my to-do list for a while now to kind of factor out the state management that Lustre does away from the actual renderer that you choose so right now just React, some nebulous time in the future, it could be Vue or Morphdome or whatever. Okay, I think there's time for one more question if there is one. Okay. Thanks for talk but if someone want to use some hardware devices to connect, does Glim support some other wrappers over Web API to speak with some hardware parts like the USB serial port, etc.? Right. Do you mean from the browser side or yeah, so like I said there aren't really any official bindings at the moment but as I also said the FFI story is very simple so it's actually quite easy to create bindings for these browsers yourself which is pretty much the situation where we're at today. I mean the biggest thing maybe just holding Glim back at the moment is the ecosystem is just very, very young and so we don't have many packages or bindings for a lot of stuff. Okay, thank you again for your talk. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.88, "text": " So, now we have Haley Thompson and we're going to talk about the distributed music programming", "tokens": [407, 11, 586, 321, 362, 389, 29172, 23460, 293, 321, 434, 516, 281, 751, 466, 264, 12631, 1318, 9410], "temperature": 0.0, "avg_logprob": -0.4433933371928201, "compression_ratio": 1.3352601156069364, "no_speech_prob": 0.17126643657684326}, {"id": 1, "seek": 0, "start": 11.88, "end": 15.4, "text": " with Gleam, Beam and the Web Audio API.", "tokens": [365, 460, 306, 335, 11, 879, 335, 293, 264, 9573, 25706, 9362, 13], "temperature": 0.0, "avg_logprob": -0.4433933371928201, "compression_ratio": 1.3352601156069364, "no_speech_prob": 0.17126643657684326}, {"id": 2, "seek": 0, "start": 15.4, "end": 17.400000000000002, "text": " Give it up.", "tokens": [5303, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.4433933371928201, "compression_ratio": 1.3352601156069364, "no_speech_prob": 0.17126643657684326}, {"id": 3, "seek": 0, "start": 17.400000000000002, "end": 27.72, "text": " Okay, so, hello everyone, yeah, today I'm going to be talking about a little web app", "tokens": [1033, 11, 370, 11, 7751, 1518, 11, 1338, 11, 965, 286, 478, 516, 281, 312, 1417, 466, 257, 707, 3670, 724], "temperature": 0.0, "avg_logprob": -0.4433933371928201, "compression_ratio": 1.3352601156069364, "no_speech_prob": 0.17126643657684326}, {"id": 4, "seek": 2772, "start": 27.72, "end": 34.12, "text": " I've been making using Beam, Gleam and the Web Audio API.", "tokens": [286, 600, 668, 1455, 1228, 879, 335, 11, 460, 306, 335, 293, 264, 9573, 25706, 9362, 13], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 5, "seek": 2772, "start": 34.12, "end": 38.04, "text": " Just before I get into that, maybe a little bit about who I am.", "tokens": [1449, 949, 286, 483, 666, 300, 11, 1310, 257, 707, 857, 466, 567, 286, 669, 13], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 6, "seek": 2772, "start": 38.04, "end": 39.04, "text": " My name is Haley.", "tokens": [1222, 1315, 307, 389, 29172, 13], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 7, "seek": 2772, "start": 39.04, "end": 43.92, "text": " I'm a front-end Elm developer, actually, so I don't really do any back-end stuff.", "tokens": [286, 478, 257, 1868, 12, 521, 2699, 76, 10754, 11, 767, 11, 370, 286, 500, 380, 534, 360, 604, 646, 12, 521, 1507, 13], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 8, "seek": 2772, "start": 43.92, "end": 47.239999999999995, "text": " I'm totally new to Beam, Erlang and Elixir.", "tokens": [286, 478, 3879, 777, 281, 879, 335, 11, 3300, 25241, 293, 2699, 970, 347, 13], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 9, "seek": 2772, "start": 47.239999999999995, "end": 54.239999999999995, "text": " I've been doing Elm professionally, almost exclusively, for about three years now and", "tokens": [286, 600, 668, 884, 2699, 76, 27941, 11, 1920, 20638, 11, 337, 466, 1045, 924, 586, 293], "temperature": 0.0, "avg_logprob": -0.19810204112201657, "compression_ratio": 1.4936170212765958, "no_speech_prob": 7.972976163728163e-05}, {"id": 10, "seek": 5424, "start": 54.24, "end": 59.96, "text": " kind of personally, for four or maybe five, and also a PhD student.", "tokens": [733, 295, 5665, 11, 337, 1451, 420, 1310, 1732, 11, 293, 611, 257, 14476, 3107, 13], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 11, "seek": 5424, "start": 59.96, "end": 64.64, "text": " I'm writing up my thesis at the moment on programming language design and particularly", "tokens": [286, 478, 3579, 493, 452, 22288, 412, 264, 1623, 322, 9410, 2856, 1715, 293, 4098], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 12, "seek": 5424, "start": 64.64, "end": 69.2, "text": " how it relates to sound and music computing.", "tokens": [577, 309, 16155, 281, 1626, 293, 1318, 15866, 13], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 13, "seek": 5424, "start": 69.2, "end": 73.68, "text": " And finally, I am a Gleam community person.", "tokens": [400, 2721, 11, 286, 669, 257, 460, 306, 335, 1768, 954, 13], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 14, "seek": 5424, "start": 73.68, "end": 77.44, "text": " If you've ever dropped into the Gleam Discord, you've probably seen me spending way too much", "tokens": [759, 291, 600, 1562, 8119, 666, 264, 460, 306, 335, 32623, 11, 291, 600, 1391, 1612, 385, 6434, 636, 886, 709], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 15, "seek": 5424, "start": 77.44, "end": 81.96000000000001, "text": " of my own time there.", "tokens": [295, 452, 1065, 565, 456, 13], "temperature": 0.0, "avg_logprob": -0.23518687422557544, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.0330340703367256e-05}, {"id": 16, "seek": 8196, "start": 81.96, "end": 85.52, "text": " So distributed audio, what the heck am I talking about?", "tokens": [407, 12631, 6278, 11, 437, 264, 12872, 669, 286, 1417, 466, 30], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 17, "seek": 8196, "start": 85.52, "end": 88.72, "text": " What am I going to be making?", "tokens": [708, 669, 286, 516, 281, 312, 1455, 30], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 18, "seek": 8196, "start": 88.72, "end": 95.11999999999999, "text": " This nondescript-looking box is called a mono, and one of the things it can be is a step", "tokens": [639, 297, 684, 279, 5944, 12, 16129, 2424, 307, 1219, 257, 35624, 11, 293, 472, 295, 264, 721, 309, 393, 312, 307, 257, 1823], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 19, "seek": 8196, "start": 95.11999999999999, "end": 96.11999999999999, "text": " sequencer.", "tokens": [5123, 16542, 13], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 20, "seek": 8196, "start": 96.11999999999999, "end": 103.11999999999999, "text": " And so what that means is each of these buttons represents a note that can be played, and", "tokens": [400, 370, 437, 300, 1355, 307, 1184, 295, 613, 9905, 8855, 257, 3637, 300, 393, 312, 3737, 11, 293], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 21, "seek": 8196, "start": 103.11999999999999, "end": 108.44, "text": " the columns are steps in time, and the rows are different notes, different frequencies.", "tokens": [264, 13766, 366, 4439, 294, 565, 11, 293, 264, 13241, 366, 819, 5570, 11, 819, 20250, 13], "temperature": 0.0, "avg_logprob": -0.15402979703293634, "compression_ratio": 1.65, "no_speech_prob": 2.5315464881714433e-05}, {"id": 22, "seek": 10844, "start": 108.44, "end": 115.12, "text": " And what I'd like to make is one of these in software, and I want to supercharge that", "tokens": [400, 437, 286, 1116, 411, 281, 652, 307, 472, 295, 613, 294, 4722, 11, 293, 286, 528, 281, 1687, 13604, 300], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 23, "seek": 10844, "start": 115.12, "end": 119.03999999999999, "text": " basically by making it networked and collaborative.", "tokens": [1936, 538, 1455, 309, 3209, 292, 293, 16555, 13], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 24, "seek": 10844, "start": 119.03999999999999, "end": 123.84, "text": " So we want everyone to be working on the same instrument, you know, on different computers", "tokens": [407, 321, 528, 1518, 281, 312, 1364, 322, 264, 912, 7198, 11, 291, 458, 11, 322, 819, 10807], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 25, "seek": 10844, "start": 123.84, "end": 127.0, "text": " over the web.", "tokens": [670, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 26, "seek": 10844, "start": 127.0, "end": 131.36, "text": " The way I structured this talk, I'm not going to be going into too many technical details", "tokens": [440, 636, 286, 18519, 341, 751, 11, 286, 478, 406, 516, 281, 312, 516, 666, 886, 867, 6191, 4365], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 27, "seek": 10844, "start": 131.36, "end": 134.68, "text": " about Gleam or the app itself.", "tokens": [466, 460, 306, 335, 420, 264, 724, 2564, 13], "temperature": 0.0, "avg_logprob": -0.10538495855128512, "compression_ratio": 1.5512820512820513, "no_speech_prob": 2.660309473867528e-05}, {"id": 28, "seek": 13468, "start": 134.68, "end": 138.6, "text": " If you were here earlier this morning, Harry's talk would have done a really good job of", "tokens": [759, 291, 645, 510, 3071, 341, 2446, 11, 9378, 311, 751, 576, 362, 1096, 257, 534, 665, 1691, 295], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 29, "seek": 13468, "start": 138.6, "end": 143.24, "text": " introducing you to Gleam, and if you missed that, the language docs are a much better", "tokens": [15424, 291, 281, 460, 306, 335, 11, 293, 498, 291, 6721, 300, 11, 264, 2856, 45623, 366, 257, 709, 1101], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 30, "seek": 13468, "start": 143.24, "end": 145.96, "text": " start than what I could give you.", "tokens": [722, 813, 437, 286, 727, 976, 291, 13], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 31, "seek": 13468, "start": 145.96, "end": 152.48000000000002, "text": " So instead, I'm first going to go over some of the languages I could have chosen and didn't,", "tokens": [407, 2602, 11, 286, 478, 700, 516, 281, 352, 670, 512, 295, 264, 8650, 286, 727, 362, 8614, 293, 994, 380, 11], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 32, "seek": 13468, "start": 152.48000000000002, "end": 156.16, "text": " and then briefly explain why I picked Gleam.", "tokens": [293, 550, 10515, 2903, 983, 286, 6183, 460, 306, 335, 13], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 33, "seek": 13468, "start": 156.16, "end": 162.24, "text": " And then I'm going to give you a very, very abridged tour of the codebase by basically", "tokens": [400, 550, 286, 478, 516, 281, 976, 291, 257, 588, 11, 588, 410, 8558, 3004, 3512, 295, 264, 3089, 17429, 538, 1936], "temperature": 0.0, "avg_logprob": -0.09336451004291403, "compression_ratio": 1.6718146718146718, "no_speech_prob": 2.1303385437931865e-05}, {"id": 34, "seek": 16224, "start": 162.24, "end": 165.68, "text": " building the thing from the ground up.", "tokens": [2390, 264, 551, 490, 264, 2727, 493, 13], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 35, "seek": 16224, "start": 165.68, "end": 168.48000000000002, "text": " So why not your favorite language?", "tokens": [407, 983, 406, 428, 2954, 2856, 30], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 36, "seek": 16224, "start": 168.48000000000002, "end": 170.48000000000002, "text": " Why not JavaScript?", "tokens": [1545, 406, 15778, 30], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 37, "seek": 16224, "start": 170.48000000000002, "end": 176.36, "text": " Well, I've been doing Elm, as I said, for three, four, five years now.", "tokens": [1042, 11, 286, 600, 668, 884, 2699, 76, 11, 382, 286, 848, 11, 337, 1045, 11, 1451, 11, 1732, 924, 586, 13], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 38, "seek": 16224, "start": 176.36, "end": 182.16000000000003, "text": " I've been in this great statically-typed, pure, functional fantasy land, and the idea", "tokens": [286, 600, 668, 294, 341, 869, 2219, 984, 12, 874, 3452, 11, 6075, 11, 11745, 13861, 2117, 11, 293, 264, 1558], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 39, "seek": 16224, "start": 182.16000000000003, "end": 188.96, "text": " of going back to a mutable, dynamically-typed, object-oriented thing terrifies me.", "tokens": [295, 516, 646, 281, 257, 5839, 712, 11, 43492, 12, 874, 3452, 11, 2657, 12, 27414, 551, 7245, 11221, 385, 13], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 40, "seek": 16224, "start": 188.96, "end": 192.16000000000003, "text": " I just don't want to do that at all.", "tokens": [286, 445, 500, 380, 528, 281, 360, 300, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1604027661410245, "compression_ratio": 1.5481171548117154, "no_speech_prob": 3.414004822843708e-05}, {"id": 41, "seek": 19216, "start": 192.16, "end": 194.48, "text": " So okay, why not Elm then?", "tokens": [407, 1392, 11, 983, 406, 2699, 76, 550, 30], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 42, "seek": 19216, "start": 194.48, "end": 197.2, "text": " If I'm so used to it, why would I not use that?", "tokens": [759, 286, 478, 370, 1143, 281, 309, 11, 983, 576, 286, 406, 764, 300, 30], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 43, "seek": 19216, "start": 197.2, "end": 203.72, "text": " Well, I actually maintain a package for doing Web Audio things in Elm, but if you've ever", "tokens": [1042, 11, 286, 767, 6909, 257, 7372, 337, 884, 9573, 25706, 721, 294, 2699, 76, 11, 457, 498, 291, 600, 1562], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 44, "seek": 19216, "start": 203.72, "end": 210.2, "text": " used Elm before, you probably know it has a rather interesting take on foreign function", "tokens": [1143, 2699, 76, 949, 11, 291, 1391, 458, 309, 575, 257, 2831, 1880, 747, 322, 5329, 2445], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 45, "seek": 19216, "start": 210.2, "end": 215.07999999999998, "text": " interfaces and interrupt with JavaScript, and I just don't want to deal with that for", "tokens": [28416, 293, 12729, 365, 15778, 11, 293, 286, 445, 500, 380, 528, 281, 2028, 365, 300, 337], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 46, "seek": 19216, "start": 215.07999999999998, "end": 217.04, "text": " this particular project.", "tokens": [341, 1729, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 47, "seek": 19216, "start": 217.04, "end": 220.92, "text": " And then it also leaves the question open on what to choose for the back end, and really", "tokens": [400, 550, 309, 611, 5510, 264, 1168, 1269, 322, 437, 281, 2826, 337, 264, 646, 917, 11, 293, 534], "temperature": 0.0, "avg_logprob": -0.1698907593549308, "compression_ratio": 1.5971731448763251, "no_speech_prob": 3.249988367315382e-05}, {"id": 48, "seek": 22092, "start": 220.92, "end": 223.79999999999998, "text": " add, like, just one language for the entire stack.", "tokens": [909, 11, 411, 11, 445, 472, 2856, 337, 264, 2302, 8630, 13], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 49, "seek": 22092, "start": 223.79999999999998, "end": 227.04, "text": " And finally, why not Elixir?", "tokens": [400, 2721, 11, 983, 406, 2699, 970, 347, 30], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 50, "seek": 22092, "start": 227.04, "end": 232.2, "text": " Well, I don't know it for a start.", "tokens": [1042, 11, 286, 500, 380, 458, 309, 337, 257, 722, 13], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 51, "seek": 22092, "start": 232.2, "end": 235.83999999999997, "text": " As I understand, I'm still going to need to use a lot of JavaScript for the audio side", "tokens": [1018, 286, 1223, 11, 286, 478, 920, 516, 281, 643, 281, 764, 257, 688, 295, 15778, 337, 264, 6278, 1252], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 52, "seek": 22092, "start": 235.83999999999997, "end": 239.92, "text": " of things, even if I use something like LiveView.", "tokens": [295, 721, 11, 754, 498, 286, 764, 746, 411, 10385, 30203, 13], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 53, "seek": 22092, "start": 239.92, "end": 247.88, "text": " And I'm a bit of a type nerd, so the dynamic typing kind of puts me off a bit.", "tokens": [400, 286, 478, 257, 857, 295, 257, 2010, 23229, 11, 370, 264, 8546, 18444, 733, 295, 8137, 385, 766, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.1912500762939453, "compression_ratio": 1.5, "no_speech_prob": 2.7148485969519243e-05}, {"id": 54, "seek": 24788, "start": 247.88, "end": 252.44, "text": " For me, I think Gleam conveniently addresses all of these things.", "tokens": [1171, 385, 11, 286, 519, 460, 306, 335, 44375, 16862, 439, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 55, "seek": 24788, "start": 252.44, "end": 256.28, "text": " So I get to use the same language across the entire stack.", "tokens": [407, 286, 483, 281, 764, 264, 912, 2856, 2108, 264, 2302, 8630, 13], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 56, "seek": 24788, "start": 256.28, "end": 261.6, "text": " Gleam targets both Arlang and JavaScript, and I get to share types across the stack", "tokens": [460, 306, 335, 12911, 1293, 1587, 25241, 293, 15778, 11, 293, 286, 483, 281, 2073, 3467, 2108, 264, 8630], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 57, "seek": 24788, "start": 261.6, "end": 262.88, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 58, "seek": 24788, "start": 262.88, "end": 269.6, "text": " So my audio code and my messaging and stuff, this can all be well typed across kind of", "tokens": [407, 452, 6278, 3089, 293, 452, 21812, 293, 1507, 11, 341, 393, 439, 312, 731, 33941, 2108, 733, 295], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 59, "seek": 24788, "start": 269.6, "end": 272.48, "text": " the network boundary.", "tokens": [264, 3209, 12866, 13], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 60, "seek": 24788, "start": 272.48, "end": 275.96, "text": " It also got a really good interop story.", "tokens": [467, 611, 658, 257, 534, 665, 728, 404, 1657, 13], "temperature": 0.0, "avg_logprob": -0.16448558460582385, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6399902960984036e-05}, {"id": 61, "seek": 27596, "start": 275.96, "end": 279.28, "text": " The FFI in Gleam is very simple, very, very easy to use.", "tokens": [440, 479, 38568, 294, 460, 306, 335, 307, 588, 2199, 11, 588, 11, 588, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 62, "seek": 27596, "start": 279.28, "end": 286.23999999999995, "text": " And so if I need to dip into JavaScript or Arlang or Elixir, that can be quite easy.", "tokens": [400, 370, 498, 286, 643, 281, 10460, 666, 15778, 420, 1587, 25241, 420, 2699, 970, 347, 11, 300, 393, 312, 1596, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 63, "seek": 27596, "start": 286.23999999999995, "end": 290.4, "text": " And also, it's a very simple language.", "tokens": [400, 611, 11, 309, 311, 257, 588, 2199, 2856, 13], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 64, "seek": 27596, "start": 290.4, "end": 294.28, "text": " So for someone like me that's very new to back end programming, this is a great kind", "tokens": [407, 337, 1580, 411, 385, 300, 311, 588, 777, 281, 646, 917, 9410, 11, 341, 307, 257, 869, 733], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 65, "seek": 27596, "start": 294.28, "end": 299.08, "text": " of soft introduction to Beam and OTP and that sort of thing.", "tokens": [295, 2787, 9339, 281, 40916, 293, 422, 16804, 293, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 66, "seek": 27596, "start": 299.08, "end": 305.71999999999997, "text": " Well, I didn't go to that slide, but that's the slide I just did.", "tokens": [1042, 11, 286, 994, 380, 352, 281, 300, 4137, 11, 457, 300, 311, 264, 4137, 286, 445, 630, 13], "temperature": 0.0, "avg_logprob": -0.1390936761839777, "compression_ratio": 1.5494071146245059, "no_speech_prob": 1.3788793694402557e-05}, {"id": 67, "seek": 30572, "start": 305.72, "end": 309.0, "text": " The first thing I want to do is make some sounds.", "tokens": [440, 700, 551, 286, 528, 281, 360, 307, 652, 512, 3263, 13], "temperature": 0.0, "avg_logprob": -0.13753659697784776, "compression_ratio": 1.5673076923076923, "no_speech_prob": 1.8377670130576007e-05}, {"id": 68, "seek": 30572, "start": 309.0, "end": 314.96000000000004, "text": " And to do that, we need to have a bit of an understanding of the web audio API.", "tokens": [400, 281, 360, 300, 11, 321, 643, 281, 362, 257, 857, 295, 364, 3701, 295, 264, 3670, 6278, 9362, 13], "temperature": 0.0, "avg_logprob": -0.13753659697784776, "compression_ratio": 1.5673076923076923, "no_speech_prob": 1.8377670130576007e-05}, {"id": 69, "seek": 30572, "start": 314.96000000000004, "end": 321.56, "text": " And so a super, super quick primer on that is it's a lowish level browser API for making", "tokens": [400, 370, 257, 1687, 11, 1687, 1702, 12595, 322, 300, 307, 309, 311, 257, 2295, 742, 1496, 11185, 9362, 337, 1455], "temperature": 0.0, "avg_logprob": -0.13753659697784776, "compression_ratio": 1.5673076923076923, "no_speech_prob": 1.8377670130576007e-05}, {"id": 70, "seek": 30572, "start": 321.56, "end": 323.88000000000005, "text": " sounds on the web.", "tokens": [3263, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.13753659697784776, "compression_ratio": 1.5673076923076923, "no_speech_prob": 1.8377670130576007e-05}, {"id": 71, "seek": 30572, "start": 323.88000000000005, "end": 330.36, "text": " You create audio nodes, so they might be sound sources like an oscillator or some signal", "tokens": [509, 1884, 6278, 13891, 11, 370, 436, 1062, 312, 1626, 7139, 411, 364, 43859, 420, 512, 6358], "temperature": 0.0, "avg_logprob": -0.13753659697784776, "compression_ratio": 1.5673076923076923, "no_speech_prob": 1.8377670130576007e-05}, {"id": 72, "seek": 33036, "start": 330.36, "end": 337.0, "text": " processing like a filter or a delay, and you connect those into a graph in JavaScript.", "tokens": [9007, 411, 257, 6608, 420, 257, 8577, 11, 293, 291, 1745, 729, 666, 257, 4295, 294, 15778, 13], "temperature": 0.0, "avg_logprob": -0.09690509332674686, "compression_ratio": 1.6991869918699187, "no_speech_prob": 1.509915637143422e-05}, {"id": 73, "seek": 33036, "start": 337.0, "end": 343.76, "text": " But all the signal processing happens in native code that we don't write and we don't control.", "tokens": [583, 439, 264, 6358, 9007, 2314, 294, 8470, 3089, 300, 321, 500, 380, 2464, 293, 321, 500, 380, 1969, 13], "temperature": 0.0, "avg_logprob": -0.09690509332674686, "compression_ratio": 1.6991869918699187, "no_speech_prob": 1.509915637143422e-05}, {"id": 74, "seek": 33036, "start": 343.76, "end": 349.48, "text": " So this is just a very brief example of what that looks like in JavaScript.", "tokens": [407, 341, 307, 445, 257, 588, 5353, 1365, 295, 437, 300, 1542, 411, 294, 15778, 13], "temperature": 0.0, "avg_logprob": -0.09690509332674686, "compression_ratio": 1.6991869918699187, "no_speech_prob": 1.509915637143422e-05}, {"id": 75, "seek": 33036, "start": 349.48, "end": 354.44, "text": " I don't know about any of you, but to me, this is really, really clunky.", "tokens": [286, 500, 380, 458, 466, 604, 295, 291, 11, 457, 281, 385, 11, 341, 307, 534, 11, 534, 596, 25837, 13], "temperature": 0.0, "avg_logprob": -0.09690509332674686, "compression_ratio": 1.6991869918699187, "no_speech_prob": 1.509915637143422e-05}, {"id": 76, "seek": 33036, "start": 354.44, "end": 358.84000000000003, "text": " We create a bunch of nodes, then we set a bunch of properties, then we have to remember", "tokens": [492, 1884, 257, 3840, 295, 13891, 11, 550, 321, 992, 257, 3840, 295, 7221, 11, 550, 321, 362, 281, 1604], "temperature": 0.0, "avg_logprob": -0.09690509332674686, "compression_ratio": 1.6991869918699187, "no_speech_prob": 1.509915637143422e-05}, {"id": 77, "seek": 35884, "start": 358.84, "end": 362.44, "text": " to connect them up, and then we have to remember to start some of them, and then at the end", "tokens": [281, 1745, 552, 493, 11, 293, 550, 321, 362, 281, 1604, 281, 722, 512, 295, 552, 11, 293, 550, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 78, "seek": 35884, "start": 362.44, "end": 364.52, "text": " hopefully we get some sound.", "tokens": [4696, 321, 483, 512, 1626, 13], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 79, "seek": 35884, "start": 364.52, "end": 370.32, "text": " Instead, what I'd like to do is get a really nice declarative API for this, something that", "tokens": [7156, 11, 437, 286, 1116, 411, 281, 360, 307, 483, 257, 534, 1481, 16694, 1166, 9362, 337, 341, 11, 746, 300], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 80, "seek": 35884, "start": 370.32, "end": 372.59999999999997, "text": " we might be used to for doing like view code.", "tokens": [321, 1062, 312, 1143, 281, 337, 884, 411, 1910, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 81, "seek": 35884, "start": 372.59999999999997, "end": 376.55999999999995, "text": " And for that, I'm going to model that with these two types in Glean.", "tokens": [400, 337, 300, 11, 286, 478, 516, 281, 2316, 300, 365, 613, 732, 3467, 294, 460, 28499, 13], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 82, "seek": 35884, "start": 376.55999999999995, "end": 382.44, "text": " So we have a node type with a filled T, which stands for type, and so that says whether", "tokens": [407, 321, 362, 257, 9984, 2010, 365, 257, 6412, 314, 11, 597, 7382, 337, 2010, 11, 293, 370, 300, 1619, 1968], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 83, "seek": 35884, "start": 382.44, "end": 384.84, "text": " it's an oscillator or a delay or a filter.", "tokens": [309, 311, 364, 43859, 420, 257, 8577, 420, 257, 6608, 13], "temperature": 0.0, "avg_logprob": -0.18943685198587085, "compression_ratio": 1.6988847583643123, "no_speech_prob": 9.682349627837539e-06}, {"id": 84, "seek": 38484, "start": 384.84, "end": 392.91999999999996, "text": " And we have a list of parameters that we want set on that node, and then a list of connections.", "tokens": [400, 321, 362, 257, 1329, 295, 9834, 300, 321, 528, 992, 322, 300, 9984, 11, 293, 550, 257, 1329, 295, 9271, 13], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 85, "seek": 38484, "start": 392.91999999999996, "end": 394.23999999999995, "text": " And then we end up with something like this.", "tokens": [400, 550, 321, 917, 493, 365, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 86, "seek": 38484, "start": 394.23999999999995, "end": 399.12, "text": " So this is the same audio graph that we just saw with a, in my opinion, a much, much nicer", "tokens": [407, 341, 307, 264, 912, 6278, 4295, 300, 321, 445, 1866, 365, 257, 11, 294, 452, 4800, 11, 257, 709, 11, 709, 22842], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 87, "seek": 38484, "start": 399.12, "end": 400.12, "text": " API.", "tokens": [9362, 13], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 88, "seek": 38484, "start": 400.12, "end": 408.35999999999996, "text": " You kind of get implicit connections based on how nested things are, kind of like a DOM", "tokens": [509, 733, 295, 483, 26947, 9271, 2361, 322, 577, 15646, 292, 721, 366, 11, 733, 295, 411, 257, 35727], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 89, "seek": 38484, "start": 408.35999999999996, "end": 413.88, "text": " tray or HTML or something.", "tokens": [16027, 420, 17995, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.1867197553316752, "compression_ratio": 1.5954545454545455, "no_speech_prob": 9.751381185196806e-06}, {"id": 90, "seek": 41388, "start": 413.88, "end": 419.36, "text": " What I'd need to do then is write a little bit of JavaScript to turn those Glean values", "tokens": [708, 286, 1116, 643, 281, 360, 550, 307, 2464, 257, 707, 857, 295, 15778, 281, 1261, 729, 460, 28499, 4190], "temperature": 0.0, "avg_logprob": -0.13047985039135018, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.550527122162748e-05}, {"id": 91, "seek": 41388, "start": 419.36, "end": 424.88, "text": " into some Web Audio code, and we're not going to go into any detail on that here.", "tokens": [666, 512, 9573, 25706, 3089, 11, 293, 321, 434, 406, 516, 281, 352, 666, 604, 2607, 322, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.13047985039135018, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.550527122162748e-05}, {"id": 92, "seek": 41388, "start": 424.88, "end": 428.68, "text": " It took me about 50 lines of JavaScript to do that, and that is the only not Glean code", "tokens": [467, 1890, 385, 466, 2625, 3876, 295, 15778, 281, 360, 300, 11, 293, 300, 307, 264, 787, 406, 460, 28499, 3089], "temperature": 0.0, "avg_logprob": -0.13047985039135018, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.550527122162748e-05}, {"id": 93, "seek": 41388, "start": 428.68, "end": 431.71999999999997, "text": " that I wrote in this whole app.", "tokens": [300, 286, 4114, 294, 341, 1379, 724, 13], "temperature": 0.0, "avg_logprob": -0.13047985039135018, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.550527122162748e-05}, {"id": 94, "seek": 41388, "start": 431.71999999999997, "end": 441.28, "text": " So assuming that all works, the next thing we want to do is render something onto a page.", "tokens": [407, 11926, 300, 439, 1985, 11, 264, 958, 551, 321, 528, 281, 360, 307, 15529, 746, 3911, 257, 3028, 13], "temperature": 0.0, "avg_logprob": -0.13047985039135018, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.550527122162748e-05}, {"id": 95, "seek": 44128, "start": 441.28, "end": 447.08, "text": " For that, we're going to use a framework that I made called Luster.", "tokens": [1171, 300, 11, 321, 434, 516, 281, 764, 257, 8388, 300, 286, 1027, 1219, 441, 8393, 13], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 96, "seek": 44128, "start": 447.08, "end": 453.2, "text": " I've said maybe like 50 times now that I'm a big Elm fan, and so Luster takes a lot of", "tokens": [286, 600, 848, 1310, 411, 2625, 1413, 586, 300, 286, 478, 257, 955, 2699, 76, 3429, 11, 293, 370, 441, 8393, 2516, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 97, "seek": 44128, "start": 453.2, "end": 459.4, "text": " the ideas from Elm, particularly its ModelView update or the Elm architecture, and it basically", "tokens": [264, 3487, 490, 2699, 76, 11, 4098, 1080, 17105, 30203, 5623, 420, 264, 2699, 76, 9482, 11, 293, 309, 1936], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 98, "seek": 44128, "start": 459.4, "end": 461.11999999999995, "text": " applies it on top of React.", "tokens": [13165, 309, 322, 1192, 295, 30644, 13], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 99, "seek": 44128, "start": 461.11999999999995, "end": 464.76, "text": " So we actually have a wrapper for React, and we can use React components and all that sort", "tokens": [407, 321, 767, 362, 257, 46906, 337, 30644, 11, 293, 321, 393, 764, 30644, 6677, 293, 439, 300, 1333], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 100, "seek": 44128, "start": 464.76, "end": 470.28, "text": " of thing with this nice kind of unidirectional stake flow.", "tokens": [295, 551, 365, 341, 1481, 733, 295, 517, 327, 621, 41048, 10407, 3095, 13], "temperature": 0.0, "avg_logprob": -0.13196467531138453, "compression_ratio": 1.5910780669144982, "no_speech_prob": 9.271966519008856e-06}, {"id": 101, "seek": 47028, "start": 470.28, "end": 476.4, "text": " So we start off with a model, and this is what we're going to derive both user interface", "tokens": [407, 321, 722, 766, 365, 257, 2316, 11, 293, 341, 307, 437, 321, 434, 516, 281, 28446, 1293, 4195, 9226], "temperature": 0.0, "avg_logprob": -0.17148850180886008, "compression_ratio": 1.6233183856502242, "no_speech_prob": 1.36508215291542e-05}, {"id": 102, "seek": 47028, "start": 476.4, "end": 478.76, "text": " and audio code from.", "tokens": [293, 6278, 3089, 490, 13], "temperature": 0.0, "avg_logprob": -0.17148850180886008, "compression_ratio": 1.6233183856502242, "no_speech_prob": 1.36508215291542e-05}, {"id": 103, "seek": 47028, "start": 478.76, "end": 485.71999999999997, "text": " And so here, I don't have the type up on the screen, but where we've got rows, a row has", "tokens": [400, 370, 510, 11, 286, 500, 380, 362, 264, 2010, 493, 322, 264, 2568, 11, 457, 689, 321, 600, 658, 13241, 11, 257, 5386, 575], "temperature": 0.0, "avg_logprob": -0.17148850180886008, "compression_ratio": 1.6233183856502242, "no_speech_prob": 1.36508215291542e-05}, {"id": 104, "seek": 47028, "start": 485.71999999999997, "end": 490.67999999999995, "text": " the note, so the frequency to play, and then an array of steps that either indicate whether", "tokens": [264, 3637, 11, 370, 264, 7893, 281, 862, 11, 293, 550, 364, 10225, 295, 4439, 300, 2139, 13330, 1968], "temperature": 0.0, "avg_logprob": -0.17148850180886008, "compression_ratio": 1.6233183856502242, "no_speech_prob": 1.36508215291542e-05}, {"id": 105, "seek": 47028, "start": 490.67999999999995, "end": 497.76, "text": " it's on or off, and we take that model and we render it into something.", "tokens": [309, 311, 322, 420, 766, 11, 293, 321, 747, 300, 2316, 293, 321, 15529, 309, 666, 746, 13], "temperature": 0.0, "avg_logprob": -0.17148850180886008, "compression_ratio": 1.6233183856502242, "no_speech_prob": 1.36508215291542e-05}, {"id": 106, "seek": 49776, "start": 497.76, "end": 502.8, "text": " Now Gleam doesn't have macros, it doesn't have a templating engine, or really anything", "tokens": [823, 460, 306, 335, 1177, 380, 362, 7912, 2635, 11, 309, 1177, 380, 362, 257, 9100, 990, 2848, 11, 420, 534, 1340], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 107, "seek": 49776, "start": 502.8, "end": 504.76, "text": " like JSX or anything like that.", "tokens": [411, 33063, 55, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 108, "seek": 49776, "start": 504.76, "end": 506.84, "text": " What we have is just functions.", "tokens": [708, 321, 362, 307, 445, 6828, 13], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 109, "seek": 49776, "start": 506.84, "end": 512.8, "text": " So here, we're calling element.dev, and we're setting a class on it, and then inside we're", "tokens": [407, 510, 11, 321, 434, 5141, 4478, 13, 40343, 11, 293, 321, 434, 3287, 257, 1508, 322, 309, 11, 293, 550, 1854, 321, 434], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 110, "seek": 49776, "start": 512.8, "end": 518.56, "text": " rendering a button, and we have this message, this update step message, and basically that's", "tokens": [22407, 257, 2960, 11, 293, 321, 362, 341, 3636, 11, 341, 5623, 1823, 3636, 11, 293, 1936, 300, 311], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 111, "seek": 49776, "start": 518.56, "end": 524.12, "text": " going to be fired whenever the button is clicked on, and that goes through the runtime into", "tokens": [516, 281, 312, 11777, 5699, 264, 2960, 307, 23370, 322, 11, 293, 300, 1709, 807, 264, 34474, 666], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 112, "seek": 49776, "start": 524.12, "end": 526.48, "text": " our update function.", "tokens": [527, 5623, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16487168457548498, "compression_ratio": 1.7598425196850394, "no_speech_prob": 3.674559593491722e-06}, {"id": 113, "seek": 52648, "start": 526.48, "end": 532.2, "text": " We change some rows, update some program state, and the cycle continues.", "tokens": [492, 1319, 512, 13241, 11, 5623, 512, 1461, 1785, 11, 293, 264, 6586, 6515, 13], "temperature": 0.0, "avg_logprob": -0.12134003639221191, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.1643734069366474e-05}, {"id": 114, "seek": 52648, "start": 532.2, "end": 538.88, "text": " So the state changes, our UI changes, more interactions, blah, blah, blah.", "tokens": [407, 264, 1785, 2962, 11, 527, 15682, 2962, 11, 544, 13280, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.12134003639221191, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.1643734069366474e-05}, {"id": 115, "seek": 52648, "start": 538.88, "end": 543.64, "text": " If all goes well, we end up with something that looks like this.", "tokens": [759, 439, 1709, 731, 11, 321, 917, 493, 365, 746, 300, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12134003639221191, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.1643734069366474e-05}, {"id": 116, "seek": 52648, "start": 543.64, "end": 550.88, "text": " And what we have here is just a simple client web app.", "tokens": [400, 437, 321, 362, 510, 307, 445, 257, 2199, 6423, 3670, 724, 13], "temperature": 0.0, "avg_logprob": -0.12134003639221191, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.1643734069366474e-05}, {"id": 117, "seek": 52648, "start": 550.88, "end": 553.48, "text": " This is the sequence that I've been talking about.", "tokens": [639, 307, 264, 8310, 300, 286, 600, 668, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.12134003639221191, "compression_ratio": 1.5742574257425743, "no_speech_prob": 1.1643734069366474e-05}, {"id": 118, "seek": 55348, "start": 553.48, "end": 559.48, "text": " This only runs on the client, so anyone that loads this up is going to get their own thing.", "tokens": [639, 787, 6676, 322, 264, 6423, 11, 370, 2878, 300, 12668, 341, 493, 307, 516, 281, 483, 641, 1065, 551, 13], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 119, "seek": 55348, "start": 559.48, "end": 564.48, "text": " And so far, we haven't spoken about back-end, so I'm assuming you're serving this on GitHub", "tokens": [400, 370, 1400, 11, 321, 2378, 380, 10759, 466, 646, 12, 521, 11, 370, 286, 478, 11926, 291, 434, 8148, 341, 322, 23331], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 120, "seek": 55348, "start": 564.48, "end": 567.84, "text": " pages or your own server or whatever.", "tokens": [7183, 420, 428, 1065, 7154, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 121, "seek": 55348, "start": 567.84, "end": 574.72, "text": " So what we want to do next is serve this with some Gleam code, and to do that, we're", "tokens": [407, 437, 321, 528, 281, 360, 958, 307, 4596, 341, 365, 512, 460, 306, 335, 3089, 11, 293, 281, 360, 300, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 122, "seek": 55348, "start": 574.72, "end": 577.36, "text": " going to use two more packages.", "tokens": [516, 281, 764, 732, 544, 17401, 13], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 123, "seek": 55348, "start": 577.36, "end": 578.8000000000001, "text": " One is called GLSEN.", "tokens": [1485, 307, 1219, 16225, 50, 2195, 13], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 124, "seek": 55348, "start": 578.8000000000001, "end": 583.36, "text": " This is a fairly low-level package that sets up a supervisor and manages a pool of connections", "tokens": [639, 307, 257, 6457, 2295, 12, 12418, 7372, 300, 6352, 493, 257, 24610, 293, 22489, 257, 7005, 295, 9271], "temperature": 0.0, "avg_logprob": -0.17012738037109376, "compression_ratio": 1.6272401433691757, "no_speech_prob": 1.1520136467879638e-05}, {"id": 125, "seek": 58336, "start": 583.36, "end": 588.64, "text": " that can manage things like TCP connections and sockets and this sort of thing.", "tokens": [300, 393, 3067, 721, 411, 48965, 9271, 293, 370, 11984, 293, 341, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 126, "seek": 58336, "start": 588.64, "end": 593.44, "text": " And on top of that, another package called mist, which is a web server written in Gleam", "tokens": [400, 322, 1192, 295, 300, 11, 1071, 7372, 1219, 3544, 11, 597, 307, 257, 3670, 7154, 3720, 294, 460, 306, 335], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 127, "seek": 58336, "start": 593.44, "end": 598.6, "text": " that provides a kind of dead simple HTTP server that you can then configure to accept web", "tokens": [300, 6417, 257, 733, 295, 3116, 2199, 33283, 7154, 300, 291, 393, 550, 22162, 281, 3241, 3670], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 128, "seek": 58336, "start": 598.6, "end": 604.6800000000001, "text": " socket connections or do SSL connections, these sorts of things.", "tokens": [19741, 9271, 420, 360, 12238, 43, 9271, 11, 613, 7527, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 129, "seek": 58336, "start": 604.6800000000001, "end": 607.64, "text": " So far, I've been heavily abridging the code.", "tokens": [407, 1400, 11, 286, 600, 668, 10950, 410, 8558, 3249, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 130, "seek": 58336, "start": 607.64, "end": 613.16, "text": " This is pretty much all you need to start serving some static files using mist and", "tokens": [639, 307, 1238, 709, 439, 291, 643, 281, 722, 8148, 512, 13437, 7098, 1228, 3544, 293], "temperature": 0.0, "avg_logprob": -0.13189112056385388, "compression_ratio": 1.6340579710144927, "no_speech_prob": 1.350108504993841e-05}, {"id": 131, "seek": 61316, "start": 613.16, "end": 615.9599999999999, "text": " GLSEN.", "tokens": [16225, 50, 2195, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 132, "seek": 61316, "start": 615.9599999999999, "end": 620.36, "text": " The magic kind of happens just in this very simple serve static asset function, which", "tokens": [440, 5585, 733, 295, 2314, 445, 294, 341, 588, 2199, 4596, 13437, 11999, 2445, 11, 597], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 133, "seek": 61316, "start": 620.36, "end": 622.24, "text": " takes a path.", "tokens": [2516, 257, 3100, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 134, "seek": 61316, "start": 622.24, "end": 628.04, "text": " Ideally we'd do some finalization on the path, but I've left that out to be brief.", "tokens": [40817, 321, 1116, 360, 512, 2572, 2144, 322, 264, 3100, 11, 457, 286, 600, 1411, 300, 484, 281, 312, 5353, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 135, "seek": 61316, "start": 628.04, "end": 630.52, "text": " Read the file if the file exists.", "tokens": [17604, 264, 3991, 498, 264, 3991, 8198, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 136, "seek": 61316, "start": 630.52, "end": 634.4399999999999, "text": " We just respond and we make sure we set the right headers, and that's it.", "tokens": [492, 445, 4196, 293, 321, 652, 988, 321, 992, 264, 558, 45101, 11, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 137, "seek": 61316, "start": 634.4399999999999, "end": 643.0799999999999, "text": " Now we can host our little web app statically with more Gleam code.", "tokens": [823, 321, 393, 3975, 527, 707, 3670, 724, 2219, 984, 365, 544, 460, 306, 335, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18777100856487566, "compression_ratio": 1.5082644628099173, "no_speech_prob": 1.1871376045746729e-05}, {"id": 138, "seek": 64308, "start": 643.08, "end": 647.4000000000001, "text": " The final piece of the puzzle then is client server communication.", "tokens": [440, 2572, 2522, 295, 264, 12805, 550, 307, 6423, 7154, 6101, 13], "temperature": 0.0, "avg_logprob": -0.12177591686007343, "compression_ratio": 1.5240384615384615, "no_speech_prob": 2.4953715183073655e-06}, {"id": 139, "seek": 64308, "start": 647.4000000000001, "end": 651.5200000000001, "text": " How do we make this distributed?", "tokens": [1012, 360, 321, 652, 341, 12631, 30], "temperature": 0.0, "avg_logprob": -0.12177591686007343, "compression_ratio": 1.5240384615384615, "no_speech_prob": 2.4953715183073655e-06}, {"id": 140, "seek": 64308, "start": 651.5200000000001, "end": 657.2, "text": " How do we have everyone connected to the same instance?", "tokens": [1012, 360, 321, 362, 1518, 4582, 281, 264, 912, 5197, 30], "temperature": 0.0, "avg_logprob": -0.12177591686007343, "compression_ratio": 1.5240384615384615, "no_speech_prob": 2.4953715183073655e-06}, {"id": 141, "seek": 64308, "start": 657.2, "end": 662.76, "text": " So for that, we need to set up web sockets and mist makes this dead simple as well.", "tokens": [407, 337, 300, 11, 321, 643, 281, 992, 493, 3670, 370, 11984, 293, 3544, 1669, 341, 3116, 2199, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12177591686007343, "compression_ratio": 1.5240384615384615, "no_speech_prob": 2.4953715183073655e-06}, {"id": 142, "seek": 64308, "start": 662.76, "end": 669.6800000000001, "text": " You just set up an upgrade handler on any particular path that you want here.", "tokens": [509, 445, 992, 493, 364, 11484, 41967, 322, 604, 1729, 3100, 300, 291, 528, 510, 13], "temperature": 0.0, "avg_logprob": -0.12177591686007343, "compression_ratio": 1.5240384615384615, "no_speech_prob": 2.4953715183073655e-06}, {"id": 143, "seek": 66968, "start": 669.68, "end": 673.3199999999999, "text": " It's just the web socket path, and that code looks like this.", "tokens": [467, 311, 445, 264, 3670, 19741, 3100, 11, 293, 300, 3089, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 144, "seek": 66968, "start": 673.3199999999999, "end": 678.76, "text": " You set up some event listeners on when the socket opens or closes, and then also how", "tokens": [509, 992, 493, 512, 2280, 23274, 322, 562, 264, 19741, 9870, 420, 24157, 11, 293, 550, 611, 577], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 145, "seek": 66968, "start": 678.76, "end": 681.76, "text": " you want to handle messages.", "tokens": [291, 528, 281, 4813, 7897, 13], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 146, "seek": 66968, "start": 681.76, "end": 689.5999999999999, "text": " On WS message here, essentially just Jason decodes the message into something well typed", "tokens": [1282, 343, 50, 3636, 510, 11, 4476, 445, 11181, 979, 4789, 264, 3636, 666, 746, 731, 33941], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 147, "seek": 66968, "start": 689.5999999999999, "end": 694.9599999999999, "text": " and sends that off to our app's main process.", "tokens": [293, 14790, 300, 766, 281, 527, 724, 311, 2135, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 148, "seek": 66968, "start": 694.9599999999999, "end": 697.9599999999999, "text": " On the front end, we need to hook up web sockets as well.", "tokens": [1282, 264, 1868, 917, 11, 321, 643, 281, 6328, 493, 3670, 370, 11984, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1623661985102388, "compression_ratio": 1.6327433628318584, "no_speech_prob": 6.131137524789665e-06}, {"id": 149, "seek": 69796, "start": 697.96, "end": 701.24, "text": " There's a package for that called LusterWebSocket.", "tokens": [821, 311, 257, 7372, 337, 300, 1219, 441, 8393, 4360, 65, 50, 31380, 13], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 150, "seek": 69796, "start": 701.24, "end": 702.24, "text": " This isn't made by me.", "tokens": [639, 1943, 380, 1027, 538, 385, 13], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 151, "seek": 69796, "start": 702.24, "end": 706.2, "text": " Someone else has very gratefully made this.", "tokens": [8734, 1646, 575, 588, 46214, 2277, 1027, 341, 13], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 152, "seek": 69796, "start": 706.2, "end": 712.96, "text": " For that, we just need to call WS.init in our app's init function, and that will set", "tokens": [1171, 300, 11, 321, 445, 643, 281, 818, 343, 50, 13, 259, 270, 294, 527, 724, 311, 3157, 2445, 11, 293, 300, 486, 992], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 153, "seek": 69796, "start": 712.96, "end": 716.76, "text": " up everything that we need, so it will do all the plumbing into the runtime to make", "tokens": [493, 1203, 300, 321, 643, 11, 370, 309, 486, 360, 439, 264, 39993, 666, 264, 34474, 281, 652], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 154, "seek": 69796, "start": 716.76, "end": 720.52, "text": " sure the events are dispatched and end up in our update function.", "tokens": [988, 264, 3931, 366, 4920, 24102, 293, 917, 493, 294, 527, 5623, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 155, "seek": 69796, "start": 720.52, "end": 727.0, "text": " So here, we pass in this WebSocket message constructor, and then whenever we get an event", "tokens": [407, 510, 11, 321, 1320, 294, 341, 9573, 50, 31380, 3636, 47479, 11, 293, 550, 5699, 321, 483, 364, 2280], "temperature": 0.0, "avg_logprob": -0.20443486385658138, "compression_ratio": 1.6742424242424243, "no_speech_prob": 4.032987271784805e-05}, {"id": 156, "seek": 72700, "start": 727.0, "end": 731.2, "text": " on the WebSocket that goes into our update function, we can change our state, do whatever", "tokens": [322, 264, 9573, 50, 31380, 300, 1709, 666, 527, 5623, 2445, 11, 321, 393, 1319, 527, 1785, 11, 360, 2035], "temperature": 0.0, "avg_logprob": -0.245149905865009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 1.721522130537778e-05}, {"id": 157, "seek": 72700, "start": 731.2, "end": 735.88, "text": " we need to do, and that will affect the app and renders and so on.", "tokens": [321, 643, 281, 360, 11, 293, 300, 486, 3345, 264, 724, 293, 6125, 433, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.245149905865009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 1.721522130537778e-05}, {"id": 158, "seek": 72700, "start": 735.88, "end": 743.0, "text": " Now, I mess, that is the wrong text, but oh well.", "tokens": [823, 11, 286, 2082, 11, 300, 307, 264, 2085, 2487, 11, 457, 1954, 731, 13], "temperature": 0.0, "avg_logprob": -0.245149905865009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 1.721522130537778e-05}, {"id": 159, "seek": 72700, "start": 743.0, "end": 748.08, "text": " I mentioned earlier that one of the great things about DREAM is that we can share types", "tokens": [286, 2835, 3071, 300, 472, 295, 264, 869, 721, 466, 413, 28649, 307, 300, 321, 393, 2073, 3467], "temperature": 0.0, "avg_logprob": -0.245149905865009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 1.721522130537778e-05}, {"id": 160, "seek": 72700, "start": 748.08, "end": 750.72, "text": " across the front and the back end.", "tokens": [2108, 264, 1868, 293, 264, 646, 917, 13], "temperature": 0.0, "avg_logprob": -0.245149905865009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 1.721522130537778e-05}, {"id": 161, "seek": 75072, "start": 750.72, "end": 757.12, "text": " And so, what we can start to do is have to type messages between client and server.", "tokens": [400, 370, 11, 437, 321, 393, 722, 281, 360, 307, 362, 281, 2010, 7897, 1296, 6423, 293, 7154, 13], "temperature": 0.0, "avg_logprob": -0.18275747980390275, "compression_ratio": 1.640625, "no_speech_prob": 6.85420991430874e-06}, {"id": 162, "seek": 75072, "start": 757.12, "end": 762.24, "text": " So here, we have a to back end message type, so this is what the clients will send to", "tokens": [407, 510, 11, 321, 362, 257, 281, 646, 917, 3636, 2010, 11, 370, 341, 307, 437, 264, 6982, 486, 2845, 281], "temperature": 0.0, "avg_logprob": -0.18275747980390275, "compression_ratio": 1.640625, "no_speech_prob": 6.85420991430874e-06}, {"id": 163, "seek": 75072, "start": 762.24, "end": 766.52, "text": " the back end to ask it to update some state change.", "tokens": [264, 646, 917, 281, 1029, 309, 281, 5623, 512, 1785, 1319, 13], "temperature": 0.0, "avg_logprob": -0.18275747980390275, "compression_ratio": 1.640625, "no_speech_prob": 6.85420991430874e-06}, {"id": 164, "seek": 75072, "start": 766.52, "end": 776.4, "text": " So for example, start the sequence, stop it, toggle a step on or off, update some parameters,", "tokens": [407, 337, 1365, 11, 722, 264, 8310, 11, 1590, 309, 11, 31225, 257, 1823, 322, 420, 766, 11, 5623, 512, 9834, 11], "temperature": 0.0, "avg_logprob": -0.18275747980390275, "compression_ratio": 1.640625, "no_speech_prob": 6.85420991430874e-06}, {"id": 165, "seek": 77640, "start": 776.4, "end": 782.92, "text": " and then we'd handle that in our apps main update function on the back end.", "tokens": [293, 550, 321, 1116, 4813, 300, 294, 527, 7733, 2135, 5623, 2445, 322, 264, 646, 917, 13], "temperature": 0.0, "avg_logprob": -0.152864881308682, "compression_ratio": 1.7052631578947368, "no_speech_prob": 6.810982540628174e-06}, {"id": 166, "seek": 77640, "start": 782.92, "end": 788.0, "text": " So here, we're updating some shared state, and this is the state that is shared across", "tokens": [407, 510, 11, 321, 434, 25113, 512, 5507, 1785, 11, 293, 341, 307, 264, 1785, 300, 307, 5507, 2108], "temperature": 0.0, "avg_logprob": -0.152864881308682, "compression_ratio": 1.7052631578947368, "no_speech_prob": 6.810982540628174e-06}, {"id": 167, "seek": 77640, "start": 788.0, "end": 794.36, "text": " all clients, and then we're broadcasting that state back to clients.", "tokens": [439, 6982, 11, 293, 550, 321, 434, 30024, 300, 1785, 646, 281, 6982, 13], "temperature": 0.0, "avg_logprob": -0.152864881308682, "compression_ratio": 1.7052631578947368, "no_speech_prob": 6.810982540628174e-06}, {"id": 168, "seek": 77640, "start": 794.36, "end": 799.28, "text": " And we do that with a to front end message, and so this is the same kind of idea in reverse.", "tokens": [400, 321, 360, 300, 365, 257, 281, 1868, 917, 3636, 11, 293, 370, 341, 307, 264, 912, 733, 295, 1558, 294, 9943, 13], "temperature": 0.0, "avg_logprob": -0.152864881308682, "compression_ratio": 1.7052631578947368, "no_speech_prob": 6.810982540628174e-06}, {"id": 169, "seek": 79928, "start": 799.28, "end": 807.56, "text": " This will tell the client to update a particular part of its model.", "tokens": [639, 486, 980, 264, 6423, 281, 5623, 257, 1729, 644, 295, 1080, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16897102843883427, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.244816970342072e-06}, {"id": 170, "seek": 79928, "start": 807.56, "end": 808.56, "text": " That looks like this.", "tokens": [663, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16897102843883427, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.244816970342072e-06}, {"id": 171, "seek": 79928, "start": 808.56, "end": 812.64, "text": " Again, we decode the JSON that we're getting from the web socket, and then we can just", "tokens": [3764, 11, 321, 979, 1429, 264, 31828, 300, 321, 434, 1242, 490, 264, 3670, 19741, 11, 293, 550, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.16897102843883427, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.244816970342072e-06}, {"id": 172, "seek": 79928, "start": 812.64, "end": 816.0, "text": " branch off of that, and this would be called in our update function.", "tokens": [9819, 766, 295, 300, 11, 293, 341, 576, 312, 1219, 294, 527, 5623, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16897102843883427, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.244816970342072e-06}, {"id": 173, "seek": 79928, "start": 816.0, "end": 823.9599999999999, "text": " And so what we end up is this really neat, tidy kind of loop where the server sends", "tokens": [400, 370, 437, 321, 917, 493, 307, 341, 534, 10654, 11, 34646, 733, 295, 6367, 689, 264, 7154, 14790], "temperature": 0.0, "avg_logprob": -0.16897102843883427, "compression_ratio": 1.5373831775700935, "no_speech_prob": 7.244816970342072e-06}, {"id": 174, "seek": 82396, "start": 823.96, "end": 830.08, "text": " a message to the client with some state to render, then user interaction happens, an", "tokens": [257, 3636, 281, 264, 6423, 365, 512, 1785, 281, 15529, 11, 550, 4195, 9285, 2314, 11, 364], "temperature": 0.0, "avg_logprob": -0.14035062988599142, "compression_ratio": 1.8840579710144927, "no_speech_prob": 8.70600160851609e-06}, {"id": 175, "seek": 82396, "start": 830.08, "end": 834.6800000000001, "text": " event is emitted from there, and instead of updating the state locally, we send a message", "tokens": [2280, 307, 44897, 490, 456, 11, 293, 2602, 295, 25113, 264, 1785, 16143, 11, 321, 2845, 257, 3636], "temperature": 0.0, "avg_logprob": -0.14035062988599142, "compression_ratio": 1.8840579710144927, "no_speech_prob": 8.70600160851609e-06}, {"id": 176, "seek": 82396, "start": 834.6800000000001, "end": 839.32, "text": " back to the back end, that updates the state on the back end, and then that state is broadcast", "tokens": [646, 281, 264, 646, 917, 11, 300, 9205, 264, 1785, 322, 264, 646, 917, 11, 293, 550, 300, 1785, 307, 9975], "temperature": 0.0, "avg_logprob": -0.14035062988599142, "compression_ratio": 1.8840579710144927, "no_speech_prob": 8.70600160851609e-06}, {"id": 177, "seek": 82396, "start": 839.32, "end": 844.36, "text": " back to the clients, and we have the same kind of event loop that we had just on the", "tokens": [646, 281, 264, 6982, 11, 293, 321, 362, 264, 912, 733, 295, 2280, 6367, 300, 321, 632, 445, 322, 264], "temperature": 0.0, "avg_logprob": -0.14035062988599142, "compression_ratio": 1.8840579710144927, "no_speech_prob": 8.70600160851609e-06}, {"id": 178, "seek": 82396, "start": 844.36, "end": 850.9200000000001, "text": " client, but now across the network.", "tokens": [6423, 11, 457, 586, 2108, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14035062988599142, "compression_ratio": 1.8840579710144927, "no_speech_prob": 8.70600160851609e-06}, {"id": 179, "seek": 85092, "start": 850.92, "end": 856.12, "text": " Now I've waffled on for a bit, I think it would be cool to maybe see a demo.", "tokens": [823, 286, 600, 261, 2518, 1493, 322, 337, 257, 857, 11, 286, 519, 309, 576, 312, 1627, 281, 1310, 536, 257, 10723, 13], "temperature": 0.0, "avg_logprob": -0.3674674400916466, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0016965370159596205}, {"id": 180, "seek": 85092, "start": 856.12, "end": 858.3199999999999, "text": " I'm not sure we can get the sound.", "tokens": [286, 478, 406, 988, 321, 393, 483, 264, 1626, 13], "temperature": 0.0, "avg_logprob": -0.3674674400916466, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0016965370159596205}, {"id": 181, "seek": 85092, "start": 858.3199999999999, "end": 864.68, "text": " I'm going to check the sound of the video guys, let's try to do what you want to do.", "tokens": [286, 478, 516, 281, 1520, 264, 1626, 295, 264, 960, 1074, 11, 718, 311, 853, 281, 360, 437, 291, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.3674674400916466, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0016965370159596205}, {"id": 182, "seek": 85092, "start": 864.68, "end": 865.68, "text": " What would you like me to do?", "tokens": [708, 576, 291, 411, 385, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.3674674400916466, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0016965370159596205}, {"id": 183, "seek": 85092, "start": 865.68, "end": 877.92, "text": " I'll try to play audio, and I will see if I can.", "tokens": [286, 603, 853, 281, 862, 6278, 11, 293, 286, 486, 536, 498, 286, 393, 13], "temperature": 0.0, "avg_logprob": -0.3674674400916466, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.0016965370159596205}, {"id": 184, "seek": 87792, "start": 877.92, "end": 884.5999999999999, "text": " Yeah, we are trying to play audio with the mini jack.", "tokens": [865, 11, 321, 366, 1382, 281, 862, 6278, 365, 264, 8382, 7109, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 185, "seek": 87792, "start": 884.5999999999999, "end": 888.04, "text": " I can just play out the speaker, it's fine.", "tokens": [286, 393, 445, 862, 484, 264, 8145, 11, 309, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 186, "seek": 87792, "start": 888.04, "end": 890.24, "text": " It's not a very big room.", "tokens": [467, 311, 406, 257, 588, 955, 1808, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 187, "seek": 87792, "start": 890.24, "end": 893.24, "text": " The mini jack audio is not coming off.", "tokens": [440, 8382, 7109, 6278, 307, 406, 1348, 766, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 188, "seek": 87792, "start": 893.24, "end": 898.0, "text": " Okay, well while they're dealing with that, I'll just explain what's happening, I think", "tokens": [1033, 11, 731, 1339, 436, 434, 6260, 365, 300, 11, 286, 603, 445, 2903, 437, 311, 2737, 11, 286, 519], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 189, "seek": 87792, "start": 898.0, "end": 899.0, "text": " it's kind of clear.", "tokens": [309, 311, 733, 295, 1850, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 190, "seek": 87792, "start": 899.0, "end": 902.9599999999999, "text": " So we have two clients open here.", "tokens": [407, 321, 362, 732, 6982, 1269, 510, 13], "temperature": 0.0, "avg_logprob": -0.30573382584945014, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.0008147623739205301}, {"id": 191, "seek": 90296, "start": 902.96, "end": 913.36, "text": " Okay, that's important, no problem.", "tokens": [1033, 11, 300, 311, 1021, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 192, "seek": 90296, "start": 913.36, "end": 920.52, "text": " Maybe it was me that was having no sound.", "tokens": [2704, 309, 390, 385, 300, 390, 1419, 572, 1626, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 193, "seek": 90296, "start": 920.52, "end": 928.32, "text": " If it was muted, maybe it was the plug in there, let's try.", "tokens": [759, 309, 390, 32808, 11, 1310, 309, 390, 264, 5452, 294, 456, 11, 718, 311, 853, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 194, "seek": 90296, "start": 928.32, "end": 929.32, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 195, "seek": 90296, "start": 929.32, "end": 930.32, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 196, "seek": 90296, "start": 930.32, "end": 932.44, "text": " It wasn't user error, it was okay.", "tokens": [467, 2067, 380, 4195, 6713, 11, 309, 390, 1392, 13], "temperature": 0.0, "avg_logprob": -0.5894973061301492, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.001547328196465969}, {"id": 197, "seek": 93244, "start": 932.44, "end": 938.32, "text": " So we have two instances going on here, for some reason that one isn't going, there we", "tokens": [407, 321, 362, 732, 14519, 516, 322, 510, 11, 337, 512, 1778, 300, 472, 1943, 380, 516, 11, 456, 321], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 198, "seek": 93244, "start": 938.32, "end": 939.32, "text": " go.", "tokens": [352, 13], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 199, "seek": 93244, "start": 939.32, "end": 944.6, "text": " So I can change the parameters on this side, you can see they're reflected on the other,", "tokens": [407, 286, 393, 1319, 264, 9834, 322, 341, 1252, 11, 291, 393, 536, 436, 434, 15502, 322, 264, 661, 11], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 200, "seek": 93244, "start": 944.6, "end": 948.9200000000001, "text": " add steps or whatever.", "tokens": [909, 4439, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 201, "seek": 93244, "start": 948.9200000000001, "end": 956.7600000000001, "text": " Yes, and so this is all totally networked, conceptually you could run this on the web", "tokens": [1079, 11, 293, 370, 341, 307, 439, 3879, 3209, 292, 11, 3410, 671, 291, 727, 1190, 341, 322, 264, 3670], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 202, "seek": 93244, "start": 956.7600000000001, "end": 961.4000000000001, "text": " and have, I mean this is just running locally but I would have hoped that people could open", "tokens": [293, 362, 11, 286, 914, 341, 307, 445, 2614, 16143, 457, 286, 576, 362, 19737, 300, 561, 727, 1269], "temperature": 0.0, "avg_logprob": -0.26804107666015625, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00033055487438105047}, {"id": 203, "seek": 96140, "start": 961.4, "end": 964.6, "text": " up here.", "tokens": [493, 510, 13], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 204, "seek": 96140, "start": 964.6, "end": 970.36, "text": " So just a recap, we've got a full stack GLEAM app, we have an ATP server on the back end,", "tokens": [407, 445, 257, 20928, 11, 321, 600, 658, 257, 1577, 8630, 460, 2634, 2865, 724, 11, 321, 362, 364, 39202, 7154, 322, 264, 646, 917, 11], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 205, "seek": 96140, "start": 970.36, "end": 975.8, "text": " we have a React app on the front end, both written in pure GLEAM, both sharing types,", "tokens": [321, 362, 257, 30644, 724, 322, 264, 1868, 917, 11, 1293, 3720, 294, 6075, 460, 2634, 2865, 11, 1293, 5414, 3467, 11], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 206, "seek": 96140, "start": 975.8, "end": 982.64, "text": " and we have this live view style of communication, but specifically or kind of crucially, this", "tokens": [293, 321, 362, 341, 1621, 1910, 3758, 295, 6101, 11, 457, 4682, 420, 733, 295, 5140, 1909, 11, 341], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 207, "seek": 96140, "start": 982.64, "end": 985.6, "text": " communication is well typed and so we know all the messages that we're supposed to be", "tokens": [6101, 307, 731, 33941, 293, 370, 321, 458, 439, 264, 7897, 300, 321, 434, 3442, 281, 312], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 208, "seek": 96140, "start": 985.6, "end": 990.52, "text": " handling on both the front end and the back end.", "tokens": [13175, 322, 1293, 264, 1868, 917, 293, 264, 646, 917, 13], "temperature": 0.0, "avg_logprob": -0.1982013327734811, "compression_ratio": 1.6967213114754098, "no_speech_prob": 8.287636774184648e-06}, {"id": 209, "seek": 99052, "start": 990.52, "end": 996.24, "text": " And this is just a quick kind of look at how many lines of code we're in this code base,", "tokens": [400, 341, 307, 445, 257, 1702, 733, 295, 574, 412, 577, 867, 3876, 295, 3089, 321, 434, 294, 341, 3089, 3096, 11], "temperature": 0.0, "avg_logprob": -0.2211026570883142, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.056147685740143e-05}, {"id": 210, "seek": 99052, "start": 996.24, "end": 1001.76, "text": " and so you can see 85 lines of JavaScript was all that was needed and everything else", "tokens": [293, 370, 291, 393, 536, 14695, 3876, 295, 15778, 390, 439, 300, 390, 2978, 293, 1203, 1646], "temperature": 0.0, "avg_logprob": -0.2211026570883142, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.056147685740143e-05}, {"id": 211, "seek": 99052, "start": 1001.76, "end": 1002.76, "text": " is pure GLEAM.", "tokens": [307, 6075, 460, 2634, 2865, 13], "temperature": 0.0, "avg_logprob": -0.2211026570883142, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.056147685740143e-05}, {"id": 212, "seek": 99052, "start": 1002.76, "end": 1008.04, "text": " Which I think is pretty cool, it's pretty exciting that you can do that today.", "tokens": [3013, 286, 519, 307, 1238, 1627, 11, 309, 311, 1238, 4670, 300, 291, 393, 360, 300, 965, 13], "temperature": 0.0, "avg_logprob": -0.2211026570883142, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.056147685740143e-05}, {"id": 213, "seek": 99052, "start": 1008.04, "end": 1018.1999999999999, "text": " So yeah, thank you for listening.", "tokens": [407, 1338, 11, 1309, 291, 337, 4764, 13], "temperature": 0.0, "avg_logprob": -0.2211026570883142, "compression_ratio": 1.4876847290640394, "no_speech_prob": 6.056147685740143e-05}, {"id": 214, "seek": 101820, "start": 1018.2, "end": 1028.72, "text": " Thank you, are there any questions, yep.", "tokens": [1044, 291, 11, 366, 456, 604, 1651, 11, 18633, 13], "temperature": 0.0, "avg_logprob": -0.3451300007956369, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0009964845376089215}, {"id": 215, "seek": 101820, "start": 1028.72, "end": 1035.2, "text": " Thank you for sharing, maybe it was apparent from your presentation but I just wanted to", "tokens": [1044, 291, 337, 5414, 11, 1310, 309, 390, 18335, 490, 428, 5860, 457, 286, 445, 1415, 281], "temperature": 0.0, "avg_logprob": -0.3451300007956369, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0009964845376089215}, {"id": 216, "seek": 101820, "start": 1035.2, "end": 1039.52, "text": " check how are the different clients synchronized.", "tokens": [1520, 577, 366, 264, 819, 6982, 19331, 1602, 13], "temperature": 0.0, "avg_logprob": -0.3451300007956369, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0009964845376089215}, {"id": 217, "seek": 101820, "start": 1039.52, "end": 1047.44, "text": " Yeah, okay, so let me go back.", "tokens": [865, 11, 1392, 11, 370, 718, 385, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.3451300007956369, "compression_ratio": 1.381578947368421, "no_speech_prob": 0.0009964845376089215}, {"id": 218, "seek": 104744, "start": 1047.44, "end": 1054.3600000000001, "text": " We had this model and when I introduced that each client had their own model and so basically", "tokens": [492, 632, 341, 2316, 293, 562, 286, 7268, 300, 1184, 6423, 632, 641, 1065, 2316, 293, 370, 1936], "temperature": 0.0, "avg_logprob": -0.1450853236885958, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0002553427475504577}, {"id": 219, "seek": 104744, "start": 1054.3600000000001, "end": 1060.88, "text": " the server has its own version of this now and it's broadcasting, every time the sequence", "tokens": [264, 7154, 575, 1080, 1065, 3037, 295, 341, 586, 293, 309, 311, 30024, 11, 633, 565, 264, 8310], "temperature": 0.0, "avg_logprob": -0.1450853236885958, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0002553427475504577}, {"id": 220, "seek": 104744, "start": 1060.88, "end": 1066.6000000000001, "text": " resets, it broadcasts the entire model to make sure everything stays in sync and then", "tokens": [725, 1385, 11, 309, 9975, 82, 264, 2302, 2316, 281, 652, 988, 1203, 10834, 294, 20271, 293, 550], "temperature": 0.0, "avg_logprob": -0.1450853236885958, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0002553427475504577}, {"id": 221, "seek": 104744, "start": 1066.6000000000001, "end": 1070.68, "text": " whenever one client changes something it broadcasts a message to tell the client to update their", "tokens": [5699, 472, 6423, 2962, 746, 309, 9975, 82, 257, 3636, 281, 980, 264, 6423, 281, 5623, 641], "temperature": 0.0, "avg_logprob": -0.1450853236885958, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0002553427475504577}, {"id": 222, "seek": 104744, "start": 1070.68, "end": 1072.56, "text": " local version.", "tokens": [2654, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1450853236885958, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0002553427475504577}, {"id": 223, "seek": 107256, "start": 1072.56, "end": 1081.32, "text": " So it depends on how the client gets this new information and that's more or less okay", "tokens": [407, 309, 5946, 322, 577, 264, 6423, 2170, 341, 777, 1589, 293, 300, 311, 544, 420, 1570, 1392], "temperature": 0.0, "avg_logprob": -0.2990188846340427, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0002279289619764313}, {"id": 224, "seek": 107256, "start": 1081.32, "end": 1084.32, "text": " enough for synchronization.", "tokens": [1547, 337, 19331, 2144, 13], "temperature": 0.0, "avg_logprob": -0.2990188846340427, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0002279289619764313}, {"id": 225, "seek": 107256, "start": 1084.32, "end": 1088.96, "text": " Yeah it seems to be kind of fine, I guess if one person is in Australia and one is over", "tokens": [865, 309, 2544, 281, 312, 733, 295, 2489, 11, 286, 2041, 498, 472, 954, 307, 294, 7060, 293, 472, 307, 670], "temperature": 0.0, "avg_logprob": -0.2990188846340427, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0002279289619764313}, {"id": 226, "seek": 107256, "start": 1088.96, "end": 1094.2, "text": " here there's going to be some noticeable ping but then you wouldn't be stupid enough to", "tokens": [510, 456, 311, 516, 281, 312, 512, 26041, 26151, 457, 550, 291, 2759, 380, 312, 6631, 1547, 281], "temperature": 0.0, "avg_logprob": -0.2990188846340427, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0002279289619764313}, {"id": 227, "seek": 107256, "start": 1094.2, "end": 1096.1599999999999, "text": " do that.", "tokens": [360, 300, 13], "temperature": 0.0, "avg_logprob": -0.2990188846340427, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.0002279289619764313}, {"id": 228, "seek": 109616, "start": 1096.16, "end": 1106.96, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.30893507669138354, "compression_ratio": 1.206896551724138, "no_speech_prob": 0.00013951834989711642}, {"id": 229, "seek": 109616, "start": 1106.96, "end": 1115.52, "text": " So I don't know much about the Gleam front end stuff, what was necessary to write in", "tokens": [407, 286, 500, 380, 458, 709, 466, 264, 460, 306, 335, 1868, 917, 1507, 11, 437, 390, 4818, 281, 2464, 294], "temperature": 0.0, "avg_logprob": -0.30893507669138354, "compression_ratio": 1.206896551724138, "no_speech_prob": 0.00013951834989711642}, {"id": 230, "seek": 109616, "start": 1115.52, "end": 1120.48, "text": " JavaScript that you couldn't write in Gleam?", "tokens": [15778, 300, 291, 2809, 380, 2464, 294, 460, 306, 335, 30], "temperature": 0.0, "avg_logprob": -0.30893507669138354, "compression_ratio": 1.206896551724138, "no_speech_prob": 0.00013951834989711642}, {"id": 231, "seek": 112048, "start": 1120.48, "end": 1129.76, "text": " Yeah, the JavaScript is just the part that actually renders the Web Audio stuff.", "tokens": [865, 11, 264, 15778, 307, 445, 264, 644, 300, 767, 6125, 433, 264, 9573, 25706, 1507, 13], "temperature": 0.0, "avg_logprob": -0.24145289829799108, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.00011306893429718912}, {"id": 232, "seek": 112048, "start": 1129.76, "end": 1133.8, "text": " So that's the APIs that are available in Gleam?", "tokens": [407, 300, 311, 264, 21445, 300, 366, 2435, 294, 460, 306, 335, 30], "temperature": 0.0, "avg_logprob": -0.24145289829799108, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.00011306893429718912}, {"id": 233, "seek": 112048, "start": 1133.8, "end": 1140.96, "text": " Well so Gleam doesn't really have any browser API bindings at the moment, I could have FFI'd", "tokens": [1042, 370, 460, 306, 335, 1177, 380, 534, 362, 604, 11185, 9362, 14786, 1109, 412, 264, 1623, 11, 286, 727, 362, 479, 38568, 1116], "temperature": 0.0, "avg_logprob": -0.24145289829799108, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.00011306893429718912}, {"id": 234, "seek": 112048, "start": 1140.96, "end": 1146.84, "text": " the whole thing and probably taken a bit more into Gleam but for that particular bit I've", "tokens": [264, 1379, 551, 293, 1391, 2726, 257, 857, 544, 666, 460, 306, 335, 457, 337, 300, 1729, 857, 286, 600], "temperature": 0.0, "avg_logprob": -0.24145289829799108, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.00011306893429718912}, {"id": 235, "seek": 114684, "start": 1146.84, "end": 1152.52, "text": " done that JavaScript myself quite a few times and so it was just quicker to just keep that", "tokens": [1096, 300, 15778, 2059, 1596, 257, 1326, 1413, 293, 370, 309, 390, 445, 16255, 281, 445, 1066, 300], "temperature": 0.0, "avg_logprob": -0.29843162354968844, "compression_ratio": 1.2916666666666667, "no_speech_prob": 0.0002242412301711738}, {"id": 236, "seek": 114684, "start": 1152.52, "end": 1154.28, "text": " little bit in JavaScript.", "tokens": [707, 857, 294, 15778, 13], "temperature": 0.0, "avg_logprob": -0.29843162354968844, "compression_ratio": 1.2916666666666667, "no_speech_prob": 0.0002242412301711738}, {"id": 237, "seek": 114684, "start": 1154.28, "end": 1158.9199999999998, "text": " Thank you, thanks.", "tokens": [1044, 291, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.29843162354968844, "compression_ratio": 1.2916666666666667, "no_speech_prob": 0.0002242412301711738}, {"id": 238, "seek": 114684, "start": 1158.9199999999998, "end": 1168.76, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.29843162354968844, "compression_ratio": 1.2916666666666667, "no_speech_prob": 0.0002242412301711738}, {"id": 239, "seek": 116876, "start": 1168.76, "end": 1178.04, "text": " In the beginning you presented an API for connecting audio nodes by using nesting, my", "tokens": [682, 264, 2863, 291, 8212, 364, 9362, 337, 11015, 6278, 13891, 538, 1228, 297, 8714, 11, 452], "temperature": 0.0, "avg_logprob": -0.2892393480267441, "compression_ratio": 1.3580246913580247, "no_speech_prob": 0.00023713857808616012}, {"id": 240, "seek": 116876, "start": 1178.04, "end": 1185.32, "text": " question is how would that work with more complex graphs that have forks and merges", "tokens": [1168, 307, 577, 576, 300, 589, 365, 544, 3997, 24877, 300, 362, 337, 1694, 293, 3551, 2880], "temperature": 0.0, "avg_logprob": -0.2892393480267441, "compression_ratio": 1.3580246913580247, "no_speech_prob": 0.00023713857808616012}, {"id": 241, "seek": 116876, "start": 1185.32, "end": 1190.8799999999999, "text": " or feedbacks?", "tokens": [420, 5824, 82, 30], "temperature": 0.0, "avg_logprob": -0.2892393480267441, "compression_ratio": 1.3580246913580247, "no_speech_prob": 0.00023713857808616012}, {"id": 242, "seek": 116876, "start": 1190.8799999999999, "end": 1194.04, "text": " So you're talking about this, right?", "tokens": [407, 291, 434, 1417, 466, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2892393480267441, "compression_ratio": 1.3580246913580247, "no_speech_prob": 0.00023713857808616012}, {"id": 243, "seek": 119404, "start": 1194.04, "end": 1200.44, "text": " Yeah, actually presented a kind of Striptown version of the actual API and there we have", "tokens": [865, 11, 767, 8212, 257, 733, 295, 20390, 662, 648, 3037, 295, 264, 3539, 9362, 293, 456, 321, 362], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 244, "seek": 119404, "start": 1200.44, "end": 1205.56, "text": " like keyed nodes so you can assign like an ID to a node and then there's like Reth nodes", "tokens": [411, 2141, 292, 13891, 370, 291, 393, 6269, 411, 364, 7348, 281, 257, 9984, 293, 550, 456, 311, 411, 497, 3293, 13891], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 245, "seek": 119404, "start": 1205.56, "end": 1211.0, "text": " as well so you can refer to other nodes in the graph outside of the tree and so that", "tokens": [382, 731, 370, 291, 393, 2864, 281, 661, 13891, 294, 264, 4295, 2380, 295, 264, 4230, 293, 370, 300], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 246, "seek": 119404, "start": 1211.0, "end": 1216.28, "text": " way you can keep this kind of tree-like structure but jump out and refer to anything you want", "tokens": [636, 291, 393, 1066, 341, 733, 295, 4230, 12, 4092, 3877, 457, 3012, 484, 293, 2864, 281, 1340, 291, 528], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 247, "seek": 119404, "start": 1216.28, "end": 1218.6, "text": " and have loops or whatever.", "tokens": [293, 362, 16121, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 248, "seek": 119404, "start": 1218.6, "end": 1222.32, "text": " And so actually that's what's happening in this app so we've got that delay that's going", "tokens": [400, 370, 767, 300, 311, 437, 311, 2737, 294, 341, 724, 370, 321, 600, 658, 300, 8577, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.16438908576965333, "compression_ratio": 1.8192307692307692, "no_speech_prob": 4.0113794966600835e-05}, {"id": 249, "seek": 122232, "start": 1222.32, "end": 1227.1599999999999, "text": " on in the background and that's the feedback loop and then it's going, yeah, does that", "tokens": [322, 294, 264, 3678, 293, 300, 311, 264, 5824, 6367, 293, 550, 309, 311, 516, 11, 1338, 11, 775, 300], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 250, "seek": 122232, "start": 1227.1599999999999, "end": 1228.1599999999999, "text": " make sense?", "tokens": [652, 2020, 30], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 251, "seek": 122232, "start": 1228.1599999999999, "end": 1229.1599999999999, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 252, "seek": 122232, "start": 1229.1599999999999, "end": 1232.1599999999999, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 253, "seek": 122232, "start": 1232.1599999999999, "end": 1241.76, "text": " Hello, sorry, I didn't see the full presentation, I arrived in the middle and maybe I will ask", "tokens": [2425, 11, 2597, 11, 286, 994, 380, 536, 264, 1577, 5860, 11, 286, 6678, 294, 264, 2808, 293, 1310, 286, 486, 1029], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 254, "seek": 122232, "start": 1241.76, "end": 1248.8, "text": " something that you already shared but I would like to know if can we apply this environment", "tokens": [746, 300, 291, 1217, 5507, 457, 286, 576, 411, 281, 458, 498, 393, 321, 3079, 341, 2823], "temperature": 0.0, "avg_logprob": -0.24253203229206363, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.0008765356615185738}, {"id": 255, "seek": 124880, "start": 1248.8, "end": 1256.68, "text": " for live coding, improvise the performance, it's mainly dedicated for building clients", "tokens": [337, 1621, 17720, 11, 29424, 908, 264, 3389, 11, 309, 311, 8704, 8374, 337, 2390, 6982], "temperature": 0.0, "avg_logprob": -0.22215959003993443, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.00011079219984821975}, {"id": 256, "seek": 124880, "start": 1256.68, "end": 1257.68, "text": " and applications?", "tokens": [293, 5821, 30], "temperature": 0.0, "avg_logprob": -0.22215959003993443, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.00011079219984821975}, {"id": 257, "seek": 124880, "start": 1257.68, "end": 1263.56, "text": " Yeah, I think you could totally transfer these ideas to live coding or performance, I mean", "tokens": [865, 11, 286, 519, 291, 727, 3879, 5003, 613, 3487, 281, 1621, 17720, 420, 3389, 11, 286, 914], "temperature": 0.0, "avg_logprob": -0.22215959003993443, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.00011079219984821975}, {"id": 258, "seek": 124880, "start": 1263.56, "end": 1271.24, "text": " ultimately it just comes down to sending messages right and so here we're sending like user", "tokens": [6284, 309, 445, 1487, 760, 281, 7750, 7897, 558, 293, 370, 510, 321, 434, 7750, 411, 4195], "temperature": 0.0, "avg_logprob": -0.22215959003993443, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.00011079219984821975}, {"id": 259, "seek": 124880, "start": 1271.24, "end": 1278.28, "text": " interaction events but you could do conceptually the same thing with code snippets or some other", "tokens": [9285, 3931, 457, 291, 727, 360, 3410, 671, 264, 912, 551, 365, 3089, 35623, 1385, 420, 512, 661], "temperature": 0.0, "avg_logprob": -0.22215959003993443, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.00011079219984821975}, {"id": 260, "seek": 127828, "start": 1278.28, "end": 1282.72, "text": " kind of data transfer, yeah.", "tokens": [733, 295, 1412, 5003, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3526965379714966, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.000548474199604243}, {"id": 261, "seek": 127828, "start": 1282.72, "end": 1285.72, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.3526965379714966, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.000548474199604243}, {"id": 262, "seek": 127828, "start": 1285.72, "end": 1295.6399999999999, "text": " Hi, Redsorg, I was wondering you said it was compatible with React and so will it be compatible", "tokens": [2421, 11, 4477, 82, 4646, 11, 286, 390, 6359, 291, 848, 309, 390, 18218, 365, 30644, 293, 370, 486, 309, 312, 18218], "temperature": 0.0, "avg_logprob": -0.3526965379714966, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.000548474199604243}, {"id": 263, "seek": 127828, "start": 1295.6399999999999, "end": 1299.44, "text": " with other frameworks like Vue or the future?", "tokens": [365, 661, 29834, 411, 691, 622, 420, 264, 2027, 30], "temperature": 0.0, "avg_logprob": -0.3526965379714966, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.000548474199604243}, {"id": 264, "seek": 127828, "start": 1299.44, "end": 1306.2, "text": " Yeah, at the moment it's just React but it's been on my to-do list for a while now to kind", "tokens": [865, 11, 412, 264, 1623, 309, 311, 445, 30644, 457, 309, 311, 668, 322, 452, 281, 12, 2595, 1329, 337, 257, 1339, 586, 281, 733], "temperature": 0.0, "avg_logprob": -0.3526965379714966, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.000548474199604243}, {"id": 265, "seek": 130620, "start": 1306.2, "end": 1312.48, "text": " of factor out the state management that Lustre does away from the actual renderer that you", "tokens": [295, 5952, 484, 264, 1785, 4592, 300, 45834, 265, 775, 1314, 490, 264, 3539, 15529, 260, 300, 291], "temperature": 0.0, "avg_logprob": -0.2905142152464235, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006808801554143429}, {"id": 266, "seek": 130620, "start": 1312.48, "end": 1319.0800000000002, "text": " choose so right now just React, some nebulous time in the future, it could be Vue or Morphdome", "tokens": [2826, 370, 558, 586, 445, 30644, 11, 512, 408, 65, 6893, 565, 294, 264, 2027, 11, 309, 727, 312, 691, 622, 420, 5146, 950, 67, 423], "temperature": 0.0, "avg_logprob": -0.2905142152464235, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006808801554143429}, {"id": 267, "seek": 130620, "start": 1319.0800000000002, "end": 1320.48, "text": " or whatever.", "tokens": [420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.2905142152464235, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006808801554143429}, {"id": 268, "seek": 130620, "start": 1320.48, "end": 1328.48, "text": " Okay, I think there's time for one more question if there is one.", "tokens": [1033, 11, 286, 519, 456, 311, 565, 337, 472, 544, 1168, 498, 456, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.2905142152464235, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006808801554143429}, {"id": 269, "seek": 130620, "start": 1328.48, "end": 1332.0800000000002, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2905142152464235, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006808801554143429}, {"id": 270, "seek": 133208, "start": 1332.08, "end": 1338.08, "text": " Thanks for talk but if someone want to use some hardware devices to connect, does Glim", "tokens": [2561, 337, 751, 457, 498, 1580, 528, 281, 764, 512, 8837, 5759, 281, 1745, 11, 775, 460, 4197], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 271, "seek": 133208, "start": 1338.08, "end": 1344.52, "text": " support some other wrappers over Web API to speak with some hardware parts like the USB", "tokens": [1406, 512, 661, 7843, 15226, 670, 9573, 9362, 281, 1710, 365, 512, 8837, 3166, 411, 264, 10109], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 272, "seek": 133208, "start": 1344.52, "end": 1346.12, "text": " serial port, etc.?", "tokens": [17436, 2436, 11, 5183, 41401], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 273, "seek": 133208, "start": 1346.12, "end": 1347.12, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 274, "seek": 133208, "start": 1347.12, "end": 1354.76, "text": " Do you mean from the browser side or yeah, so like I said there aren't really any official", "tokens": [1144, 291, 914, 490, 264, 11185, 1252, 420, 1338, 11, 370, 411, 286, 848, 456, 3212, 380, 534, 604, 4783], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 275, "seek": 133208, "start": 1354.76, "end": 1361.56, "text": " bindings at the moment but as I also said the FFI story is very simple so it's actually", "tokens": [14786, 1109, 412, 264, 1623, 457, 382, 286, 611, 848, 264, 479, 38568, 1657, 307, 588, 2199, 370, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.255648760451484, "compression_ratio": 1.5220883534136547, "no_speech_prob": 0.0006583405192941427}, {"id": 276, "seek": 136156, "start": 1361.56, "end": 1366.3999999999999, "text": " quite easy to create bindings for these browsers yourself which is pretty much the situation", "tokens": [1596, 1858, 281, 1884, 14786, 1109, 337, 613, 36069, 1803, 597, 307, 1238, 709, 264, 2590], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 277, "seek": 136156, "start": 1366.3999999999999, "end": 1367.3999999999999, "text": " where we're at today.", "tokens": [689, 321, 434, 412, 965, 13], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 278, "seek": 136156, "start": 1367.3999999999999, "end": 1372.84, "text": " I mean the biggest thing maybe just holding Glim back at the moment is the ecosystem is", "tokens": [286, 914, 264, 3880, 551, 1310, 445, 5061, 460, 4197, 646, 412, 264, 1623, 307, 264, 11311, 307], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 279, "seek": 136156, "start": 1372.84, "end": 1378.6, "text": " just very, very young and so we don't have many packages or bindings for a lot of stuff.", "tokens": [445, 588, 11, 588, 2037, 293, 370, 321, 500, 380, 362, 867, 17401, 420, 14786, 1109, 337, 257, 688, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 280, "seek": 136156, "start": 1378.6, "end": 1381.44, "text": " Okay, thank you again for your talk.", "tokens": [1033, 11, 1309, 291, 797, 337, 428, 751, 13], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 281, "seek": 136156, "start": 1381.44, "end": 1382.44, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.24969599463722922, "compression_ratio": 1.555045871559633, "no_speech_prob": 4.1285366023657843e-05}, {"id": 282, "seek": 138244, "start": 1382.44, "end": 1392.44, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.8522924582163492, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0007643129792995751}], "language": "en"}