{"text": " Hi, everybody. Thanks to be here for this talk. That's a lot of people. I'm Nicolas Frankel. I've been a developer for a long time, and I would like to ask how many of you are developers in this room? Quite a lot. Who are ops? Just as many, and who are devops, whatever you mean by it. So this talk is intended for actually developers, because I was, or I still think I'm a developer. So if you are an ops people, and for this, for you is not that super interesting. At least you can direct your developer colleagues to the talk, so that you can understand how they can ease your work. Well, perhaps you've never seen that, but I'm old or experienced, depending on how you see it. And when I was starting my career, monitoring was like a bunch of people sitting in front of screens the whole day. And actually, I was lucky. Once in the south of France, I was told, hey, this is the biggest monitoring site of all France. And actually, it really looked like this. And of course, there were people watching it. And that was the easy way. Now, I hope that you don't have that anymore, that it has become a bit more modern. Actually, there is a lot of talk now about microservices, right? Who here is doing microservices? Yeah. Yeah, because if you don't do microservices, you are not a real developer. But even if you don't do microservices, so you are not a real developer, and I encourage you not to be a real developer, in that case, you probably are doing some kind of distributed work. It's become increasingly difficult to just handle everything locally. And the problem becomes, yeah, if something bad happens, how can you locate how it works? Or even if something works as expected, how you can understand the flow of your request across the network. I love Wikipedia. And here is the observability definition by Wikipedia, which is long and in that case, not that interesting. So I have a better one afterwards for tracing. So basically, tracing helps you to understand the flow of a business request across all your components. Fabian, where is Fabian? Fabian is here, so he talked a lot about the metrics and the logging. So in this talk, I will really focus on tracing because my opinion is that, well, metrics is easy. We do metrics since ages, like we take the CPU, the memory, whatever. Now we are trying to get more business-related metrics, but it's still the same concept. Logging also. Now we do aggregated logging. Again, nothing mind-blowing. Tracing is, I think, the hardest part. So in the past, there were already some tracing pioneers. Perhaps you've used some of them. And well, now we are at the stage where we want to have something more standardized. So it starts with the trace context from the W3C. And the idea is that you start a trace and then other components will get the trace and will append their own trace to it. So it works very well in a web context. And it defines two important concepts that Fabian thanks already described. So now I am done. So I have the same stupid stuff. So here you have, oh, sorry. Yes. It reminds me of the story. I did the same to my colleagues. They didn't care about the presentation. They only remember that. Okay. So here you have a trace and here you have the different spans. So here the X1 is the parent one. And then the Y and the Z1 will take this X span as their parent span. And so this is a single trace. This is a single request across your service. Web stuff is good, but it's definitely not enough. And so for that we have the open telemetry stuff. Open telemetry is just a big bag of miracles all set into a specific project. So it's basically APIs, SDK, tools, whatever under the open telemetry level. It implements the W3C trace context. If you have been doing some kind of tracing before, you might know it because it's like the merging of open tracing and open sensors. Good thing is a CNCF project. So basically there is some hope that it will last for a couple of years. The architecture is pretty simple. Basically you've got sources, you've got the open telemetry protocol, and as Fabian mentioned, you dump everything into a collector. Collector, we should be as close as possible to your sources. And then some tools are able to read like data from it and to display it into the way that we expect to see it. What happens after the open telemetry collector is not a problem of open telemetry. Just they are collectors that are compatible, and for example you can use Yeager or Zipkin in a way that allows you to dump your data, your open telemetry data into Yeager or Zipkin into the open telemetry format. So you can reuse, and that is very important, you can reuse your infrastructure if you're already using the tools, but just switching to open telemetry. And then you are like you are using a standard, and then you can switch your open telemetry back end with less issues. Now comes the fun developer part. If you are a developer, you probably are lazy. I know, I'm a developer. So the idea is open telemetry should make your life as a developer as easy as possible to help your ops colleague, like diagnose your problems. And the easiest part if you do auto instrumentation. Auto instrumentation is only possible in cases where you have a platform, when you have a run time. Fabian mentioned Java, Java as a run time, which is the Jivem. Python as a run time. Now if you have rusts, it's not as easy. So in that case, you are stuck. My advice if you are using a run time, and probably most of you are using such run times, whether Java, whatever, use it. It's basically free. It's a low hanging fruit, and there is no coupling. So basically you don't need extra dependencies as developers in your projects. So since it's called practical introduction, let's do some practice. So here I have a bit better than the hello world, so I have tried to model like an e-commerce shop with very simple stuff. It starts just asking for products. I will go through an API gateway which will forward the product to the catalog, and the catalog doesn't know about the prices, so it will ask the prices from the pricing service, and it will ask the stocks from the stock service. The intra point is the most important thing, because it gives the parent's phrase. Everything will be from that. So in general, you have a reverse proxy or an API gateway, depending on your use case. I work on the Apache API 6 project. It uses the NGINX reverse proxy. On top you have an open resty, because you want to have Lua to script and to auto reload the configuration. Then you have lots of out of the box plugins. Let's see how it works. Now I have the code here. Is it the begin off? Good. So I might be very old, because for me it wouldn't. Okay, here that's my architecture. I'm using Docker compose, because I'm super lazy. I don't want to use Kubernetes, so I have Yeager. As I mentioned, I have all in one. I'm using the all included, so I don't need to think about having the telemetry collector and the web to check the traces. I have only one single image. Then I have API 6. Then I have the catalog, which I showed you. Of course I have a couple of variables to configure everything. I wanted to focus on tracing, so no metrics, no logs. I'm sending everything to Yeager, and then I do the same for pricing, and I do the same for the stock. And normally at this point, I already started, because in general I have issues with the Java stuff. So here I'm doing a simple curl to the product. I've got the data, which is not that important. And I can check on the web app how it works. So here I will go on the Yeager UI. I see all my services. I can find the traces. Here you can find the latest one. And here is the thing. If I click on it, it might be a bit small, right? I cannot do much better. You can already see everything that I've shown you. So I start with the product from the API gateway. It forwards it to the product to the catalog. Then I have the internal calls, and I will show you how it works. Then I have the get request made from inside the application. And then I have the stocks that responds here. Same here. And here we see something that was not mentioned on the component diagram. From the catalog to the stock, I go directly. But from the catalog to the pricing, I go back to the API gateway, which is also a way to do that for whatever reason. And so this is something that was not mentioned on the PDF, but you cannot cheat with open telemetry. It tells you exactly what happens and the flow. And the rest is the same. So regarding the code itself, I told you that I don't want anything to trouble the developer. So here I have nothing regarding open telemetry. If I write hotel, you see nothing. If I write telemetry, you see nothing. I have no dependency. The only thing that I have is my Docker file, and in my Docker file, I get the latest open telemetry agents. So you can have your developers completely oblivious, and you just provide them with this snippet, and then when you run the Java application, you just tell them, A, run with the Java agent. Low-hanging fruits, zero trouble. Any Java developer here? Not that many. Python? OK, so it will be Python. Just the same here. Here it's a bit different. I add dependencies, but actually I do nothing on it. So here I have no dependency on anything. Here I'm using a SQL database because, again, I'm lazy. I don't care that much. But here I have no dependency, no API call to open telemetry. The only thing that I have is in the Docker file again. I have this. Again, I'm using a runtime. It's super easy. I let the runtime, like, intercept the calls and everything to open telemetry. And the last fun stuff is Rust. Any Rust developer? Please don't look at my code too much. I'm not a Rust developer, so I hope it won't be too horrible. And Rust is actually, well, not that standardized. So here I don't have any runtime, so I need to make the calls by myself. The hardest part is to find which library to use, depending on which framework to use. So in this case, I found one, and perhaps there are better options. But I found this open telemetry OLTP stuff. And here this is because I'm using XM. I'm using this library. And so far, it works for me. I don't need to do a lot of stuff. I just, like, copy pasted this stuff. Copy past developer. And afterwards, in my main function, I just need to say this and this. So I added two layers. So if you don't have any platform, any runtime, you actually need your developers to care about open telemetry. Otherwise, it's fine. Now, we already have pretty good, like, results, but we want to do better. So we can also ask the developers, once they are more comfortable, to do manual instrumentation even in the case when there is a platform. Now, I will docker compose down. And it takes a bit of time. I will prepare this. And on the catalog sides, now I can have some additional codes. So this is a Spring Boot application. What I can do is add annotations. Like, I noticed there were a couple of Java developers. So it's the same with Kotlin. It's still on the JVM. So basically, I'm adding annotations. And because Spring Boot can read the annotation at runtime, it can add those calls. So I don't have to call the API explicitly. I just add some annotation, and it should be done. On the Python side, I import this trace stuff, and then I can, with the tracer, add some, again, explicit traces, so internal traces. And from the first point of view, because I already, like, did it explicitly work. And now you can see that I am in deep trouble, because it happened a lot of time. The Java application doesn't start for a demo, and that's really, really fun. So I will try to docker compose down the catalog. And docker compose, hey, what happens? Dash? Are you sure? No, no, no, no, no, no, no. Not with the new versions. Yes. That's fine. We are only here to learn. What? Stop. Thanks. The stress, the stress. Yeah. Honestly, if there is any, like, person here able to tell me why this Java application sometimes has issues starting because I've added one gig at the beginning, and it's stuck always here. So I can tell you what you should see normally. If I'm lucky, I made a screenshot. Yes, here, but it's the beginning, it's the rust one. So here, this is what you can have in Python. This is what I added explicitly. I have five minutes. Well, if the demo doesn't work, it will be much better. Then I won't have any problems with the timing. Here, you can see that this is the trace that, yeah, this is a trace that I added manually in Python. And here we can see that I filled the ID with the value. And on the Java sides, again, nope, nope. I think it will be here. This is not the manual stuff that I added. Yes, it is, you have the fetch here. You have the fetch here. So this is the span that I added manually. I'm afraid that at this point, the demo just refused working. Yes, it's still stuck. I will stop there. I won't humiliate myself further when it's done. It's done. Perhaps, if you are interested, you can follow me on Twitter. You can follow me on MasterDone. I don't know what's the ratio. More importantly, if you are interested about the GitHub repo, to do that by yourself, perhaps with better configuration of the code compose with the right memory, it would work. And though the talk was not about Apache API 6, well, have a look at Apache API 6. It's an API get away, the Apache way. Great. Are there some questions now? I never got so many uploads with a filling demo. Please remain seated so we can have a Q&A. Who had a question? Thank you. Very good talk. I have two questions. So one is about this. Let's start with the first one. Right. Yes, yes, yes. How much overhead does this bring in Python and Java or Rust? How heavy is this instrumentation? That's a very good question. And the overheads of each request depends on your own infrastructure. But I always have an answer to that. Is it better to go fast and you don't know where you are going to go a bit slower and to know where you are going? I think that whatever the cost, it's always easy to add additional resources and it doesn't cost you that much. Whereas a debug incident across a distributed system can cost you days or even like weeks in injuring costs. And you are very, very expensive, right? Okay. Thank you. And the second one is have you encountered any funny issues with multi-threading or multi-processing? Something like when your server just now... Can you come closer to your... Your server just now was not starting. So some software, when you have multi-threading or multi-processing and have you encountered any issues when the instrumentation costs you trouble? This is not production stuff. This is just better than the hello world. So I cannot tell you about prediction issues. You should find people who have these issues. As I mentioned, it's a developers-oriented talk. So it's more about pushing the developers to help up to their job. For production issues, I must admit I have no clue. Hi. In the case of runtime, does it always work with also badly written application? I mean, how bad can an application be before it stops working? I'm not sure. I understood the question. So how often do you need to do it before it stops working? No, no. I mean, let's say I use deprecated libraries, bad clients, something that doesn't work as it's supposed to be for the instrumentation perspective. I mean, I do request to the network using UDP clients, something I've written myself, some custom stuff that... I'm imagining that the instrumentation sits between some layer of the network, which is going to the Internet, for example. And so how bad can I be before it stops recognizing a request from junk? You cannot be banned. OK. Well, it's a moral issue first. But then on the platform side, the Austo instrumentation, they work with specific frameworks and tools. It's those frameworks and tools that know how to check what happens and to send the data to open telemetry. So if you don't play in this game, nothing will be sent. On the manual instrumentation side, it's an explicit call. So it depends what you want to send. Yeah. I was thinking of auto instrumentation. So let's say I do the NS resolution by myself and then I just throw a request to an IP. Let me show the Python stuff here. This is what I showed you in the screenshot. This is what I write. And this is the attributes that I want to have. So basically, if here you have something that is completely unrelated, it's up to you. That's why it's easier to start with auto instrumentation. And then once you get a general overview of what you have and your app starts saying, hey, perhaps we want to have more details here, then you can come with manual instrumentation. But start with the less expensive stuff. I didn't really answer the question. I understand it. But that's the best I can do regarding it. Sorry. Okay. Thanks for the talk. For the agent you use in the Docker file, how you can configure it, for example, for the tracing for Jagger or other stuff. Regarding the Docker file, sorry? Yeah. How you can configure the agent to send the tracing for Jagger or other stuff. The Docker file doesn't mention where you send it. The Docker file just says, hey, I will use open telemetry. And it's during configuration, it's like in the Docker Compulse file where I'm using, like, agreed upon environment variables where I'm saying you should set it here or here or you should use logging or tracing or metrics or whatever. So that's very important to, like, separate those concerns. On one side in the Docker file in the image, you say, hey, I'm ready for open telemetry. And when you actually deploy it to say, okay, open telemetry will go there for the metrics and there for the tracing and for logging, I will disable it or whatever. Thank you for... Oh, sorry. Sorry. Go ahead. Sorry. And then you have a Docker image that can be, like, reusable. Thank you for being good first-time citizens to remain seated. Next question. Thank you for your presentation. So my question is does open telemetry support error handling like sentry? If not, is there any plans to do that? It's really useful to catch crashes and capture the context of the crash. So that's it. Thank you. If it happens, when you mean crashes of open telemetry itself or of the components that are, like, under watch? Yeah, of the application that's monitored, yeah. Well, Fabian showed you how you could log and, like, bind your traces and your logs. So you could have both here. My focus was just on tracing, but you can reuse the same Docker, the same GitHub repo and just, like, here, put the logs somewhere in, I don't know, Elasticsearch or whatever. No, because it's not a sponsored room. And then you can check and you introduce some errors and then you can check how the two are bound and you can, like, drill down to where it failed. Okay, thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.0, "text": " Hi, everybody. Thanks to be here for this talk. That's a lot of people. I'm Nicolas", "tokens": [2421, 11, 2201, 13, 2561, 281, 312, 510, 337, 341, 751, 13, 663, 311, 257, 688, 295, 561, 13, 286, 478, 38268], "temperature": 0.0, "avg_logprob": -0.2084779367818461, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.16075225174427032}, {"id": 1, "seek": 0, "start": 12.0, "end": 18.16, "text": " Frankel. I've been a developer for a long time, and I would like to ask how many of", "tokens": [17288, 7124, 13, 286, 600, 668, 257, 10754, 337, 257, 938, 565, 11, 293, 286, 576, 411, 281, 1029, 577, 867, 295], "temperature": 0.0, "avg_logprob": -0.2084779367818461, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.16075225174427032}, {"id": 2, "seek": 0, "start": 18.16, "end": 28.400000000000002, "text": " you are developers in this room? Quite a lot. Who are ops? Just as many, and who are devops,", "tokens": [291, 366, 8849, 294, 341, 1808, 30, 20464, 257, 688, 13, 2102, 366, 44663, 30, 1449, 382, 867, 11, 293, 567, 366, 1905, 3370, 11], "temperature": 0.0, "avg_logprob": -0.2084779367818461, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.16075225174427032}, {"id": 3, "seek": 2840, "start": 28.4, "end": 40.04, "text": " whatever you mean by it. So this talk is intended for actually developers, because I was, or", "tokens": [2035, 291, 914, 538, 309, 13, 407, 341, 751, 307, 10226, 337, 767, 8849, 11, 570, 286, 390, 11, 420], "temperature": 0.0, "avg_logprob": -0.26438450467759284, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.00010642591951182112}, {"id": 4, "seek": 2840, "start": 40.04, "end": 47.92, "text": " I still think I'm a developer. So if you are an ops people, and for this, for you is not", "tokens": [286, 920, 519, 286, 478, 257, 10754, 13, 407, 498, 291, 366, 364, 44663, 561, 11, 293, 337, 341, 11, 337, 291, 307, 406], "temperature": 0.0, "avg_logprob": -0.26438450467759284, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.00010642591951182112}, {"id": 5, "seek": 2840, "start": 47.92, "end": 53.0, "text": " that super interesting. At least you can direct your developer colleagues to the talk, so", "tokens": [300, 1687, 1880, 13, 1711, 1935, 291, 393, 2047, 428, 10754, 7734, 281, 264, 751, 11, 370], "temperature": 0.0, "avg_logprob": -0.26438450467759284, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.00010642591951182112}, {"id": 6, "seek": 5300, "start": 53.0, "end": 63.120000000000005, "text": " that you can understand how they can ease your work. Well, perhaps you've never seen", "tokens": [300, 291, 393, 1223, 577, 436, 393, 12708, 428, 589, 13, 1042, 11, 4317, 291, 600, 1128, 1612], "temperature": 0.0, "avg_logprob": -0.1506105363368988, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00013402204785961658}, {"id": 7, "seek": 5300, "start": 63.120000000000005, "end": 71.76, "text": " that, but I'm old or experienced, depending on how you see it. And when I was starting", "tokens": [300, 11, 457, 286, 478, 1331, 420, 6751, 11, 5413, 322, 577, 291, 536, 309, 13, 400, 562, 286, 390, 2891], "temperature": 0.0, "avg_logprob": -0.1506105363368988, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00013402204785961658}, {"id": 8, "seek": 5300, "start": 71.76, "end": 77.16, "text": " my career, monitoring was like a bunch of people sitting in front of screens the whole", "tokens": [452, 3988, 11, 11028, 390, 411, 257, 3840, 295, 561, 3798, 294, 1868, 295, 11171, 264, 1379], "temperature": 0.0, "avg_logprob": -0.1506105363368988, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.00013402204785961658}, {"id": 9, "seek": 7716, "start": 77.16, "end": 83.6, "text": " day. And actually, I was lucky. Once in the south of France, I was told, hey, this is", "tokens": [786, 13, 400, 767, 11, 286, 390, 6356, 13, 3443, 294, 264, 7377, 295, 6190, 11, 286, 390, 1907, 11, 4177, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.14794083645469264, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00012468951172195375}, {"id": 10, "seek": 7716, "start": 83.6, "end": 90.39999999999999, "text": " the biggest monitoring site of all France. And actually, it really looked like this.", "tokens": [264, 3880, 11028, 3621, 295, 439, 6190, 13, 400, 767, 11, 309, 534, 2956, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14794083645469264, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00012468951172195375}, {"id": 11, "seek": 7716, "start": 90.39999999999999, "end": 97.0, "text": " And of course, there were people watching it. And that was the easy way. Now, I hope that", "tokens": [400, 295, 1164, 11, 456, 645, 561, 1976, 309, 13, 400, 300, 390, 264, 1858, 636, 13, 823, 11, 286, 1454, 300], "temperature": 0.0, "avg_logprob": -0.14794083645469264, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00012468951172195375}, {"id": 12, "seek": 7716, "start": 97.0, "end": 105.88, "text": " you don't have that anymore, that it has become a bit more modern. Actually, there is a lot", "tokens": [291, 500, 380, 362, 300, 3602, 11, 300, 309, 575, 1813, 257, 857, 544, 4363, 13, 5135, 11, 456, 307, 257, 688], "temperature": 0.0, "avg_logprob": -0.14794083645469264, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00012468951172195375}, {"id": 13, "seek": 10588, "start": 105.88, "end": 114.03999999999999, "text": " of talk now about microservices, right? Who here is doing microservices? Yeah. Yeah, because", "tokens": [295, 751, 586, 466, 15547, 47480, 11, 558, 30, 2102, 510, 307, 884, 15547, 47480, 30, 865, 13, 865, 11, 570], "temperature": 0.0, "avg_logprob": -0.11459034745411206, "compression_ratio": 1.8535353535353536, "no_speech_prob": 7.350269152084365e-05}, {"id": 14, "seek": 10588, "start": 114.03999999999999, "end": 120.0, "text": " if you don't do microservices, you are not a real developer. But even if you don't do", "tokens": [498, 291, 500, 380, 360, 15547, 47480, 11, 291, 366, 406, 257, 957, 10754, 13, 583, 754, 498, 291, 500, 380, 360], "temperature": 0.0, "avg_logprob": -0.11459034745411206, "compression_ratio": 1.8535353535353536, "no_speech_prob": 7.350269152084365e-05}, {"id": 15, "seek": 10588, "start": 120.0, "end": 125.03999999999999, "text": " microservices, so you are not a real developer, and I encourage you not to be a real developer,", "tokens": [15547, 47480, 11, 370, 291, 366, 406, 257, 957, 10754, 11, 293, 286, 5373, 291, 406, 281, 312, 257, 957, 10754, 11], "temperature": 0.0, "avg_logprob": -0.11459034745411206, "compression_ratio": 1.8535353535353536, "no_speech_prob": 7.350269152084365e-05}, {"id": 16, "seek": 10588, "start": 125.03999999999999, "end": 131.2, "text": " in that case, you probably are doing some kind of distributed work. It's become increasingly", "tokens": [294, 300, 1389, 11, 291, 1391, 366, 884, 512, 733, 295, 12631, 589, 13, 467, 311, 1813, 12980], "temperature": 0.0, "avg_logprob": -0.11459034745411206, "compression_ratio": 1.8535353535353536, "no_speech_prob": 7.350269152084365e-05}, {"id": 17, "seek": 13120, "start": 131.2, "end": 138.2, "text": " difficult to just handle everything locally. And the problem becomes, yeah, if something", "tokens": [2252, 281, 445, 4813, 1203, 16143, 13, 400, 264, 1154, 3643, 11, 1338, 11, 498, 746], "temperature": 0.0, "avg_logprob": -0.14481244321729317, "compression_ratio": 1.5085714285714287, "no_speech_prob": 7.45861980249174e-05}, {"id": 18, "seek": 13120, "start": 138.2, "end": 144.56, "text": " bad happens, how can you locate how it works? Or even if something works as expected, how", "tokens": [1578, 2314, 11, 577, 393, 291, 22370, 577, 309, 1985, 30, 1610, 754, 498, 746, 1985, 382, 5176, 11, 577], "temperature": 0.0, "avg_logprob": -0.14481244321729317, "compression_ratio": 1.5085714285714287, "no_speech_prob": 7.45861980249174e-05}, {"id": 19, "seek": 13120, "start": 144.56, "end": 154.6, "text": " you can understand the flow of your request across the network. I love Wikipedia. And", "tokens": [291, 393, 1223, 264, 3095, 295, 428, 5308, 2108, 264, 3209, 13, 286, 959, 28999, 13, 400], "temperature": 0.0, "avg_logprob": -0.14481244321729317, "compression_ratio": 1.5085714285714287, "no_speech_prob": 7.45861980249174e-05}, {"id": 20, "seek": 15460, "start": 154.6, "end": 163.28, "text": " here is the observability definition by Wikipedia, which is long and in that case, not that interesting.", "tokens": [510, 307, 264, 9951, 2310, 7123, 538, 28999, 11, 597, 307, 938, 293, 294, 300, 1389, 11, 406, 300, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11055341296725803, "compression_ratio": 1.4172661870503598, "no_speech_prob": 5.037201117374934e-05}, {"id": 21, "seek": 15460, "start": 163.28, "end": 175.79999999999998, "text": " So I have a better one afterwards for tracing. So basically, tracing helps you to understand", "tokens": [407, 286, 362, 257, 1101, 472, 10543, 337, 25262, 13, 407, 1936, 11, 25262, 3665, 291, 281, 1223], "temperature": 0.0, "avg_logprob": -0.11055341296725803, "compression_ratio": 1.4172661870503598, "no_speech_prob": 5.037201117374934e-05}, {"id": 22, "seek": 17580, "start": 175.8, "end": 184.8, "text": " the flow of a business request across all your components. Fabian, where is Fabian?", "tokens": [264, 3095, 295, 257, 1606, 5308, 2108, 439, 428, 6677, 13, 17440, 952, 11, 689, 307, 17440, 952, 30], "temperature": 0.0, "avg_logprob": -0.1274100083571214, "compression_ratio": 1.53125, "no_speech_prob": 3.0161494578351267e-05}, {"id": 23, "seek": 17580, "start": 184.8, "end": 190.24, "text": " Fabian is here, so he talked a lot about the metrics and the logging. So in this talk,", "tokens": [17440, 952, 307, 510, 11, 370, 415, 2825, 257, 688, 466, 264, 16367, 293, 264, 27991, 13, 407, 294, 341, 751, 11], "temperature": 0.0, "avg_logprob": -0.1274100083571214, "compression_ratio": 1.53125, "no_speech_prob": 3.0161494578351267e-05}, {"id": 24, "seek": 17580, "start": 190.24, "end": 199.0, "text": " I will really focus on tracing because my opinion is that, well, metrics is easy. We", "tokens": [286, 486, 534, 1879, 322, 25262, 570, 452, 4800, 307, 300, 11, 731, 11, 16367, 307, 1858, 13, 492], "temperature": 0.0, "avg_logprob": -0.1274100083571214, "compression_ratio": 1.53125, "no_speech_prob": 3.0161494578351267e-05}, {"id": 25, "seek": 17580, "start": 199.0, "end": 204.24, "text": " do metrics since ages, like we take the CPU, the memory, whatever. Now we are trying to", "tokens": [360, 16367, 1670, 12357, 11, 411, 321, 747, 264, 13199, 11, 264, 4675, 11, 2035, 13, 823, 321, 366, 1382, 281], "temperature": 0.0, "avg_logprob": -0.1274100083571214, "compression_ratio": 1.53125, "no_speech_prob": 3.0161494578351267e-05}, {"id": 26, "seek": 20424, "start": 204.24, "end": 212.28, "text": " get more business-related metrics, but it's still the same concept. Logging also. Now", "tokens": [483, 544, 1606, 12, 12004, 16367, 11, 457, 309, 311, 920, 264, 912, 3410, 13, 10824, 3249, 611, 13, 823], "temperature": 0.0, "avg_logprob": -0.15276153882344565, "compression_ratio": 1.433862433862434, "no_speech_prob": 5.78809849685058e-05}, {"id": 27, "seek": 20424, "start": 212.28, "end": 220.08, "text": " we do aggregated logging. Again, nothing mind-blowing. Tracing is, I think, the hardest part.", "tokens": [321, 360, 16743, 770, 27991, 13, 3764, 11, 1825, 1575, 12, 43788, 13, 1765, 5615, 307, 11, 286, 519, 11, 264, 13158, 644, 13], "temperature": 0.0, "avg_logprob": -0.15276153882344565, "compression_ratio": 1.433862433862434, "no_speech_prob": 5.78809849685058e-05}, {"id": 28, "seek": 20424, "start": 220.08, "end": 228.32000000000002, "text": " So in the past, there were already some tracing pioneers. Perhaps you've used some of them.", "tokens": [407, 294, 264, 1791, 11, 456, 645, 1217, 512, 25262, 47381, 13, 10517, 291, 600, 1143, 512, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15276153882344565, "compression_ratio": 1.433862433862434, "no_speech_prob": 5.78809849685058e-05}, {"id": 29, "seek": 22832, "start": 228.32, "end": 235.2, "text": " And well, now we are at the stage where we want to have something more standardized.", "tokens": [400, 731, 11, 586, 321, 366, 412, 264, 3233, 689, 321, 528, 281, 362, 746, 544, 31677, 13], "temperature": 0.0, "avg_logprob": -0.09858700503473697, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.8222848666482605e-05}, {"id": 30, "seek": 22832, "start": 235.2, "end": 250.79999999999998, "text": " So it starts with the trace context from the W3C. And the idea is that you start a trace", "tokens": [407, 309, 3719, 365, 264, 13508, 4319, 490, 264, 343, 18, 34, 13, 400, 264, 1558, 307, 300, 291, 722, 257, 13508], "temperature": 0.0, "avg_logprob": -0.09858700503473697, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.8222848666482605e-05}, {"id": 31, "seek": 25080, "start": 250.8, "end": 259.36, "text": " and then other components will get the trace and will append their own trace to it. So", "tokens": [293, 550, 661, 6677, 486, 483, 264, 13508, 293, 486, 34116, 641, 1065, 13508, 281, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.16474390029907227, "compression_ratio": 1.373015873015873, "no_speech_prob": 1.9421064280322753e-05}, {"id": 32, "seek": 25080, "start": 259.36, "end": 268.04, "text": " it works very well in a web context. And it defines two important concepts that Fabian", "tokens": [309, 1985, 588, 731, 294, 257, 3670, 4319, 13, 400, 309, 23122, 732, 1021, 10392, 300, 17440, 952], "temperature": 0.0, "avg_logprob": -0.16474390029907227, "compression_ratio": 1.373015873015873, "no_speech_prob": 1.9421064280322753e-05}, {"id": 33, "seek": 26804, "start": 268.04, "end": 284.0, "text": " thanks already described. So now I am done. So I have the same stupid stuff. So here you", "tokens": [3231, 1217, 7619, 13, 407, 586, 286, 669, 1096, 13, 407, 286, 362, 264, 912, 6631, 1507, 13, 407, 510, 291], "temperature": 0.0, "avg_logprob": -0.2523491429347618, "compression_ratio": 1.3643410852713178, "no_speech_prob": 5.856237476109527e-05}, {"id": 34, "seek": 26804, "start": 284.0, "end": 288.92, "text": " have, oh, sorry. Yes. It reminds me of the story. I did the same to my colleagues. They", "tokens": [362, 11, 1954, 11, 2597, 13, 1079, 13, 467, 12025, 385, 295, 264, 1657, 13, 286, 630, 264, 912, 281, 452, 7734, 13, 814], "temperature": 0.0, "avg_logprob": -0.2523491429347618, "compression_ratio": 1.3643410852713178, "no_speech_prob": 5.856237476109527e-05}, {"id": 35, "seek": 28892, "start": 288.92, "end": 299.28000000000003, "text": " didn't care about the presentation. They only remember that. Okay. So here you have a trace", "tokens": [994, 380, 1127, 466, 264, 5860, 13, 814, 787, 1604, 300, 13, 1033, 13, 407, 510, 291, 362, 257, 13508], "temperature": 0.0, "avg_logprob": -0.1292633612950643, "compression_ratio": 1.5773809523809523, "no_speech_prob": 7.75827265897533e-06}, {"id": 36, "seek": 28892, "start": 299.28000000000003, "end": 306.72, "text": " and here you have the different spans. So here the X1 is the parent one. And then the", "tokens": [293, 510, 291, 362, 264, 819, 44086, 13, 407, 510, 264, 1783, 16, 307, 264, 2596, 472, 13, 400, 550, 264], "temperature": 0.0, "avg_logprob": -0.1292633612950643, "compression_ratio": 1.5773809523809523, "no_speech_prob": 7.75827265897533e-06}, {"id": 37, "seek": 28892, "start": 306.72, "end": 315.40000000000003, "text": " Y and the Z1 will take this X span as their parent span. And so this is a single trace.", "tokens": [398, 293, 264, 1176, 16, 486, 747, 341, 1783, 16174, 382, 641, 2596, 16174, 13, 400, 370, 341, 307, 257, 2167, 13508, 13], "temperature": 0.0, "avg_logprob": -0.1292633612950643, "compression_ratio": 1.5773809523809523, "no_speech_prob": 7.75827265897533e-06}, {"id": 38, "seek": 31540, "start": 315.4, "end": 321.47999999999996, "text": " This is a single request across your service. Web stuff is good, but it's definitely not", "tokens": [639, 307, 257, 2167, 5308, 2108, 428, 2643, 13, 9573, 1507, 307, 665, 11, 457, 309, 311, 2138, 406], "temperature": 0.0, "avg_logprob": -0.1093021916671538, "compression_ratio": 1.3937823834196892, "no_speech_prob": 6.36671538813971e-05}, {"id": 39, "seek": 31540, "start": 321.47999999999996, "end": 331.15999999999997, "text": " enough. And so for that we have the open telemetry stuff. Open telemetry is just a big bag of", "tokens": [1547, 13, 400, 370, 337, 300, 321, 362, 264, 1269, 4304, 5537, 627, 1507, 13, 7238, 4304, 5537, 627, 307, 445, 257, 955, 3411, 295], "temperature": 0.0, "avg_logprob": -0.1093021916671538, "compression_ratio": 1.3937823834196892, "no_speech_prob": 6.36671538813971e-05}, {"id": 40, "seek": 31540, "start": 331.15999999999997, "end": 340.84, "text": " miracles all set into a specific project. So it's basically APIs, SDK, tools, whatever", "tokens": [24685, 439, 992, 666, 257, 2685, 1716, 13, 407, 309, 311, 1936, 21445, 11, 37135, 11, 3873, 11, 2035], "temperature": 0.0, "avg_logprob": -0.1093021916671538, "compression_ratio": 1.3937823834196892, "no_speech_prob": 6.36671538813971e-05}, {"id": 41, "seek": 34084, "start": 340.84, "end": 351.59999999999997, "text": " under the open telemetry level. It implements the W3C trace context. If you have been doing", "tokens": [833, 264, 1269, 4304, 5537, 627, 1496, 13, 467, 704, 17988, 264, 343, 18, 34, 13508, 4319, 13, 759, 291, 362, 668, 884], "temperature": 0.0, "avg_logprob": -0.12002562133359237, "compression_ratio": 1.450261780104712, "no_speech_prob": 3.147824827465229e-05}, {"id": 42, "seek": 34084, "start": 351.59999999999997, "end": 357.08, "text": " some kind of tracing before, you might know it because it's like the merging of open tracing", "tokens": [512, 733, 295, 25262, 949, 11, 291, 1062, 458, 309, 570, 309, 311, 411, 264, 44559, 295, 1269, 25262], "temperature": 0.0, "avg_logprob": -0.12002562133359237, "compression_ratio": 1.450261780104712, "no_speech_prob": 3.147824827465229e-05}, {"id": 43, "seek": 34084, "start": 357.08, "end": 363.28, "text": " and open sensors. Good thing is a CNCF project. So basically there is some hope that it will", "tokens": [293, 1269, 14840, 13, 2205, 551, 307, 257, 48714, 37, 1716, 13, 407, 1936, 456, 307, 512, 1454, 300, 309, 486], "temperature": 0.0, "avg_logprob": -0.12002562133359237, "compression_ratio": 1.450261780104712, "no_speech_prob": 3.147824827465229e-05}, {"id": 44, "seek": 36328, "start": 363.28, "end": 371.08, "text": " last for a couple of years. The architecture is pretty simple. Basically you've got sources,", "tokens": [1036, 337, 257, 1916, 295, 924, 13, 440, 9482, 307, 1238, 2199, 13, 8537, 291, 600, 658, 7139, 11], "temperature": 0.0, "avg_logprob": -0.13028497045690363, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.5605624867021106e-05}, {"id": 45, "seek": 36328, "start": 371.08, "end": 377.23999999999995, "text": " you've got the open telemetry protocol, and as Fabian mentioned, you dump everything into", "tokens": [291, 600, 658, 264, 1269, 4304, 5537, 627, 10336, 11, 293, 382, 17440, 952, 2835, 11, 291, 11430, 1203, 666], "temperature": 0.0, "avg_logprob": -0.13028497045690363, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.5605624867021106e-05}, {"id": 46, "seek": 36328, "start": 377.23999999999995, "end": 385.28, "text": " a collector. Collector, we should be as close as possible to your sources. And then some", "tokens": [257, 23960, 13, 4586, 20814, 11, 321, 820, 312, 382, 1998, 382, 1944, 281, 428, 7139, 13, 400, 550, 512], "temperature": 0.0, "avg_logprob": -0.13028497045690363, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.5605624867021106e-05}, {"id": 47, "seek": 36328, "start": 385.28, "end": 391.84, "text": " tools are able to read like data from it and to display it into the way that we expect", "tokens": [3873, 366, 1075, 281, 1401, 411, 1412, 490, 309, 293, 281, 4674, 309, 666, 264, 636, 300, 321, 2066], "temperature": 0.0, "avg_logprob": -0.13028497045690363, "compression_ratio": 1.577092511013216, "no_speech_prob": 1.5605624867021106e-05}, {"id": 48, "seek": 39184, "start": 391.84, "end": 404.47999999999996, "text": " to see it. What happens after the open telemetry collector is not a problem of open telemetry.", "tokens": [281, 536, 309, 13, 708, 2314, 934, 264, 1269, 4304, 5537, 627, 23960, 307, 406, 257, 1154, 295, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.12223089853922527, "compression_ratio": 1.6932515337423313, "no_speech_prob": 2.2104735762695782e-05}, {"id": 49, "seek": 39184, "start": 404.47999999999996, "end": 410.4, "text": " Just they are collectors that are compatible, and for example you can use Yeager or Zipkin", "tokens": [1449, 436, 366, 35384, 300, 366, 18218, 11, 293, 337, 1365, 291, 393, 764, 835, 3557, 420, 1176, 647, 5843], "temperature": 0.0, "avg_logprob": -0.12223089853922527, "compression_ratio": 1.6932515337423313, "no_speech_prob": 2.2104735762695782e-05}, {"id": 50, "seek": 39184, "start": 410.4, "end": 417.76, "text": " in a way that allows you to dump your data, your open telemetry data into Yeager or Zipkin", "tokens": [294, 257, 636, 300, 4045, 291, 281, 11430, 428, 1412, 11, 428, 1269, 4304, 5537, 627, 1412, 666, 835, 3557, 420, 1176, 647, 5843], "temperature": 0.0, "avg_logprob": -0.12223089853922527, "compression_ratio": 1.6932515337423313, "no_speech_prob": 2.2104735762695782e-05}, {"id": 51, "seek": 41776, "start": 417.76, "end": 422.2, "text": " into the open telemetry format. So you can reuse, and that is very important, you can", "tokens": [666, 264, 1269, 4304, 5537, 627, 7877, 13, 407, 291, 393, 26225, 11, 293, 300, 307, 588, 1021, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.17226829109611091, "compression_ratio": 1.7439613526570048, "no_speech_prob": 3.8773105188738555e-05}, {"id": 52, "seek": 41776, "start": 422.2, "end": 427.15999999999997, "text": " reuse your infrastructure if you're already using the tools, but just switching to open", "tokens": [26225, 428, 6896, 498, 291, 434, 1217, 1228, 264, 3873, 11, 457, 445, 16493, 281, 1269], "temperature": 0.0, "avg_logprob": -0.17226829109611091, "compression_ratio": 1.7439613526570048, "no_speech_prob": 3.8773105188738555e-05}, {"id": 53, "seek": 41776, "start": 427.15999999999997, "end": 432.88, "text": " telemetry. And then you are like you are using a standard, and then you can switch your open", "tokens": [4304, 5537, 627, 13, 400, 550, 291, 366, 411, 291, 366, 1228, 257, 3832, 11, 293, 550, 291, 393, 3679, 428, 1269], "temperature": 0.0, "avg_logprob": -0.17226829109611091, "compression_ratio": 1.7439613526570048, "no_speech_prob": 3.8773105188738555e-05}, {"id": 54, "seek": 41776, "start": 432.88, "end": 444.08, "text": " telemetry back end with less issues. Now comes the fun developer part. If you are a developer,", "tokens": [4304, 5537, 627, 646, 917, 365, 1570, 2663, 13, 823, 1487, 264, 1019, 10754, 644, 13, 759, 291, 366, 257, 10754, 11], "temperature": 0.0, "avg_logprob": -0.17226829109611091, "compression_ratio": 1.7439613526570048, "no_speech_prob": 3.8773105188738555e-05}, {"id": 55, "seek": 44408, "start": 444.08, "end": 453.59999999999997, "text": " you probably are lazy. I know, I'm a developer. So the idea is open telemetry should make", "tokens": [291, 1391, 366, 14847, 13, 286, 458, 11, 286, 478, 257, 10754, 13, 407, 264, 1558, 307, 1269, 4304, 5537, 627, 820, 652], "temperature": 0.0, "avg_logprob": -0.14939473959115834, "compression_ratio": 1.5229885057471264, "no_speech_prob": 7.545236439909786e-05}, {"id": 56, "seek": 44408, "start": 453.59999999999997, "end": 462.68, "text": " your life as a developer as easy as possible to help your ops colleague, like diagnose", "tokens": [428, 993, 382, 257, 10754, 382, 1858, 382, 1944, 281, 854, 428, 44663, 13532, 11, 411, 36238], "temperature": 0.0, "avg_logprob": -0.14939473959115834, "compression_ratio": 1.5229885057471264, "no_speech_prob": 7.545236439909786e-05}, {"id": 57, "seek": 44408, "start": 462.68, "end": 471.96, "text": " your problems. And the easiest part if you do auto instrumentation. Auto instrumentation", "tokens": [428, 2740, 13, 400, 264, 12889, 644, 498, 291, 360, 8399, 7198, 399, 13, 13738, 7198, 399], "temperature": 0.0, "avg_logprob": -0.14939473959115834, "compression_ratio": 1.5229885057471264, "no_speech_prob": 7.545236439909786e-05}, {"id": 58, "seek": 47196, "start": 471.96, "end": 478.03999999999996, "text": " is only possible in cases where you have a platform, when you have a run time. Fabian", "tokens": [307, 787, 1944, 294, 3331, 689, 291, 362, 257, 3663, 11, 562, 291, 362, 257, 1190, 565, 13, 17440, 952], "temperature": 0.0, "avg_logprob": -0.19265646748728565, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.998491956735961e-05}, {"id": 59, "seek": 47196, "start": 478.03999999999996, "end": 485.76, "text": " mentioned Java, Java as a run time, which is the Jivem. Python as a run time. Now if", "tokens": [2835, 10745, 11, 10745, 382, 257, 1190, 565, 11, 597, 307, 264, 508, 488, 76, 13, 15329, 382, 257, 1190, 565, 13, 823, 498], "temperature": 0.0, "avg_logprob": -0.19265646748728565, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.998491956735961e-05}, {"id": 60, "seek": 47196, "start": 485.76, "end": 496.03999999999996, "text": " you have rusts, it's not as easy. So in that case, you are stuck. My advice if you are", "tokens": [291, 362, 15259, 82, 11, 309, 311, 406, 382, 1858, 13, 407, 294, 300, 1389, 11, 291, 366, 5541, 13, 1222, 5192, 498, 291, 366], "temperature": 0.0, "avg_logprob": -0.19265646748728565, "compression_ratio": 1.5766871165644172, "no_speech_prob": 3.998491956735961e-05}, {"id": 61, "seek": 49604, "start": 496.04, "end": 503.28000000000003, "text": " using a run time, and probably most of you are using such run times, whether Java, whatever,", "tokens": [1228, 257, 1190, 565, 11, 293, 1391, 881, 295, 291, 366, 1228, 1270, 1190, 1413, 11, 1968, 10745, 11, 2035, 11], "temperature": 0.0, "avg_logprob": -0.1533888526584791, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.2800788681488484e-05}, {"id": 62, "seek": 49604, "start": 503.28000000000003, "end": 509.92, "text": " use it. It's basically free. It's a low hanging fruit, and there is no coupling. So basically", "tokens": [764, 309, 13, 467, 311, 1936, 1737, 13, 467, 311, 257, 2295, 8345, 6773, 11, 293, 456, 307, 572, 37447, 13, 407, 1936], "temperature": 0.0, "avg_logprob": -0.1533888526584791, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.2800788681488484e-05}, {"id": 63, "seek": 49604, "start": 509.92, "end": 517.08, "text": " you don't need extra dependencies as developers in your projects. So since it's called practical", "tokens": [291, 500, 380, 643, 2857, 36606, 382, 8849, 294, 428, 4455, 13, 407, 1670, 309, 311, 1219, 8496], "temperature": 0.0, "avg_logprob": -0.1533888526584791, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.2800788681488484e-05}, {"id": 64, "seek": 49604, "start": 517.08, "end": 523.8000000000001, "text": " introduction, let's do some practice. So here I have a bit better than the hello world,", "tokens": [9339, 11, 718, 311, 360, 512, 3124, 13, 407, 510, 286, 362, 257, 857, 1101, 813, 264, 7751, 1002, 11], "temperature": 0.0, "avg_logprob": -0.1533888526584791, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.2800788681488484e-05}, {"id": 65, "seek": 52380, "start": 523.8, "end": 531.68, "text": " so I have tried to model like an e-commerce shop with very simple stuff. It starts just", "tokens": [370, 286, 362, 3031, 281, 2316, 411, 364, 308, 12, 26926, 3945, 365, 588, 2199, 1507, 13, 467, 3719, 445], "temperature": 0.0, "avg_logprob": -0.12696610391139984, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.8011410146718845e-05}, {"id": 66, "seek": 52380, "start": 531.68, "end": 536.9599999999999, "text": " asking for products. I will go through an API gateway which will forward the product", "tokens": [3365, 337, 3383, 13, 286, 486, 352, 807, 364, 9362, 28532, 597, 486, 2128, 264, 1674], "temperature": 0.0, "avg_logprob": -0.12696610391139984, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.8011410146718845e-05}, {"id": 67, "seek": 52380, "start": 536.9599999999999, "end": 541.8399999999999, "text": " to the catalog, and the catalog doesn't know about the prices, so it will ask the prices", "tokens": [281, 264, 19746, 11, 293, 264, 19746, 1177, 380, 458, 466, 264, 7901, 11, 370, 309, 486, 1029, 264, 7901], "temperature": 0.0, "avg_logprob": -0.12696610391139984, "compression_ratio": 1.5174418604651163, "no_speech_prob": 5.8011410146718845e-05}, {"id": 68, "seek": 54184, "start": 541.84, "end": 554.0, "text": " from the pricing service, and it will ask the stocks from the stock service. The intra", "tokens": [490, 264, 17621, 2643, 11, 293, 309, 486, 1029, 264, 12966, 490, 264, 4127, 2643, 13, 440, 43358], "temperature": 0.0, "avg_logprob": -0.17996451258659363, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.525064676068723e-05}, {"id": 69, "seek": 54184, "start": 554.0, "end": 560.12, "text": " point is the most important thing, because it gives the parent's phrase. Everything will", "tokens": [935, 307, 264, 881, 1021, 551, 11, 570, 309, 2709, 264, 2596, 311, 9535, 13, 5471, 486], "temperature": 0.0, "avg_logprob": -0.17996451258659363, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.525064676068723e-05}, {"id": 70, "seek": 54184, "start": 560.12, "end": 566.2, "text": " be from that. So in general, you have a reverse proxy or an API gateway, depending on your", "tokens": [312, 490, 300, 13, 407, 294, 2674, 11, 291, 362, 257, 9943, 29690, 420, 364, 9362, 28532, 11, 5413, 322, 428], "temperature": 0.0, "avg_logprob": -0.17996451258659363, "compression_ratio": 1.4943820224719102, "no_speech_prob": 3.525064676068723e-05}, {"id": 71, "seek": 56620, "start": 566.2, "end": 573.84, "text": " use case. I work on the Apache API 6 project. It uses the NGINX reverse proxy. On top you", "tokens": [764, 1389, 13, 286, 589, 322, 264, 46597, 9362, 1386, 1716, 13, 467, 4960, 264, 426, 38, 1464, 55, 9943, 29690, 13, 1282, 1192, 291], "temperature": 0.0, "avg_logprob": -0.22992862799228767, "compression_ratio": 1.4526315789473685, "no_speech_prob": 3.068315709242597e-05}, {"id": 72, "seek": 56620, "start": 573.84, "end": 579.84, "text": " have an open resty, because you want to have Lua to script and to auto reload the configuration.", "tokens": [362, 364, 1269, 1472, 88, 11, 570, 291, 528, 281, 362, 441, 4398, 281, 5755, 293, 281, 8399, 25628, 264, 11694, 13], "temperature": 0.0, "avg_logprob": -0.22992862799228767, "compression_ratio": 1.4526315789473685, "no_speech_prob": 3.068315709242597e-05}, {"id": 73, "seek": 56620, "start": 579.84, "end": 589.0, "text": " Then you have lots of out of the box plugins. Let's see how it works. Now I have the code", "tokens": [1396, 291, 362, 3195, 295, 484, 295, 264, 2424, 33759, 13, 961, 311, 536, 577, 309, 1985, 13, 823, 286, 362, 264, 3089], "temperature": 0.0, "avg_logprob": -0.22992862799228767, "compression_ratio": 1.4526315789473685, "no_speech_prob": 3.068315709242597e-05}, {"id": 74, "seek": 58900, "start": 589.0, "end": 600.88, "text": " here. Is it the begin off? Good. So I might be very old, because for me it wouldn't. Okay,", "tokens": [510, 13, 1119, 309, 264, 1841, 766, 30, 2205, 13, 407, 286, 1062, 312, 588, 1331, 11, 570, 337, 385, 309, 2759, 380, 13, 1033, 11], "temperature": 0.0, "avg_logprob": -0.1743488311767578, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.00013129036233294755}, {"id": 75, "seek": 58900, "start": 600.88, "end": 606.24, "text": " here that's my architecture. I'm using Docker compose, because I'm super lazy. I don't want", "tokens": [510, 300, 311, 452, 9482, 13, 286, 478, 1228, 33772, 35925, 11, 570, 286, 478, 1687, 14847, 13, 286, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.1743488311767578, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.00013129036233294755}, {"id": 76, "seek": 58900, "start": 606.24, "end": 612.56, "text": " to use Kubernetes, so I have Yeager. As I mentioned, I have all in one. I'm using the", "tokens": [281, 764, 23145, 11, 370, 286, 362, 835, 3557, 13, 1018, 286, 2835, 11, 286, 362, 439, 294, 472, 13, 286, 478, 1228, 264], "temperature": 0.0, "avg_logprob": -0.1743488311767578, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.00013129036233294755}, {"id": 77, "seek": 58900, "start": 612.56, "end": 618.96, "text": " all included, so I don't need to think about having the telemetry collector and the web", "tokens": [439, 5556, 11, 370, 286, 500, 380, 643, 281, 519, 466, 1419, 264, 4304, 5537, 627, 23960, 293, 264, 3670], "temperature": 0.0, "avg_logprob": -0.1743488311767578, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.00013129036233294755}, {"id": 78, "seek": 61896, "start": 618.96, "end": 628.72, "text": " to check the traces. I have only one single image. Then I have API 6. Then I have the catalog,", "tokens": [281, 1520, 264, 26076, 13, 286, 362, 787, 472, 2167, 3256, 13, 1396, 286, 362, 9362, 1386, 13, 1396, 286, 362, 264, 19746, 11], "temperature": 0.0, "avg_logprob": -0.13549028684015144, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.69425215467345e-05}, {"id": 79, "seek": 61896, "start": 628.72, "end": 637.6800000000001, "text": " which I showed you. Of course I have a couple of variables to configure everything. I wanted", "tokens": [597, 286, 4712, 291, 13, 2720, 1164, 286, 362, 257, 1916, 295, 9102, 281, 22162, 1203, 13, 286, 1415], "temperature": 0.0, "avg_logprob": -0.13549028684015144, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.69425215467345e-05}, {"id": 80, "seek": 61896, "start": 637.6800000000001, "end": 645.76, "text": " to focus on tracing, so no metrics, no logs. I'm sending everything to Yeager, and then", "tokens": [281, 1879, 322, 25262, 11, 370, 572, 16367, 11, 572, 20820, 13, 286, 478, 7750, 1203, 281, 835, 3557, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.13549028684015144, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.69425215467345e-05}, {"id": 81, "seek": 64576, "start": 645.76, "end": 653.24, "text": " I do the same for pricing, and I do the same for the stock. And normally at this point,", "tokens": [286, 360, 264, 912, 337, 17621, 11, 293, 286, 360, 264, 912, 337, 264, 4127, 13, 400, 5646, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.12077167510986328, "compression_ratio": 1.5720524017467248, "no_speech_prob": 2.6067273211083375e-05}, {"id": 82, "seek": 64576, "start": 653.24, "end": 660.8, "text": " I already started, because in general I have issues with the Java stuff. So here I'm doing", "tokens": [286, 1217, 1409, 11, 570, 294, 2674, 286, 362, 2663, 365, 264, 10745, 1507, 13, 407, 510, 286, 478, 884], "temperature": 0.0, "avg_logprob": -0.12077167510986328, "compression_ratio": 1.5720524017467248, "no_speech_prob": 2.6067273211083375e-05}, {"id": 83, "seek": 64576, "start": 660.8, "end": 668.24, "text": " a simple curl to the product. I've got the data, which is not that important. And I can", "tokens": [257, 2199, 22591, 281, 264, 1674, 13, 286, 600, 658, 264, 1412, 11, 597, 307, 406, 300, 1021, 13, 400, 286, 393], "temperature": 0.0, "avg_logprob": -0.12077167510986328, "compression_ratio": 1.5720524017467248, "no_speech_prob": 2.6067273211083375e-05}, {"id": 84, "seek": 64576, "start": 668.24, "end": 675.52, "text": " check on the web app how it works. So here I will go on the Yeager UI. I see all my services.", "tokens": [1520, 322, 264, 3670, 724, 577, 309, 1985, 13, 407, 510, 286, 486, 352, 322, 264, 835, 3557, 15682, 13, 286, 536, 439, 452, 3328, 13], "temperature": 0.0, "avg_logprob": -0.12077167510986328, "compression_ratio": 1.5720524017467248, "no_speech_prob": 2.6067273211083375e-05}, {"id": 85, "seek": 67552, "start": 675.52, "end": 682.4399999999999, "text": " I can find the traces. Here you can find the latest one. And here is the thing. If I click", "tokens": [286, 393, 915, 264, 26076, 13, 1692, 291, 393, 915, 264, 6792, 472, 13, 400, 510, 307, 264, 551, 13, 759, 286, 2052], "temperature": 0.0, "avg_logprob": -0.1029210090637207, "compression_ratio": 1.470899470899471, "no_speech_prob": 8.926343070925213e-06}, {"id": 86, "seek": 67552, "start": 682.4399999999999, "end": 692.96, "text": " on it, it might be a bit small, right? I cannot do much better. You can already see everything", "tokens": [322, 309, 11, 309, 1062, 312, 257, 857, 1359, 11, 558, 30, 286, 2644, 360, 709, 1101, 13, 509, 393, 1217, 536, 1203], "temperature": 0.0, "avg_logprob": -0.1029210090637207, "compression_ratio": 1.470899470899471, "no_speech_prob": 8.926343070925213e-06}, {"id": 87, "seek": 67552, "start": 692.96, "end": 699.16, "text": " that I've shown you. So I start with the product from the API gateway. It forwards it to the", "tokens": [300, 286, 600, 4898, 291, 13, 407, 286, 722, 365, 264, 1674, 490, 264, 9362, 28532, 13, 467, 30126, 309, 281, 264], "temperature": 0.0, "avg_logprob": -0.1029210090637207, "compression_ratio": 1.470899470899471, "no_speech_prob": 8.926343070925213e-06}, {"id": 88, "seek": 69916, "start": 699.16, "end": 707.16, "text": " product to the catalog. Then I have the internal calls, and I will show you how it works. Then", "tokens": [1674, 281, 264, 19746, 13, 1396, 286, 362, 264, 6920, 5498, 11, 293, 286, 486, 855, 291, 577, 309, 1985, 13, 1396], "temperature": 0.0, "avg_logprob": -0.09886433861472389, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.7076487135200296e-06}, {"id": 89, "seek": 69916, "start": 707.16, "end": 714.04, "text": " I have the get request made from inside the application. And then I have the stocks that", "tokens": [286, 362, 264, 483, 5308, 1027, 490, 1854, 264, 3861, 13, 400, 550, 286, 362, 264, 12966, 300], "temperature": 0.0, "avg_logprob": -0.09886433861472389, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.7076487135200296e-06}, {"id": 90, "seek": 69916, "start": 714.04, "end": 722.6, "text": " responds here. Same here. And here we see something that was not mentioned on the component", "tokens": [27331, 510, 13, 10635, 510, 13, 400, 510, 321, 536, 746, 300, 390, 406, 2835, 322, 264, 6542], "temperature": 0.0, "avg_logprob": -0.09886433861472389, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.7076487135200296e-06}, {"id": 91, "seek": 72260, "start": 722.6, "end": 730.0400000000001, "text": " diagram. From the catalog to the stock, I go directly. But from the catalog to the pricing,", "tokens": [10686, 13, 3358, 264, 19746, 281, 264, 4127, 11, 286, 352, 3838, 13, 583, 490, 264, 19746, 281, 264, 17621, 11], "temperature": 0.0, "avg_logprob": -0.11454574958137843, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.7219010260305367e-05}, {"id": 92, "seek": 72260, "start": 730.0400000000001, "end": 736.28, "text": " I go back to the API gateway, which is also a way to do that for whatever reason. And", "tokens": [286, 352, 646, 281, 264, 9362, 28532, 11, 597, 307, 611, 257, 636, 281, 360, 300, 337, 2035, 1778, 13, 400], "temperature": 0.0, "avg_logprob": -0.11454574958137843, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.7219010260305367e-05}, {"id": 93, "seek": 72260, "start": 736.28, "end": 742.28, "text": " so this is something that was not mentioned on the PDF, but you cannot cheat with open", "tokens": [370, 341, 307, 746, 300, 390, 406, 2835, 322, 264, 17752, 11, 457, 291, 2644, 17470, 365, 1269], "temperature": 0.0, "avg_logprob": -0.11454574958137843, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.7219010260305367e-05}, {"id": 94, "seek": 72260, "start": 742.28, "end": 749.28, "text": " telemetry. It tells you exactly what happens and the flow. And the rest is the same. So", "tokens": [4304, 5537, 627, 13, 467, 5112, 291, 2293, 437, 2314, 293, 264, 3095, 13, 400, 264, 1472, 307, 264, 912, 13, 407], "temperature": 0.0, "avg_logprob": -0.11454574958137843, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.7219010260305367e-05}, {"id": 95, "seek": 74928, "start": 749.28, "end": 760.0799999999999, "text": " regarding the code itself, I told you that I don't want anything to trouble the developer.", "tokens": [8595, 264, 3089, 2564, 11, 286, 1907, 291, 300, 286, 500, 380, 528, 1340, 281, 5253, 264, 10754, 13], "temperature": 0.0, "avg_logprob": -0.10412377924532504, "compression_ratio": 1.7030303030303031, "no_speech_prob": 1.2988386515644379e-05}, {"id": 96, "seek": 74928, "start": 760.0799999999999, "end": 769.76, "text": " So here I have nothing regarding open telemetry. If I write hotel, you see nothing. If I write", "tokens": [407, 510, 286, 362, 1825, 8595, 1269, 4304, 5537, 627, 13, 759, 286, 2464, 7622, 11, 291, 536, 1825, 13, 759, 286, 2464], "temperature": 0.0, "avg_logprob": -0.10412377924532504, "compression_ratio": 1.7030303030303031, "no_speech_prob": 1.2988386515644379e-05}, {"id": 97, "seek": 74928, "start": 769.76, "end": 778.0, "text": " telemetry, you see nothing. I have no dependency. The only thing that I have is my Docker file,", "tokens": [4304, 5537, 627, 11, 291, 536, 1825, 13, 286, 362, 572, 33621, 13, 440, 787, 551, 300, 286, 362, 307, 452, 33772, 3991, 11], "temperature": 0.0, "avg_logprob": -0.10412377924532504, "compression_ratio": 1.7030303030303031, "no_speech_prob": 1.2988386515644379e-05}, {"id": 98, "seek": 77800, "start": 778.0, "end": 789.4, "text": " and in my Docker file, I get the latest open telemetry agents. So you can have your developers", "tokens": [293, 294, 452, 33772, 3991, 11, 286, 483, 264, 6792, 1269, 4304, 5537, 627, 12554, 13, 407, 291, 393, 362, 428, 8849], "temperature": 0.0, "avg_logprob": -0.1649160385131836, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.658756425778847e-06}, {"id": 99, "seek": 77800, "start": 789.4, "end": 796.52, "text": " completely oblivious, and you just provide them with this snippet, and then when you", "tokens": [2584, 47039, 851, 11, 293, 291, 445, 2893, 552, 365, 341, 35623, 302, 11, 293, 550, 562, 291], "temperature": 0.0, "avg_logprob": -0.1649160385131836, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.658756425778847e-06}, {"id": 100, "seek": 77800, "start": 796.52, "end": 804.04, "text": " run the Java application, you just tell them, A, run with the Java agent. Low-hanging fruits,", "tokens": [1190, 264, 10745, 3861, 11, 291, 445, 980, 552, 11, 316, 11, 1190, 365, 264, 10745, 9461, 13, 17078, 12, 71, 9741, 12148, 11], "temperature": 0.0, "avg_logprob": -0.1649160385131836, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.658756425778847e-06}, {"id": 101, "seek": 80404, "start": 804.04, "end": 819.3199999999999, "text": " zero trouble. Any Java developer here? Not that many. Python? OK, so it will be Python.", "tokens": [4018, 5253, 13, 2639, 10745, 10754, 510, 30, 1726, 300, 867, 13, 15329, 30, 2264, 11, 370, 309, 486, 312, 15329, 13], "temperature": 0.0, "avg_logprob": -0.13553709886511978, "compression_ratio": 1.2587412587412588, "no_speech_prob": 2.543658047216013e-05}, {"id": 102, "seek": 80404, "start": 819.3199999999999, "end": 829.0799999999999, "text": " Just the same here. Here it's a bit different. I add dependencies, but actually I do nothing", "tokens": [1449, 264, 912, 510, 13, 1692, 309, 311, 257, 857, 819, 13, 286, 909, 36606, 11, 457, 767, 286, 360, 1825], "temperature": 0.0, "avg_logprob": -0.13553709886511978, "compression_ratio": 1.2587412587412588, "no_speech_prob": 2.543658047216013e-05}, {"id": 103, "seek": 82908, "start": 829.08, "end": 837.1600000000001, "text": " on it. So here I have no dependency on anything. Here I'm using a SQL database because, again,", "tokens": [322, 309, 13, 407, 510, 286, 362, 572, 33621, 322, 1340, 13, 1692, 286, 478, 1228, 257, 19200, 8149, 570, 11, 797, 11], "temperature": 0.0, "avg_logprob": -0.1415077567100525, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2440020984504372e-05}, {"id": 104, "seek": 82908, "start": 837.1600000000001, "end": 844.84, "text": " I'm lazy. I don't care that much. But here I have no dependency, no API call to open telemetry.", "tokens": [286, 478, 14847, 13, 286, 500, 380, 1127, 300, 709, 13, 583, 510, 286, 362, 572, 33621, 11, 572, 9362, 818, 281, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.1415077567100525, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2440020984504372e-05}, {"id": 105, "seek": 82908, "start": 844.84, "end": 854.0, "text": " The only thing that I have is in the Docker file again. I have this. Again, I'm using", "tokens": [440, 787, 551, 300, 286, 362, 307, 294, 264, 33772, 3991, 797, 13, 286, 362, 341, 13, 3764, 11, 286, 478, 1228], "temperature": 0.0, "avg_logprob": -0.1415077567100525, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2440020984504372e-05}, {"id": 106, "seek": 85400, "start": 854.0, "end": 860.84, "text": " a runtime. It's super easy. I let the runtime, like, intercept the calls and everything to", "tokens": [257, 34474, 13, 467, 311, 1687, 1858, 13, 286, 718, 264, 34474, 11, 411, 11, 24700, 264, 5498, 293, 1203, 281], "temperature": 0.0, "avg_logprob": -0.11831857045491537, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.505301199562382e-05}, {"id": 107, "seek": 85400, "start": 860.84, "end": 871.88, "text": " open telemetry. And the last fun stuff is Rust. Any Rust developer? Please don't look", "tokens": [1269, 4304, 5537, 627, 13, 400, 264, 1036, 1019, 1507, 307, 34952, 13, 2639, 34952, 10754, 30, 2555, 500, 380, 574], "temperature": 0.0, "avg_logprob": -0.11831857045491537, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.505301199562382e-05}, {"id": 108, "seek": 85400, "start": 871.88, "end": 882.08, "text": " at my code too much. I'm not a Rust developer, so I hope it won't be too horrible. And Rust", "tokens": [412, 452, 3089, 886, 709, 13, 286, 478, 406, 257, 34952, 10754, 11, 370, 286, 1454, 309, 1582, 380, 312, 886, 9263, 13, 400, 34952], "temperature": 0.0, "avg_logprob": -0.11831857045491537, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.505301199562382e-05}, {"id": 109, "seek": 88208, "start": 882.08, "end": 887.48, "text": " is actually, well, not that standardized. So here I don't have any runtime, so I need", "tokens": [307, 767, 11, 731, 11, 406, 300, 31677, 13, 407, 510, 286, 500, 380, 362, 604, 34474, 11, 370, 286, 643], "temperature": 0.0, "avg_logprob": -0.14400572095598493, "compression_ratio": 1.446236559139785, "no_speech_prob": 1.591945874679368e-05}, {"id": 110, "seek": 88208, "start": 887.48, "end": 895.9200000000001, "text": " to make the calls by myself. The hardest part is to find which library to use, depending", "tokens": [281, 652, 264, 5498, 538, 2059, 13, 440, 13158, 644, 307, 281, 915, 597, 6405, 281, 764, 11, 5413], "temperature": 0.0, "avg_logprob": -0.14400572095598493, "compression_ratio": 1.446236559139785, "no_speech_prob": 1.591945874679368e-05}, {"id": 111, "seek": 88208, "start": 895.9200000000001, "end": 903.44, "text": " on which framework to use. So in this case, I found one, and perhaps there are better options.", "tokens": [322, 597, 8388, 281, 764, 13, 407, 294, 341, 1389, 11, 286, 1352, 472, 11, 293, 4317, 456, 366, 1101, 3956, 13], "temperature": 0.0, "avg_logprob": -0.14400572095598493, "compression_ratio": 1.446236559139785, "no_speech_prob": 1.591945874679368e-05}, {"id": 112, "seek": 90344, "start": 903.44, "end": 913.08, "text": " But I found this open telemetry OLTP stuff. And here this is because I'm using XM. I'm", "tokens": [583, 286, 1352, 341, 1269, 4304, 5537, 627, 39191, 16804, 1507, 13, 400, 510, 341, 307, 570, 286, 478, 1228, 1783, 44, 13, 286, 478], "temperature": 0.0, "avg_logprob": -0.18488225704286157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.9201545001124032e-05}, {"id": 113, "seek": 90344, "start": 913.08, "end": 919.9200000000001, "text": " using this library. And so far, it works for me. I don't need to do a lot of stuff. I just,", "tokens": [1228, 341, 6405, 13, 400, 370, 1400, 11, 309, 1985, 337, 385, 13, 286, 500, 380, 643, 281, 360, 257, 688, 295, 1507, 13, 286, 445, 11], "temperature": 0.0, "avg_logprob": -0.18488225704286157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.9201545001124032e-05}, {"id": 114, "seek": 90344, "start": 919.9200000000001, "end": 930.0400000000001, "text": " like, copy pasted this stuff. Copy past developer. And afterwards, in my main function, I just", "tokens": [411, 11, 5055, 1791, 292, 341, 1507, 13, 25653, 1791, 10754, 13, 400, 10543, 11, 294, 452, 2135, 2445, 11, 286, 445], "temperature": 0.0, "avg_logprob": -0.18488225704286157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.9201545001124032e-05}, {"id": 115, "seek": 93004, "start": 930.04, "end": 938.92, "text": " need to say this and this. So I added two layers. So if you don't have any platform,", "tokens": [643, 281, 584, 341, 293, 341, 13, 407, 286, 3869, 732, 7914, 13, 407, 498, 291, 500, 380, 362, 604, 3663, 11], "temperature": 0.0, "avg_logprob": -0.13544509217545792, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.4728339010616764e-05}, {"id": 116, "seek": 93004, "start": 938.92, "end": 944.4399999999999, "text": " any runtime, you actually need your developers to care about open telemetry. Otherwise, it's", "tokens": [604, 34474, 11, 291, 767, 643, 428, 8849, 281, 1127, 466, 1269, 4304, 5537, 627, 13, 10328, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.13544509217545792, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.4728339010616764e-05}, {"id": 117, "seek": 93004, "start": 944.4399999999999, "end": 954.4, "text": " fine. Now, we already have pretty good, like, results, but we want to do better. So we can", "tokens": [2489, 13, 823, 11, 321, 1217, 362, 1238, 665, 11, 411, 11, 3542, 11, 457, 321, 528, 281, 360, 1101, 13, 407, 321, 393], "temperature": 0.0, "avg_logprob": -0.13544509217545792, "compression_ratio": 1.46448087431694, "no_speech_prob": 1.4728339010616764e-05}, {"id": 118, "seek": 95440, "start": 954.4, "end": 962.88, "text": " also ask the developers, once they are more comfortable, to do manual instrumentation", "tokens": [611, 1029, 264, 8849, 11, 1564, 436, 366, 544, 4619, 11, 281, 360, 9688, 7198, 399], "temperature": 0.0, "avg_logprob": -0.10630889372392134, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.621115345391445e-05}, {"id": 119, "seek": 95440, "start": 962.88, "end": 975.64, "text": " even in the case when there is a platform. Now, I will docker compose down. And it takes", "tokens": [754, 294, 264, 1389, 562, 456, 307, 257, 3663, 13, 823, 11, 286, 486, 360, 9178, 35925, 760, 13, 400, 309, 2516], "temperature": 0.0, "avg_logprob": -0.10630889372392134, "compression_ratio": 1.3181818181818181, "no_speech_prob": 2.621115345391445e-05}, {"id": 120, "seek": 97564, "start": 975.64, "end": 999.72, "text": " a bit of time. I will prepare this. And on the catalog sides, now I can have some additional", "tokens": [257, 857, 295, 565, 13, 286, 486, 5940, 341, 13, 400, 322, 264, 19746, 4881, 11, 586, 286, 393, 362, 512, 4497], "temperature": 0.0, "avg_logprob": -0.14156705599564773, "compression_ratio": 1.0952380952380953, "no_speech_prob": 2.0044662960572168e-05}, {"id": 121, "seek": 99972, "start": 999.72, "end": 1012.4, "text": " codes. So this is a Spring Boot application. What I can do is add annotations. Like, I", "tokens": [14211, 13, 407, 341, 307, 257, 14013, 37263, 3861, 13, 708, 286, 393, 360, 307, 909, 25339, 763, 13, 1743, 11, 286], "temperature": 0.0, "avg_logprob": -0.1692134394790187, "compression_ratio": 1.60352422907489, "no_speech_prob": 9.00888626347296e-05}, {"id": 122, "seek": 99972, "start": 1012.4, "end": 1017.24, "text": " noticed there were a couple of Java developers. So it's the same with Kotlin. It's still on", "tokens": [5694, 456, 645, 257, 1916, 295, 10745, 8849, 13, 407, 309, 311, 264, 912, 365, 30123, 5045, 13, 467, 311, 920, 322], "temperature": 0.0, "avg_logprob": -0.1692134394790187, "compression_ratio": 1.60352422907489, "no_speech_prob": 9.00888626347296e-05}, {"id": 123, "seek": 99972, "start": 1017.24, "end": 1022.44, "text": " the JVM. So basically, I'm adding annotations. And because Spring Boot can read the annotation", "tokens": [264, 508, 53, 44, 13, 407, 1936, 11, 286, 478, 5127, 25339, 763, 13, 400, 570, 14013, 37263, 393, 1401, 264, 48654], "temperature": 0.0, "avg_logprob": -0.1692134394790187, "compression_ratio": 1.60352422907489, "no_speech_prob": 9.00888626347296e-05}, {"id": 124, "seek": 99972, "start": 1022.44, "end": 1028.72, "text": " at runtime, it can add those calls. So I don't have to call the API explicitly. I just add", "tokens": [412, 34474, 11, 309, 393, 909, 729, 5498, 13, 407, 286, 500, 380, 362, 281, 818, 264, 9362, 20803, 13, 286, 445, 909], "temperature": 0.0, "avg_logprob": -0.1692134394790187, "compression_ratio": 1.60352422907489, "no_speech_prob": 9.00888626347296e-05}, {"id": 125, "seek": 102872, "start": 1028.72, "end": 1043.2, "text": " some annotation, and it should be done. On the Python side, I import this trace stuff,", "tokens": [512, 48654, 11, 293, 309, 820, 312, 1096, 13, 1282, 264, 15329, 1252, 11, 286, 974, 341, 13508, 1507, 11], "temperature": 0.0, "avg_logprob": -0.19219484191009964, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.7746508092386648e-05}, {"id": 126, "seek": 102872, "start": 1043.2, "end": 1052.96, "text": " and then I can, with the tracer, add some, again, explicit traces, so internal traces.", "tokens": [293, 550, 286, 393, 11, 365, 264, 504, 12858, 11, 909, 512, 11, 797, 11, 13691, 26076, 11, 370, 6920, 26076, 13], "temperature": 0.0, "avg_logprob": -0.19219484191009964, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.7746508092386648e-05}, {"id": 127, "seek": 102872, "start": 1052.96, "end": 1057.52, "text": " And from the first point of view, because I already, like, did it explicitly work.", "tokens": [400, 490, 264, 700, 935, 295, 1910, 11, 570, 286, 1217, 11, 411, 11, 630, 309, 20803, 589, 13], "temperature": 0.0, "avg_logprob": -0.19219484191009964, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.7746508092386648e-05}, {"id": 128, "seek": 105752, "start": 1057.52, "end": 1061.76, "text": " And now you can see that I am in deep trouble, because it happened a lot of time. The Java", "tokens": [400, 586, 291, 393, 536, 300, 286, 669, 294, 2452, 5253, 11, 570, 309, 2011, 257, 688, 295, 565, 13, 440, 10745], "temperature": 0.0, "avg_logprob": -0.21613072024451363, "compression_ratio": 1.423913043478261, "no_speech_prob": 2.9731207177974284e-05}, {"id": 129, "seek": 105752, "start": 1061.76, "end": 1067.52, "text": " application doesn't start for a demo, and that's really, really fun. So I will try to", "tokens": [3861, 1177, 380, 722, 337, 257, 10723, 11, 293, 300, 311, 534, 11, 534, 1019, 13, 407, 286, 486, 853, 281], "temperature": 0.0, "avg_logprob": -0.21613072024451363, "compression_ratio": 1.423913043478261, "no_speech_prob": 2.9731207177974284e-05}, {"id": 130, "seek": 105752, "start": 1067.52, "end": 1079.32, "text": " docker compose down the catalog. And docker compose, hey, what happens? Dash? Are you", "tokens": [360, 9178, 35925, 760, 264, 19746, 13, 400, 360, 9178, 35925, 11, 4177, 11, 437, 2314, 30, 23453, 30, 2014, 291], "temperature": 0.0, "avg_logprob": -0.21613072024451363, "compression_ratio": 1.423913043478261, "no_speech_prob": 2.9731207177974284e-05}, {"id": 131, "seek": 107932, "start": 1079.32, "end": 1091.3999999999999, "text": " sure? No, no, no, no, no, no, no. Not with the new versions. Yes. That's fine. We are", "tokens": [988, 30, 883, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 13, 1726, 365, 264, 777, 9606, 13, 1079, 13, 663, 311, 2489, 13, 492, 366], "temperature": 0.0, "avg_logprob": -0.1873322795419132, "compression_ratio": 1.1971830985915493, "no_speech_prob": 0.00014450053276959807}, {"id": 132, "seek": 109140, "start": 1091.4, "end": 1117.3600000000001, "text": " only here to learn. What? Stop. Thanks. The stress, the stress. Yeah. Honestly, if there", "tokens": [787, 510, 281, 1466, 13, 708, 30, 5535, 13, 2561, 13, 440, 4244, 11, 264, 4244, 13, 865, 13, 12348, 11, 498, 456], "temperature": 0.0, "avg_logprob": -0.2209660565411603, "compression_ratio": 1.0864197530864197, "no_speech_prob": 0.00010843137715710327}, {"id": 133, "seek": 111736, "start": 1117.36, "end": 1127.0, "text": " is any, like, person here able to tell me why this Java application sometimes has issues", "tokens": [307, 604, 11, 411, 11, 954, 510, 1075, 281, 980, 385, 983, 341, 10745, 3861, 2171, 575, 2663], "temperature": 0.0, "avg_logprob": -0.13606958611066952, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4056375221116468e-05}, {"id": 134, "seek": 111736, "start": 1127.0, "end": 1142.0, "text": " starting because I've added one gig at the beginning, and it's stuck always here. So", "tokens": [2891, 570, 286, 600, 3869, 472, 8741, 412, 264, 2863, 11, 293, 309, 311, 5541, 1009, 510, 13, 407], "temperature": 0.0, "avg_logprob": -0.13606958611066952, "compression_ratio": 1.300751879699248, "no_speech_prob": 1.4056375221116468e-05}, {"id": 135, "seek": 114200, "start": 1142.0, "end": 1152.88, "text": " I can tell you what you should see normally. If I'm lucky, I made a screenshot. Yes, here,", "tokens": [286, 393, 980, 291, 437, 291, 820, 536, 5646, 13, 759, 286, 478, 6356, 11, 286, 1027, 257, 27712, 13, 1079, 11, 510, 11], "temperature": 0.0, "avg_logprob": -0.16518778678698418, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2599638466781471e-05}, {"id": 136, "seek": 114200, "start": 1152.88, "end": 1161.6, "text": " but it's the beginning, it's the rust one. So here, this is what you can have in Python.", "tokens": [457, 309, 311, 264, 2863, 11, 309, 311, 264, 15259, 472, 13, 407, 510, 11, 341, 307, 437, 291, 393, 362, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.16518778678698418, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2599638466781471e-05}, {"id": 137, "seek": 114200, "start": 1161.6, "end": 1165.6, "text": " This is what I added explicitly. I have five minutes. Well, if the demo doesn't work, it", "tokens": [639, 307, 437, 286, 3869, 20803, 13, 286, 362, 1732, 2077, 13, 1042, 11, 498, 264, 10723, 1177, 380, 589, 11, 309], "temperature": 0.0, "avg_logprob": -0.16518778678698418, "compression_ratio": 1.4806629834254144, "no_speech_prob": 1.2599638466781471e-05}, {"id": 138, "seek": 116560, "start": 1165.6, "end": 1172.04, "text": " will be much better. Then I won't have any problems with the timing. Here, you can see", "tokens": [486, 312, 709, 1101, 13, 1396, 286, 1582, 380, 362, 604, 2740, 365, 264, 10822, 13, 1692, 11, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.23264566186356217, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.5026763978530653e-05}, {"id": 139, "seek": 116560, "start": 1172.04, "end": 1182.1999999999998, "text": " that this is the trace that, yeah, this is a trace that I added manually in Python. And", "tokens": [300, 341, 307, 264, 13508, 300, 11, 1338, 11, 341, 307, 257, 13508, 300, 286, 3869, 16945, 294, 15329, 13, 400], "temperature": 0.0, "avg_logprob": -0.23264566186356217, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.5026763978530653e-05}, {"id": 140, "seek": 116560, "start": 1182.1999999999998, "end": 1193.1599999999999, "text": " here we can see that I filled the ID with the value. And on the Java sides, again, nope,", "tokens": [510, 321, 393, 536, 300, 286, 6412, 264, 7348, 365, 264, 2158, 13, 400, 322, 264, 10745, 4881, 11, 797, 11, 23444, 11], "temperature": 0.0, "avg_logprob": -0.23264566186356217, "compression_ratio": 1.5202312138728324, "no_speech_prob": 2.5026763978530653e-05}, {"id": 141, "seek": 119316, "start": 1193.16, "end": 1207.4, "text": " nope. I think it will be here. This is not the manual stuff that I added. Yes, it is,", "tokens": [23444, 13, 286, 519, 309, 486, 312, 510, 13, 639, 307, 406, 264, 9688, 1507, 300, 286, 3869, 13, 1079, 11, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.11440500460172955, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0001650380581850186}, {"id": 142, "seek": 119316, "start": 1207.4, "end": 1214.68, "text": " you have the fetch here. You have the fetch here. So this is the span that I added manually.", "tokens": [291, 362, 264, 23673, 510, 13, 509, 362, 264, 23673, 510, 13, 407, 341, 307, 264, 16174, 300, 286, 3869, 16945, 13], "temperature": 0.0, "avg_logprob": -0.11440500460172955, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0001650380581850186}, {"id": 143, "seek": 119316, "start": 1214.68, "end": 1222.68, "text": " I'm afraid that at this point, the demo just refused working. Yes, it's still stuck. I", "tokens": [286, 478, 4638, 300, 412, 341, 935, 11, 264, 10723, 445, 14654, 1364, 13, 1079, 11, 309, 311, 920, 5541, 13, 286], "temperature": 0.0, "avg_logprob": -0.11440500460172955, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0001650380581850186}, {"id": 144, "seek": 122268, "start": 1222.68, "end": 1232.1200000000001, "text": " will stop there. I won't humiliate myself further when it's done. It's done. Perhaps,", "tokens": [486, 1590, 456, 13, 286, 1582, 380, 29981, 473, 2059, 3052, 562, 309, 311, 1096, 13, 467, 311, 1096, 13, 10517, 11], "temperature": 0.0, "avg_logprob": -0.15984571244981555, "compression_ratio": 1.6603773584905661, "no_speech_prob": 7.232600182760507e-05}, {"id": 145, "seek": 122268, "start": 1232.1200000000001, "end": 1237.72, "text": " if you are interested, you can follow me on Twitter. You can follow me on MasterDone.", "tokens": [498, 291, 366, 3102, 11, 291, 393, 1524, 385, 322, 5794, 13, 509, 393, 1524, 385, 322, 6140, 35, 546, 13], "temperature": 0.0, "avg_logprob": -0.15984571244981555, "compression_ratio": 1.6603773584905661, "no_speech_prob": 7.232600182760507e-05}, {"id": 146, "seek": 122268, "start": 1237.72, "end": 1243.52, "text": " I don't know what's the ratio. More importantly, if you are interested about the GitHub repo,", "tokens": [286, 500, 380, 458, 437, 311, 264, 8509, 13, 5048, 8906, 11, 498, 291, 366, 3102, 466, 264, 23331, 49040, 11], "temperature": 0.0, "avg_logprob": -0.15984571244981555, "compression_ratio": 1.6603773584905661, "no_speech_prob": 7.232600182760507e-05}, {"id": 147, "seek": 122268, "start": 1243.52, "end": 1248.04, "text": " to do that by yourself, perhaps with better configuration of the code compose with the", "tokens": [281, 360, 300, 538, 1803, 11, 4317, 365, 1101, 11694, 295, 264, 3089, 35925, 365, 264], "temperature": 0.0, "avg_logprob": -0.15984571244981555, "compression_ratio": 1.6603773584905661, "no_speech_prob": 7.232600182760507e-05}, {"id": 148, "seek": 124804, "start": 1248.04, "end": 1253.52, "text": " right memory, it would work. And though the talk was not about Apache API 6, well, have", "tokens": [558, 4675, 11, 309, 576, 589, 13, 400, 1673, 264, 751, 390, 406, 466, 46597, 9362, 1386, 11, 731, 11, 362], "temperature": 0.0, "avg_logprob": -0.2577530832001657, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.00027594302082434297}, {"id": 149, "seek": 124804, "start": 1253.52, "end": 1260.0, "text": " a look at Apache API 6. It's an API get away, the Apache way. Great. Are there some questions", "tokens": [257, 574, 412, 46597, 9362, 1386, 13, 467, 311, 364, 9362, 483, 1314, 11, 264, 46597, 636, 13, 3769, 13, 2014, 456, 512, 1651], "temperature": 0.0, "avg_logprob": -0.2577530832001657, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.00027594302082434297}, {"id": 150, "seek": 124804, "start": 1260.0, "end": 1271.8799999999999, "text": " now? I never got so many uploads with a filling demo.", "tokens": [586, 30, 286, 1128, 658, 370, 867, 48611, 365, 257, 10623, 10723, 13], "temperature": 0.0, "avg_logprob": -0.2577530832001657, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.00027594302082434297}, {"id": 151, "seek": 127188, "start": 1271.88, "end": 1278.7600000000002, "text": " Please remain seated so we can have a Q&A. Who had a question?", "tokens": [2555, 6222, 20959, 370, 321, 393, 362, 257, 1249, 5, 32, 13, 2102, 632, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.1819941696611423, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.0008367923437617719}, {"id": 152, "seek": 127188, "start": 1278.7600000000002, "end": 1283.2, "text": " Thank you. Very good talk. I have two questions. So one is about this.", "tokens": [1044, 291, 13, 4372, 665, 751, 13, 286, 362, 732, 1651, 13, 407, 472, 307, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.1819941696611423, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.0008367923437617719}, {"id": 153, "seek": 127188, "start": 1283.2, "end": 1289.2, "text": " Let's start with the first one. Right. Yes, yes, yes. How much overhead does", "tokens": [961, 311, 722, 365, 264, 700, 472, 13, 1779, 13, 1079, 11, 2086, 11, 2086, 13, 1012, 709, 19922, 775], "temperature": 0.0, "avg_logprob": -0.1819941696611423, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.0008367923437617719}, {"id": 154, "seek": 127188, "start": 1289.2, "end": 1294.24, "text": " this bring in Python and Java or Rust? How heavy is this instrumentation?", "tokens": [341, 1565, 294, 15329, 293, 10745, 420, 34952, 30, 1012, 4676, 307, 341, 7198, 399, 30], "temperature": 0.0, "avg_logprob": -0.1819941696611423, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.0008367923437617719}, {"id": 155, "seek": 127188, "start": 1294.24, "end": 1300.96, "text": " That's a very good question. And the overheads of each request depends on your own infrastructure.", "tokens": [663, 311, 257, 588, 665, 1168, 13, 400, 264, 19922, 82, 295, 1184, 5308, 5946, 322, 428, 1065, 6896, 13], "temperature": 0.0, "avg_logprob": -0.1819941696611423, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.0008367923437617719}, {"id": 156, "seek": 130096, "start": 1300.96, "end": 1306.64, "text": " But I always have an answer to that. Is it better to go fast and you don't know where", "tokens": [583, 286, 1009, 362, 364, 1867, 281, 300, 13, 1119, 309, 1101, 281, 352, 2370, 293, 291, 500, 380, 458, 689], "temperature": 0.0, "avg_logprob": -0.13066898248134515, "compression_ratio": 1.6263157894736842, "no_speech_prob": 5.909233368583955e-05}, {"id": 157, "seek": 130096, "start": 1306.64, "end": 1313.1200000000001, "text": " you are going to go a bit slower and to know where you are going?", "tokens": [291, 366, 516, 281, 352, 257, 857, 14009, 293, 281, 458, 689, 291, 366, 516, 30], "temperature": 0.0, "avg_logprob": -0.13066898248134515, "compression_ratio": 1.6263157894736842, "no_speech_prob": 5.909233368583955e-05}, {"id": 158, "seek": 130096, "start": 1313.1200000000001, "end": 1321.28, "text": " I think that whatever the cost, it's always easy to add additional resources and it doesn't", "tokens": [286, 519, 300, 2035, 264, 2063, 11, 309, 311, 1009, 1858, 281, 909, 4497, 3593, 293, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.13066898248134515, "compression_ratio": 1.6263157894736842, "no_speech_prob": 5.909233368583955e-05}, {"id": 159, "seek": 130096, "start": 1321.28, "end": 1324.88, "text": " cost you that much. Whereas a debug incident across a distributed", "tokens": [2063, 291, 300, 709, 13, 13813, 257, 24083, 9348, 2108, 257, 12631], "temperature": 0.0, "avg_logprob": -0.13066898248134515, "compression_ratio": 1.6263157894736842, "no_speech_prob": 5.909233368583955e-05}, {"id": 160, "seek": 132488, "start": 1324.88, "end": 1331.2800000000002, "text": " system can cost you days or even like weeks in injuring costs. And you are very, very", "tokens": [1185, 393, 2063, 291, 1708, 420, 754, 411, 3259, 294, 5580, 1345, 5497, 13, 400, 291, 366, 588, 11, 588], "temperature": 0.0, "avg_logprob": -0.23960927589652464, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003359621041454375}, {"id": 161, "seek": 132488, "start": 1331.2800000000002, "end": 1336.0400000000002, "text": " expensive, right? Okay. Thank you. And the second one is have", "tokens": [5124, 11, 558, 30, 1033, 13, 1044, 291, 13, 400, 264, 1150, 472, 307, 362], "temperature": 0.0, "avg_logprob": -0.23960927589652464, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003359621041454375}, {"id": 162, "seek": 132488, "start": 1336.0400000000002, "end": 1342.7600000000002, "text": " you encountered any funny issues with multi-threading or multi-processing? Something like when your", "tokens": [291, 20381, 604, 4074, 2663, 365, 4825, 12, 392, 35908, 420, 4825, 12, 41075, 278, 30, 6595, 411, 562, 428], "temperature": 0.0, "avg_logprob": -0.23960927589652464, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003359621041454375}, {"id": 163, "seek": 132488, "start": 1342.7600000000002, "end": 1345.7600000000002, "text": " server just now... Can you come closer to your...", "tokens": [7154, 445, 586, 485, 1664, 291, 808, 4966, 281, 428, 485], "temperature": 0.0, "avg_logprob": -0.23960927589652464, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003359621041454375}, {"id": 164, "seek": 132488, "start": 1345.7600000000002, "end": 1352.16, "text": " Your server just now was not starting. So some software, when you have multi-threading", "tokens": [2260, 7154, 445, 586, 390, 406, 2891, 13, 407, 512, 4722, 11, 562, 291, 362, 4825, 12, 392, 35908], "temperature": 0.0, "avg_logprob": -0.23960927589652464, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0003359621041454375}, {"id": 165, "seek": 135216, "start": 1352.16, "end": 1358.68, "text": " or multi-processing and have you encountered any issues when the instrumentation costs", "tokens": [420, 4825, 12, 41075, 278, 293, 362, 291, 20381, 604, 2663, 562, 264, 7198, 399, 5497], "temperature": 0.0, "avg_logprob": -0.17419031054474587, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00021199279581196606}, {"id": 166, "seek": 135216, "start": 1358.68, "end": 1362.4, "text": " you trouble? This is not production stuff. This is just", "tokens": [291, 5253, 30, 639, 307, 406, 4265, 1507, 13, 639, 307, 445], "temperature": 0.0, "avg_logprob": -0.17419031054474587, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00021199279581196606}, {"id": 167, "seek": 135216, "start": 1362.4, "end": 1366.52, "text": " better than the hello world. So I cannot tell you about prediction issues.", "tokens": [1101, 813, 264, 7751, 1002, 13, 407, 286, 2644, 980, 291, 466, 17630, 2663, 13], "temperature": 0.0, "avg_logprob": -0.17419031054474587, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00021199279581196606}, {"id": 168, "seek": 135216, "start": 1366.52, "end": 1371.1200000000001, "text": " You should find people who have these issues. As I mentioned, it's a developers-oriented", "tokens": [509, 820, 915, 561, 567, 362, 613, 2663, 13, 1018, 286, 2835, 11, 309, 311, 257, 8849, 12, 27414], "temperature": 0.0, "avg_logprob": -0.17419031054474587, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00021199279581196606}, {"id": 169, "seek": 135216, "start": 1371.1200000000001, "end": 1375.68, "text": " talk. So it's more about pushing the developers to help", "tokens": [751, 13, 407, 309, 311, 544, 466, 7380, 264, 8849, 281, 854], "temperature": 0.0, "avg_logprob": -0.17419031054474587, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00021199279581196606}, {"id": 170, "seek": 137568, "start": 1375.68, "end": 1382.44, "text": " up to their job. For production issues, I must admit I have no clue.", "tokens": [493, 281, 641, 1691, 13, 1171, 4265, 2663, 11, 286, 1633, 9796, 286, 362, 572, 13602, 13], "temperature": 0.0, "avg_logprob": -0.2386097838913185, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0006725671701133251}, {"id": 171, "seek": 137568, "start": 1382.44, "end": 1389.1200000000001, "text": " Hi. In the case of runtime, does it always work", "tokens": [2421, 13, 682, 264, 1389, 295, 34474, 11, 775, 309, 1009, 589], "temperature": 0.0, "avg_logprob": -0.2386097838913185, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0006725671701133251}, {"id": 172, "seek": 137568, "start": 1389.1200000000001, "end": 1394.76, "text": " with also badly written application? I mean, how bad can an application be before it stops", "tokens": [365, 611, 13425, 3720, 3861, 30, 286, 914, 11, 577, 1578, 393, 364, 3861, 312, 949, 309, 10094], "temperature": 0.0, "avg_logprob": -0.2386097838913185, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0006725671701133251}, {"id": 173, "seek": 137568, "start": 1394.76, "end": 1401.3200000000002, "text": " working? I'm not sure. I understood the question.", "tokens": [1364, 30, 286, 478, 406, 988, 13, 286, 7320, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2386097838913185, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0006725671701133251}, {"id": 174, "seek": 140132, "start": 1401.32, "end": 1405.84, "text": " So how often do you need to do it before it stops working?", "tokens": [407, 577, 2049, 360, 291, 643, 281, 360, 309, 949, 309, 10094, 1364, 30], "temperature": 0.0, "avg_logprob": -0.19160377147585847, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0005771892610937357}, {"id": 175, "seek": 140132, "start": 1405.84, "end": 1414.2, "text": " No, no. I mean, let's say I use deprecated libraries, bad clients, something that doesn't", "tokens": [883, 11, 572, 13, 286, 914, 11, 718, 311, 584, 286, 764, 1367, 13867, 770, 15148, 11, 1578, 6982, 11, 746, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.19160377147585847, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0005771892610937357}, {"id": 176, "seek": 140132, "start": 1414.2, "end": 1420.04, "text": " work as it's supposed to be for the instrumentation perspective. I mean, I do request to the network", "tokens": [589, 382, 309, 311, 3442, 281, 312, 337, 264, 7198, 399, 4585, 13, 286, 914, 11, 286, 360, 5308, 281, 264, 3209], "temperature": 0.0, "avg_logprob": -0.19160377147585847, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0005771892610937357}, {"id": 177, "seek": 140132, "start": 1420.04, "end": 1429.56, "text": " using UDP clients, something I've written myself, some custom stuff that...", "tokens": [1228, 624, 11373, 6982, 11, 746, 286, 600, 3720, 2059, 11, 512, 2375, 1507, 300, 485], "temperature": 0.0, "avg_logprob": -0.19160377147585847, "compression_ratio": 1.5330188679245282, "no_speech_prob": 0.0005771892610937357}, {"id": 178, "seek": 142956, "start": 1429.56, "end": 1439.36, "text": " I'm imagining that the instrumentation sits between some layer of the network, which is", "tokens": [286, 478, 27798, 300, 264, 7198, 399, 12696, 1296, 512, 4583, 295, 264, 3209, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.18967905750981084, "compression_ratio": 1.3885350318471337, "no_speech_prob": 0.00025242887204512954}, {"id": 179, "seek": 142956, "start": 1439.36, "end": 1446.84, "text": " going to the Internet, for example. And so how bad can I be before it stops recognizing", "tokens": [516, 281, 264, 7703, 11, 337, 1365, 13, 400, 370, 577, 1578, 393, 286, 312, 949, 309, 10094, 18538], "temperature": 0.0, "avg_logprob": -0.18967905750981084, "compression_ratio": 1.3885350318471337, "no_speech_prob": 0.00025242887204512954}, {"id": 180, "seek": 142956, "start": 1446.84, "end": 1451.84, "text": " a request from junk? You cannot be banned.", "tokens": [257, 5308, 490, 19109, 30, 509, 2644, 312, 19564, 13], "temperature": 0.0, "avg_logprob": -0.18967905750981084, "compression_ratio": 1.3885350318471337, "no_speech_prob": 0.00025242887204512954}, {"id": 181, "seek": 145184, "start": 1451.84, "end": 1461.76, "text": " OK. Well, it's a moral issue first. But then on the platform side, the Austo instrumentation,", "tokens": [2264, 13, 1042, 11, 309, 311, 257, 9723, 2734, 700, 13, 583, 550, 322, 264, 3663, 1252, 11, 264, 4126, 78, 7198, 399, 11], "temperature": 0.0, "avg_logprob": -0.16917738318443298, "compression_ratio": 1.5060975609756098, "no_speech_prob": 8.076838275883347e-05}, {"id": 182, "seek": 145184, "start": 1461.76, "end": 1469.08, "text": " they work with specific frameworks and tools. It's those frameworks and tools that know", "tokens": [436, 589, 365, 2685, 29834, 293, 3873, 13, 467, 311, 729, 29834, 293, 3873, 300, 458], "temperature": 0.0, "avg_logprob": -0.16917738318443298, "compression_ratio": 1.5060975609756098, "no_speech_prob": 8.076838275883347e-05}, {"id": 183, "seek": 145184, "start": 1469.08, "end": 1475.8799999999999, "text": " how to check what happens and to send the data to open telemetry.", "tokens": [577, 281, 1520, 437, 2314, 293, 281, 2845, 264, 1412, 281, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.16917738318443298, "compression_ratio": 1.5060975609756098, "no_speech_prob": 8.076838275883347e-05}, {"id": 184, "seek": 147588, "start": 1475.88, "end": 1482.5600000000002, "text": " So if you don't play in this game, nothing will be sent.", "tokens": [407, 498, 291, 500, 380, 862, 294, 341, 1216, 11, 1825, 486, 312, 2279, 13], "temperature": 0.0, "avg_logprob": -0.13530487350270717, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.00030864981818012893}, {"id": 185, "seek": 147588, "start": 1482.5600000000002, "end": 1489.0800000000002, "text": " On the manual instrumentation side, it's an explicit call. So it depends what you want", "tokens": [1282, 264, 9688, 7198, 399, 1252, 11, 309, 311, 364, 13691, 818, 13, 407, 309, 5946, 437, 291, 528], "temperature": 0.0, "avg_logprob": -0.13530487350270717, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.00030864981818012893}, {"id": 186, "seek": 147588, "start": 1489.0800000000002, "end": 1495.16, "text": " to send. Yeah. I was thinking of auto instrumentation.", "tokens": [281, 2845, 13, 865, 13, 286, 390, 1953, 295, 8399, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.13530487350270717, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.00030864981818012893}, {"id": 187, "seek": 147588, "start": 1495.16, "end": 1503.0400000000002, "text": " So let's say I do the NS resolution by myself and then I just throw a request to an IP.", "tokens": [407, 718, 311, 584, 286, 360, 264, 15943, 8669, 538, 2059, 293, 550, 286, 445, 3507, 257, 5308, 281, 364, 8671, 13], "temperature": 0.0, "avg_logprob": -0.13530487350270717, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.00030864981818012893}, {"id": 188, "seek": 150304, "start": 1503.04, "end": 1514.24, "text": " Let me show the Python stuff here. This is what I showed you in the screenshot.", "tokens": [961, 385, 855, 264, 15329, 1507, 510, 13, 639, 307, 437, 286, 4712, 291, 294, 264, 27712, 13], "temperature": 0.0, "avg_logprob": -0.12071636415296985, "compression_ratio": 1.5031847133757963, "no_speech_prob": 3.1015879358164966e-05}, {"id": 189, "seek": 150304, "start": 1514.24, "end": 1521.0, "text": " This is what I write. And this is the attributes that I want to have.", "tokens": [639, 307, 437, 286, 2464, 13, 400, 341, 307, 264, 17212, 300, 286, 528, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.12071636415296985, "compression_ratio": 1.5031847133757963, "no_speech_prob": 3.1015879358164966e-05}, {"id": 190, "seek": 150304, "start": 1521.0, "end": 1529.3999999999999, "text": " So basically, if here you have something that is completely unrelated, it's up to you.", "tokens": [407, 1936, 11, 498, 510, 291, 362, 746, 300, 307, 2584, 38967, 11, 309, 311, 493, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.12071636415296985, "compression_ratio": 1.5031847133757963, "no_speech_prob": 3.1015879358164966e-05}, {"id": 191, "seek": 152940, "start": 1529.4, "end": 1533.44, "text": " That's why it's easier to start with auto instrumentation.", "tokens": [663, 311, 983, 309, 311, 3571, 281, 722, 365, 8399, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.13122283638297738, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.804031853680499e-05}, {"id": 192, "seek": 152940, "start": 1533.44, "end": 1539.2, "text": " And then once you get a general overview of what you have and your app starts saying,", "tokens": [400, 550, 1564, 291, 483, 257, 2674, 12492, 295, 437, 291, 362, 293, 428, 724, 3719, 1566, 11], "temperature": 0.0, "avg_logprob": -0.13122283638297738, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.804031853680499e-05}, {"id": 193, "seek": 152940, "start": 1539.2, "end": 1547.0, "text": " hey, perhaps we want to have more details here, then you can come with manual instrumentation.", "tokens": [4177, 11, 4317, 321, 528, 281, 362, 544, 4365, 510, 11, 550, 291, 393, 808, 365, 9688, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.13122283638297738, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.804031853680499e-05}, {"id": 194, "seek": 152940, "start": 1547.0, "end": 1554.3600000000001, "text": " But start with the less expensive stuff. I didn't really answer the question.", "tokens": [583, 722, 365, 264, 1570, 5124, 1507, 13, 286, 994, 380, 534, 1867, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13122283638297738, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.804031853680499e-05}, {"id": 195, "seek": 155436, "start": 1554.36, "end": 1560.24, "text": " I understand it. But that's the best I can do regarding it.", "tokens": [286, 1223, 309, 13, 583, 300, 311, 264, 1151, 286, 393, 360, 8595, 309, 13], "temperature": 0.0, "avg_logprob": -0.3632267543247768, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.000825700699351728}, {"id": 196, "seek": 155436, "start": 1560.24, "end": 1565.84, "text": " Sorry. Okay. Thanks for the talk.", "tokens": [4919, 13, 1033, 13, 2561, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.3632267543247768, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.000825700699351728}, {"id": 197, "seek": 155436, "start": 1565.84, "end": 1571.4799999999998, "text": " For the agent you use in the Docker file, how you can configure it, for example, for", "tokens": [1171, 264, 9461, 291, 764, 294, 264, 33772, 3991, 11, 577, 291, 393, 22162, 309, 11, 337, 1365, 11, 337], "temperature": 0.0, "avg_logprob": -0.3632267543247768, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.000825700699351728}, {"id": 198, "seek": 155436, "start": 1571.4799999999998, "end": 1578.8, "text": " the tracing for Jagger or other stuff. Regarding the Docker file, sorry?", "tokens": [264, 25262, 337, 9014, 1321, 420, 661, 1507, 13, 35523, 264, 33772, 3991, 11, 2597, 30], "temperature": 0.0, "avg_logprob": -0.3632267543247768, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.000825700699351728}, {"id": 199, "seek": 157880, "start": 1578.8, "end": 1585.6, "text": " Yeah. How you can configure the agent to send the tracing for Jagger or other stuff.", "tokens": [865, 13, 1012, 291, 393, 22162, 264, 9461, 281, 2845, 264, 25262, 337, 9014, 1321, 420, 661, 1507, 13], "temperature": 0.0, "avg_logprob": -0.24185640435469777, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0004195847432129085}, {"id": 200, "seek": 157880, "start": 1585.6, "end": 1588.56, "text": " The Docker file doesn't mention where you send it.", "tokens": [440, 33772, 3991, 1177, 380, 2152, 689, 291, 2845, 309, 13], "temperature": 0.0, "avg_logprob": -0.24185640435469777, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0004195847432129085}, {"id": 201, "seek": 157880, "start": 1588.56, "end": 1592.8799999999999, "text": " The Docker file just says, hey, I will use open telemetry.", "tokens": [440, 33772, 3991, 445, 1619, 11, 4177, 11, 286, 486, 764, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.24185640435469777, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0004195847432129085}, {"id": 202, "seek": 157880, "start": 1592.8799999999999, "end": 1601.08, "text": " And it's during configuration, it's like in the Docker Compulse file where I'm using,", "tokens": [400, 309, 311, 1830, 11694, 11, 309, 311, 411, 294, 264, 33772, 6620, 19258, 3991, 689, 286, 478, 1228, 11], "temperature": 0.0, "avg_logprob": -0.24185640435469777, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0004195847432129085}, {"id": 203, "seek": 157880, "start": 1601.08, "end": 1607.04, "text": " like, agreed upon environment variables where I'm saying you should set it here or here", "tokens": [411, 11, 9166, 3564, 2823, 9102, 689, 286, 478, 1566, 291, 820, 992, 309, 510, 420, 510], "temperature": 0.0, "avg_logprob": -0.24185640435469777, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0004195847432129085}, {"id": 204, "seek": 160704, "start": 1607.04, "end": 1610.56, "text": " or you should use logging or tracing or metrics or whatever.", "tokens": [420, 291, 820, 764, 27991, 420, 25262, 420, 16367, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 205, "seek": 160704, "start": 1610.56, "end": 1615.08, "text": " So that's very important to, like, separate those concerns.", "tokens": [407, 300, 311, 588, 1021, 281, 11, 411, 11, 4994, 729, 7389, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 206, "seek": 160704, "start": 1615.08, "end": 1620.2, "text": " On one side in the Docker file in the image, you say, hey, I'm ready for open telemetry.", "tokens": [1282, 472, 1252, 294, 264, 33772, 3991, 294, 264, 3256, 11, 291, 584, 11, 4177, 11, 286, 478, 1919, 337, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 207, "seek": 160704, "start": 1620.2, "end": 1625.52, "text": " And when you actually deploy it to say, okay, open telemetry will go there for the metrics", "tokens": [400, 562, 291, 767, 7274, 309, 281, 584, 11, 1392, 11, 1269, 4304, 5537, 627, 486, 352, 456, 337, 264, 16367], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 208, "seek": 160704, "start": 1625.52, "end": 1632.1599999999999, "text": " and there for the tracing and for logging, I will disable it or whatever.", "tokens": [293, 456, 337, 264, 25262, 293, 337, 27991, 11, 286, 486, 28362, 309, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 209, "seek": 160704, "start": 1632.1599999999999, "end": 1633.1599999999999, "text": " Thank you for... Oh, sorry.", "tokens": [1044, 291, 337, 485, 876, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 210, "seek": 160704, "start": 1633.1599999999999, "end": 1634.1599999999999, "text": " Sorry. Go ahead.", "tokens": [4919, 13, 1037, 2286, 13], "temperature": 0.0, "avg_logprob": -0.20551247432314115, "compression_ratio": 1.7172131147540983, "no_speech_prob": 9.435806714463979e-05}, {"id": 211, "seek": 163416, "start": 1634.16, "end": 1640.8000000000002, "text": " Sorry. And then you have a Docker image that can be, like, reusable.", "tokens": [4919, 13, 400, 550, 291, 362, 257, 33772, 3256, 300, 393, 312, 11, 411, 11, 41807, 13], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 212, "seek": 163416, "start": 1640.8000000000002, "end": 1644.0, "text": " Thank you for being good first-time citizens to remain seated.", "tokens": [1044, 291, 337, 885, 665, 700, 12, 3766, 7180, 281, 6222, 20959, 13], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 213, "seek": 163416, "start": 1644.0, "end": 1646.2, "text": " Next question.", "tokens": [3087, 1168, 13], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 214, "seek": 163416, "start": 1646.2, "end": 1647.96, "text": " Thank you for your presentation.", "tokens": [1044, 291, 337, 428, 5860, 13], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 215, "seek": 163416, "start": 1647.96, "end": 1657.24, "text": " So my question is does open telemetry support error handling like sentry?", "tokens": [407, 452, 1168, 307, 775, 1269, 4304, 5537, 627, 1406, 6713, 13175, 411, 2279, 627, 30], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 216, "seek": 163416, "start": 1657.24, "end": 1660.8000000000002, "text": " If not, is there any plans to do that?", "tokens": [759, 406, 11, 307, 456, 604, 5482, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.211114239692688, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0009232193697243929}, {"id": 217, "seek": 166080, "start": 1660.8, "end": 1666.76, "text": " It's really useful to catch crashes and capture the context of the crash.", "tokens": [467, 311, 534, 4420, 281, 3745, 28642, 293, 7983, 264, 4319, 295, 264, 8252, 13], "temperature": 0.0, "avg_logprob": -0.28222036361694336, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013101499062031507}, {"id": 218, "seek": 166080, "start": 1666.76, "end": 1669.76, "text": " So that's it. Thank you.", "tokens": [407, 300, 311, 309, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.28222036361694336, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013101499062031507}, {"id": 219, "seek": 166080, "start": 1669.76, "end": 1676.28, "text": " If it happens, when you mean crashes of open telemetry itself or of the components that", "tokens": [759, 309, 2314, 11, 562, 291, 914, 28642, 295, 1269, 4304, 5537, 627, 2564, 420, 295, 264, 6677, 300], "temperature": 0.0, "avg_logprob": -0.28222036361694336, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013101499062031507}, {"id": 220, "seek": 166080, "start": 1676.28, "end": 1678.8, "text": " are, like, under watch?", "tokens": [366, 11, 411, 11, 833, 1159, 30], "temperature": 0.0, "avg_logprob": -0.28222036361694336, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013101499062031507}, {"id": 221, "seek": 166080, "start": 1678.8, "end": 1682.36, "text": " Yeah, of the application that's monitored, yeah.", "tokens": [865, 11, 295, 264, 3861, 300, 311, 36255, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.28222036361694336, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013101499062031507}, {"id": 222, "seek": 168236, "start": 1682.36, "end": 1692.12, "text": " Well, Fabian showed you how you could log and, like, bind your traces and your logs.", "tokens": [1042, 11, 17440, 952, 4712, 291, 577, 291, 727, 3565, 293, 11, 411, 11, 14786, 428, 26076, 293, 428, 20820, 13], "temperature": 0.0, "avg_logprob": -0.1645354531028054, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.00023187653278000653}, {"id": 223, "seek": 168236, "start": 1692.12, "end": 1694.0, "text": " So you could have both here.", "tokens": [407, 291, 727, 362, 1293, 510, 13], "temperature": 0.0, "avg_logprob": -0.1645354531028054, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.00023187653278000653}, {"id": 224, "seek": 168236, "start": 1694.0, "end": 1701.32, "text": " My focus was just on tracing, but you can reuse the same Docker, the same GitHub repo", "tokens": [1222, 1879, 390, 445, 322, 25262, 11, 457, 291, 393, 26225, 264, 912, 33772, 11, 264, 912, 23331, 49040], "temperature": 0.0, "avg_logprob": -0.1645354531028054, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.00023187653278000653}, {"id": 225, "seek": 170132, "start": 1701.32, "end": 1712.6799999999998, "text": " and just, like, here, put the logs somewhere in, I don't know, Elasticsearch or whatever.", "tokens": [293, 445, 11, 411, 11, 510, 11, 829, 264, 20820, 4079, 294, 11, 286, 500, 380, 458, 11, 2699, 2750, 405, 1178, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.25948619842529297, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.0485199860995635e-05}, {"id": 226, "seek": 170132, "start": 1712.6799999999998, "end": 1718.6799999999998, "text": " No, because it's not a sponsored room.", "tokens": [883, 11, 570, 309, 311, 406, 257, 16621, 1808, 13], "temperature": 0.0, "avg_logprob": -0.25948619842529297, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.0485199860995635e-05}, {"id": 227, "seek": 170132, "start": 1718.6799999999998, "end": 1723.72, "text": " And then you can check and you introduce some errors and then you can check how the two are", "tokens": [400, 550, 291, 393, 1520, 293, 291, 5366, 512, 13603, 293, 550, 291, 393, 1520, 577, 264, 732, 366], "temperature": 0.0, "avg_logprob": -0.25948619842529297, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.0485199860995635e-05}, {"id": 228, "seek": 170132, "start": 1723.72, "end": 1728.08, "text": " bound and you can, like, drill down to where it failed.", "tokens": [5472, 293, 291, 393, 11, 411, 11, 11392, 760, 281, 689, 309, 7612, 13], "temperature": 0.0, "avg_logprob": -0.25948619842529297, "compression_ratio": 1.5077720207253886, "no_speech_prob": 4.0485199860995635e-05}, {"id": 229, "seek": 172808, "start": 1728.08, "end": 1735.08, "text": " Okay, thank you.", "tokens": [50364, 1033, 11, 1309, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5719844698905945, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.0009935018606483936}], "language": "en"}