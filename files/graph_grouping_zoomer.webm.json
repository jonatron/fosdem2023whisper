{"text": " The next board is on graph aggregation, graph grouping, especially on streaming graphs. Graph grouping is a really interesting challenge because we've all seen individualizations, hairballs, and complex graphs. What graph grouping allows you is ready to pick these graphs, group them by certain attributes, and you have kind of these better nodes that then can be selectively expanded. But you can also then, on a group graph, which is mostly a monopod type graph, in many cases, you can also run graph analytics, which is a really interesting problem. So I'm really excited to have Christopher here because both working on streaming graphs as well on temporary graphs with graph grouping is a really challenging and interesting aspect. So I'm really looking forward to the talk, and so without further ado, I come to the graph network. Yeah, thanks. Yeah, so thanks for that introduction and also thanks for accepting my abstract for this talk. Yeah, my name is Christopher Rost. I'm a PhD student of the University of Leipzig, and I'm currently writing my thesis, so I'm glad that I bring some free time to doing this talk. Yeah, so about us or our team, so that's me, and I have also a master's student working on this project. We called Graph Stream Zoomer, and this project is a result of two master's thesis of our university from Lia Salman and Rana Nurideen, and I think it's also nice to show the result of a master's thesis here at the FOSTEM. At the top is our professor of the database department. Yeah, just to say that. Okay, what you should take away from this talk. So you will see what the property Graph Stream is and why it's important to have the streaming idea inside the Graph topic, and second why you should or should not group a Graph Stream, and then you will learn what the Graph Stream Zoomer, so this specific project is, and the main idea behind it, because we provided Java API to do that. Okay, let's just start what is an event stream. I think I don't can skip that maybe. So we say that anything that happens as a specific time and that can be recorded is an event, and if an event stream is now stream of these events, so a sequence that is ordered by time. So and I think everyone knows why we need event processing, so we cannot store everything into a database or whatever to analyze it. So I want to identify these meaningful events and respond to them as quickly as possible. Okay, what is now a Graph Stream? A Graph Stream is an event stream where each event is a Graph element or some Graph update. Yeah, that could be edges, could be vertices, triples or whatever. And a Graph Update could be a modification of this. For example, the addition of an edge, the removal of an edge, the addition of a property or an edge or whatever. So this is just an overview. Okay, why should I use now a Graph Stream? Because I can execute on this Graph Stream all algorithms and also all mathematical stuff from Graph Theory on this stream of Graph Data. For example, calculate page strength concurrently with the evolving Graph of the Graph Stream. Okay, I can update my analysis results with a low latency if I combine that in a stream processing engine. And my goal is to monitor the changes or monitor the changes in the Graph or to create some notification or some reactivity. For example, if something, some average goes over threshold, then I create a notification. Okay, by now, the Graph Stream could be very heterogeneous. That means it consists of many different types and it can also occur on a high frequency. So it is advisable to summarize the Graph elements in a specific way. And we can summarize Graph elements by three criteria. For example, by time, that means Graph elements that belong together. For example, the time window, we group them together by structure. That means, for example, edges that share the same source or target vertex can be grouped together. And by content, that means vertices and edges that share the same label or a specific value of a property. And we introduced for our algorithm so-called grouping key functions. That means it is a function that maps a vertex or an edge to a grouping key. And that could be everything that is inside this vertex or edge. It could be labeled, temporal information, some kind of property or whatever. So you can map everything that is represented by a vertex or edge to a key function or to a key. And on this key, we group the Graph. So that means, at the end, the result is again a Graph stream, but the grouped representation of that. Okay, now you can question, okay, why I need this? So what are the applications of that? You can think about it as a pre-processing step. For example, before you calculate the page frames, you just group the vertices to the city attribute of users together or something like this. You also use it as a pre-processing step. Second application could be as a post-processing step. For example, after you apply the Graph stream analysis, for example, a community detection, you can now group on this cluster ID with our grouping algorithm to summarize the different communities together. You can also use it to understand the Graph stream in more detail. For example, okay, just to know which vertex or edge types exist in my Graph stream, how frequent these different types arrive, or how vertices and edges with different characteristics are connected together. So just to get deeper insights, or if you use our aggregation functions or aggregation, for example, counting or calculating an average on that, you can also get or reveal some hidden information that you would not see in the Graph stream itself. Okay, so this is an introduction. Now I explain the ideas behind this Graph stream zoomer application just by an example, and then afterwards I summarize this. For example, we're using bike rental data that can have two different Graph schemas. I named them A and B. So the first Graph schema A is that a bike rental is an edge between two station nodes. You see it on the left side. So a station has several properties like the name, the number of bikes, latitude, longitude, and so on. And the trip edge has properties like the user ID, so who rented the bike, which bike was used by the bike ID, and from and to, so when this trip happens, until when, and the duration, for example, in seconds. So this is schema A on the left side. On the right side we have a more heterogeneous schema. So we have also stations and trips as vertices here. And we have also bike nodes and user nodes with several properties. So I just divided into this because I can explain the examples that follow a bit better compared to just using a simple schema like here on the left side. Okay, so how a Graph stream of these schemas could look like this. Yeah, of schema A I have just these trip edges between two stations and all information inside. And from schema B I have here the trip nodes connected with all the other vertex types. So this is just an exemplary view how a stream of this graph data could look like. Okay, so now begin with a very simple or a simple example of our grouping algorithm. So the input of the grouping is the graph stream, I think it's clear, and we need a grouping configuration. And the grouping configuration consists of five attributes. The first is the window because we are doing windowing on our graph stream. So I can define a window size, which is here, for example, 10 minutes. And then VG key are the vertex grouping keys, that means the key functions that leads to the grouping of vertices. EG key are the edge grouping keys, that means the key functions that are needed to group the edges together. And we also have a collection of vertex aggregate functions. This is VA-GG and EA-GG are the group of edge aggregate functions. So the four on the bottom are just an array of several key functions and aggregate functions. Okay, and now having the input stream and applying that grouping, we get a result. And just looking like this, because we define for the vertex grouping keys a function that maps every vertex to an integer value. And that results in, it doesn't matter which vertex exists in our graph stream, we group everything together to one vertex. And that's the white one with an empty label. And the same for edges, that means every edge that exists in our graph stream, we group them together to a super edge, we call them super vertex and super edge that is displayed here in gray. And because of the count aggregate functions, we add a new property to the super vertex with the count of all vertices that are grouped together. And a new property to the super edge with also the count value. And this is now our result for every window that we defined on the graph stream. For example, here the first window, second window and so on. That means we are creating a stream of grouped graphs here. Okay, that is for schema A and that's the most zoomed out view. So I group everything together that exists in the graph stream. For schema B and same grouping configuration, it looks the same because it doesn't matter which type label exists, so we group everything together. So we have just different counts because we have a bit more vertices and edges and also for the second window it looks the same. Okay, so this is my first example, so the most zoomed out way. The second example are called a graph stream schema. So that means now we are using as a vertex grouping key a function that maps a vertex to its label and a function that maps the edge to its label. So what's the result here? Now our node has a label station and the count because the count aggregate function stays the same and our edge has a label trip. So it's more or less the same because our graph streamer has just one node type and one edge type and that's it also for the second window. Now it gets a bit more interesting when we are using schema B. The result here with the same grouping configuration as before is now this. That means every vertex is grouped by their label and every edge is grouped by their label and now I have like a schema representation of my graph stream. And again with all counts because we are just using count aggregation. Okay, and that's for the second window and so on and so on, I just leave here the properties. Okay, the next example we stay with the vertex grouping keys and edge grouping keys, but now I added several aggregate functions to vertices and edges. For example I say okay for the vertices I want to calculate the average of all available bikes for these stations. And for the edge aggregate functions I want to have the minimum, maximum and average duration that a trip between two vertices has. And the result would be this. So same grouped graph, but now I have three additional properties on the edges. Minimum duration, it's in seconds here, maximum duration and average duration and the average bikes available on this station also as a new property. Same for the second window. And now my last example, it is, I call it, not the last example, there's one more. So I added now here a second vertex grouping key function and that's an important thing of the graph stream group. You can also implement your own grouping key function. For example this one called getDistrict consumes the latitude and longitude property of the vertices and then calculates like a district identifier. For example here of Priscilla, whatever, so in which district that belongs to. And then the graph is grouped on this representing district identifier. And we also say that for the edges we want to group the edges on the user type. So that means for every edge, so in this data set we have a user type subscriber and something else we will see it shortly. So that means for every edge I get new now two edges, one for this one user type, one for the other one. And also here some aggregate functions added. And the result is something like this. So here exemplified for three stations. And here the district ID one, two, and three and the average latitude and longitude for example for visualization, proposes to place it on the map. And for the trips between two stations or between two district representatives, we calculated also the minimum, maximum and average duration and counted them. And you see here the green edges are for the user type customer and the red edges are for user type subscriber. So and the last example is then this one here if I say okay as vertex grouping key functions, I say please extract me the identifier of that. That means every vertex that exists in the graph stream is placed here and also for the edge identifier. That means since we have unique identifiers, every vertex and edges are placed here. And this is more or less like a snapshot of the current state of this graph stream for the specific window. So therefore I call that zoomed in. It's the most zoomed in configuration that you could use. Okay, you could imagine implementing this is not that easy. So the master students found a way using Apache Flink and its table API. So everything works distributed since we are using just the API functions of Apache Flink. But we also figured out several implementation challenges. So first was to find a good graph representation. Second one is since we are creating a workflow of this graph stream, we have to ensure the chronological ordering of every step in this workflow. As a third point is also you want to ensure the scalability. Since if we scale out this algorithm, the scalability should be also high. And also keep the state as minimum as possible and provide a low latency and high throughput. So these were several challenges the master student solved quite well. And at the end we created a grouping operator looking like this. I don't want to get into detail. This is just an architectural overview of every Flink steps we used. What is quite interesting is that we created like an operator encapsulation of this. That means the operator consumes a graph stream at input and has a graph stream as output. That means you can combine several of these grouping operators. Or if you define another graph stream algorithm that produces a graph stream as output, you can just put them before. So you can like chaining these grouping operators together. And like I said, this consists of the mapping of the input data, the duplication of vertices, grouping of vertices and edges, and then mapping it to an output graph stream. How an API would look like? It looks a bit messy, but I think it's quite fast clear what's happening here. So first you have to define the execution environment of Flink. Then we read the data from some streaming source, for example, a socket source, or some Kafka stream, or whatever you want, whatever Flink supports in our case. Then we map it to a graph stream object, which is the internal representation of our stream. The interesting part here, you define the grouping operator. So in the middle, that's the grouping config I showed you in the examples. You can define it here by an API. You set the window size. You set the vertex and edge grouping keys. You set the aggregate functions and so on. And at the end, you just execute this operator on this graph stream, and then you can define a thing or just print it to the console, or whatever you want. So that's the operator call, how you define it in the API. And current state, the students are about at 90% of the complete implementation of that. We figured out some bugs at the SQL or at the table API of Flink that were not fixed yet, so we had to define some workaround that cost us time. But like I said, we found the workaround. And the next steps are that we plan an evaluation. So how is the latency and throughput of this complete system? And we want to test it on real-world and synthetic graph streams. And maybe then publish some results, so let's see. And also, the user-defined key and aggregate functions are still under development. Okay, then that's it. That's all folks. Please check out our GitHub repository, or maybe you want to contribute, so we are open for this. The two links here at the bottom are also the icons are two other projects. The one is Gradube. This is a big temporary graph processing engine also based on Apache Flink. So there where I'm also a main contributor to that project, which was initially created by Marty Nugans, who's now working at Neo4j. And also, the temporary graph explorer is a user interface for that system, where you can play around with the evolution of a graph, but in a historical data set. Okay, so that's it. And please ask questions. Thanks. Yeah, please. On one slide, you said a problem was to decide on the, on slide 20, I think, as well. Yeah. The optimal graph representation in the streaming model. Yeah. What was the answer? And so the question was, what, so we had this challenge to find the optimal graph representation, and what was the answer? The answer was a triple stream, but a rich triple stream, we called it, since two property graph vertices are connected with an edge. That means every vertex consists of the label and possibly a big set of key value pairs as properties, and the same for the edges. And this was our optimal, because you can then model everything with this model. But the counterpart of this was in here that we have to do a vertex de-deplication. For example, if you have a self-loop, so from one vertex to another one, we have a duplicate of that vertex, so we have to de-deplicate it afterwards for this model. So this was one counterpart. Yeah, but we figured out that using every concept of the property graph model there as a triple is, that was the best choice for the students. So another, yeah? Would you comment a bit more on the scalability, like what graph size should test this on? Yeah, so the question was some words about the scalability. The scalability is an open point of future work, so we don't have concrete results of that. We tested it with some city bike data that we interpreted as a stream, so some historical data. And we could process, I think it was about 600,000 edges in a few seconds. But this is just some first results, and we have not tried it on big and high-frequent graph streams on a cluster. Because we have huge flint clusters at our university, so we can benchmark the scalability of that in a later step. Yeah, thanks. Yeah? These aggregate functions, are they part of, you know, like a Java API, and how do you define them? Yeah, so the question was how we define the aggregate functions. So we have a set of predefined aggregate functions, like the count, average, min, max, and then you have an interface you can implement against, so there's an interface called aggregate function, and then you have to implement, I think, two or three functions, and then you can define your own and use it then here on, yeah, there. Where you give the classes of the account and average property, you can give your own class, and then it will be used. Yeah? Could you elaborate more on the real-life use cases or real-life applications? So the question is if we elaborate more real-life use cases and real-life questions. So applications. Applications. So since, yeah, so we are in, so I'm at the university, that means we are missing real-world data a lot, and we need also some input from companies to provide us with real-world data that we can use. So use cases could be, we also have to, we only have this bike sharing stuff or Twitter data and whatever, and I think if you have something like this aggregated function, like here an average property, you can use, because at the end it's a time series of changing values, for example, of the average property, and of like defining a threshold and get the notification afterwards. I think this is maybe one good application afterwards. So to, for example, if you have network traffic, you see, okay, now the average, I don't know, packet size is increasing. Now I get notified, for example, like this. But this could be an application, yeah. The idea was to use like a video stream for that, but then the question is, how much graph that is, could that maybe not be done just in a regular stream processing way? So I think this is just advisable to use that if you have some quite complex relationships between entities, then you can use this system besides just an ordinary stream processing engine or complex event processing engine. So I think the unique point of this is to have the graph aspects into the streaming world, yeah. So any further questions? Yeah. So could the events are only additive, right? So you can only add to the graph, but not delete from the graph by streaming it, right? Yes, so at the moment everything is interpreted as an insult only, but since Flink supports everything, like also updates and also deletions, it is thinkable about some future work that we also can support this. Because at the end, the result of us is also, since we are using windowing, it's also an insult only stream at the end, but if we maybe think about to remove the windowing aspect, so they have something like a continuous aggregation or whatever, then we need to support like a continuous addition on the end to update already existing aggregating results. Okay, any further questions? Thank you again. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.0, "text": " The next board is on graph aggregation, graph grouping, especially on streaming graphs.", "tokens": [440, 958, 3150, 307, 322, 4295, 16743, 399, 11, 4295, 40149, 11, 2318, 322, 11791, 24877, 13], "temperature": 0.0, "avg_logprob": -0.375172259202644, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.28413334488868713}, {"id": 1, "seek": 0, "start": 12.0, "end": 18.0, "text": " Graph grouping is a really interesting challenge because we've all seen individualizations,", "tokens": [21884, 40149, 307, 257, 534, 1880, 3430, 570, 321, 600, 439, 1612, 2609, 14455, 11], "temperature": 0.0, "avg_logprob": -0.375172259202644, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.28413334488868713}, {"id": 2, "seek": 0, "start": 18.0, "end": 20.0, "text": " hairballs, and complex graphs.", "tokens": [2578, 19194, 11, 293, 3997, 24877, 13], "temperature": 0.0, "avg_logprob": -0.375172259202644, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.28413334488868713}, {"id": 3, "seek": 0, "start": 20.0, "end": 25.0, "text": " What graph grouping allows you is ready to pick these graphs, group them by certain attributes,", "tokens": [708, 4295, 40149, 4045, 291, 307, 1919, 281, 1888, 613, 24877, 11, 1594, 552, 538, 1629, 17212, 11], "temperature": 0.0, "avg_logprob": -0.375172259202644, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.28413334488868713}, {"id": 4, "seek": 2500, "start": 25.0, "end": 30.0, "text": " and you have kind of these better nodes that then can be selectively expanded.", "tokens": [293, 291, 362, 733, 295, 613, 1101, 13891, 300, 550, 393, 312, 3048, 3413, 14342, 13], "temperature": 0.0, "avg_logprob": -0.19858617880909712, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00022252560302149504}, {"id": 5, "seek": 2500, "start": 30.0, "end": 35.0, "text": " But you can also then, on a group graph, which is mostly a monopod type graph, in many cases,", "tokens": [583, 291, 393, 611, 550, 11, 322, 257, 1594, 4295, 11, 597, 307, 5240, 257, 1108, 46684, 2010, 4295, 11, 294, 867, 3331, 11], "temperature": 0.0, "avg_logprob": -0.19858617880909712, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00022252560302149504}, {"id": 6, "seek": 2500, "start": 35.0, "end": 38.0, "text": " you can also run graph analytics, which is a really interesting problem.", "tokens": [291, 393, 611, 1190, 4295, 15370, 11, 597, 307, 257, 534, 1880, 1154, 13], "temperature": 0.0, "avg_logprob": -0.19858617880909712, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00022252560302149504}, {"id": 7, "seek": 2500, "start": 38.0, "end": 46.0, "text": " So I'm really excited to have Christopher here because both working on streaming graphs", "tokens": [407, 286, 478, 534, 2919, 281, 362, 20649, 510, 570, 1293, 1364, 322, 11791, 24877], "temperature": 0.0, "avg_logprob": -0.19858617880909712, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00022252560302149504}, {"id": 8, "seek": 2500, "start": 46.0, "end": 51.0, "text": " as well on temporary graphs with graph grouping is a really challenging and interesting aspect.", "tokens": [382, 731, 322, 13413, 24877, 365, 4295, 40149, 307, 257, 534, 7595, 293, 1880, 4171, 13], "temperature": 0.0, "avg_logprob": -0.19858617880909712, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00022252560302149504}, {"id": 9, "seek": 5100, "start": 51.0, "end": 58.0, "text": " So I'm really looking forward to the talk, and so without further ado, I come to the graph network.", "tokens": [407, 286, 478, 534, 1237, 2128, 281, 264, 751, 11, 293, 370, 1553, 3052, 22450, 11, 286, 808, 281, 264, 4295, 3209, 13], "temperature": 0.0, "avg_logprob": -0.20141980839871812, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.0003320927789900452}, {"id": 10, "seek": 5100, "start": 58.0, "end": 62.0, "text": " Yeah, thanks.", "tokens": [865, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.20141980839871812, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.0003320927789900452}, {"id": 11, "seek": 5100, "start": 62.0, "end": 68.0, "text": " Yeah, so thanks for that introduction and also thanks for accepting my abstract for this talk.", "tokens": [865, 11, 370, 3231, 337, 300, 9339, 293, 611, 3231, 337, 17391, 452, 12649, 337, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.20141980839871812, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.0003320927789900452}, {"id": 12, "seek": 5100, "start": 68.0, "end": 70.0, "text": " Yeah, my name is Christopher Rost.", "tokens": [865, 11, 452, 1315, 307, 20649, 497, 555, 13], "temperature": 0.0, "avg_logprob": -0.20141980839871812, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.0003320927789900452}, {"id": 13, "seek": 5100, "start": 70.0, "end": 75.0, "text": " I'm a PhD student of the University of Leipzig, and I'm currently writing my thesis,", "tokens": [286, 478, 257, 14476, 3107, 295, 264, 3535, 295, 1456, 647, 36168, 11, 293, 286, 478, 4362, 3579, 452, 22288, 11], "temperature": 0.0, "avg_logprob": -0.20141980839871812, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.0003320927789900452}, {"id": 14, "seek": 7500, "start": 75.0, "end": 81.0, "text": " so I'm glad that I bring some free time to doing this talk.", "tokens": [370, 286, 478, 5404, 300, 286, 1565, 512, 1737, 565, 281, 884, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.15646833769032653, "compression_ratio": 1.5317919075144508, "no_speech_prob": 7.341440505115315e-05}, {"id": 15, "seek": 7500, "start": 81.0, "end": 90.0, "text": " Yeah, so about us or our team, so that's me, and I have also a master's student working on this project.", "tokens": [865, 11, 370, 466, 505, 420, 527, 1469, 11, 370, 300, 311, 385, 11, 293, 286, 362, 611, 257, 4505, 311, 3107, 1364, 322, 341, 1716, 13], "temperature": 0.0, "avg_logprob": -0.15646833769032653, "compression_ratio": 1.5317919075144508, "no_speech_prob": 7.341440505115315e-05}, {"id": 16, "seek": 7500, "start": 90.0, "end": 99.0, "text": " We called Graph Stream Zoomer, and this project is a result of two master's thesis of our university", "tokens": [492, 1219, 21884, 24904, 13453, 260, 11, 293, 341, 1716, 307, 257, 1874, 295, 732, 4505, 311, 22288, 295, 527, 5454], "temperature": 0.0, "avg_logprob": -0.15646833769032653, "compression_ratio": 1.5317919075144508, "no_speech_prob": 7.341440505115315e-05}, {"id": 17, "seek": 9900, "start": 99.0, "end": 107.0, "text": " from Lia Salman and Rana Nurideen, and I think it's also nice to show the result of a master's thesis here", "tokens": [490, 47844, 5996, 1601, 293, 497, 2095, 17612, 482, 268, 11, 293, 286, 519, 309, 311, 611, 1481, 281, 855, 264, 1874, 295, 257, 4505, 311, 22288, 510], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 18, "seek": 9900, "start": 107.0, "end": 110.0, "text": " at the FOSTEM.", "tokens": [412, 264, 479, 4367, 51, 6683, 13], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 19, "seek": 9900, "start": 110.0, "end": 113.0, "text": " At the top is our professor of the database department.", "tokens": [1711, 264, 1192, 307, 527, 8304, 295, 264, 8149, 5882, 13], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 20, "seek": 9900, "start": 113.0, "end": 115.0, "text": " Yeah, just to say that.", "tokens": [865, 11, 445, 281, 584, 300, 13], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 21, "seek": 9900, "start": 115.0, "end": 119.0, "text": " Okay, what you should take away from this talk.", "tokens": [1033, 11, 437, 291, 820, 747, 1314, 490, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 22, "seek": 9900, "start": 119.0, "end": 128.0, "text": " So you will see what the property Graph Stream is and why it's important to have the streaming idea", "tokens": [407, 291, 486, 536, 437, 264, 4707, 21884, 24904, 307, 293, 983, 309, 311, 1021, 281, 362, 264, 11791, 1558], "temperature": 0.0, "avg_logprob": -0.21764743571378747, "compression_ratio": 1.5108225108225108, "no_speech_prob": 9.296201460529119e-05}, {"id": 23, "seek": 12800, "start": 128.0, "end": 135.0, "text": " inside the Graph topic, and second why you should or should not group a Graph Stream,", "tokens": [1854, 264, 21884, 4829, 11, 293, 1150, 983, 291, 820, 420, 820, 406, 1594, 257, 21884, 24904, 11], "temperature": 0.0, "avg_logprob": -0.18371668568363897, "compression_ratio": 1.5422535211267605, "no_speech_prob": 0.00015518453437834978}, {"id": 24, "seek": 12800, "start": 135.0, "end": 140.0, "text": " and then you will learn what the Graph Stream Zoomer, so this specific project is,", "tokens": [293, 550, 291, 486, 1466, 437, 264, 21884, 24904, 13453, 260, 11, 370, 341, 2685, 1716, 307, 11], "temperature": 0.0, "avg_logprob": -0.18371668568363897, "compression_ratio": 1.5422535211267605, "no_speech_prob": 0.00015518453437834978}, {"id": 25, "seek": 14000, "start": 140.0, "end": 169.0, "text": " and the main idea behind it, because we provided Java API to do that.", "tokens": [293, 264, 2135, 1558, 2261, 309, 11, 570, 321, 5649, 10745, 9362, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.19255753755569457, "compression_ratio": 0.9583333333333334, "no_speech_prob": 0.00015091858222149312}, {"id": 26, "seek": 16900, "start": 169.0, "end": 174.0, "text": " Okay, let's just start what is an event stream.", "tokens": [1033, 11, 718, 311, 445, 722, 437, 307, 364, 2280, 4309, 13], "temperature": 0.0, "avg_logprob": -0.13108185546038903, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0001733473181957379}, {"id": 27, "seek": 16900, "start": 174.0, "end": 177.0, "text": " I think I don't can skip that maybe.", "tokens": [286, 519, 286, 500, 380, 393, 10023, 300, 1310, 13], "temperature": 0.0, "avg_logprob": -0.13108185546038903, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0001733473181957379}, {"id": 28, "seek": 16900, "start": 177.0, "end": 183.0, "text": " So we say that anything that happens as a specific time and that can be recorded is an event,", "tokens": [407, 321, 584, 300, 1340, 300, 2314, 382, 257, 2685, 565, 293, 300, 393, 312, 8287, 307, 364, 2280, 11], "temperature": 0.0, "avg_logprob": -0.13108185546038903, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0001733473181957379}, {"id": 29, "seek": 16900, "start": 183.0, "end": 192.0, "text": " and if an event stream is now stream of these events, so a sequence that is ordered by time.", "tokens": [293, 498, 364, 2280, 4309, 307, 586, 4309, 295, 613, 3931, 11, 370, 257, 8310, 300, 307, 8866, 538, 565, 13], "temperature": 0.0, "avg_logprob": -0.13108185546038903, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0001733473181957379}, {"id": 30, "seek": 19200, "start": 192.0, "end": 200.0, "text": " So and I think everyone knows why we need event processing, so we cannot store everything into a database or whatever to analyze it.", "tokens": [407, 293, 286, 519, 1518, 3255, 983, 321, 643, 2280, 9007, 11, 370, 321, 2644, 3531, 1203, 666, 257, 8149, 420, 2035, 281, 12477, 309, 13], "temperature": 0.0, "avg_logprob": -0.10640535450944996, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.1372902817092836e-05}, {"id": 31, "seek": 19200, "start": 200.0, "end": 207.0, "text": " So I want to identify these meaningful events and respond to them as quickly as possible.", "tokens": [407, 286, 528, 281, 5876, 613, 10995, 3931, 293, 4196, 281, 552, 382, 2661, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.10640535450944996, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.1372902817092836e-05}, {"id": 32, "seek": 19200, "start": 207.0, "end": 210.0, "text": " Okay, what is now a Graph Stream?", "tokens": [1033, 11, 437, 307, 586, 257, 21884, 24904, 30], "temperature": 0.0, "avg_logprob": -0.10640535450944996, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.1372902817092836e-05}, {"id": 33, "seek": 19200, "start": 210.0, "end": 217.0, "text": " A Graph Stream is an event stream where each event is a Graph element or some Graph update.", "tokens": [316, 21884, 24904, 307, 364, 2280, 4309, 689, 1184, 2280, 307, 257, 21884, 4478, 420, 512, 21884, 5623, 13], "temperature": 0.0, "avg_logprob": -0.10640535450944996, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.1372902817092836e-05}, {"id": 34, "seek": 19200, "start": 217.0, "end": 221.0, "text": " Yeah, that could be edges, could be vertices, triples or whatever.", "tokens": [865, 11, 300, 727, 312, 8819, 11, 727, 312, 32053, 11, 1376, 2622, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.10640535450944996, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.1372902817092836e-05}, {"id": 35, "seek": 22100, "start": 221.0, "end": 224.0, "text": " And a Graph Update could be a modification of this.", "tokens": [400, 257, 21884, 28923, 727, 312, 257, 26747, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.11396417410477348, "compression_ratio": 1.6696428571428572, "no_speech_prob": 3.068639125558548e-05}, {"id": 36, "seek": 22100, "start": 224.0, "end": 231.0, "text": " For example, the addition of an edge, the removal of an edge, the addition of a property or an edge or whatever.", "tokens": [1171, 1365, 11, 264, 4500, 295, 364, 4691, 11, 264, 17933, 295, 364, 4691, 11, 264, 4500, 295, 257, 4707, 420, 364, 4691, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.11396417410477348, "compression_ratio": 1.6696428571428572, "no_speech_prob": 3.068639125558548e-05}, {"id": 37, "seek": 22100, "start": 231.0, "end": 234.0, "text": " So this is just an overview.", "tokens": [407, 341, 307, 445, 364, 12492, 13], "temperature": 0.0, "avg_logprob": -0.11396417410477348, "compression_ratio": 1.6696428571428572, "no_speech_prob": 3.068639125558548e-05}, {"id": 38, "seek": 22100, "start": 234.0, "end": 237.0, "text": " Okay, why should I use now a Graph Stream?", "tokens": [1033, 11, 983, 820, 286, 764, 586, 257, 21884, 24904, 30], "temperature": 0.0, "avg_logprob": -0.11396417410477348, "compression_ratio": 1.6696428571428572, "no_speech_prob": 3.068639125558548e-05}, {"id": 39, "seek": 22100, "start": 237.0, "end": 250.0, "text": " Because I can execute on this Graph Stream all algorithms and also all mathematical stuff from Graph Theory on this stream of Graph Data.", "tokens": [1436, 286, 393, 14483, 322, 341, 21884, 24904, 439, 14642, 293, 611, 439, 18894, 1507, 490, 21884, 29009, 322, 341, 4309, 295, 21884, 11888, 13], "temperature": 0.0, "avg_logprob": -0.11396417410477348, "compression_ratio": 1.6696428571428572, "no_speech_prob": 3.068639125558548e-05}, {"id": 40, "seek": 25000, "start": 250.0, "end": 257.0, "text": " For example, calculate page strength concurrently with the evolving Graph of the Graph Stream.", "tokens": [1171, 1365, 11, 8873, 3028, 3800, 37702, 356, 365, 264, 21085, 21884, 295, 264, 21884, 24904, 13], "temperature": 0.0, "avg_logprob": -0.1578203042348226, "compression_ratio": 1.5902439024390245, "no_speech_prob": 2.1105277483002283e-05}, {"id": 41, "seek": 25000, "start": 257.0, "end": 264.0, "text": " Okay, I can update my analysis results with a low latency if I combine that in a stream processing engine.", "tokens": [1033, 11, 286, 393, 5623, 452, 5215, 3542, 365, 257, 2295, 27043, 498, 286, 10432, 300, 294, 257, 4309, 9007, 2848, 13], "temperature": 0.0, "avg_logprob": -0.1578203042348226, "compression_ratio": 1.5902439024390245, "no_speech_prob": 2.1105277483002283e-05}, {"id": 42, "seek": 25000, "start": 264.0, "end": 274.0, "text": " And my goal is to monitor the changes or monitor the changes in the Graph or to create some notification or some reactivity.", "tokens": [400, 452, 3387, 307, 281, 6002, 264, 2962, 420, 6002, 264, 2962, 294, 264, 21884, 420, 281, 1884, 512, 11554, 420, 512, 4515, 4253, 13], "temperature": 0.0, "avg_logprob": -0.1578203042348226, "compression_ratio": 1.5902439024390245, "no_speech_prob": 2.1105277483002283e-05}, {"id": 43, "seek": 27400, "start": 274.0, "end": 281.0, "text": " For example, if something, some average goes over threshold, then I create a notification.", "tokens": [1171, 1365, 11, 498, 746, 11, 512, 4274, 1709, 670, 14678, 11, 550, 286, 1884, 257, 11554, 13], "temperature": 0.0, "avg_logprob": -0.12746365865071616, "compression_ratio": 1.4691943127962086, "no_speech_prob": 2.2110945792519487e-05}, {"id": 44, "seek": 27400, "start": 281.0, "end": 287.0, "text": " Okay, by now, the Graph Stream could be very heterogeneous.", "tokens": [1033, 11, 538, 586, 11, 264, 21884, 24904, 727, 312, 588, 20789, 31112, 13], "temperature": 0.0, "avg_logprob": -0.12746365865071616, "compression_ratio": 1.4691943127962086, "no_speech_prob": 2.2110945792519487e-05}, {"id": 45, "seek": 27400, "start": 287.0, "end": 293.0, "text": " That means it consists of many different types and it can also occur on a high frequency.", "tokens": [663, 1355, 309, 14689, 295, 867, 819, 3467, 293, 309, 393, 611, 5160, 322, 257, 1090, 7893, 13], "temperature": 0.0, "avg_logprob": -0.12746365865071616, "compression_ratio": 1.4691943127962086, "no_speech_prob": 2.2110945792519487e-05}, {"id": 46, "seek": 27400, "start": 293.0, "end": 299.0, "text": " So it is advisable to summarize the Graph elements in a specific way.", "tokens": [407, 309, 307, 10280, 712, 281, 20858, 264, 21884, 4959, 294, 257, 2685, 636, 13], "temperature": 0.0, "avg_logprob": -0.12746365865071616, "compression_ratio": 1.4691943127962086, "no_speech_prob": 2.2110945792519487e-05}, {"id": 47, "seek": 29900, "start": 299.0, "end": 304.0, "text": " And we can summarize Graph elements by three criteria.", "tokens": [400, 321, 393, 20858, 21884, 4959, 538, 1045, 11101, 13], "temperature": 0.0, "avg_logprob": -0.0983041991358218, "compression_ratio": 1.8598130841121496, "no_speech_prob": 1.1837793863378465e-05}, {"id": 48, "seek": 29900, "start": 304.0, "end": 308.0, "text": " For example, by time, that means Graph elements that belong together.", "tokens": [1171, 1365, 11, 538, 565, 11, 300, 1355, 21884, 4959, 300, 5784, 1214, 13], "temperature": 0.0, "avg_logprob": -0.0983041991358218, "compression_ratio": 1.8598130841121496, "no_speech_prob": 1.1837793863378465e-05}, {"id": 49, "seek": 29900, "start": 308.0, "end": 312.0, "text": " For example, the time window, we group them together by structure.", "tokens": [1171, 1365, 11, 264, 565, 4910, 11, 321, 1594, 552, 1214, 538, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0983041991358218, "compression_ratio": 1.8598130841121496, "no_speech_prob": 1.1837793863378465e-05}, {"id": 50, "seek": 29900, "start": 312.0, "end": 317.0, "text": " That means, for example, edges that share the same source or target vertex can be grouped together.", "tokens": [663, 1355, 11, 337, 1365, 11, 8819, 300, 2073, 264, 912, 4009, 420, 3779, 28162, 393, 312, 41877, 1214, 13], "temperature": 0.0, "avg_logprob": -0.0983041991358218, "compression_ratio": 1.8598130841121496, "no_speech_prob": 1.1837793863378465e-05}, {"id": 51, "seek": 29900, "start": 317.0, "end": 324.0, "text": " And by content, that means vertices and edges that share the same label or a specific value of a property.", "tokens": [400, 538, 2701, 11, 300, 1355, 32053, 293, 8819, 300, 2073, 264, 912, 7645, 420, 257, 2685, 2158, 295, 257, 4707, 13], "temperature": 0.0, "avg_logprob": -0.0983041991358218, "compression_ratio": 1.8598130841121496, "no_speech_prob": 1.1837793863378465e-05}, {"id": 52, "seek": 32400, "start": 324.0, "end": 329.0, "text": " And we introduced for our algorithm so-called grouping key functions.", "tokens": [400, 321, 7268, 337, 527, 9284, 370, 12, 11880, 40149, 2141, 6828, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 53, "seek": 32400, "start": 329.0, "end": 334.0, "text": " That means it is a function that maps a vertex or an edge to a grouping key.", "tokens": [663, 1355, 309, 307, 257, 2445, 300, 11317, 257, 28162, 420, 364, 4691, 281, 257, 40149, 2141, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 54, "seek": 32400, "start": 334.0, "end": 338.0, "text": " And that could be everything that is inside this vertex or edge.", "tokens": [400, 300, 727, 312, 1203, 300, 307, 1854, 341, 28162, 420, 4691, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 55, "seek": 32400, "start": 338.0, "end": 342.0, "text": " It could be labeled, temporal information, some kind of property or whatever.", "tokens": [467, 727, 312, 21335, 11, 30881, 1589, 11, 512, 733, 295, 4707, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 56, "seek": 32400, "start": 342.0, "end": 348.0, "text": " So you can map everything that is represented by a vertex or edge to a key function or to a key.", "tokens": [407, 291, 393, 4471, 1203, 300, 307, 10379, 538, 257, 28162, 420, 4691, 281, 257, 2141, 2445, 420, 281, 257, 2141, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 57, "seek": 32400, "start": 348.0, "end": 351.0, "text": " And on this key, we group the Graph.", "tokens": [400, 322, 341, 2141, 11, 321, 1594, 264, 21884, 13], "temperature": 0.0, "avg_logprob": -0.11416319892519997, "compression_ratio": 1.8076923076923077, "no_speech_prob": 2.143209712812677e-05}, {"id": 58, "seek": 35100, "start": 351.0, "end": 361.0, "text": " So that means, at the end, the result is again a Graph stream, but the grouped representation of that.", "tokens": [407, 300, 1355, 11, 412, 264, 917, 11, 264, 1874, 307, 797, 257, 21884, 4309, 11, 457, 264, 41877, 10290, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12743342291448534, "compression_ratio": 1.5826446280991735, "no_speech_prob": 3.314283458166756e-05}, {"id": 59, "seek": 35100, "start": 361.0, "end": 364.0, "text": " Okay, now you can question, okay, why I need this?", "tokens": [1033, 11, 586, 291, 393, 1168, 11, 1392, 11, 983, 286, 643, 341, 30], "temperature": 0.0, "avg_logprob": -0.12743342291448534, "compression_ratio": 1.5826446280991735, "no_speech_prob": 3.314283458166756e-05}, {"id": 60, "seek": 35100, "start": 364.0, "end": 366.0, "text": " So what are the applications of that?", "tokens": [407, 437, 366, 264, 5821, 295, 300, 30], "temperature": 0.0, "avg_logprob": -0.12743342291448534, "compression_ratio": 1.5826446280991735, "no_speech_prob": 3.314283458166756e-05}, {"id": 61, "seek": 35100, "start": 366.0, "end": 370.0, "text": " You can think about it as a pre-processing step.", "tokens": [509, 393, 519, 466, 309, 382, 257, 659, 12, 41075, 278, 1823, 13], "temperature": 0.0, "avg_logprob": -0.12743342291448534, "compression_ratio": 1.5826446280991735, "no_speech_prob": 3.314283458166756e-05}, {"id": 62, "seek": 35100, "start": 370.0, "end": 379.0, "text": " For example, before you calculate the page frames, you just group the vertices to the city attribute of users together or something like this.", "tokens": [1171, 1365, 11, 949, 291, 8873, 264, 3028, 12083, 11, 291, 445, 1594, 264, 32053, 281, 264, 2307, 19667, 295, 5022, 1214, 420, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12743342291448534, "compression_ratio": 1.5826446280991735, "no_speech_prob": 3.314283458166756e-05}, {"id": 63, "seek": 37900, "start": 379.0, "end": 382.0, "text": " You also use it as a pre-processing step.", "tokens": [509, 611, 764, 309, 382, 257, 659, 12, 41075, 278, 1823, 13], "temperature": 0.0, "avg_logprob": -0.10264453668703978, "compression_ratio": 1.7004608294930876, "no_speech_prob": 3.58846336894203e-05}, {"id": 64, "seek": 37900, "start": 382.0, "end": 386.0, "text": " Second application could be as a post-processing step.", "tokens": [5736, 3861, 727, 312, 382, 257, 2183, 12, 41075, 278, 1823, 13], "temperature": 0.0, "avg_logprob": -0.10264453668703978, "compression_ratio": 1.7004608294930876, "no_speech_prob": 3.58846336894203e-05}, {"id": 65, "seek": 37900, "start": 386.0, "end": 392.0, "text": " For example, after you apply the Graph stream analysis, for example, a community detection,", "tokens": [1171, 1365, 11, 934, 291, 3079, 264, 21884, 4309, 5215, 11, 337, 1365, 11, 257, 1768, 17784, 11], "temperature": 0.0, "avg_logprob": -0.10264453668703978, "compression_ratio": 1.7004608294930876, "no_speech_prob": 3.58846336894203e-05}, {"id": 66, "seek": 37900, "start": 392.0, "end": 400.0, "text": " you can now group on this cluster ID with our grouping algorithm to summarize the different communities together.", "tokens": [291, 393, 586, 1594, 322, 341, 13630, 7348, 365, 527, 40149, 9284, 281, 20858, 264, 819, 4456, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10264453668703978, "compression_ratio": 1.7004608294930876, "no_speech_prob": 3.58846336894203e-05}, {"id": 67, "seek": 37900, "start": 400.0, "end": 405.0, "text": " You can also use it to understand the Graph stream in more detail.", "tokens": [509, 393, 611, 764, 309, 281, 1223, 264, 21884, 4309, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.10264453668703978, "compression_ratio": 1.7004608294930876, "no_speech_prob": 3.58846336894203e-05}, {"id": 68, "seek": 40500, "start": 405.0, "end": 412.0, "text": " For example, okay, just to know which vertex or edge types exist in my Graph stream,", "tokens": [1171, 1365, 11, 1392, 11, 445, 281, 458, 597, 28162, 420, 4691, 3467, 2514, 294, 452, 21884, 4309, 11], "temperature": 0.0, "avg_logprob": -0.10492689181596805, "compression_ratio": 1.6572769953051643, "no_speech_prob": 4.06496983487159e-05}, {"id": 69, "seek": 40500, "start": 412.0, "end": 421.0, "text": " how frequent these different types arrive, or how vertices and edges with different characteristics are connected together.", "tokens": [577, 18004, 613, 819, 3467, 8881, 11, 420, 577, 32053, 293, 8819, 365, 819, 10891, 366, 4582, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10492689181596805, "compression_ratio": 1.6572769953051643, "no_speech_prob": 4.06496983487159e-05}, {"id": 70, "seek": 40500, "start": 421.0, "end": 428.0, "text": " So just to get deeper insights, or if you use our aggregation functions or aggregation,", "tokens": [407, 445, 281, 483, 7731, 14310, 11, 420, 498, 291, 764, 527, 16743, 399, 6828, 420, 16743, 399, 11], "temperature": 0.0, "avg_logprob": -0.10492689181596805, "compression_ratio": 1.6572769953051643, "no_speech_prob": 4.06496983487159e-05}, {"id": 71, "seek": 40500, "start": 428.0, "end": 431.0, "text": " for example, counting or calculating an average on that,", "tokens": [337, 1365, 11, 13251, 420, 28258, 364, 4274, 322, 300, 11], "temperature": 0.0, "avg_logprob": -0.10492689181596805, "compression_ratio": 1.6572769953051643, "no_speech_prob": 4.06496983487159e-05}, {"id": 72, "seek": 43100, "start": 431.0, "end": 438.0, "text": " you can also get or reveal some hidden information that you would not see in the Graph stream itself.", "tokens": [291, 393, 611, 483, 420, 10658, 512, 7633, 1589, 300, 291, 576, 406, 536, 294, 264, 21884, 4309, 2564, 13], "temperature": 0.0, "avg_logprob": -0.12317563641455866, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.6172582036233507e-05}, {"id": 73, "seek": 43100, "start": 438.0, "end": 441.0, "text": " Okay, so this is an introduction.", "tokens": [1033, 11, 370, 341, 307, 364, 9339, 13], "temperature": 0.0, "avg_logprob": -0.12317563641455866, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.6172582036233507e-05}, {"id": 74, "seek": 43100, "start": 441.0, "end": 449.0, "text": " Now I explain the ideas behind this Graph stream zoomer application just by an example,", "tokens": [823, 286, 2903, 264, 3487, 2261, 341, 21884, 4309, 8863, 260, 3861, 445, 538, 364, 1365, 11], "temperature": 0.0, "avg_logprob": -0.12317563641455866, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.6172582036233507e-05}, {"id": 75, "seek": 43100, "start": 449.0, "end": 452.0, "text": " and then afterwards I summarize this.", "tokens": [293, 550, 10543, 286, 20858, 341, 13], "temperature": 0.0, "avg_logprob": -0.12317563641455866, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.6172582036233507e-05}, {"id": 76, "seek": 45200, "start": 452.0, "end": 462.0, "text": " For example, we're using bike rental data that can have two different Graph schemas.", "tokens": [1171, 1365, 11, 321, 434, 1228, 5656, 21468, 1412, 300, 393, 362, 732, 819, 21884, 22627, 296, 13], "temperature": 0.0, "avg_logprob": -0.09745804830030962, "compression_ratio": 1.6108374384236452, "no_speech_prob": 2.8819638828281313e-05}, {"id": 77, "seek": 45200, "start": 462.0, "end": 465.0, "text": " I named them A and B.", "tokens": [286, 4926, 552, 316, 293, 363, 13], "temperature": 0.0, "avg_logprob": -0.09745804830030962, "compression_ratio": 1.6108374384236452, "no_speech_prob": 2.8819638828281313e-05}, {"id": 78, "seek": 45200, "start": 465.0, "end": 472.0, "text": " So the first Graph schema A is that a bike rental is an edge between two station nodes.", "tokens": [407, 264, 700, 21884, 34078, 316, 307, 300, 257, 5656, 21468, 307, 364, 4691, 1296, 732, 5214, 13891, 13], "temperature": 0.0, "avg_logprob": -0.09745804830030962, "compression_ratio": 1.6108374384236452, "no_speech_prob": 2.8819638828281313e-05}, {"id": 79, "seek": 45200, "start": 472.0, "end": 474.0, "text": " You see it on the left side.", "tokens": [509, 536, 309, 322, 264, 1411, 1252, 13], "temperature": 0.0, "avg_logprob": -0.09745804830030962, "compression_ratio": 1.6108374384236452, "no_speech_prob": 2.8819638828281313e-05}, {"id": 80, "seek": 45200, "start": 474.0, "end": 481.0, "text": " So a station has several properties like the name, the number of bikes, latitude, longitude, and so on.", "tokens": [407, 257, 5214, 575, 2940, 7221, 411, 264, 1315, 11, 264, 1230, 295, 16035, 11, 45436, 11, 938, 4377, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.09745804830030962, "compression_ratio": 1.6108374384236452, "no_speech_prob": 2.8819638828281313e-05}, {"id": 81, "seek": 48100, "start": 481.0, "end": 489.0, "text": " And the trip edge has properties like the user ID, so who rented the bike, which bike was used by the bike ID,", "tokens": [400, 264, 4931, 4691, 575, 7221, 411, 264, 4195, 7348, 11, 370, 567, 32381, 264, 5656, 11, 597, 5656, 390, 1143, 538, 264, 5656, 7348, 11], "temperature": 0.0, "avg_logprob": -0.08367803975155479, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1296066077193245e-05}, {"id": 82, "seek": 48100, "start": 489.0, "end": 496.0, "text": " and from and to, so when this trip happens, until when, and the duration, for example, in seconds.", "tokens": [293, 490, 293, 281, 11, 370, 562, 341, 4931, 2314, 11, 1826, 562, 11, 293, 264, 16365, 11, 337, 1365, 11, 294, 3949, 13], "temperature": 0.0, "avg_logprob": -0.08367803975155479, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1296066077193245e-05}, {"id": 83, "seek": 48100, "start": 496.0, "end": 498.0, "text": " So this is schema A on the left side.", "tokens": [407, 341, 307, 34078, 316, 322, 264, 1411, 1252, 13], "temperature": 0.0, "avg_logprob": -0.08367803975155479, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1296066077193245e-05}, {"id": 84, "seek": 48100, "start": 498.0, "end": 501.0, "text": " On the right side we have a more heterogeneous schema.", "tokens": [1282, 264, 558, 1252, 321, 362, 257, 544, 20789, 31112, 34078, 13], "temperature": 0.0, "avg_logprob": -0.08367803975155479, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1296066077193245e-05}, {"id": 85, "seek": 48100, "start": 501.0, "end": 505.0, "text": " So we have also stations and trips as vertices here.", "tokens": [407, 321, 362, 611, 13390, 293, 16051, 382, 32053, 510, 13], "temperature": 0.0, "avg_logprob": -0.08367803975155479, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1296066077193245e-05}, {"id": 86, "seek": 50500, "start": 505.0, "end": 511.0, "text": " And we have also bike nodes and user nodes with several properties.", "tokens": [400, 321, 362, 611, 5656, 13891, 293, 4195, 13891, 365, 2940, 7221, 13], "temperature": 0.0, "avg_logprob": -0.11277495412265554, "compression_ratio": 1.4974093264248705, "no_speech_prob": 1.7491343896836042e-05}, {"id": 87, "seek": 50500, "start": 511.0, "end": 526.0, "text": " So I just divided into this because I can explain the examples that follow a bit better compared to just using a simple schema like here on the left side.", "tokens": [407, 286, 445, 6666, 666, 341, 570, 286, 393, 2903, 264, 5110, 300, 1524, 257, 857, 1101, 5347, 281, 445, 1228, 257, 2199, 34078, 411, 510, 322, 264, 1411, 1252, 13], "temperature": 0.0, "avg_logprob": -0.11277495412265554, "compression_ratio": 1.4974093264248705, "no_speech_prob": 1.7491343896836042e-05}, {"id": 88, "seek": 50500, "start": 526.0, "end": 532.0, "text": " Okay, so how a Graph stream of these schemas could look like this.", "tokens": [1033, 11, 370, 577, 257, 21884, 4309, 295, 613, 22627, 296, 727, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11277495412265554, "compression_ratio": 1.4974093264248705, "no_speech_prob": 1.7491343896836042e-05}, {"id": 89, "seek": 53200, "start": 532.0, "end": 538.0, "text": " Yeah, of schema A I have just these trip edges between two stations and all information inside.", "tokens": [865, 11, 295, 34078, 316, 286, 362, 445, 613, 4931, 8819, 1296, 732, 13390, 293, 439, 1589, 1854, 13], "temperature": 0.0, "avg_logprob": -0.09832215309143066, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.2024568604829255e-05}, {"id": 90, "seek": 53200, "start": 538.0, "end": 545.0, "text": " And from schema B I have here the trip nodes connected with all the other vertex types.", "tokens": [400, 490, 34078, 363, 286, 362, 510, 264, 4931, 13891, 4582, 365, 439, 264, 661, 28162, 3467, 13], "temperature": 0.0, "avg_logprob": -0.09832215309143066, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.2024568604829255e-05}, {"id": 91, "seek": 53200, "start": 545.0, "end": 551.0, "text": " So this is just an exemplary view how a stream of this graph data could look like.", "tokens": [407, 341, 307, 445, 364, 24112, 822, 1910, 577, 257, 4309, 295, 341, 4295, 1412, 727, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.09832215309143066, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.2024568604829255e-05}, {"id": 92, "seek": 53200, "start": 551.0, "end": 560.0, "text": " Okay, so now begin with a very simple or a simple example of our grouping algorithm.", "tokens": [1033, 11, 370, 586, 1841, 365, 257, 588, 2199, 420, 257, 2199, 1365, 295, 527, 40149, 9284, 13], "temperature": 0.0, "avg_logprob": -0.09832215309143066, "compression_ratio": 1.5530973451327434, "no_speech_prob": 1.2024568604829255e-05}, {"id": 93, "seek": 56000, "start": 560.0, "end": 569.0, "text": " So the input of the grouping is the graph stream, I think it's clear, and we need a grouping configuration.", "tokens": [407, 264, 4846, 295, 264, 40149, 307, 264, 4295, 4309, 11, 286, 519, 309, 311, 1850, 11, 293, 321, 643, 257, 40149, 11694, 13], "temperature": 0.0, "avg_logprob": -0.09749693065494686, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.3840352949046064e-05}, {"id": 94, "seek": 56000, "start": 569.0, "end": 574.0, "text": " And the grouping configuration consists of five attributes.", "tokens": [400, 264, 40149, 11694, 14689, 295, 1732, 17212, 13], "temperature": 0.0, "avg_logprob": -0.09749693065494686, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.3840352949046064e-05}, {"id": 95, "seek": 56000, "start": 574.0, "end": 579.0, "text": " The first is the window because we are doing windowing on our graph stream.", "tokens": [440, 700, 307, 264, 4910, 570, 321, 366, 884, 4910, 278, 322, 527, 4295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.09749693065494686, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.3840352949046064e-05}, {"id": 96, "seek": 56000, "start": 579.0, "end": 585.0, "text": " So I can define a window size, which is here, for example, 10 minutes.", "tokens": [407, 286, 393, 6964, 257, 4910, 2744, 11, 597, 307, 510, 11, 337, 1365, 11, 1266, 2077, 13], "temperature": 0.0, "avg_logprob": -0.09749693065494686, "compression_ratio": 1.643979057591623, "no_speech_prob": 1.3840352949046064e-05}, {"id": 97, "seek": 58500, "start": 585.0, "end": 594.0, "text": " And then VG key are the vertex grouping keys, that means the key functions that leads to the grouping of vertices.", "tokens": [400, 550, 691, 38, 2141, 366, 264, 28162, 40149, 9317, 11, 300, 1355, 264, 2141, 6828, 300, 6689, 281, 264, 40149, 295, 32053, 13], "temperature": 0.0, "avg_logprob": -0.1117926583145604, "compression_ratio": 1.9189189189189189, "no_speech_prob": 1.450324180041207e-05}, {"id": 98, "seek": 58500, "start": 594.0, "end": 602.0, "text": " EG key are the edge grouping keys, that means the key functions that are needed to group the edges together.", "tokens": [462, 38, 2141, 366, 264, 4691, 40149, 9317, 11, 300, 1355, 264, 2141, 6828, 300, 366, 2978, 281, 1594, 264, 8819, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1117926583145604, "compression_ratio": 1.9189189189189189, "no_speech_prob": 1.450324180041207e-05}, {"id": 99, "seek": 58500, "start": 602.0, "end": 606.0, "text": " And we also have a collection of vertex aggregate functions.", "tokens": [400, 321, 611, 362, 257, 5765, 295, 28162, 26118, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1117926583145604, "compression_ratio": 1.9189189189189189, "no_speech_prob": 1.450324180041207e-05}, {"id": 100, "seek": 60600, "start": 606.0, "end": 615.0, "text": " This is VA-GG and EA-GG are the group of edge aggregate functions.", "tokens": [639, 307, 18527, 12, 27561, 293, 35747, 12, 27561, 366, 264, 1594, 295, 4691, 26118, 6828, 13], "temperature": 0.0, "avg_logprob": -0.15257951079821977, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.0448773537063971e-05}, {"id": 101, "seek": 60600, "start": 615.0, "end": 621.0, "text": " So the four on the bottom are just an array of several key functions and aggregate functions.", "tokens": [407, 264, 1451, 322, 264, 2767, 366, 445, 364, 10225, 295, 2940, 2141, 6828, 293, 26118, 6828, 13], "temperature": 0.0, "avg_logprob": -0.15257951079821977, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.0448773537063971e-05}, {"id": 102, "seek": 60600, "start": 621.0, "end": 629.0, "text": " Okay, and now having the input stream and applying that grouping, we get a result.", "tokens": [1033, 11, 293, 586, 1419, 264, 4846, 4309, 293, 9275, 300, 40149, 11, 321, 483, 257, 1874, 13], "temperature": 0.0, "avg_logprob": -0.15257951079821977, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.0448773537063971e-05}, {"id": 103, "seek": 62900, "start": 629.0, "end": 638.0, "text": " And just looking like this, because we define for the vertex grouping keys a function that maps every vertex to an integer value.", "tokens": [400, 445, 1237, 411, 341, 11, 570, 321, 6964, 337, 264, 28162, 40149, 9317, 257, 2445, 300, 11317, 633, 28162, 281, 364, 24922, 2158, 13], "temperature": 0.0, "avg_logprob": -0.08124457192175168, "compression_ratio": 1.8914027149321266, "no_speech_prob": 1.8612101484904997e-05}, {"id": 104, "seek": 62900, "start": 638.0, "end": 646.0, "text": " And that results in, it doesn't matter which vertex exists in our graph stream, we group everything together to one vertex.", "tokens": [400, 300, 3542, 294, 11, 309, 1177, 380, 1871, 597, 28162, 8198, 294, 527, 4295, 4309, 11, 321, 1594, 1203, 1214, 281, 472, 28162, 13], "temperature": 0.0, "avg_logprob": -0.08124457192175168, "compression_ratio": 1.8914027149321266, "no_speech_prob": 1.8612101484904997e-05}, {"id": 105, "seek": 62900, "start": 646.0, "end": 650.0, "text": " And that's the white one with an empty label.", "tokens": [400, 300, 311, 264, 2418, 472, 365, 364, 6707, 7645, 13], "temperature": 0.0, "avg_logprob": -0.08124457192175168, "compression_ratio": 1.8914027149321266, "no_speech_prob": 1.8612101484904997e-05}, {"id": 106, "seek": 62900, "start": 650.0, "end": 657.0, "text": " And the same for edges, that means every edge that exists in our graph stream, we group them together to a super edge,", "tokens": [400, 264, 912, 337, 8819, 11, 300, 1355, 633, 4691, 300, 8198, 294, 527, 4295, 4309, 11, 321, 1594, 552, 1214, 281, 257, 1687, 4691, 11], "temperature": 0.0, "avg_logprob": -0.08124457192175168, "compression_ratio": 1.8914027149321266, "no_speech_prob": 1.8612101484904997e-05}, {"id": 107, "seek": 65700, "start": 657.0, "end": 663.0, "text": " we call them super vertex and super edge that is displayed here in gray.", "tokens": [321, 818, 552, 1687, 28162, 293, 1687, 4691, 300, 307, 16372, 510, 294, 10855, 13], "temperature": 0.0, "avg_logprob": -0.09319583892822265, "compression_ratio": 1.811965811965812, "no_speech_prob": 1.1838485988846514e-05}, {"id": 108, "seek": 65700, "start": 663.0, "end": 671.0, "text": " And because of the count aggregate functions, we add a new property to the super vertex with the count of all vertices that are grouped together.", "tokens": [400, 570, 295, 264, 1207, 26118, 6828, 11, 321, 909, 257, 777, 4707, 281, 264, 1687, 28162, 365, 264, 1207, 295, 439, 32053, 300, 366, 41877, 1214, 13], "temperature": 0.0, "avg_logprob": -0.09319583892822265, "compression_ratio": 1.811965811965812, "no_speech_prob": 1.1838485988846514e-05}, {"id": 109, "seek": 65700, "start": 671.0, "end": 676.0, "text": " And a new property to the super edge with also the count value.", "tokens": [400, 257, 777, 4707, 281, 264, 1687, 4691, 365, 611, 264, 1207, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09319583892822265, "compression_ratio": 1.811965811965812, "no_speech_prob": 1.1838485988846514e-05}, {"id": 110, "seek": 65700, "start": 676.0, "end": 681.0, "text": " And this is now our result for every window that we defined on the graph stream.", "tokens": [400, 341, 307, 586, 527, 1874, 337, 633, 4910, 300, 321, 7642, 322, 264, 4295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.09319583892822265, "compression_ratio": 1.811965811965812, "no_speech_prob": 1.1838485988846514e-05}, {"id": 111, "seek": 65700, "start": 681.0, "end": 684.0, "text": " For example, here the first window, second window and so on.", "tokens": [1171, 1365, 11, 510, 264, 700, 4910, 11, 1150, 4910, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.09319583892822265, "compression_ratio": 1.811965811965812, "no_speech_prob": 1.1838485988846514e-05}, {"id": 112, "seek": 68400, "start": 684.0, "end": 690.0, "text": " That means we are creating a stream of grouped graphs here.", "tokens": [663, 1355, 321, 366, 4084, 257, 4309, 295, 41877, 24877, 510, 13], "temperature": 0.0, "avg_logprob": -0.13613924195494834, "compression_ratio": 1.6183574879227054, "no_speech_prob": 2.1745372578152455e-05}, {"id": 113, "seek": 68400, "start": 690.0, "end": 694.0, "text": " Okay, that is for schema A and that's the most zoomed out view.", "tokens": [1033, 11, 300, 307, 337, 34078, 316, 293, 300, 311, 264, 881, 8863, 292, 484, 1910, 13], "temperature": 0.0, "avg_logprob": -0.13613924195494834, "compression_ratio": 1.6183574879227054, "no_speech_prob": 2.1745372578152455e-05}, {"id": 114, "seek": 68400, "start": 694.0, "end": 698.0, "text": " So I group everything together that exists in the graph stream.", "tokens": [407, 286, 1594, 1203, 1214, 300, 8198, 294, 264, 4295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.13613924195494834, "compression_ratio": 1.6183574879227054, "no_speech_prob": 2.1745372578152455e-05}, {"id": 115, "seek": 68400, "start": 698.0, "end": 708.0, "text": " For schema B and same grouping configuration, it looks the same because it doesn't matter which type label exists, so we group everything together.", "tokens": [1171, 34078, 363, 293, 912, 40149, 11694, 11, 309, 1542, 264, 912, 570, 309, 1177, 380, 1871, 597, 2010, 7645, 8198, 11, 370, 321, 1594, 1203, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13613924195494834, "compression_ratio": 1.6183574879227054, "no_speech_prob": 2.1745372578152455e-05}, {"id": 116, "seek": 70800, "start": 708.0, "end": 715.0, "text": " So we have just different counts because we have a bit more vertices and edges and also for the second window it looks the same.", "tokens": [407, 321, 362, 445, 819, 14893, 570, 321, 362, 257, 857, 544, 32053, 293, 8819, 293, 611, 337, 264, 1150, 4910, 309, 1542, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.07919242758499949, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.1653943147393875e-05}, {"id": 117, "seek": 70800, "start": 715.0, "end": 720.0, "text": " Okay, so this is my first example, so the most zoomed out way.", "tokens": [1033, 11, 370, 341, 307, 452, 700, 1365, 11, 370, 264, 881, 8863, 292, 484, 636, 13], "temperature": 0.0, "avg_logprob": -0.07919242758499949, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.1653943147393875e-05}, {"id": 118, "seek": 70800, "start": 720.0, "end": 726.0, "text": " The second example are called a graph stream schema.", "tokens": [440, 1150, 1365, 366, 1219, 257, 4295, 4309, 34078, 13], "temperature": 0.0, "avg_logprob": -0.07919242758499949, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.1653943147393875e-05}, {"id": 119, "seek": 70800, "start": 726.0, "end": 736.0, "text": " So that means now we are using as a vertex grouping key a function that maps a vertex to its label and a function that maps the edge to its label.", "tokens": [407, 300, 1355, 586, 321, 366, 1228, 382, 257, 28162, 40149, 2141, 257, 2445, 300, 11317, 257, 28162, 281, 1080, 7645, 293, 257, 2445, 300, 11317, 264, 4691, 281, 1080, 7645, 13], "temperature": 0.0, "avg_logprob": -0.07919242758499949, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.1653943147393875e-05}, {"id": 120, "seek": 73600, "start": 736.0, "end": 739.0, "text": " So what's the result here?", "tokens": [407, 437, 311, 264, 1874, 510, 30], "temperature": 0.0, "avg_logprob": -0.10361960764681355, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.3838520317221992e-05}, {"id": 121, "seek": 73600, "start": 739.0, "end": 749.0, "text": " Now our node has a label station and the count because the count aggregate function stays the same and our edge has a label trip.", "tokens": [823, 527, 9984, 575, 257, 7645, 5214, 293, 264, 1207, 570, 264, 1207, 26118, 2445, 10834, 264, 912, 293, 527, 4691, 575, 257, 7645, 4931, 13], "temperature": 0.0, "avg_logprob": -0.10361960764681355, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.3838520317221992e-05}, {"id": 122, "seek": 73600, "start": 749.0, "end": 757.0, "text": " So it's more or less the same because our graph streamer has just one node type and one edge type and that's it also for the second window.", "tokens": [407, 309, 311, 544, 420, 1570, 264, 912, 570, 527, 4295, 4309, 260, 575, 445, 472, 9984, 2010, 293, 472, 4691, 2010, 293, 300, 311, 309, 611, 337, 264, 1150, 4910, 13], "temperature": 0.0, "avg_logprob": -0.10361960764681355, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.3838520317221992e-05}, {"id": 123, "seek": 73600, "start": 757.0, "end": 762.0, "text": " Now it gets a bit more interesting when we are using schema B.", "tokens": [823, 309, 2170, 257, 857, 544, 1880, 562, 321, 366, 1228, 34078, 363, 13], "temperature": 0.0, "avg_logprob": -0.10361960764681355, "compression_ratio": 1.7095238095238094, "no_speech_prob": 1.3838520317221992e-05}, {"id": 124, "seek": 76200, "start": 762.0, "end": 769.0, "text": " The result here with the same grouping configuration as before is now this.", "tokens": [440, 1874, 510, 365, 264, 912, 40149, 11694, 382, 949, 307, 586, 341, 13], "temperature": 0.0, "avg_logprob": -0.09913285573323567, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.695358514552936e-05}, {"id": 125, "seek": 76200, "start": 769.0, "end": 779.0, "text": " That means every vertex is grouped by their label and every edge is grouped by their label and now I have like a schema representation of my graph stream.", "tokens": [663, 1355, 633, 28162, 307, 41877, 538, 641, 7645, 293, 633, 4691, 307, 41877, 538, 641, 7645, 293, 586, 286, 362, 411, 257, 34078, 10290, 295, 452, 4295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.09913285573323567, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.695358514552936e-05}, {"id": 126, "seek": 76200, "start": 779.0, "end": 784.0, "text": " And again with all counts because we are just using count aggregation.", "tokens": [400, 797, 365, 439, 14893, 570, 321, 366, 445, 1228, 1207, 16743, 399, 13], "temperature": 0.0, "avg_logprob": -0.09913285573323567, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.695358514552936e-05}, {"id": 127, "seek": 78400, "start": 784.0, "end": 793.0, "text": " Okay, and that's for the second window and so on and so on, I just leave here the properties.", "tokens": [1033, 11, 293, 300, 311, 337, 264, 1150, 4910, 293, 370, 322, 293, 370, 322, 11, 286, 445, 1856, 510, 264, 7221, 13], "temperature": 0.0, "avg_logprob": -0.1645844273450898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.131620153202675e-05}, {"id": 128, "seek": 78400, "start": 793.0, "end": 803.0, "text": " Okay, the next example we stay with the vertex grouping keys and edge grouping keys, but now I added several aggregate functions to vertices and edges.", "tokens": [1033, 11, 264, 958, 1365, 321, 1754, 365, 264, 28162, 40149, 9317, 293, 4691, 40149, 9317, 11, 457, 586, 286, 3869, 2940, 26118, 6828, 281, 32053, 293, 8819, 13], "temperature": 0.0, "avg_logprob": -0.1645844273450898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.131620153202675e-05}, {"id": 129, "seek": 78400, "start": 803.0, "end": 810.0, "text": " For example I say okay for the vertices I want to calculate the average of all available bikes for these stations.", "tokens": [1171, 1365, 286, 584, 1392, 337, 264, 32053, 286, 528, 281, 8873, 264, 4274, 295, 439, 2435, 16035, 337, 613, 13390, 13], "temperature": 0.0, "avg_logprob": -0.1645844273450898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.131620153202675e-05}, {"id": 130, "seek": 81000, "start": 810.0, "end": 820.0, "text": " And for the edge aggregate functions I want to have the minimum, maximum and average duration that a trip between two vertices has.", "tokens": [400, 337, 264, 4691, 26118, 6828, 286, 528, 281, 362, 264, 7285, 11, 6674, 293, 4274, 16365, 300, 257, 4931, 1296, 732, 32053, 575, 13], "temperature": 0.0, "avg_logprob": -0.138423507863825, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.2024012903566472e-05}, {"id": 131, "seek": 81000, "start": 820.0, "end": 823.0, "text": " And the result would be this.", "tokens": [400, 264, 1874, 576, 312, 341, 13], "temperature": 0.0, "avg_logprob": -0.138423507863825, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.2024012903566472e-05}, {"id": 132, "seek": 81000, "start": 823.0, "end": 829.0, "text": " So same grouped graph, but now I have three additional properties on the edges.", "tokens": [407, 912, 41877, 4295, 11, 457, 586, 286, 362, 1045, 4497, 7221, 322, 264, 8819, 13], "temperature": 0.0, "avg_logprob": -0.138423507863825, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.2024012903566472e-05}, {"id": 133, "seek": 81000, "start": 829.0, "end": 839.0, "text": " Minimum duration, it's in seconds here, maximum duration and average duration and the average bikes available on this station also as a new property.", "tokens": [2829, 332, 449, 16365, 11, 309, 311, 294, 3949, 510, 11, 6674, 16365, 293, 4274, 16365, 293, 264, 4274, 16035, 2435, 322, 341, 5214, 611, 382, 257, 777, 4707, 13], "temperature": 0.0, "avg_logprob": -0.138423507863825, "compression_ratio": 1.7300884955752212, "no_speech_prob": 1.2024012903566472e-05}, {"id": 134, "seek": 83900, "start": 839.0, "end": 843.0, "text": " Same for the second window.", "tokens": [10635, 337, 264, 1150, 4910, 13], "temperature": 0.0, "avg_logprob": -0.1586717896991306, "compression_ratio": 1.6, "no_speech_prob": 8.661665560794063e-06}, {"id": 135, "seek": 83900, "start": 843.0, "end": 851.0, "text": " And now my last example, it is, I call it, not the last example, there's one more.", "tokens": [400, 586, 452, 1036, 1365, 11, 309, 307, 11, 286, 818, 309, 11, 406, 264, 1036, 1365, 11, 456, 311, 472, 544, 13], "temperature": 0.0, "avg_logprob": -0.1586717896991306, "compression_ratio": 1.6, "no_speech_prob": 8.661665560794063e-06}, {"id": 136, "seek": 83900, "start": 851.0, "end": 859.0, "text": " So I added now here a second vertex grouping key function and that's an important thing of the graph stream group.", "tokens": [407, 286, 3869, 586, 510, 257, 1150, 28162, 40149, 2141, 2445, 293, 300, 311, 364, 1021, 551, 295, 264, 4295, 4309, 1594, 13], "temperature": 0.0, "avg_logprob": -0.1586717896991306, "compression_ratio": 1.6, "no_speech_prob": 8.661665560794063e-06}, {"id": 137, "seek": 83900, "start": 859.0, "end": 862.0, "text": " You can also implement your own grouping key function.", "tokens": [509, 393, 611, 4445, 428, 1065, 40149, 2141, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1586717896991306, "compression_ratio": 1.6, "no_speech_prob": 8.661665560794063e-06}, {"id": 138, "seek": 86200, "start": 862.0, "end": 872.0, "text": " For example this one called getDistrict consumes the latitude and longitude property of the vertices and then calculates like a district identifier.", "tokens": [1171, 1365, 341, 472, 1219, 483, 35, 468, 3740, 48823, 264, 45436, 293, 938, 4377, 4707, 295, 264, 32053, 293, 550, 4322, 1024, 411, 257, 6566, 45690, 13], "temperature": 0.0, "avg_logprob": -0.1416949006014092, "compression_ratio": 1.7465437788018434, "no_speech_prob": 5.17234639119124e-06}, {"id": 139, "seek": 86200, "start": 872.0, "end": 877.0, "text": " For example here of Priscilla, whatever, so in which district that belongs to.", "tokens": [1171, 1365, 510, 295, 2114, 49413, 11, 2035, 11, 370, 294, 597, 6566, 300, 12953, 281, 13], "temperature": 0.0, "avg_logprob": -0.1416949006014092, "compression_ratio": 1.7465437788018434, "no_speech_prob": 5.17234639119124e-06}, {"id": 140, "seek": 86200, "start": 877.0, "end": 884.0, "text": " And then the graph is grouped on this representing district identifier.", "tokens": [400, 550, 264, 4295, 307, 41877, 322, 341, 13460, 6566, 45690, 13], "temperature": 0.0, "avg_logprob": -0.1416949006014092, "compression_ratio": 1.7465437788018434, "no_speech_prob": 5.17234639119124e-06}, {"id": 141, "seek": 86200, "start": 884.0, "end": 889.0, "text": " And we also say that for the edges we want to group the edges on the user type.", "tokens": [400, 321, 611, 584, 300, 337, 264, 8819, 321, 528, 281, 1594, 264, 8819, 322, 264, 4195, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1416949006014092, "compression_ratio": 1.7465437788018434, "no_speech_prob": 5.17234639119124e-06}, {"id": 142, "seek": 88900, "start": 889.0, "end": 897.0, "text": " So that means for every edge, so in this data set we have a user type subscriber and something else we will see it shortly.", "tokens": [407, 300, 1355, 337, 633, 4691, 11, 370, 294, 341, 1412, 992, 321, 362, 257, 4195, 2010, 26122, 293, 746, 1646, 321, 486, 536, 309, 13392, 13], "temperature": 0.0, "avg_logprob": -0.14031385571769114, "compression_ratio": 1.765, "no_speech_prob": 5.336522463039728e-06}, {"id": 143, "seek": 88900, "start": 897.0, "end": 904.0, "text": " So that means for every edge I get new now two edges, one for this one user type, one for the other one.", "tokens": [407, 300, 1355, 337, 633, 4691, 286, 483, 777, 586, 732, 8819, 11, 472, 337, 341, 472, 4195, 2010, 11, 472, 337, 264, 661, 472, 13], "temperature": 0.0, "avg_logprob": -0.14031385571769114, "compression_ratio": 1.765, "no_speech_prob": 5.336522463039728e-06}, {"id": 144, "seek": 88900, "start": 904.0, "end": 908.0, "text": " And also here some aggregate functions added.", "tokens": [400, 611, 510, 512, 26118, 6828, 3869, 13], "temperature": 0.0, "avg_logprob": -0.14031385571769114, "compression_ratio": 1.765, "no_speech_prob": 5.336522463039728e-06}, {"id": 145, "seek": 88900, "start": 908.0, "end": 911.0, "text": " And the result is something like this.", "tokens": [400, 264, 1874, 307, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14031385571769114, "compression_ratio": 1.765, "no_speech_prob": 5.336522463039728e-06}, {"id": 146, "seek": 88900, "start": 911.0, "end": 915.0, "text": " So here exemplified for three stations.", "tokens": [407, 510, 24112, 2587, 337, 1045, 13390, 13], "temperature": 0.0, "avg_logprob": -0.14031385571769114, "compression_ratio": 1.765, "no_speech_prob": 5.336522463039728e-06}, {"id": 147, "seek": 91500, "start": 915.0, "end": 922.0, "text": " And here the district ID one, two, and three and the average latitude and longitude for example for visualization,", "tokens": [400, 510, 264, 6566, 7348, 472, 11, 732, 11, 293, 1045, 293, 264, 4274, 45436, 293, 938, 4377, 337, 1365, 337, 25801, 11], "temperature": 0.0, "avg_logprob": -0.20028553838315216, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.661745596327819e-06}, {"id": 148, "seek": 91500, "start": 922.0, "end": 924.0, "text": " proposes to place it on the map.", "tokens": [2365, 4201, 281, 1081, 309, 322, 264, 4471, 13], "temperature": 0.0, "avg_logprob": -0.20028553838315216, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.661745596327819e-06}, {"id": 149, "seek": 91500, "start": 924.0, "end": 931.0, "text": " And for the trips between two stations or between two district representatives,", "tokens": [400, 337, 264, 16051, 1296, 732, 13390, 420, 1296, 732, 6566, 18628, 11], "temperature": 0.0, "avg_logprob": -0.20028553838315216, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.661745596327819e-06}, {"id": 150, "seek": 91500, "start": 931.0, "end": 936.0, "text": " we calculated also the minimum, maximum and average duration and counted them.", "tokens": [321, 15598, 611, 264, 7285, 11, 6674, 293, 4274, 16365, 293, 20150, 552, 13], "temperature": 0.0, "avg_logprob": -0.20028553838315216, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.661745596327819e-06}, {"id": 151, "seek": 93600, "start": 936.0, "end": 948.0, "text": " And you see here the green edges are for the user type customer and the red edges are for user type subscriber.", "tokens": [400, 291, 536, 510, 264, 3092, 8819, 366, 337, 264, 4195, 2010, 5474, 293, 264, 2182, 8819, 366, 337, 4195, 2010, 26122, 13], "temperature": 0.0, "avg_logprob": -0.13165061445121307, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.9517236069077626e-05}, {"id": 152, "seek": 93600, "start": 948.0, "end": 956.0, "text": " So and the last example is then this one here if I say okay as vertex grouping key functions,", "tokens": [407, 293, 264, 1036, 1365, 307, 550, 341, 472, 510, 498, 286, 584, 1392, 382, 28162, 40149, 2141, 6828, 11], "temperature": 0.0, "avg_logprob": -0.13165061445121307, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.9517236069077626e-05}, {"id": 153, "seek": 93600, "start": 956.0, "end": 959.0, "text": " I say please extract me the identifier of that.", "tokens": [286, 584, 1767, 8947, 385, 264, 45690, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.13165061445121307, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.9517236069077626e-05}, {"id": 154, "seek": 93600, "start": 959.0, "end": 965.0, "text": " That means every vertex that exists in the graph stream is placed here and also for the edge identifier.", "tokens": [663, 1355, 633, 28162, 300, 8198, 294, 264, 4295, 4309, 307, 7074, 510, 293, 611, 337, 264, 4691, 45690, 13], "temperature": 0.0, "avg_logprob": -0.13165061445121307, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.9517236069077626e-05}, {"id": 155, "seek": 96500, "start": 965.0, "end": 970.0, "text": " That means since we have unique identifiers, every vertex and edges are placed here.", "tokens": [663, 1355, 1670, 321, 362, 3845, 2473, 23463, 11, 633, 28162, 293, 8819, 366, 7074, 510, 13], "temperature": 0.0, "avg_logprob": -0.15159619275261374, "compression_ratio": 1.5377777777777777, "no_speech_prob": 1.4499001736112405e-05}, {"id": 156, "seek": 96500, "start": 970.0, "end": 976.0, "text": " And this is more or less like a snapshot of the current state of this graph stream for the specific window.", "tokens": [400, 341, 307, 544, 420, 1570, 411, 257, 30163, 295, 264, 2190, 1785, 295, 341, 4295, 4309, 337, 264, 2685, 4910, 13], "temperature": 0.0, "avg_logprob": -0.15159619275261374, "compression_ratio": 1.5377777777777777, "no_speech_prob": 1.4499001736112405e-05}, {"id": 157, "seek": 96500, "start": 976.0, "end": 978.0, "text": " So therefore I call that zoomed in.", "tokens": [407, 4412, 286, 818, 300, 8863, 292, 294, 13], "temperature": 0.0, "avg_logprob": -0.15159619275261374, "compression_ratio": 1.5377777777777777, "no_speech_prob": 1.4499001736112405e-05}, {"id": 158, "seek": 96500, "start": 978.0, "end": 985.0, "text": " It's the most zoomed in configuration that you could use.", "tokens": [467, 311, 264, 881, 8863, 292, 294, 11694, 300, 291, 727, 764, 13], "temperature": 0.0, "avg_logprob": -0.15159619275261374, "compression_ratio": 1.5377777777777777, "no_speech_prob": 1.4499001736112405e-05}, {"id": 159, "seek": 96500, "start": 985.0, "end": 990.0, "text": " Okay, you could imagine implementing this is not that easy.", "tokens": [1033, 11, 291, 727, 3811, 18114, 341, 307, 406, 300, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15159619275261374, "compression_ratio": 1.5377777777777777, "no_speech_prob": 1.4499001736112405e-05}, {"id": 160, "seek": 99000, "start": 990.0, "end": 996.0, "text": " So the master students found a way using Apache Flink and its table API.", "tokens": [407, 264, 4505, 1731, 1352, 257, 636, 1228, 46597, 3235, 475, 293, 1080, 3199, 9362, 13], "temperature": 0.0, "avg_logprob": -0.08892153470944135, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9205142962164246e-05}, {"id": 161, "seek": 99000, "start": 996.0, "end": 1003.0, "text": " So everything works distributed since we are using just the API functions of Apache Flink.", "tokens": [407, 1203, 1985, 12631, 1670, 321, 366, 1228, 445, 264, 9362, 6828, 295, 46597, 3235, 475, 13], "temperature": 0.0, "avg_logprob": -0.08892153470944135, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9205142962164246e-05}, {"id": 162, "seek": 99000, "start": 1003.0, "end": 1007.0, "text": " But we also figured out several implementation challenges.", "tokens": [583, 321, 611, 8932, 484, 2940, 11420, 4759, 13], "temperature": 0.0, "avg_logprob": -0.08892153470944135, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9205142962164246e-05}, {"id": 163, "seek": 99000, "start": 1007.0, "end": 1012.0, "text": " So first was to find a good graph representation.", "tokens": [407, 700, 390, 281, 915, 257, 665, 4295, 10290, 13], "temperature": 0.0, "avg_logprob": -0.08892153470944135, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9205142962164246e-05}, {"id": 164, "seek": 99000, "start": 1012.0, "end": 1016.0, "text": " Second one is since we are creating a workflow of this graph stream,", "tokens": [5736, 472, 307, 1670, 321, 366, 4084, 257, 20993, 295, 341, 4295, 4309, 11], "temperature": 0.0, "avg_logprob": -0.08892153470944135, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9205142962164246e-05}, {"id": 165, "seek": 101600, "start": 1016.0, "end": 1022.0, "text": " we have to ensure the chronological ordering of every step in this workflow.", "tokens": [321, 362, 281, 5586, 264, 19393, 4383, 21739, 295, 633, 1823, 294, 341, 20993, 13], "temperature": 0.0, "avg_logprob": -0.13673116083014503, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.4054750863579102e-05}, {"id": 166, "seek": 101600, "start": 1022.0, "end": 1026.0, "text": " As a third point is also you want to ensure the scalability.", "tokens": [1018, 257, 2636, 935, 307, 611, 291, 528, 281, 5586, 264, 15664, 2310, 13], "temperature": 0.0, "avg_logprob": -0.13673116083014503, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.4054750863579102e-05}, {"id": 167, "seek": 101600, "start": 1026.0, "end": 1034.0, "text": " Since if we scale out this algorithm, the scalability should be also high.", "tokens": [4162, 498, 321, 4373, 484, 341, 9284, 11, 264, 15664, 2310, 820, 312, 611, 1090, 13], "temperature": 0.0, "avg_logprob": -0.13673116083014503, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.4054750863579102e-05}, {"id": 168, "seek": 101600, "start": 1034.0, "end": 1041.0, "text": " And also keep the state as minimum as possible and provide a low latency and high throughput.", "tokens": [400, 611, 1066, 264, 1785, 382, 7285, 382, 1944, 293, 2893, 257, 2295, 27043, 293, 1090, 44629, 13], "temperature": 0.0, "avg_logprob": -0.13673116083014503, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.4054750863579102e-05}, {"id": 169, "seek": 104100, "start": 1041.0, "end": 1046.0, "text": " So these were several challenges the master student solved quite well.", "tokens": [407, 613, 645, 2940, 4759, 264, 4505, 3107, 13041, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.12152809845773797, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.112010795623064e-05}, {"id": 170, "seek": 104100, "start": 1046.0, "end": 1055.0, "text": " And at the end we created a grouping operator looking like this.", "tokens": [400, 412, 264, 917, 321, 2942, 257, 40149, 12973, 1237, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12152809845773797, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.112010795623064e-05}, {"id": 171, "seek": 104100, "start": 1055.0, "end": 1057.0, "text": " I don't want to get into detail.", "tokens": [286, 500, 380, 528, 281, 483, 666, 2607, 13], "temperature": 0.0, "avg_logprob": -0.12152809845773797, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.112010795623064e-05}, {"id": 172, "seek": 104100, "start": 1057.0, "end": 1063.0, "text": " This is just an architectural overview of every Flink steps we used.", "tokens": [639, 307, 445, 364, 26621, 12492, 295, 633, 3235, 475, 4439, 321, 1143, 13], "temperature": 0.0, "avg_logprob": -0.12152809845773797, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.112010795623064e-05}, {"id": 173, "seek": 104100, "start": 1063.0, "end": 1068.0, "text": " What is quite interesting is that we created like an operator encapsulation of this.", "tokens": [708, 307, 1596, 1880, 307, 300, 321, 2942, 411, 364, 12973, 38745, 2776, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.12152809845773797, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.112010795623064e-05}, {"id": 174, "seek": 106800, "start": 1068.0, "end": 1074.0, "text": " That means the operator consumes a graph stream at input and has a graph stream as output.", "tokens": [663, 1355, 264, 12973, 48823, 257, 4295, 4309, 412, 4846, 293, 575, 257, 4295, 4309, 382, 5598, 13], "temperature": 0.0, "avg_logprob": -0.10415199279785156, "compression_ratio": 1.8757062146892656, "no_speech_prob": 2.246430995000992e-05}, {"id": 175, "seek": 106800, "start": 1074.0, "end": 1077.0, "text": " That means you can combine several of these grouping operators.", "tokens": [663, 1355, 291, 393, 10432, 2940, 295, 613, 40149, 19077, 13], "temperature": 0.0, "avg_logprob": -0.10415199279785156, "compression_ratio": 1.8757062146892656, "no_speech_prob": 2.246430995000992e-05}, {"id": 176, "seek": 106800, "start": 1077.0, "end": 1083.0, "text": " Or if you define another graph stream algorithm that produces a graph stream as output,", "tokens": [1610, 498, 291, 6964, 1071, 4295, 4309, 9284, 300, 14725, 257, 4295, 4309, 382, 5598, 11], "temperature": 0.0, "avg_logprob": -0.10415199279785156, "compression_ratio": 1.8757062146892656, "no_speech_prob": 2.246430995000992e-05}, {"id": 177, "seek": 106800, "start": 1083.0, "end": 1086.0, "text": " you can just put them before.", "tokens": [291, 393, 445, 829, 552, 949, 13], "temperature": 0.0, "avg_logprob": -0.10415199279785156, "compression_ratio": 1.8757062146892656, "no_speech_prob": 2.246430995000992e-05}, {"id": 178, "seek": 106800, "start": 1086.0, "end": 1090.0, "text": " So you can like chaining these grouping operators together.", "tokens": [407, 291, 393, 411, 417, 3686, 613, 40149, 19077, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10415199279785156, "compression_ratio": 1.8757062146892656, "no_speech_prob": 2.246430995000992e-05}, {"id": 179, "seek": 109000, "start": 1090.0, "end": 1098.0, "text": " And like I said, this consists of the mapping of the input data, the duplication of vertices,", "tokens": [400, 411, 286, 848, 11, 341, 14689, 295, 264, 18350, 295, 264, 4846, 1412, 11, 264, 17154, 399, 295, 32053, 11], "temperature": 0.0, "avg_logprob": -0.13196857661417086, "compression_ratio": 1.508108108108108, "no_speech_prob": 3.7577570765279233e-05}, {"id": 180, "seek": 109000, "start": 1098.0, "end": 1106.0, "text": " grouping of vertices and edges, and then mapping it to an output graph stream.", "tokens": [40149, 295, 32053, 293, 8819, 11, 293, 550, 18350, 309, 281, 364, 5598, 4295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.13196857661417086, "compression_ratio": 1.508108108108108, "no_speech_prob": 3.7577570765279233e-05}, {"id": 181, "seek": 109000, "start": 1106.0, "end": 1108.0, "text": " How an API would look like?", "tokens": [1012, 364, 9362, 576, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.13196857661417086, "compression_ratio": 1.508108108108108, "no_speech_prob": 3.7577570765279233e-05}, {"id": 182, "seek": 109000, "start": 1108.0, "end": 1114.0, "text": " It looks a bit messy, but I think it's quite fast clear what's happening here.", "tokens": [467, 1542, 257, 857, 16191, 11, 457, 286, 519, 309, 311, 1596, 2370, 1850, 437, 311, 2737, 510, 13], "temperature": 0.0, "avg_logprob": -0.13196857661417086, "compression_ratio": 1.508108108108108, "no_speech_prob": 3.7577570765279233e-05}, {"id": 183, "seek": 111400, "start": 1114.0, "end": 1122.0, "text": " So first you have to define the execution environment of Flink.", "tokens": [407, 700, 291, 362, 281, 6964, 264, 15058, 2823, 295, 3235, 475, 13], "temperature": 0.0, "avg_logprob": -0.11812725314846287, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.709725231397897e-05}, {"id": 184, "seek": 111400, "start": 1122.0, "end": 1128.0, "text": " Then we read the data from some streaming source, for example, a socket source,", "tokens": [1396, 321, 1401, 264, 1412, 490, 512, 11791, 4009, 11, 337, 1365, 11, 257, 19741, 4009, 11], "temperature": 0.0, "avg_logprob": -0.11812725314846287, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.709725231397897e-05}, {"id": 185, "seek": 111400, "start": 1128.0, "end": 1133.0, "text": " or some Kafka stream, or whatever you want, whatever Flink supports in our case.", "tokens": [420, 512, 47064, 4309, 11, 420, 2035, 291, 528, 11, 2035, 3235, 475, 9346, 294, 527, 1389, 13], "temperature": 0.0, "avg_logprob": -0.11812725314846287, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.709725231397897e-05}, {"id": 186, "seek": 111400, "start": 1133.0, "end": 1140.0, "text": " Then we map it to a graph stream object, which is the internal representation of our stream.", "tokens": [1396, 321, 4471, 309, 281, 257, 4295, 4309, 2657, 11, 597, 307, 264, 6920, 10290, 295, 527, 4309, 13], "temperature": 0.0, "avg_logprob": -0.11812725314846287, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.709725231397897e-05}, {"id": 187, "seek": 114000, "start": 1140.0, "end": 1144.0, "text": " The interesting part here, you define the grouping operator.", "tokens": [440, 1880, 644, 510, 11, 291, 6964, 264, 40149, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 188, "seek": 114000, "start": 1144.0, "end": 1147.0, "text": " So in the middle, that's the grouping config I showed you in the examples.", "tokens": [407, 294, 264, 2808, 11, 300, 311, 264, 40149, 6662, 286, 4712, 291, 294, 264, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 189, "seek": 114000, "start": 1147.0, "end": 1149.0, "text": " You can define it here by an API.", "tokens": [509, 393, 6964, 309, 510, 538, 364, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 190, "seek": 114000, "start": 1149.0, "end": 1151.0, "text": " You set the window size.", "tokens": [509, 992, 264, 4910, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 191, "seek": 114000, "start": 1151.0, "end": 1154.0, "text": " You set the vertex and edge grouping keys.", "tokens": [509, 992, 264, 28162, 293, 4691, 40149, 9317, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 192, "seek": 114000, "start": 1154.0, "end": 1157.0, "text": " You set the aggregate functions and so on.", "tokens": [509, 992, 264, 26118, 6828, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 193, "seek": 114000, "start": 1157.0, "end": 1161.0, "text": " And at the end, you just execute this operator on this graph stream,", "tokens": [400, 412, 264, 917, 11, 291, 445, 14483, 341, 12973, 322, 341, 4295, 4309, 11], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 194, "seek": 114000, "start": 1161.0, "end": 1167.0, "text": " and then you can define a thing or just print it to the console, or whatever you want.", "tokens": [293, 550, 291, 393, 6964, 257, 551, 420, 445, 4482, 309, 281, 264, 11076, 11, 420, 2035, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.1225896299931041, "compression_ratio": 1.7868852459016393, "no_speech_prob": 2.1426252715173177e-05}, {"id": 195, "seek": 116700, "start": 1167.0, "end": 1174.0, "text": " So that's the operator call, how you define it in the API.", "tokens": [407, 300, 311, 264, 12973, 818, 11, 577, 291, 6964, 309, 294, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1551814935146234, "compression_ratio": 1.46, "no_speech_prob": 4.120140147279017e-05}, {"id": 196, "seek": 116700, "start": 1174.0, "end": 1182.0, "text": " And current state, the students are about at 90% of the complete implementation of that.", "tokens": [400, 2190, 1785, 11, 264, 1731, 366, 466, 412, 4289, 4, 295, 264, 3566, 11420, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.1551814935146234, "compression_ratio": 1.46, "no_speech_prob": 4.120140147279017e-05}, {"id": 197, "seek": 116700, "start": 1182.0, "end": 1191.0, "text": " We figured out some bugs at the SQL or at the table API of Flink that were not fixed yet,", "tokens": [492, 8932, 484, 512, 15120, 412, 264, 19200, 420, 412, 264, 3199, 9362, 295, 3235, 475, 300, 645, 406, 6806, 1939, 11], "temperature": 0.0, "avg_logprob": -0.1551814935146234, "compression_ratio": 1.46, "no_speech_prob": 4.120140147279017e-05}, {"id": 198, "seek": 116700, "start": 1191.0, "end": 1194.0, "text": " so we had to define some workaround that cost us time.", "tokens": [370, 321, 632, 281, 6964, 512, 589, 25762, 300, 2063, 505, 565, 13], "temperature": 0.0, "avg_logprob": -0.1551814935146234, "compression_ratio": 1.46, "no_speech_prob": 4.120140147279017e-05}, {"id": 199, "seek": 119400, "start": 1194.0, "end": 1198.0, "text": " But like I said, we found the workaround.", "tokens": [583, 411, 286, 848, 11, 321, 1352, 264, 589, 25762, 13], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 200, "seek": 119400, "start": 1198.0, "end": 1201.0, "text": " And the next steps are that we plan an evaluation.", "tokens": [400, 264, 958, 4439, 366, 300, 321, 1393, 364, 13344, 13], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 201, "seek": 119400, "start": 1201.0, "end": 1204.0, "text": " So how is the latency and throughput of this complete system?", "tokens": [407, 577, 307, 264, 27043, 293, 44629, 295, 341, 3566, 1185, 30], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 202, "seek": 119400, "start": 1204.0, "end": 1208.0, "text": " And we want to test it on real-world and synthetic graph streams.", "tokens": [400, 321, 528, 281, 1500, 309, 322, 957, 12, 13217, 293, 23420, 4295, 15842, 13], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 203, "seek": 119400, "start": 1208.0, "end": 1211.0, "text": " And maybe then publish some results, so let's see.", "tokens": [400, 1310, 550, 11374, 512, 3542, 11, 370, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 204, "seek": 119400, "start": 1211.0, "end": 1219.0, "text": " And also, the user-defined key and aggregate functions are still under development.", "tokens": [400, 611, 11, 264, 4195, 12, 37716, 2141, 293, 26118, 6828, 366, 920, 833, 3250, 13], "temperature": 0.0, "avg_logprob": -0.141891353732937, "compression_ratio": 1.5301724137931034, "no_speech_prob": 1.0125362678081729e-05}, {"id": 205, "seek": 121900, "start": 1219.0, "end": 1224.0, "text": " Okay, then that's it. That's all folks.", "tokens": [1033, 11, 550, 300, 311, 309, 13, 663, 311, 439, 4024, 13], "temperature": 0.0, "avg_logprob": -0.152455969554622, "compression_ratio": 1.4855769230769231, "no_speech_prob": 5.910794425290078e-05}, {"id": 206, "seek": 121900, "start": 1224.0, "end": 1229.0, "text": " Please check out our GitHub repository, or maybe you want to contribute, so we are open for this.", "tokens": [2555, 1520, 484, 527, 23331, 25841, 11, 420, 1310, 291, 528, 281, 10586, 11, 370, 321, 366, 1269, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.152455969554622, "compression_ratio": 1.4855769230769231, "no_speech_prob": 5.910794425290078e-05}, {"id": 207, "seek": 121900, "start": 1229.0, "end": 1236.0, "text": " The two links here at the bottom are also the icons are two other projects.", "tokens": [440, 732, 6123, 510, 412, 264, 2767, 366, 611, 264, 23308, 366, 732, 661, 4455, 13], "temperature": 0.0, "avg_logprob": -0.152455969554622, "compression_ratio": 1.4855769230769231, "no_speech_prob": 5.910794425290078e-05}, {"id": 208, "seek": 121900, "start": 1236.0, "end": 1238.0, "text": " The one is Gradube.", "tokens": [440, 472, 307, 16710, 1977, 13], "temperature": 0.0, "avg_logprob": -0.152455969554622, "compression_ratio": 1.4855769230769231, "no_speech_prob": 5.910794425290078e-05}, {"id": 209, "seek": 121900, "start": 1238.0, "end": 1243.0, "text": " This is a big temporary graph processing engine also based on Apache Flink.", "tokens": [639, 307, 257, 955, 13413, 4295, 9007, 2848, 611, 2361, 322, 46597, 3235, 475, 13], "temperature": 0.0, "avg_logprob": -0.152455969554622, "compression_ratio": 1.4855769230769231, "no_speech_prob": 5.910794425290078e-05}, {"id": 210, "seek": 124300, "start": 1243.0, "end": 1251.0, "text": " So there where I'm also a main contributor to that project, which was initially created by Marty Nugans,", "tokens": [407, 456, 689, 286, 478, 611, 257, 2135, 42859, 281, 300, 1716, 11, 597, 390, 9105, 2942, 538, 29192, 426, 697, 599, 11], "temperature": 0.0, "avg_logprob": -0.18080837830253269, "compression_ratio": 1.478813559322034, "no_speech_prob": 3.214618482161313e-05}, {"id": 211, "seek": 124300, "start": 1251.0, "end": 1254.0, "text": " who's now working at Neo4j.", "tokens": [567, 311, 586, 1364, 412, 24458, 19, 73, 13], "temperature": 0.0, "avg_logprob": -0.18080837830253269, "compression_ratio": 1.478813559322034, "no_speech_prob": 3.214618482161313e-05}, {"id": 212, "seek": 124300, "start": 1254.0, "end": 1259.0, "text": " And also, the temporary graph explorer is a user interface for that system,", "tokens": [400, 611, 11, 264, 13413, 4295, 39680, 307, 257, 4195, 9226, 337, 300, 1185, 11], "temperature": 0.0, "avg_logprob": -0.18080837830253269, "compression_ratio": 1.478813559322034, "no_speech_prob": 3.214618482161313e-05}, {"id": 213, "seek": 124300, "start": 1259.0, "end": 1267.0, "text": " where you can play around with the evolution of a graph, but in a historical data set.", "tokens": [689, 291, 393, 862, 926, 365, 264, 9303, 295, 257, 4295, 11, 457, 294, 257, 8584, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18080837830253269, "compression_ratio": 1.478813559322034, "no_speech_prob": 3.214618482161313e-05}, {"id": 214, "seek": 124300, "start": 1267.0, "end": 1272.0, "text": " Okay, so that's it. And please ask questions. Thanks.", "tokens": [1033, 11, 370, 300, 311, 309, 13, 400, 1767, 1029, 1651, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.18080837830253269, "compression_ratio": 1.478813559322034, "no_speech_prob": 3.214618482161313e-05}, {"id": 215, "seek": 127200, "start": 1272.0, "end": 1281.0, "text": " Yeah, please.", "tokens": [865, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 216, "seek": 127200, "start": 1281.0, "end": 1289.0, "text": " On one slide, you said a problem was to decide on the, on slide 20, I think, as well.", "tokens": [1282, 472, 4137, 11, 291, 848, 257, 1154, 390, 281, 4536, 322, 264, 11, 322, 4137, 945, 11, 286, 519, 11, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 217, "seek": 127200, "start": 1289.0, "end": 1290.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 218, "seek": 127200, "start": 1290.0, "end": 1293.0, "text": " The optimal graph representation in the streaming model.", "tokens": [440, 16252, 4295, 10290, 294, 264, 11791, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 219, "seek": 127200, "start": 1293.0, "end": 1294.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 220, "seek": 127200, "start": 1294.0, "end": 1297.0, "text": " What was the answer?", "tokens": [708, 390, 264, 1867, 30], "temperature": 0.0, "avg_logprob": -0.2710473378499349, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0006579646724276245}, {"id": 221, "seek": 129700, "start": 1297.0, "end": 1303.0, "text": " And so the question was, what, so we had this challenge to find the optimal graph representation,", "tokens": [400, 370, 264, 1168, 390, 11, 437, 11, 370, 321, 632, 341, 3430, 281, 915, 264, 16252, 4295, 10290, 11], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 222, "seek": 129700, "start": 1303.0, "end": 1304.0, "text": " and what was the answer?", "tokens": [293, 437, 390, 264, 1867, 30], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 223, "seek": 129700, "start": 1304.0, "end": 1309.0, "text": " The answer was a triple stream, but a rich triple stream, we called it,", "tokens": [440, 1867, 390, 257, 15508, 4309, 11, 457, 257, 4593, 15508, 4309, 11, 321, 1219, 309, 11], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 224, "seek": 129700, "start": 1309.0, "end": 1316.0, "text": " since two property graph vertices are connected with an edge.", "tokens": [1670, 732, 4707, 4295, 32053, 366, 4582, 365, 364, 4691, 13], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 225, "seek": 129700, "start": 1316.0, "end": 1324.0, "text": " That means every vertex consists of the label and possibly a big set of key value pairs as properties,", "tokens": [663, 1355, 633, 28162, 14689, 295, 264, 7645, 293, 6264, 257, 955, 992, 295, 2141, 2158, 15494, 382, 7221, 11], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 226, "seek": 129700, "start": 1324.0, "end": 1326.0, "text": " and the same for the edges.", "tokens": [293, 264, 912, 337, 264, 8819, 13], "temperature": 0.0, "avg_logprob": -0.13828529558683697, "compression_ratio": 1.682608695652174, "no_speech_prob": 2.503207724657841e-05}, {"id": 227, "seek": 132600, "start": 1326.0, "end": 1332.0, "text": " And this was our optimal, because you can then model everything with this model.", "tokens": [400, 341, 390, 527, 16252, 11, 570, 291, 393, 550, 2316, 1203, 365, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 228, "seek": 132600, "start": 1332.0, "end": 1338.0, "text": " But the counterpart of this was in here that we have to do a vertex de-deplication.", "tokens": [583, 264, 22335, 295, 341, 390, 294, 510, 300, 321, 362, 281, 360, 257, 28162, 368, 12, 1479, 4770, 399, 13], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 229, "seek": 132600, "start": 1338.0, "end": 1341.0, "text": " For example, if you have a self-loop, so from one vertex to another one,", "tokens": [1171, 1365, 11, 498, 291, 362, 257, 2698, 12, 46623, 11, 370, 490, 472, 28162, 281, 1071, 472, 11], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 230, "seek": 132600, "start": 1341.0, "end": 1346.0, "text": " we have a duplicate of that vertex, so we have to de-deplicate it afterwards for this model.", "tokens": [321, 362, 257, 23976, 295, 300, 28162, 11, 370, 321, 362, 281, 368, 12, 1479, 4770, 473, 309, 10543, 337, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 231, "seek": 132600, "start": 1346.0, "end": 1348.0, "text": " So this was one counterpart.", "tokens": [407, 341, 390, 472, 22335, 13], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 232, "seek": 132600, "start": 1348.0, "end": 1355.0, "text": " Yeah, but we figured out that using every concept of the property graph model there as a triple is,", "tokens": [865, 11, 457, 321, 8932, 484, 300, 1228, 633, 3410, 295, 264, 4707, 4295, 2316, 456, 382, 257, 15508, 307, 11], "temperature": 0.0, "avg_logprob": -0.12225494384765626, "compression_ratio": 1.8, "no_speech_prob": 3.0681578209623694e-05}, {"id": 233, "seek": 135500, "start": 1355.0, "end": 1361.0, "text": " that was the best choice for the students.", "tokens": [300, 390, 264, 1151, 3922, 337, 264, 1731, 13], "temperature": 0.0, "avg_logprob": -0.2227830171585083, "compression_ratio": 1.5618556701030928, "no_speech_prob": 3.859159551211633e-05}, {"id": 234, "seek": 135500, "start": 1361.0, "end": 1363.0, "text": " So another, yeah?", "tokens": [407, 1071, 11, 1338, 30], "temperature": 0.0, "avg_logprob": -0.2227830171585083, "compression_ratio": 1.5618556701030928, "no_speech_prob": 3.859159551211633e-05}, {"id": 235, "seek": 135500, "start": 1363.0, "end": 1370.0, "text": " Would you comment a bit more on the scalability, like what graph size should test this on?", "tokens": [6068, 291, 2871, 257, 857, 544, 322, 264, 15664, 2310, 11, 411, 437, 4295, 2744, 820, 1500, 341, 322, 30], "temperature": 0.0, "avg_logprob": -0.2227830171585083, "compression_ratio": 1.5618556701030928, "no_speech_prob": 3.859159551211633e-05}, {"id": 236, "seek": 135500, "start": 1370.0, "end": 1375.0, "text": " Yeah, so the question was some words about the scalability.", "tokens": [865, 11, 370, 264, 1168, 390, 512, 2283, 466, 264, 15664, 2310, 13], "temperature": 0.0, "avg_logprob": -0.2227830171585083, "compression_ratio": 1.5618556701030928, "no_speech_prob": 3.859159551211633e-05}, {"id": 237, "seek": 135500, "start": 1375.0, "end": 1383.0, "text": " The scalability is an open point of future work, so we don't have concrete results of that.", "tokens": [440, 15664, 2310, 307, 364, 1269, 935, 295, 2027, 589, 11, 370, 321, 500, 380, 362, 9859, 3542, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.2227830171585083, "compression_ratio": 1.5618556701030928, "no_speech_prob": 3.859159551211633e-05}, {"id": 238, "seek": 138300, "start": 1383.0, "end": 1391.0, "text": " We tested it with some city bike data that we interpreted as a stream, so some historical data.", "tokens": [492, 8246, 309, 365, 512, 2307, 5656, 1412, 300, 321, 26749, 382, 257, 4309, 11, 370, 512, 8584, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12681855382146062, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.00011368284322088584}, {"id": 239, "seek": 138300, "start": 1391.0, "end": 1399.0, "text": " And we could process, I think it was about 600,000 edges in a few seconds.", "tokens": [400, 321, 727, 1399, 11, 286, 519, 309, 390, 466, 11849, 11, 1360, 8819, 294, 257, 1326, 3949, 13], "temperature": 0.0, "avg_logprob": -0.12681855382146062, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.00011368284322088584}, {"id": 240, "seek": 138300, "start": 1399.0, "end": 1408.0, "text": " But this is just some first results, and we have not tried it on big and high-frequent graph streams on a cluster.", "tokens": [583, 341, 307, 445, 512, 700, 3542, 11, 293, 321, 362, 406, 3031, 309, 322, 955, 293, 1090, 12, 19325, 28842, 4295, 15842, 322, 257, 13630, 13], "temperature": 0.0, "avg_logprob": -0.12681855382146062, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.00011368284322088584}, {"id": 241, "seek": 140800, "start": 1408.0, "end": 1417.0, "text": " Because we have huge flint clusters at our university, so we can benchmark the scalability of that in a later step.", "tokens": [1436, 321, 362, 2603, 932, 686, 23313, 412, 527, 5454, 11, 370, 321, 393, 18927, 264, 15664, 2310, 295, 300, 294, 257, 1780, 1823, 13], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 242, "seek": 140800, "start": 1417.0, "end": 1419.0, "text": " Yeah, thanks.", "tokens": [865, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 243, "seek": 140800, "start": 1419.0, "end": 1420.0, "text": " Yeah?", "tokens": [865, 30], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 244, "seek": 140800, "start": 1420.0, "end": 1425.0, "text": " These aggregate functions, are they part of, you know, like a Java API, and how do you define them?", "tokens": [1981, 26118, 6828, 11, 366, 436, 644, 295, 11, 291, 458, 11, 411, 257, 10745, 9362, 11, 293, 577, 360, 291, 6964, 552, 30], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 245, "seek": 140800, "start": 1425.0, "end": 1429.0, "text": " Yeah, so the question was how we define the aggregate functions.", "tokens": [865, 11, 370, 264, 1168, 390, 577, 321, 6964, 264, 26118, 6828, 13], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 246, "seek": 140800, "start": 1429.0, "end": 1434.0, "text": " So we have a set of predefined aggregate functions, like the count, average, min, max,", "tokens": [407, 321, 362, 257, 992, 295, 659, 37716, 26118, 6828, 11, 411, 264, 1207, 11, 4274, 11, 923, 11, 11469, 11], "temperature": 0.0, "avg_logprob": -0.18955851064144985, "compression_ratio": 1.6753246753246753, "no_speech_prob": 5.055364817962982e-05}, {"id": 247, "seek": 143400, "start": 1434.0, "end": 1439.0, "text": " and then you have an interface you can implement against, so there's an interface called aggregate function,", "tokens": [293, 550, 291, 362, 364, 9226, 291, 393, 4445, 1970, 11, 370, 456, 311, 364, 9226, 1219, 26118, 2445, 11], "temperature": 0.0, "avg_logprob": -0.15948816446157602, "compression_ratio": 1.835820895522388, "no_speech_prob": 6.590531120309606e-05}, {"id": 248, "seek": 143400, "start": 1439.0, "end": 1450.0, "text": " and then you have to implement, I think, two or three functions, and then you can define your own and use it then here on, yeah, there.", "tokens": [293, 550, 291, 362, 281, 4445, 11, 286, 519, 11, 732, 420, 1045, 6828, 11, 293, 550, 291, 393, 6964, 428, 1065, 293, 764, 309, 550, 510, 322, 11, 1338, 11, 456, 13], "temperature": 0.0, "avg_logprob": -0.15948816446157602, "compression_ratio": 1.835820895522388, "no_speech_prob": 6.590531120309606e-05}, {"id": 249, "seek": 143400, "start": 1450.0, "end": 1459.0, "text": " Where you give the classes of the account and average property, you can give your own class, and then it will be used.", "tokens": [2305, 291, 976, 264, 5359, 295, 264, 2696, 293, 4274, 4707, 11, 291, 393, 976, 428, 1065, 1508, 11, 293, 550, 309, 486, 312, 1143, 13], "temperature": 0.0, "avg_logprob": -0.15948816446157602, "compression_ratio": 1.835820895522388, "no_speech_prob": 6.590531120309606e-05}, {"id": 250, "seek": 143400, "start": 1459.0, "end": 1460.0, "text": " Yeah?", "tokens": [865, 30], "temperature": 0.0, "avg_logprob": -0.15948816446157602, "compression_ratio": 1.835820895522388, "no_speech_prob": 6.590531120309606e-05}, {"id": 251, "seek": 146000, "start": 1460.0, "end": 1467.0, "text": " Could you elaborate more on the real-life use cases or real-life applications?", "tokens": [7497, 291, 20945, 544, 322, 264, 957, 12, 9073, 764, 3331, 420, 957, 12, 9073, 5821, 30], "temperature": 0.0, "avg_logprob": -0.19738245584878578, "compression_ratio": 1.7797619047619047, "no_speech_prob": 8.199921285267919e-05}, {"id": 252, "seek": 146000, "start": 1467.0, "end": 1473.0, "text": " So the question is if we elaborate more real-life use cases and real-life questions.", "tokens": [407, 264, 1168, 307, 498, 321, 20945, 544, 957, 12, 9073, 764, 3331, 293, 957, 12, 9073, 1651, 13], "temperature": 0.0, "avg_logprob": -0.19738245584878578, "compression_ratio": 1.7797619047619047, "no_speech_prob": 8.199921285267919e-05}, {"id": 253, "seek": 146000, "start": 1473.0, "end": 1475.0, "text": " So applications.", "tokens": [407, 5821, 13], "temperature": 0.0, "avg_logprob": -0.19738245584878578, "compression_ratio": 1.7797619047619047, "no_speech_prob": 8.199921285267919e-05}, {"id": 254, "seek": 146000, "start": 1475.0, "end": 1477.0, "text": " Applications.", "tokens": [26519, 763, 13], "temperature": 0.0, "avg_logprob": -0.19738245584878578, "compression_ratio": 1.7797619047619047, "no_speech_prob": 8.199921285267919e-05}, {"id": 255, "seek": 146000, "start": 1477.0, "end": 1485.0, "text": " So since, yeah, so we are in, so I'm at the university, that means we are missing real-world data a lot,", "tokens": [407, 1670, 11, 1338, 11, 370, 321, 366, 294, 11, 370, 286, 478, 412, 264, 5454, 11, 300, 1355, 321, 366, 5361, 957, 12, 13217, 1412, 257, 688, 11], "temperature": 0.0, "avg_logprob": -0.19738245584878578, "compression_ratio": 1.7797619047619047, "no_speech_prob": 8.199921285267919e-05}, {"id": 256, "seek": 148500, "start": 1485.0, "end": 1491.0, "text": " and we need also some input from companies to provide us with real-world data that we can use.", "tokens": [293, 321, 643, 611, 512, 4846, 490, 3431, 281, 2893, 505, 365, 957, 12, 13217, 1412, 300, 321, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.09446128691085662, "compression_ratio": 1.6639344262295082, "no_speech_prob": 2.7087260605185293e-05}, {"id": 257, "seek": 148500, "start": 1491.0, "end": 1501.0, "text": " So use cases could be, we also have to, we only have this bike sharing stuff or Twitter data and whatever,", "tokens": [407, 764, 3331, 727, 312, 11, 321, 611, 362, 281, 11, 321, 787, 362, 341, 5656, 5414, 1507, 420, 5794, 1412, 293, 2035, 11], "temperature": 0.0, "avg_logprob": -0.09446128691085662, "compression_ratio": 1.6639344262295082, "no_speech_prob": 2.7087260605185293e-05}, {"id": 258, "seek": 148500, "start": 1501.0, "end": 1506.0, "text": " and I think if you have something like this aggregated function, like here an average property,", "tokens": [293, 286, 519, 498, 291, 362, 746, 411, 341, 16743, 770, 2445, 11, 411, 510, 364, 4274, 4707, 11], "temperature": 0.0, "avg_logprob": -0.09446128691085662, "compression_ratio": 1.6639344262295082, "no_speech_prob": 2.7087260605185293e-05}, {"id": 259, "seek": 148500, "start": 1506.0, "end": 1513.0, "text": " you can use, because at the end it's a time series of changing values, for example, of the average property,", "tokens": [291, 393, 764, 11, 570, 412, 264, 917, 309, 311, 257, 565, 2638, 295, 4473, 4190, 11, 337, 1365, 11, 295, 264, 4274, 4707, 11], "temperature": 0.0, "avg_logprob": -0.09446128691085662, "compression_ratio": 1.6639344262295082, "no_speech_prob": 2.7087260605185293e-05}, {"id": 260, "seek": 151300, "start": 1513.0, "end": 1517.0, "text": " and of like defining a threshold and get the notification afterwards.", "tokens": [293, 295, 411, 17827, 257, 14678, 293, 483, 264, 11554, 10543, 13], "temperature": 0.0, "avg_logprob": -0.1363391876220703, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.0067115555284545e-05}, {"id": 261, "seek": 151300, "start": 1517.0, "end": 1521.0, "text": " I think this is maybe one good application afterwards.", "tokens": [286, 519, 341, 307, 1310, 472, 665, 3861, 10543, 13], "temperature": 0.0, "avg_logprob": -0.1363391876220703, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.0067115555284545e-05}, {"id": 262, "seek": 151300, "start": 1521.0, "end": 1527.0, "text": " So to, for example, if you have network traffic, you see, okay, now the average, I don't know, packet size is increasing.", "tokens": [407, 281, 11, 337, 1365, 11, 498, 291, 362, 3209, 6419, 11, 291, 536, 11, 1392, 11, 586, 264, 4274, 11, 286, 500, 380, 458, 11, 20300, 2744, 307, 5662, 13], "temperature": 0.0, "avg_logprob": -0.1363391876220703, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.0067115555284545e-05}, {"id": 263, "seek": 151300, "start": 1527.0, "end": 1530.0, "text": " Now I get notified, for example, like this.", "tokens": [823, 286, 483, 18013, 11, 337, 1365, 11, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1363391876220703, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.0067115555284545e-05}, {"id": 264, "seek": 151300, "start": 1530.0, "end": 1532.0, "text": " But this could be an application, yeah.", "tokens": [583, 341, 727, 312, 364, 3861, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.1363391876220703, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.0067115555284545e-05}, {"id": 265, "seek": 153200, "start": 1532.0, "end": 1561.0, "text": " The idea was to use like a video stream for that, but then the question is,", "tokens": [440, 1558, 390, 281, 764, 411, 257, 960, 4309, 337, 300, 11, 457, 550, 264, 1168, 307, 11], "temperature": 0.0, "avg_logprob": -0.32725880362770776, "compression_ratio": 1.0416666666666667, "no_speech_prob": 0.00047851516865193844}, {"id": 266, "seek": 156100, "start": 1561.0, "end": 1568.0, "text": " how much graph that is, could that maybe not be done just in a regular stream processing way?", "tokens": [577, 709, 4295, 300, 307, 11, 727, 300, 1310, 406, 312, 1096, 445, 294, 257, 3890, 4309, 9007, 636, 30], "temperature": 0.0, "avg_logprob": -0.10483596605413101, "compression_ratio": 1.6510416666666667, "no_speech_prob": 6.909660442033783e-05}, {"id": 267, "seek": 156100, "start": 1568.0, "end": 1576.0, "text": " So I think this is just advisable to use that if you have some quite complex relationships between entities,", "tokens": [407, 286, 519, 341, 307, 445, 10280, 712, 281, 764, 300, 498, 291, 362, 512, 1596, 3997, 6159, 1296, 16667, 11], "temperature": 0.0, "avg_logprob": -0.10483596605413101, "compression_ratio": 1.6510416666666667, "no_speech_prob": 6.909660442033783e-05}, {"id": 268, "seek": 156100, "start": 1576.0, "end": 1585.0, "text": " then you can use this system besides just an ordinary stream processing engine or complex event processing engine.", "tokens": [550, 291, 393, 764, 341, 1185, 11868, 445, 364, 10547, 4309, 9007, 2848, 420, 3997, 2280, 9007, 2848, 13], "temperature": 0.0, "avg_logprob": -0.10483596605413101, "compression_ratio": 1.6510416666666667, "no_speech_prob": 6.909660442033783e-05}, {"id": 269, "seek": 158500, "start": 1585.0, "end": 1593.0, "text": " So I think the unique point of this is to have the graph aspects into the streaming world, yeah.", "tokens": [407, 286, 519, 264, 3845, 935, 295, 341, 307, 281, 362, 264, 4295, 7270, 666, 264, 11791, 1002, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 270, "seek": 158500, "start": 1593.0, "end": 1595.0, "text": " So any further questions?", "tokens": [407, 604, 3052, 1651, 30], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 271, "seek": 158500, "start": 1595.0, "end": 1596.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 272, "seek": 158500, "start": 1596.0, "end": 1598.0, "text": " So could the events are only additive, right?", "tokens": [407, 727, 264, 3931, 366, 787, 45558, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 273, "seek": 158500, "start": 1598.0, "end": 1603.0, "text": " So you can only add to the graph, but not delete from the graph by streaming it, right?", "tokens": [407, 291, 393, 787, 909, 281, 264, 4295, 11, 457, 406, 12097, 490, 264, 4295, 538, 11791, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 274, "seek": 158500, "start": 1603.0, "end": 1607.0, "text": " Yes, so at the moment everything is interpreted as an insult only,", "tokens": [1079, 11, 370, 412, 264, 1623, 1203, 307, 26749, 382, 364, 15285, 787, 11], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 275, "seek": 158500, "start": 1607.0, "end": 1613.0, "text": " but since Flink supports everything, like also updates and also deletions,", "tokens": [457, 1670, 3235, 475, 9346, 1203, 11, 411, 611, 9205, 293, 611, 1103, 302, 626, 11], "temperature": 0.0, "avg_logprob": -0.16689749218168712, "compression_ratio": 1.6833333333333333, "no_speech_prob": 5.2158768085064366e-05}, {"id": 276, "seek": 161300, "start": 1613.0, "end": 1618.0, "text": " it is thinkable about some future work that we also can support this.", "tokens": [309, 307, 519, 712, 466, 512, 2027, 589, 300, 321, 611, 393, 1406, 341, 13], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 277, "seek": 161300, "start": 1618.0, "end": 1623.0, "text": " Because at the end, the result of us is also, since we are using windowing,", "tokens": [1436, 412, 264, 917, 11, 264, 1874, 295, 505, 307, 611, 11, 1670, 321, 366, 1228, 4910, 278, 11], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 278, "seek": 161300, "start": 1623.0, "end": 1625.0, "text": " it's also an insult only stream at the end,", "tokens": [309, 311, 611, 364, 15285, 787, 4309, 412, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 279, "seek": 161300, "start": 1625.0, "end": 1630.0, "text": " but if we maybe think about to remove the windowing aspect,", "tokens": [457, 498, 321, 1310, 519, 466, 281, 4159, 264, 4910, 278, 4171, 11], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 280, "seek": 161300, "start": 1630.0, "end": 1634.0, "text": " so they have something like a continuous aggregation or whatever,", "tokens": [370, 436, 362, 746, 411, 257, 10957, 16743, 399, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 281, "seek": 161300, "start": 1634.0, "end": 1640.0, "text": " then we need to support like a continuous addition on the end", "tokens": [550, 321, 643, 281, 1406, 411, 257, 10957, 4500, 322, 264, 917], "temperature": 0.0, "avg_logprob": -0.10782996813456218, "compression_ratio": 1.7616822429906542, "no_speech_prob": 3.267652209615335e-05}, {"id": 282, "seek": 164000, "start": 1640.0, "end": 1646.0, "text": " to update already existing aggregating results.", "tokens": [281, 5623, 1217, 6741, 16743, 990, 3542, 13], "temperature": 0.0, "avg_logprob": -0.22352669455788352, "compression_ratio": 1.161904761904762, "no_speech_prob": 0.00018662128422874957}, {"id": 283, "seek": 164000, "start": 1646.0, "end": 1649.0, "text": " Okay, any further questions?", "tokens": [1033, 11, 604, 3052, 1651, 30], "temperature": 0.0, "avg_logprob": -0.22352669455788352, "compression_ratio": 1.161904761904762, "no_speech_prob": 0.00018662128422874957}, {"id": 284, "seek": 164900, "start": 1649.0, "end": 1671.0, "text": " Thank you again. Thanks.", "tokens": [50364, 1044, 291, 797, 13, 2561, 13, 51464], "temperature": 0.0, "avg_logprob": -0.4727109803093804, "compression_ratio": 0.8, "no_speech_prob": 0.001061654998920858}], "language": "en"}