{"text": " Okay. Okay, now we have Linus de Meijer with shorter feedback loops with the live book. Give it up. All right. Thank you. Can everybody hear me? Yeah. To my surprise, I am the first and only Belgian here presenting. Well, I can welcome you all here. Maybe the first question I want to ask, which is the most important one, who is hungry right now? All right. Sorry, I cannot help with that. But another question I would like to ask is, who has heard of Live Book? Who knows what it is, more or less? Yeah. Okay. It's a lot of people who has worked with Live Book professionally or has just, has it on their computer? Okay. Less people, but there are some. If you want to follow along and if you already installed Live Book, then you can go to my GitHub repository. I have a little notebook prepared. And I try to switch back and forth between this presentation and the Live Book. All right. The goals of today would be to introduce you to Live Book, to make sure you all understand what it is, how you can get it, how you install it, the various options. And then I think the most interesting part is how I used it in three different cases and what I learned from using it in a real project. And underneath all this, I hope I can bring across the message that Live Book really helps to start somewhere in the middle. So you don't spend time like scaffolding an application. And then just after a few days or hours, gets to the most interesting part. So Live Book enables you to start in the middle. That's my main message here. Whenever you start Live Book, you're greeted with like an introduction page. At the top you see your folder structure. There is a very nice learning section. And at the bottom there you have your sessions. So you will import a Live Book often and then a session will appear and you can hook into that. So you can, that's actually what we are going to do here. I'm going to go to my notebook that I just prepared here. And yeah, I just wanted to point out if you are just starting with Live Book, there is a very good learning section. So please go through these. Also if you're learning Elixir, it's a very good way to familiarize yourself with Elixir. And it also covers things like how you make pretty graphs or how you would use the Kino Library, which is the one that is used to actually interact with your Live Book. It's all just marked down. So it uses those, yeah, you can see here, it uses these code fences with the Elixir annotation. So it's very easy to check into your GitHub repository and make sure you can review it if you want. And GitHub also recently added the feature that it nicely formats your Live Books. They have the extension Live MD. So it integrates nicely with your version control system. The basics. We have code cells which can be executed. They contain your codes. And the first one is a little bit special in the sense that it often contains your setup. So you can pull in all your dependencies. So you can use the mix install function. And I'm going to use a few here, not too many. But I'll go over these once they become relevant. So right here we have our first code cell. We can just execute it. It takes a while to just start up. But then we can go. It is being evaluated. You can see the green dots. So it's being evaluated. And you have all those nice features that you can expect from an IDE. So you can ask it to autocomplete. If you control space one more time, you get all the documentation for that function. So you get a lot of help editing your code here. The result is being print down below here. And you also have the ability to, like I did here actually, to put stuff. And that's also being printed underneath your code cell. So that's very nice. And yeah, maybe the most or a very important feature at least, it's that you can interleave your code blocks with just regular markdown. So it's a really nice way to do a little coding and then explain what you have done and then go on to the code again. Yes, a few words about reproducibility. It's very nice to have this notebook and to know what actually will happen if you execute all those code cells. If you start from the beginning, it's very, very clear. You go from top to bottom. But what if you are going to edit in the middle, make a change somewhere? Well, Lifehook has recovered. It analyzes all those bindings that are being made in those code cells. And it makes sure that if you change something, the relevant code cells underneath them are also going to be executed again. So that's actually the way you often build up states. You have a code cell that creates a binding and then in the next code cell you can reuse that or you can use that binding so you can build upon when you go through all the code cells. I can do a little demonstration how branching sections work. So the sections are actually shown very clearly here on the side. I have a few of them, but one has the little branch icon. So this is a branching section. And this is just to show how the execution model actually works. So right here, I demonstrate that you can use the bindings from before, so from the main flow of the notebook. And if I start an infinite loop here, this is just going to stay printing in this little frame here. You will see that if I execute a code cell below, it will be queued, but it will never run because the other code is just blocking that one. But if we carry on to the next session, we can see that all is well again. So this is still blocked, but this is the main threat of execution. So we are not blocked here anymore. And just to show that we cannot access the bindings from before, I just triggered this error because we cannot access this variable. Okay, this is a pretty picture that I stole from Josef Alem. I'm not going to go into detail, but I just want to point out that everything is based or is heavily using the airline distribution mechanisms that we all know from the airline OTP ecosystem. So we have a central application here. It's a live view application, actually, with a lot of JavaScript. And we can connect to it through WebSockets. That's all being handled for us. And it does not run the code actually on the live book application itself, but in normal mode, it will spawn a new node and run your code on this new node. So we call it a runtime. This runtime is not aware of anything live book related. It is just a plain node that can execute code and you get the results back. So that's what's going on underneath. There are a lot of ways you can get live book on your computer. Recently, well, I used to like e-script installation, but since it's tied to your Elixir installation, I now switch to using the desktop application, which is getting very good at this point. You also can run it in the cloud. You can have it as a Docker image. That's all being covered. The various ways to start, not very interesting, but I think what's more interesting is my story of how I used it to mitigate risks early on, some projects that I've been doing. Yes, I just want to sum up here the benefits that I see. So it allows you to start in the middle. If you're using live book, you can jump straight into your problem space. It increases transparency, and you can use it because you can use that markdown in between to document your process. So all your thoughts, you can put them in between all those code cells, and it's, I think, way better than those obscure scripts we sometimes write, and you can also very easily share this document. So that's actually something that we did. I got some tasks to do. A client was asking something. We were doing something with machine learning and artificial intelligence. We were not aware or we did not know whether we could do it. So I sat down, made his live hook, and then documented all the steps I did, and in the end, got a pretty graph out of it, so I could convince the client that we could do it, actually, with Elixir. Just a little bit of context. I work for a small company. We often switch in between projects. The company is named Zenjoy, and we are often working as a team of two. So documentation and collaboration is very important, and also the communication with the clients is very, very important. So in this first case, we were tasked to interoperate with or to call an undocumented legacy API. It was very low level. It was not as low level as tools explained to us, so it was not like we had to do the pattern matching on the bit level, but we had to use the GenTCP module straight from Erlang, but it was very nice to have this live book environment where we could just throw the commands at this server that we could somehow use and see what came back. So in this way, we were able to create a notebook that documented all the commands we could see or how it reacted, and you see some pattern matching going on here. You also see some magic variables. So this was given to us, so we could not change this, but at least we could document it, and this became a very long document to refer back to. So this is another demonstration I wanted to do. It's not because you're in the browser that you're constrained by any way. You can still use all the process magic and all the GenServers you like, and this is just a demonstration of how you would go around and spawn a TCP server. I'm using Thousand Island here. In reality, I was using the other one, the older one, Cowboy and Ranch. Yes, thank you. But now for this demonstration, I got to use Thousand Island, and it's super nice, so you can just define your handler. It's just going to echo back whatever we send to it, and here I started up. The only caveat or the thing you have to be aware of is that you can start your children, your processes, and your supervisor tree under the Kino code cell. So whenever you reevaluate here, you see that you can't see, but another process is started or a whole tree starts. So this is a nice interop with the Lightbook environment. And once we have those, we can even... Yeah. Yeah, I guess that the gods are not with me today, but at least what I wanted to show is that you can actually draw a pretty picture of the supervision tree right here. But still, I think the server got started, and so now I can net cut into my local host on this given port, and I can see how some stuff is being echoed back to me, so at least that works. All right. Back to the presentation. So it is a nice environment to stub out a server and set up a situation where you can then use your application to interact with this stubbed version of your API. All right. I'm going to show, or this is just an example of how you would integrate your Livebook with a regular mixed project. It's all just sitting next to each other. Oftentimes, I just make a folder where the notebook lives, and then your mixed project, whatever it is, it can be a Phoenix application. You can access it if you use the part way of referencing your dependencies. A few words about a typical lifecycle that I've observed, you often start to experiment in your Livebook. On good days, you add tests, and then you move all that code into the regular application or in the regular mixed project, and you reference it from there on in the way that I just described. So you promote reusable code, and that's often a way that worked very well for me. The second case that I want to discuss is how I set up or created concurrent ETL pipeline, which is a fancy word for just loading CSV files and then maybe transforming them and dumping them into Postgres. So I really got to learn a lot about how concurrent data processing actually happens. I got to play around with Flow, which is a very nice library which builds on top of GenState. No, not GenState, the other one. GenStage, that's the one. And you can still use all the power of processes that are available in Euler and Elixir. To demonstrate, I've prepared or I want to show how you can use ECTO and then this Flow library right within your like hook application. To start off, I create a repository, just like you would do in a Phoenix application. Don't worry if you do not recognize this. This is kind of standard stuff. You have to specify the adapter, and then you can emulate whatever mixed ECTO create would do. So you make sure that your storage is up. In my case, it was already up, so that's what it reports. And then you can even make your migrations like you would if you used ECTO together with a Phoenix application. In this case, I also made sure there was an item in the database so we can query it later on. So I have to make sure that this repository actually runs. And then I migrate, well, I do a rollback to make sure nothing is being left over, and then I migrate again. So we have the end situation that I can query right here. So you can see that our fresh entry is just inserted with this new timestamp. And then we can build upon this. This is another demonstration, very short. This is a definition of a flow. Also, don't worry if you do not recognize this. That's not the key here. I just want to show that you can use all those goodies, and you're not constrained in any way. Right here, we are just emitting a value every second, and we're going to wait for three seconds and then insert an item in the database again. So you get the logging, and if we query it again, so you might recognize the nice ecto query syntax. You see I've wrapped it in a data table, and you can now see the three new items appearing. So that's very nice. Here you see me playing around and actually visualizing this ETL pipeline where every color actually is another class of objects or is being inserted in another table. It went very quickly, but when making this presentation, I also saw there is some room for improvement. So not all cylinders are firing together, but at least it was fast enough for our purposes. Another case I want to share with you is that we used the live hook to actually connect to a live running instance. So remember, as I have shown in the beginning, it's all just Erlang distribution under the hood. So instead of using the regular setup where you do an LXR standalone setting, you can also do an attached node configuration. The only thing you have to know is your node's name or the short name and a cookie which you have to agree upon. And it's very good for doing one of the tasks. Maybe you don't have a UI for something yet, and you want to do it in a live book, then this is a nice way to actually have like a super admin interface, but be aware that this is still a live environment. So if you do this, make sure to put a big disclaimer on top of your notebook to remind you of the risks involved. All right. The last thing that I want to share or show is how you would do tests in live hook. Like I said, on a good day, you write tests. And we have seen some examples in the previous presentations, how you would do like a doc test where you attach some kind of a formatted test and it's expected output. Well, since a few versions, these tests are actually automatically run. So if you define a module, in this case Christmas, you see that the doc tests are failing. I think I can easily fix it by changing the expectation. And if you run it again, the doc tests are green again. But you can also do just your regular testing. The only thing you have to think about is that you have to disable your auto-running. But then again, you can do your testing and you have to make sure you call the run function on the X unit module. So there is no excuse not to test, actually. I want to end with reference to these two resources. There is an initiative by Dockyard Academy. It's an open source curriculum to learn Elixir. And they have used the notebooks or the live books to actually teach this to students. And the other thing you might have heard about in the Elixir news is the Project Bumblebee, which allows you to actually play around with these new neural networks like GPT2 and stable diffusion. And you can just do it locally. So it's a very nice way. It integrates very nice into your live book notebook. All right. That's it for me. Thank you very much. Thank you very much. Is there any question? Could you maybe compare and contrast live books with Jupyter notebooks? Yes. That's actually a reference. Sorry. So the question was how this relates to the Jupyter notebooks, which we might also know. I think it's very much inspired by it. So it's also a computational notebook. But I also see a lot of differences, although I do not know Jupyter notebooks very well. But I think, for example, like the dependencies in the first cell, I do not think there is such a system in the Jupyter notebooks. You would have to use like Comda or Anaconda to set up your dependency. So it's a little bit less integrated. But I cannot say more about differences. But you're very right. There is a strong inspiration there. Yes. Thank you. Any other question? Cool. Thanks for the talk. I wanted to ask, actually, whether there is an option as well for a live book being available as an UI within the IDE, so kind of connected closer to the development environment? No. Not that I know of. No. No. It runs in the browser, and that's where it lives. So you can install it as a standalone application, but it's still something that lives in the browser. But you're right in the sense that it is not a full-blown IDE, and that's also one of the nuisances that I have noticed is that if you have very large code cells, for example, you are missing some features. And if you're used to VI bindings, for example, you will not find them there. Yeah. Cool. Thanks. Yes. Okay. Last question. Does this work for multiple users collaborating on things? Yes. And I should have shown this. It is one of the nicest features. Thank you for opening that door. If you're using multiple sessions or, for example, multiple users in multiple locations, you, for example, see the selections they made. You see a little cursor where they are editing or you are editing, and you're actually editing the same notebook. So, yes, it's kind of a live coding environment. Yes. I don't know. No. It's building on top of these goodies we have. Yeah. Okay. Thank you again.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.48, "text": " Okay. Okay, now we have Linus de Meijer with shorter feedback loops with the live book.", "tokens": [1033, 13, 1033, 11, 586, 321, 362, 9355, 301, 368, 1923, 1718, 260, 365, 11639, 5824, 16121, 365, 264, 1621, 1446, 13], "temperature": 0.0, "avg_logprob": -0.3768913745880127, "compression_ratio": 1.2416107382550337, "no_speech_prob": 0.2754976749420166}, {"id": 1, "seek": 0, "start": 14.48, "end": 15.48, "text": " Give it up.", "tokens": [5303, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.3768913745880127, "compression_ratio": 1.2416107382550337, "no_speech_prob": 0.2754976749420166}, {"id": 2, "seek": 0, "start": 15.48, "end": 27.12, "text": " All right. Thank you. Can everybody hear me? Yeah. To my surprise, I am the first and", "tokens": [1057, 558, 13, 1044, 291, 13, 1664, 2201, 1568, 385, 30, 865, 13, 1407, 452, 6365, 11, 286, 669, 264, 700, 293], "temperature": 0.0, "avg_logprob": -0.3768913745880127, "compression_ratio": 1.2416107382550337, "no_speech_prob": 0.2754976749420166}, {"id": 3, "seek": 2712, "start": 27.12, "end": 36.6, "text": " only Belgian here presenting. Well, I can welcome you all here. Maybe the first question", "tokens": [787, 47127, 510, 15578, 13, 1042, 11, 286, 393, 2928, 291, 439, 510, 13, 2704, 264, 700, 1168], "temperature": 0.0, "avg_logprob": -0.1513380868094308, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00029721696046181023}, {"id": 4, "seek": 2712, "start": 36.6, "end": 44.28, "text": " I want to ask, which is the most important one, who is hungry right now? All right. Sorry,", "tokens": [286, 528, 281, 1029, 11, 597, 307, 264, 881, 1021, 472, 11, 567, 307, 8067, 558, 586, 30, 1057, 558, 13, 4919, 11], "temperature": 0.0, "avg_logprob": -0.1513380868094308, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00029721696046181023}, {"id": 5, "seek": 2712, "start": 44.28, "end": 52.32, "text": " I cannot help with that. But another question I would like to ask is, who has heard of Live", "tokens": [286, 2644, 854, 365, 300, 13, 583, 1071, 1168, 286, 576, 411, 281, 1029, 307, 11, 567, 575, 2198, 295, 10385], "temperature": 0.0, "avg_logprob": -0.1513380868094308, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00029721696046181023}, {"id": 6, "seek": 5232, "start": 52.32, "end": 58.16, "text": " Book? Who knows what it is, more or less? Yeah. Okay. It's a lot of people who has worked", "tokens": [9476, 30, 2102, 3255, 437, 309, 307, 11, 544, 420, 1570, 30, 865, 13, 1033, 13, 467, 311, 257, 688, 295, 561, 567, 575, 2732], "temperature": 0.0, "avg_logprob": -0.19592865653659985, "compression_ratio": 1.5, "no_speech_prob": 0.00012583998613990843}, {"id": 7, "seek": 5232, "start": 58.16, "end": 63.92, "text": " with Live Book professionally or has just, has it on their computer? Okay. Less people,", "tokens": [365, 10385, 9476, 27941, 420, 575, 445, 11, 575, 309, 322, 641, 3820, 30, 1033, 13, 18649, 561, 11], "temperature": 0.0, "avg_logprob": -0.19592865653659985, "compression_ratio": 1.5, "no_speech_prob": 0.00012583998613990843}, {"id": 8, "seek": 5232, "start": 63.92, "end": 70.03999999999999, "text": " but there are some. If you want to follow along and if you already installed Live Book,", "tokens": [457, 456, 366, 512, 13, 759, 291, 528, 281, 1524, 2051, 293, 498, 291, 1217, 8899, 10385, 9476, 11], "temperature": 0.0, "avg_logprob": -0.19592865653659985, "compression_ratio": 1.5, "no_speech_prob": 0.00012583998613990843}, {"id": 9, "seek": 5232, "start": 70.03999999999999, "end": 76.68, "text": " then you can go to my GitHub repository. I have a little notebook prepared. And I try", "tokens": [550, 291, 393, 352, 281, 452, 23331, 25841, 13, 286, 362, 257, 707, 21060, 4927, 13, 400, 286, 853], "temperature": 0.0, "avg_logprob": -0.19592865653659985, "compression_ratio": 1.5, "no_speech_prob": 0.00012583998613990843}, {"id": 10, "seek": 7668, "start": 76.68, "end": 84.48, "text": " to switch back and forth between this presentation and the Live Book. All right. The goals of", "tokens": [281, 3679, 646, 293, 5220, 1296, 341, 5860, 293, 264, 10385, 9476, 13, 1057, 558, 13, 440, 5493, 295], "temperature": 0.0, "avg_logprob": -0.11406514836453843, "compression_ratio": 1.609865470852018, "no_speech_prob": 5.958013207418844e-05}, {"id": 11, "seek": 7668, "start": 84.48, "end": 93.2, "text": " today would be to introduce you to Live Book, to make sure you all understand what it is,", "tokens": [965, 576, 312, 281, 5366, 291, 281, 10385, 9476, 11, 281, 652, 988, 291, 439, 1223, 437, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.11406514836453843, "compression_ratio": 1.609865470852018, "no_speech_prob": 5.958013207418844e-05}, {"id": 12, "seek": 7668, "start": 93.2, "end": 97.88000000000001, "text": " how you can get it, how you install it, the various options. And then I think the most", "tokens": [577, 291, 393, 483, 309, 11, 577, 291, 3625, 309, 11, 264, 3683, 3956, 13, 400, 550, 286, 519, 264, 881], "temperature": 0.0, "avg_logprob": -0.11406514836453843, "compression_ratio": 1.609865470852018, "no_speech_prob": 5.958013207418844e-05}, {"id": 13, "seek": 7668, "start": 97.88000000000001, "end": 104.56, "text": " interesting part is how I used it in three different cases and what I learned from using", "tokens": [1880, 644, 307, 577, 286, 1143, 309, 294, 1045, 819, 3331, 293, 437, 286, 3264, 490, 1228], "temperature": 0.0, "avg_logprob": -0.11406514836453843, "compression_ratio": 1.609865470852018, "no_speech_prob": 5.958013207418844e-05}, {"id": 14, "seek": 10456, "start": 104.56, "end": 113.24000000000001, "text": " it in a real project. And underneath all this, I hope I can bring across the message that", "tokens": [309, 294, 257, 957, 1716, 13, 400, 7223, 439, 341, 11, 286, 1454, 286, 393, 1565, 2108, 264, 3636, 300], "temperature": 0.0, "avg_logprob": -0.1735596143282377, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00018704791727941483}, {"id": 15, "seek": 10456, "start": 113.24000000000001, "end": 122.16, "text": " Live Book really helps to start somewhere in the middle. So you don't spend time like", "tokens": [10385, 9476, 534, 3665, 281, 722, 4079, 294, 264, 2808, 13, 407, 291, 500, 380, 3496, 565, 411], "temperature": 0.0, "avg_logprob": -0.1735596143282377, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00018704791727941483}, {"id": 16, "seek": 10456, "start": 122.16, "end": 129.6, "text": " scaffolding an application. And then just after a few days or hours, gets to the most", "tokens": [44094, 278, 364, 3861, 13, 400, 550, 445, 934, 257, 1326, 1708, 420, 2496, 11, 2170, 281, 264, 881], "temperature": 0.0, "avg_logprob": -0.1735596143282377, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00018704791727941483}, {"id": 17, "seek": 12960, "start": 129.6, "end": 135.4, "text": " interesting part. So Live Book enables you to start in the middle. That's my main message", "tokens": [1880, 644, 13, 407, 10385, 9476, 17077, 291, 281, 722, 294, 264, 2808, 13, 663, 311, 452, 2135, 3636], "temperature": 0.0, "avg_logprob": -0.13447600603103638, "compression_ratio": 1.4831460674157304, "no_speech_prob": 3.942035618820228e-05}, {"id": 18, "seek": 12960, "start": 135.4, "end": 145.32, "text": " here. Whenever you start Live Book, you're greeted with like an introduction page. At", "tokens": [510, 13, 14159, 291, 722, 10385, 9476, 11, 291, 434, 38441, 365, 411, 364, 9339, 3028, 13, 1711], "temperature": 0.0, "avg_logprob": -0.13447600603103638, "compression_ratio": 1.4831460674157304, "no_speech_prob": 3.942035618820228e-05}, {"id": 19, "seek": 12960, "start": 145.32, "end": 152.48, "text": " the top you see your folder structure. There is a very nice learning section. And at the", "tokens": [264, 1192, 291, 536, 428, 10820, 3877, 13, 821, 307, 257, 588, 1481, 2539, 3541, 13, 400, 412, 264], "temperature": 0.0, "avg_logprob": -0.13447600603103638, "compression_ratio": 1.4831460674157304, "no_speech_prob": 3.942035618820228e-05}, {"id": 20, "seek": 15248, "start": 152.48, "end": 162.16, "text": " bottom there you have your sessions. So you will import a Live Book often and then a session", "tokens": [2767, 456, 291, 362, 428, 11081, 13, 407, 291, 486, 974, 257, 10385, 9476, 2049, 293, 550, 257, 5481], "temperature": 0.0, "avg_logprob": -0.13560175214494977, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.467182225198485e-05}, {"id": 21, "seek": 15248, "start": 162.16, "end": 167.48, "text": " will appear and you can hook into that. So you can, that's actually what we are going", "tokens": [486, 4204, 293, 291, 393, 6328, 666, 300, 13, 407, 291, 393, 11, 300, 311, 767, 437, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.13560175214494977, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.467182225198485e-05}, {"id": 22, "seek": 15248, "start": 167.48, "end": 180.6, "text": " to do here. I'm going to go to my notebook that I just prepared here. And yeah, I just", "tokens": [281, 360, 510, 13, 286, 478, 516, 281, 352, 281, 452, 21060, 300, 286, 445, 4927, 510, 13, 400, 1338, 11, 286, 445], "temperature": 0.0, "avg_logprob": -0.13560175214494977, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.467182225198485e-05}, {"id": 23, "seek": 18060, "start": 180.6, "end": 184.35999999999999, "text": " wanted to point out if you are just starting with Live Book, there is a very good learning", "tokens": [1415, 281, 935, 484, 498, 291, 366, 445, 2891, 365, 10385, 9476, 11, 456, 307, 257, 588, 665, 2539], "temperature": 0.0, "avg_logprob": -0.14688553280300565, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.0951480362564325e-05}, {"id": 24, "seek": 18060, "start": 184.35999999999999, "end": 190.16, "text": " section. So please go through these. Also if you're learning Elixir, it's a very good", "tokens": [3541, 13, 407, 1767, 352, 807, 613, 13, 2743, 498, 291, 434, 2539, 2699, 970, 347, 11, 309, 311, 257, 588, 665], "temperature": 0.0, "avg_logprob": -0.14688553280300565, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.0951480362564325e-05}, {"id": 25, "seek": 18060, "start": 190.16, "end": 197.51999999999998, "text": " way to familiarize yourself with Elixir. And it also covers things like how you make", "tokens": [636, 281, 4963, 1125, 1803, 365, 2699, 970, 347, 13, 400, 309, 611, 10538, 721, 411, 577, 291, 652], "temperature": 0.0, "avg_logprob": -0.14688553280300565, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.0951480362564325e-05}, {"id": 26, "seek": 18060, "start": 197.51999999999998, "end": 204.32, "text": " pretty graphs or how you would use the Kino Library, which is the one that is used to", "tokens": [1238, 24877, 420, 577, 291, 576, 764, 264, 591, 2982, 12806, 11, 597, 307, 264, 472, 300, 307, 1143, 281], "temperature": 0.0, "avg_logprob": -0.14688553280300565, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.0951480362564325e-05}, {"id": 27, "seek": 20432, "start": 204.32, "end": 216.28, "text": " actually interact with your Live Book. It's all just marked down. So it uses those, yeah,", "tokens": [767, 4648, 365, 428, 10385, 9476, 13, 467, 311, 439, 445, 12658, 760, 13, 407, 309, 4960, 729, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.12499031339372907, "compression_ratio": 1.4945054945054945, "no_speech_prob": 2.9752736736554652e-05}, {"id": 28, "seek": 20432, "start": 216.28, "end": 224.84, "text": " you can see here, it uses these code fences with the Elixir annotation. So it's very easy", "tokens": [291, 393, 536, 510, 11, 309, 4960, 613, 3089, 45796, 365, 264, 2699, 970, 347, 48654, 13, 407, 309, 311, 588, 1858], "temperature": 0.0, "avg_logprob": -0.12499031339372907, "compression_ratio": 1.4945054945054945, "no_speech_prob": 2.9752736736554652e-05}, {"id": 29, "seek": 20432, "start": 224.84, "end": 233.72, "text": " to check into your GitHub repository and make sure you can review it if you want. And GitHub", "tokens": [281, 1520, 666, 428, 23331, 25841, 293, 652, 988, 291, 393, 3131, 309, 498, 291, 528, 13, 400, 23331], "temperature": 0.0, "avg_logprob": -0.12499031339372907, "compression_ratio": 1.4945054945054945, "no_speech_prob": 2.9752736736554652e-05}, {"id": 30, "seek": 23372, "start": 233.72, "end": 242.8, "text": " also recently added the feature that it nicely formats your Live Books. They have the extension", "tokens": [611, 3938, 3869, 264, 4111, 300, 309, 9594, 25879, 428, 10385, 33843, 13, 814, 362, 264, 10320], "temperature": 0.0, "avg_logprob": -0.13228310479058158, "compression_ratio": 1.5224719101123596, "no_speech_prob": 3.5291810490889475e-05}, {"id": 31, "seek": 23372, "start": 242.8, "end": 254.48, "text": " Live MD. So it integrates nicely with your version control system. The basics. We have", "tokens": [10385, 22521, 13, 407, 309, 3572, 1024, 9594, 365, 428, 3037, 1969, 1185, 13, 440, 14688, 13, 492, 362], "temperature": 0.0, "avg_logprob": -0.13228310479058158, "compression_ratio": 1.5224719101123596, "no_speech_prob": 3.5291810490889475e-05}, {"id": 32, "seek": 23372, "start": 254.48, "end": 260.4, "text": " code cells which can be executed. They contain your codes. And the first one is a little", "tokens": [3089, 5438, 597, 393, 312, 17577, 13, 814, 5304, 428, 14211, 13, 400, 264, 700, 472, 307, 257, 707], "temperature": 0.0, "avg_logprob": -0.13228310479058158, "compression_ratio": 1.5224719101123596, "no_speech_prob": 3.5291810490889475e-05}, {"id": 33, "seek": 26040, "start": 260.4, "end": 268.71999999999997, "text": " bit special in the sense that it often contains your setup. So you can pull in all your dependencies.", "tokens": [857, 2121, 294, 264, 2020, 300, 309, 2049, 8306, 428, 8657, 13, 407, 291, 393, 2235, 294, 439, 428, 36606, 13], "temperature": 0.0, "avg_logprob": -0.12723441026648696, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.643072027945891e-05}, {"id": 34, "seek": 26040, "start": 268.71999999999997, "end": 276.15999999999997, "text": " So you can use the mix install function. And I'm going to use a few here, not too many.", "tokens": [407, 291, 393, 764, 264, 2890, 3625, 2445, 13, 400, 286, 478, 516, 281, 764, 257, 1326, 510, 11, 406, 886, 867, 13], "temperature": 0.0, "avg_logprob": -0.12723441026648696, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.643072027945891e-05}, {"id": 35, "seek": 26040, "start": 276.15999999999997, "end": 283.32, "text": " But I'll go over these once they become relevant. So right here we have our first code cell.", "tokens": [583, 286, 603, 352, 670, 613, 1564, 436, 1813, 7340, 13, 407, 558, 510, 321, 362, 527, 700, 3089, 2815, 13], "temperature": 0.0, "avg_logprob": -0.12723441026648696, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.643072027945891e-05}, {"id": 36, "seek": 26040, "start": 283.32, "end": 289.35999999999996, "text": " We can just execute it. It takes a while to just start up. But then we can go. It is", "tokens": [492, 393, 445, 14483, 309, 13, 467, 2516, 257, 1339, 281, 445, 722, 493, 13, 583, 550, 321, 393, 352, 13, 467, 307], "temperature": 0.0, "avg_logprob": -0.12723441026648696, "compression_ratio": 1.6026200873362446, "no_speech_prob": 1.643072027945891e-05}, {"id": 37, "seek": 28936, "start": 289.36, "end": 294.76, "text": " being evaluated. You can see the green dots. So it's being evaluated. And you have all", "tokens": [885, 25509, 13, 509, 393, 536, 264, 3092, 15026, 13, 407, 309, 311, 885, 25509, 13, 400, 291, 362, 439], "temperature": 0.0, "avg_logprob": -0.12526523533152112, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.393287185346708e-05}, {"id": 38, "seek": 28936, "start": 294.76, "end": 302.36, "text": " those nice features that you can expect from an IDE. So you can ask it to autocomplete.", "tokens": [729, 1481, 4122, 300, 291, 393, 2066, 490, 364, 40930, 13, 407, 291, 393, 1029, 309, 281, 45833, 298, 17220, 13], "temperature": 0.0, "avg_logprob": -0.12526523533152112, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.393287185346708e-05}, {"id": 39, "seek": 28936, "start": 302.36, "end": 312.0, "text": " If you control space one more time, you get all the documentation for that function. So", "tokens": [759, 291, 1969, 1901, 472, 544, 565, 11, 291, 483, 439, 264, 14333, 337, 300, 2445, 13, 407], "temperature": 0.0, "avg_logprob": -0.12526523533152112, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.393287185346708e-05}, {"id": 40, "seek": 31200, "start": 312.0, "end": 321.12, "text": " you get a lot of help editing your code here. The result is being print down below here.", "tokens": [291, 483, 257, 688, 295, 854, 10000, 428, 3089, 510, 13, 440, 1874, 307, 885, 4482, 760, 2507, 510, 13], "temperature": 0.0, "avg_logprob": -0.13599461775559646, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4040849237062503e-05}, {"id": 41, "seek": 31200, "start": 321.12, "end": 326.44, "text": " And you also have the ability to, like I did here actually, to put stuff. And that's also", "tokens": [400, 291, 611, 362, 264, 3485, 281, 11, 411, 286, 630, 510, 767, 11, 281, 829, 1507, 13, 400, 300, 311, 611], "temperature": 0.0, "avg_logprob": -0.13599461775559646, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4040849237062503e-05}, {"id": 42, "seek": 31200, "start": 326.44, "end": 332.48, "text": " being printed underneath your code cell. So that's very nice. And yeah, maybe the most", "tokens": [885, 13567, 7223, 428, 3089, 2815, 13, 407, 300, 311, 588, 1481, 13, 400, 1338, 11, 1310, 264, 881], "temperature": 0.0, "avg_logprob": -0.13599461775559646, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4040849237062503e-05}, {"id": 43, "seek": 31200, "start": 332.48, "end": 339.52, "text": " or a very important feature at least, it's that you can interleave your code blocks with", "tokens": [420, 257, 588, 1021, 4111, 412, 1935, 11, 309, 311, 300, 291, 393, 728, 306, 946, 428, 3089, 8474, 365], "temperature": 0.0, "avg_logprob": -0.13599461775559646, "compression_ratio": 1.6465116279069767, "no_speech_prob": 1.4040849237062503e-05}, {"id": 44, "seek": 33952, "start": 339.52, "end": 345.28, "text": " just regular markdown. So it's a really nice way to do a little coding and then explain", "tokens": [445, 3890, 1491, 5093, 13, 407, 309, 311, 257, 534, 1481, 636, 281, 360, 257, 707, 17720, 293, 550, 2903], "temperature": 0.0, "avg_logprob": -0.09862838966259058, "compression_ratio": 1.4754098360655739, "no_speech_prob": 1.4506010302284267e-05}, {"id": 45, "seek": 33952, "start": 345.28, "end": 359.64, "text": " what you have done and then go on to the code again. Yes, a few words about reproducibility.", "tokens": [437, 291, 362, 1096, 293, 550, 352, 322, 281, 264, 3089, 797, 13, 1079, 11, 257, 1326, 2283, 466, 11408, 537, 39802, 13], "temperature": 0.0, "avg_logprob": -0.09862838966259058, "compression_ratio": 1.4754098360655739, "no_speech_prob": 1.4506010302284267e-05}, {"id": 46, "seek": 33952, "start": 359.64, "end": 367.4, "text": " It's very nice to have this notebook and to know what actually will happen if you execute", "tokens": [467, 311, 588, 1481, 281, 362, 341, 21060, 293, 281, 458, 437, 767, 486, 1051, 498, 291, 14483], "temperature": 0.0, "avg_logprob": -0.09862838966259058, "compression_ratio": 1.4754098360655739, "no_speech_prob": 1.4506010302284267e-05}, {"id": 47, "seek": 36740, "start": 367.4, "end": 372.47999999999996, "text": " all those code cells. If you start from the beginning, it's very, very clear. You go from", "tokens": [439, 729, 3089, 5438, 13, 759, 291, 722, 490, 264, 2863, 11, 309, 311, 588, 11, 588, 1850, 13, 509, 352, 490], "temperature": 0.0, "avg_logprob": -0.144150312956389, "compression_ratio": 1.6934865900383143, "no_speech_prob": 4.260479909135029e-05}, {"id": 48, "seek": 36740, "start": 372.47999999999996, "end": 377.52, "text": " top to bottom. But what if you are going to edit in the middle, make a change somewhere?", "tokens": [1192, 281, 2767, 13, 583, 437, 498, 291, 366, 516, 281, 8129, 294, 264, 2808, 11, 652, 257, 1319, 4079, 30], "temperature": 0.0, "avg_logprob": -0.144150312956389, "compression_ratio": 1.6934865900383143, "no_speech_prob": 4.260479909135029e-05}, {"id": 49, "seek": 36740, "start": 377.52, "end": 383.79999999999995, "text": " Well, Lifehook has recovered. It analyzes all those bindings that are being made in", "tokens": [1042, 11, 7720, 71, 1212, 575, 19542, 13, 467, 6459, 12214, 439, 729, 14786, 1109, 300, 366, 885, 1027, 294], "temperature": 0.0, "avg_logprob": -0.144150312956389, "compression_ratio": 1.6934865900383143, "no_speech_prob": 4.260479909135029e-05}, {"id": 50, "seek": 36740, "start": 383.79999999999995, "end": 388.96, "text": " those code cells. And it makes sure that if you change something, the relevant code cells", "tokens": [729, 3089, 5438, 13, 400, 309, 1669, 988, 300, 498, 291, 1319, 746, 11, 264, 7340, 3089, 5438], "temperature": 0.0, "avg_logprob": -0.144150312956389, "compression_ratio": 1.6934865900383143, "no_speech_prob": 4.260479909135029e-05}, {"id": 51, "seek": 36740, "start": 388.96, "end": 396.03999999999996, "text": " underneath them are also going to be executed again. So that's actually the way you often", "tokens": [7223, 552, 366, 611, 516, 281, 312, 17577, 797, 13, 407, 300, 311, 767, 264, 636, 291, 2049], "temperature": 0.0, "avg_logprob": -0.144150312956389, "compression_ratio": 1.6934865900383143, "no_speech_prob": 4.260479909135029e-05}, {"id": 52, "seek": 39604, "start": 396.04, "end": 402.36, "text": " build up states. You have a code cell that creates a binding and then in the next code", "tokens": [1322, 493, 4368, 13, 509, 362, 257, 3089, 2815, 300, 7829, 257, 17359, 293, 550, 294, 264, 958, 3089], "temperature": 0.0, "avg_logprob": -0.14880892874180585, "compression_ratio": 1.765, "no_speech_prob": 2.7959205908700824e-05}, {"id": 53, "seek": 39604, "start": 402.36, "end": 409.0, "text": " cell you can reuse that or you can use that binding so you can build upon when you go", "tokens": [2815, 291, 393, 26225, 300, 420, 291, 393, 764, 300, 17359, 370, 291, 393, 1322, 3564, 562, 291, 352], "temperature": 0.0, "avg_logprob": -0.14880892874180585, "compression_ratio": 1.765, "no_speech_prob": 2.7959205908700824e-05}, {"id": 54, "seek": 39604, "start": 409.0, "end": 419.24, "text": " through all the code cells. I can do a little demonstration how branching sections work.", "tokens": [807, 439, 264, 3089, 5438, 13, 286, 393, 360, 257, 707, 16520, 577, 9819, 278, 10863, 589, 13], "temperature": 0.0, "avg_logprob": -0.14880892874180585, "compression_ratio": 1.765, "no_speech_prob": 2.7959205908700824e-05}, {"id": 55, "seek": 39604, "start": 419.24, "end": 425.72, "text": " So the sections are actually shown very clearly here on the side. I have a few of them, but", "tokens": [407, 264, 10863, 366, 767, 4898, 588, 4448, 510, 322, 264, 1252, 13, 286, 362, 257, 1326, 295, 552, 11, 457], "temperature": 0.0, "avg_logprob": -0.14880892874180585, "compression_ratio": 1.765, "no_speech_prob": 2.7959205908700824e-05}, {"id": 56, "seek": 42572, "start": 425.72, "end": 434.48, "text": " one has the little branch icon. So this is a branching section. And this is just to show", "tokens": [472, 575, 264, 707, 9819, 6528, 13, 407, 341, 307, 257, 9819, 278, 3541, 13, 400, 341, 307, 445, 281, 855], "temperature": 0.0, "avg_logprob": -0.12944900455759534, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.582654997240752e-05}, {"id": 57, "seek": 42572, "start": 434.48, "end": 442.04, "text": " how the execution model actually works. So right here, I demonstrate that you can use", "tokens": [577, 264, 15058, 2316, 767, 1985, 13, 407, 558, 510, 11, 286, 11698, 300, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.12944900455759534, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.582654997240752e-05}, {"id": 58, "seek": 42572, "start": 442.04, "end": 452.16, "text": " the bindings from before, so from the main flow of the notebook. And if I start an infinite", "tokens": [264, 14786, 1109, 490, 949, 11, 370, 490, 264, 2135, 3095, 295, 264, 21060, 13, 400, 498, 286, 722, 364, 13785], "temperature": 0.0, "avg_logprob": -0.12944900455759534, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.582654997240752e-05}, {"id": 59, "seek": 45216, "start": 452.16, "end": 459.84000000000003, "text": " loop here, this is just going to stay printing in this little frame here. You will see that", "tokens": [6367, 510, 11, 341, 307, 445, 516, 281, 1754, 14699, 294, 341, 707, 3920, 510, 13, 509, 486, 536, 300], "temperature": 0.0, "avg_logprob": -0.10617587386920888, "compression_ratio": 1.6919431279620853, "no_speech_prob": 5.730927659897134e-05}, {"id": 60, "seek": 45216, "start": 459.84000000000003, "end": 464.40000000000003, "text": " if I execute a code cell below, it will be queued, but it will never run because the", "tokens": [498, 286, 14483, 257, 3089, 2815, 2507, 11, 309, 486, 312, 631, 5827, 11, 457, 309, 486, 1128, 1190, 570, 264], "temperature": 0.0, "avg_logprob": -0.10617587386920888, "compression_ratio": 1.6919431279620853, "no_speech_prob": 5.730927659897134e-05}, {"id": 61, "seek": 45216, "start": 464.40000000000003, "end": 469.92, "text": " other code is just blocking that one. But if we carry on to the next session, we can", "tokens": [661, 3089, 307, 445, 17776, 300, 472, 13, 583, 498, 321, 3985, 322, 281, 264, 958, 5481, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.10617587386920888, "compression_ratio": 1.6919431279620853, "no_speech_prob": 5.730927659897134e-05}, {"id": 62, "seek": 45216, "start": 469.92, "end": 477.0, "text": " see that all is well again. So this is still blocked, but this is the main threat of execution.", "tokens": [536, 300, 439, 307, 731, 797, 13, 407, 341, 307, 920, 15470, 11, 457, 341, 307, 264, 2135, 4734, 295, 15058, 13], "temperature": 0.0, "avg_logprob": -0.10617587386920888, "compression_ratio": 1.6919431279620853, "no_speech_prob": 5.730927659897134e-05}, {"id": 63, "seek": 47700, "start": 477.0, "end": 485.0, "text": " So we are not blocked here anymore. And just to show that we cannot access the bindings", "tokens": [407, 321, 366, 406, 15470, 510, 3602, 13, 400, 445, 281, 855, 300, 321, 2644, 2105, 264, 14786, 1109], "temperature": 0.0, "avg_logprob": -0.14376098969403436, "compression_ratio": 1.4696132596685083, "no_speech_prob": 4.053756856592372e-05}, {"id": 64, "seek": 47700, "start": 485.0, "end": 496.44, "text": " from before, I just triggered this error because we cannot access this variable. Okay, this", "tokens": [490, 949, 11, 286, 445, 21710, 341, 6713, 570, 321, 2644, 2105, 341, 7006, 13, 1033, 11, 341], "temperature": 0.0, "avg_logprob": -0.14376098969403436, "compression_ratio": 1.4696132596685083, "no_speech_prob": 4.053756856592372e-05}, {"id": 65, "seek": 47700, "start": 496.44, "end": 503.64, "text": " is a pretty picture that I stole from Josef Alem. I'm not going to go into detail, but", "tokens": [307, 257, 1238, 3036, 300, 286, 16326, 490, 8635, 69, 9366, 76, 13, 286, 478, 406, 516, 281, 352, 666, 2607, 11, 457], "temperature": 0.0, "avg_logprob": -0.14376098969403436, "compression_ratio": 1.4696132596685083, "no_speech_prob": 4.053756856592372e-05}, {"id": 66, "seek": 50364, "start": 503.64, "end": 508.64, "text": " I just want to point out that everything is based or is heavily using the airline distribution", "tokens": [286, 445, 528, 281, 935, 484, 300, 1203, 307, 2361, 420, 307, 10950, 1228, 264, 29528, 7316], "temperature": 0.0, "avg_logprob": -0.13289453742209445, "compression_ratio": 1.5313807531380754, "no_speech_prob": 3.4760840208036825e-05}, {"id": 67, "seek": 50364, "start": 508.64, "end": 516.52, "text": " mechanisms that we all know from the airline OTP ecosystem. So we have a central application", "tokens": [15902, 300, 321, 439, 458, 490, 264, 29528, 422, 16804, 11311, 13, 407, 321, 362, 257, 5777, 3861], "temperature": 0.0, "avg_logprob": -0.13289453742209445, "compression_ratio": 1.5313807531380754, "no_speech_prob": 3.4760840208036825e-05}, {"id": 68, "seek": 50364, "start": 516.52, "end": 523.88, "text": " here. It's a live view application, actually, with a lot of JavaScript. And we can connect", "tokens": [510, 13, 467, 311, 257, 1621, 1910, 3861, 11, 767, 11, 365, 257, 688, 295, 15778, 13, 400, 321, 393, 1745], "temperature": 0.0, "avg_logprob": -0.13289453742209445, "compression_ratio": 1.5313807531380754, "no_speech_prob": 3.4760840208036825e-05}, {"id": 69, "seek": 50364, "start": 523.88, "end": 530.2, "text": " to it through WebSockets. That's all being handled for us. And it does not run the code", "tokens": [281, 309, 807, 9573, 50, 1560, 1385, 13, 663, 311, 439, 885, 18033, 337, 505, 13, 400, 309, 775, 406, 1190, 264, 3089], "temperature": 0.0, "avg_logprob": -0.13289453742209445, "compression_ratio": 1.5313807531380754, "no_speech_prob": 3.4760840208036825e-05}, {"id": 70, "seek": 53020, "start": 530.2, "end": 536.84, "text": " actually on the live book application itself, but in normal mode, it will spawn a new node", "tokens": [767, 322, 264, 1621, 1446, 3861, 2564, 11, 457, 294, 2710, 4391, 11, 309, 486, 17088, 257, 777, 9984], "temperature": 0.0, "avg_logprob": -0.1301579207516788, "compression_ratio": 1.627906976744186, "no_speech_prob": 4.124373663216829e-05}, {"id": 71, "seek": 53020, "start": 536.84, "end": 544.5600000000001, "text": " and run your code on this new node. So we call it a runtime. This runtime is not aware", "tokens": [293, 1190, 428, 3089, 322, 341, 777, 9984, 13, 407, 321, 818, 309, 257, 34474, 13, 639, 34474, 307, 406, 3650], "temperature": 0.0, "avg_logprob": -0.1301579207516788, "compression_ratio": 1.627906976744186, "no_speech_prob": 4.124373663216829e-05}, {"id": 72, "seek": 53020, "start": 544.5600000000001, "end": 550.2, "text": " of anything live book related. It is just a plain node that can execute code and you", "tokens": [295, 1340, 1621, 1446, 4077, 13, 467, 307, 445, 257, 11121, 9984, 300, 393, 14483, 3089, 293, 291], "temperature": 0.0, "avg_logprob": -0.1301579207516788, "compression_ratio": 1.627906976744186, "no_speech_prob": 4.124373663216829e-05}, {"id": 73, "seek": 53020, "start": 550.2, "end": 557.6400000000001, "text": " get the results back. So that's what's going on underneath. There are a lot of ways you", "tokens": [483, 264, 3542, 646, 13, 407, 300, 311, 437, 311, 516, 322, 7223, 13, 821, 366, 257, 688, 295, 2098, 291], "temperature": 0.0, "avg_logprob": -0.1301579207516788, "compression_ratio": 1.627906976744186, "no_speech_prob": 4.124373663216829e-05}, {"id": 74, "seek": 55764, "start": 557.64, "end": 566.16, "text": " can get live book on your computer. Recently, well, I used to like e-script installation,", "tokens": [393, 483, 1621, 1446, 322, 428, 3820, 13, 20072, 11, 731, 11, 286, 1143, 281, 411, 308, 12, 82, 5944, 13260, 11], "temperature": 0.0, "avg_logprob": -0.1647019589200933, "compression_ratio": 1.577092511013216, "no_speech_prob": 4.971983071300201e-05}, {"id": 75, "seek": 55764, "start": 566.16, "end": 572.1999999999999, "text": " but since it's tied to your Elixir installation, I now switch to using the desktop application,", "tokens": [457, 1670, 309, 311, 9601, 281, 428, 2699, 970, 347, 13260, 11, 286, 586, 3679, 281, 1228, 264, 14502, 3861, 11], "temperature": 0.0, "avg_logprob": -0.1647019589200933, "compression_ratio": 1.577092511013216, "no_speech_prob": 4.971983071300201e-05}, {"id": 76, "seek": 55764, "start": 572.1999999999999, "end": 578.4, "text": " which is getting very good at this point. You also can run it in the cloud. You can", "tokens": [597, 307, 1242, 588, 665, 412, 341, 935, 13, 509, 611, 393, 1190, 309, 294, 264, 4588, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.1647019589200933, "compression_ratio": 1.577092511013216, "no_speech_prob": 4.971983071300201e-05}, {"id": 77, "seek": 55764, "start": 578.4, "end": 583.88, "text": " have it as a Docker image. That's all being covered. The various ways to start, not very", "tokens": [362, 309, 382, 257, 33772, 3256, 13, 663, 311, 439, 885, 5343, 13, 440, 3683, 2098, 281, 722, 11, 406, 588], "temperature": 0.0, "avg_logprob": -0.1647019589200933, "compression_ratio": 1.577092511013216, "no_speech_prob": 4.971983071300201e-05}, {"id": 78, "seek": 58388, "start": 583.88, "end": 593.64, "text": " interesting, but I think what's more interesting is my story of how I used it to mitigate risks", "tokens": [1880, 11, 457, 286, 519, 437, 311, 544, 1880, 307, 452, 1657, 295, 577, 286, 1143, 309, 281, 27336, 10888], "temperature": 0.0, "avg_logprob": -0.09686586301620692, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.019558425876312e-05}, {"id": 79, "seek": 58388, "start": 593.64, "end": 602.56, "text": " early on, some projects that I've been doing. Yes, I just want to sum up here the benefits", "tokens": [2440, 322, 11, 512, 4455, 300, 286, 600, 668, 884, 13, 1079, 11, 286, 445, 528, 281, 2408, 493, 510, 264, 5311], "temperature": 0.0, "avg_logprob": -0.09686586301620692, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.019558425876312e-05}, {"id": 80, "seek": 58388, "start": 602.56, "end": 610.8, "text": " that I see. So it allows you to start in the middle. If you're using live book, you can", "tokens": [300, 286, 536, 13, 407, 309, 4045, 291, 281, 722, 294, 264, 2808, 13, 759, 291, 434, 1228, 1621, 1446, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.09686586301620692, "compression_ratio": 1.5138121546961325, "no_speech_prob": 3.019558425876312e-05}, {"id": 81, "seek": 61080, "start": 610.8, "end": 618.88, "text": " jump straight into your problem space. It increases transparency, and you can use it", "tokens": [3012, 2997, 666, 428, 1154, 1901, 13, 467, 8637, 17131, 11, 293, 291, 393, 764, 309], "temperature": 0.0, "avg_logprob": -0.09830891942403402, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.751030115177855e-05}, {"id": 82, "seek": 61080, "start": 618.88, "end": 625.0, "text": " because you can use that markdown in between to document your process. So all your thoughts,", "tokens": [570, 291, 393, 764, 300, 1491, 5093, 294, 1296, 281, 4166, 428, 1399, 13, 407, 439, 428, 4598, 11], "temperature": 0.0, "avg_logprob": -0.09830891942403402, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.751030115177855e-05}, {"id": 83, "seek": 61080, "start": 625.0, "end": 630.52, "text": " you can put them in between all those code cells, and it's, I think, way better than", "tokens": [291, 393, 829, 552, 294, 1296, 439, 729, 3089, 5438, 11, 293, 309, 311, 11, 286, 519, 11, 636, 1101, 813], "temperature": 0.0, "avg_logprob": -0.09830891942403402, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.751030115177855e-05}, {"id": 84, "seek": 61080, "start": 630.52, "end": 637.0, "text": " those obscure scripts we sometimes write, and you can also very easily share this document.", "tokens": [729, 34443, 23294, 321, 2171, 2464, 11, 293, 291, 393, 611, 588, 3612, 2073, 341, 4166, 13], "temperature": 0.0, "avg_logprob": -0.09830891942403402, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.751030115177855e-05}, {"id": 85, "seek": 63700, "start": 637.0, "end": 645.52, "text": " So that's actually something that we did. I got some tasks to do. A client was asking", "tokens": [407, 300, 311, 767, 746, 300, 321, 630, 13, 286, 658, 512, 9608, 281, 360, 13, 316, 6423, 390, 3365], "temperature": 0.0, "avg_logprob": -0.15452391306559246, "compression_ratio": 1.631336405529954, "no_speech_prob": 4.965956031810492e-05}, {"id": 86, "seek": 63700, "start": 645.52, "end": 651.08, "text": " something. We were doing something with machine learning and artificial intelligence. We were", "tokens": [746, 13, 492, 645, 884, 746, 365, 3479, 2539, 293, 11677, 7599, 13, 492, 645], "temperature": 0.0, "avg_logprob": -0.15452391306559246, "compression_ratio": 1.631336405529954, "no_speech_prob": 4.965956031810492e-05}, {"id": 87, "seek": 63700, "start": 651.08, "end": 656.8, "text": " not aware or we did not know whether we could do it. So I sat down, made his live hook,", "tokens": [406, 3650, 420, 321, 630, 406, 458, 1968, 321, 727, 360, 309, 13, 407, 286, 3227, 760, 11, 1027, 702, 1621, 6328, 11], "temperature": 0.0, "avg_logprob": -0.15452391306559246, "compression_ratio": 1.631336405529954, "no_speech_prob": 4.965956031810492e-05}, {"id": 88, "seek": 63700, "start": 656.8, "end": 662.52, "text": " and then documented all the steps I did, and in the end, got a pretty graph out of it,", "tokens": [293, 550, 23007, 439, 264, 4439, 286, 630, 11, 293, 294, 264, 917, 11, 658, 257, 1238, 4295, 484, 295, 309, 11], "temperature": 0.0, "avg_logprob": -0.15452391306559246, "compression_ratio": 1.631336405529954, "no_speech_prob": 4.965956031810492e-05}, {"id": 89, "seek": 66252, "start": 662.52, "end": 671.88, "text": " so I could convince the client that we could do it, actually, with Elixir.", "tokens": [370, 286, 727, 13447, 264, 6423, 300, 321, 727, 360, 309, 11, 767, 11, 365, 2699, 970, 347, 13], "temperature": 0.0, "avg_logprob": -0.17121568932590714, "compression_ratio": 1.555045871559633, "no_speech_prob": 3.790674600168131e-05}, {"id": 90, "seek": 66252, "start": 671.88, "end": 679.04, "text": " Just a little bit of context. I work for a small company. We often switch in between", "tokens": [1449, 257, 707, 857, 295, 4319, 13, 286, 589, 337, 257, 1359, 2237, 13, 492, 2049, 3679, 294, 1296], "temperature": 0.0, "avg_logprob": -0.17121568932590714, "compression_ratio": 1.555045871559633, "no_speech_prob": 3.790674600168131e-05}, {"id": 91, "seek": 66252, "start": 679.04, "end": 685.24, "text": " projects. The company is named Zenjoy, and we are often working as a team of two. So", "tokens": [4455, 13, 440, 2237, 307, 4926, 22387, 1994, 11, 293, 321, 366, 2049, 1364, 382, 257, 1469, 295, 732, 13, 407], "temperature": 0.0, "avg_logprob": -0.17121568932590714, "compression_ratio": 1.555045871559633, "no_speech_prob": 3.790674600168131e-05}, {"id": 92, "seek": 66252, "start": 685.24, "end": 690.36, "text": " documentation and collaboration is very important, and also the communication with the clients", "tokens": [14333, 293, 9363, 307, 588, 1021, 11, 293, 611, 264, 6101, 365, 264, 6982], "temperature": 0.0, "avg_logprob": -0.17121568932590714, "compression_ratio": 1.555045871559633, "no_speech_prob": 3.790674600168131e-05}, {"id": 93, "seek": 69036, "start": 690.36, "end": 699.32, "text": " is very, very important. So in this first case, we were tasked to", "tokens": [307, 588, 11, 588, 1021, 13, 407, 294, 341, 700, 1389, 11, 321, 645, 38621, 281], "temperature": 0.0, "avg_logprob": -0.18002228303389115, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.270851008594036e-05}, {"id": 94, "seek": 69036, "start": 699.32, "end": 709.72, "text": " interoperate with or to call an undocumented legacy API. It was very low level. It was not", "tokens": [728, 7192, 473, 365, 420, 281, 818, 364, 40472, 11711, 9362, 13, 467, 390, 588, 2295, 1496, 13, 467, 390, 406], "temperature": 0.0, "avg_logprob": -0.18002228303389115, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.270851008594036e-05}, {"id": 95, "seek": 69036, "start": 709.72, "end": 717.2, "text": " as low level as tools explained to us, so it was not like we had to do the pattern matching", "tokens": [382, 2295, 1496, 382, 3873, 8825, 281, 505, 11, 370, 309, 390, 406, 411, 321, 632, 281, 360, 264, 5102, 14324], "temperature": 0.0, "avg_logprob": -0.18002228303389115, "compression_ratio": 1.5121951219512195, "no_speech_prob": 6.270851008594036e-05}, {"id": 96, "seek": 71720, "start": 717.2, "end": 723.8000000000001, "text": " on the bit level, but we had to use the GenTCP module straight from Erlang, but it was very", "tokens": [322, 264, 857, 1496, 11, 457, 321, 632, 281, 764, 264, 3632, 18238, 47, 10088, 2997, 490, 3300, 25241, 11, 457, 309, 390, 588], "temperature": 0.0, "avg_logprob": -0.14862876619611468, "compression_ratio": 1.5054945054945055, "no_speech_prob": 4.5269083784660324e-05}, {"id": 97, "seek": 71720, "start": 723.8000000000001, "end": 731.2800000000001, "text": " nice to have this live book environment where we could just throw the commands at this server", "tokens": [1481, 281, 362, 341, 1621, 1446, 2823, 689, 321, 727, 445, 3507, 264, 16901, 412, 341, 7154], "temperature": 0.0, "avg_logprob": -0.14862876619611468, "compression_ratio": 1.5054945054945055, "no_speech_prob": 4.5269083784660324e-05}, {"id": 98, "seek": 71720, "start": 731.2800000000001, "end": 739.36, "text": " that we could somehow use and see what came back. So in this way, we were able to create", "tokens": [300, 321, 727, 6063, 764, 293, 536, 437, 1361, 646, 13, 407, 294, 341, 636, 11, 321, 645, 1075, 281, 1884], "temperature": 0.0, "avg_logprob": -0.14862876619611468, "compression_ratio": 1.5054945054945055, "no_speech_prob": 4.5269083784660324e-05}, {"id": 99, "seek": 73936, "start": 739.36, "end": 749.0, "text": " a notebook that documented all the commands we could see or how it reacted, and you see", "tokens": [257, 21060, 300, 23007, 439, 264, 16901, 321, 727, 536, 420, 577, 309, 34037, 11, 293, 291, 536], "temperature": 0.0, "avg_logprob": -0.11370267868041992, "compression_ratio": 1.5903614457831325, "no_speech_prob": 4.784511838806793e-05}, {"id": 100, "seek": 73936, "start": 749.0, "end": 755.28, "text": " some pattern matching going on here. You also see some magic variables. So this was given", "tokens": [512, 5102, 14324, 516, 322, 510, 13, 509, 611, 536, 512, 5585, 9102, 13, 407, 341, 390, 2212], "temperature": 0.0, "avg_logprob": -0.11370267868041992, "compression_ratio": 1.5903614457831325, "no_speech_prob": 4.784511838806793e-05}, {"id": 101, "seek": 73936, "start": 755.28, "end": 760.28, "text": " to us, so we could not change this, but at least we could document it, and this became", "tokens": [281, 505, 11, 370, 321, 727, 406, 1319, 341, 11, 457, 412, 1935, 321, 727, 4166, 309, 11, 293, 341, 3062], "temperature": 0.0, "avg_logprob": -0.11370267868041992, "compression_ratio": 1.5903614457831325, "no_speech_prob": 4.784511838806793e-05}, {"id": 102, "seek": 76028, "start": 760.28, "end": 774.04, "text": " a very long document to refer back to. So this is another demonstration I wanted to", "tokens": [257, 588, 938, 4166, 281, 2864, 646, 281, 13, 407, 341, 307, 1071, 16520, 286, 1415, 281], "temperature": 0.0, "avg_logprob": -0.139067762038287, "compression_ratio": 1.5402298850574712, "no_speech_prob": 2.6214183890260756e-05}, {"id": 103, "seek": 76028, "start": 774.04, "end": 781.4399999999999, "text": " do. It's not because you're in the browser that you're constrained by any way. You can", "tokens": [360, 13, 467, 311, 406, 570, 291, 434, 294, 264, 11185, 300, 291, 434, 38901, 538, 604, 636, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.139067762038287, "compression_ratio": 1.5402298850574712, "no_speech_prob": 2.6214183890260756e-05}, {"id": 104, "seek": 76028, "start": 781.4399999999999, "end": 787.9599999999999, "text": " still use all the process magic and all the GenServers you like, and this is just a demonstration", "tokens": [920, 764, 439, 264, 1399, 5585, 293, 439, 264, 3632, 50, 260, 840, 291, 411, 11, 293, 341, 307, 445, 257, 16520], "temperature": 0.0, "avg_logprob": -0.139067762038287, "compression_ratio": 1.5402298850574712, "no_speech_prob": 2.6214183890260756e-05}, {"id": 105, "seek": 78796, "start": 787.96, "end": 799.4000000000001, "text": " of how you would go around and spawn a TCP server. I'm using Thousand Island here. In", "tokens": [295, 577, 291, 576, 352, 926, 293, 17088, 257, 48965, 7154, 13, 286, 478, 1228, 334, 563, 474, 7637, 510, 13, 682], "temperature": 0.0, "avg_logprob": -0.18019551116150695, "compression_ratio": 1.4748603351955307, "no_speech_prob": 9.351099288323894e-05}, {"id": 106, "seek": 78796, "start": 799.4000000000001, "end": 808.9200000000001, "text": " reality, I was using the other one, the older one, Cowboy and Ranch. Yes, thank you. But", "tokens": [4103, 11, 286, 390, 1228, 264, 661, 472, 11, 264, 4906, 472, 11, 21933, 12795, 293, 37740, 13, 1079, 11, 1309, 291, 13, 583], "temperature": 0.0, "avg_logprob": -0.18019551116150695, "compression_ratio": 1.4748603351955307, "no_speech_prob": 9.351099288323894e-05}, {"id": 107, "seek": 78796, "start": 808.9200000000001, "end": 813.6, "text": " now for this demonstration, I got to use Thousand Island, and it's super nice, so you can", "tokens": [586, 337, 341, 16520, 11, 286, 658, 281, 764, 334, 563, 474, 7637, 11, 293, 309, 311, 1687, 1481, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.18019551116150695, "compression_ratio": 1.4748603351955307, "no_speech_prob": 9.351099288323894e-05}, {"id": 108, "seek": 81360, "start": 813.6, "end": 820.76, "text": " just define your handler. It's just going to echo back whatever we send to it, and here", "tokens": [445, 6964, 428, 41967, 13, 467, 311, 445, 516, 281, 14300, 646, 2035, 321, 2845, 281, 309, 11, 293, 510], "temperature": 0.0, "avg_logprob": -0.15173308745674466, "compression_ratio": 1.6559633027522935, "no_speech_prob": 3.7586636608466506e-05}, {"id": 109, "seek": 81360, "start": 820.76, "end": 827.12, "text": " I started up. The only caveat or the thing you have to be aware of is that you can start", "tokens": [286, 1409, 493, 13, 440, 787, 43012, 420, 264, 551, 291, 362, 281, 312, 3650, 295, 307, 300, 291, 393, 722], "temperature": 0.0, "avg_logprob": -0.15173308745674466, "compression_ratio": 1.6559633027522935, "no_speech_prob": 3.7586636608466506e-05}, {"id": 110, "seek": 81360, "start": 827.12, "end": 834.4, "text": " your children, your processes, and your supervisor tree under the Kino code cell. So whenever", "tokens": [428, 2227, 11, 428, 7555, 11, 293, 428, 24610, 4230, 833, 264, 591, 2982, 3089, 2815, 13, 407, 5699], "temperature": 0.0, "avg_logprob": -0.15173308745674466, "compression_ratio": 1.6559633027522935, "no_speech_prob": 3.7586636608466506e-05}, {"id": 111, "seek": 81360, "start": 834.4, "end": 841.24, "text": " you reevaluate here, you see that you can't see, but another process is started or a whole", "tokens": [291, 43060, 3337, 10107, 510, 11, 291, 536, 300, 291, 393, 380, 536, 11, 457, 1071, 1399, 307, 1409, 420, 257, 1379], "temperature": 0.0, "avg_logprob": -0.15173308745674466, "compression_ratio": 1.6559633027522935, "no_speech_prob": 3.7586636608466506e-05}, {"id": 112, "seek": 84124, "start": 841.24, "end": 848.4, "text": " tree starts. So this is a nice interop with the Lightbook environment. And once we have", "tokens": [4230, 3719, 13, 407, 341, 307, 257, 1481, 728, 404, 365, 264, 8279, 2939, 2823, 13, 400, 1564, 321, 362], "temperature": 0.0, "avg_logprob": -0.23930497100387793, "compression_ratio": 1.451086956521739, "no_speech_prob": 6.342300912365317e-05}, {"id": 113, "seek": 84124, "start": 848.4, "end": 862.84, "text": " those, we can even... Yeah. Yeah, I guess that the gods are not with me today, but at", "tokens": [729, 11, 321, 393, 754, 485, 865, 13, 865, 11, 286, 2041, 300, 264, 14049, 366, 406, 365, 385, 965, 11, 457, 412], "temperature": 0.0, "avg_logprob": -0.23930497100387793, "compression_ratio": 1.451086956521739, "no_speech_prob": 6.342300912365317e-05}, {"id": 114, "seek": 84124, "start": 862.84, "end": 869.28, "text": " least what I wanted to show is that you can actually draw a pretty picture of the supervision", "tokens": [1935, 437, 286, 1415, 281, 855, 307, 300, 291, 393, 767, 2642, 257, 1238, 3036, 295, 264, 32675], "temperature": 0.0, "avg_logprob": -0.23930497100387793, "compression_ratio": 1.451086956521739, "no_speech_prob": 6.342300912365317e-05}, {"id": 115, "seek": 86928, "start": 869.28, "end": 877.88, "text": " tree right here. But still, I think the server got started, and so now I can net cut into", "tokens": [4230, 558, 510, 13, 583, 920, 11, 286, 519, 264, 7154, 658, 1409, 11, 293, 370, 586, 286, 393, 2533, 1723, 666], "temperature": 0.0, "avg_logprob": -0.12045636895584734, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00011920130054932088}, {"id": 116, "seek": 86928, "start": 877.88, "end": 886.12, "text": " my local host on this given port, and I can see how some stuff is being echoed back to", "tokens": [452, 2654, 3975, 322, 341, 2212, 2436, 11, 293, 286, 393, 536, 577, 512, 1507, 307, 885, 14300, 292, 646, 281], "temperature": 0.0, "avg_logprob": -0.12045636895584734, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00011920130054932088}, {"id": 117, "seek": 86928, "start": 886.12, "end": 898.76, "text": " me, so at least that works. All right. Back to the presentation. So it is a nice environment", "tokens": [385, 11, 370, 412, 1935, 300, 1985, 13, 1057, 558, 13, 5833, 281, 264, 5860, 13, 407, 309, 307, 257, 1481, 2823], "temperature": 0.0, "avg_logprob": -0.12045636895584734, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00011920130054932088}, {"id": 118, "seek": 89876, "start": 898.76, "end": 907.16, "text": " to stub out a server and set up a situation where you can then use your application to", "tokens": [281, 20266, 484, 257, 7154, 293, 992, 493, 257, 2590, 689, 291, 393, 550, 764, 428, 3861, 281], "temperature": 0.0, "avg_logprob": -0.1469386418660482, "compression_ratio": 1.34375, "no_speech_prob": 3.70057241525501e-05}, {"id": 119, "seek": 89876, "start": 907.16, "end": 920.36, "text": " interact with this stubbed version of your API. All right. I'm going to show, or this", "tokens": [4648, 365, 341, 20266, 2883, 3037, 295, 428, 9362, 13, 1057, 558, 13, 286, 478, 516, 281, 855, 11, 420, 341], "temperature": 0.0, "avg_logprob": -0.1469386418660482, "compression_ratio": 1.34375, "no_speech_prob": 3.70057241525501e-05}, {"id": 120, "seek": 92036, "start": 920.36, "end": 929.08, "text": " is just an example of how you would integrate your Livebook with a regular mixed project.", "tokens": [307, 445, 364, 1365, 295, 577, 291, 576, 13365, 428, 10385, 2939, 365, 257, 3890, 7467, 1716, 13], "temperature": 0.0, "avg_logprob": -0.13597858135516827, "compression_ratio": 1.4806629834254144, "no_speech_prob": 4.3933396227657795e-05}, {"id": 121, "seek": 92036, "start": 929.08, "end": 933.52, "text": " It's all just sitting next to each other. Oftentimes, I just make a folder where the", "tokens": [467, 311, 439, 445, 3798, 958, 281, 1184, 661, 13, 46636, 11, 286, 445, 652, 257, 10820, 689, 264], "temperature": 0.0, "avg_logprob": -0.13597858135516827, "compression_ratio": 1.4806629834254144, "no_speech_prob": 4.3933396227657795e-05}, {"id": 122, "seek": 92036, "start": 933.52, "end": 940.32, "text": " notebook lives, and then your mixed project, whatever it is, it can be a Phoenix application.", "tokens": [21060, 2909, 11, 293, 550, 428, 7467, 1716, 11, 2035, 309, 307, 11, 309, 393, 312, 257, 18383, 3861, 13], "temperature": 0.0, "avg_logprob": -0.13597858135516827, "compression_ratio": 1.4806629834254144, "no_speech_prob": 4.3933396227657795e-05}, {"id": 123, "seek": 94032, "start": 940.32, "end": 953.08, "text": " You can access it if you use the part way of referencing your dependencies. A few words", "tokens": [509, 393, 2105, 309, 498, 291, 764, 264, 644, 636, 295, 40582, 428, 36606, 13, 316, 1326, 2283], "temperature": 0.0, "avg_logprob": -0.12018292397260666, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.6644891729811206e-05}, {"id": 124, "seek": 94032, "start": 953.08, "end": 961.32, "text": " about a typical lifecycle that I've observed, you often start to experiment in your Livebook.", "tokens": [466, 257, 7476, 45722, 300, 286, 600, 13095, 11, 291, 2049, 722, 281, 5120, 294, 428, 10385, 2939, 13], "temperature": 0.0, "avg_logprob": -0.12018292397260666, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.6644891729811206e-05}, {"id": 125, "seek": 94032, "start": 961.32, "end": 968.2800000000001, "text": " On good days, you add tests, and then you move all that code into the regular application", "tokens": [1282, 665, 1708, 11, 291, 909, 6921, 11, 293, 550, 291, 1286, 439, 300, 3089, 666, 264, 3890, 3861], "temperature": 0.0, "avg_logprob": -0.12018292397260666, "compression_ratio": 1.456989247311828, "no_speech_prob": 1.6644891729811206e-05}, {"id": 126, "seek": 96828, "start": 968.28, "end": 975.68, "text": " or in the regular mixed project, and you reference it from there on in the way that I just described.", "tokens": [420, 294, 264, 3890, 7467, 1716, 11, 293, 291, 6408, 309, 490, 456, 322, 294, 264, 636, 300, 286, 445, 7619, 13], "temperature": 0.0, "avg_logprob": -0.11330580711364746, "compression_ratio": 1.4789473684210526, "no_speech_prob": 1.5434459783136845e-05}, {"id": 127, "seek": 96828, "start": 975.68, "end": 985.6, "text": " So you promote reusable code, and that's often a way that worked very well for me. The second", "tokens": [407, 291, 9773, 41807, 3089, 11, 293, 300, 311, 2049, 257, 636, 300, 2732, 588, 731, 337, 385, 13, 440, 1150], "temperature": 0.0, "avg_logprob": -0.11330580711364746, "compression_ratio": 1.4789473684210526, "no_speech_prob": 1.5434459783136845e-05}, {"id": 128, "seek": 96828, "start": 985.6, "end": 992.3199999999999, "text": " case that I want to discuss is how I set up or created concurrent ETL pipeline, which", "tokens": [1389, 300, 286, 528, 281, 2248, 307, 577, 286, 992, 493, 420, 2942, 37702, 36953, 43, 15517, 11, 597], "temperature": 0.0, "avg_logprob": -0.11330580711364746, "compression_ratio": 1.4789473684210526, "no_speech_prob": 1.5434459783136845e-05}, {"id": 129, "seek": 99232, "start": 992.32, "end": 999.88, "text": " is a fancy word for just loading CSV files and then maybe transforming them and dumping", "tokens": [307, 257, 10247, 1349, 337, 445, 15114, 48814, 7098, 293, 550, 1310, 27210, 552, 293, 42224], "temperature": 0.0, "avg_logprob": -0.12086183516705623, "compression_ratio": 1.4432432432432432, "no_speech_prob": 4.680603524320759e-05}, {"id": 130, "seek": 99232, "start": 999.88, "end": 1007.88, "text": " them into Postgres. So I really got to learn a lot about how concurrent data processing", "tokens": [552, 666, 10223, 45189, 13, 407, 286, 534, 658, 281, 1466, 257, 688, 466, 577, 37702, 1412, 9007], "temperature": 0.0, "avg_logprob": -0.12086183516705623, "compression_ratio": 1.4432432432432432, "no_speech_prob": 4.680603524320759e-05}, {"id": 131, "seek": 99232, "start": 1007.88, "end": 1014.4000000000001, "text": " actually happens. I got to play around with Flow, which is a very nice library which builds", "tokens": [767, 2314, 13, 286, 658, 281, 862, 926, 365, 32792, 11, 597, 307, 257, 588, 1481, 6405, 597, 15182], "temperature": 0.0, "avg_logprob": -0.12086183516705623, "compression_ratio": 1.4432432432432432, "no_speech_prob": 4.680603524320759e-05}, {"id": 132, "seek": 101440, "start": 1014.4, "end": 1025.08, "text": " on top of GenState. No, not GenState, the other one. GenStage, that's the one. And you can", "tokens": [322, 1192, 295, 3632, 4520, 473, 13, 883, 11, 406, 3632, 4520, 473, 11, 264, 661, 472, 13, 3632, 4520, 609, 11, 300, 311, 264, 472, 13, 400, 291, 393], "temperature": 0.0, "avg_logprob": -0.2226855845390996, "compression_ratio": 1.4619565217391304, "no_speech_prob": 2.7914782549487427e-05}, {"id": 133, "seek": 101440, "start": 1025.08, "end": 1033.76, "text": " still use all the power of processes that are available in Euler and Elixir. To demonstrate,", "tokens": [920, 764, 439, 264, 1347, 295, 7555, 300, 366, 2435, 294, 462, 26318, 293, 2699, 970, 347, 13, 1407, 11698, 11], "temperature": 0.0, "avg_logprob": -0.2226855845390996, "compression_ratio": 1.4619565217391304, "no_speech_prob": 2.7914782549487427e-05}, {"id": 134, "seek": 101440, "start": 1033.76, "end": 1040.72, "text": " I've prepared or I want to show how you can use ECTO and then this Flow library right", "tokens": [286, 600, 4927, 420, 286, 528, 281, 855, 577, 291, 393, 764, 19081, 15427, 293, 550, 341, 32792, 6405, 558], "temperature": 0.0, "avg_logprob": -0.2226855845390996, "compression_ratio": 1.4619565217391304, "no_speech_prob": 2.7914782549487427e-05}, {"id": 135, "seek": 104072, "start": 1040.72, "end": 1048.4, "text": " within your like hook application. To start off, I create a repository, just like you would", "tokens": [1951, 428, 411, 6328, 3861, 13, 1407, 722, 766, 11, 286, 1884, 257, 25841, 11, 445, 411, 291, 576], "temperature": 0.0, "avg_logprob": -0.1704233255279198, "compression_ratio": 1.5169491525423728, "no_speech_prob": 6.337304057524307e-06}, {"id": 136, "seek": 104072, "start": 1048.4, "end": 1053.04, "text": " do in a Phoenix application. Don't worry if you do not recognize this. This is kind of", "tokens": [360, 294, 257, 18383, 3861, 13, 1468, 380, 3292, 498, 291, 360, 406, 5521, 341, 13, 639, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.1704233255279198, "compression_ratio": 1.5169491525423728, "no_speech_prob": 6.337304057524307e-06}, {"id": 137, "seek": 104072, "start": 1053.04, "end": 1060.68, "text": " standard stuff. You have to specify the adapter, and then you can emulate whatever mixed ECTO", "tokens": [3832, 1507, 13, 509, 362, 281, 16500, 264, 22860, 11, 293, 550, 291, 393, 45497, 2035, 7467, 19081, 15427], "temperature": 0.0, "avg_logprob": -0.1704233255279198, "compression_ratio": 1.5169491525423728, "no_speech_prob": 6.337304057524307e-06}, {"id": 138, "seek": 104072, "start": 1060.68, "end": 1065.24, "text": " create would do. So you make sure that your storage is up. In my case, it was already", "tokens": [1884, 576, 360, 13, 407, 291, 652, 988, 300, 428, 6725, 307, 493, 13, 682, 452, 1389, 11, 309, 390, 1217], "temperature": 0.0, "avg_logprob": -0.1704233255279198, "compression_ratio": 1.5169491525423728, "no_speech_prob": 6.337304057524307e-06}, {"id": 139, "seek": 106524, "start": 1065.24, "end": 1071.36, "text": " up, so that's what it reports. And then you can even make your migrations like you would", "tokens": [493, 11, 370, 300, 311, 437, 309, 7122, 13, 400, 550, 291, 393, 754, 652, 428, 6186, 12154, 411, 291, 576], "temperature": 0.0, "avg_logprob": -0.11495142085577852, "compression_ratio": 1.5701754385964912, "no_speech_prob": 1.3831920114171226e-05}, {"id": 140, "seek": 106524, "start": 1071.36, "end": 1079.4, "text": " if you used ECTO together with a Phoenix application. In this case, I also made sure there was", "tokens": [498, 291, 1143, 19081, 15427, 1214, 365, 257, 18383, 3861, 13, 682, 341, 1389, 11, 286, 611, 1027, 988, 456, 390], "temperature": 0.0, "avg_logprob": -0.11495142085577852, "compression_ratio": 1.5701754385964912, "no_speech_prob": 1.3831920114171226e-05}, {"id": 141, "seek": 106524, "start": 1079.4, "end": 1085.84, "text": " an item in the database so we can query it later on. So I have to make sure that this", "tokens": [364, 3174, 294, 264, 8149, 370, 321, 393, 14581, 309, 1780, 322, 13, 407, 286, 362, 281, 652, 988, 300, 341], "temperature": 0.0, "avg_logprob": -0.11495142085577852, "compression_ratio": 1.5701754385964912, "no_speech_prob": 1.3831920114171226e-05}, {"id": 142, "seek": 106524, "start": 1085.84, "end": 1092.64, "text": " repository actually runs. And then I migrate, well, I do a rollback to make sure nothing", "tokens": [25841, 767, 6676, 13, 400, 550, 286, 31821, 11, 731, 11, 286, 360, 257, 3373, 3207, 281, 652, 988, 1825], "temperature": 0.0, "avg_logprob": -0.11495142085577852, "compression_ratio": 1.5701754385964912, "no_speech_prob": 1.3831920114171226e-05}, {"id": 143, "seek": 109264, "start": 1092.64, "end": 1100.16, "text": " is being left over, and then I migrate again. So we have the end situation that I can query", "tokens": [307, 885, 1411, 670, 11, 293, 550, 286, 31821, 797, 13, 407, 321, 362, 264, 917, 2590, 300, 286, 393, 14581], "temperature": 0.0, "avg_logprob": -0.08534404505854068, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.00010446796659380198}, {"id": 144, "seek": 109264, "start": 1100.16, "end": 1109.8400000000001, "text": " right here. So you can see that our fresh entry is just inserted with this new timestamp.", "tokens": [558, 510, 13, 407, 291, 393, 536, 300, 527, 4451, 8729, 307, 445, 27992, 365, 341, 777, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.08534404505854068, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.00010446796659380198}, {"id": 145, "seek": 109264, "start": 1109.8400000000001, "end": 1117.5200000000002, "text": " And then we can build upon this. This is another demonstration, very short. This is a definition", "tokens": [400, 550, 321, 393, 1322, 3564, 341, 13, 639, 307, 1071, 16520, 11, 588, 2099, 13, 639, 307, 257, 7123], "temperature": 0.0, "avg_logprob": -0.08534404505854068, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.00010446796659380198}, {"id": 146, "seek": 111752, "start": 1117.52, "end": 1123.08, "text": " of a flow. Also, don't worry if you do not recognize this. That's not the key here. I", "tokens": [295, 257, 3095, 13, 2743, 11, 500, 380, 3292, 498, 291, 360, 406, 5521, 341, 13, 663, 311, 406, 264, 2141, 510, 13, 286], "temperature": 0.0, "avg_logprob": -0.1371257761691479, "compression_ratio": 1.5625, "no_speech_prob": 2.9291373721207492e-05}, {"id": 147, "seek": 111752, "start": 1123.08, "end": 1128.72, "text": " just want to show that you can use all those goodies, and you're not constrained in any", "tokens": [445, 528, 281, 855, 300, 291, 393, 764, 439, 729, 44072, 11, 293, 291, 434, 406, 38901, 294, 604], "temperature": 0.0, "avg_logprob": -0.1371257761691479, "compression_ratio": 1.5625, "no_speech_prob": 2.9291373721207492e-05}, {"id": 148, "seek": 111752, "start": 1128.72, "end": 1135.76, "text": " way. Right here, we are just emitting a value every second, and we're going to wait for", "tokens": [636, 13, 1779, 510, 11, 321, 366, 445, 846, 2414, 257, 2158, 633, 1150, 11, 293, 321, 434, 516, 281, 1699, 337], "temperature": 0.0, "avg_logprob": -0.1371257761691479, "compression_ratio": 1.5625, "no_speech_prob": 2.9291373721207492e-05}, {"id": 149, "seek": 111752, "start": 1135.76, "end": 1142.56, "text": " three seconds and then insert an item in the database again. So you get the logging, and", "tokens": [1045, 3949, 293, 550, 8969, 364, 3174, 294, 264, 8149, 797, 13, 407, 291, 483, 264, 27991, 11, 293], "temperature": 0.0, "avg_logprob": -0.1371257761691479, "compression_ratio": 1.5625, "no_speech_prob": 2.9291373721207492e-05}, {"id": 150, "seek": 114256, "start": 1142.56, "end": 1149.48, "text": " if we query it again, so you might recognize the nice ecto query syntax. You see I've", "tokens": [498, 321, 14581, 309, 797, 11, 370, 291, 1062, 5521, 264, 1481, 308, 349, 78, 14581, 28431, 13, 509, 536, 286, 600], "temperature": 0.0, "avg_logprob": -0.2003457462086397, "compression_ratio": 1.5515695067264574, "no_speech_prob": 2.26956581172999e-05}, {"id": 151, "seek": 114256, "start": 1149.48, "end": 1158.8799999999999, "text": " wrapped it in a data table, and you can now see the three new items appearing. So that's", "tokens": [14226, 309, 294, 257, 1412, 3199, 11, 293, 291, 393, 586, 536, 264, 1045, 777, 4754, 19870, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.2003457462086397, "compression_ratio": 1.5515695067264574, "no_speech_prob": 2.26956581172999e-05}, {"id": 152, "seek": 114256, "start": 1158.8799999999999, "end": 1165.44, "text": " very nice. Here you see me playing around and actually visualizing this ETL pipeline", "tokens": [588, 1481, 13, 1692, 291, 536, 385, 2433, 926, 293, 767, 5056, 3319, 341, 36953, 43, 15517], "temperature": 0.0, "avg_logprob": -0.2003457462086397, "compression_ratio": 1.5515695067264574, "no_speech_prob": 2.26956581172999e-05}, {"id": 153, "seek": 114256, "start": 1165.44, "end": 1172.24, "text": " where every color actually is another class of objects or is being inserted in another", "tokens": [689, 633, 2017, 767, 307, 1071, 1508, 295, 6565, 420, 307, 885, 27992, 294, 1071], "temperature": 0.0, "avg_logprob": -0.2003457462086397, "compression_ratio": 1.5515695067264574, "no_speech_prob": 2.26956581172999e-05}, {"id": 154, "seek": 117224, "start": 1172.24, "end": 1179.28, "text": " table. It went very quickly, but when making this presentation, I also saw there is some", "tokens": [3199, 13, 467, 1437, 588, 2661, 11, 457, 562, 1455, 341, 5860, 11, 286, 611, 1866, 456, 307, 512], "temperature": 0.0, "avg_logprob": -0.14878401160240173, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.1849357987521216e-05}, {"id": 155, "seek": 117224, "start": 1179.28, "end": 1186.48, "text": " room for improvement. So not all cylinders are firing together, but at least it was fast", "tokens": [1808, 337, 10444, 13, 407, 406, 439, 42166, 366, 16045, 1214, 11, 457, 412, 1935, 309, 390, 2370], "temperature": 0.0, "avg_logprob": -0.14878401160240173, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.1849357987521216e-05}, {"id": 156, "seek": 117224, "start": 1186.48, "end": 1195.68, "text": " enough for our purposes. Another case I want to share with you is that we used the live", "tokens": [1547, 337, 527, 9932, 13, 3996, 1389, 286, 528, 281, 2073, 365, 291, 307, 300, 321, 1143, 264, 1621], "temperature": 0.0, "avg_logprob": -0.14878401160240173, "compression_ratio": 1.456043956043956, "no_speech_prob": 5.1849357987521216e-05}, {"id": 157, "seek": 119568, "start": 1195.68, "end": 1205.96, "text": " hook to actually connect to a live running instance. So remember, as I have shown in", "tokens": [6328, 281, 767, 1745, 281, 257, 1621, 2614, 5197, 13, 407, 1604, 11, 382, 286, 362, 4898, 294], "temperature": 0.0, "avg_logprob": -0.1427943634264397, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.066553836106323e-05}, {"id": 158, "seek": 119568, "start": 1205.96, "end": 1213.04, "text": " the beginning, it's all just Erlang distribution under the hood. So instead of using the regular", "tokens": [264, 2863, 11, 309, 311, 439, 445, 3300, 25241, 7316, 833, 264, 13376, 13, 407, 2602, 295, 1228, 264, 3890], "temperature": 0.0, "avg_logprob": -0.1427943634264397, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.066553836106323e-05}, {"id": 159, "seek": 119568, "start": 1213.04, "end": 1220.24, "text": " setup where you do an LXR standalone setting, you can also do an attached node configuration.", "tokens": [8657, 689, 291, 360, 364, 441, 55, 49, 37454, 3287, 11, 291, 393, 611, 360, 364, 8570, 9984, 11694, 13], "temperature": 0.0, "avg_logprob": -0.1427943634264397, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.066553836106323e-05}, {"id": 160, "seek": 122024, "start": 1220.24, "end": 1227.52, "text": " The only thing you have to know is your node's name or the short name and a cookie which", "tokens": [440, 787, 551, 291, 362, 281, 458, 307, 428, 9984, 311, 1315, 420, 264, 2099, 1315, 293, 257, 14417, 597], "temperature": 0.0, "avg_logprob": -0.13315642873446146, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.708270949369762e-05}, {"id": 161, "seek": 122024, "start": 1227.52, "end": 1233.4, "text": " you have to agree upon. And it's very good for doing one of the tasks. Maybe you don't", "tokens": [291, 362, 281, 3986, 3564, 13, 400, 309, 311, 588, 665, 337, 884, 472, 295, 264, 9608, 13, 2704, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.13315642873446146, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.708270949369762e-05}, {"id": 162, "seek": 122024, "start": 1233.4, "end": 1242.24, "text": " have a UI for something yet, and you want to do it in a live book, then this is a nice", "tokens": [362, 257, 15682, 337, 746, 1939, 11, 293, 291, 528, 281, 360, 309, 294, 257, 1621, 1446, 11, 550, 341, 307, 257, 1481], "temperature": 0.0, "avg_logprob": -0.13315642873446146, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.708270949369762e-05}, {"id": 163, "seek": 122024, "start": 1242.24, "end": 1248.84, "text": " way to actually have like a super admin interface, but be aware that this is still a live environment.", "tokens": [636, 281, 767, 362, 411, 257, 1687, 24236, 9226, 11, 457, 312, 3650, 300, 341, 307, 920, 257, 1621, 2823, 13], "temperature": 0.0, "avg_logprob": -0.13315642873446146, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.708270949369762e-05}, {"id": 164, "seek": 124884, "start": 1248.84, "end": 1254.3999999999999, "text": " So if you do this, make sure to put a big disclaimer on top of your notebook to remind", "tokens": [407, 498, 291, 360, 341, 11, 652, 988, 281, 829, 257, 955, 40896, 322, 1192, 295, 428, 21060, 281, 4160], "temperature": 0.0, "avg_logprob": -0.13854965633816188, "compression_ratio": 1.587962962962963, "no_speech_prob": 3.316248694318347e-05}, {"id": 165, "seek": 124884, "start": 1254.3999999999999, "end": 1265.36, "text": " you of the risks involved. All right. The last thing that I want to share or show is", "tokens": [291, 295, 264, 10888, 3288, 13, 1057, 558, 13, 440, 1036, 551, 300, 286, 528, 281, 2073, 420, 855, 307], "temperature": 0.0, "avg_logprob": -0.13854965633816188, "compression_ratio": 1.587962962962963, "no_speech_prob": 3.316248694318347e-05}, {"id": 166, "seek": 124884, "start": 1265.36, "end": 1271.6799999999998, "text": " how you would do tests in live hook. Like I said, on a good day, you write tests. And", "tokens": [577, 291, 576, 360, 6921, 294, 1621, 6328, 13, 1743, 286, 848, 11, 322, 257, 665, 786, 11, 291, 2464, 6921, 13, 400], "temperature": 0.0, "avg_logprob": -0.13854965633816188, "compression_ratio": 1.587962962962963, "no_speech_prob": 3.316248694318347e-05}, {"id": 167, "seek": 124884, "start": 1271.6799999999998, "end": 1277.4399999999998, "text": " we have seen some examples in the previous presentations, how you would do like a doc", "tokens": [321, 362, 1612, 512, 5110, 294, 264, 3894, 18964, 11, 577, 291, 576, 360, 411, 257, 3211], "temperature": 0.0, "avg_logprob": -0.13854965633816188, "compression_ratio": 1.587962962962963, "no_speech_prob": 3.316248694318347e-05}, {"id": 168, "seek": 127744, "start": 1277.44, "end": 1288.88, "text": " test where you attach some kind of a formatted test and it's expected output. Well, since", "tokens": [1500, 689, 291, 5085, 512, 733, 295, 257, 1254, 32509, 1500, 293, 309, 311, 5176, 5598, 13, 1042, 11, 1670], "temperature": 0.0, "avg_logprob": -0.23146771370096408, "compression_ratio": 1.3795620437956204, "no_speech_prob": 0.00010312159429304302}, {"id": 169, "seek": 127744, "start": 1288.88, "end": 1297.52, "text": " a few versions, these tests are actually automatically run. So if you define a module, in this case", "tokens": [257, 1326, 9606, 11, 613, 6921, 366, 767, 6772, 1190, 13, 407, 498, 291, 6964, 257, 10088, 11, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.23146771370096408, "compression_ratio": 1.3795620437956204, "no_speech_prob": 0.00010312159429304302}, {"id": 170, "seek": 129752, "start": 1297.52, "end": 1308.16, "text": " Christmas, you see that the doc tests are failing. I think I can easily fix it by changing", "tokens": [5272, 11, 291, 536, 300, 264, 3211, 6921, 366, 18223, 13, 286, 519, 286, 393, 3612, 3191, 309, 538, 4473], "temperature": 0.0, "avg_logprob": -0.1051134846427224, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.449855335522443e-05}, {"id": 171, "seek": 129752, "start": 1308.16, "end": 1313.2, "text": " the expectation. And if you run it again, the doc tests are green again. But you can", "tokens": [264, 14334, 13, 400, 498, 291, 1190, 309, 797, 11, 264, 3211, 6921, 366, 3092, 797, 13, 583, 291, 393], "temperature": 0.0, "avg_logprob": -0.1051134846427224, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.449855335522443e-05}, {"id": 172, "seek": 129752, "start": 1313.2, "end": 1317.32, "text": " also do just your regular testing. The only thing you have to think about is that you", "tokens": [611, 360, 445, 428, 3890, 4997, 13, 440, 787, 551, 291, 362, 281, 519, 466, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.1051134846427224, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.449855335522443e-05}, {"id": 173, "seek": 129752, "start": 1317.32, "end": 1324.0, "text": " have to disable your auto-running. But then again, you can do your testing and you have", "tokens": [362, 281, 28362, 428, 8399, 12, 45482, 13, 583, 550, 797, 11, 291, 393, 360, 428, 4997, 293, 291, 362], "temperature": 0.0, "avg_logprob": -0.1051134846427224, "compression_ratio": 1.7277227722772277, "no_speech_prob": 1.449855335522443e-05}, {"id": 174, "seek": 132400, "start": 1324.0, "end": 1330.72, "text": " to make sure you call the run function on the X unit module. So there is no excuse not", "tokens": [281, 652, 988, 291, 818, 264, 1190, 2445, 322, 264, 1783, 4985, 10088, 13, 407, 456, 307, 572, 8960, 406], "temperature": 0.0, "avg_logprob": -0.1834217839770847, "compression_ratio": 1.4663212435233162, "no_speech_prob": 3.755461148102768e-05}, {"id": 175, "seek": 132400, "start": 1330.72, "end": 1343.48, "text": " to test, actually. I want to end with reference to these two resources. There is an initiative", "tokens": [281, 1500, 11, 767, 13, 286, 528, 281, 917, 365, 6408, 281, 613, 732, 3593, 13, 821, 307, 364, 11552], "temperature": 0.0, "avg_logprob": -0.1834217839770847, "compression_ratio": 1.4663212435233162, "no_speech_prob": 3.755461148102768e-05}, {"id": 176, "seek": 132400, "start": 1343.48, "end": 1351.4, "text": " by Dockyard Academy. It's an open source curriculum to learn Elixir. And they have used the notebooks", "tokens": [538, 1144, 547, 13862, 11735, 13, 467, 311, 364, 1269, 4009, 14302, 281, 1466, 2699, 970, 347, 13, 400, 436, 362, 1143, 264, 43782], "temperature": 0.0, "avg_logprob": -0.1834217839770847, "compression_ratio": 1.4663212435233162, "no_speech_prob": 3.755461148102768e-05}, {"id": 177, "seek": 135140, "start": 1351.4, "end": 1357.52, "text": " or the live books to actually teach this to students. And the other thing you might have", "tokens": [420, 264, 1621, 3642, 281, 767, 2924, 341, 281, 1731, 13, 400, 264, 661, 551, 291, 1062, 362], "temperature": 0.0, "avg_logprob": -0.18738833533393012, "compression_ratio": 1.5584415584415585, "no_speech_prob": 8.065710426308215e-05}, {"id": 178, "seek": 135140, "start": 1357.52, "end": 1364.8000000000002, "text": " heard about in the Elixir news is the Project Bumblebee, which allows you to actually play", "tokens": [2198, 466, 294, 264, 2699, 970, 347, 2583, 307, 264, 9849, 363, 16473, 24872, 11, 597, 4045, 291, 281, 767, 862], "temperature": 0.0, "avg_logprob": -0.18738833533393012, "compression_ratio": 1.5584415584415585, "no_speech_prob": 8.065710426308215e-05}, {"id": 179, "seek": 135140, "start": 1364.8000000000002, "end": 1372.16, "text": " around with these new neural networks like GPT2 and stable diffusion. And you can just", "tokens": [926, 365, 613, 777, 18161, 9590, 411, 26039, 51, 17, 293, 8351, 25242, 13, 400, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.18738833533393012, "compression_ratio": 1.5584415584415585, "no_speech_prob": 8.065710426308215e-05}, {"id": 180, "seek": 135140, "start": 1372.16, "end": 1380.52, "text": " do it locally. So it's a very nice way. It integrates very nice into your live book notebook.", "tokens": [360, 309, 16143, 13, 407, 309, 311, 257, 588, 1481, 636, 13, 467, 3572, 1024, 588, 1481, 666, 428, 1621, 1446, 21060, 13], "temperature": 0.0, "avg_logprob": -0.18738833533393012, "compression_ratio": 1.5584415584415585, "no_speech_prob": 8.065710426308215e-05}, {"id": 181, "seek": 138052, "start": 1380.52, "end": 1399.52, "text": " All right. That's it for me. Thank you very much. Thank you very much. Is there any question?", "tokens": [1057, 558, 13, 663, 311, 309, 337, 385, 13, 1044, 291, 588, 709, 13, 1044, 291, 588, 709, 13, 1119, 456, 604, 1168, 30], "temperature": 0.0, "avg_logprob": -0.173028750853105, "compression_ratio": 1.3865546218487395, "no_speech_prob": 0.00048628266085870564}, {"id": 182, "seek": 138052, "start": 1399.52, "end": 1404.2, "text": " Could you maybe compare and contrast live books with Jupyter notebooks?", "tokens": [7497, 291, 1310, 6794, 293, 8712, 1621, 3642, 365, 22125, 88, 391, 43782, 30], "temperature": 0.0, "avg_logprob": -0.173028750853105, "compression_ratio": 1.3865546218487395, "no_speech_prob": 0.00048628266085870564}, {"id": 183, "seek": 140420, "start": 1404.2, "end": 1411.8400000000001, "text": " Yes. That's actually a reference. Sorry. So the question was how this relates to the", "tokens": [1079, 13, 663, 311, 767, 257, 6408, 13, 4919, 13, 407, 264, 1168, 390, 577, 341, 16155, 281, 264], "temperature": 0.0, "avg_logprob": -0.13727795675899204, "compression_ratio": 1.6, "no_speech_prob": 4.065760731464252e-05}, {"id": 184, "seek": 140420, "start": 1411.8400000000001, "end": 1418.04, "text": " Jupyter notebooks, which we might also know. I think it's very much inspired by it. So", "tokens": [22125, 88, 391, 43782, 11, 597, 321, 1062, 611, 458, 13, 286, 519, 309, 311, 588, 709, 7547, 538, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.13727795675899204, "compression_ratio": 1.6, "no_speech_prob": 4.065760731464252e-05}, {"id": 185, "seek": 140420, "start": 1418.04, "end": 1424.16, "text": " it's also a computational notebook. But I also see a lot of differences, although I", "tokens": [309, 311, 611, 257, 28270, 21060, 13, 583, 286, 611, 536, 257, 688, 295, 7300, 11, 4878, 286], "temperature": 0.0, "avg_logprob": -0.13727795675899204, "compression_ratio": 1.6, "no_speech_prob": 4.065760731464252e-05}, {"id": 186, "seek": 140420, "start": 1424.16, "end": 1430.3600000000001, "text": " do not know Jupyter notebooks very well. But I think, for example, like the dependencies", "tokens": [360, 406, 458, 22125, 88, 391, 43782, 588, 731, 13, 583, 286, 519, 11, 337, 1365, 11, 411, 264, 36606], "temperature": 0.0, "avg_logprob": -0.13727795675899204, "compression_ratio": 1.6, "no_speech_prob": 4.065760731464252e-05}, {"id": 187, "seek": 143036, "start": 1430.36, "end": 1436.04, "text": " in the first cell, I do not think there is such a system in the Jupyter notebooks. You", "tokens": [294, 264, 700, 2815, 11, 286, 360, 406, 519, 456, 307, 1270, 257, 1185, 294, 264, 22125, 88, 391, 43782, 13, 509], "temperature": 0.0, "avg_logprob": -0.14537387575422014, "compression_ratio": 1.4065934065934067, "no_speech_prob": 0.0002775265311356634}, {"id": 188, "seek": 143036, "start": 1436.04, "end": 1441.12, "text": " would have to use like Comda or Anaconda to set up your dependency. So it's a little", "tokens": [576, 362, 281, 764, 411, 2432, 2675, 420, 1107, 326, 12233, 281, 992, 493, 428, 33621, 13, 407, 309, 311, 257, 707], "temperature": 0.0, "avg_logprob": -0.14537387575422014, "compression_ratio": 1.4065934065934067, "no_speech_prob": 0.0002775265311356634}, {"id": 189, "seek": 143036, "start": 1441.12, "end": 1447.9599999999998, "text": " bit less integrated. But I cannot say more about differences. But you're very right.", "tokens": [857, 1570, 10919, 13, 583, 286, 2644, 584, 544, 466, 7300, 13, 583, 291, 434, 588, 558, 13], "temperature": 0.0, "avg_logprob": -0.14537387575422014, "compression_ratio": 1.4065934065934067, "no_speech_prob": 0.0002775265311356634}, {"id": 190, "seek": 144796, "start": 1447.96, "end": 1461.04, "text": " There is a strong inspiration there. Yes. Thank you. Any other question?", "tokens": [821, 307, 257, 2068, 10249, 456, 13, 1079, 13, 1044, 291, 13, 2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.1850693281306777, "compression_ratio": 1.319672131147541, "no_speech_prob": 0.00026700724265538156}, {"id": 191, "seek": 144796, "start": 1461.04, "end": 1472.6000000000001, "text": " Cool. Thanks for the talk. I wanted to ask, actually, whether there is an option as well", "tokens": [8561, 13, 2561, 337, 264, 751, 13, 286, 1415, 281, 1029, 11, 767, 11, 1968, 456, 307, 364, 3614, 382, 731], "temperature": 0.0, "avg_logprob": -0.1850693281306777, "compression_ratio": 1.319672131147541, "no_speech_prob": 0.00026700724265538156}, {"id": 192, "seek": 147260, "start": 1472.6, "end": 1479.8799999999999, "text": " for a live book being available as an UI within the IDE, so kind of connected closer to the", "tokens": [337, 257, 1621, 1446, 885, 2435, 382, 364, 15682, 1951, 264, 40930, 11, 370, 733, 295, 4582, 4966, 281, 264], "temperature": 0.0, "avg_logprob": -0.1938998232182768, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001471330033382401}, {"id": 193, "seek": 147260, "start": 1479.8799999999999, "end": 1488.36, "text": " development environment? No. Not that I know of. No. No. It runs in the browser, and that's", "tokens": [3250, 2823, 30, 883, 13, 1726, 300, 286, 458, 295, 13, 883, 13, 883, 13, 467, 6676, 294, 264, 11185, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.1938998232182768, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001471330033382401}, {"id": 194, "seek": 147260, "start": 1488.36, "end": 1492.6799999999998, "text": " where it lives. So you can install it as a standalone application, but it's still something", "tokens": [689, 309, 2909, 13, 407, 291, 393, 3625, 309, 382, 257, 37454, 3861, 11, 457, 309, 311, 920, 746], "temperature": 0.0, "avg_logprob": -0.1938998232182768, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001471330033382401}, {"id": 195, "seek": 147260, "start": 1492.6799999999998, "end": 1499.9599999999998, "text": " that lives in the browser. But you're right in the sense that it is not a full-blown IDE,", "tokens": [300, 2909, 294, 264, 11185, 13, 583, 291, 434, 558, 294, 264, 2020, 300, 309, 307, 406, 257, 1577, 12, 5199, 648, 40930, 11], "temperature": 0.0, "avg_logprob": -0.1938998232182768, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0001471330033382401}, {"id": 196, "seek": 149996, "start": 1499.96, "end": 1505.52, "text": " and that's also one of the nuisances that I have noticed is that if you have very large", "tokens": [293, 300, 311, 611, 472, 295, 264, 3822, 271, 2676, 300, 286, 362, 5694, 307, 300, 498, 291, 362, 588, 2416], "temperature": 0.0, "avg_logprob": -0.18522826300726997, "compression_ratio": 1.5458715596330275, "no_speech_prob": 8.171705849235877e-05}, {"id": 197, "seek": 149996, "start": 1505.52, "end": 1511.28, "text": " code cells, for example, you are missing some features. And if you're used to VI bindings,", "tokens": [3089, 5438, 11, 337, 1365, 11, 291, 366, 5361, 512, 4122, 13, 400, 498, 291, 434, 1143, 281, 27619, 14786, 1109, 11], "temperature": 0.0, "avg_logprob": -0.18522826300726997, "compression_ratio": 1.5458715596330275, "no_speech_prob": 8.171705849235877e-05}, {"id": 198, "seek": 149996, "start": 1511.28, "end": 1516.0, "text": " for example, you will not find them there. Yeah. Cool. Thanks. Yes.", "tokens": [337, 1365, 11, 291, 486, 406, 915, 552, 456, 13, 865, 13, 8561, 13, 2561, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.18522826300726997, "compression_ratio": 1.5458715596330275, "no_speech_prob": 8.171705849235877e-05}, {"id": 199, "seek": 149996, "start": 1516.0, "end": 1526.8400000000001, "text": " Okay. Last question. Does this work for multiple users collaborating on things? Yes. And I", "tokens": [1033, 13, 5264, 1168, 13, 4402, 341, 589, 337, 3866, 5022, 30188, 322, 721, 30, 1079, 13, 400, 286], "temperature": 0.0, "avg_logprob": -0.18522826300726997, "compression_ratio": 1.5458715596330275, "no_speech_prob": 8.171705849235877e-05}, {"id": 200, "seek": 152684, "start": 1526.84, "end": 1533.1999999999998, "text": " should have shown this. It is one of the nicest features. Thank you for opening that door.", "tokens": [820, 362, 4898, 341, 13, 467, 307, 472, 295, 264, 45516, 4122, 13, 1044, 291, 337, 5193, 300, 2853, 13], "temperature": 0.0, "avg_logprob": -0.14342029528184372, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.0132378267589957e-05}, {"id": 201, "seek": 152684, "start": 1533.1999999999998, "end": 1540.76, "text": " If you're using multiple sessions or, for example, multiple users in multiple locations,", "tokens": [759, 291, 434, 1228, 3866, 11081, 420, 11, 337, 1365, 11, 3866, 5022, 294, 3866, 9253, 11], "temperature": 0.0, "avg_logprob": -0.14342029528184372, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.0132378267589957e-05}, {"id": 202, "seek": 152684, "start": 1540.76, "end": 1544.6799999999998, "text": " you, for example, see the selections they made. You see a little cursor where they are", "tokens": [291, 11, 337, 1365, 11, 536, 264, 47829, 436, 1027, 13, 509, 536, 257, 707, 28169, 689, 436, 366], "temperature": 0.0, "avg_logprob": -0.14342029528184372, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.0132378267589957e-05}, {"id": 203, "seek": 152684, "start": 1544.6799999999998, "end": 1552.6399999999999, "text": " editing or you are editing, and you're actually editing the same notebook. So, yes, it's kind", "tokens": [10000, 420, 291, 366, 10000, 11, 293, 291, 434, 767, 10000, 264, 912, 21060, 13, 407, 11, 2086, 11, 309, 311, 733], "temperature": 0.0, "avg_logprob": -0.14342029528184372, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.0132378267589957e-05}, {"id": 204, "seek": 155264, "start": 1552.64, "end": 1563.16, "text": " of a live coding environment. Yes. I don't know. No. It's building on top of these goodies", "tokens": [295, 257, 1621, 17720, 2823, 13, 1079, 13, 286, 500, 380, 458, 13, 883, 13, 467, 311, 2390, 322, 1192, 295, 613, 44072], "temperature": 0.0, "avg_logprob": -0.20749564048571464, "compression_ratio": 1.1636363636363636, "no_speech_prob": 0.0001720579748507589}, {"id": 205, "seek": 156316, "start": 1563.16, "end": 1583.28, "text": " we have. Yeah. Okay. Thank you again.", "tokens": [50364, 321, 362, 13, 865, 13, 1033, 13, 1044, 291, 797, 13, 51370], "temperature": 0.0, "avg_logprob": -0.3872267859322684, "compression_ratio": 0.8222222222222222, "no_speech_prob": 0.00043375659151934087}], "language": "en"}