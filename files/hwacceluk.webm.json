{"text": " Hi, everyone. So it's my pleasure to introduce Babis and Anastasios. They're going to give you the talk on using VXL for hard acceleration in your kernels. Babis, please. So hello, everyone. I'm Babis. My actual name is Haraldos Minus, but you can just call me Babis. So we're going to give a talk about hardware acceleration and our effort to having some support in the unit kernels, and we do that with VXL. So, yeah. Yeah, Kim, oh, sorry. I forgot about that. Oh, okay. Oh, let's forget. Yeah, put that over here over there, and maybe you can just keep it here. Okay. So, yeah, we already heard from Simon, so we don't have to repeat what the unit kernels are. There are a lot of projects, and we know that they are promising. It's a promising technology. We can have very fast boot times, low memory footprint, and some increased security. We also know some of the use cases for unit kernels, which are usually traditional applications that you might have heard like web servers and stuff like that. But they have also been used for NFV, and we think that they are also a good fit for serverless and in general micro services deployments, either in the cloud or the aids. And we also think that they can also be a good fit for, especially in this case, for ML and AI applications. And that sounds a bit weird because, as we know, ML and AI workloads are quite huge and heavy. So, maybe you have heard about PyTorch, maybe you have heard about TensorFlow. We're not going to touch them, don't worry. But what we want to say here is that they are very, very heavy frameworks, very difficult to add support for them. And secondly, we know that these kind of applications are usually compute-intensive applications that can take a lot of resources. And for that exact reason, we see that there is also a shift in the hardware that exists in the data centers, not only in the data centers, but also in the aids. We see devices that are equipped with a lot of new processing units. Of course, we have the traditional FPGAs and CPUs, but we also have specialized processing units like TPUs and also some ASICs. And first of all, as we know, ML and AI workloads cannot be executed in Unicernals, that's for sure, because there is no support for these frameworks. And secondly, there is no support for hardware acceleration, so there is not really any benefit if we run it in a CPU. So, I will give a smaller, I'm going to go through the acceleration stack and how we can virtualize it with the current approaches. So, in general, what we have, it's pretty simple. Usually, you have an application which is written in an acceleration framework, it can be OpenCL, it can be CUDA, it can be TensorFlow, PyTorps, all of these frameworks. Usually underneath that, you have the operator for the GPU or maybe a runtime for FPGAs. And then you also have, of course, the device driver which resides inside the kernel. So, this is what we have to virtualize. And as we know, Unicernals are virtual machines, so we can use the same techniques that we have for virtual machines, we can also use them in Unicernals. Some of these techniques are hardware partitioning, para-virtualization and remote API. So, in the case of hardware partitioning, the hardware accelerator has the capability to partition itself and we assign this small part of the accelerator to the VM and the VM can access directly to the hardware accelerator. This has very good performance. On the other hand, we need to have the entire acceleration stack inside the VM from the device driver to the application, to the acceleration framework. There is also the case of also, I forgot to mention here, that this is something that it has to be supported from the device and a device driver needs also to be in the VM. And in the case of para-virtualization, these things are getting a bit better because we can have a generic, let's say, device. And then the hypervisor simply manages the accelerator and then we can have the request to the accelerator manage from the hypervisor so we don't need to have all these kind of different drivers for every accelerator inside the VM. On the other hand, we still need to have the vendor runtime and the application and acceleration framework. In the case of remote API, we even have a lighter approach. Everything is managed from the servers. This server might be even locally in the same thing or it can't be a remote server. And what happens here is that the acceleration framework intercepts the calls from the application and forwards them to the acceleration framework that resides on the server. This has some performance overhead, of course, because of the transport that happens. And it's also framework specific. So it has to be supported, like there is remote CUDA, for example, that supports it. So great, but what is the best for unicernals? In the case of hardware partitioning, this means that we have to port the entire software acceleration stack and every device driver to the unicernal, which is not a good and not an easy task. Again, in para-virtualization, things are a bit better. We have to port only maybe one driver, but still we need to port all these acceleration stack. In the case of a remote API, this is something that sounds much more feasible because we can port only, let's say, remote CUDA, only one framework. But how easy is that? And it's not easy because, as I said before, these kind of frameworks are huge. They have very, very big code base. They have dynamic linking, which comes in contrast with the unicernals and a lot of dependencies. So it's not going to be easy to be porting any existing unicernal framework right now. So for that, we think that VXL is suitable for unicernals. So I will give to Tasos to present a bit of how VXL is working. Thank you. Thank you. So hi from my side, too. I'm going to talk a bit about the framework that we're building. So we started working on VXL to actually handle the hardware acceleration virtualization in VMs. So it's not tailored to unicernals. We have been playing with semantically exposing hardware acceleration functionality from hardware acceleration frameworks to VMs. And the software stack is shown in the figure. We use a hardware agnostic API, so we expose the whole function call of the hardware accelerated operation. And we focus on the portability and on interoperability, meaning that the same binary code originating from the application can be executed in many type of architectures, and it is decoupled from the hardware specific implementation. A closer look to the software stack, so we have an application. This application consumes the VXL API, which has specific support, specific operations. These operations are mapped through a mapping layer through VXL RT to the relevant plugins, which are shown in greenish, and they actually are the glue code between the API calls and the hardware specific implementation, which in this figure resides in the external libraries layer. And then it's the hardware where it executes whatever there is in the external libraries. So digging a bit more into how VXL works, so the core library, the core component of VXL exposes the API to the application and maps the API calls to the relevant hardware plugins, which by the way are loaded at runtime. These plugins are actually glue code between the API, the API calls, and the hardware specific implementation. So for example, we have an API call of doing image classification, image inference in general. The only thing that the application needs to submit to VXL is, I want to do image classify, this is the image, this is the model, and the parameters, and blah, blah, blah. And this gets mapped to the relevant plugin implementation. For instance, in this figure, we can use the judgment inference image classification implementation, which translates these arguments and this operation to the actual judgment inference framework provided by NVIDIA that does the image classification operation. Apart from the hardware specific plugins, we also have the transport layer plugins. So imagine this same operation, the image inference, could be executed in a VM using a virtual plugin. So this information, the operation, the arguments, the models, everything will be transferred to the host machine that will use hardware plugin. So apart from the glue code for the hardware specific implementation, we also have the VM plugins. We also, some of the plugins and the API operation support a subset of acceleration frameworks, such as a tensor flow or PyTorch. And what I mentioned earlier about the virtual plugins, so essentially what happens is that the request of the operation and the arguments is forwarded to another instance of the VXL library, either on the hypervisor layer or on a socket interface. So we currently support two modes of operations. We have a VTIO driver and currently we support firecracker and chemo. So we load the driver on the VM. This driver transfers the arguments and the operation to the backend, to the chemo backend or the firecracker backend, which in turn calls the VXL library to do the actual operation. And the other option is using sockets. So we load a socket interface, a socket agent on the host. We have the VSOC plugin on the guest and they communicate over simple sockets. I'm going to hand over to Babi for the Unicernel stuff. So how can VXL be used in Unicernels? Actually, it's quite easy compared to any other acceleration framework that exists. And the thing is that the only thing that we need to do is just have that VXLRT that you see over there. That's the only thing that we need to port. And this is a very, very thin layer of a C code. It can be easily ported to any Unicernel that exists. And we, of course, we need some kind of a transport plugin for forward requests. So as Tasos already explained, usually the application is the same application that we can run in the host or in any container or in any VM can be also used in the Unicernel, the same node changes. And it simply uses a specific API of VXL and then we simply forward the request to the host and then we have another version of VXL which is in the host and simply maps to the hardware accelerator framework that is implementing the specific function. So this, as I said, this allows us to have the same application running either in the host, either in the VM without any changes. So it's easy to debug, easy to execute. And we can also access different kind of hardware, different kind of frameworks that exist. And we don't need to change their application. We can simply change the configuration in the host. So yes, we have another acceleration framework and maybe we can think that this is not going to be easy to use. But let's take an example and see how we can extend the VXL and see if it is easier or not. So let's get a typical vector addition example in OpenCL which can be executed in the CPU or in FPGA. And the steps that usually happen is that we set up the bitstream in the FPGA and the FPGA starts the reconfiguration. Of course, we transfer the data to the FPGA. Then we invoke the kernel as soon as it's ready and we also get the results back to the host. So this is what the application is already doing. So if you have this application already running in your machine, the only thing that you have to do is that somehow you need to libyify the application. And that's instead of just exposing an API to do that. And the next thing is that you can integrate the library in the VXL as a plug-in. And we have a very simplistic API that you can use and therefore the application will be seen as a plug-in for the VXL. Later, you can also update VXL, just adding one more API to the VXLRT so the application can directly use it with the correct parameters, of course. So I will give you a sort of demo of how this works, using Unicrout specifically. I will transfer a bit so we can have maybe in the most classification at first and then we can see how this, how a BLAST CUDA operation can be executed in the CPU and the GPU without any changes. And maybe some FPGA if we have time. Okay, this is not good. This is better. So we are in a typical working environment for Unicraft. We have created our application. We have a new lib which we are not going to use actually. And we have also Unicraft. So let's go to here. So this is a repo that we have created. I will show it to you later. So this is, I want to show you. So here you can see that we only have, we only exposed nine PFS and we use it because we want to transfer the data inside the Unicrout. So we are not going to use any network. We are just going to share a directory with the VM. And the only need that you need to do is to select VXLRT and that's all. As you see, we don't have any libc because we don't need it for the specific example. So these are all the applications that are currently running in Unicraft. You can try them out by yourself. So let's, we're going to use image classification. So we'll take some time, let me, we'll take some time to build. But I will also try to show you how the application looks like as soon as it finishes. And it should finish right now, almost. Okay. And that's application. So as you can see, yeah, we can skip the reading of the file. So this application is quite simple. Like we have a session that we have to create with VXL with the host. Then we simply call the, this is the function that is called VXL image classification. It has the arguments that are also needed. And then we simply release the resources that we have used. So I will try to do an image classification for this beautiful hedgehog that we have here. And let's see what's going to happen. Okay. So all these logs that you see here are from the Deton inference plugin. And we see that we have a hedgehog. So it was identified. And the thing here is to, you can see that all of these logs are not from the Unicraft. All of these logs are from the host that is running. I can also show you this small demo with some operations for arrays using CUDA. So the same here. We're just, we're going to export the backend. First, we're going to use a no op plugin, which simply doesn't do anything. You can mostly maybe use the only 40 bug. So we have here the application, which is a SKM. And you can see that it doesn't do anything because it's just a no op plugin. It doesn't do anything special. So we can change the configuration in the host and specify that the backend that we want to use is the actual CUDA implementation for maybe CPU. Yes. Okay. So then we will run it and you will see that we have the, actually it's a min max operation. It's not a SKM. And then you can also, we will also run the same thing in a GPU. Again, we are just in the host again. We can simply change the configuration and now we start it again, the Unicernal, and we get the result from the GPU. You can also, all these debug messages, you can remove them of course. So we also have the, yes, this is also min max still. Now we will go to SKM. Do we have time still? Yeah. Okay. So yeah, we can just use this. Again, no op, nothing happens. Nothing really special. We will do the export for, to specify the CPU plugin again. And we will execute and we'll see that the execution time, it's quite not very big, but it's just remember that number. And now we will run it in the GPU and you can see here that the execution time is much better than before. And that's all. We can also solve the, the, the FPGA, which is, okay. So this is an FPGA, right? So we need to have a bit stream. And this is a black skulls application by the way. And we will run it natively in the beginning and then we will also run it in the Unicraft. So first we just run the application natively and you can see all of the logs and everything of the execution in the FPGA. And then we can, we will see how this is executed in a Unicernel. So this is, I forgot to solve that, but I will, so it will explain later what are all of these things. Usually what we have to do is just to export the VXL backend that we want to use. That's how we configure the host to use a specific plugin. And then we have the chemo command that I can explain in more details after this video. Still, this is from the Unicernel now and we access the FPGA and we have the black skulls operation running there. And we also have one more FPGA application, but I think you got the point. We have all these links for the videos and everything in our talk in Fosden. So you can also see them from there. Let me talk a bit about chemo, the chemo plugin that we have. This is a bit more, this is just from our Apple. So here we need the chemo which has the vertio backend for VXL. And if Unicraft, for example, had support for Vsoc, we didn't have to use the vertio backend, so we didn't have to modify chemo. But since we have no Vsoc support, then we have to use the vertio, and therefore we changed a bit chemo with adding the backend, as you can see here. And these are all the, you already know from the previous talk, all the configurations for Unicraft, the command line options. I will also show you our docs. We have here an extended documentation. You can find how to run VXL application in VM, how to run it remotely. We also have it, it doesn't show here, but we also have... Okay. Maybe more. Okay, so here we also have all the things that you need to do to try it out by yourself in Unicraft. And all of them are open source. You can check them out, and you can clone them by yourself. So let me return. So currently VXL has bindings for... We actually released the version 0.5, and currently there is bind... We have language bindings for C, C++, Python, Rust, and also for TensorFlow. And we have the plugin API that I talked before about extending VXL. You can also see how it is. These are all the things that we have tested and we support right now. So from the hypervisor perspective, we have support for Chemo over Ritio and Vsoc. And for these new Rust VMMs, like Firecracker, Cloud Hypervisor, and Dragon Ball. Regarding Unicernals, we have working... It's currently working in Unicraft and in Rembrandt, but we want to also port it in OSV and maybe some more Unicernal frameworks. And we also have integration with Kubernetes, Cata containers, and OpenFuzz for serverless deployment. And these are all the acceleration frameworks that we have tested and to work with VXL. So this is an inference that you saw that we did the immense classification. We have TensorFlow and PyTorch support, TensorFlow 13, OpenVino, OpenCylo, CUDA that you saw with the other demo. And regarding hardware, we have tested with GPUs, edge devices like Coral, and also FPGAs. So to sum up, hardware accelerations are... The software stack of hardware accelerators are huge and complicated to be ported easily in Unicernals. And we have VXL which is able to abstract the heterogeneity both in the hardware and in the software. And it sounds like a perfect fit for Unicernals. So if you want, you can try it out by yourselves. Here are all the links that you can use and test them out. And we would like to mention that this work is partially funded from two Horizon projects, Ceraan and 5G Complete. And we would also like to invite you in the Unicraft hackathon that will take place in Athens at the end of March. And thank you for your attention. If you have any questions, we will be happy to answer them. Thank you so much, Babi. So for the third time, we'll welcome you in Athens in late March for the hackathon. If there are any questions from the audience? Yeah, please. Thank you. Great stuff. I have a question about the potential future and the performance that we are currently maybe possibly losing to the usage of API and transport. What do you think is a potential in more increase of performance given that framework? Yeah, actually, the transport is actually, yes, it's bottleneck since you have all these transfers that take place. But we think that at the end, we will have still very good execution times, very good performance. And it's also important to mention that we can also set up the environment and everything so you can minimize the transfers. For example, you can have your model. If you have a TensorFlow model or anything, we are working on how it can be done and prefetching it before you deploy the function in the host and having everything there so you don't have to transfer from the VM to the host and vice versa and all of these things. Actually, if I may intervene, so these are two issues. The first issue is all the resources, the models, the out-of-band stuff that you can do in a separate API, in a cloud environment, in a serverless deployment. And the second thing about the actual transfers for Virtio or Visoc, the thing is that since we semantically abstract the whole operation, you don't have to do a CUDA, MIMCOPY, CUDA malloc, CUDA something, set kernel, whatever, and you don't have this latency in the transfer. So it minimizes the overhead just to the part of copying the data across, so the actual data, the input data and the output. So this is really, really minimal. So in VMs that we have tested, we have tested remotely, but the network is not that good, so we need to do more tests there. But in VMs that we have tested, the overhead is less than 5%. For an image classification of 32K to a MEG, something like that. So it's really, really small, the overhead for the transport layer, both Virtio and Visoc. The Visoc part is a bit more because it serializes the stuff through protobufs and the stack is a bit complicated, but the Virtio stuff is really super efficient. Hi, so thank you for the talk. My question would be kind of almost on the same thing, but from the security perspective. So if we kind of offload a lot of computation out of the Unicernel to the host again, I guess security, at least the isolation is a thing to think about. So if you, any words on this topic? Yeah, you can take it. It's yours. Okay, we agree. Yes, there are issues with security because essentially you need to run on Unicernel to be isolated, and now we push the execution to the host. So one of the things that we have thought about is that when you run that on a cloud environment, the vendor should make sure that whatever application is supported to be run on the host should be secure, should be audited. So the user doesn't have all the possibilities available. They cannot just exec something in the host. They will be able to exec specific stuff that are audited in libraries in the plug-in system. So one approach is this. Another response to the security implications is that at the moment you have no opportunity to run from a Unicernel hardware accelerated workload. So if you want to be able to deploy such an application somewhere, then you can run isolated. You can use the whole hardware accelerator and have the same binary that you would deploy in a non-secure environment. So you could secure the environment, but have this compatibility and software supply mode using a Unicernel, using this semantic abstraction, let's see. Any other question? Yeah. Please. So my question is similar to the first question, but I'm wondering, because you can also do GPU pass-through via KVM and just pass the GPU to a virtual machine. So I'm wondering what is the performance difference between doing that and doing it in VR? Yes. Actually, we want to evaluate that, and we need to evaluate it and see how, for example, with the even pass-through directly, like exposing the whole GPU to the VM, this could be also one baseline for the valuation. Currently, I don't remember if we do have any measurements already. Would you consider the pass-through case the same as made? Yeah, but I mean, if we have any, like, okay. Actually, from GPU virtualization, for example, I'm not sure how many VMs can be supported in one single GPU, for example. I'm not aware of any solution that can scale to, like, tens of VMs, even tens of VMs. I'm not sure if there is any existing solution for that. But, yes, we plan it. We want to do some extended evaluation compared also to some, like, let's say, virtual GPU that exists or even the pass-through and native execution. We want to do that, and hopefully, we can also publish the results in our block. Okay. Thank you. Any other questions? Yeah. So, in response to the first security question about, yeah, we are offloading now compute to the hypervisor and host. So, does it imply that there is a possibility to break out of the containerization with BXL? Well, there's, yes, yes, code is going to be executing on the host in a privileged level. Yes. But the other option is what? So, yeah, there is a trade. We are actually working. We want to see what available sources we have there. How can we make it more secure? How we can sandbox it somehow to make it look better? But on the other hand, like, for example, in FPTAs, there's no MMU, there's nothing. If you run two kernels, one kernel can access, if you kind of know what to do, one kernel can access all the memory in the whole FPTAs, for example. So, in one hand, you also need support from the hardware. And regarding, for example, the software stack, we are looking at it and see how this can, how can we extend and make it more, at least, increase the difficulty for having any. So, for example, in the Cata containers integration that we have, so when you spawn a container, you sandbox the container in a VM, our agent, the host part of the Excel is running on the same sandbox, not in the VM, outside the VM. But it runs in the sandbox. So, yes, there is code executing on the host, but it's in the sandbox. So, it's kind of a tradeoff. Anything else? Right? If not, thank you, Anastasia. Thank you, Babis. Yeah.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.56, "text": " Hi, everyone. So it's my pleasure to introduce Babis and Anastasios. They're going to give", "tokens": [2421, 11, 1518, 13, 407, 309, 311, 452, 6834, 281, 5366, 15820, 271, 293, 1107, 525, 296, 2717, 13, 814, 434, 516, 281, 976], "temperature": 0.0, "avg_logprob": -0.25947235107421873, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.3828885555267334}, {"id": 1, "seek": 0, "start": 9.56, "end": 15.48, "text": " you the talk on using VXL for hard acceleration in your kernels. Babis, please.", "tokens": [291, 264, 751, 322, 1228, 691, 55, 43, 337, 1152, 17162, 294, 428, 23434, 1625, 13, 15820, 271, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.25947235107421873, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.3828885555267334}, {"id": 2, "seek": 0, "start": 15.48, "end": 20.2, "text": " So hello, everyone. I'm Babis. My actual name is Haraldos Minus, but you can just call me", "tokens": [407, 7751, 11, 1518, 13, 286, 478, 15820, 271, 13, 1222, 3539, 1315, 307, 3653, 3976, 329, 2829, 301, 11, 457, 291, 393, 445, 818, 385], "temperature": 0.0, "avg_logprob": -0.25947235107421873, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.3828885555267334}, {"id": 3, "seek": 0, "start": 20.2, "end": 26.92, "text": " Babis. So we're going to give a talk about hardware acceleration and our effort to having", "tokens": [15820, 271, 13, 407, 321, 434, 516, 281, 976, 257, 751, 466, 8837, 17162, 293, 527, 4630, 281, 1419], "temperature": 0.0, "avg_logprob": -0.25947235107421873, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.3828885555267334}, {"id": 4, "seek": 2692, "start": 26.92, "end": 33.92, "text": " some support in the unit kernels, and we do that with VXL. So, yeah.", "tokens": [512, 1406, 294, 264, 4985, 23434, 1625, 11, 293, 321, 360, 300, 365, 691, 55, 43, 13, 407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.30470194135393414, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.0018542901379987597}, {"id": 5, "seek": 2692, "start": 33.92, "end": 36.92, "text": " Yeah, Kim, oh, sorry. I forgot about that.", "tokens": [865, 11, 5652, 11, 1954, 11, 2597, 13, 286, 5298, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.30470194135393414, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.0018542901379987597}, {"id": 6, "seek": 2692, "start": 36.92, "end": 37.92, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.30470194135393414, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.0018542901379987597}, {"id": 7, "seek": 2692, "start": 37.92, "end": 44.92, "text": " Oh, let's forget. Yeah, put that over here over there, and maybe you can just keep it here.", "tokens": [876, 11, 718, 311, 2870, 13, 865, 11, 829, 300, 670, 510, 670, 456, 11, 293, 1310, 291, 393, 445, 1066, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.30470194135393414, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.0018542901379987597}, {"id": 8, "seek": 2692, "start": 44.92, "end": 52.040000000000006, "text": " Okay. So, yeah, we already heard from Simon, so we don't have to repeat what the unit kernels", "tokens": [1033, 13, 407, 11, 1338, 11, 321, 1217, 2198, 490, 13193, 11, 370, 321, 500, 380, 362, 281, 7149, 437, 264, 4985, 23434, 1625], "temperature": 0.0, "avg_logprob": -0.30470194135393414, "compression_ratio": 1.566326530612245, "no_speech_prob": 0.0018542901379987597}, {"id": 9, "seek": 5204, "start": 52.04, "end": 58.6, "text": " are. There are a lot of projects, and we know that they are promising. It's a promising technology.", "tokens": [366, 13, 821, 366, 257, 688, 295, 4455, 11, 293, 321, 458, 300, 436, 366, 20257, 13, 467, 311, 257, 20257, 2899, 13], "temperature": 0.0, "avg_logprob": -0.13470032048779865, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0009623935911804438}, {"id": 10, "seek": 5204, "start": 58.6, "end": 66.28, "text": " We can have very fast boot times, low memory footprint, and some increased security. We", "tokens": [492, 393, 362, 588, 2370, 11450, 1413, 11, 2295, 4675, 24222, 11, 293, 512, 6505, 3825, 13, 492], "temperature": 0.0, "avg_logprob": -0.13470032048779865, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0009623935911804438}, {"id": 11, "seek": 5204, "start": 66.28, "end": 71.56, "text": " also know some of the use cases for unit kernels, which are usually traditional applications", "tokens": [611, 458, 512, 295, 264, 764, 3331, 337, 4985, 23434, 1625, 11, 597, 366, 2673, 5164, 5821], "temperature": 0.0, "avg_logprob": -0.13470032048779865, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0009623935911804438}, {"id": 12, "seek": 5204, "start": 71.56, "end": 76.88, "text": " that you might have heard like web servers and stuff like that. But they have also been", "tokens": [300, 291, 1062, 362, 2198, 411, 3670, 15909, 293, 1507, 411, 300, 13, 583, 436, 362, 611, 668], "temperature": 0.0, "avg_logprob": -0.13470032048779865, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0009623935911804438}, {"id": 13, "seek": 7688, "start": 76.88, "end": 82.11999999999999, "text": " used for NFV, and we think that they are also a good fit for serverless and in general micro", "tokens": [1143, 337, 13576, 53, 11, 293, 321, 519, 300, 436, 366, 611, 257, 665, 3318, 337, 7154, 1832, 293, 294, 2674, 4532], "temperature": 0.0, "avg_logprob": -0.17191307361309344, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.00012999348109588027}, {"id": 14, "seek": 7688, "start": 82.11999999999999, "end": 88.47999999999999, "text": " services deployments, either in the cloud or the aids. And we also think that they can", "tokens": [3328, 7274, 1117, 11, 2139, 294, 264, 4588, 420, 264, 28447, 13, 400, 321, 611, 519, 300, 436, 393], "temperature": 0.0, "avg_logprob": -0.17191307361309344, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.00012999348109588027}, {"id": 15, "seek": 7688, "start": 88.47999999999999, "end": 93.96, "text": " also be a good fit for, especially in this case, for ML and AI applications. And that", "tokens": [611, 312, 257, 665, 3318, 337, 11, 2318, 294, 341, 1389, 11, 337, 21601, 293, 7318, 5821, 13, 400, 300], "temperature": 0.0, "avg_logprob": -0.17191307361309344, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.00012999348109588027}, {"id": 16, "seek": 7688, "start": 93.96, "end": 101.72, "text": " sounds a bit weird because, as we know, ML and AI workloads are quite huge and heavy.", "tokens": [3263, 257, 857, 3657, 570, 11, 382, 321, 458, 11, 21601, 293, 7318, 32452, 366, 1596, 2603, 293, 4676, 13], "temperature": 0.0, "avg_logprob": -0.17191307361309344, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.00012999348109588027}, {"id": 17, "seek": 10172, "start": 101.72, "end": 106.92, "text": " So, maybe you have heard about PyTorch, maybe you have heard about TensorFlow. We're not", "tokens": [407, 11, 1310, 291, 362, 2198, 466, 9953, 51, 284, 339, 11, 1310, 291, 362, 2198, 466, 37624, 13, 492, 434, 406], "temperature": 0.0, "avg_logprob": -0.16158386639186315, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00019901455380022526}, {"id": 18, "seek": 10172, "start": 106.92, "end": 112.32, "text": " going to touch them, don't worry. But what we want to say here is that they are very,", "tokens": [516, 281, 2557, 552, 11, 500, 380, 3292, 13, 583, 437, 321, 528, 281, 584, 510, 307, 300, 436, 366, 588, 11], "temperature": 0.0, "avg_logprob": -0.16158386639186315, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00019901455380022526}, {"id": 19, "seek": 10172, "start": 112.32, "end": 117.96000000000001, "text": " very heavy frameworks, very difficult to add support for them. And secondly, we know that", "tokens": [588, 4676, 29834, 11, 588, 2252, 281, 909, 1406, 337, 552, 13, 400, 26246, 11, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.16158386639186315, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00019901455380022526}, {"id": 20, "seek": 10172, "start": 117.96000000000001, "end": 127.0, "text": " these kind of applications are usually compute-intensive applications that can take a lot of resources.", "tokens": [613, 733, 295, 5821, 366, 2673, 14722, 12, 686, 2953, 5821, 300, 393, 747, 257, 688, 295, 3593, 13], "temperature": 0.0, "avg_logprob": -0.16158386639186315, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00019901455380022526}, {"id": 21, "seek": 10172, "start": 127.0, "end": 131.28, "text": " And for that exact reason, we see that there is also a shift in the hardware that exists", "tokens": [400, 337, 300, 1900, 1778, 11, 321, 536, 300, 456, 307, 611, 257, 5513, 294, 264, 8837, 300, 8198], "temperature": 0.0, "avg_logprob": -0.16158386639186315, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00019901455380022526}, {"id": 22, "seek": 13128, "start": 131.28, "end": 136.76, "text": " in the data centers, not only in the data centers, but also in the aids. We see devices", "tokens": [294, 264, 1412, 10898, 11, 406, 787, 294, 264, 1412, 10898, 11, 457, 611, 294, 264, 28447, 13, 492, 536, 5759], "temperature": 0.0, "avg_logprob": -0.17275089687771267, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00017077136726584285}, {"id": 23, "seek": 13128, "start": 136.76, "end": 142.2, "text": " that are equipped with a lot of new processing units. Of course, we have the traditional", "tokens": [300, 366, 15218, 365, 257, 688, 295, 777, 9007, 6815, 13, 2720, 1164, 11, 321, 362, 264, 5164], "temperature": 0.0, "avg_logprob": -0.17275089687771267, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00017077136726584285}, {"id": 24, "seek": 13128, "start": 142.2, "end": 152.8, "text": " FPGAs and CPUs, but we also have specialized processing units like TPUs and also some ASICs.", "tokens": [36655, 38, 10884, 293, 13199, 82, 11, 457, 321, 611, 362, 19813, 9007, 6815, 411, 314, 8115, 82, 293, 611, 512, 7469, 2532, 82, 13], "temperature": 0.0, "avg_logprob": -0.17275089687771267, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00017077136726584285}, {"id": 25, "seek": 13128, "start": 152.8, "end": 159.8, "text": " And first of all, as we know, ML and AI workloads cannot be executed in Unicernals, that's for", "tokens": [400, 700, 295, 439, 11, 382, 321, 458, 11, 21601, 293, 7318, 32452, 2644, 312, 17577, 294, 1156, 299, 1248, 1124, 11, 300, 311, 337], "temperature": 0.0, "avg_logprob": -0.17275089687771267, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00017077136726584285}, {"id": 26, "seek": 15980, "start": 159.8, "end": 164.0, "text": " sure, because there is no support for these frameworks. And secondly, there is no support", "tokens": [988, 11, 570, 456, 307, 572, 1406, 337, 613, 29834, 13, 400, 26246, 11, 456, 307, 572, 1406], "temperature": 0.0, "avg_logprob": -0.16302361271598123, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00040517374873161316}, {"id": 27, "seek": 15980, "start": 164.0, "end": 172.36, "text": " for hardware acceleration, so there is not really any benefit if we run it in a CPU.", "tokens": [337, 8837, 17162, 11, 370, 456, 307, 406, 534, 604, 5121, 498, 321, 1190, 309, 294, 257, 13199, 13], "temperature": 0.0, "avg_logprob": -0.16302361271598123, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00040517374873161316}, {"id": 28, "seek": 15980, "start": 172.36, "end": 181.96, "text": " So, I will give a smaller, I'm going to go through the acceleration stack and how we", "tokens": [407, 11, 286, 486, 976, 257, 4356, 11, 286, 478, 516, 281, 352, 807, 264, 17162, 8630, 293, 577, 321], "temperature": 0.0, "avg_logprob": -0.16302361271598123, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00040517374873161316}, {"id": 29, "seek": 15980, "start": 181.96, "end": 188.36, "text": " can virtualize it with the current approaches. So, in general, what we have, it's pretty", "tokens": [393, 6374, 1125, 309, 365, 264, 2190, 11587, 13, 407, 11, 294, 2674, 11, 437, 321, 362, 11, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.16302361271598123, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00040517374873161316}, {"id": 30, "seek": 18836, "start": 188.36, "end": 192.64000000000001, "text": " simple. Usually, you have an application which is written in an acceleration framework,", "tokens": [2199, 13, 11419, 11, 291, 362, 364, 3861, 597, 307, 3720, 294, 364, 17162, 8388, 11], "temperature": 0.0, "avg_logprob": -0.20194525359779275, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.00018902725423686206}, {"id": 31, "seek": 18836, "start": 192.64000000000001, "end": 198.48000000000002, "text": " it can be OpenCL, it can be CUDA, it can be TensorFlow, PyTorps, all of these frameworks.", "tokens": [309, 393, 312, 7238, 22458, 11, 309, 393, 312, 29777, 7509, 11, 309, 393, 312, 37624, 11, 9953, 51, 284, 1878, 11, 439, 295, 613, 29834, 13], "temperature": 0.0, "avg_logprob": -0.20194525359779275, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.00018902725423686206}, {"id": 32, "seek": 18836, "start": 198.48000000000002, "end": 206.28000000000003, "text": " Usually underneath that, you have the operator for the GPU or maybe a runtime for FPGAs.", "tokens": [11419, 7223, 300, 11, 291, 362, 264, 12973, 337, 264, 18407, 420, 1310, 257, 34474, 337, 36655, 38, 10884, 13], "temperature": 0.0, "avg_logprob": -0.20194525359779275, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.00018902725423686206}, {"id": 33, "seek": 18836, "start": 206.28000000000003, "end": 212.24, "text": " And then you also have, of course, the device driver which resides inside the kernel. So,", "tokens": [400, 550, 291, 611, 362, 11, 295, 1164, 11, 264, 4302, 6787, 597, 47157, 1854, 264, 28256, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.20194525359779275, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.00018902725423686206}, {"id": 34, "seek": 21224, "start": 212.24, "end": 218.84, "text": " this is what we have to virtualize. And as we know, Unicernals are virtual machines,", "tokens": [341, 307, 437, 321, 362, 281, 6374, 1125, 13, 400, 382, 321, 458, 11, 1156, 299, 1248, 1124, 366, 6374, 8379, 11], "temperature": 0.0, "avg_logprob": -0.142504268222385, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.00016050807607825845}, {"id": 35, "seek": 21224, "start": 218.84, "end": 222.28, "text": " so we can use the same techniques that we have for virtual machines, we can also use", "tokens": [370, 321, 393, 764, 264, 912, 7512, 300, 321, 362, 337, 6374, 8379, 11, 321, 393, 611, 764], "temperature": 0.0, "avg_logprob": -0.142504268222385, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.00016050807607825845}, {"id": 36, "seek": 21224, "start": 222.28, "end": 228.52, "text": " them in Unicernals. Some of these techniques are hardware partitioning, para-virtualization", "tokens": [552, 294, 1156, 299, 1248, 1124, 13, 2188, 295, 613, 7512, 366, 8837, 24808, 278, 11, 1690, 12, 85, 2498, 901, 2144], "temperature": 0.0, "avg_logprob": -0.142504268222385, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.00016050807607825845}, {"id": 37, "seek": 21224, "start": 228.52, "end": 237.76000000000002, "text": " and remote API. So, in the case of hardware partitioning, the hardware accelerator has", "tokens": [293, 8607, 9362, 13, 407, 11, 294, 264, 1389, 295, 8837, 24808, 278, 11, 264, 8837, 39889, 575], "temperature": 0.0, "avg_logprob": -0.142504268222385, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.00016050807607825845}, {"id": 38, "seek": 23776, "start": 237.76, "end": 245.76, "text": " the capability to partition itself and we assign this small part of the accelerator", "tokens": [264, 13759, 281, 24808, 2564, 293, 321, 6269, 341, 1359, 644, 295, 264, 39889], "temperature": 0.0, "avg_logprob": -0.14871209546139366, "compression_ratio": 1.76, "no_speech_prob": 6.666207627858967e-05}, {"id": 39, "seek": 23776, "start": 245.76, "end": 252.48, "text": " to the VM and the VM can access directly to the hardware accelerator. This has very", "tokens": [281, 264, 18038, 293, 264, 18038, 393, 2105, 3838, 281, 264, 8837, 39889, 13, 639, 575, 588], "temperature": 0.0, "avg_logprob": -0.14871209546139366, "compression_ratio": 1.76, "no_speech_prob": 6.666207627858967e-05}, {"id": 40, "seek": 23776, "start": 252.48, "end": 256.8, "text": " good performance. On the other hand, we need to have the entire acceleration stack inside", "tokens": [665, 3389, 13, 1282, 264, 661, 1011, 11, 321, 643, 281, 362, 264, 2302, 17162, 8630, 1854], "temperature": 0.0, "avg_logprob": -0.14871209546139366, "compression_ratio": 1.76, "no_speech_prob": 6.666207627858967e-05}, {"id": 41, "seek": 23776, "start": 256.8, "end": 265.76, "text": " the VM from the device driver to the application, to the acceleration framework. There is also", "tokens": [264, 18038, 490, 264, 4302, 6787, 281, 264, 3861, 11, 281, 264, 17162, 8388, 13, 821, 307, 611], "temperature": 0.0, "avg_logprob": -0.14871209546139366, "compression_ratio": 1.76, "no_speech_prob": 6.666207627858967e-05}, {"id": 42, "seek": 26576, "start": 265.76, "end": 270.84, "text": " the case of also, I forgot to mention here, that this is something that it has to be supported", "tokens": [264, 1389, 295, 611, 11, 286, 5298, 281, 2152, 510, 11, 300, 341, 307, 746, 300, 309, 575, 281, 312, 8104], "temperature": 0.0, "avg_logprob": -0.16286256465506047, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00016314706590492278}, {"id": 43, "seek": 26576, "start": 270.84, "end": 278.24, "text": " from the device and a device driver needs also to be in the VM. And in the case of para-virtualization,", "tokens": [490, 264, 4302, 293, 257, 4302, 6787, 2203, 611, 281, 312, 294, 264, 18038, 13, 400, 294, 264, 1389, 295, 1690, 12, 85, 2498, 901, 2144, 11], "temperature": 0.0, "avg_logprob": -0.16286256465506047, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00016314706590492278}, {"id": 44, "seek": 26576, "start": 278.24, "end": 284.32, "text": " these things are getting a bit better because we can have a generic, let's say, device.", "tokens": [613, 721, 366, 1242, 257, 857, 1101, 570, 321, 393, 362, 257, 19577, 11, 718, 311, 584, 11, 4302, 13], "temperature": 0.0, "avg_logprob": -0.16286256465506047, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00016314706590492278}, {"id": 45, "seek": 26576, "start": 284.32, "end": 294.12, "text": " And then the hypervisor simply manages the accelerator and then we can have the request", "tokens": [400, 550, 264, 9848, 16457, 2935, 22489, 264, 39889, 293, 550, 321, 393, 362, 264, 5308], "temperature": 0.0, "avg_logprob": -0.16286256465506047, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00016314706590492278}, {"id": 46, "seek": 29412, "start": 294.12, "end": 298.84000000000003, "text": " to the accelerator manage from the hypervisor so we don't need to have all these kind of", "tokens": [281, 264, 39889, 3067, 490, 264, 9848, 16457, 370, 321, 500, 380, 643, 281, 362, 439, 613, 733, 295], "temperature": 0.0, "avg_logprob": -0.20209317207336425, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.00013937977200839669}, {"id": 47, "seek": 29412, "start": 298.84000000000003, "end": 304.52, "text": " different drivers for every accelerator inside the VM. On the other hand, we still need to", "tokens": [819, 11590, 337, 633, 39889, 1854, 264, 18038, 13, 1282, 264, 661, 1011, 11, 321, 920, 643, 281], "temperature": 0.0, "avg_logprob": -0.20209317207336425, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.00013937977200839669}, {"id": 48, "seek": 29412, "start": 304.52, "end": 310.52, "text": " have the vendor runtime and the application and acceleration framework. In the case of", "tokens": [362, 264, 24321, 34474, 293, 264, 3861, 293, 17162, 8388, 13, 682, 264, 1389, 295], "temperature": 0.0, "avg_logprob": -0.20209317207336425, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.00013937977200839669}, {"id": 49, "seek": 29412, "start": 310.52, "end": 318.72, "text": " remote API, we even have a lighter approach. Everything is managed from the servers. This", "tokens": [8607, 9362, 11, 321, 754, 362, 257, 11546, 3109, 13, 5471, 307, 6453, 490, 264, 15909, 13, 639], "temperature": 0.0, "avg_logprob": -0.20209317207336425, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.00013937977200839669}, {"id": 50, "seek": 31872, "start": 318.72, "end": 324.32000000000005, "text": " server might be even locally in the same thing or it can't be a remote server. And what happens", "tokens": [7154, 1062, 312, 754, 16143, 294, 264, 912, 551, 420, 309, 393, 380, 312, 257, 8607, 7154, 13, 400, 437, 2314], "temperature": 0.0, "avg_logprob": -0.20758490964590784, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.00022362067829817533}, {"id": 51, "seek": 31872, "start": 324.32000000000005, "end": 332.20000000000005, "text": " here is that the acceleration framework intercepts the calls from the application and forwards", "tokens": [510, 307, 300, 264, 17162, 8388, 24700, 82, 264, 5498, 490, 264, 3861, 293, 30126], "temperature": 0.0, "avg_logprob": -0.20758490964590784, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.00022362067829817533}, {"id": 52, "seek": 31872, "start": 332.20000000000005, "end": 339.96000000000004, "text": " them to the acceleration framework that resides on the server. This has some performance overhead,", "tokens": [552, 281, 264, 17162, 8388, 300, 47157, 322, 264, 7154, 13, 639, 575, 512, 3389, 19922, 11], "temperature": 0.0, "avg_logprob": -0.20758490964590784, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.00022362067829817533}, {"id": 53, "seek": 31872, "start": 339.96000000000004, "end": 346.76000000000005, "text": " of course, because of the transport that happens. And it's also framework specific. So it has", "tokens": [295, 1164, 11, 570, 295, 264, 5495, 300, 2314, 13, 400, 309, 311, 611, 8388, 2685, 13, 407, 309, 575], "temperature": 0.0, "avg_logprob": -0.20758490964590784, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.00022362067829817533}, {"id": 54, "seek": 34676, "start": 346.76, "end": 354.2, "text": " to be supported, like there is remote CUDA, for example, that supports it. So great, but", "tokens": [281, 312, 8104, 11, 411, 456, 307, 8607, 29777, 7509, 11, 337, 1365, 11, 300, 9346, 309, 13, 407, 869, 11, 457], "temperature": 0.0, "avg_logprob": -0.18488092182063254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00028101744828745723}, {"id": 55, "seek": 34676, "start": 354.2, "end": 359.2, "text": " what is the best for unicernals? In the case of hardware partitioning, this means that", "tokens": [437, 307, 264, 1151, 337, 517, 299, 1248, 1124, 30, 682, 264, 1389, 295, 8837, 24808, 278, 11, 341, 1355, 300], "temperature": 0.0, "avg_logprob": -0.18488092182063254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00028101744828745723}, {"id": 56, "seek": 34676, "start": 359.2, "end": 364.59999999999997, "text": " we have to port the entire software acceleration stack and every device driver to the unicernal,", "tokens": [321, 362, 281, 2436, 264, 2302, 4722, 17162, 8630, 293, 633, 4302, 6787, 281, 264, 517, 299, 1248, 304, 11], "temperature": 0.0, "avg_logprob": -0.18488092182063254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00028101744828745723}, {"id": 57, "seek": 34676, "start": 364.59999999999997, "end": 369.56, "text": " which is not a good and not an easy task. Again, in para-virtualization, things are", "tokens": [597, 307, 406, 257, 665, 293, 406, 364, 1858, 5633, 13, 3764, 11, 294, 1690, 12, 85, 2498, 901, 2144, 11, 721, 366], "temperature": 0.0, "avg_logprob": -0.18488092182063254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00028101744828745723}, {"id": 58, "seek": 34676, "start": 369.56, "end": 375.0, "text": " a bit better. We have to port only maybe one driver, but still we need to port all these", "tokens": [257, 857, 1101, 13, 492, 362, 281, 2436, 787, 1310, 472, 6787, 11, 457, 920, 321, 643, 281, 2436, 439, 613], "temperature": 0.0, "avg_logprob": -0.18488092182063254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00028101744828745723}, {"id": 59, "seek": 37500, "start": 375.0, "end": 379.4, "text": " acceleration stack. In the case of a remote API, this is something that sounds much more", "tokens": [17162, 8630, 13, 682, 264, 1389, 295, 257, 8607, 9362, 11, 341, 307, 746, 300, 3263, 709, 544], "temperature": 0.0, "avg_logprob": -0.16740269713349396, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0003221773076802492}, {"id": 60, "seek": 37500, "start": 379.4, "end": 386.96, "text": " feasible because we can port only, let's say, remote CUDA, only one framework. But how easy", "tokens": [26648, 570, 321, 393, 2436, 787, 11, 718, 311, 584, 11, 8607, 29777, 7509, 11, 787, 472, 8388, 13, 583, 577, 1858], "temperature": 0.0, "avg_logprob": -0.16740269713349396, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0003221773076802492}, {"id": 61, "seek": 37500, "start": 386.96, "end": 393.04, "text": " is that? And it's not easy because, as I said before, these kind of frameworks are huge.", "tokens": [307, 300, 30, 400, 309, 311, 406, 1858, 570, 11, 382, 286, 848, 949, 11, 613, 733, 295, 29834, 366, 2603, 13], "temperature": 0.0, "avg_logprob": -0.16740269713349396, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0003221773076802492}, {"id": 62, "seek": 37500, "start": 393.04, "end": 404.16, "text": " They have very, very big code base. They have dynamic linking, which comes in contrast with", "tokens": [814, 362, 588, 11, 588, 955, 3089, 3096, 13, 814, 362, 8546, 25775, 11, 597, 1487, 294, 8712, 365], "temperature": 0.0, "avg_logprob": -0.16740269713349396, "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.0003221773076802492}, {"id": 63, "seek": 40416, "start": 404.16, "end": 411.8, "text": " the unicernals and a lot of dependencies. So it's not going to be easy to be porting", "tokens": [264, 517, 299, 1248, 1124, 293, 257, 688, 295, 36606, 13, 407, 309, 311, 406, 516, 281, 312, 1858, 281, 312, 2436, 278], "temperature": 0.0, "avg_logprob": -0.18295434805063102, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.00036960290162824094}, {"id": 64, "seek": 40416, "start": 411.8, "end": 422.20000000000005, "text": " any existing unicernal framework right now. So for that, we think that VXL is suitable", "tokens": [604, 6741, 517, 299, 1248, 304, 8388, 558, 586, 13, 407, 337, 300, 11, 321, 519, 300, 691, 55, 43, 307, 12873], "temperature": 0.0, "avg_logprob": -0.18295434805063102, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.00036960290162824094}, {"id": 65, "seek": 40416, "start": 422.20000000000005, "end": 430.12, "text": " for unicernals. So I will give to Tasos to present a bit of how VXL is working.", "tokens": [337, 517, 299, 1248, 1124, 13, 407, 286, 486, 976, 281, 27293, 329, 281, 1974, 257, 857, 295, 577, 691, 55, 43, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.18295434805063102, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.00036960290162824094}, {"id": 66, "seek": 43012, "start": 430.12, "end": 447.64, "text": " Thank you. Thank you. So hi from my side, too. I'm going to talk a bit about the framework", "tokens": [1044, 291, 13, 1044, 291, 13, 407, 4879, 490, 452, 1252, 11, 886, 13, 286, 478, 516, 281, 751, 257, 857, 466, 264, 8388], "temperature": 0.0, "avg_logprob": -0.21904044248619858, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.0026269820518791676}, {"id": 67, "seek": 43012, "start": 447.64, "end": 459.12, "text": " that we're building. So we started working on VXL to actually handle the hardware acceleration", "tokens": [300, 321, 434, 2390, 13, 407, 321, 1409, 1364, 322, 691, 55, 43, 281, 767, 4813, 264, 8837, 17162], "temperature": 0.0, "avg_logprob": -0.21904044248619858, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.0026269820518791676}, {"id": 68, "seek": 45912, "start": 459.12, "end": 468.28000000000003, "text": " virtualization in VMs. So it's not tailored to unicernals. We have been playing with semantically", "tokens": [6374, 2144, 294, 18038, 82, 13, 407, 309, 311, 406, 34858, 281, 517, 299, 1248, 1124, 13, 492, 362, 668, 2433, 365, 4361, 49505], "temperature": 0.0, "avg_logprob": -0.12743649338230942, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0008214379195123911}, {"id": 69, "seek": 45912, "start": 468.28000000000003, "end": 476.84000000000003, "text": " exposing hardware acceleration functionality from hardware acceleration frameworks to VMs.", "tokens": [33178, 8837, 17162, 14980, 490, 8837, 17162, 29834, 281, 18038, 82, 13], "temperature": 0.0, "avg_logprob": -0.12743649338230942, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0008214379195123911}, {"id": 70, "seek": 45912, "start": 476.84000000000003, "end": 487.52, "text": " And the software stack is shown in the figure. We use a hardware agnostic API, so we expose", "tokens": [400, 264, 4722, 8630, 307, 4898, 294, 264, 2573, 13, 492, 764, 257, 8837, 623, 77, 19634, 9362, 11, 370, 321, 19219], "temperature": 0.0, "avg_logprob": -0.12743649338230942, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0008214379195123911}, {"id": 71, "seek": 48752, "start": 487.52, "end": 497.68, "text": " the whole function call of the hardware accelerated operation. And we focus on the portability", "tokens": [264, 1379, 2445, 818, 295, 264, 8837, 29763, 6916, 13, 400, 321, 1879, 322, 264, 2436, 2310], "temperature": 0.0, "avg_logprob": -0.14722136528261245, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0009313276968896389}, {"id": 72, "seek": 48752, "start": 497.68, "end": 505.68, "text": " and on interoperability, meaning that the same binary code originating from the application", "tokens": [293, 322, 728, 7192, 2310, 11, 3620, 300, 264, 912, 17434, 3089, 4957, 990, 490, 264, 3861], "temperature": 0.0, "avg_logprob": -0.14722136528261245, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0009313276968896389}, {"id": 73, "seek": 48752, "start": 505.68, "end": 512.36, "text": " can be executed in many type of architectures, and it is decoupled from the hardware specific", "tokens": [393, 312, 17577, 294, 867, 2010, 295, 6331, 1303, 11, 293, 309, 307, 979, 263, 15551, 490, 264, 8837, 2685], "temperature": 0.0, "avg_logprob": -0.14722136528261245, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0009313276968896389}, {"id": 74, "seek": 51236, "start": 512.36, "end": 520.84, "text": " implementation. A closer look to the software stack, so we have an application. This application", "tokens": [11420, 13, 316, 4966, 574, 281, 264, 4722, 8630, 11, 370, 321, 362, 364, 3861, 13, 639, 3861], "temperature": 0.0, "avg_logprob": -0.16323338785479147, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0004146687570028007}, {"id": 75, "seek": 51236, "start": 520.84, "end": 531.96, "text": " consumes the VXL API, which has specific support, specific operations. These operations are", "tokens": [48823, 264, 691, 55, 43, 9362, 11, 597, 575, 2685, 1406, 11, 2685, 7705, 13, 1981, 7705, 366], "temperature": 0.0, "avg_logprob": -0.16323338785479147, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0004146687570028007}, {"id": 76, "seek": 51236, "start": 531.96, "end": 541.5600000000001, "text": " mapped through a mapping layer through VXL RT to the relevant plugins, which are shown", "tokens": [33318, 807, 257, 18350, 4583, 807, 691, 55, 43, 21797, 281, 264, 7340, 33759, 11, 597, 366, 4898], "temperature": 0.0, "avg_logprob": -0.16323338785479147, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.0004146687570028007}, {"id": 77, "seek": 54156, "start": 541.56, "end": 550.56, "text": " in greenish, and they actually are the glue code between the API calls and the hardware", "tokens": [294, 3092, 742, 11, 293, 436, 767, 366, 264, 8998, 3089, 1296, 264, 9362, 5498, 293, 264, 8837], "temperature": 0.0, "avg_logprob": -0.15070901772914788, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.0005585930193774402}, {"id": 78, "seek": 54156, "start": 550.56, "end": 559.4799999999999, "text": " specific implementation, which in this figure resides in the external libraries layer. And", "tokens": [2685, 11420, 11, 597, 294, 341, 2573, 47157, 294, 264, 8320, 15148, 4583, 13, 400], "temperature": 0.0, "avg_logprob": -0.15070901772914788, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.0005585930193774402}, {"id": 79, "seek": 55948, "start": 559.48, "end": 572.28, "text": " then it's the hardware where it executes whatever there is in the external libraries. So digging", "tokens": [550, 309, 311, 264, 8837, 689, 309, 4454, 1819, 2035, 456, 307, 294, 264, 8320, 15148, 13, 407, 17343], "temperature": 0.0, "avg_logprob": -0.09673680091390804, "compression_ratio": 1.378787878787879, "no_speech_prob": 7.671065395697951e-05}, {"id": 80, "seek": 55948, "start": 572.28, "end": 584.6800000000001, "text": " a bit more into how VXL works, so the core library, the core component of VXL exposes", "tokens": [257, 857, 544, 666, 577, 691, 55, 43, 1985, 11, 370, 264, 4965, 6405, 11, 264, 4965, 6542, 295, 691, 55, 43, 1278, 4201], "temperature": 0.0, "avg_logprob": -0.09673680091390804, "compression_ratio": 1.378787878787879, "no_speech_prob": 7.671065395697951e-05}, {"id": 81, "seek": 58468, "start": 584.68, "end": 592.5999999999999, "text": " the API to the application and maps the API calls to the relevant hardware plugins, which", "tokens": [264, 9362, 281, 264, 3861, 293, 11317, 264, 9362, 5498, 281, 264, 7340, 8837, 33759, 11, 597], "temperature": 0.0, "avg_logprob": -0.15329019725322723, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00013194064376875758}, {"id": 82, "seek": 58468, "start": 592.5999999999999, "end": 603.1999999999999, "text": " by the way are loaded at runtime. These plugins are actually glue code between the API, the", "tokens": [538, 264, 636, 366, 13210, 412, 34474, 13, 1981, 33759, 366, 767, 8998, 3089, 1296, 264, 9362, 11, 264], "temperature": 0.0, "avg_logprob": -0.15329019725322723, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00013194064376875758}, {"id": 83, "seek": 58468, "start": 603.1999999999999, "end": 610.68, "text": " API calls, and the hardware specific implementation. So for example, we have an API call of doing", "tokens": [9362, 5498, 11, 293, 264, 8837, 2685, 11420, 13, 407, 337, 1365, 11, 321, 362, 364, 9362, 818, 295, 884], "temperature": 0.0, "avg_logprob": -0.15329019725322723, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00013194064376875758}, {"id": 84, "seek": 61068, "start": 610.68, "end": 616.04, "text": " image classification, image inference in general. The only thing that the application needs", "tokens": [3256, 21538, 11, 3256, 38253, 294, 2674, 13, 440, 787, 551, 300, 264, 3861, 2203], "temperature": 0.0, "avg_logprob": -0.19499527324329724, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.000474300526548177}, {"id": 85, "seek": 61068, "start": 616.04, "end": 625.0, "text": " to submit to VXL is, I want to do image classify, this is the image, this is the model, and", "tokens": [281, 10315, 281, 691, 55, 43, 307, 11, 286, 528, 281, 360, 3256, 33872, 11, 341, 307, 264, 3256, 11, 341, 307, 264, 2316, 11, 293], "temperature": 0.0, "avg_logprob": -0.19499527324329724, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.000474300526548177}, {"id": 86, "seek": 61068, "start": 625.0, "end": 630.8399999999999, "text": " the parameters, and blah, blah, blah. And this gets mapped to the relevant plugin implementation.", "tokens": [264, 9834, 11, 293, 12288, 11, 12288, 11, 12288, 13, 400, 341, 2170, 33318, 281, 264, 7340, 23407, 11420, 13], "temperature": 0.0, "avg_logprob": -0.19499527324329724, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.000474300526548177}, {"id": 87, "seek": 61068, "start": 630.8399999999999, "end": 637.8, "text": " For instance, in this figure, we can use the judgment inference image classification implementation,", "tokens": [1171, 5197, 11, 294, 341, 2573, 11, 321, 393, 764, 264, 12216, 38253, 3256, 21538, 11420, 11], "temperature": 0.0, "avg_logprob": -0.19499527324329724, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.000474300526548177}, {"id": 88, "seek": 63780, "start": 637.8, "end": 644.12, "text": " which translates these arguments and this operation to the actual judgment inference", "tokens": [597, 28468, 613, 12869, 293, 341, 6916, 281, 264, 3539, 12216, 38253], "temperature": 0.0, "avg_logprob": -0.17367910871318742, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0002444812562316656}, {"id": 89, "seek": 63780, "start": 644.12, "end": 653.0799999999999, "text": " framework provided by NVIDIA that does the image classification operation. Apart from", "tokens": [8388, 5649, 538, 426, 3958, 6914, 300, 775, 264, 3256, 21538, 6916, 13, 24111, 490], "temperature": 0.0, "avg_logprob": -0.17367910871318742, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0002444812562316656}, {"id": 90, "seek": 63780, "start": 653.0799999999999, "end": 663.4399999999999, "text": " the hardware specific plugins, we also have the transport layer plugins. So imagine this", "tokens": [264, 8837, 2685, 33759, 11, 321, 611, 362, 264, 5495, 4583, 33759, 13, 407, 3811, 341], "temperature": 0.0, "avg_logprob": -0.17367910871318742, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.0002444812562316656}, {"id": 91, "seek": 66344, "start": 663.44, "end": 670.9200000000001, "text": " same operation, the image inference, could be executed in a VM using a virtual plugin.", "tokens": [912, 6916, 11, 264, 3256, 38253, 11, 727, 312, 17577, 294, 257, 18038, 1228, 257, 6374, 23407, 13], "temperature": 0.0, "avg_logprob": -0.16550695700723617, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00041668929043225944}, {"id": 92, "seek": 66344, "start": 670.9200000000001, "end": 679.72, "text": " So this information, the operation, the arguments, the models, everything will be transferred", "tokens": [407, 341, 1589, 11, 264, 6916, 11, 264, 12869, 11, 264, 5245, 11, 1203, 486, 312, 15809], "temperature": 0.0, "avg_logprob": -0.16550695700723617, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00041668929043225944}, {"id": 93, "seek": 66344, "start": 679.72, "end": 688.8000000000001, "text": " to the host machine that will use hardware plugin. So apart from the glue code for the", "tokens": [281, 264, 3975, 3479, 300, 486, 764, 8837, 23407, 13, 407, 4936, 490, 264, 8998, 3089, 337, 264], "temperature": 0.0, "avg_logprob": -0.16550695700723617, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.00041668929043225944}, {"id": 94, "seek": 68880, "start": 688.8, "end": 701.4399999999999, "text": " hardware specific implementation, we also have the VM plugins. We also, some of the", "tokens": [8837, 2685, 11420, 11, 321, 611, 362, 264, 18038, 33759, 13, 492, 611, 11, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.20412103946392351, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.00046827163896523416}, {"id": 95, "seek": 68880, "start": 701.4399999999999, "end": 710.4399999999999, "text": " plugins and the API operation support a subset of acceleration frameworks, such as a tensor", "tokens": [33759, 293, 264, 9362, 6916, 1406, 257, 25993, 295, 17162, 29834, 11, 1270, 382, 257, 40863], "temperature": 0.0, "avg_logprob": -0.20412103946392351, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.00046827163896523416}, {"id": 96, "seek": 71044, "start": 710.44, "end": 722.2, "text": " flow or PyTorch. And what I mentioned earlier about the virtual plugins, so essentially", "tokens": [3095, 420, 9953, 51, 284, 339, 13, 400, 437, 286, 2835, 3071, 466, 264, 6374, 33759, 11, 370, 4476], "temperature": 0.0, "avg_logprob": -0.19793069930303664, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.0008172323578037322}, {"id": 97, "seek": 71044, "start": 722.2, "end": 729.2, "text": " what happens is that the request of the operation and the arguments is forwarded to another", "tokens": [437, 2314, 307, 300, 264, 5308, 295, 264, 6916, 293, 264, 12869, 307, 2128, 292, 281, 1071], "temperature": 0.0, "avg_logprob": -0.19793069930303664, "compression_ratio": 1.3458646616541354, "no_speech_prob": 0.0008172323578037322}, {"id": 98, "seek": 72920, "start": 729.2, "end": 741.36, "text": " instance of the VXL library, either on the hypervisor layer or on a socket interface.", "tokens": [5197, 295, 264, 691, 55, 43, 6405, 11, 2139, 322, 264, 9848, 16457, 4583, 420, 322, 257, 19741, 9226, 13], "temperature": 0.0, "avg_logprob": -0.2064550026603367, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0020132388453930616}, {"id": 99, "seek": 72920, "start": 741.36, "end": 751.0, "text": " So we currently support two modes of operations. We have a VTIO driver and currently we support", "tokens": [407, 321, 4362, 1406, 732, 14068, 295, 7705, 13, 492, 362, 257, 691, 5422, 46, 6787, 293, 4362, 321, 1406], "temperature": 0.0, "avg_logprob": -0.2064550026603367, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0020132388453930616}, {"id": 100, "seek": 75100, "start": 751.0, "end": 761.44, "text": " firecracker and chemo. So we load the driver on the VM. This driver transfers the arguments", "tokens": [2610, 10757, 23599, 293, 4771, 78, 13, 407, 321, 3677, 264, 6787, 322, 264, 18038, 13, 639, 6787, 29137, 264, 12869], "temperature": 0.0, "avg_logprob": -0.13094627857208252, "compression_ratio": 1.6488095238095237, "no_speech_prob": 0.00026182332658208907}, {"id": 101, "seek": 75100, "start": 761.44, "end": 767.28, "text": " and the operation to the backend, to the chemo backend or the firecracker backend, which", "tokens": [293, 264, 6916, 281, 264, 38087, 11, 281, 264, 4771, 78, 38087, 420, 264, 2610, 10757, 23599, 38087, 11, 597], "temperature": 0.0, "avg_logprob": -0.13094627857208252, "compression_ratio": 1.6488095238095237, "no_speech_prob": 0.00026182332658208907}, {"id": 102, "seek": 75100, "start": 767.28, "end": 774.88, "text": " in turn calls the VXL library to do the actual operation. And the other option is using sockets.", "tokens": [294, 1261, 5498, 264, 691, 55, 43, 6405, 281, 360, 264, 3539, 6916, 13, 400, 264, 661, 3614, 307, 1228, 370, 11984, 13], "temperature": 0.0, "avg_logprob": -0.13094627857208252, "compression_ratio": 1.6488095238095237, "no_speech_prob": 0.00026182332658208907}, {"id": 103, "seek": 77488, "start": 774.88, "end": 782.76, "text": " So we load a socket interface, a socket agent on the host. We have the VSOC plugin on the", "tokens": [407, 321, 3677, 257, 19741, 9226, 11, 257, 19741, 9461, 322, 264, 3975, 13, 492, 362, 264, 691, 17188, 34, 23407, 322, 264], "temperature": 0.0, "avg_logprob": -0.21710757856015805, "compression_ratio": 1.1265822784810127, "no_speech_prob": 0.0003463467874098569}, {"id": 104, "seek": 78276, "start": 782.76, "end": 805.24, "text": " guest and they communicate over simple sockets. I'm going to hand over to Babi for the Unicernel", "tokens": [8341, 293, 436, 7890, 670, 2199, 370, 11984, 13, 286, 478, 516, 281, 1011, 670, 281, 15820, 72, 337, 264, 1156, 299, 1248, 338], "temperature": 0.0, "avg_logprob": -0.35884127020835876, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.0002317069302080199}, {"id": 105, "seek": 78276, "start": 805.24, "end": 806.24, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.35884127020835876, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.0002317069302080199}, {"id": 106, "seek": 80624, "start": 806.24, "end": 814.04, "text": " So how can VXL be used in Unicernels? Actually, it's quite easy compared to any other acceleration", "tokens": [407, 577, 393, 691, 55, 43, 312, 1143, 294, 1156, 299, 1248, 1625, 30, 5135, 11, 309, 311, 1596, 1858, 5347, 281, 604, 661, 17162], "temperature": 0.0, "avg_logprob": -0.18683943381676307, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002964014420285821}, {"id": 107, "seek": 80624, "start": 814.04, "end": 821.84, "text": " framework that exists. And the thing is that the only thing that we need to do is just", "tokens": [8388, 300, 8198, 13, 400, 264, 551, 307, 300, 264, 787, 551, 300, 321, 643, 281, 360, 307, 445], "temperature": 0.0, "avg_logprob": -0.18683943381676307, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002964014420285821}, {"id": 108, "seek": 80624, "start": 821.84, "end": 827.72, "text": " have that VXLRT that you see over there. That's the only thing that we need to port. And this", "tokens": [362, 300, 691, 55, 31722, 51, 300, 291, 536, 670, 456, 13, 663, 311, 264, 787, 551, 300, 321, 643, 281, 2436, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.18683943381676307, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002964014420285821}, {"id": 109, "seek": 80624, "start": 827.72, "end": 832.8, "text": " is a very, very thin layer of a C code. It can be easily ported to any Unicernel that", "tokens": [307, 257, 588, 11, 588, 5862, 4583, 295, 257, 383, 3089, 13, 467, 393, 312, 3612, 2436, 292, 281, 604, 1156, 299, 1248, 338, 300], "temperature": 0.0, "avg_logprob": -0.18683943381676307, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002964014420285821}, {"id": 110, "seek": 83280, "start": 832.8, "end": 844.52, "text": " exists. And we, of course, we need some kind of a transport plugin for forward requests.", "tokens": [8198, 13, 400, 321, 11, 295, 1164, 11, 321, 643, 512, 733, 295, 257, 5495, 23407, 337, 2128, 12475, 13], "temperature": 0.0, "avg_logprob": -0.19541363139728923, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00015794282080605626}, {"id": 111, "seek": 83280, "start": 844.52, "end": 849.68, "text": " So as Tasos already explained, usually the application is the same application that we", "tokens": [407, 382, 27293, 329, 1217, 8825, 11, 2673, 264, 3861, 307, 264, 912, 3861, 300, 321], "temperature": 0.0, "avg_logprob": -0.19541363139728923, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00015794282080605626}, {"id": 112, "seek": 83280, "start": 849.68, "end": 855.0, "text": " can run in the host or in any container or in any VM can be also used in the Unicernel,", "tokens": [393, 1190, 294, 264, 3975, 420, 294, 604, 10129, 420, 294, 604, 18038, 393, 312, 611, 1143, 294, 264, 1156, 299, 1248, 338, 11], "temperature": 0.0, "avg_logprob": -0.19541363139728923, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00015794282080605626}, {"id": 113, "seek": 83280, "start": 855.0, "end": 861.76, "text": " the same node changes. And it simply uses a specific API of VXL and then we simply forward", "tokens": [264, 912, 9984, 2962, 13, 400, 309, 2935, 4960, 257, 2685, 9362, 295, 691, 55, 43, 293, 550, 321, 2935, 2128], "temperature": 0.0, "avg_logprob": -0.19541363139728923, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00015794282080605626}, {"id": 114, "seek": 86176, "start": 861.76, "end": 866.0, "text": " the request to the host and then we have another version of VXL which is in the host", "tokens": [264, 5308, 281, 264, 3975, 293, 550, 321, 362, 1071, 3037, 295, 691, 55, 43, 597, 307, 294, 264, 3975], "temperature": 0.0, "avg_logprob": -0.14872335303913464, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.00011548298061825335}, {"id": 115, "seek": 86176, "start": 866.0, "end": 872.24, "text": " and simply maps to the hardware accelerator framework that is implementing the specific", "tokens": [293, 2935, 11317, 281, 264, 8837, 39889, 8388, 300, 307, 18114, 264, 2685], "temperature": 0.0, "avg_logprob": -0.14872335303913464, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.00011548298061825335}, {"id": 116, "seek": 86176, "start": 872.24, "end": 874.6, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.14872335303913464, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.00011548298061825335}, {"id": 117, "seek": 86176, "start": 874.6, "end": 880.96, "text": " So this, as I said, this allows us to have the same application running either in the", "tokens": [407, 341, 11, 382, 286, 848, 11, 341, 4045, 505, 281, 362, 264, 912, 3861, 2614, 2139, 294, 264], "temperature": 0.0, "avg_logprob": -0.14872335303913464, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.00011548298061825335}, {"id": 118, "seek": 86176, "start": 880.96, "end": 890.08, "text": " host, either in the VM without any changes. So it's easy to debug, easy to execute. And", "tokens": [3975, 11, 2139, 294, 264, 18038, 1553, 604, 2962, 13, 407, 309, 311, 1858, 281, 24083, 11, 1858, 281, 14483, 13, 400], "temperature": 0.0, "avg_logprob": -0.14872335303913464, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.00011548298061825335}, {"id": 119, "seek": 89008, "start": 890.08, "end": 896.44, "text": " we can also access different kind of hardware, different kind of frameworks that exist. And", "tokens": [321, 393, 611, 2105, 819, 733, 295, 8837, 11, 819, 733, 295, 29834, 300, 2514, 13, 400], "temperature": 0.0, "avg_logprob": -0.1340438418918186, "compression_ratio": 1.6486486486486487, "no_speech_prob": 9.186499664792791e-05}, {"id": 120, "seek": 89008, "start": 896.44, "end": 900.36, "text": " we don't need to change their application. We can simply change the configuration in", "tokens": [321, 500, 380, 643, 281, 1319, 641, 3861, 13, 492, 393, 2935, 1319, 264, 11694, 294], "temperature": 0.0, "avg_logprob": -0.1340438418918186, "compression_ratio": 1.6486486486486487, "no_speech_prob": 9.186499664792791e-05}, {"id": 121, "seek": 89008, "start": 900.36, "end": 902.5200000000001, "text": " the host.", "tokens": [264, 3975, 13], "temperature": 0.0, "avg_logprob": -0.1340438418918186, "compression_ratio": 1.6486486486486487, "no_speech_prob": 9.186499664792791e-05}, {"id": 122, "seek": 89008, "start": 902.5200000000001, "end": 908.2, "text": " So yes, we have another acceleration framework and maybe we can think that this is not going", "tokens": [407, 2086, 11, 321, 362, 1071, 17162, 8388, 293, 1310, 321, 393, 519, 300, 341, 307, 406, 516], "temperature": 0.0, "avg_logprob": -0.1340438418918186, "compression_ratio": 1.6486486486486487, "no_speech_prob": 9.186499664792791e-05}, {"id": 123, "seek": 89008, "start": 908.2, "end": 915.12, "text": " to be easy to use. But let's take an example and see how we can extend the VXL and see", "tokens": [281, 312, 1858, 281, 764, 13, 583, 718, 311, 747, 364, 1365, 293, 536, 577, 321, 393, 10101, 264, 691, 55, 43, 293, 536], "temperature": 0.0, "avg_logprob": -0.1340438418918186, "compression_ratio": 1.6486486486486487, "no_speech_prob": 9.186499664792791e-05}, {"id": 124, "seek": 91512, "start": 915.12, "end": 920.28, "text": " if it is easier or not. So let's get a typical vector addition example in OpenCL which can", "tokens": [498, 309, 307, 3571, 420, 406, 13, 407, 718, 311, 483, 257, 7476, 8062, 4500, 1365, 294, 7238, 22458, 597, 393], "temperature": 0.0, "avg_logprob": -0.18429431718649322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00017178697453346103}, {"id": 125, "seek": 91512, "start": 920.28, "end": 926.92, "text": " be executed in the CPU or in FPGA. And the steps that usually happen is that we set up", "tokens": [312, 17577, 294, 264, 13199, 420, 294, 36655, 12570, 13, 400, 264, 4439, 300, 2673, 1051, 307, 300, 321, 992, 493], "temperature": 0.0, "avg_logprob": -0.18429431718649322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00017178697453346103}, {"id": 126, "seek": 91512, "start": 926.92, "end": 933.2, "text": " the bitstream in the FPGA and the FPGA starts the reconfiguration. Of course, we transfer", "tokens": [264, 857, 9291, 294, 264, 36655, 12570, 293, 264, 36655, 12570, 3719, 264, 9993, 20646, 8167, 13, 2720, 1164, 11, 321, 5003], "temperature": 0.0, "avg_logprob": -0.18429431718649322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00017178697453346103}, {"id": 127, "seek": 91512, "start": 933.2, "end": 940.0, "text": " the data to the FPGA. Then we invoke the kernel as soon as it's ready and we also get the", "tokens": [264, 1412, 281, 264, 36655, 12570, 13, 1396, 321, 41117, 264, 28256, 382, 2321, 382, 309, 311, 1919, 293, 321, 611, 483, 264], "temperature": 0.0, "avg_logprob": -0.18429431718649322, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00017178697453346103}, {"id": 128, "seek": 94000, "start": 940.0, "end": 945.6, "text": " results back to the host. So this is what the application is already doing. So if you", "tokens": [3542, 646, 281, 264, 3975, 13, 407, 341, 307, 437, 264, 3861, 307, 1217, 884, 13, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.14762230509335234, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00018777606601361185}, {"id": 129, "seek": 94000, "start": 945.6, "end": 949.52, "text": " have this application already running in your machine, the only thing that you have to do", "tokens": [362, 341, 3861, 1217, 2614, 294, 428, 3479, 11, 264, 787, 551, 300, 291, 362, 281, 360], "temperature": 0.0, "avg_logprob": -0.14762230509335234, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00018777606601361185}, {"id": 130, "seek": 94000, "start": 949.52, "end": 956.28, "text": " is that somehow you need to libyify the application. And that's instead of just exposing an API", "tokens": [307, 300, 6063, 291, 643, 281, 375, 2322, 2505, 264, 3861, 13, 400, 300, 311, 2602, 295, 445, 33178, 364, 9362], "temperature": 0.0, "avg_logprob": -0.14762230509335234, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00018777606601361185}, {"id": 131, "seek": 94000, "start": 956.28, "end": 958.76, "text": " to do that.", "tokens": [281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14762230509335234, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00018777606601361185}, {"id": 132, "seek": 94000, "start": 958.76, "end": 965.08, "text": " And the next thing is that you can integrate the library in the VXL as a plug-in. And we", "tokens": [400, 264, 958, 551, 307, 300, 291, 393, 13365, 264, 6405, 294, 264, 691, 55, 43, 382, 257, 5452, 12, 259, 13, 400, 321], "temperature": 0.0, "avg_logprob": -0.14762230509335234, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00018777606601361185}, {"id": 133, "seek": 96508, "start": 965.08, "end": 970.96, "text": " have a very simplistic API that you can use and therefore the application will be seen", "tokens": [362, 257, 588, 44199, 9362, 300, 291, 393, 764, 293, 4412, 264, 3861, 486, 312, 1612], "temperature": 0.0, "avg_logprob": -0.13721617846421794, "compression_ratio": 1.4829545454545454, "no_speech_prob": 3.11997537210118e-05}, {"id": 134, "seek": 96508, "start": 970.96, "end": 978.4000000000001, "text": " as a plug-in for the VXL. Later, you can also update VXL, just adding one more API to the", "tokens": [382, 257, 5452, 12, 259, 337, 264, 691, 55, 43, 13, 11965, 11, 291, 393, 611, 5623, 691, 55, 43, 11, 445, 5127, 472, 544, 9362, 281, 264], "temperature": 0.0, "avg_logprob": -0.13721617846421794, "compression_ratio": 1.4829545454545454, "no_speech_prob": 3.11997537210118e-05}, {"id": 135, "seek": 96508, "start": 978.4000000000001, "end": 985.48, "text": " VXLRT so the application can directly use it with the correct parameters, of course.", "tokens": [691, 55, 31722, 51, 370, 264, 3861, 393, 3838, 764, 309, 365, 264, 3006, 9834, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13721617846421794, "compression_ratio": 1.4829545454545454, "no_speech_prob": 3.11997537210118e-05}, {"id": 136, "seek": 98548, "start": 985.48, "end": 1000.12, "text": " So I will give you a sort of demo of how this works, using Unicrout specifically. I will", "tokens": [407, 286, 486, 976, 291, 257, 1333, 295, 10723, 295, 577, 341, 1985, 11, 1228, 1156, 299, 81, 346, 4682, 13, 286, 486], "temperature": 0.0, "avg_logprob": -0.36108033316476007, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0003541722835507244}, {"id": 137, "seek": 98548, "start": 1000.12, "end": 1004.24, "text": " transfer a bit so we can have maybe in the most classification at first and then we can", "tokens": [5003, 257, 857, 370, 321, 393, 362, 1310, 294, 264, 881, 21538, 412, 700, 293, 550, 321, 393], "temperature": 0.0, "avg_logprob": -0.36108033316476007, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0003541722835507244}, {"id": 138, "seek": 98548, "start": 1004.24, "end": 1011.12, "text": " see how this, how a BLAST CUDA operation can be executed in the CPU and the GPU without", "tokens": [536, 577, 341, 11, 577, 257, 15132, 20398, 29777, 7509, 6916, 393, 312, 17577, 294, 264, 13199, 293, 264, 18407, 1553], "temperature": 0.0, "avg_logprob": -0.36108033316476007, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0003541722835507244}, {"id": 139, "seek": 101112, "start": 1011.12, "end": 1022.04, "text": " any changes. And maybe some FPGA if we have time. Okay, this is not good. This is better.", "tokens": [604, 2962, 13, 400, 1310, 512, 36655, 12570, 498, 321, 362, 565, 13, 1033, 11, 341, 307, 406, 665, 13, 639, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14259676057465223, "compression_ratio": 1.2877697841726619, "no_speech_prob": 0.0003076609573327005}, {"id": 140, "seek": 101112, "start": 1022.04, "end": 1037.12, "text": " So we are in a typical working environment for Unicraft. We have created our application.", "tokens": [407, 321, 366, 294, 257, 7476, 1364, 2823, 337, 1156, 299, 4469, 13, 492, 362, 2942, 527, 3861, 13], "temperature": 0.0, "avg_logprob": -0.14259676057465223, "compression_ratio": 1.2877697841726619, "no_speech_prob": 0.0003076609573327005}, {"id": 141, "seek": 103712, "start": 1037.12, "end": 1043.9199999999998, "text": " We have a new lib which we are not going to use actually. And we have also Unicraft.", "tokens": [492, 362, 257, 777, 22854, 597, 321, 366, 406, 516, 281, 764, 767, 13, 400, 321, 362, 611, 1156, 299, 4469, 13], "temperature": 0.0, "avg_logprob": -0.1685315034328363, "compression_ratio": 1.6211180124223603, "no_speech_prob": 0.00012416669051162899}, {"id": 142, "seek": 103712, "start": 1043.9199999999998, "end": 1050.7199999999998, "text": " So let's go to here. So this is a repo that we have created. I will show it to you later.", "tokens": [407, 718, 311, 352, 281, 510, 13, 407, 341, 307, 257, 49040, 300, 321, 362, 2942, 13, 286, 486, 855, 309, 281, 291, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1685315034328363, "compression_ratio": 1.6211180124223603, "no_speech_prob": 0.00012416669051162899}, {"id": 143, "seek": 103712, "start": 1050.7199999999998, "end": 1063.04, "text": " So this is, I want to show you. So here you can see that we only have, we only exposed", "tokens": [407, 341, 307, 11, 286, 528, 281, 855, 291, 13, 407, 510, 291, 393, 536, 300, 321, 787, 362, 11, 321, 787, 9495], "temperature": 0.0, "avg_logprob": -0.1685315034328363, "compression_ratio": 1.6211180124223603, "no_speech_prob": 0.00012416669051162899}, {"id": 144, "seek": 106304, "start": 1063.04, "end": 1068.1599999999999, "text": " nine PFS and we use it because we want to transfer the data inside the Unicrout. So", "tokens": [4949, 430, 29318, 293, 321, 764, 309, 570, 321, 528, 281, 5003, 264, 1412, 1854, 264, 1156, 299, 81, 346, 13, 407], "temperature": 0.0, "avg_logprob": -0.14880103119148697, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0004770265659317374}, {"id": 145, "seek": 106304, "start": 1068.1599999999999, "end": 1073.68, "text": " we are not going to use any network. We are just going to share a directory with the VM.", "tokens": [321, 366, 406, 516, 281, 764, 604, 3209, 13, 492, 366, 445, 516, 281, 2073, 257, 21120, 365, 264, 18038, 13], "temperature": 0.0, "avg_logprob": -0.14880103119148697, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0004770265659317374}, {"id": 146, "seek": 106304, "start": 1073.68, "end": 1078.52, "text": " And the only need that you need to do is to select VXLRT and that's all. As you see,", "tokens": [400, 264, 787, 643, 300, 291, 643, 281, 360, 307, 281, 3048, 691, 55, 31722, 51, 293, 300, 311, 439, 13, 1018, 291, 536, 11], "temperature": 0.0, "avg_logprob": -0.14880103119148697, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0004770265659317374}, {"id": 147, "seek": 106304, "start": 1078.52, "end": 1086.3999999999999, "text": " we don't have any libc because we don't need it for the specific example. So these are", "tokens": [321, 500, 380, 362, 604, 22854, 66, 570, 321, 500, 380, 643, 309, 337, 264, 2685, 1365, 13, 407, 613, 366], "temperature": 0.0, "avg_logprob": -0.14880103119148697, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0004770265659317374}, {"id": 148, "seek": 106304, "start": 1086.3999999999999, "end": 1092.68, "text": " all the applications that are currently running in Unicraft. You can try them out by yourself.", "tokens": [439, 264, 5821, 300, 366, 4362, 2614, 294, 1156, 299, 4469, 13, 509, 393, 853, 552, 484, 538, 1803, 13], "temperature": 0.0, "avg_logprob": -0.14880103119148697, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0004770265659317374}, {"id": 149, "seek": 109268, "start": 1092.68, "end": 1109.5600000000002, "text": " So let's, we're going to use image classification. So we'll take some time, let me, we'll take", "tokens": [407, 718, 311, 11, 321, 434, 516, 281, 764, 3256, 21538, 13, 407, 321, 603, 747, 512, 565, 11, 718, 385, 11, 321, 603, 747], "temperature": 0.0, "avg_logprob": -0.24158208847045898, "compression_ratio": 1.44, "no_speech_prob": 0.0007767148199491203}, {"id": 150, "seek": 109268, "start": 1109.5600000000002, "end": 1115.76, "text": " some time to build. But I will also try to show you how the application looks like as", "tokens": [512, 565, 281, 1322, 13, 583, 286, 486, 611, 853, 281, 855, 291, 577, 264, 3861, 1542, 411, 382], "temperature": 0.0, "avg_logprob": -0.24158208847045898, "compression_ratio": 1.44, "no_speech_prob": 0.0007767148199491203}, {"id": 151, "seek": 111576, "start": 1115.76, "end": 1126.2, "text": " soon as it finishes. And it should finish right now, almost. Okay. And that's application.", "tokens": [2321, 382, 309, 23615, 13, 400, 309, 820, 2413, 558, 586, 11, 1920, 13, 1033, 13, 400, 300, 311, 3861, 13], "temperature": 0.0, "avg_logprob": -0.15600141760421127, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000357395620085299}, {"id": 152, "seek": 111576, "start": 1126.2, "end": 1132.52, "text": " So as you can see, yeah, we can skip the reading of the file. So this application is quite", "tokens": [407, 382, 291, 393, 536, 11, 1338, 11, 321, 393, 10023, 264, 3760, 295, 264, 3991, 13, 407, 341, 3861, 307, 1596], "temperature": 0.0, "avg_logprob": -0.15600141760421127, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000357395620085299}, {"id": 153, "seek": 111576, "start": 1132.52, "end": 1137.8, "text": " simple. Like we have a session that we have to create with VXL with the host. Then we", "tokens": [2199, 13, 1743, 321, 362, 257, 5481, 300, 321, 362, 281, 1884, 365, 691, 55, 43, 365, 264, 3975, 13, 1396, 321], "temperature": 0.0, "avg_logprob": -0.15600141760421127, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000357395620085299}, {"id": 154, "seek": 113780, "start": 1137.8, "end": 1146.3999999999999, "text": " simply call the, this is the function that is called VXL image classification. It has", "tokens": [2935, 818, 264, 11, 341, 307, 264, 2445, 300, 307, 1219, 691, 55, 43, 3256, 21538, 13, 467, 575], "temperature": 0.0, "avg_logprob": -0.18567150358169798, "compression_ratio": 1.592814371257485, "no_speech_prob": 8.167260239133611e-05}, {"id": 155, "seek": 113780, "start": 1146.3999999999999, "end": 1155.8, "text": " the arguments that are also needed. And then we simply release the resources that we have", "tokens": [264, 12869, 300, 366, 611, 2978, 13, 400, 550, 321, 2935, 4374, 264, 3593, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.18567150358169798, "compression_ratio": 1.592814371257485, "no_speech_prob": 8.167260239133611e-05}, {"id": 156, "seek": 113780, "start": 1155.8, "end": 1165.6, "text": " used. So I will try to do an image classification for this beautiful hedgehog that we have", "tokens": [1143, 13, 407, 286, 486, 853, 281, 360, 364, 3256, 21538, 337, 341, 2238, 25304, 27084, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.18567150358169798, "compression_ratio": 1.592814371257485, "no_speech_prob": 8.167260239133611e-05}, {"id": 157, "seek": 116560, "start": 1165.6, "end": 1175.48, "text": " here. And let's see what's going to happen. Okay. So all these logs that you see here", "tokens": [510, 13, 400, 718, 311, 536, 437, 311, 516, 281, 1051, 13, 1033, 13, 407, 439, 613, 20820, 300, 291, 536, 510], "temperature": 0.0, "avg_logprob": -0.19668843871668765, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.00021534565894398838}, {"id": 158, "seek": 116560, "start": 1175.48, "end": 1188.1999999999998, "text": " are from the Deton inference plugin. And we see that we have a hedgehog. So it was identified.", "tokens": [366, 490, 264, 4237, 266, 38253, 23407, 13, 400, 321, 536, 300, 321, 362, 257, 25304, 27084, 13, 407, 309, 390, 9234, 13], "temperature": 0.0, "avg_logprob": -0.19668843871668765, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.00021534565894398838}, {"id": 159, "seek": 116560, "start": 1188.1999999999998, "end": 1195.1999999999998, "text": " And the thing here is to, you can see that all of these logs are not from the Unicraft.", "tokens": [400, 264, 551, 510, 307, 281, 11, 291, 393, 536, 300, 439, 295, 613, 20820, 366, 406, 490, 264, 1156, 299, 4469, 13], "temperature": 0.0, "avg_logprob": -0.19668843871668765, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.00021534565894398838}, {"id": 160, "seek": 119520, "start": 1195.2, "end": 1204.04, "text": " All of these logs are from the host that is running. I can also show you this small demo", "tokens": [1057, 295, 613, 20820, 366, 490, 264, 3975, 300, 307, 2614, 13, 286, 393, 611, 855, 291, 341, 1359, 10723], "temperature": 0.0, "avg_logprob": -0.18937839780535018, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004830348480027169}, {"id": 161, "seek": 119520, "start": 1204.04, "end": 1217.68, "text": " with some operations for arrays using CUDA. So the same here. We're just, we're going", "tokens": [365, 512, 7705, 337, 41011, 1228, 29777, 7509, 13, 407, 264, 912, 510, 13, 492, 434, 445, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.18937839780535018, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004830348480027169}, {"id": 162, "seek": 119520, "start": 1217.68, "end": 1223.44, "text": " to export the backend. First, we're going to use a no op plugin, which simply doesn't", "tokens": [281, 10725, 264, 38087, 13, 2386, 11, 321, 434, 516, 281, 764, 257, 572, 999, 23407, 11, 597, 2935, 1177, 380], "temperature": 0.0, "avg_logprob": -0.18937839780535018, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004830348480027169}, {"id": 163, "seek": 122344, "start": 1223.44, "end": 1232.64, "text": " do anything. You can mostly maybe use the only 40 bug. So we have here the application,", "tokens": [360, 1340, 13, 509, 393, 5240, 1310, 764, 264, 787, 3356, 7426, 13, 407, 321, 362, 510, 264, 3861, 11], "temperature": 0.0, "avg_logprob": -0.21044439665028747, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0003335593792144209}, {"id": 164, "seek": 122344, "start": 1232.64, "end": 1240.24, "text": " which is a SKM. And you can see that it doesn't do anything because it's just a no op plugin.", "tokens": [597, 307, 257, 21483, 44, 13, 400, 291, 393, 536, 300, 309, 1177, 380, 360, 1340, 570, 309, 311, 445, 257, 572, 999, 23407, 13], "temperature": 0.0, "avg_logprob": -0.21044439665028747, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0003335593792144209}, {"id": 165, "seek": 122344, "start": 1240.24, "end": 1247.1200000000001, "text": " It doesn't do anything special. So we can change the configuration in the host and specify", "tokens": [467, 1177, 380, 360, 1340, 2121, 13, 407, 321, 393, 1319, 264, 11694, 294, 264, 3975, 293, 16500], "temperature": 0.0, "avg_logprob": -0.21044439665028747, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0003335593792144209}, {"id": 166, "seek": 124712, "start": 1247.12, "end": 1258.76, "text": " that the backend that we want to use is the actual CUDA implementation for maybe CPU. Yes.", "tokens": [300, 264, 38087, 300, 321, 528, 281, 764, 307, 264, 3539, 29777, 7509, 11420, 337, 1310, 13199, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.16332020171701092, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.00018947626813314855}, {"id": 167, "seek": 124712, "start": 1258.76, "end": 1265.8, "text": " Okay. So then we will run it and you will see that we have the, actually it's a min", "tokens": [1033, 13, 407, 550, 321, 486, 1190, 309, 293, 291, 486, 536, 300, 321, 362, 264, 11, 767, 309, 311, 257, 923], "temperature": 0.0, "avg_logprob": -0.16332020171701092, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.00018947626813314855}, {"id": 168, "seek": 124712, "start": 1265.8, "end": 1273.6, "text": " max operation. It's not a SKM. And then you can also, we will also run the same thing", "tokens": [11469, 6916, 13, 467, 311, 406, 257, 21483, 44, 13, 400, 550, 291, 393, 611, 11, 321, 486, 611, 1190, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.16332020171701092, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.00018947626813314855}, {"id": 169, "seek": 127360, "start": 1273.6, "end": 1281.56, "text": " in a GPU. Again, we are just in the host again. We can simply change the configuration", "tokens": [294, 257, 18407, 13, 3764, 11, 321, 366, 445, 294, 264, 3975, 797, 13, 492, 393, 2935, 1319, 264, 11694], "temperature": 0.0, "avg_logprob": -0.1522815704345703, "compression_ratio": 1.34375, "no_speech_prob": 0.000128301398945041}, {"id": 170, "seek": 127360, "start": 1281.56, "end": 1291.48, "text": " and now we start it again, the Unicernal, and we get the result from the GPU. You can", "tokens": [293, 586, 321, 722, 309, 797, 11, 264, 1156, 299, 1248, 304, 11, 293, 321, 483, 264, 1874, 490, 264, 18407, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.1522815704345703, "compression_ratio": 1.34375, "no_speech_prob": 0.000128301398945041}, {"id": 171, "seek": 129148, "start": 1291.48, "end": 1304.4, "text": " also, all these debug messages, you can remove them of course. So we also have the, yes,", "tokens": [611, 11, 439, 613, 24083, 7897, 11, 291, 393, 4159, 552, 295, 1164, 13, 407, 321, 611, 362, 264, 11, 2086, 11], "temperature": 0.0, "avg_logprob": -0.21237065657129828, "compression_ratio": 1.3282442748091603, "no_speech_prob": 0.00015966262435540557}, {"id": 172, "seek": 129148, "start": 1304.4, "end": 1313.4, "text": " this is also min max still. Now we will go to SKM. Do we have time still? Yeah. Okay.", "tokens": [341, 307, 611, 923, 11469, 920, 13, 823, 321, 486, 352, 281, 21483, 44, 13, 1144, 321, 362, 565, 920, 30, 865, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.21237065657129828, "compression_ratio": 1.3282442748091603, "no_speech_prob": 0.00015966262435540557}, {"id": 173, "seek": 131340, "start": 1313.4, "end": 1322.3200000000002, "text": " So yeah, we can just use this. Again, no op, nothing happens. Nothing really special. We", "tokens": [407, 1338, 11, 321, 393, 445, 764, 341, 13, 3764, 11, 572, 999, 11, 1825, 2314, 13, 6693, 534, 2121, 13, 492], "temperature": 0.0, "avg_logprob": -0.20615309057101397, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.00043173684389330447}, {"id": 174, "seek": 131340, "start": 1322.3200000000002, "end": 1330.24, "text": " will do the export for, to specify the CPU plugin again. And we will execute and we'll", "tokens": [486, 360, 264, 10725, 337, 11, 281, 16500, 264, 13199, 23407, 797, 13, 400, 321, 486, 14483, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.20615309057101397, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.00043173684389330447}, {"id": 175, "seek": 131340, "start": 1330.24, "end": 1341.2800000000002, "text": " see that the execution time, it's quite not very big, but it's just remember that number.", "tokens": [536, 300, 264, 15058, 565, 11, 309, 311, 1596, 406, 588, 955, 11, 457, 309, 311, 445, 1604, 300, 1230, 13], "temperature": 0.0, "avg_logprob": -0.20615309057101397, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.00043173684389330447}, {"id": 176, "seek": 134128, "start": 1341.28, "end": 1348.36, "text": " And now we will run it in the GPU and you can see here that the execution time is much", "tokens": [400, 586, 321, 486, 1190, 309, 294, 264, 18407, 293, 291, 393, 536, 510, 300, 264, 15058, 565, 307, 709], "temperature": 0.0, "avg_logprob": -0.20977102793180025, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.00023215299006551504}, {"id": 177, "seek": 134128, "start": 1348.36, "end": 1368.6, "text": " better than before. And that's all. We can also solve the, the, the FPGA, which is, okay.", "tokens": [1101, 813, 949, 13, 400, 300, 311, 439, 13, 492, 393, 611, 5039, 264, 11, 264, 11, 264, 36655, 12570, 11, 597, 307, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.20977102793180025, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.00023215299006551504}, {"id": 178, "seek": 136860, "start": 1368.6, "end": 1375.6399999999999, "text": " So this is an FPGA, right? So we need to have a bit stream. And this is a black skulls application", "tokens": [407, 341, 307, 364, 36655, 12570, 11, 558, 30, 407, 321, 643, 281, 362, 257, 857, 4309, 13, 400, 341, 307, 257, 2211, 11743, 82, 3861], "temperature": 0.0, "avg_logprob": -0.1883437006097091, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00015362493286374956}, {"id": 179, "seek": 136860, "start": 1375.6399999999999, "end": 1381.1999999999998, "text": " by the way. And we will run it natively in the beginning and then we will also run it", "tokens": [538, 264, 636, 13, 400, 321, 486, 1190, 309, 8470, 356, 294, 264, 2863, 293, 550, 321, 486, 611, 1190, 309], "temperature": 0.0, "avg_logprob": -0.1883437006097091, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00015362493286374956}, {"id": 180, "seek": 136860, "start": 1381.1999999999998, "end": 1390.4399999999998, "text": " in the Unicraft. So first we just run the application natively and you can see all of", "tokens": [294, 264, 1156, 299, 4469, 13, 407, 700, 321, 445, 1190, 264, 3861, 8470, 356, 293, 291, 393, 536, 439, 295], "temperature": 0.0, "avg_logprob": -0.1883437006097091, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00015362493286374956}, {"id": 181, "seek": 139044, "start": 1390.44, "end": 1400.2, "text": " the logs and everything of the execution in the FPGA. And then we can, we will see how", "tokens": [264, 20820, 293, 1203, 295, 264, 15058, 294, 264, 36655, 12570, 13, 400, 550, 321, 393, 11, 321, 486, 536, 577], "temperature": 0.0, "avg_logprob": -0.1986863515148424, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.0004171296604909003}, {"id": 182, "seek": 139044, "start": 1400.2, "end": 1413.56, "text": " this is executed in a Unicernel. So this is, I forgot to solve that, but I will, so it", "tokens": [341, 307, 17577, 294, 257, 1156, 299, 1248, 338, 13, 407, 341, 307, 11, 286, 5298, 281, 5039, 300, 11, 457, 286, 486, 11, 370, 309], "temperature": 0.0, "avg_logprob": -0.1986863515148424, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.0004171296604909003}, {"id": 183, "seek": 139044, "start": 1413.56, "end": 1417.8400000000001, "text": " will explain later what are all of these things. Usually what we have to do is just", "tokens": [486, 2903, 1780, 437, 366, 439, 295, 613, 721, 13, 11419, 437, 321, 362, 281, 360, 307, 445], "temperature": 0.0, "avg_logprob": -0.1986863515148424, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.0004171296604909003}, {"id": 184, "seek": 141784, "start": 1417.84, "end": 1424.0, "text": " to export the VXL backend that we want to use. That's how we configure the host to use", "tokens": [281, 10725, 264, 691, 55, 43, 38087, 300, 321, 528, 281, 764, 13, 663, 311, 577, 321, 22162, 264, 3975, 281, 764], "temperature": 0.0, "avg_logprob": -0.12091950152782684, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0002962666912935674}, {"id": 185, "seek": 141784, "start": 1424.0, "end": 1429.24, "text": " a specific plugin. And then we have the chemo command that I can explain in more details", "tokens": [257, 2685, 23407, 13, 400, 550, 321, 362, 264, 4771, 78, 5622, 300, 286, 393, 2903, 294, 544, 4365], "temperature": 0.0, "avg_logprob": -0.12091950152782684, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0002962666912935674}, {"id": 186, "seek": 141784, "start": 1429.24, "end": 1438.8, "text": " after this video. Still, this is from the Unicernel now and we access the FPGA and we have the", "tokens": [934, 341, 960, 13, 8291, 11, 341, 307, 490, 264, 1156, 299, 1248, 338, 586, 293, 321, 2105, 264, 36655, 12570, 293, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.12091950152782684, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0002962666912935674}, {"id": 187, "seek": 141784, "start": 1438.8, "end": 1446.8799999999999, "text": " black skulls operation running there. And we also have one more FPGA application, but", "tokens": [2211, 11743, 82, 6916, 2614, 456, 13, 400, 321, 611, 362, 472, 544, 36655, 12570, 3861, 11, 457], "temperature": 0.0, "avg_logprob": -0.12091950152782684, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0002962666912935674}, {"id": 188, "seek": 144688, "start": 1446.88, "end": 1455.2, "text": " I think you got the point. We have all these links for the videos and everything in our", "tokens": [286, 519, 291, 658, 264, 935, 13, 492, 362, 439, 613, 6123, 337, 264, 2145, 293, 1203, 294, 527], "temperature": 0.0, "avg_logprob": -0.18321039802149722, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0013478304026648402}, {"id": 189, "seek": 144688, "start": 1455.2, "end": 1465.1200000000001, "text": " talk in Fosden. So you can also see them from there. Let me talk a bit about chemo, the", "tokens": [751, 294, 479, 329, 1556, 13, 407, 291, 393, 611, 536, 552, 490, 456, 13, 961, 385, 751, 257, 857, 466, 4771, 78, 11, 264], "temperature": 0.0, "avg_logprob": -0.18321039802149722, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0013478304026648402}, {"id": 190, "seek": 144688, "start": 1465.1200000000001, "end": 1472.16, "text": " chemo plugin that we have. This is a bit more, this is just from our Apple. So here we need", "tokens": [4771, 78, 23407, 300, 321, 362, 13, 639, 307, 257, 857, 544, 11, 341, 307, 445, 490, 527, 6373, 13, 407, 510, 321, 643], "temperature": 0.0, "avg_logprob": -0.18321039802149722, "compression_ratio": 1.5257142857142858, "no_speech_prob": 0.0013478304026648402}, {"id": 191, "seek": 147216, "start": 1472.16, "end": 1483.0400000000002, "text": " the chemo which has the vertio backend for VXL. And if Unicraft, for example, had support", "tokens": [264, 4771, 78, 597, 575, 264, 6509, 1004, 38087, 337, 691, 55, 43, 13, 400, 498, 1156, 299, 4469, 11, 337, 1365, 11, 632, 1406], "temperature": 0.0, "avg_logprob": -0.19760257441823076, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0004037225735373795}, {"id": 192, "seek": 147216, "start": 1483.0400000000002, "end": 1488.0400000000002, "text": " for Vsoc, we didn't have to use the vertio backend, so we didn't have to modify chemo.", "tokens": [337, 691, 539, 66, 11, 321, 994, 380, 362, 281, 764, 264, 6509, 1004, 38087, 11, 370, 321, 994, 380, 362, 281, 16927, 4771, 78, 13], "temperature": 0.0, "avg_logprob": -0.19760257441823076, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0004037225735373795}, {"id": 193, "seek": 147216, "start": 1488.0400000000002, "end": 1498.3200000000002, "text": " But since we have no Vsoc support, then we have to use the vertio, and therefore we changed", "tokens": [583, 1670, 321, 362, 572, 691, 539, 66, 1406, 11, 550, 321, 362, 281, 764, 264, 6509, 1004, 11, 293, 4412, 321, 3105], "temperature": 0.0, "avg_logprob": -0.19760257441823076, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0004037225735373795}, {"id": 194, "seek": 149832, "start": 1498.32, "end": 1508.04, "text": " a bit chemo with adding the backend, as you can see here. And these are all the, you already", "tokens": [257, 857, 4771, 78, 365, 5127, 264, 38087, 11, 382, 291, 393, 536, 510, 13, 400, 613, 366, 439, 264, 11, 291, 1217], "temperature": 0.0, "avg_logprob": -0.16353955476180368, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006967037334106863}, {"id": 195, "seek": 149832, "start": 1508.04, "end": 1515.6, "text": " know from the previous talk, all the configurations for Unicraft, the command line options. I", "tokens": [458, 490, 264, 3894, 751, 11, 439, 264, 31493, 337, 1156, 299, 4469, 11, 264, 5622, 1622, 3956, 13, 286], "temperature": 0.0, "avg_logprob": -0.16353955476180368, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006967037334106863}, {"id": 196, "seek": 149832, "start": 1515.6, "end": 1527.24, "text": " will also show you our docs. We have here an extended documentation. You can find how", "tokens": [486, 611, 855, 291, 527, 45623, 13, 492, 362, 510, 364, 10913, 14333, 13, 509, 393, 915, 577], "temperature": 0.0, "avg_logprob": -0.16353955476180368, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006967037334106863}, {"id": 197, "seek": 152724, "start": 1527.24, "end": 1535.16, "text": " to run VXL application in VM, how to run it remotely. We also have it, it doesn't show", "tokens": [281, 1190, 691, 55, 43, 3861, 294, 18038, 11, 577, 281, 1190, 309, 20824, 13, 492, 611, 362, 309, 11, 309, 1177, 380, 855], "temperature": 0.0, "avg_logprob": -0.20269197684067947, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.0009945605415850878}, {"id": 198, "seek": 152724, "start": 1535.16, "end": 1550.64, "text": " here, but we also have... Okay. Maybe more. Okay, so here we also have all the things", "tokens": [510, 11, 457, 321, 611, 362, 485, 1033, 13, 2704, 544, 13, 1033, 11, 370, 510, 321, 611, 362, 439, 264, 721], "temperature": 0.0, "avg_logprob": -0.20269197684067947, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.0009945605415850878}, {"id": 199, "seek": 155064, "start": 1550.64, "end": 1559.0, "text": " that you need to do to try it out by yourself in Unicraft. And all of them are open source.", "tokens": [300, 291, 643, 281, 360, 281, 853, 309, 484, 538, 1803, 294, 1156, 299, 4469, 13, 400, 439, 295, 552, 366, 1269, 4009, 13], "temperature": 0.0, "avg_logprob": -0.18195033073425293, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011200912558706477}, {"id": 200, "seek": 155064, "start": 1559.0, "end": 1569.0400000000002, "text": " You can check them out, and you can clone them by yourself. So let me return. So currently", "tokens": [509, 393, 1520, 552, 484, 11, 293, 291, 393, 26506, 552, 538, 1803, 13, 407, 718, 385, 2736, 13, 407, 4362], "temperature": 0.0, "avg_logprob": -0.18195033073425293, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011200912558706477}, {"id": 201, "seek": 155064, "start": 1569.0400000000002, "end": 1578.96, "text": " VXL has bindings for... We actually released the version 0.5, and currently there is bind...", "tokens": [691, 55, 43, 575, 14786, 1109, 337, 485, 492, 767, 4736, 264, 3037, 1958, 13, 20, 11, 293, 4362, 456, 307, 14786, 485], "temperature": 0.0, "avg_logprob": -0.18195033073425293, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011200912558706477}, {"id": 202, "seek": 157896, "start": 1578.96, "end": 1589.2, "text": " We have language bindings for C, C++, Python, Rust, and also for TensorFlow. And we have", "tokens": [492, 362, 2856, 14786, 1109, 337, 383, 11, 383, 25472, 11, 15329, 11, 34952, 11, 293, 611, 337, 37624, 13, 400, 321, 362], "temperature": 0.0, "avg_logprob": -0.16203496191236708, "compression_ratio": 1.4052631578947368, "no_speech_prob": 0.000937255856115371}, {"id": 203, "seek": 157896, "start": 1589.2, "end": 1597.6000000000001, "text": " the plugin API that I talked before about extending VXL. You can also see how it is.", "tokens": [264, 23407, 9362, 300, 286, 2825, 949, 466, 24360, 691, 55, 43, 13, 509, 393, 611, 536, 577, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16203496191236708, "compression_ratio": 1.4052631578947368, "no_speech_prob": 0.000937255856115371}, {"id": 204, "seek": 157896, "start": 1597.6000000000001, "end": 1603.72, "text": " These are all the things that we have tested and we support right now. So from the hypervisor", "tokens": [1981, 366, 439, 264, 721, 300, 321, 362, 8246, 293, 321, 1406, 558, 586, 13, 407, 490, 264, 9848, 16457], "temperature": 0.0, "avg_logprob": -0.16203496191236708, "compression_ratio": 1.4052631578947368, "no_speech_prob": 0.000937255856115371}, {"id": 205, "seek": 160372, "start": 1603.72, "end": 1610.76, "text": " perspective, we have support for Chemo over Ritio and Vsoc. And for these new Rust VMMs,", "tokens": [4585, 11, 321, 362, 1406, 337, 21357, 78, 670, 497, 270, 1004, 293, 691, 539, 66, 13, 400, 337, 613, 777, 34952, 18038, 26386, 11], "temperature": 0.0, "avg_logprob": -0.2922042369842529, "compression_ratio": 1.3705583756345177, "no_speech_prob": 0.00038750050589442253}, {"id": 206, "seek": 160372, "start": 1610.76, "end": 1622.24, "text": " like Firecracker, Cloud Hypervisor, and Dragon Ball. Regarding Unicernals, we have working...", "tokens": [411, 7652, 10757, 23599, 11, 8061, 29592, 16457, 11, 293, 11517, 10744, 13, 35523, 1156, 299, 1248, 1124, 11, 321, 362, 1364, 485], "temperature": 0.0, "avg_logprob": -0.2922042369842529, "compression_ratio": 1.3705583756345177, "no_speech_prob": 0.00038750050589442253}, {"id": 207, "seek": 160372, "start": 1622.24, "end": 1627.88, "text": " It's currently working in Unicraft and in Rembrandt, but we want to also port it in OSV", "tokens": [467, 311, 4362, 1364, 294, 1156, 299, 4469, 293, 294, 4080, 30476, 83, 11, 457, 321, 528, 281, 611, 2436, 309, 294, 12731, 53], "temperature": 0.0, "avg_logprob": -0.2922042369842529, "compression_ratio": 1.3705583756345177, "no_speech_prob": 0.00038750050589442253}, {"id": 208, "seek": 162788, "start": 1627.88, "end": 1634.48, "text": " and maybe some more Unicernal frameworks. And we also have integration with Kubernetes,", "tokens": [293, 1310, 512, 544, 1156, 299, 1248, 304, 29834, 13, 400, 321, 611, 362, 10980, 365, 23145, 11], "temperature": 0.0, "avg_logprob": -0.26483864007994184, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.0006421399884857237}, {"id": 209, "seek": 162788, "start": 1634.48, "end": 1643.0, "text": " Cata containers, and OpenFuzz for serverless deployment. And these are all the acceleration", "tokens": [383, 3274, 17089, 11, 293, 7238, 37, 16740, 337, 7154, 1832, 19317, 13, 400, 613, 366, 439, 264, 17162], "temperature": 0.0, "avg_logprob": -0.26483864007994184, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.0006421399884857237}, {"id": 210, "seek": 162788, "start": 1643.0, "end": 1648.0800000000002, "text": " frameworks that we have tested and to work with VXL. So this is an inference that you", "tokens": [29834, 300, 321, 362, 8246, 293, 281, 589, 365, 691, 55, 43, 13, 407, 341, 307, 364, 38253, 300, 291], "temperature": 0.0, "avg_logprob": -0.26483864007994184, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.0006421399884857237}, {"id": 211, "seek": 162788, "start": 1648.0800000000002, "end": 1654.92, "text": " saw that we did the immense classification. We have TensorFlow and PyTorch support, TensorFlow", "tokens": [1866, 300, 321, 630, 264, 22920, 21538, 13, 492, 362, 37624, 293, 9953, 51, 284, 339, 1406, 11, 37624], "temperature": 0.0, "avg_logprob": -0.26483864007994184, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.0006421399884857237}, {"id": 212, "seek": 165492, "start": 1654.92, "end": 1662.28, "text": " 13, OpenVino, OpenCylo, CUDA that you saw with the other demo. And regarding hardware,", "tokens": [3705, 11, 7238, 53, 2982, 11, 7238, 34, 88, 752, 11, 29777, 7509, 300, 291, 1866, 365, 264, 661, 10723, 13, 400, 8595, 8837, 11], "temperature": 0.0, "avg_logprob": -0.3217101946268996, "compression_ratio": 1.4301075268817205, "no_speech_prob": 0.00019811818492598832}, {"id": 213, "seek": 165492, "start": 1662.28, "end": 1675.16, "text": " we have tested with GPUs, edge devices like Coral, and also FPGAs. So to sum up, hardware", "tokens": [321, 362, 8246, 365, 18407, 82, 11, 4691, 5759, 411, 3925, 304, 11, 293, 611, 36655, 38, 10884, 13, 407, 281, 2408, 493, 11, 8837], "temperature": 0.0, "avg_logprob": -0.3217101946268996, "compression_ratio": 1.4301075268817205, "no_speech_prob": 0.00019811818492598832}, {"id": 214, "seek": 165492, "start": 1675.16, "end": 1681.44, "text": " accelerations are... The software stack of hardware accelerators are huge and complicated", "tokens": [10172, 763, 366, 485, 440, 4722, 8630, 295, 8837, 10172, 3391, 366, 2603, 293, 6179], "temperature": 0.0, "avg_logprob": -0.3217101946268996, "compression_ratio": 1.4301075268817205, "no_speech_prob": 0.00019811818492598832}, {"id": 215, "seek": 168144, "start": 1681.44, "end": 1690.88, "text": " to be ported easily in Unicernals. And we have VXL which is able to abstract the heterogeneity", "tokens": [281, 312, 2436, 292, 3612, 294, 1156, 299, 1248, 1124, 13, 400, 321, 362, 691, 55, 43, 597, 307, 1075, 281, 12649, 264, 20789, 23360, 507], "temperature": 0.0, "avg_logprob": -0.09054017368751237, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0006380712729878724}, {"id": 216, "seek": 168144, "start": 1690.88, "end": 1697.72, "text": " both in the hardware and in the software. And it sounds like a perfect fit for Unicernals.", "tokens": [1293, 294, 264, 8837, 293, 294, 264, 4722, 13, 400, 309, 3263, 411, 257, 2176, 3318, 337, 1156, 299, 1248, 1124, 13], "temperature": 0.0, "avg_logprob": -0.09054017368751237, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0006380712729878724}, {"id": 217, "seek": 168144, "start": 1697.72, "end": 1703.92, "text": " So if you want, you can try it out by yourselves. Here are all the links that you can use and", "tokens": [407, 498, 291, 528, 11, 291, 393, 853, 309, 484, 538, 14791, 13, 1692, 366, 439, 264, 6123, 300, 291, 393, 764, 293], "temperature": 0.0, "avg_logprob": -0.09054017368751237, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0006380712729878724}, {"id": 218, "seek": 170392, "start": 1703.92, "end": 1713.2, "text": " test them out. And we would like to mention that this work is partially funded from two", "tokens": [1500, 552, 484, 13, 400, 321, 576, 411, 281, 2152, 300, 341, 589, 307, 18886, 14385, 490, 732], "temperature": 0.0, "avg_logprob": -0.20602882740109466, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0006998428143560886}, {"id": 219, "seek": 170392, "start": 1713.2, "end": 1720.1200000000001, "text": " Horizon projects, Ceraan and 5G Complete. And we would also like to invite you in the", "tokens": [40102, 4455, 11, 383, 1663, 282, 293, 1025, 38, 34687, 13, 400, 321, 576, 611, 411, 281, 7980, 291, 294, 264], "temperature": 0.0, "avg_logprob": -0.20602882740109466, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0006998428143560886}, {"id": 220, "seek": 170392, "start": 1720.1200000000001, "end": 1726.52, "text": " Unicraft hackathon that will take place in Athens at the end of March. And thank you", "tokens": [1156, 299, 4469, 10339, 18660, 300, 486, 747, 1081, 294, 32530, 412, 264, 917, 295, 6129, 13, 400, 1309, 291], "temperature": 0.0, "avg_logprob": -0.20602882740109466, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0006998428143560886}, {"id": 221, "seek": 172652, "start": 1726.52, "end": 1736.8, "text": " for your attention. If you have any questions, we will be happy to answer them.", "tokens": [337, 428, 3202, 13, 759, 291, 362, 604, 1651, 11, 321, 486, 312, 2055, 281, 1867, 552, 13], "temperature": 0.0, "avg_logprob": -0.21509083339146207, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013006404042243958}, {"id": 222, "seek": 172652, "start": 1736.8, "end": 1740.36, "text": " Thank you so much, Babi. So for the third time, we'll welcome you in Athens in late March", "tokens": [1044, 291, 370, 709, 11, 15820, 72, 13, 407, 337, 264, 2636, 565, 11, 321, 603, 2928, 291, 294, 32530, 294, 3469, 6129], "temperature": 0.0, "avg_logprob": -0.21509083339146207, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013006404042243958}, {"id": 223, "seek": 172652, "start": 1740.36, "end": 1750.12, "text": " for the hackathon. If there are any questions from the audience? Yeah, please. Thank you.", "tokens": [337, 264, 10339, 18660, 13, 759, 456, 366, 604, 1651, 490, 264, 4034, 30, 865, 11, 1767, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.21509083339146207, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.0013006404042243958}, {"id": 224, "seek": 175012, "start": 1750.12, "end": 1758.8, "text": " Great stuff. I have a question about the potential future and the performance that we are currently", "tokens": [3769, 1507, 13, 286, 362, 257, 1168, 466, 264, 3995, 2027, 293, 264, 3389, 300, 321, 366, 4362], "temperature": 0.0, "avg_logprob": -0.198728396097819, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0015963866608217359}, {"id": 225, "seek": 175012, "start": 1758.8, "end": 1764.8799999999999, "text": " maybe possibly losing to the usage of API and transport. What do you think is a potential", "tokens": [1310, 6264, 7027, 281, 264, 14924, 295, 9362, 293, 5495, 13, 708, 360, 291, 519, 307, 257, 3995], "temperature": 0.0, "avg_logprob": -0.198728396097819, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0015963866608217359}, {"id": 226, "seek": 175012, "start": 1764.8799999999999, "end": 1770.04, "text": " in more increase of performance given that framework?", "tokens": [294, 544, 3488, 295, 3389, 2212, 300, 8388, 30], "temperature": 0.0, "avg_logprob": -0.198728396097819, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0015963866608217359}, {"id": 227, "seek": 175012, "start": 1770.04, "end": 1776.1599999999999, "text": " Yeah, actually, the transport is actually, yes, it's bottleneck since you have all these", "tokens": [865, 11, 767, 11, 264, 5495, 307, 767, 11, 2086, 11, 309, 311, 44641, 547, 1670, 291, 362, 439, 613], "temperature": 0.0, "avg_logprob": -0.198728396097819, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0015963866608217359}, {"id": 228, "seek": 177616, "start": 1776.16, "end": 1789.0400000000002, "text": " transfers that take place. But we think that at the end, we will have still very good execution", "tokens": [29137, 300, 747, 1081, 13, 583, 321, 519, 300, 412, 264, 917, 11, 321, 486, 362, 920, 588, 665, 15058], "temperature": 0.0, "avg_logprob": -0.15574073791503906, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0004160247044637799}, {"id": 229, "seek": 177616, "start": 1789.0400000000002, "end": 1797.44, "text": " times, very good performance. And it's also important to mention that we can also set", "tokens": [1413, 11, 588, 665, 3389, 13, 400, 309, 311, 611, 1021, 281, 2152, 300, 321, 393, 611, 992], "temperature": 0.0, "avg_logprob": -0.15574073791503906, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0004160247044637799}, {"id": 230, "seek": 177616, "start": 1797.44, "end": 1802.2, "text": " up the environment and everything so you can minimize the transfers. For example, you can", "tokens": [493, 264, 2823, 293, 1203, 370, 291, 393, 17522, 264, 29137, 13, 1171, 1365, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.15574073791503906, "compression_ratio": 1.5574712643678161, "no_speech_prob": 0.0004160247044637799}, {"id": 231, "seek": 180220, "start": 1802.2, "end": 1808.6000000000001, "text": " have your model. If you have a TensorFlow model or anything, we are working on how it", "tokens": [362, 428, 2316, 13, 759, 291, 362, 257, 37624, 2316, 420, 1340, 11, 321, 366, 1364, 322, 577, 309], "temperature": 0.0, "avg_logprob": -0.17207831256794479, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.00718275923281908}, {"id": 232, "seek": 180220, "start": 1808.6000000000001, "end": 1813.72, "text": " can be done and prefetching it before you deploy the function in the host and having", "tokens": [393, 312, 1096, 293, 18417, 7858, 278, 309, 949, 291, 7274, 264, 2445, 294, 264, 3975, 293, 1419], "temperature": 0.0, "avg_logprob": -0.17207831256794479, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.00718275923281908}, {"id": 233, "seek": 180220, "start": 1813.72, "end": 1818.1200000000001, "text": " everything there so you don't have to transfer from the VM to the host and vice versa and", "tokens": [1203, 456, 370, 291, 500, 380, 362, 281, 5003, 490, 264, 18038, 281, 264, 3975, 293, 11964, 25650, 293], "temperature": 0.0, "avg_logprob": -0.17207831256794479, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.00718275923281908}, {"id": 234, "seek": 180220, "start": 1818.1200000000001, "end": 1823.48, "text": " all of these things. Actually, if I may intervene, so these are", "tokens": [439, 295, 613, 721, 13, 5135, 11, 498, 286, 815, 30407, 11, 370, 613, 366], "temperature": 0.0, "avg_logprob": -0.17207831256794479, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.00718275923281908}, {"id": 235, "seek": 180220, "start": 1823.48, "end": 1830.3600000000001, "text": " two issues. The first issue is all the resources, the models, the out-of-band stuff that you", "tokens": [732, 2663, 13, 440, 700, 2734, 307, 439, 264, 3593, 11, 264, 5245, 11, 264, 484, 12, 2670, 12, 4235, 1507, 300, 291], "temperature": 0.0, "avg_logprob": -0.17207831256794479, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.00718275923281908}, {"id": 236, "seek": 183036, "start": 1830.36, "end": 1839.36, "text": " can do in a separate API, in a cloud environment, in a serverless deployment. And the second", "tokens": [393, 360, 294, 257, 4994, 9362, 11, 294, 257, 4588, 2823, 11, 294, 257, 7154, 1832, 19317, 13, 400, 264, 1150], "temperature": 0.0, "avg_logprob": -0.2527627472830291, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.0023254361003637314}, {"id": 237, "seek": 183036, "start": 1839.36, "end": 1846.6399999999999, "text": " thing about the actual transfers for Virtio or Visoc, the thing is that since we semantically", "tokens": [551, 466, 264, 3539, 29137, 337, 19447, 1004, 420, 10410, 905, 11, 264, 551, 307, 300, 1670, 321, 4361, 49505], "temperature": 0.0, "avg_logprob": -0.2527627472830291, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.0023254361003637314}, {"id": 238, "seek": 183036, "start": 1846.6399999999999, "end": 1853.24, "text": " abstract the whole operation, you don't have to do a CUDA, MIMCOPY, CUDA malloc, CUDA something,", "tokens": [12649, 264, 1379, 6916, 11, 291, 500, 380, 362, 281, 360, 257, 29777, 7509, 11, 376, 6324, 34, 12059, 56, 11, 29777, 7509, 16026, 905, 11, 29777, 7509, 746, 11], "temperature": 0.0, "avg_logprob": -0.2527627472830291, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.0023254361003637314}, {"id": 239, "seek": 183036, "start": 1853.24, "end": 1859.6799999999998, "text": " set kernel, whatever, and you don't have this latency in the transfer. So it minimizes", "tokens": [992, 28256, 11, 2035, 11, 293, 291, 500, 380, 362, 341, 27043, 294, 264, 5003, 13, 407, 309, 4464, 5660], "temperature": 0.0, "avg_logprob": -0.2527627472830291, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.0023254361003637314}, {"id": 240, "seek": 185968, "start": 1859.68, "end": 1866.28, "text": " the overhead just to the part of copying the data across, so the actual data, the input", "tokens": [264, 19922, 445, 281, 264, 644, 295, 27976, 264, 1412, 2108, 11, 370, 264, 3539, 1412, 11, 264, 4846], "temperature": 0.0, "avg_logprob": -0.15862339799122144, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0012250366853550076}, {"id": 241, "seek": 185968, "start": 1866.28, "end": 1873.92, "text": " data and the output. So this is really, really minimal. So in VMs that we have tested, we", "tokens": [1412, 293, 264, 5598, 13, 407, 341, 307, 534, 11, 534, 13206, 13, 407, 294, 18038, 82, 300, 321, 362, 8246, 11, 321], "temperature": 0.0, "avg_logprob": -0.15862339799122144, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0012250366853550076}, {"id": 242, "seek": 185968, "start": 1873.92, "end": 1879.3600000000001, "text": " have tested remotely, but the network is not that good, so we need to do more tests there.", "tokens": [362, 8246, 20824, 11, 457, 264, 3209, 307, 406, 300, 665, 11, 370, 321, 643, 281, 360, 544, 6921, 456, 13], "temperature": 0.0, "avg_logprob": -0.15862339799122144, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0012250366853550076}, {"id": 243, "seek": 185968, "start": 1879.3600000000001, "end": 1886.16, "text": " But in VMs that we have tested, the overhead is less than 5%. For an image classification", "tokens": [583, 294, 18038, 82, 300, 321, 362, 8246, 11, 264, 19922, 307, 1570, 813, 1025, 6856, 1171, 364, 3256, 21538], "temperature": 0.0, "avg_logprob": -0.15862339799122144, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0012250366853550076}, {"id": 244, "seek": 188616, "start": 1886.16, "end": 1894.8000000000002, "text": " of 32K to a MEG, something like that. So it's really, really small, the overhead for the", "tokens": [295, 8858, 42, 281, 257, 12003, 38, 11, 746, 411, 300, 13, 407, 309, 311, 534, 11, 534, 1359, 11, 264, 19922, 337, 264], "temperature": 0.0, "avg_logprob": -0.2283543480767144, "compression_ratio": 1.5, "no_speech_prob": 0.004497908521443605}, {"id": 245, "seek": 188616, "start": 1894.8000000000002, "end": 1900.24, "text": " transport layer, both Virtio and Visoc. The Visoc part is a bit more because it serializes", "tokens": [5495, 4583, 11, 1293, 19447, 1004, 293, 10410, 905, 13, 440, 10410, 905, 644, 307, 257, 857, 544, 570, 309, 17436, 5660], "temperature": 0.0, "avg_logprob": -0.2283543480767144, "compression_ratio": 1.5, "no_speech_prob": 0.004497908521443605}, {"id": 246, "seek": 188616, "start": 1900.24, "end": 1905.8000000000002, "text": " the stuff through protobufs and the stack is a bit complicated, but the Virtio stuff", "tokens": [264, 1507, 807, 1742, 996, 2947, 82, 293, 264, 8630, 307, 257, 857, 6179, 11, 457, 264, 19447, 1004, 1507], "temperature": 0.0, "avg_logprob": -0.2283543480767144, "compression_ratio": 1.5, "no_speech_prob": 0.004497908521443605}, {"id": 247, "seek": 188616, "start": 1905.8000000000002, "end": 1909.24, "text": " is really super efficient.", "tokens": [307, 534, 1687, 7148, 13], "temperature": 0.0, "avg_logprob": -0.2283543480767144, "compression_ratio": 1.5, "no_speech_prob": 0.004497908521443605}, {"id": 248, "seek": 190924, "start": 1909.24, "end": 1918.56, "text": " Hi, so thank you for the talk. My question would be kind of almost on the same thing,", "tokens": [2421, 11, 370, 1309, 291, 337, 264, 751, 13, 1222, 1168, 576, 312, 733, 295, 1920, 322, 264, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.2282131643856273, "compression_ratio": 1.5656565656565657, "no_speech_prob": 0.004894664045423269}, {"id": 249, "seek": 190924, "start": 1918.56, "end": 1923.92, "text": " but from the security perspective. So if we kind of offload a lot of computation out of", "tokens": [457, 490, 264, 3825, 4585, 13, 407, 498, 321, 733, 295, 766, 2907, 257, 688, 295, 24903, 484, 295], "temperature": 0.0, "avg_logprob": -0.2282131643856273, "compression_ratio": 1.5656565656565657, "no_speech_prob": 0.004894664045423269}, {"id": 250, "seek": 190924, "start": 1923.92, "end": 1931.56, "text": " the Unicernel to the host again, I guess security, at least the isolation is a thing", "tokens": [264, 1156, 299, 1248, 338, 281, 264, 3975, 797, 11, 286, 2041, 3825, 11, 412, 1935, 264, 16001, 307, 257, 551], "temperature": 0.0, "avg_logprob": -0.2282131643856273, "compression_ratio": 1.5656565656565657, "no_speech_prob": 0.004894664045423269}, {"id": 251, "seek": 190924, "start": 1931.56, "end": 1935.48, "text": " to think about. So if you, any words on this topic?", "tokens": [281, 519, 466, 13, 407, 498, 291, 11, 604, 2283, 322, 341, 4829, 30], "temperature": 0.0, "avg_logprob": -0.2282131643856273, "compression_ratio": 1.5656565656565657, "no_speech_prob": 0.004894664045423269}, {"id": 252, "seek": 193548, "start": 1935.48, "end": 1940.3600000000001, "text": " Yeah, you can take it. It's yours.", "tokens": [865, 11, 291, 393, 747, 309, 13, 467, 311, 6342, 13], "temperature": 0.0, "avg_logprob": -0.2996595726638544, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.006475502159446478}, {"id": 253, "seek": 193548, "start": 1940.3600000000001, "end": 1950.3600000000001, "text": " Okay, we agree. Yes, there are issues with security because essentially you need to run", "tokens": [1033, 11, 321, 3986, 13, 1079, 11, 456, 366, 2663, 365, 3825, 570, 4476, 291, 643, 281, 1190], "temperature": 0.0, "avg_logprob": -0.2996595726638544, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.006475502159446478}, {"id": 254, "seek": 193548, "start": 1950.3600000000001, "end": 1958.56, "text": " on Unicernel to be isolated, and now we push the execution to the host. So one of the things", "tokens": [322, 1156, 299, 1248, 338, 281, 312, 14621, 11, 293, 586, 321, 2944, 264, 15058, 281, 264, 3975, 13, 407, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.2996595726638544, "compression_ratio": 1.3870967741935485, "no_speech_prob": 0.006475502159446478}, {"id": 255, "seek": 195856, "start": 1958.56, "end": 1965.56, "text": " that we have thought about is that when you run that on a cloud environment, the vendor", "tokens": [300, 321, 362, 1194, 466, 307, 300, 562, 291, 1190, 300, 322, 257, 4588, 2823, 11, 264, 24321], "temperature": 0.0, "avg_logprob": -0.11982588768005371, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0021898807026445866}, {"id": 256, "seek": 195856, "start": 1965.56, "end": 1972.76, "text": " should make sure that whatever application is supported to be run on the host should", "tokens": [820, 652, 988, 300, 2035, 3861, 307, 8104, 281, 312, 1190, 322, 264, 3975, 820], "temperature": 0.0, "avg_logprob": -0.11982588768005371, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0021898807026445866}, {"id": 257, "seek": 195856, "start": 1972.76, "end": 1979.24, "text": " be secure, should be audited. So the user doesn't have all the possibilities available.", "tokens": [312, 7144, 11, 820, 312, 2379, 1226, 13, 407, 264, 4195, 1177, 380, 362, 439, 264, 12178, 2435, 13], "temperature": 0.0, "avg_logprob": -0.11982588768005371, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0021898807026445866}, {"id": 258, "seek": 195856, "start": 1979.24, "end": 1984.6799999999998, "text": " They cannot just exec something in the host. They will be able to exec specific stuff that", "tokens": [814, 2644, 445, 4454, 746, 294, 264, 3975, 13, 814, 486, 312, 1075, 281, 4454, 2685, 1507, 300], "temperature": 0.0, "avg_logprob": -0.11982588768005371, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0021898807026445866}, {"id": 259, "seek": 198468, "start": 1984.68, "end": 1994.28, "text": " are audited in libraries in the plug-in system. So one approach is this. Another response", "tokens": [366, 2379, 1226, 294, 15148, 294, 264, 5452, 12, 259, 1185, 13, 407, 472, 3109, 307, 341, 13, 3996, 4134], "temperature": 0.0, "avg_logprob": -0.13447077572345734, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.0012663898523896933}, {"id": 260, "seek": 198468, "start": 1994.28, "end": 2004.3200000000002, "text": " to the security implications is that at the moment you have no opportunity to run from", "tokens": [281, 264, 3825, 16602, 307, 300, 412, 264, 1623, 291, 362, 572, 2650, 281, 1190, 490], "temperature": 0.0, "avg_logprob": -0.13447077572345734, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.0012663898523896933}, {"id": 261, "seek": 198468, "start": 2004.3200000000002, "end": 2013.3600000000001, "text": " a Unicernel hardware accelerated workload. So if you want to be able to deploy such an", "tokens": [257, 1156, 299, 1248, 338, 8837, 29763, 20139, 13, 407, 498, 291, 528, 281, 312, 1075, 281, 7274, 1270, 364], "temperature": 0.0, "avg_logprob": -0.13447077572345734, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.0012663898523896933}, {"id": 262, "seek": 201336, "start": 2013.36, "end": 2026.0, "text": " application somewhere, then you can run isolated. You can use the whole hardware accelerator", "tokens": [3861, 4079, 11, 550, 291, 393, 1190, 14621, 13, 509, 393, 764, 264, 1379, 8837, 39889], "temperature": 0.0, "avg_logprob": -0.148622921534947, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018904886674135923}, {"id": 263, "seek": 201336, "start": 2026.0, "end": 2031.6399999999999, "text": " and have the same binary that you would deploy in a non-secure environment. So you could", "tokens": [293, 362, 264, 912, 17434, 300, 291, 576, 7274, 294, 257, 2107, 12, 8159, 540, 2823, 13, 407, 291, 727], "temperature": 0.0, "avg_logprob": -0.148622921534947, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018904886674135923}, {"id": 264, "seek": 201336, "start": 2031.6399999999999, "end": 2042.4399999999998, "text": " secure the environment, but have this compatibility and software supply mode using a Unicernel,", "tokens": [7144, 264, 2823, 11, 457, 362, 341, 34237, 293, 4722, 5847, 4391, 1228, 257, 1156, 299, 1248, 338, 11], "temperature": 0.0, "avg_logprob": -0.148622921534947, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018904886674135923}, {"id": 265, "seek": 204244, "start": 2042.44, "end": 2049.44, "text": " using this semantic abstraction, let's see.", "tokens": [1228, 341, 47982, 37765, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.3318042755126953, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.005025607068091631}, {"id": 266, "seek": 204244, "start": 2049.44, "end": 2056.44, "text": " Any other question? Yeah. Please.", "tokens": [2639, 661, 1168, 30, 865, 13, 2555, 13], "temperature": 0.0, "avg_logprob": -0.3318042755126953, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.005025607068091631}, {"id": 267, "seek": 204244, "start": 2056.44, "end": 2062.44, "text": " So my question is similar to the first question, but I'm wondering, because you can also do", "tokens": [407, 452, 1168, 307, 2531, 281, 264, 700, 1168, 11, 457, 286, 478, 6359, 11, 570, 291, 393, 611, 360], "temperature": 0.0, "avg_logprob": -0.3318042755126953, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.005025607068091631}, {"id": 268, "seek": 206244, "start": 2062.44, "end": 2073.6, "text": " GPU pass-through via KVM and just pass the GPU to a virtual machine. So I'm wondering", "tokens": [18407, 1320, 12, 11529, 5766, 591, 53, 44, 293, 445, 1320, 264, 18407, 281, 257, 6374, 3479, 13, 407, 286, 478, 6359], "temperature": 0.0, "avg_logprob": -0.22963651743802158, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.002866811351850629}, {"id": 269, "seek": 206244, "start": 2073.6, "end": 2079.92, "text": " what is the performance difference between doing that and doing it in VR?", "tokens": [437, 307, 264, 3389, 2649, 1296, 884, 300, 293, 884, 309, 294, 13722, 30], "temperature": 0.0, "avg_logprob": -0.22963651743802158, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.002866811351850629}, {"id": 270, "seek": 206244, "start": 2079.92, "end": 2085.0, "text": " Yes. Actually, we want to evaluate that, and we need to evaluate it and see how, for example,", "tokens": [1079, 13, 5135, 11, 321, 528, 281, 13059, 300, 11, 293, 321, 643, 281, 13059, 309, 293, 536, 577, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.22963651743802158, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.002866811351850629}, {"id": 271, "seek": 206244, "start": 2085.0, "end": 2090.56, "text": " with the even pass-through directly, like exposing the whole GPU to the VM, this could", "tokens": [365, 264, 754, 1320, 12, 11529, 3838, 11, 411, 33178, 264, 1379, 18407, 281, 264, 18038, 11, 341, 727], "temperature": 0.0, "avg_logprob": -0.22963651743802158, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.002866811351850629}, {"id": 272, "seek": 209056, "start": 2090.56, "end": 2100.64, "text": " be also one baseline for the valuation. Currently, I don't remember if we do have any measurements", "tokens": [312, 611, 472, 20518, 337, 264, 38546, 13, 19964, 11, 286, 500, 380, 1604, 498, 321, 360, 362, 604, 15383], "temperature": 0.0, "avg_logprob": -0.3568922974342524, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.0004699703131336719}, {"id": 273, "seek": 209056, "start": 2100.64, "end": 2101.64, "text": " already.", "tokens": [1217, 13], "temperature": 0.0, "avg_logprob": -0.3568922974342524, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.0004699703131336719}, {"id": 274, "seek": 209056, "start": 2101.64, "end": 2105.56, "text": " Would you consider the pass-through case the same as made?", "tokens": [6068, 291, 1949, 264, 1320, 12, 11529, 1389, 264, 912, 382, 1027, 30], "temperature": 0.0, "avg_logprob": -0.3568922974342524, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.0004699703131336719}, {"id": 275, "seek": 210556, "start": 2105.56, "end": 2123.7999999999997, "text": " Yeah, but I mean, if we have any, like, okay. Actually, from GPU virtualization, for example,", "tokens": [865, 11, 457, 286, 914, 11, 498, 321, 362, 604, 11, 411, 11, 1392, 13, 5135, 11, 490, 18407, 6374, 2144, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.23919366907190392, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.00014696996368002146}, {"id": 276, "seek": 210556, "start": 2123.7999999999997, "end": 2131.72, "text": " I'm not sure how many VMs can be supported in one single GPU, for example. I'm not aware", "tokens": [286, 478, 406, 988, 577, 867, 18038, 82, 393, 312, 8104, 294, 472, 2167, 18407, 11, 337, 1365, 13, 286, 478, 406, 3650], "temperature": 0.0, "avg_logprob": -0.23919366907190392, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.00014696996368002146}, {"id": 277, "seek": 213172, "start": 2131.72, "end": 2142.3199999999997, "text": " of any solution that can scale to, like, tens of VMs, even tens of VMs. I'm not sure if", "tokens": [295, 604, 3827, 300, 393, 4373, 281, 11, 411, 11, 10688, 295, 18038, 82, 11, 754, 10688, 295, 18038, 82, 13, 286, 478, 406, 988, 498], "temperature": 0.0, "avg_logprob": -0.19910135024633163, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00010184736311202869}, {"id": 278, "seek": 213172, "start": 2142.3199999999997, "end": 2150.3599999999997, "text": " there is any existing solution for that. But, yes, we plan it. We want to do some extended", "tokens": [456, 307, 604, 6741, 3827, 337, 300, 13, 583, 11, 2086, 11, 321, 1393, 309, 13, 492, 528, 281, 360, 512, 10913], "temperature": 0.0, "avg_logprob": -0.19910135024633163, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00010184736311202869}, {"id": 279, "seek": 213172, "start": 2150.3599999999997, "end": 2159.2799999999997, "text": " evaluation compared also to some, like, let's say, virtual GPU that exists or even the pass-through", "tokens": [13344, 5347, 611, 281, 512, 11, 411, 11, 718, 311, 584, 11, 6374, 18407, 300, 8198, 420, 754, 264, 1320, 12, 11529], "temperature": 0.0, "avg_logprob": -0.19910135024633163, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00010184736311202869}, {"id": 280, "seek": 215928, "start": 2159.28, "end": 2167.0800000000004, "text": " and native execution. We want to do that, and hopefully, we can also publish the results", "tokens": [293, 8470, 15058, 13, 492, 528, 281, 360, 300, 11, 293, 4696, 11, 321, 393, 611, 11374, 264, 3542], "temperature": 0.0, "avg_logprob": -0.30488736701734137, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.00018371143960393965}, {"id": 281, "seek": 215928, "start": 2167.0800000000004, "end": 2169.0800000000004, "text": " in our block.", "tokens": [294, 527, 3461, 13], "temperature": 0.0, "avg_logprob": -0.30488736701734137, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.00018371143960393965}, {"id": 282, "seek": 215928, "start": 2169.0800000000004, "end": 2171.6000000000004, "text": " Okay. Thank you.", "tokens": [1033, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.30488736701734137, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.00018371143960393965}, {"id": 283, "seek": 215928, "start": 2171.6000000000004, "end": 2174.7200000000003, "text": " Any other questions? Yeah.", "tokens": [2639, 661, 1651, 30, 865, 13], "temperature": 0.0, "avg_logprob": -0.30488736701734137, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.00018371143960393965}, {"id": 284, "seek": 215928, "start": 2174.7200000000003, "end": 2186.1600000000003, "text": " So, in response to the first security question about, yeah, we are offloading now compute", "tokens": [407, 11, 294, 4134, 281, 264, 700, 3825, 1168, 466, 11, 1338, 11, 321, 366, 766, 2907, 278, 586, 14722], "temperature": 0.0, "avg_logprob": -0.30488736701734137, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.00018371143960393965}, {"id": 285, "seek": 218616, "start": 2186.16, "end": 2192.52, "text": " to the hypervisor and host. So, does it imply that there is a possibility to break out of", "tokens": [281, 264, 9848, 16457, 293, 3975, 13, 407, 11, 775, 309, 33616, 300, 456, 307, 257, 7959, 281, 1821, 484, 295], "temperature": 0.0, "avg_logprob": -0.29174715677897134, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.004336351063102484}, {"id": 286, "seek": 218616, "start": 2192.52, "end": 2196.48, "text": " the containerization with BXL?", "tokens": [264, 10129, 2144, 365, 363, 55, 43, 30], "temperature": 0.0, "avg_logprob": -0.29174715677897134, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.004336351063102484}, {"id": 287, "seek": 218616, "start": 2196.48, "end": 2211.2799999999997, "text": " Well, there's, yes, yes, code is going to be executing on the host in a privileged level.", "tokens": [1042, 11, 456, 311, 11, 2086, 11, 2086, 11, 3089, 307, 516, 281, 312, 32368, 322, 264, 3975, 294, 257, 25293, 1496, 13], "temperature": 0.0, "avg_logprob": -0.29174715677897134, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.004336351063102484}, {"id": 288, "seek": 221128, "start": 2211.28, "end": 2224.88, "text": " Yes. But the other option is what? So, yeah, there is a trade.", "tokens": [1079, 13, 583, 264, 661, 3614, 307, 437, 30, 407, 11, 1338, 11, 456, 307, 257, 4923, 13], "temperature": 0.0, "avg_logprob": -0.1768364979670598, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.0017398636555299163}, {"id": 289, "seek": 221128, "start": 2224.88, "end": 2230.32, "text": " We are actually working. We want to see what available sources we have there. How can we", "tokens": [492, 366, 767, 1364, 13, 492, 528, 281, 536, 437, 2435, 7139, 321, 362, 456, 13, 1012, 393, 321], "temperature": 0.0, "avg_logprob": -0.1768364979670598, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.0017398636555299163}, {"id": 290, "seek": 221128, "start": 2230.32, "end": 2236.32, "text": " make it more secure? How we can sandbox it somehow to make it look better? But on the", "tokens": [652, 309, 544, 7144, 30, 1012, 321, 393, 42115, 309, 6063, 281, 652, 309, 574, 1101, 30, 583, 322, 264], "temperature": 0.0, "avg_logprob": -0.1768364979670598, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.0017398636555299163}, {"id": 291, "seek": 223632, "start": 2236.32, "end": 2242.84, "text": " other hand, like, for example, in FPTAs, there's no MMU, there's nothing. If you run two kernels,", "tokens": [661, 1011, 11, 411, 11, 337, 1365, 11, 294, 36655, 51, 10884, 11, 456, 311, 572, 34191, 52, 11, 456, 311, 1825, 13, 759, 291, 1190, 732, 23434, 1625, 11], "temperature": 0.0, "avg_logprob": -0.15111819435568416, "compression_ratio": 1.6839622641509433, "no_speech_prob": 9.273125033359975e-05}, {"id": 292, "seek": 223632, "start": 2242.84, "end": 2247.2000000000003, "text": " one kernel can access, if you kind of know what to do, one kernel can access all the", "tokens": [472, 28256, 393, 2105, 11, 498, 291, 733, 295, 458, 437, 281, 360, 11, 472, 28256, 393, 2105, 439, 264], "temperature": 0.0, "avg_logprob": -0.15111819435568416, "compression_ratio": 1.6839622641509433, "no_speech_prob": 9.273125033359975e-05}, {"id": 293, "seek": 223632, "start": 2247.2000000000003, "end": 2251.6400000000003, "text": " memory in the whole FPTAs, for example. So, in one hand, you also need support from the", "tokens": [4675, 294, 264, 1379, 36655, 51, 10884, 11, 337, 1365, 13, 407, 11, 294, 472, 1011, 11, 291, 611, 643, 1406, 490, 264], "temperature": 0.0, "avg_logprob": -0.15111819435568416, "compression_ratio": 1.6839622641509433, "no_speech_prob": 9.273125033359975e-05}, {"id": 294, "seek": 223632, "start": 2251.6400000000003, "end": 2257.56, "text": " hardware. And regarding, for example, the software stack, we are looking at it and see", "tokens": [8837, 13, 400, 8595, 11, 337, 1365, 11, 264, 4722, 8630, 11, 321, 366, 1237, 412, 309, 293, 536], "temperature": 0.0, "avg_logprob": -0.15111819435568416, "compression_ratio": 1.6839622641509433, "no_speech_prob": 9.273125033359975e-05}, {"id": 295, "seek": 225756, "start": 2257.56, "end": 2269.88, "text": " how this can, how can we extend and make it more, at least, increase the difficulty for", "tokens": [577, 341, 393, 11, 577, 393, 321, 10101, 293, 652, 309, 544, 11, 412, 1935, 11, 3488, 264, 10360, 337], "temperature": 0.0, "avg_logprob": -0.3116287015519052, "compression_ratio": 1.4, "no_speech_prob": 0.0008196976268664002}, {"id": 296, "seek": 225756, "start": 2269.88, "end": 2270.88, "text": " having any.", "tokens": [1419, 604, 13], "temperature": 0.0, "avg_logprob": -0.3116287015519052, "compression_ratio": 1.4, "no_speech_prob": 0.0008196976268664002}, {"id": 297, "seek": 225756, "start": 2270.88, "end": 2279.84, "text": " So, for example, in the Cata containers integration that we have, so when you spawn a container,", "tokens": [407, 11, 337, 1365, 11, 294, 264, 383, 3274, 17089, 10980, 300, 321, 362, 11, 370, 562, 291, 17088, 257, 10129, 11], "temperature": 0.0, "avg_logprob": -0.3116287015519052, "compression_ratio": 1.4, "no_speech_prob": 0.0008196976268664002}, {"id": 298, "seek": 227984, "start": 2279.84, "end": 2289.2000000000003, "text": " you sandbox the container in a VM, our agent, the host part of the Excel is running on the", "tokens": [291, 42115, 264, 10129, 294, 257, 18038, 11, 527, 9461, 11, 264, 3975, 644, 295, 264, 19060, 307, 2614, 322, 264], "temperature": 0.0, "avg_logprob": -0.20340474859460608, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0013823667541146278}, {"id": 299, "seek": 227984, "start": 2289.2000000000003, "end": 2297.1600000000003, "text": " same sandbox, not in the VM, outside the VM. But it runs in the sandbox. So, yes, there", "tokens": [912, 42115, 11, 406, 294, 264, 18038, 11, 2380, 264, 18038, 13, 583, 309, 6676, 294, 264, 42115, 13, 407, 11, 2086, 11, 456], "temperature": 0.0, "avg_logprob": -0.20340474859460608, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0013823667541146278}, {"id": 300, "seek": 227984, "start": 2297.1600000000003, "end": 2307.36, "text": " is code executing on the host, but it's in the sandbox. So, it's kind of a tradeoff.", "tokens": [307, 3089, 32368, 322, 264, 3975, 11, 457, 309, 311, 294, 264, 42115, 13, 407, 11, 309, 311, 733, 295, 257, 4923, 4506, 13], "temperature": 0.0, "avg_logprob": -0.20340474859460608, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0013823667541146278}, {"id": 301, "seek": 230736, "start": 2307.36, "end": 2315.44, "text": " Anything else? Right? If not, thank you, Anastasia. Thank you, Babis. Yeah.", "tokens": [50364, 11998, 1646, 30, 1779, 30, 759, 406, 11, 1309, 291, 11, 1107, 525, 25251, 13, 1044, 291, 11, 15820, 271, 13, 865, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3317683293269231, "compression_ratio": 1.0, "no_speech_prob": 0.0021685895044356585}], "language": "en"}