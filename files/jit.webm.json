{"text": " Hello. I'll get started. Okay. My talk is entitled, The Next Frontier in Open Source Java Compilers, Just in Time, Compilation as a Service. Whoops, this isn't working. My name is Rich Agarty. I've been a software engineer for way too many years. I'm currently a developer advocate at IBM. So, we're all Java developers. We understand what a JVM and a JIT is. We'll do the JVM, execute your Java application during runtime. It sends the hot methods to the JIT to be compiled. With that in mind, we're going to talk about JIT as a service today. And we're going to break it down into three parts. First, I'm going to talk about a problem, right, which is Java running on cloud, specifically in distributed dynamic environments like microservices. Then we're going to talk about the reason, which is going to take us back to the JVM and the JIT, which has some great technology. It's great technology but does have some issues. And then the solution, which is the JIT as a service. So, is Java a good fit on the cloud? So, for context, we'll talk about legacy Java apps, enterprise apps running. They're all monoliths running on dedicated servers or VMs to ensure great performance. We loaded with a lot of memory and a lot of CPUs. They took forever to start, but it didn't matter because it never went down. We have clients running Java applications for years. If they did upgrade, it would be every six months to a year, do some simple refreshes. That was the world of legacy Java enterprise apps. Now we move to the cloud. That same monolith is a bunch of microservices talking to each other. They're all running in containers, managed by some cloud provider with a Kubernetes implementation to orchestrate. And we have auto-scaling up and down to meet demand. So the main motivators behind this, obviously, are flexibility and scalability. Easier to roll out new releases. You can have teams assigned to specific microservices and never touching other microservices. Once you're on the cloud, you can take advantage of the latest, greatest cloud technologies like serverless coming out. Obviously, you'd have less infrastructure to maintain and manage. And the ultimate goal is saving money. So before we start counting all our money, we've got to think about what about performance? So there's two variables that impact cost and performance. It's container size and the number of instances of your application you're running. Here's a graph showing all the ways we can get these variables wrong. Starting down here, containers are way too small. We're not running enough instances. It's pretty cheap, but the performance is unacceptable. On the opposite side, we have our containers are too big. Way too many instances running. Great performance, wasting money. So we need to get over here. This is a sweet spot. We got our container size just right. We have just enough instances for the demand. That's what we want to get to. Very hard to do. In fact, most conferences have a lot of talks about how to get here or their fixes for this problem. So before we can figure out how to fix it, we've got to figure out why it's so hard. And in order to do that, we've got to talk about the JVM in a JIT. So first of good, device-independent Java became so popular because we write once, run anywhere, in theory. 25 years of constant improvement, a lot of involvement from the community in it. The JIT itself, optimized code that runs great. It uses profiler, so it can optimize a code that you can't get doing it statically. Has very efficient garbage collection. And when the JVM collects more profile data in the JIT, compiles more methods, your code gets better and better. So the longer your job application runs, the better it gets. Now, the bad. So that initial execution of your code is interpreted, so it's relatively slow. Those hotspot methods compiled by the JIT can create CPU and memory spikes. CPU spikes cause lower quality of service, meaning performance. And your memory spikes cause out-of-memory issues, including crashes. In fact, the number one reason JVM, or a main reason JVM crashes because of out-of-memory issues. And we have slow startup and slow ramp-up time. So we want to distinguish between the two. Startup time is the time that it takes for that application to process first request, usually during an interpretation time. And ramp-up time is the time it takes a JIT to compile everything it wants to compile to get to that optimized version of your code. So here we have some graphs to back that up. Here we take a Java Enterprise application, and you can see on the left we got CPU spikes here happening initially, all because of JIT compilations. Same thing with the memory side. We got these large spikes that we have to account for. So let's go back to that graph I had finding that sweet spot. Now we have a little more information, but still we need to figure out a way to right-size those provisioned containers. And we got to make our auto-scaling efficient. So we have very little control over scaling. We control the size of our containers, but as far as scaling goes, we just have to set the environment enough up correctly so that auto-scaling is efficient. So on the container size portion of it, the main issue is we need to over-provision to handle those out-of-memory spikes, which is very hard to do, because JVMs have a non-deterministic behavior, meaning you can run the same application over and over, and you're going to get different spikes at different times. So you've got to run a series of tests with loading to figure out, to get that number kind of right. And on the auto-scaling part of things, again, we talk about the slow start-up and ramp-up times. The slower those are, the less effective your auto-scaling is going to be. And the CPU spikes can cause other issues. A lot of auto-scalers, the threshold for starting new instances is CPU load. So if you start a new instance and it's spinning, doing JIT compiles, your auto-scaler may detect that as a false positive, say, oh, you need, the demand is going up, you need more instances, when in this case, you really didn't. So it makes it very inefficient. So the solution to this problem is we need to minimize or eliminate those CPU spikes and memory spikes, and we've got to improve that start-up and ramp-up time. So we are proposing here, we're going to talk about JIT as a service, which is going to solve these issues, or help solve these issues. So the theory behind it is we're going to decouple the JIT compiler from the JVM and let it run as an independent process. Then we're going to offload those JIT compilations to that remote process from the client JVMs. As you can see here, we have two client JVMs talking to two remote JITs over here. We have the JIT still locally in the JVM that can be used if these become unavailable for some reason. Everything since we're all in containers is automatically managed by the orchestrator to make sure that we have their scaled correctly. This is actually a model to microsolution, so we're taking the model, as in this case, as a JVM. We're splitting it up into the JIT and everything left over in the other microservice. And again, like I mentioned, the local JIT still is available if this service goes down. So this actual technology does exist today, and it's called the JIT server, and it's a part of the Eclipse OpenJ9 JVM. It comes with the, it's also called the SAMRU cloud compiler when used with SAMRU runtimes, and I'll get to that in a minute. And I'm sure everyone here knows OpenJ9 combines with OpenJDK to form a full JDK and totally open-source it free to download. And here's a GitHub repo there. A little history of OpenJ9. It started life as the J9 JVM by IBM over 25 years ago. And the reason IBM developed it was because they had a whole range of devices they needed to support, and they wanted to make sure Java ran on all of them. That's all the way from handheld scanners to mainframes. So it was designed to go from small to large in both types of environments where you have a lot of memory or very, very little. And about five years ago, IBM decided to open-source it to the Eclipse Foundation. And OpenJ9 is renowned for its small footprint fast start-up and ramp-up time, which we'll get to in a minute. And again, even though it's got a new name, it's OpenJ9. All of IBM enterprise clients have been running their applications on this JVM for years. So there's a lot of history of success with it. Here's some OpenJ9 performance compared to Hotspot. Again, this doesn't take into account the JIT server. This is just the JVMs themselves going left to right here. OpenJ9's in green. Hotspot's in orange. So in certain circumstances, we got to see 51% faster start-up time, 50% smaller footprint after start-up. And it ramps up quicker than Hotspot. And at the very end, after a total full load, we have a 33% smaller footprint with OpenJ9. So, several run times. So that is IBM's OpenJDK distribution. Just like all the, someone just mentioned, there's a ton of distributions out there. This is IBM's. And it's the only one that comes with Eclipse OpenJ9 JVM. It's available no cost. It's stable. IBM puts their name behind it. So it comes in two editions, open source and certified. The only difference being the licensing and what platforms are supported. And if you're wondering what Samaru comes from, the name comes from, Mount Samaru is the tallest mountain on the island of, anyone know? Java, there you go. See how that makes sense? If I had a t-shirt, I would have given you that. Alright, from the perspective of the server or the client talking to this new JIT server, this is the advantages they're going to get. From a provisioning aspect, now it's going to be very easy to size our containers, right? We don't have to worry about those spikes anymore. So now we just, we level set based on the demand or the needs of the application itself. Performance wise, we're going to see improved ramp-up time, basically because the JIT server is going to be offloading. We're going to offload all the compiles in the CPU cycles to the JIT server. And there's also a feature in this JIT server called AOT cache. So it's going to store any method it compiles. So another instance of the same container application calling it, and then they'll have that method, it'll just return it. No compilation needed. Then from a cost standpoint, obviously any time you reduce your resource cost or your resource amounts, you're going to get a savings in cost. And I mentioned earlier the efficient auto scaling, you're only going to pay for what you need. Resiliency, remember the JVM still has their local JIT. So if the JIT server goes down, it could still keep going. So this is kind of an interesting chart. This is pretty big. So we're going to talk about some of the examples of where we see savings. So this is an experiment where we took four \u2013 let me see my pointer works \u2013 we took four job applications and we decided to size them correctly for the amount of the memory and CPU they needed doing all those load tests to figure out what this amount should be. And we have multiple instances of them. So the color indicates the application. You can see all the different replications. The relative size is shown with the scale of the square. And in this case, we used OpenShift to lay it out for us and it came out to use three nodes to handle all of this, all these applications in your instances. Then we introduced the JIT server, ran the same test. Here's our JIT server here, the brown. It's the biggest container in the nodes. But you notice the size of all of our containers for the applications goes way down. So we have the same number of instances in both cases, but we've just saved 33% of the resources. And if you're wondering how they perform \u2013 whoops, went too far \u2013 you see no difference. The orange is the baseline, the blue is the JIT server. And from a stable state, meaning once they've performed, they perform exactly the same. But we're, again, saving 33% of the resources. Now we'll take a look at some of the effects on auto-scaling in Kubernetes. Here we're running an application and we're setting our threshold, I think it's up there, at 50% of CPU. And you can see here all these plateaus are when the auto-scaler is going to launch another pod. And you can see how the JIT server in blue responds better. Shorter dips and they recover faster. And overall, your performance is going to be better with a JIT server. Also, that other thing I talked about with false positives. So, again, the auto-scaler is not going to be tricked into thinking that that CPU load from JIT compiles is the reason for demand. So you're going to get better behavior in auto-scaling. Two minutes. All right. When to use it? Obviously when the JVM is \u2013 we're in a memory and CPU constrained environment. Recommendations, you always use 10 to 20 client JVMs when you're talking to a JIT server. Because remember, that JIT server does take its own container. And it is communication over the network, so only adding encryption if you absolutely need it. So some final thoughts. We talked about the JIT provides great advantage that optimize code, but compilations do add overhead. So we disaggregate JIT from the JVM and we came up with this JIT compilation as a service. It's available in Eclipse OpenJ9, also called the SAMRU Cloud. It's called the Eclipse OpenJ9 JIT server. That's the technology. It's also called the SAMRU Cloud Compiler. It's available on Linux Java 8, 11, and 17. Really good with microcontainers. In fact, that's the only reason I'm bringing it up today. It's Kubernetes ready. You can improve your ramp-up time, auto-scaling. And here's the key point here I'll end with. So this is a Java solution to a Java problem. Initially I talked about that sweet spot space. So there's a lot of companies, a lot of vendors trying to figure out how to make that work better. And a lot of them involve doing other things besides what Java's all about, running the JVM, running the JIT. So it is a Java solution to your Java problem. That's it for me today. That QR code will take you to a page I have that has a bunch of articles on how to use it, also the slides and other good materials about it. That's it for me. Thank you very much. It sounds amazing. It's amazing. It really is amazing. Well, why wouldn't you? Open J9 is a perfectly, I mean, it's a viable JVM. It's nothing special, right? And nothing unique about it that makes you change your code. It's a JVM that just points to the open JDK, the open J9 JVM. Okay, here it comes. I think so because I've seen examples of using those apps in tests. Check that, yeah. Yeah, okay. That may be a problem. She go out and check the latest coverage of that. Well, the way the AOT cache will work in this case for the JIT server, it's going to keep all that information and the profile has to match from the requesting JVM, right? So if it matches, it'll use it, right? Because also on the clients, they also have their own cache. They'll keep it, but they go away once they go away, right? Or when you start a new instance of that app, you have a brand new flush cache. I'm sorry. Yeah, so that's what we were talking about. You want to go static. You're going to get a smaller image running statically, but you lose all the benefits of the JIT. Over time, yes. So that may be a great solution for short-lived apps, right? But the longer your job app runs, the more you're going to benefit from that optimized code, right? Yes? So Eclipse on the J9 is not a certain set-byte, but my main server is also a set-byte for open edition, but today it has available binaries. But for Eclipse, they are not able to actually release the binaries because they cannot actually access the TCK certification process. So that whole TCK issue is a, I don't know. Well, I guess I could say, it seems to be an issue more with IBM and Oracle, right? So our own tests are going to be, they're going to encompass all the TCK stuff. Open J9 is managed by Eclipse, but 99% of the contributions are from IBM. It's a big part of their business. It's not going to go anywhere. If you have to do open source, this is like the best of the most worlds, I think. It's available. It's open. You can see it, but you know you have a vendor who has their business based on it that it's not going to go anywhere, and they're going to put a lot of resources to making it better. So, you know, I'm just telling you right now that we just came up with a JIT server. We're going into beta on Instant On. I don't know if you've heard of that. It's based on CryU. So we're going to be able to take snapshots of those images, and you can put those in your containers. Those are going to start up in milliseconds. So JIT basically handles the JIT server, handles the ramp up time, but Instant On will handle the start up time. So we're talking milliseconds. That's coming out in the next couple of months or so. Anyway, thank you. Well, if you don't have the JIT, then you're going to be running interpreted. That's like the worst of everything. Oh, well, it won't be. But you still want to use the JIT remotely. Oh, you're talking about locally. It will not be used. It will not be used. By the way, yeah. And by the way, the JIT server is just another persona of the JVM. It's just running under a different persona. No, it won't do that. Okay. Thank you very much. Okay. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.76, "text": " Hello. I'll get started. Okay. My talk is entitled, The Next Frontier in Open Source", "tokens": [2425, 13, 286, 603, 483, 1409, 13, 1033, 13, 1222, 751, 307, 17838, 11, 440, 3087, 17348, 811, 294, 7238, 29629], "temperature": 0.0, "avg_logprob": -0.2639228202201225, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.10707227885723114}, {"id": 1, "seek": 0, "start": 13.76, "end": 21.2, "text": " Java Compilers, Just in Time, Compilation as a Service. Whoops, this isn't working.", "tokens": [10745, 6620, 388, 433, 11, 1449, 294, 6161, 11, 6620, 16067, 382, 257, 9561, 13, 45263, 11, 341, 1943, 380, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2639228202201225, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.10707227885723114}, {"id": 2, "seek": 0, "start": 21.2, "end": 25.28, "text": " My name is Rich Agarty. I've been a software engineer for way too many years. I'm currently", "tokens": [1222, 1315, 307, 6781, 2725, 446, 88, 13, 286, 600, 668, 257, 4722, 11403, 337, 636, 886, 867, 924, 13, 286, 478, 4362], "temperature": 0.0, "avg_logprob": -0.2639228202201225, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.10707227885723114}, {"id": 3, "seek": 2528, "start": 25.28, "end": 34.64, "text": " a developer advocate at IBM. So, we're all Java developers. We understand what a JVM", "tokens": [257, 10754, 14608, 412, 23487, 13, 407, 11, 321, 434, 439, 10745, 8849, 13, 492, 1223, 437, 257, 508, 53, 44], "temperature": 0.0, "avg_logprob": -0.1255008125305176, "compression_ratio": 1.56, "no_speech_prob": 3.163059227517806e-05}, {"id": 4, "seek": 2528, "start": 34.64, "end": 42.24, "text": " and a JIT is. We'll do the JVM, execute your Java application during runtime. It sends the", "tokens": [293, 257, 508, 3927, 307, 13, 492, 603, 360, 264, 508, 53, 44, 11, 14483, 428, 10745, 3861, 1830, 34474, 13, 467, 14790, 264], "temperature": 0.0, "avg_logprob": -0.1255008125305176, "compression_ratio": 1.56, "no_speech_prob": 3.163059227517806e-05}, {"id": 5, "seek": 2528, "start": 42.24, "end": 47.0, "text": " hot methods to the JIT to be compiled. With that in mind, we're going to talk about JIT", "tokens": [2368, 7150, 281, 264, 508, 3927, 281, 312, 36548, 13, 2022, 300, 294, 1575, 11, 321, 434, 516, 281, 751, 466, 508, 3927], "temperature": 0.0, "avg_logprob": -0.1255008125305176, "compression_ratio": 1.56, "no_speech_prob": 3.163059227517806e-05}, {"id": 6, "seek": 2528, "start": 47.0, "end": 50.92, "text": " as a service today. And we're going to break it down into three parts. First, I'm going", "tokens": [382, 257, 2643, 965, 13, 400, 321, 434, 516, 281, 1821, 309, 760, 666, 1045, 3166, 13, 2386, 11, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.1255008125305176, "compression_ratio": 1.56, "no_speech_prob": 3.163059227517806e-05}, {"id": 7, "seek": 5092, "start": 50.92, "end": 56.68, "text": " to talk about a problem, right, which is Java running on cloud, specifically in distributed", "tokens": [281, 751, 466, 257, 1154, 11, 558, 11, 597, 307, 10745, 2614, 322, 4588, 11, 4682, 294, 12631], "temperature": 0.0, "avg_logprob": -0.12980586832219904, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.3828312148689292e-05}, {"id": 8, "seek": 5092, "start": 56.68, "end": 63.28, "text": " dynamic environments like microservices. Then we're going to talk about the reason, which", "tokens": [8546, 12388, 411, 15547, 47480, 13, 1396, 321, 434, 516, 281, 751, 466, 264, 1778, 11, 597], "temperature": 0.0, "avg_logprob": -0.12980586832219904, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.3828312148689292e-05}, {"id": 9, "seek": 5092, "start": 63.28, "end": 68.04, "text": " is going to take us back to the JVM and the JIT, which has some great technology. It's", "tokens": [307, 516, 281, 747, 505, 646, 281, 264, 508, 53, 44, 293, 264, 508, 3927, 11, 597, 575, 512, 869, 2899, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.12980586832219904, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.3828312148689292e-05}, {"id": 10, "seek": 5092, "start": 68.04, "end": 73.2, "text": " great technology but does have some issues. And then the solution, which is the JIT as", "tokens": [869, 2899, 457, 775, 362, 512, 2663, 13, 400, 550, 264, 3827, 11, 597, 307, 264, 508, 3927, 382], "temperature": 0.0, "avg_logprob": -0.12980586832219904, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.3828312148689292e-05}, {"id": 11, "seek": 7320, "start": 73.2, "end": 84.12, "text": " a service. So, is Java a good fit on the cloud? So, for context, we'll talk about legacy", "tokens": [257, 2643, 13, 407, 11, 307, 10745, 257, 665, 3318, 322, 264, 4588, 30, 407, 11, 337, 4319, 11, 321, 603, 751, 466, 11711], "temperature": 0.0, "avg_logprob": -0.13955464927099084, "compression_ratio": 1.51931330472103, "no_speech_prob": 1.695369246590417e-05}, {"id": 12, "seek": 7320, "start": 84.12, "end": 89.0, "text": " Java apps, enterprise apps running. They're all monoliths running on dedicated servers", "tokens": [10745, 7733, 11, 14132, 7733, 2614, 13, 814, 434, 439, 1108, 29131, 82, 2614, 322, 8374, 15909], "temperature": 0.0, "avg_logprob": -0.13955464927099084, "compression_ratio": 1.51931330472103, "no_speech_prob": 1.695369246590417e-05}, {"id": 13, "seek": 7320, "start": 89.0, "end": 98.80000000000001, "text": " or VMs to ensure great performance. We loaded with a lot of memory and a lot of CPUs. They", "tokens": [420, 18038, 82, 281, 5586, 869, 3389, 13, 492, 13210, 365, 257, 688, 295, 4675, 293, 257, 688, 295, 13199, 82, 13, 814], "temperature": 0.0, "avg_logprob": -0.13955464927099084, "compression_ratio": 1.51931330472103, "no_speech_prob": 1.695369246590417e-05}, {"id": 14, "seek": 7320, "start": 98.80000000000001, "end": 102.92, "text": " took forever to start, but it didn't matter because it never went down. We have clients", "tokens": [1890, 5680, 281, 722, 11, 457, 309, 994, 380, 1871, 570, 309, 1128, 1437, 760, 13, 492, 362, 6982], "temperature": 0.0, "avg_logprob": -0.13955464927099084, "compression_ratio": 1.51931330472103, "no_speech_prob": 1.695369246590417e-05}, {"id": 15, "seek": 10292, "start": 102.92, "end": 108.28, "text": " running Java applications for years. If they did upgrade, it would be every six months to", "tokens": [2614, 10745, 5821, 337, 924, 13, 759, 436, 630, 11484, 11, 309, 576, 312, 633, 2309, 2493, 281], "temperature": 0.0, "avg_logprob": -0.11523032188415527, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.6261614038958214e-05}, {"id": 16, "seek": 10292, "start": 108.28, "end": 117.2, "text": " a year, do some simple refreshes. That was the world of legacy Java enterprise apps.", "tokens": [257, 1064, 11, 360, 512, 2199, 15134, 279, 13, 663, 390, 264, 1002, 295, 11711, 10745, 14132, 7733, 13], "temperature": 0.0, "avg_logprob": -0.11523032188415527, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.6261614038958214e-05}, {"id": 17, "seek": 10292, "start": 117.2, "end": 125.0, "text": " Now we move to the cloud. That same monolith is a bunch of microservices talking to each", "tokens": [823, 321, 1286, 281, 264, 4588, 13, 663, 912, 1108, 29131, 307, 257, 3840, 295, 15547, 47480, 1417, 281, 1184], "temperature": 0.0, "avg_logprob": -0.11523032188415527, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.6261614038958214e-05}, {"id": 18, "seek": 10292, "start": 125.0, "end": 131.8, "text": " other. They're all running in containers, managed by some cloud provider with a Kubernetes", "tokens": [661, 13, 814, 434, 439, 2614, 294, 17089, 11, 6453, 538, 512, 4588, 12398, 365, 257, 23145], "temperature": 0.0, "avg_logprob": -0.11523032188415527, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.6261614038958214e-05}, {"id": 19, "seek": 13180, "start": 131.8, "end": 143.12, "text": " implementation to orchestrate. And we have auto-scaling up and down to meet demand.", "tokens": [11420, 281, 14161, 4404, 13, 400, 321, 362, 8399, 12, 4417, 4270, 493, 293, 760, 281, 1677, 4733, 13], "temperature": 0.0, "avg_logprob": -0.16455646840537466, "compression_ratio": 1.5446428571428572, "no_speech_prob": 3.217274570488371e-05}, {"id": 20, "seek": 13180, "start": 143.12, "end": 148.12, "text": " So the main motivators behind this, obviously, are flexibility and scalability. Easier to", "tokens": [407, 264, 2135, 5426, 3391, 2261, 341, 11, 2745, 11, 366, 12635, 293, 15664, 2310, 13, 46879, 811, 281], "temperature": 0.0, "avg_logprob": -0.16455646840537466, "compression_ratio": 1.5446428571428572, "no_speech_prob": 3.217274570488371e-05}, {"id": 21, "seek": 13180, "start": 148.12, "end": 152.8, "text": " roll out new releases. You can have teams assigned to specific microservices and never", "tokens": [3373, 484, 777, 16952, 13, 509, 393, 362, 5491, 13279, 281, 2685, 15547, 47480, 293, 1128], "temperature": 0.0, "avg_logprob": -0.16455646840537466, "compression_ratio": 1.5446428571428572, "no_speech_prob": 3.217274570488371e-05}, {"id": 22, "seek": 13180, "start": 152.8, "end": 158.4, "text": " touching other microservices. Once you're on the cloud, you can take advantage of the", "tokens": [11175, 661, 15547, 47480, 13, 3443, 291, 434, 322, 264, 4588, 11, 291, 393, 747, 5002, 295, 264], "temperature": 0.0, "avg_logprob": -0.16455646840537466, "compression_ratio": 1.5446428571428572, "no_speech_prob": 3.217274570488371e-05}, {"id": 23, "seek": 15840, "start": 158.4, "end": 164.16, "text": " latest, greatest cloud technologies like serverless coming out. Obviously, you'd have less infrastructure", "tokens": [6792, 11, 6636, 4588, 7943, 411, 7154, 1832, 1348, 484, 13, 7580, 11, 291, 1116, 362, 1570, 6896], "temperature": 0.0, "avg_logprob": -0.1504579468777305, "compression_ratio": 1.5110132158590308, "no_speech_prob": 1.643765062908642e-05}, {"id": 24, "seek": 15840, "start": 164.16, "end": 172.4, "text": " to maintain and manage. And the ultimate goal is saving money.", "tokens": [281, 6909, 293, 3067, 13, 400, 264, 9705, 3387, 307, 6816, 1460, 13], "temperature": 0.0, "avg_logprob": -0.1504579468777305, "compression_ratio": 1.5110132158590308, "no_speech_prob": 1.643765062908642e-05}, {"id": 25, "seek": 15840, "start": 172.4, "end": 178.68, "text": " So before we start counting all our money, we've got to think about what about performance?", "tokens": [407, 949, 321, 722, 13251, 439, 527, 1460, 11, 321, 600, 658, 281, 519, 466, 437, 466, 3389, 30], "temperature": 0.0, "avg_logprob": -0.1504579468777305, "compression_ratio": 1.5110132158590308, "no_speech_prob": 1.643765062908642e-05}, {"id": 26, "seek": 15840, "start": 178.68, "end": 184.76, "text": " So there's two variables that impact cost and performance. It's container size and", "tokens": [407, 456, 311, 732, 9102, 300, 2712, 2063, 293, 3389, 13, 467, 311, 10129, 2744, 293], "temperature": 0.0, "avg_logprob": -0.1504579468777305, "compression_ratio": 1.5110132158590308, "no_speech_prob": 1.643765062908642e-05}, {"id": 27, "seek": 18476, "start": 184.76, "end": 191.23999999999998, "text": " the number of instances of your application you're running. Here's a graph showing all", "tokens": [264, 1230, 295, 14519, 295, 428, 3861, 291, 434, 2614, 13, 1692, 311, 257, 4295, 4099, 439], "temperature": 0.0, "avg_logprob": -0.16401549916208527, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.2463849745690823e-05}, {"id": 28, "seek": 18476, "start": 191.23999999999998, "end": 197.6, "text": " the ways we can get these variables wrong. Starting down here, containers are way too", "tokens": [264, 2098, 321, 393, 483, 613, 9102, 2085, 13, 16217, 760, 510, 11, 17089, 366, 636, 886], "temperature": 0.0, "avg_logprob": -0.16401549916208527, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.2463849745690823e-05}, {"id": 29, "seek": 18476, "start": 197.6, "end": 202.95999999999998, "text": " small. We're not running enough instances. It's pretty cheap, but the performance is", "tokens": [1359, 13, 492, 434, 406, 2614, 1547, 14519, 13, 467, 311, 1238, 7084, 11, 457, 264, 3389, 307], "temperature": 0.0, "avg_logprob": -0.16401549916208527, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.2463849745690823e-05}, {"id": 30, "seek": 18476, "start": 202.95999999999998, "end": 209.76, "text": " unacceptable. On the opposite side, we have our containers are too big. Way too many instances", "tokens": [31812, 13, 1282, 264, 6182, 1252, 11, 321, 362, 527, 17089, 366, 886, 955, 13, 9558, 886, 867, 14519], "temperature": 0.0, "avg_logprob": -0.16401549916208527, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.2463849745690823e-05}, {"id": 31, "seek": 20976, "start": 209.76, "end": 215.44, "text": " running. Great performance, wasting money. So we need to get over here. This is a sweet", "tokens": [2614, 13, 3769, 3389, 11, 20457, 1460, 13, 407, 321, 643, 281, 483, 670, 510, 13, 639, 307, 257, 3844], "temperature": 0.0, "avg_logprob": -0.125995725648016, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.884834430005867e-05}, {"id": 32, "seek": 20976, "start": 215.44, "end": 223.44, "text": " spot. We got our container size just right. We have just enough instances for the demand.", "tokens": [4008, 13, 492, 658, 527, 10129, 2744, 445, 558, 13, 492, 362, 445, 1547, 14519, 337, 264, 4733, 13], "temperature": 0.0, "avg_logprob": -0.125995725648016, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.884834430005867e-05}, {"id": 33, "seek": 20976, "start": 223.44, "end": 228.0, "text": " That's what we want to get to. Very hard to do. In fact, most conferences have a lot", "tokens": [663, 311, 437, 321, 528, 281, 483, 281, 13, 4372, 1152, 281, 360, 13, 682, 1186, 11, 881, 22032, 362, 257, 688], "temperature": 0.0, "avg_logprob": -0.125995725648016, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.884834430005867e-05}, {"id": 34, "seek": 20976, "start": 228.0, "end": 234.0, "text": " of talks about how to get here or their fixes for this problem. So before we can figure", "tokens": [295, 6686, 466, 577, 281, 483, 510, 420, 641, 32539, 337, 341, 1154, 13, 407, 949, 321, 393, 2573], "temperature": 0.0, "avg_logprob": -0.125995725648016, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.884834430005867e-05}, {"id": 35, "seek": 20976, "start": 234.0, "end": 238.51999999999998, "text": " out how to fix it, we've got to figure out why it's so hard. And in order to do that,", "tokens": [484, 577, 281, 3191, 309, 11, 321, 600, 658, 281, 2573, 484, 983, 309, 311, 370, 1152, 13, 400, 294, 1668, 281, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.125995725648016, "compression_ratio": 1.6577946768060836, "no_speech_prob": 2.884834430005867e-05}, {"id": 36, "seek": 23852, "start": 238.52, "end": 248.76000000000002, "text": " we've got to talk about the JVM in a JIT. So first of good, device-independent Java became", "tokens": [321, 600, 658, 281, 751, 466, 264, 508, 53, 44, 294, 257, 508, 3927, 13, 407, 700, 295, 665, 11, 4302, 12, 471, 4217, 317, 10745, 3062], "temperature": 0.0, "avg_logprob": -0.2029545405139662, "compression_ratio": 1.3826530612244898, "no_speech_prob": 2.4287473934236914e-05}, {"id": 37, "seek": 23852, "start": 248.76000000000002, "end": 254.88, "text": " so popular because we write once, run anywhere, in theory. 25 years of constant improvement,", "tokens": [370, 3743, 570, 321, 2464, 1564, 11, 1190, 4992, 11, 294, 5261, 13, 3552, 924, 295, 5754, 10444, 11], "temperature": 0.0, "avg_logprob": -0.2029545405139662, "compression_ratio": 1.3826530612244898, "no_speech_prob": 2.4287473934236914e-05}, {"id": 38, "seek": 23852, "start": 254.88, "end": 261.84000000000003, "text": " a lot of involvement from the community in it. The JIT itself, optimized code that runs", "tokens": [257, 688, 295, 17447, 490, 264, 1768, 294, 309, 13, 440, 508, 3927, 2564, 11, 26941, 3089, 300, 6676], "temperature": 0.0, "avg_logprob": -0.2029545405139662, "compression_ratio": 1.3826530612244898, "no_speech_prob": 2.4287473934236914e-05}, {"id": 39, "seek": 26184, "start": 261.84, "end": 269.08, "text": " great. It uses profiler, so it can optimize a code that you can't get doing it statically.", "tokens": [869, 13, 467, 4960, 1740, 5441, 11, 370, 309, 393, 19719, 257, 3089, 300, 291, 393, 380, 483, 884, 309, 2219, 984, 13], "temperature": 0.0, "avg_logprob": -0.13392613443096033, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.976022551592905e-05}, {"id": 40, "seek": 26184, "start": 269.08, "end": 273.76, "text": " Has very efficient garbage collection. And when the JVM collects more profile data in", "tokens": [8646, 588, 7148, 14150, 5765, 13, 400, 562, 264, 508, 53, 44, 39897, 544, 7964, 1412, 294], "temperature": 0.0, "avg_logprob": -0.13392613443096033, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.976022551592905e-05}, {"id": 41, "seek": 26184, "start": 273.76, "end": 278.0, "text": " the JIT, compiles more methods, your code gets better and better. So the longer your", "tokens": [264, 508, 3927, 11, 715, 4680, 544, 7150, 11, 428, 3089, 2170, 1101, 293, 1101, 13, 407, 264, 2854, 428], "temperature": 0.0, "avg_logprob": -0.13392613443096033, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.976022551592905e-05}, {"id": 42, "seek": 26184, "start": 278.0, "end": 286.59999999999997, "text": " job application runs, the better it gets. Now, the bad. So that initial execution of", "tokens": [1691, 3861, 6676, 11, 264, 1101, 309, 2170, 13, 823, 11, 264, 1578, 13, 407, 300, 5883, 15058, 295], "temperature": 0.0, "avg_logprob": -0.13392613443096033, "compression_ratio": 1.5871559633027523, "no_speech_prob": 2.976022551592905e-05}, {"id": 43, "seek": 28660, "start": 286.6, "end": 293.52000000000004, "text": " your code is interpreted, so it's relatively slow. Those hotspot methods compiled by the", "tokens": [428, 3089, 307, 26749, 11, 370, 309, 311, 7226, 2964, 13, 3950, 36121, 17698, 7150, 36548, 538, 264], "temperature": 0.0, "avg_logprob": -0.1356264352798462, "compression_ratio": 1.6017316017316017, "no_speech_prob": 4.468249244382605e-05}, {"id": 44, "seek": 28660, "start": 293.52000000000004, "end": 303.24, "text": " JIT can create CPU and memory spikes. CPU spikes cause lower quality of service, meaning performance.", "tokens": [508, 3927, 393, 1884, 13199, 293, 4675, 28997, 13, 13199, 28997, 3082, 3126, 3125, 295, 2643, 11, 3620, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1356264352798462, "compression_ratio": 1.6017316017316017, "no_speech_prob": 4.468249244382605e-05}, {"id": 45, "seek": 28660, "start": 303.24, "end": 307.56, "text": " And your memory spikes cause out-of-memory issues, including crashes. In fact, the number", "tokens": [400, 428, 4675, 28997, 3082, 484, 12, 2670, 12, 17886, 827, 2663, 11, 3009, 28642, 13, 682, 1186, 11, 264, 1230], "temperature": 0.0, "avg_logprob": -0.1356264352798462, "compression_ratio": 1.6017316017316017, "no_speech_prob": 4.468249244382605e-05}, {"id": 46, "seek": 28660, "start": 307.56, "end": 313.44, "text": " one reason JVM, or a main reason JVM crashes because of out-of-memory issues. And we have", "tokens": [472, 1778, 508, 53, 44, 11, 420, 257, 2135, 1778, 508, 53, 44, 28642, 570, 295, 484, 12, 2670, 12, 17886, 827, 2663, 13, 400, 321, 362], "temperature": 0.0, "avg_logprob": -0.1356264352798462, "compression_ratio": 1.6017316017316017, "no_speech_prob": 4.468249244382605e-05}, {"id": 47, "seek": 31344, "start": 313.44, "end": 318.48, "text": " slow startup and slow ramp-up time. So we want to distinguish between the two. Startup", "tokens": [2964, 18578, 293, 2964, 12428, 12, 1010, 565, 13, 407, 321, 528, 281, 20206, 1296, 264, 732, 13, 6481, 1010], "temperature": 0.0, "avg_logprob": -0.13428440272251022, "compression_ratio": 1.7480314960629921, "no_speech_prob": 8.746371895540506e-05}, {"id": 48, "seek": 31344, "start": 318.48, "end": 323.76, "text": " time is the time that it takes for that application to process first request, usually during an", "tokens": [565, 307, 264, 565, 300, 309, 2516, 337, 300, 3861, 281, 1399, 700, 5308, 11, 2673, 1830, 364], "temperature": 0.0, "avg_logprob": -0.13428440272251022, "compression_ratio": 1.7480314960629921, "no_speech_prob": 8.746371895540506e-05}, {"id": 49, "seek": 31344, "start": 323.76, "end": 327.76, "text": " interpretation time. And ramp-up time is the time it takes a JIT to compile everything", "tokens": [14174, 565, 13, 400, 12428, 12, 1010, 565, 307, 264, 565, 309, 2516, 257, 508, 3927, 281, 31413, 1203], "temperature": 0.0, "avg_logprob": -0.13428440272251022, "compression_ratio": 1.7480314960629921, "no_speech_prob": 8.746371895540506e-05}, {"id": 50, "seek": 31344, "start": 327.76, "end": 336.12, "text": " it wants to compile to get to that optimized version of your code. So here we have some", "tokens": [309, 2738, 281, 31413, 281, 483, 281, 300, 26941, 3037, 295, 428, 3089, 13, 407, 510, 321, 362, 512], "temperature": 0.0, "avg_logprob": -0.13428440272251022, "compression_ratio": 1.7480314960629921, "no_speech_prob": 8.746371895540506e-05}, {"id": 51, "seek": 31344, "start": 336.12, "end": 341.6, "text": " graphs to back that up. Here we take a Java Enterprise application, and you can see on", "tokens": [24877, 281, 646, 300, 493, 13, 1692, 321, 747, 257, 10745, 26696, 3861, 11, 293, 291, 393, 536, 322], "temperature": 0.0, "avg_logprob": -0.13428440272251022, "compression_ratio": 1.7480314960629921, "no_speech_prob": 8.746371895540506e-05}, {"id": 52, "seek": 34160, "start": 341.6, "end": 348.84000000000003, "text": " the left we got CPU spikes here happening initially, all because of JIT compilations.", "tokens": [264, 1411, 321, 658, 13199, 28997, 510, 2737, 9105, 11, 439, 570, 295, 508, 3927, 715, 388, 763, 13], "temperature": 0.0, "avg_logprob": -0.1408917180607828, "compression_ratio": 1.5213675213675213, "no_speech_prob": 1.9520663045113906e-05}, {"id": 53, "seek": 34160, "start": 348.84000000000003, "end": 359.52000000000004, "text": " Same thing with the memory side. We got these large spikes that we have to account for.", "tokens": [10635, 551, 365, 264, 4675, 1252, 13, 492, 658, 613, 2416, 28997, 300, 321, 362, 281, 2696, 337, 13], "temperature": 0.0, "avg_logprob": -0.1408917180607828, "compression_ratio": 1.5213675213675213, "no_speech_prob": 1.9520663045113906e-05}, {"id": 54, "seek": 34160, "start": 359.52000000000004, "end": 363.24, "text": " So let's go back to that graph I had finding that sweet spot. Now we have a little more", "tokens": [407, 718, 311, 352, 646, 281, 300, 4295, 286, 632, 5006, 300, 3844, 4008, 13, 823, 321, 362, 257, 707, 544], "temperature": 0.0, "avg_logprob": -0.1408917180607828, "compression_ratio": 1.5213675213675213, "no_speech_prob": 1.9520663045113906e-05}, {"id": 55, "seek": 34160, "start": 363.24, "end": 369.28000000000003, "text": " information, but still we need to figure out a way to right-size those provisioned containers.", "tokens": [1589, 11, 457, 920, 321, 643, 281, 2573, 484, 257, 636, 281, 558, 12, 27553, 729, 17225, 292, 17089, 13], "temperature": 0.0, "avg_logprob": -0.1408917180607828, "compression_ratio": 1.5213675213675213, "no_speech_prob": 1.9520663045113906e-05}, {"id": 56, "seek": 36928, "start": 369.28, "end": 375.4, "text": " And we got to make our auto-scaling efficient. So we have very little control over scaling.", "tokens": [400, 321, 658, 281, 652, 527, 8399, 12, 4417, 4270, 7148, 13, 407, 321, 362, 588, 707, 1969, 670, 21589, 13], "temperature": 0.0, "avg_logprob": -0.10437524041464162, "compression_ratio": 1.6884422110552764, "no_speech_prob": 1.0950963769573718e-05}, {"id": 57, "seek": 36928, "start": 375.4, "end": 379.15999999999997, "text": " We control the size of our containers, but as far as scaling goes, we just have to set", "tokens": [492, 1969, 264, 2744, 295, 527, 17089, 11, 457, 382, 1400, 382, 21589, 1709, 11, 321, 445, 362, 281, 992], "temperature": 0.0, "avg_logprob": -0.10437524041464162, "compression_ratio": 1.6884422110552764, "no_speech_prob": 1.0950963769573718e-05}, {"id": 58, "seek": 36928, "start": 379.15999999999997, "end": 390.28, "text": " the environment enough up correctly so that auto-scaling is efficient.", "tokens": [264, 2823, 1547, 493, 8944, 370, 300, 8399, 12, 4417, 4270, 307, 7148, 13], "temperature": 0.0, "avg_logprob": -0.10437524041464162, "compression_ratio": 1.6884422110552764, "no_speech_prob": 1.0950963769573718e-05}, {"id": 59, "seek": 36928, "start": 390.28, "end": 396.32, "text": " So on the container size portion of it, the main issue is we need to over-provision to", "tokens": [407, 322, 264, 10129, 2744, 8044, 295, 309, 11, 264, 2135, 2734, 307, 321, 643, 281, 670, 12, 4318, 6763, 281], "temperature": 0.0, "avg_logprob": -0.10437524041464162, "compression_ratio": 1.6884422110552764, "no_speech_prob": 1.0950963769573718e-05}, {"id": 60, "seek": 39632, "start": 396.32, "end": 405.96, "text": " handle those out-of-memory spikes, which is very hard to do, because JVMs have a non-deterministic", "tokens": [4813, 729, 484, 12, 2670, 12, 17886, 827, 28997, 11, 597, 307, 588, 1152, 281, 360, 11, 570, 508, 53, 26386, 362, 257, 2107, 12, 49136, 259, 3142], "temperature": 0.0, "avg_logprob": -0.1289371109008789, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.01398397621233e-05}, {"id": 61, "seek": 39632, "start": 405.96, "end": 410.08, "text": " behavior, meaning you can run the same application over and over, and you're going to get different", "tokens": [5223, 11, 3620, 291, 393, 1190, 264, 912, 3861, 670, 293, 670, 11, 293, 291, 434, 516, 281, 483, 819], "temperature": 0.0, "avg_logprob": -0.1289371109008789, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.01398397621233e-05}, {"id": 62, "seek": 39632, "start": 410.08, "end": 414.28, "text": " spikes at different times. So you've got to run a series of tests with loading to figure", "tokens": [28997, 412, 819, 1413, 13, 407, 291, 600, 658, 281, 1190, 257, 2638, 295, 6921, 365, 15114, 281, 2573], "temperature": 0.0, "avg_logprob": -0.1289371109008789, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.01398397621233e-05}, {"id": 63, "seek": 39632, "start": 414.28, "end": 422.68, "text": " out, to get that number kind of right. And on the auto-scaling part of things, again,", "tokens": [484, 11, 281, 483, 300, 1230, 733, 295, 558, 13, 400, 322, 264, 8399, 12, 4417, 4270, 644, 295, 721, 11, 797, 11], "temperature": 0.0, "avg_logprob": -0.1289371109008789, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.01398397621233e-05}, {"id": 64, "seek": 42268, "start": 422.68, "end": 426.96, "text": " we talk about the slow start-up and ramp-up times. The slower those are, the less effective", "tokens": [321, 751, 466, 264, 2964, 722, 12, 1010, 293, 12428, 12, 1010, 1413, 13, 440, 14009, 729, 366, 11, 264, 1570, 4942], "temperature": 0.0, "avg_logprob": -0.13697576994943148, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.3179482013802044e-05}, {"id": 65, "seek": 42268, "start": 426.96, "end": 434.76, "text": " your auto-scaling is going to be. And the CPU spikes can cause other issues. A lot of auto-scalers,", "tokens": [428, 8399, 12, 4417, 4270, 307, 516, 281, 312, 13, 400, 264, 13199, 28997, 393, 3082, 661, 2663, 13, 316, 688, 295, 8399, 12, 4417, 304, 433, 11], "temperature": 0.0, "avg_logprob": -0.13697576994943148, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.3179482013802044e-05}, {"id": 66, "seek": 42268, "start": 434.76, "end": 440.36, "text": " the threshold for starting new instances is CPU load. So if you start a new instance", "tokens": [264, 14678, 337, 2891, 777, 14519, 307, 13199, 3677, 13, 407, 498, 291, 722, 257, 777, 5197], "temperature": 0.0, "avg_logprob": -0.13697576994943148, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.3179482013802044e-05}, {"id": 67, "seek": 42268, "start": 440.36, "end": 448.04, "text": " and it's spinning, doing JIT compiles, your auto-scaler may detect that as a false positive,", "tokens": [293, 309, 311, 15640, 11, 884, 508, 3927, 715, 4680, 11, 428, 8399, 12, 4417, 17148, 815, 5531, 300, 382, 257, 7908, 3353, 11], "temperature": 0.0, "avg_logprob": -0.13697576994943148, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.3179482013802044e-05}, {"id": 68, "seek": 44804, "start": 448.04, "end": 452.64000000000004, "text": " say, oh, you need, the demand is going up, you need more instances, when in this case,", "tokens": [584, 11, 1954, 11, 291, 643, 11, 264, 4733, 307, 516, 493, 11, 291, 643, 544, 14519, 11, 562, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.11727604814755019, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.593282286194153e-05}, {"id": 69, "seek": 44804, "start": 452.64000000000004, "end": 462.16, "text": " you really didn't. So it makes it very inefficient. So the solution to this problem is we need", "tokens": [291, 534, 994, 380, 13, 407, 309, 1669, 309, 588, 43495, 13, 407, 264, 3827, 281, 341, 1154, 307, 321, 643], "temperature": 0.0, "avg_logprob": -0.11727604814755019, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.593282286194153e-05}, {"id": 70, "seek": 44804, "start": 462.16, "end": 467.0, "text": " to minimize or eliminate those CPU spikes and memory spikes, and we've got to improve", "tokens": [281, 17522, 420, 13819, 729, 13199, 28997, 293, 4675, 28997, 11, 293, 321, 600, 658, 281, 3470], "temperature": 0.0, "avg_logprob": -0.11727604814755019, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.593282286194153e-05}, {"id": 71, "seek": 44804, "start": 467.0, "end": 475.64000000000004, "text": " that start-up and ramp-up time. So we are proposing here, we're going to talk about", "tokens": [300, 722, 12, 1010, 293, 12428, 12, 1010, 565, 13, 407, 321, 366, 29939, 510, 11, 321, 434, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.11727604814755019, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.593282286194153e-05}, {"id": 72, "seek": 47564, "start": 475.64, "end": 481.91999999999996, "text": " JIT as a service, which is going to solve these issues, or help solve these issues.", "tokens": [508, 3927, 382, 257, 2643, 11, 597, 307, 516, 281, 5039, 613, 2663, 11, 420, 854, 5039, 613, 2663, 13], "temperature": 0.0, "avg_logprob": -0.08459294572168467, "compression_ratio": 1.7178217821782178, "no_speech_prob": 1.5683574019931257e-05}, {"id": 73, "seek": 47564, "start": 481.91999999999996, "end": 486.47999999999996, "text": " So the theory behind it is we're going to decouple the JIT compiler from the JVM and", "tokens": [407, 264, 5261, 2261, 309, 307, 321, 434, 516, 281, 979, 263, 781, 264, 508, 3927, 31958, 490, 264, 508, 53, 44, 293], "temperature": 0.0, "avg_logprob": -0.08459294572168467, "compression_ratio": 1.7178217821782178, "no_speech_prob": 1.5683574019931257e-05}, {"id": 74, "seek": 47564, "start": 486.47999999999996, "end": 492.91999999999996, "text": " let it run as an independent process. Then we're going to offload those JIT compilations", "tokens": [718, 309, 1190, 382, 364, 6695, 1399, 13, 1396, 321, 434, 516, 281, 766, 2907, 729, 508, 3927, 715, 388, 763], "temperature": 0.0, "avg_logprob": -0.08459294572168467, "compression_ratio": 1.7178217821782178, "no_speech_prob": 1.5683574019931257e-05}, {"id": 75, "seek": 47564, "start": 492.91999999999996, "end": 499.44, "text": " to that remote process from the client JVMs. As you can see here, we have two client JVMs", "tokens": [281, 300, 8607, 1399, 490, 264, 6423, 508, 53, 26386, 13, 1018, 291, 393, 536, 510, 11, 321, 362, 732, 6423, 508, 53, 26386], "temperature": 0.0, "avg_logprob": -0.08459294572168467, "compression_ratio": 1.7178217821782178, "no_speech_prob": 1.5683574019931257e-05}, {"id": 76, "seek": 49944, "start": 499.44, "end": 509.52, "text": " talking to two remote JITs over here. We have the JIT still locally in the JVM that can", "tokens": [1417, 281, 732, 8607, 508, 3927, 82, 670, 510, 13, 492, 362, 264, 508, 3927, 920, 16143, 294, 264, 508, 53, 44, 300, 393], "temperature": 0.0, "avg_logprob": -0.1475320787572149, "compression_ratio": 1.491891891891892, "no_speech_prob": 8.265016731456853e-06}, {"id": 77, "seek": 49944, "start": 509.52, "end": 517.0, "text": " be used if these become unavailable for some reason. Everything since we're all in containers", "tokens": [312, 1143, 498, 613, 1813, 36541, 32699, 337, 512, 1778, 13, 5471, 1670, 321, 434, 439, 294, 17089], "temperature": 0.0, "avg_logprob": -0.1475320787572149, "compression_ratio": 1.491891891891892, "no_speech_prob": 8.265016731456853e-06}, {"id": 78, "seek": 49944, "start": 517.0, "end": 524.52, "text": " is automatically managed by the orchestrator to make sure that we have their scaled correctly.", "tokens": [307, 6772, 6453, 538, 264, 14161, 19802, 281, 652, 988, 300, 321, 362, 641, 36039, 8944, 13], "temperature": 0.0, "avg_logprob": -0.1475320787572149, "compression_ratio": 1.491891891891892, "no_speech_prob": 8.265016731456853e-06}, {"id": 79, "seek": 52452, "start": 524.52, "end": 529.56, "text": " This is actually a model to microsolution, so we're taking the model, as in this case,", "tokens": [639, 307, 767, 257, 2316, 281, 15547, 3386, 11, 370, 321, 434, 1940, 264, 2316, 11, 382, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.1723440111297922, "compression_ratio": 1.6026785714285714, "no_speech_prob": 6.639744697167771e-06}, {"id": 80, "seek": 52452, "start": 529.56, "end": 535.3199999999999, "text": " as a JVM. We're splitting it up into the JIT and everything left over in the other microservice.", "tokens": [382, 257, 508, 53, 44, 13, 492, 434, 30348, 309, 493, 666, 264, 508, 3927, 293, 1203, 1411, 670, 294, 264, 661, 15547, 25006, 13], "temperature": 0.0, "avg_logprob": -0.1723440111297922, "compression_ratio": 1.6026785714285714, "no_speech_prob": 6.639744697167771e-06}, {"id": 81, "seek": 52452, "start": 535.3199999999999, "end": 546.0, "text": " And again, like I mentioned, the local JIT still is available if this service goes down.", "tokens": [400, 797, 11, 411, 286, 2835, 11, 264, 2654, 508, 3927, 920, 307, 2435, 498, 341, 2643, 1709, 760, 13], "temperature": 0.0, "avg_logprob": -0.1723440111297922, "compression_ratio": 1.6026785714285714, "no_speech_prob": 6.639744697167771e-06}, {"id": 82, "seek": 52452, "start": 546.0, "end": 551.84, "text": " So this actual technology does exist today, and it's called the JIT server, and it's a", "tokens": [407, 341, 3539, 2899, 775, 2514, 965, 11, 293, 309, 311, 1219, 264, 508, 3927, 7154, 11, 293, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.1723440111297922, "compression_ratio": 1.6026785714285714, "no_speech_prob": 6.639744697167771e-06}, {"id": 83, "seek": 55184, "start": 551.84, "end": 559.6800000000001, "text": " part of the Eclipse OpenJ9 JVM. It comes with the, it's also called the SAMRU cloud compiler", "tokens": [644, 295, 264, 462, 27197, 7238, 41, 24, 508, 53, 44, 13, 467, 1487, 365, 264, 11, 309, 311, 611, 1219, 264, 9617, 49, 52, 4588, 31958], "temperature": 0.0, "avg_logprob": -0.15508622943230396, "compression_ratio": 1.51931330472103, "no_speech_prob": 3.589935295167379e-05}, {"id": 84, "seek": 55184, "start": 559.6800000000001, "end": 564.44, "text": " when used with SAMRU runtimes, and I'll get to that in a minute. And I'm sure everyone", "tokens": [562, 1143, 365, 9617, 49, 52, 49435, 1532, 11, 293, 286, 603, 483, 281, 300, 294, 257, 3456, 13, 400, 286, 478, 988, 1518], "temperature": 0.0, "avg_logprob": -0.15508622943230396, "compression_ratio": 1.51931330472103, "no_speech_prob": 3.589935295167379e-05}, {"id": 85, "seek": 55184, "start": 564.44, "end": 570.1600000000001, "text": " here knows OpenJ9 combines with OpenJDK to form a full JDK and totally open-source it", "tokens": [510, 3255, 7238, 41, 24, 29520, 365, 7238, 41, 35, 42, 281, 1254, 257, 1577, 37082, 42, 293, 3879, 1269, 12, 41676, 309], "temperature": 0.0, "avg_logprob": -0.15508622943230396, "compression_ratio": 1.51931330472103, "no_speech_prob": 3.589935295167379e-05}, {"id": 86, "seek": 55184, "start": 570.1600000000001, "end": 580.1600000000001, "text": " free to download. And here's a GitHub repo there. A little history of OpenJ9. It started", "tokens": [1737, 281, 5484, 13, 400, 510, 311, 257, 23331, 49040, 456, 13, 316, 707, 2503, 295, 7238, 41, 24, 13, 467, 1409], "temperature": 0.0, "avg_logprob": -0.15508622943230396, "compression_ratio": 1.51931330472103, "no_speech_prob": 3.589935295167379e-05}, {"id": 87, "seek": 58016, "start": 580.16, "end": 587.6, "text": " life as the J9 JVM by IBM over 25 years ago. And the reason IBM developed it was because", "tokens": [993, 382, 264, 508, 24, 508, 53, 44, 538, 23487, 670, 3552, 924, 2057, 13, 400, 264, 1778, 23487, 4743, 309, 390, 570], "temperature": 0.0, "avg_logprob": -0.10118705614478188, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.718562119407579e-05}, {"id": 88, "seek": 58016, "start": 587.6, "end": 592.16, "text": " they had a whole range of devices they needed to support, and they wanted to make sure Java", "tokens": [436, 632, 257, 1379, 3613, 295, 5759, 436, 2978, 281, 1406, 11, 293, 436, 1415, 281, 652, 988, 10745], "temperature": 0.0, "avg_logprob": -0.10118705614478188, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.718562119407579e-05}, {"id": 89, "seek": 58016, "start": 592.16, "end": 598.4, "text": " ran on all of them. That's all the way from handheld scanners to mainframes. So it was", "tokens": [5872, 322, 439, 295, 552, 13, 663, 311, 439, 264, 636, 490, 37634, 795, 25792, 281, 2135, 43277, 13, 407, 309, 390], "temperature": 0.0, "avg_logprob": -0.10118705614478188, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.718562119407579e-05}, {"id": 90, "seek": 58016, "start": 598.4, "end": 603.04, "text": " designed to go from small to large in both types of environments where you have a lot", "tokens": [4761, 281, 352, 490, 1359, 281, 2416, 294, 1293, 3467, 295, 12388, 689, 291, 362, 257, 688], "temperature": 0.0, "avg_logprob": -0.10118705614478188, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.718562119407579e-05}, {"id": 91, "seek": 58016, "start": 603.04, "end": 608.24, "text": " of memory or very, very little. And about five years ago, IBM decided to open-source", "tokens": [295, 4675, 420, 588, 11, 588, 707, 13, 400, 466, 1732, 924, 2057, 11, 23487, 3047, 281, 1269, 12, 41676], "temperature": 0.0, "avg_logprob": -0.10118705614478188, "compression_ratio": 1.5985401459854014, "no_speech_prob": 7.718562119407579e-05}, {"id": 92, "seek": 60824, "start": 608.24, "end": 614.32, "text": " it to the Eclipse Foundation. And OpenJ9 is renowned for its small footprint fast start-up", "tokens": [309, 281, 264, 462, 27197, 10335, 13, 400, 7238, 41, 24, 307, 34065, 337, 1080, 1359, 24222, 2370, 722, 12, 1010], "temperature": 0.0, "avg_logprob": -0.13580737113952637, "compression_ratio": 1.5, "no_speech_prob": 4.0017512219492346e-05}, {"id": 93, "seek": 60824, "start": 614.32, "end": 618.48, "text": " and ramp-up time, which we'll get to in a minute. And again, even though it's got a", "tokens": [293, 12428, 12, 1010, 565, 11, 597, 321, 603, 483, 281, 294, 257, 3456, 13, 400, 797, 11, 754, 1673, 309, 311, 658, 257], "temperature": 0.0, "avg_logprob": -0.13580737113952637, "compression_ratio": 1.5, "no_speech_prob": 4.0017512219492346e-05}, {"id": 94, "seek": 60824, "start": 618.48, "end": 626.28, "text": " new name, it's OpenJ9. All of IBM enterprise clients have been running their applications", "tokens": [777, 1315, 11, 309, 311, 7238, 41, 24, 13, 1057, 295, 23487, 14132, 6982, 362, 668, 2614, 641, 5821], "temperature": 0.0, "avg_logprob": -0.13580737113952637, "compression_ratio": 1.5, "no_speech_prob": 4.0017512219492346e-05}, {"id": 95, "seek": 60824, "start": 626.28, "end": 637.6800000000001, "text": " on this JVM for years. So there's a lot of history of success with it. Here's some OpenJ9", "tokens": [322, 341, 508, 53, 44, 337, 924, 13, 407, 456, 311, 257, 688, 295, 2503, 295, 2245, 365, 309, 13, 1692, 311, 512, 7238, 41, 24], "temperature": 0.0, "avg_logprob": -0.13580737113952637, "compression_ratio": 1.5, "no_speech_prob": 4.0017512219492346e-05}, {"id": 96, "seek": 63768, "start": 637.68, "end": 643.9599999999999, "text": " performance compared to Hotspot. Again, this doesn't take into account the JIT server.", "tokens": [3389, 5347, 281, 389, 1971, 17698, 13, 3764, 11, 341, 1177, 380, 747, 666, 2696, 264, 508, 3927, 7154, 13], "temperature": 0.0, "avg_logprob": -0.12416916082401087, "compression_ratio": 1.4872881355932204, "no_speech_prob": 2.1106150597915985e-05}, {"id": 97, "seek": 63768, "start": 643.9599999999999, "end": 651.3599999999999, "text": " This is just the JVMs themselves going left to right here. OpenJ9's in green. Hotspot's", "tokens": [639, 307, 445, 264, 508, 53, 26386, 2969, 516, 1411, 281, 558, 510, 13, 7238, 41, 24, 311, 294, 3092, 13, 389, 1971, 17698, 311], "temperature": 0.0, "avg_logprob": -0.12416916082401087, "compression_ratio": 1.4872881355932204, "no_speech_prob": 2.1106150597915985e-05}, {"id": 98, "seek": 63768, "start": 651.3599999999999, "end": 658.0, "text": " in orange. So in certain circumstances, we got to see 51% faster start-up time, 50% smaller", "tokens": [294, 7671, 13, 407, 294, 1629, 9121, 11, 321, 658, 281, 536, 18485, 4, 4663, 722, 12, 1010, 565, 11, 2625, 4, 4356], "temperature": 0.0, "avg_logprob": -0.12416916082401087, "compression_ratio": 1.4872881355932204, "no_speech_prob": 2.1106150597915985e-05}, {"id": 99, "seek": 63768, "start": 658.0, "end": 665.56, "text": " footprint after start-up. And it ramps up quicker than Hotspot. And at the very end,", "tokens": [24222, 934, 722, 12, 1010, 13, 400, 309, 12428, 82, 493, 16255, 813, 389, 1971, 17698, 13, 400, 412, 264, 588, 917, 11], "temperature": 0.0, "avg_logprob": -0.12416916082401087, "compression_ratio": 1.4872881355932204, "no_speech_prob": 2.1106150597915985e-05}, {"id": 100, "seek": 66556, "start": 665.56, "end": 678.5999999999999, "text": " after a total full load, we have a 33% smaller footprint with OpenJ9. So, several run times.", "tokens": [934, 257, 3217, 1577, 3677, 11, 321, 362, 257, 11816, 4, 4356, 24222, 365, 7238, 41, 24, 13, 407, 11, 2940, 1190, 1413, 13], "temperature": 0.0, "avg_logprob": -0.16242570241292317, "compression_ratio": 1.4385026737967914, "no_speech_prob": 1.7499014575150795e-05}, {"id": 101, "seek": 66556, "start": 678.5999999999999, "end": 684.64, "text": " So that is IBM's OpenJDK distribution. Just like all the, someone just mentioned, there's", "tokens": [407, 300, 307, 23487, 311, 7238, 41, 35, 42, 7316, 13, 1449, 411, 439, 264, 11, 1580, 445, 2835, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.16242570241292317, "compression_ratio": 1.4385026737967914, "no_speech_prob": 1.7499014575150795e-05}, {"id": 102, "seek": 66556, "start": 684.64, "end": 689.1199999999999, "text": " a ton of distributions out there. This is IBM's. And it's the only one that comes with", "tokens": [257, 2952, 295, 37870, 484, 456, 13, 639, 307, 23487, 311, 13, 400, 309, 311, 264, 787, 472, 300, 1487, 365], "temperature": 0.0, "avg_logprob": -0.16242570241292317, "compression_ratio": 1.4385026737967914, "no_speech_prob": 1.7499014575150795e-05}, {"id": 103, "seek": 68912, "start": 689.12, "end": 696.32, "text": " Eclipse OpenJ9 JVM. It's available no cost. It's stable. IBM puts their name behind it.", "tokens": [462, 27197, 7238, 41, 24, 508, 53, 44, 13, 467, 311, 2435, 572, 2063, 13, 467, 311, 8351, 13, 23487, 8137, 641, 1315, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.16678694461254365, "compression_ratio": 1.5443037974683544, "no_speech_prob": 2.7944797693635337e-05}, {"id": 104, "seek": 68912, "start": 696.32, "end": 703.08, "text": " So it comes in two editions, open source and certified. The only difference being the licensing", "tokens": [407, 309, 1487, 294, 732, 44840, 11, 1269, 4009, 293, 18580, 13, 440, 787, 2649, 885, 264, 29759], "temperature": 0.0, "avg_logprob": -0.16678694461254365, "compression_ratio": 1.5443037974683544, "no_speech_prob": 2.7944797693635337e-05}, {"id": 105, "seek": 68912, "start": 703.08, "end": 708.68, "text": " and what platforms are supported. And if you're wondering what Samaru comes from, the name", "tokens": [293, 437, 9473, 366, 8104, 13, 400, 498, 291, 434, 6359, 437, 4832, 16870, 1487, 490, 11, 264, 1315], "temperature": 0.0, "avg_logprob": -0.16678694461254365, "compression_ratio": 1.5443037974683544, "no_speech_prob": 2.7944797693635337e-05}, {"id": 106, "seek": 68912, "start": 708.68, "end": 716.5600000000001, "text": " comes from, Mount Samaru is the tallest mountain on the island of, anyone know? Java, there", "tokens": [1487, 490, 11, 8426, 4832, 16870, 307, 264, 42075, 6937, 322, 264, 6077, 295, 11, 2878, 458, 30, 10745, 11, 456], "temperature": 0.0, "avg_logprob": -0.16678694461254365, "compression_ratio": 1.5443037974683544, "no_speech_prob": 2.7944797693635337e-05}, {"id": 107, "seek": 71656, "start": 716.56, "end": 722.76, "text": " you go. See how that makes sense? If I had a t-shirt, I would have given you that. Alright,", "tokens": [291, 352, 13, 3008, 577, 300, 1669, 2020, 30, 759, 286, 632, 257, 256, 12, 15313, 11, 286, 576, 362, 2212, 291, 300, 13, 2798, 11], "temperature": 0.0, "avg_logprob": -0.1379671197188528, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.983247345080599e-05}, {"id": 108, "seek": 71656, "start": 722.76, "end": 729.0799999999999, "text": " from the perspective of the server or the client talking to this new JIT server, this", "tokens": [490, 264, 4585, 295, 264, 7154, 420, 264, 6423, 1417, 281, 341, 777, 508, 3927, 7154, 11, 341], "temperature": 0.0, "avg_logprob": -0.1379671197188528, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.983247345080599e-05}, {"id": 109, "seek": 71656, "start": 729.0799999999999, "end": 734.56, "text": " is the advantages they're going to get. From a provisioning aspect, now it's going to be", "tokens": [307, 264, 14906, 436, 434, 516, 281, 483, 13, 3358, 257, 17225, 278, 4171, 11, 586, 309, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1379671197188528, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.983247345080599e-05}, {"id": 110, "seek": 71656, "start": 734.56, "end": 739.1199999999999, "text": " very easy to size our containers, right? We don't have to worry about those spikes anymore.", "tokens": [588, 1858, 281, 2744, 527, 17089, 11, 558, 30, 492, 500, 380, 362, 281, 3292, 466, 729, 28997, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1379671197188528, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.983247345080599e-05}, {"id": 111, "seek": 73912, "start": 739.12, "end": 748.08, "text": " So now we just, we level set based on the demand or the needs of the application itself.", "tokens": [407, 586, 321, 445, 11, 321, 1496, 992, 2361, 322, 264, 4733, 420, 264, 2203, 295, 264, 3861, 2564, 13], "temperature": 0.0, "avg_logprob": -0.17063917572965326, "compression_ratio": 1.600896860986547, "no_speech_prob": 4.156203885941068e-06}, {"id": 112, "seek": 73912, "start": 748.08, "end": 751.96, "text": " Performance wise, we're going to see improved ramp-up time, basically because the JIT server", "tokens": [25047, 10829, 11, 321, 434, 516, 281, 536, 9689, 12428, 12, 1010, 565, 11, 1936, 570, 264, 508, 3927, 7154], "temperature": 0.0, "avg_logprob": -0.17063917572965326, "compression_ratio": 1.600896860986547, "no_speech_prob": 4.156203885941068e-06}, {"id": 113, "seek": 73912, "start": 751.96, "end": 758.24, "text": " is going to be offloading. We're going to offload all the compiles in the CPU cycles", "tokens": [307, 516, 281, 312, 766, 2907, 278, 13, 492, 434, 516, 281, 766, 2907, 439, 264, 715, 4680, 294, 264, 13199, 17796], "temperature": 0.0, "avg_logprob": -0.17063917572965326, "compression_ratio": 1.600896860986547, "no_speech_prob": 4.156203885941068e-06}, {"id": 114, "seek": 73912, "start": 758.24, "end": 764.72, "text": " to the JIT server. And there's also a feature in this JIT server called AOT cache. So it's", "tokens": [281, 264, 508, 3927, 7154, 13, 400, 456, 311, 611, 257, 4111, 294, 341, 508, 3927, 7154, 1219, 316, 5068, 19459, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.17063917572965326, "compression_ratio": 1.600896860986547, "no_speech_prob": 4.156203885941068e-06}, {"id": 115, "seek": 76472, "start": 764.72, "end": 771.84, "text": " going to store any method it compiles. So another instance of the same container application", "tokens": [516, 281, 3531, 604, 3170, 309, 715, 4680, 13, 407, 1071, 5197, 295, 264, 912, 10129, 3861], "temperature": 0.0, "avg_logprob": -0.18107761245176016, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.7962234526057728e-05}, {"id": 116, "seek": 76472, "start": 771.84, "end": 779.1600000000001, "text": " calling it, and then they'll have that method, it'll just return it. No compilation needed.", "tokens": [5141, 309, 11, 293, 550, 436, 603, 362, 300, 3170, 11, 309, 603, 445, 2736, 309, 13, 883, 40261, 2978, 13], "temperature": 0.0, "avg_logprob": -0.18107761245176016, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.7962234526057728e-05}, {"id": 117, "seek": 76472, "start": 779.1600000000001, "end": 784.4, "text": " Then from a cost standpoint, obviously any time you reduce your resource cost or your", "tokens": [1396, 490, 257, 2063, 15827, 11, 2745, 604, 565, 291, 5407, 428, 7684, 2063, 420, 428], "temperature": 0.0, "avg_logprob": -0.18107761245176016, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.7962234526057728e-05}, {"id": 118, "seek": 76472, "start": 784.4, "end": 789.64, "text": " resource amounts, you're going to get a savings in cost. And I mentioned earlier the efficient", "tokens": [7684, 11663, 11, 291, 434, 516, 281, 483, 257, 13454, 294, 2063, 13, 400, 286, 2835, 3071, 264, 7148], "temperature": 0.0, "avg_logprob": -0.18107761245176016, "compression_ratio": 1.6150442477876106, "no_speech_prob": 2.7962234526057728e-05}, {"id": 119, "seek": 78964, "start": 789.64, "end": 799.96, "text": " auto scaling, you're only going to pay for what you need. Resiliency, remember the JVM", "tokens": [8399, 21589, 11, 291, 434, 787, 516, 281, 1689, 337, 437, 291, 643, 13, 5015, 388, 7848, 11, 1604, 264, 508, 53, 44], "temperature": 0.0, "avg_logprob": -0.11068734620746813, "compression_ratio": 1.5644444444444445, "no_speech_prob": 6.046667749615153e-06}, {"id": 120, "seek": 78964, "start": 799.96, "end": 809.52, "text": " still has their local JIT. So if the JIT server goes down, it could still keep going.", "tokens": [920, 575, 641, 2654, 508, 3927, 13, 407, 498, 264, 508, 3927, 7154, 1709, 760, 11, 309, 727, 920, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.11068734620746813, "compression_ratio": 1.5644444444444445, "no_speech_prob": 6.046667749615153e-06}, {"id": 121, "seek": 78964, "start": 809.52, "end": 813.4399999999999, "text": " So this is kind of an interesting chart. This is pretty big. So we're going to talk about", "tokens": [407, 341, 307, 733, 295, 364, 1880, 6927, 13, 639, 307, 1238, 955, 13, 407, 321, 434, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.11068734620746813, "compression_ratio": 1.5644444444444445, "no_speech_prob": 6.046667749615153e-06}, {"id": 122, "seek": 78964, "start": 813.4399999999999, "end": 819.52, "text": " some of the examples of where we see savings. So this is an experiment where we took four", "tokens": [512, 295, 264, 5110, 295, 689, 321, 536, 13454, 13, 407, 341, 307, 364, 5120, 689, 321, 1890, 1451], "temperature": 0.0, "avg_logprob": -0.11068734620746813, "compression_ratio": 1.5644444444444445, "no_speech_prob": 6.046667749615153e-06}, {"id": 123, "seek": 81952, "start": 819.52, "end": 827.0799999999999, "text": " \u2013 let me see my pointer works \u2013 we took four job applications and we decided to size", "tokens": [1662, 718, 385, 536, 452, 23918, 1985, 1662, 321, 1890, 1451, 1691, 5821, 293, 321, 3047, 281, 2744], "temperature": 0.0, "avg_logprob": -0.15118021965026857, "compression_ratio": 1.536480686695279, "no_speech_prob": 1.1838792488561012e-05}, {"id": 124, "seek": 81952, "start": 827.0799999999999, "end": 832.92, "text": " them correctly for the amount of the memory and CPU they needed doing all those load tests", "tokens": [552, 8944, 337, 264, 2372, 295, 264, 4675, 293, 13199, 436, 2978, 884, 439, 729, 3677, 6921], "temperature": 0.0, "avg_logprob": -0.15118021965026857, "compression_ratio": 1.536480686695279, "no_speech_prob": 1.1838792488561012e-05}, {"id": 125, "seek": 81952, "start": 832.92, "end": 838.24, "text": " to figure out what this amount should be. And we have multiple instances of them. So", "tokens": [281, 2573, 484, 437, 341, 2372, 820, 312, 13, 400, 321, 362, 3866, 14519, 295, 552, 13, 407], "temperature": 0.0, "avg_logprob": -0.15118021965026857, "compression_ratio": 1.536480686695279, "no_speech_prob": 1.1838792488561012e-05}, {"id": 126, "seek": 81952, "start": 838.24, "end": 843.28, "text": " the color indicates the application. You can see all the different replications. The relative", "tokens": [264, 2017, 16203, 264, 3861, 13, 509, 393, 536, 439, 264, 819, 3248, 24847, 13, 440, 4972], "temperature": 0.0, "avg_logprob": -0.15118021965026857, "compression_ratio": 1.536480686695279, "no_speech_prob": 1.1838792488561012e-05}, {"id": 127, "seek": 84328, "start": 843.28, "end": 850.0, "text": " size is shown with the scale of the square. And in this case, we used OpenShift to lay", "tokens": [2744, 307, 4898, 365, 264, 4373, 295, 264, 3732, 13, 400, 294, 341, 1389, 11, 321, 1143, 7238, 7774, 2008, 281, 2360], "temperature": 0.0, "avg_logprob": -0.11524823616290915, "compression_ratio": 1.7072243346007605, "no_speech_prob": 8.528267244400922e-06}, {"id": 128, "seek": 84328, "start": 850.0, "end": 855.8399999999999, "text": " it out for us and it came out to use three nodes to handle all of this, all these applications", "tokens": [309, 484, 337, 505, 293, 309, 1361, 484, 281, 764, 1045, 13891, 281, 4813, 439, 295, 341, 11, 439, 613, 5821], "temperature": 0.0, "avg_logprob": -0.11524823616290915, "compression_ratio": 1.7072243346007605, "no_speech_prob": 8.528267244400922e-06}, {"id": 129, "seek": 84328, "start": 855.8399999999999, "end": 861.0799999999999, "text": " in your instances. Then we introduced the JIT server, ran the same test. Here's our", "tokens": [294, 428, 14519, 13, 1396, 321, 7268, 264, 508, 3927, 7154, 11, 5872, 264, 912, 1500, 13, 1692, 311, 527], "temperature": 0.0, "avg_logprob": -0.11524823616290915, "compression_ratio": 1.7072243346007605, "no_speech_prob": 8.528267244400922e-06}, {"id": 130, "seek": 84328, "start": 861.0799999999999, "end": 866.76, "text": " JIT server here, the brown. It's the biggest container in the nodes. But you notice the", "tokens": [508, 3927, 7154, 510, 11, 264, 6292, 13, 467, 311, 264, 3880, 10129, 294, 264, 13891, 13, 583, 291, 3449, 264], "temperature": 0.0, "avg_logprob": -0.11524823616290915, "compression_ratio": 1.7072243346007605, "no_speech_prob": 8.528267244400922e-06}, {"id": 131, "seek": 84328, "start": 866.76, "end": 871.8399999999999, "text": " size of all of our containers for the applications goes way down. So we have the same number of", "tokens": [2744, 295, 439, 295, 527, 17089, 337, 264, 5821, 1709, 636, 760, 13, 407, 321, 362, 264, 912, 1230, 295], "temperature": 0.0, "avg_logprob": -0.11524823616290915, "compression_ratio": 1.7072243346007605, "no_speech_prob": 8.528267244400922e-06}, {"id": 132, "seek": 87184, "start": 871.84, "end": 878.44, "text": " instances in both cases, but we've just saved 33% of the resources. And if you're", "tokens": [14519, 294, 1293, 3331, 11, 457, 321, 600, 445, 6624, 11816, 4, 295, 264, 3593, 13, 400, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.20565053691034732, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.2114554667496122e-05}, {"id": 133, "seek": 87184, "start": 878.44, "end": 886.76, "text": " wondering how they perform \u2013 whoops, went too far \u2013 you see no difference. The orange", "tokens": [6359, 577, 436, 2042, 1662, 567, 3370, 11, 1437, 886, 1400, 1662, 291, 536, 572, 2649, 13, 440, 7671], "temperature": 0.0, "avg_logprob": -0.20565053691034732, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.2114554667496122e-05}, {"id": 134, "seek": 87184, "start": 886.76, "end": 894.24, "text": " is the baseline, the blue is the JIT server. And from a stable state, meaning once they've", "tokens": [307, 264, 20518, 11, 264, 3344, 307, 264, 508, 3927, 7154, 13, 400, 490, 257, 8351, 1785, 11, 3620, 1564, 436, 600], "temperature": 0.0, "avg_logprob": -0.20565053691034732, "compression_ratio": 1.3789473684210527, "no_speech_prob": 2.2114554667496122e-05}, {"id": 135, "seek": 89424, "start": 894.24, "end": 901.96, "text": " performed, they perform exactly the same. But we're, again, saving 33% of the resources.", "tokens": [10332, 11, 436, 2042, 2293, 264, 912, 13, 583, 321, 434, 11, 797, 11, 6816, 11816, 4, 295, 264, 3593, 13], "temperature": 0.0, "avg_logprob": -0.19583783251173953, "compression_ratio": 1.5244444444444445, "no_speech_prob": 1.5203492694126908e-05}, {"id": 136, "seek": 89424, "start": 901.96, "end": 907.0, "text": " Now we'll take a look at some of the effects on auto-scaling in Kubernetes. Here we're", "tokens": [823, 321, 603, 747, 257, 574, 412, 512, 295, 264, 5065, 322, 8399, 12, 4417, 4270, 294, 23145, 13, 1692, 321, 434], "temperature": 0.0, "avg_logprob": -0.19583783251173953, "compression_ratio": 1.5244444444444445, "no_speech_prob": 1.5203492694126908e-05}, {"id": 137, "seek": 89424, "start": 907.0, "end": 912.88, "text": " running an application and we're setting our threshold, I think it's up there, at", "tokens": [2614, 364, 3861, 293, 321, 434, 3287, 527, 14678, 11, 286, 519, 309, 311, 493, 456, 11, 412], "temperature": 0.0, "avg_logprob": -0.19583783251173953, "compression_ratio": 1.5244444444444445, "no_speech_prob": 1.5203492694126908e-05}, {"id": 138, "seek": 89424, "start": 912.88, "end": 920.52, "text": " 50% of CPU. And you can see here all these plateaus are when the auto-scaler is going", "tokens": [2625, 4, 295, 13199, 13, 400, 291, 393, 536, 510, 439, 613, 5924, 8463, 366, 562, 264, 8399, 12, 4417, 17148, 307, 516], "temperature": 0.0, "avg_logprob": -0.19583783251173953, "compression_ratio": 1.5244444444444445, "no_speech_prob": 1.5203492694126908e-05}, {"id": 139, "seek": 92052, "start": 920.52, "end": 929.12, "text": " to launch another pod. And you can see how the JIT server in blue responds better. Shorter", "tokens": [281, 4025, 1071, 2497, 13, 400, 291, 393, 536, 577, 264, 508, 3927, 7154, 294, 3344, 27331, 1101, 13, 1160, 6122], "temperature": 0.0, "avg_logprob": -0.15394242255242316, "compression_ratio": 1.5964125560538116, "no_speech_prob": 5.140255598234944e-05}, {"id": 140, "seek": 92052, "start": 929.12, "end": 936.0799999999999, "text": " dips and they recover faster. And overall, your performance is going to be better with", "tokens": [47814, 293, 436, 8114, 4663, 13, 400, 4787, 11, 428, 3389, 307, 516, 281, 312, 1101, 365], "temperature": 0.0, "avg_logprob": -0.15394242255242316, "compression_ratio": 1.5964125560538116, "no_speech_prob": 5.140255598234944e-05}, {"id": 141, "seek": 92052, "start": 936.0799999999999, "end": 943.68, "text": " a JIT server. Also, that other thing I talked about with false positives. So, again, the", "tokens": [257, 508, 3927, 7154, 13, 2743, 11, 300, 661, 551, 286, 2825, 466, 365, 7908, 35127, 13, 407, 11, 797, 11, 264], "temperature": 0.0, "avg_logprob": -0.15394242255242316, "compression_ratio": 1.5964125560538116, "no_speech_prob": 5.140255598234944e-05}, {"id": 142, "seek": 92052, "start": 943.68, "end": 948.88, "text": " auto-scaler is not going to be tricked into thinking that that CPU load from JIT compiles", "tokens": [8399, 12, 4417, 17148, 307, 406, 516, 281, 312, 39345, 666, 1953, 300, 300, 13199, 3677, 490, 508, 3927, 715, 4680], "temperature": 0.0, "avg_logprob": -0.15394242255242316, "compression_ratio": 1.5964125560538116, "no_speech_prob": 5.140255598234944e-05}, {"id": 143, "seek": 94888, "start": 948.88, "end": 954.52, "text": " is the reason for demand. So you're going to get better behavior in auto-scaling. Two", "tokens": [307, 264, 1778, 337, 4733, 13, 407, 291, 434, 516, 281, 483, 1101, 5223, 294, 8399, 12, 4417, 4270, 13, 4453], "temperature": 0.0, "avg_logprob": -0.17840035756429037, "compression_ratio": 1.4314516129032258, "no_speech_prob": 6.011548975948244e-05}, {"id": 144, "seek": 94888, "start": 954.52, "end": 961.24, "text": " minutes. All right. When to use it? Obviously when the JVM is \u2013 we're in a memory and", "tokens": [2077, 13, 1057, 558, 13, 1133, 281, 764, 309, 30, 7580, 562, 264, 508, 53, 44, 307, 1662, 321, 434, 294, 257, 4675, 293], "temperature": 0.0, "avg_logprob": -0.17840035756429037, "compression_ratio": 1.4314516129032258, "no_speech_prob": 6.011548975948244e-05}, {"id": 145, "seek": 94888, "start": 961.24, "end": 968.04, "text": " CPU constrained environment. Recommendations, you always use 10 to 20 client JVMs when you're", "tokens": [13199, 38901, 2823, 13, 49545, 521, 763, 11, 291, 1009, 764, 1266, 281, 945, 6423, 508, 53, 26386, 562, 291, 434], "temperature": 0.0, "avg_logprob": -0.17840035756429037, "compression_ratio": 1.4314516129032258, "no_speech_prob": 6.011548975948244e-05}, {"id": 146, "seek": 94888, "start": 968.04, "end": 974.68, "text": " talking to a JIT server. Because remember, that JIT server does take its own container.", "tokens": [1417, 281, 257, 508, 3927, 7154, 13, 1436, 1604, 11, 300, 508, 3927, 7154, 775, 747, 1080, 1065, 10129, 13], "temperature": 0.0, "avg_logprob": -0.17840035756429037, "compression_ratio": 1.4314516129032258, "no_speech_prob": 6.011548975948244e-05}, {"id": 147, "seek": 97468, "start": 974.68, "end": 980.0, "text": " And it is communication over the network, so only adding encryption if you absolutely", "tokens": [400, 309, 307, 6101, 670, 264, 3209, 11, 370, 787, 5127, 29575, 498, 291, 3122], "temperature": 0.0, "avg_logprob": -0.13572257953685718, "compression_ratio": 1.4854771784232366, "no_speech_prob": 1.0127826499228831e-05}, {"id": 148, "seek": 97468, "start": 980.0, "end": 987.1999999999999, "text": " need it. So some final thoughts. We talked about the JIT provides great advantage that", "tokens": [643, 309, 13, 407, 512, 2572, 4598, 13, 492, 2825, 466, 264, 508, 3927, 6417, 869, 5002, 300], "temperature": 0.0, "avg_logprob": -0.13572257953685718, "compression_ratio": 1.4854771784232366, "no_speech_prob": 1.0127826499228831e-05}, {"id": 149, "seek": 97468, "start": 987.1999999999999, "end": 994.5999999999999, "text": " optimize code, but compilations do add overhead. So we disaggregate JIT from the JVM and we", "tokens": [19719, 3089, 11, 457, 715, 388, 763, 360, 909, 19922, 13, 407, 321, 10414, 11027, 473, 508, 3927, 490, 264, 508, 53, 44, 293, 321], "temperature": 0.0, "avg_logprob": -0.13572257953685718, "compression_ratio": 1.4854771784232366, "no_speech_prob": 1.0127826499228831e-05}, {"id": 150, "seek": 97468, "start": 994.5999999999999, "end": 1000.88, "text": " came up with this JIT compilation as a service. It's available in Eclipse OpenJ9, also called", "tokens": [1361, 493, 365, 341, 508, 3927, 40261, 382, 257, 2643, 13, 467, 311, 2435, 294, 462, 27197, 7238, 41, 24, 11, 611, 1219], "temperature": 0.0, "avg_logprob": -0.13572257953685718, "compression_ratio": 1.4854771784232366, "no_speech_prob": 1.0127826499228831e-05}, {"id": 151, "seek": 100088, "start": 1000.88, "end": 1005.92, "text": " the SAMRU Cloud. It's called the Eclipse OpenJ9 JIT server. That's the technology.", "tokens": [264, 9617, 49, 52, 8061, 13, 467, 311, 1219, 264, 462, 27197, 7238, 41, 24, 508, 3927, 7154, 13, 663, 311, 264, 2899, 13], "temperature": 0.0, "avg_logprob": -0.166386731966274, "compression_ratio": 1.5667870036101084, "no_speech_prob": 1.4061834917811211e-05}, {"id": 152, "seek": 100088, "start": 1005.92, "end": 1012.36, "text": " It's also called the SAMRU Cloud Compiler. It's available on Linux Java 8, 11, and 17.", "tokens": [467, 311, 611, 1219, 264, 9617, 49, 52, 8061, 6620, 5441, 13, 467, 311, 2435, 322, 18734, 10745, 1649, 11, 2975, 11, 293, 3282, 13], "temperature": 0.0, "avg_logprob": -0.166386731966274, "compression_ratio": 1.5667870036101084, "no_speech_prob": 1.4061834917811211e-05}, {"id": 153, "seek": 100088, "start": 1012.36, "end": 1015.24, "text": " Really good with microcontainers. In fact, that's the only reason I'm bringing it up", "tokens": [4083, 665, 365, 4532, 9000, 491, 433, 13, 682, 1186, 11, 300, 311, 264, 787, 1778, 286, 478, 5062, 309, 493], "temperature": 0.0, "avg_logprob": -0.166386731966274, "compression_ratio": 1.5667870036101084, "no_speech_prob": 1.4061834917811211e-05}, {"id": 154, "seek": 100088, "start": 1015.24, "end": 1023.16, "text": " today. It's Kubernetes ready. You can improve your ramp-up time, auto-scaling. And here's", "tokens": [965, 13, 467, 311, 23145, 1919, 13, 509, 393, 3470, 428, 12428, 12, 1010, 565, 11, 8399, 12, 4417, 4270, 13, 400, 510, 311], "temperature": 0.0, "avg_logprob": -0.166386731966274, "compression_ratio": 1.5667870036101084, "no_speech_prob": 1.4061834917811211e-05}, {"id": 155, "seek": 100088, "start": 1023.16, "end": 1029.96, "text": " the key point here I'll end with. So this is a Java solution to a Java problem. Initially", "tokens": [264, 2141, 935, 510, 286, 603, 917, 365, 13, 407, 341, 307, 257, 10745, 3827, 281, 257, 10745, 1154, 13, 29446], "temperature": 0.0, "avg_logprob": -0.166386731966274, "compression_ratio": 1.5667870036101084, "no_speech_prob": 1.4061834917811211e-05}, {"id": 156, "seek": 102996, "start": 1029.96, "end": 1034.72, "text": " I talked about that sweet spot space. So there's a lot of companies, a lot of vendors trying", "tokens": [286, 2825, 466, 300, 3844, 4008, 1901, 13, 407, 456, 311, 257, 688, 295, 3431, 11, 257, 688, 295, 22056, 1382], "temperature": 0.0, "avg_logprob": -0.09904202726698413, "compression_ratio": 1.5973451327433628, "no_speech_prob": 6.0058697272324935e-05}, {"id": 157, "seek": 102996, "start": 1034.72, "end": 1040.24, "text": " to figure out how to make that work better. And a lot of them involve doing other things", "tokens": [281, 2573, 484, 577, 281, 652, 300, 589, 1101, 13, 400, 257, 688, 295, 552, 9494, 884, 661, 721], "temperature": 0.0, "avg_logprob": -0.09904202726698413, "compression_ratio": 1.5973451327433628, "no_speech_prob": 6.0058697272324935e-05}, {"id": 158, "seek": 102996, "start": 1040.24, "end": 1048.0, "text": " besides what Java's all about, running the JVM, running the JIT. So it is a Java solution", "tokens": [11868, 437, 10745, 311, 439, 466, 11, 2614, 264, 508, 53, 44, 11, 2614, 264, 508, 3927, 13, 407, 309, 307, 257, 10745, 3827], "temperature": 0.0, "avg_logprob": -0.09904202726698413, "compression_ratio": 1.5973451327433628, "no_speech_prob": 6.0058697272324935e-05}, {"id": 159, "seek": 102996, "start": 1048.0, "end": 1057.2, "text": " to your Java problem. That's it for me today. That QR code will take you to a page I have", "tokens": [281, 428, 10745, 1154, 13, 663, 311, 309, 337, 385, 965, 13, 663, 32784, 3089, 486, 747, 291, 281, 257, 3028, 286, 362], "temperature": 0.0, "avg_logprob": -0.09904202726698413, "compression_ratio": 1.5973451327433628, "no_speech_prob": 6.0058697272324935e-05}, {"id": 160, "seek": 105720, "start": 1057.2, "end": 1062.3600000000001, "text": " that has a bunch of articles on how to use it, also the slides and other good materials", "tokens": [300, 575, 257, 3840, 295, 11290, 322, 577, 281, 764, 309, 11, 611, 264, 9788, 293, 661, 665, 5319], "temperature": 0.0, "avg_logprob": -0.24928772755158254, "compression_ratio": 1.2477064220183487, "no_speech_prob": 9.273248724639416e-05}, {"id": 161, "seek": 105720, "start": 1062.3600000000001, "end": 1066.0, "text": " about it. That's it for me. Thank you very much.", "tokens": [466, 309, 13, 663, 311, 309, 337, 385, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.24928772755158254, "compression_ratio": 1.2477064220183487, "no_speech_prob": 9.273248724639416e-05}, {"id": 162, "seek": 106600, "start": 1066.0, "end": 1095.12, "text": " It sounds amazing. It's amazing. It really is amazing. Well, why wouldn't you? Open", "tokens": [467, 3263, 2243, 13, 467, 311, 2243, 13, 467, 534, 307, 2243, 13, 1042, 11, 983, 2759, 380, 291, 30, 7238], "temperature": 0.0, "avg_logprob": -0.29924468994140624, "compression_ratio": 1.1690140845070423, "no_speech_prob": 0.0005667282384820282}, {"id": 163, "seek": 109512, "start": 1095.12, "end": 1101.32, "text": " J9 is a perfectly, I mean, it's a viable JVM. It's nothing special, right? And nothing", "tokens": [508, 24, 307, 257, 6239, 11, 286, 914, 11, 309, 311, 257, 22024, 508, 53, 44, 13, 467, 311, 1825, 2121, 11, 558, 30, 400, 1825], "temperature": 0.0, "avg_logprob": -0.17347872032309478, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.0004162398399785161}, {"id": 164, "seek": 109512, "start": 1101.32, "end": 1106.84, "text": " unique about it that makes you change your code. It's a JVM that just points to the", "tokens": [3845, 466, 309, 300, 1669, 291, 1319, 428, 3089, 13, 467, 311, 257, 508, 53, 44, 300, 445, 2793, 281, 264], "temperature": 0.0, "avg_logprob": -0.17347872032309478, "compression_ratio": 1.2878787878787878, "no_speech_prob": 0.0004162398399785161}, {"id": 165, "seek": 110684, "start": 1106.84, "end": 1133.32, "text": " open JDK, the open J9 JVM. Okay, here it comes. I think so because I've seen examples", "tokens": [1269, 508, 35, 42, 11, 264, 1269, 508, 24, 508, 53, 44, 13, 1033, 11, 510, 309, 1487, 13, 286, 519, 370, 570, 286, 600, 1612, 5110], "temperature": 0.0, "avg_logprob": -0.17064548307849514, "compression_ratio": 0.9883720930232558, "no_speech_prob": 0.00020967474847566336}, {"id": 166, "seek": 113332, "start": 1133.32, "end": 1145.32, "text": " of using those apps in tests. Check that, yeah. Yeah, okay. That may be a problem. She", "tokens": [295, 1228, 729, 7733, 294, 6921, 13, 6881, 300, 11, 1338, 13, 865, 11, 1392, 13, 663, 815, 312, 257, 1154, 13, 1240], "temperature": 0.0, "avg_logprob": -0.3170420328776042, "compression_ratio": 1.036144578313253, "no_speech_prob": 0.00015781758702360094}, {"id": 167, "seek": 114532, "start": 1145.32, "end": 1165.2, "text": " go out and check the latest coverage of that. Well, the way the AOT cache will work in this", "tokens": [352, 484, 293, 1520, 264, 6792, 9645, 295, 300, 13, 1042, 11, 264, 636, 264, 316, 5068, 19459, 486, 589, 294, 341], "temperature": 0.0, "avg_logprob": -0.17224522431691489, "compression_ratio": 1.325925925925926, "no_speech_prob": 4.253005317877978e-05}, {"id": 168, "seek": 114532, "start": 1165.2, "end": 1172.0, "text": " case for the JIT server, it's going to keep all that information and the profile has to", "tokens": [1389, 337, 264, 508, 3927, 7154, 11, 309, 311, 516, 281, 1066, 439, 300, 1589, 293, 264, 7964, 575, 281], "temperature": 0.0, "avg_logprob": -0.17224522431691489, "compression_ratio": 1.325925925925926, "no_speech_prob": 4.253005317877978e-05}, {"id": 169, "seek": 117200, "start": 1172.0, "end": 1178.28, "text": " match from the requesting JVM, right? So if it matches, it'll use it, right? Because", "tokens": [2995, 490, 264, 31937, 508, 53, 44, 11, 558, 30, 407, 498, 309, 10676, 11, 309, 603, 764, 309, 11, 558, 30, 1436], "temperature": 0.0, "avg_logprob": -0.13021623766100085, "compression_ratio": 1.5204678362573099, "no_speech_prob": 9.600562771083787e-05}, {"id": 170, "seek": 117200, "start": 1178.28, "end": 1182.74, "text": " also on the clients, they also have their own cache. They'll keep it, but they go away", "tokens": [611, 322, 264, 6982, 11, 436, 611, 362, 641, 1065, 19459, 13, 814, 603, 1066, 309, 11, 457, 436, 352, 1314], "temperature": 0.0, "avg_logprob": -0.13021623766100085, "compression_ratio": 1.5204678362573099, "no_speech_prob": 9.600562771083787e-05}, {"id": 171, "seek": 117200, "start": 1182.74, "end": 1187.08, "text": " once they go away, right? Or when you start a new instance of that app, you have a brand", "tokens": [1564, 436, 352, 1314, 11, 558, 30, 1610, 562, 291, 722, 257, 777, 5197, 295, 300, 724, 11, 291, 362, 257, 3360], "temperature": 0.0, "avg_logprob": -0.13021623766100085, "compression_ratio": 1.5204678362573099, "no_speech_prob": 9.600562771083787e-05}, {"id": 172, "seek": 118708, "start": 1187.08, "end": 1207.72, "text": " new flush cache. I'm sorry. Yeah, so that's what we were talking about. You want to go", "tokens": [777, 19568, 19459, 13, 286, 478, 2597, 13, 865, 11, 370, 300, 311, 437, 321, 645, 1417, 466, 13, 509, 528, 281, 352], "temperature": 0.0, "avg_logprob": -0.1707230198140047, "compression_ratio": 1.3533834586466165, "no_speech_prob": 0.00011409600119804963}, {"id": 173, "seek": 118708, "start": 1207.72, "end": 1212.48, "text": " static. You're going to get a smaller image running statically, but you lose all the benefits", "tokens": [13437, 13, 509, 434, 516, 281, 483, 257, 4356, 3256, 2614, 2219, 984, 11, 457, 291, 3624, 439, 264, 5311], "temperature": 0.0, "avg_logprob": -0.1707230198140047, "compression_ratio": 1.3533834586466165, "no_speech_prob": 0.00011409600119804963}, {"id": 174, "seek": 121248, "start": 1212.48, "end": 1219.84, "text": " of the JIT. Over time, yes. So that may be a great solution for short-lived apps, right?", "tokens": [295, 264, 508, 3927, 13, 4886, 565, 11, 2086, 13, 407, 300, 815, 312, 257, 869, 3827, 337, 2099, 12, 46554, 7733, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.34558783225642825, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.000240710360230878}, {"id": 175, "seek": 121248, "start": 1219.84, "end": 1223.04, "text": " But the longer your job app runs, the more you're going to benefit from that optimized", "tokens": [583, 264, 2854, 428, 1691, 724, 6676, 11, 264, 544, 291, 434, 516, 281, 5121, 490, 300, 26941], "temperature": 0.0, "avg_logprob": -0.34558783225642825, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.000240710360230878}, {"id": 176, "seek": 121248, "start": 1223.04, "end": 1233.44, "text": " code, right? Yes? So Eclipse on the J9 is not a certain set-byte, but my main server is", "tokens": [3089, 11, 558, 30, 1079, 30, 407, 462, 27197, 322, 264, 508, 24, 307, 406, 257, 1629, 992, 12, 2322, 975, 11, 457, 452, 2135, 7154, 307], "temperature": 0.0, "avg_logprob": -0.34558783225642825, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.000240710360230878}, {"id": 177, "seek": 121248, "start": 1233.44, "end": 1240.28, "text": " also a set-byte for open edition, but today it has available binaries. But for Eclipse,", "tokens": [611, 257, 992, 12, 2322, 975, 337, 1269, 11377, 11, 457, 965, 309, 575, 2435, 5171, 4889, 13, 583, 337, 462, 27197, 11], "temperature": 0.0, "avg_logprob": -0.34558783225642825, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.000240710360230878}, {"id": 178, "seek": 124028, "start": 1240.28, "end": 1247.28, "text": " they are not able to actually release the binaries because they cannot actually access", "tokens": [436, 366, 406, 1075, 281, 767, 4374, 264, 5171, 4889, 570, 436, 2644, 767, 2105], "temperature": 0.0, "avg_logprob": -0.24445105290067368, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.0001372659462504089}, {"id": 179, "seek": 124028, "start": 1247.28, "end": 1255.72, "text": " the TCK certification process. So that whole TCK issue is a, I don't know. Well, I guess", "tokens": [264, 314, 9419, 21775, 1399, 13, 407, 300, 1379, 314, 9419, 2734, 307, 257, 11, 286, 500, 380, 458, 13, 1042, 11, 286, 2041], "temperature": 0.0, "avg_logprob": -0.24445105290067368, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.0001372659462504089}, {"id": 180, "seek": 124028, "start": 1255.72, "end": 1262.0, "text": " I could say, it seems to be an issue more with IBM and Oracle, right? So our own tests", "tokens": [286, 727, 584, 11, 309, 2544, 281, 312, 364, 2734, 544, 365, 23487, 293, 25654, 11, 558, 30, 407, 527, 1065, 6921], "temperature": 0.0, "avg_logprob": -0.24445105290067368, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.0001372659462504089}, {"id": 181, "seek": 126200, "start": 1262.0, "end": 1278.96, "text": " are going to be, they're going to encompass all the TCK stuff. Open J9 is managed by Eclipse,", "tokens": [366, 516, 281, 312, 11, 436, 434, 516, 281, 28268, 439, 264, 314, 9419, 1507, 13, 7238, 508, 24, 307, 6453, 538, 462, 27197, 11], "temperature": 0.0, "avg_logprob": -0.12543698361045436, "compression_ratio": 1.4432432432432432, "no_speech_prob": 9.894855611491948e-05}, {"id": 182, "seek": 126200, "start": 1278.96, "end": 1283.6, "text": " but 99% of the contributions are from IBM. It's a big part of their business. It's not", "tokens": [457, 11803, 4, 295, 264, 15725, 366, 490, 23487, 13, 467, 311, 257, 955, 644, 295, 641, 1606, 13, 467, 311, 406], "temperature": 0.0, "avg_logprob": -0.12543698361045436, "compression_ratio": 1.4432432432432432, "no_speech_prob": 9.894855611491948e-05}, {"id": 183, "seek": 126200, "start": 1283.6, "end": 1288.0, "text": " going to go anywhere. If you have to do open source, this is like the best of the most", "tokens": [516, 281, 352, 4992, 13, 759, 291, 362, 281, 360, 1269, 4009, 11, 341, 307, 411, 264, 1151, 295, 264, 881], "temperature": 0.0, "avg_logprob": -0.12543698361045436, "compression_ratio": 1.4432432432432432, "no_speech_prob": 9.894855611491948e-05}, {"id": 184, "seek": 128800, "start": 1288.0, "end": 1292.52, "text": " worlds, I think. It's available. It's open. You can see it, but you know you have a vendor", "tokens": [13401, 11, 286, 519, 13, 467, 311, 2435, 13, 467, 311, 1269, 13, 509, 393, 536, 309, 11, 457, 291, 458, 291, 362, 257, 24321], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 185, "seek": 128800, "start": 1292.52, "end": 1296.0, "text": " who has their business based on it that it's not going to go anywhere, and they're going", "tokens": [567, 575, 641, 1606, 2361, 322, 309, 300, 309, 311, 406, 516, 281, 352, 4992, 11, 293, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 186, "seek": 128800, "start": 1296.0, "end": 1300.36, "text": " to put a lot of resources to making it better. So, you know, I'm just telling you right", "tokens": [281, 829, 257, 688, 295, 3593, 281, 1455, 309, 1101, 13, 407, 11, 291, 458, 11, 286, 478, 445, 3585, 291, 558], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 187, "seek": 128800, "start": 1300.36, "end": 1304.92, "text": " now that we just came up with a JIT server. We're going into beta on Instant On. I don't", "tokens": [586, 300, 321, 445, 1361, 493, 365, 257, 508, 3927, 7154, 13, 492, 434, 516, 666, 9861, 322, 38707, 1282, 13, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 188, "seek": 128800, "start": 1304.92, "end": 1309.28, "text": " know if you've heard of that. It's based on CryU. So we're going to be able to take snapshots", "tokens": [458, 498, 291, 600, 2198, 295, 300, 13, 467, 311, 2361, 322, 12267, 52, 13, 407, 321, 434, 516, 281, 312, 1075, 281, 747, 19206, 27495], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 189, "seek": 128800, "start": 1309.28, "end": 1312.76, "text": " of those images, and you can put those in your containers. Those are going to start", "tokens": [295, 729, 5267, 11, 293, 291, 393, 829, 729, 294, 428, 17089, 13, 3950, 366, 516, 281, 722], "temperature": 0.0, "avg_logprob": -0.1536173724488124, "compression_ratio": 1.7508196721311475, "no_speech_prob": 0.00011056088987970725}, {"id": 190, "seek": 131276, "start": 1312.76, "end": 1318.92, "text": " up in milliseconds. So JIT basically handles the JIT server, handles the ramp up time,", "tokens": [493, 294, 34184, 13, 407, 508, 3927, 1936, 18722, 264, 508, 3927, 7154, 11, 18722, 264, 12428, 493, 565, 11], "temperature": 0.0, "avg_logprob": -0.16651527616712783, "compression_ratio": 1.4274193548387097, "no_speech_prob": 9.595324081601575e-05}, {"id": 191, "seek": 131276, "start": 1318.92, "end": 1323.32, "text": " but Instant On will handle the start up time. So we're talking milliseconds. That's coming", "tokens": [457, 38707, 1282, 486, 4813, 264, 722, 493, 565, 13, 407, 321, 434, 1417, 34184, 13, 663, 311, 1348], "temperature": 0.0, "avg_logprob": -0.16651527616712783, "compression_ratio": 1.4274193548387097, "no_speech_prob": 9.595324081601575e-05}, {"id": 192, "seek": 132332, "start": 1323.32, "end": 1347.3999999999999, "text": " out in the next couple of months or so. Anyway, thank you. Well, if you don't have the JIT,", "tokens": [484, 294, 264, 958, 1916, 295, 2493, 420, 370, 13, 5684, 11, 1309, 291, 13, 1042, 11, 498, 291, 500, 380, 362, 264, 508, 3927, 11], "temperature": 0.0, "avg_logprob": -0.14623432159423827, "compression_ratio": 1.0224719101123596, "no_speech_prob": 0.00010546383418841287}, {"id": 193, "seek": 134740, "start": 1347.4, "end": 1353.92, "text": " then you're going to be running interpreted. That's like the worst of everything. Oh, well,", "tokens": [550, 291, 434, 516, 281, 312, 2614, 26749, 13, 663, 311, 411, 264, 5855, 295, 1203, 13, 876, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.16252277447627142, "compression_ratio": 1.6559633027522935, "no_speech_prob": 7.479790656361729e-05}, {"id": 194, "seek": 134740, "start": 1353.92, "end": 1361.64, "text": " it won't be. But you still want to use the JIT remotely. Oh, you're talking about locally.", "tokens": [309, 1582, 380, 312, 13, 583, 291, 920, 528, 281, 764, 264, 508, 3927, 20824, 13, 876, 11, 291, 434, 1417, 466, 16143, 13], "temperature": 0.0, "avg_logprob": -0.16252277447627142, "compression_ratio": 1.6559633027522935, "no_speech_prob": 7.479790656361729e-05}, {"id": 195, "seek": 134740, "start": 1361.64, "end": 1367.5600000000002, "text": " It will not be used. It will not be used. By the way, yeah. And by the way, the JIT server", "tokens": [467, 486, 406, 312, 1143, 13, 467, 486, 406, 312, 1143, 13, 3146, 264, 636, 11, 1338, 13, 400, 538, 264, 636, 11, 264, 508, 3927, 7154], "temperature": 0.0, "avg_logprob": -0.16252277447627142, "compression_ratio": 1.6559633027522935, "no_speech_prob": 7.479790656361729e-05}, {"id": 196, "seek": 134740, "start": 1367.5600000000002, "end": 1373.16, "text": " is just another persona of the JVM. It's just running under a different persona. No, it", "tokens": [307, 445, 1071, 12184, 295, 264, 508, 53, 44, 13, 467, 311, 445, 2614, 833, 257, 819, 12184, 13, 883, 11, 309], "temperature": 0.0, "avg_logprob": -0.16252277447627142, "compression_ratio": 1.6559633027522935, "no_speech_prob": 7.479790656361729e-05}, {"id": 197, "seek": 137316, "start": 1373.16, "end": 1378.3600000000001, "text": " won't do that. Okay. Thank you very much. Okay. Thank you.", "tokens": [50364, 1582, 380, 360, 300, 13, 1033, 13, 1044, 291, 588, 709, 13, 1033, 13, 1044, 291, 13, 50624], "temperature": 0.0, "avg_logprob": -0.20217652320861818, "compression_ratio": 1.1372549019607843, "no_speech_prob": 0.0004087536653969437}], "language": "en"}