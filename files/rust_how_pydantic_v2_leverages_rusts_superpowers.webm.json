{"text": " So, we have Samuel here to talk about Pydantic 2 and how it leverages REST superpowers. Thank you very much. Can you hear me at the back? Great. It's a bit about me. I'm Samuel. I've been a software developer for 10 years, among other things. I've been doing open source quite a lot for the last five years, mostly Python projects, but moving a bit into REST over the last few years. The most high profile Python project that I maintain is Pydantic, which I started back in 2017 and has subsequently kind of taken over my life. I've been working on it full time for the last year. So, what I'm going to talk about today, I'm going to give you a bit of an introduction to Pydantic, some hype numbers for some vanity, but also for some context of why making Pydantic better is worthwhile. I'm going to explain why I decided to rebuild Pydantic completely. I'm going to talk a bit about how I've done that with REST, and I guess most importantly why doing it in REST is the right choice. I'm kind of preaching to the converted, but hey, what I'm not going to do is a like, hello world. This is how you would build a Python extension in REST. There were lots of other talks on that. They're great. And also the PyO3 documentation is amazing, so I think it's more interesting to go into a bit of depth on the challenges, the advantages than just to do the hello world example again. What is it? Well, Pydantic is a data validation library in Python. It's not the first. It's definitely not the last. It started off as a side project like so many open source projects. Nothing special. I maintained it my spare time. People came along occasionally, said nice things, reported bugs. Occasionally said not very nice things, and then something weird happened, and its usage went crazy. So the first thing that happened, which you can't really see on this graph in 2018, my friend Sebastian Ramirez started the fast API project, which is a web framework in Python, which uses Pydantic and has now got, I don't know, how many thousand stars, 60,000 stars or something. It's got a lot of attention. You can see fast API growth there. That got a lot of people, I think, to first find out about Pydantic, but something else happened at the beginning of 2021 to cause Pydantic's download numbers to go crazy. Now, I'm well aware that Aaron Armin's speech at talk earlier kind of pre-trolled me before I'd even made my talk, saying that download numbers are a terrible metric, but they are the only metric, so that's what we have to use. It's also worth saying that I have actually looked at Pydantic's downloads in terms of as a dependency and as a direct download. It's not that easy to do with PyPI, but it looks like about 15 million downloads a month are from, as a dependency of another package, and the remaining 25 or so million are people installing Pydantic directly, so it seems like people are using it not just as a dependency of another library. I've included Django on there as the middle line, because it's the most high-profile, most well-known web framework in Python. Not to be critical of it, it's amazing, it's changed my life. I mean, no disrespect by saying we've overtaken it, but just that Pydantic's usage has gone mad. In terms of how it's used, it's used by lots of organizations you would expect, all the fang companies, something like 19 out of the top 25 people companies in NASDAQ, but also by organizations which you wouldn't expect, like JPMorgan, use it quite a lot, I don't know in what regard. But it's quite interesting, if you have an open source project, if you look in analytics at the referrers, lots of those big, very security-centric companies forget to turn off the referrer header from their internal systems, and they name their internal systems things like github.jbmorgan.net, so you can see which companies are using your dependencies by looking at those referrers. So, for example, Apple have no public demonstration of using Pydantic at all, but six different enterprise instances of github within Apple use Pydantic, and you can even see which ones they are. They're like maps.github.maps.apple.com, github.serie.apple.com, etc. It's also used by some cool organizations. It's used by NASA for processing imagery from James Webb, and it's used by the international panel on climate change for processing the data that they give to the UN on climate change every month, which is the stuff that I'm most proud of and why I want to make Pydantic better. So, what's so great about Pydantic? Why are so many people using it? The short answer is I don't know, because you can't go and ask those people, you can't look at a graph, but it can't really tell you, but we can kind of look at Pydantic and what people say, and we can kind of guess at what's made it popular. So, this is, I know we're in the Rust room, we've got some Python code, don't worry, we'll get to Rust later. This is some Python code that demonstrates what Pydantic does. So, we have a model which kind of represents a talk, which has four fields in this case. Obviously, title is a string, attendance is an integer, the number of people who came, when, which is a date time, or none, and has a default value of none, and then the mistakes I make, which is a list of two pools with the time they were made at and a description. So, and then lastly, last line, we instantiate an instance of talk using that data, and if there was a mistake, we'd get an error, if there wasn't a mistake, we'd obviously get the instance. The first thing that makes Pydantic special, and the reason that people like it, is because we use Python type hints to define the types. That's become reasonably commonplace now, there are a whole suite of different libraries that do the same thing, either because it's obvious or because they're copying Pydantic, but Pydantic was the first to do that, because type hints were kind of new in 2017, and obviously, the main advantage is it's easy to learn, you don't need to learn a new kind of DSL to define stuff, but it's also compatible with static type checking with all the rest of your code, with your IDE. Once you defined your model, and if Pydantic's worked correctly, then you know you've got a proper instance of talk. The frustration that caused me to create Pydantic was that type annotations existed, they sat there in the code, you could read them, but they did nothing at runtime, and so, effectively, could we try and make them work? The second and slightly more controversial thing that Pydantic does, which I think is one of the reasons that people find it easy to use, is because we default to coercion. So, you can see a tendance there, although it needs to be an integer, it's defined as a string. Pydantic will automatically coerce from, for example, a valid string to an integer, but it'll also do other coercions that are a bit more commonplace like coercing a string as an isodate format into a datetime object, and same for the durations. Some people hate that, some people complain about it a lot, I suspect that lots of people who don't even realize they're using it, they process environment variables, or JSON, or URL arguments, and they're always strings, and Pydantic just works and they don't even see it. A few other reasons I think we're quite popular, we're fast-ish, we're friendly-ish on the bug tracker, I don't promise not to ever be cross with people, and we're reasonably feature-complete. So, that was Pydantic, it's great, lots of people are using it, what's the problem? Well, it started off as a side project for me, it wasn't designed to be successful, and the internals stink. I'm very proud of what Pydantic is doing in terms of how it's being used, I'm not proud of what's under the hood, and so I've been keen for a long time to fix the internals. Also, second way in which I'm in kind of trouble before my talk was talking about API compatibility, we're going to have to break a lot of things in Pydantic V2 to get it right, but that's the right thing to do, I think, to get the future API to be correct and stable and not break again. And while we're building V2, why don't we do some other stuff, so make it even faster, it's already quite fast, but if you think about that number of downloads, you think about the number of CPU cycles globally every day devoted to doing validation with Python, but that's currently with Pydantic, that's currently all in Python, that's probably quite a lot of carbon dioxide that's being released, effectively unnecessarily, because we could make Pydantic significantly faster. Strict mode, I already talked about, because while often you don't need it, there are legitimate cases where you want strict mode. We have functional validators, which is effectively running some Python code to validate a field, they're useful, but they would be more useful if they could operate like an onion, so like middleware where you take both a value and a handler, and call the handler if you want to, once you've done some processing of the value, that would be super valuable, another thing we could add, composability. So Pydantic, as I showed you earlier, is based on the Pydantic model, often your root type doesn't need to be a Pydantic model or shouldn't be a Pydantic model, it might be a list, it might be a tuple, it might be a list of models, it might be a type dict, which is a common new type in Python, and then lastly, maintainability, since I maintain Pydantic, I want maintaining it to be fun, so about a year ago, last March, I started as a kind of experiment, could I rebuild some of it in Rust, a year later, I'm still working on it full time, and we're nearly there. So what does it mean to validate Python data in Rust? What's the process? Well, phase one, we need to take a Pydantic model and convert it to a Rust structure, so unlike libraries like CERD, we're not compiling models in Rust, the compiled Rust code doesn't know anything about the models it's going to receive, because obviously Python developers don't want to be compiling Rust code to get their model to work. So we have to have a, in Rust terms, dynamic definition of our schema, which we can then use for validation. The way we build that is effectively these validators, which are structs that contain both characteristics of what they're going to validate, but also other validators recursively such that you can define complex structures. So in this case, our outermost validator is a model validator, which effectively just instantiates an instance of torque and sets its attributes from a dictionary. It contains another validator, which is a type dict validator, which contains the definition of all the fields, which have effectively the key that they're going to look for and then a validator that they're going to run. The first two are reasonably obvious. I've added a few constraints to show how you would manage those constraints. And then the third one, the when field is obviously a union, which in turn contains a vect of validators to run effectively in turn to try and find the value. And then the last one, which is the kind of more complex type, contains this list validator, which contains tuple validator, which contains two more validators. And we can build up effectively infinitely complex schemas from a relatively simple, I say relatively simple principle at the outset, which is we have a validator. It's going to contain some other stuff. So what does that look like in code? I said I was going to show you some Rust code. I'm going to show you some Rust code because I think this is the most clear way of explaining what it is that we do. So the root of everything is this trait validator, which contains effectively three things. It contains a const, a static string, which is used for defining, as I'll show you later, which validator we're going to use for a given bit of data build, which is a simple function to construct an instance of itself in the generic sense. And then the validate function that goes off and does the validation. We then take all of those, well, we then implement that trait for all of the common types that we want. So I think we have 58, 48 or so different validators, and then we bang all of them into one massive enum. Then the magic bit, which is provided by enum dispatch, which is a Rust crate that effectively implements a trait on an enum if every member of that enum implements that trait. Effectively, it goes and does a big procedural macro to create an instance, an implementation of the function, which is just a big match, choosing which function to call. But it's significantly faster than dine. And in fact, in some cases, it can abstract away everything and be as fast as just calling the implementation directly. So I said earlier that we needed to use this constant, the expected type. We use that in another effectively big enum to go through and we take the type attribute out of this schema, which is a Python dictionary. And we use that to effectively look up which validator we're going to go and build. And again, I've shown a few here, but there's obviously a bunch more. This in real life is not implemented as a big match statement like this. It's a macro that builds this function, but it's clearer here if you get the idea. So I showed you earlier this validate function and I kind of skipped over the input argument. So the input argument is just an implementation of a trait. That trait input is like the beginnings of which are defined here. And it effectively gives you all the things that the validation functions are going to need on a value. So is none strict string, lack string, int, float, et cetera, et cetera, but also more complex types like date, date time, dictionary, et cetera, et cetera. And then we implement that trait on both a Python value and on a JSON value, which means that we can parse Rust directly without having to go via Python. That's super valuable for two reasons. One for performance reasons. So if our input is a string and if we were to then parse it into Python objects and then take that into our validator and run all of that, that would be much lower than parsing in Rust and then running the validator in Rust straight away. The other big advantage is to do with strict mode. So I said earlier that people want strict mode, but they say they want strict mode, but often they don't. So what people will say is, I want totally strict Python, why isn't it strict? And then you'll say, well, do you want to load data from JSON? And they say, yeah, of course I do. And you say, well, how are you going to define a date? And they're like, oh, well, obviously I'll use a standard date format. But that's not strict then. You're parsing a string. And they're like, oh, that's fine because it should know in that case it's coming from JSON. Well, obviously, how are we going to do that? By parsing JSON directly and in future potentially other types, we can implement our strict date method, both on that JSON input. We can say, well, we're in JSON. We don't have a date type. So we're going to have to do something. So we're going to parse a string. And effectively, the strict date implementation for JSON will parse a string. And therefore, we can have a strict mode that's actually useful, which we wouldn't have had if we couldn't have had in pydantic v1, where the validation logic doesn't know anything about where the date is coming from. Even if we have a parse JSON function, all it's doing is parsing JSON to Python and then doing validation. So then that's all very well. That defines effectively how we do our validation. What's the interface to Python? So that's where we have this schema validator rust struct, which using the PyClass decorator is also available as a Python class. And all it really contains is a validator, which of course can in turn contain other validators, as I said earlier. And its implementation, which are all then exposed as Python methods, are new, which just construct it. So we call the build validator and get back an instance of our validator, which we then store and return the type. Actually, this is much more complicated. One of the cleverest and most infuriating bits of pydantic core is that we, as you can imagine, this schema for defining validation becomes quite complex. It's very easy to make a mistake. So we validate it using pydantic core itself, which when it works is magic, and when it doesn't work leads to impossible errors, because obviously all of the things that you're looking at as members of dictionaries are in turn the names of bits of validation. So it's complete hell, but it works, and it makes it very hard to build an invalid or not build the validator that you want. And then we have these two implementations, two functions which do validate Python objects, as I said earlier, which call validate, and same with JSON, where we parse a JSON using SERD to a JSON value and then call validate again with that input. This code is obviously heavily simplified so that it fits. It doesn't fit on the page, but nearly fits on the page. So not everything is exactly as it really is, but I think that kind of gives you an idea of how we build up these validators. The other thing missed here, we also do the whole thing again for serialization. So the serialization from both a pydantic model to a Python dictionary and from a pydantic model straight to JSON is all written in Rust, and it does useful things like filtering out elements as you go along, and it's effectively the same structure, uses the same schema, but it's just dedicated to serialization rather than validation. So what does the Python interface then look like? So what I didn't explain earlier is that pydantic v2, which is going to be released, fingers crossed in Q1 this year, is made up of two packages. We have pydantic itself, which is a pure Python package, and then we have pydantic core, which is almost all Rust code. We have a little bit of shim of Python to explain what's going on, but it's really just the Rust code I've been showing you. So what pydantic now does, all that pydantic effectively takes care of is converting those type annotations I showed you earlier into a pydantic core schema and then building a validator. So looking at an example here, we obviously import schema validator, which I just showed you from, that's come up at the wrong time, from pydantic core, and then the base level, the schema for the base validator is model, and it contains, as I said earlier, a class, which is the Python class to instantiate, and another schema, which in turn defines the fields. And that inner validator is then defined by a type dict validator, as I said earlier. So this is completely valid Python code. This will run now. So yeah, we have a type dict validator which contains fields, which in turn are those fields which I showed you earlier. So title attendances of type int when I talked about earlier. The most interesting thing here is, if you look at the when validator, it gets a bit confusing. It's schema is of type default, which in turn contains another schema, which is of type nullable, which is the simplest union, either a value or none. The default validator contains another member, which is the default value, in this case none, and the inner schema is then nullable, which in turn contains another inner schema, which is then the date time. So that's how we define effectively default values and null or nullable. So one of the other mistakes in Pyrantic in the past was that we kind of conflate, effectively Python made a mistake about 10 years ago where they used, they had a alias for union of something and none, that they called optional, which then meant that I didn't want to have a thing called optional, but was not optional. And so we conflated nullable with optional in Pyrantic and rightly it confused everyone. And so the solution, the solution from Python was to start using the pipe operator for unions and to just basically ignore optional. They can't really get rid of it, but they just pretend it didn't really happen. My solution is to define default and nullable as completely separate things and we're not going to use the optional type anywhere in our docs. We're just going to use union of thing and none to avoid that confusion. And then I think mistakes, I hope it kind of makes sense to you. Again, it's this like schema within schema within schema, which become validator within validator. And we take our code, as I showed you earlier, run validation. In this case, we call validate Python. We've got some Python code, but we could just as well have a JSON string and call validate JSON. And then we have a talk instance, which lets us access the members of it as you normally would. So where does Rust excel in these applications? Why build this in Rust? There are a bunch of obvious reasons to use Rust, performance being the number one, multi-threading and not having the global interpreter lock in Python is another one. The third is using high-quality existing Rust libraries to build libraries in Python instead of implementing it yourself. So I maintain two other Python libraries written in Rust watch files, which uses the notify crate to do file-watching and then RTOML, which, as you can guess, is a TOML parser using the TOML library from Rust. And the RTOML library is the fastest Python TOML parser out there. And actually watch files is becoming more and more popular. It's the default now with u-vehicle, which is one of the web servers. But perhaps less obviously in terms of where Rust fits in best. Deeply recursive code, as I've just showed you, with these validators within validators. There's no stack. And so we don't have a penalty for recursion. We do have to be very, very careful, because, as I'm sure you all know, if you have recursion in Rust and you don't catch it, you just get a segfault. And that would be very, very upsetting to Python developers who've never seen one before. So there's an enormous amount of, as a significant amount of code in Pydantic Core dedicated to catching recursion, we have to have, is it two or three different sorts of guard to protect against recursion in all possible different situations, because it's effectively the worst thing that we can have, is that there is some data structure that you can pass to Pydantic, which causes your entire Python process to segfault, and you wouldn't know where to even start looking. So that's a blessing. The lack of a stack is a blessing and a curse. And then the second big advantage, I think, of where Rust excels, is in the small modular components. So where I was showing you before, these relatively small, in terms of code footprint validators, which in turn hold other ones, there's obviously no performance penalty for having these functions in Rust. I say almost, because we actually have to use box around validators because they hold themselves. So there is a bit of an overhead of going into the heap, but it's relatively small, particularly compared to Python. And then the lastly complex error handling, obviously in Python, you don't know what's going to error and what exceptions you're going to get in Rust. Putting to one side the comment about panic earlier, you can in general know what errors you're going to get and catch them and construct validation errors in the case of Pydantic, which is a great deal easier than it would ever have been to write that code in Python. So the way I want to think about the future development of Python is not as Python versus Rust, but effectively as Python as the user interface for Rust, or the application developer interface for Rust. So I'd love to see more and more libraries do what we've done with Pydantic Core and effectively implement their low-level components in Rust. So my dream is a world in which, thinking about the lifecycle of a HTTP request, but you could think the same about some NL pipeline or many other applications, we effectively, the vast majority of the execution is Rust or C, but then all of the application logic can be in Python. So effectively we get to a point where we have 100% of developer time spent in high-level languages, but only 1% of CPU dedicated to actually running Python code, which is slower and is always going to be slower. I don't think there's ever a world in which someone's going to come up with a language that is as fast and as safe as Rust, but also as quick to write as Python. So I don't think it should be one versus the other. It should be building the low-level, building the Rails, perhaps a bad term, but the Rails in Rust, and building the train in Python. It doesn't work, but you get where I'm coming from. Anyway, on that note, thank you very much. A few links there, particularly thanks to the PyO3 team who built the bindings for Rust in Python, which is amazing. And if you want a laugh, there's a very, very funny issue on GitHub where a very angry man says why we should never use Rust. So if you want to read that, I then took some time to take them to pieces, which was quite satisfying, although a waste of time. So have a look at that. Questions? First, especially for the sanitation, are you thinking to publish a library of Rust? The job is already done, and you could have a public API in a library ready to validate Rust data. I don't understand quite what... So you wrote the library in Rust, so could you publish just an API to validate JSON, for example, from Rust, instead of through Python? Absolutely, you could, and it would be useful if you wanted to somehow construct the schema at runtime fast, but it's never going to be anywhere near as performant as said, because you were not compiling... We can't do anything at compile time. Secondly, it's currently all completely intertwined with the PyO3 library and the Python types. So there is a future nascent possible project, Tidantic, which is Pidantic for TypeScript, where we take the PyO3 types, we effectively replace them with a new library which has a compile time switch between the Python bindings and the JavaScript bindings or the Wasm bindings, and then we can build Tidantic. That's a future plan, but a long way off. Right now, it wouldn't really be worth it, because you would get lots of slowdown from Python and from compile time. So we need a completely different library, just for us, like you're saying. Yeah, SIRD is amazing. I don't think I'm going to go and try and compete with that. At least, it's great for that application. Thanks for the talk. Recently, I think the Python library cryptography introduced Rust, and had some complaints from people using obscure build processes where Rust didn't work. Are you expecting anything from that? So I will actually bring up now. Now I'm going to get into how to... Effectively, go and read that issue, where, among other things, I... Oh, how do I get out of this mode? So, rant, rant, rant, rant from him. Effectively, I went through the... just over a quarter of a billion downloads over the last 12 months of Pydantic, and I worked out looking at the distribution of the different operating systems and, like, libc implementations, et cetera, that 99.9859% of people would have got a binary if they had installed Pydantic Core then. That number will be higher now, because there will be fewer esoteric operating systems. Most of the other ones, most of the failed ones, if you look, are actually installing, say, they're installing Python onto iOS. I don't know what that means, or whether it could ever work, but... Also, the other thing I would say is, Pydantic Core is already compiled to WebAssembly, so you can already run it in the browser. So I understand why people complained, but I think it's not a concern for... it's a straw man for most people. So that's why you slapped down. Yeah. And, again, if there's another... if there's a distribution that we don't... if we release 60 different binaries, if there's another one, we'll try and compile for it and release the binary. There's a question right at the back, I think, just to... I'll get back to the talk rather than... where are we? Is there a way to use the Django models as Pydantic models? Say again? To use the Django to have, like, a binding or to translate the Django model directly into a Pydantic model? There's no way at the moment. There's a number of different ORMs, I know of, built on top of Pydantic, which effectively allow that... if you were wanting specifically Django, there's a project called DjangoNinja that makes extensive use of Pydantic. I don't know that much about it, but if you actually wanted Pydantic models, you'd probably want some kind of code reformat to convert them. So I look at DjangoNinja, I'm sure what they're doing is the best of what's possible right now. Okay, thank you. If you had additional time, say, after finishing Pydantic, are there any other projects where you'd like to follow this vision of, like, a Rust core with Python user space or, like, API? Yeah, there are a number of ones. So there's already OR JSON, which is a very, very fast if unsafe in the sense of littered with unsafe JSON parser, which is very, very fast. The obvious one is a web framework where you do, like I kind of showed here, like the HTTP parsing, the routing, all in Rust. That's not very easy using ASGI. There are already a few projects doing that. So that would be the obvious one, but there's no winner yet. Currently, the best low-level web framework is Starlit, which FastAPI is built on, but I think it does use Rust for, it uses a Rust library for HTTP parsing or a C library. So some of it's already happening, but no obvious candidate right now. What I would say, though, is libraries like Rich, no criticism of Will, but, like, Rich is incredibly complicated. It's for terminal output. It's not so much performance critical, but it's really quite involved in complex logic. I would much prefer to write that logic in Rust than Python. Yeah, I think there are lots of candidates. Tonya online is asking, what do you mean by Python as the application layer? So I guess I could have added some example code here, but you can imagine a Python function, which is a view endpoint in a web framework, which takes in some validated arguments from done by the Pydantic. You then decide in Python to make a query to the database to get back the user's name from the ID, and then you return a JSON object containing data about the user. If you think about that, all of the code outside the Python functions, excuse me, could be written in a faster language, whether it be the database query accessing the database, TSL termination, HTTP parsing, routing, validation, but effectively using Python to define as a way to effectively configure Rust code or configure compile code. Yes, hello. I have a question, just a Pydantic one. Is there any support or are you planning any support alternative schema types like Protobuf, or JRPC, or Avro? Possibly in future. What I have a plan for is, I don't want to build them into Pydantic. Pydantic's already big, but there is a, obviously you can parse them to Python now, parse them and then validate them as a Python object. There is a plan effectively to take the, this, that's the one. Which you would then construct in Rust, parse as a Python value into Pydantic core, which would then extract the raw underlying Rust instance and then validate that. And that would allow you to get basically a Python validation effectively, but without having to, us having to either have compile time dependencies or build it all into Pydantic core. I think that's our last question that we have time for. One comment we did get from Matrix was that this code is a bit small on the, on the display, so if you upload the, I will do, yeah. Perfect. So if you're watching the stream, the slides will be uploaded and you can read the code. Oh, I'll put them on Twitter as well, but yeah, definitely. I'll upload them as well. Awesome. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.0, "text": " So, we have Samuel here to talk about Pydantic 2 and how it leverages REST superpowers.", "tokens": [407, 11, 321, 362, 23036, 510, 281, 751, 466, 430, 6655, 7128, 568, 293, 577, 309, 12451, 1660, 497, 14497, 1687, 47953, 13], "temperature": 0.0, "avg_logprob": -0.25750111861967706, "compression_ratio": 1.3296703296703296, "no_speech_prob": 0.23311316967010498}, {"id": 1, "seek": 0, "start": 12.0, "end": 18.0, "text": " Thank you very much. Can you hear me at the back?", "tokens": [1044, 291, 588, 709, 13, 1664, 291, 1568, 385, 412, 264, 646, 30], "temperature": 0.0, "avg_logprob": -0.25750111861967706, "compression_ratio": 1.3296703296703296, "no_speech_prob": 0.23311316967010498}, {"id": 2, "seek": 0, "start": 18.0, "end": 26.0, "text": " Great. It's a bit about me. I'm Samuel. I've been a software developer for 10 years, among other things.", "tokens": [3769, 13, 467, 311, 257, 857, 466, 385, 13, 286, 478, 23036, 13, 286, 600, 668, 257, 4722, 10754, 337, 1266, 924, 11, 3654, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.25750111861967706, "compression_ratio": 1.3296703296703296, "no_speech_prob": 0.23311316967010498}, {"id": 3, "seek": 2600, "start": 26.0, "end": 35.0, "text": " I've been doing open source quite a lot for the last five years, mostly Python projects, but moving a bit into REST over the last few years.", "tokens": [286, 600, 668, 884, 1269, 4009, 1596, 257, 688, 337, 264, 1036, 1732, 924, 11, 5240, 15329, 4455, 11, 457, 2684, 257, 857, 666, 497, 14497, 670, 264, 1036, 1326, 924, 13], "temperature": 0.0, "avg_logprob": -0.10219441839011319, "compression_ratio": 1.5318181818181817, "no_speech_prob": 8.127811452141032e-05}, {"id": 4, "seek": 2600, "start": 35.0, "end": 47.0, "text": " The most high profile Python project that I maintain is Pydantic, which I started back in 2017 and has subsequently kind of taken over my life.", "tokens": [440, 881, 1090, 7964, 15329, 1716, 300, 286, 6909, 307, 430, 6655, 7128, 11, 597, 286, 1409, 646, 294, 6591, 293, 575, 26514, 733, 295, 2726, 670, 452, 993, 13], "temperature": 0.0, "avg_logprob": -0.10219441839011319, "compression_ratio": 1.5318181818181817, "no_speech_prob": 8.127811452141032e-05}, {"id": 5, "seek": 2600, "start": 47.0, "end": 52.0, "text": " I've been working on it full time for the last year.", "tokens": [286, 600, 668, 1364, 322, 309, 1577, 565, 337, 264, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.10219441839011319, "compression_ratio": 1.5318181818181817, "no_speech_prob": 8.127811452141032e-05}, {"id": 6, "seek": 5200, "start": 52.0, "end": 64.0, "text": " So, what I'm going to talk about today, I'm going to give you a bit of an introduction to Pydantic, some hype numbers for some vanity, but also for some context of why making Pydantic better is worthwhile.", "tokens": [407, 11, 437, 286, 478, 516, 281, 751, 466, 965, 11, 286, 478, 516, 281, 976, 291, 257, 857, 295, 364, 9339, 281, 430, 6655, 7128, 11, 512, 24144, 3547, 337, 512, 44622, 11, 457, 611, 337, 512, 4319, 295, 983, 1455, 430, 6655, 7128, 1101, 307, 28159, 13], "temperature": 0.0, "avg_logprob": -0.0803883325485956, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.4469918571412563e-05}, {"id": 7, "seek": 5200, "start": 64.0, "end": 69.0, "text": " I'm going to explain why I decided to rebuild Pydantic completely.", "tokens": [286, 478, 516, 281, 2903, 983, 286, 3047, 281, 16877, 430, 6655, 7128, 2584, 13], "temperature": 0.0, "avg_logprob": -0.0803883325485956, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.4469918571412563e-05}, {"id": 8, "seek": 5200, "start": 69.0, "end": 77.0, "text": " I'm going to talk a bit about how I've done that with REST, and I guess most importantly why doing it in REST is the right choice.", "tokens": [286, 478, 516, 281, 751, 257, 857, 466, 577, 286, 600, 1096, 300, 365, 497, 14497, 11, 293, 286, 2041, 881, 8906, 983, 884, 309, 294, 497, 14497, 307, 264, 558, 3922, 13], "temperature": 0.0, "avg_logprob": -0.0803883325485956, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.4469918571412563e-05}, {"id": 9, "seek": 7700, "start": 77.0, "end": 82.0, "text": " I'm kind of preaching to the converted, but hey, what I'm not going to do is a like, hello world.", "tokens": [286, 478, 733, 295, 25381, 281, 264, 16424, 11, 457, 4177, 11, 437, 286, 478, 406, 516, 281, 360, 307, 257, 411, 11, 7751, 1002, 13], "temperature": 0.0, "avg_logprob": -0.12776580778490595, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.842133295023814e-05}, {"id": 10, "seek": 7700, "start": 82.0, "end": 88.0, "text": " This is how you would build a Python extension in REST. There were lots of other talks on that. They're great.", "tokens": [639, 307, 577, 291, 576, 1322, 257, 15329, 10320, 294, 497, 14497, 13, 821, 645, 3195, 295, 661, 6686, 322, 300, 13, 814, 434, 869, 13], "temperature": 0.0, "avg_logprob": -0.12776580778490595, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.842133295023814e-05}, {"id": 11, "seek": 7700, "start": 88.0, "end": 101.0, "text": " And also the PyO3 documentation is amazing, so I think it's more interesting to go into a bit of depth on the challenges, the advantages than just to do the hello world example again.", "tokens": [400, 611, 264, 9953, 46, 18, 14333, 307, 2243, 11, 370, 286, 519, 309, 311, 544, 1880, 281, 352, 666, 257, 857, 295, 7161, 322, 264, 4759, 11, 264, 14906, 813, 445, 281, 360, 264, 7751, 1002, 1365, 797, 13], "temperature": 0.0, "avg_logprob": -0.12776580778490595, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.842133295023814e-05}, {"id": 12, "seek": 7700, "start": 101.0, "end": 106.0, "text": " What is it? Well, Pydantic is a data validation library in Python.", "tokens": [708, 307, 309, 30, 1042, 11, 430, 6655, 7128, 307, 257, 1412, 24071, 6405, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12776580778490595, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.842133295023814e-05}, {"id": 13, "seek": 10600, "start": 106.0, "end": 112.0, "text": " It's not the first. It's definitely not the last. It started off as a side project like so many open source projects.", "tokens": [467, 311, 406, 264, 700, 13, 467, 311, 2138, 406, 264, 1036, 13, 467, 1409, 766, 382, 257, 1252, 1716, 411, 370, 867, 1269, 4009, 4455, 13], "temperature": 0.0, "avg_logprob": -0.11146187378188312, "compression_ratio": 1.7080536912751678, "no_speech_prob": 0.00014394382014870644}, {"id": 14, "seek": 10600, "start": 112.0, "end": 119.0, "text": " Nothing special. I maintained it my spare time. People came along occasionally, said nice things, reported bugs.", "tokens": [6693, 2121, 13, 286, 17578, 309, 452, 13798, 565, 13, 3432, 1361, 2051, 16895, 11, 848, 1481, 721, 11, 7055, 15120, 13], "temperature": 0.0, "avg_logprob": -0.11146187378188312, "compression_ratio": 1.7080536912751678, "no_speech_prob": 0.00014394382014870644}, {"id": 15, "seek": 10600, "start": 119.0, "end": 125.0, "text": " Occasionally said not very nice things, and then something weird happened, and its usage went crazy.", "tokens": [26191, 6822, 379, 848, 406, 588, 1481, 721, 11, 293, 550, 746, 3657, 2011, 11, 293, 1080, 14924, 1437, 3219, 13], "temperature": 0.0, "avg_logprob": -0.11146187378188312, "compression_ratio": 1.7080536912751678, "no_speech_prob": 0.00014394382014870644}, {"id": 16, "seek": 10600, "start": 125.0, "end": 135.0, "text": " So the first thing that happened, which you can't really see on this graph in 2018, my friend Sebastian Ramirez started the fast API project, which is a web framework in Python,", "tokens": [407, 264, 700, 551, 300, 2011, 11, 597, 291, 393, 380, 534, 536, 322, 341, 4295, 294, 6096, 11, 452, 1277, 31102, 9078, 50231, 1409, 264, 2370, 9362, 1716, 11, 597, 307, 257, 3670, 8388, 294, 15329, 11], "temperature": 0.0, "avg_logprob": -0.11146187378188312, "compression_ratio": 1.7080536912751678, "no_speech_prob": 0.00014394382014870644}, {"id": 17, "seek": 13500, "start": 135.0, "end": 142.0, "text": " which uses Pydantic and has now got, I don't know, how many thousand stars, 60,000 stars or something.", "tokens": [597, 4960, 430, 6655, 7128, 293, 575, 586, 658, 11, 286, 500, 380, 458, 11, 577, 867, 4714, 6105, 11, 4060, 11, 1360, 6105, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.09739059406322437, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0002654475683812052}, {"id": 18, "seek": 13500, "start": 142.0, "end": 151.0, "text": " It's got a lot of attention. You can see fast API growth there. That got a lot of people, I think, to first find out about Pydantic,", "tokens": [467, 311, 658, 257, 688, 295, 3202, 13, 509, 393, 536, 2370, 9362, 4599, 456, 13, 663, 658, 257, 688, 295, 561, 11, 286, 519, 11, 281, 700, 915, 484, 466, 430, 6655, 7128, 11], "temperature": 0.0, "avg_logprob": -0.09739059406322437, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0002654475683812052}, {"id": 19, "seek": 13500, "start": 151.0, "end": 158.0, "text": " but something else happened at the beginning of 2021 to cause Pydantic's download numbers to go crazy.", "tokens": [457, 746, 1646, 2011, 412, 264, 2863, 295, 7201, 281, 3082, 430, 6655, 7128, 311, 5484, 3547, 281, 352, 3219, 13], "temperature": 0.0, "avg_logprob": -0.09739059406322437, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.0002654475683812052}, {"id": 20, "seek": 15800, "start": 158.0, "end": 168.0, "text": " Now, I'm well aware that Aaron Armin's speech at talk earlier kind of pre-trolled me before I'd even made my talk, saying that download numbers are a terrible metric,", "tokens": [823, 11, 286, 478, 731, 3650, 300, 14018, 1587, 2367, 311, 6218, 412, 751, 3071, 733, 295, 659, 12, 83, 28850, 385, 949, 286, 1116, 754, 1027, 452, 751, 11, 1566, 300, 5484, 3547, 366, 257, 6237, 20678, 11], "temperature": 0.0, "avg_logprob": -0.10705231012922994, "compression_ratio": 1.6181818181818182, "no_speech_prob": 4.07675324822776e-05}, {"id": 21, "seek": 15800, "start": 168.0, "end": 180.0, "text": " but they are the only metric, so that's what we have to use. It's also worth saying that I have actually looked at Pydantic's downloads in terms of as a dependency and as a direct download.", "tokens": [457, 436, 366, 264, 787, 20678, 11, 370, 300, 311, 437, 321, 362, 281, 764, 13, 467, 311, 611, 3163, 1566, 300, 286, 362, 767, 2956, 412, 430, 6655, 7128, 311, 36553, 294, 2115, 295, 382, 257, 33621, 293, 382, 257, 2047, 5484, 13], "temperature": 0.0, "avg_logprob": -0.10705231012922994, "compression_ratio": 1.6181818181818182, "no_speech_prob": 4.07675324822776e-05}, {"id": 22, "seek": 18000, "start": 180.0, "end": 188.0, "text": " It's not that easy to do with PyPI, but it looks like about 15 million downloads a month are from, as a dependency of another package,", "tokens": [467, 311, 406, 300, 1858, 281, 360, 365, 9953, 31701, 11, 457, 309, 1542, 411, 466, 2119, 2459, 36553, 257, 1618, 366, 490, 11, 382, 257, 33621, 295, 1071, 7372, 11], "temperature": 0.0, "avg_logprob": -0.09996623618929994, "compression_ratio": 1.6053639846743295, "no_speech_prob": 4.607409573509358e-05}, {"id": 23, "seek": 18000, "start": 188.0, "end": 197.0, "text": " and the remaining 25 or so million are people installing Pydantic directly, so it seems like people are using it not just as a dependency of another library.", "tokens": [293, 264, 8877, 3552, 420, 370, 2459, 366, 561, 20762, 430, 6655, 7128, 3838, 11, 370, 309, 2544, 411, 561, 366, 1228, 309, 406, 445, 382, 257, 33621, 295, 1071, 6405, 13], "temperature": 0.0, "avg_logprob": -0.09996623618929994, "compression_ratio": 1.6053639846743295, "no_speech_prob": 4.607409573509358e-05}, {"id": 24, "seek": 18000, "start": 197.0, "end": 205.0, "text": " I've included Django on there as the middle line, because it's the most high-profile, most well-known web framework in Python.", "tokens": [286, 600, 5556, 33464, 17150, 322, 456, 382, 264, 2808, 1622, 11, 570, 309, 311, 264, 881, 1090, 12, 29175, 794, 11, 881, 731, 12, 6861, 3670, 8388, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.09996623618929994, "compression_ratio": 1.6053639846743295, "no_speech_prob": 4.607409573509358e-05}, {"id": 25, "seek": 20500, "start": 205.0, "end": 216.0, "text": " Not to be critical of it, it's amazing, it's changed my life. I mean, no disrespect by saying we've overtaken it, but just that Pydantic's usage has gone mad.", "tokens": [1726, 281, 312, 4924, 295, 309, 11, 309, 311, 2243, 11, 309, 311, 3105, 452, 993, 13, 286, 914, 11, 572, 27058, 538, 1566, 321, 600, 17038, 9846, 309, 11, 457, 445, 300, 430, 6655, 7128, 311, 14924, 575, 2780, 5244, 13], "temperature": 0.0, "avg_logprob": -0.1296446503711348, "compression_ratio": 1.582142857142857, "no_speech_prob": 0.00010709270281950012}, {"id": 26, "seek": 20500, "start": 216.0, "end": 226.0, "text": " In terms of how it's used, it's used by lots of organizations you would expect, all the fang companies, something like 19 out of the top 25 people companies in NASDAQ,", "tokens": [682, 2115, 295, 577, 309, 311, 1143, 11, 309, 311, 1143, 538, 3195, 295, 6150, 291, 576, 2066, 11, 439, 264, 283, 656, 3431, 11, 746, 411, 1294, 484, 295, 264, 1192, 3552, 561, 3431, 294, 10182, 7509, 48, 11], "temperature": 0.0, "avg_logprob": -0.1296446503711348, "compression_ratio": 1.582142857142857, "no_speech_prob": 0.00010709270281950012}, {"id": 27, "seek": 20500, "start": 226.0, "end": 234.0, "text": " but also by organizations which you wouldn't expect, like JPMorgan, use it quite a lot, I don't know in what regard.", "tokens": [457, 611, 538, 6150, 597, 291, 2759, 380, 2066, 11, 411, 508, 18819, 12372, 11, 764, 309, 1596, 257, 688, 11, 286, 500, 380, 458, 294, 437, 3843, 13], "temperature": 0.0, "avg_logprob": -0.1296446503711348, "compression_ratio": 1.582142857142857, "no_speech_prob": 0.00010709270281950012}, {"id": 28, "seek": 23400, "start": 234.0, "end": 239.0, "text": " But it's quite interesting, if you have an open source project, if you look in analytics at the referrers,", "tokens": [583, 309, 311, 1596, 1880, 11, 498, 291, 362, 364, 1269, 4009, 1716, 11, 498, 291, 574, 294, 15370, 412, 264, 2864, 81, 433, 11], "temperature": 0.0, "avg_logprob": -0.09180543755972258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 8.325707312906161e-05}, {"id": 29, "seek": 23400, "start": 239.0, "end": 246.0, "text": " lots of those big, very security-centric companies forget to turn off the referrer header from their internal systems,", "tokens": [3195, 295, 729, 955, 11, 588, 3825, 12, 45300, 3431, 2870, 281, 1261, 766, 264, 2864, 9797, 23117, 490, 641, 6920, 3652, 11], "temperature": 0.0, "avg_logprob": -0.09180543755972258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 8.325707312906161e-05}, {"id": 30, "seek": 23400, "start": 246.0, "end": 256.0, "text": " and they name their internal systems things like github.jbmorgan.net, so you can see which companies are using your dependencies by looking at those referrers.", "tokens": [293, 436, 1315, 641, 6920, 3652, 721, 411, 290, 355, 836, 13, 73, 65, 76, 12372, 13, 7129, 11, 370, 291, 393, 536, 597, 3431, 366, 1228, 428, 36606, 538, 1237, 412, 729, 2864, 81, 433, 13], "temperature": 0.0, "avg_logprob": -0.09180543755972258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 8.325707312906161e-05}, {"id": 31, "seek": 25600, "start": 256.0, "end": 265.0, "text": " So, for example, Apple have no public demonstration of using Pydantic at all, but six different enterprise instances of github within Apple use Pydantic,", "tokens": [407, 11, 337, 1365, 11, 6373, 362, 572, 1908, 16520, 295, 1228, 430, 6655, 7128, 412, 439, 11, 457, 2309, 819, 14132, 14519, 295, 290, 355, 836, 1951, 6373, 764, 430, 6655, 7128, 11], "temperature": 0.0, "avg_logprob": -0.10410271415227576, "compression_ratio": 1.4804469273743017, "no_speech_prob": 6.691851012874395e-05}, {"id": 32, "seek": 25600, "start": 265.0, "end": 276.0, "text": " and you can even see which ones they are. They're like maps.github.maps.apple.com, github.serie.apple.com, etc.", "tokens": [293, 291, 393, 754, 536, 597, 2306, 436, 366, 13, 814, 434, 411, 11317, 13, 70, 355, 836, 13, 76, 2382, 13, 21316, 13, 1112, 11, 290, 355, 836, 13, 12484, 414, 13, 21316, 13, 1112, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.10410271415227576, "compression_ratio": 1.4804469273743017, "no_speech_prob": 6.691851012874395e-05}, {"id": 33, "seek": 27600, "start": 276.0, "end": 286.0, "text": " It's also used by some cool organizations. It's used by NASA for processing imagery from James Webb, and it's used by the international panel on climate change", "tokens": [467, 311, 611, 1143, 538, 512, 1627, 6150, 13, 467, 311, 1143, 538, 12077, 337, 9007, 24340, 490, 5678, 49649, 11, 293, 309, 311, 1143, 538, 264, 5058, 4831, 322, 5659, 1319], "temperature": 0.0, "avg_logprob": -0.09371975608493971, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.00010018059401772916}, {"id": 34, "seek": 27600, "start": 286.0, "end": 295.0, "text": " for processing the data that they give to the UN on climate change every month, which is the stuff that I'm most proud of and why I want to make Pydantic better.", "tokens": [337, 9007, 264, 1412, 300, 436, 976, 281, 264, 8229, 322, 5659, 1319, 633, 1618, 11, 597, 307, 264, 1507, 300, 286, 478, 881, 4570, 295, 293, 983, 286, 528, 281, 652, 430, 6655, 7128, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09371975608493971, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.00010018059401772916}, {"id": 35, "seek": 27600, "start": 295.0, "end": 303.0, "text": " So, what's so great about Pydantic? Why are so many people using it? The short answer is I don't know, because you can't go and ask those people,", "tokens": [407, 11, 437, 311, 370, 869, 466, 430, 6655, 7128, 30, 1545, 366, 370, 867, 561, 1228, 309, 30, 440, 2099, 1867, 307, 286, 500, 380, 458, 11, 570, 291, 393, 380, 352, 293, 1029, 729, 561, 11], "temperature": 0.0, "avg_logprob": -0.09371975608493971, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.00010018059401772916}, {"id": 36, "seek": 30300, "start": 303.0, "end": 311.0, "text": " you can't look at a graph, but it can't really tell you, but we can kind of look at Pydantic and what people say, and we can kind of guess at what's made it popular.", "tokens": [291, 393, 380, 574, 412, 257, 4295, 11, 457, 309, 393, 380, 534, 980, 291, 11, 457, 321, 393, 733, 295, 574, 412, 430, 6655, 7128, 293, 437, 561, 584, 11, 293, 321, 393, 733, 295, 2041, 412, 437, 311, 1027, 309, 3743, 13], "temperature": 0.0, "avg_logprob": -0.1397935280433068, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.00027582840994000435}, {"id": 37, "seek": 30300, "start": 311.0, "end": 319.0, "text": " So, this is, I know we're in the Rust room, we've got some Python code, don't worry, we'll get to Rust later. This is some Python code that demonstrates what Pydantic does.", "tokens": [407, 11, 341, 307, 11, 286, 458, 321, 434, 294, 264, 34952, 1808, 11, 321, 600, 658, 512, 15329, 3089, 11, 500, 380, 3292, 11, 321, 603, 483, 281, 34952, 1780, 13, 639, 307, 512, 15329, 3089, 300, 31034, 437, 430, 6655, 7128, 775, 13], "temperature": 0.0, "avg_logprob": -0.1397935280433068, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.00027582840994000435}, {"id": 38, "seek": 30300, "start": 319.0, "end": 328.0, "text": " So, we have a model which kind of represents a talk, which has four fields in this case. Obviously, title is a string, attendance is an integer,", "tokens": [407, 11, 321, 362, 257, 2316, 597, 733, 295, 8855, 257, 751, 11, 597, 575, 1451, 7909, 294, 341, 1389, 13, 7580, 11, 4876, 307, 257, 6798, 11, 24337, 307, 364, 24922, 11], "temperature": 0.0, "avg_logprob": -0.1397935280433068, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.00027582840994000435}, {"id": 39, "seek": 32800, "start": 328.0, "end": 341.0, "text": " the number of people who came, when, which is a date time, or none, and has a default value of none, and then the mistakes I make, which is a list of two pools with the time they were made at and a description.", "tokens": [264, 1230, 295, 561, 567, 1361, 11, 562, 11, 597, 307, 257, 4002, 565, 11, 420, 6022, 11, 293, 575, 257, 7576, 2158, 295, 6022, 11, 293, 550, 264, 8038, 286, 652, 11, 597, 307, 257, 1329, 295, 732, 28688, 365, 264, 565, 436, 645, 1027, 412, 293, 257, 3855, 13], "temperature": 0.0, "avg_logprob": -0.14489188561072716, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0002299065381521359}, {"id": 40, "seek": 32800, "start": 341.0, "end": 352.0, "text": " So, and then lastly, last line, we instantiate an instance of talk using that data, and if there was a mistake, we'd get an error, if there wasn't a mistake, we'd obviously get the instance.", "tokens": [407, 11, 293, 550, 16386, 11, 1036, 1622, 11, 321, 9836, 13024, 364, 5197, 295, 751, 1228, 300, 1412, 11, 293, 498, 456, 390, 257, 6146, 11, 321, 1116, 483, 364, 6713, 11, 498, 456, 2067, 380, 257, 6146, 11, 321, 1116, 2745, 483, 264, 5197, 13], "temperature": 0.0, "avg_logprob": -0.14489188561072716, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0002299065381521359}, {"id": 41, "seek": 35200, "start": 352.0, "end": 359.0, "text": " The first thing that makes Pydantic special, and the reason that people like it, is because we use Python type hints to define the types.", "tokens": [440, 700, 551, 300, 1669, 430, 6655, 7128, 2121, 11, 293, 264, 1778, 300, 561, 411, 309, 11, 307, 570, 321, 764, 15329, 2010, 27271, 281, 6964, 264, 3467, 13], "temperature": 0.0, "avg_logprob": -0.08439573609685323, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00013258082617539912}, {"id": 42, "seek": 35200, "start": 359.0, "end": 369.0, "text": " That's become reasonably commonplace now, there are a whole suite of different libraries that do the same thing, either because it's obvious or because they're copying Pydantic, but Pydantic was the first to do that,", "tokens": [663, 311, 1813, 23551, 2689, 6742, 586, 11, 456, 366, 257, 1379, 14205, 295, 819, 15148, 300, 360, 264, 912, 551, 11, 2139, 570, 309, 311, 6322, 420, 570, 436, 434, 27976, 430, 6655, 7128, 11, 457, 430, 6655, 7128, 390, 264, 700, 281, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.08439573609685323, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00013258082617539912}, {"id": 43, "seek": 36900, "start": 369.0, "end": 384.0, "text": " because type hints were kind of new in 2017, and obviously, the main advantage is it's easy to learn, you don't need to learn a new kind of DSL to define stuff, but it's also compatible with static type checking with all the rest of your code, with your IDE.", "tokens": [570, 2010, 27271, 645, 733, 295, 777, 294, 6591, 11, 293, 2745, 11, 264, 2135, 5002, 307, 309, 311, 1858, 281, 1466, 11, 291, 500, 380, 643, 281, 1466, 257, 777, 733, 295, 15816, 43, 281, 6964, 1507, 11, 457, 309, 311, 611, 18218, 365, 13437, 2010, 8568, 365, 439, 264, 1472, 295, 428, 3089, 11, 365, 428, 40930, 13], "temperature": 0.0, "avg_logprob": -0.10951605150776525, "compression_ratio": 1.5560165975103735, "no_speech_prob": 6.0220325394766405e-05}, {"id": 44, "seek": 36900, "start": 384.0, "end": 392.0, "text": " Once you defined your model, and if Pydantic's worked correctly, then you know you've got a proper instance of talk.", "tokens": [3443, 291, 7642, 428, 2316, 11, 293, 498, 430, 6655, 7128, 311, 2732, 8944, 11, 550, 291, 458, 291, 600, 658, 257, 2296, 5197, 295, 751, 13], "temperature": 0.0, "avg_logprob": -0.10951605150776525, "compression_ratio": 1.5560165975103735, "no_speech_prob": 6.0220325394766405e-05}, {"id": 45, "seek": 39200, "start": 392.0, "end": 406.0, "text": " The frustration that caused me to create Pydantic was that type annotations existed, they sat there in the code, you could read them, but they did nothing at runtime, and so, effectively, could we try and make them work?", "tokens": [440, 20491, 300, 7008, 385, 281, 1884, 430, 6655, 7128, 390, 300, 2010, 25339, 763, 13135, 11, 436, 3227, 456, 294, 264, 3089, 11, 291, 727, 1401, 552, 11, 457, 436, 630, 1825, 412, 34474, 11, 293, 370, 11, 8659, 11, 727, 321, 853, 293, 652, 552, 589, 30], "temperature": 0.0, "avg_logprob": -0.07826333978901738, "compression_ratio": 1.6375, "no_speech_prob": 3.987450327258557e-05}, {"id": 46, "seek": 39200, "start": 406.0, "end": 415.0, "text": " The second and slightly more controversial thing that Pydantic does, which I think is one of the reasons that people find it easy to use, is because we default to coercion.", "tokens": [440, 1150, 293, 4748, 544, 17323, 551, 300, 430, 6655, 7128, 775, 11, 597, 286, 519, 307, 472, 295, 264, 4112, 300, 561, 915, 309, 1858, 281, 764, 11, 307, 570, 321, 7576, 281, 49741, 313, 13], "temperature": 0.0, "avg_logprob": -0.07826333978901738, "compression_ratio": 1.6375, "no_speech_prob": 3.987450327258557e-05}, {"id": 47, "seek": 41500, "start": 415.0, "end": 436.0, "text": " So, you can see a tendance there, although it needs to be an integer, it's defined as a string. Pydantic will automatically coerce from, for example, a valid string to an integer, but it'll also do other coercions that are a bit more commonplace like coercing a string as an isodate format into a datetime object, and same for the durations.", "tokens": [407, 11, 291, 393, 536, 257, 3928, 719, 456, 11, 4878, 309, 2203, 281, 312, 364, 24922, 11, 309, 311, 7642, 382, 257, 6798, 13, 430, 6655, 7128, 486, 6772, 598, 260, 384, 490, 11, 337, 1365, 11, 257, 7363, 6798, 281, 364, 24922, 11, 457, 309, 603, 611, 360, 661, 49741, 626, 300, 366, 257, 857, 544, 2689, 6742, 411, 598, 260, 2175, 257, 6798, 382, 364, 307, 378, 473, 7877, 666, 257, 1137, 9764, 2657, 11, 293, 912, 337, 264, 4861, 763, 13], "temperature": 0.0, "avg_logprob": -0.158534446459138, "compression_ratio": 1.6084905660377358, "no_speech_prob": 8.804464596323669e-05}, {"id": 48, "seek": 43600, "start": 436.0, "end": 453.0, "text": " Some people hate that, some people complain about it a lot, I suspect that lots of people who don't even realize they're using it, they process environment variables, or JSON, or URL arguments, and they're always strings, and Pydantic just works and they don't even see it.", "tokens": [2188, 561, 4700, 300, 11, 512, 561, 11024, 466, 309, 257, 688, 11, 286, 9091, 300, 3195, 295, 561, 567, 500, 380, 754, 4325, 436, 434, 1228, 309, 11, 436, 1399, 2823, 9102, 11, 420, 31828, 11, 420, 12905, 12869, 11, 293, 436, 434, 1009, 13985, 11, 293, 430, 6655, 7128, 445, 1985, 293, 436, 500, 380, 754, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.10520745153012483, "compression_ratio": 1.6762589928057554, "no_speech_prob": 9.058552677743137e-05}, {"id": 49, "seek": 43600, "start": 453.0, "end": 463.0, "text": " A few other reasons I think we're quite popular, we're fast-ish, we're friendly-ish on the bug tracker, I don't promise not to ever be cross with people, and we're reasonably feature-complete.", "tokens": [316, 1326, 661, 4112, 286, 519, 321, 434, 1596, 3743, 11, 321, 434, 2370, 12, 742, 11, 321, 434, 9208, 12, 742, 322, 264, 7426, 37516, 11, 286, 500, 380, 6228, 406, 281, 1562, 312, 3278, 365, 561, 11, 293, 321, 434, 23551, 4111, 12, 1112, 17220, 13], "temperature": 0.0, "avg_logprob": -0.10520745153012483, "compression_ratio": 1.6762589928057554, "no_speech_prob": 9.058552677743137e-05}, {"id": 50, "seek": 46300, "start": 463.0, "end": 475.0, "text": " So, that was Pydantic, it's great, lots of people are using it, what's the problem? Well, it started off as a side project for me, it wasn't designed to be successful, and the internals stink.", "tokens": [407, 11, 300, 390, 430, 6655, 7128, 11, 309, 311, 869, 11, 3195, 295, 561, 366, 1228, 309, 11, 437, 311, 264, 1154, 30, 1042, 11, 309, 1409, 766, 382, 257, 1252, 1716, 337, 385, 11, 309, 2067, 380, 4761, 281, 312, 4406, 11, 293, 264, 2154, 1124, 35843, 13], "temperature": 0.0, "avg_logprob": -0.058708306655143074, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00021006062161177397}, {"id": 51, "seek": 46300, "start": 475.0, "end": 484.0, "text": " I'm very proud of what Pydantic is doing in terms of how it's being used, I'm not proud of what's under the hood, and so I've been keen for a long time to fix the internals.", "tokens": [286, 478, 588, 4570, 295, 437, 430, 6655, 7128, 307, 884, 294, 2115, 295, 577, 309, 311, 885, 1143, 11, 286, 478, 406, 4570, 295, 437, 311, 833, 264, 13376, 11, 293, 370, 286, 600, 668, 20297, 337, 257, 938, 565, 281, 3191, 264, 2154, 1124, 13], "temperature": 0.0, "avg_logprob": -0.058708306655143074, "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.00021006062161177397}, {"id": 52, "seek": 48400, "start": 484.0, "end": 502.0, "text": " Also, second way in which I'm in kind of trouble before my talk was talking about API compatibility, we're going to have to break a lot of things in Pydantic V2 to get it right, but that's the right thing to do, I think, to get the future API to be correct and stable and not break again.", "tokens": [2743, 11, 1150, 636, 294, 597, 286, 478, 294, 733, 295, 5253, 949, 452, 751, 390, 1417, 466, 9362, 34237, 11, 321, 434, 516, 281, 362, 281, 1821, 257, 688, 295, 721, 294, 430, 6655, 7128, 691, 17, 281, 483, 309, 558, 11, 457, 300, 311, 264, 558, 551, 281, 360, 11, 286, 519, 11, 281, 483, 264, 2027, 9362, 281, 312, 3006, 293, 8351, 293, 406, 1821, 797, 13], "temperature": 0.0, "avg_logprob": -0.10694123603202202, "compression_ratio": 1.5, "no_speech_prob": 0.000159246294060722}, {"id": 53, "seek": 50200, "start": 502.0, "end": 517.0, "text": " And while we're building V2, why don't we do some other stuff, so make it even faster, it's already quite fast, but if you think about that number of downloads, you think about the number of CPU cycles globally every day devoted to doing validation with Python,", "tokens": [400, 1339, 321, 434, 2390, 691, 17, 11, 983, 500, 380, 321, 360, 512, 661, 1507, 11, 370, 652, 309, 754, 4663, 11, 309, 311, 1217, 1596, 2370, 11, 457, 498, 291, 519, 466, 300, 1230, 295, 36553, 11, 291, 519, 466, 264, 1230, 295, 13199, 17796, 18958, 633, 786, 21815, 281, 884, 24071, 365, 15329, 11], "temperature": 0.0, "avg_logprob": -0.07184353812796171, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.00017795190797187388}, {"id": 54, "seek": 51700, "start": 517.0, "end": 536.0, "text": " but that's currently with Pydantic, that's currently all in Python, that's probably quite a lot of carbon dioxide that's being released, effectively unnecessarily, because we could make Pydantic significantly faster. Strict mode, I already talked about, because while often you don't need it, there are legitimate cases where you want strict mode.", "tokens": [457, 300, 311, 4362, 365, 430, 6655, 7128, 11, 300, 311, 4362, 439, 294, 15329, 11, 300, 311, 1391, 1596, 257, 688, 295, 5954, 19590, 300, 311, 885, 4736, 11, 8659, 16799, 3289, 11, 570, 321, 727, 652, 430, 6655, 7128, 10591, 4663, 13, 745, 3740, 4391, 11, 286, 1217, 2825, 466, 11, 570, 1339, 2049, 291, 500, 380, 643, 309, 11, 456, 366, 17956, 3331, 689, 291, 528, 10910, 4391, 13], "temperature": 0.0, "avg_logprob": -0.11954589893943385, "compression_ratio": 1.6214953271028036, "no_speech_prob": 0.00010691344505175948}, {"id": 55, "seek": 53600, "start": 536.0, "end": 549.0, "text": " We have functional validators, which is effectively running some Python code to validate a field, they're useful, but they would be more useful if they could operate like an onion, so like middleware where you take both a value and a handler,", "tokens": [492, 362, 11745, 7363, 3391, 11, 597, 307, 8659, 2614, 512, 15329, 3089, 281, 29562, 257, 2519, 11, 436, 434, 4420, 11, 457, 436, 576, 312, 544, 4420, 498, 436, 727, 9651, 411, 364, 10916, 11, 370, 411, 2808, 3039, 689, 291, 747, 1293, 257, 2158, 293, 257, 41967, 11], "temperature": 0.0, "avg_logprob": -0.1080826653374566, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.00023953737400006503}, {"id": 56, "seek": 53600, "start": 549.0, "end": 557.0, "text": " and call the handler if you want to, once you've done some processing of the value, that would be super valuable, another thing we could add, composability.", "tokens": [293, 818, 264, 41967, 498, 291, 528, 281, 11, 1564, 291, 600, 1096, 512, 9007, 295, 264, 2158, 11, 300, 576, 312, 1687, 8263, 11, 1071, 551, 321, 727, 909, 11, 10199, 2310, 13], "temperature": 0.0, "avg_logprob": -0.1080826653374566, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.00023953737400006503}, {"id": 57, "seek": 55700, "start": 557.0, "end": 573.0, "text": " So Pydantic, as I showed you earlier, is based on the Pydantic model, often your root type doesn't need to be a Pydantic model or shouldn't be a Pydantic model, it might be a list, it might be a tuple, it might be a list of models, it might be a type dict, which is a common new type in Python,", "tokens": [407, 430, 6655, 7128, 11, 382, 286, 4712, 291, 3071, 11, 307, 2361, 322, 264, 430, 6655, 7128, 2316, 11, 2049, 428, 5593, 2010, 1177, 380, 643, 281, 312, 257, 430, 6655, 7128, 2316, 420, 4659, 380, 312, 257, 430, 6655, 7128, 2316, 11, 309, 1062, 312, 257, 1329, 11, 309, 1062, 312, 257, 2604, 781, 11, 309, 1062, 312, 257, 1329, 295, 5245, 11, 309, 1062, 312, 257, 2010, 12569, 11, 597, 307, 257, 2689, 777, 2010, 294, 15329, 11], "temperature": 0.0, "avg_logprob": -0.11182925280402689, "compression_ratio": 1.872611464968153, "no_speech_prob": 0.00037824836908839643}, {"id": 58, "seek": 57300, "start": 573.0, "end": 592.0, "text": " and then lastly, maintainability, since I maintain Pydantic, I want maintaining it to be fun, so about a year ago, last March, I started as a kind of experiment, could I rebuild some of it in Rust, a year later, I'm still working on it full time, and we're nearly there.", "tokens": [293, 550, 16386, 11, 6909, 2310, 11, 1670, 286, 6909, 430, 6655, 7128, 11, 286, 528, 14916, 309, 281, 312, 1019, 11, 370, 466, 257, 1064, 2057, 11, 1036, 6129, 11, 286, 1409, 382, 257, 733, 295, 5120, 11, 727, 286, 16877, 512, 295, 309, 294, 34952, 11, 257, 1064, 1780, 11, 286, 478, 920, 1364, 322, 309, 1577, 565, 11, 293, 321, 434, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.10648329157224844, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.00010496586037334055}, {"id": 59, "seek": 59200, "start": 592.0, "end": 613.0, "text": " So what does it mean to validate Python data in Rust? What's the process? Well, phase one, we need to take a Pydantic model and convert it to a Rust structure, so unlike libraries like CERD, we're not compiling models in Rust,", "tokens": [407, 437, 775, 309, 914, 281, 29562, 15329, 1412, 294, 34952, 30, 708, 311, 264, 1399, 30, 1042, 11, 5574, 472, 11, 321, 643, 281, 747, 257, 430, 6655, 7128, 2316, 293, 7620, 309, 281, 257, 34952, 3877, 11, 370, 8343, 15148, 411, 383, 1598, 35, 11, 321, 434, 406, 715, 4883, 5245, 294, 34952, 11], "temperature": 0.0, "avg_logprob": -0.117157514890035, "compression_ratio": 1.4037267080745341, "no_speech_prob": 2.5796491172513925e-05}, {"id": 60, "seek": 61300, "start": 613.0, "end": 622.0, "text": " the compiled Rust code doesn't know anything about the models it's going to receive, because obviously Python developers don't want to be compiling Rust code to get their model to work.", "tokens": [264, 36548, 34952, 3089, 1177, 380, 458, 1340, 466, 264, 5245, 309, 311, 516, 281, 4774, 11, 570, 2745, 15329, 8849, 500, 380, 528, 281, 312, 715, 4883, 34952, 3089, 281, 483, 641, 2316, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.07503545985502355, "compression_ratio": 1.53125, "no_speech_prob": 0.00015471826191060245}, {"id": 61, "seek": 61300, "start": 622.0, "end": 628.0, "text": " So we have to have a, in Rust terms, dynamic definition of our schema, which we can then use for validation.", "tokens": [407, 321, 362, 281, 362, 257, 11, 294, 34952, 2115, 11, 8546, 7123, 295, 527, 34078, 11, 597, 321, 393, 550, 764, 337, 24071, 13], "temperature": 0.0, "avg_logprob": -0.07503545985502355, "compression_ratio": 1.53125, "no_speech_prob": 0.00015471826191060245}, {"id": 62, "seek": 62800, "start": 628.0, "end": 643.0, "text": " The way we build that is effectively these validators, which are structs that contain both characteristics of what they're going to validate, but also other validators recursively such that you can define complex structures.", "tokens": [440, 636, 321, 1322, 300, 307, 8659, 613, 7363, 3391, 11, 597, 366, 6594, 82, 300, 5304, 1293, 10891, 295, 437, 436, 434, 516, 281, 29562, 11, 457, 611, 661, 7363, 3391, 20560, 3413, 1270, 300, 291, 393, 6964, 3997, 9227, 13], "temperature": 0.0, "avg_logprob": -0.0821355270302814, "compression_ratio": 1.4933333333333334, "no_speech_prob": 8.231428364524618e-05}, {"id": 63, "seek": 64300, "start": 643.0, "end": 667.0, "text": " So in this case, our outermost validator is a model validator, which effectively just instantiates an instance of torque and sets its attributes from a dictionary. It contains another validator, which is a type dict validator, which contains the definition of all the fields, which have effectively the key that they're going to look for and then a validator that they're going to run.", "tokens": [407, 294, 341, 1389, 11, 527, 484, 966, 555, 7363, 1639, 307, 257, 2316, 7363, 1639, 11, 597, 8659, 445, 9836, 72, 1024, 364, 5197, 295, 16437, 293, 6352, 1080, 17212, 490, 257, 25890, 13, 467, 8306, 1071, 7363, 1639, 11, 597, 307, 257, 2010, 12569, 7363, 1639, 11, 597, 8306, 264, 7123, 295, 439, 264, 7909, 11, 597, 362, 8659, 264, 2141, 300, 436, 434, 516, 281, 574, 337, 293, 550, 257, 7363, 1639, 300, 436, 434, 516, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.13575279989907907, "compression_ratio": 1.8599033816425121, "no_speech_prob": 3.204725726391189e-05}, {"id": 64, "seek": 66700, "start": 667.0, "end": 688.0, "text": " The first two are reasonably obvious. I've added a few constraints to show how you would manage those constraints. And then the third one, the when field is obviously a union, which in turn contains a vect of validators to run effectively in turn to try and find the value.", "tokens": [440, 700, 732, 366, 23551, 6322, 13, 286, 600, 3869, 257, 1326, 18491, 281, 855, 577, 291, 576, 3067, 729, 18491, 13, 400, 550, 264, 2636, 472, 11, 264, 562, 2519, 307, 2745, 257, 11671, 11, 597, 294, 1261, 8306, 257, 1241, 349, 295, 7363, 3391, 281, 1190, 8659, 294, 1261, 281, 853, 293, 915, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.11082607699978736, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.00019881436310242862}, {"id": 65, "seek": 68800, "start": 688.0, "end": 707.0, "text": " And then the last one, which is the kind of more complex type, contains this list validator, which contains tuple validator, which contains two more validators. And we can build up effectively infinitely complex schemas from a relatively simple, I say relatively simple principle at the outset, which is we have a validator.", "tokens": [400, 550, 264, 1036, 472, 11, 597, 307, 264, 733, 295, 544, 3997, 2010, 11, 8306, 341, 1329, 7363, 1639, 11, 597, 8306, 2604, 781, 7363, 1639, 11, 597, 8306, 732, 544, 7363, 3391, 13, 400, 321, 393, 1322, 493, 8659, 36227, 3997, 22627, 296, 490, 257, 7226, 2199, 11, 286, 584, 7226, 2199, 8665, 412, 264, 44618, 11, 597, 307, 321, 362, 257, 7363, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1138855441586002, "compression_ratio": 1.798206278026906, "no_speech_prob": 0.00011308056127745658}, {"id": 66, "seek": 68800, "start": 707.0, "end": 714.0, "text": " It's going to contain some other stuff. So what does that look like in code?", "tokens": [467, 311, 516, 281, 5304, 512, 661, 1507, 13, 407, 437, 775, 300, 574, 411, 294, 3089, 30], "temperature": 0.0, "avg_logprob": -0.1138855441586002, "compression_ratio": 1.798206278026906, "no_speech_prob": 0.00011308056127745658}, {"id": 67, "seek": 71400, "start": 714.0, "end": 723.0, "text": " I said I was going to show you some Rust code. I'm going to show you some Rust code because I think this is the most clear way of explaining what it is that we do.", "tokens": [286, 848, 286, 390, 516, 281, 855, 291, 512, 34952, 3089, 13, 286, 478, 516, 281, 855, 291, 512, 34952, 3089, 570, 286, 519, 341, 307, 264, 881, 1850, 636, 295, 13468, 437, 309, 307, 300, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.07968144581235688, "compression_ratio": 1.8609022556390977, "no_speech_prob": 6.077448415453546e-05}, {"id": 68, "seek": 71400, "start": 723.0, "end": 743.0, "text": " So the root of everything is this trait validator, which contains effectively three things. It contains a const, a static string, which is used for defining, as I'll show you later, which validator we're going to use for a given bit of data build, which is a simple function to construct an instance of itself in the generic sense.", "tokens": [407, 264, 5593, 295, 1203, 307, 341, 22538, 7363, 1639, 11, 597, 8306, 8659, 1045, 721, 13, 467, 8306, 257, 1817, 11, 257, 13437, 6798, 11, 597, 307, 1143, 337, 17827, 11, 382, 286, 603, 855, 291, 1780, 11, 597, 7363, 1639, 321, 434, 516, 281, 764, 337, 257, 2212, 857, 295, 1412, 1322, 11, 597, 307, 257, 2199, 2445, 281, 7690, 364, 5197, 295, 2564, 294, 264, 19577, 2020, 13], "temperature": 0.0, "avg_logprob": -0.07968144581235688, "compression_ratio": 1.8609022556390977, "no_speech_prob": 6.077448415453546e-05}, {"id": 69, "seek": 74300, "start": 743.0, "end": 748.0, "text": " And then the validate function that goes off and does the validation.", "tokens": [400, 550, 264, 29562, 2445, 300, 1709, 766, 293, 775, 264, 24071, 13], "temperature": 0.0, "avg_logprob": -0.08631126540047782, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.765117723261937e-05}, {"id": 70, "seek": 74300, "start": 748.0, "end": 762.0, "text": " We then take all of those, well, we then implement that trait for all of the common types that we want. So I think we have 58, 48 or so different validators, and then we bang all of them into one massive enum.", "tokens": [492, 550, 747, 439, 295, 729, 11, 731, 11, 321, 550, 4445, 300, 22538, 337, 439, 295, 264, 2689, 3467, 300, 321, 528, 13, 407, 286, 519, 321, 362, 21786, 11, 11174, 420, 370, 819, 7363, 3391, 11, 293, 550, 321, 8550, 439, 295, 552, 666, 472, 5994, 465, 449, 13], "temperature": 0.0, "avg_logprob": -0.08631126540047782, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.765117723261937e-05}, {"id": 71, "seek": 76200, "start": 762.0, "end": 775.0, "text": " Then the magic bit, which is provided by enum dispatch, which is a Rust crate that effectively implements a trait on an enum if every member of that enum implements that trait.", "tokens": [1396, 264, 5585, 857, 11, 597, 307, 5649, 538, 465, 449, 36729, 11, 597, 307, 257, 34952, 42426, 300, 8659, 704, 17988, 257, 22538, 322, 364, 465, 449, 498, 633, 4006, 295, 300, 465, 449, 704, 17988, 300, 22538, 13], "temperature": 0.0, "avg_logprob": -0.09424990194815176, "compression_ratio": 1.735, "no_speech_prob": 6.339674291666597e-05}, {"id": 72, "seek": 76200, "start": 775.0, "end": 786.0, "text": " Effectively, it goes and does a big procedural macro to create an instance, an implementation of the function, which is just a big match, choosing which function to call.", "tokens": [17764, 3413, 11, 309, 1709, 293, 775, 257, 955, 43951, 18887, 281, 1884, 364, 5197, 11, 364, 11420, 295, 264, 2445, 11, 597, 307, 445, 257, 955, 2995, 11, 10875, 597, 2445, 281, 818, 13], "temperature": 0.0, "avg_logprob": -0.09424990194815176, "compression_ratio": 1.735, "no_speech_prob": 6.339674291666597e-05}, {"id": 73, "seek": 78600, "start": 786.0, "end": 792.0, "text": " But it's significantly faster than dine.", "tokens": [583, 309, 311, 10591, 4663, 813, 274, 533, 13], "temperature": 0.0, "avg_logprob": -0.09751456243950024, "compression_ratio": 1.4268292682926829, "no_speech_prob": 5.7983150327345356e-05}, {"id": 74, "seek": 78600, "start": 792.0, "end": 802.0, "text": " And in fact, in some cases, it can abstract away everything and be as fast as just calling the implementation directly.", "tokens": [400, 294, 1186, 11, 294, 512, 3331, 11, 309, 393, 12649, 1314, 1203, 293, 312, 382, 2370, 382, 445, 5141, 264, 11420, 3838, 13], "temperature": 0.0, "avg_logprob": -0.09751456243950024, "compression_ratio": 1.4268292682926829, "no_speech_prob": 5.7983150327345356e-05}, {"id": 75, "seek": 78600, "start": 802.0, "end": 806.0, "text": " So I said earlier that we needed to use this constant, the expected type.", "tokens": [407, 286, 848, 3071, 300, 321, 2978, 281, 764, 341, 5754, 11, 264, 5176, 2010, 13], "temperature": 0.0, "avg_logprob": -0.09751456243950024, "compression_ratio": 1.4268292682926829, "no_speech_prob": 5.7983150327345356e-05}, {"id": 76, "seek": 80600, "start": 806.0, "end": 816.0, "text": " We use that in another effectively big enum to go through and we take the type attribute out of this schema, which is a Python dictionary.", "tokens": [492, 764, 300, 294, 1071, 8659, 955, 465, 449, 281, 352, 807, 293, 321, 747, 264, 2010, 19667, 484, 295, 341, 34078, 11, 597, 307, 257, 15329, 25890, 13], "temperature": 0.0, "avg_logprob": -0.07519454449678944, "compression_ratio": 1.641025641025641, "no_speech_prob": 7.597579678986222e-05}, {"id": 77, "seek": 80600, "start": 816.0, "end": 820.0, "text": " And we use that to effectively look up which validator we're going to go and build.", "tokens": [400, 321, 764, 300, 281, 8659, 574, 493, 597, 7363, 1639, 321, 434, 516, 281, 352, 293, 1322, 13], "temperature": 0.0, "avg_logprob": -0.07519454449678944, "compression_ratio": 1.641025641025641, "no_speech_prob": 7.597579678986222e-05}, {"id": 78, "seek": 80600, "start": 820.0, "end": 823.0, "text": " And again, I've shown a few here, but there's obviously a bunch more.", "tokens": [400, 797, 11, 286, 600, 4898, 257, 1326, 510, 11, 457, 456, 311, 2745, 257, 3840, 544, 13], "temperature": 0.0, "avg_logprob": -0.07519454449678944, "compression_ratio": 1.641025641025641, "no_speech_prob": 7.597579678986222e-05}, {"id": 79, "seek": 80600, "start": 823.0, "end": 828.0, "text": " This in real life is not implemented as a big match statement like this.", "tokens": [639, 294, 957, 993, 307, 406, 12270, 382, 257, 955, 2995, 5629, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.07519454449678944, "compression_ratio": 1.641025641025641, "no_speech_prob": 7.597579678986222e-05}, {"id": 80, "seek": 80600, "start": 828.0, "end": 833.0, "text": " It's a macro that builds this function, but it's clearer here if you get the idea.", "tokens": [467, 311, 257, 18887, 300, 15182, 341, 2445, 11, 457, 309, 311, 26131, 510, 498, 291, 483, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.07519454449678944, "compression_ratio": 1.641025641025641, "no_speech_prob": 7.597579678986222e-05}, {"id": 81, "seek": 83300, "start": 833.0, "end": 838.0, "text": " So I showed you earlier this validate function and I kind of skipped over the input argument.", "tokens": [407, 286, 4712, 291, 3071, 341, 7363, 473, 2445, 293, 286, 733, 295, 30193, 670, 264, 4846, 6770, 13], "temperature": 0.0, "avg_logprob": -0.097107068912403, "compression_ratio": 1.6446700507614214, "no_speech_prob": 6.550887337652966e-05}, {"id": 82, "seek": 83300, "start": 838.0, "end": 842.0, "text": " So the input argument is just an implementation of a trait.", "tokens": [407, 264, 4846, 6770, 307, 445, 364, 11420, 295, 257, 22538, 13], "temperature": 0.0, "avg_logprob": -0.097107068912403, "compression_ratio": 1.6446700507614214, "no_speech_prob": 6.550887337652966e-05}, {"id": 83, "seek": 83300, "start": 842.0, "end": 848.0, "text": " That trait input is like the beginnings of which are defined here.", "tokens": [663, 22538, 4846, 307, 411, 264, 37281, 295, 597, 366, 7642, 510, 13], "temperature": 0.0, "avg_logprob": -0.097107068912403, "compression_ratio": 1.6446700507614214, "no_speech_prob": 6.550887337652966e-05}, {"id": 84, "seek": 83300, "start": 848.0, "end": 853.0, "text": " And it effectively gives you all the things that the validation functions are going to need on a value.", "tokens": [400, 309, 8659, 2709, 291, 439, 264, 721, 300, 264, 24071, 6828, 366, 516, 281, 643, 322, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.097107068912403, "compression_ratio": 1.6446700507614214, "no_speech_prob": 6.550887337652966e-05}, {"id": 85, "seek": 85300, "start": 853.0, "end": 866.0, "text": " So is none strict string, lack string, int, float, et cetera, et cetera, but also more complex types like date, date time, dictionary, et cetera, et cetera.", "tokens": [407, 307, 6022, 10910, 6798, 11, 5011, 6798, 11, 560, 11, 15706, 11, 1030, 11458, 11, 1030, 11458, 11, 457, 611, 544, 3997, 3467, 411, 4002, 11, 4002, 565, 11, 25890, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.10771466385234486, "compression_ratio": 1.6232558139534883, "no_speech_prob": 9.48637261899421e-06}, {"id": 86, "seek": 85300, "start": 866.0, "end": 873.0, "text": " And then we implement that trait on both a Python value and on a JSON value,", "tokens": [400, 550, 321, 4445, 300, 22538, 322, 1293, 257, 15329, 2158, 293, 322, 257, 31828, 2158, 11], "temperature": 0.0, "avg_logprob": -0.10771466385234486, "compression_ratio": 1.6232558139534883, "no_speech_prob": 9.48637261899421e-06}, {"id": 87, "seek": 85300, "start": 873.0, "end": 879.0, "text": " which means that we can parse Rust directly without having to go via Python.", "tokens": [597, 1355, 300, 321, 393, 48377, 34952, 3838, 1553, 1419, 281, 352, 5766, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10771466385234486, "compression_ratio": 1.6232558139534883, "no_speech_prob": 9.48637261899421e-06}, {"id": 88, "seek": 85300, "start": 879.0, "end": 881.0, "text": " That's super valuable for two reasons.", "tokens": [663, 311, 1687, 8263, 337, 732, 4112, 13], "temperature": 0.0, "avg_logprob": -0.10771466385234486, "compression_ratio": 1.6232558139534883, "no_speech_prob": 9.48637261899421e-06}, {"id": 89, "seek": 88100, "start": 881.0, "end": 883.0, "text": " One for performance reasons.", "tokens": [1485, 337, 3389, 4112, 13], "temperature": 0.0, "avg_logprob": -0.07127868561517625, "compression_ratio": 1.7520661157024793, "no_speech_prob": 7.342106255237013e-05}, {"id": 90, "seek": 88100, "start": 883.0, "end": 892.0, "text": " So if our input is a string and if we were to then parse it into Python objects and then take that into our validator and run all of that,", "tokens": [407, 498, 527, 4846, 307, 257, 6798, 293, 498, 321, 645, 281, 550, 48377, 309, 666, 15329, 6565, 293, 550, 747, 300, 666, 527, 7363, 1639, 293, 1190, 439, 295, 300, 11], "temperature": 0.0, "avg_logprob": -0.07127868561517625, "compression_ratio": 1.7520661157024793, "no_speech_prob": 7.342106255237013e-05}, {"id": 91, "seek": 88100, "start": 892.0, "end": 898.0, "text": " that would be much lower than parsing in Rust and then running the validator in Rust straight away.", "tokens": [300, 576, 312, 709, 3126, 813, 21156, 278, 294, 34952, 293, 550, 2614, 264, 7363, 1639, 294, 34952, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.07127868561517625, "compression_ratio": 1.7520661157024793, "no_speech_prob": 7.342106255237013e-05}, {"id": 92, "seek": 88100, "start": 898.0, "end": 902.0, "text": " The other big advantage is to do with strict mode.", "tokens": [440, 661, 955, 5002, 307, 281, 360, 365, 10910, 4391, 13], "temperature": 0.0, "avg_logprob": -0.07127868561517625, "compression_ratio": 1.7520661157024793, "no_speech_prob": 7.342106255237013e-05}, {"id": 93, "seek": 88100, "start": 902.0, "end": 907.0, "text": " So I said earlier that people want strict mode, but they say they want strict mode, but often they don't.", "tokens": [407, 286, 848, 3071, 300, 561, 528, 10910, 4391, 11, 457, 436, 584, 436, 528, 10910, 4391, 11, 457, 2049, 436, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.07127868561517625, "compression_ratio": 1.7520661157024793, "no_speech_prob": 7.342106255237013e-05}, {"id": 94, "seek": 90700, "start": 907.0, "end": 911.0, "text": " So what people will say is, I want totally strict Python, why isn't it strict?", "tokens": [407, 437, 561, 486, 584, 307, 11, 286, 528, 3879, 10910, 15329, 11, 983, 1943, 380, 309, 10910, 30], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 95, "seek": 90700, "start": 911.0, "end": 913.0, "text": " And then you'll say, well, do you want to load data from JSON?", "tokens": [400, 550, 291, 603, 584, 11, 731, 11, 360, 291, 528, 281, 3677, 1412, 490, 31828, 30], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 96, "seek": 90700, "start": 913.0, "end": 915.0, "text": " And they say, yeah, of course I do.", "tokens": [400, 436, 584, 11, 1338, 11, 295, 1164, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 97, "seek": 90700, "start": 915.0, "end": 918.0, "text": " And you say, well, how are you going to define a date?", "tokens": [400, 291, 584, 11, 731, 11, 577, 366, 291, 516, 281, 6964, 257, 4002, 30], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 98, "seek": 90700, "start": 918.0, "end": 921.0, "text": " And they're like, oh, well, obviously I'll use a standard date format.", "tokens": [400, 436, 434, 411, 11, 1954, 11, 731, 11, 2745, 286, 603, 764, 257, 3832, 4002, 7877, 13], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 99, "seek": 90700, "start": 921.0, "end": 923.0, "text": " But that's not strict then.", "tokens": [583, 300, 311, 406, 10910, 550, 13], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 100, "seek": 90700, "start": 923.0, "end": 925.0, "text": " You're parsing a string.", "tokens": [509, 434, 21156, 278, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 101, "seek": 90700, "start": 925.0, "end": 928.0, "text": " And they're like, oh, that's fine because it should know in that case it's coming from JSON.", "tokens": [400, 436, 434, 411, 11, 1954, 11, 300, 311, 2489, 570, 309, 820, 458, 294, 300, 1389, 309, 311, 1348, 490, 31828, 13], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 102, "seek": 90700, "start": 928.0, "end": 930.0, "text": " Well, obviously, how are we going to do that?", "tokens": [1042, 11, 2745, 11, 577, 366, 321, 516, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.12860090780578204, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0001254740491276607}, {"id": 103, "seek": 93000, "start": 930.0, "end": 938.0, "text": " By parsing JSON directly and in future potentially other types, we can implement our strict date method,", "tokens": [3146, 21156, 278, 31828, 3838, 293, 294, 2027, 7263, 661, 3467, 11, 321, 393, 4445, 527, 10910, 4002, 3170, 11], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 104, "seek": 93000, "start": 938.0, "end": 941.0, "text": " both on that JSON input.", "tokens": [1293, 322, 300, 31828, 4846, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 105, "seek": 93000, "start": 941.0, "end": 944.0, "text": " We can say, well, we're in JSON.", "tokens": [492, 393, 584, 11, 731, 11, 321, 434, 294, 31828, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 106, "seek": 93000, "start": 944.0, "end": 945.0, "text": " We don't have a date type.", "tokens": [492, 500, 380, 362, 257, 4002, 2010, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 107, "seek": 93000, "start": 945.0, "end": 946.0, "text": " So we're going to have to do something.", "tokens": [407, 321, 434, 516, 281, 362, 281, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 108, "seek": 93000, "start": 946.0, "end": 948.0, "text": " So we're going to parse a string.", "tokens": [407, 321, 434, 516, 281, 48377, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 109, "seek": 93000, "start": 948.0, "end": 953.0, "text": " And effectively, the strict date implementation for JSON will parse a string.", "tokens": [400, 8659, 11, 264, 10910, 4002, 11420, 337, 31828, 486, 48377, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 110, "seek": 93000, "start": 953.0, "end": 956.0, "text": " And therefore, we can have a strict mode that's actually useful,", "tokens": [400, 4412, 11, 321, 393, 362, 257, 10910, 4391, 300, 311, 767, 4420, 11], "temperature": 0.0, "avg_logprob": -0.07344478260387075, "compression_ratio": 1.7885462555066078, "no_speech_prob": 0.0001181509651360102}, {"id": 111, "seek": 95600, "start": 956.0, "end": 960.0, "text": " which we wouldn't have had if we couldn't have had in pydantic v1,", "tokens": [597, 321, 2759, 380, 362, 632, 498, 321, 2809, 380, 362, 632, 294, 10664, 67, 7128, 371, 16, 11], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 112, "seek": 95600, "start": 960.0, "end": 964.0, "text": " where the validation logic doesn't know anything about where the date is coming from.", "tokens": [689, 264, 24071, 9952, 1177, 380, 458, 1340, 466, 689, 264, 4002, 307, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 113, "seek": 95600, "start": 964.0, "end": 966.0, "text": " Even if we have a parse JSON function,", "tokens": [2754, 498, 321, 362, 257, 48377, 31828, 2445, 11], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 114, "seek": 95600, "start": 966.0, "end": 972.0, "text": " all it's doing is parsing JSON to Python and then doing validation.", "tokens": [439, 309, 311, 884, 307, 21156, 278, 31828, 281, 15329, 293, 550, 884, 24071, 13], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 115, "seek": 95600, "start": 972.0, "end": 974.0, "text": " So then that's all very well.", "tokens": [407, 550, 300, 311, 439, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 116, "seek": 95600, "start": 974.0, "end": 976.0, "text": " That defines effectively how we do our validation.", "tokens": [663, 23122, 8659, 577, 321, 360, 527, 24071, 13], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 117, "seek": 95600, "start": 976.0, "end": 977.0, "text": " What's the interface to Python?", "tokens": [708, 311, 264, 9226, 281, 15329, 30], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 118, "seek": 95600, "start": 977.0, "end": 982.0, "text": " So that's where we have this schema validator rust struct,", "tokens": [407, 300, 311, 689, 321, 362, 341, 34078, 7363, 1639, 15259, 6594, 11], "temperature": 0.0, "avg_logprob": -0.1255850959242436, "compression_ratio": 1.7171314741035857, "no_speech_prob": 3.817814285866916e-05}, {"id": 119, "seek": 98200, "start": 982.0, "end": 989.0, "text": " which using the PyClass decorator is also available as a Python class.", "tokens": [597, 1228, 264, 9953, 44621, 7919, 1639, 307, 611, 2435, 382, 257, 15329, 1508, 13], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 120, "seek": 98200, "start": 989.0, "end": 992.0, "text": " And all it really contains is a validator,", "tokens": [400, 439, 309, 534, 8306, 307, 257, 7363, 1639, 11], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 121, "seek": 98200, "start": 992.0, "end": 997.0, "text": " which of course can in turn contain other validators, as I said earlier.", "tokens": [597, 295, 1164, 393, 294, 1261, 5304, 661, 7363, 3391, 11, 382, 286, 848, 3071, 13], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 122, "seek": 98200, "start": 997.0, "end": 1003.0, "text": " And its implementation, which are all then exposed as Python methods, are new,", "tokens": [400, 1080, 11420, 11, 597, 366, 439, 550, 9495, 382, 15329, 7150, 11, 366, 777, 11], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 123, "seek": 98200, "start": 1003.0, "end": 1004.0, "text": " which just construct it.", "tokens": [597, 445, 7690, 309, 13], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 124, "seek": 98200, "start": 1004.0, "end": 1009.0, "text": " So we call the build validator and get back an instance of our validator,", "tokens": [407, 321, 818, 264, 1322, 7363, 1639, 293, 483, 646, 364, 5197, 295, 527, 7363, 1639, 11], "temperature": 0.0, "avg_logprob": -0.09058709298410723, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.0001185851069749333}, {"id": 125, "seek": 100900, "start": 1009.0, "end": 1013.0, "text": " which we then store and return the type.", "tokens": [597, 321, 550, 3531, 293, 2736, 264, 2010, 13], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 126, "seek": 100900, "start": 1013.0, "end": 1015.0, "text": " Actually, this is much more complicated.", "tokens": [5135, 11, 341, 307, 709, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 127, "seek": 100900, "start": 1015.0, "end": 1021.0, "text": " One of the cleverest and most infuriating bits of pydantic core is that we,", "tokens": [1485, 295, 264, 13494, 377, 293, 881, 1536, 9744, 990, 9239, 295, 10664, 67, 7128, 4965, 307, 300, 321, 11], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 128, "seek": 100900, "start": 1021.0, "end": 1025.0, "text": " as you can imagine, this schema for defining validation becomes quite complex.", "tokens": [382, 291, 393, 3811, 11, 341, 34078, 337, 17827, 24071, 3643, 1596, 3997, 13], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 129, "seek": 100900, "start": 1025.0, "end": 1027.0, "text": " It's very easy to make a mistake.", "tokens": [467, 311, 588, 1858, 281, 652, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 130, "seek": 100900, "start": 1027.0, "end": 1031.0, "text": " So we validate it using pydantic core itself,", "tokens": [407, 321, 29562, 309, 1228, 10664, 67, 7128, 4965, 2564, 11], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 131, "seek": 100900, "start": 1031.0, "end": 1033.0, "text": " which when it works is magic,", "tokens": [597, 562, 309, 1985, 307, 5585, 11], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 132, "seek": 100900, "start": 1033.0, "end": 1036.0, "text": " and when it doesn't work leads to impossible errors,", "tokens": [293, 562, 309, 1177, 380, 589, 6689, 281, 6243, 13603, 11], "temperature": 0.0, "avg_logprob": -0.08804871211542148, "compression_ratio": 1.6088709677419355, "no_speech_prob": 4.6015782572794706e-05}, {"id": 133, "seek": 103600, "start": 1036.0, "end": 1041.0, "text": " because obviously all of the things that you're looking at as members of dictionaries", "tokens": [570, 2745, 439, 295, 264, 721, 300, 291, 434, 1237, 412, 382, 2679, 295, 22352, 4889], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 134, "seek": 103600, "start": 1041.0, "end": 1044.0, "text": " are in turn the names of bits of validation.", "tokens": [366, 294, 1261, 264, 5288, 295, 9239, 295, 24071, 13], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 135, "seek": 103600, "start": 1044.0, "end": 1046.0, "text": " So it's complete hell, but it works,", "tokens": [407, 309, 311, 3566, 4921, 11, 457, 309, 1985, 11], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 136, "seek": 103600, "start": 1046.0, "end": 1053.0, "text": " and it makes it very hard to build an invalid or not build the validator that you want.", "tokens": [293, 309, 1669, 309, 588, 1152, 281, 1322, 364, 34702, 420, 406, 1322, 264, 7363, 1639, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 137, "seek": 103600, "start": 1053.0, "end": 1055.0, "text": " And then we have these two implementations,", "tokens": [400, 550, 321, 362, 613, 732, 4445, 763, 11], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 138, "seek": 103600, "start": 1055.0, "end": 1058.0, "text": " two functions which do validate Python objects,", "tokens": [732, 6828, 597, 360, 29562, 15329, 6565, 11], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 139, "seek": 103600, "start": 1058.0, "end": 1060.0, "text": " as I said earlier, which call validate,", "tokens": [382, 286, 848, 3071, 11, 597, 818, 29562, 11], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 140, "seek": 103600, "start": 1060.0, "end": 1065.0, "text": " and same with JSON, where we parse a JSON using SERD to a JSON value", "tokens": [293, 912, 365, 31828, 11, 689, 321, 48377, 257, 31828, 1228, 36772, 35, 281, 257, 31828, 2158], "temperature": 0.0, "avg_logprob": -0.11711118975256243, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.00010646324517438188}, {"id": 141, "seek": 106500, "start": 1065.0, "end": 1068.0, "text": " and then call validate again with that input.", "tokens": [293, 550, 818, 29562, 797, 365, 300, 4846, 13], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 142, "seek": 106500, "start": 1068.0, "end": 1072.0, "text": " This code is obviously heavily simplified so that it fits.", "tokens": [639, 3089, 307, 2745, 10950, 26335, 370, 300, 309, 9001, 13], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 143, "seek": 106500, "start": 1072.0, "end": 1074.0, "text": " It doesn't fit on the page, but nearly fits on the page.", "tokens": [467, 1177, 380, 3318, 322, 264, 3028, 11, 457, 6217, 9001, 322, 264, 3028, 13], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 144, "seek": 106500, "start": 1074.0, "end": 1076.0, "text": " So not everything is exactly as it really is,", "tokens": [407, 406, 1203, 307, 2293, 382, 309, 534, 307, 11], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 145, "seek": 106500, "start": 1076.0, "end": 1082.0, "text": " but I think that kind of gives you an idea of how we build up these validators.", "tokens": [457, 286, 519, 300, 733, 295, 2709, 291, 364, 1558, 295, 577, 321, 1322, 493, 613, 7363, 3391, 13], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 146, "seek": 106500, "start": 1082.0, "end": 1083.0, "text": " The other thing missed here,", "tokens": [440, 661, 551, 6721, 510, 11], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 147, "seek": 106500, "start": 1083.0, "end": 1086.0, "text": " we also do the whole thing again for serialization.", "tokens": [321, 611, 360, 264, 1379, 551, 797, 337, 17436, 2144, 13], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 148, "seek": 106500, "start": 1086.0, "end": 1092.0, "text": " So the serialization from both a pydantic model to a Python dictionary", "tokens": [407, 264, 17436, 2144, 490, 1293, 257, 10664, 67, 7128, 2316, 281, 257, 15329, 25890], "temperature": 0.0, "avg_logprob": -0.06923442137868781, "compression_ratio": 1.6884615384615385, "no_speech_prob": 6.918640428921208e-05}, {"id": 149, "seek": 109200, "start": 1092.0, "end": 1096.0, "text": " and from a pydantic model straight to JSON is all written in Rust,", "tokens": [293, 490, 257, 10664, 67, 7128, 2316, 2997, 281, 31828, 307, 439, 3720, 294, 34952, 11], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 150, "seek": 109200, "start": 1096.0, "end": 1101.0, "text": " and it does useful things like filtering out elements as you go along,", "tokens": [293, 309, 775, 4420, 721, 411, 30822, 484, 4959, 382, 291, 352, 2051, 11], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 151, "seek": 109200, "start": 1101.0, "end": 1106.0, "text": " and it's effectively the same structure, uses the same schema,", "tokens": [293, 309, 311, 8659, 264, 912, 3877, 11, 4960, 264, 912, 34078, 11], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 152, "seek": 109200, "start": 1106.0, "end": 1112.0, "text": " but it's just dedicated to serialization rather than validation.", "tokens": [457, 309, 311, 445, 8374, 281, 17436, 2144, 2831, 813, 24071, 13], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 153, "seek": 109200, "start": 1112.0, "end": 1116.0, "text": " So what does the Python interface then look like?", "tokens": [407, 437, 775, 264, 15329, 9226, 550, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 154, "seek": 109200, "start": 1116.0, "end": 1119.0, "text": " So what I didn't explain earlier is that pydantic v2,", "tokens": [407, 437, 286, 994, 380, 2903, 3071, 307, 300, 10664, 67, 7128, 371, 17, 11], "temperature": 0.0, "avg_logprob": -0.09666902460950486, "compression_ratio": 1.550420168067227, "no_speech_prob": 2.7413329007686116e-05}, {"id": 155, "seek": 111900, "start": 1119.0, "end": 1122.0, "text": " which is going to be released, fingers crossed in Q1 this year,", "tokens": [597, 307, 516, 281, 312, 4736, 11, 7350, 14622, 294, 1249, 16, 341, 1064, 11], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 156, "seek": 111900, "start": 1122.0, "end": 1124.0, "text": " is made up of two packages.", "tokens": [307, 1027, 493, 295, 732, 17401, 13], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 157, "seek": 111900, "start": 1124.0, "end": 1127.0, "text": " We have pydantic itself, which is a pure Python package,", "tokens": [492, 362, 10664, 67, 7128, 2564, 11, 597, 307, 257, 6075, 15329, 7372, 11], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 158, "seek": 111900, "start": 1127.0, "end": 1131.0, "text": " and then we have pydantic core, which is almost all Rust code.", "tokens": [293, 550, 321, 362, 10664, 67, 7128, 4965, 11, 597, 307, 1920, 439, 34952, 3089, 13], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 159, "seek": 111900, "start": 1131.0, "end": 1134.0, "text": " We have a little bit of shim of Python to explain what's going on,", "tokens": [492, 362, 257, 707, 857, 295, 402, 332, 295, 15329, 281, 2903, 437, 311, 516, 322, 11], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 160, "seek": 111900, "start": 1134.0, "end": 1137.0, "text": " but it's really just the Rust code I've been showing you.", "tokens": [457, 309, 311, 534, 445, 264, 34952, 3089, 286, 600, 668, 4099, 291, 13], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 161, "seek": 111900, "start": 1137.0, "end": 1140.0, "text": " So what pydantic now does,", "tokens": [407, 437, 10664, 67, 7128, 586, 775, 11], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 162, "seek": 111900, "start": 1140.0, "end": 1144.0, "text": " all that pydantic effectively takes care of is converting those type annotations", "tokens": [439, 300, 10664, 67, 7128, 8659, 2516, 1127, 295, 307, 29942, 729, 2010, 25339, 763], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 163, "seek": 111900, "start": 1144.0, "end": 1148.0, "text": " I showed you earlier into a pydantic core schema", "tokens": [286, 4712, 291, 3071, 666, 257, 10664, 67, 7128, 4965, 34078], "temperature": 0.0, "avg_logprob": -0.06966986273327012, "compression_ratio": 1.7607142857142857, "no_speech_prob": 6.407783803297207e-05}, {"id": 164, "seek": 114800, "start": 1148.0, "end": 1150.0, "text": " and then building a validator.", "tokens": [293, 550, 2390, 257, 7363, 1639, 13], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 165, "seek": 114800, "start": 1150.0, "end": 1152.0, "text": " So looking at an example here,", "tokens": [407, 1237, 412, 364, 1365, 510, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 166, "seek": 114800, "start": 1152.0, "end": 1154.0, "text": " we obviously import schema validator,", "tokens": [321, 2745, 974, 34078, 7363, 1639, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 167, "seek": 114800, "start": 1154.0, "end": 1158.0, "text": " which I just showed you from, that's come up at the wrong time,", "tokens": [597, 286, 445, 4712, 291, 490, 11, 300, 311, 808, 493, 412, 264, 2085, 565, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 168, "seek": 114800, "start": 1158.0, "end": 1161.0, "text": " from pydantic core,", "tokens": [490, 10664, 67, 7128, 4965, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 169, "seek": 114800, "start": 1161.0, "end": 1167.0, "text": " and then the base level,", "tokens": [293, 550, 264, 3096, 1496, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 170, "seek": 114800, "start": 1167.0, "end": 1172.0, "text": " the schema for the base validator is model,", "tokens": [264, 34078, 337, 264, 3096, 7363, 1639, 307, 2316, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 171, "seek": 114800, "start": 1172.0, "end": 1174.0, "text": " and it contains, as I said earlier, a class,", "tokens": [293, 309, 8306, 11, 382, 286, 848, 3071, 11, 257, 1508, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 172, "seek": 114800, "start": 1174.0, "end": 1176.0, "text": " which is the Python class to instantiate,", "tokens": [597, 307, 264, 15329, 1508, 281, 9836, 13024, 11], "temperature": 0.0, "avg_logprob": -0.09681339263916015, "compression_ratio": 1.6298076923076923, "no_speech_prob": 3.312533954158425e-05}, {"id": 173, "seek": 117600, "start": 1176.0, "end": 1182.0, "text": " and another schema, which in turn defines the fields.", "tokens": [293, 1071, 34078, 11, 597, 294, 1261, 23122, 264, 7909, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 174, "seek": 117600, "start": 1182.0, "end": 1187.0, "text": " And that inner validator is then defined by a type dict validator,", "tokens": [400, 300, 7284, 7363, 1639, 307, 550, 7642, 538, 257, 2010, 12569, 7363, 1639, 11], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 175, "seek": 117600, "start": 1187.0, "end": 1189.0, "text": " as I said earlier.", "tokens": [382, 286, 848, 3071, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 176, "seek": 117600, "start": 1189.0, "end": 1191.0, "text": " So this is completely valid Python code.", "tokens": [407, 341, 307, 2584, 7363, 15329, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 177, "seek": 117600, "start": 1191.0, "end": 1193.0, "text": " This will run now.", "tokens": [639, 486, 1190, 586, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 178, "seek": 117600, "start": 1193.0, "end": 1197.0, "text": " So yeah, we have a type dict validator which contains fields,", "tokens": [407, 1338, 11, 321, 362, 257, 2010, 12569, 7363, 1639, 597, 8306, 7909, 11], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 179, "seek": 117600, "start": 1197.0, "end": 1200.0, "text": " which in turn are those fields which I showed you earlier.", "tokens": [597, 294, 1261, 366, 729, 7909, 597, 286, 4712, 291, 3071, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 180, "seek": 117600, "start": 1200.0, "end": 1205.0, "text": " So title attendances of type int when I talked about earlier.", "tokens": [407, 4876, 6888, 2676, 295, 2010, 560, 562, 286, 2825, 466, 3071, 13], "temperature": 0.0, "avg_logprob": -0.1615634578289372, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.454348891158588e-05}, {"id": 181, "seek": 120500, "start": 1205.0, "end": 1207.0, "text": " The most interesting thing here is,", "tokens": [440, 881, 1880, 551, 510, 307, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 182, "seek": 120500, "start": 1207.0, "end": 1210.0, "text": " if you look at the when validator,", "tokens": [498, 291, 574, 412, 264, 562, 7363, 1639, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 183, "seek": 120500, "start": 1210.0, "end": 1212.0, "text": " it gets a bit confusing.", "tokens": [309, 2170, 257, 857, 13181, 13], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 184, "seek": 120500, "start": 1212.0, "end": 1215.0, "text": " It's schema is of type default,", "tokens": [467, 311, 34078, 307, 295, 2010, 7576, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 185, "seek": 120500, "start": 1215.0, "end": 1219.0, "text": " which in turn contains another schema,", "tokens": [597, 294, 1261, 8306, 1071, 34078, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 186, "seek": 120500, "start": 1219.0, "end": 1221.0, "text": " which is of type nullable,", "tokens": [597, 307, 295, 2010, 18184, 712, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 187, "seek": 120500, "start": 1221.0, "end": 1224.0, "text": " which is the simplest union, either a value or none.", "tokens": [597, 307, 264, 22811, 11671, 11, 2139, 257, 2158, 420, 6022, 13], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 188, "seek": 120500, "start": 1224.0, "end": 1229.0, "text": " The default validator contains another member,", "tokens": [440, 7576, 7363, 1639, 8306, 1071, 4006, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 189, "seek": 120500, "start": 1229.0, "end": 1232.0, "text": " which is the default value, in this case none,", "tokens": [597, 307, 264, 7576, 2158, 11, 294, 341, 1389, 6022, 11], "temperature": 0.0, "avg_logprob": -0.1003701460988898, "compression_ratio": 1.780104712041885, "no_speech_prob": 7.204380381153896e-05}, {"id": 190, "seek": 123200, "start": 1232.0, "end": 1235.0, "text": " and the inner schema is then nullable,", "tokens": [293, 264, 7284, 34078, 307, 550, 18184, 712, 11], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 191, "seek": 123200, "start": 1235.0, "end": 1237.0, "text": " which in turn contains another inner schema,", "tokens": [597, 294, 1261, 8306, 1071, 7284, 34078, 11], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 192, "seek": 123200, "start": 1237.0, "end": 1239.0, "text": " which is then the date time.", "tokens": [597, 307, 550, 264, 4002, 565, 13], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 193, "seek": 123200, "start": 1239.0, "end": 1242.0, "text": " So that's how we define effectively default values", "tokens": [407, 300, 311, 577, 321, 6964, 8659, 7576, 4190], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 194, "seek": 123200, "start": 1242.0, "end": 1244.0, "text": " and null or nullable.", "tokens": [293, 18184, 420, 18184, 712, 13], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 195, "seek": 123200, "start": 1244.0, "end": 1247.0, "text": " So one of the other mistakes in Pyrantic in the past", "tokens": [407, 472, 295, 264, 661, 8038, 294, 430, 6016, 7128, 294, 264, 1791], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 196, "seek": 123200, "start": 1247.0, "end": 1249.0, "text": " was that we kind of conflate,", "tokens": [390, 300, 321, 733, 295, 1497, 17593, 11], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 197, "seek": 123200, "start": 1249.0, "end": 1251.0, "text": " effectively Python made a mistake about 10 years ago", "tokens": [8659, 15329, 1027, 257, 6146, 466, 1266, 924, 2057], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 198, "seek": 123200, "start": 1251.0, "end": 1256.0, "text": " where they used, they had a alias for union of something and none,", "tokens": [689, 436, 1143, 11, 436, 632, 257, 419, 4609, 337, 11671, 295, 746, 293, 6022, 11], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 199, "seek": 123200, "start": 1256.0, "end": 1258.0, "text": " that they called optional,", "tokens": [300, 436, 1219, 17312, 11], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 200, "seek": 123200, "start": 1258.0, "end": 1261.0, "text": " which then meant that I didn't want to have a thing", "tokens": [597, 550, 4140, 300, 286, 994, 380, 528, 281, 362, 257, 551], "temperature": 0.0, "avg_logprob": -0.11980339837452722, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.00011988948244834319}, {"id": 201, "seek": 126100, "start": 1261.0, "end": 1263.0, "text": " called optional, but was not optional.", "tokens": [1219, 17312, 11, 457, 390, 406, 17312, 13], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 202, "seek": 126100, "start": 1263.0, "end": 1267.0, "text": " And so we conflated nullable with optional in Pyrantic", "tokens": [400, 370, 321, 1497, 38539, 18184, 712, 365, 17312, 294, 430, 6016, 7128], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 203, "seek": 126100, "start": 1267.0, "end": 1269.0, "text": " and rightly it confused everyone.", "tokens": [293, 32879, 309, 9019, 1518, 13], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 204, "seek": 126100, "start": 1269.0, "end": 1272.0, "text": " And so the solution, the solution from Python", "tokens": [400, 370, 264, 3827, 11, 264, 3827, 490, 15329], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 205, "seek": 126100, "start": 1272.0, "end": 1275.0, "text": " was to start using the pipe operator for unions", "tokens": [390, 281, 722, 1228, 264, 11240, 12973, 337, 24914], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 206, "seek": 126100, "start": 1275.0, "end": 1277.0, "text": " and to just basically ignore optional.", "tokens": [293, 281, 445, 1936, 11200, 17312, 13], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 207, "seek": 126100, "start": 1277.0, "end": 1279.0, "text": " They can't really get rid of it,", "tokens": [814, 393, 380, 534, 483, 3973, 295, 309, 11], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 208, "seek": 126100, "start": 1279.0, "end": 1282.0, "text": " but they just pretend it didn't really happen.", "tokens": [457, 436, 445, 11865, 309, 994, 380, 534, 1051, 13], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 209, "seek": 126100, "start": 1282.0, "end": 1286.0, "text": " My solution is to define default", "tokens": [1222, 3827, 307, 281, 6964, 7576], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 210, "seek": 126100, "start": 1286.0, "end": 1288.0, "text": " and nullable as completely separate things", "tokens": [293, 18184, 712, 382, 2584, 4994, 721], "temperature": 0.0, "avg_logprob": -0.09553074386884582, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.0001359145826427266}, {"id": 211, "seek": 128800, "start": 1288.0, "end": 1292.0, "text": " and we're not going to use the optional type anywhere in our docs.", "tokens": [293, 321, 434, 406, 516, 281, 764, 264, 17312, 2010, 4992, 294, 527, 45623, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 212, "seek": 128800, "start": 1292.0, "end": 1295.0, "text": " We're just going to use union of thing and none", "tokens": [492, 434, 445, 516, 281, 764, 11671, 295, 551, 293, 6022], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 213, "seek": 128800, "start": 1295.0, "end": 1298.0, "text": " to avoid that confusion.", "tokens": [281, 5042, 300, 15075, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 214, "seek": 128800, "start": 1298.0, "end": 1301.0, "text": " And then I think mistakes, I hope it kind of makes sense to you.", "tokens": [400, 550, 286, 519, 8038, 11, 286, 1454, 309, 733, 295, 1669, 2020, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 215, "seek": 128800, "start": 1301.0, "end": 1304.0, "text": " Again, it's this like schema within schema within schema,", "tokens": [3764, 11, 309, 311, 341, 411, 34078, 1951, 34078, 1951, 34078, 11], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 216, "seek": 128800, "start": 1304.0, "end": 1307.0, "text": " which become validator within validator.", "tokens": [597, 1813, 7363, 1639, 1951, 7363, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 217, "seek": 128800, "start": 1307.0, "end": 1311.0, "text": " And we take our code, as I showed you earlier,", "tokens": [400, 321, 747, 527, 3089, 11, 382, 286, 4712, 291, 3071, 11], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 218, "seek": 128800, "start": 1311.0, "end": 1312.0, "text": " run validation.", "tokens": [1190, 24071, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 219, "seek": 128800, "start": 1312.0, "end": 1314.0, "text": " In this case, we call validate Python.", "tokens": [682, 341, 1389, 11, 321, 818, 29562, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 220, "seek": 128800, "start": 1314.0, "end": 1315.0, "text": " We've got some Python code,", "tokens": [492, 600, 658, 512, 15329, 3089, 11], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 221, "seek": 128800, "start": 1315.0, "end": 1317.0, "text": " but we could just as well have a JSON string", "tokens": [457, 321, 727, 445, 382, 731, 362, 257, 31828, 6798], "temperature": 0.0, "avg_logprob": -0.1146078109741211, "compression_ratio": 1.7638376383763839, "no_speech_prob": 6.093757110647857e-05}, {"id": 222, "seek": 131700, "start": 1317.0, "end": 1319.0, "text": " and call validate JSON.", "tokens": [293, 818, 29562, 31828, 13], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 223, "seek": 131700, "start": 1319.0, "end": 1320.0, "text": " And then we have a talk instance,", "tokens": [400, 550, 321, 362, 257, 751, 5197, 11], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 224, "seek": 131700, "start": 1320.0, "end": 1328.0, "text": " which lets us access the members of it as you normally would.", "tokens": [597, 6653, 505, 2105, 264, 2679, 295, 309, 382, 291, 5646, 576, 13], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 225, "seek": 131700, "start": 1328.0, "end": 1331.0, "text": " So where does Rust excel in these applications?", "tokens": [407, 689, 775, 34952, 24015, 294, 613, 5821, 30], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 226, "seek": 131700, "start": 1331.0, "end": 1334.0, "text": " Why build this in Rust?", "tokens": [1545, 1322, 341, 294, 34952, 30], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 227, "seek": 131700, "start": 1334.0, "end": 1337.0, "text": " There are a bunch of obvious reasons to use Rust,", "tokens": [821, 366, 257, 3840, 295, 6322, 4112, 281, 764, 34952, 11], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 228, "seek": 131700, "start": 1337.0, "end": 1339.0, "text": " performance being the number one,", "tokens": [3389, 885, 264, 1230, 472, 11], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 229, "seek": 131700, "start": 1339.0, "end": 1341.0, "text": " multi-threading and not having the global interpreter lock", "tokens": [4825, 12, 392, 35908, 293, 406, 1419, 264, 4338, 34132, 4017], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 230, "seek": 131700, "start": 1341.0, "end": 1344.0, "text": " in Python is another one.", "tokens": [294, 15329, 307, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.10473455127916838, "compression_ratio": 1.506276150627615, "no_speech_prob": 4.12862173106987e-05}, {"id": 231, "seek": 134400, "start": 1344.0, "end": 1349.0, "text": " The third is using high-quality existing Rust libraries", "tokens": [440, 2636, 307, 1228, 1090, 12, 11286, 6741, 34952, 15148], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 232, "seek": 134400, "start": 1349.0, "end": 1352.0, "text": " to build libraries in Python instead of implementing it yourself.", "tokens": [281, 1322, 15148, 294, 15329, 2602, 295, 18114, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 233, "seek": 134400, "start": 1352.0, "end": 1355.0, "text": " So I maintain two other Python libraries written in Rust", "tokens": [407, 286, 6909, 732, 661, 15329, 15148, 3720, 294, 34952], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 234, "seek": 134400, "start": 1355.0, "end": 1360.0, "text": " watch files, which uses the notify crate to do file-watching", "tokens": [1159, 7098, 11, 597, 4960, 264, 36560, 42426, 281, 360, 3991, 12, 15219, 278], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 235, "seek": 134400, "start": 1360.0, "end": 1362.0, "text": " and then RTOML, which, as you can guess,", "tokens": [293, 550, 497, 51, 5251, 43, 11, 597, 11, 382, 291, 393, 2041, 11], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 236, "seek": 134400, "start": 1362.0, "end": 1367.0, "text": " is a TOML parser using the TOML library from Rust.", "tokens": [307, 257, 314, 5251, 43, 21156, 260, 1228, 264, 314, 5251, 43, 6405, 490, 34952, 13], "temperature": 0.0, "avg_logprob": -0.18330304006512246, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.00012120058090658858}, {"id": 237, "seek": 136700, "start": 1367.0, "end": 1374.0, "text": " And the RTOML library is the fastest Python TOML parser", "tokens": [400, 264, 497, 51, 5251, 43, 6405, 307, 264, 14573, 15329, 314, 5251, 43, 21156, 260], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 238, "seek": 136700, "start": 1374.0, "end": 1377.0, "text": " out there.", "tokens": [484, 456, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 239, "seek": 136700, "start": 1377.0, "end": 1379.0, "text": " And actually watch files is becoming more and more popular.", "tokens": [400, 767, 1159, 7098, 307, 5617, 544, 293, 544, 3743, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 240, "seek": 136700, "start": 1379.0, "end": 1380.0, "text": " It's the default now with u-vehicle,", "tokens": [467, 311, 264, 7576, 586, 365, 344, 12, 303, 71, 3520, 11], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 241, "seek": 136700, "start": 1380.0, "end": 1383.0, "text": " which is one of the web servers.", "tokens": [597, 307, 472, 295, 264, 3670, 15909, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 242, "seek": 136700, "start": 1383.0, "end": 1387.0, "text": " But perhaps less obviously in terms of where Rust fits in best.", "tokens": [583, 4317, 1570, 2745, 294, 2115, 295, 689, 34952, 9001, 294, 1151, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 243, "seek": 136700, "start": 1387.0, "end": 1389.0, "text": " Deeply recursive code, as I've just showed you,", "tokens": [14895, 356, 20560, 488, 3089, 11, 382, 286, 600, 445, 4712, 291, 11], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 244, "seek": 136700, "start": 1389.0, "end": 1391.0, "text": " with these validators within validators.", "tokens": [365, 613, 7363, 3391, 1951, 7363, 3391, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 245, "seek": 136700, "start": 1391.0, "end": 1392.0, "text": " There's no stack.", "tokens": [821, 311, 572, 8630, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 246, "seek": 136700, "start": 1392.0, "end": 1395.0, "text": " And so we don't have a penalty for recursion.", "tokens": [400, 370, 321, 500, 380, 362, 257, 16263, 337, 20560, 313, 13], "temperature": 0.0, "avg_logprob": -0.11578731226727246, "compression_ratio": 1.541044776119403, "no_speech_prob": 4.4606906158151105e-05}, {"id": 247, "seek": 139500, "start": 1395.0, "end": 1397.0, "text": " We do have to be very, very careful,", "tokens": [492, 360, 362, 281, 312, 588, 11, 588, 5026, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 248, "seek": 139500, "start": 1397.0, "end": 1399.0, "text": " because, as I'm sure you all know,", "tokens": [570, 11, 382, 286, 478, 988, 291, 439, 458, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 249, "seek": 139500, "start": 1399.0, "end": 1401.0, "text": " if you have recursion in Rust and you don't catch it,", "tokens": [498, 291, 362, 20560, 313, 294, 34952, 293, 291, 500, 380, 3745, 309, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 250, "seek": 139500, "start": 1401.0, "end": 1402.0, "text": " you just get a segfault.", "tokens": [291, 445, 483, 257, 3896, 69, 5107, 13], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 251, "seek": 139500, "start": 1402.0, "end": 1406.0, "text": " And that would be very, very upsetting to Python developers", "tokens": [400, 300, 576, 312, 588, 11, 588, 44109, 281, 15329, 8849], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 252, "seek": 139500, "start": 1406.0, "end": 1408.0, "text": " who've never seen one before.", "tokens": [567, 600, 1128, 1612, 472, 949, 13], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 253, "seek": 139500, "start": 1408.0, "end": 1411.0, "text": " So there's an enormous amount of, as a significant amount of code", "tokens": [407, 456, 311, 364, 11322, 2372, 295, 11, 382, 257, 4776, 2372, 295, 3089], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 254, "seek": 139500, "start": 1411.0, "end": 1414.0, "text": " in Pydantic Core dedicated to catching recursion,", "tokens": [294, 9953, 67, 7128, 14798, 8374, 281, 16124, 20560, 313, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 255, "seek": 139500, "start": 1414.0, "end": 1417.0, "text": " we have to have, is it two or three different sorts of guard", "tokens": [321, 362, 281, 362, 11, 307, 309, 732, 420, 1045, 819, 7527, 295, 6290], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 256, "seek": 139500, "start": 1417.0, "end": 1420.0, "text": " to protect against recursion in all possible different situations,", "tokens": [281, 2371, 1970, 20560, 313, 294, 439, 1944, 819, 6851, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 257, "seek": 139500, "start": 1420.0, "end": 1423.0, "text": " because it's effectively the worst thing that we can have,", "tokens": [570, 309, 311, 8659, 264, 5855, 551, 300, 321, 393, 362, 11], "temperature": 0.0, "avg_logprob": -0.11117932567857716, "compression_ratio": 1.7459807073954985, "no_speech_prob": 8.492441702401266e-05}, {"id": 258, "seek": 142300, "start": 1423.0, "end": 1425.0, "text": " is that there is some data structure", "tokens": [307, 300, 456, 307, 512, 1412, 3877], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 259, "seek": 142300, "start": 1425.0, "end": 1426.0, "text": " that you can pass to Pydantic,", "tokens": [300, 291, 393, 1320, 281, 9953, 67, 7128, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 260, "seek": 142300, "start": 1426.0, "end": 1430.0, "text": " which causes your entire Python process to segfault,", "tokens": [597, 7700, 428, 2302, 15329, 1399, 281, 3896, 69, 5107, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 261, "seek": 142300, "start": 1430.0, "end": 1433.0, "text": " and you wouldn't know where to even start looking.", "tokens": [293, 291, 2759, 380, 458, 689, 281, 754, 722, 1237, 13], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 262, "seek": 142300, "start": 1433.0, "end": 1435.0, "text": " So that's a blessing.", "tokens": [407, 300, 311, 257, 13869, 13], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 263, "seek": 142300, "start": 1435.0, "end": 1438.0, "text": " The lack of a stack is a blessing and a curse.", "tokens": [440, 5011, 295, 257, 8630, 307, 257, 13869, 293, 257, 17139, 13], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 264, "seek": 142300, "start": 1438.0, "end": 1440.0, "text": " And then the second big advantage,", "tokens": [400, 550, 264, 1150, 955, 5002, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 265, "seek": 142300, "start": 1440.0, "end": 1443.0, "text": " I think, of where Rust excels,", "tokens": [286, 519, 11, 295, 689, 34952, 1624, 1625, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 266, "seek": 142300, "start": 1443.0, "end": 1445.0, "text": " is in the small modular components.", "tokens": [307, 294, 264, 1359, 31111, 6677, 13], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 267, "seek": 142300, "start": 1445.0, "end": 1447.0, "text": " So where I was showing you before,", "tokens": [407, 689, 286, 390, 4099, 291, 949, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 268, "seek": 142300, "start": 1447.0, "end": 1451.0, "text": " these relatively small, in terms of code footprint validators,", "tokens": [613, 7226, 1359, 11, 294, 2115, 295, 3089, 24222, 7363, 3391, 11], "temperature": 0.0, "avg_logprob": -0.08966198587805276, "compression_ratio": 1.6236162361623616, "no_speech_prob": 5.683216295437887e-05}, {"id": 269, "seek": 145100, "start": 1451.0, "end": 1454.0, "text": " which in turn hold other ones,", "tokens": [597, 294, 1261, 1797, 661, 2306, 11], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 270, "seek": 145100, "start": 1454.0, "end": 1456.0, "text": " there's obviously no performance penalty", "tokens": [456, 311, 2745, 572, 3389, 16263], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 271, "seek": 145100, "start": 1456.0, "end": 1458.0, "text": " for having these functions in Rust.", "tokens": [337, 1419, 613, 6828, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 272, "seek": 145100, "start": 1458.0, "end": 1461.0, "text": " I say almost, because we actually have to use box", "tokens": [286, 584, 1920, 11, 570, 321, 767, 362, 281, 764, 2424], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 273, "seek": 145100, "start": 1461.0, "end": 1463.0, "text": " around validators because they hold themselves.", "tokens": [926, 7363, 3391, 570, 436, 1797, 2969, 13], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 274, "seek": 145100, "start": 1463.0, "end": 1468.0, "text": " So there is a bit of an overhead of going into the heap,", "tokens": [407, 456, 307, 257, 857, 295, 364, 19922, 295, 516, 666, 264, 33591, 11], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 275, "seek": 145100, "start": 1468.0, "end": 1471.0, "text": " but it's relatively small, particularly compared to Python.", "tokens": [457, 309, 311, 7226, 1359, 11, 4098, 5347, 281, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 276, "seek": 145100, "start": 1471.0, "end": 1474.0, "text": " And then the lastly complex error handling,", "tokens": [400, 550, 264, 16386, 3997, 6713, 13175, 11], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 277, "seek": 145100, "start": 1474.0, "end": 1477.0, "text": " obviously in Python, you don't know what's going to error", "tokens": [2745, 294, 15329, 11, 291, 500, 380, 458, 437, 311, 516, 281, 6713], "temperature": 0.0, "avg_logprob": -0.1045555205572219, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.00010979809303535149}, {"id": 278, "seek": 147700, "start": 1477.0, "end": 1481.0, "text": " and what exceptions you're going to get in Rust.", "tokens": [293, 437, 22847, 291, 434, 516, 281, 483, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 279, "seek": 147700, "start": 1481.0, "end": 1483.0, "text": " Putting to one side the comment about panic earlier,", "tokens": [31367, 281, 472, 1252, 264, 2871, 466, 14783, 3071, 11], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 280, "seek": 147700, "start": 1483.0, "end": 1485.0, "text": " you can in general know what errors you're going to get", "tokens": [291, 393, 294, 2674, 458, 437, 13603, 291, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 281, "seek": 147700, "start": 1485.0, "end": 1489.0, "text": " and catch them and construct validation errors", "tokens": [293, 3745, 552, 293, 7690, 24071, 13603], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 282, "seek": 147700, "start": 1489.0, "end": 1492.0, "text": " in the case of Pydantic, which is a great deal easier", "tokens": [294, 264, 1389, 295, 9953, 67, 7128, 11, 597, 307, 257, 869, 2028, 3571], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 283, "seek": 147700, "start": 1492.0, "end": 1496.0, "text": " than it would ever have been to write that code in Python.", "tokens": [813, 309, 576, 1562, 362, 668, 281, 2464, 300, 3089, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 284, "seek": 147700, "start": 1496.0, "end": 1500.0, "text": " So the way I want to think about the future development", "tokens": [407, 264, 636, 286, 528, 281, 519, 466, 264, 2027, 3250], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 285, "seek": 147700, "start": 1500.0, "end": 1503.0, "text": " of Python is not as Python versus Rust,", "tokens": [295, 15329, 307, 406, 382, 15329, 5717, 34952, 11], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 286, "seek": 147700, "start": 1503.0, "end": 1506.0, "text": " but effectively as Python as the user interface for Rust,", "tokens": [457, 8659, 382, 15329, 382, 264, 4195, 9226, 337, 34952, 11], "temperature": 0.0, "avg_logprob": -0.07078556287086617, "compression_ratio": 1.7252747252747254, "no_speech_prob": 3.4121105272788554e-05}, {"id": 287, "seek": 150600, "start": 1506.0, "end": 1510.0, "text": " or the application developer interface for Rust.", "tokens": [420, 264, 3861, 10754, 9226, 337, 34952, 13], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 288, "seek": 150600, "start": 1510.0, "end": 1512.0, "text": " So I'd love to see more and more libraries", "tokens": [407, 286, 1116, 959, 281, 536, 544, 293, 544, 15148], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 289, "seek": 150600, "start": 1512.0, "end": 1514.0, "text": " do what we've done with Pydantic Core", "tokens": [360, 437, 321, 600, 1096, 365, 9953, 67, 7128, 14798], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 290, "seek": 150600, "start": 1514.0, "end": 1520.0, "text": " and effectively implement their low-level components in Rust.", "tokens": [293, 8659, 4445, 641, 2295, 12, 12418, 6677, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 291, "seek": 150600, "start": 1520.0, "end": 1523.0, "text": " So my dream is a world in which,", "tokens": [407, 452, 3055, 307, 257, 1002, 294, 597, 11], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 292, "seek": 150600, "start": 1523.0, "end": 1526.0, "text": " thinking about the lifecycle of a HTTP request,", "tokens": [1953, 466, 264, 45722, 295, 257, 33283, 5308, 11], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 293, "seek": 150600, "start": 1526.0, "end": 1529.0, "text": " but you could think the same about some NL pipeline", "tokens": [457, 291, 727, 519, 264, 912, 466, 512, 426, 43, 15517], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 294, "seek": 150600, "start": 1529.0, "end": 1531.0, "text": " or many other applications,", "tokens": [420, 867, 661, 5821, 11], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 295, "seek": 150600, "start": 1531.0, "end": 1535.0, "text": " we effectively, the vast majority of the execution", "tokens": [321, 8659, 11, 264, 8369, 6286, 295, 264, 15058], "temperature": 0.0, "avg_logprob": -0.12723415973139743, "compression_ratio": 1.580392156862745, "no_speech_prob": 3.321336043882184e-05}, {"id": 296, "seek": 153500, "start": 1535.0, "end": 1537.0, "text": " is Rust or C,", "tokens": [307, 34952, 420, 383, 11], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 297, "seek": 153500, "start": 1537.0, "end": 1540.0, "text": " but then all of the application logic can be in Python.", "tokens": [457, 550, 439, 295, 264, 3861, 9952, 393, 312, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 298, "seek": 153500, "start": 1540.0, "end": 1543.0, "text": " So effectively we get to a point where we have 100%", "tokens": [407, 8659, 321, 483, 281, 257, 935, 689, 321, 362, 2319, 4], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 299, "seek": 153500, "start": 1543.0, "end": 1545.0, "text": " of developer time spent in high-level languages,", "tokens": [295, 10754, 565, 4418, 294, 1090, 12, 12418, 8650, 11], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 300, "seek": 153500, "start": 1545.0, "end": 1550.0, "text": " but only 1% of CPU dedicated to actually running Python code,", "tokens": [457, 787, 502, 4, 295, 13199, 8374, 281, 767, 2614, 15329, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 301, "seek": 153500, "start": 1550.0, "end": 1552.0, "text": " which is slower and is always going to be slower.", "tokens": [597, 307, 14009, 293, 307, 1009, 516, 281, 312, 14009, 13], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 302, "seek": 153500, "start": 1552.0, "end": 1554.0, "text": " I don't think there's ever a world in which", "tokens": [286, 500, 380, 519, 456, 311, 1562, 257, 1002, 294, 597], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 303, "seek": 153500, "start": 1554.0, "end": 1556.0, "text": " someone's going to come up with a language that is as fast", "tokens": [1580, 311, 516, 281, 808, 493, 365, 257, 2856, 300, 307, 382, 2370], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 304, "seek": 153500, "start": 1556.0, "end": 1559.0, "text": " and as safe as Rust, but also as quick to write as Python.", "tokens": [293, 382, 3273, 382, 34952, 11, 457, 611, 382, 1702, 281, 2464, 382, 15329, 13], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 305, "seek": 153500, "start": 1559.0, "end": 1562.0, "text": " So I don't think it should be one versus the other.", "tokens": [407, 286, 500, 380, 519, 309, 820, 312, 472, 5717, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.07379878524446139, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.00010176876821788028}, {"id": 306, "seek": 156200, "start": 1562.0, "end": 1565.0, "text": " It should be building the low-level, building the Rails,", "tokens": [467, 820, 312, 2390, 264, 2295, 12, 12418, 11, 2390, 264, 48526, 11], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 307, "seek": 156200, "start": 1565.0, "end": 1568.0, "text": " perhaps a bad term, but the Rails in Rust,", "tokens": [4317, 257, 1578, 1433, 11, 457, 264, 48526, 294, 34952, 11], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 308, "seek": 156200, "start": 1568.0, "end": 1572.0, "text": " and building the train in Python.", "tokens": [293, 2390, 264, 3847, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 309, "seek": 156200, "start": 1572.0, "end": 1575.0, "text": " It doesn't work, but you get where I'm coming from.", "tokens": [467, 1177, 380, 589, 11, 457, 291, 483, 689, 286, 478, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 310, "seek": 156200, "start": 1575.0, "end": 1579.0, "text": " Anyway, on that note, thank you very much.", "tokens": [5684, 11, 322, 300, 3637, 11, 1309, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 311, "seek": 156200, "start": 1579.0, "end": 1582.0, "text": " A few links there, particularly thanks to the PyO3 team", "tokens": [316, 1326, 6123, 456, 11, 4098, 3231, 281, 264, 9953, 46, 18, 1469], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 312, "seek": 156200, "start": 1582.0, "end": 1585.0, "text": " who built the bindings for Rust in Python, which is amazing.", "tokens": [567, 3094, 264, 14786, 1109, 337, 34952, 294, 15329, 11, 597, 307, 2243, 13], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 313, "seek": 156200, "start": 1585.0, "end": 1589.0, "text": " And if you want a laugh, there's a very, very funny issue", "tokens": [400, 498, 291, 528, 257, 5801, 11, 456, 311, 257, 588, 11, 588, 4074, 2734], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 314, "seek": 156200, "start": 1589.0, "end": 1591.0, "text": " on GitHub where a very angry man says", "tokens": [322, 23331, 689, 257, 588, 6884, 587, 1619], "temperature": 0.0, "avg_logprob": -0.11588781599014525, "compression_ratio": 1.6455223880597014, "no_speech_prob": 4.184509816695936e-05}, {"id": 315, "seek": 159100, "start": 1591.0, "end": 1593.0, "text": " why we should never use Rust.", "tokens": [983, 321, 820, 1128, 764, 34952, 13], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 316, "seek": 159100, "start": 1593.0, "end": 1596.0, "text": " So if you want to read that, I then took some time", "tokens": [407, 498, 291, 528, 281, 1401, 300, 11, 286, 550, 1890, 512, 565], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 317, "seek": 159100, "start": 1596.0, "end": 1598.0, "text": " to take them to pieces, which was quite satisfying,", "tokens": [281, 747, 552, 281, 3755, 11, 597, 390, 1596, 18348, 11], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 318, "seek": 159100, "start": 1598.0, "end": 1599.0, "text": " although a waste of time.", "tokens": [4878, 257, 5964, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 319, "seek": 159100, "start": 1599.0, "end": 1601.0, "text": " So have a look at that.", "tokens": [407, 362, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 320, "seek": 159100, "start": 1601.0, "end": 1602.0, "text": " Questions?", "tokens": [27738, 30], "temperature": 0.0, "avg_logprob": -0.09042267004648845, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.00023419606441166252}, {"id": 321, "seek": 160200, "start": 1602.0, "end": 1624.0, "text": " First, especially for the sanitation,", "tokens": [2386, 11, 2318, 337, 264, 50146, 11], "temperature": 0.0, "avg_logprob": -0.24921818702451645, "compression_ratio": 1.1237113402061856, "no_speech_prob": 0.0011143044102936983}, {"id": 322, "seek": 160200, "start": 1624.0, "end": 1627.0, "text": " are you thinking to publish a library of Rust?", "tokens": [366, 291, 1953, 281, 11374, 257, 6405, 295, 34952, 30], "temperature": 0.0, "avg_logprob": -0.24921818702451645, "compression_ratio": 1.1237113402061856, "no_speech_prob": 0.0011143044102936983}, {"id": 323, "seek": 160200, "start": 1627.0, "end": 1629.0, "text": " The job is already done,", "tokens": [440, 1691, 307, 1217, 1096, 11], "temperature": 0.0, "avg_logprob": -0.24921818702451645, "compression_ratio": 1.1237113402061856, "no_speech_prob": 0.0011143044102936983}, {"id": 324, "seek": 162900, "start": 1629.0, "end": 1633.0, "text": " and you could have a public API in a library", "tokens": [293, 291, 727, 362, 257, 1908, 9362, 294, 257, 6405], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 325, "seek": 162900, "start": 1633.0, "end": 1636.0, "text": " ready to validate Rust data.", "tokens": [1919, 281, 29562, 34952, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 326, "seek": 162900, "start": 1636.0, "end": 1639.0, "text": " I don't understand quite what...", "tokens": [286, 500, 380, 1223, 1596, 437, 485], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 327, "seek": 162900, "start": 1639.0, "end": 1642.0, "text": " So you wrote the library in Rust,", "tokens": [407, 291, 4114, 264, 6405, 294, 34952, 11], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 328, "seek": 162900, "start": 1642.0, "end": 1647.0, "text": " so could you publish just an API to validate JSON,", "tokens": [370, 727, 291, 11374, 445, 364, 9362, 281, 29562, 31828, 11], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 329, "seek": 162900, "start": 1647.0, "end": 1650.0, "text": " for example, from Rust, instead of through Python?", "tokens": [337, 1365, 11, 490, 34952, 11, 2602, 295, 807, 15329, 30], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 330, "seek": 162900, "start": 1650.0, "end": 1653.0, "text": " Absolutely, you could, and it would be useful", "tokens": [7021, 11, 291, 727, 11, 293, 309, 576, 312, 4420], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 331, "seek": 162900, "start": 1653.0, "end": 1656.0, "text": " if you wanted to somehow construct the schema", "tokens": [498, 291, 1415, 281, 6063, 7690, 264, 34078], "temperature": 0.0, "avg_logprob": -0.14279315176974522, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.0011555212549865246}, {"id": 332, "seek": 165600, "start": 1656.0, "end": 1659.0, "text": " at runtime fast, but it's never going to be anywhere", "tokens": [412, 34474, 2370, 11, 457, 309, 311, 1128, 516, 281, 312, 4992], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 333, "seek": 165600, "start": 1659.0, "end": 1661.0, "text": " near as performant as said,", "tokens": [2651, 382, 2042, 394, 382, 848, 11], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 334, "seek": 165600, "start": 1661.0, "end": 1665.0, "text": " because you were not compiling...", "tokens": [570, 291, 645, 406, 715, 4883, 485], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 335, "seek": 165600, "start": 1665.0, "end": 1667.0, "text": " We can't do anything at compile time.", "tokens": [492, 393, 380, 360, 1340, 412, 31413, 565, 13], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 336, "seek": 165600, "start": 1667.0, "end": 1670.0, "text": " Secondly, it's currently all completely intertwined", "tokens": [19483, 11, 309, 311, 4362, 439, 2584, 44400, 2001], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 337, "seek": 165600, "start": 1670.0, "end": 1672.0, "text": " with the PyO3 library and the Python types.", "tokens": [365, 264, 9953, 46, 18, 6405, 293, 264, 15329, 3467, 13], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 338, "seek": 165600, "start": 1672.0, "end": 1676.0, "text": " So there is a future nascent possible project,", "tokens": [407, 456, 307, 257, 2027, 5382, 2207, 1944, 1716, 11], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 339, "seek": 165600, "start": 1676.0, "end": 1678.0, "text": " Tidantic, which is Pidantic for TypeScript,", "tokens": [314, 327, 7128, 11, 597, 307, 430, 327, 7128, 337, 15576, 14237, 11], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 340, "seek": 165600, "start": 1678.0, "end": 1680.0, "text": " where we take the PyO3 types,", "tokens": [689, 321, 747, 264, 9953, 46, 18, 3467, 11], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 341, "seek": 165600, "start": 1680.0, "end": 1682.0, "text": " we effectively replace them with a new library", "tokens": [321, 8659, 7406, 552, 365, 257, 777, 6405], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 342, "seek": 165600, "start": 1682.0, "end": 1685.0, "text": " which has a compile time switch between the Python bindings", "tokens": [597, 575, 257, 31413, 565, 3679, 1296, 264, 15329, 14786, 1109], "temperature": 0.0, "avg_logprob": -0.1094459607050969, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.0006180699565447867}, {"id": 343, "seek": 168500, "start": 1685.0, "end": 1688.0, "text": " and the JavaScript bindings or the Wasm bindings,", "tokens": [293, 264, 15778, 14786, 1109, 420, 264, 3027, 76, 14786, 1109, 11], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 344, "seek": 168500, "start": 1688.0, "end": 1690.0, "text": " and then we can build Tidantic.", "tokens": [293, 550, 321, 393, 1322, 314, 327, 7128, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 345, "seek": 168500, "start": 1690.0, "end": 1692.0, "text": " That's a future plan, but a long way off.", "tokens": [663, 311, 257, 2027, 1393, 11, 457, 257, 938, 636, 766, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 346, "seek": 168500, "start": 1692.0, "end": 1694.0, "text": " Right now, it wouldn't really be worth it,", "tokens": [1779, 586, 11, 309, 2759, 380, 534, 312, 3163, 309, 11], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 347, "seek": 168500, "start": 1694.0, "end": 1696.0, "text": " because you would get lots of slowdown from Python", "tokens": [570, 291, 576, 483, 3195, 295, 2964, 5093, 490, 15329], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 348, "seek": 168500, "start": 1696.0, "end": 1697.0, "text": " and from compile time.", "tokens": [293, 490, 31413, 565, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 349, "seek": 168500, "start": 1697.0, "end": 1699.0, "text": " So we need a completely different library,", "tokens": [407, 321, 643, 257, 2584, 819, 6405, 11], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 350, "seek": 168500, "start": 1699.0, "end": 1701.0, "text": " just for us, like you're saying.", "tokens": [445, 337, 505, 11, 411, 291, 434, 1566, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 351, "seek": 168500, "start": 1701.0, "end": 1702.0, "text": " Yeah, SIRD is amazing.", "tokens": [865, 11, 318, 7740, 35, 307, 2243, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 352, "seek": 168500, "start": 1702.0, "end": 1705.0, "text": " I don't think I'm going to go and try and compete with that.", "tokens": [286, 500, 380, 519, 286, 478, 516, 281, 352, 293, 853, 293, 11831, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 353, "seek": 168500, "start": 1705.0, "end": 1710.0, "text": " At least, it's great for that application.", "tokens": [1711, 1935, 11, 309, 311, 869, 337, 300, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 354, "seek": 168500, "start": 1710.0, "end": 1712.0, "text": " Thanks for the talk.", "tokens": [2561, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 355, "seek": 168500, "start": 1712.0, "end": 1714.0, "text": " Recently, I think the Python library cryptography", "tokens": [20072, 11, 286, 519, 264, 15329, 6405, 9844, 5820], "temperature": 0.0, "avg_logprob": -0.1616675853729248, "compression_ratio": 1.5913312693498451, "no_speech_prob": 0.00016964544192887843}, {"id": 356, "seek": 171400, "start": 1714.0, "end": 1717.0, "text": " introduced Rust, and had some complaints", "tokens": [7268, 34952, 11, 293, 632, 512, 19585], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 357, "seek": 171400, "start": 1717.0, "end": 1719.0, "text": " from people using obscure build processes", "tokens": [490, 561, 1228, 34443, 1322, 7555], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 358, "seek": 171400, "start": 1719.0, "end": 1721.0, "text": " where Rust didn't work.", "tokens": [689, 34952, 994, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 359, "seek": 171400, "start": 1721.0, "end": 1723.0, "text": " Are you expecting anything from that?", "tokens": [2014, 291, 9650, 1340, 490, 300, 30], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 360, "seek": 171400, "start": 1723.0, "end": 1726.0, "text": " So I will actually bring up now.", "tokens": [407, 286, 486, 767, 1565, 493, 586, 13], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 361, "seek": 171400, "start": 1726.0, "end": 1728.0, "text": " Now I'm going to get into how to...", "tokens": [823, 286, 478, 516, 281, 483, 666, 577, 281, 485], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 362, "seek": 171400, "start": 1728.0, "end": 1730.0, "text": " Effectively, go and read that issue,", "tokens": [17764, 3413, 11, 352, 293, 1401, 300, 2734, 11], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 363, "seek": 171400, "start": 1730.0, "end": 1733.0, "text": " where, among other things, I...", "tokens": [689, 11, 3654, 661, 721, 11, 286, 485], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 364, "seek": 171400, "start": 1733.0, "end": 1738.0, "text": " Oh, how do I get out of this mode?", "tokens": [876, 11, 577, 360, 286, 483, 484, 295, 341, 4391, 30], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 365, "seek": 171400, "start": 1738.0, "end": 1741.0, "text": " So, rant, rant, rant, rant from him.", "tokens": [407, 11, 45332, 11, 45332, 11, 45332, 11, 45332, 490, 796, 13], "temperature": 0.0, "avg_logprob": -0.1345527397011811, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00020403432426974177}, {"id": 366, "seek": 174100, "start": 1741.0, "end": 1745.0, "text": " Effectively, I went through the...", "tokens": [17764, 3413, 11, 286, 1437, 807, 264, 485], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 367, "seek": 174100, "start": 1745.0, "end": 1747.0, "text": " just over a quarter of a billion downloads", "tokens": [445, 670, 257, 6555, 295, 257, 5218, 36553], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 368, "seek": 174100, "start": 1747.0, "end": 1749.0, "text": " over the last 12 months of Pydantic,", "tokens": [670, 264, 1036, 2272, 2493, 295, 9953, 67, 7128, 11], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 369, "seek": 174100, "start": 1749.0, "end": 1751.0, "text": " and I worked out looking at the distribution", "tokens": [293, 286, 2732, 484, 1237, 412, 264, 7316], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 370, "seek": 174100, "start": 1751.0, "end": 1753.0, "text": " of the different operating systems", "tokens": [295, 264, 819, 7447, 3652], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 371, "seek": 174100, "start": 1753.0, "end": 1757.0, "text": " and, like, libc implementations, et cetera,", "tokens": [293, 11, 411, 11, 22854, 66, 4445, 763, 11, 1030, 11458, 11], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 372, "seek": 174100, "start": 1757.0, "end": 1761.0, "text": " that 99.9859% of people would have got a binary", "tokens": [300, 11803, 13, 22516, 19600, 4, 295, 561, 576, 362, 658, 257, 17434], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 373, "seek": 174100, "start": 1761.0, "end": 1763.0, "text": " if they had installed Pydantic Core then.", "tokens": [498, 436, 632, 8899, 9953, 67, 7128, 14798, 550, 13], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 374, "seek": 174100, "start": 1763.0, "end": 1764.0, "text": " That number will be higher now,", "tokens": [663, 1230, 486, 312, 2946, 586, 11], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 375, "seek": 174100, "start": 1764.0, "end": 1767.0, "text": " because there will be fewer esoteric operating systems.", "tokens": [570, 456, 486, 312, 13366, 785, 21585, 299, 7447, 3652, 13], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 376, "seek": 174100, "start": 1767.0, "end": 1770.0, "text": " Most of the other ones, most of the failed ones,", "tokens": [4534, 295, 264, 661, 2306, 11, 881, 295, 264, 7612, 2306, 11], "temperature": 0.0, "avg_logprob": -0.10774368792772293, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.0004165532882325351}, {"id": 377, "seek": 177000, "start": 1770.0, "end": 1772.0, "text": " if you look, are actually installing,", "tokens": [498, 291, 574, 11, 366, 767, 20762, 11], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 378, "seek": 177000, "start": 1772.0, "end": 1775.0, "text": " say, they're installing Python onto iOS.", "tokens": [584, 11, 436, 434, 20762, 15329, 3911, 17430, 13], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 379, "seek": 177000, "start": 1775.0, "end": 1776.0, "text": " I don't know what that means,", "tokens": [286, 500, 380, 458, 437, 300, 1355, 11], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 380, "seek": 177000, "start": 1776.0, "end": 1779.0, "text": " or whether it could ever work, but...", "tokens": [420, 1968, 309, 727, 1562, 589, 11, 457, 485], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 381, "seek": 177000, "start": 1779.0, "end": 1781.0, "text": " Also, the other thing I would say is,", "tokens": [2743, 11, 264, 661, 551, 286, 576, 584, 307, 11], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 382, "seek": 177000, "start": 1781.0, "end": 1783.0, "text": " Pydantic Core is already compiled to WebAssembly,", "tokens": [9953, 67, 7128, 14798, 307, 1217, 36548, 281, 9573, 10884, 19160, 11], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 383, "seek": 177000, "start": 1783.0, "end": 1785.0, "text": " so you can already run it in the browser.", "tokens": [370, 291, 393, 1217, 1190, 309, 294, 264, 11185, 13], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 384, "seek": 177000, "start": 1785.0, "end": 1787.0, "text": " So I understand why people complained,", "tokens": [407, 286, 1223, 983, 561, 33951, 11], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 385, "seek": 177000, "start": 1787.0, "end": 1789.0, "text": " but I think it's not a concern for...", "tokens": [457, 286, 519, 309, 311, 406, 257, 3136, 337, 485], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 386, "seek": 177000, "start": 1789.0, "end": 1791.0, "text": " it's a straw man for most people.", "tokens": [309, 311, 257, 10099, 587, 337, 881, 561, 13], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 387, "seek": 177000, "start": 1791.0, "end": 1793.0, "text": " So that's why you slapped down.", "tokens": [407, 300, 311, 983, 291, 43309, 760, 13], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 388, "seek": 177000, "start": 1793.0, "end": 1794.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 389, "seek": 177000, "start": 1794.0, "end": 1796.0, "text": " And, again, if there's another...", "tokens": [400, 11, 797, 11, 498, 456, 311, 1071, 485], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 390, "seek": 177000, "start": 1796.0, "end": 1798.0, "text": " if there's a distribution that we don't...", "tokens": [498, 456, 311, 257, 7316, 300, 321, 500, 380, 485], "temperature": 0.0, "avg_logprob": -0.1279342979784833, "compression_ratio": 1.6567656765676568, "no_speech_prob": 0.0002009662421187386}, {"id": 391, "seek": 179800, "start": 1798.0, "end": 1800.0, "text": " if we release 60 different binaries,", "tokens": [498, 321, 4374, 4060, 819, 5171, 4889, 11], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 392, "seek": 179800, "start": 1800.0, "end": 1803.0, "text": " if there's another one, we'll try and compile for it", "tokens": [498, 456, 311, 1071, 472, 11, 321, 603, 853, 293, 31413, 337, 309], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 393, "seek": 179800, "start": 1803.0, "end": 1805.0, "text": " and release the binary.", "tokens": [293, 4374, 264, 17434, 13], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 394, "seek": 179800, "start": 1808.0, "end": 1811.0, "text": " There's a question right at the back, I think, just to...", "tokens": [821, 311, 257, 1168, 558, 412, 264, 646, 11, 286, 519, 11, 445, 281, 485], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 395, "seek": 179800, "start": 1813.0, "end": 1815.0, "text": " I'll get back to the talk rather than...", "tokens": [286, 603, 483, 646, 281, 264, 751, 2831, 813, 485], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 396, "seek": 179800, "start": 1815.0, "end": 1817.0, "text": " where are we?", "tokens": [689, 366, 321, 30], "temperature": 0.0, "avg_logprob": -0.16954003209653107, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0003013000823557377}, {"id": 397, "seek": 181700, "start": 1817.0, "end": 1830.0, "text": " Is there a way to use the Django models as Pydantic models?", "tokens": [1119, 456, 257, 636, 281, 764, 264, 33464, 17150, 5245, 382, 9953, 67, 7128, 5245, 30], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 398, "seek": 181700, "start": 1830.0, "end": 1833.0, "text": " Say again?", "tokens": [6463, 797, 30], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 399, "seek": 181700, "start": 1833.0, "end": 1836.0, "text": " To use the Django to have, like, a binding", "tokens": [1407, 764, 264, 33464, 17150, 281, 362, 11, 411, 11, 257, 17359], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 400, "seek": 181700, "start": 1836.0, "end": 1841.0, "text": " or to translate the Django model directly into a Pydantic model?", "tokens": [420, 281, 13799, 264, 33464, 17150, 2316, 3838, 666, 257, 9953, 67, 7128, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 401, "seek": 181700, "start": 1841.0, "end": 1843.0, "text": " There's no way at the moment.", "tokens": [821, 311, 572, 636, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 402, "seek": 181700, "start": 1843.0, "end": 1845.0, "text": " There's a number of different ORMs,", "tokens": [821, 311, 257, 1230, 295, 819, 19654, 26386, 11], "temperature": 0.0, "avg_logprob": -0.1793572809789088, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0010189166059717536}, {"id": 403, "seek": 184500, "start": 1845.0, "end": 1847.0, "text": " I know of, built on top of Pydantic,", "tokens": [286, 458, 295, 11, 3094, 322, 1192, 295, 9953, 67, 7128, 11], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 404, "seek": 184500, "start": 1847.0, "end": 1849.0, "text": " which effectively allow that...", "tokens": [597, 8659, 2089, 300, 485], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 405, "seek": 184500, "start": 1849.0, "end": 1851.0, "text": " if you were wanting specifically Django,", "tokens": [498, 291, 645, 7935, 4682, 33464, 17150, 11], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 406, "seek": 184500, "start": 1851.0, "end": 1853.0, "text": " there's a project called DjangoNinja", "tokens": [456, 311, 257, 1716, 1219, 33464, 17150, 45, 259, 2938], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 407, "seek": 184500, "start": 1853.0, "end": 1855.0, "text": " that makes extensive use of Pydantic.", "tokens": [300, 1669, 13246, 764, 295, 9953, 67, 7128, 13], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 408, "seek": 184500, "start": 1855.0, "end": 1857.0, "text": " I don't know that much about it,", "tokens": [286, 500, 380, 458, 300, 709, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 409, "seek": 184500, "start": 1857.0, "end": 1859.0, "text": " but if you actually wanted Pydantic models,", "tokens": [457, 498, 291, 767, 1415, 9953, 67, 7128, 5245, 11], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 410, "seek": 184500, "start": 1859.0, "end": 1862.0, "text": " you'd probably want some kind of code reformat to convert them.", "tokens": [291, 1116, 1391, 528, 512, 733, 295, 3089, 8290, 267, 281, 7620, 552, 13], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 411, "seek": 184500, "start": 1862.0, "end": 1864.0, "text": " So I look at DjangoNinja,", "tokens": [407, 286, 574, 412, 33464, 17150, 45, 259, 2938, 11], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 412, "seek": 184500, "start": 1864.0, "end": 1867.0, "text": " I'm sure what they're doing is the best of what's possible right now.", "tokens": [286, 478, 988, 437, 436, 434, 884, 307, 264, 1151, 295, 437, 311, 1944, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.09567458289010185, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.0003809477493632585}, {"id": 413, "seek": 186700, "start": 1867.0, "end": 1869.0, "text": " Okay, thank you.", "tokens": [1033, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 414, "seek": 186700, "start": 1873.0, "end": 1877.0, "text": " If you had additional time, say, after finishing Pydantic,", "tokens": [759, 291, 632, 4497, 565, 11, 584, 11, 934, 12693, 9953, 67, 7128, 11], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 415, "seek": 186700, "start": 1877.0, "end": 1881.0, "text": " are there any other projects where you'd like to follow this vision", "tokens": [366, 456, 604, 661, 4455, 689, 291, 1116, 411, 281, 1524, 341, 5201], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 416, "seek": 186700, "start": 1881.0, "end": 1885.0, "text": " of, like, a Rust core with Python user space or, like, API?", "tokens": [295, 11, 411, 11, 257, 34952, 4965, 365, 15329, 4195, 1901, 420, 11, 411, 11, 9362, 30], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 417, "seek": 186700, "start": 1885.0, "end": 1888.0, "text": " Yeah, there are a number of ones.", "tokens": [865, 11, 456, 366, 257, 1230, 295, 2306, 13], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 418, "seek": 186700, "start": 1888.0, "end": 1891.0, "text": " So there's already OR JSON, which is a very, very fast", "tokens": [407, 456, 311, 1217, 19654, 31828, 11, 597, 307, 257, 588, 11, 588, 2370], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 419, "seek": 186700, "start": 1891.0, "end": 1896.0, "text": " if unsafe in the sense of littered with unsafe JSON parser,", "tokens": [498, 35948, 294, 264, 2020, 295, 26540, 292, 365, 35948, 31828, 21156, 260, 11], "temperature": 0.0, "avg_logprob": -0.1397538091622147, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.00027636776212602854}, {"id": 420, "seek": 189600, "start": 1896.0, "end": 1898.0, "text": " which is very, very fast.", "tokens": [597, 307, 588, 11, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 421, "seek": 189600, "start": 1898.0, "end": 1901.0, "text": " The obvious one is a web framework where you do,", "tokens": [440, 6322, 472, 307, 257, 3670, 8388, 689, 291, 360, 11], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 422, "seek": 189600, "start": 1901.0, "end": 1903.0, "text": " like I kind of showed here,", "tokens": [411, 286, 733, 295, 4712, 510, 11], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 423, "seek": 189600, "start": 1903.0, "end": 1907.0, "text": " like the HTTP parsing, the routing, all in Rust.", "tokens": [411, 264, 33283, 21156, 278, 11, 264, 32722, 11, 439, 294, 34952, 13], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 424, "seek": 189600, "start": 1907.0, "end": 1911.0, "text": " That's not very easy using ASGI.", "tokens": [663, 311, 406, 588, 1858, 1228, 7469, 26252, 13], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 425, "seek": 189600, "start": 1911.0, "end": 1914.0, "text": " There are already a few projects doing that.", "tokens": [821, 366, 1217, 257, 1326, 4455, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 426, "seek": 189600, "start": 1914.0, "end": 1917.0, "text": " So that would be the obvious one, but there's no winner yet.", "tokens": [407, 300, 576, 312, 264, 6322, 472, 11, 457, 456, 311, 572, 8507, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 427, "seek": 189600, "start": 1917.0, "end": 1921.0, "text": " Currently, the best low-level web framework is Starlit,", "tokens": [19964, 11, 264, 1151, 2295, 12, 12418, 3670, 8388, 307, 5705, 23062, 11], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 428, "seek": 189600, "start": 1921.0, "end": 1923.0, "text": " which FastAPI is built on,", "tokens": [597, 15968, 4715, 40, 307, 3094, 322, 11], "temperature": 0.0, "avg_logprob": -0.1199067234992981, "compression_ratio": 1.5738396624472575, "no_speech_prob": 4.560130764730275e-05}, {"id": 429, "seek": 192300, "start": 1923.0, "end": 1926.0, "text": " but I think it does use Rust for, it uses a Rust library", "tokens": [457, 286, 519, 309, 775, 764, 34952, 337, 11, 309, 4960, 257, 34952, 6405], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 430, "seek": 192300, "start": 1926.0, "end": 1930.0, "text": " for HTTP parsing or a C library.", "tokens": [337, 33283, 21156, 278, 420, 257, 383, 6405, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 431, "seek": 192300, "start": 1930.0, "end": 1934.0, "text": " So some of it's already happening, but no obvious candidate right now.", "tokens": [407, 512, 295, 309, 311, 1217, 2737, 11, 457, 572, 6322, 11532, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 432, "seek": 192300, "start": 1936.0, "end": 1939.0, "text": " What I would say, though, is libraries like Rich,", "tokens": [708, 286, 576, 584, 11, 1673, 11, 307, 15148, 411, 6781, 11], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 433, "seek": 192300, "start": 1939.0, "end": 1942.0, "text": " no criticism of Will, but, like, Rich is incredibly complicated.", "tokens": [572, 15835, 295, 3099, 11, 457, 11, 411, 11, 6781, 307, 6252, 6179, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 434, "seek": 192300, "start": 1942.0, "end": 1944.0, "text": " It's for terminal output.", "tokens": [467, 311, 337, 14709, 5598, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 435, "seek": 192300, "start": 1944.0, "end": 1946.0, "text": " It's not so much performance critical,", "tokens": [467, 311, 406, 370, 709, 3389, 4924, 11], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 436, "seek": 192300, "start": 1946.0, "end": 1948.0, "text": " but it's really quite involved in complex logic.", "tokens": [457, 309, 311, 534, 1596, 3288, 294, 3997, 9952, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 437, "seek": 192300, "start": 1948.0, "end": 1951.0, "text": " I would much prefer to write that logic in Rust than Python.", "tokens": [286, 576, 709, 4382, 281, 2464, 300, 9952, 294, 34952, 813, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10492134882398874, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.188684346852824e-05}, {"id": 438, "seek": 195100, "start": 1951.0, "end": 1954.0, "text": " Yeah, I think there are lots of candidates.", "tokens": [865, 11, 286, 519, 456, 366, 3195, 295, 11255, 13], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 439, "seek": 195100, "start": 1959.0, "end": 1961.0, "text": " Tonya online is asking,", "tokens": [10902, 64, 2950, 307, 3365, 11], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 440, "seek": 195100, "start": 1961.0, "end": 1965.0, "text": " what do you mean by Python as the application layer?", "tokens": [437, 360, 291, 914, 538, 15329, 382, 264, 3861, 4583, 30], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 441, "seek": 195100, "start": 1965.0, "end": 1970.0, "text": " So I guess I could have added some example code here,", "tokens": [407, 286, 2041, 286, 727, 362, 3869, 512, 1365, 3089, 510, 11], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 442, "seek": 195100, "start": 1970.0, "end": 1975.0, "text": " but you can imagine a Python function,", "tokens": [457, 291, 393, 3811, 257, 15329, 2445, 11], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 443, "seek": 195100, "start": 1975.0, "end": 1978.0, "text": " which is a view endpoint in a web framework,", "tokens": [597, 307, 257, 1910, 35795, 294, 257, 3670, 8388, 11], "temperature": 0.0, "avg_logprob": -0.12029213972494636, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0001175195284304209}, {"id": 444, "seek": 197800, "start": 1978.0, "end": 1981.0, "text": " which takes in some validated arguments", "tokens": [597, 2516, 294, 512, 40693, 12869], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 445, "seek": 197800, "start": 1981.0, "end": 1983.0, "text": " from done by the Pydantic.", "tokens": [490, 1096, 538, 264, 9953, 67, 7128, 13], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 446, "seek": 197800, "start": 1983.0, "end": 1987.0, "text": " You then decide in Python to make a query to the database", "tokens": [509, 550, 4536, 294, 15329, 281, 652, 257, 14581, 281, 264, 8149], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 447, "seek": 197800, "start": 1987.0, "end": 1989.0, "text": " to get back the user's name from the ID,", "tokens": [281, 483, 646, 264, 4195, 311, 1315, 490, 264, 7348, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 448, "seek": 197800, "start": 1989.0, "end": 1993.0, "text": " and then you return a JSON object containing data about the user.", "tokens": [293, 550, 291, 2736, 257, 31828, 2657, 19273, 1412, 466, 264, 4195, 13], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 449, "seek": 197800, "start": 1993.0, "end": 1995.0, "text": " If you think about that,", "tokens": [759, 291, 519, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 450, "seek": 197800, "start": 1995.0, "end": 1998.0, "text": " all of the code outside the Python functions,", "tokens": [439, 295, 264, 3089, 2380, 264, 15329, 6828, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 451, "seek": 197800, "start": 1998.0, "end": 2001.0, "text": " excuse me, could be written in a faster language,", "tokens": [8960, 385, 11, 727, 312, 3720, 294, 257, 4663, 2856, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 452, "seek": 197800, "start": 2001.0, "end": 2003.0, "text": " whether it be the database query accessing the database,", "tokens": [1968, 309, 312, 264, 8149, 14581, 26440, 264, 8149, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 453, "seek": 197800, "start": 2003.0, "end": 2007.0, "text": " TSL termination, HTTP parsing, routing, validation,", "tokens": [37645, 43, 1433, 2486, 11, 33283, 21156, 278, 11, 32722, 11, 24071, 11], "temperature": 0.0, "avg_logprob": -0.12192520425339376, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.00015148888633120805}, {"id": 454, "seek": 200700, "start": 2007.0, "end": 2011.0, "text": " but effectively using Python to define as a way", "tokens": [457, 8659, 1228, 15329, 281, 6964, 382, 257, 636], "temperature": 0.0, "avg_logprob": -0.10546341277005379, "compression_ratio": 1.4861111111111112, "no_speech_prob": 8.256172441178933e-05}, {"id": 455, "seek": 200700, "start": 2011.0, "end": 2015.0, "text": " to effectively configure Rust code or configure compile code.", "tokens": [281, 8659, 22162, 34952, 3089, 420, 22162, 31413, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10546341277005379, "compression_ratio": 1.4861111111111112, "no_speech_prob": 8.256172441178933e-05}, {"id": 456, "seek": 200700, "start": 2027.0, "end": 2028.0, "text": " Yes, hello.", "tokens": [1079, 11, 7751, 13], "temperature": 0.0, "avg_logprob": -0.10546341277005379, "compression_ratio": 1.4861111111111112, "no_speech_prob": 8.256172441178933e-05}, {"id": 457, "seek": 200700, "start": 2028.0, "end": 2032.0, "text": " I have a question, just a Pydantic one.", "tokens": [286, 362, 257, 1168, 11, 445, 257, 9953, 67, 7128, 472, 13], "temperature": 0.0, "avg_logprob": -0.10546341277005379, "compression_ratio": 1.4861111111111112, "no_speech_prob": 8.256172441178933e-05}, {"id": 458, "seek": 200700, "start": 2032.0, "end": 2035.0, "text": " Is there any support or are you planning any support", "tokens": [1119, 456, 604, 1406, 420, 366, 291, 5038, 604, 1406], "temperature": 0.0, "avg_logprob": -0.10546341277005379, "compression_ratio": 1.4861111111111112, "no_speech_prob": 8.256172441178933e-05}, {"id": 459, "seek": 203500, "start": 2035.0, "end": 2037.0, "text": " alternative schema types like Protobuf,", "tokens": [8535, 34078, 3467, 411, 10019, 996, 2947, 11], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 460, "seek": 203500, "start": 2037.0, "end": 2040.0, "text": " or JRPC, or Avro?", "tokens": [420, 508, 49, 12986, 11, 420, 11667, 340, 30], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 461, "seek": 203500, "start": 2040.0, "end": 2042.0, "text": " Possibly in future.", "tokens": [33112, 3545, 294, 2027, 13], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 462, "seek": 203500, "start": 2042.0, "end": 2044.0, "text": " What I have a plan for is,", "tokens": [708, 286, 362, 257, 1393, 337, 307, 11], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 463, "seek": 203500, "start": 2044.0, "end": 2046.0, "text": " I don't want to build them into Pydantic.", "tokens": [286, 500, 380, 528, 281, 1322, 552, 666, 9953, 67, 7128, 13], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 464, "seek": 203500, "start": 2046.0, "end": 2048.0, "text": " Pydantic's already big, but there is a,", "tokens": [9953, 67, 7128, 311, 1217, 955, 11, 457, 456, 307, 257, 11], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 465, "seek": 203500, "start": 2048.0, "end": 2050.0, "text": " obviously you can parse them to Python now,", "tokens": [2745, 291, 393, 48377, 552, 281, 15329, 586, 11], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 466, "seek": 203500, "start": 2050.0, "end": 2052.0, "text": " parse them and then validate them as a Python object.", "tokens": [48377, 552, 293, 550, 29562, 552, 382, 257, 15329, 2657, 13], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 467, "seek": 203500, "start": 2052.0, "end": 2056.0, "text": " There is a plan effectively to take the,", "tokens": [821, 307, 257, 1393, 8659, 281, 747, 264, 11], "temperature": 0.0, "avg_logprob": -0.18584534928605362, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.0002666442305780947}, {"id": 468, "seek": 205600, "start": 2056.0, "end": 2072.0, "text": " this, that's the one.", "tokens": [341, 11, 300, 311, 264, 472, 13], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 469, "seek": 205600, "start": 2072.0, "end": 2074.0, "text": " Which you would then construct in Rust,", "tokens": [3013, 291, 576, 550, 7690, 294, 34952, 11], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 470, "seek": 205600, "start": 2074.0, "end": 2077.0, "text": " parse as a Python value into Pydantic core,", "tokens": [48377, 382, 257, 15329, 2158, 666, 9953, 67, 7128, 4965, 11], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 471, "seek": 205600, "start": 2077.0, "end": 2080.0, "text": " which would then extract the raw underlying Rust instance", "tokens": [597, 576, 550, 8947, 264, 8936, 14217, 34952, 5197], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 472, "seek": 205600, "start": 2080.0, "end": 2082.0, "text": " and then validate that.", "tokens": [293, 550, 29562, 300, 13], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 473, "seek": 205600, "start": 2082.0, "end": 2084.0, "text": " And that would allow you to get basically", "tokens": [400, 300, 576, 2089, 291, 281, 483, 1936], "temperature": 0.0, "avg_logprob": -0.2000069925861974, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.0001158859595307149}, {"id": 474, "seek": 208400, "start": 2084.0, "end": 2086.0, "text": " a Python validation effectively,", "tokens": [257, 15329, 24071, 8659, 11], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 475, "seek": 208400, "start": 2086.0, "end": 2088.0, "text": " but without having to,", "tokens": [457, 1553, 1419, 281, 11], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 476, "seek": 208400, "start": 2088.0, "end": 2090.0, "text": " us having to either have compile time dependencies", "tokens": [505, 1419, 281, 2139, 362, 31413, 565, 36606], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 477, "seek": 208400, "start": 2090.0, "end": 2094.0, "text": " or build it all into Pydantic core.", "tokens": [420, 1322, 309, 439, 666, 9953, 67, 7128, 4965, 13], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 478, "seek": 208400, "start": 2094.0, "end": 2097.0, "text": " I think that's our last question that we have time for.", "tokens": [286, 519, 300, 311, 527, 1036, 1168, 300, 321, 362, 565, 337, 13], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 479, "seek": 208400, "start": 2097.0, "end": 2101.0, "text": " One comment we did get from Matrix was that", "tokens": [1485, 2871, 321, 630, 483, 490, 36274, 390, 300], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 480, "seek": 208400, "start": 2101.0, "end": 2104.0, "text": " this code is a bit small on the,", "tokens": [341, 3089, 307, 257, 857, 1359, 322, 264, 11], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 481, "seek": 208400, "start": 2104.0, "end": 2106.0, "text": " on the display, so if you upload the,", "tokens": [322, 264, 4674, 11, 370, 498, 291, 6580, 264, 11], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 482, "seek": 208400, "start": 2106.0, "end": 2108.0, "text": " I will do, yeah.", "tokens": [286, 486, 360, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 483, "seek": 208400, "start": 2108.0, "end": 2109.0, "text": " Perfect.", "tokens": [10246, 13], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 484, "seek": 208400, "start": 2109.0, "end": 2111.0, "text": " So if you're watching the stream,", "tokens": [407, 498, 291, 434, 1976, 264, 4309, 11], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 485, "seek": 208400, "start": 2111.0, "end": 2113.0, "text": " the slides will be uploaded and you can read the code.", "tokens": [264, 9788, 486, 312, 17135, 293, 291, 393, 1401, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.16550620784604453, "compression_ratio": 1.6653696498054475, "no_speech_prob": 0.0008026044815778732}, {"id": 486, "seek": 211300, "start": 2113.0, "end": 2115.0, "text": " Oh, I'll put them on Twitter as well, but yeah, definitely.", "tokens": [876, 11, 286, 603, 829, 552, 322, 5794, 382, 731, 11, 457, 1338, 11, 2138, 13], "temperature": 0.0, "avg_logprob": -0.16489322185516359, "compression_ratio": 1.145631067961165, "no_speech_prob": 0.00036365524283610284}, {"id": 487, "seek": 211300, "start": 2115.0, "end": 2116.0, "text": " I'll upload them as well.", "tokens": [286, 603, 6580, 552, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16489322185516359, "compression_ratio": 1.145631067961165, "no_speech_prob": 0.00036365524283610284}, {"id": 488, "seek": 211300, "start": 2116.0, "end": 2117.0, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.16489322185516359, "compression_ratio": 1.145631067961165, "no_speech_prob": 0.00036365524283610284}, {"id": 489, "seek": 211700, "start": 2117.0, "end": 2146.0, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15177425742149353, "compression_ratio": 0.7142857142857143, "no_speech_prob": 5.1195536798331887e-05}], "language": "en"}