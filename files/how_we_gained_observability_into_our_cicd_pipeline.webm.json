{"text": " So, I hope it will be fun enough for you to wake up at the end of the day and very excited to be here at FOSDEM and specifically at the CI CD Dev Room. And today I'd like to share with you about how we gained observability into our CI CD pipeline and how you can do too. So let's start with a day in the life of a DoD developer on duty, at least in my company. And it goes like that. So the first thing the DoD does in the morning, at least it used to be before we did this exercise, is going into the Jenkins. We worked with Jenkins, but the takeaways, by the way, will be very applicable to any other system you work with, so nothing too specific here. Going into Jenkins at the beginning of the morning, we're looking at the status there, the pipelines for the last few hours over the night, and of course checking if anything is red, and most importantly, if there's a red master. And if you can obviously finish your coffee or jump straight into the investigation. And to be honest, sometimes people actually forgot to go into the Jenkins and check this, so that's another topic we'll maybe touch upon. So you go in, and then you need to go, let's say you see a failure, you see something red, you need to start going one by one on the different runs, and start figuring out, understanding what failed, where it failed, why it failed, and so on. And it's important that you actually, you needed to go one by one on the different runs, and we have several runs, we have the backend, we have the app, we have smoke tests, several of these, and start getting the picture, getting the pattern across, and understanding cross runs, across branches, what's going on. And on top of all of that, it was very difficult to compare with historical behavior, with the past behavior, to understand what's, and anomaly, what's the steady state for these days, and so on. So, and just to give you a few examples of questions that we found it difficult or time-consuming to answer, things such as, did all runs fail on the same step, did all runs fail for the same reason, is that on a specific branch, is that on a specific machine, if something's taking longer, is that normal, is that anomalous, what's the benchmark? And so these sorts of questions, it took us too long to answer, and we realized we need to improve. A word about myself, my name is Dotan Horvitz, I'm the Principal Developer Advocate at a company called Logs.io, Logs.io provides a cloud-native observability platform that's built on popular open-source tools such as you probably know, Prometheus, OpenSearch, OpenTelemetry, Yeager, and others. I come from a background as a developer, a solutions architect, even a product manager, and most importantly, I'm an advocate of open-source and communities. I run a podcast called Open Observability Talks about open-source DevOps observability, so if you're interested in these topics and you like podcasts, do check it out. I also run, organize, co-organize several communities, the local chapter of the CNCF, the cloud-native computing foundation in Tel Aviv, Kubernetes Community Days, DevOps Days, et cetera, and you can find me everywhere at Horvitz. So if you have something interesting, you tweet, feel free to tag me. So before I get into how we improved our CI CD pipeline or capabilities, let's first understand what we want to improve on. And actually, I see very often that people jump into solving before really understanding the metric, the KPI that they want to improve, and very basically, therefore, primary metrics for, let's say, DevOps performance, and you can see there on the screen, there's the deployment frequency, lead time for changes, change failure rate, and MPTR, mean time to recovery. I don't have time to go over all of these, but very important, so if you're new to this and if you want to read a bit more about that, I left a QR code and a short link for you at the bottom for a 101 on the Dora metrics, do check it out, I think it's priceless. And in our case, we needed to improve on the lead time for changes or sometimes called cycle time, which is the amount of time it takes a commit to get into production, which in our case was the time was too long, too high, and was holding us back. So we are experts at observability in our engineering team. That's what we do for a living, so it was very clear to us that what we're missing in our case is observability into our CICD pipeline. And to be fair with Jenkins, and there are lots of things to complain about Jenkins, but there is some capabilities within Jenkins. You can go into a specific pipeline run, you can see the different steps, you can see how much time an individual step took. Using some plugins, you can also visualize the graph and we even wired Jenkins to get alerts on Slack, but that wasn't good enough for us. And the reason that we wanted to find a way to monitor aggregated and filtered information according to our own timescale, according to our own filters, obviously to see things across branches, across runs, to compare with historical data, with our own filtering, so that's where we aimed at. And we launched this internal project with these requirements, four requirements. One, first and foremost, as we need the dashboard, we need the dashboard with aggregated views to be able to see the aggregated data across pipelines, across runs, across branches as we talked about. Finally we wanted to have access to historical data to be able to compare, to understand trends, to identify patterns, anomalies, and so on. Thirdly, we wanted reports and alerts to be able to automate as much as possible. And lastly, we wanted some ability to view flaky tests, test performance, and to be able to understand their impact on the pipeline. So that was the project requirements and how we did that. Essentially it takes four steps, collect, store, visualize, and report. And I'll show you exactly how it's done and what each step entails. In terms of the tech stack, we were very versed with the Elk stack, Elasticsearch, Kabbana. Then we also switched over to OpenSearch and OpenSearch dashboards after Elastic re-licensed and it was no longer open source. So that was our natural point to start our observability journey. And I'll show you how we did these four steps with this tech stack. So the first step is collect. And for that we instrumented the pipeline to collect all the relevant information and put it in environment variables. Which information, you can see some examples here on the screen, the branch, the Kamecha, the machine IP, the run type, whether it's scheduled, triggered by merge to master or something else, fail step, step duration, build number, anything essentially that you find useful for investigation later. My recommendation, collect it and persist it. So that's the collect phase and after collect comes store. And for that we created a new summary step at the end of the pipeline one where we ran a command to collect all of that information that we did in the first step and created a JSON and persisted it to Elasticsearch, as I mentioned then move to OpenSearch. And it's important to say again for the fairness of Jenkins and for the Jenkins experts here, Jenkins does have some built in persistency capabilities. And we tried them out, but it wasn't good enough for us. And the reason is that by default Jenkins essentially keeps all the bills and stores them on the Jenkins machine, which burdens these machines of course. And then you start needing to limit the number of bills and the duration, how many days and so on and so forth. So that wasn't good enough for us. We needed a more powerful access to historical data. We wanted to persist historical data in our own control, the duration, the retention and most importantly off of the Jenkins servers so as not to risk and overload the critical path. So that's about store and after store. Once we have all the data in Elasticsearch or OpenSearch, now it's very easy to build command dashboards or OpenSearch dashboards and visualizations on top of that. And then comes the question, sorry, then comes the question, okay, so which visualizations should I build? And for that, and that's a tip, take it with you, go back to the pains, go back to the questions that you found it hard to answer and this would be the starting point. So if you remember before we mentioned things such as did all runs fail on the same step, did all runs fail for the same reason, how many fail, is that a specific branch, is that a specific machine and so on, these are the questions that we guide you then to choose the right visualizations for your dashboard. And I'll give you some examples here. So let's start with the top line view. You want to understand the health of your house table, your pipeline is. So visualize the success and failure rates, you can do that overall in general or at a specific time window on a graph, very easy to see the first glance, what's the health status of your pipeline. You want to find problematic steps, then visualize failures segmented by pipeline steps, again very easy to see the spiking step there. You want to detect problematic build machines, visualize failures segmented by machine and that by the way saved us a lot of wasted time going and checking bugs in the release code. When we saw such a thing, we just go, you kill the machine, you let the auto scaler spin up a new instance and you start clean and in many cases it solves the problem. So lots of time saved, in general this aspect of code based or environmental based issues is definitely a challenge I'm assuming, not just for me, so I'll get back to that soon. Another example duration per step, again very easy to see where and at the time is spent. So that's the visualize part and after visualize comes the reporting and alerting phase. And if you remember before the DOD, the developer on duty, needed to go manually and check Jenkins and then the health check, now the DOD gets start of day report directly to Slack and actually as you can see the report already contains the link to the dashboard and even a snapshot of the dashboard embedded within the Slack so that at the first glance even without going into the dashboard you can see if you can finish your coffee or if there's something alerting that you need to click that link and go start investigating. And of course it doesn't have to be a schedule report, it could be also you can define triggered alerts on any of that, the fields, the data that we collected in the first phase and the collect phase so and you can do any complex queries or conditions that you want, you want to do something like if the sum of failures goes above x or the average duration goes above y trigger an alert. So essentially anything that you can formalize as a Lucene query, you can automate as an alert and that's some alerting layer that we built on top of elastic search and open search for that. One last note, I'm giving the examples from Slack because that's what we use in our environment but you're not limited obviously to Slack, you have support for many notification endpoints depending on your systems, pager duty, victorops, ops genie, MS themes, whatever. We personally work with Slack so that the examples are with Slack. So that's how we build observability into the Jenkins pipelines but as we all know especially here in the CI CD dev room, Jenkins, CI CD is much more than just Jenkins. So what else? So we wanted to analyze if you remember the original requirements to analyze flaky tests and test performance and following the same process, collecting all the relevant information from test run and storing it in elastic search and open search and then creating a cabana dashboard or open search dashboards and as you can see very all the relevant usual suspects that you'd expect, the test duration, fail test, flaky test, failure count and rate moving averages, fail test by branch over time, all of the things that you would need in order to analyze and understand the impact of your test and the flaky tests in your system. And similarly after visualize you can also report, we created reports to Slack, we have a dedicated Slack channel for that, following the same pattern. One important point is about the openness. So once you have the data in open search or in elastic search, it's very easy for different teams to create different visualizations on top of that same data. So I took another extreme, a different team that didn't like the graphs and preferred the table views and the counters to visualize, again, very similarly, test stats and so on. And that's the beauty of it. So just to summarize, we instrumented Jenkins pipeline to collect relevant data and put it in environment variables, then at the end of the pipeline we created a JSON with all this data and persisted it to elastic search open search, then we created Kibana dashboards on top of that data and lastly we created reports and alerts on that data. So four steps, collect, store, visualize and report. So that was our first step in the journey but we didn't stop there. The next step was we asked ourselves, what can we do in order to investigate performance of specific pipeline runs? So you have a run that takes a lot of time, you want to optimize, but where is the problem? And that's actually what distributed tracing is ideal for. How many people know what distributed tracing is with a show of hands? Okay, I see that most of us, there are a few that know, so maybe I'll say a word about that soon. Very importantly, Jenkins has the capability to emit trace data spans, just like it does for logs, so it's already built in. So we decided to visualize jobs and pipeline executions as distributed tracing. That was the next step. And for those who don't know, distributed tracing essentially helps pinpoint where issues occur and where latency is in production environments, in distributed systems, it's not specific for CICD. If you think about a microservice architecture and a request coming in and flowing through a chain of interacting microservices, then when something goes wrong, you get an error on that request, you want to know where the error is within this chain, or if there's a latency, you want to know where the latency is. That's distributed tracing in a nutshell. And the way it works is that each step in this call chain, or in our case, each step in the pipeline, creates and emits a span. You can think about a span as a structured log that also contains the trace ID, the start time, the duration, and some other context. And then there is a back end that collects all these spans, reconstruct the trace, and then visualizes it typically in this timeline view or gun chart that you can see on the right-hand side. So now that we understand the distributed tracing, let's see how we add distributed tracing type of performance, pipeline performance into a CICD pipeline. And same process. For the collect step, collect. And for the collect step, we decided to use an open telemetry collector who doesn't know about open telemetry, who doesn't know the project, just so I have a background, okay. I have a few, so I'll say a word about that. And anyway, I added a link, you see a QR code and a link at the lower corner there for a beginner's guide to open telemetry that I wrote. I gave a talk about open telemetry at KubeCon Europe, so you'll find it useful. But very briefly, it's an observability platform for collecting logs, metrics, and traces. So it's not specific only to traces in an open unified standard manner. It's an open source project under the CNCF, the Cloud Native Computing Foundation. And at the time, it's a fairly young project by the time, the tracing piece of open telemetry was already GA generally available, so we decided to go with that. Today, by the way, also metrics is soon to be GA, it's already in release candidate, and logging is still not there. So what do you need to do if you choose open telemetry? You need to set up the open telemetry collector, it's sort of an agent for it to send. You need to install the Jenkins open telemetry plug-in, very easy to do that on the UI. And then you need to configure the Jenkins open telemetry plug-in to send to the open telemetry collector and point over OTLP over GRPC protocol. That's the collect phase, and after collect comes store. For the back end, we used Jega. Jega is also a very popular open source under the CNCF, specifically for distributed tracing. And we use Jega to monitor our own production environment, so that was our natural choice also for this. We also have a Jager-based service, so we just use that. But anything that I show here, actually you can use with any Jager distro, whichever one you use, managed or self-serve. And if you do run your own, by the way, I added the link on how to deploy Jager on Kubernetes in production, so you have a link there, a short link that I added, a very useful guide. So what do you need to do? You need to configure open telemetry collector to export in open telemetry collector terms to export to Jager in the right format, all the aggregated information. And once you have that, then you can visualize, the visualized part is much easier in this case, because you have a Jager UI with predefined dashboard, you don't need to start composing visuals. Essentially, what you can see here on the left-hand side, you can see this indented tree structure, and then on the right, the gun chart. Each line here is a span, and it's very easy to see the pipeline sequence. The text is a bit small, but you can see, for each step of the pipeline, you can see the duration, how much it took, you see which ones ran in parallel, and which ones ran sequentially. If you have a very long latency on the overall, you can see where most of the time is being spent, where the critical path, where you best optimize, and so on. And by the way, Jager also offers other views, like recently added the flame graph, and you have trace statistics, and graph view, and so on. But this is what people are used to, so I'm showing the timeline view. So that's on Jager, and of course, as we said before, CICD is more than just Jenkins, so what we can do beyond just Jenkins, and what you can do is actually to instrument additional pieces like Maven, Ansible, and other elements to get final granularity into your traces and steps. For example, here, the things that you see in yellow is Maven build steps. So what before used to be one black box span in the trace. Suddenly, now you can click, open, and see the different build steps, each one with its own duration, each one with its own context, and so on. So that's in a nutshell how we added tracing to our CICD pipeline. The next step is, as I mentioned before, many of the pipelines actually failed not because of the released code, but because of the CICD environment. So we decided to monitor metrics from the Jenkins servers and the environment. It goes to the system, the containers, the JVM, essentially anything that could break irrespective of the released code, and following the same flow. So the first step, collect, we use the telegraph, we use that in production, so we use that here as well, that's an open source by inflex data, and essentially you need two steps. You need to first enable, configure, sorry, Jenkins to expose metrics in Prometheus format. We work a lot with Prometheus for metrics, so that was our natural choice, and that's a simple configuration in the Jenkins web UI, and then you need to install telegraph if you don't already have that, and then make sure that it configured to scrape the metrics off of the Jenkins server using the Prometheus input plugin. So that's the first step. The second step is on the store side. As I mentioned, we use Prometheus for metrics, so we use that as well here. We even have our own managed Prometheus, so we use that, but anything that I show here is identical whether you use Prometheus or any Prometheus compatible backend. And essentially you need to configure telegraph to send the metrics to Prometheus, and you have two ways to do that. You can do that in pull mode or in push mode. So pull mode is the default for Prometheus, essentially when you configure a telegraph to expose a slash metrics endpoint, and then it can be exposed for Prometheus to scrape it from. If you want to do that, you use the Prometheus client output plugin, or if you want to do it in push mode, then you use the HTTP output plugin. Just an important note, make sure that you set the data format to Prometheus remote write. So that's the store phase, and then once you have all the data in Prometheus, then it's very easy to create Grafana dashboards on top of that. And I gave some examples here. You can filter, of course, by build type, by branch, machine ID, build number, and so on. And you can monitor in this example, this is a system monitoring, so CPU, memory, disk usage, load, and so on. You can monitor the Docker container, like the CPU, IO, inbound, outbound, disk usage, obviously the running, stopped, paused containers by Jenkins machine, everything that you'd expect, and JVM metrics, by being a Java implementation, thread count, heap memory, garbage collection, duration, things like that. You can even, of course, monitor the Jenkins nodes, queues, executors themselves. So again, you have an example dashboard here. You can see the queue size, status breakdown, the Jenkins jobs, the count executed over time, breakdown by job status, and so on. So this is the types, just to, obviously, lots of other visualizations that you can create, and you can also create alerts. I won't show that in the lack of time, so just to summarize what we've seen. Treat your CICD the same as you treat your production. For your production, use whatever, elastic search, open search, Grafana to monitor to create observability. Do the same with your CICD pipeline, and preferably leverage the same stack, the same tool chain for that, and don't reinvent the wheel. That was our journey. As I mentioned, we wanted dashboards and aggregated views to see several pipelines across different run branches over time, and so on. We wanted historical data and controlled persistence off of the Jenkins servers to determine the duration, the retention of that data. We wanted reports and alerts to automate as much as possible, and lastly, we wanted test performance, flaky tests, and so on. You saw how we achieved that. Four steps. If there's one thing to take out of that talk, take this one, collect, store, visualize, and report an alert. And what we gained, just to summarize, significant improvement in our lead time for changes, in our cycle time, if you remember the Dora metrics at the beginning. On the way, we also got an improved developer-on-duty experience, much less of a sufferer there. It's based on open source. Very important. We're here on FOSDEM. So based on open search, open telemetry, Yeager, Prometheus, Telegraph, you saw the stack. If you want more information, you have here a QR code for a guide to CICD observability that I wrote. You're welcome to take a short or a bit short link and read more about this, but this was very much in a nutshell. Thank you very much for listening. I'm Doton Horvitz, and enjoy the rest of the conference. I don't know if we have time for questions. No. So I'm here if you have questions or if you have a sticker, and may the open source be with you. Thank you. We have time for questions, if there are any. We have time for questions, so if you want, we can just see for a few minutes. Is that a question? Yeah, the other question in the back. Okay. Which one do you want to be the first one to ask a question? Thanks. So have you considered persistence? How long do you store your metrics and your traces? Have you wondered about that? And for how long at a time you store your metrics? So we have. That was part of the original challenge when we used the Jenkins persistence, because when you persist it on the nodes themselves, and obviously you're very limited, there's the plugin that you can configure per days or per number of bills and so on. When you do it off of that critical path, you have much more room to maneuver, and then it depends on the amount of data you collect. We started small, so we collected for longer periods, but the more it came with the app, the more the appetite grew, and people wanted more and more types of metrics and time series data, so we needed to be a bit more conservative, but it's very much dependent on your practices in terms of the data. Yeah, the question was more about the process, so iterative, you explained it, so it starts small. Yeah, exactly. And iterative is the best, because it really depends, you need to learn the patterns of your data consumption, the telemetry, and then you can optimize the balance between having the observability and not overloading and overpricing costs. Right. Thank you very, very interesting. Thank you. There was another question in the back, yeah? Thank you. So what was the most surprising insight that you've learned, good or bad, and how did you react to it? I think I was most surprised personally about the amount of failures that occur because of the environment and what kinds of things, and how simple it is to just kill the machine, kill the instance, let the auto-scaler spin it back up, and you save yourself a lot of hassle and a lot of waking people up at night, so that was astonishing. How many things are irrespective of the code and just environmental, and we took a lot of learnings out there to make the environment more robust, to get people to clean after them, to automate the cleanups and things like that, that's what me was insightful. Thank you. Any other questions? Then I have one last one, sorry. No, no worries. My question is, who are usually the people looking at the dashboard, because I maintain a lot of dashboard in the past, and sometimes I had a feeling that I was the only one looking at those dashboards, so I'm just wondering if you identify a type of people who really benefit from those dashboards. So it's a very interesting question because we also learned and we changed the org structure several times, so it moves between Dev and DevOps. We now have a release engineering team, so they are the main stakeholders to look at that, but this dashboard is the goal, as I said, the developer on duty, so everyone that is now on call needs to see that, that's for sure, and the tier two, tier three, so let's say the chain for that. You also use that as a high level also by the team leads in the developer side of things, so these are the main stakeholders, depending on if it's the critical part of the developer on duty and the tiers, or if it's the overall thing the health state in general by the release engineer. Thank you. Thank you very much, everyone.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.6, "text": " So, I hope it will be fun enough for you to wake up at the end of the day and very excited", "tokens": [407, 11, 286, 1454, 309, 486, 312, 1019, 1547, 337, 291, 281, 6634, 493, 412, 264, 917, 295, 264, 786, 293, 588, 2919], "temperature": 0.0, "avg_logprob": -0.26489433684906405, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.06838194280862808}, {"id": 1, "seek": 0, "start": 14.6, "end": 18.92, "text": " to be here at FOSDEM and specifically at the CI CD Dev Room.", "tokens": [281, 312, 510, 412, 479, 4367, 35, 6683, 293, 4682, 412, 264, 37777, 6743, 9096, 19190, 13], "temperature": 0.0, "avg_logprob": -0.26489433684906405, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.06838194280862808}, {"id": 2, "seek": 0, "start": 18.92, "end": 23.84, "text": " And today I'd like to share with you about how we gained observability into our CI CD", "tokens": [400, 965, 286, 1116, 411, 281, 2073, 365, 291, 466, 577, 321, 12634, 9951, 2310, 666, 527, 37777, 6743], "temperature": 0.0, "avg_logprob": -0.26489433684906405, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.06838194280862808}, {"id": 3, "seek": 0, "start": 23.84, "end": 28.12, "text": " pipeline and how you can do too.", "tokens": [15517, 293, 577, 291, 393, 360, 886, 13], "temperature": 0.0, "avg_logprob": -0.26489433684906405, "compression_ratio": 1.4136125654450262, "no_speech_prob": 0.06838194280862808}, {"id": 4, "seek": 2812, "start": 28.12, "end": 36.6, "text": " So let's start with a day in the life of a DoD developer on duty, at least in my company.", "tokens": [407, 718, 311, 722, 365, 257, 786, 294, 264, 993, 295, 257, 1144, 35, 10754, 322, 9776, 11, 412, 1935, 294, 452, 2237, 13], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 5, "seek": 2812, "start": 36.6, "end": 38.120000000000005, "text": " And it goes like that.", "tokens": [400, 309, 1709, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 6, "seek": 2812, "start": 38.120000000000005, "end": 42.400000000000006, "text": " So the first thing the DoD does in the morning, at least it used to be before we did this", "tokens": [407, 264, 700, 551, 264, 1144, 35, 775, 294, 264, 2446, 11, 412, 1935, 309, 1143, 281, 312, 949, 321, 630, 341], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 7, "seek": 2812, "start": 42.400000000000006, "end": 45.400000000000006, "text": " exercise, is going into the Jenkins.", "tokens": [5380, 11, 307, 516, 666, 264, 41273, 13], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 8, "seek": 2812, "start": 45.400000000000006, "end": 51.88, "text": " We worked with Jenkins, but the takeaways, by the way, will be very applicable to any", "tokens": [492, 2732, 365, 41273, 11, 457, 264, 45584, 11, 538, 264, 636, 11, 486, 312, 588, 21142, 281, 604], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 9, "seek": 2812, "start": 51.88, "end": 57.28, "text": " other system you work with, so nothing too specific here.", "tokens": [661, 1185, 291, 589, 365, 11, 370, 1825, 886, 2685, 510, 13], "temperature": 0.0, "avg_logprob": -0.16917920793805805, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0004890749114565551}, {"id": 10, "seek": 5728, "start": 57.28, "end": 62.64, "text": " Going into Jenkins at the beginning of the morning, we're looking at the status there,", "tokens": [10963, 666, 41273, 412, 264, 2863, 295, 264, 2446, 11, 321, 434, 1237, 412, 264, 6558, 456, 11], "temperature": 0.0, "avg_logprob": -0.18334999490291515, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0007927615661174059}, {"id": 11, "seek": 5728, "start": 62.64, "end": 67.44, "text": " the pipelines for the last few hours over the night, and of course checking if anything", "tokens": [264, 40168, 337, 264, 1036, 1326, 2496, 670, 264, 1818, 11, 293, 295, 1164, 8568, 498, 1340], "temperature": 0.0, "avg_logprob": -0.18334999490291515, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0007927615661174059}, {"id": 12, "seek": 5728, "start": 67.44, "end": 73.64, "text": " is red, and most importantly, if there's a red master.", "tokens": [307, 2182, 11, 293, 881, 8906, 11, 498, 456, 311, 257, 2182, 4505, 13], "temperature": 0.0, "avg_logprob": -0.18334999490291515, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0007927615661174059}, {"id": 13, "seek": 5728, "start": 73.64, "end": 80.36, "text": " And if you can obviously finish your coffee or jump straight into the investigation.", "tokens": [400, 498, 291, 393, 2745, 2413, 428, 4982, 420, 3012, 2997, 666, 264, 9627, 13], "temperature": 0.0, "avg_logprob": -0.18334999490291515, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0007927615661174059}, {"id": 14, "seek": 5728, "start": 80.36, "end": 85.12, "text": " And to be honest, sometimes people actually forgot to go into the Jenkins and check this,", "tokens": [400, 281, 312, 3245, 11, 2171, 561, 767, 5298, 281, 352, 666, 264, 41273, 293, 1520, 341, 11], "temperature": 0.0, "avg_logprob": -0.18334999490291515, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0007927615661174059}, {"id": 15, "seek": 8512, "start": 85.12, "end": 88.96000000000001, "text": " so that's another topic we'll maybe touch upon.", "tokens": [370, 300, 311, 1071, 4829, 321, 603, 1310, 2557, 3564, 13], "temperature": 0.0, "avg_logprob": -0.16628757177614698, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.00047224905574694276}, {"id": 16, "seek": 8512, "start": 88.96000000000001, "end": 92.48, "text": " So you go in, and then you need to go, let's say you see a failure, you see something red,", "tokens": [407, 291, 352, 294, 11, 293, 550, 291, 643, 281, 352, 11, 718, 311, 584, 291, 536, 257, 7763, 11, 291, 536, 746, 2182, 11], "temperature": 0.0, "avg_logprob": -0.16628757177614698, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.00047224905574694276}, {"id": 17, "seek": 8512, "start": 92.48, "end": 99.92, "text": " you need to start going one by one on the different runs, and start figuring out, understanding", "tokens": [291, 643, 281, 722, 516, 472, 538, 472, 322, 264, 819, 6676, 11, 293, 722, 15213, 484, 11, 3701], "temperature": 0.0, "avg_logprob": -0.16628757177614698, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.00047224905574694276}, {"id": 18, "seek": 8512, "start": 99.92, "end": 105.64, "text": " what failed, where it failed, why it failed, and so on.", "tokens": [437, 7612, 11, 689, 309, 7612, 11, 983, 309, 7612, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16628757177614698, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.00047224905574694276}, {"id": 19, "seek": 8512, "start": 105.64, "end": 111.80000000000001, "text": " And it's important that you actually, you needed to go one by one on the different runs,", "tokens": [400, 309, 311, 1021, 300, 291, 767, 11, 291, 2978, 281, 352, 472, 538, 472, 322, 264, 819, 6676, 11], "temperature": 0.0, "avg_logprob": -0.16628757177614698, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.00047224905574694276}, {"id": 20, "seek": 11180, "start": 111.8, "end": 115.92, "text": " and we have several runs, we have the backend, we have the app, we have smoke tests, several", "tokens": [293, 321, 362, 2940, 6676, 11, 321, 362, 264, 38087, 11, 321, 362, 264, 724, 11, 321, 362, 8439, 6921, 11, 2940], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 21, "seek": 11180, "start": 115.92, "end": 121.6, "text": " of these, and start getting the picture, getting the pattern across, and understanding cross", "tokens": [295, 613, 11, 293, 722, 1242, 264, 3036, 11, 1242, 264, 5102, 2108, 11, 293, 3701, 3278], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 22, "seek": 11180, "start": 121.6, "end": 125.44, "text": " runs, across branches, what's going on.", "tokens": [6676, 11, 2108, 14770, 11, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 23, "seek": 11180, "start": 125.44, "end": 130.64, "text": " And on top of all of that, it was very difficult to compare with historical behavior, with", "tokens": [400, 322, 1192, 295, 439, 295, 300, 11, 309, 390, 588, 2252, 281, 6794, 365, 8584, 5223, 11, 365], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 24, "seek": 11180, "start": 130.64, "end": 136.28, "text": " the past behavior, to understand what's, and anomaly, what's the steady state for these", "tokens": [264, 1791, 5223, 11, 281, 1223, 437, 311, 11, 293, 42737, 11, 437, 311, 264, 13211, 1785, 337, 613], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 25, "seek": 11180, "start": 136.28, "end": 138.92, "text": " days, and so on.", "tokens": [1708, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.19587162053473642, "compression_ratio": 1.8963963963963963, "no_speech_prob": 0.0006639408529736102}, {"id": 26, "seek": 13892, "start": 138.92, "end": 144.95999999999998, "text": " So, and just to give you a few examples of questions that we found it difficult or time-consuming", "tokens": [407, 11, 293, 445, 281, 976, 291, 257, 1326, 5110, 295, 1651, 300, 321, 1352, 309, 2252, 420, 565, 12, 21190, 24919], "temperature": 0.0, "avg_logprob": -0.1353747084900573, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0004741579759865999}, {"id": 27, "seek": 13892, "start": 144.95999999999998, "end": 151.07999999999998, "text": " to answer, things such as, did all runs fail on the same step, did all runs fail for the", "tokens": [281, 1867, 11, 721, 1270, 382, 11, 630, 439, 6676, 3061, 322, 264, 912, 1823, 11, 630, 439, 6676, 3061, 337, 264], "temperature": 0.0, "avg_logprob": -0.1353747084900573, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0004741579759865999}, {"id": 28, "seek": 13892, "start": 151.07999999999998, "end": 159.67999999999998, "text": " same reason, is that on a specific branch, is that on a specific machine, if something's", "tokens": [912, 1778, 11, 307, 300, 322, 257, 2685, 9819, 11, 307, 300, 322, 257, 2685, 3479, 11, 498, 746, 311], "temperature": 0.0, "avg_logprob": -0.1353747084900573, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0004741579759865999}, {"id": 29, "seek": 13892, "start": 159.67999999999998, "end": 166.39999999999998, "text": " taking longer, is that normal, is that anomalous, what's the benchmark?", "tokens": [1940, 2854, 11, 307, 300, 2710, 11, 307, 300, 24769, 11553, 11, 437, 311, 264, 18927, 30], "temperature": 0.0, "avg_logprob": -0.1353747084900573, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0004741579759865999}, {"id": 30, "seek": 16640, "start": 166.4, "end": 174.32, "text": " And so these sorts of questions, it took us too long to answer, and we realized we need", "tokens": [400, 370, 613, 7527, 295, 1651, 11, 309, 1890, 505, 886, 938, 281, 1867, 11, 293, 321, 5334, 321, 643], "temperature": 0.0, "avg_logprob": -0.1841583369690695, "compression_ratio": 1.4334975369458127, "no_speech_prob": 0.0003235436452087015}, {"id": 31, "seek": 16640, "start": 174.32, "end": 175.32, "text": " to improve.", "tokens": [281, 3470, 13], "temperature": 0.0, "avg_logprob": -0.1841583369690695, "compression_ratio": 1.4334975369458127, "no_speech_prob": 0.0003235436452087015}, {"id": 32, "seek": 16640, "start": 175.32, "end": 182.36, "text": " A word about myself, my name is Dotan Horvitz, I'm the Principal Developer Advocate at a company", "tokens": [316, 1349, 466, 2059, 11, 452, 1315, 307, 38753, 282, 10691, 85, 6862, 11, 286, 478, 264, 38575, 44915, 13634, 42869, 412, 257, 2237], "temperature": 0.0, "avg_logprob": -0.1841583369690695, "compression_ratio": 1.4334975369458127, "no_speech_prob": 0.0003235436452087015}, {"id": 33, "seek": 16640, "start": 182.36, "end": 190.28, "text": " called Logs.io, Logs.io provides a cloud-native observability platform that's built on popular", "tokens": [1219, 10824, 82, 13, 1004, 11, 10824, 82, 13, 1004, 6417, 257, 4588, 12, 77, 1166, 9951, 2310, 3663, 300, 311, 3094, 322, 3743], "temperature": 0.0, "avg_logprob": -0.1841583369690695, "compression_ratio": 1.4334975369458127, "no_speech_prob": 0.0003235436452087015}, {"id": 34, "seek": 19028, "start": 190.28, "end": 196.84, "text": " open-source tools such as you probably know, Prometheus, OpenSearch, OpenTelemetry, Yeager,", "tokens": [1269, 12, 41676, 3873, 1270, 382, 291, 1391, 458, 11, 2114, 649, 42209, 11, 7238, 10637, 1178, 11, 7238, 14233, 306, 5537, 627, 11, 835, 3557, 11], "temperature": 0.0, "avg_logprob": -0.19141769409179688, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0009652774315327406}, {"id": 35, "seek": 19028, "start": 196.84, "end": 197.84, "text": " and others.", "tokens": [293, 2357, 13], "temperature": 0.0, "avg_logprob": -0.19141769409179688, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0009652774315327406}, {"id": 36, "seek": 19028, "start": 197.84, "end": 205.68, "text": " I come from a background as a developer, a solutions architect, even a product manager,", "tokens": [286, 808, 490, 257, 3678, 382, 257, 10754, 11, 257, 6547, 6331, 11, 754, 257, 1674, 6598, 11], "temperature": 0.0, "avg_logprob": -0.19141769409179688, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0009652774315327406}, {"id": 37, "seek": 19028, "start": 205.68, "end": 210.88, "text": " and most importantly, I'm an advocate of open-source and communities.", "tokens": [293, 881, 8906, 11, 286, 478, 364, 14608, 295, 1269, 12, 41676, 293, 4456, 13], "temperature": 0.0, "avg_logprob": -0.19141769409179688, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0009652774315327406}, {"id": 38, "seek": 19028, "start": 210.88, "end": 217.64, "text": " I run a podcast called Open Observability Talks about open-source DevOps observability,", "tokens": [286, 1190, 257, 7367, 1219, 7238, 42547, 2310, 8780, 82, 466, 1269, 12, 41676, 43051, 9951, 2310, 11], "temperature": 0.0, "avg_logprob": -0.19141769409179688, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0009652774315327406}, {"id": 39, "seek": 21764, "start": 217.64, "end": 221.6, "text": " so if you're interested in these topics and you like podcasts, do check it out.", "tokens": [370, 498, 291, 434, 3102, 294, 613, 8378, 293, 291, 411, 24045, 11, 360, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.1755967848371751, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0006034317775629461}, {"id": 40, "seek": 21764, "start": 221.6, "end": 227.72, "text": " I also run, organize, co-organize several communities, the local chapter of the CNCF,", "tokens": [286, 611, 1190, 11, 13859, 11, 598, 12, 12372, 1125, 2940, 4456, 11, 264, 2654, 7187, 295, 264, 14589, 34, 37, 11], "temperature": 0.0, "avg_logprob": -0.1755967848371751, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0006034317775629461}, {"id": 41, "seek": 21764, "start": 227.72, "end": 232.44, "text": " the cloud-native computing foundation in Tel Aviv, Kubernetes Community Days, DevOps", "tokens": [264, 4588, 12, 77, 1166, 15866, 7030, 294, 27729, 11667, 592, 11, 23145, 10421, 26007, 11, 43051], "temperature": 0.0, "avg_logprob": -0.1755967848371751, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0006034317775629461}, {"id": 42, "seek": 21764, "start": 232.44, "end": 237.48, "text": " Days, et cetera, and you can find me everywhere at Horvitz.", "tokens": [26007, 11, 1030, 11458, 11, 293, 291, 393, 915, 385, 5315, 412, 10691, 85, 6862, 13], "temperature": 0.0, "avg_logprob": -0.1755967848371751, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0006034317775629461}, {"id": 43, "seek": 21764, "start": 237.48, "end": 242.2, "text": " So if you have something interesting, you tweet, feel free to tag me.", "tokens": [407, 498, 291, 362, 746, 1880, 11, 291, 15258, 11, 841, 1737, 281, 6162, 385, 13], "temperature": 0.0, "avg_logprob": -0.1755967848371751, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0006034317775629461}, {"id": 44, "seek": 24220, "start": 242.2, "end": 250.95999999999998, "text": " So before I get into how we improved our CI CD pipeline or capabilities, let's first", "tokens": [407, 949, 286, 483, 666, 577, 321, 9689, 527, 37777, 6743, 15517, 420, 10862, 11, 718, 311, 700], "temperature": 0.0, "avg_logprob": -0.16497389027770137, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.00035857598413713276}, {"id": 45, "seek": 24220, "start": 250.95999999999998, "end": 254.35999999999999, "text": " understand what we want to improve on.", "tokens": [1223, 437, 321, 528, 281, 3470, 322, 13], "temperature": 0.0, "avg_logprob": -0.16497389027770137, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.00035857598413713276}, {"id": 46, "seek": 24220, "start": 254.35999999999999, "end": 259.56, "text": " And actually, I see very often that people jump into solving before really understanding", "tokens": [400, 767, 11, 286, 536, 588, 2049, 300, 561, 3012, 666, 12606, 949, 534, 3701], "temperature": 0.0, "avg_logprob": -0.16497389027770137, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.00035857598413713276}, {"id": 47, "seek": 24220, "start": 259.56, "end": 267.44, "text": " the metric, the KPI that they want to improve, and very basically, therefore, primary metrics", "tokens": [264, 20678, 11, 264, 591, 31701, 300, 436, 528, 281, 3470, 11, 293, 588, 1936, 11, 4412, 11, 6194, 16367], "temperature": 0.0, "avg_logprob": -0.16497389027770137, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.00035857598413713276}, {"id": 48, "seek": 26744, "start": 267.44, "end": 274.92, "text": " for, let's say, DevOps performance, and you can see there on the screen, there's the", "tokens": [337, 11, 718, 311, 584, 11, 43051, 3389, 11, 293, 291, 393, 536, 456, 322, 264, 2568, 11, 456, 311, 264], "temperature": 0.0, "avg_logprob": -0.14409384727478028, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0003337395319249481}, {"id": 49, "seek": 26744, "start": 274.92, "end": 282.56, "text": " deployment frequency, lead time for changes, change failure rate, and MPTR, mean time to", "tokens": [19317, 7893, 11, 1477, 565, 337, 2962, 11, 1319, 7763, 3314, 11, 293, 14146, 25936, 11, 914, 565, 281], "temperature": 0.0, "avg_logprob": -0.14409384727478028, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0003337395319249481}, {"id": 50, "seek": 26744, "start": 282.56, "end": 284.04, "text": " recovery.", "tokens": [8597, 13], "temperature": 0.0, "avg_logprob": -0.14409384727478028, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0003337395319249481}, {"id": 51, "seek": 26744, "start": 284.04, "end": 288.6, "text": " I don't have time to go over all of these, but very important, so if you're new to this", "tokens": [286, 500, 380, 362, 565, 281, 352, 670, 439, 295, 613, 11, 457, 588, 1021, 11, 370, 498, 291, 434, 777, 281, 341], "temperature": 0.0, "avg_logprob": -0.14409384727478028, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0003337395319249481}, {"id": 52, "seek": 26744, "start": 288.6, "end": 294.24, "text": " and if you want to read a bit more about that, I left a QR code and a short link for you", "tokens": [293, 498, 291, 528, 281, 1401, 257, 857, 544, 466, 300, 11, 286, 1411, 257, 32784, 3089, 293, 257, 2099, 2113, 337, 291], "temperature": 0.0, "avg_logprob": -0.14409384727478028, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0003337395319249481}, {"id": 53, "seek": 29424, "start": 294.24, "end": 302.04, "text": " at the bottom for a 101 on the Dora metrics, do check it out, I think it's priceless.", "tokens": [412, 264, 2767, 337, 257, 21055, 322, 264, 413, 3252, 16367, 11, 360, 1520, 309, 484, 11, 286, 519, 309, 311, 582, 41817, 13], "temperature": 0.0, "avg_logprob": -0.1125369495815701, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0002133408997906372}, {"id": 54, "seek": 29424, "start": 302.04, "end": 307.56, "text": " And in our case, we needed to improve on the lead time for changes or sometimes called", "tokens": [400, 294, 527, 1389, 11, 321, 2978, 281, 3470, 322, 264, 1477, 565, 337, 2962, 420, 2171, 1219], "temperature": 0.0, "avg_logprob": -0.1125369495815701, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0002133408997906372}, {"id": 55, "seek": 29424, "start": 307.56, "end": 314.48, "text": " cycle time, which is the amount of time it takes a commit to get into production, which", "tokens": [6586, 565, 11, 597, 307, 264, 2372, 295, 565, 309, 2516, 257, 5599, 281, 483, 666, 4265, 11, 597], "temperature": 0.0, "avg_logprob": -0.1125369495815701, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0002133408997906372}, {"id": 56, "seek": 29424, "start": 314.48, "end": 322.12, "text": " in our case was the time was too long, too high, and was holding us back.", "tokens": [294, 527, 1389, 390, 264, 565, 390, 886, 938, 11, 886, 1090, 11, 293, 390, 5061, 505, 646, 13], "temperature": 0.0, "avg_logprob": -0.1125369495815701, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.0002133408997906372}, {"id": 57, "seek": 32212, "start": 322.12, "end": 327.64, "text": " So we are experts at observability in our engineering team.", "tokens": [407, 321, 366, 8572, 412, 9951, 2310, 294, 527, 7043, 1469, 13], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 58, "seek": 32212, "start": 327.64, "end": 331.84000000000003, "text": " That's what we do for a living, so it was very clear to us that what we're missing in", "tokens": [663, 311, 437, 321, 360, 337, 257, 2647, 11, 370, 309, 390, 588, 1850, 281, 505, 300, 437, 321, 434, 5361, 294], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 59, "seek": 32212, "start": 331.84000000000003, "end": 337.12, "text": " our case is observability into our CICD pipeline.", "tokens": [527, 1389, 307, 9951, 2310, 666, 527, 383, 2532, 35, 15517, 13], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 60, "seek": 32212, "start": 337.12, "end": 342.0, "text": " And to be fair with Jenkins, and there are lots of things to complain about Jenkins,", "tokens": [400, 281, 312, 3143, 365, 41273, 11, 293, 456, 366, 3195, 295, 721, 281, 11024, 466, 41273, 11], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 61, "seek": 32212, "start": 342.0, "end": 344.56, "text": " but there is some capabilities within Jenkins.", "tokens": [457, 456, 307, 512, 10862, 1951, 41273, 13], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 62, "seek": 32212, "start": 344.56, "end": 349.72, "text": " You can go into a specific pipeline run, you can see the different steps, you can see how", "tokens": [509, 393, 352, 666, 257, 2685, 15517, 1190, 11, 291, 393, 536, 264, 819, 4439, 11, 291, 393, 536, 577], "temperature": 0.0, "avg_logprob": -0.13790644339795383, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00012070607772329822}, {"id": 63, "seek": 34972, "start": 349.72, "end": 353.6, "text": " much time an individual step took.", "tokens": [709, 565, 364, 2609, 1823, 1890, 13], "temperature": 0.0, "avg_logprob": -0.13668161415192018, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0002032966585829854}, {"id": 64, "seek": 34972, "start": 353.6, "end": 359.32000000000005, "text": " Using some plugins, you can also visualize the graph and we even wired Jenkins to get", "tokens": [11142, 512, 33759, 11, 291, 393, 611, 23273, 264, 4295, 293, 321, 754, 27415, 41273, 281, 483], "temperature": 0.0, "avg_logprob": -0.13668161415192018, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0002032966585829854}, {"id": 65, "seek": 34972, "start": 359.32000000000005, "end": 365.64000000000004, "text": " alerts on Slack, but that wasn't good enough for us.", "tokens": [28061, 322, 37211, 11, 457, 300, 2067, 380, 665, 1547, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.13668161415192018, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0002032966585829854}, {"id": 66, "seek": 34972, "start": 365.64000000000004, "end": 371.68, "text": " And the reason that we wanted to find a way to monitor aggregated and filtered information", "tokens": [400, 264, 1778, 300, 321, 1415, 281, 915, 257, 636, 281, 6002, 16743, 770, 293, 37111, 1589], "temperature": 0.0, "avg_logprob": -0.13668161415192018, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0002032966585829854}, {"id": 67, "seek": 34972, "start": 371.68, "end": 377.16, "text": " according to our own timescale, according to our own filters, obviously to see things", "tokens": [4650, 281, 527, 1065, 1413, 37088, 11, 4650, 281, 527, 1065, 15995, 11, 2745, 281, 536, 721], "temperature": 0.0, "avg_logprob": -0.13668161415192018, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0002032966585829854}, {"id": 68, "seek": 37716, "start": 377.16, "end": 383.76000000000005, "text": " across branches, across runs, to compare with historical data, with our own filtering,", "tokens": [2108, 14770, 11, 2108, 6676, 11, 281, 6794, 365, 8584, 1412, 11, 365, 527, 1065, 30822, 11], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 69, "seek": 37716, "start": 383.76000000000005, "end": 386.40000000000003, "text": " so that's where we aimed at.", "tokens": [370, 300, 311, 689, 321, 20540, 412, 13], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 70, "seek": 37716, "start": 386.40000000000003, "end": 392.36, "text": " And we launched this internal project with these requirements, four requirements.", "tokens": [400, 321, 8730, 341, 6920, 1716, 365, 613, 7728, 11, 1451, 7728, 13], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 71, "seek": 37716, "start": 392.36, "end": 397.92, "text": " One, first and foremost, as we need the dashboard, we need the dashboard with aggregated views", "tokens": [1485, 11, 700, 293, 18864, 11, 382, 321, 643, 264, 18342, 11, 321, 643, 264, 18342, 365, 16743, 770, 6809], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 72, "seek": 37716, "start": 397.92, "end": 404.08000000000004, "text": " to be able to see the aggregated data across pipelines, across runs, across branches as", "tokens": [281, 312, 1075, 281, 536, 264, 16743, 770, 1412, 2108, 40168, 11, 2108, 6676, 11, 2108, 14770, 382], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 73, "seek": 37716, "start": 404.08000000000004, "end": 406.28000000000003, "text": " we talked about.", "tokens": [321, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.20171005167859665, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.00027650254196487367}, {"id": 74, "seek": 40628, "start": 406.28, "end": 412.23999999999995, "text": " Finally we wanted to have access to historical data to be able to compare, to understand", "tokens": [6288, 321, 1415, 281, 362, 2105, 281, 8584, 1412, 281, 312, 1075, 281, 6794, 11, 281, 1223], "temperature": 0.0, "avg_logprob": -0.15756898576563055, "compression_ratio": 1.7574257425742574, "no_speech_prob": 0.00012628048716578633}, {"id": 75, "seek": 40628, "start": 412.23999999999995, "end": 417.47999999999996, "text": " trends, to identify patterns, anomalies, and so on.", "tokens": [13892, 11, 281, 5876, 8294, 11, 24769, 48872, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.15756898576563055, "compression_ratio": 1.7574257425742574, "no_speech_prob": 0.00012628048716578633}, {"id": 76, "seek": 40628, "start": 417.47999999999996, "end": 424.28, "text": " Thirdly, we wanted reports and alerts to be able to automate as much as possible.", "tokens": [12548, 356, 11, 321, 1415, 7122, 293, 28061, 281, 312, 1075, 281, 31605, 382, 709, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.15756898576563055, "compression_ratio": 1.7574257425742574, "no_speech_prob": 0.00012628048716578633}, {"id": 77, "seek": 40628, "start": 424.28, "end": 429.84, "text": " And lastly, we wanted some ability to view flaky tests, test performance, and to be able", "tokens": [400, 16386, 11, 321, 1415, 512, 3485, 281, 1910, 932, 15681, 6921, 11, 1500, 3389, 11, 293, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.15756898576563055, "compression_ratio": 1.7574257425742574, "no_speech_prob": 0.00012628048716578633}, {"id": 78, "seek": 40628, "start": 429.84, "end": 433.55999999999995, "text": " to understand their impact on the pipeline.", "tokens": [281, 1223, 641, 2712, 322, 264, 15517, 13], "temperature": 0.0, "avg_logprob": -0.15756898576563055, "compression_ratio": 1.7574257425742574, "no_speech_prob": 0.00012628048716578633}, {"id": 79, "seek": 43356, "start": 433.56, "end": 440.24, "text": " So that was the project requirements and how we did that.", "tokens": [407, 300, 390, 264, 1716, 7728, 293, 577, 321, 630, 300, 13], "temperature": 0.0, "avg_logprob": -0.17521989345550537, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002065245935227722}, {"id": 80, "seek": 43356, "start": 440.24, "end": 449.4, "text": " Essentially it takes four steps, collect, store, visualize, and report.", "tokens": [23596, 309, 2516, 1451, 4439, 11, 2500, 11, 3531, 11, 23273, 11, 293, 2275, 13], "temperature": 0.0, "avg_logprob": -0.17521989345550537, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002065245935227722}, {"id": 81, "seek": 43356, "start": 449.4, "end": 453.6, "text": " And I'll show you exactly how it's done and what each step entails.", "tokens": [400, 286, 603, 855, 291, 2293, 577, 309, 311, 1096, 293, 437, 1184, 1823, 50133, 13], "temperature": 0.0, "avg_logprob": -0.17521989345550537, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002065245935227722}, {"id": 82, "seek": 43356, "start": 453.6, "end": 459.92, "text": " In terms of the tech stack, we were very versed with the Elk stack, Elasticsearch, Kabbana.", "tokens": [682, 2115, 295, 264, 7553, 8630, 11, 321, 645, 588, 1774, 292, 365, 264, 2699, 74, 8630, 11, 2699, 2750, 405, 1178, 11, 591, 10797, 2095, 13], "temperature": 0.0, "avg_logprob": -0.17521989345550537, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002065245935227722}, {"id": 83, "seek": 45992, "start": 459.92, "end": 465.44, "text": " Then we also switched over to OpenSearch and OpenSearch dashboards after Elastic re-licensed", "tokens": [1396, 321, 611, 16858, 670, 281, 7238, 10637, 1178, 293, 7238, 10637, 1178, 8240, 17228, 934, 2699, 2750, 319, 12, 1050, 16332], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 84, "seek": 45992, "start": 465.44, "end": 467.64000000000004, "text": " and it was no longer open source.", "tokens": [293, 309, 390, 572, 2854, 1269, 4009, 13], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 85, "seek": 45992, "start": 467.64000000000004, "end": 473.0, "text": " So that was our natural point to start our observability journey.", "tokens": [407, 300, 390, 527, 3303, 935, 281, 722, 527, 9951, 2310, 4671, 13], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 86, "seek": 45992, "start": 473.0, "end": 478.16, "text": " And I'll show you how we did these four steps with this tech stack.", "tokens": [400, 286, 603, 855, 291, 577, 321, 630, 613, 1451, 4439, 365, 341, 7553, 8630, 13], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 87, "seek": 45992, "start": 478.16, "end": 480.56, "text": " So the first step is collect.", "tokens": [407, 264, 700, 1823, 307, 2500, 13], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 88, "seek": 45992, "start": 480.56, "end": 485.56, "text": " And for that we instrumented the pipeline to collect all the relevant information and", "tokens": [400, 337, 300, 321, 7198, 292, 264, 15517, 281, 2500, 439, 264, 7340, 1589, 293], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 89, "seek": 45992, "start": 485.56, "end": 488.28000000000003, "text": " put it in environment variables.", "tokens": [829, 309, 294, 2823, 9102, 13], "temperature": 0.0, "avg_logprob": -0.15904208062921912, "compression_ratio": 1.616600790513834, "no_speech_prob": 7.301500590983778e-05}, {"id": 90, "seek": 48828, "start": 488.28, "end": 494.2, "text": " Which information, you can see some examples here on the screen, the branch, the Kamecha,", "tokens": [3013, 1589, 11, 291, 393, 536, 512, 5110, 510, 322, 264, 2568, 11, 264, 9819, 11, 264, 591, 529, 4413, 11], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 91, "seek": 48828, "start": 494.2, "end": 499.84, "text": " the machine IP, the run type, whether it's scheduled, triggered by merge to master or", "tokens": [264, 3479, 8671, 11, 264, 1190, 2010, 11, 1968, 309, 311, 15678, 11, 21710, 538, 22183, 281, 4505, 420], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 92, "seek": 48828, "start": 499.84, "end": 505.4, "text": " something else, fail step, step duration, build number, anything essentially that you", "tokens": [746, 1646, 11, 3061, 1823, 11, 1823, 16365, 11, 1322, 1230, 11, 1340, 4476, 300, 291], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 93, "seek": 48828, "start": 505.4, "end": 508.32, "text": " find useful for investigation later.", "tokens": [915, 4420, 337, 9627, 1780, 13], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 94, "seek": 48828, "start": 508.32, "end": 512.16, "text": " My recommendation, collect it and persist it.", "tokens": [1222, 11879, 11, 2500, 309, 293, 13233, 309, 13], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 95, "seek": 48828, "start": 512.16, "end": 516.92, "text": " So that's the collect phase and after collect comes store.", "tokens": [407, 300, 311, 264, 2500, 5574, 293, 934, 2500, 1487, 3531, 13], "temperature": 0.0, "avg_logprob": -0.23244678851255438, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0001542658283142373}, {"id": 96, "seek": 51692, "start": 516.92, "end": 523.4399999999999, "text": " And for that we created a new summary step at the end of the pipeline one where we ran", "tokens": [400, 337, 300, 321, 2942, 257, 777, 12691, 1823, 412, 264, 917, 295, 264, 15517, 472, 689, 321, 5872], "temperature": 0.0, "avg_logprob": -0.1558596402749248, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.00010706649482017383}, {"id": 97, "seek": 51692, "start": 523.4399999999999, "end": 529.56, "text": " a command to collect all of that information that we did in the first step and created", "tokens": [257, 5622, 281, 2500, 439, 295, 300, 1589, 300, 321, 630, 294, 264, 700, 1823, 293, 2942], "temperature": 0.0, "avg_logprob": -0.1558596402749248, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.00010706649482017383}, {"id": 98, "seek": 51692, "start": 529.56, "end": 539.8, "text": " a JSON and persisted it to Elasticsearch, as I mentioned then move to OpenSearch.", "tokens": [257, 31828, 293, 13233, 292, 309, 281, 2699, 2750, 405, 1178, 11, 382, 286, 2835, 550, 1286, 281, 7238, 10637, 1178, 13], "temperature": 0.0, "avg_logprob": -0.1558596402749248, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.00010706649482017383}, {"id": 99, "seek": 51692, "start": 539.8, "end": 544.52, "text": " And it's important to say again for the fairness of Jenkins and for the Jenkins experts here,", "tokens": [400, 309, 311, 1021, 281, 584, 797, 337, 264, 29765, 295, 41273, 293, 337, 264, 41273, 8572, 510, 11], "temperature": 0.0, "avg_logprob": -0.1558596402749248, "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.00010706649482017383}, {"id": 100, "seek": 54452, "start": 544.52, "end": 548.12, "text": " Jenkins does have some built in persistency capabilities.", "tokens": [41273, 775, 362, 512, 3094, 294, 13233, 3020, 10862, 13], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 101, "seek": 54452, "start": 548.12, "end": 552.0, "text": " And we tried them out, but it wasn't good enough for us.", "tokens": [400, 321, 3031, 552, 484, 11, 457, 309, 2067, 380, 665, 1547, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 102, "seek": 54452, "start": 552.0, "end": 558.28, "text": " And the reason is that by default Jenkins essentially keeps all the bills and stores", "tokens": [400, 264, 1778, 307, 300, 538, 7576, 41273, 4476, 5965, 439, 264, 12433, 293, 9512], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 103, "seek": 54452, "start": 558.28, "end": 563.76, "text": " them on the Jenkins machine, which burdens these machines of course.", "tokens": [552, 322, 264, 41273, 3479, 11, 597, 37882, 613, 8379, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 104, "seek": 54452, "start": 563.76, "end": 569.3199999999999, "text": " And then you start needing to limit the number of bills and the duration, how many days and", "tokens": [400, 550, 291, 722, 18006, 281, 4948, 264, 1230, 295, 12433, 293, 264, 16365, 11, 577, 867, 1708, 293], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 105, "seek": 54452, "start": 569.3199999999999, "end": 570.3199999999999, "text": " so on and so forth.", "tokens": [370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 106, "seek": 54452, "start": 570.3199999999999, "end": 573.1999999999999, "text": " So that wasn't good enough for us.", "tokens": [407, 300, 2067, 380, 665, 1547, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.13819180645988982, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.00022159077343530953}, {"id": 107, "seek": 57320, "start": 573.2, "end": 576.96, "text": " We needed a more powerful access to historical data.", "tokens": [492, 2978, 257, 544, 4005, 2105, 281, 8584, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 108, "seek": 57320, "start": 576.96, "end": 584.0400000000001, "text": " We wanted to persist historical data in our own control, the duration, the retention and", "tokens": [492, 1415, 281, 13233, 8584, 1412, 294, 527, 1065, 1969, 11, 264, 16365, 11, 264, 22871, 293], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 109, "seek": 57320, "start": 584.0400000000001, "end": 591.48, "text": " most importantly off of the Jenkins servers so as not to risk and overload the critical", "tokens": [881, 8906, 766, 295, 264, 41273, 15909, 370, 382, 406, 281, 3148, 293, 28777, 264, 4924], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 110, "seek": 57320, "start": 591.48, "end": 594.08, "text": " path.", "tokens": [3100, 13], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 111, "seek": 57320, "start": 594.08, "end": 596.1600000000001, "text": " So that's about store and after store.", "tokens": [407, 300, 311, 466, 3531, 293, 934, 3531, 13], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 112, "seek": 57320, "start": 596.1600000000001, "end": 601.4000000000001, "text": " Once we have all the data in Elasticsearch or OpenSearch, now it's very easy to build", "tokens": [3443, 321, 362, 439, 264, 1412, 294, 2699, 2750, 405, 1178, 420, 7238, 10637, 1178, 11, 586, 309, 311, 588, 1858, 281, 1322], "temperature": 0.0, "avg_logprob": -0.13885546254587697, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.0001803980121621862}, {"id": 113, "seek": 60140, "start": 601.4, "end": 607.48, "text": " command dashboards or OpenSearch dashboards and visualizations on top of that.", "tokens": [5622, 8240, 17228, 420, 7238, 10637, 1178, 8240, 17228, 293, 5056, 14455, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 114, "seek": 60140, "start": 607.48, "end": 613.1999999999999, "text": " And then comes the question, sorry, then comes the question, okay, so which visualizations", "tokens": [400, 550, 1487, 264, 1168, 11, 2597, 11, 550, 1487, 264, 1168, 11, 1392, 11, 370, 597, 5056, 14455], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 115, "seek": 60140, "start": 613.1999999999999, "end": 616.16, "text": " should I build?", "tokens": [820, 286, 1322, 30], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 116, "seek": 60140, "start": 616.16, "end": 620.9599999999999, "text": " And for that, and that's a tip, take it with you, go back to the pains, go back to the", "tokens": [400, 337, 300, 11, 293, 300, 311, 257, 4125, 11, 747, 309, 365, 291, 11, 352, 646, 281, 264, 29774, 11, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 117, "seek": 60140, "start": 620.9599999999999, "end": 625.56, "text": " questions that you found it hard to answer and this would be the starting point.", "tokens": [1651, 300, 291, 1352, 309, 1152, 281, 1867, 293, 341, 576, 312, 264, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 118, "seek": 60140, "start": 625.56, "end": 629.92, "text": " So if you remember before we mentioned things such as did all runs fail on the same step,", "tokens": [407, 498, 291, 1604, 949, 321, 2835, 721, 1270, 382, 630, 439, 6676, 3061, 322, 264, 912, 1823, 11], "temperature": 0.0, "avg_logprob": -0.17280165354410806, "compression_ratio": 1.772, "no_speech_prob": 0.0001794791460270062}, {"id": 119, "seek": 62992, "start": 629.92, "end": 635.88, "text": " did all runs fail for the same reason, how many fail, is that a specific branch, is that", "tokens": [630, 439, 6676, 3061, 337, 264, 912, 1778, 11, 577, 867, 3061, 11, 307, 300, 257, 2685, 9819, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 120, "seek": 62992, "start": 635.88, "end": 641.64, "text": " a specific machine and so on, these are the questions that we guide you then to choose", "tokens": [257, 2685, 3479, 293, 370, 322, 11, 613, 366, 264, 1651, 300, 321, 5934, 291, 550, 281, 2826], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 121, "seek": 62992, "start": 641.64, "end": 644.7199999999999, "text": " the right visualizations for your dashboard.", "tokens": [264, 558, 5056, 14455, 337, 428, 18342, 13], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 122, "seek": 62992, "start": 644.7199999999999, "end": 647.68, "text": " And I'll give you some examples here.", "tokens": [400, 286, 603, 976, 291, 512, 5110, 510, 13], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 123, "seek": 62992, "start": 647.68, "end": 650.0, "text": " So let's start with the top line view.", "tokens": [407, 718, 311, 722, 365, 264, 1192, 1622, 1910, 13], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 124, "seek": 62992, "start": 650.0, "end": 654.24, "text": " You want to understand the health of your house table, your pipeline is.", "tokens": [509, 528, 281, 1223, 264, 1585, 295, 428, 1782, 3199, 11, 428, 15517, 307, 13], "temperature": 0.0, "avg_logprob": -0.15683686105828537, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00015036629338283092}, {"id": 125, "seek": 65424, "start": 654.24, "end": 660.84, "text": " So visualize the success and failure rates, you can do that overall in general or at a", "tokens": [407, 23273, 264, 2245, 293, 7763, 6846, 11, 291, 393, 360, 300, 4787, 294, 2674, 420, 412, 257], "temperature": 0.0, "avg_logprob": -0.18839488794774184, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00027527622296474874}, {"id": 126, "seek": 65424, "start": 660.84, "end": 667.52, "text": " specific time window on a graph, very easy to see the first glance, what's the health", "tokens": [2685, 565, 4910, 322, 257, 4295, 11, 588, 1858, 281, 536, 264, 700, 21094, 11, 437, 311, 264, 1585], "temperature": 0.0, "avg_logprob": -0.18839488794774184, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00027527622296474874}, {"id": 127, "seek": 65424, "start": 667.52, "end": 671.84, "text": " status of your pipeline.", "tokens": [6558, 295, 428, 15517, 13], "temperature": 0.0, "avg_logprob": -0.18839488794774184, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00027527622296474874}, {"id": 128, "seek": 65424, "start": 671.84, "end": 678.2, "text": " You want to find problematic steps, then visualize failures segmented by pipeline steps, again", "tokens": [509, 528, 281, 915, 19011, 4439, 11, 550, 23273, 20774, 9469, 292, 538, 15517, 4439, 11, 797], "temperature": 0.0, "avg_logprob": -0.18839488794774184, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00027527622296474874}, {"id": 129, "seek": 65424, "start": 678.2, "end": 682.52, "text": " very easy to see the spiking step there.", "tokens": [588, 1858, 281, 536, 264, 637, 13085, 1823, 456, 13], "temperature": 0.0, "avg_logprob": -0.18839488794774184, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.00027527622296474874}, {"id": 130, "seek": 68252, "start": 682.52, "end": 688.4, "text": " You want to detect problematic build machines, visualize failures segmented by machine and", "tokens": [509, 528, 281, 5531, 19011, 1322, 8379, 11, 23273, 20774, 9469, 292, 538, 3479, 293], "temperature": 0.0, "avg_logprob": -0.14071581223431756, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.0003419527201913297}, {"id": 131, "seek": 68252, "start": 688.4, "end": 696.4399999999999, "text": " that by the way saved us a lot of wasted time going and checking bugs in the release code.", "tokens": [300, 538, 264, 636, 6624, 505, 257, 688, 295, 19496, 565, 516, 293, 8568, 15120, 294, 264, 4374, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14071581223431756, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.0003419527201913297}, {"id": 132, "seek": 68252, "start": 696.4399999999999, "end": 701.24, "text": " When we saw such a thing, we just go, you kill the machine, you let the auto scaler spin", "tokens": [1133, 321, 1866, 1270, 257, 551, 11, 321, 445, 352, 11, 291, 1961, 264, 3479, 11, 291, 718, 264, 8399, 15664, 260, 6060], "temperature": 0.0, "avg_logprob": -0.14071581223431756, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.0003419527201913297}, {"id": 133, "seek": 68252, "start": 701.24, "end": 706.24, "text": " up a new instance and you start clean and in many cases it solves the problem.", "tokens": [493, 257, 777, 5197, 293, 291, 722, 2541, 293, 294, 867, 3331, 309, 39890, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14071581223431756, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.0003419527201913297}, {"id": 134, "seek": 70624, "start": 706.24, "end": 715.12, "text": " So lots of time saved, in general this aspect of code based or environmental based issues", "tokens": [407, 3195, 295, 565, 6624, 11, 294, 2674, 341, 4171, 295, 3089, 2361, 420, 8303, 2361, 2663], "temperature": 0.0, "avg_logprob": -0.1741446148265492, "compression_ratio": 1.4432432432432432, "no_speech_prob": 0.00011685382196446881}, {"id": 135, "seek": 70624, "start": 715.12, "end": 722.36, "text": " is definitely a challenge I'm assuming, not just for me, so I'll get back to that soon.", "tokens": [307, 2138, 257, 3430, 286, 478, 11926, 11, 406, 445, 337, 385, 11, 370, 286, 603, 483, 646, 281, 300, 2321, 13], "temperature": 0.0, "avg_logprob": -0.1741446148265492, "compression_ratio": 1.4432432432432432, "no_speech_prob": 0.00011685382196446881}, {"id": 136, "seek": 70624, "start": 722.36, "end": 730.32, "text": " Another example duration per step, again very easy to see where and at the time is spent.", "tokens": [3996, 1365, 16365, 680, 1823, 11, 797, 588, 1858, 281, 536, 689, 293, 412, 264, 565, 307, 4418, 13], "temperature": 0.0, "avg_logprob": -0.1741446148265492, "compression_ratio": 1.4432432432432432, "no_speech_prob": 0.00011685382196446881}, {"id": 137, "seek": 73032, "start": 730.32, "end": 737.84, "text": " So that's the visualize part and after visualize comes the reporting and alerting phase.", "tokens": [407, 300, 311, 264, 23273, 644, 293, 934, 23273, 1487, 264, 10031, 293, 419, 27187, 5574, 13], "temperature": 0.0, "avg_logprob": -0.16969168329813394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00023998701362870634}, {"id": 138, "seek": 73032, "start": 737.84, "end": 743.12, "text": " And if you remember before the DOD, the developer on duty, needed to go manually and check Jenkins", "tokens": [400, 498, 291, 1604, 949, 264, 10699, 35, 11, 264, 10754, 322, 9776, 11, 2978, 281, 352, 16945, 293, 1520, 41273], "temperature": 0.0, "avg_logprob": -0.16969168329813394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00023998701362870634}, {"id": 139, "seek": 73032, "start": 743.12, "end": 752.5600000000001, "text": " and then the health check, now the DOD gets start of day report directly to Slack and", "tokens": [293, 550, 264, 1585, 1520, 11, 586, 264, 10699, 35, 2170, 722, 295, 786, 2275, 3838, 281, 37211, 293], "temperature": 0.0, "avg_logprob": -0.16969168329813394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00023998701362870634}, {"id": 140, "seek": 73032, "start": 752.5600000000001, "end": 757.2, "text": " actually as you can see the report already contains the link to the dashboard and even", "tokens": [767, 382, 291, 393, 536, 264, 2275, 1217, 8306, 264, 2113, 281, 264, 18342, 293, 754], "temperature": 0.0, "avg_logprob": -0.16969168329813394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00023998701362870634}, {"id": 141, "seek": 75720, "start": 757.2, "end": 763.44, "text": " a snapshot of the dashboard embedded within the Slack so that at the first glance even", "tokens": [257, 30163, 295, 264, 18342, 16741, 1951, 264, 37211, 370, 300, 412, 264, 700, 21094, 754], "temperature": 0.0, "avg_logprob": -0.15526860368018056, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0002440606476739049}, {"id": 142, "seek": 75720, "start": 763.44, "end": 768.9200000000001, "text": " without going into the dashboard you can see if you can finish your coffee or if there's", "tokens": [1553, 516, 666, 264, 18342, 291, 393, 536, 498, 291, 393, 2413, 428, 4982, 420, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.15526860368018056, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0002440606476739049}, {"id": 143, "seek": 75720, "start": 768.9200000000001, "end": 774.32, "text": " something alerting that you need to click that link and go start investigating.", "tokens": [746, 419, 27187, 300, 291, 643, 281, 2052, 300, 2113, 293, 352, 722, 22858, 13], "temperature": 0.0, "avg_logprob": -0.15526860368018056, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0002440606476739049}, {"id": 144, "seek": 75720, "start": 774.32, "end": 778.12, "text": " And of course it doesn't have to be a schedule report, it could be also you can define triggered", "tokens": [400, 295, 1164, 309, 1177, 380, 362, 281, 312, 257, 7567, 2275, 11, 309, 727, 312, 611, 291, 393, 6964, 21710], "temperature": 0.0, "avg_logprob": -0.15526860368018056, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0002440606476739049}, {"id": 145, "seek": 75720, "start": 778.12, "end": 784.1600000000001, "text": " alerts on any of that, the fields, the data that we collected in the first phase and the", "tokens": [28061, 322, 604, 295, 300, 11, 264, 7909, 11, 264, 1412, 300, 321, 11087, 294, 264, 700, 5574, 293, 264], "temperature": 0.0, "avg_logprob": -0.15526860368018056, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0002440606476739049}, {"id": 146, "seek": 78416, "start": 784.16, "end": 791.0799999999999, "text": " collect phase so and you can do any complex queries or conditions that you want, you want", "tokens": [2500, 5574, 370, 293, 291, 393, 360, 604, 3997, 24109, 420, 4487, 300, 291, 528, 11, 291, 528], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 147, "seek": 78416, "start": 791.0799999999999, "end": 796.92, "text": " to do something like if the sum of failures goes above x or the average duration goes", "tokens": [281, 360, 746, 411, 498, 264, 2408, 295, 20774, 1709, 3673, 2031, 420, 264, 4274, 16365, 1709], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 148, "seek": 78416, "start": 796.92, "end": 799.0, "text": " above y trigger an alert.", "tokens": [3673, 288, 7875, 364, 9615, 13], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 149, "seek": 78416, "start": 799.0, "end": 804.28, "text": " So essentially anything that you can formalize as a Lucene query, you can automate as an", "tokens": [407, 4476, 1340, 300, 291, 393, 9860, 1125, 382, 257, 9593, 1450, 14581, 11, 291, 393, 31605, 382, 364], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 150, "seek": 78416, "start": 804.28, "end": 809.4, "text": " alert and that's some alerting layer that we built on top of elastic search and open", "tokens": [9615, 293, 300, 311, 512, 419, 27187, 4583, 300, 321, 3094, 322, 1192, 295, 17115, 3164, 293, 1269], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 151, "seek": 78416, "start": 809.4, "end": 812.28, "text": " search for that.", "tokens": [3164, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18505404392878214, "compression_ratio": 1.7117903930131004, "no_speech_prob": 3.489659138722345e-05}, {"id": 152, "seek": 81228, "start": 812.28, "end": 817.0799999999999, "text": " One last note, I'm giving the examples from Slack because that's what we use in our environment", "tokens": [1485, 1036, 3637, 11, 286, 478, 2902, 264, 5110, 490, 37211, 570, 300, 311, 437, 321, 764, 294, 527, 2823], "temperature": 0.0, "avg_logprob": -0.1725345125385359, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.00010641370317898691}, {"id": 153, "seek": 81228, "start": 817.0799999999999, "end": 822.88, "text": " but you're not limited obviously to Slack, you have support for many notification endpoints", "tokens": [457, 291, 434, 406, 5567, 2745, 281, 37211, 11, 291, 362, 1406, 337, 867, 11554, 917, 20552], "temperature": 0.0, "avg_logprob": -0.1725345125385359, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.00010641370317898691}, {"id": 154, "seek": 81228, "start": 822.88, "end": 828.24, "text": " depending on your systems, pager duty, victorops, ops genie, MS themes, whatever.", "tokens": [5413, 322, 428, 3652, 11, 280, 3557, 9776, 11, 4403, 284, 3370, 11, 44663, 1049, 414, 11, 7395, 13544, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1725345125385359, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.00010641370317898691}, {"id": 155, "seek": 81228, "start": 828.24, "end": 833.68, "text": " We personally work with Slack so that the examples are with Slack.", "tokens": [492, 5665, 589, 365, 37211, 370, 300, 264, 5110, 366, 365, 37211, 13], "temperature": 0.0, "avg_logprob": -0.1725345125385359, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.00010641370317898691}, {"id": 156, "seek": 81228, "start": 833.68, "end": 840.28, "text": " So that's how we build observability into the Jenkins pipelines but as we all know especially", "tokens": [407, 300, 311, 577, 321, 1322, 9951, 2310, 666, 264, 41273, 40168, 457, 382, 321, 439, 458, 2318], "temperature": 0.0, "avg_logprob": -0.1725345125385359, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.00010641370317898691}, {"id": 157, "seek": 84028, "start": 840.28, "end": 846.6, "text": " here in the CI CD dev room, Jenkins, CI CD is much more than just Jenkins.", "tokens": [510, 294, 264, 37777, 6743, 1905, 1808, 11, 41273, 11, 37777, 6743, 307, 709, 544, 813, 445, 41273, 13], "temperature": 0.0, "avg_logprob": -0.17358650069638906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.174862498184666e-05}, {"id": 158, "seek": 84028, "start": 846.6, "end": 849.88, "text": " So what else?", "tokens": [407, 437, 1646, 30], "temperature": 0.0, "avg_logprob": -0.17358650069638906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.174862498184666e-05}, {"id": 159, "seek": 84028, "start": 849.88, "end": 854.72, "text": " So we wanted to analyze if you remember the original requirements to analyze flaky tests", "tokens": [407, 321, 1415, 281, 12477, 498, 291, 1604, 264, 3380, 7728, 281, 12477, 932, 15681, 6921], "temperature": 0.0, "avg_logprob": -0.17358650069638906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.174862498184666e-05}, {"id": 160, "seek": 84028, "start": 854.72, "end": 862.52, "text": " and test performance and following the same process, collecting all the relevant information", "tokens": [293, 1500, 3389, 293, 3480, 264, 912, 1399, 11, 12510, 439, 264, 7340, 1589], "temperature": 0.0, "avg_logprob": -0.17358650069638906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.174862498184666e-05}, {"id": 161, "seek": 84028, "start": 862.52, "end": 869.72, "text": " from test run and storing it in elastic search and open search and then creating a cabana", "tokens": [490, 1500, 1190, 293, 26085, 309, 294, 17115, 3164, 293, 1269, 3164, 293, 550, 4084, 257, 5487, 2095], "temperature": 0.0, "avg_logprob": -0.17358650069638906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.174862498184666e-05}, {"id": 162, "seek": 86972, "start": 869.72, "end": 876.76, "text": " dashboard or open search dashboards and as you can see very all the relevant usual suspects", "tokens": [18342, 420, 1269, 3164, 8240, 17228, 293, 382, 291, 393, 536, 588, 439, 264, 7340, 7713, 35667], "temperature": 0.0, "avg_logprob": -0.1782004524679745, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.0003929078811779618}, {"id": 163, "seek": 86972, "start": 876.76, "end": 883.0, "text": " that you'd expect, the test duration, fail test, flaky test, failure count and rate moving", "tokens": [300, 291, 1116, 2066, 11, 264, 1500, 16365, 11, 3061, 1500, 11, 932, 15681, 1500, 11, 7763, 1207, 293, 3314, 2684], "temperature": 0.0, "avg_logprob": -0.1782004524679745, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.0003929078811779618}, {"id": 164, "seek": 86972, "start": 883.0, "end": 888.1600000000001, "text": " averages, fail test by branch over time, all of the things that you would need in order", "tokens": [42257, 11, 3061, 1500, 538, 9819, 670, 565, 11, 439, 295, 264, 721, 300, 291, 576, 643, 294, 1668], "temperature": 0.0, "avg_logprob": -0.1782004524679745, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.0003929078811779618}, {"id": 165, "seek": 86972, "start": 888.1600000000001, "end": 896.1600000000001, "text": " to analyze and understand the impact of your test and the flaky tests in your system.", "tokens": [281, 12477, 293, 1223, 264, 2712, 295, 428, 1500, 293, 264, 932, 15681, 6921, 294, 428, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1782004524679745, "compression_ratio": 1.771144278606965, "no_speech_prob": 0.0003929078811779618}, {"id": 166, "seek": 89616, "start": 896.16, "end": 903.28, "text": " And similarly after visualize you can also report, we created reports to Slack, we have", "tokens": [400, 14138, 934, 23273, 291, 393, 611, 2275, 11, 321, 2942, 7122, 281, 37211, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.133047243443931, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00014727312372997403}, {"id": 167, "seek": 89616, "start": 903.28, "end": 908.12, "text": " a dedicated Slack channel for that, following the same pattern.", "tokens": [257, 8374, 37211, 2269, 337, 300, 11, 3480, 264, 912, 5102, 13], "temperature": 0.0, "avg_logprob": -0.133047243443931, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00014727312372997403}, {"id": 168, "seek": 89616, "start": 908.12, "end": 910.8, "text": " One important point is about the openness.", "tokens": [1485, 1021, 935, 307, 466, 264, 36200, 13], "temperature": 0.0, "avg_logprob": -0.133047243443931, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00014727312372997403}, {"id": 169, "seek": 89616, "start": 910.8, "end": 916.56, "text": " So once you have the data in open search or in elastic search, it's very easy for different", "tokens": [407, 1564, 291, 362, 264, 1412, 294, 1269, 3164, 420, 294, 17115, 3164, 11, 309, 311, 588, 1858, 337, 819], "temperature": 0.0, "avg_logprob": -0.133047243443931, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00014727312372997403}, {"id": 170, "seek": 89616, "start": 916.56, "end": 921.12, "text": " teams to create different visualizations on top of that same data.", "tokens": [5491, 281, 1884, 819, 5056, 14455, 322, 1192, 295, 300, 912, 1412, 13], "temperature": 0.0, "avg_logprob": -0.133047243443931, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00014727312372997403}, {"id": 171, "seek": 92112, "start": 921.12, "end": 926.16, "text": " So I took another extreme, a different team that didn't like the graphs and preferred", "tokens": [407, 286, 1890, 1071, 8084, 11, 257, 819, 1469, 300, 994, 380, 411, 264, 24877, 293, 16494], "temperature": 0.0, "avg_logprob": -0.17168124691470638, "compression_ratio": 1.5528455284552845, "no_speech_prob": 8.481422264594585e-05}, {"id": 172, "seek": 92112, "start": 926.16, "end": 937.24, "text": " the table views and the counters to visualize, again, very similarly, test stats and so on.", "tokens": [264, 3199, 6809, 293, 264, 39338, 281, 23273, 11, 797, 11, 588, 14138, 11, 1500, 18152, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.17168124691470638, "compression_ratio": 1.5528455284552845, "no_speech_prob": 8.481422264594585e-05}, {"id": 173, "seek": 92112, "start": 937.24, "end": 940.44, "text": " And that's the beauty of it.", "tokens": [400, 300, 311, 264, 6643, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.17168124691470638, "compression_ratio": 1.5528455284552845, "no_speech_prob": 8.481422264594585e-05}, {"id": 174, "seek": 92112, "start": 940.44, "end": 946.0, "text": " So just to summarize, we instrumented Jenkins pipeline to collect relevant data and put", "tokens": [407, 445, 281, 20858, 11, 321, 7198, 292, 41273, 15517, 281, 2500, 7340, 1412, 293, 829], "temperature": 0.0, "avg_logprob": -0.17168124691470638, "compression_ratio": 1.5528455284552845, "no_speech_prob": 8.481422264594585e-05}, {"id": 175, "seek": 92112, "start": 946.0, "end": 950.72, "text": " it in environment variables, then at the end of the pipeline we created a JSON with all", "tokens": [309, 294, 2823, 9102, 11, 550, 412, 264, 917, 295, 264, 15517, 321, 2942, 257, 31828, 365, 439], "temperature": 0.0, "avg_logprob": -0.17168124691470638, "compression_ratio": 1.5528455284552845, "no_speech_prob": 8.481422264594585e-05}, {"id": 176, "seek": 95072, "start": 950.72, "end": 957.6, "text": " this data and persisted it to elastic search open search, then we created Kibana dashboards", "tokens": [341, 1412, 293, 13233, 292, 309, 281, 17115, 3164, 1269, 3164, 11, 550, 321, 2942, 591, 897, 2095, 8240, 17228], "temperature": 0.0, "avg_logprob": -0.15073231104257945, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00031992621370591223}, {"id": 177, "seek": 95072, "start": 957.6, "end": 963.36, "text": " on top of that data and lastly we created reports and alerts on that data.", "tokens": [322, 1192, 295, 300, 1412, 293, 16386, 321, 2942, 7122, 293, 28061, 322, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15073231104257945, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00031992621370591223}, {"id": 178, "seek": 95072, "start": 963.36, "end": 970.0400000000001, "text": " So four steps, collect, store, visualize and report.", "tokens": [407, 1451, 4439, 11, 2500, 11, 3531, 11, 23273, 293, 2275, 13], "temperature": 0.0, "avg_logprob": -0.15073231104257945, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00031992621370591223}, {"id": 179, "seek": 95072, "start": 970.0400000000001, "end": 973.88, "text": " So that was our first step in the journey but we didn't stop there.", "tokens": [407, 300, 390, 527, 700, 1823, 294, 264, 4671, 457, 321, 994, 380, 1590, 456, 13], "temperature": 0.0, "avg_logprob": -0.15073231104257945, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00031992621370591223}, {"id": 180, "seek": 97388, "start": 973.88, "end": 982.4399999999999, "text": " The next step was we asked ourselves, what can we do in order to investigate performance", "tokens": [440, 958, 1823, 390, 321, 2351, 4175, 11, 437, 393, 321, 360, 294, 1668, 281, 15013, 3389], "temperature": 0.0, "avg_logprob": -0.18454852918299233, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00013105629477649927}, {"id": 181, "seek": 97388, "start": 982.4399999999999, "end": 984.68, "text": " of specific pipeline runs?", "tokens": [295, 2685, 15517, 6676, 30], "temperature": 0.0, "avg_logprob": -0.18454852918299233, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00013105629477649927}, {"id": 182, "seek": 97388, "start": 984.68, "end": 990.12, "text": " So you have a run that takes a lot of time, you want to optimize, but where is the problem?", "tokens": [407, 291, 362, 257, 1190, 300, 2516, 257, 688, 295, 565, 11, 291, 528, 281, 19719, 11, 457, 689, 307, 264, 1154, 30], "temperature": 0.0, "avg_logprob": -0.18454852918299233, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00013105629477649927}, {"id": 183, "seek": 97388, "start": 990.12, "end": 996.48, "text": " And that's actually what distributed tracing is ideal for.", "tokens": [400, 300, 311, 767, 437, 12631, 25262, 307, 7157, 337, 13], "temperature": 0.0, "avg_logprob": -0.18454852918299233, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00013105629477649927}, {"id": 184, "seek": 97388, "start": 996.48, "end": 1000.2, "text": " How many people know what distributed tracing is with a show of hands?", "tokens": [1012, 867, 561, 458, 437, 12631, 25262, 307, 365, 257, 855, 295, 2377, 30], "temperature": 0.0, "avg_logprob": -0.18454852918299233, "compression_ratio": 1.574766355140187, "no_speech_prob": 0.00013105629477649927}, {"id": 185, "seek": 100020, "start": 1000.2, "end": 1004.44, "text": " Okay, I see that most of us, there are a few that know, so maybe I'll say a word about", "tokens": [1033, 11, 286, 536, 300, 881, 295, 505, 11, 456, 366, 257, 1326, 300, 458, 11, 370, 1310, 286, 603, 584, 257, 1349, 466], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 186, "seek": 100020, "start": 1004.44, "end": 1006.08, "text": " that soon.", "tokens": [300, 2321, 13], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 187, "seek": 100020, "start": 1006.08, "end": 1012.6, "text": " Very importantly, Jenkins has the capability to emit trace data spans, just like it does", "tokens": [4372, 8906, 11, 41273, 575, 264, 13759, 281, 32084, 13508, 1412, 44086, 11, 445, 411, 309, 775], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 188, "seek": 100020, "start": 1012.6, "end": 1015.36, "text": " for logs, so it's already built in.", "tokens": [337, 20820, 11, 370, 309, 311, 1217, 3094, 294, 13], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 189, "seek": 100020, "start": 1015.36, "end": 1020.72, "text": " So we decided to visualize jobs and pipeline executions as distributed tracing.", "tokens": [407, 321, 3047, 281, 23273, 4782, 293, 15517, 4454, 3666, 382, 12631, 25262, 13], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 190, "seek": 100020, "start": 1020.72, "end": 1025.32, "text": " That was the next step.", "tokens": [663, 390, 264, 958, 1823, 13], "temperature": 0.0, "avg_logprob": -0.15325751087882303, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.00043826340697705746}, {"id": 191, "seek": 102532, "start": 1025.32, "end": 1032.8, "text": " And for those who don't know, distributed tracing essentially helps pinpoint where issues occur", "tokens": [400, 337, 729, 567, 500, 380, 458, 11, 12631, 25262, 4476, 3665, 40837, 689, 2663, 5160], "temperature": 0.0, "avg_logprob": -0.16529334463724277, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0004322793975006789}, {"id": 192, "seek": 102532, "start": 1032.8, "end": 1039.56, "text": " and where latency is in production environments, in distributed systems, it's not specific", "tokens": [293, 689, 27043, 307, 294, 4265, 12388, 11, 294, 12631, 3652, 11, 309, 311, 406, 2685], "temperature": 0.0, "avg_logprob": -0.16529334463724277, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0004322793975006789}, {"id": 193, "seek": 102532, "start": 1039.56, "end": 1040.56, "text": " for CICD.", "tokens": [337, 383, 2532, 35, 13], "temperature": 0.0, "avg_logprob": -0.16529334463724277, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0004322793975006789}, {"id": 194, "seek": 102532, "start": 1040.56, "end": 1045.08, "text": " If you think about a microservice architecture and a request coming in and flowing through", "tokens": [759, 291, 519, 466, 257, 15547, 25006, 9482, 293, 257, 5308, 1348, 294, 293, 13974, 807], "temperature": 0.0, "avg_logprob": -0.16529334463724277, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0004322793975006789}, {"id": 195, "seek": 102532, "start": 1045.08, "end": 1051.0, "text": " a chain of interacting microservices, then when something goes wrong, you get an error", "tokens": [257, 5021, 295, 18017, 15547, 47480, 11, 550, 562, 746, 1709, 2085, 11, 291, 483, 364, 6713], "temperature": 0.0, "avg_logprob": -0.16529334463724277, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0004322793975006789}, {"id": 196, "seek": 105100, "start": 1051.0, "end": 1055.28, "text": " on that request, you want to know where the error is within this chain, or if there's a", "tokens": [322, 300, 5308, 11, 291, 528, 281, 458, 689, 264, 6713, 307, 1951, 341, 5021, 11, 420, 498, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 197, "seek": 105100, "start": 1055.28, "end": 1058.68, "text": " latency, you want to know where the latency is.", "tokens": [27043, 11, 291, 528, 281, 458, 689, 264, 27043, 307, 13], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 198, "seek": 105100, "start": 1058.68, "end": 1060.52, "text": " That's distributed tracing in a nutshell.", "tokens": [663, 311, 12631, 25262, 294, 257, 37711, 13], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 199, "seek": 105100, "start": 1060.52, "end": 1065.76, "text": " And the way it works is that each step in this call chain, or in our case, each step", "tokens": [400, 264, 636, 309, 1985, 307, 300, 1184, 1823, 294, 341, 818, 5021, 11, 420, 294, 527, 1389, 11, 1184, 1823], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 200, "seek": 105100, "start": 1065.76, "end": 1070.32, "text": " in the pipeline, creates and emits a span.", "tokens": [294, 264, 15517, 11, 7829, 293, 846, 1208, 257, 16174, 13], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 201, "seek": 105100, "start": 1070.32, "end": 1075.52, "text": " You can think about a span as a structured log that also contains the trace ID, the start", "tokens": [509, 393, 519, 466, 257, 16174, 382, 257, 18519, 3565, 300, 611, 8306, 264, 13508, 7348, 11, 264, 722], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 202, "seek": 105100, "start": 1075.52, "end": 1078.16, "text": " time, the duration, and some other context.", "tokens": [565, 11, 264, 16365, 11, 293, 512, 661, 4319, 13], "temperature": 0.0, "avg_logprob": -0.16569655980819312, "compression_ratio": 1.7845528455284554, "no_speech_prob": 8.400314254686236e-05}, {"id": 203, "seek": 107816, "start": 1078.16, "end": 1082.3600000000001, "text": " And then there is a back end that collects all these spans, reconstruct the trace, and", "tokens": [400, 550, 456, 307, 257, 646, 917, 300, 39897, 439, 613, 44086, 11, 31499, 264, 13508, 11, 293], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 204, "seek": 107816, "start": 1082.3600000000001, "end": 1091.5600000000002, "text": " then visualizes it typically in this timeline view or gun chart that you can see on the", "tokens": [550, 5056, 5660, 309, 5850, 294, 341, 12933, 1910, 420, 3874, 6927, 300, 291, 393, 536, 322, 264], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 205, "seek": 107816, "start": 1091.5600000000002, "end": 1093.1200000000001, "text": " right-hand side.", "tokens": [558, 12, 5543, 1252, 13], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 206, "seek": 107816, "start": 1093.1200000000001, "end": 1098.72, "text": " So now that we understand the distributed tracing, let's see how we add distributed", "tokens": [407, 586, 300, 321, 1223, 264, 12631, 25262, 11, 718, 311, 536, 577, 321, 909, 12631], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 207, "seek": 107816, "start": 1098.72, "end": 1105.52, "text": " tracing type of performance, pipeline performance into a CICD pipeline.", "tokens": [25262, 2010, 295, 3389, 11, 15517, 3389, 666, 257, 383, 2532, 35, 15517, 13], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 208, "seek": 107816, "start": 1105.52, "end": 1107.44, "text": " And same process.", "tokens": [400, 912, 1399, 13], "temperature": 0.0, "avg_logprob": -0.18396538295102924, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.00039603642653673887}, {"id": 209, "seek": 110744, "start": 1107.44, "end": 1109.28, "text": " For the collect step, collect.", "tokens": [1171, 264, 2500, 1823, 11, 2500, 13], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 210, "seek": 110744, "start": 1109.28, "end": 1118.8400000000001, "text": " And for the collect step, we decided to use an open telemetry collector who doesn't know", "tokens": [400, 337, 264, 2500, 1823, 11, 321, 3047, 281, 764, 364, 1269, 4304, 5537, 627, 23960, 567, 1177, 380, 458], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 211, "seek": 110744, "start": 1118.8400000000001, "end": 1123.04, "text": " about open telemetry, who doesn't know the project, just so I have a background, okay.", "tokens": [466, 1269, 4304, 5537, 627, 11, 567, 1177, 380, 458, 264, 1716, 11, 445, 370, 286, 362, 257, 3678, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 212, "seek": 110744, "start": 1123.04, "end": 1127.52, "text": " I have a few, so I'll say a word about that.", "tokens": [286, 362, 257, 1326, 11, 370, 286, 603, 584, 257, 1349, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 213, "seek": 110744, "start": 1127.52, "end": 1134.0800000000002, "text": " And anyway, I added a link, you see a QR code and a link at the lower corner there for a", "tokens": [400, 4033, 11, 286, 3869, 257, 2113, 11, 291, 536, 257, 32784, 3089, 293, 257, 2113, 412, 264, 3126, 4538, 456, 337, 257], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 214, "seek": 110744, "start": 1134.0800000000002, "end": 1136.88, "text": " beginner's guide to open telemetry that I wrote.", "tokens": [22080, 311, 5934, 281, 1269, 4304, 5537, 627, 300, 286, 4114, 13], "temperature": 0.0, "avg_logprob": -0.22778197697230748, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.0013404203345999122}, {"id": 215, "seek": 113688, "start": 1136.88, "end": 1141.3600000000001, "text": " I gave a talk about open telemetry at KubeCon Europe, so you'll find it useful.", "tokens": [286, 2729, 257, 751, 466, 1269, 4304, 5537, 627, 412, 591, 1977, 9838, 3315, 11, 370, 291, 603, 915, 309, 4420, 13], "temperature": 0.0, "avg_logprob": -0.1678243251073928, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0002744391094893217}, {"id": 216, "seek": 113688, "start": 1141.3600000000001, "end": 1149.64, "text": " But very briefly, it's an observability platform for collecting logs, metrics, and traces.", "tokens": [583, 588, 10515, 11, 309, 311, 364, 9951, 2310, 3663, 337, 12510, 20820, 11, 16367, 11, 293, 26076, 13], "temperature": 0.0, "avg_logprob": -0.1678243251073928, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0002744391094893217}, {"id": 217, "seek": 113688, "start": 1149.64, "end": 1156.44, "text": " So it's not specific only to traces in an open unified standard manner.", "tokens": [407, 309, 311, 406, 2685, 787, 281, 26076, 294, 364, 1269, 26787, 3832, 9060, 13], "temperature": 0.0, "avg_logprob": -0.1678243251073928, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0002744391094893217}, {"id": 218, "seek": 113688, "start": 1156.44, "end": 1162.7600000000002, "text": " It's an open source project under the CNCF, the Cloud Native Computing Foundation.", "tokens": [467, 311, 364, 1269, 4009, 1716, 833, 264, 48714, 37, 11, 264, 8061, 15093, 37804, 278, 10335, 13], "temperature": 0.0, "avg_logprob": -0.1678243251073928, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0002744391094893217}, {"id": 219, "seek": 116276, "start": 1162.76, "end": 1168.32, "text": " And at the time, it's a fairly young project by the time, the tracing piece of open telemetry", "tokens": [400, 412, 264, 565, 11, 309, 311, 257, 6457, 2037, 1716, 538, 264, 565, 11, 264, 25262, 2522, 295, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 220, "seek": 116276, "start": 1168.32, "end": 1172.2, "text": " was already GA generally available, so we decided to go with that.", "tokens": [390, 1217, 22841, 5101, 2435, 11, 370, 321, 3047, 281, 352, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 221, "seek": 116276, "start": 1172.2, "end": 1178.16, "text": " Today, by the way, also metrics is soon to be GA, it's already in release candidate,", "tokens": [2692, 11, 538, 264, 636, 11, 611, 16367, 307, 2321, 281, 312, 22841, 11, 309, 311, 1217, 294, 4374, 11532, 11], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 222, "seek": 116276, "start": 1178.16, "end": 1181.52, "text": " and logging is still not there.", "tokens": [293, 27991, 307, 920, 406, 456, 13], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 223, "seek": 116276, "start": 1181.52, "end": 1183.48, "text": " So what do you need to do if you choose open telemetry?", "tokens": [407, 437, 360, 291, 643, 281, 360, 498, 291, 2826, 1269, 4304, 5537, 627, 30], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 224, "seek": 116276, "start": 1183.48, "end": 1188.12, "text": " You need to set up the open telemetry collector, it's sort of an agent for it to send.", "tokens": [509, 643, 281, 992, 493, 264, 1269, 4304, 5537, 627, 23960, 11, 309, 311, 1333, 295, 364, 9461, 337, 309, 281, 2845, 13], "temperature": 0.0, "avg_logprob": -0.16619128039759448, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00011609258217504248}, {"id": 225, "seek": 118812, "start": 1188.12, "end": 1194.12, "text": " You need to install the Jenkins open telemetry plug-in, very easy to do that on the UI.", "tokens": [509, 643, 281, 3625, 264, 41273, 1269, 4304, 5537, 627, 5452, 12, 259, 11, 588, 1858, 281, 360, 300, 322, 264, 15682, 13], "temperature": 0.0, "avg_logprob": -0.18816229013296273, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022160446678753942}, {"id": 226, "seek": 118812, "start": 1194.12, "end": 1199.3999999999999, "text": " And then you need to configure the Jenkins open telemetry plug-in to send to the open", "tokens": [400, 550, 291, 643, 281, 22162, 264, 41273, 1269, 4304, 5537, 627, 5452, 12, 259, 281, 2845, 281, 264, 1269], "temperature": 0.0, "avg_logprob": -0.18816229013296273, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022160446678753942}, {"id": 227, "seek": 118812, "start": 1199.3999999999999, "end": 1205.2399999999998, "text": " telemetry collector and point over OTLP over GRPC protocol.", "tokens": [4304, 5537, 627, 23960, 293, 935, 670, 38617, 45196, 670, 10903, 12986, 10336, 13], "temperature": 0.0, "avg_logprob": -0.18816229013296273, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022160446678753942}, {"id": 228, "seek": 118812, "start": 1205.2399999999998, "end": 1210.28, "text": " That's the collect phase, and after collect comes store.", "tokens": [663, 311, 264, 2500, 5574, 11, 293, 934, 2500, 1487, 3531, 13], "temperature": 0.0, "avg_logprob": -0.18816229013296273, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022160446678753942}, {"id": 229, "seek": 118812, "start": 1210.28, "end": 1212.28, "text": " For the back end, we used Jega.", "tokens": [1171, 264, 646, 917, 11, 321, 1143, 508, 6335, 13], "temperature": 0.0, "avg_logprob": -0.18816229013296273, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022160446678753942}, {"id": 230, "seek": 121228, "start": 1212.28, "end": 1221.3999999999999, "text": " Jega is also a very popular open source under the CNCF, specifically for distributed tracing.", "tokens": [508, 6335, 307, 611, 257, 588, 3743, 1269, 4009, 833, 264, 48714, 37, 11, 4682, 337, 12631, 25262, 13], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 231, "seek": 121228, "start": 1221.3999999999999, "end": 1225.24, "text": " And we use Jega to monitor our own production environment, so that was our natural choice", "tokens": [400, 321, 764, 508, 6335, 281, 6002, 527, 1065, 4265, 2823, 11, 370, 300, 390, 527, 3303, 3922], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 232, "seek": 121228, "start": 1225.24, "end": 1227.2, "text": " also for this.", "tokens": [611, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 233, "seek": 121228, "start": 1227.2, "end": 1232.24, "text": " We also have a Jager-based service, so we just use that.", "tokens": [492, 611, 362, 257, 508, 3557, 12, 6032, 2643, 11, 370, 321, 445, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 234, "seek": 121228, "start": 1232.24, "end": 1238.24, "text": " But anything that I show here, actually you can use with any Jager distro, whichever one", "tokens": [583, 1340, 300, 286, 855, 510, 11, 767, 291, 393, 764, 365, 604, 508, 3557, 1483, 340, 11, 24123, 472], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 235, "seek": 121228, "start": 1238.24, "end": 1241.3999999999999, "text": " you use, managed or self-serve.", "tokens": [291, 764, 11, 6453, 420, 2698, 12, 82, 3768, 13], "temperature": 0.0, "avg_logprob": -0.1575272531792669, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.00014693758566863835}, {"id": 236, "seek": 124140, "start": 1241.4, "end": 1246.6000000000001, "text": " And if you do run your own, by the way, I added the link on how to deploy Jager on Kubernetes", "tokens": [400, 498, 291, 360, 1190, 428, 1065, 11, 538, 264, 636, 11, 286, 3869, 264, 2113, 322, 577, 281, 7274, 508, 3557, 322, 23145], "temperature": 0.0, "avg_logprob": -0.13871302604675292, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0003797696845140308}, {"id": 237, "seek": 124140, "start": 1246.6000000000001, "end": 1253.48, "text": " in production, so you have a link there, a short link that I added, a very useful guide.", "tokens": [294, 4265, 11, 370, 291, 362, 257, 2113, 456, 11, 257, 2099, 2113, 300, 286, 3869, 11, 257, 588, 4420, 5934, 13], "temperature": 0.0, "avg_logprob": -0.13871302604675292, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0003797696845140308}, {"id": 238, "seek": 124140, "start": 1253.48, "end": 1254.48, "text": " So what do you need to do?", "tokens": [407, 437, 360, 291, 643, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.13871302604675292, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0003797696845140308}, {"id": 239, "seek": 124140, "start": 1254.48, "end": 1260.8000000000002, "text": " You need to configure open telemetry collector to export in open telemetry collector terms", "tokens": [509, 643, 281, 22162, 1269, 4304, 5537, 627, 23960, 281, 10725, 294, 1269, 4304, 5537, 627, 23960, 2115], "temperature": 0.0, "avg_logprob": -0.13871302604675292, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0003797696845140308}, {"id": 240, "seek": 124140, "start": 1260.8000000000002, "end": 1267.1200000000001, "text": " to export to Jager in the right format, all the aggregated information.", "tokens": [281, 10725, 281, 508, 3557, 294, 264, 558, 7877, 11, 439, 264, 16743, 770, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13871302604675292, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0003797696845140308}, {"id": 241, "seek": 126712, "start": 1267.12, "end": 1271.6399999999999, "text": " And once you have that, then you can visualize, the visualized part is much easier in this", "tokens": [400, 1564, 291, 362, 300, 11, 550, 291, 393, 23273, 11, 264, 5056, 1602, 644, 307, 709, 3571, 294, 341], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 242, "seek": 126712, "start": 1271.6399999999999, "end": 1277.6399999999999, "text": " case, because you have a Jager UI with predefined dashboard, you don't need to start composing", "tokens": [1389, 11, 570, 291, 362, 257, 508, 3557, 15682, 365, 659, 37716, 18342, 11, 291, 500, 380, 643, 281, 722, 715, 6110], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 243, "seek": 126712, "start": 1277.6399999999999, "end": 1278.6399999999999, "text": " visuals.", "tokens": [26035, 13], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 244, "seek": 126712, "start": 1278.6399999999999, "end": 1285.6799999999998, "text": " Essentially, what you can see here on the left-hand side, you can see this indented", "tokens": [23596, 11, 437, 291, 393, 536, 510, 322, 264, 1411, 12, 5543, 1252, 11, 291, 393, 536, 341, 1016, 6003], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 245, "seek": 126712, "start": 1285.6799999999998, "end": 1287.8799999999999, "text": " tree structure, and then on the right, the gun chart.", "tokens": [4230, 3877, 11, 293, 550, 322, 264, 558, 11, 264, 3874, 6927, 13], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 246, "seek": 126712, "start": 1287.8799999999999, "end": 1293.8, "text": " Each line here is a span, and it's very easy to see the pipeline sequence.", "tokens": [6947, 1622, 510, 307, 257, 16174, 11, 293, 309, 311, 588, 1858, 281, 536, 264, 15517, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1779461081968535, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0004527795244939625}, {"id": 247, "seek": 129380, "start": 1293.8, "end": 1298.52, "text": " The text is a bit small, but you can see, for each step of the pipeline, you can see", "tokens": [440, 2487, 307, 257, 857, 1359, 11, 457, 291, 393, 536, 11, 337, 1184, 1823, 295, 264, 15517, 11, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.11121074842370075, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.00033601917675696313}, {"id": 248, "seek": 129380, "start": 1298.52, "end": 1305.32, "text": " the duration, how much it took, you see which ones ran in parallel, and which ones ran sequentially.", "tokens": [264, 16365, 11, 577, 709, 309, 1890, 11, 291, 536, 597, 2306, 5872, 294, 8952, 11, 293, 597, 2306, 5872, 5123, 3137, 13], "temperature": 0.0, "avg_logprob": -0.11121074842370075, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.00033601917675696313}, {"id": 249, "seek": 129380, "start": 1305.32, "end": 1310.32, "text": " If you have a very long latency on the overall, you can see where most of the time is being", "tokens": [759, 291, 362, 257, 588, 938, 27043, 322, 264, 4787, 11, 291, 393, 536, 689, 881, 295, 264, 565, 307, 885], "temperature": 0.0, "avg_logprob": -0.11121074842370075, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.00033601917675696313}, {"id": 250, "seek": 129380, "start": 1310.32, "end": 1316.08, "text": " spent, where the critical path, where you best optimize, and so on.", "tokens": [4418, 11, 689, 264, 4924, 3100, 11, 689, 291, 1151, 19719, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.11121074842370075, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.00033601917675696313}, {"id": 251, "seek": 129380, "start": 1316.08, "end": 1322.28, "text": " And by the way, Jager also offers other views, like recently added the flame graph, and you", "tokens": [400, 538, 264, 636, 11, 508, 3557, 611, 7736, 661, 6809, 11, 411, 3938, 3869, 264, 13287, 4295, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.11121074842370075, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.00033601917675696313}, {"id": 252, "seek": 132228, "start": 1322.28, "end": 1326.72, "text": " have trace statistics, and graph view, and so on.", "tokens": [362, 13508, 12523, 11, 293, 4295, 1910, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 253, "seek": 132228, "start": 1326.72, "end": 1331.12, "text": " But this is what people are used to, so I'm showing the timeline view.", "tokens": [583, 341, 307, 437, 561, 366, 1143, 281, 11, 370, 286, 478, 4099, 264, 12933, 1910, 13], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 254, "seek": 132228, "start": 1331.12, "end": 1337.16, "text": " So that's on Jager, and of course, as we said before, CICD is more than just Jenkins, so", "tokens": [407, 300, 311, 322, 508, 3557, 11, 293, 295, 1164, 11, 382, 321, 848, 949, 11, 383, 2532, 35, 307, 544, 813, 445, 41273, 11, 370], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 255, "seek": 132228, "start": 1337.16, "end": 1344.32, "text": " what we can do beyond just Jenkins, and what you can do is actually to instrument additional", "tokens": [437, 321, 393, 360, 4399, 445, 41273, 11, 293, 437, 291, 393, 360, 307, 767, 281, 7198, 4497], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 256, "seek": 132228, "start": 1344.32, "end": 1351.24, "text": " pieces like Maven, Ansible, and other elements to get final granularity into your traces", "tokens": [3755, 411, 4042, 553, 11, 14590, 964, 11, 293, 661, 4959, 281, 483, 2572, 39962, 507, 666, 428, 26076], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 257, "seek": 132228, "start": 1351.24, "end": 1352.24, "text": " and steps.", "tokens": [293, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13851569114475076, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.00012838825932703912}, {"id": 258, "seek": 135224, "start": 1352.24, "end": 1357.04, "text": " For example, here, the things that you see in yellow is Maven build steps.", "tokens": [1171, 1365, 11, 510, 11, 264, 721, 300, 291, 536, 294, 5566, 307, 4042, 553, 1322, 4439, 13], "temperature": 0.0, "avg_logprob": -0.12369670867919921, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00013757027045357972}, {"id": 259, "seek": 135224, "start": 1357.04, "end": 1361.52, "text": " So what before used to be one black box span in the trace.", "tokens": [407, 437, 949, 1143, 281, 312, 472, 2211, 2424, 16174, 294, 264, 13508, 13], "temperature": 0.0, "avg_logprob": -0.12369670867919921, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00013757027045357972}, {"id": 260, "seek": 135224, "start": 1361.52, "end": 1366.0, "text": " Suddenly, now you can click, open, and see the different build steps, each one with its", "tokens": [21194, 11, 586, 291, 393, 2052, 11, 1269, 11, 293, 536, 264, 819, 1322, 4439, 11, 1184, 472, 365, 1080], "temperature": 0.0, "avg_logprob": -0.12369670867919921, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00013757027045357972}, {"id": 261, "seek": 135224, "start": 1366.0, "end": 1370.44, "text": " own duration, each one with its own context, and so on.", "tokens": [1065, 16365, 11, 1184, 472, 365, 1080, 1065, 4319, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12369670867919921, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00013757027045357972}, {"id": 262, "seek": 135224, "start": 1370.44, "end": 1377.28, "text": " So that's in a nutshell how we added tracing to our CICD pipeline.", "tokens": [407, 300, 311, 294, 257, 37711, 577, 321, 3869, 25262, 281, 527, 383, 2532, 35, 15517, 13], "temperature": 0.0, "avg_logprob": -0.12369670867919921, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.00013757027045357972}, {"id": 263, "seek": 137728, "start": 1377.28, "end": 1382.2, "text": " The next step is, as I mentioned before, many of the pipelines actually failed not because", "tokens": [440, 958, 1823, 307, 11, 382, 286, 2835, 949, 11, 867, 295, 264, 40168, 767, 7612, 406, 570], "temperature": 0.0, "avg_logprob": -0.12778005495176212, "compression_ratio": 1.6651982378854626, "no_speech_prob": 6.541467882925645e-05}, {"id": 264, "seek": 137728, "start": 1382.2, "end": 1386.2, "text": " of the released code, but because of the CICD environment.", "tokens": [295, 264, 4736, 3089, 11, 457, 570, 295, 264, 383, 2532, 35, 2823, 13], "temperature": 0.0, "avg_logprob": -0.12778005495176212, "compression_ratio": 1.6651982378854626, "no_speech_prob": 6.541467882925645e-05}, {"id": 265, "seek": 137728, "start": 1386.2, "end": 1389.8799999999999, "text": " So we decided to monitor metrics from the Jenkins servers and the environment.", "tokens": [407, 321, 3047, 281, 6002, 16367, 490, 264, 41273, 15909, 293, 264, 2823, 13], "temperature": 0.0, "avg_logprob": -0.12778005495176212, "compression_ratio": 1.6651982378854626, "no_speech_prob": 6.541467882925645e-05}, {"id": 266, "seek": 137728, "start": 1389.8799999999999, "end": 1395.6, "text": " It goes to the system, the containers, the JVM, essentially anything that could break", "tokens": [467, 1709, 281, 264, 1185, 11, 264, 17089, 11, 264, 508, 53, 44, 11, 4476, 1340, 300, 727, 1821], "temperature": 0.0, "avg_logprob": -0.12778005495176212, "compression_ratio": 1.6651982378854626, "no_speech_prob": 6.541467882925645e-05}, {"id": 267, "seek": 137728, "start": 1395.6, "end": 1400.16, "text": " irrespective of the released code, and following the same flow.", "tokens": [3418, 19575, 488, 295, 264, 4736, 3089, 11, 293, 3480, 264, 912, 3095, 13], "temperature": 0.0, "avg_logprob": -0.12778005495176212, "compression_ratio": 1.6651982378854626, "no_speech_prob": 6.541467882925645e-05}, {"id": 268, "seek": 140016, "start": 1400.16, "end": 1407.88, "text": " So the first step, collect, we use the telegraph, we use that in production, so we use that", "tokens": [407, 264, 700, 1823, 11, 2500, 11, 321, 764, 264, 4304, 34091, 11, 321, 764, 300, 294, 4265, 11, 370, 321, 764, 300], "temperature": 0.0, "avg_logprob": -0.17894963716205797, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.362358231446706e-05}, {"id": 269, "seek": 140016, "start": 1407.88, "end": 1415.1200000000001, "text": " here as well, that's an open source by inflex data, and essentially you need two steps.", "tokens": [510, 382, 731, 11, 300, 311, 364, 1269, 4009, 538, 1536, 2021, 1412, 11, 293, 4476, 291, 643, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.17894963716205797, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.362358231446706e-05}, {"id": 270, "seek": 140016, "start": 1415.1200000000001, "end": 1423.1200000000001, "text": " You need to first enable, configure, sorry, Jenkins to expose metrics in Prometheus format.", "tokens": [509, 643, 281, 700, 9528, 11, 22162, 11, 2597, 11, 41273, 281, 19219, 16367, 294, 2114, 649, 42209, 7877, 13], "temperature": 0.0, "avg_logprob": -0.17894963716205797, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.362358231446706e-05}, {"id": 271, "seek": 140016, "start": 1423.1200000000001, "end": 1429.0, "text": " We work a lot with Prometheus for metrics, so that was our natural choice, and that's", "tokens": [492, 589, 257, 688, 365, 2114, 649, 42209, 337, 16367, 11, 370, 300, 390, 527, 3303, 3922, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.17894963716205797, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.362358231446706e-05}, {"id": 272, "seek": 142900, "start": 1429.0, "end": 1433.72, "text": " a simple configuration in the Jenkins web UI, and then you need to install telegraph", "tokens": [257, 2199, 11694, 294, 264, 41273, 3670, 15682, 11, 293, 550, 291, 643, 281, 3625, 4304, 34091], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 273, "seek": 142900, "start": 1433.72, "end": 1438.12, "text": " if you don't already have that, and then make sure that it configured to scrape the", "tokens": [498, 291, 500, 380, 1217, 362, 300, 11, 293, 550, 652, 988, 300, 309, 30538, 281, 32827, 264], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 274, "seek": 142900, "start": 1438.12, "end": 1445.44, "text": " metrics off of the Jenkins server using the Prometheus input plugin.", "tokens": [16367, 766, 295, 264, 41273, 7154, 1228, 264, 2114, 649, 42209, 4846, 23407, 13], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 275, "seek": 142900, "start": 1445.44, "end": 1446.44, "text": " So that's the first step.", "tokens": [407, 300, 311, 264, 700, 1823, 13], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 276, "seek": 142900, "start": 1446.44, "end": 1451.56, "text": " The second step is on the store side.", "tokens": [440, 1150, 1823, 307, 322, 264, 3531, 1252, 13], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 277, "seek": 142900, "start": 1451.56, "end": 1454.88, "text": " As I mentioned, we use Prometheus for metrics, so we use that as well here.", "tokens": [1018, 286, 2835, 11, 321, 764, 2114, 649, 42209, 337, 16367, 11, 370, 321, 764, 300, 382, 731, 510, 13], "temperature": 0.0, "avg_logprob": -0.12565245772853043, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.000146342848893255}, {"id": 278, "seek": 145488, "start": 1454.88, "end": 1459.7600000000002, "text": " We even have our own managed Prometheus, so we use that, but anything that I show here", "tokens": [492, 754, 362, 527, 1065, 6453, 2114, 649, 42209, 11, 370, 321, 764, 300, 11, 457, 1340, 300, 286, 855, 510], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 279, "seek": 145488, "start": 1459.7600000000002, "end": 1466.8000000000002, "text": " is identical whether you use Prometheus or any Prometheus compatible backend.", "tokens": [307, 14800, 1968, 291, 764, 2114, 649, 42209, 420, 604, 2114, 649, 42209, 18218, 38087, 13], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 280, "seek": 145488, "start": 1466.8000000000002, "end": 1470.92, "text": " And essentially you need to configure telegraph to send the metrics to Prometheus, and you", "tokens": [400, 4476, 291, 643, 281, 22162, 4304, 34091, 281, 2845, 264, 16367, 281, 2114, 649, 42209, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 281, "seek": 145488, "start": 1470.92, "end": 1472.0400000000002, "text": " have two ways to do that.", "tokens": [362, 732, 2098, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 282, "seek": 145488, "start": 1472.0400000000002, "end": 1475.0, "text": " You can do that in pull mode or in push mode.", "tokens": [509, 393, 360, 300, 294, 2235, 4391, 420, 294, 2944, 4391, 13], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 283, "seek": 145488, "start": 1475.0, "end": 1480.68, "text": " So pull mode is the default for Prometheus, essentially when you configure a telegraph", "tokens": [407, 2235, 4391, 307, 264, 7576, 337, 2114, 649, 42209, 11, 4476, 562, 291, 22162, 257, 4304, 34091], "temperature": 0.0, "avg_logprob": -0.11526620276620454, "compression_ratio": 1.8237885462555066, "no_speech_prob": 7.788807124597952e-05}, {"id": 284, "seek": 148068, "start": 1480.68, "end": 1487.5600000000002, "text": " to expose a slash metrics endpoint, and then it can be exposed for Prometheus to scrape", "tokens": [281, 19219, 257, 17330, 16367, 35795, 11, 293, 550, 309, 393, 312, 9495, 337, 2114, 649, 42209, 281, 32827], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 285, "seek": 148068, "start": 1487.5600000000002, "end": 1488.5600000000002, "text": " it from.", "tokens": [309, 490, 13], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 286, "seek": 148068, "start": 1488.5600000000002, "end": 1493.28, "text": " If you want to do that, you use the Prometheus client output plugin, or if you want to do", "tokens": [759, 291, 528, 281, 360, 300, 11, 291, 764, 264, 2114, 649, 42209, 6423, 5598, 23407, 11, 420, 498, 291, 528, 281, 360], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 287, "seek": 148068, "start": 1493.28, "end": 1496.72, "text": " it in push mode, then you use the HTTP output plugin.", "tokens": [309, 294, 2944, 4391, 11, 550, 291, 764, 264, 33283, 5598, 23407, 13], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 288, "seek": 148068, "start": 1496.72, "end": 1502.68, "text": " Just an important note, make sure that you set the data format to Prometheus remote write.", "tokens": [1449, 364, 1021, 3637, 11, 652, 988, 300, 291, 992, 264, 1412, 7877, 281, 2114, 649, 42209, 8607, 2464, 13], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 289, "seek": 148068, "start": 1502.68, "end": 1506.52, "text": " So that's the store phase, and then once you have all the data in Prometheus, then it's", "tokens": [407, 300, 311, 264, 3531, 5574, 11, 293, 550, 1564, 291, 362, 439, 264, 1412, 294, 2114, 649, 42209, 11, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.09637894008470618, "compression_ratio": 1.837719298245614, "no_speech_prob": 7.470464333891869e-05}, {"id": 290, "seek": 150652, "start": 1506.52, "end": 1511.68, "text": " very easy to create Grafana dashboards on top of that.", "tokens": [588, 1858, 281, 1884, 8985, 69, 2095, 8240, 17228, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 291, "seek": 150652, "start": 1511.68, "end": 1514.52, "text": " And I gave some examples here.", "tokens": [400, 286, 2729, 512, 5110, 510, 13], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 292, "seek": 150652, "start": 1514.52, "end": 1519.12, "text": " You can filter, of course, by build type, by branch, machine ID, build number, and so", "tokens": [509, 393, 6608, 11, 295, 1164, 11, 538, 1322, 2010, 11, 538, 9819, 11, 3479, 7348, 11, 1322, 1230, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 293, "seek": 150652, "start": 1519.12, "end": 1520.36, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 294, "seek": 150652, "start": 1520.36, "end": 1525.24, "text": " And you can monitor in this example, this is a system monitoring, so CPU, memory, disk", "tokens": [400, 291, 393, 6002, 294, 341, 1365, 11, 341, 307, 257, 1185, 11028, 11, 370, 13199, 11, 4675, 11, 12355], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 295, "seek": 150652, "start": 1525.24, "end": 1527.08, "text": " usage, load, and so on.", "tokens": [14924, 11, 3677, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 296, "seek": 150652, "start": 1527.08, "end": 1535.6, "text": " You can monitor the Docker container, like the CPU, IO, inbound, outbound, disk usage,", "tokens": [509, 393, 6002, 264, 33772, 10129, 11, 411, 264, 13199, 11, 39839, 11, 294, 18767, 11, 484, 18767, 11, 12355, 14924, 11], "temperature": 0.0, "avg_logprob": -0.19732650550636086, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00027721250080503523}, {"id": 297, "seek": 153560, "start": 1535.6, "end": 1541.6, "text": " obviously the running, stopped, paused containers by Jenkins machine, everything that you'd", "tokens": [2745, 264, 2614, 11, 5936, 11, 46860, 17089, 538, 41273, 3479, 11, 1203, 300, 291, 1116], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 298, "seek": 153560, "start": 1541.6, "end": 1550.76, "text": " expect, and JVM metrics, by being a Java implementation, thread count, heap memory, garbage collection,", "tokens": [2066, 11, 293, 508, 53, 44, 16367, 11, 538, 885, 257, 10745, 11420, 11, 7207, 1207, 11, 33591, 4675, 11, 14150, 5765, 11], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 299, "seek": 153560, "start": 1550.76, "end": 1552.7199999999998, "text": " duration, things like that.", "tokens": [16365, 11, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 300, "seek": 153560, "start": 1552.7199999999998, "end": 1557.36, "text": " You can even, of course, monitor the Jenkins nodes, queues, executors themselves.", "tokens": [509, 393, 754, 11, 295, 1164, 11, 6002, 264, 41273, 13891, 11, 631, 1247, 11, 7568, 830, 2969, 13], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 301, "seek": 153560, "start": 1557.36, "end": 1560.0, "text": " So again, you have an example dashboard here.", "tokens": [407, 797, 11, 291, 362, 364, 1365, 18342, 510, 13], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 302, "seek": 153560, "start": 1560.0, "end": 1565.1999999999998, "text": " You can see the queue size, status breakdown, the Jenkins jobs, the count executed over", "tokens": [509, 393, 536, 264, 18639, 2744, 11, 6558, 18188, 11, 264, 41273, 4782, 11, 264, 1207, 17577, 670], "temperature": 0.0, "avg_logprob": -0.2191871427140146, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.00019559481006581336}, {"id": 303, "seek": 156520, "start": 1565.2, "end": 1568.16, "text": " time, breakdown by job status, and so on.", "tokens": [565, 11, 18188, 538, 1691, 6558, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 304, "seek": 156520, "start": 1568.16, "end": 1572.6000000000001, "text": " So this is the types, just to, obviously, lots of other visualizations that you can", "tokens": [407, 341, 307, 264, 3467, 11, 445, 281, 11, 2745, 11, 3195, 295, 661, 5056, 14455, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 305, "seek": 156520, "start": 1572.6000000000001, "end": 1574.96, "text": " create, and you can also create alerts.", "tokens": [1884, 11, 293, 291, 393, 611, 1884, 28061, 13], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 306, "seek": 156520, "start": 1574.96, "end": 1583.6000000000001, "text": " I won't show that in the lack of time, so just to summarize what we've seen.", "tokens": [286, 1582, 380, 855, 300, 294, 264, 5011, 295, 565, 11, 370, 445, 281, 20858, 437, 321, 600, 1612, 13], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 307, "seek": 156520, "start": 1583.6000000000001, "end": 1587.4, "text": " Treat your CICD the same as you treat your production.", "tokens": [20298, 428, 383, 2532, 35, 264, 912, 382, 291, 2387, 428, 4265, 13], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 308, "seek": 156520, "start": 1587.4, "end": 1593.32, "text": " For your production, use whatever, elastic search, open search, Grafana to monitor to", "tokens": [1171, 428, 4265, 11, 764, 2035, 11, 17115, 3164, 11, 1269, 3164, 11, 8985, 69, 2095, 281, 6002, 281], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 309, "seek": 156520, "start": 1593.32, "end": 1594.92, "text": " create observability.", "tokens": [1884, 9951, 2310, 13], "temperature": 0.0, "avg_logprob": -0.19629574681187537, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.0001749037328409031}, {"id": 310, "seek": 159492, "start": 1594.92, "end": 1602.4, "text": " Do the same with your CICD pipeline, and preferably leverage the same stack, the same tool chain", "tokens": [1144, 264, 912, 365, 428, 383, 2532, 35, 15517, 11, 293, 45916, 13982, 264, 912, 8630, 11, 264, 912, 2290, 5021], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 311, "seek": 159492, "start": 1602.4, "end": 1606.24, "text": " for that, and don't reinvent the wheel.", "tokens": [337, 300, 11, 293, 500, 380, 33477, 264, 5589, 13], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 312, "seek": 159492, "start": 1606.24, "end": 1607.92, "text": " That was our journey.", "tokens": [663, 390, 527, 4671, 13], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 313, "seek": 159492, "start": 1607.92, "end": 1613.64, "text": " As I mentioned, we wanted dashboards and aggregated views to see several pipelines across different", "tokens": [1018, 286, 2835, 11, 321, 1415, 8240, 17228, 293, 16743, 770, 6809, 281, 536, 2940, 40168, 2108, 819], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 314, "seek": 159492, "start": 1613.64, "end": 1616.1200000000001, "text": " run branches over time, and so on.", "tokens": [1190, 14770, 670, 565, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 315, "seek": 159492, "start": 1616.1200000000001, "end": 1622.8400000000001, "text": " We wanted historical data and controlled persistence off of the Jenkins servers to determine the", "tokens": [492, 1415, 8584, 1412, 293, 10164, 37617, 766, 295, 264, 41273, 15909, 281, 6997, 264], "temperature": 0.0, "avg_logprob": -0.19373669831649118, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.00024637722526676953}, {"id": 316, "seek": 162284, "start": 1622.84, "end": 1625.36, "text": " duration, the retention of that data.", "tokens": [16365, 11, 264, 22871, 295, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 317, "seek": 162284, "start": 1625.36, "end": 1630.08, "text": " We wanted reports and alerts to automate as much as possible, and lastly, we wanted test", "tokens": [492, 1415, 7122, 293, 28061, 281, 31605, 382, 709, 382, 1944, 11, 293, 16386, 11, 321, 1415, 1500], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 318, "seek": 162284, "start": 1630.08, "end": 1633.08, "text": " performance, flaky tests, and so on.", "tokens": [3389, 11, 932, 15681, 6921, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 319, "seek": 162284, "start": 1633.08, "end": 1635.6799999999998, "text": " You saw how we achieved that.", "tokens": [509, 1866, 577, 321, 11042, 300, 13], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 320, "seek": 162284, "start": 1635.6799999999998, "end": 1636.6799999999998, "text": " Four steps.", "tokens": [7451, 4439, 13], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 321, "seek": 162284, "start": 1636.6799999999998, "end": 1642.76, "text": " If there's one thing to take out of that talk, take this one, collect, store, visualize,", "tokens": [759, 456, 311, 472, 551, 281, 747, 484, 295, 300, 751, 11, 747, 341, 472, 11, 2500, 11, 3531, 11, 23273, 11], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 322, "seek": 162284, "start": 1642.76, "end": 1645.28, "text": " and report an alert.", "tokens": [293, 2275, 364, 9615, 13], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 323, "seek": 162284, "start": 1645.28, "end": 1651.8799999999999, "text": " And what we gained, just to summarize, significant improvement in our lead time for changes,", "tokens": [400, 437, 321, 12634, 11, 445, 281, 20858, 11, 4776, 10444, 294, 527, 1477, 565, 337, 2962, 11], "temperature": 0.0, "avg_logprob": -0.16907837194040282, "compression_ratio": 1.6126482213438735, "no_speech_prob": 7.357346476055682e-05}, {"id": 324, "seek": 165188, "start": 1651.88, "end": 1657.2, "text": " in our cycle time, if you remember the Dora metrics at the beginning.", "tokens": [294, 527, 6586, 565, 11, 498, 291, 1604, 264, 413, 3252, 16367, 412, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 325, "seek": 165188, "start": 1657.2, "end": 1664.96, "text": " On the way, we also got an improved developer-on-duty experience, much less of a sufferer there.", "tokens": [1282, 264, 636, 11, 321, 611, 658, 364, 9689, 10754, 12, 266, 12, 67, 6432, 1752, 11, 709, 1570, 295, 257, 9753, 260, 456, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 326, "seek": 165188, "start": 1664.96, "end": 1666.3200000000002, "text": " It's based on open source.", "tokens": [467, 311, 2361, 322, 1269, 4009, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 327, "seek": 165188, "start": 1666.3200000000002, "end": 1667.3200000000002, "text": " Very important.", "tokens": [4372, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 328, "seek": 165188, "start": 1667.3200000000002, "end": 1668.3200000000002, "text": " We're here on FOSDEM.", "tokens": [492, 434, 510, 322, 479, 4367, 35, 6683, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 329, "seek": 165188, "start": 1668.3200000000002, "end": 1673.2800000000002, "text": " So based on open search, open telemetry, Yeager, Prometheus, Telegraph, you saw the stack.", "tokens": [407, 2361, 322, 1269, 3164, 11, 1269, 4304, 5537, 627, 11, 835, 3557, 11, 2114, 649, 42209, 11, 1989, 6363, 2662, 11, 291, 1866, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 330, "seek": 165188, "start": 1673.2800000000002, "end": 1678.8000000000002, "text": " If you want more information, you have here a QR code for a guide to CICD observability", "tokens": [759, 291, 528, 544, 1589, 11, 291, 362, 510, 257, 32784, 3089, 337, 257, 5934, 281, 383, 2532, 35, 9951, 2310], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 331, "seek": 165188, "start": 1678.8000000000002, "end": 1679.8000000000002, "text": " that I wrote.", "tokens": [300, 286, 4114, 13], "temperature": 0.0, "avg_logprob": -0.15438898526705228, "compression_ratio": 1.5306859205776173, "no_speech_prob": 0.00044891698053106666}, {"id": 332, "seek": 167980, "start": 1679.8, "end": 1685.84, "text": " You're welcome to take a short or a bit short link and read more about this, but this was", "tokens": [509, 434, 2928, 281, 747, 257, 2099, 420, 257, 857, 2099, 2113, 293, 1401, 544, 466, 341, 11, 457, 341, 390], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 333, "seek": 167980, "start": 1685.84, "end": 1688.44, "text": " very much in a nutshell.", "tokens": [588, 709, 294, 257, 37711, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 334, "seek": 167980, "start": 1688.44, "end": 1690.52, "text": " Thank you very much for listening.", "tokens": [1044, 291, 588, 709, 337, 4764, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 335, "seek": 167980, "start": 1690.52, "end": 1693.48, "text": " I'm Doton Horvitz, and enjoy the rest of the conference.", "tokens": [286, 478, 413, 27794, 10691, 85, 6862, 11, 293, 2103, 264, 1472, 295, 264, 7586, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 336, "seek": 167980, "start": 1693.48, "end": 1700.28, "text": " I don't know if we have time for questions.", "tokens": [286, 500, 380, 458, 498, 321, 362, 565, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 337, "seek": 167980, "start": 1700.28, "end": 1701.28, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 338, "seek": 167980, "start": 1701.28, "end": 1704.68, "text": " So I'm here if you have questions or if you have a sticker, and may the open source be", "tokens": [407, 286, 478, 510, 498, 291, 362, 1651, 420, 498, 291, 362, 257, 20400, 11, 293, 815, 264, 1269, 4009, 312], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 339, "seek": 167980, "start": 1704.68, "end": 1705.68, "text": " with you.", "tokens": [365, 291, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 340, "seek": 167980, "start": 1705.68, "end": 1706.68, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2576038360595703, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0070551009848713875}, {"id": 341, "seek": 170668, "start": 1706.68, "end": 1711.96, "text": " We have time for questions, if there are any.", "tokens": [492, 362, 565, 337, 1651, 11, 498, 456, 366, 604, 13], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 342, "seek": 170668, "start": 1711.96, "end": 1715.44, "text": " We have time for questions, so if you want, we can just see for a few minutes.", "tokens": [492, 362, 565, 337, 1651, 11, 370, 498, 291, 528, 11, 321, 393, 445, 536, 337, 257, 1326, 2077, 13], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 343, "seek": 170668, "start": 1715.44, "end": 1716.44, "text": " Is that a question?", "tokens": [1119, 300, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 344, "seek": 170668, "start": 1716.44, "end": 1719.44, "text": " Yeah, the other question in the back.", "tokens": [865, 11, 264, 661, 1168, 294, 264, 646, 13], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 345, "seek": 170668, "start": 1719.44, "end": 1720.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 346, "seek": 170668, "start": 1720.44, "end": 1726.68, "text": " Which one do you want to be the first one to ask a question?", "tokens": [3013, 472, 360, 291, 528, 281, 312, 264, 700, 472, 281, 1029, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 347, "seek": 170668, "start": 1726.68, "end": 1727.68, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 348, "seek": 170668, "start": 1727.68, "end": 1731.0800000000002, "text": " So have you considered persistence?", "tokens": [407, 362, 291, 4888, 37617, 30], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 349, "seek": 170668, "start": 1731.0800000000002, "end": 1733.92, "text": " How long do you store your metrics and your traces?", "tokens": [1012, 938, 360, 291, 3531, 428, 16367, 293, 428, 26076, 30], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 350, "seek": 170668, "start": 1733.92, "end": 1735.3200000000002, "text": " Have you wondered about that?", "tokens": [3560, 291, 17055, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.3687655947623997, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.006024654023349285}, {"id": 351, "seek": 173532, "start": 1735.32, "end": 1738.32, "text": " And for how long at a time you store your metrics?", "tokens": [400, 337, 577, 938, 412, 257, 565, 291, 3531, 428, 16367, 30], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 352, "seek": 173532, "start": 1738.32, "end": 1739.32, "text": " So we have.", "tokens": [407, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 353, "seek": 173532, "start": 1739.32, "end": 1743.36, "text": " That was part of the original challenge when we used the Jenkins persistence, because when", "tokens": [663, 390, 644, 295, 264, 3380, 3430, 562, 321, 1143, 264, 41273, 37617, 11, 570, 562], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 354, "seek": 173532, "start": 1743.36, "end": 1746.8799999999999, "text": " you persist it on the nodes themselves, and obviously you're very limited, there's the", "tokens": [291, 13233, 309, 322, 264, 13891, 2969, 11, 293, 2745, 291, 434, 588, 5567, 11, 456, 311, 264], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 355, "seek": 173532, "start": 1746.8799999999999, "end": 1752.8799999999999, "text": " plugin that you can configure per days or per number of bills and so on.", "tokens": [23407, 300, 291, 393, 22162, 680, 1708, 420, 680, 1230, 295, 12433, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 356, "seek": 173532, "start": 1752.8799999999999, "end": 1757.3999999999999, "text": " When you do it off of that critical path, you have much more room to maneuver, and then", "tokens": [1133, 291, 360, 309, 766, 295, 300, 4924, 3100, 11, 291, 362, 709, 544, 1808, 281, 25976, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 357, "seek": 173532, "start": 1757.3999999999999, "end": 1759.76, "text": " it depends on the amount of data you collect.", "tokens": [309, 5946, 322, 264, 2372, 295, 1412, 291, 2500, 13], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 358, "seek": 173532, "start": 1759.76, "end": 1765.24, "text": " We started small, so we collected for longer periods, but the more it came with the app,", "tokens": [492, 1409, 1359, 11, 370, 321, 11087, 337, 2854, 13804, 11, 457, 264, 544, 309, 1361, 365, 264, 724, 11], "temperature": 0.0, "avg_logprob": -0.15847692916642375, "compression_ratio": 1.7015873015873015, "no_speech_prob": 0.0036177595611661673}, {"id": 359, "seek": 176524, "start": 1765.24, "end": 1769.4, "text": " the more the appetite grew, and people wanted more and more types of metrics and time series", "tokens": [264, 544, 264, 23996, 6109, 11, 293, 561, 1415, 544, 293, 544, 3467, 295, 16367, 293, 565, 2638], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 360, "seek": 176524, "start": 1769.4, "end": 1775.56, "text": " data, so we needed to be a bit more conservative, but it's very much dependent on your practices", "tokens": [1412, 11, 370, 321, 2978, 281, 312, 257, 857, 544, 13780, 11, 457, 309, 311, 588, 709, 12334, 322, 428, 7525], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 361, "seek": 176524, "start": 1775.56, "end": 1776.56, "text": " in terms of the data.", "tokens": [294, 2115, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 362, "seek": 176524, "start": 1776.56, "end": 1782.44, "text": " Yeah, the question was more about the process, so iterative, you explained it, so it starts", "tokens": [865, 11, 264, 1168, 390, 544, 466, 264, 1399, 11, 370, 17138, 1166, 11, 291, 8825, 309, 11, 370, 309, 3719], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 363, "seek": 176524, "start": 1782.44, "end": 1783.44, "text": " small.", "tokens": [1359, 13], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 364, "seek": 176524, "start": 1783.44, "end": 1784.44, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 365, "seek": 176524, "start": 1784.44, "end": 1786.68, "text": " And iterative is the best, because it really depends, you need to learn the patterns of", "tokens": [400, 17138, 1166, 307, 264, 1151, 11, 570, 309, 534, 5946, 11, 291, 643, 281, 1466, 264, 8294, 295], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 366, "seek": 176524, "start": 1786.68, "end": 1792.36, "text": " your data consumption, the telemetry, and then you can optimize the balance between having", "tokens": [428, 1412, 12126, 11, 264, 4304, 5537, 627, 11, 293, 550, 291, 393, 19719, 264, 4772, 1296, 1419], "temperature": 0.0, "avg_logprob": -0.22247167271891916, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.0026948414742946625}, {"id": 367, "seek": 179236, "start": 1792.36, "end": 1795.6799999999998, "text": " the observability and not overloading and overpricing costs.", "tokens": [264, 9951, 2310, 293, 406, 28777, 278, 293, 670, 79, 1341, 278, 5497, 13], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 368, "seek": 179236, "start": 1795.6799999999998, "end": 1796.6799999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 369, "seek": 179236, "start": 1796.6799999999998, "end": 1797.6799999999998, "text": " Thank you very, very interesting.", "tokens": [1044, 291, 588, 11, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 370, "seek": 179236, "start": 1797.6799999999998, "end": 1798.6799999999998, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 371, "seek": 179236, "start": 1798.6799999999998, "end": 1800.32, "text": " There was another question in the back, yeah?", "tokens": [821, 390, 1071, 1168, 294, 264, 646, 11, 1338, 30], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 372, "seek": 179236, "start": 1800.32, "end": 1801.32, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 373, "seek": 179236, "start": 1801.32, "end": 1806.6, "text": " So what was the most surprising insight that you've learned, good or bad, and how did you", "tokens": [407, 437, 390, 264, 881, 8830, 11269, 300, 291, 600, 3264, 11, 665, 420, 1578, 11, 293, 577, 630, 291], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 374, "seek": 179236, "start": 1806.6, "end": 1808.1999999999998, "text": " react to it?", "tokens": [4515, 281, 309, 30], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 375, "seek": 179236, "start": 1808.1999999999998, "end": 1813.04, "text": " I think I was most surprised personally about the amount of failures that occur because", "tokens": [286, 519, 286, 390, 881, 6100, 5665, 466, 264, 2372, 295, 20774, 300, 5160, 570], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 376, "seek": 179236, "start": 1813.04, "end": 1818.6799999999998, "text": " of the environment and what kinds of things, and how simple it is to just kill the machine,", "tokens": [295, 264, 2823, 293, 437, 3685, 295, 721, 11, 293, 577, 2199, 309, 307, 281, 445, 1961, 264, 3479, 11], "temperature": 0.0, "avg_logprob": -0.22476293245951334, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0010813428089022636}, {"id": 377, "seek": 181868, "start": 1818.68, "end": 1822.72, "text": " kill the instance, let the auto-scaler spin it back up, and you save yourself a lot of", "tokens": [1961, 264, 5197, 11, 718, 264, 8399, 12, 4417, 17148, 6060, 309, 646, 493, 11, 293, 291, 3155, 1803, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 378, "seek": 181868, "start": 1822.72, "end": 1826.48, "text": " hassle and a lot of waking people up at night, so that was astonishing.", "tokens": [39526, 293, 257, 688, 295, 20447, 561, 493, 412, 1818, 11, 370, 300, 390, 35264, 13], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 379, "seek": 181868, "start": 1826.48, "end": 1830.48, "text": " How many things are irrespective of the code and just environmental, and we took a lot", "tokens": [1012, 867, 721, 366, 3418, 19575, 488, 295, 264, 3089, 293, 445, 8303, 11, 293, 321, 1890, 257, 688], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 380, "seek": 181868, "start": 1830.48, "end": 1834.2, "text": " of learnings out there to make the environment more robust, to get people to clean after", "tokens": [295, 2539, 82, 484, 456, 281, 652, 264, 2823, 544, 13956, 11, 281, 483, 561, 281, 2541, 934], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 381, "seek": 181868, "start": 1834.2, "end": 1839.6000000000001, "text": " them, to automate the cleanups and things like that, that's what me was insightful.", "tokens": [552, 11, 281, 31605, 264, 2541, 7528, 293, 721, 411, 300, 11, 300, 311, 437, 385, 390, 46401, 13], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 382, "seek": 181868, "start": 1839.6000000000001, "end": 1840.6000000000001, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 383, "seek": 181868, "start": 1840.6000000000001, "end": 1841.6000000000001, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 384, "seek": 181868, "start": 1841.6000000000001, "end": 1844.6000000000001, "text": " Then I have one last one, sorry.", "tokens": [1396, 286, 362, 472, 1036, 472, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 385, "seek": 181868, "start": 1844.6000000000001, "end": 1845.6000000000001, "text": " No, no worries.", "tokens": [883, 11, 572, 16340, 13], "temperature": 0.0, "avg_logprob": -0.2220977641918041, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.00029374085715971887}, {"id": 386, "seek": 184560, "start": 1845.6, "end": 1850.8799999999999, "text": " My question is, who are usually the people looking at the dashboard, because I maintain", "tokens": [1222, 1168, 307, 11, 567, 366, 2673, 264, 561, 1237, 412, 264, 18342, 11, 570, 286, 6909], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 387, "seek": 184560, "start": 1850.8799999999999, "end": 1854.1999999999998, "text": " a lot of dashboard in the past, and sometimes I had a feeling that I was the only one looking", "tokens": [257, 688, 295, 18342, 294, 264, 1791, 11, 293, 2171, 286, 632, 257, 2633, 300, 286, 390, 264, 787, 472, 1237], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 388, "seek": 184560, "start": 1854.1999999999998, "end": 1858.7199999999998, "text": " at those dashboards, so I'm just wondering if you identify a type of people who really", "tokens": [412, 729, 8240, 17228, 11, 370, 286, 478, 445, 6359, 498, 291, 5876, 257, 2010, 295, 561, 567, 534], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 389, "seek": 184560, "start": 1858.7199999999998, "end": 1860.9599999999998, "text": " benefit from those dashboards.", "tokens": [5121, 490, 729, 8240, 17228, 13], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 390, "seek": 184560, "start": 1860.9599999999998, "end": 1867.48, "text": " So it's a very interesting question because we also learned and we changed the org structure", "tokens": [407, 309, 311, 257, 588, 1880, 1168, 570, 321, 611, 3264, 293, 321, 3105, 264, 14045, 3877], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 391, "seek": 184560, "start": 1867.48, "end": 1870.7199999999998, "text": " several times, so it moves between Dev and DevOps.", "tokens": [2940, 1413, 11, 370, 309, 6067, 1296, 9096, 293, 43051, 13], "temperature": 0.0, "avg_logprob": -0.21050658453078497, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.003086706856265664}, {"id": 392, "seek": 187072, "start": 1870.72, "end": 1876.2, "text": " We now have a release engineering team, so they are the main stakeholders to look at that,", "tokens": [492, 586, 362, 257, 4374, 7043, 1469, 11, 370, 436, 366, 264, 2135, 17779, 281, 574, 412, 300, 11], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 393, "seek": 187072, "start": 1876.2, "end": 1880.76, "text": " but this dashboard is the goal, as I said, the developer on duty, so everyone that is", "tokens": [457, 341, 18342, 307, 264, 3387, 11, 382, 286, 848, 11, 264, 10754, 322, 9776, 11, 370, 1518, 300, 307], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 394, "seek": 187072, "start": 1880.76, "end": 1887.3600000000001, "text": " now on call needs to see that, that's for sure, and the tier two, tier three, so let's", "tokens": [586, 322, 818, 2203, 281, 536, 300, 11, 300, 311, 337, 988, 11, 293, 264, 12362, 732, 11, 12362, 1045, 11, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 395, "seek": 187072, "start": 1887.3600000000001, "end": 1890.04, "text": " say the chain for that.", "tokens": [584, 264, 5021, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 396, "seek": 187072, "start": 1890.04, "end": 1895.68, "text": " You also use that as a high level also by the team leads in the developer side of things,", "tokens": [509, 611, 764, 300, 382, 257, 1090, 1496, 611, 538, 264, 1469, 6689, 294, 264, 10754, 1252, 295, 721, 11], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 397, "seek": 187072, "start": 1895.68, "end": 1899.44, "text": " so these are the main stakeholders, depending on if it's the critical part of the developer", "tokens": [370, 613, 366, 264, 2135, 17779, 11, 5413, 322, 498, 309, 311, 264, 4924, 644, 295, 264, 10754], "temperature": 0.0, "avg_logprob": -0.16517956000714262, "compression_ratio": 1.83203125, "no_speech_prob": 0.00045426766155287623}, {"id": 398, "seek": 189944, "start": 1899.44, "end": 1904.6000000000001, "text": " on duty and the tiers, or if it's the overall thing the health state in general by the release", "tokens": [322, 9776, 293, 264, 40563, 11, 420, 498, 309, 311, 264, 4787, 551, 264, 1585, 1785, 294, 2674, 538, 264, 4374], "temperature": 0.0, "avg_logprob": -0.25063958982142004, "compression_ratio": 1.355140186915888, "no_speech_prob": 0.006089878734201193}, {"id": 399, "seek": 189944, "start": 1904.6000000000001, "end": 1905.6000000000001, "text": " engineer.", "tokens": [11403, 13], "temperature": 0.0, "avg_logprob": -0.25063958982142004, "compression_ratio": 1.355140186915888, "no_speech_prob": 0.006089878734201193}, {"id": 400, "seek": 189944, "start": 1905.6000000000001, "end": 1906.6000000000001, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.25063958982142004, "compression_ratio": 1.355140186915888, "no_speech_prob": 0.006089878734201193}, {"id": 401, "seek": 190660, "start": 1906.6, "end": 1933.1599999999999, "text": " Thank you very much, everyone.", "tokens": [50364, 1044, 291, 588, 709, 11, 1518, 13, 51692], "temperature": 0.0, "avg_logprob": -0.5939821243286133, "compression_ratio": 0.8571428571428571, "no_speech_prob": 0.00023139074619393796}], "language": "en"}