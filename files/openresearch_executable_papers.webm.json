{"text": " Hello. Thank you very much for having us here. I'm Daniele Guido, together with my colleague Elisabeth Gerard. We are coming from the University of Luxembourg from the Center of Contemporary Digital History, where we're running this new journal, this new idea of journal, together with a publisher, a well-known publisher in the open access publication, which is The Groiter. So the idea is the journal of digitalhistory.org, and then the idea is how to bring reproducible papers in the humanities, and in digital history in our specific case. And then that's why we join forces with them. So it's a joint venture with them directly, so the team is relatively small compared to other projects, and then we have two perspectives that we decided to put together. On our side, we understand that academic publishing is a bit too traditional, especially in history. And then our researchers, they currently work on Jupyter Notebook to run their own experiment and so on. So the idea was can we pass from experiment on Jupyter Notebook to actual publication also in our domain. And on the other side, they wanted to test out this hypothesis, because they really want to engage with new publication practices, and this joint venture would just a good match. Then, well, reproducible papers in digital history means a lot of things, because first of all, we have now massive digitization process of primary sources and sacred literature and also new digital material, like the Twitter archive that we've been seeing before. On one side, the details of the code are crucial, so sharing the data set is one thing, but the other thing is really how this data has been created, so where the condition of production of this data. So this is very important for us as historians, not for me, for my colleague. And then interpretation, so how the data set has been built, which were the limits. All this question needs to be addressed in a different way. And then at the same time, we have, of course, new standards, not only in digital history, but also the famous fair principle, so findable, accessible, interoperable, and reusable data. And we need to meet this criteria also with our journal. And this idea of Mullen is the one of a braided narrative. So it advocates for bringing together two things. One is the narrative, so the argumentation of our publication. The other one is the interpretation of data, and say that they can be done in a narrative way. This is where we put these so-called multilayers together. So this one is like every article published in our journal has a fingerprint, sort of identity, where this level, like the narrative, the hermeneutic level, and the data layer are together. So this is the representation of one Jupyter notebook, which is normally linear, cell by cell. We just distort it, we put it in a circle, and here you can test it out. So this was also a tool, it is also a tool for our authors, which we own them a lot, because they are our primary tester, and it is still an experimental journal. And you could tweak with data, you can change with the content, and you see how the fingerprint is changing. This was just an experiment at the beginning, but then it really becomes integrated, it is down there, integrated into the main interface of the journal. And we saw that indeed they were very different. They were very different, and we can see also the code style of every Jupyter notebook, how the author decided to narrate the arguments. So I will go quickly, sorry. And then this is like the basic layer, so the narrative layer that looks like an MV viewer with steroids in the sense that we have figures, we have tables, we have bibliography with Zotero and Psy2C. And then above all, it is a very thin layer on top of the Jupyter notebook, because we use the usual output of the notebook, so this is very, yeah, an augmented MV viewer. And then we have, as it is a braided narrative, we decided to have this metaphor of this level one on top of the other. So this is a sort of animation, on the left you see the full hermeneutic layer, and on the other side you can see how it slides through the, like behind the narrative layer. And the data layer is for the moment, the part on top, right top, which we use MyBinder, fantastic service to publish online your notebooks. And we wanted this article not only to be show off of the data set, but also a small history lab, so that people could just click on the button and get to the data and understand how the data has been composed. The good thing is that we decided to keep this MyBinder as this source of truth. So the article that you see published is exactly the same copy, with just a different way of interacting with a different layer. So this is how it looks like on MyBinder, so it's a classical Jupyter notebook, and for every notebook we have a GitHub repo where we store all the requirements and all the images in the data set. We have to put together the fair metadata, but still, so it's under construction. Then what does it mean having Jupyter notebooks for publishing? We see that in the literature there are a lot of critics who shouldn't use Jupyter notebooks because it's too complex, it's impossible to replicate and so on and so forth. But then for us, it was really the simplest solution. So at the same time, to publish with Jupyter, we had to make our pipeline a bit more complex than usual. So we have a first review directly on the abstract, where we start communicating with the authors, understanding their needs, creating a writing environment for them that can be replicated with Python, sorry, with Docker containers for Python and Air. And then there's the first technical review, she's in charge of the first technical review, which is the most complicated one because there's a lot of checks. We saw some projects already, we needed to have checks. And then we have a lot of other open source software that enters this pipeline, like for the preview of the notebook with the GitHub app, MB Viewer, we have MyBinder, and this is just for the first technical review because then the article is being sent to the reviewer for the double banana review. So before even reviewing, we had to do this huge job because they have to review also the data and the pertinence of the dataset. And then finally, there is one important thing, it's English editing. So how to edit something that which is already being run, so without running itself. So this could be a tool for translators, tool for correctors that they're not into the Jupyter world. So how to do that? We have Jupyter text, we're still testing some plug-in to see if this could work without touching the final output. And then the final technical review, so after all this has been shipped, we have a DOI. So the article is now published and needs to be indexing and there is the problem of long-term archiving, which is a big problem for many reasons. First of all, like the libraries that get deprecated, also API that disappeared. So how to really reproduce this in the future? And then finally, the dataset needs to be included into, we have Dataverse, but we're looking for Zenodo in order to match the fair metadata. And time is up, I have a question for you, of course. Thank you very much, first of all. And then if you have want to contact us, just collaborate or work together on Jupyter publication, JDH admin at uni.lu. And then the question is, how can we actually collaborate on something which is a notebook that requires quite a threshold of expertise, not only for the researcher, but for the people that are around, and how to maintain all this and how to make this history love living for more than one year. Thank you. Yeah, well, I repeat the question. So he asked me if the double blind review, how can we keep it actually a real natural double blind? So she anonymized the data on GitHub. So we have specific repositories that have been created after the communication with the authors, where we only have the code without the names, but then you still have the bibliography, so it's easy to, it's a very small word, one of the digital history. But this is the way to maintain double blind. And then we're going to send the review where both the MyBinder and the version of the article on our website with a hidden URL. So this is the only thing that we can do. For sure, the double blind, we have the problem that we cannot really use the pull request directly on the GitHub repository. So in fact, there is some replication between the GitHub repository. Sometimes after with the peer review, there is some requisite that he come back to a technical review because there is a revision. So there is this question about how we re-synchronize the notebook together. There is some authors that they have good enough with GitHub, but to review a notebook with the output with the metadata to track what has been changed. That's why, yes, this it was, but yes, the questions that you have, we are testing with review and be or not also to maybe use some markdown or just Python script to produce several output in order to not sometimes touch about this metadata that they are inside the notebook. And there was another question, but I don't know if we have time. Yes. Yes. Yes. Please. Last one. Last one. Of the SS brightness of your data sets. Sorry, how to assess? Of the SS brightness of your data sets. Yeah, that's the very big, big, big question. So the idea behind the braided narrative is then you tell the story around the data on one side and on the other side you keep the data like with the Zenodo metadata coherent or probably with what Paul showed us before with Ricardo. So having like an external check on the metadata and on the data set itself. At the same time, the initial, the first technical review is the one where we assess actually the data. So if the data set are complete, coherent, we don't judge them because then we know that there are conditions of production. That needs to be, we try to make this as more explicit as possible. That's. Yes, exactly. And this, like that's why the long-term maintenance. So now we only have nine articles, but we have 28 in the pipeline in the coming year. So it's really now it's getting us up speed and we have more and more interaction with others which makes things more complicated. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.44, "text": " Hello. Thank you very much for having us here. I'm Daniele Guido, together with my colleague", "tokens": [50364, 2425, 13, 1044, 291, 588, 709, 337, 1419, 505, 510, 13, 286, 478, 3394, 15949, 2694, 2925, 11, 1214, 365, 452, 13532, 51136], "temperature": 0.0, "avg_logprob": -0.2753760580923043, "compression_ratio": 1.2925170068027212, "no_speech_prob": 0.1660682111978531}, {"id": 1, "seek": 0, "start": 15.44, "end": 23.240000000000002, "text": " Elisabeth Gerard. We are coming from the University of Luxembourg from the Center of Contemporary", "tokens": [51136, 2699, 271, 11744, 9409, 515, 13, 492, 366, 1348, 490, 264, 3535, 295, 25767, 443, 43638, 490, 264, 5169, 295, 4839, 11840, 822, 51526], "temperature": 0.0, "avg_logprob": -0.2753760580923043, "compression_ratio": 1.2925170068027212, "no_speech_prob": 0.1660682111978531}, {"id": 2, "seek": 2324, "start": 23.24, "end": 29.84, "text": " Digital History, where we're running this new journal, this new idea of journal, together", "tokens": [50364, 15522, 12486, 11, 689, 321, 434, 2614, 341, 777, 6708, 11, 341, 777, 1558, 295, 6708, 11, 1214, 50694], "temperature": 0.0, "avg_logprob": -0.2632770538330078, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.21228305995464325}, {"id": 3, "seek": 2324, "start": 29.84, "end": 36.92, "text": " with a publisher, a well-known publisher in the open access publication, which is The", "tokens": [50694, 365, 257, 25088, 11, 257, 731, 12, 6861, 25088, 294, 264, 1269, 2105, 19953, 11, 597, 307, 440, 51048], "temperature": 0.0, "avg_logprob": -0.2632770538330078, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.21228305995464325}, {"id": 4, "seek": 2324, "start": 36.92, "end": 46.4, "text": " Groiter. So the idea is the journal of digitalhistory.org, and then the idea is how to bring reproducible", "tokens": [51048, 12981, 1681, 13, 407, 264, 1558, 307, 264, 6708, 295, 4562, 33236, 827, 13, 4646, 11, 293, 550, 264, 1558, 307, 577, 281, 1565, 11408, 32128, 51522], "temperature": 0.0, "avg_logprob": -0.2632770538330078, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.21228305995464325}, {"id": 5, "seek": 4640, "start": 46.4, "end": 53.4, "text": " papers in the humanities, and in digital history in our specific case. And then that's", "tokens": [50364, 10577, 294, 264, 36140, 11, 293, 294, 4562, 2503, 294, 527, 2685, 1389, 13, 400, 550, 300, 311, 50714], "temperature": 0.0, "avg_logprob": -0.20034047535487584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.3326807916164398}, {"id": 6, "seek": 4640, "start": 53.4, "end": 58.6, "text": " why we join forces with them. So it's a joint venture with them directly, so the team is", "tokens": [50714, 983, 321, 3917, 5874, 365, 552, 13, 407, 309, 311, 257, 7225, 18474, 365, 552, 3838, 11, 370, 264, 1469, 307, 50974], "temperature": 0.0, "avg_logprob": -0.20034047535487584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.3326807916164398}, {"id": 7, "seek": 4640, "start": 58.6, "end": 64.92, "text": " relatively small compared to other projects, and then we have two perspectives that we", "tokens": [50974, 7226, 1359, 5347, 281, 661, 4455, 11, 293, 550, 321, 362, 732, 16766, 300, 321, 51290], "temperature": 0.0, "avg_logprob": -0.20034047535487584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.3326807916164398}, {"id": 8, "seek": 4640, "start": 64.92, "end": 73.56, "text": " decided to put together. On our side, we understand that academic publishing is a bit too traditional,", "tokens": [51290, 3047, 281, 829, 1214, 13, 1282, 527, 1252, 11, 321, 1223, 300, 7778, 17832, 307, 257, 857, 886, 5164, 11, 51722], "temperature": 0.0, "avg_logprob": -0.20034047535487584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.3326807916164398}, {"id": 9, "seek": 7356, "start": 73.56, "end": 81.24000000000001, "text": " especially in history. And then our researchers, they currently work on Jupyter Notebook to", "tokens": [50364, 2318, 294, 2503, 13, 400, 550, 527, 10309, 11, 436, 4362, 589, 322, 22125, 88, 391, 11633, 2939, 281, 50748], "temperature": 0.0, "avg_logprob": -0.16912177671869116, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.03033817559480667}, {"id": 10, "seek": 7356, "start": 81.24000000000001, "end": 89.24000000000001, "text": " run their own experiment and so on. So the idea was can we pass from experiment on Jupyter", "tokens": [50748, 1190, 641, 1065, 5120, 293, 370, 322, 13, 407, 264, 1558, 390, 393, 321, 1320, 490, 5120, 322, 22125, 88, 391, 51148], "temperature": 0.0, "avg_logprob": -0.16912177671869116, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.03033817559480667}, {"id": 11, "seek": 7356, "start": 89.24000000000001, "end": 94.08, "text": " Notebook to actual publication also in our domain. And on the other side, they wanted", "tokens": [51148, 11633, 2939, 281, 3539, 19953, 611, 294, 527, 9274, 13, 400, 322, 264, 661, 1252, 11, 436, 1415, 51390], "temperature": 0.0, "avg_logprob": -0.16912177671869116, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.03033817559480667}, {"id": 12, "seek": 7356, "start": 94.08, "end": 99.0, "text": " to test out this hypothesis, because they really want to engage with new publication", "tokens": [51390, 281, 1500, 484, 341, 17291, 11, 570, 436, 534, 528, 281, 4683, 365, 777, 19953, 51636], "temperature": 0.0, "avg_logprob": -0.16912177671869116, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.03033817559480667}, {"id": 13, "seek": 9900, "start": 99.0, "end": 106.08, "text": " practices, and this joint venture would just a good match.", "tokens": [50364, 7525, 11, 293, 341, 7225, 18474, 576, 445, 257, 665, 2995, 13, 50718], "temperature": 0.0, "avg_logprob": -0.24285267017505788, "compression_ratio": 1.4478527607361964, "no_speech_prob": 0.06935037672519684}, {"id": 14, "seek": 9900, "start": 106.08, "end": 114.56, "text": " Then, well, reproducible papers in digital history means a lot of things, because first", "tokens": [50718, 1396, 11, 731, 11, 11408, 32128, 10577, 294, 4562, 2503, 1355, 257, 688, 295, 721, 11, 570, 700, 51142], "temperature": 0.0, "avg_logprob": -0.24285267017505788, "compression_ratio": 1.4478527607361964, "no_speech_prob": 0.06935037672519684}, {"id": 15, "seek": 9900, "start": 114.56, "end": 123.32, "text": " of all, we have now massive digitization process of primary sources and sacred literature", "tokens": [51142, 295, 439, 11, 321, 362, 586, 5994, 14293, 2144, 1399, 295, 6194, 7139, 293, 15757, 10394, 51580], "temperature": 0.0, "avg_logprob": -0.24285267017505788, "compression_ratio": 1.4478527607361964, "no_speech_prob": 0.06935037672519684}, {"id": 16, "seek": 12332, "start": 123.32, "end": 131.0, "text": " and also new digital material, like the Twitter archive that we've been seeing before. On", "tokens": [50364, 293, 611, 777, 4562, 2527, 11, 411, 264, 5794, 23507, 300, 321, 600, 668, 2577, 949, 13, 1282, 50748], "temperature": 0.0, "avg_logprob": -0.22643175017967654, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.15788666903972626}, {"id": 17, "seek": 12332, "start": 131.0, "end": 135.51999999999998, "text": " one side, the details of the code are crucial, so sharing the data set is one thing, but", "tokens": [50748, 472, 1252, 11, 264, 4365, 295, 264, 3089, 366, 11462, 11, 370, 5414, 264, 1412, 992, 307, 472, 551, 11, 457, 50974], "temperature": 0.0, "avg_logprob": -0.22643175017967654, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.15788666903972626}, {"id": 18, "seek": 12332, "start": 135.51999999999998, "end": 141.56, "text": " the other thing is really how this data has been created, so where the condition of production", "tokens": [50974, 264, 661, 551, 307, 534, 577, 341, 1412, 575, 668, 2942, 11, 370, 689, 264, 4188, 295, 4265, 51276], "temperature": 0.0, "avg_logprob": -0.22643175017967654, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.15788666903972626}, {"id": 19, "seek": 12332, "start": 141.56, "end": 149.04, "text": " of this data. So this is very important for us as historians, not for me, for my colleague.", "tokens": [51276, 295, 341, 1412, 13, 407, 341, 307, 588, 1021, 337, 505, 382, 26442, 11, 406, 337, 385, 11, 337, 452, 13532, 13, 51650], "temperature": 0.0, "avg_logprob": -0.22643175017967654, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.15788666903972626}, {"id": 20, "seek": 14904, "start": 149.04, "end": 156.12, "text": " And then interpretation, so how the data set has been built, which were the limits. All", "tokens": [50364, 400, 550, 14174, 11, 370, 577, 264, 1412, 992, 575, 668, 3094, 11, 597, 645, 264, 10406, 13, 1057, 50718], "temperature": 0.0, "avg_logprob": -0.22805491553412544, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.27309396862983704}, {"id": 21, "seek": 14904, "start": 156.12, "end": 162.79999999999998, "text": " this question needs to be addressed in a different way. And then at the same time, we have, of", "tokens": [50718, 341, 1168, 2203, 281, 312, 13847, 294, 257, 819, 636, 13, 400, 550, 412, 264, 912, 565, 11, 321, 362, 11, 295, 51052], "temperature": 0.0, "avg_logprob": -0.22805491553412544, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.27309396862983704}, {"id": 22, "seek": 14904, "start": 162.79999999999998, "end": 168.32, "text": " course, new standards, not only in digital history, but also the famous fair principle,", "tokens": [51052, 1164, 11, 777, 7787, 11, 406, 787, 294, 4562, 2503, 11, 457, 611, 264, 4618, 3143, 8665, 11, 51328], "temperature": 0.0, "avg_logprob": -0.22805491553412544, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.27309396862983704}, {"id": 23, "seek": 14904, "start": 168.32, "end": 177.48, "text": " so findable, accessible, interoperable, and reusable data. And we need to meet this criteria", "tokens": [51328, 370, 915, 712, 11, 9515, 11, 728, 7192, 712, 11, 293, 41807, 1412, 13, 400, 321, 643, 281, 1677, 341, 11101, 51786], "temperature": 0.0, "avg_logprob": -0.22805491553412544, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.27309396862983704}, {"id": 24, "seek": 17748, "start": 177.48, "end": 186.23999999999998, "text": " also with our journal. And this idea of Mullen is the one of a braided narrative. So it advocates", "tokens": [50364, 611, 365, 527, 6708, 13, 400, 341, 1558, 295, 376, 32516, 307, 264, 472, 295, 257, 1548, 2112, 9977, 13, 407, 309, 25160, 50802], "temperature": 0.0, "avg_logprob": -0.2424742933632671, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.11932309716939926}, {"id": 25, "seek": 17748, "start": 186.23999999999998, "end": 194.12, "text": " for bringing together two things. One is the narrative, so the argumentation of our publication.", "tokens": [50802, 337, 5062, 1214, 732, 721, 13, 1485, 307, 264, 9977, 11, 370, 264, 6770, 399, 295, 527, 19953, 13, 51196], "temperature": 0.0, "avg_logprob": -0.2424742933632671, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.11932309716939926}, {"id": 26, "seek": 17748, "start": 194.12, "end": 199.6, "text": " The other one is the interpretation of data, and say that they can be done in a narrative", "tokens": [51196, 440, 661, 472, 307, 264, 14174, 295, 1412, 11, 293, 584, 300, 436, 393, 312, 1096, 294, 257, 9977, 51470], "temperature": 0.0, "avg_logprob": -0.2424742933632671, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.11932309716939926}, {"id": 27, "seek": 19960, "start": 199.6, "end": 209.76, "text": " way. This is where we put these so-called multilayers together. So this one is like every article", "tokens": [50364, 636, 13, 639, 307, 689, 321, 829, 613, 370, 12, 11880, 2120, 388, 320, 433, 1214, 13, 407, 341, 472, 307, 411, 633, 7222, 50872], "temperature": 0.0, "avg_logprob": -0.20493467648824057, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.32800182700157166}, {"id": 28, "seek": 19960, "start": 209.76, "end": 215.32, "text": " published in our journal has a fingerprint, sort of identity, where this level, like the", "tokens": [50872, 6572, 294, 527, 6708, 575, 257, 30715, 11, 1333, 295, 6575, 11, 689, 341, 1496, 11, 411, 264, 51150], "temperature": 0.0, "avg_logprob": -0.20493467648824057, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.32800182700157166}, {"id": 29, "seek": 19960, "start": 215.32, "end": 222.92, "text": " narrative, the hermeneutic level, and the data layer are together. So this is the representation", "tokens": [51150, 9977, 11, 264, 720, 76, 1450, 325, 299, 1496, 11, 293, 264, 1412, 4583, 366, 1214, 13, 407, 341, 307, 264, 10290, 51530], "temperature": 0.0, "avg_logprob": -0.20493467648824057, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.32800182700157166}, {"id": 30, "seek": 22292, "start": 222.92, "end": 229.64, "text": " of one Jupyter notebook, which is normally linear, cell by cell. We just distort it,", "tokens": [50364, 295, 472, 22125, 88, 391, 21060, 11, 597, 307, 5646, 8213, 11, 2815, 538, 2815, 13, 492, 445, 37555, 309, 11, 50700], "temperature": 0.0, "avg_logprob": -0.19198843773375165, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0971762165427208}, {"id": 31, "seek": 22292, "start": 229.64, "end": 235.92, "text": " we put it in a circle, and here you can test it out. So this was also a tool, it is also", "tokens": [50700, 321, 829, 309, 294, 257, 6329, 11, 293, 510, 291, 393, 1500, 309, 484, 13, 407, 341, 390, 611, 257, 2290, 11, 309, 307, 611, 51014], "temperature": 0.0, "avg_logprob": -0.19198843773375165, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0971762165427208}, {"id": 32, "seek": 22292, "start": 235.92, "end": 243.23999999999998, "text": " a tool for our authors, which we own them a lot, because they are our primary tester,", "tokens": [51014, 257, 2290, 337, 527, 16552, 11, 597, 321, 1065, 552, 257, 688, 11, 570, 436, 366, 527, 6194, 36101, 11, 51380], "temperature": 0.0, "avg_logprob": -0.19198843773375165, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0971762165427208}, {"id": 33, "seek": 22292, "start": 243.23999999999998, "end": 251.88, "text": " and it is still an experimental journal. And you could tweak with data, you can change", "tokens": [51380, 293, 309, 307, 920, 364, 17069, 6708, 13, 400, 291, 727, 29879, 365, 1412, 11, 291, 393, 1319, 51812], "temperature": 0.0, "avg_logprob": -0.19198843773375165, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0971762165427208}, {"id": 34, "seek": 25188, "start": 251.88, "end": 259.2, "text": " with the content, and you see how the fingerprint is changing. This was just an experiment at", "tokens": [50364, 365, 264, 2701, 11, 293, 291, 536, 577, 264, 30715, 307, 4473, 13, 639, 390, 445, 364, 5120, 412, 50730], "temperature": 0.0, "avg_logprob": -0.28360827006990946, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.08465129137039185}, {"id": 35, "seek": 25188, "start": 259.2, "end": 267.52, "text": " the beginning, but then it really becomes integrated, it is down there, integrated into the main", "tokens": [50730, 264, 2863, 11, 457, 550, 309, 534, 3643, 10919, 11, 309, 307, 760, 456, 11, 10919, 666, 264, 2135, 51146], "temperature": 0.0, "avg_logprob": -0.28360827006990946, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.08465129137039185}, {"id": 36, "seek": 25188, "start": 267.52, "end": 276.2, "text": " interface of the journal. And we saw that indeed they were very different. They were", "tokens": [51146, 9226, 295, 264, 6708, 13, 400, 321, 1866, 300, 6451, 436, 645, 588, 819, 13, 814, 645, 51580], "temperature": 0.0, "avg_logprob": -0.28360827006990946, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.08465129137039185}, {"id": 37, "seek": 27620, "start": 276.2, "end": 283.59999999999997, "text": " very different, and we can see also the code style of every Jupyter notebook, how the author", "tokens": [50364, 588, 819, 11, 293, 321, 393, 536, 611, 264, 3089, 3758, 295, 633, 22125, 88, 391, 21060, 11, 577, 264, 3793, 50734], "temperature": 0.0, "avg_logprob": -0.21614384164615552, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.07378357648849487}, {"id": 38, "seek": 27620, "start": 283.59999999999997, "end": 290.92, "text": " decided to narrate the arguments. So I will go quickly, sorry. And then this is like the basic", "tokens": [50734, 3047, 281, 6397, 473, 264, 12869, 13, 407, 286, 486, 352, 2661, 11, 2597, 13, 400, 550, 341, 307, 411, 264, 3875, 51100], "temperature": 0.0, "avg_logprob": -0.21614384164615552, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.07378357648849487}, {"id": 39, "seek": 27620, "start": 290.92, "end": 296.76, "text": " layer, so the narrative layer that looks like an MV viewer with steroids in the sense that we", "tokens": [51100, 4583, 11, 370, 264, 9977, 4583, 300, 1542, 411, 364, 17663, 16767, 365, 45717, 294, 264, 2020, 300, 321, 51392], "temperature": 0.0, "avg_logprob": -0.21614384164615552, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.07378357648849487}, {"id": 40, "seek": 27620, "start": 296.76, "end": 303.64, "text": " have figures, we have tables, we have bibliography with Zotero and Psy2C. And then above all,", "tokens": [51392, 362, 9624, 11, 321, 362, 8020, 11, 321, 362, 34344, 5820, 365, 1176, 310, 2032, 293, 430, 3187, 17, 34, 13, 400, 550, 3673, 439, 11, 51736], "temperature": 0.0, "avg_logprob": -0.21614384164615552, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.07378357648849487}, {"id": 41, "seek": 30364, "start": 303.88, "end": 312.03999999999996, "text": " it is a very thin layer on top of the Jupyter notebook, because we use the usual output of the", "tokens": [50376, 309, 307, 257, 588, 5862, 4583, 322, 1192, 295, 264, 22125, 88, 391, 21060, 11, 570, 321, 764, 264, 7713, 5598, 295, 264, 50784], "temperature": 0.0, "avg_logprob": -0.1737490984109732, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.04819868132472038}, {"id": 42, "seek": 30364, "start": 312.03999999999996, "end": 320.64, "text": " notebook, so this is very, yeah, an augmented MV viewer. And then we have, as it is a braided", "tokens": [50784, 21060, 11, 370, 341, 307, 588, 11, 1338, 11, 364, 36155, 17663, 16767, 13, 400, 550, 321, 362, 11, 382, 309, 307, 257, 1548, 2112, 51214], "temperature": 0.0, "avg_logprob": -0.1737490984109732, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.04819868132472038}, {"id": 43, "seek": 30364, "start": 320.64, "end": 326.24, "text": " narrative, we decided to have this metaphor of this level one on top of the other. So this is a", "tokens": [51214, 9977, 11, 321, 3047, 281, 362, 341, 19157, 295, 341, 1496, 472, 322, 1192, 295, 264, 661, 13, 407, 341, 307, 257, 51494], "temperature": 0.0, "avg_logprob": -0.1737490984109732, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.04819868132472038}, {"id": 44, "seek": 30364, "start": 326.24, "end": 331.91999999999996, "text": " sort of animation, on the left you see the full hermeneutic layer, and on the other side you", "tokens": [51494, 1333, 295, 9603, 11, 322, 264, 1411, 291, 536, 264, 1577, 720, 76, 1450, 325, 299, 4583, 11, 293, 322, 264, 661, 1252, 291, 51778], "temperature": 0.0, "avg_logprob": -0.1737490984109732, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.04819868132472038}, {"id": 45, "seek": 33192, "start": 331.96000000000004, "end": 339.28000000000003, "text": " can see how it slides through the, like behind the narrative layer. And the data layer is for the", "tokens": [50366, 393, 536, 577, 309, 9788, 807, 264, 11, 411, 2261, 264, 9977, 4583, 13, 400, 264, 1412, 4583, 307, 337, 264, 50732], "temperature": 0.0, "avg_logprob": -0.23867699835035536, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.014062262140214443}, {"id": 46, "seek": 33192, "start": 339.28000000000003, "end": 348.44, "text": " moment, the part on top, right top, which we use MyBinder, fantastic service to publish online", "tokens": [50732, 1623, 11, 264, 644, 322, 1192, 11, 558, 1192, 11, 597, 321, 764, 1222, 33, 5669, 11, 5456, 2643, 281, 11374, 2950, 51190], "temperature": 0.0, "avg_logprob": -0.23867699835035536, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.014062262140214443}, {"id": 47, "seek": 33192, "start": 348.44, "end": 356.36, "text": " your notebooks. And we wanted this article not only to be show off of the data set, but also", "tokens": [51190, 428, 43782, 13, 400, 321, 1415, 341, 7222, 406, 787, 281, 312, 855, 766, 295, 264, 1412, 992, 11, 457, 611, 51586], "temperature": 0.0, "avg_logprob": -0.23867699835035536, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.014062262140214443}, {"id": 48, "seek": 35636, "start": 356.96000000000004, "end": 363.76, "text": " a small history lab, so that people could just click on the button and get to the data and understand", "tokens": [50394, 257, 1359, 2503, 2715, 11, 370, 300, 561, 727, 445, 2052, 322, 264, 2960, 293, 483, 281, 264, 1412, 293, 1223, 50734], "temperature": 0.0, "avg_logprob": -0.17276678694055436, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.02013525553047657}, {"id": 49, "seek": 35636, "start": 363.76, "end": 370.64, "text": " how the data has been composed. The good thing is that we decided to keep this MyBinder as this", "tokens": [50734, 577, 264, 1412, 575, 668, 18204, 13, 440, 665, 551, 307, 300, 321, 3047, 281, 1066, 341, 1222, 33, 5669, 382, 341, 51078], "temperature": 0.0, "avg_logprob": -0.17276678694055436, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.02013525553047657}, {"id": 50, "seek": 35636, "start": 370.64, "end": 378.76, "text": " source of truth. So the article that you see published is exactly the same copy, with just a", "tokens": [51078, 4009, 295, 3494, 13, 407, 264, 7222, 300, 291, 536, 6572, 307, 2293, 264, 912, 5055, 11, 365, 445, 257, 51484], "temperature": 0.0, "avg_logprob": -0.17276678694055436, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.02013525553047657}, {"id": 51, "seek": 35636, "start": 378.76, "end": 384.84000000000003, "text": " different way of interacting with a different layer. So this is how it looks like on MyBinder,", "tokens": [51484, 819, 636, 295, 18017, 365, 257, 819, 4583, 13, 407, 341, 307, 577, 309, 1542, 411, 322, 1222, 33, 5669, 11, 51788], "temperature": 0.0, "avg_logprob": -0.17276678694055436, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.02013525553047657}, {"id": 52, "seek": 38484, "start": 384.84, "end": 392.79999999999995, "text": " so it's a classical Jupyter notebook, and for every notebook we have a GitHub repo where we", "tokens": [50364, 370, 309, 311, 257, 13735, 22125, 88, 391, 21060, 11, 293, 337, 633, 21060, 321, 362, 257, 23331, 49040, 689, 321, 50762], "temperature": 0.0, "avg_logprob": -0.21340689451798148, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.008449502289295197}, {"id": 53, "seek": 38484, "start": 392.79999999999995, "end": 400.44, "text": " store all the requirements and all the images in the data set. We have to put together the", "tokens": [50762, 3531, 439, 264, 7728, 293, 439, 264, 5267, 294, 264, 1412, 992, 13, 492, 362, 281, 829, 1214, 264, 51144], "temperature": 0.0, "avg_logprob": -0.21340689451798148, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.008449502289295197}, {"id": 54, "seek": 38484, "start": 400.44, "end": 409.91999999999996, "text": " fair metadata, but still, so it's under construction. Then what does it mean having Jupyter", "tokens": [51144, 3143, 26603, 11, 457, 920, 11, 370, 309, 311, 833, 6435, 13, 1396, 437, 775, 309, 914, 1419, 22125, 88, 391, 51618], "temperature": 0.0, "avg_logprob": -0.21340689451798148, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.008449502289295197}, {"id": 55, "seek": 40992, "start": 409.92, "end": 416.8, "text": " notebooks for publishing? We see that in the literature there are a lot of critics who shouldn't", "tokens": [50364, 43782, 337, 17832, 30, 492, 536, 300, 294, 264, 10394, 456, 366, 257, 688, 295, 22503, 567, 4659, 380, 50708], "temperature": 0.0, "avg_logprob": -0.20007280508677164, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.1074175164103508}, {"id": 56, "seek": 40992, "start": 416.8, "end": 423.32, "text": " use Jupyter notebooks because it's too complex, it's impossible to replicate and so on and so forth.", "tokens": [50708, 764, 22125, 88, 391, 43782, 570, 309, 311, 886, 3997, 11, 309, 311, 6243, 281, 25356, 293, 370, 322, 293, 370, 5220, 13, 51034], "temperature": 0.0, "avg_logprob": -0.20007280508677164, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.1074175164103508}, {"id": 57, "seek": 40992, "start": 423.32, "end": 431.76, "text": " But then for us, it was really the simplest solution. So at the same time, to publish with", "tokens": [51034, 583, 550, 337, 505, 11, 309, 390, 534, 264, 22811, 3827, 13, 407, 412, 264, 912, 565, 11, 281, 11374, 365, 51456], "temperature": 0.0, "avg_logprob": -0.20007280508677164, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.1074175164103508}, {"id": 58, "seek": 40992, "start": 431.76, "end": 437.16, "text": " Jupyter, we had to make our pipeline a bit more complex than usual. So we have a first review", "tokens": [51456, 22125, 88, 391, 11, 321, 632, 281, 652, 527, 15517, 257, 857, 544, 3997, 813, 7713, 13, 407, 321, 362, 257, 700, 3131, 51726], "temperature": 0.0, "avg_logprob": -0.20007280508677164, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.1074175164103508}, {"id": 59, "seek": 43716, "start": 437.24, "end": 443.28000000000003, "text": " directly on the abstract, where we start communicating with the authors, understanding", "tokens": [50368, 3838, 322, 264, 12649, 11, 689, 321, 722, 17559, 365, 264, 16552, 11, 3701, 50670], "temperature": 0.0, "avg_logprob": -0.25710497146997696, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.033077239990234375}, {"id": 60, "seek": 43716, "start": 443.28000000000003, "end": 450.0, "text": " their needs, creating a writing environment for them that can be replicated with Python,", "tokens": [50670, 641, 2203, 11, 4084, 257, 3579, 2823, 337, 552, 300, 393, 312, 46365, 365, 15329, 11, 51006], "temperature": 0.0, "avg_logprob": -0.25710497146997696, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.033077239990234375}, {"id": 61, "seek": 43716, "start": 450.0, "end": 456.08000000000004, "text": " sorry, with Docker containers for Python and Air. And then there's the first technical review,", "tokens": [51006, 2597, 11, 365, 33772, 17089, 337, 15329, 293, 5774, 13, 400, 550, 456, 311, 264, 700, 6191, 3131, 11, 51310], "temperature": 0.0, "avg_logprob": -0.25710497146997696, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.033077239990234375}, {"id": 62, "seek": 43716, "start": 456.08000000000004, "end": 460.96000000000004, "text": " she's in charge of the first technical review, which is the most complicated one because there's", "tokens": [51310, 750, 311, 294, 4602, 295, 264, 700, 6191, 3131, 11, 597, 307, 264, 881, 6179, 472, 570, 456, 311, 51554], "temperature": 0.0, "avg_logprob": -0.25710497146997696, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.033077239990234375}, {"id": 63, "seek": 46096, "start": 461.44, "end": 468.79999999999995, "text": " a lot of checks. We saw some projects already, we needed to have checks. And then we have a lot", "tokens": [50388, 257, 688, 295, 13834, 13, 492, 1866, 512, 4455, 1217, 11, 321, 2978, 281, 362, 13834, 13, 400, 550, 321, 362, 257, 688, 50756], "temperature": 0.0, "avg_logprob": -0.19625838084887431, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04589613899588585}, {"id": 64, "seek": 46096, "start": 468.79999999999995, "end": 473.52, "text": " of other open source software that enters this pipeline, like for the preview of the notebook", "tokens": [50756, 295, 661, 1269, 4009, 4722, 300, 18780, 341, 15517, 11, 411, 337, 264, 14281, 295, 264, 21060, 50992], "temperature": 0.0, "avg_logprob": -0.19625838084887431, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04589613899588585}, {"id": 65, "seek": 46096, "start": 473.52, "end": 480.08, "text": " with the GitHub app, MB Viewer, we have MyBinder, and this is just for the first technical review", "tokens": [50992, 365, 264, 23331, 724, 11, 28866, 13909, 260, 11, 321, 362, 1222, 33, 5669, 11, 293, 341, 307, 445, 337, 264, 700, 6191, 3131, 51320], "temperature": 0.0, "avg_logprob": -0.19625838084887431, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04589613899588585}, {"id": 66, "seek": 46096, "start": 480.08, "end": 485.88, "text": " because then the article is being sent to the reviewer for the double banana review. So before", "tokens": [51320, 570, 550, 264, 7222, 307, 885, 2279, 281, 264, 3131, 260, 337, 264, 3834, 14194, 3131, 13, 407, 949, 51610], "temperature": 0.0, "avg_logprob": -0.19625838084887431, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04589613899588585}, {"id": 67, "seek": 48588, "start": 485.96, "end": 492.84, "text": " even reviewing, we had to do this huge job because they have to review also the data and the pertinence", "tokens": [50368, 754, 19576, 11, 321, 632, 281, 360, 341, 2603, 1691, 570, 436, 362, 281, 3131, 611, 264, 1412, 293, 264, 13269, 259, 655, 50712], "temperature": 0.0, "avg_logprob": -0.18107988396469427, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.05570102855563164}, {"id": 68, "seek": 48588, "start": 492.84, "end": 500.15999999999997, "text": " of the dataset. And then finally, there is one important thing, it's English editing. So how", "tokens": [50712, 295, 264, 28872, 13, 400, 550, 2721, 11, 456, 307, 472, 1021, 551, 11, 309, 311, 3669, 10000, 13, 407, 577, 51078], "temperature": 0.0, "avg_logprob": -0.18107988396469427, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.05570102855563164}, {"id": 69, "seek": 48588, "start": 500.15999999999997, "end": 506.56, "text": " to edit something that which is already being run, so without running itself. So this could be a", "tokens": [51078, 281, 8129, 746, 300, 597, 307, 1217, 885, 1190, 11, 370, 1553, 2614, 2564, 13, 407, 341, 727, 312, 257, 51398], "temperature": 0.0, "avg_logprob": -0.18107988396469427, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.05570102855563164}, {"id": 70, "seek": 48588, "start": 506.56, "end": 512.2, "text": " tool for translators, tool for correctors that they're not into the Jupyter world. So how to do", "tokens": [51398, 2290, 337, 5105, 3391, 11, 2290, 337, 3006, 830, 300, 436, 434, 406, 666, 264, 22125, 88, 391, 1002, 13, 407, 577, 281, 360, 51680], "temperature": 0.0, "avg_logprob": -0.18107988396469427, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.05570102855563164}, {"id": 71, "seek": 51220, "start": 512.2, "end": 518.84, "text": " that? We have Jupyter text, we're still testing some plug-in to see if this could work without", "tokens": [50364, 300, 30, 492, 362, 22125, 88, 391, 2487, 11, 321, 434, 920, 4997, 512, 5452, 12, 259, 281, 536, 498, 341, 727, 589, 1553, 50696], "temperature": 0.0, "avg_logprob": -0.14227704903514116, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.010194459930062294}, {"id": 72, "seek": 51220, "start": 518.84, "end": 524.84, "text": " touching the final output. And then the final technical review, so after all this has been", "tokens": [50696, 11175, 264, 2572, 5598, 13, 400, 550, 264, 2572, 6191, 3131, 11, 370, 934, 439, 341, 575, 668, 50996], "temperature": 0.0, "avg_logprob": -0.14227704903514116, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.010194459930062294}, {"id": 73, "seek": 51220, "start": 524.84, "end": 530.44, "text": " shipped, we have a DOI. So the article is now published and needs to be indexing and there is", "tokens": [50996, 25312, 11, 321, 362, 257, 10699, 40, 13, 407, 264, 7222, 307, 586, 6572, 293, 2203, 281, 312, 8186, 278, 293, 456, 307, 51276], "temperature": 0.0, "avg_logprob": -0.14227704903514116, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.010194459930062294}, {"id": 74, "seek": 51220, "start": 530.44, "end": 536.84, "text": " the problem of long-term archiving, which is a big problem for many reasons. First of all,", "tokens": [51276, 264, 1154, 295, 938, 12, 7039, 3912, 2123, 11, 597, 307, 257, 955, 1154, 337, 867, 4112, 13, 2386, 295, 439, 11, 51596], "temperature": 0.0, "avg_logprob": -0.14227704903514116, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.010194459930062294}, {"id": 75, "seek": 53684, "start": 536.9200000000001, "end": 546.6, "text": " like the libraries that get deprecated, also API that disappeared. So how to really reproduce", "tokens": [50368, 411, 264, 15148, 300, 483, 1367, 13867, 770, 11, 611, 9362, 300, 13954, 13, 407, 577, 281, 534, 29501, 50852], "temperature": 0.0, "avg_logprob": -0.17480888366699218, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.060333359986543655}, {"id": 76, "seek": 53684, "start": 546.6, "end": 551.8000000000001, "text": " this in the future? And then finally, the dataset needs to be included into, we have", "tokens": [50852, 341, 294, 264, 2027, 30, 400, 550, 2721, 11, 264, 28872, 2203, 281, 312, 5556, 666, 11, 321, 362, 51112], "temperature": 0.0, "avg_logprob": -0.17480888366699218, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.060333359986543655}, {"id": 77, "seek": 53684, "start": 551.8000000000001, "end": 558.36, "text": " Dataverse, but we're looking for Zenodo in order to match the fair metadata. And time is up,", "tokens": [51112, 11888, 4308, 11, 457, 321, 434, 1237, 337, 22387, 17423, 294, 1668, 281, 2995, 264, 3143, 26603, 13, 400, 565, 307, 493, 11, 51440], "temperature": 0.0, "avg_logprob": -0.17480888366699218, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.060333359986543655}, {"id": 78, "seek": 53684, "start": 558.36, "end": 565.1600000000001, "text": " I have a question for you, of course. Thank you very much, first of all. And then if you have", "tokens": [51440, 286, 362, 257, 1168, 337, 291, 11, 295, 1164, 13, 1044, 291, 588, 709, 11, 700, 295, 439, 13, 400, 550, 498, 291, 362, 51780], "temperature": 0.0, "avg_logprob": -0.17480888366699218, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.060333359986543655}, {"id": 79, "seek": 56516, "start": 565.16, "end": 570.28, "text": " want to contact us, just collaborate or work together on Jupyter publication,", "tokens": [50364, 528, 281, 3385, 505, 11, 445, 18338, 420, 589, 1214, 322, 22125, 88, 391, 19953, 11, 50620], "temperature": 0.0, "avg_logprob": -0.19216916701372933, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.032561492174863815}, {"id": 80, "seek": 56516, "start": 570.92, "end": 576.8399999999999, "text": " JDH admin at uni.lu. And then the question is, how can we actually collaborate on something", "tokens": [50652, 37082, 39, 24236, 412, 36435, 13, 2781, 13, 400, 550, 264, 1168, 307, 11, 577, 393, 321, 767, 18338, 322, 746, 50948], "temperature": 0.0, "avg_logprob": -0.19216916701372933, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.032561492174863815}, {"id": 81, "seek": 56516, "start": 576.8399999999999, "end": 584.68, "text": " which is a notebook that requires quite a threshold of expertise, not only for the researcher,", "tokens": [50948, 597, 307, 257, 21060, 300, 7029, 1596, 257, 14678, 295, 11769, 11, 406, 787, 337, 264, 21751, 11, 51340], "temperature": 0.0, "avg_logprob": -0.19216916701372933, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.032561492174863815}, {"id": 82, "seek": 56516, "start": 584.68, "end": 591.0, "text": " but for the people that are around, and how to maintain all this and how to make this history", "tokens": [51340, 457, 337, 264, 561, 300, 366, 926, 11, 293, 577, 281, 6909, 439, 341, 293, 577, 281, 652, 341, 2503, 51656], "temperature": 0.0, "avg_logprob": -0.19216916701372933, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.032561492174863815}, {"id": 83, "seek": 59100, "start": 591.0, "end": 594.04, "text": " love living for more than one year. Thank you.", "tokens": [50364, 959, 2647, 337, 544, 813, 472, 1064, 13, 1044, 291, 13, 50516], "temperature": 0.0, "avg_logprob": -0.5278489930289132, "compression_ratio": 0.8846153846153846, "no_speech_prob": 0.14995478093624115}, {"id": 84, "seek": 62100, "start": 621.08, "end": 630.6, "text": " Yeah, well, I repeat the question. So he asked me if the double blind review, how can we keep it", "tokens": [50368, 865, 11, 731, 11, 286, 7149, 264, 1168, 13, 407, 415, 2351, 385, 498, 264, 3834, 6865, 3131, 11, 577, 393, 321, 1066, 309, 50844], "temperature": 0.0, "avg_logprob": -0.20213872332905614, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.032620735466480255}, {"id": 85, "seek": 62100, "start": 630.6, "end": 638.6, "text": " actually a real natural double blind? So she anonymized the data on GitHub. So we have", "tokens": [50844, 767, 257, 957, 3303, 3834, 6865, 30, 407, 750, 37293, 1602, 264, 1412, 322, 23331, 13, 407, 321, 362, 51244], "temperature": 0.0, "avg_logprob": -0.20213872332905614, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.032620735466480255}, {"id": 86, "seek": 62100, "start": 638.6, "end": 643.32, "text": " specific repositories that have been created after the communication with the authors,", "tokens": [51244, 2685, 22283, 2083, 300, 362, 668, 2942, 934, 264, 6101, 365, 264, 16552, 11, 51480], "temperature": 0.0, "avg_logprob": -0.20213872332905614, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.032620735466480255}, {"id": 87, "seek": 62100, "start": 643.32, "end": 650.12, "text": " where we only have the code without the names, but then you still have the bibliography,", "tokens": [51480, 689, 321, 787, 362, 264, 3089, 1553, 264, 5288, 11, 457, 550, 291, 920, 362, 264, 34344, 5820, 11, 51820], "temperature": 0.0, "avg_logprob": -0.20213872332905614, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.032620735466480255}, {"id": 88, "seek": 65012, "start": 650.2, "end": 656.12, "text": " so it's easy to, it's a very small word, one of the digital history. But this is the way to", "tokens": [50368, 370, 309, 311, 1858, 281, 11, 309, 311, 257, 588, 1359, 1349, 11, 472, 295, 264, 4562, 2503, 13, 583, 341, 307, 264, 636, 281, 50664], "temperature": 0.0, "avg_logprob": -0.19778931605351435, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0368933342397213}, {"id": 89, "seek": 65012, "start": 656.12, "end": 662.6, "text": " maintain double blind. And then we're going to send the review where both the MyBinder and the", "tokens": [50664, 6909, 3834, 6865, 13, 400, 550, 321, 434, 516, 281, 2845, 264, 3131, 689, 1293, 264, 1222, 33, 5669, 293, 264, 50988], "temperature": 0.0, "avg_logprob": -0.19778931605351435, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0368933342397213}, {"id": 90, "seek": 65012, "start": 662.6, "end": 671.96, "text": " version of the article on our website with a hidden URL. So this is the only thing that we can do.", "tokens": [50988, 3037, 295, 264, 7222, 322, 527, 3144, 365, 257, 7633, 12905, 13, 407, 341, 307, 264, 787, 551, 300, 321, 393, 360, 13, 51456], "temperature": 0.0, "avg_logprob": -0.19778931605351435, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0368933342397213}, {"id": 91, "seek": 67196, "start": 672.76, "end": 679.08, "text": " For sure, the double blind, we have the problem that we cannot really use the", "tokens": [50404, 1171, 988, 11, 264, 3834, 6865, 11, 321, 362, 264, 1154, 300, 321, 2644, 534, 764, 264, 50720], "temperature": 0.0, "avg_logprob": -0.27535892787732574, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.05138513073325157}, {"id": 92, "seek": 67196, "start": 679.08, "end": 684.52, "text": " pull request directly on the GitHub repository. So in fact, there is some replication between", "tokens": [50720, 2235, 5308, 3838, 322, 264, 23331, 25841, 13, 407, 294, 1186, 11, 456, 307, 512, 39911, 1296, 50992], "temperature": 0.0, "avg_logprob": -0.27535892787732574, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.05138513073325157}, {"id": 93, "seek": 67196, "start": 684.52, "end": 691.24, "text": " the GitHub repository. Sometimes after with the peer review, there is some", "tokens": [50992, 264, 23331, 25841, 13, 4803, 934, 365, 264, 15108, 3131, 11, 456, 307, 512, 51328], "temperature": 0.0, "avg_logprob": -0.27535892787732574, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.05138513073325157}, {"id": 94, "seek": 67196, "start": 692.2800000000001, "end": 699.1600000000001, "text": " requisite that he come back to a technical review because there is a revision. So there", "tokens": [51380, 49878, 642, 300, 415, 808, 646, 281, 257, 6191, 3131, 570, 456, 307, 257, 34218, 13, 407, 456, 51724], "temperature": 0.0, "avg_logprob": -0.27535892787732574, "compression_ratio": 1.7578947368421052, "no_speech_prob": 0.05138513073325157}, {"id": 95, "seek": 69916, "start": 699.16, "end": 704.92, "text": " is this question about how we re-synchronize the notebook together. There is some authors that", "tokens": [50364, 307, 341, 1168, 466, 577, 321, 319, 12, 82, 36420, 1125, 264, 21060, 1214, 13, 821, 307, 512, 16552, 300, 50652], "temperature": 0.0, "avg_logprob": -0.23459971897185794, "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.035289376974105835}, {"id": 96, "seek": 69916, "start": 708.92, "end": 717.0, "text": " they have good enough with GitHub, but to review a notebook with the output with the metadata", "tokens": [50852, 436, 362, 665, 1547, 365, 23331, 11, 457, 281, 3131, 257, 21060, 365, 264, 5598, 365, 264, 26603, 51256], "temperature": 0.0, "avg_logprob": -0.23459971897185794, "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.035289376974105835}, {"id": 97, "seek": 69916, "start": 717.0, "end": 720.8399999999999, "text": " to track what has been changed. That's why, yes, this it was,", "tokens": [51256, 281, 2837, 437, 575, 668, 3105, 13, 663, 311, 983, 11, 2086, 11, 341, 309, 390, 11, 51448], "temperature": 0.0, "avg_logprob": -0.23459971897185794, "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.035289376974105835}, {"id": 98, "seek": 72084, "start": 721.5600000000001, "end": 729.5600000000001, "text": " but yes, the questions that you have, we are testing with review and be or not also to maybe", "tokens": [50400, 457, 2086, 11, 264, 1651, 300, 291, 362, 11, 321, 366, 4997, 365, 3131, 293, 312, 420, 406, 611, 281, 1310, 50800], "temperature": 0.0, "avg_logprob": -0.299818708185564, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.06374397873878479}, {"id": 99, "seek": 72084, "start": 730.6800000000001, "end": 738.0400000000001, "text": " use some markdown or just Python script to produce several output in order to not", "tokens": [50856, 764, 512, 1491, 5093, 420, 445, 15329, 5755, 281, 5258, 2940, 5598, 294, 1668, 281, 406, 51224], "temperature": 0.0, "avg_logprob": -0.299818708185564, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.06374397873878479}, {"id": 100, "seek": 72084, "start": 739.08, "end": 744.12, "text": " sometimes touch about this metadata that they are inside the notebook.", "tokens": [51276, 2171, 2557, 466, 341, 26603, 300, 436, 366, 1854, 264, 21060, 13, 51528], "temperature": 0.0, "avg_logprob": -0.299818708185564, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.06374397873878479}, {"id": 101, "seek": 74412, "start": 744.12, "end": 751.96, "text": " And there was another question, but I don't know if we have time. Yes. Yes. Yes. Please.", "tokens": [50364, 400, 456, 390, 1071, 1168, 11, 457, 286, 500, 380, 458, 498, 321, 362, 565, 13, 1079, 13, 1079, 13, 1079, 13, 2555, 13, 50756], "temperature": 0.0, "avg_logprob": -0.40183368259006075, "compression_ratio": 1.7359550561797752, "no_speech_prob": 0.02007203921675682}, {"id": 102, "seek": 74412, "start": 751.96, "end": 753.96, "text": " Last one. Last one.", "tokens": [50756, 5264, 472, 13, 5264, 472, 13, 50856], "temperature": 0.0, "avg_logprob": -0.40183368259006075, "compression_ratio": 1.7359550561797752, "no_speech_prob": 0.02007203921675682}, {"id": 103, "seek": 74412, "start": 753.96, "end": 757.96, "text": " Of the SS brightness of your data sets.", "tokens": [50856, 2720, 264, 12238, 21367, 295, 428, 1412, 6352, 13, 51056], "temperature": 0.0, "avg_logprob": -0.40183368259006075, "compression_ratio": 1.7359550561797752, "no_speech_prob": 0.02007203921675682}, {"id": 104, "seek": 74412, "start": 757.96, "end": 762.36, "text": " Sorry, how to assess? Of the SS brightness of your data sets.", "tokens": [51056, 4919, 11, 577, 281, 5877, 30, 2720, 264, 12238, 21367, 295, 428, 1412, 6352, 13, 51276], "temperature": 0.0, "avg_logprob": -0.40183368259006075, "compression_ratio": 1.7359550561797752, "no_speech_prob": 0.02007203921675682}, {"id": 105, "seek": 74412, "start": 762.36, "end": 769.16, "text": " Yeah, that's the very big, big, big question. So the idea behind the braided narrative is then you", "tokens": [51276, 865, 11, 300, 311, 264, 588, 955, 11, 955, 11, 955, 1168, 13, 407, 264, 1558, 2261, 264, 1548, 2112, 9977, 307, 550, 291, 51616], "temperature": 0.0, "avg_logprob": -0.40183368259006075, "compression_ratio": 1.7359550561797752, "no_speech_prob": 0.02007203921675682}, {"id": 106, "seek": 76916, "start": 769.16, "end": 776.28, "text": " tell the story around the data on one side and on the other side you keep the data like with the", "tokens": [50364, 980, 264, 1657, 926, 264, 1412, 322, 472, 1252, 293, 322, 264, 661, 1252, 291, 1066, 264, 1412, 411, 365, 264, 50720], "temperature": 0.0, "avg_logprob": -0.13544662519433032, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.04009658470749855}, {"id": 107, "seek": 76916, "start": 776.28, "end": 785.4, "text": " Zenodo metadata coherent or probably with what Paul showed us before with Ricardo. So having", "tokens": [50720, 22387, 17423, 26603, 36239, 420, 1391, 365, 437, 4552, 4712, 505, 949, 365, 42634, 13, 407, 1419, 51176], "temperature": 0.0, "avg_logprob": -0.13544662519433032, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.04009658470749855}, {"id": 108, "seek": 76916, "start": 786.52, "end": 792.28, "text": " like an external check on the metadata and on the data set itself. At the same time,", "tokens": [51232, 411, 364, 8320, 1520, 322, 264, 26603, 293, 322, 264, 1412, 992, 2564, 13, 1711, 264, 912, 565, 11, 51520], "temperature": 0.0, "avg_logprob": -0.13544662519433032, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.04009658470749855}, {"id": 109, "seek": 76916, "start": 792.28, "end": 798.36, "text": " the initial, the first technical review is the one where we assess actually the data. So if the", "tokens": [51520, 264, 5883, 11, 264, 700, 6191, 3131, 307, 264, 472, 689, 321, 5877, 767, 264, 1412, 13, 407, 498, 264, 51824], "temperature": 0.0, "avg_logprob": -0.13544662519433032, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.04009658470749855}, {"id": 110, "seek": 79836, "start": 798.36, "end": 804.76, "text": " data set are complete, coherent, we don't judge them because then we know that there are conditions", "tokens": [50364, 1412, 992, 366, 3566, 11, 36239, 11, 321, 500, 380, 6995, 552, 570, 550, 321, 458, 300, 456, 366, 4487, 50684], "temperature": 0.0, "avg_logprob": -0.2124685559953962, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.016903722658753395}, {"id": 111, "seek": 79836, "start": 804.76, "end": 810.28, "text": " of production. That needs to be, we try to make this as more explicit as possible.", "tokens": [50684, 295, 4265, 13, 663, 2203, 281, 312, 11, 321, 853, 281, 652, 341, 382, 544, 13691, 382, 1944, 13, 50960], "temperature": 0.0, "avg_logprob": -0.2124685559953962, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.016903722658753395}, {"id": 112, "seek": 79836, "start": 810.28, "end": 812.28, "text": " That's.", "tokens": [50960, 663, 311, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2124685559953962, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.016903722658753395}, {"id": 113, "seek": 79836, "start": 813.24, "end": 818.92, "text": " Yes, exactly. And this, like that's why the long-term maintenance. So now we only have", "tokens": [51108, 1079, 11, 2293, 13, 400, 341, 11, 411, 300, 311, 983, 264, 938, 12, 7039, 11258, 13, 407, 586, 321, 787, 362, 51392], "temperature": 0.0, "avg_logprob": -0.2124685559953962, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.016903722658753395}, {"id": 114, "seek": 79836, "start": 818.92, "end": 826.36, "text": " nine articles, but we have 28 in the pipeline in the coming year. So it's really now it's", "tokens": [51392, 4949, 11290, 11, 457, 321, 362, 7562, 294, 264, 15517, 294, 264, 1348, 1064, 13, 407, 309, 311, 534, 586, 309, 311, 51764], "temperature": 0.0, "avg_logprob": -0.2124685559953962, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.016903722658753395}, {"id": 115, "seek": 82636, "start": 826.36, "end": 831.96, "text": " getting us up speed and we have more and more interaction with others which makes things more", "tokens": [50364, 1242, 505, 493, 3073, 293, 321, 362, 544, 293, 544, 9285, 365, 2357, 597, 1669, 721, 544, 50644], "temperature": 0.0, "avg_logprob": -0.33257883566397206, "compression_ratio": 1.21875, "no_speech_prob": 0.06585295498371124}, {"id": 116, "seek": 82636, "start": 831.96, "end": 833.96, "text": " complicated. Thank you.", "tokens": [50644, 6179, 13, 1044, 291, 13, 50744], "temperature": 0.0, "avg_logprob": -0.33257883566397206, "compression_ratio": 1.21875, "no_speech_prob": 0.06585295498371124}], "language": "en"}