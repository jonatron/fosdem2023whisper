{"text": " So, welcome to the Post-Quest Girl Dev Room. If you weren't here before, can I please ask you to silence your phones and extend a very warm welcome to Peter Zaitsev. Okay, well, thank you. We are going to talk about query performance today. But before that, let me understand a little bit who do we have here. Now, which of you would mostly see yourself as DBA, SRE, CSADMIN, kind of on the operations side? Just, you know, can we have a... Okay. Now, which of you are developers? Ooh, lots of developers. Okay. Now, in terms of developers, right, now if you do it again, but now for sort of like a front-end developers, right, something not, you know, database kernel or something, you know, other complicated stuff, but something more simple. Front-end developers. Any? Yeah! Hello! Okay. Well, anyway, so one of the good points of this talk for me, right, is really to try to bridge the gap what I see a lot between how their operations people, right, the people who are deeply vested in a databases, right, development, think about them versus people who are just happened to use those databases for the application, right? And often their relationship to the database is, well, really quite different, right? As a database kernel developers, we often deeply care about all those kind of internal algorithms, have a, you know, discussion, what is the best way to implement these and that cases. But for many developers writing applications, well, you know, we think about databases, you know, as you think about like a plumbing, right, well, it just got to work, you don't want to think about it, well, it just, if it doesn't, then that becomes a problem, right? They think about database in many cases as a black box. And I think that is increasingly happening now, especially when we have so many databases which are deployed in a cloud as a database as a service, right? Because in this case, especially, well, you just have a database and somebody else focuses on the other stuff. So what does that database mean from developer standpoint in many cases? Well, that means you get some sort of service point, you should use in your application, right, you can connect to that service point and get that quickly with no problem, right? And then you run the queries you need to run, right, of a database or maybe even what your RAM framework, right, or something generates. Now what do you want from those queries? Well, as a selfish developer, you want those queries to run with no errors. You want to make sure they get your correct results, right? And you want to make sure you run them within response time, which is appropriate for your application and for query time and for query kind. And I think that is very important to understand here what if I am looking as a developer and a database from performance standpoint, I understand that as how quickly that database responds to my queries, right? Now if you think about their software design in general, right, and I think especially maybe not developers, but architects often have to care about a whole bunch of other things beyond just the performance. For example, we often have to care about security, right? And typically security costs stuff, right? It comes with overhead, both in terms of performance overhead and, you know, organizational overhead and so on and so forth, right? That's done to factor authentication always takes another couple of seconds, right? But that makes us more secure. Availability is also important, as well as things like costs. I think that is especially important, again, in the modern age when they have a lot of cloud, which is elastic, right? But that elasticity comes also with spend, right? You often can say, hey, you know what, if I just need my queries to run faster, I can blow up my instant size, right, or something else. But well, guess what, that also will be expensive, right, if you're not doing efficiently. And there is, let's say, a bunch of other things you want to consider about, right? So I don't want to simplify that, let's say, to what everything is also only about query performance, but that is what I am going to focus in my talk. Now when you think about response time from the database standpoint, we often think about that from a query context, right? Well I see my database responds to the queries XYZ, you know, in average or something, right? You think about that query basics. But if you really look at from a business standpoint, right, how your boss or boss is boss is boss, right, where it thinks about that, it's mostly about the users which are using your applications, right? And I typically would define it what really folks are after is what the users of your applications, right, and all users, right, have outstanding experience in terms of performance for all their interactions. Because in application, you often may have different interactions, right, and I want to make sure I have a search which is fast and place in an order which is fast, right, and whatever other things all working quickly. Now as database engineers, we often want to talk about performance and availability as a different thing, right, like saying, well, no, no, no, the database was up, it just was overloaded, so that query took 15 seconds, oh, 15 minutes, right, or something like that, right? But the reality is for the user, their very bad performance is really indistinguishable from downtime, because, well, A, people have a limited experience, right, and if something is taking too long, we'll just go into closer page, and even if you have some, something of unlimited patience, there is going to be a whole bunch of timeouts, including your browser timeouts which will, you know, show you what the page cannot load well before 15 minutes, right? So I think that is another important thing which I find also important talking to some, maybe business people about why spend resources on performance, query performance optimization, and so on and so forth, right, because, well, you know what, if it doesn't perform, it is down, right? Another thing what I would point out, right, is in many cases, you see people talking about the averages, right, while the query performance was so many, you know, milliseconds or something in average, right, and while it may be helpful for comparison standpoint compared to yesterday, really, it is not very helpful, right, because, well, the average maybe what you're looking for may be way too many queries which are too slow, right, just balanced by the queries which are high, right, and as I wrote here, I really like this saying, we won't leave the man who tried to cross a river in average one meter deep, right, where once leave the man. So in this regard, I think it's very helpful to look at things like a percentile response time at the very least, right, if you want to look at one number because you're looking at simplicity, 99 percentile for query response time is much better than average response time. What is even better is, of course, is to look some sort of distribution, you know, query histogram distribution and how it changes over time. That often can give you a lot of insight. The thing from percentile, though, is it's interesting how it works as you go from that query to the user experience, right, you spoke about, because think about this, right, typically when you may have a single user interaction as a page view, it may require multiple sequential queries, right, or even maybe some queries run in parallel, right, which all need to be fast in order for user to get the outcome they're looking for, right, and then typically user through his session will have a multiple of those page views, right, so that 99 percentile being excellent may only translate to half the users having that kind of outstanding experience through all the session, right, that is why if you look at companies which have a large number of users, they would either have some very high percentiles, like 99.9 percentile response time as a goal, right, or would have those tolerances, you know, rather high, right, so there is a, well, additional sort of accommodation for if there's going to be many, many queries, and I think to consider when you measure query performance is how you relate to errors, right, in certain cases I've seen people saying, well, you know, we only go into either measure response time for only successful queries, or we're going to put successful queries and queries which are completed with errors in the same bucket, right, which really can really, you know, change a picture for you a lot. Why is that? Well, because actually if you think about the errors, they can be both fast errors and slow errors, right, imagine, for example, table was dropped for some reason, well, then all the queries hitting that table will return the error and vary very quickly, right, because well, there's nothing they can do, on the other hand, if there is something, let's say some data is locked, right, and some timeouts happen, that may take quite a while before error is returned, right, and you better not to mix those with the rest of your successful queries but to be able to, you know, look at that separately. You also want to look at the query performance not just as an overall number but how it changes over response time with a reasonably high resolution. Why is that important? One thing is what in many cases you would see query performance kind of slowly drops before it goes so bad what that seems like downtime or really, you know, really incident for all kind of reasons, right, maybe you have some application which has a bad query, right, and then you had the one instance of that query running, two, three, four, five, now you have a hundred instances of that bad query running, right, so saturating all the system resources, guess what, all the other query performance, right, is going down. If you are able to, if you are going to notice that what some queries are out of bounds, right, and maybe alert on it or something, you are able to take an action before the small problem becomes, basically because of downtime. The other reason, of course, there is shit that is always going on, right, there is something that database doesn't have a background, if you have a cloud there is so sort of other things happening which you may not even know anything about it. Like for example, block, elastic block storage, right, similar stuff, right, well, guess what, it doesn't always have uniform performance, you know, sometimes something is happening at like Amazon back end but you know what, you don't really know anything about that. They don't tell you each time they have to replace a hard drive, right, somewhere, right, or, you know, rebalance the load for some reason, right, but those things they can pop up. Often you may see something like, oh, I have that like a spike in a query response time which I can see for all queries on my, all instances, and wow, that's very likely like something is environmental. Now when you look at the query instrumentation, one of the questions I see people asking is where do you want to instrument the query, right, and we can instrument the query on the application data point, right, an application issues that query, right, and we often have some, you know, tools like, you know, new relic insights which are doing, you know, just that. And hey, that query took this amount of response time and this is very good data because it actually includes real response time as application observed it. If there was some, let's say, network delay, right, or for whatever reason, that is included in that response time where if you just measure from the time database received the query since it pushed the result in the network, right, that is not included. But measuring on a database gives you a lot of other valuable stuff like you can get a lot more insight about what has been going on on the database size right while the query was executed. Most typically when you get the query result and you get response time, maybe, you know, some other little additional information like, oh, this query returns so many rows, so many bytes, right, but not specifically, you know, how much CPU it uses, right, and all the other important things you may want to use, okay. So let's go back to our definition of a response time from a business point of view, right, and we can say, well, what we are looking for have our old users to have an outstanding experience of all of their applications, right, great. Now how do we translate that to the database, right, and kind of maybe breed that gap without what the boss wants and what DBA is able to answer. Now I think there is some great work in this regard done by Google which have been working on this L-square commenter project which allows to pass a lot of metadata, right, from your application all the way down to your query. The cool thing they've done is also integrating that directly with some of the frameworks, right, so it's kind of, hey, you know, you need to do nothing, right, and you just get that automatic information. What could be valuable query metadata possibilities, right, if you ask me, well, here is a bunch, right, there is this actual user and tenant which we can do application or functionality, right, often single database is used by a lot of applications, right, and we want to know where the query comes from, right. I see a lot of DBAs, especially from a large company, say, well, you know what, here is this nasty query came in. It was not here yesterday, but it's very hard to figure out who is responsible for introducing that and how you can come and hit his head with something heavy, right. That's maybe hard, right, without proper instrumentation. You also, as a primary breakdown, want to look at the query, and I mean by query in this case, query of all parameters, you know, normalized, because often you would see the different queries responsible for different functions, and through that have a different response time tolerances, right, let's say some very quick lookup queries, you often want them to complete in a fraction of millisecond as acceptable stuff, while some of you search queries write to some reports, well, may take a few seconds, and that will be quite acceptable, and it's good not to mix those all together, right, in this case. In many cases, when you have the SAS applications, we would have a multiple user, so what often calls like multiple tenants, like one of the ways you split them is to have a different schemas or different databases for all of them, and that is also, I find, very helpful to being able to separate that, so you can see, oh, this query is not slow for everybody, but when we drill down, we can see only that particular tenant is slow, and vice is slow, because unlike other, he has five million images in his album, right, if you would think about some, you know, for the hosting application, so that's just an example. Another thing what we find very helpful is being able to go for a query, right, or to look to understand what tables it touches and reverse to find out all the queries which touches specific table. Why is that helpful? Well in many cases, our database operations are table specific, right, you may think, hey, you know what, I'm dropping this index, as I don't need it, or maybe I add an index, I add a column, right, you do some sort of maybe kind of partition table, right, you can do a lot of things with a table in scope, right, and then it would be very interesting to understand how that particular, how all the queries which touch that table have been affected, because we are much likely to be affected by that change compared to everybody else, right, I find that's a pretty, pretty cool feature. Database user is another one. If you do not have something like a squirrel command to enable, you often do not really see very well from what application given query comes in. But one of the practice you may follow at least is having a different application touching the same database using different user names, right, different users with different privileges, right, if nothing else that is a very good security practice, right, and that is where filtering and breakdown allows that. In a large, large, short environment, we also want to make sure we aggregate the data from a many database hosts, right, and can compare between each other. Typically when you have a short application, you want the load and hence response time between different database hosts to be kind of similar. But often it is not, right, it's often hard to achieve a perfect balance in between the nodes as one cause of the differences, but also things may just, you know, happen, you know, like you may have a settings which drift away on different nodes, you may have some, you know, differences, right, in the performance, especially in the cloud, which, you know, happen virtually from nowhere, right, I mean, I know a lot of people work in the cloud, you know, you know, sometimes you just get a lemon, right, or just like a bad node, which for some reason doesn't perform as well as its peers, right, and just want to, you know, maybe toss it and get another one, better one, right, but to do that you better understand what that is not performing particularly well. And the same also applies to their application server or web server. Again, like if you deploy application on, let's say, 100 application servers or web nodes, right, you may say, well, it's all should be the same. I have my, you know, automation which takes care of that. But again, well, things are not always as they should be. In many cases, you have something which doesn't work out. I have seen so many cases when people say, well, you know what, I already fixed that nasty query and I deployed the fix. When you look at that, well, it's actually was not deployed all the instances for whatever reason. Or you may say, well, you know what, my, I'm using a caching to reduce the query load on the database, but that caching is misconfigured to otherwise inaccessible on some of their web nodes, right. A lot of stuff can happen. Or maybe you're lucky and one of your web nodes was actually hacked and is also getting some additional queries to, you know, download your data and send it to someone. So I find making sure you can look at the query patterns separated by the different client hosts very, are something very valuable. I already mentioned with a SQL commenter which allows you to extend some additional metadata, which I think can be quite cool, right. And you can find the usage for custom tags in many cases. I've seen people, for example, tagging different instance types when I'm saying, well, you know what, this kind of new generation instance looks good. So let me put some of them in production and being able to compare. Well, is it actually working better? Sometimes yes. Sometimes, you know, no. The database version, right, maybe you're running out when you, minor Postgres release, you want to do it like on some subset of the nodes and to make sure there's no, no regressions, right. I mean, I think it's always good in this case to practice, you know, trust by verify, right, because sometimes you do run into unexpected changes, you know, you can validate the configuration changes this way and so on and so forth. Query plan is another area which I think is quite, quite interesting. In many cases, you'll find the same query depending on the parameters, right, or some other situations will have different plans. And if that different plans may have different query performance, and it is a very helpful if you can break down the performance by the different plans a query has, so you can understand if that is a plan issue or not, right. Otherwise, you may be looking at the query and say, well, you know what, something is fast, something is slow, you know, why is that, not very clear, their plans give us a very good information. Now when you find the query and see that as a problematic and you need to make it go fast, in this case, it's very good to understand there is that response time developers care so much about is coming from. And there are quite a few possibilities here. Some of them are instrumented better than others. For example, if you're looking at data crunch and disk IO, right, those are typically pretty well instrumented, you can find how much, you know, of CPU query consumes or that does. In terms of contention, that is typically more problematic, right, to say, hey, you know, what exactly those kind of internal synchronization object query had to wait, right, that is more tricky. You know, waits on CPU availability is even more tricky, right. And what I mean by this is this, right, so if you have a system which has much more runnable threads, runnable processes, right, than available CPU, then they will spend a lot of time waiting for available CPU, right, and that is very hard to see on its impact to the query response time. You typically can see that from the general node stats, like, hey, my CPU is back, I have like a ton of runnable CPU, right, CPU is also in recent kernels, you can see the information about their run queue latency, which is very cool, right, that tells you how long the processes had to wait to be scheduled on CPU after they are ready to start running. So a whole bunch of stuff here, some of them are easy, some of their work is still remaining. Now, from our standpoint, with all this kind of view on approach to the query monitor, we have been working at the extension for my square, oh, for Postgres, sorry, called the PgStat monitor, well, and look, we specifically built it for Postgres, not for MySQL, even though we had a lot more experience with MySQL, because Postgres SQL extension interface is awesome and much more powerful than MySQL, right, so you can read a little bit about this here, and this is extension which allows a lot more insights and getting kind of such slicing and dicing, which I mentioned, right, if you think about the traditional Postgres SQL extension PgStats statements, it really aggregates all the data from the start, right, which is very helpful to be used directly, what we look at the modern observability system through where we expect to have many Postgres SQL instances anyway, right, and some system getting that stuff constantly and considerate at that, so that means we are capturing a lot of information but keep it only on for a relatively short time in a Postgres SQL instance, right, and that allows to get much more granular information without requiring a huge amount of resources, which would be required if you would have it for a time, so you can, you know, read more about what that does on the web pages. Now some folks asked me, saying, well, folks, like, why do you work on a separate extension of the PgStats monitors, and my answer to that is we really wanted to experiment with different approaches, right, to find what works, what doesn't, how users do, and that is always easy to do in a separate extension, right, and then if something is liked by the community, then we can see how we can get that in an official list of extensions, so that is their feedback, is very valuable. And also if you look in this case while we are providing PgStats statements compatibility, right, so you can get that view from the same extension instead of getting another two extensions with additional overhead, PgStat monitor has kind of different ways to aggregate and present the data, right, which kind of, well, you cannot get in the same, in the same view. Okay, now as I spoke about the query performance, I wanted to highlight a couple of other things which are quite interesting to consider when you are looking at the queries where I see a number of issues. One is what I would call the bad queries versus victims, right. In certain cases, or like in many cases, right, you may see even your otherwise good queries like, hey, this is just a lookup by the primary key starting to be a lot slower than it usually is, not because something changes the relation to that query, but because of some other bad queries, right, have been running in parallel. And imagine that if you will oversaturate your node, right, the hundreds of bad queries running at the same time, right, well, then everything will become slow. And I think that's important to understand what if you are seeing some query being slow, you cannot just think about that as that query problem, it may be entirely something else. The next thing to consider is currently running queries. That is also rather interesting, right, because they may not be reflected in the log, right, or something which say, oh, that query completed and it was, you know, five minutes response time or 15 seconds, whatever, right. But running queries can be a problem. And in many cases, that is actually how things start to snowball, right, you have some application or even kind of user starts a lot of, you know, bad queries, you know, forgot like a where clause and a join, right, or something like that, and they just, you know, run for a long time, right, so you want to make sure you're paying attention to that as well. The next is to consider what not all activities are directly visible from a query standpoint. The database often tend to do a bunch of background activities, right. Additionally you may have something else, like maybe you are taking a snapshot, right, or taking a backup in the other way, which use also the system resources, right, which are not seen from query standpoint, but same important. You also have a lot of things which can be happening on the cloud level, right, again, which can be, you know, completely invisible for us. And wherever you are looking, again, at the query performance, it's important to consider where, you know, maybe something going on, right, additionally what those queries tell you. Next question is about, or last thing I would say, is about sampling. In certain cases I see people saying, well, you know what, let us only capture queries over X time. A lot of APM frameworks, right, for example, you know, like New Relics and such may be very focused on that, saying, hey, you know what, we are going to also give you some examples of the queries which take more than, you know, one second or whatever execution time. So focus on those. Well, and yes, looking at those queries may make sense, right, if they take a long time, that may be a problem, but it is often what your medium of performance queries, right, I would say are creating a whole bunch of load on your system, and they contribute the greatest response time to user application, right, and ignoring those can be problematic. Well, that is the main overview, right, I hope, what that was, that was helpful, right, and my main goal here is to make sure maybe to give you some thinking tools, right, as you noticed, that is not like particularly technical talk, right, which tells you how exactly to find out which indexes to create or something, but hopefully you get some tools in this case, how to start, how to approach that, which can prevent you from tuning by the credit card, you know, scaling the instances to inappropriate sizes, because hey, that is good for both your wallet as well as good for environment, right, we do not need those servers generating more heat than absolutely needed. Well, with that, it is all I have, and I would be happy to take some questions. Hey, thank you very much for your talk, my question is about when do you have to increase the box, as a developer, you are in front of a situation where you need to decide between optimizing or asking the CEO to just pay more, because you have a time constraint, so do you have the thumb rules where in front of a problem you would say, okay, better to optimize or better to increase the box, you know, when, how can you decide with me? The question is to, like, wherever it is better to increase the box size, right, or optimize the query. Well, and I think it is interesting, right, that it is not often either a question, right, I think the time in this case is also often essence, and many cases I have seen people saying if they have a problem, right, in this case, and they absolutely need to get like a application up, scale the box, right, and then kind of can currently work on the query optimization, right, and to bring it back and scale down. I think that is a very, very reasonable approach, right, because, well, it gives you kind of more briefing room. What is important in this case, as in many things in life, is not to be lazy, right, like you don't want to just, you know, scale the box and forget about that, you want to scale the box, optimize the queries and so on, right. Now I often, when I look at the queries, right, as you look at that, you can see which of them are low-hanging fruits, right, or when a query is already optimized pretty well, right. If you are saying, well, you know what, actually, majority of a workload is driven by lookups for, by the primary key for a table which is already in memory, you can say, well, you know what, there is very little I can do to optimize this thing, right. If you are saying, oh, that is a query which does massive join, if no indexes, well, totally different store, right, you may be able to make that to run thousand times faster, right, with relatively easy index add. Any other question? Hi, so as part of your slice and dice approach to monitoring queries, would you advise that concurrently queries in the, on the application side are never written as dynamic queries or as like anonymous prepared statements and only follow, say, named prepared statements so that you know we have a fixed set of queries that are always the same? Well, the question is, I would say, like it's kind of like a cart in the horse, right, like from, from my standpoint, right, like you can of course talk about those kind of practices, but developers like to do what is there, what keeps them productive, right, and in many cases saying, well, you know what, oh, you don't use like, or and frameworks, right, on the device and that, that is complicated, right. Now even if you're using dynamic queries, typically, they're still going to be at the, relate to a limited number of variations, right, and especially limited number of most important for variations which are going to be generated, you will still see that from the query type, right. So in many cases, like if you look at that, I would say like a whole set of queries, you would find, well, this application has, let's say, 10,000 of the distant queries, but if I look at top 20, that will be responsible like for 90, 99 percent response time, right, and that of course can change, right, but often focusing on those firsts, right, as well as maybe taking care of outliers, right, is a good kind of practice, how then you deal with that information that you have, makes sense. Any other question? Hello, thank you for the talk. What is the overhead of, to collect this statistic, because if you have, like, very, very much of, that is a good question, right, of course there is, I would say it varies, right, typically there is more overhead if you have like this, like a very simple fast queries, right, if you have like a logic queries for, which takes, you know, many seconds for them, it's less like, our design goal, right, which we are able to get is being similar to PGSTAT statements, right, and, you know, be a couple of percent or so, right, which I think in my opinion, right, many people when they think about that observability, you will tend to obsess about the overhead, right, but really often having that insights, right, often allow you to get so many things optimized when they matter, right, what the benefits are far outweighed. Do you have any advice for catching bad queries before they reach production and kind of like guarding these things? Oh yeah, absolutely. Like missing indexes or whatever, before they even. That is a very good question, right, so I didn't talk about this, but it's also a question where, right, in my opinion, and I think that's also what is very helpful with the open source solution, right, what you can really deploy it everywhere in, including your kind of CI, CD environment, right, because what I often see people saying, well, you know what, data dog, right, is expensive, it's only in production, right, what you want to do is make sure you have solutions in development so you can catch bad queries before they hit in production, but also assume you're not going to catch all the bad queries, right, some queries will only maybe misbehave in production, right, the other good practice which comes to that is you make sure you're like a test environment is good, right, so you can test a variety of queries relevant to your application and you have a good data set, right, for that. I think in this regard, there is like some cool features coming out from Neon, for example, like giving like branches, branching, right, then you can get like, oh, the full copy of production database, you know, mess with it, run tests on it on a full-size data set, right, instead of testing on, you know, table with 100 rows, right, which is kind of useless. Cool. Any other question? Okay, thank you very much. Okay, thank you. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.92, "text": " So, welcome to the Post-Quest Girl Dev Room.", "tokens": [407, 11, 2928, 281, 264, 10223, 12, 8547, 377, 8502, 9096, 19190, 13], "temperature": 0.0, "avg_logprob": -0.3324465560913086, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.37753361463546753}, {"id": 1, "seek": 0, "start": 7.92, "end": 13.92, "text": " If you weren't here before, can I please ask you to silence your phones and extend a very", "tokens": [759, 291, 4999, 380, 510, 949, 11, 393, 286, 1767, 1029, 291, 281, 12239, 428, 10216, 293, 10101, 257, 588], "temperature": 0.0, "avg_logprob": -0.3324465560913086, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.37753361463546753}, {"id": 2, "seek": 0, "start": 13.92, "end": 17.92, "text": " warm welcome to Peter Zaitsev.", "tokens": [4561, 2928, 281, 6508, 1176, 1001, 405, 85, 13], "temperature": 0.0, "avg_logprob": -0.3324465560913086, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.37753361463546753}, {"id": 3, "seek": 1792, "start": 17.92, "end": 27.0, "text": " Okay, well, thank you.", "tokens": [1033, 11, 731, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 4, "seek": 1792, "start": 27.0, "end": 30.64, "text": " We are going to talk about query performance today.", "tokens": [492, 366, 516, 281, 751, 466, 14581, 3389, 965, 13], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 5, "seek": 1792, "start": 30.64, "end": 35.760000000000005, "text": " But before that, let me understand a little bit who do we have here.", "tokens": [583, 949, 300, 11, 718, 385, 1223, 257, 707, 857, 567, 360, 321, 362, 510, 13], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 6, "seek": 1792, "start": 35.760000000000005, "end": 43.2, "text": " Now, which of you would mostly see yourself as DBA, SRE, CSADMIN, kind of on the operations", "tokens": [823, 11, 597, 295, 291, 576, 5240, 536, 1803, 382, 413, 9295, 11, 318, 3850, 11, 9460, 6112, 42190, 11, 733, 295, 322, 264, 7705], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 7, "seek": 1792, "start": 43.2, "end": 44.2, "text": " side?", "tokens": [1252, 30], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 8, "seek": 1792, "start": 44.2, "end": 45.8, "text": " Just, you know, can we have a...", "tokens": [1449, 11, 291, 458, 11, 393, 321, 362, 257, 485], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 9, "seek": 1792, "start": 45.8, "end": 46.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.26584950360384857, "compression_ratio": 1.3592233009708738, "no_speech_prob": 0.004835905972868204}, {"id": 10, "seek": 4680, "start": 46.8, "end": 49.08, "text": " Now, which of you are developers?", "tokens": [823, 11, 597, 295, 291, 366, 8849, 30], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 11, "seek": 4680, "start": 49.08, "end": 51.68, "text": " Ooh, lots of developers.", "tokens": [7951, 11, 3195, 295, 8849, 13], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 12, "seek": 4680, "start": 51.68, "end": 52.68, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 13, "seek": 4680, "start": 52.68, "end": 58.76, "text": " Now, in terms of developers, right, now if you do it again, but now for sort of like", "tokens": [823, 11, 294, 2115, 295, 8849, 11, 558, 11, 586, 498, 291, 360, 309, 797, 11, 457, 586, 337, 1333, 295, 411], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 14, "seek": 4680, "start": 58.76, "end": 63.04, "text": " a front-end developers, right, something not, you know, database kernel or something, you", "tokens": [257, 1868, 12, 521, 8849, 11, 558, 11, 746, 406, 11, 291, 458, 11, 8149, 28256, 420, 746, 11, 291], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 15, "seek": 4680, "start": 63.04, "end": 67.12, "text": " know, other complicated stuff, but something more simple.", "tokens": [458, 11, 661, 6179, 1507, 11, 457, 746, 544, 2199, 13], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 16, "seek": 4680, "start": 67.12, "end": 68.12, "text": " Front-end developers.", "tokens": [17348, 12, 521, 8849, 13], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 17, "seek": 4680, "start": 68.12, "end": 69.12, "text": " Any?", "tokens": [2639, 30], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 18, "seek": 4680, "start": 69.12, "end": 70.12, "text": " Yeah!", "tokens": [865, 0], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 19, "seek": 4680, "start": 70.12, "end": 71.12, "text": " Hello!", "tokens": [2425, 0], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 20, "seek": 4680, "start": 71.12, "end": 72.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2904247427886387, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.0004617779632098973}, {"id": 21, "seek": 7212, "start": 72.12, "end": 82.84, "text": " Well, anyway, so one of the good points of this talk for me, right, is really to try to", "tokens": [1042, 11, 4033, 11, 370, 472, 295, 264, 665, 2793, 295, 341, 751, 337, 385, 11, 558, 11, 307, 534, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.24586027318781073, "compression_ratio": 1.593939393939394, "no_speech_prob": 0.0002897154481615871}, {"id": 22, "seek": 7212, "start": 82.84, "end": 89.52000000000001, "text": " bridge the gap what I see a lot between how their operations people, right, the people", "tokens": [7283, 264, 7417, 437, 286, 536, 257, 688, 1296, 577, 641, 7705, 561, 11, 558, 11, 264, 561], "temperature": 0.0, "avg_logprob": -0.24586027318781073, "compression_ratio": 1.593939393939394, "no_speech_prob": 0.0002897154481615871}, {"id": 23, "seek": 7212, "start": 89.52000000000001, "end": 99.44, "text": " who are deeply vested in a databases, right, development, think about them versus people", "tokens": [567, 366, 8760, 49317, 294, 257, 22380, 11, 558, 11, 3250, 11, 519, 466, 552, 5717, 561], "temperature": 0.0, "avg_logprob": -0.24586027318781073, "compression_ratio": 1.593939393939394, "no_speech_prob": 0.0002897154481615871}, {"id": 24, "seek": 9944, "start": 99.44, "end": 104.64, "text": " who are just happened to use those databases for the application, right?", "tokens": [567, 366, 445, 2011, 281, 764, 729, 22380, 337, 264, 3861, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 25, "seek": 9944, "start": 104.64, "end": 110.36, "text": " And often their relationship to the database is, well, really quite different, right?", "tokens": [400, 2049, 641, 2480, 281, 264, 8149, 307, 11, 731, 11, 534, 1596, 819, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 26, "seek": 9944, "start": 110.36, "end": 115.96, "text": " As a database kernel developers, we often deeply care about all those kind of internal", "tokens": [1018, 257, 8149, 28256, 8849, 11, 321, 2049, 8760, 1127, 466, 439, 729, 733, 295, 6920], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 27, "seek": 9944, "start": 115.96, "end": 121.12, "text": " algorithms, have a, you know, discussion, what is the best way to implement these and", "tokens": [14642, 11, 362, 257, 11, 291, 458, 11, 5017, 11, 437, 307, 264, 1151, 636, 281, 4445, 613, 293], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 28, "seek": 9944, "start": 121.12, "end": 122.72, "text": " that cases.", "tokens": [300, 3331, 13], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 29, "seek": 9944, "start": 122.72, "end": 128.72, "text": " But for many developers writing applications, well, you know, we think about databases, you", "tokens": [583, 337, 867, 8849, 3579, 5821, 11, 731, 11, 291, 458, 11, 321, 519, 466, 22380, 11, 291], "temperature": 0.0, "avg_logprob": -0.18311239941285387, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.448451025178656e-05}, {"id": 30, "seek": 12872, "start": 128.72, "end": 133.44, "text": " know, as you think about like a plumbing, right, well, it just got to work, you don't", "tokens": [458, 11, 382, 291, 519, 466, 411, 257, 39993, 11, 558, 11, 731, 11, 309, 445, 658, 281, 589, 11, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.18108693291159236, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.0473345759673975e-05}, {"id": 31, "seek": 12872, "start": 133.44, "end": 140.24, "text": " want to think about it, well, it just, if it doesn't, then that becomes a problem, right?", "tokens": [528, 281, 519, 466, 309, 11, 731, 11, 309, 445, 11, 498, 309, 1177, 380, 11, 550, 300, 3643, 257, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18108693291159236, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.0473345759673975e-05}, {"id": 32, "seek": 12872, "start": 140.24, "end": 144.32, "text": " They think about database in many cases as a black box.", "tokens": [814, 519, 466, 8149, 294, 867, 3331, 382, 257, 2211, 2424, 13], "temperature": 0.0, "avg_logprob": -0.18108693291159236, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.0473345759673975e-05}, {"id": 33, "seek": 12872, "start": 144.32, "end": 151.16, "text": " And I think that is increasingly happening now, especially when we have so many databases", "tokens": [400, 286, 519, 300, 307, 12980, 2737, 586, 11, 2318, 562, 321, 362, 370, 867, 22380], "temperature": 0.0, "avg_logprob": -0.18108693291159236, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.0473345759673975e-05}, {"id": 34, "seek": 12872, "start": 151.16, "end": 155.64, "text": " which are deployed in a cloud as a database as a service, right?", "tokens": [597, 366, 17826, 294, 257, 4588, 382, 257, 8149, 382, 257, 2643, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18108693291159236, "compression_ratio": 1.7079646017699115, "no_speech_prob": 3.0473345759673975e-05}, {"id": 35, "seek": 15564, "start": 155.64, "end": 163.11999999999998, "text": " Because in this case, especially, well, you just have a database and somebody else focuses", "tokens": [1436, 294, 341, 1389, 11, 2318, 11, 731, 11, 291, 445, 362, 257, 8149, 293, 2618, 1646, 16109], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 36, "seek": 15564, "start": 163.11999999999998, "end": 164.11999999999998, "text": " on the other stuff.", "tokens": [322, 264, 661, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 37, "seek": 15564, "start": 164.11999999999998, "end": 168.04, "text": " So what does that database mean from developer standpoint in many cases?", "tokens": [407, 437, 775, 300, 8149, 914, 490, 10754, 15827, 294, 867, 3331, 30], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 38, "seek": 15564, "start": 168.04, "end": 172.39999999999998, "text": " Well, that means you get some sort of service point, you should use in your application,", "tokens": [1042, 11, 300, 1355, 291, 483, 512, 1333, 295, 2643, 935, 11, 291, 820, 764, 294, 428, 3861, 11], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 39, "seek": 15564, "start": 172.39999999999998, "end": 178.51999999999998, "text": " right, you can connect to that service point and get that quickly with no problem, right?", "tokens": [558, 11, 291, 393, 1745, 281, 300, 2643, 935, 293, 483, 300, 2661, 365, 572, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 40, "seek": 15564, "start": 178.51999999999998, "end": 184.72, "text": " And then you run the queries you need to run, right, of a database or maybe even what", "tokens": [400, 550, 291, 1190, 264, 24109, 291, 643, 281, 1190, 11, 558, 11, 295, 257, 8149, 420, 1310, 754, 437], "temperature": 0.0, "avg_logprob": -0.17653883828057182, "compression_ratio": 1.7431906614785992, "no_speech_prob": 1.1438336514402181e-05}, {"id": 41, "seek": 18472, "start": 184.72, "end": 189.4, "text": " your RAM framework, right, or something generates.", "tokens": [428, 14561, 8388, 11, 558, 11, 420, 746, 23815, 13], "temperature": 0.0, "avg_logprob": -0.17669931242737588, "compression_ratio": 1.59, "no_speech_prob": 2.7441767088021152e-05}, {"id": 42, "seek": 18472, "start": 189.4, "end": 191.44, "text": " Now what do you want from those queries?", "tokens": [823, 437, 360, 291, 528, 490, 729, 24109, 30], "temperature": 0.0, "avg_logprob": -0.17669931242737588, "compression_ratio": 1.59, "no_speech_prob": 2.7441767088021152e-05}, {"id": 43, "seek": 18472, "start": 191.44, "end": 197.16, "text": " Well, as a selfish developer, you want those queries to run with no errors.", "tokens": [1042, 11, 382, 257, 19074, 10754, 11, 291, 528, 729, 24109, 281, 1190, 365, 572, 13603, 13], "temperature": 0.0, "avg_logprob": -0.17669931242737588, "compression_ratio": 1.59, "no_speech_prob": 2.7441767088021152e-05}, {"id": 44, "seek": 18472, "start": 197.16, "end": 201.16, "text": " You want to make sure they get your correct results, right?", "tokens": [509, 528, 281, 652, 988, 436, 483, 428, 3006, 3542, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17669931242737588, "compression_ratio": 1.59, "no_speech_prob": 2.7441767088021152e-05}, {"id": 45, "seek": 18472, "start": 201.16, "end": 210.6, "text": " And you want to make sure you run them within response time, which is appropriate for your", "tokens": [400, 291, 528, 281, 652, 988, 291, 1190, 552, 1951, 4134, 565, 11, 597, 307, 6854, 337, 428], "temperature": 0.0, "avg_logprob": -0.17669931242737588, "compression_ratio": 1.59, "no_speech_prob": 2.7441767088021152e-05}, {"id": 46, "seek": 21060, "start": 210.6, "end": 215.23999999999998, "text": " application and for query time and for query kind.", "tokens": [3861, 293, 337, 14581, 565, 293, 337, 14581, 733, 13], "temperature": 0.0, "avg_logprob": -0.14801425933837892, "compression_ratio": 1.6359447004608294, "no_speech_prob": 3.600567652028985e-05}, {"id": 47, "seek": 21060, "start": 215.23999999999998, "end": 221.92, "text": " And I think that is very important to understand here what if I am looking as a developer and", "tokens": [400, 286, 519, 300, 307, 588, 1021, 281, 1223, 510, 437, 498, 286, 669, 1237, 382, 257, 10754, 293], "temperature": 0.0, "avg_logprob": -0.14801425933837892, "compression_ratio": 1.6359447004608294, "no_speech_prob": 3.600567652028985e-05}, {"id": 48, "seek": 21060, "start": 221.92, "end": 228.95999999999998, "text": " a database from performance standpoint, I understand that as how quickly that database", "tokens": [257, 8149, 490, 3389, 15827, 11, 286, 1223, 300, 382, 577, 2661, 300, 8149], "temperature": 0.0, "avg_logprob": -0.14801425933837892, "compression_ratio": 1.6359447004608294, "no_speech_prob": 3.600567652028985e-05}, {"id": 49, "seek": 21060, "start": 228.95999999999998, "end": 233.64, "text": " responds to my queries, right?", "tokens": [27331, 281, 452, 24109, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14801425933837892, "compression_ratio": 1.6359447004608294, "no_speech_prob": 3.600567652028985e-05}, {"id": 50, "seek": 21060, "start": 233.64, "end": 240.0, "text": " Now if you think about their software design in general, right, and I think especially maybe", "tokens": [823, 498, 291, 519, 466, 641, 4722, 1715, 294, 2674, 11, 558, 11, 293, 286, 519, 2318, 1310], "temperature": 0.0, "avg_logprob": -0.14801425933837892, "compression_ratio": 1.6359447004608294, "no_speech_prob": 3.600567652028985e-05}, {"id": 51, "seek": 24000, "start": 240.0, "end": 244.2, "text": " not developers, but architects often have to care about a whole bunch of other things", "tokens": [406, 8849, 11, 457, 30491, 2049, 362, 281, 1127, 466, 257, 1379, 3840, 295, 661, 721], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 52, "seek": 24000, "start": 244.2, "end": 246.48, "text": " beyond just the performance.", "tokens": [4399, 445, 264, 3389, 13], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 53, "seek": 24000, "start": 246.48, "end": 250.72, "text": " For example, we often have to care about security, right?", "tokens": [1171, 1365, 11, 321, 2049, 362, 281, 1127, 466, 3825, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 54, "seek": 24000, "start": 250.72, "end": 254.64, "text": " And typically security costs stuff, right?", "tokens": [400, 5850, 3825, 5497, 1507, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 55, "seek": 24000, "start": 254.64, "end": 260.84, "text": " It comes with overhead, both in terms of performance overhead and, you know, organizational overhead", "tokens": [467, 1487, 365, 19922, 11, 1293, 294, 2115, 295, 3389, 19922, 293, 11, 291, 458, 11, 24730, 19922], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 56, "seek": 24000, "start": 260.84, "end": 262.08, "text": " and so on and so forth, right?", "tokens": [293, 370, 322, 293, 370, 5220, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 57, "seek": 24000, "start": 262.08, "end": 266.6, "text": " That's done to factor authentication always takes another couple of seconds, right?", "tokens": [663, 311, 1096, 281, 5952, 26643, 1009, 2516, 1071, 1916, 295, 3949, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21603988647460937, "compression_ratio": 1.7449392712550607, "no_speech_prob": 6.487937207566574e-05}, {"id": 58, "seek": 26660, "start": 266.6, "end": 270.84000000000003, "text": " But that makes us more secure.", "tokens": [583, 300, 1669, 505, 544, 7144, 13], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 59, "seek": 26660, "start": 270.84000000000003, "end": 276.28000000000003, "text": " Availability is also important, as well as things like costs.", "tokens": [11667, 864, 2310, 307, 611, 1021, 11, 382, 731, 382, 721, 411, 5497, 13], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 60, "seek": 26660, "start": 276.28000000000003, "end": 281.76000000000005, "text": " I think that is especially important, again, in the modern age when they have a lot of", "tokens": [286, 519, 300, 307, 2318, 1021, 11, 797, 11, 294, 264, 4363, 3205, 562, 436, 362, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 61, "seek": 26660, "start": 281.76000000000005, "end": 285.12, "text": " cloud, which is elastic, right?", "tokens": [4588, 11, 597, 307, 17115, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 62, "seek": 26660, "start": 285.12, "end": 289.12, "text": " But that elasticity comes also with spend, right?", "tokens": [583, 300, 46260, 1487, 611, 365, 3496, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 63, "seek": 26660, "start": 289.12, "end": 293.72, "text": " You often can say, hey, you know what, if I just need my queries to run faster, I can", "tokens": [509, 2049, 393, 584, 11, 4177, 11, 291, 458, 437, 11, 498, 286, 445, 643, 452, 24109, 281, 1190, 4663, 11, 286, 393], "temperature": 0.0, "avg_logprob": -0.16712628414756373, "compression_ratio": 1.5772727272727274, "no_speech_prob": 4.808436642633751e-05}, {"id": 64, "seek": 29372, "start": 293.72, "end": 297.40000000000003, "text": " blow up my instant size, right, or something else.", "tokens": [6327, 493, 452, 9836, 2744, 11, 558, 11, 420, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.13972588146434112, "compression_ratio": 1.606837606837607, "no_speech_prob": 4.6783225116087124e-05}, {"id": 65, "seek": 29372, "start": 297.40000000000003, "end": 304.64000000000004, "text": " But well, guess what, that also will be expensive, right, if you're not doing efficiently.", "tokens": [583, 731, 11, 2041, 437, 11, 300, 611, 486, 312, 5124, 11, 558, 11, 498, 291, 434, 406, 884, 19621, 13], "temperature": 0.0, "avg_logprob": -0.13972588146434112, "compression_ratio": 1.606837606837607, "no_speech_prob": 4.6783225116087124e-05}, {"id": 66, "seek": 29372, "start": 304.64000000000004, "end": 310.16, "text": " And there is, let's say, a bunch of other things you want to consider about, right?", "tokens": [400, 456, 307, 11, 718, 311, 584, 11, 257, 3840, 295, 661, 721, 291, 528, 281, 1949, 466, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13972588146434112, "compression_ratio": 1.606837606837607, "no_speech_prob": 4.6783225116087124e-05}, {"id": 67, "seek": 29372, "start": 310.16, "end": 315.52000000000004, "text": " So I don't want to simplify that, let's say, to what everything is also only about query", "tokens": [407, 286, 500, 380, 528, 281, 20460, 300, 11, 718, 311, 584, 11, 281, 437, 1203, 307, 611, 787, 466, 14581], "temperature": 0.0, "avg_logprob": -0.13972588146434112, "compression_ratio": 1.606837606837607, "no_speech_prob": 4.6783225116087124e-05}, {"id": 68, "seek": 29372, "start": 315.52000000000004, "end": 322.64000000000004, "text": " performance, but that is what I am going to focus in my talk.", "tokens": [3389, 11, 457, 300, 307, 437, 286, 669, 516, 281, 1879, 294, 452, 751, 13], "temperature": 0.0, "avg_logprob": -0.13972588146434112, "compression_ratio": 1.606837606837607, "no_speech_prob": 4.6783225116087124e-05}, {"id": 69, "seek": 32264, "start": 322.64, "end": 330.32, "text": " Now when you think about response time from the database standpoint, we often think about", "tokens": [823, 562, 291, 519, 466, 4134, 565, 490, 264, 8149, 15827, 11, 321, 2049, 519, 466], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 70, "seek": 32264, "start": 330.32, "end": 333.84, "text": " that from a query context, right?", "tokens": [300, 490, 257, 14581, 4319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 71, "seek": 32264, "start": 333.84, "end": 339.4, "text": " Well I see my database responds to the queries XYZ, you know, in average or something, right?", "tokens": [1042, 286, 536, 452, 8149, 27331, 281, 264, 24109, 48826, 57, 11, 291, 458, 11, 294, 4274, 420, 746, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 72, "seek": 32264, "start": 339.4, "end": 341.24, "text": " You think about that query basics.", "tokens": [509, 519, 466, 300, 14581, 14688, 13], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 73, "seek": 32264, "start": 341.24, "end": 345.52, "text": " But if you really look at from a business standpoint, right, how your boss or boss is", "tokens": [583, 498, 291, 534, 574, 412, 490, 257, 1606, 15827, 11, 558, 11, 577, 428, 5741, 420, 5741, 307], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 74, "seek": 32264, "start": 345.52, "end": 351.59999999999997, "text": " boss is boss, right, where it thinks about that, it's mostly about the users which are", "tokens": [5741, 307, 5741, 11, 558, 11, 689, 309, 7309, 466, 300, 11, 309, 311, 5240, 466, 264, 5022, 597, 366], "temperature": 0.0, "avg_logprob": -0.1896824746761682, "compression_ratio": 1.8162393162393162, "no_speech_prob": 3.2857031328603625e-05}, {"id": 75, "seek": 35160, "start": 351.6, "end": 353.56, "text": " using your applications, right?", "tokens": [1228, 428, 5821, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 76, "seek": 35160, "start": 353.56, "end": 359.86, "text": " And I typically would define it what really folks are after is what the users of your", "tokens": [400, 286, 5850, 576, 6964, 309, 437, 534, 4024, 366, 934, 307, 437, 264, 5022, 295, 428], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 77, "seek": 35160, "start": 359.86, "end": 365.8, "text": " applications, right, and all users, right, have outstanding experience in terms of performance", "tokens": [5821, 11, 558, 11, 293, 439, 5022, 11, 558, 11, 362, 14485, 1752, 294, 2115, 295, 3389], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 78, "seek": 35160, "start": 365.8, "end": 368.76000000000005, "text": " for all their interactions.", "tokens": [337, 439, 641, 13280, 13], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 79, "seek": 35160, "start": 368.76000000000005, "end": 372.6, "text": " Because in application, you often may have different interactions, right, and I want", "tokens": [1436, 294, 3861, 11, 291, 2049, 815, 362, 819, 13280, 11, 558, 11, 293, 286, 528], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 80, "seek": 35160, "start": 372.6, "end": 377.44, "text": " to make sure I have a search which is fast and place in an order which is fast, right,", "tokens": [281, 652, 988, 286, 362, 257, 3164, 597, 307, 2370, 293, 1081, 294, 364, 1668, 597, 307, 2370, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.18416694800059, "compression_ratio": 1.8392857142857142, "no_speech_prob": 0.00010469068365637213}, {"id": 81, "seek": 37744, "start": 377.44, "end": 383.24, "text": " and whatever other things all working quickly.", "tokens": [293, 2035, 661, 721, 439, 1364, 2661, 13], "temperature": 0.0, "avg_logprob": -0.19762958799089705, "compression_ratio": 1.5458937198067633, "no_speech_prob": 2.9334634746192023e-05}, {"id": 82, "seek": 37744, "start": 383.24, "end": 390.28, "text": " Now as database engineers, we often want to talk about performance and availability as", "tokens": [823, 382, 8149, 11955, 11, 321, 2049, 528, 281, 751, 466, 3389, 293, 17945, 382], "temperature": 0.0, "avg_logprob": -0.19762958799089705, "compression_ratio": 1.5458937198067633, "no_speech_prob": 2.9334634746192023e-05}, {"id": 83, "seek": 37744, "start": 390.28, "end": 395.84, "text": " a different thing, right, like saying, well, no, no, no, the database was up, it just was", "tokens": [257, 819, 551, 11, 558, 11, 411, 1566, 11, 731, 11, 572, 11, 572, 11, 572, 11, 264, 8149, 390, 493, 11, 309, 445, 390], "temperature": 0.0, "avg_logprob": -0.19762958799089705, "compression_ratio": 1.5458937198067633, "no_speech_prob": 2.9334634746192023e-05}, {"id": 84, "seek": 37744, "start": 395.84, "end": 402.12, "text": " overloaded, so that query took 15 seconds, oh, 15 minutes, right, or something like that,", "tokens": [28777, 292, 11, 370, 300, 14581, 1890, 2119, 3949, 11, 1954, 11, 2119, 2077, 11, 558, 11, 420, 746, 411, 300, 11], "temperature": 0.0, "avg_logprob": -0.19762958799089705, "compression_ratio": 1.5458937198067633, "no_speech_prob": 2.9334634746192023e-05}, {"id": 85, "seek": 37744, "start": 402.12, "end": 403.12, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.19762958799089705, "compression_ratio": 1.5458937198067633, "no_speech_prob": 2.9334634746192023e-05}, {"id": 86, "seek": 40312, "start": 403.12, "end": 409.36, "text": " But the reality is for the user, their very bad performance is really indistinguishable", "tokens": [583, 264, 4103, 307, 337, 264, 4195, 11, 641, 588, 1578, 3389, 307, 534, 1016, 468, 7050, 742, 712], "temperature": 0.0, "avg_logprob": -0.25397890264337714, "compression_ratio": 1.6118721461187215, "no_speech_prob": 7.612400077050552e-05}, {"id": 87, "seek": 40312, "start": 409.36, "end": 417.8, "text": " from downtime, because, well, A, people have a limited experience, right, and if something", "tokens": [490, 49648, 11, 570, 11, 731, 11, 316, 11, 561, 362, 257, 5567, 1752, 11, 558, 11, 293, 498, 746], "temperature": 0.0, "avg_logprob": -0.25397890264337714, "compression_ratio": 1.6118721461187215, "no_speech_prob": 7.612400077050552e-05}, {"id": 88, "seek": 40312, "start": 417.8, "end": 423.6, "text": " is taking too long, we'll just go into closer page, and even if you have some, something", "tokens": [307, 1940, 886, 938, 11, 321, 603, 445, 352, 666, 4966, 3028, 11, 293, 754, 498, 291, 362, 512, 11, 746], "temperature": 0.0, "avg_logprob": -0.25397890264337714, "compression_ratio": 1.6118721461187215, "no_speech_prob": 7.612400077050552e-05}, {"id": 89, "seek": 40312, "start": 423.6, "end": 427.04, "text": " of unlimited patience, there is going to be a whole bunch of timeouts, including your", "tokens": [295, 21950, 14826, 11, 456, 307, 516, 281, 312, 257, 1379, 3840, 295, 565, 7711, 11, 3009, 428], "temperature": 0.0, "avg_logprob": -0.25397890264337714, "compression_ratio": 1.6118721461187215, "no_speech_prob": 7.612400077050552e-05}, {"id": 90, "seek": 42704, "start": 427.04, "end": 433.6, "text": " browser timeouts which will, you know, show you what the page cannot load well before", "tokens": [11185, 565, 7711, 597, 486, 11, 291, 458, 11, 855, 291, 437, 264, 3028, 2644, 3677, 731, 949], "temperature": 0.0, "avg_logprob": -0.20929634178077783, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.8124743291991763e-05}, {"id": 91, "seek": 42704, "start": 433.6, "end": 435.76000000000005, "text": " 15 minutes, right?", "tokens": [2119, 2077, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20929634178077783, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.8124743291991763e-05}, {"id": 92, "seek": 42704, "start": 435.76000000000005, "end": 444.08000000000004, "text": " So I think that is another important thing which I find also important talking to some,", "tokens": [407, 286, 519, 300, 307, 1071, 1021, 551, 597, 286, 915, 611, 1021, 1417, 281, 512, 11], "temperature": 0.0, "avg_logprob": -0.20929634178077783, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.8124743291991763e-05}, {"id": 93, "seek": 42704, "start": 444.08000000000004, "end": 449.84000000000003, "text": " maybe business people about why spend resources on performance, query performance optimization,", "tokens": [1310, 1606, 561, 466, 983, 3496, 3593, 322, 3389, 11, 14581, 3389, 19618, 11], "temperature": 0.0, "avg_logprob": -0.20929634178077783, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.8124743291991763e-05}, {"id": 94, "seek": 42704, "start": 449.84000000000003, "end": 453.72, "text": " and so on and so forth, right, because, well, you know what, if it doesn't perform, it is", "tokens": [293, 370, 322, 293, 370, 5220, 11, 558, 11, 570, 11, 731, 11, 291, 458, 437, 11, 498, 309, 1177, 380, 2042, 11, 309, 307], "temperature": 0.0, "avg_logprob": -0.20929634178077783, "compression_ratio": 1.6293103448275863, "no_speech_prob": 1.8124743291991763e-05}, {"id": 95, "seek": 45372, "start": 453.72, "end": 458.68, "text": " down, right?", "tokens": [760, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2298340595943827, "compression_ratio": 1.5935828877005347, "no_speech_prob": 2.386035703239031e-05}, {"id": 96, "seek": 45372, "start": 458.68, "end": 469.48, "text": " Another thing what I would point out, right, is in many cases, you see people talking about", "tokens": [3996, 551, 437, 286, 576, 935, 484, 11, 558, 11, 307, 294, 867, 3331, 11, 291, 536, 561, 1417, 466], "temperature": 0.0, "avg_logprob": -0.2298340595943827, "compression_ratio": 1.5935828877005347, "no_speech_prob": 2.386035703239031e-05}, {"id": 97, "seek": 45372, "start": 469.48, "end": 476.56, "text": " the averages, right, while the query performance was so many, you know, milliseconds or something", "tokens": [264, 42257, 11, 558, 11, 1339, 264, 14581, 3389, 390, 370, 867, 11, 291, 458, 11, 34184, 420, 746], "temperature": 0.0, "avg_logprob": -0.2298340595943827, "compression_ratio": 1.5935828877005347, "no_speech_prob": 2.386035703239031e-05}, {"id": 98, "seek": 45372, "start": 476.56, "end": 483.68, "text": " in average, right, and while it may be helpful for comparison standpoint compared to yesterday,", "tokens": [294, 4274, 11, 558, 11, 293, 1339, 309, 815, 312, 4961, 337, 9660, 15827, 5347, 281, 5186, 11], "temperature": 0.0, "avg_logprob": -0.2298340595943827, "compression_ratio": 1.5935828877005347, "no_speech_prob": 2.386035703239031e-05}, {"id": 99, "seek": 48368, "start": 483.68, "end": 494.92, "text": " really, it is not very helpful, right, because, well, the average maybe what you're looking", "tokens": [534, 11, 309, 307, 406, 588, 4961, 11, 558, 11, 570, 11, 731, 11, 264, 4274, 1310, 437, 291, 434, 1237], "temperature": 0.0, "avg_logprob": -0.20485728127615793, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.0002543146547395736}, {"id": 100, "seek": 48368, "start": 494.92, "end": 501.2, "text": " for may be way too many queries which are too slow, right, just balanced by the queries", "tokens": [337, 815, 312, 636, 886, 867, 24109, 597, 366, 886, 2964, 11, 558, 11, 445, 13902, 538, 264, 24109], "temperature": 0.0, "avg_logprob": -0.20485728127615793, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.0002543146547395736}, {"id": 101, "seek": 48368, "start": 501.2, "end": 511.72, "text": " which are high, right, and as I wrote here, I really like this saying, we won't leave", "tokens": [597, 366, 1090, 11, 558, 11, 293, 382, 286, 4114, 510, 11, 286, 534, 411, 341, 1566, 11, 321, 1582, 380, 1856], "temperature": 0.0, "avg_logprob": -0.20485728127615793, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.0002543146547395736}, {"id": 102, "seek": 51172, "start": 511.72, "end": 519.24, "text": " the man who tried to cross a river in average one meter deep, right, where once leave the", "tokens": [264, 587, 567, 3031, 281, 3278, 257, 6810, 294, 4274, 472, 9255, 2452, 11, 558, 11, 689, 1564, 1856, 264], "temperature": 0.0, "avg_logprob": -0.19225230481889513, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.0001053300584317185}, {"id": 103, "seek": 51172, "start": 519.24, "end": 524.44, "text": " man.", "tokens": [587, 13], "temperature": 0.0, "avg_logprob": -0.19225230481889513, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.0001053300584317185}, {"id": 104, "seek": 51172, "start": 524.44, "end": 531.5600000000001, "text": " So in this regard, I think it's very helpful to look at things like a percentile response", "tokens": [407, 294, 341, 3843, 11, 286, 519, 309, 311, 588, 4961, 281, 574, 412, 721, 411, 257, 3043, 794, 4134], "temperature": 0.0, "avg_logprob": -0.19225230481889513, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.0001053300584317185}, {"id": 105, "seek": 51172, "start": 531.5600000000001, "end": 535.88, "text": " time at the very least, right, if you want to look at one number because you're looking", "tokens": [565, 412, 264, 588, 1935, 11, 558, 11, 498, 291, 528, 281, 574, 412, 472, 1230, 570, 291, 434, 1237], "temperature": 0.0, "avg_logprob": -0.19225230481889513, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.0001053300584317185}, {"id": 106, "seek": 53588, "start": 535.88, "end": 543.52, "text": " at simplicity, 99 percentile for query response time is much better than average response time.", "tokens": [412, 25632, 11, 11803, 3043, 794, 337, 14581, 4134, 565, 307, 709, 1101, 813, 4274, 4134, 565, 13], "temperature": 0.0, "avg_logprob": -0.17560661540311925, "compression_ratio": 1.5942857142857143, "no_speech_prob": 9.860780119197443e-05}, {"id": 107, "seek": 53588, "start": 543.52, "end": 548.32, "text": " What is even better is, of course, is to look some sort of distribution, you know, query", "tokens": [708, 307, 754, 1101, 307, 11, 295, 1164, 11, 307, 281, 574, 512, 1333, 295, 7316, 11, 291, 458, 11, 14581], "temperature": 0.0, "avg_logprob": -0.17560661540311925, "compression_ratio": 1.5942857142857143, "no_speech_prob": 9.860780119197443e-05}, {"id": 108, "seek": 53588, "start": 548.32, "end": 552.12, "text": " histogram distribution and how it changes over time.", "tokens": [49816, 7316, 293, 577, 309, 2962, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.17560661540311925, "compression_ratio": 1.5942857142857143, "no_speech_prob": 9.860780119197443e-05}, {"id": 109, "seek": 53588, "start": 552.12, "end": 556.84, "text": " That often can give you a lot of insight.", "tokens": [663, 2049, 393, 976, 291, 257, 688, 295, 11269, 13], "temperature": 0.0, "avg_logprob": -0.17560661540311925, "compression_ratio": 1.5942857142857143, "no_speech_prob": 9.860780119197443e-05}, {"id": 110, "seek": 55684, "start": 556.84, "end": 565.88, "text": " The thing from percentile, though, is it's interesting how it works as you go from that", "tokens": [440, 551, 490, 3043, 794, 11, 1673, 11, 307, 309, 311, 1880, 577, 309, 1985, 382, 291, 352, 490, 300], "temperature": 0.0, "avg_logprob": -0.171914659697434, "compression_ratio": 1.6697247706422018, "no_speech_prob": 3.073725019930862e-05}, {"id": 111, "seek": 55684, "start": 565.88, "end": 572.8000000000001, "text": " query to the user experience, right, you spoke about, because think about this, right, typically", "tokens": [14581, 281, 264, 4195, 1752, 11, 558, 11, 291, 7179, 466, 11, 570, 519, 466, 341, 11, 558, 11, 5850], "temperature": 0.0, "avg_logprob": -0.171914659697434, "compression_ratio": 1.6697247706422018, "no_speech_prob": 3.073725019930862e-05}, {"id": 112, "seek": 55684, "start": 572.8000000000001, "end": 580.9200000000001, "text": " when you may have a single user interaction as a page view, it may require multiple sequential", "tokens": [562, 291, 815, 362, 257, 2167, 4195, 9285, 382, 257, 3028, 1910, 11, 309, 815, 3651, 3866, 42881], "temperature": 0.0, "avg_logprob": -0.171914659697434, "compression_ratio": 1.6697247706422018, "no_speech_prob": 3.073725019930862e-05}, {"id": 113, "seek": 55684, "start": 580.9200000000001, "end": 586.4000000000001, "text": " queries, right, or even maybe some queries run in parallel, right, which all need to", "tokens": [24109, 11, 558, 11, 420, 754, 1310, 512, 24109, 1190, 294, 8952, 11, 558, 11, 597, 439, 643, 281], "temperature": 0.0, "avg_logprob": -0.171914659697434, "compression_ratio": 1.6697247706422018, "no_speech_prob": 3.073725019930862e-05}, {"id": 114, "seek": 58640, "start": 586.4, "end": 592.16, "text": " be fast in order for user to get the outcome they're looking for, right, and then typically", "tokens": [312, 2370, 294, 1668, 337, 4195, 281, 483, 264, 9700, 436, 434, 1237, 337, 11, 558, 11, 293, 550, 5850], "temperature": 0.0, "avg_logprob": -0.15143539287425853, "compression_ratio": 1.6651162790697673, "no_speech_prob": 3.6341578379506245e-05}, {"id": 115, "seek": 58640, "start": 592.16, "end": 600.16, "text": " user through his session will have a multiple of those page views, right, so that 99 percentile", "tokens": [4195, 807, 702, 5481, 486, 362, 257, 3866, 295, 729, 3028, 6809, 11, 558, 11, 370, 300, 11803, 3043, 794], "temperature": 0.0, "avg_logprob": -0.15143539287425853, "compression_ratio": 1.6651162790697673, "no_speech_prob": 3.6341578379506245e-05}, {"id": 116, "seek": 58640, "start": 600.16, "end": 607.68, "text": " being excellent may only translate to half the users having that kind of outstanding", "tokens": [885, 7103, 815, 787, 13799, 281, 1922, 264, 5022, 1419, 300, 733, 295, 14485], "temperature": 0.0, "avg_logprob": -0.15143539287425853, "compression_ratio": 1.6651162790697673, "no_speech_prob": 3.6341578379506245e-05}, {"id": 117, "seek": 58640, "start": 607.68, "end": 615.0, "text": " experience through all the session, right, that is why if you look at companies which", "tokens": [1752, 807, 439, 264, 5481, 11, 558, 11, 300, 307, 983, 498, 291, 574, 412, 3431, 597], "temperature": 0.0, "avg_logprob": -0.15143539287425853, "compression_ratio": 1.6651162790697673, "no_speech_prob": 3.6341578379506245e-05}, {"id": 118, "seek": 61500, "start": 615.0, "end": 624.64, "text": " have a large number of users, they would either have some very high percentiles, like 99.9", "tokens": [362, 257, 2416, 1230, 295, 5022, 11, 436, 576, 2139, 362, 512, 588, 1090, 3043, 4680, 11, 411, 11803, 13, 24], "temperature": 0.0, "avg_logprob": -0.1971888405936105, "compression_ratio": 1.560693641618497, "no_speech_prob": 4.6644210669910535e-05}, {"id": 119, "seek": 61500, "start": 624.64, "end": 633.12, "text": " percentile response time as a goal, right, or would have those tolerances, you know,", "tokens": [3043, 794, 4134, 565, 382, 257, 3387, 11, 558, 11, 420, 576, 362, 729, 11125, 2676, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1971888405936105, "compression_ratio": 1.560693641618497, "no_speech_prob": 4.6644210669910535e-05}, {"id": 120, "seek": 61500, "start": 633.12, "end": 641.36, "text": " rather high, right, so there is a, well, additional sort of accommodation for if there's going", "tokens": [2831, 1090, 11, 558, 11, 370, 456, 307, 257, 11, 731, 11, 4497, 1333, 295, 27363, 337, 498, 456, 311, 516], "temperature": 0.0, "avg_logprob": -0.1971888405936105, "compression_ratio": 1.560693641618497, "no_speech_prob": 4.6644210669910535e-05}, {"id": 121, "seek": 64136, "start": 641.36, "end": 648.5600000000001, "text": " to be many, many queries, and I think to consider when you measure query performance is how", "tokens": [281, 312, 867, 11, 867, 24109, 11, 293, 286, 519, 281, 1949, 562, 291, 3481, 14581, 3389, 307, 577], "temperature": 0.0, "avg_logprob": -0.208439746534968, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.642112980945967e-05}, {"id": 122, "seek": 64136, "start": 648.5600000000001, "end": 653.96, "text": " you relate to errors, right, in certain cases I've seen people saying, well, you know,", "tokens": [291, 10961, 281, 13603, 11, 558, 11, 294, 1629, 3331, 286, 600, 1612, 561, 1566, 11, 731, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.208439746534968, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.642112980945967e-05}, {"id": 123, "seek": 64136, "start": 653.96, "end": 659.5600000000001, "text": " we only go into either measure response time for only successful queries, or we're going", "tokens": [321, 787, 352, 666, 2139, 3481, 4134, 565, 337, 787, 4406, 24109, 11, 420, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.208439746534968, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.642112980945967e-05}, {"id": 124, "seek": 64136, "start": 659.5600000000001, "end": 665.6, "text": " to put successful queries and queries which are completed with errors in the same bucket,", "tokens": [281, 829, 4406, 24109, 293, 24109, 597, 366, 7365, 365, 13603, 294, 264, 912, 13058, 11], "temperature": 0.0, "avg_logprob": -0.208439746534968, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.642112980945967e-05}, {"id": 125, "seek": 66560, "start": 665.6, "end": 672.88, "text": " right, which really can really, you know, change a picture for you a lot.", "tokens": [558, 11, 597, 534, 393, 534, 11, 291, 458, 11, 1319, 257, 3036, 337, 291, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1537759780883789, "compression_ratio": 1.6713615023474178, "no_speech_prob": 2.615260200400371e-05}, {"id": 126, "seek": 66560, "start": 672.88, "end": 673.88, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.1537759780883789, "compression_ratio": 1.6713615023474178, "no_speech_prob": 2.615260200400371e-05}, {"id": 127, "seek": 66560, "start": 673.88, "end": 680.72, "text": " Well, because actually if you think about the errors, they can be both fast errors and", "tokens": [1042, 11, 570, 767, 498, 291, 519, 466, 264, 13603, 11, 436, 393, 312, 1293, 2370, 13603, 293], "temperature": 0.0, "avg_logprob": -0.1537759780883789, "compression_ratio": 1.6713615023474178, "no_speech_prob": 2.615260200400371e-05}, {"id": 128, "seek": 66560, "start": 680.72, "end": 687.12, "text": " slow errors, right, imagine, for example, table was dropped for some reason, well, then", "tokens": [2964, 13603, 11, 558, 11, 3811, 11, 337, 1365, 11, 3199, 390, 8119, 337, 512, 1778, 11, 731, 11, 550], "temperature": 0.0, "avg_logprob": -0.1537759780883789, "compression_ratio": 1.6713615023474178, "no_speech_prob": 2.615260200400371e-05}, {"id": 129, "seek": 66560, "start": 687.12, "end": 691.2, "text": " all the queries hitting that table will return the error and vary very quickly, right, because", "tokens": [439, 264, 24109, 8850, 300, 3199, 486, 2736, 264, 6713, 293, 10559, 588, 2661, 11, 558, 11, 570], "temperature": 0.0, "avg_logprob": -0.1537759780883789, "compression_ratio": 1.6713615023474178, "no_speech_prob": 2.615260200400371e-05}, {"id": 130, "seek": 69120, "start": 691.2, "end": 696.5600000000001, "text": " well, there's nothing they can do, on the other hand, if there is something, let's say", "tokens": [731, 11, 456, 311, 1825, 436, 393, 360, 11, 322, 264, 661, 1011, 11, 498, 456, 307, 746, 11, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.13231188751930414, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00011901265679625794}, {"id": 131, "seek": 69120, "start": 696.5600000000001, "end": 704.4000000000001, "text": " some data is locked, right, and some timeouts happen, that may take quite a while before", "tokens": [512, 1412, 307, 9376, 11, 558, 11, 293, 512, 565, 7711, 1051, 11, 300, 815, 747, 1596, 257, 1339, 949], "temperature": 0.0, "avg_logprob": -0.13231188751930414, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00011901265679625794}, {"id": 132, "seek": 69120, "start": 704.4000000000001, "end": 711.6400000000001, "text": " error is returned, right, and you better not to mix those with the rest of your successful", "tokens": [6713, 307, 8752, 11, 558, 11, 293, 291, 1101, 406, 281, 2890, 729, 365, 264, 1472, 295, 428, 4406], "temperature": 0.0, "avg_logprob": -0.13231188751930414, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00011901265679625794}, {"id": 133, "seek": 69120, "start": 711.6400000000001, "end": 719.12, "text": " queries but to be able to, you know, look at that separately.", "tokens": [24109, 457, 281, 312, 1075, 281, 11, 291, 458, 11, 574, 412, 300, 14759, 13], "temperature": 0.0, "avg_logprob": -0.13231188751930414, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00011901265679625794}, {"id": 134, "seek": 71912, "start": 719.12, "end": 726.2, "text": " You also want to look at the query performance not just as an overall number but how it changes", "tokens": [509, 611, 528, 281, 574, 412, 264, 14581, 3389, 406, 445, 382, 364, 4787, 1230, 457, 577, 309, 2962], "temperature": 0.0, "avg_logprob": -0.1586290941400043, "compression_ratio": 1.4770114942528736, "no_speech_prob": 4.211953637422994e-05}, {"id": 135, "seek": 71912, "start": 726.2, "end": 730.84, "text": " over response time with a reasonably high resolution.", "tokens": [670, 4134, 565, 365, 257, 23551, 1090, 8669, 13], "temperature": 0.0, "avg_logprob": -0.1586290941400043, "compression_ratio": 1.4770114942528736, "no_speech_prob": 4.211953637422994e-05}, {"id": 136, "seek": 71912, "start": 730.84, "end": 733.0, "text": " Why is that important?", "tokens": [1545, 307, 300, 1021, 30], "temperature": 0.0, "avg_logprob": -0.1586290941400043, "compression_ratio": 1.4770114942528736, "no_speech_prob": 4.211953637422994e-05}, {"id": 137, "seek": 71912, "start": 733.0, "end": 740.48, "text": " One thing is what in many cases you would see query performance kind of slowly drops", "tokens": [1485, 551, 307, 437, 294, 867, 3331, 291, 576, 536, 14581, 3389, 733, 295, 5692, 11438], "temperature": 0.0, "avg_logprob": -0.1586290941400043, "compression_ratio": 1.4770114942528736, "no_speech_prob": 4.211953637422994e-05}, {"id": 138, "seek": 74048, "start": 740.48, "end": 749.44, "text": " before it goes so bad what that seems like downtime or really, you know, really incident", "tokens": [949, 309, 1709, 370, 1578, 437, 300, 2544, 411, 49648, 420, 534, 11, 291, 458, 11, 534, 9348], "temperature": 0.0, "avg_logprob": -0.18560785717434353, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.226962871849537e-05}, {"id": 139, "seek": 74048, "start": 749.44, "end": 754.24, "text": " for all kind of reasons, right, maybe you have some application which has a bad query,", "tokens": [337, 439, 733, 295, 4112, 11, 558, 11, 1310, 291, 362, 512, 3861, 597, 575, 257, 1578, 14581, 11], "temperature": 0.0, "avg_logprob": -0.18560785717434353, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.226962871849537e-05}, {"id": 140, "seek": 74048, "start": 754.24, "end": 759.48, "text": " right, and then you had the one instance of that query running, two, three, four, five,", "tokens": [558, 11, 293, 550, 291, 632, 264, 472, 5197, 295, 300, 14581, 2614, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11], "temperature": 0.0, "avg_logprob": -0.18560785717434353, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.226962871849537e-05}, {"id": 141, "seek": 74048, "start": 759.48, "end": 764.36, "text": " now you have a hundred instances of that bad query running, right, so saturating all the", "tokens": [586, 291, 362, 257, 3262, 14519, 295, 300, 1578, 14581, 2614, 11, 558, 11, 370, 21160, 990, 439, 264], "temperature": 0.0, "avg_logprob": -0.18560785717434353, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.226962871849537e-05}, {"id": 142, "seek": 74048, "start": 764.36, "end": 769.96, "text": " system resources, guess what, all the other query performance, right, is going down.", "tokens": [1185, 3593, 11, 2041, 437, 11, 439, 264, 661, 14581, 3389, 11, 558, 11, 307, 516, 760, 13], "temperature": 0.0, "avg_logprob": -0.18560785717434353, "compression_ratio": 1.8438818565400843, "no_speech_prob": 3.226962871849537e-05}, {"id": 143, "seek": 76996, "start": 769.96, "end": 776.4000000000001, "text": " If you are able to, if you are going to notice that what some queries are out of bounds,", "tokens": [759, 291, 366, 1075, 281, 11, 498, 291, 366, 516, 281, 3449, 300, 437, 512, 24109, 366, 484, 295, 29905, 11], "temperature": 0.0, "avg_logprob": -0.2576003121857596, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.217496699420735e-05}, {"id": 144, "seek": 76996, "start": 776.4000000000001, "end": 783.08, "text": " right, and maybe alert on it or something, you are able to take an action before the", "tokens": [558, 11, 293, 1310, 9615, 322, 309, 420, 746, 11, 291, 366, 1075, 281, 747, 364, 3069, 949, 264], "temperature": 0.0, "avg_logprob": -0.2576003121857596, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.217496699420735e-05}, {"id": 145, "seek": 76996, "start": 783.08, "end": 788.6, "text": " small problem becomes, basically because of downtime.", "tokens": [1359, 1154, 3643, 11, 1936, 570, 295, 49648, 13], "temperature": 0.0, "avg_logprob": -0.2576003121857596, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.217496699420735e-05}, {"id": 146, "seek": 76996, "start": 788.6, "end": 794.12, "text": " The other reason, of course, there is shit that is always going on, right, there is something", "tokens": [440, 661, 1778, 11, 295, 1164, 11, 456, 307, 4611, 300, 307, 1009, 516, 322, 11, 558, 11, 456, 307, 746], "temperature": 0.0, "avg_logprob": -0.2576003121857596, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.217496699420735e-05}, {"id": 147, "seek": 76996, "start": 794.12, "end": 798.2800000000001, "text": " that database doesn't have a background, if you have a cloud there is so sort of other", "tokens": [300, 8149, 1177, 380, 362, 257, 3678, 11, 498, 291, 362, 257, 4588, 456, 307, 370, 1333, 295, 661], "temperature": 0.0, "avg_logprob": -0.2576003121857596, "compression_ratio": 1.7662337662337662, "no_speech_prob": 9.217496699420735e-05}, {"id": 148, "seek": 79828, "start": 798.28, "end": 803.9599999999999, "text": " things happening which you may not even know anything about it.", "tokens": [721, 2737, 597, 291, 815, 406, 754, 458, 1340, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.22340062585207496, "compression_ratio": 1.7325102880658436, "no_speech_prob": 8.199595322366804e-05}, {"id": 149, "seek": 79828, "start": 803.9599999999999, "end": 810.12, "text": " Like for example, block, elastic block storage, right, similar stuff, right, well, guess what,", "tokens": [1743, 337, 1365, 11, 3461, 11, 17115, 3461, 6725, 11, 558, 11, 2531, 1507, 11, 558, 11, 731, 11, 2041, 437, 11], "temperature": 0.0, "avg_logprob": -0.22340062585207496, "compression_ratio": 1.7325102880658436, "no_speech_prob": 8.199595322366804e-05}, {"id": 150, "seek": 79828, "start": 810.12, "end": 815.48, "text": " it doesn't always have uniform performance, you know, sometimes something is happening", "tokens": [309, 1177, 380, 1009, 362, 9452, 3389, 11, 291, 458, 11, 2171, 746, 307, 2737], "temperature": 0.0, "avg_logprob": -0.22340062585207496, "compression_ratio": 1.7325102880658436, "no_speech_prob": 8.199595322366804e-05}, {"id": 151, "seek": 79828, "start": 815.48, "end": 820.76, "text": " at like Amazon back end but you know what, you don't really know anything about that.", "tokens": [412, 411, 6795, 646, 917, 457, 291, 458, 437, 11, 291, 500, 380, 534, 458, 1340, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.22340062585207496, "compression_ratio": 1.7325102880658436, "no_speech_prob": 8.199595322366804e-05}, {"id": 152, "seek": 79828, "start": 820.76, "end": 825.52, "text": " They don't tell you each time they have to replace a hard drive, right, somewhere, right,", "tokens": [814, 500, 380, 980, 291, 1184, 565, 436, 362, 281, 7406, 257, 1152, 3332, 11, 558, 11, 4079, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.22340062585207496, "compression_ratio": 1.7325102880658436, "no_speech_prob": 8.199595322366804e-05}, {"id": 153, "seek": 82552, "start": 825.52, "end": 830.88, "text": " or, you know, rebalance the load for some reason, right, but those things they can pop", "tokens": [420, 11, 291, 458, 11, 319, 29215, 264, 3677, 337, 512, 1778, 11, 558, 11, 457, 729, 721, 436, 393, 1665], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 154, "seek": 82552, "start": 830.88, "end": 831.88, "text": " up.", "tokens": [493, 13], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 155, "seek": 82552, "start": 831.88, "end": 836.48, "text": " Often you may see something like, oh, I have that like a spike in a query response time", "tokens": [20043, 291, 815, 536, 746, 411, 11, 1954, 11, 286, 362, 300, 411, 257, 21053, 294, 257, 14581, 4134, 565], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 156, "seek": 82552, "start": 836.48, "end": 842.24, "text": " which I can see for all queries on my, all instances, and wow, that's very likely like", "tokens": [597, 286, 393, 536, 337, 439, 24109, 322, 452, 11, 439, 14519, 11, 293, 6076, 11, 300, 311, 588, 3700, 411], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 157, "seek": 82552, "start": 842.24, "end": 849.12, "text": " something is environmental.", "tokens": [746, 307, 8303, 13], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 158, "seek": 82552, "start": 849.12, "end": 854.8, "text": " Now when you look at the query instrumentation, one of the questions I see people asking is", "tokens": [823, 562, 291, 574, 412, 264, 14581, 7198, 399, 11, 472, 295, 264, 1651, 286, 536, 561, 3365, 307], "temperature": 0.0, "avg_logprob": -0.21791040779340384, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00011740187619579956}, {"id": 159, "seek": 85480, "start": 854.8, "end": 860.5999999999999, "text": " where do you want to instrument the query, right, and we can instrument the query on", "tokens": [689, 360, 291, 528, 281, 7198, 264, 14581, 11, 558, 11, 293, 321, 393, 7198, 264, 14581, 322], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 160, "seek": 85480, "start": 860.5999999999999, "end": 865.5999999999999, "text": " the application data point, right, an application issues that query, right, and we often have", "tokens": [264, 3861, 1412, 935, 11, 558, 11, 364, 3861, 2663, 300, 14581, 11, 558, 11, 293, 321, 2049, 362], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 161, "seek": 85480, "start": 865.5999999999999, "end": 870.52, "text": " some, you know, tools like, you know, new relic insights which are doing, you know, just", "tokens": [512, 11, 291, 458, 11, 3873, 411, 11, 291, 458, 11, 777, 1039, 299, 14310, 597, 366, 884, 11, 291, 458, 11, 445], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 162, "seek": 85480, "start": 870.52, "end": 871.52, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 163, "seek": 85480, "start": 871.52, "end": 877.04, "text": " And hey, that query took this amount of response time and this is very good data because it", "tokens": [400, 4177, 11, 300, 14581, 1890, 341, 2372, 295, 4134, 565, 293, 341, 307, 588, 665, 1412, 570, 309], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 164, "seek": 85480, "start": 877.04, "end": 882.3599999999999, "text": " actually includes real response time as application observed it.", "tokens": [767, 5974, 957, 4134, 565, 382, 3861, 13095, 309, 13], "temperature": 0.0, "avg_logprob": -0.23801736377534413, "compression_ratio": 1.9545454545454546, "no_speech_prob": 5.453786070575006e-05}, {"id": 165, "seek": 88236, "start": 882.36, "end": 889.08, "text": " If there was some, let's say, network delay, right, or for whatever reason, that is included", "tokens": [759, 456, 390, 512, 11, 718, 311, 584, 11, 3209, 8577, 11, 558, 11, 420, 337, 2035, 1778, 11, 300, 307, 5556], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 166, "seek": 88236, "start": 889.08, "end": 895.0, "text": " in that response time where if you just measure from the time database received the query", "tokens": [294, 300, 4134, 565, 689, 498, 291, 445, 3481, 490, 264, 565, 8149, 4613, 264, 14581], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 167, "seek": 88236, "start": 895.0, "end": 900.08, "text": " since it pushed the result in the network, right, that is not included.", "tokens": [1670, 309, 9152, 264, 1874, 294, 264, 3209, 11, 558, 11, 300, 307, 406, 5556, 13], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 168, "seek": 88236, "start": 900.08, "end": 904.44, "text": " But measuring on a database gives you a lot of other valuable stuff like you can get a", "tokens": [583, 13389, 322, 257, 8149, 2709, 291, 257, 688, 295, 661, 8263, 1507, 411, 291, 393, 483, 257], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 169, "seek": 88236, "start": 904.44, "end": 909.5600000000001, "text": " lot more insight about what has been going on on the database size right while the query", "tokens": [688, 544, 11269, 466, 437, 575, 668, 516, 322, 322, 264, 8149, 2744, 558, 1339, 264, 14581], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 170, "seek": 88236, "start": 909.5600000000001, "end": 910.96, "text": " was executed.", "tokens": [390, 17577, 13], "temperature": 0.0, "avg_logprob": -0.20118790752482865, "compression_ratio": 1.783132530120482, "no_speech_prob": 7.682570139877498e-05}, {"id": 171, "seek": 91096, "start": 910.96, "end": 916.0400000000001, "text": " Most typically when you get the query result and you get response time, maybe, you know,", "tokens": [4534, 5850, 562, 291, 483, 264, 14581, 1874, 293, 291, 483, 4134, 565, 11, 1310, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17890450560930862, "compression_ratio": 1.7041666666666666, "no_speech_prob": 3.7768706533825025e-05}, {"id": 172, "seek": 91096, "start": 916.0400000000001, "end": 920.48, "text": " some other little additional information like, oh, this query returns so many rows, so many", "tokens": [512, 661, 707, 4497, 1589, 411, 11, 1954, 11, 341, 14581, 11247, 370, 867, 13241, 11, 370, 867], "temperature": 0.0, "avg_logprob": -0.17890450560930862, "compression_ratio": 1.7041666666666666, "no_speech_prob": 3.7768706533825025e-05}, {"id": 173, "seek": 91096, "start": 920.48, "end": 926.5600000000001, "text": " bytes, right, but not specifically, you know, how much CPU it uses, right, and all the other", "tokens": [36088, 11, 558, 11, 457, 406, 4682, 11, 291, 458, 11, 577, 709, 13199, 309, 4960, 11, 558, 11, 293, 439, 264, 661], "temperature": 0.0, "avg_logprob": -0.17890450560930862, "compression_ratio": 1.7041666666666666, "no_speech_prob": 3.7768706533825025e-05}, {"id": 174, "seek": 91096, "start": 926.5600000000001, "end": 931.08, "text": " important things you may want to use, okay.", "tokens": [1021, 721, 291, 815, 528, 281, 764, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.17890450560930862, "compression_ratio": 1.7041666666666666, "no_speech_prob": 3.7768706533825025e-05}, {"id": 175, "seek": 91096, "start": 931.08, "end": 937.84, "text": " So let's go back to our definition of a response time from a business point of view, right,", "tokens": [407, 718, 311, 352, 646, 281, 527, 7123, 295, 257, 4134, 565, 490, 257, 1606, 935, 295, 1910, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.17890450560930862, "compression_ratio": 1.7041666666666666, "no_speech_prob": 3.7768706533825025e-05}, {"id": 176, "seek": 93784, "start": 937.84, "end": 942.6800000000001, "text": " and we can say, well, what we are looking for have our old users to have an outstanding", "tokens": [293, 321, 393, 584, 11, 731, 11, 437, 321, 366, 1237, 337, 362, 527, 1331, 5022, 281, 362, 364, 14485], "temperature": 0.0, "avg_logprob": -0.1913829762884911, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.0587493306957185e-05}, {"id": 177, "seek": 93784, "start": 942.6800000000001, "end": 946.84, "text": " experience of all of their applications, right, great.", "tokens": [1752, 295, 439, 295, 641, 5821, 11, 558, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.1913829762884911, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.0587493306957185e-05}, {"id": 178, "seek": 93784, "start": 946.84, "end": 953.84, "text": " Now how do we translate that to the database, right, and kind of maybe breed that gap without", "tokens": [823, 577, 360, 321, 13799, 300, 281, 264, 8149, 11, 558, 11, 293, 733, 295, 1310, 18971, 300, 7417, 1553], "temperature": 0.0, "avg_logprob": -0.1913829762884911, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.0587493306957185e-05}, {"id": 179, "seek": 93784, "start": 953.84, "end": 958.2, "text": " what the boss wants and what DBA is able to answer.", "tokens": [437, 264, 5741, 2738, 293, 437, 413, 9295, 307, 1075, 281, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1913829762884911, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.0587493306957185e-05}, {"id": 180, "seek": 93784, "start": 958.2, "end": 964.5600000000001, "text": " Now I think there is some great work in this regard done by Google which have been working", "tokens": [823, 286, 519, 456, 307, 512, 869, 589, 294, 341, 3843, 1096, 538, 3329, 597, 362, 668, 1364], "temperature": 0.0, "avg_logprob": -0.1913829762884911, "compression_ratio": 1.6127659574468085, "no_speech_prob": 1.0587493306957185e-05}, {"id": 181, "seek": 96456, "start": 964.56, "end": 970.9599999999999, "text": " on this L-square commenter project which allows to pass a lot of metadata, right, from your", "tokens": [322, 341, 441, 12, 33292, 543, 2871, 260, 1716, 597, 4045, 281, 1320, 257, 688, 295, 26603, 11, 558, 11, 490, 428], "temperature": 0.0, "avg_logprob": -0.20072945584072155, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.2747010007151403e-05}, {"id": 182, "seek": 96456, "start": 970.9599999999999, "end": 975.3599999999999, "text": " application all the way down to your query.", "tokens": [3861, 439, 264, 636, 760, 281, 428, 14581, 13], "temperature": 0.0, "avg_logprob": -0.20072945584072155, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.2747010007151403e-05}, {"id": 183, "seek": 96456, "start": 975.3599999999999, "end": 981.04, "text": " The cool thing they've done is also integrating that directly with some of the frameworks,", "tokens": [440, 1627, 551, 436, 600, 1096, 307, 611, 26889, 300, 3838, 365, 512, 295, 264, 29834, 11], "temperature": 0.0, "avg_logprob": -0.20072945584072155, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.2747010007151403e-05}, {"id": 184, "seek": 96456, "start": 981.04, "end": 985.5999999999999, "text": " right, so it's kind of, hey, you know, you need to do nothing, right, and you just get", "tokens": [558, 11, 370, 309, 311, 733, 295, 11, 4177, 11, 291, 458, 11, 291, 643, 281, 360, 1825, 11, 558, 11, 293, 291, 445, 483], "temperature": 0.0, "avg_logprob": -0.20072945584072155, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.2747010007151403e-05}, {"id": 185, "seek": 96456, "start": 985.5999999999999, "end": 990.56, "text": " that automatic information.", "tokens": [300, 12509, 1589, 13], "temperature": 0.0, "avg_logprob": -0.20072945584072155, "compression_ratio": 1.5934579439252337, "no_speech_prob": 2.2747010007151403e-05}, {"id": 186, "seek": 99056, "start": 990.56, "end": 1000.3599999999999, "text": " What could be valuable query metadata possibilities, right, if you ask me, well, here is a bunch,", "tokens": [708, 727, 312, 8263, 14581, 26603, 12178, 11, 558, 11, 498, 291, 1029, 385, 11, 731, 11, 510, 307, 257, 3840, 11], "temperature": 0.0, "avg_logprob": -0.22108548647397525, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.5531408027745783e-05}, {"id": 187, "seek": 99056, "start": 1000.3599999999999, "end": 1009.4399999999999, "text": " right, there is this actual user and tenant which we can do application or functionality,", "tokens": [558, 11, 456, 307, 341, 3539, 4195, 293, 31000, 597, 321, 393, 360, 3861, 420, 14980, 11], "temperature": 0.0, "avg_logprob": -0.22108548647397525, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.5531408027745783e-05}, {"id": 188, "seek": 99056, "start": 1009.4399999999999, "end": 1013.64, "text": " right, often single database is used by a lot of applications, right, and we want to", "tokens": [558, 11, 2049, 2167, 8149, 307, 1143, 538, 257, 688, 295, 5821, 11, 558, 11, 293, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.22108548647397525, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.5531408027745783e-05}, {"id": 189, "seek": 99056, "start": 1013.64, "end": 1016.8, "text": " know where the query comes from, right.", "tokens": [458, 689, 264, 14581, 1487, 490, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.22108548647397525, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.5531408027745783e-05}, {"id": 190, "seek": 101680, "start": 1016.8, "end": 1022.12, "text": " I see a lot of DBAs, especially from a large company, say, well, you know what, here is", "tokens": [286, 536, 257, 688, 295, 413, 9295, 82, 11, 2318, 490, 257, 2416, 2237, 11, 584, 11, 731, 11, 291, 458, 437, 11, 510, 307], "temperature": 0.0, "avg_logprob": -0.15360408359103733, "compression_ratio": 1.5225225225225225, "no_speech_prob": 5.7468128943583e-05}, {"id": 191, "seek": 101680, "start": 1022.12, "end": 1023.88, "text": " this nasty query came in.", "tokens": [341, 17923, 14581, 1361, 294, 13], "temperature": 0.0, "avg_logprob": -0.15360408359103733, "compression_ratio": 1.5225225225225225, "no_speech_prob": 5.7468128943583e-05}, {"id": 192, "seek": 101680, "start": 1023.88, "end": 1030.48, "text": " It was not here yesterday, but it's very hard to figure out who is responsible for introducing", "tokens": [467, 390, 406, 510, 5186, 11, 457, 309, 311, 588, 1152, 281, 2573, 484, 567, 307, 6250, 337, 15424], "temperature": 0.0, "avg_logprob": -0.15360408359103733, "compression_ratio": 1.5225225225225225, "no_speech_prob": 5.7468128943583e-05}, {"id": 193, "seek": 101680, "start": 1030.48, "end": 1036.04, "text": " that and how you can come and hit his head with something heavy, right.", "tokens": [300, 293, 577, 291, 393, 808, 293, 2045, 702, 1378, 365, 746, 4676, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.15360408359103733, "compression_ratio": 1.5225225225225225, "no_speech_prob": 5.7468128943583e-05}, {"id": 194, "seek": 101680, "start": 1036.04, "end": 1044.76, "text": " That's maybe hard, right, without proper instrumentation.", "tokens": [663, 311, 1310, 1152, 11, 558, 11, 1553, 2296, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.15360408359103733, "compression_ratio": 1.5225225225225225, "no_speech_prob": 5.7468128943583e-05}, {"id": 195, "seek": 104476, "start": 1044.76, "end": 1050.52, "text": " You also, as a primary breakdown, want to look at the query, and I mean by query in", "tokens": [509, 611, 11, 382, 257, 6194, 18188, 11, 528, 281, 574, 412, 264, 14581, 11, 293, 286, 914, 538, 14581, 294], "temperature": 0.0, "avg_logprob": -0.18757897904775675, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3535065338364802e-05}, {"id": 196, "seek": 104476, "start": 1050.52, "end": 1056.52, "text": " this case, query of all parameters, you know, normalized, because often you would see the", "tokens": [341, 1389, 11, 14581, 295, 439, 9834, 11, 291, 458, 11, 48704, 11, 570, 2049, 291, 576, 536, 264], "temperature": 0.0, "avg_logprob": -0.18757897904775675, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3535065338364802e-05}, {"id": 197, "seek": 104476, "start": 1056.52, "end": 1060.72, "text": " different queries responsible for different functions, and through that have a different", "tokens": [819, 24109, 6250, 337, 819, 6828, 11, 293, 807, 300, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.18757897904775675, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3535065338364802e-05}, {"id": 198, "seek": 104476, "start": 1060.72, "end": 1064.8799999999999, "text": " response time tolerances, right, let's say some very quick lookup queries, you often", "tokens": [4134, 565, 11125, 2676, 11, 558, 11, 718, 311, 584, 512, 588, 1702, 574, 1010, 24109, 11, 291, 2049], "temperature": 0.0, "avg_logprob": -0.18757897904775675, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3535065338364802e-05}, {"id": 199, "seek": 104476, "start": 1064.8799999999999, "end": 1070.92, "text": " want them to complete in a fraction of millisecond as acceptable stuff, while some of you search", "tokens": [528, 552, 281, 3566, 294, 257, 14135, 295, 27940, 18882, 382, 15513, 1507, 11, 1339, 512, 295, 291, 3164], "temperature": 0.0, "avg_logprob": -0.18757897904775675, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3535065338364802e-05}, {"id": 200, "seek": 107092, "start": 1070.92, "end": 1075.6000000000001, "text": " queries write to some reports, well, may take a few seconds, and that will be quite", "tokens": [24109, 2464, 281, 512, 7122, 11, 731, 11, 815, 747, 257, 1326, 3949, 11, 293, 300, 486, 312, 1596], "temperature": 0.0, "avg_logprob": -0.23675105215489178, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.982512887683697e-05}, {"id": 201, "seek": 107092, "start": 1075.6000000000001, "end": 1082.6000000000001, "text": " acceptable, and it's good not to mix those all together, right, in this case.", "tokens": [15513, 11, 293, 309, 311, 665, 406, 281, 2890, 729, 439, 1214, 11, 558, 11, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.23675105215489178, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.982512887683697e-05}, {"id": 202, "seek": 107092, "start": 1082.6000000000001, "end": 1088.76, "text": " In many cases, when you have the SAS applications, we would have a multiple user, so what often", "tokens": [682, 867, 3331, 11, 562, 291, 362, 264, 33441, 5821, 11, 321, 576, 362, 257, 3866, 4195, 11, 370, 437, 2049], "temperature": 0.0, "avg_logprob": -0.23675105215489178, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.982512887683697e-05}, {"id": 203, "seek": 107092, "start": 1088.76, "end": 1095.72, "text": " calls like multiple tenants, like one of the ways you split them is to have a different", "tokens": [5498, 411, 3866, 31216, 11, 411, 472, 295, 264, 2098, 291, 7472, 552, 307, 281, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.23675105215489178, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.982512887683697e-05}, {"id": 204, "seek": 109572, "start": 1095.72, "end": 1101.92, "text": " schemas or different databases for all of them, and that is also, I find, very helpful to", "tokens": [22627, 296, 420, 819, 22380, 337, 439, 295, 552, 11, 293, 300, 307, 611, 11, 286, 915, 11, 588, 4961, 281], "temperature": 0.0, "avg_logprob": -0.16309111605408372, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.00024374373606406152}, {"id": 205, "seek": 109572, "start": 1101.92, "end": 1108.68, "text": " being able to separate that, so you can see, oh, this query is not slow for everybody,", "tokens": [885, 1075, 281, 4994, 300, 11, 370, 291, 393, 536, 11, 1954, 11, 341, 14581, 307, 406, 2964, 337, 2201, 11], "temperature": 0.0, "avg_logprob": -0.16309111605408372, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.00024374373606406152}, {"id": 206, "seek": 109572, "start": 1108.68, "end": 1117.1200000000001, "text": " but when we drill down, we can see only that particular tenant is slow, and vice is slow,", "tokens": [457, 562, 321, 11392, 760, 11, 321, 393, 536, 787, 300, 1729, 31000, 307, 2964, 11, 293, 11964, 307, 2964, 11], "temperature": 0.0, "avg_logprob": -0.16309111605408372, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.00024374373606406152}, {"id": 207, "seek": 109572, "start": 1117.1200000000001, "end": 1123.32, "text": " because unlike other, he has five million images in his album, right, if you would think about", "tokens": [570, 8343, 661, 11, 415, 575, 1732, 2459, 5267, 294, 702, 6030, 11, 558, 11, 498, 291, 576, 519, 466], "temperature": 0.0, "avg_logprob": -0.16309111605408372, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.00024374373606406152}, {"id": 208, "seek": 112332, "start": 1123.32, "end": 1133.96, "text": " some, you know, for the hosting application, so that's just an example.", "tokens": [512, 11, 291, 458, 11, 337, 264, 16058, 3861, 11, 370, 300, 311, 445, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.21903288042223132, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.9821588365593925e-05}, {"id": 209, "seek": 112332, "start": 1133.96, "end": 1140.04, "text": " Another thing what we find very helpful is being able to go for a query, right, or to", "tokens": [3996, 551, 437, 321, 915, 588, 4961, 307, 885, 1075, 281, 352, 337, 257, 14581, 11, 558, 11, 420, 281], "temperature": 0.0, "avg_logprob": -0.21903288042223132, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.9821588365593925e-05}, {"id": 210, "seek": 112332, "start": 1140.04, "end": 1147.84, "text": " look to understand what tables it touches and reverse to find out all the queries which", "tokens": [574, 281, 1223, 437, 8020, 309, 17431, 293, 9943, 281, 915, 484, 439, 264, 24109, 597], "temperature": 0.0, "avg_logprob": -0.21903288042223132, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.9821588365593925e-05}, {"id": 211, "seek": 112332, "start": 1147.84, "end": 1150.9199999999998, "text": " touches specific table.", "tokens": [17431, 2685, 3199, 13], "temperature": 0.0, "avg_logprob": -0.21903288042223132, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.9821588365593925e-05}, {"id": 212, "seek": 112332, "start": 1150.9199999999998, "end": 1152.8, "text": " Why is that helpful?", "tokens": [1545, 307, 300, 4961, 30], "temperature": 0.0, "avg_logprob": -0.21903288042223132, "compression_ratio": 1.5343915343915344, "no_speech_prob": 5.9821588365593925e-05}, {"id": 213, "seek": 115280, "start": 1152.8, "end": 1159.56, "text": " Well in many cases, our database operations are table specific, right, you may think,", "tokens": [1042, 294, 867, 3331, 11, 527, 8149, 7705, 366, 3199, 2685, 11, 558, 11, 291, 815, 519, 11], "temperature": 0.0, "avg_logprob": -0.14965560038884482, "compression_ratio": 1.6794258373205742, "no_speech_prob": 5.734690057579428e-05}, {"id": 214, "seek": 115280, "start": 1159.56, "end": 1167.28, "text": " hey, you know what, I'm dropping this index, as I don't need it, or maybe I add an index,", "tokens": [4177, 11, 291, 458, 437, 11, 286, 478, 13601, 341, 8186, 11, 382, 286, 500, 380, 643, 309, 11, 420, 1310, 286, 909, 364, 8186, 11], "temperature": 0.0, "avg_logprob": -0.14965560038884482, "compression_ratio": 1.6794258373205742, "no_speech_prob": 5.734690057579428e-05}, {"id": 215, "seek": 115280, "start": 1167.28, "end": 1172.1599999999999, "text": " I add a column, right, you do some sort of maybe kind of partition table, right, you", "tokens": [286, 909, 257, 7738, 11, 558, 11, 291, 360, 512, 1333, 295, 1310, 733, 295, 24808, 3199, 11, 558, 11, 291], "temperature": 0.0, "avg_logprob": -0.14965560038884482, "compression_ratio": 1.6794258373205742, "no_speech_prob": 5.734690057579428e-05}, {"id": 216, "seek": 115280, "start": 1172.1599999999999, "end": 1177.24, "text": " can do a lot of things with a table in scope, right, and then it would be very interesting", "tokens": [393, 360, 257, 688, 295, 721, 365, 257, 3199, 294, 11923, 11, 558, 11, 293, 550, 309, 576, 312, 588, 1880], "temperature": 0.0, "avg_logprob": -0.14965560038884482, "compression_ratio": 1.6794258373205742, "no_speech_prob": 5.734690057579428e-05}, {"id": 217, "seek": 117724, "start": 1177.24, "end": 1184.0, "text": " to understand how that particular, how all the queries which touch that table have been", "tokens": [281, 1223, 577, 300, 1729, 11, 577, 439, 264, 24109, 597, 2557, 300, 3199, 362, 668], "temperature": 0.0, "avg_logprob": -0.19580984115600586, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.0138664442347363e-05}, {"id": 218, "seek": 117724, "start": 1184.0, "end": 1189.16, "text": " affected, because we are much likely to be affected by that change compared to everybody", "tokens": [8028, 11, 570, 321, 366, 709, 3700, 281, 312, 8028, 538, 300, 1319, 5347, 281, 2201], "temperature": 0.0, "avg_logprob": -0.19580984115600586, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.0138664442347363e-05}, {"id": 219, "seek": 117724, "start": 1189.16, "end": 1195.0, "text": " else, right, I find that's a pretty, pretty cool feature.", "tokens": [1646, 11, 558, 11, 286, 915, 300, 311, 257, 1238, 11, 1238, 1627, 4111, 13], "temperature": 0.0, "avg_logprob": -0.19580984115600586, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.0138664442347363e-05}, {"id": 220, "seek": 117724, "start": 1195.0, "end": 1197.68, "text": " Database user is another one.", "tokens": [40461, 651, 4195, 307, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.19580984115600586, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.0138664442347363e-05}, {"id": 221, "seek": 117724, "start": 1197.68, "end": 1203.88, "text": " If you do not have something like a squirrel command to enable, you often do not really", "tokens": [759, 291, 360, 406, 362, 746, 411, 257, 28565, 5622, 281, 9528, 11, 291, 2049, 360, 406, 534], "temperature": 0.0, "avg_logprob": -0.19580984115600586, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.0138664442347363e-05}, {"id": 222, "seek": 120388, "start": 1203.88, "end": 1209.44, "text": " see very well from what application given query comes in.", "tokens": [536, 588, 731, 490, 437, 3861, 2212, 14581, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.18984477519989013, "compression_ratio": 1.7951219512195122, "no_speech_prob": 4.564582559396513e-05}, {"id": 223, "seek": 120388, "start": 1209.44, "end": 1213.2800000000002, "text": " But one of the practice you may follow at least is having a different application touching", "tokens": [583, 472, 295, 264, 3124, 291, 815, 1524, 412, 1935, 307, 1419, 257, 819, 3861, 11175], "temperature": 0.0, "avg_logprob": -0.18984477519989013, "compression_ratio": 1.7951219512195122, "no_speech_prob": 4.564582559396513e-05}, {"id": 224, "seek": 120388, "start": 1213.2800000000002, "end": 1220.16, "text": " the same database using different user names, right, different users with different privileges,", "tokens": [264, 912, 8149, 1228, 819, 4195, 5288, 11, 558, 11, 819, 5022, 365, 819, 32588, 11], "temperature": 0.0, "avg_logprob": -0.18984477519989013, "compression_ratio": 1.7951219512195122, "no_speech_prob": 4.564582559396513e-05}, {"id": 225, "seek": 120388, "start": 1220.16, "end": 1228.2, "text": " right, if nothing else that is a very good security practice, right, and that is where", "tokens": [558, 11, 498, 1825, 1646, 300, 307, 257, 588, 665, 3825, 3124, 11, 558, 11, 293, 300, 307, 689], "temperature": 0.0, "avg_logprob": -0.18984477519989013, "compression_ratio": 1.7951219512195122, "no_speech_prob": 4.564582559396513e-05}, {"id": 226, "seek": 120388, "start": 1228.2, "end": 1233.68, "text": " filtering and breakdown allows that.", "tokens": [30822, 293, 18188, 4045, 300, 13], "temperature": 0.0, "avg_logprob": -0.18984477519989013, "compression_ratio": 1.7951219512195122, "no_speech_prob": 4.564582559396513e-05}, {"id": 227, "seek": 123368, "start": 1233.68, "end": 1240.6000000000001, "text": " In a large, large, short environment, we also want to make sure we aggregate the data from", "tokens": [682, 257, 2416, 11, 2416, 11, 2099, 2823, 11, 321, 611, 528, 281, 652, 988, 321, 26118, 264, 1412, 490], "temperature": 0.0, "avg_logprob": -0.17481169493302054, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.5147974156425335e-05}, {"id": 228, "seek": 123368, "start": 1240.6000000000001, "end": 1245.3600000000001, "text": " a many database hosts, right, and can compare between each other.", "tokens": [257, 867, 8149, 21573, 11, 558, 11, 293, 393, 6794, 1296, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.17481169493302054, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.5147974156425335e-05}, {"id": 229, "seek": 123368, "start": 1245.3600000000001, "end": 1249.4, "text": " Typically when you have a short application, you want the load and hence response time", "tokens": [23129, 562, 291, 362, 257, 2099, 3861, 11, 291, 528, 264, 3677, 293, 16678, 4134, 565], "temperature": 0.0, "avg_logprob": -0.17481169493302054, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.5147974156425335e-05}, {"id": 230, "seek": 123368, "start": 1249.4, "end": 1253.24, "text": " between different database hosts to be kind of similar.", "tokens": [1296, 819, 8149, 21573, 281, 312, 733, 295, 2531, 13], "temperature": 0.0, "avg_logprob": -0.17481169493302054, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.5147974156425335e-05}, {"id": 231, "seek": 123368, "start": 1253.24, "end": 1258.76, "text": " But often it is not, right, it's often hard to achieve a perfect balance in between the", "tokens": [583, 2049, 309, 307, 406, 11, 558, 11, 309, 311, 2049, 1152, 281, 4584, 257, 2176, 4772, 294, 1296, 264], "temperature": 0.0, "avg_logprob": -0.17481169493302054, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.5147974156425335e-05}, {"id": 232, "seek": 125876, "start": 1258.76, "end": 1266.24, "text": " nodes as one cause of the differences, but also things may just, you know, happen, you", "tokens": [13891, 382, 472, 3082, 295, 264, 7300, 11, 457, 611, 721, 815, 445, 11, 291, 458, 11, 1051, 11, 291], "temperature": 0.0, "avg_logprob": -0.1931715863091605, "compression_ratio": 1.976958525345622, "no_speech_prob": 0.0001512385788373649}, {"id": 233, "seek": 125876, "start": 1266.24, "end": 1271.8, "text": " know, like you may have a settings which drift away on different nodes, you may have", "tokens": [458, 11, 411, 291, 815, 362, 257, 6257, 597, 19699, 1314, 322, 819, 13891, 11, 291, 815, 362], "temperature": 0.0, "avg_logprob": -0.1931715863091605, "compression_ratio": 1.976958525345622, "no_speech_prob": 0.0001512385788373649}, {"id": 234, "seek": 125876, "start": 1271.8, "end": 1278.32, "text": " some, you know, differences, right, in the performance, especially in the cloud, which,", "tokens": [512, 11, 291, 458, 11, 7300, 11, 558, 11, 294, 264, 3389, 11, 2318, 294, 264, 4588, 11, 597, 11], "temperature": 0.0, "avg_logprob": -0.1931715863091605, "compression_ratio": 1.976958525345622, "no_speech_prob": 0.0001512385788373649}, {"id": 235, "seek": 125876, "start": 1278.32, "end": 1284.08, "text": " you know, happen virtually from nowhere, right, I mean, I know a lot of people work", "tokens": [291, 458, 11, 1051, 14103, 490, 11159, 11, 558, 11, 286, 914, 11, 286, 458, 257, 688, 295, 561, 589], "temperature": 0.0, "avg_logprob": -0.1931715863091605, "compression_ratio": 1.976958525345622, "no_speech_prob": 0.0001512385788373649}, {"id": 236, "seek": 125876, "start": 1284.08, "end": 1287.44, "text": " in the cloud, you know, you know, sometimes you just get a lemon, right, or just like", "tokens": [294, 264, 4588, 11, 291, 458, 11, 291, 458, 11, 2171, 291, 445, 483, 257, 11356, 11, 558, 11, 420, 445, 411], "temperature": 0.0, "avg_logprob": -0.1931715863091605, "compression_ratio": 1.976958525345622, "no_speech_prob": 0.0001512385788373649}, {"id": 237, "seek": 128744, "start": 1287.44, "end": 1292.44, "text": " a bad node, which for some reason doesn't perform as well as its peers, right, and just", "tokens": [257, 1578, 9984, 11, 597, 337, 512, 1778, 1177, 380, 2042, 382, 731, 382, 1080, 16739, 11, 558, 11, 293, 445], "temperature": 0.0, "avg_logprob": -0.21386578618263713, "compression_ratio": 1.7412280701754386, "no_speech_prob": 3.444406684138812e-05}, {"id": 238, "seek": 128744, "start": 1292.44, "end": 1298.0800000000002, "text": " want to, you know, maybe toss it and get another one, better one, right, but to do that you", "tokens": [528, 281, 11, 291, 458, 11, 1310, 14432, 309, 293, 483, 1071, 472, 11, 1101, 472, 11, 558, 11, 457, 281, 360, 300, 291], "temperature": 0.0, "avg_logprob": -0.21386578618263713, "compression_ratio": 1.7412280701754386, "no_speech_prob": 3.444406684138812e-05}, {"id": 239, "seek": 128744, "start": 1298.0800000000002, "end": 1304.8, "text": " better understand what that is not performing particularly well.", "tokens": [1101, 1223, 437, 300, 307, 406, 10205, 4098, 731, 13], "temperature": 0.0, "avg_logprob": -0.21386578618263713, "compression_ratio": 1.7412280701754386, "no_speech_prob": 3.444406684138812e-05}, {"id": 240, "seek": 128744, "start": 1304.8, "end": 1309.6000000000001, "text": " And the same also applies to their application server or web server.", "tokens": [400, 264, 912, 611, 13165, 281, 641, 3861, 7154, 420, 3670, 7154, 13], "temperature": 0.0, "avg_logprob": -0.21386578618263713, "compression_ratio": 1.7412280701754386, "no_speech_prob": 3.444406684138812e-05}, {"id": 241, "seek": 128744, "start": 1309.6000000000001, "end": 1315.0, "text": " Again, like if you deploy application on, let's say, 100 application servers or web", "tokens": [3764, 11, 411, 498, 291, 7274, 3861, 322, 11, 718, 311, 584, 11, 2319, 3861, 15909, 420, 3670], "temperature": 0.0, "avg_logprob": -0.21386578618263713, "compression_ratio": 1.7412280701754386, "no_speech_prob": 3.444406684138812e-05}, {"id": 242, "seek": 131500, "start": 1315.0, "end": 1318.32, "text": " nodes, right, you may say, well, it's all should be the same.", "tokens": [13891, 11, 558, 11, 291, 815, 584, 11, 731, 11, 309, 311, 439, 820, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 243, "seek": 131500, "start": 1318.32, "end": 1322.28, "text": " I have my, you know, automation which takes care of that.", "tokens": [286, 362, 452, 11, 291, 458, 11, 17769, 597, 2516, 1127, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 244, "seek": 131500, "start": 1322.28, "end": 1329.36, "text": " But again, well, things are not always as they should be.", "tokens": [583, 797, 11, 731, 11, 721, 366, 406, 1009, 382, 436, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 245, "seek": 131500, "start": 1329.36, "end": 1333.56, "text": " In many cases, you have something which doesn't work out.", "tokens": [682, 867, 3331, 11, 291, 362, 746, 597, 1177, 380, 589, 484, 13], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 246, "seek": 131500, "start": 1333.56, "end": 1337.72, "text": " I have seen so many cases when people say, well, you know what, I already fixed that", "tokens": [286, 362, 1612, 370, 867, 3331, 562, 561, 584, 11, 731, 11, 291, 458, 437, 11, 286, 1217, 6806, 300], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 247, "seek": 131500, "start": 1337.72, "end": 1340.36, "text": " nasty query and I deployed the fix.", "tokens": [17923, 14581, 293, 286, 17826, 264, 3191, 13], "temperature": 0.0, "avg_logprob": -0.15389301753280185, "compression_ratio": 1.6635514018691588, "no_speech_prob": 9.401157876709476e-05}, {"id": 248, "seek": 134036, "start": 1340.36, "end": 1345.8, "text": " When you look at that, well, it's actually was not deployed all the instances for whatever", "tokens": [1133, 291, 574, 412, 300, 11, 731, 11, 309, 311, 767, 390, 406, 17826, 439, 264, 14519, 337, 2035], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 249, "seek": 134036, "start": 1345.8, "end": 1346.8, "text": " reason.", "tokens": [1778, 13], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 250, "seek": 134036, "start": 1346.8, "end": 1351.36, "text": " Or you may say, well, you know what, my, I'm using a caching to reduce the query load", "tokens": [1610, 291, 815, 584, 11, 731, 11, 291, 458, 437, 11, 452, 11, 286, 478, 1228, 257, 269, 2834, 281, 5407, 264, 14581, 3677], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 251, "seek": 134036, "start": 1351.36, "end": 1357.3999999999999, "text": " on the database, but that caching is misconfigured to otherwise inaccessible on some of their", "tokens": [322, 264, 8149, 11, 457, 300, 269, 2834, 307, 27631, 20646, 3831, 281, 5911, 33230, 780, 964, 322, 512, 295, 641], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 252, "seek": 134036, "start": 1357.3999999999999, "end": 1358.8, "text": " web nodes, right.", "tokens": [3670, 13891, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 253, "seek": 134036, "start": 1358.8, "end": 1361.56, "text": " A lot of stuff can happen.", "tokens": [316, 688, 295, 1507, 393, 1051, 13], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 254, "seek": 134036, "start": 1361.56, "end": 1367.04, "text": " Or maybe you're lucky and one of your web nodes was actually hacked and is also getting", "tokens": [1610, 1310, 291, 434, 6356, 293, 472, 295, 428, 3670, 13891, 390, 767, 36218, 293, 307, 611, 1242], "temperature": 0.0, "avg_logprob": -0.21078073978424072, "compression_ratio": 1.630952380952381, "no_speech_prob": 3.589177504181862e-05}, {"id": 255, "seek": 136704, "start": 1367.04, "end": 1374.76, "text": " some additional queries to, you know, download your data and send it to someone.", "tokens": [512, 4497, 24109, 281, 11, 291, 458, 11, 5484, 428, 1412, 293, 2845, 309, 281, 1580, 13], "temperature": 0.0, "avg_logprob": -0.20833973825713734, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.8998303858097643e-05}, {"id": 256, "seek": 136704, "start": 1374.76, "end": 1381.12, "text": " So I find making sure you can look at the query patterns separated by the different", "tokens": [407, 286, 915, 1455, 988, 291, 393, 574, 412, 264, 14581, 8294, 12005, 538, 264, 819], "temperature": 0.0, "avg_logprob": -0.20833973825713734, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.8998303858097643e-05}, {"id": 257, "seek": 136704, "start": 1381.12, "end": 1385.8, "text": " client hosts very, are something very valuable.", "tokens": [6423, 21573, 588, 11, 366, 746, 588, 8263, 13], "temperature": 0.0, "avg_logprob": -0.20833973825713734, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.8998303858097643e-05}, {"id": 258, "seek": 136704, "start": 1385.8, "end": 1392.84, "text": " I already mentioned with a SQL commenter which allows you to extend some additional", "tokens": [286, 1217, 2835, 365, 257, 19200, 2871, 260, 597, 4045, 291, 281, 10101, 512, 4497], "temperature": 0.0, "avg_logprob": -0.20833973825713734, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.8998303858097643e-05}, {"id": 259, "seek": 136704, "start": 1392.84, "end": 1396.6, "text": " metadata, which I think can be quite cool, right.", "tokens": [26603, 11, 597, 286, 519, 393, 312, 1596, 1627, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.20833973825713734, "compression_ratio": 1.5727272727272728, "no_speech_prob": 1.8998303858097643e-05}, {"id": 260, "seek": 139660, "start": 1396.6, "end": 1400.8, "text": " And you can find the usage for custom tags in many cases.", "tokens": [400, 291, 393, 915, 264, 14924, 337, 2375, 18632, 294, 867, 3331, 13], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 261, "seek": 139660, "start": 1400.8, "end": 1405.48, "text": " I've seen people, for example, tagging different instance types when I'm saying, well, you", "tokens": [286, 600, 1612, 561, 11, 337, 1365, 11, 6162, 3249, 819, 5197, 3467, 562, 286, 478, 1566, 11, 731, 11, 291], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 262, "seek": 139660, "start": 1405.48, "end": 1410.1999999999998, "text": " know what, this kind of new generation instance looks good.", "tokens": [458, 437, 11, 341, 733, 295, 777, 5125, 5197, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 263, "seek": 139660, "start": 1410.1999999999998, "end": 1413.7199999999998, "text": " So let me put some of them in production and being able to compare.", "tokens": [407, 718, 385, 829, 512, 295, 552, 294, 4265, 293, 885, 1075, 281, 6794, 13], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 264, "seek": 139660, "start": 1413.7199999999998, "end": 1415.6799999999998, "text": " Well, is it actually working better?", "tokens": [1042, 11, 307, 309, 767, 1364, 1101, 30], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 265, "seek": 139660, "start": 1415.6799999999998, "end": 1417.52, "text": " Sometimes yes.", "tokens": [4803, 2086, 13], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 266, "seek": 139660, "start": 1417.52, "end": 1421.1599999999999, "text": " Sometimes, you know, no.", "tokens": [4803, 11, 291, 458, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 267, "seek": 139660, "start": 1421.1599999999999, "end": 1426.52, "text": " The database version, right, maybe you're running out when you, minor Postgres release, you", "tokens": [440, 8149, 3037, 11, 558, 11, 1310, 291, 434, 2614, 484, 562, 291, 11, 6696, 10223, 45189, 4374, 11, 291], "temperature": 0.0, "avg_logprob": -0.2162377447144598, "compression_ratio": 1.6123188405797102, "no_speech_prob": 5.3112733439775184e-05}, {"id": 268, "seek": 142652, "start": 1426.52, "end": 1434.16, "text": " want to do it like on some subset of the nodes and to make sure there's no, no regressions,", "tokens": [528, 281, 360, 309, 411, 322, 512, 25993, 295, 264, 13891, 293, 281, 652, 988, 456, 311, 572, 11, 572, 1121, 735, 626, 11], "temperature": 0.0, "avg_logprob": -0.17029730121741135, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.395035936497152e-05}, {"id": 269, "seek": 142652, "start": 1434.16, "end": 1435.16, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.17029730121741135, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.395035936497152e-05}, {"id": 270, "seek": 142652, "start": 1435.16, "end": 1440.68, "text": " I mean, I think it's always good in this case to practice, you know, trust by verify, right,", "tokens": [286, 914, 11, 286, 519, 309, 311, 1009, 665, 294, 341, 1389, 281, 3124, 11, 291, 458, 11, 3361, 538, 16888, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.17029730121741135, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.395035936497152e-05}, {"id": 271, "seek": 142652, "start": 1440.68, "end": 1447.8, "text": " because sometimes you do run into unexpected changes, you know, you can validate the configuration", "tokens": [570, 2171, 291, 360, 1190, 666, 13106, 2962, 11, 291, 458, 11, 291, 393, 29562, 264, 11694], "temperature": 0.0, "avg_logprob": -0.17029730121741135, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.395035936497152e-05}, {"id": 272, "seek": 142652, "start": 1447.8, "end": 1453.56, "text": " changes this way and so on and so forth.", "tokens": [2962, 341, 636, 293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17029730121741135, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.395035936497152e-05}, {"id": 273, "seek": 145356, "start": 1453.56, "end": 1458.36, "text": " Query plan is another area which I think is quite, quite interesting.", "tokens": [2326, 2109, 1393, 307, 1071, 1859, 597, 286, 519, 307, 1596, 11, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 274, "seek": 145356, "start": 1458.36, "end": 1462.9199999999998, "text": " In many cases, you'll find the same query depending on the parameters, right, or some", "tokens": [682, 867, 3331, 11, 291, 603, 915, 264, 912, 14581, 5413, 322, 264, 9834, 11, 558, 11, 420, 512], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 275, "seek": 145356, "start": 1462.9199999999998, "end": 1466.2, "text": " other situations will have different plans.", "tokens": [661, 6851, 486, 362, 819, 5482, 13], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 276, "seek": 145356, "start": 1466.2, "end": 1472.32, "text": " And if that different plans may have different query performance, and it is a very helpful", "tokens": [400, 498, 300, 819, 5482, 815, 362, 819, 14581, 3389, 11, 293, 309, 307, 257, 588, 4961], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 277, "seek": 145356, "start": 1472.32, "end": 1479.6799999999998, "text": " if you can break down the performance by the different plans a query has, so you can understand", "tokens": [498, 291, 393, 1821, 760, 264, 3389, 538, 264, 819, 5482, 257, 14581, 575, 11, 370, 291, 393, 1223], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 278, "seek": 145356, "start": 1479.6799999999998, "end": 1482.2, "text": " if that is a plan issue or not, right.", "tokens": [498, 300, 307, 257, 1393, 2734, 420, 406, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.21503169863831764, "compression_ratio": 1.831896551724138, "no_speech_prob": 4.757326314575039e-05}, {"id": 279, "seek": 148220, "start": 1482.2, "end": 1485.0800000000002, "text": " Otherwise, you may be looking at the query and say, well, you know what, something is", "tokens": [10328, 11, 291, 815, 312, 1237, 412, 264, 14581, 293, 584, 11, 731, 11, 291, 458, 437, 11, 746, 307], "temperature": 0.0, "avg_logprob": -0.24648448399135045, "compression_ratio": 1.6964285714285714, "no_speech_prob": 6.072593532735482e-05}, {"id": 280, "seek": 148220, "start": 1485.0800000000002, "end": 1493.24, "text": " fast, something is slow, you know, why is that, not very clear, their plans give us", "tokens": [2370, 11, 746, 307, 2964, 11, 291, 458, 11, 983, 307, 300, 11, 406, 588, 1850, 11, 641, 5482, 976, 505], "temperature": 0.0, "avg_logprob": -0.24648448399135045, "compression_ratio": 1.6964285714285714, "no_speech_prob": 6.072593532735482e-05}, {"id": 281, "seek": 148220, "start": 1493.24, "end": 1497.3600000000001, "text": " a very good information.", "tokens": [257, 588, 665, 1589, 13], "temperature": 0.0, "avg_logprob": -0.24648448399135045, "compression_ratio": 1.6964285714285714, "no_speech_prob": 6.072593532735482e-05}, {"id": 282, "seek": 148220, "start": 1497.3600000000001, "end": 1507.52, "text": " Now when you find the query and see that as a problematic and you need to make it go fast,", "tokens": [823, 562, 291, 915, 264, 14581, 293, 536, 300, 382, 257, 19011, 293, 291, 643, 281, 652, 309, 352, 2370, 11], "temperature": 0.0, "avg_logprob": -0.24648448399135045, "compression_ratio": 1.6964285714285714, "no_speech_prob": 6.072593532735482e-05}, {"id": 283, "seek": 150752, "start": 1507.52, "end": 1514.4, "text": " in this case, it's very good to understand there is that response time developers care", "tokens": [294, 341, 1389, 11, 309, 311, 588, 665, 281, 1223, 456, 307, 300, 4134, 565, 8849, 1127], "temperature": 0.0, "avg_logprob": -0.1636501185099284, "compression_ratio": 1.4663461538461537, "no_speech_prob": 7.175741484388709e-05}, {"id": 284, "seek": 150752, "start": 1514.4, "end": 1519.32, "text": " so much about is coming from.", "tokens": [370, 709, 466, 307, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.1636501185099284, "compression_ratio": 1.4663461538461537, "no_speech_prob": 7.175741484388709e-05}, {"id": 285, "seek": 150752, "start": 1519.32, "end": 1524.72, "text": " And there are quite a few possibilities here.", "tokens": [400, 456, 366, 1596, 257, 1326, 12178, 510, 13], "temperature": 0.0, "avg_logprob": -0.1636501185099284, "compression_ratio": 1.4663461538461537, "no_speech_prob": 7.175741484388709e-05}, {"id": 286, "seek": 150752, "start": 1524.72, "end": 1529.8799999999999, "text": " Some of them are instrumented better than others.", "tokens": [2188, 295, 552, 366, 7198, 292, 1101, 813, 2357, 13], "temperature": 0.0, "avg_logprob": -0.1636501185099284, "compression_ratio": 1.4663461538461537, "no_speech_prob": 7.175741484388709e-05}, {"id": 287, "seek": 150752, "start": 1529.8799999999999, "end": 1535.8, "text": " For example, if you're looking at data crunch and disk IO, right, those are typically pretty", "tokens": [1171, 1365, 11, 498, 291, 434, 1237, 412, 1412, 13386, 293, 12355, 39839, 11, 558, 11, 729, 366, 5850, 1238], "temperature": 0.0, "avg_logprob": -0.1636501185099284, "compression_ratio": 1.4663461538461537, "no_speech_prob": 7.175741484388709e-05}, {"id": 288, "seek": 153580, "start": 1535.8, "end": 1543.52, "text": " well instrumented, you can find how much, you know, of CPU query consumes or that does.", "tokens": [731, 7198, 292, 11, 291, 393, 915, 577, 709, 11, 291, 458, 11, 295, 13199, 14581, 48823, 420, 300, 775, 13], "temperature": 0.0, "avg_logprob": -0.2622076498495566, "compression_ratio": 1.521505376344086, "no_speech_prob": 8.608496864326298e-05}, {"id": 289, "seek": 153580, "start": 1543.52, "end": 1548.84, "text": " In terms of contention, that is typically more problematic, right, to say, hey, you", "tokens": [682, 2115, 295, 660, 1251, 11, 300, 307, 5850, 544, 19011, 11, 558, 11, 281, 584, 11, 4177, 11, 291], "temperature": 0.0, "avg_logprob": -0.2622076498495566, "compression_ratio": 1.521505376344086, "no_speech_prob": 8.608496864326298e-05}, {"id": 290, "seek": 153580, "start": 1548.84, "end": 1556.04, "text": " know, what exactly those kind of internal synchronization object query had to wait,", "tokens": [458, 11, 437, 2293, 729, 733, 295, 6920, 19331, 2144, 2657, 14581, 632, 281, 1699, 11], "temperature": 0.0, "avg_logprob": -0.2622076498495566, "compression_ratio": 1.521505376344086, "no_speech_prob": 8.608496864326298e-05}, {"id": 291, "seek": 153580, "start": 1556.04, "end": 1560.04, "text": " right, that is more tricky.", "tokens": [558, 11, 300, 307, 544, 12414, 13], "temperature": 0.0, "avg_logprob": -0.2622076498495566, "compression_ratio": 1.521505376344086, "no_speech_prob": 8.608496864326298e-05}, {"id": 292, "seek": 156004, "start": 1560.04, "end": 1568.08, "text": " You know, waits on CPU availability is even more tricky, right.", "tokens": [509, 458, 11, 40597, 322, 13199, 17945, 307, 754, 544, 12414, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.1978614628314972, "compression_ratio": 1.4596273291925466, "no_speech_prob": 0.00015303351392503828}, {"id": 293, "seek": 156004, "start": 1568.08, "end": 1575.0, "text": " And what I mean by this is this, right, so if you have a system which has much more", "tokens": [400, 437, 286, 914, 538, 341, 307, 341, 11, 558, 11, 370, 498, 291, 362, 257, 1185, 597, 575, 709, 544], "temperature": 0.0, "avg_logprob": -0.1978614628314972, "compression_ratio": 1.4596273291925466, "no_speech_prob": 0.00015303351392503828}, {"id": 294, "seek": 156004, "start": 1575.0, "end": 1581.28, "text": " runnable threads, runnable processes, right, than available CPU, then they will spend a", "tokens": [1190, 77, 712, 19314, 11, 1190, 77, 712, 7555, 11, 558, 11, 813, 2435, 13199, 11, 550, 436, 486, 3496, 257], "temperature": 0.0, "avg_logprob": -0.1978614628314972, "compression_ratio": 1.4596273291925466, "no_speech_prob": 0.00015303351392503828}, {"id": 295, "seek": 158128, "start": 1581.28, "end": 1591.08, "text": " lot of time waiting for available CPU, right, and that is very hard to see on its impact", "tokens": [688, 295, 565, 3806, 337, 2435, 13199, 11, 558, 11, 293, 300, 307, 588, 1152, 281, 536, 322, 1080, 2712], "temperature": 0.0, "avg_logprob": -0.18862512635021675, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00012030268408125266}, {"id": 296, "seek": 158128, "start": 1591.08, "end": 1592.68, "text": " to the query response time.", "tokens": [281, 264, 14581, 4134, 565, 13], "temperature": 0.0, "avg_logprob": -0.18862512635021675, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00012030268408125266}, {"id": 297, "seek": 158128, "start": 1592.68, "end": 1599.12, "text": " You typically can see that from the general node stats, like, hey, my CPU is back, I have", "tokens": [509, 5850, 393, 536, 300, 490, 264, 2674, 9984, 18152, 11, 411, 11, 4177, 11, 452, 13199, 307, 646, 11, 286, 362], "temperature": 0.0, "avg_logprob": -0.18862512635021675, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00012030268408125266}, {"id": 298, "seek": 158128, "start": 1599.12, "end": 1610.36, "text": " like a ton of runnable CPU, right, CPU is also in recent kernels, you can see the information", "tokens": [411, 257, 2952, 295, 1190, 77, 712, 13199, 11, 558, 11, 13199, 307, 611, 294, 5162, 23434, 1625, 11, 291, 393, 536, 264, 1589], "temperature": 0.0, "avg_logprob": -0.18862512635021675, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00012030268408125266}, {"id": 299, "seek": 161036, "start": 1610.36, "end": 1617.84, "text": " about their run queue latency, which is very cool, right, that tells you how long the processes", "tokens": [466, 641, 1190, 18639, 27043, 11, 597, 307, 588, 1627, 11, 558, 11, 300, 5112, 291, 577, 938, 264, 7555], "temperature": 0.0, "avg_logprob": -0.15112362485943417, "compression_ratio": 1.461111111111111, "no_speech_prob": 7.294423994608223e-05}, {"id": 300, "seek": 161036, "start": 1617.84, "end": 1626.84, "text": " had to wait to be scheduled on CPU after they are ready to start running.", "tokens": [632, 281, 1699, 281, 312, 15678, 322, 13199, 934, 436, 366, 1919, 281, 722, 2614, 13], "temperature": 0.0, "avg_logprob": -0.15112362485943417, "compression_ratio": 1.461111111111111, "no_speech_prob": 7.294423994608223e-05}, {"id": 301, "seek": 161036, "start": 1626.84, "end": 1634.9599999999998, "text": " So a whole bunch of stuff here, some of them are easy, some of their work is still remaining.", "tokens": [407, 257, 1379, 3840, 295, 1507, 510, 11, 512, 295, 552, 366, 1858, 11, 512, 295, 641, 589, 307, 920, 8877, 13], "temperature": 0.0, "avg_logprob": -0.15112362485943417, "compression_ratio": 1.461111111111111, "no_speech_prob": 7.294423994608223e-05}, {"id": 302, "seek": 163496, "start": 1634.96, "end": 1645.2, "text": " Now, from our standpoint, with all this kind of view on approach to the query monitor,", "tokens": [823, 11, 490, 527, 15827, 11, 365, 439, 341, 733, 295, 1910, 322, 3109, 281, 264, 14581, 6002, 11], "temperature": 0.0, "avg_logprob": -0.323846689860026, "compression_ratio": 1.3255813953488371, "no_speech_prob": 9.467052586842328e-05}, {"id": 303, "seek": 163496, "start": 1645.2, "end": 1657.28, "text": " we have been working at the extension for my square, oh, for Postgres, sorry, called", "tokens": [321, 362, 668, 1364, 412, 264, 10320, 337, 452, 3732, 11, 1954, 11, 337, 10223, 45189, 11, 2597, 11, 1219], "temperature": 0.0, "avg_logprob": -0.323846689860026, "compression_ratio": 1.3255813953488371, "no_speech_prob": 9.467052586842328e-05}, {"id": 304, "seek": 165728, "start": 1657.28, "end": 1669.36, "text": " the PgStat monitor, well, and look, we specifically built it for Postgres, not for MySQL, even", "tokens": [264, 430, 70, 4520, 267, 6002, 11, 731, 11, 293, 574, 11, 321, 4682, 3094, 309, 337, 10223, 45189, 11, 406, 337, 1222, 39934, 11, 754], "temperature": 0.0, "avg_logprob": -0.2701299156941159, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.435482646338642e-05}, {"id": 305, "seek": 165728, "start": 1669.36, "end": 1675.68, "text": " though we had a lot more experience with MySQL, because Postgres SQL extension interface is", "tokens": [1673, 321, 632, 257, 688, 544, 1752, 365, 1222, 39934, 11, 570, 10223, 45189, 19200, 10320, 9226, 307], "temperature": 0.0, "avg_logprob": -0.2701299156941159, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.435482646338642e-05}, {"id": 306, "seek": 165728, "start": 1675.68, "end": 1685.68, "text": " awesome and much more powerful than MySQL, right, so you can read a little bit about", "tokens": [3476, 293, 709, 544, 4005, 813, 1222, 39934, 11, 558, 11, 370, 291, 393, 1401, 257, 707, 857, 466], "temperature": 0.0, "avg_logprob": -0.2701299156941159, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.435482646338642e-05}, {"id": 307, "seek": 168568, "start": 1685.68, "end": 1693.0800000000002, "text": " this here, and this is extension which allows a lot more insights and getting kind of such", "tokens": [341, 510, 11, 293, 341, 307, 10320, 597, 4045, 257, 688, 544, 14310, 293, 1242, 733, 295, 1270], "temperature": 0.0, "avg_logprob": -0.2084164064983989, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.00016158985090442002}, {"id": 308, "seek": 168568, "start": 1693.0800000000002, "end": 1699.02, "text": " slicing and dicing, which I mentioned, right, if you think about the traditional Postgres", "tokens": [46586, 293, 274, 5776, 11, 597, 286, 2835, 11, 558, 11, 498, 291, 519, 466, 264, 5164, 10223, 45189], "temperature": 0.0, "avg_logprob": -0.2084164064983989, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.00016158985090442002}, {"id": 309, "seek": 168568, "start": 1699.02, "end": 1706.76, "text": " SQL extension PgStats statements, it really aggregates all the data from the start, right,", "tokens": [19200, 10320, 430, 70, 4520, 1720, 12363, 11, 309, 534, 16743, 1024, 439, 264, 1412, 490, 264, 722, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.2084164064983989, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.00016158985090442002}, {"id": 310, "seek": 168568, "start": 1706.76, "end": 1712.1200000000001, "text": " which is very helpful to be used directly, what we look at the modern observability system", "tokens": [597, 307, 588, 4961, 281, 312, 1143, 3838, 11, 437, 321, 574, 412, 264, 4363, 9951, 2310, 1185], "temperature": 0.0, "avg_logprob": -0.2084164064983989, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.00016158985090442002}, {"id": 311, "seek": 171212, "start": 1712.12, "end": 1719.6, "text": " through where we expect to have many Postgres SQL instances anyway, right, and some system", "tokens": [807, 689, 321, 2066, 281, 362, 867, 10223, 45189, 19200, 14519, 4033, 11, 558, 11, 293, 512, 1185], "temperature": 0.0, "avg_logprob": -0.1885239650041629, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.607678056345321e-05}, {"id": 312, "seek": 171212, "start": 1719.6, "end": 1724.76, "text": " getting that stuff constantly and considerate at that, so that means we are capturing a", "tokens": [1242, 300, 1507, 6460, 293, 1949, 473, 412, 300, 11, 370, 300, 1355, 321, 366, 23384, 257], "temperature": 0.0, "avg_logprob": -0.1885239650041629, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.607678056345321e-05}, {"id": 313, "seek": 171212, "start": 1724.76, "end": 1733.84, "text": " lot of information but keep it only on for a relatively short time in a Postgres SQL", "tokens": [688, 295, 1589, 457, 1066, 309, 787, 322, 337, 257, 7226, 2099, 565, 294, 257, 10223, 45189, 19200], "temperature": 0.0, "avg_logprob": -0.1885239650041629, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.607678056345321e-05}, {"id": 314, "seek": 171212, "start": 1733.84, "end": 1741.6, "text": " instance, right, and that allows to get much more granular information without requiring", "tokens": [5197, 11, 558, 11, 293, 300, 4045, 281, 483, 709, 544, 39962, 1589, 1553, 24165], "temperature": 0.0, "avg_logprob": -0.1885239650041629, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.607678056345321e-05}, {"id": 315, "seek": 174160, "start": 1741.6, "end": 1747.76, "text": " a huge amount of resources, which would be required if you would have it for a time,", "tokens": [257, 2603, 2372, 295, 3593, 11, 597, 576, 312, 4739, 498, 291, 576, 362, 309, 337, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.1748178567779198, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.262728129513562e-05}, {"id": 316, "seek": 174160, "start": 1747.76, "end": 1753.32, "text": " so you can, you know, read more about what that does on the web pages.", "tokens": [370, 291, 393, 11, 291, 458, 11, 1401, 544, 466, 437, 300, 775, 322, 264, 3670, 7183, 13], "temperature": 0.0, "avg_logprob": -0.1748178567779198, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.262728129513562e-05}, {"id": 317, "seek": 174160, "start": 1753.32, "end": 1761.36, "text": " Now some folks asked me, saying, well, folks, like, why do you work on a separate extension", "tokens": [823, 512, 4024, 2351, 385, 11, 1566, 11, 731, 11, 4024, 11, 411, 11, 983, 360, 291, 589, 322, 257, 4994, 10320], "temperature": 0.0, "avg_logprob": -0.1748178567779198, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.262728129513562e-05}, {"id": 318, "seek": 174160, "start": 1761.36, "end": 1768.6399999999999, "text": " of the PgStats monitors, and my answer to that is we really wanted to experiment with", "tokens": [295, 264, 430, 70, 4520, 1720, 26518, 11, 293, 452, 1867, 281, 300, 307, 321, 534, 1415, 281, 5120, 365], "temperature": 0.0, "avg_logprob": -0.1748178567779198, "compression_ratio": 1.5416666666666667, "no_speech_prob": 6.262728129513562e-05}, {"id": 319, "seek": 176864, "start": 1768.64, "end": 1772.72, "text": " different approaches, right, to find what works, what doesn't, how users do, and that", "tokens": [819, 11587, 11, 558, 11, 281, 915, 437, 1985, 11, 437, 1177, 380, 11, 577, 5022, 360, 11, 293, 300], "temperature": 0.0, "avg_logprob": -0.17469040552775064, "compression_ratio": 1.6290322580645162, "no_speech_prob": 7.3868221079465e-05}, {"id": 320, "seek": 176864, "start": 1772.72, "end": 1784.3600000000001, "text": " is always easy to do in a separate extension, right, and then if something is liked by the", "tokens": [307, 1009, 1858, 281, 360, 294, 257, 4994, 10320, 11, 558, 11, 293, 550, 498, 746, 307, 4501, 538, 264], "temperature": 0.0, "avg_logprob": -0.17469040552775064, "compression_ratio": 1.6290322580645162, "no_speech_prob": 7.3868221079465e-05}, {"id": 321, "seek": 176864, "start": 1784.3600000000001, "end": 1792.1200000000001, "text": " community, then we can see how we can get that in an official list of extensions, so", "tokens": [1768, 11, 550, 321, 393, 536, 577, 321, 393, 483, 300, 294, 364, 4783, 1329, 295, 25129, 11, 370], "temperature": 0.0, "avg_logprob": -0.17469040552775064, "compression_ratio": 1.6290322580645162, "no_speech_prob": 7.3868221079465e-05}, {"id": 322, "seek": 176864, "start": 1792.1200000000001, "end": 1796.0400000000002, "text": " that is their feedback, is very valuable.", "tokens": [300, 307, 641, 5824, 11, 307, 588, 8263, 13], "temperature": 0.0, "avg_logprob": -0.17469040552775064, "compression_ratio": 1.6290322580645162, "no_speech_prob": 7.3868221079465e-05}, {"id": 323, "seek": 179604, "start": 1796.04, "end": 1801.32, "text": " And also if you look in this case while we are providing PgStats statements compatibility,", "tokens": [400, 611, 498, 291, 574, 294, 341, 1389, 1339, 321, 366, 6530, 430, 70, 4520, 1720, 12363, 34237, 11], "temperature": 0.0, "avg_logprob": -0.19962380149147727, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.55280577018857e-05}, {"id": 324, "seek": 179604, "start": 1801.32, "end": 1807.1599999999999, "text": " right, so you can get that view from the same extension instead of getting another two extensions", "tokens": [558, 11, 370, 291, 393, 483, 300, 1910, 490, 264, 912, 10320, 2602, 295, 1242, 1071, 732, 25129], "temperature": 0.0, "avg_logprob": -0.19962380149147727, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.55280577018857e-05}, {"id": 325, "seek": 179604, "start": 1807.1599999999999, "end": 1814.36, "text": " with additional overhead, PgStat monitor has kind of different ways to aggregate and present", "tokens": [365, 4497, 19922, 11, 430, 70, 4520, 267, 6002, 575, 733, 295, 819, 2098, 281, 26118, 293, 1974], "temperature": 0.0, "avg_logprob": -0.19962380149147727, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.55280577018857e-05}, {"id": 326, "seek": 179604, "start": 1814.36, "end": 1823.36, "text": " the data, right, which kind of, well, you cannot get in the same, in the same view.", "tokens": [264, 1412, 11, 558, 11, 597, 733, 295, 11, 731, 11, 291, 2644, 483, 294, 264, 912, 11, 294, 264, 912, 1910, 13], "temperature": 0.0, "avg_logprob": -0.19962380149147727, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.55280577018857e-05}, {"id": 327, "seek": 182336, "start": 1823.36, "end": 1832.24, "text": " Okay, now as I spoke about the query performance, I wanted to highlight a couple of other things", "tokens": [1033, 11, 586, 382, 286, 7179, 466, 264, 14581, 3389, 11, 286, 1415, 281, 5078, 257, 1916, 295, 661, 721], "temperature": 0.0, "avg_logprob": -0.16899774991548977, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.4741064155241475e-05}, {"id": 328, "seek": 182336, "start": 1832.24, "end": 1839.32, "text": " which are quite interesting to consider when you are looking at the queries where I see", "tokens": [597, 366, 1596, 1880, 281, 1949, 562, 291, 366, 1237, 412, 264, 24109, 689, 286, 536], "temperature": 0.0, "avg_logprob": -0.16899774991548977, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.4741064155241475e-05}, {"id": 329, "seek": 182336, "start": 1839.32, "end": 1842.0, "text": " a number of issues.", "tokens": [257, 1230, 295, 2663, 13], "temperature": 0.0, "avg_logprob": -0.16899774991548977, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.4741064155241475e-05}, {"id": 330, "seek": 182336, "start": 1842.0, "end": 1848.4399999999998, "text": " One is what I would call the bad queries versus victims, right.", "tokens": [1485, 307, 437, 286, 576, 818, 264, 1578, 24109, 5717, 11448, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.16899774991548977, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.4741064155241475e-05}, {"id": 331, "seek": 184844, "start": 1848.44, "end": 1855.68, "text": " In certain cases, or like in many cases, right, you may see even your otherwise good queries", "tokens": [682, 1629, 3331, 11, 420, 411, 294, 867, 3331, 11, 558, 11, 291, 815, 536, 754, 428, 5911, 665, 24109], "temperature": 0.0, "avg_logprob": -0.1998898936252968, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.0783936886582524e-05}, {"id": 332, "seek": 184844, "start": 1855.68, "end": 1864.92, "text": " like, hey, this is just a lookup by the primary key starting to be a lot slower than it usually", "tokens": [411, 11, 4177, 11, 341, 307, 445, 257, 574, 1010, 538, 264, 6194, 2141, 2891, 281, 312, 257, 688, 14009, 813, 309, 2673], "temperature": 0.0, "avg_logprob": -0.1998898936252968, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.0783936886582524e-05}, {"id": 333, "seek": 184844, "start": 1864.92, "end": 1869.8400000000001, "text": " is, not because something changes the relation to that query, but because of some other bad", "tokens": [307, 11, 406, 570, 746, 2962, 264, 9721, 281, 300, 14581, 11, 457, 570, 295, 512, 661, 1578], "temperature": 0.0, "avg_logprob": -0.1998898936252968, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.0783936886582524e-05}, {"id": 334, "seek": 184844, "start": 1869.8400000000001, "end": 1872.28, "text": " queries, right, have been running in parallel.", "tokens": [24109, 11, 558, 11, 362, 668, 2614, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.1998898936252968, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.0783936886582524e-05}, {"id": 335, "seek": 184844, "start": 1872.28, "end": 1877.6000000000001, "text": " And imagine that if you will oversaturate your node, right, the hundreds of bad queries", "tokens": [400, 3811, 300, 498, 291, 486, 15488, 19493, 473, 428, 9984, 11, 558, 11, 264, 6779, 295, 1578, 24109], "temperature": 0.0, "avg_logprob": -0.1998898936252968, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.0783936886582524e-05}, {"id": 336, "seek": 187760, "start": 1877.6, "end": 1881.04, "text": " running at the same time, right, well, then everything will become slow.", "tokens": [2614, 412, 264, 912, 565, 11, 558, 11, 731, 11, 550, 1203, 486, 1813, 2964, 13], "temperature": 0.0, "avg_logprob": -0.15930835405985513, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.204640597687103e-05}, {"id": 337, "seek": 187760, "start": 1881.04, "end": 1886.48, "text": " And I think that's important to understand what if you are seeing some query being slow,", "tokens": [400, 286, 519, 300, 311, 1021, 281, 1223, 437, 498, 291, 366, 2577, 512, 14581, 885, 2964, 11], "temperature": 0.0, "avg_logprob": -0.15930835405985513, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.204640597687103e-05}, {"id": 338, "seek": 187760, "start": 1886.48, "end": 1899.9599999999998, "text": " you cannot just think about that as that query problem, it may be entirely something else.", "tokens": [291, 2644, 445, 519, 466, 300, 382, 300, 14581, 1154, 11, 309, 815, 312, 7696, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.15930835405985513, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.204640597687103e-05}, {"id": 339, "seek": 189996, "start": 1899.96, "end": 1907.88, "text": " The next thing to consider is currently running queries.", "tokens": [440, 958, 551, 281, 1949, 307, 4362, 2614, 24109, 13], "temperature": 0.0, "avg_logprob": -0.22068825250939478, "compression_ratio": 1.5463414634146342, "no_speech_prob": 2.308713192178402e-05}, {"id": 340, "seek": 189996, "start": 1907.88, "end": 1912.8400000000001, "text": " That is also rather interesting, right, because they may not be reflected in the log, right,", "tokens": [663, 307, 611, 2831, 1880, 11, 558, 11, 570, 436, 815, 406, 312, 15502, 294, 264, 3565, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.22068825250939478, "compression_ratio": 1.5463414634146342, "no_speech_prob": 2.308713192178402e-05}, {"id": 341, "seek": 189996, "start": 1912.8400000000001, "end": 1919.1200000000001, "text": " or something which say, oh, that query completed and it was, you know, five minutes response", "tokens": [420, 746, 597, 584, 11, 1954, 11, 300, 14581, 7365, 293, 309, 390, 11, 291, 458, 11, 1732, 2077, 4134], "temperature": 0.0, "avg_logprob": -0.22068825250939478, "compression_ratio": 1.5463414634146342, "no_speech_prob": 2.308713192178402e-05}, {"id": 342, "seek": 189996, "start": 1919.1200000000001, "end": 1922.2, "text": " time or 15 seconds, whatever, right.", "tokens": [565, 420, 2119, 3949, 11, 2035, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.22068825250939478, "compression_ratio": 1.5463414634146342, "no_speech_prob": 2.308713192178402e-05}, {"id": 343, "seek": 189996, "start": 1922.2, "end": 1926.64, "text": " But running queries can be a problem.", "tokens": [583, 2614, 24109, 393, 312, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.22068825250939478, "compression_ratio": 1.5463414634146342, "no_speech_prob": 2.308713192178402e-05}, {"id": 344, "seek": 192664, "start": 1926.64, "end": 1932.44, "text": " And in many cases, that is actually how things start to snowball, right, you have some application", "tokens": [400, 294, 867, 3331, 11, 300, 307, 767, 577, 721, 722, 281, 46143, 11, 558, 11, 291, 362, 512, 3861], "temperature": 0.0, "avg_logprob": -0.19974909330669202, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.362176645779982e-05}, {"id": 345, "seek": 192664, "start": 1932.44, "end": 1937.16, "text": " or even kind of user starts a lot of, you know, bad queries, you know, forgot like", "tokens": [420, 754, 733, 295, 4195, 3719, 257, 688, 295, 11, 291, 458, 11, 1578, 24109, 11, 291, 458, 11, 5298, 411], "temperature": 0.0, "avg_logprob": -0.19974909330669202, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.362176645779982e-05}, {"id": 346, "seek": 192664, "start": 1937.16, "end": 1943.3600000000001, "text": " a where clause and a join, right, or something like that, and they just, you know, run for", "tokens": [257, 689, 25925, 293, 257, 3917, 11, 558, 11, 420, 746, 411, 300, 11, 293, 436, 445, 11, 291, 458, 11, 1190, 337], "temperature": 0.0, "avg_logprob": -0.19974909330669202, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.362176645779982e-05}, {"id": 347, "seek": 192664, "start": 1943.3600000000001, "end": 1949.96, "text": " a long time, right, so you want to make sure you're paying attention to that as well.", "tokens": [257, 938, 565, 11, 558, 11, 370, 291, 528, 281, 652, 988, 291, 434, 6229, 3202, 281, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19974909330669202, "compression_ratio": 1.6807511737089202, "no_speech_prob": 6.362176645779982e-05}, {"id": 348, "seek": 194996, "start": 1949.96, "end": 1958.24, "text": " The next is to consider what not all activities are directly visible from a query standpoint.", "tokens": [440, 958, 307, 281, 1949, 437, 406, 439, 5354, 366, 3838, 8974, 490, 257, 14581, 15827, 13], "temperature": 0.0, "avg_logprob": -0.22456499735514324, "compression_ratio": 1.6694915254237288, "no_speech_prob": 4.662919673137367e-05}, {"id": 349, "seek": 194996, "start": 1958.24, "end": 1964.32, "text": " The database often tend to do a bunch of background activities, right.", "tokens": [440, 8149, 2049, 3928, 281, 360, 257, 3840, 295, 3678, 5354, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.22456499735514324, "compression_ratio": 1.6694915254237288, "no_speech_prob": 4.662919673137367e-05}, {"id": 350, "seek": 194996, "start": 1964.32, "end": 1969.24, "text": " Additionally you may have something else, like maybe you are taking a snapshot, right,", "tokens": [19927, 291, 815, 362, 746, 1646, 11, 411, 1310, 291, 366, 1940, 257, 30163, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.22456499735514324, "compression_ratio": 1.6694915254237288, "no_speech_prob": 4.662919673137367e-05}, {"id": 351, "seek": 194996, "start": 1969.24, "end": 1974.04, "text": " or taking a backup in the other way, which use also the system resources, right, which", "tokens": [420, 1940, 257, 14807, 294, 264, 661, 636, 11, 597, 764, 611, 264, 1185, 3593, 11, 558, 11, 597], "temperature": 0.0, "avg_logprob": -0.22456499735514324, "compression_ratio": 1.6694915254237288, "no_speech_prob": 4.662919673137367e-05}, {"id": 352, "seek": 194996, "start": 1974.04, "end": 1978.8, "text": " are not seen from query standpoint, but same important.", "tokens": [366, 406, 1612, 490, 14581, 15827, 11, 457, 912, 1021, 13], "temperature": 0.0, "avg_logprob": -0.22456499735514324, "compression_ratio": 1.6694915254237288, "no_speech_prob": 4.662919673137367e-05}, {"id": 353, "seek": 197880, "start": 1978.8, "end": 1984.8, "text": " You also have a lot of things which can be happening on the cloud level, right, again,", "tokens": [509, 611, 362, 257, 688, 295, 721, 597, 393, 312, 2737, 322, 264, 4588, 1496, 11, 558, 11, 797, 11], "temperature": 0.0, "avg_logprob": -0.18188448305483218, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.950431578734424e-05}, {"id": 354, "seek": 197880, "start": 1984.8, "end": 1989.84, "text": " which can be, you know, completely invisible for us.", "tokens": [597, 393, 312, 11, 291, 458, 11, 2584, 14603, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.18188448305483218, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.950431578734424e-05}, {"id": 355, "seek": 197880, "start": 1989.84, "end": 1995.84, "text": " And wherever you are looking, again, at the query performance, it's important to consider", "tokens": [400, 8660, 291, 366, 1237, 11, 797, 11, 412, 264, 14581, 3389, 11, 309, 311, 1021, 281, 1949], "temperature": 0.0, "avg_logprob": -0.18188448305483218, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.950431578734424e-05}, {"id": 356, "seek": 197880, "start": 1995.84, "end": 2001.6399999999999, "text": " where, you know, maybe something going on, right, additionally what those queries tell", "tokens": [689, 11, 291, 458, 11, 1310, 746, 516, 322, 11, 558, 11, 43181, 437, 729, 24109, 980], "temperature": 0.0, "avg_logprob": -0.18188448305483218, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.950431578734424e-05}, {"id": 357, "seek": 197880, "start": 2001.6399999999999, "end": 2003.6, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.18188448305483218, "compression_ratio": 1.621212121212121, "no_speech_prob": 1.950431578734424e-05}, {"id": 358, "seek": 200360, "start": 2003.6, "end": 2009.8, "text": " Next question is about, or last thing I would say, is about sampling.", "tokens": [3087, 1168, 307, 466, 11, 420, 1036, 551, 286, 576, 584, 11, 307, 466, 21179, 13], "temperature": 0.0, "avg_logprob": -0.14381678899129233, "compression_ratio": 1.5695067264573992, "no_speech_prob": 8.093335054581985e-05}, {"id": 359, "seek": 200360, "start": 2009.8, "end": 2015.9599999999998, "text": " In certain cases I see people saying, well, you know what, let us only capture queries", "tokens": [682, 1629, 3331, 286, 536, 561, 1566, 11, 731, 11, 291, 458, 437, 11, 718, 505, 787, 7983, 24109], "temperature": 0.0, "avg_logprob": -0.14381678899129233, "compression_ratio": 1.5695067264573992, "no_speech_prob": 8.093335054581985e-05}, {"id": 360, "seek": 200360, "start": 2015.9599999999998, "end": 2019.4399999999998, "text": " over X time.", "tokens": [670, 1783, 565, 13], "temperature": 0.0, "avg_logprob": -0.14381678899129233, "compression_ratio": 1.5695067264573992, "no_speech_prob": 8.093335054581985e-05}, {"id": 361, "seek": 200360, "start": 2019.4399999999998, "end": 2024.3999999999999, "text": " A lot of APM frameworks, right, for example, you know, like New Relics and such may be very", "tokens": [316, 688, 295, 5372, 44, 29834, 11, 558, 11, 337, 1365, 11, 291, 458, 11, 411, 1873, 8738, 1167, 293, 1270, 815, 312, 588], "temperature": 0.0, "avg_logprob": -0.14381678899129233, "compression_ratio": 1.5695067264573992, "no_speech_prob": 8.093335054581985e-05}, {"id": 362, "seek": 200360, "start": 2024.3999999999999, "end": 2029.24, "text": " focused on that, saying, hey, you know what, we are going to also give you some examples", "tokens": [5178, 322, 300, 11, 1566, 11, 4177, 11, 291, 458, 437, 11, 321, 366, 516, 281, 611, 976, 291, 512, 5110], "temperature": 0.0, "avg_logprob": -0.14381678899129233, "compression_ratio": 1.5695067264573992, "no_speech_prob": 8.093335054581985e-05}, {"id": 363, "seek": 202924, "start": 2029.24, "end": 2033.64, "text": " of the queries which take more than, you know, one second or whatever execution time.", "tokens": [295, 264, 24109, 597, 747, 544, 813, 11, 291, 458, 11, 472, 1150, 420, 2035, 15058, 565, 13], "temperature": 0.0, "avg_logprob": -0.18148316728307845, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00011206644558114931}, {"id": 364, "seek": 202924, "start": 2033.64, "end": 2034.8, "text": " So focus on those.", "tokens": [407, 1879, 322, 729, 13], "temperature": 0.0, "avg_logprob": -0.18148316728307845, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00011206644558114931}, {"id": 365, "seek": 202924, "start": 2034.8, "end": 2040.08, "text": " Well, and yes, looking at those queries may make sense, right, if they take a long time,", "tokens": [1042, 11, 293, 2086, 11, 1237, 412, 729, 24109, 815, 652, 2020, 11, 558, 11, 498, 436, 747, 257, 938, 565, 11], "temperature": 0.0, "avg_logprob": -0.18148316728307845, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00011206644558114931}, {"id": 366, "seek": 202924, "start": 2040.08, "end": 2051.32, "text": " that may be a problem, but it is often what your medium of performance queries, right,", "tokens": [300, 815, 312, 257, 1154, 11, 457, 309, 307, 2049, 437, 428, 6399, 295, 3389, 24109, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.18148316728307845, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00011206644558114931}, {"id": 367, "seek": 202924, "start": 2051.32, "end": 2056.36, "text": " I would say are creating a whole bunch of load on your system, and they contribute the", "tokens": [286, 576, 584, 366, 4084, 257, 1379, 3840, 295, 3677, 322, 428, 1185, 11, 293, 436, 10586, 264], "temperature": 0.0, "avg_logprob": -0.18148316728307845, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00011206644558114931}, {"id": 368, "seek": 205636, "start": 2056.36, "end": 2062.6, "text": " greatest response time to user application, right, and ignoring those can be problematic.", "tokens": [6636, 4134, 565, 281, 4195, 3861, 11, 558, 11, 293, 26258, 729, 393, 312, 19011, 13], "temperature": 0.0, "avg_logprob": -0.16491404245066088, "compression_ratio": 1.6826923076923077, "no_speech_prob": 5.342020085663535e-05}, {"id": 369, "seek": 205636, "start": 2062.6, "end": 2073.6800000000003, "text": " Well, that is the main overview, right, I hope, what that was, that was helpful, right,", "tokens": [1042, 11, 300, 307, 264, 2135, 12492, 11, 558, 11, 286, 1454, 11, 437, 300, 390, 11, 300, 390, 4961, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.16491404245066088, "compression_ratio": 1.6826923076923077, "no_speech_prob": 5.342020085663535e-05}, {"id": 370, "seek": 205636, "start": 2073.6800000000003, "end": 2079.2000000000003, "text": " and my main goal here is to make sure maybe to give you some thinking tools, right, as", "tokens": [293, 452, 2135, 3387, 510, 307, 281, 652, 988, 1310, 281, 976, 291, 512, 1953, 3873, 11, 558, 11, 382], "temperature": 0.0, "avg_logprob": -0.16491404245066088, "compression_ratio": 1.6826923076923077, "no_speech_prob": 5.342020085663535e-05}, {"id": 371, "seek": 205636, "start": 2079.2000000000003, "end": 2084.96, "text": " you noticed, that is not like particularly technical talk, right, which tells you how", "tokens": [291, 5694, 11, 300, 307, 406, 411, 4098, 6191, 751, 11, 558, 11, 597, 5112, 291, 577], "temperature": 0.0, "avg_logprob": -0.16491404245066088, "compression_ratio": 1.6826923076923077, "no_speech_prob": 5.342020085663535e-05}, {"id": 372, "seek": 208496, "start": 2084.96, "end": 2090.88, "text": " exactly to find out which indexes to create or something, but hopefully you get some tools", "tokens": [2293, 281, 915, 484, 597, 8186, 279, 281, 1884, 420, 746, 11, 457, 4696, 291, 483, 512, 3873], "temperature": 0.0, "avg_logprob": -0.15565341017967046, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00010936283797491342}, {"id": 373, "seek": 208496, "start": 2090.88, "end": 2098.16, "text": " in this case, how to start, how to approach that, which can prevent you from tuning by", "tokens": [294, 341, 1389, 11, 577, 281, 722, 11, 577, 281, 3109, 300, 11, 597, 393, 4871, 291, 490, 15164, 538], "temperature": 0.0, "avg_logprob": -0.15565341017967046, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00010936283797491342}, {"id": 374, "seek": 208496, "start": 2098.16, "end": 2105.7200000000003, "text": " the credit card, you know, scaling the instances to inappropriate sizes, because hey, that", "tokens": [264, 5397, 2920, 11, 291, 458, 11, 21589, 264, 14519, 281, 26723, 11602, 11, 570, 4177, 11, 300], "temperature": 0.0, "avg_logprob": -0.15565341017967046, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00010936283797491342}, {"id": 375, "seek": 208496, "start": 2105.7200000000003, "end": 2112.64, "text": " is good for both your wallet as well as good for environment, right, we do not need those", "tokens": [307, 665, 337, 1293, 428, 16599, 382, 731, 382, 665, 337, 2823, 11, 558, 11, 321, 360, 406, 643, 729], "temperature": 0.0, "avg_logprob": -0.15565341017967046, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00010936283797491342}, {"id": 376, "seek": 211264, "start": 2112.64, "end": 2117.52, "text": " servers generating more heat than absolutely needed.", "tokens": [15909, 17746, 544, 3738, 813, 3122, 2978, 13], "temperature": 0.0, "avg_logprob": -0.32594021388462613, "compression_ratio": 1.2110091743119267, "no_speech_prob": 0.001090249395929277}, {"id": 377, "seek": 211264, "start": 2117.52, "end": 2125.96, "text": " Well, with that, it is all I have, and I would be happy to take some questions.", "tokens": [1042, 11, 365, 300, 11, 309, 307, 439, 286, 362, 11, 293, 286, 576, 312, 2055, 281, 747, 512, 1651, 13], "temperature": 0.0, "avg_logprob": -0.32594021388462613, "compression_ratio": 1.2110091743119267, "no_speech_prob": 0.001090249395929277}, {"id": 378, "seek": 212596, "start": 2125.96, "end": 2151.4, "text": " Hey, thank you very much for your talk, my question is about when do you have to increase", "tokens": [1911, 11, 1309, 291, 588, 709, 337, 428, 751, 11, 452, 1168, 307, 466, 562, 360, 291, 362, 281, 3488], "temperature": 0.0, "avg_logprob": -0.28154313564300537, "compression_ratio": 1.0987654320987654, "no_speech_prob": 0.006799944676458836}, {"id": 379, "seek": 215140, "start": 2151.4, "end": 2158.12, "text": " the box, as a developer, you are in front of a situation where you need to decide between", "tokens": [264, 2424, 11, 382, 257, 10754, 11, 291, 366, 294, 1868, 295, 257, 2590, 689, 291, 643, 281, 4536, 1296], "temperature": 0.0, "avg_logprob": -0.2558772997422652, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.006938540376722813}, {"id": 380, "seek": 215140, "start": 2158.12, "end": 2164.84, "text": " optimizing or asking the CEO to just pay more, because you have a time constraint, so do", "tokens": [40425, 420, 3365, 264, 9282, 281, 445, 1689, 544, 11, 570, 291, 362, 257, 565, 25534, 11, 370, 360], "temperature": 0.0, "avg_logprob": -0.2558772997422652, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.006938540376722813}, {"id": 381, "seek": 215140, "start": 2164.84, "end": 2171.44, "text": " you have the thumb rules where in front of a problem you would say, okay, better to optimize", "tokens": [291, 362, 264, 9298, 4474, 689, 294, 1868, 295, 257, 1154, 291, 576, 584, 11, 1392, 11, 1101, 281, 19719], "temperature": 0.0, "avg_logprob": -0.2558772997422652, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.006938540376722813}, {"id": 382, "seek": 215140, "start": 2171.44, "end": 2176.32, "text": " or better to increase the box, you know, when, how can you decide with me?", "tokens": [420, 1101, 281, 3488, 264, 2424, 11, 291, 458, 11, 562, 11, 577, 393, 291, 4536, 365, 385, 30], "temperature": 0.0, "avg_logprob": -0.2558772997422652, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.006938540376722813}, {"id": 383, "seek": 217632, "start": 2176.32, "end": 2182.92, "text": " The question is to, like, wherever it is better to increase the box size, right, or optimize", "tokens": [440, 1168, 307, 281, 11, 411, 11, 8660, 309, 307, 1101, 281, 3488, 264, 2424, 2744, 11, 558, 11, 420, 19719], "temperature": 0.0, "avg_logprob": -0.254573448417113, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00039899564580991864}, {"id": 384, "seek": 217632, "start": 2182.92, "end": 2183.92, "text": " the query.", "tokens": [264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.254573448417113, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00039899564580991864}, {"id": 385, "seek": 217632, "start": 2183.92, "end": 2188.04, "text": " Well, and I think it is interesting, right, that it is not often either a question, right,", "tokens": [1042, 11, 293, 286, 519, 309, 307, 1880, 11, 558, 11, 300, 309, 307, 406, 2049, 2139, 257, 1168, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.254573448417113, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00039899564580991864}, {"id": 386, "seek": 217632, "start": 2188.04, "end": 2192.7200000000003, "text": " I think the time in this case is also often essence, and many cases I have seen people", "tokens": [286, 519, 264, 565, 294, 341, 1389, 307, 611, 2049, 12801, 11, 293, 867, 3331, 286, 362, 1612, 561], "temperature": 0.0, "avg_logprob": -0.254573448417113, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00039899564580991864}, {"id": 387, "seek": 217632, "start": 2192.7200000000003, "end": 2199.6000000000004, "text": " saying if they have a problem, right, in this case, and they absolutely need to get like", "tokens": [1566, 498, 436, 362, 257, 1154, 11, 558, 11, 294, 341, 1389, 11, 293, 436, 3122, 643, 281, 483, 411], "temperature": 0.0, "avg_logprob": -0.254573448417113, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.00039899564580991864}, {"id": 388, "seek": 219960, "start": 2199.6, "end": 2207.48, "text": " a application up, scale the box, right, and then kind of can currently work on the query", "tokens": [257, 3861, 493, 11, 4373, 264, 2424, 11, 558, 11, 293, 550, 733, 295, 393, 4362, 589, 322, 264, 14581], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 389, "seek": 219960, "start": 2207.48, "end": 2212.08, "text": " optimization, right, and to bring it back and scale down.", "tokens": [19618, 11, 558, 11, 293, 281, 1565, 309, 646, 293, 4373, 760, 13], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 390, "seek": 219960, "start": 2212.08, "end": 2216.96, "text": " I think that is a very, very reasonable approach, right, because, well, it gives you kind of", "tokens": [286, 519, 300, 307, 257, 588, 11, 588, 10585, 3109, 11, 558, 11, 570, 11, 731, 11, 309, 2709, 291, 733, 295], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 391, "seek": 219960, "start": 2216.96, "end": 2218.2799999999997, "text": " more briefing room.", "tokens": [544, 28878, 1808, 13], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 392, "seek": 219960, "start": 2218.2799999999997, "end": 2223.88, "text": " What is important in this case, as in many things in life, is not to be lazy, right,", "tokens": [708, 307, 1021, 294, 341, 1389, 11, 382, 294, 867, 721, 294, 993, 11, 307, 406, 281, 312, 14847, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 393, "seek": 219960, "start": 2223.88, "end": 2227.6, "text": " like you don't want to just, you know, scale the box and forget about that, you want to", "tokens": [411, 291, 500, 380, 528, 281, 445, 11, 291, 458, 11, 4373, 264, 2424, 293, 2870, 466, 300, 11, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.166127930339585, "compression_ratio": 1.7419354838709677, "no_speech_prob": 3.5337518056621775e-05}, {"id": 394, "seek": 222760, "start": 2227.6, "end": 2231.2799999999997, "text": " scale the box, optimize the queries and so on, right.", "tokens": [4373, 264, 2424, 11, 19719, 264, 24109, 293, 370, 322, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 395, "seek": 222760, "start": 2231.2799999999997, "end": 2237.6, "text": " Now I often, when I look at the queries, right, as you look at that, you can see which", "tokens": [823, 286, 2049, 11, 562, 286, 574, 412, 264, 24109, 11, 558, 11, 382, 291, 574, 412, 300, 11, 291, 393, 536, 597], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 396, "seek": 222760, "start": 2237.6, "end": 2244.2, "text": " of them are low-hanging fruits, right, or when a query is already optimized pretty well,", "tokens": [295, 552, 366, 2295, 12, 71, 9741, 12148, 11, 558, 11, 420, 562, 257, 14581, 307, 1217, 26941, 1238, 731, 11], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 397, "seek": 222760, "start": 2244.2, "end": 2245.2, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 398, "seek": 222760, "start": 2245.2, "end": 2250.8399999999997, "text": " If you are saying, well, you know what, actually, majority of a workload is driven by lookups", "tokens": [759, 291, 366, 1566, 11, 731, 11, 291, 458, 437, 11, 767, 11, 6286, 295, 257, 20139, 307, 9555, 538, 574, 7528], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 399, "seek": 222760, "start": 2250.8399999999997, "end": 2256.3199999999997, "text": " for, by the primary key for a table which is already in memory, you can say, well, you", "tokens": [337, 11, 538, 264, 6194, 2141, 337, 257, 3199, 597, 307, 1217, 294, 4675, 11, 291, 393, 584, 11, 731, 11, 291], "temperature": 0.0, "avg_logprob": -0.18248696612496662, "compression_ratio": 1.8130434782608695, "no_speech_prob": 9.257745114155114e-05}, {"id": 400, "seek": 225632, "start": 2256.32, "end": 2259.76, "text": " know what, there is very little I can do to optimize this thing, right.", "tokens": [458, 437, 11, 456, 307, 588, 707, 286, 393, 360, 281, 19719, 341, 551, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2582331967641072, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.014597652712837e-05}, {"id": 401, "seek": 225632, "start": 2259.76, "end": 2265.2400000000002, "text": " If you are saying, oh, that is a query which does massive join, if no indexes, well, totally", "tokens": [759, 291, 366, 1566, 11, 1954, 11, 300, 307, 257, 14581, 597, 775, 5994, 3917, 11, 498, 572, 8186, 279, 11, 731, 11, 3879], "temperature": 0.0, "avg_logprob": -0.2582331967641072, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.014597652712837e-05}, {"id": 402, "seek": 225632, "start": 2265.2400000000002, "end": 2269.56, "text": " different store, right, you may be able to make that to run thousand times faster, right,", "tokens": [819, 3531, 11, 558, 11, 291, 815, 312, 1075, 281, 652, 300, 281, 1190, 4714, 1413, 4663, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.2582331967641072, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.014597652712837e-05}, {"id": 403, "seek": 225632, "start": 2269.56, "end": 2275.36, "text": " with relatively easy index add.", "tokens": [365, 7226, 1858, 8186, 909, 13], "temperature": 0.0, "avg_logprob": -0.2582331967641072, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.014597652712837e-05}, {"id": 404, "seek": 225632, "start": 2275.36, "end": 2282.36, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.2582331967641072, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.014597652712837e-05}, {"id": 405, "seek": 228236, "start": 2282.36, "end": 2299.56, "text": " Hi, so as part of your slice and dice approach to monitoring queries, would you advise that", "tokens": [2421, 11, 370, 382, 644, 295, 428, 13153, 293, 10313, 3109, 281, 11028, 24109, 11, 576, 291, 18312, 300], "temperature": 0.0, "avg_logprob": -0.19256831378471562, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.0012853173539042473}, {"id": 406, "seek": 228236, "start": 2299.56, "end": 2306.6400000000003, "text": " concurrently queries in the, on the application side are never written as dynamic queries", "tokens": [37702, 356, 24109, 294, 264, 11, 322, 264, 3861, 1252, 366, 1128, 3720, 382, 8546, 24109], "temperature": 0.0, "avg_logprob": -0.19256831378471562, "compression_ratio": 1.4365079365079365, "no_speech_prob": 0.0012853173539042473}, {"id": 407, "seek": 230664, "start": 2306.64, "end": 2312.6, "text": " or as like anonymous prepared statements and only follow, say, named prepared statements", "tokens": [420, 382, 411, 24932, 4927, 12363, 293, 787, 1524, 11, 584, 11, 4926, 4927, 12363], "temperature": 0.0, "avg_logprob": -0.2252088744064857, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00022167802671901882}, {"id": 408, "seek": 230664, "start": 2312.6, "end": 2318.92, "text": " so that you know we have a fixed set of queries that are always the same?", "tokens": [370, 300, 291, 458, 321, 362, 257, 6806, 992, 295, 24109, 300, 366, 1009, 264, 912, 30], "temperature": 0.0, "avg_logprob": -0.2252088744064857, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00022167802671901882}, {"id": 409, "seek": 230664, "start": 2318.92, "end": 2324.96, "text": " Well, the question is, I would say, like it's kind of like a cart in the horse, right, like", "tokens": [1042, 11, 264, 1168, 307, 11, 286, 576, 584, 11, 411, 309, 311, 733, 295, 411, 257, 5467, 294, 264, 6832, 11, 558, 11, 411], "temperature": 0.0, "avg_logprob": -0.2252088744064857, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00022167802671901882}, {"id": 410, "seek": 230664, "start": 2324.96, "end": 2331.12, "text": " from, from my standpoint, right, like you can of course talk about those kind of practices,", "tokens": [490, 11, 490, 452, 15827, 11, 558, 11, 411, 291, 393, 295, 1164, 751, 466, 729, 733, 295, 7525, 11], "temperature": 0.0, "avg_logprob": -0.2252088744064857, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00022167802671901882}, {"id": 411, "seek": 233112, "start": 2331.12, "end": 2336.8399999999997, "text": " but developers like to do what is there, what keeps them productive, right, and in many", "tokens": [457, 8849, 411, 281, 360, 437, 307, 456, 11, 437, 5965, 552, 13304, 11, 558, 11, 293, 294, 867], "temperature": 0.0, "avg_logprob": -0.2683529573328355, "compression_ratio": 1.7155172413793103, "no_speech_prob": 3.2155050575966015e-05}, {"id": 412, "seek": 233112, "start": 2336.8399999999997, "end": 2341.6, "text": " cases saying, well, you know what, oh, you don't use like, or and frameworks, right,", "tokens": [3331, 1566, 11, 731, 11, 291, 458, 437, 11, 1954, 11, 291, 500, 380, 764, 411, 11, 420, 293, 29834, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.2683529573328355, "compression_ratio": 1.7155172413793103, "no_speech_prob": 3.2155050575966015e-05}, {"id": 413, "seek": 233112, "start": 2341.6, "end": 2345.44, "text": " on the device and that, that is complicated, right.", "tokens": [322, 264, 4302, 293, 300, 11, 300, 307, 6179, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2683529573328355, "compression_ratio": 1.7155172413793103, "no_speech_prob": 3.2155050575966015e-05}, {"id": 414, "seek": 233112, "start": 2345.44, "end": 2353.08, "text": " Now even if you're using dynamic queries, typically, they're still going to be at the,", "tokens": [823, 754, 498, 291, 434, 1228, 8546, 24109, 11, 5850, 11, 436, 434, 920, 516, 281, 312, 412, 264, 11], "temperature": 0.0, "avg_logprob": -0.2683529573328355, "compression_ratio": 1.7155172413793103, "no_speech_prob": 3.2155050575966015e-05}, {"id": 415, "seek": 233112, "start": 2353.08, "end": 2357.24, "text": " relate to a limited number of variations, right, and especially limited number of most", "tokens": [10961, 281, 257, 5567, 1230, 295, 17840, 11, 558, 11, 293, 2318, 5567, 1230, 295, 881], "temperature": 0.0, "avg_logprob": -0.2683529573328355, "compression_ratio": 1.7155172413793103, "no_speech_prob": 3.2155050575966015e-05}, {"id": 416, "seek": 235724, "start": 2357.24, "end": 2362.16, "text": " important for variations which are going to be generated, you will still see that from", "tokens": [1021, 337, 17840, 597, 366, 516, 281, 312, 10833, 11, 291, 486, 920, 536, 300, 490], "temperature": 0.0, "avg_logprob": -0.20781373505545134, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.0542340280371718e-05}, {"id": 417, "seek": 235724, "start": 2362.16, "end": 2363.64, "text": " the query type, right.", "tokens": [264, 14581, 2010, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.20781373505545134, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.0542340280371718e-05}, {"id": 418, "seek": 235724, "start": 2363.64, "end": 2368.4799999999996, "text": " So in many cases, like if you look at that, I would say like a whole set of queries,", "tokens": [407, 294, 867, 3331, 11, 411, 498, 291, 574, 412, 300, 11, 286, 576, 584, 411, 257, 1379, 992, 295, 24109, 11], "temperature": 0.0, "avg_logprob": -0.20781373505545134, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.0542340280371718e-05}, {"id": 419, "seek": 235724, "start": 2368.4799999999996, "end": 2376.4399999999996, "text": " you would find, well, this application has, let's say, 10,000 of the distant queries,", "tokens": [291, 576, 915, 11, 731, 11, 341, 3861, 575, 11, 718, 311, 584, 11, 1266, 11, 1360, 295, 264, 17275, 24109, 11], "temperature": 0.0, "avg_logprob": -0.20781373505545134, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.0542340280371718e-05}, {"id": 420, "seek": 235724, "start": 2376.4399999999996, "end": 2384.6, "text": " but if I look at top 20, that will be responsible like for 90, 99 percent response time, right,", "tokens": [457, 498, 286, 574, 412, 1192, 945, 11, 300, 486, 312, 6250, 411, 337, 4289, 11, 11803, 3043, 4134, 565, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.20781373505545134, "compression_ratio": 1.613733905579399, "no_speech_prob": 2.0542340280371718e-05}, {"id": 421, "seek": 238460, "start": 2384.6, "end": 2389.7599999999998, "text": " and that of course can change, right, but often focusing on those firsts, right, as well", "tokens": [293, 300, 295, 1164, 393, 1319, 11, 558, 11, 457, 2049, 8416, 322, 729, 700, 82, 11, 558, 11, 382, 731], "temperature": 0.0, "avg_logprob": -0.28089459737141925, "compression_ratio": 1.5202702702702702, "no_speech_prob": 0.000144668752909638}, {"id": 422, "seek": 238460, "start": 2389.7599999999998, "end": 2395.24, "text": " as maybe taking care of outliers, right, is a good kind of practice, how then you deal", "tokens": [382, 1310, 1940, 1127, 295, 484, 23646, 11, 558, 11, 307, 257, 665, 733, 295, 3124, 11, 577, 550, 291, 2028], "temperature": 0.0, "avg_logprob": -0.28089459737141925, "compression_ratio": 1.5202702702702702, "no_speech_prob": 0.000144668752909638}, {"id": 423, "seek": 238460, "start": 2395.24, "end": 2400.44, "text": " with that information that you have, makes sense.", "tokens": [365, 300, 1589, 300, 291, 362, 11, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.28089459737141925, "compression_ratio": 1.5202702702702702, "no_speech_prob": 0.000144668752909638}, {"id": 424, "seek": 240044, "start": 2400.44, "end": 2417.88, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.4339937028430757, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.0015087452484294772}, {"id": 425, "seek": 240044, "start": 2417.88, "end": 2419.88, "text": " Hello, thank you for the talk.", "tokens": [2425, 11, 1309, 291, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.4339937028430757, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.0015087452484294772}, {"id": 426, "seek": 240044, "start": 2419.88, "end": 2426.48, "text": " What is the overhead of, to collect this statistic, because if you have, like, very, very much", "tokens": [708, 307, 264, 19922, 295, 11, 281, 2500, 341, 29588, 11, 570, 498, 291, 362, 11, 411, 11, 588, 11, 588, 709], "temperature": 0.0, "avg_logprob": -0.4339937028430757, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.0015087452484294772}, {"id": 427, "seek": 242648, "start": 2426.48, "end": 2434.04, "text": " of, that is a good question, right, of course there is, I would say it varies, right, typically", "tokens": [295, 11, 300, 307, 257, 665, 1168, 11, 558, 11, 295, 1164, 456, 307, 11, 286, 576, 584, 309, 21716, 11, 558, 11, 5850], "temperature": 0.0, "avg_logprob": -0.3145178765365758, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00011853012256324291}, {"id": 428, "seek": 242648, "start": 2434.04, "end": 2438.32, "text": " there is more overhead if you have like this, like a very simple fast queries, right, if", "tokens": [456, 307, 544, 19922, 498, 291, 362, 411, 341, 11, 411, 257, 588, 2199, 2370, 24109, 11, 558, 11, 498], "temperature": 0.0, "avg_logprob": -0.3145178765365758, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00011853012256324291}, {"id": 429, "seek": 242648, "start": 2438.32, "end": 2444.52, "text": " you have like a logic queries for, which takes, you know, many seconds for them, it's", "tokens": [291, 362, 411, 257, 9952, 24109, 337, 11, 597, 2516, 11, 291, 458, 11, 867, 3949, 337, 552, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.3145178765365758, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00011853012256324291}, {"id": 430, "seek": 242648, "start": 2444.52, "end": 2452.52, "text": " less like, our design goal, right, which we are able to get is being similar to PGSTAT", "tokens": [1570, 411, 11, 527, 1715, 3387, 11, 558, 11, 597, 321, 366, 1075, 281, 483, 307, 885, 2531, 281, 40975, 6840, 2218], "temperature": 0.0, "avg_logprob": -0.3145178765365758, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00011853012256324291}, {"id": 431, "seek": 245252, "start": 2452.52, "end": 2460.96, "text": " statements, right, and, you know, be a couple of percent or so, right, which I think in", "tokens": [12363, 11, 558, 11, 293, 11, 291, 458, 11, 312, 257, 1916, 295, 3043, 420, 370, 11, 558, 11, 597, 286, 519, 294], "temperature": 0.0, "avg_logprob": -0.18917327768662395, "compression_ratio": 1.6604938271604939, "no_speech_prob": 0.00014877135981805623}, {"id": 432, "seek": 245252, "start": 2460.96, "end": 2465.6, "text": " my opinion, right, many people when they think about that observability, you will tend to", "tokens": [452, 4800, 11, 558, 11, 867, 561, 562, 436, 519, 466, 300, 9951, 2310, 11, 291, 486, 3928, 281], "temperature": 0.0, "avg_logprob": -0.18917327768662395, "compression_ratio": 1.6604938271604939, "no_speech_prob": 0.00014877135981805623}, {"id": 433, "seek": 245252, "start": 2465.6, "end": 2473.6, "text": " obsess about the overhead, right, but really often having that insights, right, often allow", "tokens": [35803, 466, 264, 19922, 11, 558, 11, 457, 534, 2049, 1419, 300, 14310, 11, 558, 11, 2049, 2089], "temperature": 0.0, "avg_logprob": -0.18917327768662395, "compression_ratio": 1.6604938271604939, "no_speech_prob": 0.00014877135981805623}, {"id": 434, "seek": 247360, "start": 2473.6, "end": 2484.56, "text": " you to get so many things optimized when they matter, right, what the benefits are far outweighed.", "tokens": [291, 281, 483, 370, 867, 721, 26941, 562, 436, 1871, 11, 558, 11, 437, 264, 5311, 366, 1400, 484, 826, 910, 292, 13], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 435, "seek": 247360, "start": 2484.56, "end": 2490.92, "text": " Do you have any advice for catching bad queries before they reach production and kind of like", "tokens": [1144, 291, 362, 604, 5192, 337, 16124, 1578, 24109, 949, 436, 2524, 4265, 293, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 436, "seek": 247360, "start": 2490.92, "end": 2491.92, "text": " guarding these things?", "tokens": [44077, 613, 721, 30], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 437, "seek": 247360, "start": 2491.92, "end": 2492.92, "text": " Oh yeah, absolutely.", "tokens": [876, 1338, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 438, "seek": 247360, "start": 2492.92, "end": 2494.88, "text": " Like missing indexes or whatever, before they even.", "tokens": [1743, 5361, 8186, 279, 420, 2035, 11, 949, 436, 754, 13], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 439, "seek": 247360, "start": 2494.88, "end": 2498.36, "text": " That is a very good question, right, so I didn't talk about this, but it's also a question", "tokens": [663, 307, 257, 588, 665, 1168, 11, 558, 11, 370, 286, 994, 380, 751, 466, 341, 11, 457, 309, 311, 611, 257, 1168], "temperature": 0.0, "avg_logprob": -0.24861292494941004, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00017570241470821202}, {"id": 440, "seek": 249836, "start": 2498.36, "end": 2506.56, "text": " where, right, in my opinion, and I think that's also what is very helpful with the open source", "tokens": [689, 11, 558, 11, 294, 452, 4800, 11, 293, 286, 519, 300, 311, 611, 437, 307, 588, 4961, 365, 264, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.18409361588327508, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.812763002817519e-05}, {"id": 441, "seek": 249836, "start": 2506.56, "end": 2512.6, "text": " solution, right, what you can really deploy it everywhere in, including your kind of CI,", "tokens": [3827, 11, 558, 11, 437, 291, 393, 534, 7274, 309, 5315, 294, 11, 3009, 428, 733, 295, 37777, 11], "temperature": 0.0, "avg_logprob": -0.18409361588327508, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.812763002817519e-05}, {"id": 442, "seek": 249836, "start": 2512.6, "end": 2517.28, "text": " CD environment, right, because what I often see people saying, well, you know what, data", "tokens": [6743, 2823, 11, 558, 11, 570, 437, 286, 2049, 536, 561, 1566, 11, 731, 11, 291, 458, 437, 11, 1412], "temperature": 0.0, "avg_logprob": -0.18409361588327508, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.812763002817519e-05}, {"id": 443, "seek": 249836, "start": 2517.28, "end": 2524.1600000000003, "text": " dog, right, is expensive, it's only in production, right, what you want to do is make sure you", "tokens": [3000, 11, 558, 11, 307, 5124, 11, 309, 311, 787, 294, 4265, 11, 558, 11, 437, 291, 528, 281, 360, 307, 652, 988, 291], "temperature": 0.0, "avg_logprob": -0.18409361588327508, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.812763002817519e-05}, {"id": 444, "seek": 252416, "start": 2524.16, "end": 2531.8799999999997, "text": " have solutions in development so you can catch bad queries before they hit in production,", "tokens": [362, 6547, 294, 3250, 370, 291, 393, 3745, 1578, 24109, 949, 436, 2045, 294, 4265, 11], "temperature": 0.0, "avg_logprob": -0.15798248563494002, "compression_ratio": 1.7438423645320198, "no_speech_prob": 8.910134056350216e-06}, {"id": 445, "seek": 252416, "start": 2531.8799999999997, "end": 2536.2799999999997, "text": " but also assume you're not going to catch all the bad queries, right, some queries", "tokens": [457, 611, 6552, 291, 434, 406, 516, 281, 3745, 439, 264, 1578, 24109, 11, 558, 11, 512, 24109], "temperature": 0.0, "avg_logprob": -0.15798248563494002, "compression_ratio": 1.7438423645320198, "no_speech_prob": 8.910134056350216e-06}, {"id": 446, "seek": 252416, "start": 2536.2799999999997, "end": 2542.3999999999996, "text": " will only maybe misbehave in production, right, the other good practice which comes to that", "tokens": [486, 787, 1310, 3346, 650, 24284, 294, 4265, 11, 558, 11, 264, 661, 665, 3124, 597, 1487, 281, 300], "temperature": 0.0, "avg_logprob": -0.15798248563494002, "compression_ratio": 1.7438423645320198, "no_speech_prob": 8.910134056350216e-06}, {"id": 447, "seek": 252416, "start": 2542.3999999999996, "end": 2547.68, "text": " is you make sure you're like a test environment is good, right, so you can test a variety", "tokens": [307, 291, 652, 988, 291, 434, 411, 257, 1500, 2823, 307, 665, 11, 558, 11, 370, 291, 393, 1500, 257, 5673], "temperature": 0.0, "avg_logprob": -0.15798248563494002, "compression_ratio": 1.7438423645320198, "no_speech_prob": 8.910134056350216e-06}, {"id": 448, "seek": 254768, "start": 2547.68, "end": 2554.8399999999997, "text": " of queries relevant to your application and you have a good data set, right, for that.", "tokens": [295, 24109, 7340, 281, 428, 3861, 293, 291, 362, 257, 665, 1412, 992, 11, 558, 11, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.2621928944307215, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.872059020679444e-05}, {"id": 449, "seek": 254768, "start": 2554.8399999999997, "end": 2560.3599999999997, "text": " I think in this regard, there is like some cool features coming out from Neon, for example,", "tokens": [286, 519, 294, 341, 3843, 11, 456, 307, 411, 512, 1627, 4122, 1348, 484, 490, 1734, 266, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.2621928944307215, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.872059020679444e-05}, {"id": 450, "seek": 254768, "start": 2560.3599999999997, "end": 2565.12, "text": " like giving like branches, branching, right, then you can get like, oh, the full copy of", "tokens": [411, 2902, 411, 14770, 11, 9819, 278, 11, 558, 11, 550, 291, 393, 483, 411, 11, 1954, 11, 264, 1577, 5055, 295], "temperature": 0.0, "avg_logprob": -0.2621928944307215, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.872059020679444e-05}, {"id": 451, "seek": 254768, "start": 2565.12, "end": 2571.2, "text": " production database, you know, mess with it, run tests on it on a full-size data set, right,", "tokens": [4265, 8149, 11, 291, 458, 11, 2082, 365, 309, 11, 1190, 6921, 322, 309, 322, 257, 1577, 12, 27553, 1412, 992, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.2621928944307215, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.872059020679444e-05}, {"id": 452, "seek": 254768, "start": 2571.2, "end": 2576.08, "text": " instead of testing on, you know, table with 100 rows, right, which is kind of useless.", "tokens": [2602, 295, 4997, 322, 11, 291, 458, 11, 3199, 365, 2319, 13241, 11, 558, 11, 597, 307, 733, 295, 14115, 13], "temperature": 0.0, "avg_logprob": -0.2621928944307215, "compression_ratio": 1.7126436781609196, "no_speech_prob": 6.872059020679444e-05}, {"id": 453, "seek": 257608, "start": 2576.08, "end": 2583.08, "text": " Cool. Any other question? Okay, thank you very much.", "tokens": [8561, 13, 2639, 661, 1168, 30, 1033, 11, 1309, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.5025744438171387, "compression_ratio": 1.2121212121212122, "no_speech_prob": 0.00252127880230546}, {"id": 454, "seek": 257608, "start": 2583.08, "end": 2584.08, "text": " Okay, thank you.", "tokens": [1033, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.5025744438171387, "compression_ratio": 1.2121212121212122, "no_speech_prob": 0.00252127880230546}, {"id": 455, "seek": 258408, "start": 2584.08, "end": 2610.08, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 51664], "temperature": 0.0, "avg_logprob": -0.8086793422698975, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.000538520747795701}], "language": "en"}