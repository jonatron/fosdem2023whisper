{"text": " Hi, everyone. Well, I'm very impressed to have such a large audience for such a small tool. But, well, I'm Beatrice. I work at the French Media Lab. And today I'm going to present PIMI, which is a tool to study image propagation. The Media Lab is a lab where social scientists try to, among other things, study the traces that people leave online. And for now, they are quite well equipped with tools to study text. But when they ask me, OK, how can I study mean propagation? I'm still struggling to give them answers. So what does it mean to study mean propagation? It means, OK, being able to recognize that some parts of an image are copied or partially copied. So what this tool does, it's very simple. It's able to create clusters of images and group together images that are total or partial copies of each other. It's able to deal with image transformation, so if the image is cropped or zoomed. And it's able to adapt to copper's characteristics. So it will try to make the best of your data sets, depending on the number of images you have or the type of images you have. What PIMI is not able to do is to cluster semantically similar images. So it's not the tool that you are going to use if you want to create clusters of cats and clusters of dogs or, I don't know, find images of violence versus images of peace. And it's not able to do some face recognition. So again, you will not be able to make some clusters of pictures of Elizabeth II versus clusters of images of Emmanuel Macron. What you could imagine doing, and we could imagine also work together if you are a researcher working on those subjects, is to study the propagation of MIMA on social networks, as I was saying. But also you could study the usage of the press agency photos in a press corpus or stock photos as well. You could also study the dissemination of fake news based on image montage. Or you could study the editorial choices between different media, depending on whether they use the same images or not. So let me do a quick demo of how it looks for now. It's not on the screen. Okay, let's forget about that. I'm very sorry. Well, I'll try to make it work. Okay, well, it's still not showing totally all clusters. So we create clusters of images. So this is a data set that is created by the French Inria and that is presenting some degradation on images. So they take an original picture and they apply some filters or they crop the images to see if they are able to group the images together. So we can see that we have pretty correct results on that data set. And this is our results on some images that we collected ourselves on Twitter using Elon Musk as a query. And so we try to clusters those images. So as you can see, we have images of Elon Musk. We are able to group together some images that are crops of others. So this is probably the source image of the montage that has been done here. But we can also see that we have some problems with the tool. For example, here we have a cluster with two images that have been assembled together and we create a cluster of actually two images. But well, that's the state of the tool for now. And now I try to come back to my slides. Okay, so how does it work? For people who work in computer vision, I'm going probably to say some things that are quite basic, but I'll try to make it clear for people who do not do computer vision. So it is not based on colors at all. It's used like the grayscale of images. And it tries to detect points of interest on a picture. And then it uses these local key points as vectors. And then those vectors are indexed in a database that is able to perform some very quick similarity search. of the tool. As I say, there is that problem of parts of images that create clusters that are bigger than they should be. So our plan is to be able to detect images that are actually those links between two clusters. So to be able to detect that this image is actually containing two images and to be able to deal with part of images. And also what we would like to do is to show images in their context, to be able to show the tweets that contains those images or Instagram posts, et cetera. Or at least to show additional metadata for the users. And also we would like to show you the graph of image similarities so that the clusters that are resulting from that graph are not interpretable. And to improve our tool, we need your use cases because for now we have those two, three databases. But we would be very glad to do some partnerships with other researchers to improve the tool. Thank you very much for your attention. If you want to look at the slides, we have the references to all the images used and to the papers of the algorithms used by Pini. I'm open for questions. We had a bit of trouble with the sound stream, but it's back on now. So yeah, you should repeat the question. Okay, I'll do. Yes. So thank you very much for that. We'll try to find similarities. Oh, sorry. I have to repeat the question. So the question was, if I understand well, how to reproduce that use case, not on images, but on other types of documents that would be, I guess, some features. 3D counterparts. And I'd say, well, as long as you can, like, represent your data in the shape of vectors, then you're ready to use face to, like, do some, some search for nearest neighbor in your database. And then you can go for the whole pipeline, create some graphs, find communities in the graph, and go for it. But I'm not sure Pini is your tool, but, but, well, the architecture of Pini could be, of course, a model. Yes. Is there any project current or you're completely ongoing that Media Labs has used before, or is it still largely in development? It is largely in the development. Sorry, I repeat the question. So are there some projects at the Media Lab that are currently using Pini? And the response is no. Yes. Sorry, can you consider any other ways that can be considered? Yes. Have you considered other ways of presenting picture similarity or using picture similarity, or the types of image similarity, if I'm here in the Sunwell? Well, I'd say that that was what I was saying in my second slide. There are other types of image similarity, for example, semantical similarity. And, well, maybe in a few months, if we have like a robust architecture, we could maybe include some other types of vectorization of images. But for now, well, there are already tools that do that. Like, there is something called Clip Server that helps you find similar images from clip vectors that are like semantical vectors. So you could use that tool. It's great. Yes. Yes. So the question is, is the tool really able to distinguish the thing that is of interest to us, the fact that we are talking about a dog? So the tool is only able to find partial copies in an image. So the tool would probably be able to say that all those images contain the same parts of face of a dog. So it would probably be able to group all those images together. The problem is that if there are other images in the database that contain the rest of the images, then they would probably also be grouped in the same cluster. So that's why what we are currently doing about parts of images would let us improve the cluster so that it's purified from the rest of the images. And we could have a cluster of the face of that specific dog and then a cluster of that taco in the second cluster. Yes. What kind of clusterization do you use on the graph? Well, for now, we have the best result with, excuse me, what kind of clusterization do you use on the graph? For now, we have our best result using pure connected components. So actually, the specification we do on the graph to reduce the number of links between images is enough to have separated connected components in the graph. And so we take each connected component and it's our cluster. What we would like to do is to try to mix with some Luvain community detection, but actually for now, it's not the thing that works best. Yes. I'm not sure I understand the question. Can you try to rephrase it? Okay. What things are you looking at to improve the model? Well, there are many things we are looking at. For now, mainly, we look at techniques to do a better graph specification in order to find more coherent clusters. We are not so much working on the local descriptors part of the tool for now. Have you considered using the direct link to the Twitter images or social media images online? Did I repeat everything? Well, yes. We would like people to be able to see images in their context because, actually, they won't understand what's happening if they just have images. They need to see, okay, why was this image published? Who answered, et cetera. This would probably mean that we need to add at least the links to the pulse or maybe some kind of visualization of it. We have a bit of time here. Any more questions? We can take one or two. If not, we can switch quietly. All the next questions. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.64, "text": " Hi, everyone. Well, I'm very impressed to have such a large audience for such a small", "tokens": [2421, 11, 1518, 13, 1042, 11, 286, 478, 588, 11679, 281, 362, 1270, 257, 2416, 4034, 337, 1270, 257, 1359], "temperature": 0.0, "avg_logprob": -0.25738643955540014, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.26503998041152954}, {"id": 1, "seek": 0, "start": 13.64, "end": 21.080000000000002, "text": " tool. But, well, I'm Beatrice. I work at the French Media Lab. And today I'm going to", "tokens": [2290, 13, 583, 11, 731, 11, 286, 478, 16031, 21299, 13, 286, 589, 412, 264, 5522, 14741, 10137, 13, 400, 965, 286, 478, 516, 281], "temperature": 0.0, "avg_logprob": -0.25738643955540014, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.26503998041152954}, {"id": 2, "seek": 0, "start": 21.080000000000002, "end": 29.64, "text": " present PIMI, which is a tool to study image propagation. The Media Lab is a lab where", "tokens": [1974, 430, 6324, 40, 11, 597, 307, 257, 2290, 281, 2979, 3256, 38377, 13, 440, 14741, 10137, 307, 257, 2715, 689], "temperature": 0.0, "avg_logprob": -0.25738643955540014, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.26503998041152954}, {"id": 3, "seek": 2964, "start": 29.64, "end": 39.52, "text": " social scientists try to, among other things, study the traces that people leave online.", "tokens": [2093, 7708, 853, 281, 11, 3654, 661, 721, 11, 2979, 264, 26076, 300, 561, 1856, 2950, 13], "temperature": 0.0, "avg_logprob": -0.18035758109319777, "compression_ratio": 1.359375, "no_speech_prob": 0.000136363145429641}, {"id": 4, "seek": 2964, "start": 39.52, "end": 46.88, "text": " And for now, they are quite well equipped with tools to study text. But when they ask", "tokens": [400, 337, 586, 11, 436, 366, 1596, 731, 15218, 365, 3873, 281, 2979, 2487, 13, 583, 562, 436, 1029], "temperature": 0.0, "avg_logprob": -0.18035758109319777, "compression_ratio": 1.359375, "no_speech_prob": 0.000136363145429641}, {"id": 5, "seek": 4688, "start": 46.88, "end": 59.92, "text": " me, OK, how can I study mean propagation? I'm still struggling to give them answers.", "tokens": [385, 11, 2264, 11, 577, 393, 286, 2979, 914, 38377, 30, 286, 478, 920, 9314, 281, 976, 552, 6338, 13], "temperature": 0.0, "avg_logprob": -0.13712264509762034, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.00014361705689225346}, {"id": 6, "seek": 4688, "start": 59.92, "end": 65.72, "text": " So what does it mean to study mean propagation? It means, OK, being able to recognize that", "tokens": [407, 437, 775, 309, 914, 281, 2979, 914, 38377, 30, 467, 1355, 11, 2264, 11, 885, 1075, 281, 5521, 300], "temperature": 0.0, "avg_logprob": -0.13712264509762034, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.00014361705689225346}, {"id": 7, "seek": 4688, "start": 65.72, "end": 76.24000000000001, "text": " some parts of an image are copied or partially copied. So what this tool does, it's very", "tokens": [512, 3166, 295, 364, 3256, 366, 25365, 420, 18886, 25365, 13, 407, 437, 341, 2290, 775, 11, 309, 311, 588], "temperature": 0.0, "avg_logprob": -0.13712264509762034, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.00014361705689225346}, {"id": 8, "seek": 7624, "start": 76.24, "end": 85.75999999999999, "text": " simple. It's able to create clusters of images and group together images that are total or", "tokens": [2199, 13, 467, 311, 1075, 281, 1884, 23313, 295, 5267, 293, 1594, 1214, 5267, 300, 366, 3217, 420], "temperature": 0.0, "avg_logprob": -0.15540512309354895, "compression_ratio": 1.6303317535545023, "no_speech_prob": 3.1389325158670545e-05}, {"id": 9, "seek": 7624, "start": 85.75999999999999, "end": 91.67999999999999, "text": " partial copies of each other. It's able to deal with image transformation, so if the", "tokens": [14641, 14341, 295, 1184, 661, 13, 467, 311, 1075, 281, 2028, 365, 3256, 9887, 11, 370, 498, 264], "temperature": 0.0, "avg_logprob": -0.15540512309354895, "compression_ratio": 1.6303317535545023, "no_speech_prob": 3.1389325158670545e-05}, {"id": 10, "seek": 7624, "start": 91.67999999999999, "end": 100.8, "text": " image is cropped or zoomed. And it's able to adapt to copper's characteristics. So", "tokens": [3256, 307, 4848, 3320, 420, 8863, 292, 13, 400, 309, 311, 1075, 281, 6231, 281, 15007, 311, 10891, 13, 407], "temperature": 0.0, "avg_logprob": -0.15540512309354895, "compression_ratio": 1.6303317535545023, "no_speech_prob": 3.1389325158670545e-05}, {"id": 11, "seek": 7624, "start": 100.8, "end": 105.52, "text": " it will try to make the best of your data sets, depending on the number of images you", "tokens": [309, 486, 853, 281, 652, 264, 1151, 295, 428, 1412, 6352, 11, 5413, 322, 264, 1230, 295, 5267, 291], "temperature": 0.0, "avg_logprob": -0.15540512309354895, "compression_ratio": 1.6303317535545023, "no_speech_prob": 3.1389325158670545e-05}, {"id": 12, "seek": 10552, "start": 105.52, "end": 115.6, "text": " have or the type of images you have. What PIMI is not able to do is to cluster semantically", "tokens": [362, 420, 264, 2010, 295, 5267, 291, 362, 13, 708, 430, 6324, 40, 307, 406, 1075, 281, 360, 307, 281, 13630, 4361, 49505], "temperature": 0.0, "avg_logprob": -0.12851721899850027, "compression_ratio": 1.5202312138728324, "no_speech_prob": 7.637390808667988e-05}, {"id": 13, "seek": 10552, "start": 115.6, "end": 122.16, "text": " similar images. So it's not the tool that you are going to use if you want to create", "tokens": [2531, 5267, 13, 407, 309, 311, 406, 264, 2290, 300, 291, 366, 516, 281, 764, 498, 291, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.12851721899850027, "compression_ratio": 1.5202312138728324, "no_speech_prob": 7.637390808667988e-05}, {"id": 14, "seek": 10552, "start": 122.16, "end": 129.84, "text": " clusters of cats and clusters of dogs or, I don't know, find images of violence versus", "tokens": [23313, 295, 11111, 293, 23313, 295, 7197, 420, 11, 286, 500, 380, 458, 11, 915, 5267, 295, 6270, 5717], "temperature": 0.0, "avg_logprob": -0.12851721899850027, "compression_ratio": 1.5202312138728324, "no_speech_prob": 7.637390808667988e-05}, {"id": 15, "seek": 12984, "start": 129.84, "end": 138.96, "text": " images of peace. And it's not able to do some face recognition. So again, you will not", "tokens": [5267, 295, 4336, 13, 400, 309, 311, 406, 1075, 281, 360, 512, 1851, 11150, 13, 407, 797, 11, 291, 486, 406], "temperature": 0.0, "avg_logprob": -0.1290010270618257, "compression_ratio": 1.3821138211382114, "no_speech_prob": 0.00020428025163710117}, {"id": 16, "seek": 12984, "start": 138.96, "end": 146.96, "text": " be able to make some clusters of pictures of Elizabeth II versus clusters of images", "tokens": [312, 1075, 281, 652, 512, 23313, 295, 5242, 295, 12978, 6351, 5717, 23313, 295, 5267], "temperature": 0.0, "avg_logprob": -0.1290010270618257, "compression_ratio": 1.3821138211382114, "no_speech_prob": 0.00020428025163710117}, {"id": 17, "seek": 14696, "start": 146.96, "end": 160.8, "text": " of Emmanuel Macron. What you could imagine doing, and we could imagine also work together", "tokens": [295, 44421, 32806, 13, 708, 291, 727, 3811, 884, 11, 293, 321, 727, 3811, 611, 589, 1214], "temperature": 0.0, "avg_logprob": -0.21516584249643178, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.00020377883629407734}, {"id": 18, "seek": 14696, "start": 160.8, "end": 169.96, "text": " if you are a researcher working on those subjects, is to study the propagation of MIMA on social", "tokens": [498, 291, 366, 257, 21751, 1364, 322, 729, 13066, 11, 307, 281, 2979, 264, 38377, 295, 376, 6324, 32, 322, 2093], "temperature": 0.0, "avg_logprob": -0.21516584249643178, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.00020377883629407734}, {"id": 19, "seek": 14696, "start": 169.96, "end": 176.92000000000002, "text": " networks, as I was saying. But also you could study the usage of the press agency photos", "tokens": [9590, 11, 382, 286, 390, 1566, 13, 583, 611, 291, 727, 2979, 264, 14924, 295, 264, 1886, 7934, 5787], "temperature": 0.0, "avg_logprob": -0.21516584249643178, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.00020377883629407734}, {"id": 20, "seek": 17692, "start": 176.92, "end": 183.95999999999998, "text": " in a press corpus or stock photos as well. You could also study the dissemination of", "tokens": [294, 257, 1886, 1181, 31624, 420, 4127, 5787, 382, 731, 13, 509, 727, 611, 2979, 264, 34585, 399, 295], "temperature": 0.0, "avg_logprob": -0.13912104070186615, "compression_ratio": 1.5, "no_speech_prob": 0.00015331590839195997}, {"id": 21, "seek": 17692, "start": 183.95999999999998, "end": 193.0, "text": " fake news based on image montage. Or you could study the editorial choices between different", "tokens": [7592, 2583, 2361, 322, 3256, 40184, 13, 1610, 291, 727, 2979, 264, 33412, 7994, 1296, 819], "temperature": 0.0, "avg_logprob": -0.13912104070186615, "compression_ratio": 1.5, "no_speech_prob": 0.00015331590839195997}, {"id": 22, "seek": 17692, "start": 193.0, "end": 203.6, "text": " media, depending on whether they use the same images or not. So let me do a quick demo of", "tokens": [3021, 11, 5413, 322, 1968, 436, 764, 264, 912, 5267, 420, 406, 13, 407, 718, 385, 360, 257, 1702, 10723, 295], "temperature": 0.0, "avg_logprob": -0.13912104070186615, "compression_ratio": 1.5, "no_speech_prob": 0.00015331590839195997}, {"id": 23, "seek": 20360, "start": 203.6, "end": 228.6, "text": " how it looks for now. It's not on the screen. Okay, let's forget about that.", "tokens": [577, 309, 1542, 337, 586, 13, 467, 311, 406, 322, 264, 2568, 13, 1033, 11, 718, 311, 2870, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.28123233795166014, "compression_ratio": 1.0, "no_speech_prob": 0.001424317597411573}, {"id": 24, "seek": 22860, "start": 228.6, "end": 257.6, "text": " I'm very sorry.", "tokens": [286, 478, 588, 2597, 13], "temperature": 0.0, "avg_logprob": -0.7068202230665419, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.0026909513399004936}, {"id": 25, "seek": 25760, "start": 257.6, "end": 273.6, "text": " Well, I'll try to make it work. Okay, well, it's still not showing totally all clusters.", "tokens": [1042, 11, 286, 603, 853, 281, 652, 309, 589, 13, 1033, 11, 731, 11, 309, 311, 920, 406, 4099, 3879, 439, 23313, 13], "temperature": 0.0, "avg_logprob": -0.2366467515627543, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.002185956807807088}, {"id": 26, "seek": 25760, "start": 273.6, "end": 280.24, "text": " So we create clusters of images. So this is a data set that is created by the French", "tokens": [407, 321, 1884, 23313, 295, 5267, 13, 407, 341, 307, 257, 1412, 992, 300, 307, 2942, 538, 264, 5522], "temperature": 0.0, "avg_logprob": -0.2366467515627543, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.002185956807807088}, {"id": 27, "seek": 28024, "start": 280.24, "end": 289.64, "text": " Inria and that is presenting some degradation on images. So they take an original picture", "tokens": [682, 4668, 293, 300, 307, 15578, 512, 40519, 322, 5267, 13, 407, 436, 747, 364, 3380, 3036], "temperature": 0.0, "avg_logprob": -0.17161487397693453, "compression_ratio": 1.598802395209581, "no_speech_prob": 7.856692536734045e-05}, {"id": 28, "seek": 28024, "start": 289.64, "end": 296.0, "text": " and they apply some filters or they crop the images to see if they are able to group the", "tokens": [293, 436, 3079, 512, 15995, 420, 436, 9086, 264, 5267, 281, 536, 498, 436, 366, 1075, 281, 1594, 264], "temperature": 0.0, "avg_logprob": -0.17161487397693453, "compression_ratio": 1.598802395209581, "no_speech_prob": 7.856692536734045e-05}, {"id": 29, "seek": 28024, "start": 296.0, "end": 303.56, "text": " images together. So we can see that we have pretty correct results on that data set. And", "tokens": [5267, 1214, 13, 407, 321, 393, 536, 300, 321, 362, 1238, 3006, 3542, 322, 300, 1412, 992, 13, 400], "temperature": 0.0, "avg_logprob": -0.17161487397693453, "compression_ratio": 1.598802395209581, "no_speech_prob": 7.856692536734045e-05}, {"id": 30, "seek": 30356, "start": 303.56, "end": 310.88, "text": " this is our results on some images that we collected ourselves on Twitter using Elon", "tokens": [341, 307, 527, 3542, 322, 512, 5267, 300, 321, 11087, 4175, 322, 5794, 1228, 28498], "temperature": 0.0, "avg_logprob": -0.11087635069182425, "compression_ratio": 1.5497076023391814, "no_speech_prob": 8.293289283756167e-05}, {"id": 31, "seek": 30356, "start": 310.88, "end": 320.12, "text": " Musk as a query. And so we try to clusters those images. So as you can see, we have images", "tokens": [26019, 382, 257, 14581, 13, 400, 370, 321, 853, 281, 23313, 729, 5267, 13, 407, 382, 291, 393, 536, 11, 321, 362, 5267], "temperature": 0.0, "avg_logprob": -0.11087635069182425, "compression_ratio": 1.5497076023391814, "no_speech_prob": 8.293289283756167e-05}, {"id": 32, "seek": 30356, "start": 320.12, "end": 330.52, "text": " of Elon Musk. We are able to group together some images that are crops of others. So this", "tokens": [295, 28498, 26019, 13, 492, 366, 1075, 281, 1594, 1214, 512, 5267, 300, 366, 16829, 295, 2357, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.11087635069182425, "compression_ratio": 1.5497076023391814, "no_speech_prob": 8.293289283756167e-05}, {"id": 33, "seek": 33052, "start": 330.52, "end": 338.68, "text": " is probably the source image of the montage that has been done here. But we can also see", "tokens": [307, 1391, 264, 4009, 3256, 295, 264, 40184, 300, 575, 668, 1096, 510, 13, 583, 321, 393, 611, 536], "temperature": 0.0, "avg_logprob": -0.12114554066811839, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00010647261660778895}, {"id": 34, "seek": 33052, "start": 338.68, "end": 346.56, "text": " that we have some problems with the tool. For example, here we have a cluster with two", "tokens": [300, 321, 362, 512, 2740, 365, 264, 2290, 13, 1171, 1365, 11, 510, 321, 362, 257, 13630, 365, 732], "temperature": 0.0, "avg_logprob": -0.12114554066811839, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00010647261660778895}, {"id": 35, "seek": 33052, "start": 346.56, "end": 355.44, "text": " images that have been assembled together and we create a cluster of actually two images.", "tokens": [5267, 300, 362, 668, 24204, 1214, 293, 321, 1884, 257, 13630, 295, 767, 732, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12114554066811839, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00010647261660778895}, {"id": 36, "seek": 35544, "start": 355.44, "end": 385.4, "text": " But well, that's the state of the tool for now. And now I try to come back to my slides.", "tokens": [583, 731, 11, 300, 311, 264, 1785, 295, 264, 2290, 337, 586, 13, 400, 586, 286, 853, 281, 808, 646, 281, 452, 9788, 13], "temperature": 0.0, "avg_logprob": -0.18818536826542445, "compression_ratio": 1.0731707317073171, "no_speech_prob": 0.0006321725668385625}, {"id": 37, "seek": 38540, "start": 385.4, "end": 393.2, "text": " Okay, so how does it work? For people who work in computer vision, I'm going probably", "tokens": [1033, 11, 370, 577, 775, 309, 589, 30, 1171, 561, 567, 589, 294, 3820, 5201, 11, 286, 478, 516, 1391], "temperature": 0.0, "avg_logprob": -0.14288957913716635, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0003039244038518518}, {"id": 38, "seek": 38540, "start": 393.2, "end": 400.15999999999997, "text": " to say some things that are quite basic, but I'll try to make it clear for people who", "tokens": [281, 584, 512, 721, 300, 366, 1596, 3875, 11, 457, 286, 603, 853, 281, 652, 309, 1850, 337, 561, 567], "temperature": 0.0, "avg_logprob": -0.14288957913716635, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0003039244038518518}, {"id": 39, "seek": 38540, "start": 400.15999999999997, "end": 407.44, "text": " do not do computer vision. So it is not based on colors at all. It's used like the grayscale", "tokens": [360, 406, 360, 3820, 5201, 13, 407, 309, 307, 406, 2361, 322, 4577, 412, 439, 13, 467, 311, 1143, 411, 264, 677, 3772, 37088], "temperature": 0.0, "avg_logprob": -0.14288957913716635, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0003039244038518518}, {"id": 40, "seek": 40744, "start": 407.44, "end": 418.36, "text": " of images. And it tries to detect points of interest on a picture. And then it uses these", "tokens": [295, 5267, 13, 400, 309, 9898, 281, 5531, 2793, 295, 1179, 322, 257, 3036, 13, 400, 550, 309, 4960, 613], "temperature": 0.0, "avg_logprob": -0.20767498016357422, "compression_ratio": 1.4830508474576272, "no_speech_prob": 8.875109779182822e-05}, {"id": 41, "seek": 40744, "start": 418.36, "end": 430.68, "text": " local key points as vectors. And then those vectors are indexed in a database that is", "tokens": [2654, 2141, 2793, 382, 18875, 13, 400, 550, 729, 18875, 366, 8186, 292, 294, 257, 8149, 300, 307], "temperature": 0.0, "avg_logprob": -0.20767498016357422, "compression_ratio": 1.4830508474576272, "no_speech_prob": 8.875109779182822e-05}, {"id": 42, "seek": 43068, "start": 430.68, "end": 441.12, "text": " able to perform some very quick similarity search.", "tokens": [50364, 1075, 281, 2042, 512, 588, 1702, 32194, 3164, 13, 50886], "temperature": 0.0, "avg_logprob": -0.2833568255106608, "compression_ratio": 0.8771929824561403, "no_speech_prob": 0.00018321255629416555}, {"id": 43, "seek": 79068, "start": 790.68, "end": 817.4799999999999, "text": " of the tool. As I say, there is that problem of parts of images that create clusters that", "tokens": [295, 264, 2290, 13, 1018, 286, 584, 11, 456, 307, 300, 1154, 295, 3166, 295, 5267, 300, 1884, 23313, 300], "temperature": 0.0, "avg_logprob": -0.25625338157018024, "compression_ratio": 1.1265822784810127, "no_speech_prob": 0.20295029878616333}, {"id": 44, "seek": 81748, "start": 817.48, "end": 824.88, "text": " are bigger than they should be. So our plan is to be able to detect images that are actually", "tokens": [366, 3801, 813, 436, 820, 312, 13, 407, 527, 1393, 307, 281, 312, 1075, 281, 5531, 5267, 300, 366, 767], "temperature": 0.0, "avg_logprob": -0.0819706842303276, "compression_ratio": 1.7006369426751593, "no_speech_prob": 0.00027449068147689104}, {"id": 45, "seek": 81748, "start": 824.88, "end": 830.88, "text": " those links between two clusters. So to be able to detect that this image is actually", "tokens": [729, 6123, 1296, 732, 23313, 13, 407, 281, 312, 1075, 281, 5531, 300, 341, 3256, 307, 767], "temperature": 0.0, "avg_logprob": -0.0819706842303276, "compression_ratio": 1.7006369426751593, "no_speech_prob": 0.00027449068147689104}, {"id": 46, "seek": 81748, "start": 830.88, "end": 840.72, "text": " containing two images and to be able to deal with part of images. And also what we would", "tokens": [19273, 732, 5267, 293, 281, 312, 1075, 281, 2028, 365, 644, 295, 5267, 13, 400, 611, 437, 321, 576], "temperature": 0.0, "avg_logprob": -0.0819706842303276, "compression_ratio": 1.7006369426751593, "no_speech_prob": 0.00027449068147689104}, {"id": 47, "seek": 84072, "start": 840.72, "end": 848.32, "text": " like to do is to show images in their context, to be able to show the tweets that contains", "tokens": [411, 281, 360, 307, 281, 855, 5267, 294, 641, 4319, 11, 281, 312, 1075, 281, 855, 264, 25671, 300, 8306], "temperature": 0.0, "avg_logprob": -0.15442159440782335, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0383078410523012e-05}, {"id": 48, "seek": 84072, "start": 848.32, "end": 855.44, "text": " those images or Instagram posts, et cetera. Or at least to show additional metadata for", "tokens": [729, 5267, 420, 5281, 12300, 11, 1030, 11458, 13, 1610, 412, 1935, 281, 855, 4497, 26603, 337], "temperature": 0.0, "avg_logprob": -0.15442159440782335, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0383078410523012e-05}, {"id": 49, "seek": 84072, "start": 855.44, "end": 864.2, "text": " the users. And also we would like to show you the graph of image similarities so that", "tokens": [264, 5022, 13, 400, 611, 321, 576, 411, 281, 855, 291, 264, 4295, 295, 3256, 24197, 370, 300], "temperature": 0.0, "avg_logprob": -0.15442159440782335, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0383078410523012e-05}, {"id": 50, "seek": 86420, "start": 864.2, "end": 875.72, "text": " the clusters that are resulting from that graph are not interpretable. And to improve", "tokens": [264, 23313, 300, 366, 16505, 490, 300, 4295, 366, 406, 7302, 712, 13, 400, 281, 3470], "temperature": 0.0, "avg_logprob": -0.21514810834612166, "compression_ratio": 1.380952380952381, "no_speech_prob": 7.104992255335674e-05}, {"id": 51, "seek": 86420, "start": 875.72, "end": 889.5600000000001, "text": " our tool, we need your use cases because for now we have those two, three databases. But", "tokens": [527, 2290, 11, 321, 643, 428, 764, 3331, 570, 337, 586, 321, 362, 729, 732, 11, 1045, 22380, 13, 583], "temperature": 0.0, "avg_logprob": -0.21514810834612166, "compression_ratio": 1.380952380952381, "no_speech_prob": 7.104992255335674e-05}, {"id": 52, "seek": 88956, "start": 889.56, "end": 897.68, "text": " we would be very glad to do some partnerships with other researchers to improve the tool.", "tokens": [321, 576, 312, 588, 5404, 281, 360, 512, 18245, 365, 661, 10309, 281, 3470, 264, 2290, 13], "temperature": 0.0, "avg_logprob": -0.14684793032132662, "compression_ratio": 1.5085714285714287, "no_speech_prob": 6.1010669014649466e-05}, {"id": 53, "seek": 88956, "start": 897.68, "end": 904.9599999999999, "text": " Thank you very much for your attention. If you want to look at the slides, we have the", "tokens": [1044, 291, 588, 709, 337, 428, 3202, 13, 759, 291, 528, 281, 574, 412, 264, 9788, 11, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.14684793032132662, "compression_ratio": 1.5085714285714287, "no_speech_prob": 6.1010669014649466e-05}, {"id": 54, "seek": 88956, "start": 904.9599999999999, "end": 913.88, "text": " references to all the images used and to the papers of the algorithms used by Pini. I'm", "tokens": [15400, 281, 439, 264, 5267, 1143, 293, 281, 264, 10577, 295, 264, 14642, 1143, 538, 430, 3812, 13, 286, 478], "temperature": 0.0, "avg_logprob": -0.14684793032132662, "compression_ratio": 1.5085714285714287, "no_speech_prob": 6.1010669014649466e-05}, {"id": 55, "seek": 91388, "start": 913.88, "end": 927.56, "text": " open for questions. We had a bit of trouble with the sound stream, but it's back on now.", "tokens": [1269, 337, 1651, 13, 492, 632, 257, 857, 295, 5253, 365, 264, 1626, 4309, 11, 457, 309, 311, 646, 322, 586, 13], "temperature": 0.0, "avg_logprob": -0.43157726287841797, "compression_ratio": 1.2932330827067668, "no_speech_prob": 0.0005459777312353253}, {"id": 56, "seek": 91388, "start": 927.56, "end": 941.56, "text": " So yeah, you should repeat the question. Okay, I'll do. Yes. So thank you very much", "tokens": [407, 1338, 11, 291, 820, 7149, 264, 1168, 13, 1033, 11, 286, 603, 360, 13, 1079, 13, 407, 1309, 291, 588, 709], "temperature": 0.0, "avg_logprob": -0.43157726287841797, "compression_ratio": 1.2932330827067668, "no_speech_prob": 0.0005459777312353253}, {"id": 57, "seek": 94156, "start": 941.56, "end": 968.1199999999999, "text": " for that. We'll try to find similarities. Oh, sorry. I have to repeat the question.", "tokens": [337, 300, 13, 492, 603, 853, 281, 915, 24197, 13, 876, 11, 2597, 13, 286, 362, 281, 7149, 264, 1168, 13], "temperature": 0.4, "avg_logprob": -0.8242433166503906, "compression_ratio": 1.0246913580246915, "no_speech_prob": 0.0027383719570934772}, {"id": 58, "seek": 96812, "start": 968.12, "end": 982.96, "text": " So the question was, if I understand well, how to reproduce that use case, not on images,", "tokens": [407, 264, 1168, 390, 11, 498, 286, 1223, 731, 11, 577, 281, 29501, 300, 764, 1389, 11, 406, 322, 5267, 11], "temperature": 0.0, "avg_logprob": -0.22132435250789562, "compression_ratio": 1.3014705882352942, "no_speech_prob": 0.00023961911210790277}, {"id": 59, "seek": 96812, "start": 982.96, "end": 995.88, "text": " but on other types of documents that would be, I guess, some features. 3D counterparts.", "tokens": [457, 322, 661, 3467, 295, 8512, 300, 576, 312, 11, 286, 2041, 11, 512, 4122, 13, 805, 35, 33287, 13], "temperature": 0.0, "avg_logprob": -0.22132435250789562, "compression_ratio": 1.3014705882352942, "no_speech_prob": 0.00023961911210790277}, {"id": 60, "seek": 99588, "start": 995.88, "end": 1004.92, "text": " And I'd say, well, as long as you can, like, represent your data in the shape of vectors,", "tokens": [400, 286, 1116, 584, 11, 731, 11, 382, 938, 382, 291, 393, 11, 411, 11, 2906, 428, 1412, 294, 264, 3909, 295, 18875, 11], "temperature": 0.0, "avg_logprob": -0.17184321981080822, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0003912549000233412}, {"id": 61, "seek": 99588, "start": 1004.92, "end": 1011.84, "text": " then you're ready to use face to, like, do some, some search for nearest neighbor in", "tokens": [550, 291, 434, 1919, 281, 764, 1851, 281, 11, 411, 11, 360, 512, 11, 512, 3164, 337, 23831, 5987, 294], "temperature": 0.0, "avg_logprob": -0.17184321981080822, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0003912549000233412}, {"id": 62, "seek": 99588, "start": 1011.84, "end": 1018.32, "text": " your database. And then you can go for the whole pipeline, create some graphs, find communities", "tokens": [428, 8149, 13, 400, 550, 291, 393, 352, 337, 264, 1379, 15517, 11, 1884, 512, 24877, 11, 915, 4456], "temperature": 0.0, "avg_logprob": -0.17184321981080822, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0003912549000233412}, {"id": 63, "seek": 101832, "start": 1018.32, "end": 1028.48, "text": " in the graph, and go for it. But I'm not sure Pini is your tool, but, but, well, the architecture", "tokens": [294, 264, 4295, 11, 293, 352, 337, 309, 13, 583, 286, 478, 406, 988, 430, 3812, 307, 428, 2290, 11, 457, 11, 457, 11, 731, 11, 264, 9482], "temperature": 0.0, "avg_logprob": -0.3014494660612825, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.00027455075178295374}, {"id": 64, "seek": 101832, "start": 1028.48, "end": 1038.0800000000002, "text": " of Pini could be, of course, a model. Yes. Is there any project current or you're completely", "tokens": [295, 430, 3812, 727, 312, 11, 295, 1164, 11, 257, 2316, 13, 1079, 13, 1119, 456, 604, 1716, 2190, 420, 291, 434, 2584], "temperature": 0.0, "avg_logprob": -0.3014494660612825, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.00027455075178295374}, {"id": 65, "seek": 101832, "start": 1038.0800000000002, "end": 1046.24, "text": " ongoing that Media Labs has used before, or is it still largely in development? It is", "tokens": [10452, 300, 14741, 40047, 575, 1143, 949, 11, 420, 307, 309, 920, 11611, 294, 3250, 30, 467, 307], "temperature": 0.0, "avg_logprob": -0.3014494660612825, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.00027455075178295374}, {"id": 66, "seek": 104624, "start": 1046.24, "end": 1055.28, "text": " largely in the development. Sorry, I repeat the question. So are there some projects at", "tokens": [11611, 294, 264, 3250, 13, 4919, 11, 286, 7149, 264, 1168, 13, 407, 366, 456, 512, 4455, 412], "temperature": 0.0, "avg_logprob": -0.21972092338230298, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.00020392773149069399}, {"id": 67, "seek": 104624, "start": 1055.28, "end": 1070.2, "text": " the Media Lab that are currently using Pini? And the response is no. Yes. Sorry, can you", "tokens": [264, 14741, 10137, 300, 366, 4362, 1228, 430, 3812, 30, 400, 264, 4134, 307, 572, 13, 1079, 13, 4919, 11, 393, 291], "temperature": 0.0, "avg_logprob": -0.21972092338230298, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.00020392773149069399}, {"id": 68, "seek": 107020, "start": 1070.2, "end": 1086.48, "text": " consider any other ways that can be considered? Yes. Have you considered other ways of presenting", "tokens": [1949, 604, 661, 2098, 300, 393, 312, 4888, 30, 1079, 13, 3560, 291, 4888, 661, 2098, 295, 15578], "temperature": 0.0, "avg_logprob": -0.4473230315417778, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.00018502243619877845}, {"id": 69, "seek": 107020, "start": 1086.48, "end": 1093.04, "text": " picture similarity or using picture similarity, or the types of image similarity, if I'm", "tokens": [3036, 32194, 420, 1228, 3036, 32194, 11, 420, 264, 3467, 295, 3256, 32194, 11, 498, 286, 478], "temperature": 0.0, "avg_logprob": -0.4473230315417778, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.00018502243619877845}, {"id": 70, "seek": 109304, "start": 1093.04, "end": 1107.8, "text": " here in the Sunwell? Well, I'd say that that was what I was saying in my second slide. There", "tokens": [510, 294, 264, 6163, 6326, 30, 1042, 11, 286, 1116, 584, 300, 300, 390, 437, 286, 390, 1566, 294, 452, 1150, 4137, 13, 821], "temperature": 0.0, "avg_logprob": -0.25664103264902155, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.00021245925745461136}, {"id": 71, "seek": 109304, "start": 1107.8, "end": 1117.56, "text": " are other types of image similarity, for example, semantical similarity. And, well, maybe in", "tokens": [366, 661, 3467, 295, 3256, 32194, 11, 337, 1365, 11, 4361, 394, 804, 32194, 13, 400, 11, 731, 11, 1310, 294], "temperature": 0.0, "avg_logprob": -0.25664103264902155, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.00021245925745461136}, {"id": 72, "seek": 111756, "start": 1117.56, "end": 1128.3999999999999, "text": " a few months, if we have like a robust architecture, we could maybe include some other types of", "tokens": [257, 1326, 2493, 11, 498, 321, 362, 411, 257, 13956, 9482, 11, 321, 727, 1310, 4090, 512, 661, 3467, 295], "temperature": 0.0, "avg_logprob": -0.1352518061374096, "compression_ratio": 1.3555555555555556, "no_speech_prob": 5.629640509141609e-05}, {"id": 73, "seek": 111756, "start": 1128.3999999999999, "end": 1140.36, "text": " vectorization of images. But for now, well, there are already tools that do that. Like,", "tokens": [8062, 2144, 295, 5267, 13, 583, 337, 586, 11, 731, 11, 456, 366, 1217, 3873, 300, 360, 300, 13, 1743, 11], "temperature": 0.0, "avg_logprob": -0.1352518061374096, "compression_ratio": 1.3555555555555556, "no_speech_prob": 5.629640509141609e-05}, {"id": 74, "seek": 114036, "start": 1140.36, "end": 1150.6799999999998, "text": " there is something called Clip Server that helps you find similar images from clip vectors", "tokens": [456, 307, 746, 1219, 2033, 647, 25684, 300, 3665, 291, 915, 2531, 5267, 490, 7353, 18875], "temperature": 0.0, "avg_logprob": -0.24571611616346573, "compression_ratio": 1.359375, "no_speech_prob": 0.00018421538698021322}, {"id": 75, "seek": 114036, "start": 1150.6799999999998, "end": 1166.08, "text": " that are like semantical vectors. So you could use that tool. It's great. Yes. Yes.", "tokens": [300, 366, 411, 4361, 394, 804, 18875, 13, 407, 291, 727, 764, 300, 2290, 13, 467, 311, 869, 13, 1079, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.24571611616346573, "compression_ratio": 1.359375, "no_speech_prob": 0.00018421538698021322}, {"id": 76, "seek": 116608, "start": 1166.08, "end": 1193.96, "text": " So the question is, is the tool really able to distinguish the thing that is of interest", "tokens": [407, 264, 1168, 307, 11, 307, 264, 2290, 534, 1075, 281, 20206, 264, 551, 300, 307, 295, 1179], "temperature": 0.0, "avg_logprob": -0.14257450537248093, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0018450258066877723}, {"id": 77, "seek": 119396, "start": 1193.96, "end": 1204.1200000000001, "text": " to us, the fact that we are talking about a dog? So the tool is only able to find partial", "tokens": [281, 505, 11, 264, 1186, 300, 321, 366, 1417, 466, 257, 3000, 30, 407, 264, 2290, 307, 787, 1075, 281, 915, 14641], "temperature": 0.0, "avg_logprob": -0.08411491781041242, "compression_ratio": 1.7581699346405228, "no_speech_prob": 0.00013122151722200215}, {"id": 78, "seek": 119396, "start": 1204.1200000000001, "end": 1211.28, "text": " copies in an image. So the tool would probably be able to say that all those images contain", "tokens": [14341, 294, 364, 3256, 13, 407, 264, 2290, 576, 1391, 312, 1075, 281, 584, 300, 439, 729, 5267, 5304], "temperature": 0.0, "avg_logprob": -0.08411491781041242, "compression_ratio": 1.7581699346405228, "no_speech_prob": 0.00013122151722200215}, {"id": 79, "seek": 119396, "start": 1211.28, "end": 1219.68, "text": " the same parts of face of a dog. So it would probably be able to group all those images", "tokens": [264, 912, 3166, 295, 1851, 295, 257, 3000, 13, 407, 309, 576, 1391, 312, 1075, 281, 1594, 439, 729, 5267], "temperature": 0.0, "avg_logprob": -0.08411491781041242, "compression_ratio": 1.7581699346405228, "no_speech_prob": 0.00013122151722200215}, {"id": 80, "seek": 121968, "start": 1219.68, "end": 1226.76, "text": " together. The problem is that if there are other images in the database that contain", "tokens": [1214, 13, 440, 1154, 307, 300, 498, 456, 366, 661, 5267, 294, 264, 8149, 300, 5304], "temperature": 0.0, "avg_logprob": -0.08696666311045162, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.9901620261371136e-05}, {"id": 81, "seek": 121968, "start": 1226.76, "end": 1233.2, "text": " the rest of the images, then they would probably also be grouped in the same cluster. So that's", "tokens": [264, 1472, 295, 264, 5267, 11, 550, 436, 576, 1391, 611, 312, 41877, 294, 264, 912, 13630, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.08696666311045162, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.9901620261371136e-05}, {"id": 82, "seek": 121968, "start": 1233.2, "end": 1243.0800000000002, "text": " why what we are currently doing about parts of images would let us improve the cluster", "tokens": [983, 437, 321, 366, 4362, 884, 466, 3166, 295, 5267, 576, 718, 505, 3470, 264, 13630], "temperature": 0.0, "avg_logprob": -0.08696666311045162, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.9901620261371136e-05}, {"id": 83, "seek": 124308, "start": 1243.08, "end": 1253.8799999999999, "text": " so that it's purified from the rest of the images. And we could have a cluster of the", "tokens": [370, 300, 309, 311, 1864, 2587, 490, 264, 1472, 295, 264, 5267, 13, 400, 321, 727, 362, 257, 13630, 295, 264], "temperature": 0.0, "avg_logprob": -0.18083715438842773, "compression_ratio": 1.4741379310344827, "no_speech_prob": 0.00013915171439293772}, {"id": 84, "seek": 124308, "start": 1253.8799999999999, "end": 1267.1999999999998, "text": " face of that specific dog and then a cluster of that taco in the second cluster. Yes.", "tokens": [1851, 295, 300, 2685, 3000, 293, 550, 257, 13630, 295, 300, 34101, 294, 264, 1150, 13630, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.18083715438842773, "compression_ratio": 1.4741379310344827, "no_speech_prob": 0.00013915171439293772}, {"id": 85, "seek": 126720, "start": 1267.2, "end": 1273.2, "text": " What kind of clusterization do you use on the graph? Well, for now, we have the best", "tokens": [708, 733, 295, 13630, 2144, 360, 291, 764, 322, 264, 4295, 30, 1042, 11, 337, 586, 11, 321, 362, 264, 1151], "temperature": 0.0, "avg_logprob": -0.16936953171439792, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.0005297476891428232}, {"id": 86, "seek": 126720, "start": 1273.2, "end": 1283.44, "text": " result with, excuse me, what kind of clusterization do you use on the graph? For now, we have", "tokens": [1874, 365, 11, 8960, 385, 11, 437, 733, 295, 13630, 2144, 360, 291, 764, 322, 264, 4295, 30, 1171, 586, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.16936953171439792, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.0005297476891428232}, {"id": 87, "seek": 126720, "start": 1283.44, "end": 1291.6000000000001, "text": " our best result using pure connected components. So actually, the specification we do on the", "tokens": [527, 1151, 1874, 1228, 6075, 4582, 6677, 13, 407, 767, 11, 264, 31256, 321, 360, 322, 264], "temperature": 0.0, "avg_logprob": -0.16936953171439792, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.0005297476891428232}, {"id": 88, "seek": 129160, "start": 1291.6, "end": 1299.36, "text": " graph to reduce the number of links between images is enough to have separated connected", "tokens": [4295, 281, 5407, 264, 1230, 295, 6123, 1296, 5267, 307, 1547, 281, 362, 12005, 4582], "temperature": 0.0, "avg_logprob": -0.16899502661920363, "compression_ratio": 1.5141242937853108, "no_speech_prob": 0.00016779328871052712}, {"id": 89, "seek": 129160, "start": 1299.36, "end": 1306.24, "text": " components in the graph. And so we take each connected component and it's our cluster.", "tokens": [6677, 294, 264, 4295, 13, 400, 370, 321, 747, 1184, 4582, 6542, 293, 309, 311, 527, 13630, 13], "temperature": 0.0, "avg_logprob": -0.16899502661920363, "compression_ratio": 1.5141242937853108, "no_speech_prob": 0.00016779328871052712}, {"id": 90, "seek": 129160, "start": 1306.24, "end": 1315.1999999999998, "text": " What we would like to do is to try to mix with some Luvain community detection, but actually", "tokens": [708, 321, 576, 411, 281, 360, 307, 281, 853, 281, 2890, 365, 512, 5047, 85, 491, 1768, 17784, 11, 457, 767], "temperature": 0.0, "avg_logprob": -0.16899502661920363, "compression_ratio": 1.5141242937853108, "no_speech_prob": 0.00016779328871052712}, {"id": 91, "seek": 131520, "start": 1315.2, "end": 1341.1200000000001, "text": " for now, it's not the thing that works best. Yes.", "tokens": [337, 586, 11, 309, 311, 406, 264, 551, 300, 1985, 1151, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.35326051712036133, "compression_ratio": 0.9245283018867925, "no_speech_prob": 0.0001531642337795347}, {"id": 92, "seek": 134112, "start": 1341.12, "end": 1352.1999999999998, "text": " I'm not sure I understand the question. Can you try to rephrase it? Okay. What things", "tokens": [286, 478, 406, 988, 286, 1223, 264, 1168, 13, 1664, 291, 853, 281, 319, 44598, 651, 309, 30, 1033, 13, 708, 721], "temperature": 0.0, "avg_logprob": -0.1667633989582891, "compression_ratio": 1.3412698412698412, "no_speech_prob": 0.0006817057146690786}, {"id": 93, "seek": 134112, "start": 1352.1999999999998, "end": 1367.08, "text": " are you looking at to improve the model? Well, there are many things we are looking", "tokens": [366, 291, 1237, 412, 281, 3470, 264, 2316, 30, 1042, 11, 456, 366, 867, 721, 321, 366, 1237], "temperature": 0.0, "avg_logprob": -0.1667633989582891, "compression_ratio": 1.3412698412698412, "no_speech_prob": 0.0006817057146690786}, {"id": 94, "seek": 136708, "start": 1367.08, "end": 1379.1599999999999, "text": " at. For now, mainly, we look at techniques to do a better graph specification in order", "tokens": [412, 13, 1171, 586, 11, 8704, 11, 321, 574, 412, 7512, 281, 360, 257, 1101, 4295, 31256, 294, 1668], "temperature": 0.0, "avg_logprob": -0.1311884702638138, "compression_ratio": 1.3257575757575757, "no_speech_prob": 0.0002490492188371718}, {"id": 95, "seek": 136708, "start": 1379.1599999999999, "end": 1393.6399999999999, "text": " to find more coherent clusters. We are not so much working on the local descriptors part", "tokens": [281, 915, 544, 36239, 23313, 13, 492, 366, 406, 370, 709, 1364, 322, 264, 2654, 31280, 830, 644], "temperature": 0.0, "avg_logprob": -0.1311884702638138, "compression_ratio": 1.3257575757575757, "no_speech_prob": 0.0002490492188371718}, {"id": 96, "seek": 139364, "start": 1393.64, "end": 1423.5200000000002, "text": " of the tool for now. Have you considered using the direct link", "tokens": [295, 264, 2290, 337, 586, 13, 3560, 291, 4888, 1228, 264, 2047, 2113], "temperature": 0.0, "avg_logprob": -0.5628369836246266, "compression_ratio": 0.9393939393939394, "no_speech_prob": 0.001622299663722515}, {"id": 97, "seek": 142352, "start": 1423.52, "end": 1435.6399999999999, "text": " to the Twitter images or social media images online? Did I repeat everything? Well, yes.", "tokens": [281, 264, 5794, 5267, 420, 2093, 3021, 5267, 2950, 30, 2589, 286, 7149, 1203, 30, 1042, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.21783178244064103, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0003826984902843833}, {"id": 98, "seek": 142352, "start": 1435.6399999999999, "end": 1442.76, "text": " We would like people to be able to see images in their context because, actually, they won't", "tokens": [492, 576, 411, 561, 281, 312, 1075, 281, 536, 5267, 294, 641, 4319, 570, 11, 767, 11, 436, 1582, 380], "temperature": 0.0, "avg_logprob": -0.21783178244064103, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0003826984902843833}, {"id": 99, "seek": 142352, "start": 1442.76, "end": 1448.8799999999999, "text": " understand what's happening if they just have images. They need to see, okay, why was this", "tokens": [1223, 437, 311, 2737, 498, 436, 445, 362, 5267, 13, 814, 643, 281, 536, 11, 1392, 11, 983, 390, 341], "temperature": 0.0, "avg_logprob": -0.21783178244064103, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0003826984902843833}, {"id": 100, "seek": 144888, "start": 1448.88, "end": 1459.64, "text": " image published? Who answered, et cetera. This would probably mean that we need to add", "tokens": [3256, 6572, 30, 2102, 10103, 11, 1030, 11458, 13, 639, 576, 1391, 914, 300, 321, 643, 281, 909], "temperature": 0.0, "avg_logprob": -0.31753242015838623, "compression_ratio": 1.288, "no_speech_prob": 0.0004207094316370785}, {"id": 101, "seek": 144888, "start": 1459.64, "end": 1466.0400000000002, "text": " at least the links to the pulse or maybe some kind of visualization of it.", "tokens": [412, 1935, 264, 6123, 281, 264, 17709, 420, 1310, 512, 733, 295, 25801, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.31753242015838623, "compression_ratio": 1.288, "no_speech_prob": 0.0004207094316370785}, {"id": 102, "seek": 146604, "start": 1466.04, "end": 1482.44, "text": " We have a bit of time here. Any more questions? We can take one or two. If not, we can switch", "tokens": [492, 362, 257, 857, 295, 565, 510, 13, 2639, 544, 1651, 30, 492, 393, 747, 472, 420, 732, 13, 759, 406, 11, 321, 393, 3679], "temperature": 0.0, "avg_logprob": -0.5584395272391183, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.009478947147727013}, {"id": 103, "seek": 146604, "start": 1482.44, "end": 1483.44, "text": " quietly. All the next questions.", "tokens": [19141, 13, 1057, 264, 958, 1651, 13], "temperature": 0.0, "avg_logprob": -0.5584395272391183, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.009478947147727013}, {"id": 104, "seek": 148344, "start": 1483.44, "end": 1500.42, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51213], "temperature": 1.0, "avg_logprob": -1.3063206672668457, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0007083679083734751}], "language": "en"}