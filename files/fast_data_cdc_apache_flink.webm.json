{"text": " Welcome, good morning everybody. So today I want to talk a little bit about change data capture, CDC, stream processing, with the Patrick link. This talk is split into three parts. The first part is for people that have never heard of link before. The second part is maybe a little bit more deep, but I think it's really, really deep. And then the third part, we could dive really, really deep into under the hood. So just to make you particularly interested in the software and what we are doing here. So yeah, I already got an introduction, but just to summarize it. So like from the open source side, part of the link even before it became part of the Patrick software foundation in 2014. I'm a member of the management committee of the Patrick link. In the years I also made it to the top contributors according to additions in the top one. I don't know which refactoring I did to the top one contributor. Yeah, and among the core people that try to design things equal every day in the world. Can you realize, yeah, I went through a couple of companies. The latest one where I was a co-founder was in Maroc. Maroc got acquired by Confluent beginning of this year. And now I'm a principal software engineer at Confluent. So let's talk about it. Before I start with an introduction to the link, I would actually like to talk about stream processing in general. Because when you do stream processing, you basically always can identify roughly like four building blocks. So let's talk about those building blocks first. So first of all, you need streams, right? You want to have data, you maybe want to create some pipeline from source to sync. You might want to distribute your streams because you have a lot of data. So maybe you want to scale out and scale in depending on the load. You want to join streams together. You want to enrich streams. Maybe there is a control stream and the main stream. So you want to dynamically modify the behavior of the application while the application is running. And yeah, sometimes there is a bug in your application. Or you just want to trace certain behavior, then you also want to be in play streams. Time, working with time is also a very, very important concept. Because on one side, you want to make progress in your pipeline. But at some points, you also want to sync for an ISO if you have two streams. Maybe you want to wait for the other stream. Maybe you want to block or you want to buffer some of the streams. Maybe if the second event doesn't come in, you want to time out after some time. Maybe you also want to replay historical data. So you want to fast forward the time. You don't want to wait another hour to fire an hour window. No, this should be quicker. Then when we talk about buffering, what I just said, or in general, storing data state is a very important component for processing. State can be, for example, a machine learning model that is updated from time to time to classify your incoming streams. It can be a cache if you don't want to look up in the database for every record that comes in. In a general state, a low state can be an order of classified. And state also, at some point, needs to be acquired. If you have a state full streaming application, a very useful component is helped by actually making sure that I can create this natural of my streaming repository. So I wanted a state backup to my streaming application. I wanted to make a version of it, so every night I want to create a screenshot version. Maybe I want a full streaming application in a staging cluster, in a gelatin cluster, and play around with the data in the state. Maybe I want to do some testing, or I just want to time track the process of my application. So let's talk a little bit about what makes Flink unique compared to other competitors. First of all, what I just showed is that Flink is one of the best stream processors for all of these use cases and building blocks. So when you design a streaming application, you can start with a whiteboard and you just draw some circles. What do you actually want to do? Maybe you want to reform some sources. Maybe you want to normalize your data. You want to filter some data out. You want to join the data and in the end you really want to sync it somewhere else. But this is how it starts. And this is also how you have to reason about when you're creating a pipeline. And what Flink does under the hood is it has a parallelism and scalability built in. So you don't have to think of threading or network transfers or anything like that. Under the hood there are sharks, there are petitions depending on the connectors. There are sub-tasks that in parallel execute operations. Each of these tasks, some of these tasks can stay full and can have some storage local to the operator. Very important. So the state basically scales out and scales in with the operator. You don't need to vary to a database which would increase data. And then, of course, it travels and then the whole pipeline runs. And now comes the important part. What Flink explained really unique is that this possibility of creating an existence snapshot of your entire stream of technology. So there are, like, what we call the checkpoint barriers which are traveling through the topology and make a backup of each state of the operator. And then this snapshot is then persisted on a long-term storage like S3 or HDFS or some other distributed kind of system. When we talk about use cases, there are plenty of use cases. We have the process transactions, docs, IOTs, any kind of events, user interactions. People use it for broad detection for machine learning, for event-driven applications, for ETL, for data integration, for analytics. So Flink has become, over the last 10 years, it has become like a very large platform. You can connect various connectors from business stream systems, you can read and write files, databases, key value stores. And as I said, like also event-driven applications where maybe you want to send out an email or don't need a connector. You can also implement something custom that talks to somewhere as the API. So let's look at this scene. So I also want to quickly talk about Flink's API. So this is the API stack. The two main APIs are data stream API, table API or table table API. And there is also stateful functions. Stateful functions is a sub-project that tries to execute an actor model on a page of Flink, but will not go into detail here. So first of all, like all the APIs are built on a data flow runtime, so there's no batching or anything involved under the hood. It's really a data flow runtime. Whatever the result is ready, it will be streamed to the next operator. On top of that, there is a loader, the stream operator API, which you can use. But yeah, this is for X, of course, I would say. And then we have the mainstream APIs on top. And the specialty about table table API is that there is an optimizer on a planning stage in between. So the C helps you when you're not creating the most efficient pipelines. The optimizer will make sure that the streaming will be executed more efficiently. Yeah, let's also quickly look at the APIs. So this is like a basic example of creating a stream for just three elements. And then you're executing this on a cluster or in your IDE, then you're retrieving the result back. And you have an iterator locally, you can just print it locally. It's not very useful, but this is a minimal example of the Java API. The important thing is that the stream API basically exposes all the building blocks that I mentioned on my previous slide. So you can have very abstract operator typologies, and you can use built-in functions like map, process, and connect. Which each of them takes different functions, and then you can really define your business logic in those. They use different functions, and you can also use completely arbitrary Java records. For example, Python records that flow between the operator and conceptually. This is interesting when we talk about change data capture. Conceptually, the data stream API does not know about changes. It only knows about records, so there is no change flag or anything like that. So conceptually, the data stream API is an app that can only or insert only as long. And also when you look at the output, one, two, three, four, five, six. So let's take a look at table API and simple API. So usually you just say to this table API, order in SQL, because it's a unified API. You can decide whether you want to electrify your pipeline programmatically, or whether you want to use standard SQL for defining your topology. In the end, you also execute and you can also print locally in your IDP. Here, this API abstracts all building blocks. So you have no access to timers or state or anything like this. This will be on the foot. Also the operator topology is determined by the planner, not by you. The nice thing here is you can focus on your business logic. And you do this declaratively to optimize your business durations and make something out of it. Internally, it uses highly efficient records, also up to the engine, not to you. What you will see maybe is like a road type. If you really want to go out of table API, then you see a road type, which can work as a business program. And the interesting thing here is that conceptually, we are working with tables here, tables and views, you know, databases. But under the hood, there is actually a change level. And that's what I want to show in the following slides. But you can also see that, like for example, if you're disappearing here, you will get this output when you run it in the IDQ. And you already see that there is, of course, an F0 column with the 123 output. But there is an additional column, first column, which already shows that there is some change like attached to every record. In this case, it's just insert. The nice thing about Linux APIs is that you can mix and match them. So you can, for example, start with the S3 API, and then you go to table API, or the other way around. If you have SQL, you can do the detail in SQL first. And then if you have some more complex logic, like timer services, or like a very complex state, or whatever, then you can go to the S3 API, do it there, and then you can switch back to SQL, or you can just use the SQL for Nectar. But you find the entire pipeline in the S3 API. So that is up to you. But yeah, the APIs for that are present to go back and forth between those two. So now let's really talk about change.s3 processing. If you think about data processing, like in most of the cases, the main data processing is always consuming a stream of changes. Because if this would not be like a continuous input stream, then your company, your project, whatever it would actually be, right? So like it is actually very common that data flows in continuously. And the Flinky APIs, the Flink runtime sees everything basically as a stream. And it just distinguishes between a bounded stream and unbounded stream. So bounded means you define a start and an end, and the end was coming there. Unbounded means you start somewhere, and now it can be somewhere in the past, and then you start processing the future. So this is up to you. Yeah, if you really think a bit about this, actually batch processing is just a special case of stream processing. So batch processing means that through the bounded nature of the stream, I can maybe do some more specialized operators like sorting, for example. It's easier in such a thing. And you can also use different algorithms if you have sorted like this to a sort of a join, or something like this. So that the runtime has special operators and special handling of bounded streams. But in general, you can process everything for the stream. So both bounded and unbounded data. So how does actually things look like? So how can I work with streams and things like that? So the first answer to this, or I mentioned before, you actually don't work with streams. So what you work with is dynamic tables. So this is just a concept we call the dynamic tables. It's a concept similar to materialized views and materialized view maintenance. So what you do as a user is you define your tables. So on the left side, we have transactions on the right side. We have maybe revenue. And then you define, in the middle, you define a standing, running SQL31, which gets translated into a pipeline methodology next to the private branch. So then the question is, OK, if we have this big SQL kind of a database, and the answer to that is no, it's not a database, because we are not in charge of the data. So you can bring your own data and your own systems. So it's more like a process. It leads from all different kinds of systems. So if a table is not a stream, or if I don't work with streams, how does that actually relate with each other? And an interesting piece of interesting term here is called stream table duality. So you can basically see a stream as the change log of a continuously changing table. So it is possible, and I will also show an example shortly, that you can convert from a table into a stream and from a stream into a table. You can do a back and forth mapping after this possible. Usually, as a user, you don't see that. Under the hood, the runtime, all the sources, all the things, all the operators, they work with change logs under the hood. So in Flink, we have four different kinds of change tags for each record. So we have insertions, insertions are also the default input and output for bounded hash queries. And then we have update four, which basically removes a previously inputted result. Then we have update after to update something. And then we have to feed one of the last results. When we see only insert only in a log, then we call this an only or insert only log. If it contains some kind of division or update before, we call this updating table or an updating stream. And if it never contains an update before, but only update afters, then there's a primary key involved, and then we call this absurdity. So let me make a quick example. So again, we have on the left side those actions on the right side with value, and in the middle we have summing and sumproving by name of different sections. So what happens now is, like in the logical table, there is a new record coming in called Alice. This is how it will be represented in the change tag under the hood. And then this is what comes out. So we are summing here. So 256 is the first result that we are already looking at. So now the next variable comes in. Again, it will be added also to the last table. But now it comes in. There's another Alice, and we want to move like this. So that means the sum is not updated or we need to update the sum to the newest number. That means, first, we have to remove the old record. And if we want to materialize our change log into a table, so we also have to remove the row in the table. And then we can finally add the updated row in the table. And this is what the change log looks like. And if you would apply this change log to a SQL or to some key values there, or like the search or so, then the result would be there. And if we would define a primary key on the sync table, actually we don't need this update before, because then it would be now searching operation. And yeah, we can basically save 50% of traffic if we do not want to support that with the rows in the sync table. So I already mentioned that each sync and each source, that they declare a change log model which changes they and they can't consume. And yeah, I give like a quick example of various connectors. So when we, for example, read from a file system, this is usually a scan operation. So it is very common that when you read from a file that this is just insert only. There are no updates coming through the file system. Sometimes they do, but in the general case, you just scan through the Kafka file for example. Okay, so Kafka, in the early days, Kafka was actually just as a log for every N record that came in through Kafka was also considered like an insert only record. Then later, Kafka also added some absurd functionality. So we also have a connector called Kafka Absurd for that. That means when a value in Kafka is null, it means addition. So the Kafka Absurd connector, for example, would produce insertions and divisions. If you define the JVC connector, JVC also doesn't have this concept of updates. So in the same case, we would scan the entire table and just scan to only produce insertions. So we have all the insertions for JVC. But that comes like the most complex case. What happens if you use, for example, an easy one? You connect it to the database to consume the change of this particular from the database. You put this into Kafka and then you consume from Kafka. In this case, for example, this could, for example, all kinds of changes that can then be evaluated by the end. The optimizer basically tracks the changes through the entire topology and the sync prepares what it can digest. The optimizer could react sometimes with an error message, but sometimes there's more to it. So let's quickly also talk about these two different modes. So I already said that sometimes you can do upserts where there's no update before. And sometimes you need all four kind of changes. And this is called like retract versus absurd. So retract has this nice property that there is no primary key required. This works for almost all external systems, which is great. You can also support up with the pros, which you cannot support in the upsert. The table is called an absurd table. And interestingly, also retracts, so like this retracting of the previous admitted record is actually often required in distributed systems. And I also have a little example on the right side, but I will show shortly. So let me definitely explain this first. So this is a count of a count. So we are creating an histogram. The lexical variable itself is not so important. What is important? What is actually flowing in the cluster through the operators? So whatever record comes in, the first operator will identify this. Okay, this is the first time, so the count is one. And then since we want to do the count of a count, the next operator will also count this as a one, and it will keep some state. How many records have I seen for this particular count? So now comes the second record in. And we have to after the count. So now the count is two, but one anymore. And interestingly, if we do a hash partition for some collectors, it might be that the count ends up at a completely different operator. But what happens with the old count? So now you have two threads or two operators, parallel instances of the operator that have a count and that they need to remove the count in the other operator. This is why this case rejection is required, because the update before needs to go to the subclass one and remove the man outdated record. But in general, absurd is an optimization. It reduces traffic, reduces computation. And if it's possible, it's great, but usually there is a lot of reflections flowing on in the... And also have some examples here. Like if you would do an explain on some SQL query in the SQL, the bottom part is what you would see. So let's assume we have a table of transactions and a table of payment and a table of result. The table of result can consume all kinds of changes. I just took my table also. And you join transactions and payments. And in the explain, you also see that there is... You can get information in the explain about the change of mode. For example, if the input here is insert only, insert only, then also the join will produce insert only result. And for example, if we do an outer join, in this case the left outer join, then things become a bit more complex. Here you have insert only, insert only, but since that outer join will emit another first, like if there's one thing, like one record comes in, there's no matching record so far, and you have to emit another first for the other side, and then when the other side is coming in, then you have to remove another again and emit the final result. And that's why, for example, here, you have all kinds of changes coming out of the join. And then we can even make it more complicated. What happens if we define a primary key on transactions and payments? Then the optimizer will recognize, okay, that input spec and the right input spec will contain now a key key. That is great. So I can remove the update before, so you can see that there is one, that particular is not necessary anymore, because we can do upserts on the results. So this query is obviously more efficient than the other one. And the other good optimizer can also range between those different modes. I don't want to get details here, but if it's possible, like it's necessary, you can go from updating to the collection. But that's not also under the course of the information. And depending on the operators, you also can switch between these modes. So for example, if you have a regular join, to append only tables, then also the resulting table will be append only. And I showed already that if there's one of the tables updating, the results will be updating. And if there's some outer join, then the result is always updating. And now comes the interesting part. If you have append only table, and you join it to an updating table, there is a special kind of join, which we call temporal join. A temporal join will actually produce an append only table, because it looks at the table at a point, a specific point in time. That's a very interesting operator. Unfortunately, we don't have enough time, but I just want to show you an example of this very, very useful join operator. So let's assume we have some orders table, and orders have a currency, and there is a currency rates table. And obviously, you don't want to join those two tables with the latest currency rates, but you actually want to know what was the currency rate at the time when the order was created. And this syntax here with the persistent time as of actually allows you to consume all the changes from the rates table and join it with orders at the point on the order. This is just one example of a very sophisticated join operation. And by the way, for system time as of this season, we're going to have to be able to understand so carefully how we see what we're going to do. So I also have prepared a demo. I think we still have seven minutes left, so it should be good to see. So I also want to show you some of the CPC capabilities. So I will run everything in my IDE. I will use Java for this example. I have a MySQL container, and I'm running it, so we'll start with a SQL container. I'm processing it. And this container will create a MySQL instance, and it will also be filled already with, I think, three or four rows. I have a few examples here. I can simply run the examples and the main method of the IDE. So what I'm doing here is I'm creating different tables to connect to a SQL. One is a JPC one, which fully screens the table once, and the other one is a CPC one, which, like, continuously monitors the tables and the JPC. So let's just run this. So here we see the first three results. As you can see, the application has not stopped. So it is waiting for more records to come, and now I want to insert more values into MySQL. I could have used MySQL to see a line for that, but I can also use my SQL to see what I see. So I am having a regular, I can set into transaction JPC, values, blah, blah, blah, and I can run this main method here. So it's a bit overkill to use Spring for that, for just one value, but I think it's flexible enough you can also use it for the hash query of just setting one record into the database, and as you can see, we can show my SQL and from my SQL, via CPC to the link, and then to the next CPC. We have also more sophisticated examples here, and I don't think that we have more time for that, but we can do a lot of things with Spring SQL. I think I could spend a day and talk a little bit more about the way this works. So, put on the grid. Yeah, Spring SQL and it is very powerful, has been crafted over years and years and years, by many, many companies, many teams. It's very flexible for integration, for integrating various systems of different semantics, and there is way more. So, I just showed some operators, but we have a large, large coverage of SQL standard, so over-windows support for aggregating, for Spring, we support the recognized laws for pattern matching and complex plan processing. We have time for obsessions, windows for like, cutting your screen into pieces. Then there is a huge CPC connector ecosystem, not part of the fourth thing, but also quite useful as a little think-of-stars already. Then, something new, fatal store, which tries to be like the first streaming data warehouse kind of thing. It's not a very early version, but it's very promising. So, yeah, I would recommend to, yeah, maybe look into one of these sub-projects as well. Not only things, it's big, but also the ecosystem, around the thing, the growths and growths and growths. I'm happy to take questions. I think we have three minutes left, but otherwise I will also speak outside for any questions. Thank you very much. Thank you. Yeah, so like... Can you please repeat the question? The question is like, how does it, like, handle like, transactions that also take a lot of time before the transaction has ended. So, in general, I think we are not very good at transaction handling in things, but like you have with Data Stream API, you have a lot of possibilities to, like for example, you can buffer all the data from this transaction and stay at it after terrible time you want to, and then just wait until the transaction closes, and then you're creating the execution of the transaction, and that is possible. So, yeah, personally, I would maybe do some stuff in the Data Stream API first until the transaction ended, and then push that. Thank you. Yeah, thank you both. So, in terms of running times, do I just think from bottom all, if it's an hybrid, I shift my interpretation? No, I think it's creating its own, its own runtime. So, you will be able to do mutual apps, some types of software, some types of software, some types of software, and then just look at the, like, I don't know if there's a lot of data in the stream. No, I don't. Is that all there? Okay. Let me check. There's a library that's called the HGIS, which is not for you to be used, but we can also buy it, and that's the idea for a line. But in general, yeah, the platform, it's rather a platform, that's where it looks. Now, the next logical question, how far does it scale? I would say, as the question was, how far does it scale, I can tell you that probably from the system, and like, there's single state, there's a billion, all of us, and all of us in the day or so. Yeah, and there is Apple, that processes things, like all the big banks use it for credit card fraud detection and stuff like that. So, I don't think that, like most companies in this room, they will not reach the scalability limits of Link, because yeah, we are not at, unless here some Apple or Alibaba people are in this room, then maybe, I don't know. You said that the frame does not own data, but then there is this table store project, so is it like move towards, you know, more like ownership? Yeah, this table store is a very, very interesting approach, like it started last year, or two years ago, it's rather new. I think it was last year, early last year. It doesn't really fit to Apache Flink, but it's still, it's very useful, and yeah, we will see, maybe it will leave the, maybe it will leave the Apache Software Foundation soon, but yeah, not allowed to, and not the Software Foundation, but the Flink project itself, we will see, because it doesn't fit really well, but it's in general, like we still have this vision of Flink as a database. Yeah, we will see. Thank you. Sir, a question. This is about big states, because you mentioned that you can have like terabytes of state, but when you create a checkpoint, and if this checkpoint will be very big, and storage of it can be long, is it like a huge DC post to write into the store? Yeah, so the question was, like how can we actually snapshot large state in general? And this is exactly where Flink distinguishes, like where it differs from competitors, because there is a lot of engineering involved to make this as efficient as possible. I'm sure there's even more to do, it's still not perfectly efficient, there's more optimizations that you can do, but for example, there is like differential snapshots involved, there is local recovery involved, or like there are many, many algorithms under the hood to make it as quickly as possible. But yeah, of course, like if you have terabytes of state, and the machine has died completely, and then you obviously need to restore these terabytes of state from S3 into your task manager again, and this can take time. So like it tries its best, but yeah, of course, you need to do benchmarks for your use case. One last question. Yeah, so we guarantee exactly once and to end if the connectors, source and sync support that, like especially for state, so there are no duplicates in state, we might need to reprocess data during a failure, but yeah, like end to end, exactly once semantics are possible. Okay, then yeah, thank you very much, and I'm waiting outside.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Welcome, good morning everybody.", "tokens": [4027, 11, 665, 2446, 2201, 13], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 1, "seek": 0, "start": 7.0, "end": 11.0, "text": " So today I want to talk a little bit about change data capture, CDC,", "tokens": [407, 965, 286, 528, 281, 751, 257, 707, 857, 466, 1319, 1412, 7983, 11, 17133, 11], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 2, "seek": 0, "start": 11.0, "end": 14.0, "text": " stream processing, with the Patrick link.", "tokens": [4309, 9007, 11, 365, 264, 13980, 2113, 13], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 3, "seek": 0, "start": 14.0, "end": 17.0, "text": " This talk is split into three parts.", "tokens": [639, 751, 307, 7472, 666, 1045, 3166, 13], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 4, "seek": 0, "start": 17.0, "end": 20.0, "text": " The first part is for people that have never heard of link before.", "tokens": [440, 700, 644, 307, 337, 561, 300, 362, 1128, 2198, 295, 2113, 949, 13], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 5, "seek": 0, "start": 20.0, "end": 25.0, "text": " The second part is maybe a little bit more deep, but I think it's really, really deep.", "tokens": [440, 1150, 644, 307, 1310, 257, 707, 857, 544, 2452, 11, 457, 286, 519, 309, 311, 534, 11, 534, 2452, 13], "temperature": 0.0, "avg_logprob": -0.45698056275817167, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.47247323393821716}, {"id": 6, "seek": 2500, "start": 25.0, "end": 30.0, "text": " And then the third part, we could dive really, really deep into under the hood.", "tokens": [400, 550, 264, 2636, 644, 11, 321, 727, 9192, 534, 11, 534, 2452, 666, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.27828330993652345, "compression_ratio": 1.6392156862745098, "no_speech_prob": 0.0004671993665397167}, {"id": 7, "seek": 2500, "start": 30.0, "end": 35.0, "text": " So just to make you particularly interested in the software and what we are doing here.", "tokens": [407, 445, 281, 652, 291, 4098, 3102, 294, 264, 4722, 293, 437, 321, 366, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.27828330993652345, "compression_ratio": 1.6392156862745098, "no_speech_prob": 0.0004671993665397167}, {"id": 8, "seek": 2500, "start": 35.0, "end": 39.0, "text": " So yeah, I already got an introduction, but just to summarize it.", "tokens": [407, 1338, 11, 286, 1217, 658, 364, 9339, 11, 457, 445, 281, 20858, 309, 13], "temperature": 0.0, "avg_logprob": -0.27828330993652345, "compression_ratio": 1.6392156862745098, "no_speech_prob": 0.0004671993665397167}, {"id": 9, "seek": 2500, "start": 39.0, "end": 48.0, "text": " So like from the open source side, part of the link even before it became part of the Patrick software foundation in 2014.", "tokens": [407, 411, 490, 264, 1269, 4009, 1252, 11, 644, 295, 264, 2113, 754, 949, 309, 3062, 644, 295, 264, 13980, 4722, 7030, 294, 8227, 13], "temperature": 0.0, "avg_logprob": -0.27828330993652345, "compression_ratio": 1.6392156862745098, "no_speech_prob": 0.0004671993665397167}, {"id": 10, "seek": 2500, "start": 48.0, "end": 51.0, "text": " I'm a member of the management committee of the Patrick link.", "tokens": [286, 478, 257, 4006, 295, 264, 4592, 7482, 295, 264, 13980, 2113, 13], "temperature": 0.0, "avg_logprob": -0.27828330993652345, "compression_ratio": 1.6392156862745098, "no_speech_prob": 0.0004671993665397167}, {"id": 11, "seek": 5100, "start": 51.0, "end": 56.0, "text": " In the years I also made it to the top contributors according to additions in the top one.", "tokens": [682, 264, 924, 286, 611, 1027, 309, 281, 264, 1192, 45627, 4650, 281, 35113, 294, 264, 1192, 472, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 12, "seek": 5100, "start": 56.0, "end": 60.0, "text": " I don't know which refactoring I did to the top one contributor.", "tokens": [286, 500, 380, 458, 597, 1895, 578, 3662, 286, 630, 281, 264, 1192, 472, 42859, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 13, "seek": 5100, "start": 60.0, "end": 67.0, "text": " Yeah, and among the core people that try to design things equal every day in the world.", "tokens": [865, 11, 293, 3654, 264, 4965, 561, 300, 853, 281, 1715, 721, 2681, 633, 786, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 14, "seek": 5100, "start": 67.0, "end": 71.0, "text": " Can you realize, yeah, I went through a couple of companies.", "tokens": [1664, 291, 4325, 11, 1338, 11, 286, 1437, 807, 257, 1916, 295, 3431, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 15, "seek": 5100, "start": 71.0, "end": 75.0, "text": " The latest one where I was a co-founder was in Maroc.", "tokens": [440, 6792, 472, 689, 286, 390, 257, 598, 12, 33348, 390, 294, 2039, 905, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 16, "seek": 5100, "start": 75.0, "end": 79.0, "text": " Maroc got acquired by Confluent beginning of this year.", "tokens": [2039, 905, 658, 17554, 538, 11701, 43518, 2863, 295, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3258455258990646, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0011797657934948802}, {"id": 17, "seek": 7900, "start": 79.0, "end": 83.0, "text": " And now I'm a principal software engineer at Confluent.", "tokens": [400, 586, 286, 478, 257, 9716, 4722, 11403, 412, 11701, 43518, 13], "temperature": 0.0, "avg_logprob": -0.12743189305434993, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0005373214371502399}, {"id": 18, "seek": 7900, "start": 83.0, "end": 85.0, "text": " So let's talk about it.", "tokens": [407, 718, 311, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.12743189305434993, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0005373214371502399}, {"id": 19, "seek": 7900, "start": 85.0, "end": 92.0, "text": " Before I start with an introduction to the link, I would actually like to talk about stream processing in general.", "tokens": [4546, 286, 722, 365, 364, 9339, 281, 264, 2113, 11, 286, 576, 767, 411, 281, 751, 466, 4309, 9007, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.12743189305434993, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0005373214371502399}, {"id": 20, "seek": 7900, "start": 92.0, "end": 100.0, "text": " Because when you do stream processing, you basically always can identify roughly like four building blocks.", "tokens": [1436, 562, 291, 360, 4309, 9007, 11, 291, 1936, 1009, 393, 5876, 9810, 411, 1451, 2390, 8474, 13], "temperature": 0.0, "avg_logprob": -0.12743189305434993, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0005373214371502399}, {"id": 21, "seek": 7900, "start": 100.0, "end": 102.0, "text": " So let's talk about those building blocks first.", "tokens": [407, 718, 311, 751, 466, 729, 2390, 8474, 700, 13], "temperature": 0.0, "avg_logprob": -0.12743189305434993, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0005373214371502399}, {"id": 22, "seek": 10200, "start": 102.0, "end": 109.0, "text": " So first of all, you need streams, right? You want to have data, you maybe want to create some pipeline from source to sync.", "tokens": [407, 700, 295, 439, 11, 291, 643, 15842, 11, 558, 30, 509, 528, 281, 362, 1412, 11, 291, 1310, 528, 281, 1884, 512, 15517, 490, 4009, 281, 20271, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 23, "seek": 10200, "start": 109.0, "end": 116.0, "text": " You might want to distribute your streams because you have a lot of data.", "tokens": [509, 1062, 528, 281, 20594, 428, 15842, 570, 291, 362, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 24, "seek": 10200, "start": 116.0, "end": 119.0, "text": " So maybe you want to scale out and scale in depending on the load.", "tokens": [407, 1310, 291, 528, 281, 4373, 484, 293, 4373, 294, 5413, 322, 264, 3677, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 25, "seek": 10200, "start": 119.0, "end": 122.0, "text": " You want to join streams together.", "tokens": [509, 528, 281, 3917, 15842, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 26, "seek": 10200, "start": 122.0, "end": 123.0, "text": " You want to enrich streams.", "tokens": [509, 528, 281, 18849, 15842, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 27, "seek": 10200, "start": 123.0, "end": 126.0, "text": " Maybe there is a control stream and the main stream.", "tokens": [2704, 456, 307, 257, 1969, 4309, 293, 264, 2135, 4309, 13], "temperature": 0.0, "avg_logprob": -0.18569376050811454, "compression_ratio": 1.8229665071770336, "no_speech_prob": 0.0008255222928710282}, {"id": 28, "seek": 12600, "start": 126.0, "end": 132.0, "text": " So you want to dynamically modify the behavior of the application while the application is running.", "tokens": [407, 291, 528, 281, 43492, 16927, 264, 5223, 295, 264, 3861, 1339, 264, 3861, 307, 2614, 13], "temperature": 0.0, "avg_logprob": -0.17456241087480026, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0006142737693153322}, {"id": 29, "seek": 12600, "start": 132.0, "end": 135.0, "text": " And yeah, sometimes there is a bug in your application.", "tokens": [400, 1338, 11, 2171, 456, 307, 257, 7426, 294, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.17456241087480026, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0006142737693153322}, {"id": 30, "seek": 12600, "start": 135.0, "end": 141.0, "text": " Or you just want to trace certain behavior, then you also want to be in play streams.", "tokens": [1610, 291, 445, 528, 281, 13508, 1629, 5223, 11, 550, 291, 611, 528, 281, 312, 294, 862, 15842, 13], "temperature": 0.0, "avg_logprob": -0.17456241087480026, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0006142737693153322}, {"id": 31, "seek": 12600, "start": 141.0, "end": 146.0, "text": " Time, working with time is also a very, very important concept.", "tokens": [6161, 11, 1364, 365, 565, 307, 611, 257, 588, 11, 588, 1021, 3410, 13], "temperature": 0.0, "avg_logprob": -0.17456241087480026, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0006142737693153322}, {"id": 32, "seek": 12600, "start": 146.0, "end": 151.0, "text": " Because on one side, you want to make progress in your pipeline.", "tokens": [1436, 322, 472, 1252, 11, 291, 528, 281, 652, 4205, 294, 428, 15517, 13], "temperature": 0.0, "avg_logprob": -0.17456241087480026, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0006142737693153322}, {"id": 33, "seek": 15100, "start": 151.0, "end": 156.0, "text": " But at some points, you also want to sync for an ISO if you have two streams.", "tokens": [583, 412, 512, 2793, 11, 291, 611, 528, 281, 20271, 337, 364, 25042, 498, 291, 362, 732, 15842, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 34, "seek": 15100, "start": 156.0, "end": 158.0, "text": " Maybe you want to wait for the other stream.", "tokens": [2704, 291, 528, 281, 1699, 337, 264, 661, 4309, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 35, "seek": 15100, "start": 158.0, "end": 163.0, "text": " Maybe you want to block or you want to buffer some of the streams.", "tokens": [2704, 291, 528, 281, 3461, 420, 291, 528, 281, 21762, 512, 295, 264, 15842, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 36, "seek": 15100, "start": 163.0, "end": 168.0, "text": " Maybe if the second event doesn't come in, you want to time out after some time.", "tokens": [2704, 498, 264, 1150, 2280, 1177, 380, 808, 294, 11, 291, 528, 281, 565, 484, 934, 512, 565, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 37, "seek": 15100, "start": 168.0, "end": 171.0, "text": " Maybe you also want to replay historical data.", "tokens": [2704, 291, 611, 528, 281, 23836, 8584, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 38, "seek": 15100, "start": 171.0, "end": 174.0, "text": " So you want to fast forward the time.", "tokens": [407, 291, 528, 281, 2370, 2128, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 39, "seek": 15100, "start": 174.0, "end": 177.0, "text": " You don't want to wait another hour to fire an hour window.", "tokens": [509, 500, 380, 528, 281, 1699, 1071, 1773, 281, 2610, 364, 1773, 4910, 13], "temperature": 0.0, "avg_logprob": -0.1444539078721055, "compression_ratio": 1.966824644549763, "no_speech_prob": 0.0004853741265833378}, {"id": 40, "seek": 17700, "start": 177.0, "end": 181.0, "text": " No, this should be quicker.", "tokens": [883, 11, 341, 820, 312, 16255, 13], "temperature": 0.0, "avg_logprob": -0.16323399543762207, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000299902050755918}, {"id": 41, "seek": 17700, "start": 181.0, "end": 190.0, "text": " Then when we talk about buffering, what I just said, or in general, storing data state is a very important component for processing.", "tokens": [1396, 562, 321, 751, 466, 9204, 1794, 11, 437, 286, 445, 848, 11, 420, 294, 2674, 11, 26085, 1412, 1785, 307, 257, 588, 1021, 6542, 337, 9007, 13], "temperature": 0.0, "avg_logprob": -0.16323399543762207, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000299902050755918}, {"id": 42, "seek": 17700, "start": 190.0, "end": 198.0, "text": " State can be, for example, a machine learning model that is updated from time to time to classify your incoming streams.", "tokens": [4533, 393, 312, 11, 337, 1365, 11, 257, 3479, 2539, 2316, 300, 307, 10588, 490, 565, 281, 565, 281, 33872, 428, 22341, 15842, 13], "temperature": 0.0, "avg_logprob": -0.16323399543762207, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000299902050755918}, {"id": 43, "seek": 17700, "start": 198.0, "end": 204.0, "text": " It can be a cache if you don't want to look up in the database for every record that comes in.", "tokens": [467, 393, 312, 257, 19459, 498, 291, 500, 380, 528, 281, 574, 493, 294, 264, 8149, 337, 633, 2136, 300, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.16323399543762207, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.000299902050755918}, {"id": 44, "seek": 20400, "start": 204.0, "end": 211.0, "text": " In a general state, a low state can be an order of classified.", "tokens": [682, 257, 2674, 1785, 11, 257, 2295, 1785, 393, 312, 364, 1668, 295, 20627, 13], "temperature": 0.0, "avg_logprob": -0.4226567715029173, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0010922722285613418}, {"id": 45, "seek": 20400, "start": 211.0, "end": 216.0, "text": " And state also, at some point, needs to be acquired.", "tokens": [400, 1785, 611, 11, 412, 512, 935, 11, 2203, 281, 312, 17554, 13], "temperature": 0.0, "avg_logprob": -0.4226567715029173, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0010922722285613418}, {"id": 46, "seek": 20400, "start": 216.0, "end": 225.0, "text": " If you have a state full streaming application, a very useful component is helped by actually making sure that I can create this natural of my streaming repository.", "tokens": [759, 291, 362, 257, 1785, 1577, 11791, 3861, 11, 257, 588, 4420, 6542, 307, 4254, 538, 767, 1455, 988, 300, 286, 393, 1884, 341, 3303, 295, 452, 11791, 25841, 13], "temperature": 0.0, "avg_logprob": -0.4226567715029173, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0010922722285613418}, {"id": 47, "seek": 20400, "start": 225.0, "end": 228.0, "text": " So I wanted a state backup to my streaming application.", "tokens": [407, 286, 1415, 257, 1785, 14807, 281, 452, 11791, 3861, 13], "temperature": 0.0, "avg_logprob": -0.4226567715029173, "compression_ratio": 1.6231884057971016, "no_speech_prob": 0.0010922722285613418}, {"id": 48, "seek": 22800, "start": 228.0, "end": 235.0, "text": " I wanted to make a version of it, so every night I want to create a screenshot version.", "tokens": [286, 1415, 281, 652, 257, 3037, 295, 309, 11, 370, 633, 1818, 286, 528, 281, 1884, 257, 27712, 3037, 13], "temperature": 0.0, "avg_logprob": -0.42538100794741984, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0006549247191287577}, {"id": 49, "seek": 22800, "start": 235.0, "end": 243.0, "text": " Maybe I want a full streaming application in a staging cluster, in a gelatin cluster, and play around with the data in the state.", "tokens": [2704, 286, 528, 257, 1577, 11791, 3861, 294, 257, 41085, 13630, 11, 294, 257, 45174, 13630, 11, 293, 862, 926, 365, 264, 1412, 294, 264, 1785, 13], "temperature": 0.0, "avg_logprob": -0.42538100794741984, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0006549247191287577}, {"id": 50, "seek": 22800, "start": 243.0, "end": 252.0, "text": " Maybe I want to do some testing, or I just want to time track the process of my application.", "tokens": [2704, 286, 528, 281, 360, 512, 4997, 11, 420, 286, 445, 528, 281, 565, 2837, 264, 1399, 295, 452, 3861, 13], "temperature": 0.0, "avg_logprob": -0.42538100794741984, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0006549247191287577}, {"id": 51, "seek": 25200, "start": 252.0, "end": 259.0, "text": " So let's talk a little bit about what makes Flink unique compared to other competitors.", "tokens": [407, 718, 311, 751, 257, 707, 857, 466, 437, 1669, 3235, 475, 3845, 5347, 281, 661, 18333, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 52, "seek": 25200, "start": 259.0, "end": 267.0, "text": " First of all, what I just showed is that Flink is one of the best stream processors for all of these use cases and building blocks.", "tokens": [2386, 295, 439, 11, 437, 286, 445, 4712, 307, 300, 3235, 475, 307, 472, 295, 264, 1151, 4309, 27751, 337, 439, 295, 613, 764, 3331, 293, 2390, 8474, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 53, "seek": 25200, "start": 267.0, "end": 275.0, "text": " So when you design a streaming application, you can start with a whiteboard and you just draw some circles.", "tokens": [407, 562, 291, 1715, 257, 11791, 3861, 11, 291, 393, 722, 365, 257, 2418, 3787, 293, 291, 445, 2642, 512, 13040, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 54, "seek": 25200, "start": 275.0, "end": 276.0, "text": " What do you actually want to do?", "tokens": [708, 360, 291, 767, 528, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 55, "seek": 25200, "start": 276.0, "end": 278.0, "text": " Maybe you want to reform some sources.", "tokens": [2704, 291, 528, 281, 8290, 512, 7139, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 56, "seek": 25200, "start": 278.0, "end": 280.0, "text": " Maybe you want to normalize your data.", "tokens": [2704, 291, 528, 281, 2710, 1125, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 57, "seek": 25200, "start": 280.0, "end": 281.0, "text": " You want to filter some data out.", "tokens": [509, 528, 281, 6608, 512, 1412, 484, 13], "temperature": 0.0, "avg_logprob": -0.17319934651003047, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0002775905595626682}, {"id": 58, "seek": 28100, "start": 281.0, "end": 286.0, "text": " You want to join the data and in the end you really want to sync it somewhere else.", "tokens": [509, 528, 281, 3917, 264, 1412, 293, 294, 264, 917, 291, 534, 528, 281, 20271, 309, 4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 59, "seek": 28100, "start": 286.0, "end": 287.0, "text": " But this is how it starts.", "tokens": [583, 341, 307, 577, 309, 3719, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 60, "seek": 28100, "start": 287.0, "end": 292.0, "text": " And this is also how you have to reason about when you're creating a pipeline.", "tokens": [400, 341, 307, 611, 577, 291, 362, 281, 1778, 466, 562, 291, 434, 4084, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 61, "seek": 28100, "start": 292.0, "end": 298.0, "text": " And what Flink does under the hood is it has a parallelism and scalability built in.", "tokens": [400, 437, 3235, 475, 775, 833, 264, 13376, 307, 309, 575, 257, 8952, 1434, 293, 15664, 2310, 3094, 294, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 62, "seek": 28100, "start": 298.0, "end": 303.0, "text": " So you don't have to think of threading or network transfers or anything like that.", "tokens": [407, 291, 500, 380, 362, 281, 519, 295, 7207, 278, 420, 3209, 29137, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 63, "seek": 28100, "start": 303.0, "end": 308.0, "text": " Under the hood there are sharks, there are petitions depending on the connectors.", "tokens": [6974, 264, 13376, 456, 366, 26312, 11, 456, 366, 3817, 2451, 5413, 322, 264, 31865, 13], "temperature": 0.0, "avg_logprob": -0.11919833518363335, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0003615730383899063}, {"id": 64, "seek": 30800, "start": 308.0, "end": 314.0, "text": " There are sub-tasks that in parallel execute operations.", "tokens": [821, 366, 1422, 12, 83, 296, 1694, 300, 294, 8952, 14483, 7705, 13], "temperature": 0.0, "avg_logprob": -0.28005535571606127, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.0006540233734995127}, {"id": 65, "seek": 30800, "start": 314.0, "end": 322.0, "text": " Each of these tasks, some of these tasks can stay full and can have some storage local to the operator.", "tokens": [6947, 295, 613, 9608, 11, 512, 295, 613, 9608, 393, 1754, 1577, 293, 393, 362, 512, 6725, 2654, 281, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.28005535571606127, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.0006540233734995127}, {"id": 66, "seek": 30800, "start": 322.0, "end": 323.0, "text": " Very important.", "tokens": [4372, 1021, 13], "temperature": 0.0, "avg_logprob": -0.28005535571606127, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.0006540233734995127}, {"id": 67, "seek": 30800, "start": 323.0, "end": 328.0, "text": " So the state basically scales out and scales in with the operator.", "tokens": [407, 264, 1785, 1936, 17408, 484, 293, 17408, 294, 365, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.28005535571606127, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.0006540233734995127}, {"id": 68, "seek": 30800, "start": 328.0, "end": 334.0, "text": " You don't need to vary to a database which would increase data.", "tokens": [509, 500, 380, 643, 281, 10559, 281, 257, 8149, 597, 576, 3488, 1412, 13], "temperature": 0.0, "avg_logprob": -0.28005535571606127, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.0006540233734995127}, {"id": 69, "seek": 33400, "start": 334.0, "end": 341.0, "text": " And then, of course, it travels and then the whole pipeline runs.", "tokens": [400, 550, 11, 295, 1164, 11, 309, 19863, 293, 550, 264, 1379, 15517, 6676, 13], "temperature": 0.0, "avg_logprob": -0.3841074122938999, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0014147005276754498}, {"id": 70, "seek": 33400, "start": 341.0, "end": 343.0, "text": " And now comes the important part.", "tokens": [400, 586, 1487, 264, 1021, 644, 13], "temperature": 0.0, "avg_logprob": -0.3841074122938999, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0014147005276754498}, {"id": 71, "seek": 33400, "start": 343.0, "end": 352.0, "text": " What Flink explained really unique is that this possibility of creating an existence snapshot of your entire stream of technology.", "tokens": [708, 3235, 475, 8825, 534, 3845, 307, 300, 341, 7959, 295, 4084, 364, 9123, 30163, 295, 428, 2302, 4309, 295, 2899, 13], "temperature": 0.0, "avg_logprob": -0.3841074122938999, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0014147005276754498}, {"id": 72, "seek": 33400, "start": 352.0, "end": 358.0, "text": " So there are, like, what we call the checkpoint barriers which are traveling through the topology", "tokens": [407, 456, 366, 11, 411, 11, 437, 321, 818, 264, 42269, 13565, 597, 366, 9712, 807, 264, 1192, 1793], "temperature": 0.0, "avg_logprob": -0.3841074122938999, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0014147005276754498}, {"id": 73, "seek": 33400, "start": 358.0, "end": 362.0, "text": " and make a backup of each state of the operator.", "tokens": [293, 652, 257, 14807, 295, 1184, 1785, 295, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.3841074122938999, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.0014147005276754498}, {"id": 74, "seek": 36200, "start": 362.0, "end": 375.0, "text": " And then this snapshot is then persisted on a long-term storage like S3 or HDFS or some other distributed kind of system.", "tokens": [400, 550, 341, 30163, 307, 550, 13233, 292, 322, 257, 938, 12, 7039, 6725, 411, 318, 18, 420, 12149, 29318, 420, 512, 661, 12631, 733, 295, 1185, 13], "temperature": 0.0, "avg_logprob": -0.2682398659842355, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.0006629670388065279}, {"id": 75, "seek": 36200, "start": 375.0, "end": 380.0, "text": " When we talk about use cases, there are plenty of use cases.", "tokens": [1133, 321, 751, 466, 764, 3331, 11, 456, 366, 7140, 295, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.2682398659842355, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.0006629670388065279}, {"id": 76, "seek": 36200, "start": 380.0, "end": 388.0, "text": " We have the process transactions, docs, IOTs, any kind of events, user interactions.", "tokens": [492, 362, 264, 1399, 16856, 11, 45623, 11, 286, 5068, 82, 11, 604, 733, 295, 3931, 11, 4195, 13280, 13], "temperature": 0.0, "avg_logprob": -0.2682398659842355, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.0006629670388065279}, {"id": 77, "seek": 38800, "start": 388.0, "end": 397.0, "text": " People use it for broad detection for machine learning, for event-driven applications, for ETL, for data integration, for analytics.", "tokens": [3432, 764, 309, 337, 4152, 17784, 337, 3479, 2539, 11, 337, 2280, 12, 25456, 5821, 11, 337, 36953, 43, 11, 337, 1412, 10980, 11, 337, 15370, 13], "temperature": 0.0, "avg_logprob": -0.26456927045991147, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.0003617955662775785}, {"id": 78, "seek": 38800, "start": 397.0, "end": 403.0, "text": " So Flink has become, over the last 10 years, it has become like a very large platform.", "tokens": [407, 3235, 475, 575, 1813, 11, 670, 264, 1036, 1266, 924, 11, 309, 575, 1813, 411, 257, 588, 2416, 3663, 13], "temperature": 0.0, "avg_logprob": -0.26456927045991147, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.0003617955662775785}, {"id": 79, "seek": 38800, "start": 403.0, "end": 413.0, "text": " You can connect various connectors from business stream systems, you can read and write files, databases, key value stores.", "tokens": [509, 393, 1745, 3683, 31865, 490, 1606, 4309, 3652, 11, 291, 393, 1401, 293, 2464, 7098, 11, 22380, 11, 2141, 2158, 9512, 13], "temperature": 0.0, "avg_logprob": -0.26456927045991147, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.0003617955662775785}, {"id": 80, "seek": 41300, "start": 413.0, "end": 420.0, "text": " And as I said, like also event-driven applications where maybe you want to send out an email or don't need a connector.", "tokens": [400, 382, 286, 848, 11, 411, 611, 2280, 12, 25456, 5821, 689, 1310, 291, 528, 281, 2845, 484, 364, 3796, 420, 500, 380, 643, 257, 19127, 13], "temperature": 0.0, "avg_logprob": -0.2551942519199701, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.000309450930217281}, {"id": 81, "seek": 41300, "start": 420.0, "end": 424.0, "text": " You can also implement something custom that talks to somewhere as the API.", "tokens": [509, 393, 611, 4445, 746, 2375, 300, 6686, 281, 4079, 382, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2551942519199701, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.000309450930217281}, {"id": 82, "seek": 41300, "start": 424.0, "end": 429.0, "text": " So let's look at this scene.", "tokens": [407, 718, 311, 574, 412, 341, 4145, 13], "temperature": 0.0, "avg_logprob": -0.2551942519199701, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.000309450930217281}, {"id": 83, "seek": 41300, "start": 429.0, "end": 433.0, "text": " So I also want to quickly talk about Flink's API.", "tokens": [407, 286, 611, 528, 281, 2661, 751, 466, 3235, 475, 311, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2551942519199701, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.000309450930217281}, {"id": 84, "seek": 41300, "start": 433.0, "end": 437.0, "text": " So this is the API stack.", "tokens": [407, 341, 307, 264, 9362, 8630, 13], "temperature": 0.0, "avg_logprob": -0.2551942519199701, "compression_ratio": 1.5228426395939085, "no_speech_prob": 0.000309450930217281}, {"id": 85, "seek": 43700, "start": 437.0, "end": 444.0, "text": " The two main APIs are data stream API, table API or table table API.", "tokens": [440, 732, 2135, 21445, 366, 1412, 4309, 9362, 11, 3199, 9362, 420, 3199, 3199, 9362, 13], "temperature": 0.0, "avg_logprob": -0.32522840348501053, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0005961508140899241}, {"id": 86, "seek": 43700, "start": 444.0, "end": 446.0, "text": " And there is also stateful functions.", "tokens": [400, 456, 307, 611, 1785, 906, 6828, 13], "temperature": 0.0, "avg_logprob": -0.32522840348501053, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0005961508140899241}, {"id": 87, "seek": 43700, "start": 446.0, "end": 460.0, "text": " Stateful functions is a sub-project that tries to execute an actor model on a page of Flink, but will not go into detail here.", "tokens": [4533, 906, 6828, 307, 257, 1422, 12, 4318, 1020, 300, 9898, 281, 14483, 364, 8747, 2316, 322, 257, 3028, 295, 3235, 475, 11, 457, 486, 406, 352, 666, 2607, 510, 13], "temperature": 0.0, "avg_logprob": -0.32522840348501053, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0005961508140899241}, {"id": 88, "seek": 46000, "start": 460.0, "end": 468.0, "text": " So first of all, like all the APIs are built on a data flow runtime, so there's no batching or anything involved under the hood.", "tokens": [407, 700, 295, 439, 11, 411, 439, 264, 21445, 366, 3094, 322, 257, 1412, 3095, 34474, 11, 370, 456, 311, 572, 15245, 278, 420, 1340, 3288, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 89, "seek": 46000, "start": 468.0, "end": 470.0, "text": " It's really a data flow runtime.", "tokens": [467, 311, 534, 257, 1412, 3095, 34474, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 90, "seek": 46000, "start": 470.0, "end": 475.0, "text": " Whatever the result is ready, it will be streamed to the next operator.", "tokens": [8541, 264, 1874, 307, 1919, 11, 309, 486, 312, 4309, 292, 281, 264, 958, 12973, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 91, "seek": 46000, "start": 475.0, "end": 481.0, "text": " On top of that, there is a loader, the stream operator API, which you can use.", "tokens": [1282, 1192, 295, 300, 11, 456, 307, 257, 3677, 260, 11, 264, 4309, 12973, 9362, 11, 597, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 92, "seek": 46000, "start": 481.0, "end": 483.0, "text": " But yeah, this is for X, of course, I would say.", "tokens": [583, 1338, 11, 341, 307, 337, 1783, 11, 295, 1164, 11, 286, 576, 584, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 93, "seek": 46000, "start": 483.0, "end": 486.0, "text": " And then we have the mainstream APIs on top.", "tokens": [400, 550, 321, 362, 264, 15960, 21445, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.2321836739255671, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0002522789582144469}, {"id": 94, "seek": 48600, "start": 486.0, "end": 495.0, "text": " And the specialty about table table API is that there is an optimizer on a planning stage in between.", "tokens": [400, 264, 22000, 466, 3199, 3199, 9362, 307, 300, 456, 307, 364, 5028, 6545, 322, 257, 5038, 3233, 294, 1296, 13], "temperature": 0.0, "avg_logprob": -0.19764245880974662, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00028531489078886807}, {"id": 95, "seek": 48600, "start": 495.0, "end": 501.0, "text": " So the C helps you when you're not creating the most efficient pipelines.", "tokens": [407, 264, 383, 3665, 291, 562, 291, 434, 406, 4084, 264, 881, 7148, 40168, 13], "temperature": 0.0, "avg_logprob": -0.19764245880974662, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00028531489078886807}, {"id": 96, "seek": 48600, "start": 501.0, "end": 507.0, "text": " The optimizer will make sure that the streaming will be executed more efficiently.", "tokens": [440, 5028, 6545, 486, 652, 988, 300, 264, 11791, 486, 312, 17577, 544, 19621, 13], "temperature": 0.0, "avg_logprob": -0.19764245880974662, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00028531489078886807}, {"id": 97, "seek": 48600, "start": 507.0, "end": 510.0, "text": " Yeah, let's also quickly look at the APIs.", "tokens": [865, 11, 718, 311, 611, 2661, 574, 412, 264, 21445, 13], "temperature": 0.0, "avg_logprob": -0.19764245880974662, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.00028531489078886807}, {"id": 98, "seek": 51000, "start": 510.0, "end": 519.0, "text": " So this is like a basic example of creating a stream for just three elements.", "tokens": [407, 341, 307, 411, 257, 3875, 1365, 295, 4084, 257, 4309, 337, 445, 1045, 4959, 13], "temperature": 0.0, "avg_logprob": -0.2897225041543284, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.00033918043482117355}, {"id": 99, "seek": 51000, "start": 519.0, "end": 527.0, "text": " And then you're executing this on a cluster or in your IDE, then you're retrieving the result back.", "tokens": [400, 550, 291, 434, 32368, 341, 322, 257, 13630, 420, 294, 428, 40930, 11, 550, 291, 434, 19817, 798, 264, 1874, 646, 13], "temperature": 0.0, "avg_logprob": -0.2897225041543284, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.00033918043482117355}, {"id": 100, "seek": 51000, "start": 527.0, "end": 531.0, "text": " And you have an iterator locally, you can just print it locally.", "tokens": [400, 291, 362, 364, 17138, 1639, 16143, 11, 291, 393, 445, 4482, 309, 16143, 13], "temperature": 0.0, "avg_logprob": -0.2897225041543284, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.00033918043482117355}, {"id": 101, "seek": 53100, "start": 531.0, "end": 541.0, "text": " It's not very useful, but this is a minimal example of the Java API.", "tokens": [467, 311, 406, 588, 4420, 11, 457, 341, 307, 257, 13206, 1365, 295, 264, 10745, 9362, 13], "temperature": 0.0, "avg_logprob": -0.15962226494498874, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0003045556368306279}, {"id": 102, "seek": 53100, "start": 541.0, "end": 551.0, "text": " The important thing is that the stream API basically exposes all the building blocks that I mentioned on my previous slide.", "tokens": [440, 1021, 551, 307, 300, 264, 4309, 9362, 1936, 1278, 4201, 439, 264, 2390, 8474, 300, 286, 2835, 322, 452, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.15962226494498874, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0003045556368306279}, {"id": 103, "seek": 55100, "start": 551.0, "end": 562.0, "text": " So you can have very abstract operator typologies, and you can use built-in functions like map, process, and connect.", "tokens": [407, 291, 393, 362, 588, 12649, 12973, 2125, 6204, 11, 293, 291, 393, 764, 3094, 12, 259, 6828, 411, 4471, 11, 1399, 11, 293, 1745, 13], "temperature": 0.0, "avg_logprob": -0.25867516012752756, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0003047620411962271}, {"id": 104, "seek": 55100, "start": 562.0, "end": 568.0, "text": " Which each of them takes different functions, and then you can really define your business logic in those.", "tokens": [3013, 1184, 295, 552, 2516, 819, 6828, 11, 293, 550, 291, 393, 534, 6964, 428, 1606, 9952, 294, 729, 13], "temperature": 0.0, "avg_logprob": -0.25867516012752756, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0003047620411962271}, {"id": 105, "seek": 55100, "start": 568.0, "end": 573.0, "text": " They use different functions, and you can also use completely arbitrary Java records.", "tokens": [814, 764, 819, 6828, 11, 293, 291, 393, 611, 764, 2584, 23211, 10745, 7724, 13], "temperature": 0.0, "avg_logprob": -0.25867516012752756, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0003047620411962271}, {"id": 106, "seek": 55100, "start": 573.0, "end": 580.0, "text": " For example, Python records that flow between the operator and conceptually.", "tokens": [1171, 1365, 11, 15329, 7724, 300, 3095, 1296, 264, 12973, 293, 3410, 671, 13], "temperature": 0.0, "avg_logprob": -0.25867516012752756, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0003047620411962271}, {"id": 107, "seek": 58000, "start": 580.0, "end": 583.0, "text": " This is interesting when we talk about change data capture.", "tokens": [639, 307, 1880, 562, 321, 751, 466, 1319, 1412, 7983, 13], "temperature": 0.0, "avg_logprob": -0.24276853115000624, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0005250237882137299}, {"id": 108, "seek": 58000, "start": 583.0, "end": 587.0, "text": " Conceptually, the data stream API does not know about changes.", "tokens": [47482, 671, 11, 264, 1412, 4309, 9362, 775, 406, 458, 466, 2962, 13], "temperature": 0.0, "avg_logprob": -0.24276853115000624, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0005250237882137299}, {"id": 109, "seek": 58000, "start": 587.0, "end": 593.0, "text": " It only knows about records, so there is no change flag or anything like that.", "tokens": [467, 787, 3255, 466, 7724, 11, 370, 456, 307, 572, 1319, 7166, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.24276853115000624, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0005250237882137299}, {"id": 110, "seek": 58000, "start": 593.0, "end": 599.0, "text": " So conceptually, the data stream API is an app that can only or insert only as long.", "tokens": [407, 3410, 671, 11, 264, 1412, 4309, 9362, 307, 364, 724, 300, 393, 787, 420, 8969, 787, 382, 938, 13], "temperature": 0.0, "avg_logprob": -0.24276853115000624, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0005250237882137299}, {"id": 111, "seek": 58000, "start": 599.0, "end": 606.0, "text": " And also when you look at the output, one, two, three, four, five, six.", "tokens": [400, 611, 562, 291, 574, 412, 264, 5598, 11, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 13], "temperature": 0.0, "avg_logprob": -0.24276853115000624, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0005250237882137299}, {"id": 112, "seek": 60600, "start": 606.0, "end": 610.0, "text": " So let's take a look at table API and simple API.", "tokens": [407, 718, 311, 747, 257, 574, 412, 3199, 9362, 293, 2199, 9362, 13], "temperature": 0.0, "avg_logprob": -0.26456419626871747, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.00040294454083777964}, {"id": 113, "seek": 60600, "start": 610.0, "end": 617.0, "text": " So usually you just say to this table API, order in SQL, because it's a unified API.", "tokens": [407, 2673, 291, 445, 584, 281, 341, 3199, 9362, 11, 1668, 294, 19200, 11, 570, 309, 311, 257, 26787, 9362, 13], "temperature": 0.0, "avg_logprob": -0.26456419626871747, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.00040294454083777964}, {"id": 114, "seek": 60600, "start": 617.0, "end": 622.0, "text": " You can decide whether you want to electrify your pipeline programmatically,", "tokens": [509, 393, 4536, 1968, 291, 528, 281, 7072, 2505, 428, 15517, 37648, 5030, 11], "temperature": 0.0, "avg_logprob": -0.26456419626871747, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.00040294454083777964}, {"id": 115, "seek": 60600, "start": 622.0, "end": 628.0, "text": " or whether you want to use standard SQL for defining your topology.", "tokens": [420, 1968, 291, 528, 281, 764, 3832, 19200, 337, 17827, 428, 1192, 1793, 13], "temperature": 0.0, "avg_logprob": -0.26456419626871747, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.00040294454083777964}, {"id": 116, "seek": 62800, "start": 628.0, "end": 636.0, "text": " In the end, you also execute and you can also print locally in your IDP.", "tokens": [682, 264, 917, 11, 291, 611, 14483, 293, 291, 393, 611, 4482, 16143, 294, 428, 7348, 47, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 117, "seek": 62800, "start": 636.0, "end": 640.0, "text": " Here, this API abstracts all building blocks.", "tokens": [1692, 11, 341, 9362, 12649, 82, 439, 2390, 8474, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 118, "seek": 62800, "start": 640.0, "end": 643.0, "text": " So you have no access to timers or state or anything like this.", "tokens": [407, 291, 362, 572, 2105, 281, 524, 433, 420, 1785, 420, 1340, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 119, "seek": 62800, "start": 643.0, "end": 645.0, "text": " This will be on the foot.", "tokens": [639, 486, 312, 322, 264, 2671, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 120, "seek": 62800, "start": 645.0, "end": 650.0, "text": " Also the operator topology is determined by the planner, not by you.", "tokens": [2743, 264, 12973, 1192, 1793, 307, 9540, 538, 264, 31268, 11, 406, 538, 291, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 121, "seek": 62800, "start": 650.0, "end": 655.0, "text": " The nice thing here is you can focus on your business logic.", "tokens": [440, 1481, 551, 510, 307, 291, 393, 1879, 322, 428, 1606, 9952, 13], "temperature": 0.0, "avg_logprob": -0.17082060938296112, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0005018314113840461}, {"id": 122, "seek": 65500, "start": 655.0, "end": 660.0, "text": " And you do this declaratively to optimize your business durations", "tokens": [400, 291, 360, 341, 16694, 19020, 281, 19719, 428, 1606, 4861, 763], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 123, "seek": 65500, "start": 660.0, "end": 663.0, "text": " and make something out of it.", "tokens": [293, 652, 746, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 124, "seek": 65500, "start": 663.0, "end": 671.0, "text": " Internally, it uses highly efficient records, also up to the engine, not to you.", "tokens": [4844, 379, 11, 309, 4960, 5405, 7148, 7724, 11, 611, 493, 281, 264, 2848, 11, 406, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 125, "seek": 65500, "start": 671.0, "end": 674.0, "text": " What you will see maybe is like a road type.", "tokens": [708, 291, 486, 536, 1310, 307, 411, 257, 3060, 2010, 13], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 126, "seek": 65500, "start": 674.0, "end": 678.0, "text": " If you really want to go out of table API, then you see a road type,", "tokens": [759, 291, 534, 528, 281, 352, 484, 295, 3199, 9362, 11, 550, 291, 536, 257, 3060, 2010, 11], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 127, "seek": 65500, "start": 678.0, "end": 682.0, "text": " which can work as a business program.", "tokens": [597, 393, 589, 382, 257, 1606, 1461, 13], "temperature": 0.0, "avg_logprob": -0.2359812232885468, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0003677269851323217}, {"id": 128, "seek": 68200, "start": 682.0, "end": 687.0, "text": " And the interesting thing here is that conceptually, we are working with tables here,", "tokens": [400, 264, 1880, 551, 510, 307, 300, 3410, 671, 11, 321, 366, 1364, 365, 8020, 510, 11], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 129, "seek": 68200, "start": 687.0, "end": 690.0, "text": " tables and views, you know, databases.", "tokens": [8020, 293, 6809, 11, 291, 458, 11, 22380, 13], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 130, "seek": 68200, "start": 690.0, "end": 694.0, "text": " But under the hood, there is actually a change level.", "tokens": [583, 833, 264, 13376, 11, 456, 307, 767, 257, 1319, 1496, 13], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 131, "seek": 68200, "start": 694.0, "end": 698.0, "text": " And that's what I want to show in the following slides.", "tokens": [400, 300, 311, 437, 286, 528, 281, 855, 294, 264, 3480, 9788, 13], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 132, "seek": 68200, "start": 698.0, "end": 703.0, "text": " But you can also see that, like for example, if you're disappearing here,", "tokens": [583, 291, 393, 611, 536, 300, 11, 411, 337, 1365, 11, 498, 291, 434, 34900, 510, 11], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 133, "seek": 68200, "start": 703.0, "end": 708.0, "text": " you will get this output when you run it in the IDQ.", "tokens": [291, 486, 483, 341, 5598, 562, 291, 1190, 309, 294, 264, 7348, 48, 13], "temperature": 0.0, "avg_logprob": -0.2682349681854248, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.0004565196286421269}, {"id": 134, "seek": 70800, "start": 708.0, "end": 715.0, "text": " And you already see that there is, of course, an F0 column with the 123 output.", "tokens": [400, 291, 1217, 536, 300, 456, 307, 11, 295, 1164, 11, 364, 479, 15, 7738, 365, 264, 34466, 5598, 13], "temperature": 0.0, "avg_logprob": -0.19901221990585327, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0002726516395341605}, {"id": 135, "seek": 70800, "start": 715.0, "end": 718.0, "text": " But there is an additional column, first column,", "tokens": [583, 456, 307, 364, 4497, 7738, 11, 700, 7738, 11], "temperature": 0.0, "avg_logprob": -0.19901221990585327, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0002726516395341605}, {"id": 136, "seek": 70800, "start": 718.0, "end": 725.0, "text": " which already shows that there is some change like attached to every record.", "tokens": [597, 1217, 3110, 300, 456, 307, 512, 1319, 411, 8570, 281, 633, 2136, 13], "temperature": 0.0, "avg_logprob": -0.19901221990585327, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0002726516395341605}, {"id": 137, "seek": 70800, "start": 725.0, "end": 729.0, "text": " In this case, it's just insert.", "tokens": [682, 341, 1389, 11, 309, 311, 445, 8969, 13], "temperature": 0.0, "avg_logprob": -0.19901221990585327, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0002726516395341605}, {"id": 138, "seek": 70800, "start": 729.0, "end": 734.0, "text": " The nice thing about Linux APIs is that you can mix and match them.", "tokens": [440, 1481, 551, 466, 18734, 21445, 307, 300, 291, 393, 2890, 293, 2995, 552, 13], "temperature": 0.0, "avg_logprob": -0.19901221990585327, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0002726516395341605}, {"id": 139, "seek": 73400, "start": 734.0, "end": 738.0, "text": " So you can, for example, start with the S3 API,", "tokens": [407, 291, 393, 11, 337, 1365, 11, 722, 365, 264, 318, 18, 9362, 11], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 140, "seek": 73400, "start": 738.0, "end": 741.0, "text": " and then you go to table API, or the other way around.", "tokens": [293, 550, 291, 352, 281, 3199, 9362, 11, 420, 264, 661, 636, 926, 13], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 141, "seek": 73400, "start": 741.0, "end": 744.0, "text": " If you have SQL, you can do the detail in SQL first.", "tokens": [759, 291, 362, 19200, 11, 291, 393, 360, 264, 2607, 294, 19200, 700, 13], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 142, "seek": 73400, "start": 744.0, "end": 747.0, "text": " And then if you have some more complex logic,", "tokens": [400, 550, 498, 291, 362, 512, 544, 3997, 9952, 11], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 143, "seek": 73400, "start": 747.0, "end": 751.0, "text": " like timer services, or like a very complex state,", "tokens": [411, 19247, 3328, 11, 420, 411, 257, 588, 3997, 1785, 11], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 144, "seek": 73400, "start": 751.0, "end": 755.0, "text": " or whatever, then you can go to the S3 API, do it there,", "tokens": [420, 2035, 11, 550, 291, 393, 352, 281, 264, 318, 18, 9362, 11, 360, 309, 456, 11], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 145, "seek": 73400, "start": 755.0, "end": 759.0, "text": " and then you can switch back to SQL, or you can just use the SQL for Nectar.", "tokens": [293, 550, 291, 393, 3679, 646, 281, 19200, 11, 420, 291, 393, 445, 764, 264, 19200, 337, 426, 557, 289, 13], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 146, "seek": 73400, "start": 759.0, "end": 762.0, "text": " But you find the entire pipeline in the S3 API.", "tokens": [583, 291, 915, 264, 2302, 15517, 294, 264, 318, 18, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2659632267842766, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0002859644591808319}, {"id": 147, "seek": 76200, "start": 762.0, "end": 765.0, "text": " So that is up to you.", "tokens": [407, 300, 307, 493, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.20512336492538452, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.00014176346303429455}, {"id": 148, "seek": 76200, "start": 765.0, "end": 775.0, "text": " But yeah, the APIs for that are present to go back and forth between those two.", "tokens": [583, 1338, 11, 264, 21445, 337, 300, 366, 1974, 281, 352, 646, 293, 5220, 1296, 729, 732, 13], "temperature": 0.0, "avg_logprob": -0.20512336492538452, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.00014176346303429455}, {"id": 149, "seek": 76200, "start": 775.0, "end": 783.0, "text": " So now let's really talk about change.s3 processing.", "tokens": [407, 586, 718, 311, 534, 751, 466, 1319, 13, 82, 18, 9007, 13], "temperature": 0.0, "avg_logprob": -0.20512336492538452, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.00014176346303429455}, {"id": 150, "seek": 76200, "start": 783.0, "end": 786.0, "text": " If you think about data processing,", "tokens": [759, 291, 519, 466, 1412, 9007, 11], "temperature": 0.0, "avg_logprob": -0.20512336492538452, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.00014176346303429455}, {"id": 151, "seek": 76200, "start": 786.0, "end": 788.0, "text": " like in most of the cases,", "tokens": [411, 294, 881, 295, 264, 3331, 11], "temperature": 0.0, "avg_logprob": -0.20512336492538452, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.00014176346303429455}, {"id": 152, "seek": 78800, "start": 788.0, "end": 792.0, "text": " the main data processing is always consuming a stream of changes.", "tokens": [264, 2135, 1412, 9007, 307, 1009, 19867, 257, 4309, 295, 2962, 13], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 153, "seek": 78800, "start": 792.0, "end": 797.0, "text": " Because if this would not be like a continuous input stream,", "tokens": [1436, 498, 341, 576, 406, 312, 411, 257, 10957, 4846, 4309, 11], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 154, "seek": 78800, "start": 797.0, "end": 801.0, "text": " then your company, your project, whatever it would actually be,", "tokens": [550, 428, 2237, 11, 428, 1716, 11, 2035, 309, 576, 767, 312, 11], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 155, "seek": 78800, "start": 801.0, "end": 802.0, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 156, "seek": 78800, "start": 802.0, "end": 807.0, "text": " So like it is actually very common that data flows in continuously.", "tokens": [407, 411, 309, 307, 767, 588, 2689, 300, 1412, 12867, 294, 15684, 13], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 157, "seek": 78800, "start": 807.0, "end": 815.0, "text": " And the Flinky APIs, the Flink runtime sees everything basically as a stream.", "tokens": [400, 264, 3235, 22998, 21445, 11, 264, 3235, 475, 34474, 8194, 1203, 1936, 382, 257, 4309, 13], "temperature": 0.0, "avg_logprob": -0.30483834140272026, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.00036763559910468757}, {"id": 158, "seek": 81500, "start": 815.0, "end": 820.0, "text": " And it just distinguishes between a bounded stream and unbounded stream.", "tokens": [400, 309, 445, 11365, 16423, 1296, 257, 37498, 4309, 293, 517, 18767, 292, 4309, 13], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 159, "seek": 81500, "start": 820.0, "end": 825.0, "text": " So bounded means you define a start and an end,", "tokens": [407, 37498, 1355, 291, 6964, 257, 722, 293, 364, 917, 11], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 160, "seek": 81500, "start": 825.0, "end": 828.0, "text": " and the end was coming there.", "tokens": [293, 264, 917, 390, 1348, 456, 13], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 161, "seek": 81500, "start": 828.0, "end": 832.0, "text": " Unbounded means you start somewhere,", "tokens": [1156, 18767, 292, 1355, 291, 722, 4079, 11], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 162, "seek": 81500, "start": 832.0, "end": 834.0, "text": " and now it can be somewhere in the past,", "tokens": [293, 586, 309, 393, 312, 4079, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 163, "seek": 81500, "start": 834.0, "end": 837.0, "text": " and then you start processing the future.", "tokens": [293, 550, 291, 722, 9007, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 164, "seek": 81500, "start": 837.0, "end": 839.0, "text": " So this is up to you.", "tokens": [407, 341, 307, 493, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.14735242797107231, "compression_ratio": 1.7696969696969698, "no_speech_prob": 0.00016579916700720787}, {"id": 165, "seek": 83900, "start": 839.0, "end": 845.0, "text": " Yeah, if you really think a bit about this,", "tokens": [865, 11, 498, 291, 534, 519, 257, 857, 466, 341, 11], "temperature": 0.0, "avg_logprob": -0.13074211393083846, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0003969323297496885}, {"id": 166, "seek": 83900, "start": 845.0, "end": 850.0, "text": " actually batch processing is just a special case of stream processing.", "tokens": [767, 15245, 9007, 307, 445, 257, 2121, 1389, 295, 4309, 9007, 13], "temperature": 0.0, "avg_logprob": -0.13074211393083846, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0003969323297496885}, {"id": 167, "seek": 83900, "start": 850.0, "end": 856.0, "text": " So batch processing means that through the bounded nature of the stream,", "tokens": [407, 15245, 9007, 1355, 300, 807, 264, 37498, 3687, 295, 264, 4309, 11], "temperature": 0.0, "avg_logprob": -0.13074211393083846, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0003969323297496885}, {"id": 168, "seek": 83900, "start": 856.0, "end": 861.0, "text": " I can maybe do some more specialized operators like sorting, for example.", "tokens": [286, 393, 1310, 360, 512, 544, 19813, 19077, 411, 32411, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13074211393083846, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0003969323297496885}, {"id": 169, "seek": 83900, "start": 861.0, "end": 864.0, "text": " It's easier in such a thing.", "tokens": [467, 311, 3571, 294, 1270, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.13074211393083846, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0003969323297496885}, {"id": 170, "seek": 86400, "start": 864.0, "end": 869.0, "text": " And you can also use different algorithms if you have sorted like this to a sort of a join,", "tokens": [400, 291, 393, 611, 764, 819, 14642, 498, 291, 362, 25462, 411, 341, 281, 257, 1333, 295, 257, 3917, 11], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 171, "seek": 86400, "start": 869.0, "end": 870.0, "text": " or something like this.", "tokens": [420, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 172, "seek": 86400, "start": 870.0, "end": 876.0, "text": " So that the runtime has special operators and special handling of bounded streams.", "tokens": [407, 300, 264, 34474, 575, 2121, 19077, 293, 2121, 13175, 295, 37498, 15842, 13], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 173, "seek": 86400, "start": 876.0, "end": 881.0, "text": " But in general, you can process everything for the stream.", "tokens": [583, 294, 2674, 11, 291, 393, 1399, 1203, 337, 264, 4309, 13], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 174, "seek": 86400, "start": 881.0, "end": 887.0, "text": " So both bounded and unbounded data.", "tokens": [407, 1293, 37498, 293, 517, 18767, 292, 1412, 13], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 175, "seek": 86400, "start": 887.0, "end": 891.0, "text": " So how does actually things look like?", "tokens": [407, 577, 775, 767, 721, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.31108807354438595, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0003561359771993011}, {"id": 176, "seek": 89100, "start": 891.0, "end": 895.0, "text": " So how can I work with streams and things like that?", "tokens": [407, 577, 393, 286, 589, 365, 15842, 293, 721, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 177, "seek": 89100, "start": 895.0, "end": 899.0, "text": " So the first answer to this, or I mentioned before,", "tokens": [407, 264, 700, 1867, 281, 341, 11, 420, 286, 2835, 949, 11], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 178, "seek": 89100, "start": 899.0, "end": 901.0, "text": " you actually don't work with streams.", "tokens": [291, 767, 500, 380, 589, 365, 15842, 13], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 179, "seek": 89100, "start": 901.0, "end": 904.0, "text": " So what you work with is dynamic tables.", "tokens": [407, 437, 291, 589, 365, 307, 8546, 8020, 13], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 180, "seek": 89100, "start": 904.0, "end": 907.0, "text": " So this is just a concept we call the dynamic tables.", "tokens": [407, 341, 307, 445, 257, 3410, 321, 818, 264, 8546, 8020, 13], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 181, "seek": 89100, "start": 907.0, "end": 912.0, "text": " It's a concept similar to materialized views and materialized view maintenance.", "tokens": [467, 311, 257, 3410, 2531, 281, 2527, 1602, 6809, 293, 2527, 1602, 1910, 11258, 13], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 182, "seek": 89100, "start": 912.0, "end": 917.0, "text": " So what you do as a user is you define your tables.", "tokens": [407, 437, 291, 360, 382, 257, 4195, 307, 291, 6964, 428, 8020, 13], "temperature": 0.0, "avg_logprob": -0.15714312091316143, "compression_ratio": 1.8, "no_speech_prob": 0.00020317619782872498}, {"id": 183, "seek": 91700, "start": 917.0, "end": 921.0, "text": " So on the left side, we have transactions on the right side.", "tokens": [407, 322, 264, 1411, 1252, 11, 321, 362, 16856, 322, 264, 558, 1252, 13], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 184, "seek": 91700, "start": 921.0, "end": 922.0, "text": " We have maybe revenue.", "tokens": [492, 362, 1310, 9324, 13], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 185, "seek": 91700, "start": 922.0, "end": 928.0, "text": " And then you define, in the middle, you define a standing, running SQL31,", "tokens": [400, 550, 291, 6964, 11, 294, 264, 2808, 11, 291, 6964, 257, 4877, 11, 2614, 19200, 12967, 11], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 186, "seek": 91700, "start": 928.0, "end": 935.0, "text": " which gets translated into a pipeline methodology next to the private branch.", "tokens": [597, 2170, 16805, 666, 257, 15517, 24850, 958, 281, 264, 4551, 9819, 13], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 187, "seek": 91700, "start": 935.0, "end": 940.0, "text": " So then the question is, OK, if we have this big SQL kind of a database,", "tokens": [407, 550, 264, 1168, 307, 11, 2264, 11, 498, 321, 362, 341, 955, 19200, 733, 295, 257, 8149, 11], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 188, "seek": 91700, "start": 940.0, "end": 944.0, "text": " and the answer to that is no, it's not a database,", "tokens": [293, 264, 1867, 281, 300, 307, 572, 11, 309, 311, 406, 257, 8149, 11], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 189, "seek": 91700, "start": 944.0, "end": 946.0, "text": " because we are not in charge of the data.", "tokens": [570, 321, 366, 406, 294, 4602, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2685812153947463, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.00012918905122205615}, {"id": 190, "seek": 94600, "start": 946.0, "end": 949.0, "text": " So you can bring your own data and your own systems.", "tokens": [407, 291, 393, 1565, 428, 1065, 1412, 293, 428, 1065, 3652, 13], "temperature": 0.0, "avg_logprob": -0.19693688785328584, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.00020950160978827626}, {"id": 191, "seek": 94600, "start": 949.0, "end": 952.0, "text": " So it's more like a process.", "tokens": [407, 309, 311, 544, 411, 257, 1399, 13], "temperature": 0.0, "avg_logprob": -0.19693688785328584, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.00020950160978827626}, {"id": 192, "seek": 94600, "start": 952.0, "end": 955.0, "text": " It leads from all different kinds of systems.", "tokens": [467, 6689, 490, 439, 819, 3685, 295, 3652, 13], "temperature": 0.0, "avg_logprob": -0.19693688785328584, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.00020950160978827626}, {"id": 193, "seek": 94600, "start": 955.0, "end": 967.0, "text": " So if a table is not a stream, or if I don't work with streams,", "tokens": [407, 498, 257, 3199, 307, 406, 257, 4309, 11, 420, 498, 286, 500, 380, 589, 365, 15842, 11], "temperature": 0.0, "avg_logprob": -0.19693688785328584, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.00020950160978827626}, {"id": 194, "seek": 94600, "start": 967.0, "end": 970.0, "text": " how does that actually relate with each other?", "tokens": [577, 775, 300, 767, 10961, 365, 1184, 661, 30], "temperature": 0.0, "avg_logprob": -0.19693688785328584, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.00020950160978827626}, {"id": 195, "seek": 97000, "start": 970.0, "end": 980.0, "text": " And an interesting piece of interesting term here is called stream table duality.", "tokens": [400, 364, 1880, 2522, 295, 1880, 1433, 510, 307, 1219, 4309, 3199, 11848, 507, 13], "temperature": 0.0, "avg_logprob": -0.11310823543651684, "compression_ratio": 1.6961325966850829, "no_speech_prob": 0.0002302684442838654}, {"id": 196, "seek": 97000, "start": 980.0, "end": 987.0, "text": " So you can basically see a stream as the change log of a continuously changing table.", "tokens": [407, 291, 393, 1936, 536, 257, 4309, 382, 264, 1319, 3565, 295, 257, 15684, 4473, 3199, 13], "temperature": 0.0, "avg_logprob": -0.11310823543651684, "compression_ratio": 1.6961325966850829, "no_speech_prob": 0.0002302684442838654}, {"id": 197, "seek": 97000, "start": 987.0, "end": 991.0, "text": " So it is possible, and I will also show an example shortly,", "tokens": [407, 309, 307, 1944, 11, 293, 286, 486, 611, 855, 364, 1365, 13392, 11], "temperature": 0.0, "avg_logprob": -0.11310823543651684, "compression_ratio": 1.6961325966850829, "no_speech_prob": 0.0002302684442838654}, {"id": 198, "seek": 97000, "start": 991.0, "end": 996.0, "text": " that you can convert from a table into a stream and from a stream into a table.", "tokens": [300, 291, 393, 7620, 490, 257, 3199, 666, 257, 4309, 293, 490, 257, 4309, 666, 257, 3199, 13], "temperature": 0.0, "avg_logprob": -0.11310823543651684, "compression_ratio": 1.6961325966850829, "no_speech_prob": 0.0002302684442838654}, {"id": 199, "seek": 99600, "start": 996.0, "end": 1000.0, "text": " You can do a back and forth mapping after this possible.", "tokens": [509, 393, 360, 257, 646, 293, 5220, 18350, 934, 341, 1944, 13], "temperature": 0.0, "avg_logprob": -0.18941787437156396, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00030392437474802136}, {"id": 200, "seek": 99600, "start": 1000.0, "end": 1004.0, "text": " Usually, as a user, you don't see that.", "tokens": [11419, 11, 382, 257, 4195, 11, 291, 500, 380, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.18941787437156396, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00030392437474802136}, {"id": 201, "seek": 99600, "start": 1004.0, "end": 1009.0, "text": " Under the hood, the runtime, all the sources, all the things, all the operators,", "tokens": [6974, 264, 13376, 11, 264, 34474, 11, 439, 264, 7139, 11, 439, 264, 721, 11, 439, 264, 19077, 11], "temperature": 0.0, "avg_logprob": -0.18941787437156396, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00030392437474802136}, {"id": 202, "seek": 99600, "start": 1009.0, "end": 1012.0, "text": " they work with change logs under the hood.", "tokens": [436, 589, 365, 1319, 20820, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.18941787437156396, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00030392437474802136}, {"id": 203, "seek": 99600, "start": 1012.0, "end": 1017.0, "text": " So in Flink, we have four different kinds of change tags for each record.", "tokens": [407, 294, 3235, 475, 11, 321, 362, 1451, 819, 3685, 295, 1319, 18632, 337, 1184, 2136, 13], "temperature": 0.0, "avg_logprob": -0.18941787437156396, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.00030392437474802136}, {"id": 204, "seek": 101700, "start": 1017.0, "end": 1027.0, "text": " So we have insertions, insertions are also the default input and output for bounded hash queries.", "tokens": [407, 321, 362, 8969, 626, 11, 8969, 626, 366, 611, 264, 7576, 4846, 293, 5598, 337, 37498, 22019, 24109, 13], "temperature": 0.0, "avg_logprob": -0.291756180097472, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0009324142592959106}, {"id": 205, "seek": 101700, "start": 1027.0, "end": 1033.0, "text": " And then we have update four, which basically removes a previously inputted result.", "tokens": [400, 550, 321, 362, 5623, 1451, 11, 597, 1936, 30445, 257, 8046, 4846, 14727, 1874, 13], "temperature": 0.0, "avg_logprob": -0.291756180097472, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0009324142592959106}, {"id": 206, "seek": 101700, "start": 1033.0, "end": 1039.0, "text": " Then we have update after to update something.", "tokens": [1396, 321, 362, 5623, 934, 281, 5623, 746, 13], "temperature": 0.0, "avg_logprob": -0.291756180097472, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0009324142592959106}, {"id": 207, "seek": 103900, "start": 1039.0, "end": 1048.0, "text": " And then we have to feed one of the last results.", "tokens": [400, 550, 321, 362, 281, 3154, 472, 295, 264, 1036, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2154533565044403, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0003337172674946487}, {"id": 208, "seek": 103900, "start": 1048.0, "end": 1055.0, "text": " When we see only insert only in a log, then we call this an only or insert only log.", "tokens": [1133, 321, 536, 787, 8969, 787, 294, 257, 3565, 11, 550, 321, 818, 341, 364, 787, 420, 8969, 787, 3565, 13], "temperature": 0.0, "avg_logprob": -0.2154533565044403, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0003337172674946487}, {"id": 209, "seek": 103900, "start": 1055.0, "end": 1060.0, "text": " If it contains some kind of division or update before,", "tokens": [759, 309, 8306, 512, 733, 295, 10044, 420, 5623, 949, 11], "temperature": 0.0, "avg_logprob": -0.2154533565044403, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0003337172674946487}, {"id": 210, "seek": 103900, "start": 1060.0, "end": 1064.0, "text": " we call this updating table or an updating stream.", "tokens": [321, 818, 341, 25113, 3199, 420, 364, 25113, 4309, 13], "temperature": 0.0, "avg_logprob": -0.2154533565044403, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0003337172674946487}, {"id": 211, "seek": 106400, "start": 1064.0, "end": 1072.0, "text": " And if it never contains an update before, but only update afters,", "tokens": [400, 498, 309, 1128, 8306, 364, 5623, 949, 11, 457, 787, 5623, 934, 82, 11], "temperature": 0.0, "avg_logprob": -0.25928613509254894, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.00015791745681781322}, {"id": 212, "seek": 106400, "start": 1072.0, "end": 1079.0, "text": " then there's a primary key involved, and then we call this absurdity.", "tokens": [550, 456, 311, 257, 6194, 2141, 3288, 11, 293, 550, 321, 818, 341, 19774, 507, 13], "temperature": 0.0, "avg_logprob": -0.25928613509254894, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.00015791745681781322}, {"id": 213, "seek": 106400, "start": 1079.0, "end": 1082.0, "text": " So let me make a quick example.", "tokens": [407, 718, 385, 652, 257, 1702, 1365, 13], "temperature": 0.0, "avg_logprob": -0.25928613509254894, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.00015791745681781322}, {"id": 214, "seek": 106400, "start": 1082.0, "end": 1086.0, "text": " So again, we have on the left side those actions on the right side with value,", "tokens": [407, 797, 11, 321, 362, 322, 264, 1411, 1252, 729, 5909, 322, 264, 558, 1252, 365, 2158, 11], "temperature": 0.0, "avg_logprob": -0.25928613509254894, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.00015791745681781322}, {"id": 215, "seek": 106400, "start": 1086.0, "end": 1091.0, "text": " and in the middle we have summing and sumproving by name of different sections.", "tokens": [293, 294, 264, 2808, 321, 362, 2408, 2810, 293, 2408, 4318, 798, 538, 1315, 295, 819, 10863, 13], "temperature": 0.0, "avg_logprob": -0.25928613509254894, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.00015791745681781322}, {"id": 216, "seek": 109100, "start": 1091.0, "end": 1099.0, "text": " So what happens now is, like in the logical table, there is a new record coming in called Alice.", "tokens": [407, 437, 2314, 586, 307, 11, 411, 294, 264, 14978, 3199, 11, 456, 307, 257, 777, 2136, 1348, 294, 1219, 16004, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 217, "seek": 109100, "start": 1099.0, "end": 1103.0, "text": " This is how it will be represented in the change tag under the hood.", "tokens": [639, 307, 577, 309, 486, 312, 10379, 294, 264, 1319, 6162, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 218, "seek": 109100, "start": 1103.0, "end": 1106.0, "text": " And then this is what comes out.", "tokens": [400, 550, 341, 307, 437, 1487, 484, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 219, "seek": 109100, "start": 1106.0, "end": 1108.0, "text": " So we are summing here.", "tokens": [407, 321, 366, 2408, 2810, 510, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 220, "seek": 109100, "start": 1108.0, "end": 1113.0, "text": " So 256 is the first result that we are already looking at.", "tokens": [407, 38882, 307, 264, 700, 1874, 300, 321, 366, 1217, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 221, "seek": 109100, "start": 1113.0, "end": 1115.0, "text": " So now the next variable comes in.", "tokens": [407, 586, 264, 958, 7006, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.25412272310805045, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008380115032196045}, {"id": 222, "seek": 111500, "start": 1115.0, "end": 1121.0, "text": " Again, it will be added also to the last table.", "tokens": [3764, 11, 309, 486, 312, 3869, 611, 281, 264, 1036, 3199, 13], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 223, "seek": 111500, "start": 1121.0, "end": 1123.0, "text": " But now it comes in.", "tokens": [583, 586, 309, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 224, "seek": 111500, "start": 1123.0, "end": 1126.0, "text": " There's another Alice, and we want to move like this.", "tokens": [821, 311, 1071, 16004, 11, 293, 321, 528, 281, 1286, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 225, "seek": 111500, "start": 1126.0, "end": 1134.0, "text": " So that means the sum is not updated or we need to update the sum to the newest number.", "tokens": [407, 300, 1355, 264, 2408, 307, 406, 10588, 420, 321, 643, 281, 5623, 264, 2408, 281, 264, 17569, 1230, 13], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 226, "seek": 111500, "start": 1134.0, "end": 1139.0, "text": " That means, first, we have to remove the old record.", "tokens": [663, 1355, 11, 700, 11, 321, 362, 281, 4159, 264, 1331, 2136, 13], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 227, "seek": 111500, "start": 1139.0, "end": 1143.0, "text": " And if we want to materialize our change log into a table,", "tokens": [400, 498, 321, 528, 281, 2527, 1125, 527, 1319, 3565, 666, 257, 3199, 11], "temperature": 0.0, "avg_logprob": -0.23829856126204782, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.00037331823841668665}, {"id": 228, "seek": 114300, "start": 1143.0, "end": 1146.0, "text": " so we also have to remove the row in the table.", "tokens": [370, 321, 611, 362, 281, 4159, 264, 5386, 294, 264, 3199, 13], "temperature": 0.0, "avg_logprob": -0.20066733299931394, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.0002646991633810103}, {"id": 229, "seek": 114300, "start": 1146.0, "end": 1153.0, "text": " And then we can finally add the updated row in the table.", "tokens": [400, 550, 321, 393, 2721, 909, 264, 10588, 5386, 294, 264, 3199, 13], "temperature": 0.0, "avg_logprob": -0.20066733299931394, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.0002646991633810103}, {"id": 230, "seek": 114300, "start": 1153.0, "end": 1155.0, "text": " And this is what the change log looks like.", "tokens": [400, 341, 307, 437, 264, 1319, 3565, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.20066733299931394, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.0002646991633810103}, {"id": 231, "seek": 114300, "start": 1155.0, "end": 1161.0, "text": " And if you would apply this change log to a SQL or to some key values there,", "tokens": [400, 498, 291, 576, 3079, 341, 1319, 3565, 281, 257, 19200, 420, 281, 512, 2141, 4190, 456, 11], "temperature": 0.0, "avg_logprob": -0.20066733299931394, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.0002646991633810103}, {"id": 232, "seek": 114300, "start": 1161.0, "end": 1169.0, "text": " or like the search or so, then the result would be there.", "tokens": [420, 411, 264, 3164, 420, 370, 11, 550, 264, 1874, 576, 312, 456, 13], "temperature": 0.0, "avg_logprob": -0.20066733299931394, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.0002646991633810103}, {"id": 233, "seek": 116900, "start": 1169.0, "end": 1176.0, "text": " And if we would define a primary key on the sync table,", "tokens": [400, 498, 321, 576, 6964, 257, 6194, 2141, 322, 264, 20271, 3199, 11], "temperature": 0.0, "avg_logprob": -0.19561237607683454, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.00036754432949237525}, {"id": 234, "seek": 116900, "start": 1176.0, "end": 1179.0, "text": " actually we don't need this update before,", "tokens": [767, 321, 500, 380, 643, 341, 5623, 949, 11], "temperature": 0.0, "avg_logprob": -0.19561237607683454, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.00036754432949237525}, {"id": 235, "seek": 116900, "start": 1179.0, "end": 1182.0, "text": " because then it would be now searching operation.", "tokens": [570, 550, 309, 576, 312, 586, 10808, 6916, 13], "temperature": 0.0, "avg_logprob": -0.19561237607683454, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.00036754432949237525}, {"id": 236, "seek": 116900, "start": 1182.0, "end": 1187.0, "text": " And yeah, we can basically save 50% of traffic", "tokens": [400, 1338, 11, 321, 393, 1936, 3155, 2625, 4, 295, 6419], "temperature": 0.0, "avg_logprob": -0.19561237607683454, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.00036754432949237525}, {"id": 237, "seek": 116900, "start": 1187.0, "end": 1195.0, "text": " if we do not want to support that with the rows in the sync table.", "tokens": [498, 321, 360, 406, 528, 281, 1406, 300, 365, 264, 13241, 294, 264, 20271, 3199, 13], "temperature": 0.0, "avg_logprob": -0.19561237607683454, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.00036754432949237525}, {"id": 238, "seek": 119500, "start": 1195.0, "end": 1201.0, "text": " So I already mentioned that each sync and each source,", "tokens": [407, 286, 1217, 2835, 300, 1184, 20271, 293, 1184, 4009, 11], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 239, "seek": 119500, "start": 1201.0, "end": 1205.0, "text": " that they declare a change log model which changes they and they can't consume.", "tokens": [300, 436, 19710, 257, 1319, 3565, 2316, 597, 2962, 436, 293, 436, 393, 380, 14732, 13], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 240, "seek": 119500, "start": 1205.0, "end": 1210.0, "text": " And yeah, I give like a quick example of various connectors.", "tokens": [400, 1338, 11, 286, 976, 411, 257, 1702, 1365, 295, 3683, 31865, 13], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 241, "seek": 119500, "start": 1210.0, "end": 1213.0, "text": " So when we, for example, read from a file system,", "tokens": [407, 562, 321, 11, 337, 1365, 11, 1401, 490, 257, 3991, 1185, 11], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 242, "seek": 119500, "start": 1213.0, "end": 1215.0, "text": " this is usually a scan operation.", "tokens": [341, 307, 2673, 257, 11049, 6916, 13], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 243, "seek": 119500, "start": 1215.0, "end": 1220.0, "text": " So it is very common that when you read from a file that this is just insert only.", "tokens": [407, 309, 307, 588, 2689, 300, 562, 291, 1401, 490, 257, 3991, 300, 341, 307, 445, 8969, 787, 13], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 244, "seek": 119500, "start": 1220.0, "end": 1223.0, "text": " There are no updates coming through the file system.", "tokens": [821, 366, 572, 9205, 1348, 807, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.19952445257277715, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00017366543761454523}, {"id": 245, "seek": 122300, "start": 1223.0, "end": 1227.0, "text": " Sometimes they do, but in the general case,", "tokens": [4803, 436, 360, 11, 457, 294, 264, 2674, 1389, 11], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 246, "seek": 122300, "start": 1227.0, "end": 1231.0, "text": " you just scan through the Kafka file for example.", "tokens": [291, 445, 11049, 807, 264, 47064, 3991, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 247, "seek": 122300, "start": 1231.0, "end": 1234.0, "text": " Okay, so Kafka, in the early days,", "tokens": [1033, 11, 370, 47064, 11, 294, 264, 2440, 1708, 11], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 248, "seek": 122300, "start": 1234.0, "end": 1237.0, "text": " Kafka was actually just as a log for every N record", "tokens": [47064, 390, 767, 445, 382, 257, 3565, 337, 633, 426, 2136], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 249, "seek": 122300, "start": 1237.0, "end": 1242.0, "text": " that came in through Kafka was also considered like an insert only record.", "tokens": [300, 1361, 294, 807, 47064, 390, 611, 4888, 411, 364, 8969, 787, 2136, 13], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 250, "seek": 122300, "start": 1242.0, "end": 1247.0, "text": " Then later, Kafka also added some absurd functionality.", "tokens": [1396, 1780, 11, 47064, 611, 3869, 512, 19774, 14980, 13], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 251, "seek": 122300, "start": 1247.0, "end": 1250.0, "text": " So we also have a connector called Kafka Absurd for that.", "tokens": [407, 321, 611, 362, 257, 19127, 1219, 47064, 5813, 10752, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.17915080456023522, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022668542806059122}, {"id": 252, "seek": 125000, "start": 1250.0, "end": 1256.0, "text": " That means when a value in Kafka is null, it means addition.", "tokens": [663, 1355, 562, 257, 2158, 294, 47064, 307, 18184, 11, 309, 1355, 4500, 13], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 253, "seek": 125000, "start": 1256.0, "end": 1260.0, "text": " So the Kafka Absurd connector, for example,", "tokens": [407, 264, 47064, 5813, 10752, 19127, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 254, "seek": 125000, "start": 1260.0, "end": 1267.0, "text": " would produce insertions and divisions.", "tokens": [576, 5258, 8969, 626, 293, 24328, 13], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 255, "seek": 125000, "start": 1267.0, "end": 1269.0, "text": " If you define the JVC connector,", "tokens": [759, 291, 6964, 264, 508, 53, 34, 19127, 11], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 256, "seek": 125000, "start": 1269.0, "end": 1273.0, "text": " JVC also doesn't have this concept of updates.", "tokens": [508, 53, 34, 611, 1177, 380, 362, 341, 3410, 295, 9205, 13], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 257, "seek": 125000, "start": 1273.0, "end": 1276.0, "text": " So in the same case, we would scan the entire table", "tokens": [407, 294, 264, 912, 1389, 11, 321, 576, 11049, 264, 2302, 3199], "temperature": 0.0, "avg_logprob": -0.16307856486393854, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0001606570731382817}, {"id": 258, "seek": 127600, "start": 1276.0, "end": 1280.0, "text": " and just scan to only produce insertions.", "tokens": [293, 445, 11049, 281, 787, 5258, 8969, 626, 13], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 259, "seek": 127600, "start": 1280.0, "end": 1283.0, "text": " So we have all the insertions for JVC.", "tokens": [407, 321, 362, 439, 264, 8969, 626, 337, 508, 53, 34, 13], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 260, "seek": 127600, "start": 1283.0, "end": 1286.0, "text": " But that comes like the most complex case.", "tokens": [583, 300, 1487, 411, 264, 881, 3997, 1389, 13], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 261, "seek": 127600, "start": 1286.0, "end": 1289.0, "text": " What happens if you use, for example, an easy one?", "tokens": [708, 2314, 498, 291, 764, 11, 337, 1365, 11, 364, 1858, 472, 30], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 262, "seek": 127600, "start": 1289.0, "end": 1295.0, "text": " You connect it to the database to consume the change of this particular from the database.", "tokens": [509, 1745, 309, 281, 264, 8149, 281, 14732, 264, 1319, 295, 341, 1729, 490, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 263, "seek": 127600, "start": 1295.0, "end": 1299.0, "text": " You put this into Kafka and then you consume from Kafka.", "tokens": [509, 829, 341, 666, 47064, 293, 550, 291, 14732, 490, 47064, 13], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 264, "seek": 127600, "start": 1299.0, "end": 1303.0, "text": " In this case, for example, this could, for example,", "tokens": [682, 341, 1389, 11, 337, 1365, 11, 341, 727, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.23414217127431738, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0002057685487670824}, {"id": 265, "seek": 130300, "start": 1303.0, "end": 1309.0, "text": " all kinds of changes that can then be evaluated by the end.", "tokens": [439, 3685, 295, 2962, 300, 393, 550, 312, 25509, 538, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 266, "seek": 130300, "start": 1309.0, "end": 1314.0, "text": " The optimizer basically tracks the changes through the entire topology", "tokens": [440, 5028, 6545, 1936, 10218, 264, 2962, 807, 264, 2302, 1192, 1793], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 267, "seek": 130300, "start": 1314.0, "end": 1318.0, "text": " and the sync prepares what it can digest.", "tokens": [293, 264, 20271, 39418, 437, 309, 393, 13884, 13], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 268, "seek": 130300, "start": 1318.0, "end": 1321.0, "text": " The optimizer could react sometimes with an error message,", "tokens": [440, 5028, 6545, 727, 4515, 2171, 365, 364, 6713, 3636, 11], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 269, "seek": 130300, "start": 1321.0, "end": 1325.0, "text": " but sometimes there's more to it.", "tokens": [457, 2171, 456, 311, 544, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 270, "seek": 130300, "start": 1325.0, "end": 1330.0, "text": " So let's quickly also talk about these two different modes.", "tokens": [407, 718, 311, 2661, 611, 751, 466, 613, 732, 819, 14068, 13], "temperature": 0.0, "avg_logprob": -0.22175001796287827, "compression_ratio": 1.625, "no_speech_prob": 0.00017089047469198704}, {"id": 271, "seek": 133000, "start": 1330.0, "end": 1333.0, "text": " So I already said that sometimes you can do upserts", "tokens": [407, 286, 1217, 848, 300, 2171, 291, 393, 360, 15497, 911, 82], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 272, "seek": 133000, "start": 1333.0, "end": 1337.0, "text": " where there's no update before.", "tokens": [689, 456, 311, 572, 5623, 949, 13], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 273, "seek": 133000, "start": 1337.0, "end": 1341.0, "text": " And sometimes you need all four kind of changes.", "tokens": [400, 2171, 291, 643, 439, 1451, 733, 295, 2962, 13], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 274, "seek": 133000, "start": 1341.0, "end": 1344.0, "text": " And this is called like retract versus absurd.", "tokens": [400, 341, 307, 1219, 411, 41107, 5717, 19774, 13], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 275, "seek": 133000, "start": 1344.0, "end": 1350.0, "text": " So retract has this nice property that there is no primary key required.", "tokens": [407, 41107, 575, 341, 1481, 4707, 300, 456, 307, 572, 6194, 2141, 4739, 13], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 276, "seek": 133000, "start": 1350.0, "end": 1355.0, "text": " This works for almost all external systems, which is great.", "tokens": [639, 1985, 337, 1920, 439, 8320, 3652, 11, 597, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.2040543067149627, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00031882396433502436}, {"id": 277, "seek": 135500, "start": 1355.0, "end": 1360.0, "text": " You can also support up with the pros, which you cannot support in the upsert.", "tokens": [509, 393, 611, 1406, 493, 365, 264, 6267, 11, 597, 291, 2644, 1406, 294, 264, 15497, 911, 13], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 278, "seek": 135500, "start": 1360.0, "end": 1363.0, "text": " The table is called an absurd table.", "tokens": [440, 3199, 307, 1219, 364, 19774, 3199, 13], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 279, "seek": 135500, "start": 1363.0, "end": 1366.0, "text": " And interestingly, also retracts,", "tokens": [400, 25873, 11, 611, 41107, 82, 11], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 280, "seek": 135500, "start": 1366.0, "end": 1372.0, "text": " so like this retracting of the previous admitted record", "tokens": [370, 411, 341, 41107, 278, 295, 264, 3894, 14920, 2136], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 281, "seek": 135500, "start": 1372.0, "end": 1375.0, "text": " is actually often required in distributed systems.", "tokens": [307, 767, 2049, 4739, 294, 12631, 3652, 13], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 282, "seek": 135500, "start": 1375.0, "end": 1380.0, "text": " And I also have a little example on the right side, but I will show shortly.", "tokens": [400, 286, 611, 362, 257, 707, 1365, 322, 264, 558, 1252, 11, 457, 286, 486, 855, 13392, 13], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 283, "seek": 135500, "start": 1380.0, "end": 1384.0, "text": " So let me definitely explain this first.", "tokens": [407, 718, 385, 2138, 2903, 341, 700, 13], "temperature": 0.0, "avg_logprob": -0.26628449142620125, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.0010239246767014265}, {"id": 284, "seek": 138400, "start": 1384.0, "end": 1386.0, "text": " So this is a count of a count.", "tokens": [407, 341, 307, 257, 1207, 295, 257, 1207, 13], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 285, "seek": 138400, "start": 1386.0, "end": 1389.0, "text": " So we are creating an histogram.", "tokens": [407, 321, 366, 4084, 364, 49816, 13], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 286, "seek": 138400, "start": 1389.0, "end": 1392.0, "text": " The lexical variable itself is not so important.", "tokens": [440, 476, 87, 804, 7006, 2564, 307, 406, 370, 1021, 13], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 287, "seek": 138400, "start": 1392.0, "end": 1393.0, "text": " What is important?", "tokens": [708, 307, 1021, 30], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 288, "seek": 138400, "start": 1393.0, "end": 1399.0, "text": " What is actually flowing in the cluster through the operators?", "tokens": [708, 307, 767, 13974, 294, 264, 13630, 807, 264, 19077, 30], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 289, "seek": 138400, "start": 1399.0, "end": 1402.0, "text": " So whatever record comes in,", "tokens": [407, 2035, 2136, 1487, 294, 11], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 290, "seek": 138400, "start": 1402.0, "end": 1407.0, "text": " the first operator will identify this.", "tokens": [264, 700, 12973, 486, 5876, 341, 13], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 291, "seek": 138400, "start": 1407.0, "end": 1410.0, "text": " Okay, this is the first time, so the count is one.", "tokens": [1033, 11, 341, 307, 264, 700, 565, 11, 370, 264, 1207, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.2192840137700925, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.0005244749481789768}, {"id": 292, "seek": 141000, "start": 1410.0, "end": 1416.0, "text": " And then since we want to do the count of a count, the next operator", "tokens": [400, 550, 1670, 321, 528, 281, 360, 264, 1207, 295, 257, 1207, 11, 264, 958, 12973], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 293, "seek": 141000, "start": 1416.0, "end": 1420.0, "text": " will also count this as a one, and it will keep some state.", "tokens": [486, 611, 1207, 341, 382, 257, 472, 11, 293, 309, 486, 1066, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 294, "seek": 141000, "start": 1420.0, "end": 1427.0, "text": " How many records have I seen for this particular count?", "tokens": [1012, 867, 7724, 362, 286, 1612, 337, 341, 1729, 1207, 30], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 295, "seek": 141000, "start": 1427.0, "end": 1431.0, "text": " So now comes the second record in.", "tokens": [407, 586, 1487, 264, 1150, 2136, 294, 13], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 296, "seek": 141000, "start": 1431.0, "end": 1433.0, "text": " And we have to after the count.", "tokens": [400, 321, 362, 281, 934, 264, 1207, 13], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 297, "seek": 141000, "start": 1433.0, "end": 1436.0, "text": " So now the count is two, but one anymore.", "tokens": [407, 586, 264, 1207, 307, 732, 11, 457, 472, 3602, 13], "temperature": 0.0, "avg_logprob": -0.20478967873446913, "compression_ratio": 1.60989010989011, "no_speech_prob": 0.00047120676026679575}, {"id": 298, "seek": 143600, "start": 1436.0, "end": 1440.0, "text": " And interestingly, if we do a hash partition for some collectors,", "tokens": [400, 25873, 11, 498, 321, 360, 257, 22019, 24808, 337, 512, 35384, 11], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 299, "seek": 143600, "start": 1440.0, "end": 1445.0, "text": " it might be that the count ends up at a completely different operator.", "tokens": [309, 1062, 312, 300, 264, 1207, 5314, 493, 412, 257, 2584, 819, 12973, 13], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 300, "seek": 143600, "start": 1445.0, "end": 1448.0, "text": " But what happens with the old count?", "tokens": [583, 437, 2314, 365, 264, 1331, 1207, 30], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 301, "seek": 143600, "start": 1448.0, "end": 1452.0, "text": " So now you have two threads or two operators,", "tokens": [407, 586, 291, 362, 732, 19314, 420, 732, 19077, 11], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 302, "seek": 143600, "start": 1452.0, "end": 1455.0, "text": " parallel instances of the operator that have a count", "tokens": [8952, 14519, 295, 264, 12973, 300, 362, 257, 1207], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 303, "seek": 143600, "start": 1455.0, "end": 1461.0, "text": " and that they need to remove the count in the other operator.", "tokens": [293, 300, 436, 643, 281, 4159, 264, 1207, 294, 264, 661, 12973, 13], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 304, "seek": 143600, "start": 1461.0, "end": 1464.0, "text": " This is why this case rejection is required,", "tokens": [639, 307, 983, 341, 1389, 26044, 307, 4739, 11], "temperature": 0.0, "avg_logprob": -0.17542343554289444, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.000329630944179371}, {"id": 305, "seek": 146400, "start": 1464.0, "end": 1467.0, "text": " because the update before needs to go to the subclass one", "tokens": [570, 264, 5623, 949, 2203, 281, 352, 281, 264, 1422, 11665, 472], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 306, "seek": 146400, "start": 1467.0, "end": 1471.0, "text": " and remove the man outdated record.", "tokens": [293, 4159, 264, 587, 36313, 2136, 13], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 307, "seek": 146400, "start": 1471.0, "end": 1475.0, "text": " But in general, absurd is an optimization.", "tokens": [583, 294, 2674, 11, 19774, 307, 364, 19618, 13], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 308, "seek": 146400, "start": 1475.0, "end": 1477.0, "text": " It reduces traffic, reduces computation.", "tokens": [467, 18081, 6419, 11, 18081, 24903, 13], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 309, "seek": 146400, "start": 1477.0, "end": 1480.0, "text": " And if it's possible, it's great,", "tokens": [400, 498, 309, 311, 1944, 11, 309, 311, 869, 11], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 310, "seek": 146400, "start": 1480.0, "end": 1486.0, "text": " but usually there is a lot of reflections flowing on in the...", "tokens": [457, 2673, 456, 307, 257, 688, 295, 30679, 13974, 322, 294, 264, 485], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 311, "seek": 146400, "start": 1486.0, "end": 1490.0, "text": " And also have some examples here.", "tokens": [400, 611, 362, 512, 5110, 510, 13], "temperature": 0.0, "avg_logprob": -0.28641707808883105, "compression_ratio": 1.5323383084577114, "no_speech_prob": 0.0005954675143584609}, {"id": 312, "seek": 149000, "start": 1490.0, "end": 1497.0, "text": " Like if you would do an explain on some SQL query in the SQL,", "tokens": [1743, 498, 291, 576, 360, 364, 2903, 322, 512, 19200, 14581, 294, 264, 19200, 11], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 313, "seek": 149000, "start": 1497.0, "end": 1501.0, "text": " the bottom part is what you would see.", "tokens": [264, 2767, 644, 307, 437, 291, 576, 536, 13], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 314, "seek": 149000, "start": 1501.0, "end": 1505.0, "text": " So let's assume we have a table of transactions and a table of payment", "tokens": [407, 718, 311, 6552, 321, 362, 257, 3199, 295, 16856, 293, 257, 3199, 295, 10224], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 315, "seek": 149000, "start": 1505.0, "end": 1507.0, "text": " and a table of result.", "tokens": [293, 257, 3199, 295, 1874, 13], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 316, "seek": 149000, "start": 1507.0, "end": 1509.0, "text": " The table of result can consume all kinds of changes.", "tokens": [440, 3199, 295, 1874, 393, 14732, 439, 3685, 295, 2962, 13], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 317, "seek": 149000, "start": 1509.0, "end": 1511.0, "text": " I just took my table also.", "tokens": [286, 445, 1890, 452, 3199, 611, 13], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 318, "seek": 149000, "start": 1511.0, "end": 1516.0, "text": " And you join transactions and payments.", "tokens": [400, 291, 3917, 16856, 293, 14348, 13], "temperature": 0.0, "avg_logprob": -0.27844819357228834, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.0011453261831775308}, {"id": 319, "seek": 151600, "start": 1516.0, "end": 1520.0, "text": " And in the explain, you also see that there is...", "tokens": [400, 294, 264, 2903, 11, 291, 611, 536, 300, 456, 307, 485], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 320, "seek": 151600, "start": 1520.0, "end": 1524.0, "text": " You can get information in the explain about the change of mode.", "tokens": [509, 393, 483, 1589, 294, 264, 2903, 466, 264, 1319, 295, 4391, 13], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 321, "seek": 151600, "start": 1524.0, "end": 1529.0, "text": " For example, if the input here is insert only,", "tokens": [1171, 1365, 11, 498, 264, 4846, 510, 307, 8969, 787, 11], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 322, "seek": 151600, "start": 1529.0, "end": 1536.0, "text": " insert only, then also the join will produce insert only result.", "tokens": [8969, 787, 11, 550, 611, 264, 3917, 486, 5258, 8969, 787, 1874, 13], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 323, "seek": 151600, "start": 1536.0, "end": 1540.0, "text": " And for example, if we do an outer join,", "tokens": [400, 337, 1365, 11, 498, 321, 360, 364, 10847, 3917, 11], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 324, "seek": 151600, "start": 1540.0, "end": 1542.0, "text": " in this case the left outer join,", "tokens": [294, 341, 1389, 264, 1411, 10847, 3917, 11], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 325, "seek": 151600, "start": 1542.0, "end": 1544.0, "text": " then things become a bit more complex.", "tokens": [550, 721, 1813, 257, 857, 544, 3997, 13], "temperature": 0.0, "avg_logprob": -0.17314355269722317, "compression_ratio": 1.761658031088083, "no_speech_prob": 0.0005507591413334012}, {"id": 326, "seek": 154400, "start": 1544.0, "end": 1547.0, "text": " Here you have insert only, insert only,", "tokens": [1692, 291, 362, 8969, 787, 11, 8969, 787, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 327, "seek": 154400, "start": 1547.0, "end": 1550.0, "text": " but since that outer join will emit another first,", "tokens": [457, 1670, 300, 10847, 3917, 486, 32084, 1071, 700, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 328, "seek": 154400, "start": 1550.0, "end": 1553.0, "text": " like if there's one thing, like one record comes in,", "tokens": [411, 498, 456, 311, 472, 551, 11, 411, 472, 2136, 1487, 294, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 329, "seek": 154400, "start": 1553.0, "end": 1555.0, "text": " there's no matching record so far,", "tokens": [456, 311, 572, 14324, 2136, 370, 1400, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 330, "seek": 154400, "start": 1555.0, "end": 1558.0, "text": " and you have to emit another first for the other side,", "tokens": [293, 291, 362, 281, 32084, 1071, 700, 337, 264, 661, 1252, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 331, "seek": 154400, "start": 1558.0, "end": 1560.0, "text": " and then when the other side is coming in,", "tokens": [293, 550, 562, 264, 661, 1252, 307, 1348, 294, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 332, "seek": 154400, "start": 1560.0, "end": 1562.0, "text": " then you have to remove another again", "tokens": [550, 291, 362, 281, 4159, 1071, 797], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 333, "seek": 154400, "start": 1562.0, "end": 1565.0, "text": " and emit the final result.", "tokens": [293, 32084, 264, 2572, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 334, "seek": 154400, "start": 1565.0, "end": 1567.0, "text": " And that's why, for example, here,", "tokens": [400, 300, 311, 983, 11, 337, 1365, 11, 510, 11], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 335, "seek": 154400, "start": 1567.0, "end": 1573.0, "text": " you have all kinds of changes coming out of the join.", "tokens": [291, 362, 439, 3685, 295, 2962, 1348, 484, 295, 264, 3917, 13], "temperature": 0.0, "avg_logprob": -0.1795930782286059, "compression_ratio": 1.9196428571428572, "no_speech_prob": 0.0012178310425952077}, {"id": 336, "seek": 157300, "start": 1573.0, "end": 1575.0, "text": " And then we can even make it more complicated.", "tokens": [400, 550, 321, 393, 754, 652, 309, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 337, "seek": 157300, "start": 1575.0, "end": 1578.0, "text": " What happens if we define a primary key", "tokens": [708, 2314, 498, 321, 6964, 257, 6194, 2141], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 338, "seek": 157300, "start": 1578.0, "end": 1582.0, "text": " on transactions and payments?", "tokens": [322, 16856, 293, 14348, 30], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 339, "seek": 157300, "start": 1582.0, "end": 1585.0, "text": " Then the optimizer will recognize,", "tokens": [1396, 264, 5028, 6545, 486, 5521, 11], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 340, "seek": 157300, "start": 1585.0, "end": 1588.0, "text": " okay, that input spec and the right input spec", "tokens": [1392, 11, 300, 4846, 1608, 293, 264, 558, 4846, 1608], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 341, "seek": 157300, "start": 1588.0, "end": 1590.0, "text": " will contain now a key key.", "tokens": [486, 5304, 586, 257, 2141, 2141, 13], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 342, "seek": 157300, "start": 1590.0, "end": 1591.0, "text": " That is great.", "tokens": [663, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 343, "seek": 157300, "start": 1591.0, "end": 1593.0, "text": " So I can remove the update before,", "tokens": [407, 286, 393, 4159, 264, 5623, 949, 11], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 344, "seek": 157300, "start": 1593.0, "end": 1595.0, "text": " so you can see that there is one,", "tokens": [370, 291, 393, 536, 300, 456, 307, 472, 11], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 345, "seek": 157300, "start": 1595.0, "end": 1597.0, "text": " that particular is not necessary anymore,", "tokens": [300, 1729, 307, 406, 4818, 3602, 11], "temperature": 0.0, "avg_logprob": -0.27686862410785995, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.0017021193634718657}, {"id": 346, "seek": 159700, "start": 1597.0, "end": 1604.0, "text": " because we can do upserts on the results.", "tokens": [570, 321, 393, 360, 15497, 911, 82, 322, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 347, "seek": 159700, "start": 1604.0, "end": 1610.0, "text": " So this query is obviously more efficient than the other one.", "tokens": [407, 341, 14581, 307, 2745, 544, 7148, 813, 264, 661, 472, 13], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 348, "seek": 159700, "start": 1610.0, "end": 1612.0, "text": " And the other good optimizer can also range", "tokens": [400, 264, 661, 665, 5028, 6545, 393, 611, 3613], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 349, "seek": 159700, "start": 1612.0, "end": 1615.0, "text": " between those different modes.", "tokens": [1296, 729, 819, 14068, 13], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 350, "seek": 159700, "start": 1615.0, "end": 1617.0, "text": " I don't want to get details here,", "tokens": [286, 500, 380, 528, 281, 483, 4365, 510, 11], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 351, "seek": 159700, "start": 1617.0, "end": 1619.0, "text": " but if it's possible, like it's necessary,", "tokens": [457, 498, 309, 311, 1944, 11, 411, 309, 311, 4818, 11], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 352, "seek": 159700, "start": 1619.0, "end": 1624.0, "text": " you can go from updating to the collection.", "tokens": [291, 393, 352, 490, 25113, 281, 264, 5765, 13], "temperature": 0.0, "avg_logprob": -0.3206059293049138, "compression_ratio": 1.495, "no_speech_prob": 0.000838855339679867}, {"id": 353, "seek": 162400, "start": 1624.0, "end": 1628.0, "text": " But that's not also under the course of the", "tokens": [583, 300, 311, 406, 611, 833, 264, 1164, 295, 264], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 354, "seek": 162400, "start": 1628.0, "end": 1630.0, "text": " information.", "tokens": [1589, 13], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 355, "seek": 162400, "start": 1630.0, "end": 1633.0, "text": " And depending on the operators,", "tokens": [400, 5413, 322, 264, 19077, 11], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 356, "seek": 162400, "start": 1633.0, "end": 1638.0, "text": " you also can switch between these modes.", "tokens": [291, 611, 393, 3679, 1296, 613, 14068, 13], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 357, "seek": 162400, "start": 1638.0, "end": 1641.0, "text": " So for example, if you have a regular join,", "tokens": [407, 337, 1365, 11, 498, 291, 362, 257, 3890, 3917, 11], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 358, "seek": 162400, "start": 1641.0, "end": 1643.0, "text": " to append only tables,", "tokens": [281, 34116, 787, 8020, 11], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 359, "seek": 162400, "start": 1643.0, "end": 1646.0, "text": " then also the resulting table will be append only.", "tokens": [550, 611, 264, 16505, 3199, 486, 312, 34116, 787, 13], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 360, "seek": 162400, "start": 1646.0, "end": 1648.0, "text": " And I showed already that", "tokens": [400, 286, 4712, 1217, 300], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 361, "seek": 162400, "start": 1648.0, "end": 1650.0, "text": " if there's one of the tables updating,", "tokens": [498, 456, 311, 472, 295, 264, 8020, 25113, 11], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 362, "seek": 162400, "start": 1650.0, "end": 1652.0, "text": " the results will be updating.", "tokens": [264, 3542, 486, 312, 25113, 13], "temperature": 0.0, "avg_logprob": -0.25835658134298123, "compression_ratio": 1.6682926829268292, "no_speech_prob": 0.0006516054854728281}, {"id": 363, "seek": 165200, "start": 1652.0, "end": 1654.0, "text": " And if there's some outer join,", "tokens": [400, 498, 456, 311, 512, 10847, 3917, 11], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 364, "seek": 165200, "start": 1654.0, "end": 1656.0, "text": " then the result is always updating.", "tokens": [550, 264, 1874, 307, 1009, 25113, 13], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 365, "seek": 165200, "start": 1656.0, "end": 1660.0, "text": " And now comes the interesting part.", "tokens": [400, 586, 1487, 264, 1880, 644, 13], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 366, "seek": 165200, "start": 1660.0, "end": 1663.0, "text": " If you have append only table,", "tokens": [759, 291, 362, 34116, 787, 3199, 11], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 367, "seek": 165200, "start": 1663.0, "end": 1665.0, "text": " and you join it to an updating table,", "tokens": [293, 291, 3917, 309, 281, 364, 25113, 3199, 11], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 368, "seek": 165200, "start": 1665.0, "end": 1667.0, "text": " there is a special kind of join,", "tokens": [456, 307, 257, 2121, 733, 295, 3917, 11], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 369, "seek": 165200, "start": 1667.0, "end": 1669.0, "text": " which we call temporal join.", "tokens": [597, 321, 818, 30881, 3917, 13], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 370, "seek": 165200, "start": 1669.0, "end": 1671.0, "text": " A temporal join will actually produce", "tokens": [316, 30881, 3917, 486, 767, 5258], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 371, "seek": 165200, "start": 1671.0, "end": 1674.0, "text": " an append only table,", "tokens": [364, 34116, 787, 3199, 11], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 372, "seek": 165200, "start": 1674.0, "end": 1677.0, "text": " because it looks at the table", "tokens": [570, 309, 1542, 412, 264, 3199], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 373, "seek": 165200, "start": 1677.0, "end": 1679.0, "text": " at a point, a specific point in time.", "tokens": [412, 257, 935, 11, 257, 2685, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.1659658117201722, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.000752513762563467}, {"id": 374, "seek": 167900, "start": 1679.0, "end": 1682.0, "text": " That's a very interesting operator.", "tokens": [663, 311, 257, 588, 1880, 12973, 13], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 375, "seek": 167900, "start": 1682.0, "end": 1684.0, "text": " Unfortunately, we don't have enough time,", "tokens": [8590, 11, 321, 500, 380, 362, 1547, 565, 11], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 376, "seek": 167900, "start": 1684.0, "end": 1687.0, "text": " but I just want to show you an example", "tokens": [457, 286, 445, 528, 281, 855, 291, 364, 1365], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 377, "seek": 167900, "start": 1687.0, "end": 1692.0, "text": " of this very, very useful join operator.", "tokens": [295, 341, 588, 11, 588, 4420, 3917, 12973, 13], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 378, "seek": 167900, "start": 1692.0, "end": 1696.0, "text": " So let's assume we have some orders table,", "tokens": [407, 718, 311, 6552, 321, 362, 512, 9470, 3199, 11], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 379, "seek": 167900, "start": 1696.0, "end": 1698.0, "text": " and orders have a currency,", "tokens": [293, 9470, 362, 257, 13346, 11], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 380, "seek": 167900, "start": 1698.0, "end": 1700.0, "text": " and there is a currency rates table.", "tokens": [293, 456, 307, 257, 13346, 6846, 3199, 13], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 381, "seek": 167900, "start": 1700.0, "end": 1704.0, "text": " And obviously, you don't want to join those two tables", "tokens": [400, 2745, 11, 291, 500, 380, 528, 281, 3917, 729, 732, 8020], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 382, "seek": 167900, "start": 1704.0, "end": 1707.0, "text": " with the latest currency rates,", "tokens": [365, 264, 6792, 13346, 6846, 11], "temperature": 0.0, "avg_logprob": -0.10951830943425496, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0010113510070368648}, {"id": 383, "seek": 170700, "start": 1707.0, "end": 1709.0, "text": " but you actually want to know", "tokens": [457, 291, 767, 528, 281, 458], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 384, "seek": 170700, "start": 1709.0, "end": 1711.0, "text": " what was the currency rate at the time", "tokens": [437, 390, 264, 13346, 3314, 412, 264, 565], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 385, "seek": 170700, "start": 1711.0, "end": 1714.0, "text": " when the order was created.", "tokens": [562, 264, 1668, 390, 2942, 13], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 386, "seek": 170700, "start": 1714.0, "end": 1717.0, "text": " And this syntax here with the persistent time as of", "tokens": [400, 341, 28431, 510, 365, 264, 24315, 565, 382, 295], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 387, "seek": 170700, "start": 1717.0, "end": 1720.0, "text": " actually allows you to consume", "tokens": [767, 4045, 291, 281, 14732], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 388, "seek": 170700, "start": 1720.0, "end": 1723.0, "text": " all the changes from the rates table", "tokens": [439, 264, 2962, 490, 264, 6846, 3199], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 389, "seek": 170700, "start": 1723.0, "end": 1725.0, "text": " and join it with orders", "tokens": [293, 3917, 309, 365, 9470], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 390, "seek": 170700, "start": 1725.0, "end": 1727.0, "text": " at the point on the order.", "tokens": [412, 264, 935, 322, 264, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 391, "seek": 170700, "start": 1727.0, "end": 1729.0, "text": " This is just one example", "tokens": [639, 307, 445, 472, 1365], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 392, "seek": 170700, "start": 1729.0, "end": 1733.0, "text": " of a very sophisticated join operation.", "tokens": [295, 257, 588, 16950, 3917, 6916, 13], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 393, "seek": 170700, "start": 1733.0, "end": 1735.0, "text": " And by the way, for system time as of this season,", "tokens": [400, 538, 264, 636, 11, 337, 1185, 565, 382, 295, 341, 3196, 11], "temperature": 0.0, "avg_logprob": -0.1466570641230611, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.0004862997157033533}, {"id": 394, "seek": 173500, "start": 1735.0, "end": 1737.0, "text": " we're going to have to be able to understand", "tokens": [321, 434, 516, 281, 362, 281, 312, 1075, 281, 1223], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 395, "seek": 173500, "start": 1737.0, "end": 1740.0, "text": " so carefully how we see what we're going to do.", "tokens": [370, 7500, 577, 321, 536, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 396, "seek": 173500, "start": 1740.0, "end": 1742.0, "text": " So I also have prepared a demo.", "tokens": [407, 286, 611, 362, 4927, 257, 10723, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 397, "seek": 173500, "start": 1742.0, "end": 1744.0, "text": " I think we still have seven minutes left,", "tokens": [286, 519, 321, 920, 362, 3407, 2077, 1411, 11], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 398, "seek": 173500, "start": 1744.0, "end": 1749.0, "text": " so it should be good to see.", "tokens": [370, 309, 820, 312, 665, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 399, "seek": 173500, "start": 1749.0, "end": 1753.0, "text": " So I also want to show you some of the CPC capabilities.", "tokens": [407, 286, 611, 528, 281, 855, 291, 512, 295, 264, 383, 12986, 10862, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 400, "seek": 173500, "start": 1753.0, "end": 1755.0, "text": " So I will run everything in my IDE.", "tokens": [407, 286, 486, 1190, 1203, 294, 452, 40930, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 401, "seek": 173500, "start": 1755.0, "end": 1758.0, "text": " I will use Java for this example.", "tokens": [286, 486, 764, 10745, 337, 341, 1365, 13], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 402, "seek": 173500, "start": 1758.0, "end": 1762.0, "text": " I have a MySQL container,", "tokens": [286, 362, 257, 1222, 39934, 10129, 11], "temperature": 0.0, "avg_logprob": -0.39204661051432294, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013631534529849887}, {"id": 403, "seek": 176200, "start": 1762.0, "end": 1766.0, "text": " and I'm running it, so we'll start with a SQL container.", "tokens": [293, 286, 478, 2614, 309, 11, 370, 321, 603, 722, 365, 257, 19200, 10129, 13], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 404, "seek": 176200, "start": 1766.0, "end": 1771.0, "text": " I'm processing it.", "tokens": [286, 478, 9007, 309, 13], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 405, "seek": 176200, "start": 1771.0, "end": 1778.0, "text": " And this container will create a MySQL instance,", "tokens": [400, 341, 10129, 486, 1884, 257, 1222, 39934, 5197, 11], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 406, "seek": 176200, "start": 1778.0, "end": 1780.0, "text": " and it will also be filled already", "tokens": [293, 309, 486, 611, 312, 6412, 1217], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 407, "seek": 176200, "start": 1780.0, "end": 1783.0, "text": " with, I think, three or four rows.", "tokens": [365, 11, 286, 519, 11, 1045, 420, 1451, 13241, 13], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 408, "seek": 176200, "start": 1783.0, "end": 1786.0, "text": " I have a few examples here.", "tokens": [286, 362, 257, 1326, 5110, 510, 13], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 409, "seek": 176200, "start": 1786.0, "end": 1788.0, "text": " I can simply run the examples", "tokens": [286, 393, 2935, 1190, 264, 5110], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 410, "seek": 176200, "start": 1788.0, "end": 1790.0, "text": " and the main method of the IDE.", "tokens": [293, 264, 2135, 3170, 295, 264, 40930, 13], "temperature": 0.0, "avg_logprob": -0.3232036413148392, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.0006052729440853}, {"id": 411, "seek": 179000, "start": 1790.0, "end": 1792.0, "text": " So what I'm doing here", "tokens": [407, 437, 286, 478, 884, 510], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 412, "seek": 179000, "start": 1792.0, "end": 1795.0, "text": " is I'm creating different tables to connect to a SQL.", "tokens": [307, 286, 478, 4084, 819, 8020, 281, 1745, 281, 257, 19200, 13], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 413, "seek": 179000, "start": 1795.0, "end": 1797.0, "text": " One is a JPC one,", "tokens": [1485, 307, 257, 508, 12986, 472, 11], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 414, "seek": 179000, "start": 1797.0, "end": 1799.0, "text": " which fully screens the table once,", "tokens": [597, 4498, 11171, 264, 3199, 1564, 11], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 415, "seek": 179000, "start": 1799.0, "end": 1801.0, "text": " and the other one is a CPC one,", "tokens": [293, 264, 661, 472, 307, 257, 383, 12986, 472, 11], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 416, "seek": 179000, "start": 1801.0, "end": 1803.0, "text": " which, like, continuously", "tokens": [597, 11, 411, 11, 15684], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 417, "seek": 179000, "start": 1803.0, "end": 1808.0, "text": " monitors the tables and the JPC.", "tokens": [26518, 264, 8020, 293, 264, 508, 12986, 13], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 418, "seek": 179000, "start": 1808.0, "end": 1811.0, "text": " So let's just run this.", "tokens": [407, 718, 311, 445, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.3017052888870239, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0013005950022488832}, {"id": 419, "seek": 181100, "start": 1811.0, "end": 1826.0, "text": " So here we see the first three results.", "tokens": [407, 510, 321, 536, 264, 700, 1045, 3542, 13], "temperature": 0.0, "avg_logprob": -0.23339845173394502, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.001476135803386569}, {"id": 420, "seek": 181100, "start": 1826.0, "end": 1829.0, "text": " As you can see, the application has not stopped.", "tokens": [1018, 291, 393, 536, 11, 264, 3861, 575, 406, 5936, 13], "temperature": 0.0, "avg_logprob": -0.23339845173394502, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.001476135803386569}, {"id": 421, "seek": 181100, "start": 1829.0, "end": 1832.0, "text": " So it is waiting for more records to come,", "tokens": [407, 309, 307, 3806, 337, 544, 7724, 281, 808, 11], "temperature": 0.0, "avg_logprob": -0.23339845173394502, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.001476135803386569}, {"id": 422, "seek": 181100, "start": 1832.0, "end": 1835.0, "text": " and now I want to insert more values into MySQL.", "tokens": [293, 586, 286, 528, 281, 8969, 544, 4190, 666, 1222, 39934, 13], "temperature": 0.0, "avg_logprob": -0.23339845173394502, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.001476135803386569}, {"id": 423, "seek": 181100, "start": 1835.0, "end": 1840.0, "text": " I could have used MySQL to see a line for that,", "tokens": [286, 727, 362, 1143, 1222, 39934, 281, 536, 257, 1622, 337, 300, 11], "temperature": 0.0, "avg_logprob": -0.23339845173394502, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.001476135803386569}, {"id": 424, "seek": 184000, "start": 1840.0, "end": 1843.0, "text": " but I can also use my SQL to see what I see.", "tokens": [457, 286, 393, 611, 764, 452, 19200, 281, 536, 437, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 425, "seek": 184000, "start": 1843.0, "end": 1847.0, "text": " So I am having a regular,", "tokens": [407, 286, 669, 1419, 257, 3890, 11], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 426, "seek": 184000, "start": 1847.0, "end": 1851.0, "text": " I can set into transaction JPC, values, blah, blah, blah,", "tokens": [286, 393, 992, 666, 14425, 508, 12986, 11, 4190, 11, 12288, 11, 12288, 11, 12288, 11], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 427, "seek": 184000, "start": 1851.0, "end": 1854.0, "text": " and I can run this main method here.", "tokens": [293, 286, 393, 1190, 341, 2135, 3170, 510, 13], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 428, "seek": 184000, "start": 1854.0, "end": 1857.0, "text": " So it's a bit overkill to use Spring for that,", "tokens": [407, 309, 311, 257, 857, 670, 34213, 281, 764, 14013, 337, 300, 11], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 429, "seek": 184000, "start": 1857.0, "end": 1859.0, "text": " for just one value,", "tokens": [337, 445, 472, 2158, 11], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 430, "seek": 184000, "start": 1859.0, "end": 1861.0, "text": " but I think it's flexible enough", "tokens": [457, 286, 519, 309, 311, 11358, 1547], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 431, "seek": 184000, "start": 1861.0, "end": 1864.0, "text": " you can also use it for the hash query", "tokens": [291, 393, 611, 764, 309, 337, 264, 22019, 14581], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 432, "seek": 184000, "start": 1864.0, "end": 1868.0, "text": " of just setting one record into the database,", "tokens": [295, 445, 3287, 472, 2136, 666, 264, 8149, 11], "temperature": 0.0, "avg_logprob": -0.26150924188119395, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.00035592401400208473}, {"id": 433, "seek": 186800, "start": 1868.0, "end": 1870.0, "text": " and as you can see,", "tokens": [293, 382, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 434, "seek": 186800, "start": 1870.0, "end": 1874.0, "text": " we can show my SQL and from my SQL,", "tokens": [321, 393, 855, 452, 19200, 293, 490, 452, 19200, 11], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 435, "seek": 186800, "start": 1874.0, "end": 1878.0, "text": " via CPC to the link,", "tokens": [5766, 383, 12986, 281, 264, 2113, 11], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 436, "seek": 186800, "start": 1878.0, "end": 1881.0, "text": " and then to the next CPC.", "tokens": [293, 550, 281, 264, 958, 383, 12986, 13], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 437, "seek": 186800, "start": 1881.0, "end": 1884.0, "text": " We have also more sophisticated examples here,", "tokens": [492, 362, 611, 544, 16950, 5110, 510, 11], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 438, "seek": 186800, "start": 1884.0, "end": 1888.0, "text": " and I don't think that we have more time for that,", "tokens": [293, 286, 500, 380, 519, 300, 321, 362, 544, 565, 337, 300, 11], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 439, "seek": 186800, "start": 1888.0, "end": 1892.0, "text": " but we can do a lot of things with Spring SQL.", "tokens": [457, 321, 393, 360, 257, 688, 295, 721, 365, 14013, 19200, 13], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 440, "seek": 186800, "start": 1892.0, "end": 1894.0, "text": " I think I could spend a day", "tokens": [286, 519, 286, 727, 3496, 257, 786], "temperature": 0.0, "avg_logprob": -0.267834588383021, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.0006602024077437818}, {"id": 441, "seek": 189400, "start": 1894.0, "end": 1899.0, "text": " and talk a little bit more about the way this works.", "tokens": [293, 751, 257, 707, 857, 544, 466, 264, 636, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 442, "seek": 189400, "start": 1899.0, "end": 1903.0, "text": " So, put on the grid.", "tokens": [407, 11, 829, 322, 264, 10748, 13], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 443, "seek": 189400, "start": 1903.0, "end": 1906.0, "text": " Yeah, Spring SQL and it is very powerful,", "tokens": [865, 11, 14013, 19200, 293, 309, 307, 588, 4005, 11], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 444, "seek": 189400, "start": 1906.0, "end": 1910.0, "text": " has been crafted over years and years and years,", "tokens": [575, 668, 36213, 670, 924, 293, 924, 293, 924, 11], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 445, "seek": 189400, "start": 1910.0, "end": 1913.0, "text": " by many, many companies, many teams.", "tokens": [538, 867, 11, 867, 3431, 11, 867, 5491, 13], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 446, "seek": 189400, "start": 1913.0, "end": 1917.0, "text": " It's very flexible for integration,", "tokens": [467, 311, 588, 11358, 337, 10980, 11], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 447, "seek": 189400, "start": 1917.0, "end": 1921.0, "text": " for integrating various systems of different semantics,", "tokens": [337, 26889, 3683, 3652, 295, 819, 4361, 45298, 11], "temperature": 0.0, "avg_logprob": -0.3439520835876465, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0010553011670708656}, {"id": 448, "seek": 192100, "start": 1921.0, "end": 1924.0, "text": " and there is way more.", "tokens": [293, 456, 307, 636, 544, 13], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 449, "seek": 192100, "start": 1924.0, "end": 1927.0, "text": " So, I just showed some operators,", "tokens": [407, 11, 286, 445, 4712, 512, 19077, 11], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 450, "seek": 192100, "start": 1927.0, "end": 1931.0, "text": " but we have a large, large coverage of SQL standard,", "tokens": [457, 321, 362, 257, 2416, 11, 2416, 9645, 295, 19200, 3832, 11], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 451, "seek": 192100, "start": 1931.0, "end": 1934.0, "text": " so over-windows support for aggregating,", "tokens": [370, 670, 12, 12199, 1509, 1406, 337, 16743, 990, 11], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 452, "seek": 192100, "start": 1934.0, "end": 1938.0, "text": " for Spring, we support the recognized laws", "tokens": [337, 14013, 11, 321, 1406, 264, 9823, 6064], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 453, "seek": 192100, "start": 1938.0, "end": 1941.0, "text": " for pattern matching and complex plan processing.", "tokens": [337, 5102, 14324, 293, 3997, 1393, 9007, 13], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 454, "seek": 192100, "start": 1941.0, "end": 1945.0, "text": " We have time for obsessions, windows for like,", "tokens": [492, 362, 565, 337, 3181, 9069, 11, 9309, 337, 411, 11], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 455, "seek": 192100, "start": 1945.0, "end": 1947.0, "text": " cutting your screen into pieces.", "tokens": [6492, 428, 2568, 666, 3755, 13], "temperature": 0.0, "avg_logprob": -0.3374305374320896, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.0004708365013357252}, {"id": 456, "seek": 194700, "start": 1947.0, "end": 1951.0, "text": " Then there is a huge CPC connector ecosystem,", "tokens": [1396, 456, 307, 257, 2603, 383, 12986, 19127, 11311, 11], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 457, "seek": 194700, "start": 1951.0, "end": 1953.0, "text": " not part of the fourth thing,", "tokens": [406, 644, 295, 264, 6409, 551, 11], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 458, "seek": 194700, "start": 1953.0, "end": 1956.0, "text": " but also quite useful as a little think-of-stars already.", "tokens": [457, 611, 1596, 4420, 382, 257, 707, 519, 12, 2670, 12, 46903, 1217, 13], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 459, "seek": 194700, "start": 1956.0, "end": 1959.0, "text": " Then, something new, fatal store,", "tokens": [1396, 11, 746, 777, 11, 24069, 3531, 11], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 460, "seek": 194700, "start": 1959.0, "end": 1963.0, "text": " which tries to be like the first streaming data warehouse", "tokens": [597, 9898, 281, 312, 411, 264, 700, 11791, 1412, 22244], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 461, "seek": 194700, "start": 1963.0, "end": 1965.0, "text": " kind of thing.", "tokens": [733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 462, "seek": 194700, "start": 1965.0, "end": 1968.0, "text": " It's not a very early version, but it's very promising.", "tokens": [467, 311, 406, 257, 588, 2440, 3037, 11, 457, 309, 311, 588, 20257, 13], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 463, "seek": 194700, "start": 1968.0, "end": 1971.0, "text": " So, yeah, I would recommend to, yeah,", "tokens": [407, 11, 1338, 11, 286, 576, 2748, 281, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 464, "seek": 194700, "start": 1971.0, "end": 1974.0, "text": " maybe look into one of these sub-projects as well.", "tokens": [1310, 574, 666, 472, 295, 613, 1422, 12, 4318, 1020, 82, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.23236770289284842, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.0003730415483005345}, {"id": 465, "seek": 197400, "start": 1974.0, "end": 1978.0, "text": " Not only things, it's big, but also the ecosystem,", "tokens": [1726, 787, 721, 11, 309, 311, 955, 11, 457, 611, 264, 11311, 11], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 466, "seek": 197400, "start": 1978.0, "end": 1982.0, "text": " around the thing, the growths and growths and growths.", "tokens": [926, 264, 551, 11, 264, 4599, 82, 293, 4599, 82, 293, 4599, 82, 13], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 467, "seek": 197400, "start": 1982.0, "end": 1984.0, "text": " I'm happy to take questions.", "tokens": [286, 478, 2055, 281, 747, 1651, 13], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 468, "seek": 197400, "start": 1984.0, "end": 1986.0, "text": " I think we have three minutes left,", "tokens": [286, 519, 321, 362, 1045, 2077, 1411, 11], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 469, "seek": 197400, "start": 1986.0, "end": 1989.0, "text": " but otherwise I will also speak outside for any questions.", "tokens": [457, 5911, 286, 486, 611, 1710, 2380, 337, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 470, "seek": 197400, "start": 1989.0, "end": 1991.0, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.30336467425028485, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.003524636849761009}, {"id": 471, "seek": 199100, "start": 1991.0, "end": 1993.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.45478185017903644, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.000744726974517107}, {"id": 472, "seek": 202100, "start": 2022.0, "end": 2024.0, "text": " Yeah, so like...", "tokens": [865, 11, 370, 411, 485], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 473, "seek": 202100, "start": 2024.0, "end": 2026.0, "text": " Can you please repeat the question?", "tokens": [1664, 291, 1767, 7149, 264, 1168, 30], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 474, "seek": 202100, "start": 2026.0, "end": 2028.0, "text": " The question is like, how does it, like,", "tokens": [440, 1168, 307, 411, 11, 577, 775, 309, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 475, "seek": 202100, "start": 2028.0, "end": 2031.0, "text": " handle like, transactions that also take a lot of time", "tokens": [4813, 411, 11, 16856, 300, 611, 747, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 476, "seek": 202100, "start": 2031.0, "end": 2034.0, "text": " before the transaction has ended.", "tokens": [949, 264, 14425, 575, 4590, 13], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 477, "seek": 202100, "start": 2034.0, "end": 2037.0, "text": " So, in general, I think we are not very good at", "tokens": [407, 11, 294, 2674, 11, 286, 519, 321, 366, 406, 588, 665, 412], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 478, "seek": 202100, "start": 2037.0, "end": 2040.0, "text": " transaction handling in things,", "tokens": [14425, 13175, 294, 721, 11], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 479, "seek": 202100, "start": 2040.0, "end": 2043.0, "text": " but like you have with Data Stream API,", "tokens": [457, 411, 291, 362, 365, 11888, 24904, 9362, 11], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 480, "seek": 202100, "start": 2043.0, "end": 2045.0, "text": " you have a lot of possibilities to,", "tokens": [291, 362, 257, 688, 295, 12178, 281, 11], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 481, "seek": 202100, "start": 2045.0, "end": 2048.0, "text": " like for example, you can buffer all the data from this", "tokens": [411, 337, 1365, 11, 291, 393, 21762, 439, 264, 1412, 490, 341], "temperature": 0.0, "avg_logprob": -0.23953208573367618, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.643399715423584}, {"id": 482, "seek": 204800, "start": 2048.0, "end": 2053.0, "text": " transaction and stay at it after terrible time you want to,", "tokens": [14425, 293, 1754, 412, 309, 934, 6237, 565, 291, 528, 281, 11], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 483, "seek": 204800, "start": 2053.0, "end": 2056.0, "text": " and then just wait until the transaction closes,", "tokens": [293, 550, 445, 1699, 1826, 264, 14425, 24157, 11], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 484, "seek": 204800, "start": 2056.0, "end": 2060.0, "text": " and then you're creating the execution of the transaction,", "tokens": [293, 550, 291, 434, 4084, 264, 15058, 295, 264, 14425, 11], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 485, "seek": 204800, "start": 2060.0, "end": 2061.0, "text": " and that is possible.", "tokens": [293, 300, 307, 1944, 13], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 486, "seek": 204800, "start": 2061.0, "end": 2064.0, "text": " So, yeah, personally, I would maybe do some stuff", "tokens": [407, 11, 1338, 11, 5665, 11, 286, 576, 1310, 360, 512, 1507], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 487, "seek": 204800, "start": 2064.0, "end": 2067.0, "text": " in the Data Stream API first until the transaction ended,", "tokens": [294, 264, 11888, 24904, 9362, 700, 1826, 264, 14425, 4590, 11], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 488, "seek": 204800, "start": 2067.0, "end": 2069.0, "text": " and then push that.", "tokens": [293, 550, 2944, 300, 13], "temperature": 0.0, "avg_logprob": -0.34269130377121915, "compression_ratio": 1.7043010752688172, "no_speech_prob": 0.0027646725066006184}, {"id": 489, "seek": 206900, "start": 2069.0, "end": 2074.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 490, "seek": 206900, "start": 2074.0, "end": 2076.0, "text": " Yeah, thank you both.", "tokens": [865, 11, 1309, 291, 1293, 13], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 491, "seek": 206900, "start": 2076.0, "end": 2078.0, "text": " So, in terms of running times,", "tokens": [407, 11, 294, 2115, 295, 2614, 1413, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 492, "seek": 206900, "start": 2078.0, "end": 2081.0, "text": " do I just think from bottom all,", "tokens": [360, 286, 445, 519, 490, 2767, 439, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 493, "seek": 206900, "start": 2081.0, "end": 2085.0, "text": " if it's an hybrid, I shift my interpretation?", "tokens": [498, 309, 311, 364, 13051, 11, 286, 5513, 452, 14174, 30], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 494, "seek": 206900, "start": 2085.0, "end": 2087.0, "text": " No, I think it's creating its own,", "tokens": [883, 11, 286, 519, 309, 311, 4084, 1080, 1065, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 495, "seek": 206900, "start": 2087.0, "end": 2089.0, "text": " its own runtime.", "tokens": [1080, 1065, 34474, 13], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 496, "seek": 206900, "start": 2089.0, "end": 2091.0, "text": " So, you will be able to do mutual apps,", "tokens": [407, 11, 291, 486, 312, 1075, 281, 360, 16917, 7733, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 497, "seek": 206900, "start": 2091.0, "end": 2093.0, "text": " some types of software,", "tokens": [512, 3467, 295, 4722, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 498, "seek": 206900, "start": 2093.0, "end": 2095.0, "text": " some types of software,", "tokens": [512, 3467, 295, 4722, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 499, "seek": 206900, "start": 2095.0, "end": 2097.0, "text": " some types of software,", "tokens": [512, 3467, 295, 4722, 11], "temperature": 0.0, "avg_logprob": -0.5999593734741211, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.007221900392323732}, {"id": 500, "seek": 209700, "start": 2097.0, "end": 2099.0, "text": " and then just look at the,", "tokens": [293, 550, 445, 574, 412, 264, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 501, "seek": 209700, "start": 2099.0, "end": 2102.0, "text": " like, I don't know if there's a lot of data in the stream.", "tokens": [411, 11, 286, 500, 380, 458, 498, 456, 311, 257, 688, 295, 1412, 294, 264, 4309, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 502, "seek": 209700, "start": 2102.0, "end": 2103.0, "text": " No, I don't.", "tokens": [883, 11, 286, 500, 380, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 503, "seek": 209700, "start": 2103.0, "end": 2104.0, "text": " Is that all there?", "tokens": [1119, 300, 439, 456, 30], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 504, "seek": 209700, "start": 2104.0, "end": 2105.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 505, "seek": 209700, "start": 2105.0, "end": 2106.0, "text": " Let me check.", "tokens": [961, 385, 1520, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 506, "seek": 209700, "start": 2106.0, "end": 2108.0, "text": " There's a library that's called the HGIS,", "tokens": [821, 311, 257, 6405, 300, 311, 1219, 264, 389, 38, 2343, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 507, "seek": 209700, "start": 2108.0, "end": 2110.0, "text": " which is not for you to be used,", "tokens": [597, 307, 406, 337, 291, 281, 312, 1143, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 508, "seek": 209700, "start": 2110.0, "end": 2112.0, "text": " but we can also buy it,", "tokens": [457, 321, 393, 611, 2256, 309, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 509, "seek": 209700, "start": 2112.0, "end": 2114.0, "text": " and that's the idea for a line.", "tokens": [293, 300, 311, 264, 1558, 337, 257, 1622, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 510, "seek": 209700, "start": 2114.0, "end": 2115.0, "text": " But in general, yeah,", "tokens": [583, 294, 2674, 11, 1338, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 511, "seek": 209700, "start": 2115.0, "end": 2118.0, "text": " the platform, it's rather a platform,", "tokens": [264, 3663, 11, 309, 311, 2831, 257, 3663, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 512, "seek": 209700, "start": 2118.0, "end": 2120.0, "text": " that's where it looks.", "tokens": [300, 311, 689, 309, 1542, 13], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 513, "seek": 209700, "start": 2122.0, "end": 2123.0, "text": " Now, the next logical question,", "tokens": [823, 11, 264, 958, 14978, 1168, 11], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 514, "seek": 209700, "start": 2123.0, "end": 2125.0, "text": " how far does it scale?", "tokens": [577, 1400, 775, 309, 4373, 30], "temperature": 0.2, "avg_logprob": -0.6024125417073568, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0038454222958534956}, {"id": 515, "seek": 212500, "start": 2125.0, "end": 2127.0, "text": " I would say,", "tokens": [286, 576, 584, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 516, "seek": 212500, "start": 2127.0, "end": 2129.0, "text": " as the question was,", "tokens": [382, 264, 1168, 390, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 517, "seek": 212500, "start": 2129.0, "end": 2131.0, "text": " how far does it scale,", "tokens": [577, 1400, 775, 309, 4373, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 518, "seek": 212500, "start": 2133.0, "end": 2135.0, "text": " I can tell you that", "tokens": [286, 393, 980, 291, 300], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 519, "seek": 212500, "start": 2135.0, "end": 2137.0, "text": " probably from the system,", "tokens": [1391, 490, 264, 1185, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 520, "seek": 212500, "start": 2137.0, "end": 2139.0, "text": " and like,", "tokens": [293, 411, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 521, "seek": 212500, "start": 2139.0, "end": 2141.0, "text": " there's single state,", "tokens": [456, 311, 2167, 1785, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 522, "seek": 212500, "start": 2141.0, "end": 2143.0, "text": " there's a billion,", "tokens": [456, 311, 257, 5218, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 523, "seek": 212500, "start": 2143.0, "end": 2144.0, "text": " all of us,", "tokens": [439, 295, 505, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 524, "seek": 212500, "start": 2144.0, "end": 2146.0, "text": " and all of us in the day or so.", "tokens": [293, 439, 295, 505, 294, 264, 786, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 525, "seek": 212500, "start": 2147.0, "end": 2149.0, "text": " Yeah, and there is Apple,", "tokens": [865, 11, 293, 456, 307, 6373, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 526, "seek": 212500, "start": 2149.0, "end": 2151.0, "text": " that processes things,", "tokens": [300, 7555, 721, 11], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 527, "seek": 212500, "start": 2151.0, "end": 2153.0, "text": " like all the big banks", "tokens": [411, 439, 264, 955, 10237], "temperature": 0.0, "avg_logprob": -0.4131869475046794, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0002860654203686863}, {"id": 528, "seek": 215300, "start": 2153.0, "end": 2156.0, "text": " use it for credit card fraud detection", "tokens": [764, 309, 337, 5397, 2920, 14560, 17784], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 529, "seek": 215300, "start": 2156.0, "end": 2157.0, "text": " and stuff like that.", "tokens": [293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 530, "seek": 215300, "start": 2157.0, "end": 2160.0, "text": " So, I don't think that,", "tokens": [407, 11, 286, 500, 380, 519, 300, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 531, "seek": 215300, "start": 2160.0, "end": 2163.0, "text": " like most companies in this room,", "tokens": [411, 881, 3431, 294, 341, 1808, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 532, "seek": 215300, "start": 2163.0, "end": 2166.0, "text": " they will not reach the scalability limits of Link,", "tokens": [436, 486, 406, 2524, 264, 15664, 2310, 10406, 295, 8466, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 533, "seek": 215300, "start": 2166.0, "end": 2168.0, "text": " because yeah, we are not at,", "tokens": [570, 1338, 11, 321, 366, 406, 412, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 534, "seek": 215300, "start": 2168.0, "end": 2171.0, "text": " unless here some Apple or Alibaba people", "tokens": [5969, 510, 512, 6373, 420, 967, 897, 5509, 561], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 535, "seek": 215300, "start": 2171.0, "end": 2172.0, "text": " are in this room,", "tokens": [366, 294, 341, 1808, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 536, "seek": 215300, "start": 2172.0, "end": 2174.0, "text": " then maybe, I don't know.", "tokens": [550, 1310, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 537, "seek": 215300, "start": 2177.0, "end": 2179.0, "text": " You said that the frame does not own data,", "tokens": [509, 848, 300, 264, 3920, 775, 406, 1065, 1412, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 538, "seek": 215300, "start": 2179.0, "end": 2181.0, "text": " but then there is this table store project,", "tokens": [457, 550, 456, 307, 341, 3199, 3531, 1716, 11], "temperature": 0.0, "avg_logprob": -0.16819629153689822, "compression_ratio": 1.6017316017316017, "no_speech_prob": 3.119056418654509e-05}, {"id": 539, "seek": 218100, "start": 2181.0, "end": 2183.0, "text": " so is it like move towards,", "tokens": [370, 307, 309, 411, 1286, 3030, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 540, "seek": 218100, "start": 2183.0, "end": 2185.0, "text": " you know, more like ownership?", "tokens": [291, 458, 11, 544, 411, 15279, 30], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 541, "seek": 218100, "start": 2185.0, "end": 2187.0, "text": " Yeah, this table store is a very,", "tokens": [865, 11, 341, 3199, 3531, 307, 257, 588, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 542, "seek": 218100, "start": 2187.0, "end": 2189.0, "text": " very interesting approach,", "tokens": [588, 1880, 3109, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 543, "seek": 218100, "start": 2189.0, "end": 2191.0, "text": " like it started last year,", "tokens": [411, 309, 1409, 1036, 1064, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 544, "seek": 218100, "start": 2191.0, "end": 2192.0, "text": " or two years ago,", "tokens": [420, 732, 924, 2057, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 545, "seek": 218100, "start": 2192.0, "end": 2193.0, "text": " it's rather new.", "tokens": [309, 311, 2831, 777, 13], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 546, "seek": 218100, "start": 2193.0, "end": 2195.0, "text": " I think it was last year, early last year.", "tokens": [286, 519, 309, 390, 1036, 1064, 11, 2440, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 547, "seek": 218100, "start": 2196.0, "end": 2199.0, "text": " It doesn't really fit to Apache Flink,", "tokens": [467, 1177, 380, 534, 3318, 281, 46597, 3235, 475, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 548, "seek": 218100, "start": 2199.0, "end": 2201.0, "text": " but it's still, it's very useful,", "tokens": [457, 309, 311, 920, 11, 309, 311, 588, 4420, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 549, "seek": 218100, "start": 2201.0, "end": 2203.0, "text": " and yeah, we will see,", "tokens": [293, 1338, 11, 321, 486, 536, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 550, "seek": 218100, "start": 2203.0, "end": 2205.0, "text": " maybe it will leave the,", "tokens": [1310, 309, 486, 1856, 264, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 551, "seek": 218100, "start": 2205.0, "end": 2207.0, "text": " maybe it will leave the Apache Software Foundation soon,", "tokens": [1310, 309, 486, 1856, 264, 46597, 27428, 10335, 2321, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 552, "seek": 218100, "start": 2207.0, "end": 2209.0, "text": " but yeah, not allowed to,", "tokens": [457, 1338, 11, 406, 4350, 281, 11], "temperature": 0.0, "avg_logprob": -0.16578714171452308, "compression_ratio": 1.7398373983739837, "no_speech_prob": 9.731777390697971e-05}, {"id": 553, "seek": 220900, "start": 2209.0, "end": 2211.0, "text": " and not the Software Foundation,", "tokens": [293, 406, 264, 27428, 10335, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 554, "seek": 220900, "start": 2211.0, "end": 2213.0, "text": " but the Flink project itself,", "tokens": [457, 264, 3235, 475, 1716, 2564, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 555, "seek": 220900, "start": 2213.0, "end": 2214.0, "text": " we will see,", "tokens": [321, 486, 536, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 556, "seek": 220900, "start": 2216.0, "end": 2218.0, "text": " because it doesn't fit really well,", "tokens": [570, 309, 1177, 380, 3318, 534, 731, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 557, "seek": 220900, "start": 2218.0, "end": 2219.0, "text": " but it's in general,", "tokens": [457, 309, 311, 294, 2674, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 558, "seek": 220900, "start": 2219.0, "end": 2222.0, "text": " like we still have this vision of Flink as a database.", "tokens": [411, 321, 920, 362, 341, 5201, 295, 3235, 475, 382, 257, 8149, 13], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 559, "seek": 220900, "start": 2222.0, "end": 2223.0, "text": " Yeah, we will see.", "tokens": [865, 11, 321, 486, 536, 13], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 560, "seek": 220900, "start": 2223.0, "end": 2224.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 561, "seek": 220900, "start": 2226.0, "end": 2227.0, "text": " Sir, a question.", "tokens": [6144, 11, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 562, "seek": 220900, "start": 2227.0, "end": 2229.0, "text": " This is about big states,", "tokens": [639, 307, 466, 955, 4368, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 563, "seek": 220900, "start": 2229.0, "end": 2230.0, "text": " because you mentioned that you can have like", "tokens": [570, 291, 2835, 300, 291, 393, 362, 411], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 564, "seek": 220900, "start": 2230.0, "end": 2231.0, "text": " terabytes of state,", "tokens": [1796, 24538, 295, 1785, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 565, "seek": 220900, "start": 2231.0, "end": 2234.0, "text": " but when you create a checkpoint,", "tokens": [457, 562, 291, 1884, 257, 42269, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 566, "seek": 220900, "start": 2234.0, "end": 2237.0, "text": " and if this checkpoint will be very big,", "tokens": [293, 498, 341, 42269, 486, 312, 588, 955, 11], "temperature": 0.0, "avg_logprob": -0.1966346802750254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.41820437926799e-05}, {"id": 567, "seek": 223700, "start": 2237.0, "end": 2240.0, "text": " and storage of it can be long,", "tokens": [293, 6725, 295, 309, 393, 312, 938, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 568, "seek": 223700, "start": 2240.0, "end": 2243.0, "text": " is it like a huge DC post to write into the store?", "tokens": [307, 309, 411, 257, 2603, 9114, 2183, 281, 2464, 666, 264, 3531, 30], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 569, "seek": 223700, "start": 2243.0, "end": 2244.0, "text": " Yeah, so the question was,", "tokens": [865, 11, 370, 264, 1168, 390, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 570, "seek": 223700, "start": 2244.0, "end": 2248.0, "text": " like how can we actually snapshot large state in general?", "tokens": [411, 577, 393, 321, 767, 30163, 2416, 1785, 294, 2674, 30], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 571, "seek": 223700, "start": 2248.0, "end": 2251.0, "text": " And this is exactly where Flink distinguishes,", "tokens": [400, 341, 307, 2293, 689, 3235, 475, 11365, 16423, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 572, "seek": 223700, "start": 2251.0, "end": 2254.0, "text": " like where it differs from competitors,", "tokens": [411, 689, 309, 37761, 490, 18333, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 573, "seek": 223700, "start": 2254.0, "end": 2256.0, "text": " because there is a lot of engineering involved", "tokens": [570, 456, 307, 257, 688, 295, 7043, 3288], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 574, "seek": 223700, "start": 2256.0, "end": 2259.0, "text": " to make this as efficient as possible.", "tokens": [281, 652, 341, 382, 7148, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 575, "seek": 223700, "start": 2259.0, "end": 2261.0, "text": " I'm sure there's even more to do,", "tokens": [286, 478, 988, 456, 311, 754, 544, 281, 360, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 576, "seek": 223700, "start": 2261.0, "end": 2263.0, "text": " it's still not perfectly efficient,", "tokens": [309, 311, 920, 406, 6239, 7148, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 577, "seek": 223700, "start": 2263.0, "end": 2265.0, "text": " there's more optimizations that you can do,", "tokens": [456, 311, 544, 5028, 14455, 300, 291, 393, 360, 11], "temperature": 0.0, "avg_logprob": -0.13725001637528583, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00016575508925598115}, {"id": 578, "seek": 226500, "start": 2265.0, "end": 2269.0, "text": " but for example, there is like differential snapshots", "tokens": [457, 337, 1365, 11, 456, 307, 411, 15756, 19206, 27495], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 579, "seek": 226500, "start": 2269.0, "end": 2273.0, "text": " involved, there is local recovery involved,", "tokens": [3288, 11, 456, 307, 2654, 8597, 3288, 11], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 580, "seek": 226500, "start": 2273.0, "end": 2277.0, "text": " or like there are many, many algorithms under the hood", "tokens": [420, 411, 456, 366, 867, 11, 867, 14642, 833, 264, 13376], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 581, "seek": 226500, "start": 2277.0, "end": 2280.0, "text": " to make it as quickly as possible.", "tokens": [281, 652, 309, 382, 2661, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 582, "seek": 226500, "start": 2280.0, "end": 2281.0, "text": " But yeah, of course,", "tokens": [583, 1338, 11, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 583, "seek": 226500, "start": 2281.0, "end": 2283.0, "text": " like if you have terabytes of state,", "tokens": [411, 498, 291, 362, 1796, 24538, 295, 1785, 11], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 584, "seek": 226500, "start": 2283.0, "end": 2285.0, "text": " and the machine has died completely,", "tokens": [293, 264, 3479, 575, 4539, 2584, 11], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 585, "seek": 226500, "start": 2285.0, "end": 2289.0, "text": " and then you obviously need to restore these terabytes", "tokens": [293, 550, 291, 2745, 643, 281, 15227, 613, 1796, 24538], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 586, "seek": 226500, "start": 2289.0, "end": 2293.0, "text": " of state from S3 into your task manager again,", "tokens": [295, 1785, 490, 318, 18, 666, 428, 5633, 6598, 797, 11], "temperature": 0.0, "avg_logprob": -0.11637331008911132, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.9305570933502167e-05}, {"id": 587, "seek": 229300, "start": 2293.0, "end": 2295.0, "text": " and this can take time.", "tokens": [293, 341, 393, 747, 565, 13], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 588, "seek": 229300, "start": 2295.0, "end": 2297.0, "text": " So like it tries its best,", "tokens": [407, 411, 309, 9898, 1080, 1151, 11], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 589, "seek": 229300, "start": 2297.0, "end": 2298.0, "text": " but yeah, of course,", "tokens": [457, 1338, 11, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 590, "seek": 229300, "start": 2298.0, "end": 2301.0, "text": " you need to do benchmarks for your use case.", "tokens": [291, 643, 281, 360, 43751, 337, 428, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 591, "seek": 229300, "start": 2309.0, "end": 2310.0, "text": " One last question.", "tokens": [1485, 1036, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 592, "seek": 229300, "start": 2319.0, "end": 2322.0, "text": " Yeah, so we guarantee exactly once and to end", "tokens": [865, 11, 370, 321, 10815, 2293, 1564, 293, 281, 917], "temperature": 0.0, "avg_logprob": -0.18350284977963097, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00010381618631072342}, {"id": 593, "seek": 232200, "start": 2322.0, "end": 2326.0, "text": " if the connectors, source and sync support that,", "tokens": [498, 264, 31865, 11, 4009, 293, 20271, 1406, 300, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 594, "seek": 232200, "start": 2326.0, "end": 2328.0, "text": " like especially for state,", "tokens": [411, 2318, 337, 1785, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 595, "seek": 232200, "start": 2328.0, "end": 2330.0, "text": " so there are no duplicates in state,", "tokens": [370, 456, 366, 572, 17154, 1024, 294, 1785, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 596, "seek": 232200, "start": 2330.0, "end": 2334.0, "text": " we might need to reprocess data during a failure,", "tokens": [321, 1062, 643, 281, 35257, 780, 1412, 1830, 257, 7763, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 597, "seek": 232200, "start": 2334.0, "end": 2336.0, "text": " but yeah, like end to end,", "tokens": [457, 1338, 11, 411, 917, 281, 917, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 598, "seek": 232200, "start": 2336.0, "end": 2339.0, "text": " exactly once semantics are possible.", "tokens": [2293, 1564, 4361, 45298, 366, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 599, "seek": 232200, "start": 2341.0, "end": 2342.0, "text": " Okay, then yeah, thank you very much,", "tokens": [1033, 11, 550, 1338, 11, 1309, 291, 588, 709, 11], "temperature": 0.0, "avg_logprob": -0.13212544085031533, "compression_ratio": 1.4974093264248705, "no_speech_prob": 2.9298234949237667e-05}, {"id": 600, "seek": 234200, "start": 2342.0, "end": 2352.0, "text": " and I'm waiting outside.", "tokens": [50364, 293, 286, 478, 3806, 2380, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10542130470275879, "compression_ratio": 0.75, "no_speech_prob": 4.121577148907818e-05}], "language": "en"}