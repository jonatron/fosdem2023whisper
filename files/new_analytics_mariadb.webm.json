{"text": " Hi everybody. Let's begin. Thank you all for coming. My name is Roman Nostrin. I'm from MariaDB Corporation from a column store team and today I will be sitting just because the manipulator doesn't work. So today I will tell you about new features that comes in analytics and maybe not so new, but something I find important. For those who don't know what MariaDB column store is, it's an all-lib storage engine for MariaDB server. It's columnar oriented. It's MPP and it has fancy two-tier distributed storage. When the data is distributed across nodes and all the nodes have the possibility to distribute it even more using DB routes. Where DB routes are basically the Linux mounts. And I will start with the most obscure in my opinion, most obscure topic and that is the MariaDB column store version numbering. Why it is obscure? Because I still don't understand the reasons that server team takes to put this or another version of column store into the community server. But anyway, first of all, column store migrated to yet another version in numbering to simplify the understanding what is the actual column store version in the community server. And that is there are three digits, first goes here, then month and then the page number. And at the bottom side of the slide, you can see the actual mapping that shows us which column store versions are published with the community server. The most notable thing here is the last row that tells when the next table, the most featureful release will be published with the community server. Let's first glance at the features available at the current stable that is shipped with the community 10.5 and 10.6. Most notable is the filtering vectorization for x86. And guess what? Seemed processing is not the ultimate answer to life universe and everything. I mean, the benchmarks, when I first did them, when I first run them, they showed me a 10x speedup comparing with the non vectorized code. But when I run the full blown query processing pipeline, I just got 30 to 40% of the speedup. So this is a great speedup, but not the ultimate answer. Anyway, the next feature is an external group buy. And that is what it is. It allows a group buy operation to crunch the data sets, data sets that doesn't fit into memory. And as usually, it consists of two phases. The first phase calculates the partial aggregates. And when they don't fit into memory, it all flots them and stores on disk. And the second phase sequentially goes over the stored pre-aggregates and merge them together to produce the resulting data set. As I have seen in another open source OLAP engines, the second phase takes 2x memory comparing to the first phase in the worst case. This feature is disabled by default. And one needs to explicitly enable it with the settings in a column store.xml. These settings are shown at the bottom of the slide. The next feature is LZ4 compression for data files. We had heard a lot that LZ4 beats snappy in terms of the compression speed, so we decided to give it a try. And guess what? Snappy still delivers better compression ratio. It is about 5% better comparing to LZ4 compressed data files. So there is always a trade-off between space and the compression speed that does the query processing speed. This is also disabled by default in the current stable. And to enable it, you need to set this MariaDB client session variable. So we basically connect to MariaDB and set this variable just before you create a table. Or you can set it in the configuration file of MariaDB server. And we decided to make this a new default since the new stable that is coming this April or May. What's the benefit? It means that the compression and decompression ratio are very fine. Good point. It might be obscure. The compression ratio for snappy is still better, but the decompression speed is faster for LZ4. Not really, because ingestion speed is not the big question. I mean, everybody wants to ingest their data as fast as possible, but let's be honest, we are interested in the select, not in certs in the first place. Yeah, that is true for LTP. Okay, and now we come to the new upcoming stable over column store. And there are some features that I will not discuss in details, but there are some interesting ones that I will look into later. So let's get quick answer these ones. We are officially support ARM64 platform with vectorization as well using Neon. So this platform builds produces slightly faster runtime comparing to our normal nightly CI routines. But I have seen that it consumes 5% more over RAM, running the DML heavy workload like updates or deletes. The next tool is called MCS Rebuild AM. And to explain why we decide to write this tool, I need to make a small D tool. So how does column store the data? There are two parts, the data itself and the metadata that describes where the data is. Like when you have multiple nodes, you need to tell the query coordinator where to go for the data files. So the metadata describes the location of data and column store tries its best to preserve this metadata in tact and it has multiple copies of meta. So if one meta is corrupted for some reason, because life is not ideal, there are multiple copies that you can use as a backup to restore. However, if one loses its meta, there is no way to access the data itself properly, even if the data files are intact. So this tool allows one to produce the metadata from the data files. The key way is you have to create all the tables starting with column store older than 6.4.4 to allow this feature to work. The next feature is distributed JSON function support. Technically speaking, there were JSON functions in the column store from the very beginning, but unfortunately, to use them, you need to fall back to a very slow mode, we call it table mode. That is basically MariaDB crunching the data, but it asks for a full table scan from a column store. So it's not very scalable, not fast. So we implemented this in terms of last year JSON. And now you can use all but one functions. And these are JSON object egg and the JSON table. The last one is very, how to say, MariaDB specific. It produces the relation. That is also true. But I mean, by specific, I mean, we cannot produce the MariaDB relation, to be honest. So we decided to postpone it because nobody is very interested in this one. But we implemented. Good suggestion. Anybody is interested and maybe anybody knows what JSON table does? Okay, that's a good statistic. So now, and here are features that I want to discuss in some more details. First is auxiliary column. What it does, it basically speds up the leads from 3x up to 50x, depending on a SQL schema. And what does it mean? The more columns you have in the table, the faster the speed up will be. It is disabled. It has an additional sped up configuration option. I will explain it a bit later. But let's take a look at the bottom half of a slide. This is a very simplified data files layout to explain how delete operation works previously before the patch on the left and after the patch on the right. So let's concentrate on the left part. You can see the violet and blue empties. So when delete comes into the table, it replaces the actual values in the columns, because we are columnar, with the special magic values that are called empty, that are specific for a data type. And it does it in place. And moreover, the delete operation has to store the actual block for a version before it changes the data in place. So there are two block copies to do, and there are four in place changes that has to be flushed. So what does the auxiliary column changes in this pattern? First, you can see on the right side, there is an additional column. There is basically a flag. So delete operation now goes over this auxiliary column and changes the flag over there. And this auxiliary column is only one byte. So you need to store only one block of data, and you don't need to change to do in place changes in all the columns. That's where the sped up comes from. As I said, there is an additional, there is an additional opportunity to sped up it even more, and this is to enable a fast delete. I will not discuss it in details, but internally, it doesn't update the metadata file, so it makes it even faster to delete the operator. Basically, you have a new column indicating if it's deleted or not, and you just update it. Yes, that's true. We have plans to use this even more, this auxiliary column, but I will discuss it later when we finally implement these features. And to be honest, this fast delete gives an opportunity to implement the append-only update. That should be also a very fast boost for update operation, but stay tuned, not today. Next feature is extent map scalability improvement, and what extent map is. If you recall, couple slides before, I already mentioned the metadata that describes where the data is, and extent map is the core of this metadata. It's basically a structure and memory structure that allows to map from a globally unique block number. Block is a minimally possible data unit in a cluster, and the extent map allows to map from this block number to a tuple of OID, not partition segment, and vice versa, from the tuple to a globally unique block number. And these operations are used extensively in the cluster because you always want to know where the data is and what is the OID that this block belongs to. Originally, the extent map was an array basically with the OIN lookup complexity, and this array was replaced with, oh, I need to mention why it is a problem. Imagine the extent map that is 200 max. So, going over 200 max, and one entry entry is only 100 bytes only, so imagine how many entries are in these 200 max. It takes a lot to look up in such an enormous array. An array, this array was replaced with a red-black tree. This makes the block-to-tuple mapping to, this changes the complexity of block-to-tuple mapping to log n, or log n. And to facilitate another conversion, another mapping from tuple to block, we implemented the extent map index. There is basically a burger of couple hash maps on top and a very tiny arrays at the bottom. So, this gives us the mapping operation complexity of OC. And here is at the bottom, you will see the results. These are except of the CP import logs. One is, one demonstrates that the preprocessed step takes roughly 30 seconds, and after the patch, you see that it decreases to four seconds. It was originally. So, if you have a huge extent map, it will give even faster operations. The next feature is a primproc and ex-manager processes merger. If anybody here uses comestory, he knows there are a bunch of processes. And the central ones are primproc and ex-manager, where ex-manager is a coordinator for the query processing. And primproc is an actual worker. So, there were a lot of additional headache when the local processes must communicate between each other over the same, in the same node. So, same node communication goes over loopback. And this traffic is not compressed, comparing to the different nodes communication that is compressed. So, combining these two run times, we get the four to seven percent overall sped up. And this gives us another opportunities for optimizations that I will mention later. Union push down. Previously, there was no way to directly run the queries, like you can see at the bottom half of the bottom, the most bottom line when the union closes at the top. Because MariaDB processing path for unions and simple selects, it differs. And columnstore works using the notion of select handler that allows MariaDB server to push the whole query down to columnstore. So, when there was a union, we cannot use this path and we have to do, we call it a subquery wrap. So, we were wrapping these into a subquery and this allows us to use the select handler, comparing to a table mode that is slower. The next big feature is a full TPCH support. And why it is big? Because it is mostly concerned with the optimizer, that let's face it columnstore lag previously. There are two queries, with two features that columnstore lack and these two are correlated subquery with aggregates. It's basically a scalar query, but with aggregate. It returns a single row that you can use in comparison. And guess what? To enable this feature, we just disabled the error message in the code. So, it was a very, very tiny change. And columnstore naturally supports such queries. However, the last query type that was presented at the bottom half is way more elaborate to handle. And that is the common conjunction detection and rewriting. As you can see, there are two joint conditions at the bottom half of the slide. They are marked with the violet. And these conditions are common. However, columnstore cannot handle the joint conditions if they are combined with a junction or basically. So, to handle this query, we need to go over all the conditions that are ordered and get the common one and put it at the top. For a general case, it's very complex to find such patterns and applies them in a way that it doesn't make the symmetrical meaning of the query itself. However, we manage this and this feature will benefit not only TPCH query that didn't work previously, but all others. External distinct. As you might know, columnstore was not great doing this thing. To be honest, it was hash map based. So, it cannot do an external operation and it was tightly coupled with order by. So, to allow this thing to be enacted, to become an external, we need to untie them, to uncouple them, and we just applied the existing goodbye facility that already has the external support, external operation support. So, this gives us a future optimization opportunities and also it allows the distinct to scale now. So, it will be fast. The most notable changes maybe because I was the order is the order by rewrite. The current implementation of order by facility is based on a priority queue. That is great for top K queries like mentioned in the second bullet, maybe the third. However, the priority queue timings are just terrifying when you run the query on a huge data set without limit or the limit is big and relatively big. So, I replace the priority queue I'll go with the different approach. It has 2.5 phases. It first calculated the sort of runs in parallel, completely in parallel. And the new middle phase, it calculates non-overlap intermutation ranges for the second phase. So, the second phase officially merges non-overlap ranges in parallel. So, you have first phase and second phase and these two phases are completely parallel. They don't overlap each other. So, in the end, the column star now has a choice between the priority queue based sorting and this new algorithm that it will pick looking at the order by usage pattern of the query. And these are the results. As you can see, the most interesting is last two bullets. There is a comparison between the previous code, the previous version that is based on a priority queue, the data taken from a TPCDS generator. And the first query uses the integer sorting key columns. And the next, the second query uses the character key columns. So, as you can see, for integers, it brings like four times faster. But I need to confess this is only scalability factor, scale factor 10. So, it's not a big dataset, anyway. For a bigger dataset, you can have like 20x or maybe even bigger. And to be honest, I don't compare with the other open source engine just yet because I'm going to come up with a separate story about this sorting. So, these are the links. If you're interested in column star, first is the code itself and the second is JIRA. So, if you find bags, please post them. We will appreciate it. Thank you all for coming again. you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.44, "text": " Hi everybody. Let's begin. Thank you all for coming. My name is Roman Nostrin. I'm from", "tokens": [50364, 2421, 2201, 13, 961, 311, 1841, 13, 1044, 291, 439, 337, 1348, 13, 1222, 1315, 307, 8566, 426, 555, 12629, 13, 286, 478, 490, 50936], "temperature": 0.0, "avg_logprob": -0.25843201822309353, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.4264211654663086}, {"id": 1, "seek": 0, "start": 11.44, "end": 19.080000000000002, "text": " MariaDB Corporation from a column store team and today I will be sitting just because the", "tokens": [50936, 12734, 27735, 26464, 490, 257, 7738, 3531, 1469, 293, 965, 286, 486, 312, 3798, 445, 570, 264, 51318], "temperature": 0.0, "avg_logprob": -0.25843201822309353, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.4264211654663086}, {"id": 2, "seek": 0, "start": 19.080000000000002, "end": 28.2, "text": " manipulator doesn't work. So today I will tell you about new features that comes in", "tokens": [51318, 9258, 16381, 1177, 380, 589, 13, 407, 965, 286, 486, 980, 291, 466, 777, 4122, 300, 1487, 294, 51774], "temperature": 0.0, "avg_logprob": -0.25843201822309353, "compression_ratio": 1.3957219251336899, "no_speech_prob": 0.4264211654663086}, {"id": 3, "seek": 2820, "start": 28.2, "end": 37.76, "text": " analytics and maybe not so new, but something I find important. For those who don't know", "tokens": [50364, 15370, 293, 1310, 406, 370, 777, 11, 457, 746, 286, 915, 1021, 13, 1171, 729, 567, 500, 380, 458, 50842], "temperature": 0.0, "avg_logprob": -0.27183670467800564, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.09338030219078064}, {"id": 4, "seek": 2820, "start": 37.76, "end": 46.879999999999995, "text": " what MariaDB column store is, it's an all-lib storage engine for MariaDB server. It's columnar", "tokens": [50842, 437, 12734, 27735, 7738, 3531, 307, 11, 309, 311, 364, 439, 12, 38270, 6725, 2848, 337, 12734, 27735, 7154, 13, 467, 311, 7738, 289, 51298], "temperature": 0.0, "avg_logprob": -0.27183670467800564, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.09338030219078064}, {"id": 5, "seek": 2820, "start": 46.879999999999995, "end": 53.0, "text": " oriented. It's MPP and it has fancy two-tier distributed storage. When the data is distributed", "tokens": [51298, 21841, 13, 467, 311, 14146, 47, 293, 309, 575, 10247, 732, 12, 25402, 12631, 6725, 13, 1133, 264, 1412, 307, 12631, 51604], "temperature": 0.0, "avg_logprob": -0.27183670467800564, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.09338030219078064}, {"id": 6, "seek": 5300, "start": 53.0, "end": 58.36, "text": " across nodes and all the nodes have the possibility to distribute it even more using", "tokens": [50364, 2108, 13891, 293, 439, 264, 13891, 362, 264, 7959, 281, 20594, 309, 754, 544, 1228, 50632], "temperature": 0.0, "avg_logprob": -0.2502427101135254, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.047468401491642}, {"id": 7, "seek": 5300, "start": 58.36, "end": 68.96000000000001, "text": " DB routes. Where DB routes are basically the Linux mounts. And I will start with the most", "tokens": [50632, 26754, 18242, 13, 2305, 26754, 18242, 366, 1936, 264, 18734, 40982, 13, 400, 286, 486, 722, 365, 264, 881, 51162], "temperature": 0.0, "avg_logprob": -0.2502427101135254, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.047468401491642}, {"id": 8, "seek": 5300, "start": 68.96000000000001, "end": 76.12, "text": " obscure in my opinion, most obscure topic and that is the MariaDB column store version numbering.", "tokens": [51162, 34443, 294, 452, 4800, 11, 881, 34443, 4829, 293, 300, 307, 264, 12734, 27735, 7738, 3531, 3037, 1230, 278, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2502427101135254, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.047468401491642}, {"id": 9, "seek": 7612, "start": 76.12, "end": 87.32000000000001, "text": " Why it is obscure? Because I still don't understand the reasons that server team takes to put this", "tokens": [50364, 1545, 309, 307, 34443, 30, 1436, 286, 920, 500, 380, 1223, 264, 4112, 300, 7154, 1469, 2516, 281, 829, 341, 50924], "temperature": 0.0, "avg_logprob": -0.1943163637255059, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.018182802945375443}, {"id": 10, "seek": 7612, "start": 87.32000000000001, "end": 93.56, "text": " or another version of column store into the community server. But anyway, first of all,", "tokens": [50924, 420, 1071, 3037, 295, 7738, 3531, 666, 264, 1768, 7154, 13, 583, 4033, 11, 700, 295, 439, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1943163637255059, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.018182802945375443}, {"id": 11, "seek": 7612, "start": 93.56, "end": 99.4, "text": " column store migrated to yet another version in numbering to simplify the understanding what", "tokens": [51236, 7738, 3531, 48329, 281, 1939, 1071, 3037, 294, 1230, 278, 281, 20460, 264, 3701, 437, 51528], "temperature": 0.0, "avg_logprob": -0.1943163637255059, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.018182802945375443}, {"id": 12, "seek": 9940, "start": 99.4, "end": 108.08000000000001, "text": " is the actual column store version in the community server. And that is there are three digits,", "tokens": [50364, 307, 264, 3539, 7738, 3531, 3037, 294, 264, 1768, 7154, 13, 400, 300, 307, 456, 366, 1045, 27011, 11, 50798], "temperature": 0.0, "avg_logprob": -0.2005487320915101, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.11989086121320724}, {"id": 13, "seek": 9940, "start": 108.08000000000001, "end": 114.68, "text": " first goes here, then month and then the page number. And at the bottom side of the slide,", "tokens": [50798, 700, 1709, 510, 11, 550, 1618, 293, 550, 264, 3028, 1230, 13, 400, 412, 264, 2767, 1252, 295, 264, 4137, 11, 51128], "temperature": 0.0, "avg_logprob": -0.2005487320915101, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.11989086121320724}, {"id": 14, "seek": 9940, "start": 114.68, "end": 124.32000000000001, "text": " you can see the actual mapping that shows us which column store versions are published", "tokens": [51128, 291, 393, 536, 264, 3539, 18350, 300, 3110, 505, 597, 7738, 3531, 9606, 366, 6572, 51610], "temperature": 0.0, "avg_logprob": -0.2005487320915101, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.11989086121320724}, {"id": 15, "seek": 12432, "start": 124.32, "end": 132.72, "text": " with the community server. The most notable thing here is the last row that tells when", "tokens": [50364, 365, 264, 1768, 7154, 13, 440, 881, 22556, 551, 510, 307, 264, 1036, 5386, 300, 5112, 562, 50784], "temperature": 0.0, "avg_logprob": -0.17567429049261685, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.0698675736784935}, {"id": 16, "seek": 12432, "start": 132.72, "end": 141.04, "text": " the next table, the most featureful release will be published with the community server.", "tokens": [50784, 264, 958, 3199, 11, 264, 881, 4111, 906, 4374, 486, 312, 6572, 365, 264, 1768, 7154, 13, 51200], "temperature": 0.0, "avg_logprob": -0.17567429049261685, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.0698675736784935}, {"id": 17, "seek": 12432, "start": 141.04, "end": 149.28, "text": " Let's first glance at the features available at the current stable that is shipped with", "tokens": [51200, 961, 311, 700, 21094, 412, 264, 4122, 2435, 412, 264, 2190, 8351, 300, 307, 25312, 365, 51612], "temperature": 0.0, "avg_logprob": -0.17567429049261685, "compression_ratio": 1.6335403726708075, "no_speech_prob": 0.0698675736784935}, {"id": 18, "seek": 14928, "start": 150.04, "end": 159.48, "text": " the community 10.5 and 10.6. Most notable is the filtering vectorization for x86. And guess what?", "tokens": [50402, 264, 1768, 1266, 13, 20, 293, 1266, 13, 21, 13, 4534, 22556, 307, 264, 30822, 8062, 2144, 337, 2031, 22193, 13, 400, 2041, 437, 30, 50874], "temperature": 0.0, "avg_logprob": -0.25862653096516924, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.13584508001804352}, {"id": 19, "seek": 14928, "start": 159.48, "end": 166.8, "text": " Seemed processing is not the ultimate answer to life universe and everything. I mean, the benchmarks,", "tokens": [50874, 1100, 15485, 9007, 307, 406, 264, 9705, 1867, 281, 993, 6445, 293, 1203, 13, 286, 914, 11, 264, 43751, 11, 51240], "temperature": 0.0, "avg_logprob": -0.25862653096516924, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.13584508001804352}, {"id": 20, "seek": 14928, "start": 166.8, "end": 174.6, "text": " when I first did them, when I first run them, they showed me a 10x speedup comparing with", "tokens": [51240, 562, 286, 700, 630, 552, 11, 562, 286, 700, 1190, 552, 11, 436, 4712, 385, 257, 1266, 87, 3073, 1010, 15763, 365, 51630], "temperature": 0.0, "avg_logprob": -0.25862653096516924, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.13584508001804352}, {"id": 21, "seek": 17460, "start": 174.6, "end": 182.64, "text": " the non vectorized code. But when I run the full blown query processing pipeline,", "tokens": [50364, 264, 2107, 8062, 1602, 3089, 13, 583, 562, 286, 1190, 264, 1577, 16479, 14581, 9007, 15517, 11, 50766], "temperature": 0.0, "avg_logprob": -0.2543124568705656, "compression_ratio": 1.2589928057553956, "no_speech_prob": 0.033005937933921814}, {"id": 22, "seek": 17460, "start": 182.64, "end": 194.56, "text": " I just got 30 to 40% of the speedup. So this is a great speedup, but not the ultimate answer.", "tokens": [50766, 286, 445, 658, 2217, 281, 3356, 4, 295, 264, 3073, 1010, 13, 407, 341, 307, 257, 869, 3073, 1010, 11, 457, 406, 264, 9705, 1867, 13, 51362], "temperature": 0.0, "avg_logprob": -0.2543124568705656, "compression_ratio": 1.2589928057553956, "no_speech_prob": 0.033005937933921814}, {"id": 23, "seek": 19456, "start": 194.72, "end": 204.32, "text": " Anyway, the next feature is an external group buy. And that is what it is. It allows a group buy", "tokens": [50372, 5684, 11, 264, 958, 4111, 307, 364, 8320, 1594, 2256, 13, 400, 300, 307, 437, 309, 307, 13, 467, 4045, 257, 1594, 2256, 50852], "temperature": 0.0, "avg_logprob": -0.20517709810439855, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0867481604218483}, {"id": 24, "seek": 19456, "start": 204.32, "end": 211.36, "text": " operation to crunch the data sets, data sets that doesn't fit into memory. And as usually,", "tokens": [50852, 6916, 281, 13386, 264, 1412, 6352, 11, 1412, 6352, 300, 1177, 380, 3318, 666, 4675, 13, 400, 382, 2673, 11, 51204], "temperature": 0.0, "avg_logprob": -0.20517709810439855, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0867481604218483}, {"id": 25, "seek": 19456, "start": 211.36, "end": 218.12, "text": " it consists of two phases. The first phase calculates the partial aggregates. And when they don't fit", "tokens": [51204, 309, 14689, 295, 732, 18764, 13, 440, 700, 5574, 4322, 1024, 264, 14641, 16743, 1024, 13, 400, 562, 436, 500, 380, 3318, 51542], "temperature": 0.0, "avg_logprob": -0.20517709810439855, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0867481604218483}, {"id": 26, "seek": 21812, "start": 218.20000000000002, "end": 225.92000000000002, "text": " into memory, it all flots them and stores on disk. And the second phase sequentially goes over the", "tokens": [50368, 666, 4675, 11, 309, 439, 932, 1971, 552, 293, 9512, 322, 12355, 13, 400, 264, 1150, 5574, 5123, 3137, 1709, 670, 264, 50754], "temperature": 0.0, "avg_logprob": -0.22486610412597657, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.06028982624411583}, {"id": 27, "seek": 21812, "start": 225.92000000000002, "end": 233.76, "text": " stored pre-aggregates and merge them together to produce the resulting data set. As I have seen", "tokens": [50754, 12187, 659, 12, 559, 11027, 1024, 293, 22183, 552, 1214, 281, 5258, 264, 16505, 1412, 992, 13, 1018, 286, 362, 1612, 51146], "temperature": 0.0, "avg_logprob": -0.22486610412597657, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.06028982624411583}, {"id": 28, "seek": 21812, "start": 233.76, "end": 242.84, "text": " in another open source OLAP engines, the second phase takes 2x memory comparing to the first", "tokens": [51146, 294, 1071, 1269, 4009, 39191, 4715, 12982, 11, 264, 1150, 5574, 2516, 568, 87, 4675, 15763, 281, 264, 700, 51600], "temperature": 0.0, "avg_logprob": -0.22486610412597657, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.06028982624411583}, {"id": 29, "seek": 24284, "start": 243.8, "end": 252.16, "text": " phase in the worst case. This feature is disabled by default. And one needs to explicitly enable", "tokens": [50412, 5574, 294, 264, 5855, 1389, 13, 639, 4111, 307, 15191, 538, 7576, 13, 400, 472, 2203, 281, 20803, 9528, 50830], "temperature": 0.0, "avg_logprob": -0.19213666280110678, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04102208465337753}, {"id": 30, "seek": 24284, "start": 252.16, "end": 258.48, "text": " it with the settings in a column store.xml. These settings are shown at the bottom of the slide.", "tokens": [50830, 309, 365, 264, 6257, 294, 257, 7738, 3531, 13, 87, 15480, 13, 1981, 6257, 366, 4898, 412, 264, 2767, 295, 264, 4137, 13, 51146], "temperature": 0.0, "avg_logprob": -0.19213666280110678, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04102208465337753}, {"id": 31, "seek": 24284, "start": 258.48, "end": 270.32, "text": " The next feature is LZ4 compression for data files. We had heard a lot that LZ4 beats snappy in", "tokens": [51146, 440, 958, 4111, 307, 441, 57, 19, 19355, 337, 1412, 7098, 13, 492, 632, 2198, 257, 688, 300, 441, 57, 19, 16447, 14528, 7966, 294, 51738], "temperature": 0.0, "avg_logprob": -0.19213666280110678, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04102208465337753}, {"id": 32, "seek": 27032, "start": 270.36, "end": 278.12, "text": " terms of the compression speed, so we decided to give it a try. And guess what? Snappy still", "tokens": [50366, 2115, 295, 264, 19355, 3073, 11, 370, 321, 3047, 281, 976, 309, 257, 853, 13, 400, 2041, 437, 30, 41539, 7966, 920, 50754], "temperature": 0.0, "avg_logprob": -0.16585323389838724, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.019537992775440216}, {"id": 33, "seek": 27032, "start": 278.12, "end": 287.2, "text": " delivers better compression ratio. It is about 5% better comparing to LZ4 compressed data files.", "tokens": [50754, 24860, 1101, 19355, 8509, 13, 467, 307, 466, 1025, 4, 1101, 15763, 281, 441, 57, 19, 30353, 1412, 7098, 13, 51208], "temperature": 0.0, "avg_logprob": -0.16585323389838724, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.019537992775440216}, {"id": 34, "seek": 27032, "start": 287.2, "end": 294.36, "text": " So there is always a trade-off between space and the compression speed that does the query", "tokens": [51208, 407, 456, 307, 1009, 257, 4923, 12, 4506, 1296, 1901, 293, 264, 19355, 3073, 300, 775, 264, 14581, 51566], "temperature": 0.0, "avg_logprob": -0.16585323389838724, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.019537992775440216}, {"id": 35, "seek": 29436, "start": 294.40000000000003, "end": 303.04, "text": " processing speed. This is also disabled by default in the current stable. And to enable it,", "tokens": [50366, 9007, 3073, 13, 639, 307, 611, 15191, 538, 7576, 294, 264, 2190, 8351, 13, 400, 281, 9528, 309, 11, 50798], "temperature": 0.0, "avg_logprob": -0.19914141297340393, "compression_ratio": 1.56, "no_speech_prob": 0.1124730184674263}, {"id": 36, "seek": 29436, "start": 303.04, "end": 309.68, "text": " you need to set this MariaDB client session variable. So we basically connect to MariaDB", "tokens": [50798, 291, 643, 281, 992, 341, 12734, 27735, 6423, 5481, 7006, 13, 407, 321, 1936, 1745, 281, 12734, 27735, 51130], "temperature": 0.0, "avg_logprob": -0.19914141297340393, "compression_ratio": 1.56, "no_speech_prob": 0.1124730184674263}, {"id": 37, "seek": 29436, "start": 309.68, "end": 316.68, "text": " and set this variable just before you create a table. Or you can set it in the configuration", "tokens": [51130, 293, 992, 341, 7006, 445, 949, 291, 1884, 257, 3199, 13, 1610, 291, 393, 992, 309, 294, 264, 11694, 51480], "temperature": 0.0, "avg_logprob": -0.19914141297340393, "compression_ratio": 1.56, "no_speech_prob": 0.1124730184674263}, {"id": 38, "seek": 31668, "start": 316.68, "end": 326.8, "text": " file of MariaDB server. And we decided to make this a new default since the new stable that is", "tokens": [50364, 3991, 295, 12734, 27735, 7154, 13, 400, 321, 3047, 281, 652, 341, 257, 777, 7576, 1670, 264, 777, 8351, 300, 307, 50870], "temperature": 0.0, "avg_logprob": -0.31785106658935547, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.1582348793745041}, {"id": 39, "seek": 31668, "start": 326.8, "end": 338.2, "text": " coming this April or May. What's the benefit? It means that the compression and decompression", "tokens": [50870, 1348, 341, 6929, 420, 1891, 13, 708, 311, 264, 5121, 30, 467, 1355, 300, 264, 19355, 293, 22867, 2775, 51440], "temperature": 0.0, "avg_logprob": -0.31785106658935547, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.1582348793745041}, {"id": 40, "seek": 33820, "start": 339.2, "end": 347.36, "text": " ratio are very fine. Good point. It might be obscure. The compression ratio for snappy is still", "tokens": [50414, 8509, 366, 588, 2489, 13, 2205, 935, 13, 467, 1062, 312, 34443, 13, 440, 19355, 8509, 337, 14528, 7966, 307, 920, 50822], "temperature": 0.0, "avg_logprob": -0.255829717071963, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.07880309224128723}, {"id": 41, "seek": 33820, "start": 347.36, "end": 356.88, "text": " better, but the decompression speed is faster for LZ4. Not really, because ingestion speed", "tokens": [50822, 1101, 11, 457, 264, 22867, 2775, 3073, 307, 4663, 337, 441, 57, 19, 13, 1726, 534, 11, 570, 3957, 31342, 3073, 51298], "temperature": 0.0, "avg_logprob": -0.255829717071963, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.07880309224128723}, {"id": 42, "seek": 33820, "start": 356.88, "end": 362.48, "text": " is not the big question. I mean, everybody wants to ingest their data as fast as possible,", "tokens": [51298, 307, 406, 264, 955, 1168, 13, 286, 914, 11, 2201, 2738, 281, 3957, 377, 641, 1412, 382, 2370, 382, 1944, 11, 51578], "temperature": 0.0, "avg_logprob": -0.255829717071963, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.07880309224128723}, {"id": 43, "seek": 36248, "start": 362.48, "end": 369.40000000000003, "text": " but let's be honest, we are interested in the select, not in certs in the first place.", "tokens": [50364, 457, 718, 311, 312, 3245, 11, 321, 366, 3102, 294, 264, 3048, 11, 406, 294, 5351, 82, 294, 264, 700, 1081, 13, 50710], "temperature": 0.0, "avg_logprob": -0.3111420411330003, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.07226672023534775}, {"id": 44, "seek": 36248, "start": 369.40000000000003, "end": 384.88, "text": " Yeah, that is true for LTP. Okay, and now we come to the new upcoming stable over column store.", "tokens": [50710, 865, 11, 300, 307, 2074, 337, 441, 16804, 13, 1033, 11, 293, 586, 321, 808, 281, 264, 777, 11500, 8351, 670, 7738, 3531, 13, 51484], "temperature": 0.0, "avg_logprob": -0.3111420411330003, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.07226672023534775}, {"id": 45, "seek": 38488, "start": 385.8, "end": 393.24, "text": " And there are some features that I will not discuss in details, but there are some interesting ones", "tokens": [50410, 400, 456, 366, 512, 4122, 300, 286, 486, 406, 2248, 294, 4365, 11, 457, 456, 366, 512, 1880, 2306, 50782], "temperature": 0.0, "avg_logprob": -0.268683522939682, "compression_ratio": 1.5, "no_speech_prob": 0.41868072748184204}, {"id": 46, "seek": 38488, "start": 393.24, "end": 403.08, "text": " that I will look into later. So let's get quick answer these ones. We are officially", "tokens": [50782, 300, 286, 486, 574, 666, 1780, 13, 407, 718, 311, 483, 1702, 1867, 613, 2306, 13, 492, 366, 12053, 51274], "temperature": 0.0, "avg_logprob": -0.268683522939682, "compression_ratio": 1.5, "no_speech_prob": 0.41868072748184204}, {"id": 47, "seek": 38488, "start": 403.08, "end": 414.36, "text": " support ARM64 platform with vectorization as well using Neon. So this platform builds produces", "tokens": [51274, 1406, 45209, 19395, 3663, 365, 8062, 2144, 382, 731, 1228, 1734, 266, 13, 407, 341, 3663, 15182, 14725, 51838], "temperature": 0.0, "avg_logprob": -0.268683522939682, "compression_ratio": 1.5, "no_speech_prob": 0.41868072748184204}, {"id": 48, "seek": 41436, "start": 414.36, "end": 425.0, "text": " slightly faster runtime comparing to our normal nightly CI routines. But I have seen that it", "tokens": [50364, 4748, 4663, 34474, 15763, 281, 527, 2710, 1818, 356, 37777, 33827, 13, 583, 286, 362, 1612, 300, 309, 50896], "temperature": 0.0, "avg_logprob": -0.3448052073633948, "compression_ratio": 1.2411347517730495, "no_speech_prob": 0.035931430757045746}, {"id": 49, "seek": 41436, "start": 425.0, "end": 433.0, "text": " consumes 5% more over RAM, running the DML heavy workload like updates or deletes.", "tokens": [50896, 48823, 1025, 4, 544, 670, 14561, 11, 2614, 264, 413, 12683, 4676, 20139, 411, 9205, 420, 1103, 37996, 13, 51296], "temperature": 0.0, "avg_logprob": -0.3448052073633948, "compression_ratio": 1.2411347517730495, "no_speech_prob": 0.035931430757045746}, {"id": 50, "seek": 43300, "start": 433.0, "end": 445.0, "text": " The next tool is called MCS Rebuild AM. And to explain why we decide to write this", "tokens": [50364, 440, 958, 2290, 307, 1219, 8797, 50, 1300, 11516, 6475, 13, 400, 281, 2903, 983, 321, 4536, 281, 2464, 341, 50964], "temperature": 0.0, "avg_logprob": -0.2218764168875558, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.051252253353595734}, {"id": 51, "seek": 43300, "start": 445.0, "end": 454.2, "text": " tool, I need to make a small D tool. So how does column store the data? There are two parts,", "tokens": [50964, 2290, 11, 286, 643, 281, 652, 257, 1359, 413, 2290, 13, 407, 577, 775, 7738, 3531, 264, 1412, 30, 821, 366, 732, 3166, 11, 51424], "temperature": 0.0, "avg_logprob": -0.2218764168875558, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.051252253353595734}, {"id": 52, "seek": 43300, "start": 454.2, "end": 459.52, "text": " the data itself and the metadata that describes where the data is. Like when you have multiple", "tokens": [51424, 264, 1412, 2564, 293, 264, 26603, 300, 15626, 689, 264, 1412, 307, 13, 1743, 562, 291, 362, 3866, 51690], "temperature": 0.0, "avg_logprob": -0.2218764168875558, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.051252253353595734}, {"id": 53, "seek": 45952, "start": 459.52, "end": 466.56, "text": " nodes, you need to tell the query coordinator where to go for the data files. So the metadata", "tokens": [50364, 13891, 11, 291, 643, 281, 980, 264, 14581, 27394, 689, 281, 352, 337, 264, 1412, 7098, 13, 407, 264, 26603, 50716], "temperature": 0.0, "avg_logprob": -0.2119869589805603, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.020614933222532272}, {"id": 54, "seek": 45952, "start": 466.56, "end": 476.91999999999996, "text": " describes the location of data and column store tries its best to preserve this metadata in", "tokens": [50716, 15626, 264, 4914, 295, 1412, 293, 7738, 3531, 9898, 1080, 1151, 281, 15665, 341, 26603, 294, 51234], "temperature": 0.0, "avg_logprob": -0.2119869589805603, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.020614933222532272}, {"id": 55, "seek": 45952, "start": 476.91999999999996, "end": 483.76, "text": " tact and it has multiple copies of meta. So if one meta is corrupted for some reason, because", "tokens": [51234, 9959, 293, 309, 575, 3866, 14341, 295, 19616, 13, 407, 498, 472, 19616, 307, 39480, 337, 512, 1778, 11, 570, 51576], "temperature": 0.0, "avg_logprob": -0.2119869589805603, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.020614933222532272}, {"id": 56, "seek": 48376, "start": 484.32, "end": 494.56, "text": " life is not ideal, there are multiple copies that you can use as a backup to restore. However,", "tokens": [50392, 993, 307, 406, 7157, 11, 456, 366, 3866, 14341, 300, 291, 393, 764, 382, 257, 14807, 281, 15227, 13, 2908, 11, 50904], "temperature": 0.0, "avg_logprob": -0.16612471171787807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.25781482458114624}, {"id": 57, "seek": 48376, "start": 494.56, "end": 502.24, "text": " if one loses its meta, there is no way to access the data itself properly, even if the data files", "tokens": [50904, 498, 472, 18293, 1080, 19616, 11, 456, 307, 572, 636, 281, 2105, 264, 1412, 2564, 6108, 11, 754, 498, 264, 1412, 7098, 51288], "temperature": 0.0, "avg_logprob": -0.16612471171787807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.25781482458114624}, {"id": 58, "seek": 48376, "start": 502.24, "end": 510.36, "text": " are intact. So this tool allows one to produce the metadata from the data files. The key", "tokens": [51288, 366, 23493, 13, 407, 341, 2290, 4045, 472, 281, 5258, 264, 26603, 490, 264, 1412, 7098, 13, 440, 2141, 51694], "temperature": 0.0, "avg_logprob": -0.16612471171787807, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.25781482458114624}, {"id": 59, "seek": 51036, "start": 510.36, "end": 520.44, "text": " way is you have to create all the tables starting with column store older than 6.4.4 to allow this", "tokens": [50364, 636, 307, 291, 362, 281, 1884, 439, 264, 8020, 2891, 365, 7738, 3531, 4906, 813, 1386, 13, 19, 13, 19, 281, 2089, 341, 50868], "temperature": 0.0, "avg_logprob": -0.19729078485724633, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.1926182061433792}, {"id": 60, "seek": 51036, "start": 520.44, "end": 527.5600000000001, "text": " feature to work. The next feature is distributed JSON function support. Technically speaking,", "tokens": [50868, 4111, 281, 589, 13, 440, 958, 4111, 307, 12631, 31828, 2445, 1406, 13, 42494, 4124, 11, 51224], "temperature": 0.0, "avg_logprob": -0.19729078485724633, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.1926182061433792}, {"id": 61, "seek": 51036, "start": 528.12, "end": 532.76, "text": " there were JSON functions in the column store from the very beginning, but unfortunately,", "tokens": [51252, 456, 645, 31828, 6828, 294, 264, 7738, 3531, 490, 264, 588, 2863, 11, 457, 7015, 11, 51484], "temperature": 0.0, "avg_logprob": -0.19729078485724633, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.1926182061433792}, {"id": 62, "seek": 51036, "start": 532.76, "end": 538.44, "text": " to use them, you need to fall back to a very slow mode, we call it table mode. That is basically", "tokens": [51484, 281, 764, 552, 11, 291, 643, 281, 2100, 646, 281, 257, 588, 2964, 4391, 11, 321, 818, 309, 3199, 4391, 13, 663, 307, 1936, 51768], "temperature": 0.0, "avg_logprob": -0.19729078485724633, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.1926182061433792}, {"id": 63, "seek": 53844, "start": 539.24, "end": 550.5200000000001, "text": " MariaDB crunching the data, but it asks for a full table scan from a column store. So it's not very", "tokens": [50404, 12734, 27735, 13386, 278, 264, 1412, 11, 457, 309, 8962, 337, 257, 1577, 3199, 11049, 490, 257, 7738, 3531, 13, 407, 309, 311, 406, 588, 50968], "temperature": 0.0, "avg_logprob": -0.18090967031625602, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.03470894694328308}, {"id": 64, "seek": 53844, "start": 550.5200000000001, "end": 560.0400000000001, "text": " scalable, not fast. So we implemented this in terms of last year JSON. And now you can use", "tokens": [50968, 38481, 11, 406, 2370, 13, 407, 321, 12270, 341, 294, 2115, 295, 1036, 1064, 31828, 13, 400, 586, 291, 393, 764, 51444], "temperature": 0.0, "avg_logprob": -0.18090967031625602, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.03470894694328308}, {"id": 65, "seek": 56004, "start": 560.28, "end": 569.8, "text": " all but one functions. And these are JSON object egg and the JSON table. The last one is very,", "tokens": [50376, 439, 457, 472, 6828, 13, 400, 613, 366, 31828, 2657, 3777, 293, 264, 31828, 3199, 13, 440, 1036, 472, 307, 588, 11, 50852], "temperature": 0.0, "avg_logprob": -0.21833324432373047, "compression_ratio": 1.5125, "no_speech_prob": 0.1142139732837677}, {"id": 66, "seek": 56004, "start": 570.52, "end": 575.7199999999999, "text": " how to say, MariaDB specific. It produces the relation.", "tokens": [50888, 577, 281, 584, 11, 12734, 27735, 2685, 13, 467, 14725, 264, 9721, 13, 51148], "temperature": 0.0, "avg_logprob": -0.21833324432373047, "compression_ratio": 1.5125, "no_speech_prob": 0.1142139732837677}, {"id": 67, "seek": 56004, "start": 579.7199999999999, "end": 588.1999999999999, "text": " That is also true. But I mean, by specific, I mean, we cannot produce the MariaDB relation,", "tokens": [51348, 663, 307, 611, 2074, 13, 583, 286, 914, 11, 538, 2685, 11, 286, 914, 11, 321, 2644, 5258, 264, 12734, 27735, 9721, 11, 51772], "temperature": 0.0, "avg_logprob": -0.21833324432373047, "compression_ratio": 1.5125, "no_speech_prob": 0.1142139732837677}, {"id": 68, "seek": 58820, "start": 588.2, "end": 595.88, "text": " to be honest. So we decided to postpone it because nobody is very interested in this one.", "tokens": [50364, 281, 312, 3245, 13, 407, 321, 3047, 281, 28973, 546, 309, 570, 5079, 307, 588, 3102, 294, 341, 472, 13, 50748], "temperature": 0.0, "avg_logprob": -0.17791358436026225, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.02277342975139618}, {"id": 69, "seek": 58820, "start": 597.4000000000001, "end": 606.12, "text": " But we implemented. Good suggestion. Anybody is interested and maybe anybody knows what", "tokens": [50824, 583, 321, 12270, 13, 2205, 16541, 13, 19082, 307, 3102, 293, 1310, 4472, 3255, 437, 51260], "temperature": 0.0, "avg_logprob": -0.17791358436026225, "compression_ratio": 1.372093023255814, "no_speech_prob": 0.02277342975139618}, {"id": 70, "seek": 60612, "start": 606.12, "end": 619.5600000000001, "text": " JSON table does? Okay, that's a good statistic. So now, and here are features that I want to", "tokens": [50364, 31828, 3199, 775, 30, 1033, 11, 300, 311, 257, 665, 29588, 13, 407, 586, 11, 293, 510, 366, 4122, 300, 286, 528, 281, 51036], "temperature": 0.0, "avg_logprob": -0.19352071935480292, "compression_ratio": 1.3701923076923077, "no_speech_prob": 0.06979230046272278}, {"id": 71, "seek": 60612, "start": 619.5600000000001, "end": 628.12, "text": " discuss in some more details. First is auxiliary column. What it does, it basically speds up the", "tokens": [51036, 2248, 294, 512, 544, 4365, 13, 2386, 307, 43741, 7738, 13, 708, 309, 775, 11, 309, 1936, 637, 5147, 493, 264, 51464], "temperature": 0.0, "avg_logprob": -0.19352071935480292, "compression_ratio": 1.3701923076923077, "no_speech_prob": 0.06979230046272278}, {"id": 72, "seek": 60612, "start": 628.12, "end": 635.88, "text": " leads from 3x up to 50x, depending on a SQL schema. And what does it mean? The more columns you", "tokens": [51464, 6689, 490, 805, 87, 493, 281, 2625, 87, 11, 5413, 322, 257, 19200, 34078, 13, 400, 437, 775, 309, 914, 30, 440, 544, 13766, 291, 51852], "temperature": 0.0, "avg_logprob": -0.19352071935480292, "compression_ratio": 1.3701923076923077, "no_speech_prob": 0.06979230046272278}, {"id": 73, "seek": 63588, "start": 635.88, "end": 642.28, "text": " have in the table, the faster the speed up will be. It is disabled. It has an additional", "tokens": [50364, 362, 294, 264, 3199, 11, 264, 4663, 264, 3073, 493, 486, 312, 13, 467, 307, 15191, 13, 467, 575, 364, 4497, 50684], "temperature": 0.0, "avg_logprob": -0.12438287942305855, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.014354397542774677}, {"id": 74, "seek": 63588, "start": 642.28, "end": 647.56, "text": " sped up configuration option. I will explain it a bit later. But let's take a look at the bottom", "tokens": [50684, 637, 292, 493, 11694, 3614, 13, 286, 486, 2903, 309, 257, 857, 1780, 13, 583, 718, 311, 747, 257, 574, 412, 264, 2767, 50948], "temperature": 0.0, "avg_logprob": -0.12438287942305855, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.014354397542774677}, {"id": 75, "seek": 63588, "start": 647.56, "end": 657.32, "text": " half of a slide. This is a very simplified data files layout to explain how delete operation", "tokens": [50948, 1922, 295, 257, 4137, 13, 639, 307, 257, 588, 26335, 1412, 7098, 13333, 281, 2903, 577, 12097, 6916, 51436], "temperature": 0.0, "avg_logprob": -0.12438287942305855, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.014354397542774677}, {"id": 76, "seek": 63588, "start": 657.32, "end": 662.12, "text": " works previously before the patch on the left and after the patch on the right. So let's concentrate", "tokens": [51436, 1985, 8046, 949, 264, 9972, 322, 264, 1411, 293, 934, 264, 9972, 322, 264, 558, 13, 407, 718, 311, 18089, 51676], "temperature": 0.0, "avg_logprob": -0.12438287942305855, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.014354397542774677}, {"id": 77, "seek": 66212, "start": 662.12, "end": 670.12, "text": " on the left part. You can see the violet and blue empties. So when delete comes into the table,", "tokens": [50364, 322, 264, 1411, 644, 13, 509, 393, 536, 264, 46480, 293, 3344, 6113, 530, 13, 407, 562, 12097, 1487, 666, 264, 3199, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1347639560699463, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.06522160768508911}, {"id": 78, "seek": 66212, "start": 670.12, "end": 676.44, "text": " it replaces the actual values in the columns, because we are columnar, with the special magic", "tokens": [50764, 309, 46734, 264, 3539, 4190, 294, 264, 13766, 11, 570, 321, 366, 7738, 289, 11, 365, 264, 2121, 5585, 51080], "temperature": 0.0, "avg_logprob": -0.1347639560699463, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.06522160768508911}, {"id": 79, "seek": 66212, "start": 676.44, "end": 684.76, "text": " values that are called empty, that are specific for a data type. And it does it in place. And", "tokens": [51080, 4190, 300, 366, 1219, 6707, 11, 300, 366, 2685, 337, 257, 1412, 2010, 13, 400, 309, 775, 309, 294, 1081, 13, 400, 51496], "temperature": 0.0, "avg_logprob": -0.1347639560699463, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.06522160768508911}, {"id": 80, "seek": 68476, "start": 684.76, "end": 692.36, "text": " moreover, the delete operation has to store the actual block for a version before it changes", "tokens": [50364, 544, 3570, 11, 264, 12097, 6916, 575, 281, 3531, 264, 3539, 3461, 337, 257, 3037, 949, 309, 2962, 50744], "temperature": 0.0, "avg_logprob": -0.09103129931858607, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.08047343045473099}, {"id": 81, "seek": 68476, "start": 692.36, "end": 701.96, "text": " the data in place. So there are two block copies to do, and there are four in place changes that", "tokens": [50744, 264, 1412, 294, 1081, 13, 407, 456, 366, 732, 3461, 14341, 281, 360, 11, 293, 456, 366, 1451, 294, 1081, 2962, 300, 51224], "temperature": 0.0, "avg_logprob": -0.09103129931858607, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.08047343045473099}, {"id": 82, "seek": 68476, "start": 701.96, "end": 711.0, "text": " has to be flushed. So what does the auxiliary column changes in this pattern? First, you can see", "tokens": [51224, 575, 281, 312, 19568, 292, 13, 407, 437, 775, 264, 43741, 7738, 2962, 294, 341, 5102, 30, 2386, 11, 291, 393, 536, 51676], "temperature": 0.0, "avg_logprob": -0.09103129931858607, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.08047343045473099}, {"id": 83, "seek": 71100, "start": 711.0, "end": 717.4, "text": " on the right side, there is an additional column. There is basically a flag. So delete operation", "tokens": [50364, 322, 264, 558, 1252, 11, 456, 307, 364, 4497, 7738, 13, 821, 307, 1936, 257, 7166, 13, 407, 12097, 6916, 50684], "temperature": 0.0, "avg_logprob": -0.08602614255295586, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.05045283958315849}, {"id": 84, "seek": 71100, "start": 717.4, "end": 724.04, "text": " now goes over this auxiliary column and changes the flag over there. And this auxiliary column is", "tokens": [50684, 586, 1709, 670, 341, 43741, 7738, 293, 2962, 264, 7166, 670, 456, 13, 400, 341, 43741, 7738, 307, 51016], "temperature": 0.0, "avg_logprob": -0.08602614255295586, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.05045283958315849}, {"id": 85, "seek": 71100, "start": 724.04, "end": 730.92, "text": " only one byte. So you need to store only one block of data, and you don't need to change to do in", "tokens": [51016, 787, 472, 40846, 13, 407, 291, 643, 281, 3531, 787, 472, 3461, 295, 1412, 11, 293, 291, 500, 380, 643, 281, 1319, 281, 360, 294, 51360], "temperature": 0.0, "avg_logprob": -0.08602614255295586, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.05045283958315849}, {"id": 86, "seek": 71100, "start": 730.92, "end": 738.44, "text": " place changes in all the columns. That's where the sped up comes from. As I said, there is an", "tokens": [51360, 1081, 2962, 294, 439, 264, 13766, 13, 663, 311, 689, 264, 637, 292, 493, 1487, 490, 13, 1018, 286, 848, 11, 456, 307, 364, 51736], "temperature": 0.0, "avg_logprob": -0.08602614255295586, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.05045283958315849}, {"id": 87, "seek": 73844, "start": 738.44, "end": 744.6800000000001, "text": " additional, there is an additional opportunity to sped up it even more, and this is to enable", "tokens": [50364, 4497, 11, 456, 307, 364, 4497, 2650, 281, 637, 292, 493, 309, 754, 544, 11, 293, 341, 307, 281, 9528, 50676], "temperature": 0.0, "avg_logprob": -0.2586009117864793, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.03339758515357971}, {"id": 88, "seek": 73844, "start": 744.6800000000001, "end": 752.0400000000001, "text": " a fast delete. I will not discuss it in details, but internally, it doesn't update the metadata", "tokens": [50676, 257, 2370, 12097, 13, 286, 486, 406, 2248, 309, 294, 4365, 11, 457, 19501, 11, 309, 1177, 380, 5623, 264, 26603, 51044], "temperature": 0.0, "avg_logprob": -0.2586009117864793, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.03339758515357971}, {"id": 89, "seek": 73844, "start": 752.0400000000001, "end": 755.8000000000001, "text": " file, so it makes it even faster to delete the operator.", "tokens": [51044, 3991, 11, 370, 309, 1669, 309, 754, 4663, 281, 12097, 264, 12973, 13, 51232], "temperature": 0.0, "avg_logprob": -0.2586009117864793, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.03339758515357971}, {"id": 90, "seek": 75580, "start": 756.3599999999999, "end": 762.12, "text": " Basically, you have a new column indicating if it's deleted or not, and you just update it.", "tokens": [50392, 8537, 11, 291, 362, 257, 777, 7738, 25604, 498, 309, 311, 22981, 420, 406, 11, 293, 291, 445, 5623, 309, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16242083565133517, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.09083142131567001}, {"id": 91, "seek": 75580, "start": 762.12, "end": 770.28, "text": " Yes, that's true. We have plans to use this even more, this auxiliary column,", "tokens": [50680, 1079, 11, 300, 311, 2074, 13, 492, 362, 5482, 281, 764, 341, 754, 544, 11, 341, 43741, 7738, 11, 51088], "temperature": 0.0, "avg_logprob": -0.16242083565133517, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.09083142131567001}, {"id": 92, "seek": 75580, "start": 772.8399999999999, "end": 776.1999999999999, "text": " but I will discuss it later when we finally implement these features.", "tokens": [51216, 457, 286, 486, 2248, 309, 1780, 562, 321, 2721, 4445, 613, 4122, 13, 51384], "temperature": 0.0, "avg_logprob": -0.16242083565133517, "compression_ratio": 1.4058823529411764, "no_speech_prob": 0.09083142131567001}, {"id": 93, "seek": 77620, "start": 776.5200000000001, "end": 787.72, "text": " And to be honest, this fast delete gives an opportunity to implement the append-only", "tokens": [50380, 400, 281, 312, 3245, 11, 341, 2370, 12097, 2709, 364, 2650, 281, 4445, 264, 34116, 12, 25202, 50940], "temperature": 0.0, "avg_logprob": -0.18910113247958096, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03497875854372978}, {"id": 94, "seek": 77620, "start": 789.24, "end": 796.12, "text": " update. That should be also a very fast boost for update operation, but stay tuned, not today.", "tokens": [51016, 5623, 13, 663, 820, 312, 611, 257, 588, 2370, 9194, 337, 5623, 6916, 11, 457, 1754, 10870, 11, 406, 965, 13, 51360], "temperature": 0.0, "avg_logprob": -0.18910113247958096, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03497875854372978}, {"id": 95, "seek": 77620, "start": 797.4000000000001, "end": 803.72, "text": " Next feature is extent map scalability improvement, and what extent map is. If you recall, couple", "tokens": [51424, 3087, 4111, 307, 8396, 4471, 15664, 2310, 10444, 11, 293, 437, 8396, 4471, 307, 13, 759, 291, 9901, 11, 1916, 51740], "temperature": 0.0, "avg_logprob": -0.18910113247958096, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.03497875854372978}, {"id": 96, "seek": 80372, "start": 803.72, "end": 809.5600000000001, "text": " slides before, I already mentioned the metadata that describes where the data is, and extent map", "tokens": [50364, 9788, 949, 11, 286, 1217, 2835, 264, 26603, 300, 15626, 689, 264, 1412, 307, 11, 293, 8396, 4471, 50656], "temperature": 0.0, "avg_logprob": -0.1340622901916504, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.05106884241104126}, {"id": 97, "seek": 80372, "start": 809.5600000000001, "end": 817.32, "text": " is the core of this metadata. It's basically a structure and memory structure that allows to map", "tokens": [50656, 307, 264, 4965, 295, 341, 26603, 13, 467, 311, 1936, 257, 3877, 293, 4675, 3877, 300, 4045, 281, 4471, 51044], "temperature": 0.0, "avg_logprob": -0.1340622901916504, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.05106884241104126}, {"id": 98, "seek": 80372, "start": 817.32, "end": 825.64, "text": " from a globally unique block number. Block is a minimally possible data unit in a cluster,", "tokens": [51044, 490, 257, 18958, 3845, 3461, 1230, 13, 17500, 307, 257, 4464, 379, 1944, 1412, 4985, 294, 257, 13630, 11, 51460], "temperature": 0.0, "avg_logprob": -0.1340622901916504, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.05106884241104126}, {"id": 99, "seek": 82564, "start": 826.36, "end": 834.04, "text": " and the extent map allows to map from this block number to a tuple of OID, not partition segment,", "tokens": [50400, 293, 264, 8396, 4471, 4045, 281, 4471, 490, 341, 3461, 1230, 281, 257, 2604, 781, 295, 422, 2777, 11, 406, 24808, 9469, 11, 50784], "temperature": 0.0, "avg_logprob": -0.17921079529656303, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0502353198826313}, {"id": 100, "seek": 82564, "start": 834.04, "end": 841.24, "text": " and vice versa, from the tuple to a globally unique block number. And these operations are used", "tokens": [50784, 293, 11964, 25650, 11, 490, 264, 2604, 781, 281, 257, 18958, 3845, 3461, 1230, 13, 400, 613, 7705, 366, 1143, 51144], "temperature": 0.0, "avg_logprob": -0.17921079529656303, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0502353198826313}, {"id": 101, "seek": 82564, "start": 841.24, "end": 847.72, "text": " extensively in the cluster because you always want to know where the data is and what is the OID that", "tokens": [51144, 32636, 294, 264, 13630, 570, 291, 1009, 528, 281, 458, 689, 264, 1412, 307, 293, 437, 307, 264, 422, 2777, 300, 51468], "temperature": 0.0, "avg_logprob": -0.17921079529656303, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0502353198826313}, {"id": 102, "seek": 84772, "start": 847.72, "end": 855.64, "text": " this block belongs to. Originally, the extent map was an array basically with the OIN lookup", "tokens": [50364, 341, 3461, 12953, 281, 13, 28696, 11, 264, 8396, 4471, 390, 364, 10225, 1936, 365, 264, 422, 1464, 574, 1010, 50760], "temperature": 0.0, "avg_logprob": -0.1993901570638021, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0848616436123848}, {"id": 103, "seek": 84772, "start": 856.2, "end": 864.28, "text": " complexity, and this array was replaced with, oh, I need to mention why it is a problem. Imagine the", "tokens": [50788, 14024, 11, 293, 341, 10225, 390, 10772, 365, 11, 1954, 11, 286, 643, 281, 2152, 983, 309, 307, 257, 1154, 13, 11739, 264, 51192], "temperature": 0.0, "avg_logprob": -0.1993901570638021, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0848616436123848}, {"id": 104, "seek": 84772, "start": 864.28, "end": 875.5600000000001, "text": " extent map that is 200 max. So, going over 200 max, and one entry entry is only 100 bytes only,", "tokens": [51192, 8396, 4471, 300, 307, 2331, 11469, 13, 407, 11, 516, 670, 2331, 11469, 11, 293, 472, 8729, 8729, 307, 787, 2319, 36088, 787, 11, 51756], "temperature": 0.0, "avg_logprob": -0.1993901570638021, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0848616436123848}, {"id": 105, "seek": 87556, "start": 875.56, "end": 885.4, "text": " so imagine how many entries are in these 200 max. It takes a lot to look up in such an enormous array.", "tokens": [50364, 370, 3811, 577, 867, 23041, 366, 294, 613, 2331, 11469, 13, 467, 2516, 257, 688, 281, 574, 493, 294, 1270, 364, 11322, 10225, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1749916849909602, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02355279214680195}, {"id": 106, "seek": 87556, "start": 886.1999999999999, "end": 893.0799999999999, "text": " An array, this array was replaced with a red-black tree. This makes the block-to-tuple mapping", "tokens": [50896, 1107, 10225, 11, 341, 10225, 390, 10772, 365, 257, 2182, 12, 36141, 4230, 13, 639, 1669, 264, 3461, 12, 1353, 12, 9179, 781, 18350, 51240], "temperature": 0.0, "avg_logprob": -0.1749916849909602, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02355279214680195}, {"id": 107, "seek": 87556, "start": 894.28, "end": 899.56, "text": " to, this changes the complexity of block-to-tuple mapping to log n,", "tokens": [51300, 281, 11, 341, 2962, 264, 14024, 295, 3461, 12, 1353, 12, 9179, 781, 18350, 281, 3565, 297, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1749916849909602, "compression_ratio": 1.5773809523809523, "no_speech_prob": 0.02355279214680195}, {"id": 108, "seek": 89956, "start": 900.52, "end": 911.4799999999999, "text": " or log n. And to facilitate another conversion, another mapping from tuple to block,", "tokens": [50412, 420, 3565, 297, 13, 400, 281, 20207, 1071, 14298, 11, 1071, 18350, 490, 2604, 781, 281, 3461, 11, 50960], "temperature": 0.0, "avg_logprob": -0.21973782493954613, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.0621439591050148}, {"id": 109, "seek": 89956, "start": 911.4799999999999, "end": 918.3599999999999, "text": " we implemented the extent map index. There is basically a burger of couple hash maps on top", "tokens": [50960, 321, 12270, 264, 8396, 4471, 8186, 13, 821, 307, 1936, 257, 16393, 295, 1916, 22019, 11317, 322, 1192, 51304], "temperature": 0.0, "avg_logprob": -0.21973782493954613, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.0621439591050148}, {"id": 110, "seek": 91836, "start": 918.36, "end": 930.92, "text": " and a very tiny arrays at the bottom. So, this gives us the mapping operation complexity of OC.", "tokens": [50364, 293, 257, 588, 5870, 41011, 412, 264, 2767, 13, 407, 11, 341, 2709, 505, 264, 18350, 6916, 14024, 295, 42278, 13, 50992], "temperature": 0.0, "avg_logprob": -0.23140975407191686, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.05097530037164688}, {"id": 111, "seek": 91836, "start": 932.6, "end": 942.28, "text": " And here is at the bottom, you will see the results. These are except of the CP import logs.", "tokens": [51076, 400, 510, 307, 412, 264, 2767, 11, 291, 486, 536, 264, 3542, 13, 1981, 366, 3993, 295, 264, 22431, 974, 20820, 13, 51560], "temperature": 0.0, "avg_logprob": -0.23140975407191686, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.05097530037164688}, {"id": 112, "seek": 94228, "start": 942.28, "end": 949.0, "text": " One is, one demonstrates that the preprocessed step takes roughly 30 seconds, and after the", "tokens": [50364, 1485, 307, 11, 472, 31034, 300, 264, 2666, 340, 780, 292, 1823, 2516, 9810, 2217, 3949, 11, 293, 934, 264, 50700], "temperature": 0.0, "avg_logprob": -0.25456661067596853, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.0394304133951664}, {"id": 113, "seek": 94228, "start": 949.0, "end": 957.4, "text": " patch, you see that it decreases to four seconds. It was originally. So, if you have a huge extent", "tokens": [50700, 9972, 11, 291, 536, 300, 309, 24108, 281, 1451, 3949, 13, 467, 390, 7993, 13, 407, 11, 498, 291, 362, 257, 2603, 8396, 51120], "temperature": 0.0, "avg_logprob": -0.25456661067596853, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.0394304133951664}, {"id": 114, "seek": 94228, "start": 957.4, "end": 967.3199999999999, "text": " map, it will give even faster operations. The next feature is a primproc and ex-manager", "tokens": [51120, 4471, 11, 309, 486, 976, 754, 4663, 7705, 13, 440, 958, 4111, 307, 257, 2886, 4318, 66, 293, 454, 12, 1601, 3557, 51616], "temperature": 0.0, "avg_logprob": -0.25456661067596853, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.0394304133951664}, {"id": 115, "seek": 96732, "start": 967.6400000000001, "end": 973.24, "text": " processes merger. If anybody here uses comestory, he knows there are a bunch of processes.", "tokens": [50380, 7555, 48002, 13, 759, 4472, 510, 4960, 395, 377, 827, 11, 415, 3255, 456, 366, 257, 3840, 295, 7555, 13, 50660], "temperature": 0.0, "avg_logprob": -0.19002703184722572, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.08561275154352188}, {"id": 116, "seek": 96732, "start": 973.24, "end": 978.2, "text": " And the central ones are primproc and ex-manager, where ex-manager is a coordinator for the query", "tokens": [50660, 400, 264, 5777, 2306, 366, 2886, 4318, 66, 293, 454, 12, 1601, 3557, 11, 689, 454, 12, 1601, 3557, 307, 257, 27394, 337, 264, 14581, 50908], "temperature": 0.0, "avg_logprob": -0.19002703184722572, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.08561275154352188}, {"id": 117, "seek": 96732, "start": 978.2, "end": 989.0, "text": " processing. And primproc is an actual worker. So, there were a lot of additional headache when", "tokens": [50908, 9007, 13, 400, 2886, 4318, 66, 307, 364, 3539, 11346, 13, 407, 11, 456, 645, 257, 688, 295, 4497, 23520, 562, 51448], "temperature": 0.0, "avg_logprob": -0.19002703184722572, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.08561275154352188}, {"id": 118, "seek": 96732, "start": 989.0, "end": 996.84, "text": " the local processes must communicate between each other over the same, in the same node.", "tokens": [51448, 264, 2654, 7555, 1633, 7890, 1296, 1184, 661, 670, 264, 912, 11, 294, 264, 912, 9984, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19002703184722572, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.08561275154352188}, {"id": 119, "seek": 99684, "start": 996.84, "end": 1003.08, "text": " So, same node communication goes over loopback. And this traffic is not compressed, comparing to", "tokens": [50364, 407, 11, 912, 9984, 6101, 1709, 670, 6367, 3207, 13, 400, 341, 6419, 307, 406, 30353, 11, 15763, 281, 50676], "temperature": 0.0, "avg_logprob": -0.1845568072411322, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.035155266523361206}, {"id": 120, "seek": 99684, "start": 1003.08, "end": 1010.84, "text": " the different nodes communication that is compressed. So, combining these two run times,", "tokens": [50676, 264, 819, 13891, 6101, 300, 307, 30353, 13, 407, 11, 21928, 613, 732, 1190, 1413, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1845568072411322, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.035155266523361206}, {"id": 121, "seek": 99684, "start": 1010.84, "end": 1017.64, "text": " we get the four to seven percent overall sped up. And this gives us another opportunities for", "tokens": [51064, 321, 483, 264, 1451, 281, 3407, 3043, 4787, 637, 292, 493, 13, 400, 341, 2709, 505, 1071, 4786, 337, 51404], "temperature": 0.0, "avg_logprob": -0.1845568072411322, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.035155266523361206}, {"id": 122, "seek": 101764, "start": 1017.64, "end": 1029.32, "text": " optimizations that I will mention later. Union push down. Previously, there was no way to", "tokens": [50364, 5028, 14455, 300, 286, 486, 2152, 1780, 13, 8133, 2944, 760, 13, 33606, 11, 456, 390, 572, 636, 281, 50948], "temperature": 0.0, "avg_logprob": -0.13993396759033203, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.2283577173948288}, {"id": 123, "seek": 101764, "start": 1029.32, "end": 1037.4, "text": " directly run the queries, like you can see at the bottom half of the bottom, the most bottom line", "tokens": [50948, 3838, 1190, 264, 24109, 11, 411, 291, 393, 536, 412, 264, 2767, 1922, 295, 264, 2767, 11, 264, 881, 2767, 1622, 51352], "temperature": 0.0, "avg_logprob": -0.13993396759033203, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.2283577173948288}, {"id": 124, "seek": 101764, "start": 1038.12, "end": 1044.68, "text": " when the union closes at the top. Because MariaDB processing path for unions and simple", "tokens": [51388, 562, 264, 11671, 24157, 412, 264, 1192, 13, 1436, 12734, 27735, 9007, 3100, 337, 24914, 293, 2199, 51716], "temperature": 0.0, "avg_logprob": -0.13993396759033203, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.2283577173948288}, {"id": 125, "seek": 104468, "start": 1044.68, "end": 1053.88, "text": " selects, it differs. And columnstore works using the notion of select handler that allows MariaDB", "tokens": [50364, 3048, 82, 11, 309, 37761, 13, 400, 7738, 21624, 1985, 1228, 264, 10710, 295, 3048, 41967, 300, 4045, 12734, 27735, 50824], "temperature": 0.0, "avg_logprob": -0.12197565730613998, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.08176645636558533}, {"id": 126, "seek": 104468, "start": 1053.88, "end": 1060.2, "text": " server to push the whole query down to columnstore. So, when there was a union, we cannot use this", "tokens": [50824, 7154, 281, 2944, 264, 1379, 14581, 760, 281, 7738, 21624, 13, 407, 11, 562, 456, 390, 257, 11671, 11, 321, 2644, 764, 341, 51140], "temperature": 0.0, "avg_logprob": -0.12197565730613998, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.08176645636558533}, {"id": 127, "seek": 104468, "start": 1060.2, "end": 1069.24, "text": " path and we have to do, we call it a subquery wrap. So, we were wrapping these into a subquery and", "tokens": [51140, 3100, 293, 321, 362, 281, 360, 11, 321, 818, 309, 257, 1422, 358, 2109, 7019, 13, 407, 11, 321, 645, 21993, 613, 666, 257, 1422, 358, 2109, 293, 51592], "temperature": 0.0, "avg_logprob": -0.12197565730613998, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.08176645636558533}, {"id": 128, "seek": 106924, "start": 1069.24, "end": 1074.76, "text": " this allows us to use the select handler, comparing to a table mode that is slower.", "tokens": [50364, 341, 4045, 505, 281, 764, 264, 3048, 41967, 11, 15763, 281, 257, 3199, 4391, 300, 307, 14009, 13, 50640], "temperature": 0.0, "avg_logprob": -0.15747331536334494, "compression_ratio": 1.417989417989418, "no_speech_prob": 0.02966594509780407}, {"id": 129, "seek": 106924, "start": 1077.32, "end": 1085.72, "text": " The next big feature is a full TPCH support. And why it is big? Because it is mostly concerned", "tokens": [50768, 440, 958, 955, 4111, 307, 257, 1577, 314, 12986, 39, 1406, 13, 400, 983, 309, 307, 955, 30, 1436, 309, 307, 5240, 5922, 51188], "temperature": 0.0, "avg_logprob": -0.15747331536334494, "compression_ratio": 1.417989417989418, "no_speech_prob": 0.02966594509780407}, {"id": 130, "seek": 106924, "start": 1085.72, "end": 1093.32, "text": " with the optimizer, that let's face it columnstore lag previously. There are two queries,", "tokens": [51188, 365, 264, 5028, 6545, 11, 300, 718, 311, 1851, 309, 7738, 21624, 8953, 8046, 13, 821, 366, 732, 24109, 11, 51568], "temperature": 0.0, "avg_logprob": -0.15747331536334494, "compression_ratio": 1.417989417989418, "no_speech_prob": 0.02966594509780407}, {"id": 131, "seek": 109332, "start": 1094.28, "end": 1104.52, "text": " with two features that columnstore lack and these two are correlated subquery with aggregates.", "tokens": [50412, 365, 732, 4122, 300, 7738, 21624, 5011, 293, 613, 732, 366, 38574, 1422, 358, 2109, 365, 16743, 1024, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1526394037099985, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.07393240183591843}, {"id": 132, "seek": 109332, "start": 1104.52, "end": 1111.8799999999999, "text": " It's basically a scalar query, but with aggregate. It returns a single row that you can use in", "tokens": [50924, 467, 311, 1936, 257, 39684, 14581, 11, 457, 365, 26118, 13, 467, 11247, 257, 2167, 5386, 300, 291, 393, 764, 294, 51292], "temperature": 0.0, "avg_logprob": -0.1526394037099985, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.07393240183591843}, {"id": 133, "seek": 109332, "start": 1111.8799999999999, "end": 1119.48, "text": " comparison. And guess what? To enable this feature, we just disabled the error message", "tokens": [51292, 9660, 13, 400, 2041, 437, 30, 1407, 9528, 341, 4111, 11, 321, 445, 15191, 264, 6713, 3636, 51672], "temperature": 0.0, "avg_logprob": -0.1526394037099985, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.07393240183591843}, {"id": 134, "seek": 111948, "start": 1119.48, "end": 1127.8, "text": " in the code. So, it was a very, very tiny change. And columnstore naturally supports such queries.", "tokens": [50364, 294, 264, 3089, 13, 407, 11, 309, 390, 257, 588, 11, 588, 5870, 1319, 13, 400, 7738, 21624, 8195, 9346, 1270, 24109, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12097012656075613, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.035620786249637604}, {"id": 135, "seek": 111948, "start": 1127.8, "end": 1136.3600000000001, "text": " However, the last query type that was presented at the bottom half is way more elaborate to handle.", "tokens": [50780, 2908, 11, 264, 1036, 14581, 2010, 300, 390, 8212, 412, 264, 2767, 1922, 307, 636, 544, 20945, 281, 4813, 13, 51208], "temperature": 0.0, "avg_logprob": -0.12097012656075613, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.035620786249637604}, {"id": 136, "seek": 111948, "start": 1137.24, "end": 1147.64, "text": " And that is the common conjunction detection and rewriting. As you can see, there are two", "tokens": [51252, 400, 300, 307, 264, 2689, 27482, 17784, 293, 319, 19868, 13, 1018, 291, 393, 536, 11, 456, 366, 732, 51772], "temperature": 0.0, "avg_logprob": -0.12097012656075613, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.035620786249637604}, {"id": 137, "seek": 114764, "start": 1147.64, "end": 1153.4, "text": " joint conditions at the bottom half of the slide. They are marked with the violet.", "tokens": [50364, 7225, 4487, 412, 264, 2767, 1922, 295, 264, 4137, 13, 814, 366, 12658, 365, 264, 46480, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14931784735785592, "compression_ratio": 1.736318407960199, "no_speech_prob": 0.021042730659246445}, {"id": 138, "seek": 114764, "start": 1154.0400000000002, "end": 1159.16, "text": " And these conditions are common. However, columnstore cannot handle the", "tokens": [50684, 400, 613, 4487, 366, 2689, 13, 2908, 11, 7738, 21624, 2644, 4813, 264, 50940], "temperature": 0.0, "avg_logprob": -0.14931784735785592, "compression_ratio": 1.736318407960199, "no_speech_prob": 0.021042730659246445}, {"id": 139, "seek": 114764, "start": 1160.0400000000002, "end": 1168.2, "text": " joint conditions if they are combined with a junction or basically. So, to handle this query,", "tokens": [50984, 7225, 4487, 498, 436, 366, 9354, 365, 257, 33718, 420, 1936, 13, 407, 11, 281, 4813, 341, 14581, 11, 51392], "temperature": 0.0, "avg_logprob": -0.14931784735785592, "compression_ratio": 1.736318407960199, "no_speech_prob": 0.021042730659246445}, {"id": 140, "seek": 114764, "start": 1168.2, "end": 1176.2, "text": " we need to go over all the conditions that are ordered and get the common one and put it at the top.", "tokens": [51392, 321, 643, 281, 352, 670, 439, 264, 4487, 300, 366, 8866, 293, 483, 264, 2689, 472, 293, 829, 309, 412, 264, 1192, 13, 51792], "temperature": 0.0, "avg_logprob": -0.14931784735785592, "compression_ratio": 1.736318407960199, "no_speech_prob": 0.021042730659246445}, {"id": 141, "seek": 117764, "start": 1178.6000000000001, "end": 1186.44, "text": " For a general case, it's very complex to find such patterns and applies them in a way that it", "tokens": [50412, 1171, 257, 2674, 1389, 11, 309, 311, 588, 3997, 281, 915, 1270, 8294, 293, 13165, 552, 294, 257, 636, 300, 309, 50804], "temperature": 0.0, "avg_logprob": -0.13260345747976593, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.014866754412651062}, {"id": 142, "seek": 117764, "start": 1186.44, "end": 1193.96, "text": " doesn't make the symmetrical meaning of the query itself. However, we manage this and this", "tokens": [50804, 1177, 380, 652, 264, 40360, 3620, 295, 264, 14581, 2564, 13, 2908, 11, 321, 3067, 341, 293, 341, 51180], "temperature": 0.0, "avg_logprob": -0.13260345747976593, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.014866754412651062}, {"id": 143, "seek": 117764, "start": 1193.96, "end": 1198.92, "text": " feature will benefit not only TPCH query that didn't work previously, but all others.", "tokens": [51180, 4111, 486, 5121, 406, 787, 314, 12986, 39, 14581, 300, 994, 380, 589, 8046, 11, 457, 439, 2357, 13, 51428], "temperature": 0.0, "avg_logprob": -0.13260345747976593, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.014866754412651062}, {"id": 144, "seek": 119892, "start": 1199.64, "end": 1212.04, "text": " External distinct. As you might know, columnstore was not great doing this thing. To be honest,", "tokens": [50400, 48277, 10644, 13, 1018, 291, 1062, 458, 11, 7738, 21624, 390, 406, 869, 884, 341, 551, 13, 1407, 312, 3245, 11, 51020], "temperature": 0.0, "avg_logprob": -0.20041351318359374, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.03621343895792961}, {"id": 145, "seek": 119892, "start": 1212.04, "end": 1219.24, "text": " it was hash map based. So, it cannot do an external operation and it was tightly coupled", "tokens": [51020, 309, 390, 22019, 4471, 2361, 13, 407, 11, 309, 2644, 360, 364, 8320, 6916, 293, 309, 390, 21952, 29482, 51380], "temperature": 0.0, "avg_logprob": -0.20041351318359374, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.03621343895792961}, {"id": 146, "seek": 119892, "start": 1219.24, "end": 1226.04, "text": " with order by. So, to allow this thing to be enacted, to become an external, we need to", "tokens": [51380, 365, 1668, 538, 13, 407, 11, 281, 2089, 341, 551, 281, 312, 41313, 11, 281, 1813, 364, 8320, 11, 321, 643, 281, 51720], "temperature": 0.0, "avg_logprob": -0.20041351318359374, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.03621343895792961}, {"id": 147, "seek": 122604, "start": 1226.04, "end": 1233.3999999999999, "text": " untie them, to uncouple them, and we just applied the existing goodbye facility that already has", "tokens": [50364, 1701, 414, 552, 11, 281, 6219, 263, 781, 552, 11, 293, 321, 445, 6456, 264, 6741, 12084, 8973, 300, 1217, 575, 50732], "temperature": 0.0, "avg_logprob": -0.1572713711682488, "compression_ratio": 1.5935828877005347, "no_speech_prob": 0.02178407832980156}, {"id": 148, "seek": 122604, "start": 1233.3999999999999, "end": 1242.6, "text": " the external support, external operation support. So, this gives us a future optimization opportunities", "tokens": [50732, 264, 8320, 1406, 11, 8320, 6916, 1406, 13, 407, 11, 341, 2709, 505, 257, 2027, 19618, 4786, 51192], "temperature": 0.0, "avg_logprob": -0.1572713711682488, "compression_ratio": 1.5935828877005347, "no_speech_prob": 0.02178407832980156}, {"id": 149, "seek": 122604, "start": 1242.6, "end": 1250.6, "text": " and also it allows the distinct to scale now. So, it will be fast. The most notable changes maybe", "tokens": [51192, 293, 611, 309, 4045, 264, 10644, 281, 4373, 586, 13, 407, 11, 309, 486, 312, 2370, 13, 440, 881, 22556, 2962, 1310, 51592], "temperature": 0.0, "avg_logprob": -0.1572713711682488, "compression_ratio": 1.5935828877005347, "no_speech_prob": 0.02178407832980156}, {"id": 150, "seek": 125060, "start": 1250.6, "end": 1260.76, "text": " because I was the order is the order by rewrite. The current implementation of order by facility", "tokens": [50364, 570, 286, 390, 264, 1668, 307, 264, 1668, 538, 28132, 13, 440, 2190, 11420, 295, 1668, 538, 8973, 50872], "temperature": 0.0, "avg_logprob": -0.15615152212289662, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.05606038123369217}, {"id": 151, "seek": 125060, "start": 1260.76, "end": 1269.24, "text": " is based on a priority queue. That is great for top K queries like mentioned in the second bullet,", "tokens": [50872, 307, 2361, 322, 257, 9365, 18639, 13, 663, 307, 869, 337, 1192, 591, 24109, 411, 2835, 294, 264, 1150, 11632, 11, 51296], "temperature": 0.0, "avg_logprob": -0.15615152212289662, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.05606038123369217}, {"id": 152, "seek": 125060, "start": 1270.12, "end": 1277.24, "text": " maybe the third. However, the priority queue timings are just terrifying when you run the", "tokens": [51340, 1310, 264, 2636, 13, 2908, 11, 264, 9365, 18639, 524, 1109, 366, 445, 18106, 562, 291, 1190, 264, 51696], "temperature": 0.0, "avg_logprob": -0.15615152212289662, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.05606038123369217}, {"id": 153, "seek": 127724, "start": 1277.24, "end": 1284.68, "text": " query on a huge data set without limit or the limit is big and relatively big. So, I replace the", "tokens": [50364, 14581, 322, 257, 2603, 1412, 992, 1553, 4948, 420, 264, 4948, 307, 955, 293, 7226, 955, 13, 407, 11, 286, 7406, 264, 50736], "temperature": 0.0, "avg_logprob": -0.23387003580729165, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.043873462826013565}, {"id": 154, "seek": 127724, "start": 1285.56, "end": 1293.24, "text": " priority queue I'll go with the different approach. It has 2.5 phases. It first calculated the sort", "tokens": [50780, 9365, 18639, 286, 603, 352, 365, 264, 819, 3109, 13, 467, 575, 568, 13, 20, 18764, 13, 467, 700, 15598, 264, 1333, 51164], "temperature": 0.0, "avg_logprob": -0.23387003580729165, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.043873462826013565}, {"id": 155, "seek": 127724, "start": 1293.24, "end": 1300.6, "text": " of runs in parallel, completely in parallel. And the new middle phase, it calculates non-overlap", "tokens": [51164, 295, 6676, 294, 8952, 11, 2584, 294, 8952, 13, 400, 264, 777, 2808, 5574, 11, 309, 4322, 1024, 2107, 12, 3570, 75, 569, 51532], "temperature": 0.0, "avg_logprob": -0.23387003580729165, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.043873462826013565}, {"id": 156, "seek": 130060, "start": 1300.6, "end": 1309.08, "text": " intermutation ranges for the second phase. So, the second phase officially merges non-overlap", "tokens": [50364, 728, 76, 11380, 22526, 337, 264, 1150, 5574, 13, 407, 11, 264, 1150, 5574, 12053, 3551, 2880, 2107, 12, 3570, 75, 569, 50788], "temperature": 0.0, "avg_logprob": -0.16321721795487076, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.17366492748260498}, {"id": 157, "seek": 130060, "start": 1309.08, "end": 1315.9599999999998, "text": " ranges in parallel. So, you have first phase and second phase and these two phases are completely", "tokens": [50788, 22526, 294, 8952, 13, 407, 11, 291, 362, 700, 5574, 293, 1150, 5574, 293, 613, 732, 18764, 366, 2584, 51132], "temperature": 0.0, "avg_logprob": -0.16321721795487076, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.17366492748260498}, {"id": 158, "seek": 130060, "start": 1315.9599999999998, "end": 1325.32, "text": " parallel. They don't overlap each other. So, in the end, the column star now has a choice between the", "tokens": [51132, 8952, 13, 814, 500, 380, 19959, 1184, 661, 13, 407, 11, 294, 264, 917, 11, 264, 7738, 3543, 586, 575, 257, 3922, 1296, 264, 51600], "temperature": 0.0, "avg_logprob": -0.16321721795487076, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.17366492748260498}, {"id": 159, "seek": 132532, "start": 1325.3999999999999, "end": 1331.24, "text": " priority queue based sorting and this new algorithm that it will pick looking at the", "tokens": [50368, 9365, 18639, 2361, 32411, 293, 341, 777, 9284, 300, 309, 486, 1888, 1237, 412, 264, 50660], "temperature": 0.0, "avg_logprob": -0.14826713502407074, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.032993242144584656}, {"id": 160, "seek": 132532, "start": 1332.84, "end": 1343.0, "text": " order by usage pattern of the query. And these are the results. As you can see, the most interesting", "tokens": [50740, 1668, 538, 14924, 5102, 295, 264, 14581, 13, 400, 613, 366, 264, 3542, 13, 1018, 291, 393, 536, 11, 264, 881, 1880, 51248], "temperature": 0.0, "avg_logprob": -0.14826713502407074, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.032993242144584656}, {"id": 161, "seek": 132532, "start": 1343.0, "end": 1352.36, "text": " is last two bullets. There is a comparison between the previous code, the previous version that is", "tokens": [51248, 307, 1036, 732, 20132, 13, 821, 307, 257, 9660, 1296, 264, 3894, 3089, 11, 264, 3894, 3037, 300, 307, 51716], "temperature": 0.0, "avg_logprob": -0.14826713502407074, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.032993242144584656}, {"id": 162, "seek": 135236, "start": 1352.36, "end": 1364.1999999999998, "text": " based on a priority queue, the data taken from a TPCDS generator. And the first query uses the", "tokens": [50364, 2361, 322, 257, 9365, 18639, 11, 264, 1412, 2726, 490, 257, 314, 12986, 11844, 19265, 13, 400, 264, 700, 14581, 4960, 264, 50956], "temperature": 0.0, "avg_logprob": -0.2035369078318278, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.04485883191227913}, {"id": 163, "seek": 135236, "start": 1364.1999999999998, "end": 1374.04, "text": " integer sorting key columns. And the next, the second query uses the character key columns. So,", "tokens": [50956, 24922, 32411, 2141, 13766, 13, 400, 264, 958, 11, 264, 1150, 14581, 4960, 264, 2517, 2141, 13766, 13, 407, 11, 51448], "temperature": 0.0, "avg_logprob": -0.2035369078318278, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.04485883191227913}, {"id": 164, "seek": 137404, "start": 1374.04, "end": 1382.76, "text": " as you can see, for integers, it brings like four times faster. But I need to confess this is only", "tokens": [50364, 382, 291, 393, 536, 11, 337, 41674, 11, 309, 5607, 411, 1451, 1413, 4663, 13, 583, 286, 643, 281, 19367, 341, 307, 787, 50800], "temperature": 0.0, "avg_logprob": -0.18059350298596666, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.06402672827243805}, {"id": 165, "seek": 137404, "start": 1383.32, "end": 1391.6399999999999, "text": " scalability factor, scale factor 10. So, it's not a big dataset, anyway. For a bigger dataset,", "tokens": [50828, 15664, 2310, 5952, 11, 4373, 5952, 1266, 13, 407, 11, 309, 311, 406, 257, 955, 28872, 11, 4033, 13, 1171, 257, 3801, 28872, 11, 51244], "temperature": 0.0, "avg_logprob": -0.18059350298596666, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.06402672827243805}, {"id": 166, "seek": 137404, "start": 1391.6399999999999, "end": 1401.3999999999999, "text": " you can have like 20x or maybe even bigger. And to be honest, I don't compare with the", "tokens": [51244, 291, 393, 362, 411, 945, 87, 420, 1310, 754, 3801, 13, 400, 281, 312, 3245, 11, 286, 500, 380, 6794, 365, 264, 51732], "temperature": 0.0, "avg_logprob": -0.18059350298596666, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.06402672827243805}, {"id": 167, "seek": 140140, "start": 1401.48, "end": 1408.76, "text": " other open source engine just yet because I'm going to come up with a separate story about this", "tokens": [50368, 661, 1269, 4009, 2848, 445, 1939, 570, 286, 478, 516, 281, 808, 493, 365, 257, 4994, 1657, 466, 341, 50732], "temperature": 0.0, "avg_logprob": -0.17156213521957397, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.012405388988554478}, {"id": 168, "seek": 140140, "start": 1408.76, "end": 1415.96, "text": " sorting. So, these are the links. If you're interested in column star, first is the code itself and", "tokens": [50732, 32411, 13, 407, 11, 613, 366, 264, 6123, 13, 759, 291, 434, 3102, 294, 7738, 3543, 11, 700, 307, 264, 3089, 2564, 293, 51092], "temperature": 0.0, "avg_logprob": -0.17156213521957397, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.012405388988554478}, {"id": 169, "seek": 140140, "start": 1415.96, "end": 1422.6000000000001, "text": " the second is JIRA. So, if you find bags, please post them. We will appreciate it. Thank you all for", "tokens": [51092, 264, 1150, 307, 50172, 3750, 13, 407, 11, 498, 291, 915, 10405, 11, 1767, 2183, 552, 13, 492, 486, 4449, 309, 13, 1044, 291, 439, 337, 51424], "temperature": 0.0, "avg_logprob": -0.17156213521957397, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.012405388988554478}, {"id": 170, "seek": 140140, "start": 1422.6000000000001, "end": 1429.4, "text": " coming again.", "tokens": [51424, 1348, 797, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17156213521957397, "compression_ratio": 1.4832535885167464, "no_speech_prob": 0.012405388988554478}, {"id": 171, "seek": 143140, "start": 1431.4, "end": 1432.7800000000002, "text": " you", "tokens": [50404, 291, 50433], "temperature": 0.0, "avg_logprob": -0.9381336569786072, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9077211618423462}], "language": "en"}