{"text": " So, welcome to my talk about Trixity, one matrix SDK for almost everything. I added it written in Kotlin a few days ago, so maybe there are some Kotlin fanboys here. Yeah, let me first introduce myself. I am Benedict and my friends often see me as a killjoy when it comes to data protection and data security, but I convince them to come to matrix anyhow. So I have 20 users, family and friends on my own matrix home server. My first contact with matrix was four to five years ago and I gained a lot of experience with it since then. And so much I found it connect to X and this is just a company that is developing a Timmy and that is a TI messenger for the medical health factor in Germany. Now let's start with Trixity. Trixity is a matrix SDK and it is for developing clients, bots, app servers and servers. It is multi-platform capable, so everyone thinks Kotlin is JVM only, it is not. You can compile it to JS and you can compile it to native, that's important for iOS. So we all have that targets with Trixity. And it's also developed test-driven, so we have a high test coverage and it is licensed under the Apache 2 license. You may wonder why another SDK? So back in January 2020 there were only a few multi-platform SDKs to choose from. If I remember correctly there was the Matrix Rust SDK, but it was in a very early stage and there was the Dart SDK, but very likely this forces you to use Flutter in the UI. So there is no real free choice which UI framework you want to use, especially when you want to use native UI technologies, for example on Android Compose or on iOS Swift. Additionally, most SDKs didn't have a very strict typing of events and the rest end points. And also the extensibility was a bit limited. Even if the next point is not that important, SDKs were really bound to its purpose. So you've had a SDK for a client, for a server, for a bot and so on. So you have to learn a new SDK for each target, each purpose of application for Matrix. Why choose Kotlin? Choose Kotlin because it is a statically typed language which compiles, as I mentioned, to JVM, JS and native. And you don't need bindings like in Rust, when you use JS you get JS, you don't need to make bindings over VASM or something. And on native you can just call it from your Swift or Objective-C code in X code and have access to trixity. Moreover, besides shared common code, it is possible to write platform-specific code. You just define a common interface and depending on the platform, the actual implementation can be different. This way you have access to platform-specific APIs and libraries which can be very helpful when implementing encryption like AES for attachments. So you have on each platform, can you use the native encryption algorithm the platform gives you already. And last but not least, you can define your own domain-specific language. You will see later what I did with that. So let's start with the core of trixity. The core contains all basic data structures of the spec and its serialization algorithms. This includes events, identifiers like user IDs, event IDs and so on and other things like cross-signing keys, device keys. One goal of developing trixity was the ability to add custom events which are strictly typed. So this is achieved by mapping event types to just a serializer. In this example, we add a new type of m.room.pum.cat of the Kotlin type cat event content. So you have access to all fields of this cat event content and don't have to mess around with the JSON. The next layer of trixity is the API layer. Each API has its model which defines all endpoints of the API. The actual client and server implementation just use these endpoints. And so as a consequence, there is no need to define things twice. They are using the same Kotlin object. So a Kotlin object represents an endpoint on the matrix side, Kotlin class, not Kotlin object, sorry. The best way to show this to you is with an example. This example is the endpoint leave room. You just implement matrix endpoint, give him the types and in this case, unit is the response. So we don't get a JSON as response, just a hated HTTP OK or an empty JSON. And you can also define a request, a URL, HTTP method and all that. And you can use this to call, to use a client, a matrix client on the client side to call these endpoints. So you create a leave room object, a request and you get the response. That's all on the client side. And the same thing on the server side. So you define an endpoint, give it the type, you expect as a request and in the context object you have access to the request and can answer with a response. To make it a bit more easier for developers, there is a bit of abstraction on top of that. So you can also just call leave room. So you don't have to know which endpoint are there existing. You just type point on your IDE and see, OK, there's a leave room, I can leave a room. And the same on the server side. So you just need to implement an interface and see all your endpoints you have to implement to be a fully-featured matrix server API. Regardless of the API, there is Trick City ARM and Trick City Crypto. Trick City ARM is just a wrapper for a lip-arm. As mentioned, a platform-independent implementation doesn't need to worry about the actual platform-specific implementations. So you have, when you use Trick City ARM, you don't need to know how lip-arm is accessed. So on the JVM, I use JNA. On JS, this is done via VASM and on native, just see interrupt from Kotlin. Lip-arm is also packaged into Trick City ARM. So as a developer, you don't need to ship the build-c library. And it is just loaded automatically. So you don't need to init your encryption, like in other libraries, as it's just loaded. My plan is to migrate that to Vodosemak, but currently, UNIFFI, we heard of that in another talk, does not support VASM targets. So currently, I can Vodosemak just only use in Kotlin, JVM and native, but I also want to use JavaScript. So this project is currently on eyes. Trick City Crypto currently implements the key management and allows to decrypt and encrypt events. And in the future, it will be more, so you can reuse the completely, complete crypto stuff, for example, in app services. Trick City Client allows you to, oh, sorry. The most abstract layer are Trick City Client and Trick City App Service. While Trick City App Service is still very basic and does not have a persistent layer, Trick City Client allows you to choose which database and which media store implementation you want to use. And on top of that, there is something that isn't released yet. We are not sure how to release, because we have to make money with our company. It is Trick City Messenger. This is just the view model representation of a messenger. So you only have to implement a thin UI layer, where when the user clicks the button, the UI sends this to the view model, and the view model says, OK, send a message, or go to this room, or any other stuff. And with this approach, we have implemented an iOS client in a few weeks with one person. So Trick City Client allows you to implement a fully-featured Matrix client or bot. So if you were at the Matrix Rust SDK talk, you can just use their representation and instead of Rust, you write Kotlin. So everything that Matrix Rust SDK does also does Trick City. Some features like Sliding Zinc aren't there, because we want to follow the stable Matrix specs, so we don't implement any MSCs. So we have all the E2E features, the exchangeability data stores and media stores, we have reactive cache on top of that, notification, thumbnail generation, all that stuff you need to implement the client. There are already some media store wrappers that we implemented for all targets, targets expect browsers, we just use the FI system and on browsers we use indexDB. Next I want to talk about how I accidentally created a cache. So on the left side you see the relation between the UI, Trick City and the storage layer. And because reactive UIs are really common, I wanted Trick City to give the UI access to the data in a reactive way. So if anything changes, the UI should immediately know about this. But the question is how? On the one hand there are a few databases which support listeners to react to changes to the database, but on the other hand this would limit support for multiple supported databases because finding a common interface for listeners would be hard. So I started implementing an intermediate layer based on Kotlin flows. The flow in Kotlin is a reactive data structure. So you have a producer on the one side and a consumer on the other side. So if the producer changes anything, the consumers immediately know about that. And what does the intermediate layer, it talks to a very thin database layer which only knows about save, read and delete data. And if someone wants data from this layer, it just reads it from the database or if someone changes something in this layer, it just writes it to the database. And the values are kept in this layer as long as they are subscribed from anyone. So this means that if anyone else subscribes to a value, he will immediately get the current value because there is no additional database call needed because it is persisted in the intermediate layer. This goes so far that even if there are no subscribers anymore, I just keep the value a bit longer in this layer. So if someone asks for a value for example 10 seconds later and the value is still stored, he gets the value and there is no database call needed. And you can now guess what I implemented, it's just cache. So as you see with this cache, everything in Trixity is reactive. These are just a few examples, so you can just get all users or check if a user can invite another one, you immediately get the notification if anything has changed. As mentioned, the database layer is really thin, so we implemented many database layers. So SQL-based one, we are via exposed for JVM-based targets. We implemented one with Realm that can be also used on native targets like iOS and for browsers we have IndexedDB. So most of the data changes, when a Zinc is processed, most of the data changes when a Zinc is processed. So it is way more performant to make a large transaction around the Zinc. So you don't have a transaction, every time the cache writes something into the database, Trixity just spends a large transaction around Zinc, so you have thousands of writes in one transaction. So everything fine, no, then there was Realm, and Realm is just a really fast database, but Realm only allows one write transaction at the time. So if another one wants to write to the database, he needs to wait until the first transaction ended. And the problem is that while the Zinc is running, it may be needed that we have to wait for outdated keys to be updated to decrypt on stuff. So if the outdated keys part of Trixity want to write something in the database, he needs to wait until the Zinc is ended, but the Zinc waits for the keys to be updated, so we have a deadlock there. This is one of the reasons why I introduced Azure Zinc transactions. The other reason was that most of the time the Zinc processing, as I find out with some benchmark, was wasted due to writing to the database. So processing a Zinc takes a long time because there are so many IO operations that the user have to wait until all operations are done. So what does a Zinc transaction in Trixity mean that all changes to the database are collected and processed in the background? So database operations are decoupled from the cache layer, and they are just written in the background. If everything fails, it is war-backed, but that's irrelevant in the normal use case. So we can process even more Zincs at the same time as if we would wait that the transaction has finished. And this gave Trixity a huge performance boost. Actually I released it last week, and I've wrote an integration test which just fails if it is not 50% faster. So it is always green, I don't know. The next thing I did completely different in Trixity are timelines. So normally Zincs are sent as fragment from the server to the client. So one fragment contains a few timeline events, and if there's a gap, you get a token. So you know as a client, okay, there's a gap, I need to fetch to fill that gap, and so on. And these fragments normally are saved as is to the database in clients. In Trixity, I use another approach. There I have each timeline event pointing to each other. And if there's a gap, the timeline event knows about this. So this allows Trixity to again benefit from content flows. So we have a producer that is the room starting from a timeline event, and a subscriber who wants the next timeline event to fill its timeline. So this allows us to go really fast through the timeline and bid the timeline under the top, and it makes it easier to fill the gaps, because we don't have another layer, fragments, we just have timeline events. And this way, it's also possible to very easy connect upgraded rooms. So that one I released yesterday, I think, or two days ago. So the timeline event just shows to another timeline in another room. So timelines with room upgrades are invisible for users of Trixity. You just get an infinite timeline until you reach the oldest room and the first timeline event. And finally, a small example. So if you want to write a bot, that's a good start to use Trixity just to get a feeling about it, how it works. You can just call get timeline events from now on. And what this does is it subscribes to the flow that I mentioned, which built the timeline, and rates until the timeline event is decrypted, because the timeline itself also is a flow. So if everything changes, it is redacted, or there's a reaction, or a replacement, the timeline event flow changes. So this get timeline events from now on just wraps it down, so you get a timeline event that is decrypted. And you can see we can just check what type it has, and when we have checked the type, we have access to body, and then we have send message. So when you call send message, you don't have to worry about if the room is encrypted or not. You can just use the DSL that I created to write text messages, image messages, and so on, and you can also form relations with that. So you can say like here, yeah, this is a reply to the timeline event I just got. And this has extensible events in mind, so if in the future there are other content blocks that are added, we can just extend the DSL, and you can very, very intuitive write your content with Trixity into an event, into an extensible event. So here are some projects that are using Trixity and that I know about. There is a Spotify bot, a Mensa bot. Someone has created some extensions to better use it for bots and so on. And there is Trixity examples. That is from me, this is just a ping bot, part of it you saw here. It is E2E enabled, you can just run it in your browser, on your Linux machine, or on your iOS client, or via the JVM on Android. And there is also Timmy Messenger, that's our messenger from our company, but it's not open source yet. We plan to, but we don't know how, because licensing. Yeah, just try it out and come to me if you have questions. I'm a bit around. This is the matrix room, this is my matrix ID. And if we have a bit of time, I just can show you a small demo, I think. Yeah, I made a small performance comparison. It's not representative, because it just runs once on my machine, and there was no warm up or multiple runs. Yeah, on the left side you see our Timmy client, but basically it's just using Trixity. On the right side there is Element, and in the middle is Fluffy Chat. And now you can give me your bets, who is the fastest. Yeah, let's see, when the red zoom comes, the response from the server reached the client. So I just looked into the Synapse logs when the response was sent. So we just wait a few seconds, and then we see who is first. And you can look into opening rooms, because we have this caching, it is very fast in our client, but I must say Fluffy Chat is also very fast regarding opening rooms. So, oh, Trixity was the fastest. And we can open rooms, and you see open rooms is also a lot faster than on Element. And there was Fluffy Chat, and Fluffy Chat also is very fast. Yeah, I also have a desktop demo, but there Neko is the fastest. This is Neko, this is Timmy, Element on the web. It's a bit hard, this comparison, because Element runs in the web and does not have the multi-threading other clients have. So Neko, just three seconds. I can just chat around, and the next is Timmy on the left, top left side, also very fast opening rooms, and switching rooms, because it is cached all the time in events. Then there was Element, and I think also Fluffy Chat, yeah, Fluffy Chat also. Yeah, okay, that was my talk, thank you. Questions? How do you prevent data loss with your async transactions? The transactions are run each after. So if one transaction fails, the other transactions are just run, and the next one starts with the alt token. What happens if, say, your battery runs out whilst a bunch of transactions are queued? If your battery runs out when all those transactions are queued, so they haven't been written to the database. Yeah, then they are gone. Your client has to do the work again, but mostly this doesn't happen. If you close your client, all transactions are written that are just opened, but it depends on your platform if it is killed hardly, or a trick city have a bit of time to write the transactions back to the database. But it's still very fast to write, so it's just a bit snappier on mobile devices, which are not that fast. Like my smartphone from 2016, Element, I can't run Element on that, because it's too slow, and sending messages, 10 seconds later, the message is, oh, okay, yes, yes, now. Now we send the message. I don't have this problem, because zooms are just faster than the slow I owe writing to the database we have on old smartphones, for example. Another question. It's nice to write. I like DSLs. In Kotlin, we have them all over the language, and it feels very intuitive, because your IDE gives you suggestions, what methods they are, and it's a lot easier to read, I think. There's Rust, and there's Kotlin, but is there any way to realize the amount of things that the user has to learn to use all these things? I didn't understand the question, acoustically. There's a lot of language learning to make any progress. Is there any effort to unify this, or towards Rust, maybe? To be honest, I don't like Rust. I just like a higher level of implementing stuff, so we didn't spoke, I didn't spoke with the Matrix Rust team. I think we are done with the time, and the last question from the audience would be that we can open the windows and the doors a bit to get more air in. Thank you very much. Thank you very much. Yep. That's it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.76, "text": " So, welcome to my talk about Trixity, one matrix SDK for almost everything.", "tokens": [407, 11, 2928, 281, 452, 751, 466, 314, 6579, 507, 11, 472, 8141, 37135, 337, 1920, 1203, 13], "temperature": 0.0, "avg_logprob": -0.26974110854299443, "compression_ratio": 1.3289473684210527, "no_speech_prob": 0.22940273582935333}, {"id": 1, "seek": 0, "start": 13.76, "end": 21.52, "text": " I added it written in Kotlin a few days ago, so maybe there are some Kotlin fanboys here.", "tokens": [286, 3869, 309, 3720, 294, 30123, 5045, 257, 1326, 1708, 2057, 11, 370, 1310, 456, 366, 512, 30123, 5045, 3429, 31638, 510, 13], "temperature": 0.0, "avg_logprob": -0.26974110854299443, "compression_ratio": 1.3289473684210527, "no_speech_prob": 0.22940273582935333}, {"id": 2, "seek": 0, "start": 21.52, "end": 24.400000000000002, "text": " Yeah, let me first introduce myself.", "tokens": [865, 11, 718, 385, 700, 5366, 2059, 13], "temperature": 0.0, "avg_logprob": -0.26974110854299443, "compression_ratio": 1.3289473684210527, "no_speech_prob": 0.22940273582935333}, {"id": 3, "seek": 2440, "start": 24.4, "end": 30.24, "text": " I am Benedict and my friends often see me as a killjoy when it comes to data protection", "tokens": [286, 669, 47837, 293, 452, 1855, 2049, 536, 385, 382, 257, 1961, 1994, 562, 309, 1487, 281, 1412, 6334], "temperature": 0.0, "avg_logprob": -0.15637690680367605, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00010799559095175937}, {"id": 4, "seek": 2440, "start": 30.24, "end": 35.76, "text": " and data security, but I convince them to come to matrix anyhow.", "tokens": [293, 1412, 3825, 11, 457, 286, 13447, 552, 281, 808, 281, 8141, 44995, 13], "temperature": 0.0, "avg_logprob": -0.15637690680367605, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00010799559095175937}, {"id": 5, "seek": 2440, "start": 35.76, "end": 42.44, "text": " So I have 20 users, family and friends on my own matrix home server.", "tokens": [407, 286, 362, 945, 5022, 11, 1605, 293, 1855, 322, 452, 1065, 8141, 1280, 7154, 13], "temperature": 0.0, "avg_logprob": -0.15637690680367605, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00010799559095175937}, {"id": 6, "seek": 2440, "start": 42.44, "end": 48.92, "text": " My first contact with matrix was four to five years ago and I gained a lot of experience", "tokens": [1222, 700, 3385, 365, 8141, 390, 1451, 281, 1732, 924, 2057, 293, 286, 12634, 257, 688, 295, 1752], "temperature": 0.0, "avg_logprob": -0.15637690680367605, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00010799559095175937}, {"id": 7, "seek": 2440, "start": 48.92, "end": 51.76, "text": " with it since then.", "tokens": [365, 309, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.15637690680367605, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00010799559095175937}, {"id": 8, "seek": 5176, "start": 51.76, "end": 59.879999999999995, "text": " And so much I found it connect to X and this is just a company that is developing a Timmy", "tokens": [400, 370, 709, 286, 1352, 309, 1745, 281, 1783, 293, 341, 307, 445, 257, 2237, 300, 307, 6416, 257, 7172, 2226], "temperature": 0.0, "avg_logprob": -0.18015895843505858, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0001031879146466963}, {"id": 9, "seek": 5176, "start": 59.879999999999995, "end": 66.28, "text": " and that is a TI messenger for the medical health factor in Germany.", "tokens": [293, 300, 307, 257, 28819, 26599, 337, 264, 4625, 1585, 5952, 294, 7244, 13], "temperature": 0.0, "avg_logprob": -0.18015895843505858, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0001031879146466963}, {"id": 10, "seek": 5176, "start": 66.28, "end": 68.6, "text": " Now let's start with Trixity.", "tokens": [823, 718, 311, 722, 365, 314, 6579, 507, 13], "temperature": 0.0, "avg_logprob": -0.18015895843505858, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0001031879146466963}, {"id": 11, "seek": 5176, "start": 68.6, "end": 79.03999999999999, "text": " Trixity is a matrix SDK and it is for developing clients, bots, app servers and servers.", "tokens": [314, 6579, 507, 307, 257, 8141, 37135, 293, 309, 307, 337, 6416, 6982, 11, 35410, 11, 724, 15909, 293, 15909, 13], "temperature": 0.0, "avg_logprob": -0.18015895843505858, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0001031879146466963}, {"id": 12, "seek": 7904, "start": 79.04, "end": 87.24000000000001, "text": " It is multi-platform capable, so everyone thinks Kotlin is JVM only, it is not.", "tokens": [467, 307, 4825, 12, 39975, 837, 8189, 11, 370, 1518, 7309, 30123, 5045, 307, 508, 53, 44, 787, 11, 309, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.18798353558494932, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.890565727080684e-05}, {"id": 13, "seek": 7904, "start": 87.24000000000001, "end": 93.52000000000001, "text": " You can compile it to JS and you can compile it to native, that's important for iOS.", "tokens": [509, 393, 31413, 309, 281, 33063, 293, 291, 393, 31413, 309, 281, 8470, 11, 300, 311, 1021, 337, 17430, 13], "temperature": 0.0, "avg_logprob": -0.18798353558494932, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.890565727080684e-05}, {"id": 14, "seek": 7904, "start": 93.52000000000001, "end": 99.52000000000001, "text": " So we all have that targets with Trixity.", "tokens": [407, 321, 439, 362, 300, 12911, 365, 314, 6579, 507, 13], "temperature": 0.0, "avg_logprob": -0.18798353558494932, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.890565727080684e-05}, {"id": 15, "seek": 7904, "start": 99.52000000000001, "end": 106.92, "text": " And it's also developed test-driven, so we have a high test coverage and it is licensed", "tokens": [400, 309, 311, 611, 4743, 1500, 12, 25456, 11, 370, 321, 362, 257, 1090, 1500, 9645, 293, 309, 307, 25225], "temperature": 0.0, "avg_logprob": -0.18798353558494932, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.890565727080684e-05}, {"id": 16, "seek": 10692, "start": 106.92, "end": 111.64, "text": " under the Apache 2 license.", "tokens": [833, 264, 46597, 568, 10476, 13], "temperature": 0.0, "avg_logprob": -0.1450644616157778, "compression_ratio": 1.3313953488372092, "no_speech_prob": 0.00012698888895101845}, {"id": 17, "seek": 10692, "start": 111.64, "end": 115.36, "text": " You may wonder why another SDK?", "tokens": [509, 815, 2441, 983, 1071, 37135, 30], "temperature": 0.0, "avg_logprob": -0.1450644616157778, "compression_ratio": 1.3313953488372092, "no_speech_prob": 0.00012698888895101845}, {"id": 18, "seek": 10692, "start": 115.36, "end": 123.68, "text": " So back in January 2020 there were only a few multi-platform SDKs to choose from.", "tokens": [407, 646, 294, 7061, 4808, 456, 645, 787, 257, 1326, 4825, 12, 39975, 837, 37135, 82, 281, 2826, 490, 13], "temperature": 0.0, "avg_logprob": -0.1450644616157778, "compression_ratio": 1.3313953488372092, "no_speech_prob": 0.00012698888895101845}, {"id": 19, "seek": 10692, "start": 123.68, "end": 132.0, "text": " If I remember correctly there was the Matrix Rust SDK, but it was in a very early stage", "tokens": [759, 286, 1604, 8944, 456, 390, 264, 36274, 34952, 37135, 11, 457, 309, 390, 294, 257, 588, 2440, 3233], "temperature": 0.0, "avg_logprob": -0.1450644616157778, "compression_ratio": 1.3313953488372092, "no_speech_prob": 0.00012698888895101845}, {"id": 20, "seek": 13200, "start": 132.0, "end": 141.52, "text": " and there was the Dart SDK, but very likely this forces you to use Flutter in the UI.", "tokens": [293, 456, 390, 264, 30271, 37135, 11, 457, 588, 3700, 341, 5874, 291, 281, 764, 3235, 9947, 294, 264, 15682, 13], "temperature": 0.0, "avg_logprob": -0.1473484626183143, "compression_ratio": 1.4357541899441342, "no_speech_prob": 7.126510899979621e-05}, {"id": 21, "seek": 13200, "start": 141.52, "end": 148.44, "text": " So there is no real free choice which UI framework you want to use, especially when you want", "tokens": [407, 456, 307, 572, 957, 1737, 3922, 597, 15682, 8388, 291, 528, 281, 764, 11, 2318, 562, 291, 528], "temperature": 0.0, "avg_logprob": -0.1473484626183143, "compression_ratio": 1.4357541899441342, "no_speech_prob": 7.126510899979621e-05}, {"id": 22, "seek": 13200, "start": 148.44, "end": 155.6, "text": " to use native UI technologies, for example on Android Compose or on iOS Swift.", "tokens": [281, 764, 8470, 15682, 7943, 11, 337, 1365, 322, 8853, 6620, 541, 420, 322, 17430, 25539, 13], "temperature": 0.0, "avg_logprob": -0.1473484626183143, "compression_ratio": 1.4357541899441342, "no_speech_prob": 7.126510899979621e-05}, {"id": 23, "seek": 15560, "start": 155.6, "end": 163.76, "text": " Additionally, most SDKs didn't have a very strict typing of events and the rest end points.", "tokens": [19927, 11, 881, 37135, 82, 994, 380, 362, 257, 588, 10910, 18444, 295, 3931, 293, 264, 1472, 917, 2793, 13], "temperature": 0.0, "avg_logprob": -0.15391373339994455, "compression_ratio": 1.5, "no_speech_prob": 0.00014166194887366146}, {"id": 24, "seek": 15560, "start": 163.76, "end": 168.07999999999998, "text": " And also the extensibility was a bit limited.", "tokens": [400, 611, 264, 1279, 694, 2841, 390, 257, 857, 5567, 13], "temperature": 0.0, "avg_logprob": -0.15391373339994455, "compression_ratio": 1.5, "no_speech_prob": 0.00014166194887366146}, {"id": 25, "seek": 15560, "start": 168.07999999999998, "end": 174.5, "text": " Even if the next point is not that important, SDKs were really bound to its purpose.", "tokens": [2754, 498, 264, 958, 935, 307, 406, 300, 1021, 11, 37135, 82, 645, 534, 5472, 281, 1080, 4334, 13], "temperature": 0.0, "avg_logprob": -0.15391373339994455, "compression_ratio": 1.5, "no_speech_prob": 0.00014166194887366146}, {"id": 26, "seek": 15560, "start": 174.5, "end": 180.24, "text": " So you've had a SDK for a client, for a server, for a bot and so on.", "tokens": [407, 291, 600, 632, 257, 37135, 337, 257, 6423, 11, 337, 257, 7154, 11, 337, 257, 10592, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.15391373339994455, "compression_ratio": 1.5, "no_speech_prob": 0.00014166194887366146}, {"id": 27, "seek": 18024, "start": 180.24, "end": 190.68, "text": " So you have to learn a new SDK for each target, each purpose of application for Matrix.", "tokens": [407, 291, 362, 281, 1466, 257, 777, 37135, 337, 1184, 3779, 11, 1184, 4334, 295, 3861, 337, 36274, 13], "temperature": 0.0, "avg_logprob": -0.23353550351899247, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.00011194153921678662}, {"id": 28, "seek": 18024, "start": 190.68, "end": 192.28, "text": " Why choose Kotlin?", "tokens": [1545, 2826, 30123, 5045, 30], "temperature": 0.0, "avg_logprob": -0.23353550351899247, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.00011194153921678662}, {"id": 29, "seek": 18024, "start": 192.28, "end": 199.32000000000002, "text": " Choose Kotlin because it is a statically typed language which compiles, as I mentioned, to", "tokens": [21661, 30123, 5045, 570, 309, 307, 257, 2219, 984, 33941, 2856, 597, 715, 4680, 11, 382, 286, 2835, 11, 281], "temperature": 0.0, "avg_logprob": -0.23353550351899247, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.00011194153921678662}, {"id": 30, "seek": 18024, "start": 199.32000000000002, "end": 201.56, "text": " JVM, JS and native.", "tokens": [508, 53, 44, 11, 33063, 293, 8470, 13], "temperature": 0.0, "avg_logprob": -0.23353550351899247, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.00011194153921678662}, {"id": 31, "seek": 18024, "start": 201.56, "end": 209.8, "text": " And you don't need bindings like in Rust, when you use JS you get JS, you don't need", "tokens": [400, 291, 500, 380, 643, 14786, 1109, 411, 294, 34952, 11, 562, 291, 764, 33063, 291, 483, 33063, 11, 291, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.23353550351899247, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.00011194153921678662}, {"id": 32, "seek": 20980, "start": 209.8, "end": 213.48000000000002, "text": " to make bindings over VASM or something.", "tokens": [281, 652, 14786, 1109, 670, 691, 3160, 44, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 33, "seek": 20980, "start": 213.48000000000002, "end": 219.76000000000002, "text": " And on native you can just call it from your Swift or Objective-C code in X code and have", "tokens": [400, 322, 8470, 291, 393, 445, 818, 309, 490, 428, 25539, 420, 24753, 488, 12, 34, 3089, 294, 1783, 3089, 293, 362], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 34, "seek": 20980, "start": 219.76000000000002, "end": 222.44, "text": " access to trixity.", "tokens": [2105, 281, 504, 970, 507, 13], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 35, "seek": 20980, "start": 222.44, "end": 231.28, "text": " Moreover, besides shared common code, it is possible to write platform-specific code.", "tokens": [19838, 11, 11868, 5507, 2689, 3089, 11, 309, 307, 1944, 281, 2464, 3663, 12, 29258, 3089, 13], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 36, "seek": 20980, "start": 231.28, "end": 236.04000000000002, "text": " You just define a common interface and depending on the platform, the actual implementation", "tokens": [509, 445, 6964, 257, 2689, 9226, 293, 5413, 322, 264, 3663, 11, 264, 3539, 11420], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 37, "seek": 20980, "start": 236.04000000000002, "end": 237.96, "text": " can be different.", "tokens": [393, 312, 819, 13], "temperature": 0.0, "avg_logprob": -0.25826929928211684, "compression_ratio": 1.5065502183406114, "no_speech_prob": 7.36197762307711e-05}, {"id": 38, "seek": 23796, "start": 237.96, "end": 243.84, "text": " This way you have access to platform-specific APIs and libraries which can be very helpful", "tokens": [639, 636, 291, 362, 2105, 281, 3663, 12, 29258, 21445, 293, 15148, 597, 393, 312, 588, 4961], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 39, "seek": 23796, "start": 243.84, "end": 248.84, "text": " when implementing encryption like AES for attachments.", "tokens": [562, 18114, 29575, 411, 316, 2358, 337, 37987, 13], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 40, "seek": 23796, "start": 248.84, "end": 253.84, "text": " So you have on each platform, can you use the native encryption algorithm the platform", "tokens": [407, 291, 362, 322, 1184, 3663, 11, 393, 291, 764, 264, 8470, 29575, 9284, 264, 3663], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 41, "seek": 23796, "start": 253.84, "end": 256.64, "text": " gives you already.", "tokens": [2709, 291, 1217, 13], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 42, "seek": 23796, "start": 256.64, "end": 263.32, "text": " And last but not least, you can define your own domain-specific language.", "tokens": [400, 1036, 457, 406, 1935, 11, 291, 393, 6964, 428, 1065, 9274, 12, 29258, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 43, "seek": 23796, "start": 263.32, "end": 266.6, "text": " You will see later what I did with that.", "tokens": [509, 486, 536, 1780, 437, 286, 630, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.13312876501748727, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.00015805242583155632}, {"id": 44, "seek": 26660, "start": 266.6, "end": 269.88, "text": " So let's start with the core of trixity.", "tokens": [407, 718, 311, 722, 365, 264, 4965, 295, 504, 970, 507, 13], "temperature": 0.0, "avg_logprob": -0.12002548304471103, "compression_ratio": 1.4853801169590644, "no_speech_prob": 4.903434819425456e-05}, {"id": 45, "seek": 26660, "start": 269.88, "end": 278.40000000000003, "text": " The core contains all basic data structures of the spec and its serialization algorithms.", "tokens": [440, 4965, 8306, 439, 3875, 1412, 9227, 295, 264, 1608, 293, 1080, 17436, 2144, 14642, 13], "temperature": 0.0, "avg_logprob": -0.12002548304471103, "compression_ratio": 1.4853801169590644, "no_speech_prob": 4.903434819425456e-05}, {"id": 46, "seek": 26660, "start": 278.40000000000003, "end": 285.76000000000005, "text": " This includes events, identifiers like user IDs, event IDs and so on and other things", "tokens": [639, 5974, 3931, 11, 2473, 23463, 411, 4195, 48212, 11, 2280, 48212, 293, 370, 322, 293, 661, 721], "temperature": 0.0, "avg_logprob": -0.12002548304471103, "compression_ratio": 1.4853801169590644, "no_speech_prob": 4.903434819425456e-05}, {"id": 47, "seek": 26660, "start": 285.76000000000005, "end": 291.68, "text": " like cross-signing keys, device keys.", "tokens": [411, 3278, 12, 82, 9676, 9317, 11, 4302, 9317, 13], "temperature": 0.0, "avg_logprob": -0.12002548304471103, "compression_ratio": 1.4853801169590644, "no_speech_prob": 4.903434819425456e-05}, {"id": 48, "seek": 29168, "start": 291.68, "end": 298.72, "text": " One goal of developing trixity was the ability to add custom events which are strictly typed.", "tokens": [1485, 3387, 295, 6416, 504, 970, 507, 390, 264, 3485, 281, 909, 2375, 3931, 597, 366, 20792, 33941, 13], "temperature": 0.0, "avg_logprob": -0.1928135518277629, "compression_ratio": 1.6172248803827751, "no_speech_prob": 7.957730849739164e-05}, {"id": 49, "seek": 29168, "start": 298.72, "end": 305.16, "text": " So this is achieved by mapping event types to just a serializer.", "tokens": [407, 341, 307, 11042, 538, 18350, 2280, 3467, 281, 445, 257, 17436, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1928135518277629, "compression_ratio": 1.6172248803827751, "no_speech_prob": 7.957730849739164e-05}, {"id": 50, "seek": 29168, "start": 305.16, "end": 314.0, "text": " In this example, we add a new type of m.room.pum.cat of the Kotlin type cat event content.", "tokens": [682, 341, 1365, 11, 321, 909, 257, 777, 2010, 295, 275, 13, 2861, 13, 79, 449, 13, 18035, 295, 264, 30123, 5045, 2010, 3857, 2280, 2701, 13], "temperature": 0.0, "avg_logprob": -0.1928135518277629, "compression_ratio": 1.6172248803827751, "no_speech_prob": 7.957730849739164e-05}, {"id": 51, "seek": 29168, "start": 314.0, "end": 319.56, "text": " So you have access to all fields of this cat event content and don't have to mess around", "tokens": [407, 291, 362, 2105, 281, 439, 7909, 295, 341, 3857, 2280, 2701, 293, 500, 380, 362, 281, 2082, 926], "temperature": 0.0, "avg_logprob": -0.1928135518277629, "compression_ratio": 1.6172248803827751, "no_speech_prob": 7.957730849739164e-05}, {"id": 52, "seek": 31956, "start": 319.56, "end": 323.72, "text": " with the JSON.", "tokens": [365, 264, 31828, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 53, "seek": 31956, "start": 323.72, "end": 328.24, "text": " The next layer of trixity is the API layer.", "tokens": [440, 958, 4583, 295, 504, 970, 507, 307, 264, 9362, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 54, "seek": 31956, "start": 328.24, "end": 334.24, "text": " Each API has its model which defines all endpoints of the API.", "tokens": [6947, 9362, 575, 1080, 2316, 597, 23122, 439, 917, 20552, 295, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 55, "seek": 31956, "start": 334.24, "end": 339.4, "text": " The actual client and server implementation just use these endpoints.", "tokens": [440, 3539, 6423, 293, 7154, 11420, 445, 764, 613, 917, 20552, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 56, "seek": 31956, "start": 339.4, "end": 344.64, "text": " And so as a consequence, there is no need to define things twice.", "tokens": [400, 370, 382, 257, 18326, 11, 456, 307, 572, 643, 281, 6964, 721, 6091, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 57, "seek": 31956, "start": 344.64, "end": 346.88, "text": " They are using the same Kotlin object.", "tokens": [814, 366, 1228, 264, 912, 30123, 5045, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13002754449844361, "compression_ratio": 1.48, "no_speech_prob": 0.00016831367975100875}, {"id": 58, "seek": 34688, "start": 346.88, "end": 353.68, "text": " So a Kotlin object represents an endpoint on the matrix side, Kotlin class, not Kotlin", "tokens": [407, 257, 30123, 5045, 2657, 8855, 364, 35795, 322, 264, 8141, 1252, 11, 30123, 5045, 1508, 11, 406, 30123, 5045], "temperature": 0.0, "avg_logprob": -0.20323941614720728, "compression_ratio": 1.6292134831460674, "no_speech_prob": 5.81407621211838e-05}, {"id": 59, "seek": 34688, "start": 353.68, "end": 356.12, "text": " object, sorry.", "tokens": [2657, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.20323941614720728, "compression_ratio": 1.6292134831460674, "no_speech_prob": 5.81407621211838e-05}, {"id": 60, "seek": 34688, "start": 356.12, "end": 360.92, "text": " The best way to show this to you is with an example.", "tokens": [440, 1151, 636, 281, 855, 341, 281, 291, 307, 365, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.20323941614720728, "compression_ratio": 1.6292134831460674, "no_speech_prob": 5.81407621211838e-05}, {"id": 61, "seek": 34688, "start": 360.92, "end": 364.24, "text": " This example is the endpoint leave room.", "tokens": [639, 1365, 307, 264, 35795, 1856, 1808, 13], "temperature": 0.0, "avg_logprob": -0.20323941614720728, "compression_ratio": 1.6292134831460674, "no_speech_prob": 5.81407621211838e-05}, {"id": 62, "seek": 34688, "start": 364.24, "end": 373.15999999999997, "text": " You just implement matrix endpoint, give him the types and in this case, unit is the response.", "tokens": [509, 445, 4445, 8141, 35795, 11, 976, 796, 264, 3467, 293, 294, 341, 1389, 11, 4985, 307, 264, 4134, 13], "temperature": 0.0, "avg_logprob": -0.20323941614720728, "compression_ratio": 1.6292134831460674, "no_speech_prob": 5.81407621211838e-05}, {"id": 63, "seek": 37316, "start": 373.16, "end": 380.68, "text": " So we don't get a JSON as response, just a hated HTTP OK or an empty JSON.", "tokens": [407, 321, 500, 380, 483, 257, 31828, 382, 4134, 11, 445, 257, 17398, 33283, 2264, 420, 364, 6707, 31828, 13], "temperature": 0.0, "avg_logprob": -0.22072002985706068, "compression_ratio": 1.5, "no_speech_prob": 4.066003384650685e-05}, {"id": 64, "seek": 37316, "start": 380.68, "end": 388.20000000000005, "text": " And you can also define a request, a URL, HTTP method and all that.", "tokens": [400, 291, 393, 611, 6964, 257, 5308, 11, 257, 12905, 11, 33283, 3170, 293, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.22072002985706068, "compression_ratio": 1.5, "no_speech_prob": 4.066003384650685e-05}, {"id": 65, "seek": 37316, "start": 388.20000000000005, "end": 400.76000000000005, "text": " And you can use this to call, to use a client, a matrix client on the client side to call", "tokens": [400, 291, 393, 764, 341, 281, 818, 11, 281, 764, 257, 6423, 11, 257, 8141, 6423, 322, 264, 6423, 1252, 281, 818], "temperature": 0.0, "avg_logprob": -0.22072002985706068, "compression_ratio": 1.5, "no_speech_prob": 4.066003384650685e-05}, {"id": 66, "seek": 37316, "start": 400.76000000000005, "end": 401.76000000000005, "text": " these endpoints.", "tokens": [613, 917, 20552, 13], "temperature": 0.0, "avg_logprob": -0.22072002985706068, "compression_ratio": 1.5, "no_speech_prob": 4.066003384650685e-05}, {"id": 67, "seek": 40176, "start": 401.76, "end": 406.84, "text": " So you create a leave room object, a request and you get the response.", "tokens": [407, 291, 1884, 257, 1856, 1808, 2657, 11, 257, 5308, 293, 291, 483, 264, 4134, 13], "temperature": 0.0, "avg_logprob": -0.1360193566430973, "compression_ratio": 1.6836158192090396, "no_speech_prob": 7.954750617500395e-05}, {"id": 68, "seek": 40176, "start": 406.84, "end": 408.88, "text": " That's all on the client side.", "tokens": [663, 311, 439, 322, 264, 6423, 1252, 13], "temperature": 0.0, "avg_logprob": -0.1360193566430973, "compression_ratio": 1.6836158192090396, "no_speech_prob": 7.954750617500395e-05}, {"id": 69, "seek": 40176, "start": 408.88, "end": 412.59999999999997, "text": " And the same thing on the server side.", "tokens": [400, 264, 912, 551, 322, 264, 7154, 1252, 13], "temperature": 0.0, "avg_logprob": -0.1360193566430973, "compression_ratio": 1.6836158192090396, "no_speech_prob": 7.954750617500395e-05}, {"id": 70, "seek": 40176, "start": 412.59999999999997, "end": 419.44, "text": " So you define an endpoint, give it the type, you expect as a request and in the context", "tokens": [407, 291, 6964, 364, 35795, 11, 976, 309, 264, 2010, 11, 291, 2066, 382, 257, 5308, 293, 294, 264, 4319], "temperature": 0.0, "avg_logprob": -0.1360193566430973, "compression_ratio": 1.6836158192090396, "no_speech_prob": 7.954750617500395e-05}, {"id": 71, "seek": 40176, "start": 419.44, "end": 427.03999999999996, "text": " object you have access to the request and can answer with a response.", "tokens": [2657, 291, 362, 2105, 281, 264, 5308, 293, 393, 1867, 365, 257, 4134, 13], "temperature": 0.0, "avg_logprob": -0.1360193566430973, "compression_ratio": 1.6836158192090396, "no_speech_prob": 7.954750617500395e-05}, {"id": 72, "seek": 42704, "start": 427.04, "end": 432.36, "text": " To make it a bit more easier for developers, there is a bit of abstraction on top of that.", "tokens": [1407, 652, 309, 257, 857, 544, 3571, 337, 8849, 11, 456, 307, 257, 857, 295, 37765, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 73, "seek": 42704, "start": 432.36, "end": 435.84000000000003, "text": " So you can also just call leave room.", "tokens": [407, 291, 393, 611, 445, 818, 1856, 1808, 13], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 74, "seek": 42704, "start": 435.84000000000003, "end": 440.64000000000004, "text": " So you don't have to know which endpoint are there existing.", "tokens": [407, 291, 500, 380, 362, 281, 458, 597, 35795, 366, 456, 6741, 13], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 75, "seek": 42704, "start": 440.64000000000004, "end": 446.76, "text": " You just type point on your IDE and see, OK, there's a leave room, I can leave a room.", "tokens": [509, 445, 2010, 935, 322, 428, 40930, 293, 536, 11, 2264, 11, 456, 311, 257, 1856, 1808, 11, 286, 393, 1856, 257, 1808, 13], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 76, "seek": 42704, "start": 446.76, "end": 448.64000000000004, "text": " And the same on the server side.", "tokens": [400, 264, 912, 322, 264, 7154, 1252, 13], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 77, "seek": 42704, "start": 448.64000000000004, "end": 454.68, "text": " So you just need to implement an interface and see all your endpoints you have to implement", "tokens": [407, 291, 445, 643, 281, 4445, 364, 9226, 293, 536, 439, 428, 917, 20552, 291, 362, 281, 4445], "temperature": 0.0, "avg_logprob": -0.14907407314977913, "compression_ratio": 1.6919831223628692, "no_speech_prob": 5.9132966271135956e-05}, {"id": 78, "seek": 45468, "start": 454.68, "end": 460.92, "text": " to be a fully-featured matrix server API.", "tokens": [281, 312, 257, 4498, 12, 2106, 1503, 67, 8141, 7154, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2449484986144227, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.00016308337217196822}, {"id": 79, "seek": 45468, "start": 460.92, "end": 466.32, "text": " Regardless of the API, there is Trick City ARM and Trick City Crypto.", "tokens": [25148, 295, 264, 9362, 11, 456, 307, 43367, 4392, 45209, 293, 43367, 4392, 34809, 78, 13], "temperature": 0.0, "avg_logprob": -0.2449484986144227, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.00016308337217196822}, {"id": 80, "seek": 45468, "start": 466.32, "end": 469.68, "text": " Trick City ARM is just a wrapper for a lip-arm.", "tokens": [43367, 4392, 45209, 307, 445, 257, 46906, 337, 257, 8280, 12, 4452, 13], "temperature": 0.0, "avg_logprob": -0.2449484986144227, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.00016308337217196822}, {"id": 81, "seek": 45468, "start": 469.68, "end": 477.64, "text": " As mentioned, a platform-independent implementation doesn't need to worry about the actual platform-specific", "tokens": [1018, 2835, 11, 257, 3663, 12, 471, 4217, 317, 11420, 1177, 380, 643, 281, 3292, 466, 264, 3539, 3663, 12, 29258], "temperature": 0.0, "avg_logprob": -0.2449484986144227, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.00016308337217196822}, {"id": 82, "seek": 45468, "start": 477.64, "end": 478.96000000000004, "text": " implementations.", "tokens": [4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.2449484986144227, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.00016308337217196822}, {"id": 83, "seek": 47896, "start": 478.96, "end": 488.79999999999995, "text": " So you have, when you use Trick City ARM, you don't need to know how lip-arm is accessed.", "tokens": [407, 291, 362, 11, 562, 291, 764, 43367, 4392, 45209, 11, 291, 500, 380, 643, 281, 458, 577, 8280, 12, 4452, 307, 34211, 13], "temperature": 0.0, "avg_logprob": -0.19383452488825872, "compression_ratio": 1.4023668639053255, "no_speech_prob": 4.6060442400630563e-05}, {"id": 84, "seek": 47896, "start": 488.79999999999995, "end": 491.28, "text": " So on the JVM, I use JNA.", "tokens": [407, 322, 264, 508, 53, 44, 11, 286, 764, 508, 5321, 13], "temperature": 0.0, "avg_logprob": -0.19383452488825872, "compression_ratio": 1.4023668639053255, "no_speech_prob": 4.6060442400630563e-05}, {"id": 85, "seek": 47896, "start": 491.28, "end": 500.96, "text": " On JS, this is done via VASM and on native, just see interrupt from Kotlin.", "tokens": [1282, 33063, 11, 341, 307, 1096, 5766, 691, 3160, 44, 293, 322, 8470, 11, 445, 536, 12729, 490, 30123, 5045, 13], "temperature": 0.0, "avg_logprob": -0.19383452488825872, "compression_ratio": 1.4023668639053255, "no_speech_prob": 4.6060442400630563e-05}, {"id": 86, "seek": 47896, "start": 500.96, "end": 503.56, "text": " Lip-arm is also packaged into Trick City ARM.", "tokens": [27475, 12, 4452, 307, 611, 38162, 666, 43367, 4392, 45209, 13], "temperature": 0.0, "avg_logprob": -0.19383452488825872, "compression_ratio": 1.4023668639053255, "no_speech_prob": 4.6060442400630563e-05}, {"id": 87, "seek": 50356, "start": 503.56, "end": 511.32, "text": " So as a developer, you don't need to ship the build-c library.", "tokens": [407, 382, 257, 10754, 11, 291, 500, 380, 643, 281, 5374, 264, 1322, 12, 66, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2652475118637085, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.293720070971176e-05}, {"id": 88, "seek": 50356, "start": 511.32, "end": 514.0, "text": " And it is just loaded automatically.", "tokens": [400, 309, 307, 445, 13210, 6772, 13], "temperature": 0.0, "avg_logprob": -0.2652475118637085, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.293720070971176e-05}, {"id": 89, "seek": 50356, "start": 514.0, "end": 522.56, "text": " So you don't need to init your encryption, like in other libraries, as it's just loaded.", "tokens": [407, 291, 500, 380, 643, 281, 3157, 428, 29575, 11, 411, 294, 661, 15148, 11, 382, 309, 311, 445, 13210, 13], "temperature": 0.0, "avg_logprob": -0.2652475118637085, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.293720070971176e-05}, {"id": 90, "seek": 50356, "start": 522.56, "end": 530.48, "text": " My plan is to migrate that to Vodosemak, but currently, UNIFFI, we heard of that in another", "tokens": [1222, 1393, 307, 281, 31821, 300, 281, 691, 378, 541, 8520, 11, 457, 4362, 11, 8229, 12775, 38568, 11, 321, 2198, 295, 300, 294, 1071], "temperature": 0.0, "avg_logprob": -0.2652475118637085, "compression_ratio": 1.4893617021276595, "no_speech_prob": 6.293720070971176e-05}, {"id": 91, "seek": 53048, "start": 530.48, "end": 534.88, "text": " talk, does not support VASM targets.", "tokens": [751, 11, 775, 406, 1406, 691, 3160, 44, 12911, 13], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 92, "seek": 53048, "start": 534.88, "end": 543.36, "text": " So currently, I can Vodosemak just only use in Kotlin, JVM and native, but I also want", "tokens": [407, 4362, 11, 286, 393, 691, 378, 541, 8520, 445, 787, 764, 294, 30123, 5045, 11, 508, 53, 44, 293, 8470, 11, 457, 286, 611, 528], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 93, "seek": 53048, "start": 543.36, "end": 544.6, "text": " to use JavaScript.", "tokens": [281, 764, 15778, 13], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 94, "seek": 53048, "start": 544.6, "end": 549.04, "text": " So this project is currently on eyes.", "tokens": [407, 341, 1716, 307, 4362, 322, 2575, 13], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 95, "seek": 53048, "start": 549.04, "end": 554.4, "text": " Trick City Crypto currently implements the key management and allows to decrypt and encrypt", "tokens": [43367, 4392, 34809, 78, 4362, 704, 17988, 264, 2141, 4592, 293, 4045, 281, 979, 627, 662, 293, 17972, 662], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 96, "seek": 53048, "start": 554.4, "end": 555.8000000000001, "text": " events.", "tokens": [3931, 13], "temperature": 0.0, "avg_logprob": -0.17161179737872387, "compression_ratio": 1.4659685863874345, "no_speech_prob": 0.0002126709878211841}, {"id": 97, "seek": 55580, "start": 555.8, "end": 561.88, "text": " And in the future, it will be more, so you can reuse the completely, complete crypto", "tokens": [400, 294, 264, 2027, 11, 309, 486, 312, 544, 11, 370, 291, 393, 26225, 264, 2584, 11, 3566, 17240], "temperature": 0.0, "avg_logprob": -0.1938950220743815, "compression_ratio": 1.689119170984456, "no_speech_prob": 4.002812784165144e-05}, {"id": 98, "seek": 55580, "start": 561.88, "end": 568.76, "text": " stuff, for example, in app services.", "tokens": [1507, 11, 337, 1365, 11, 294, 724, 3328, 13], "temperature": 0.0, "avg_logprob": -0.1938950220743815, "compression_ratio": 1.689119170984456, "no_speech_prob": 4.002812784165144e-05}, {"id": 99, "seek": 55580, "start": 568.76, "end": 573.1999999999999, "text": " Trick City Client allows you to, oh, sorry.", "tokens": [43367, 4392, 2033, 1196, 4045, 291, 281, 11, 1954, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.1938950220743815, "compression_ratio": 1.689119170984456, "no_speech_prob": 4.002812784165144e-05}, {"id": 100, "seek": 55580, "start": 573.1999999999999, "end": 580.16, "text": " The most abstract layer are Trick City Client and Trick City App Service.", "tokens": [440, 881, 12649, 4583, 366, 43367, 4392, 2033, 1196, 293, 43367, 4392, 3132, 9561, 13], "temperature": 0.0, "avg_logprob": -0.1938950220743815, "compression_ratio": 1.689119170984456, "no_speech_prob": 4.002812784165144e-05}, {"id": 101, "seek": 55580, "start": 580.16, "end": 584.8399999999999, "text": " While Trick City App Service is still very basic and does not have a persistent layer,", "tokens": [3987, 43367, 4392, 3132, 9561, 307, 920, 588, 3875, 293, 775, 406, 362, 257, 24315, 4583, 11], "temperature": 0.0, "avg_logprob": -0.1938950220743815, "compression_ratio": 1.689119170984456, "no_speech_prob": 4.002812784165144e-05}, {"id": 102, "seek": 58484, "start": 584.84, "end": 589.84, "text": " Trick City Client allows you to choose which database and which media store implementation", "tokens": [43367, 4392, 2033, 1196, 4045, 291, 281, 2826, 597, 8149, 293, 597, 3021, 3531, 11420], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 103, "seek": 58484, "start": 589.84, "end": 591.96, "text": " you want to use.", "tokens": [291, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 104, "seek": 58484, "start": 591.96, "end": 596.84, "text": " And on top of that, there is something that isn't released yet.", "tokens": [400, 322, 1192, 295, 300, 11, 456, 307, 746, 300, 1943, 380, 4736, 1939, 13], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 105, "seek": 58484, "start": 596.84, "end": 604.0, "text": " We are not sure how to release, because we have to make money with our company.", "tokens": [492, 366, 406, 988, 577, 281, 4374, 11, 570, 321, 362, 281, 652, 1460, 365, 527, 2237, 13], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 106, "seek": 58484, "start": 604.0, "end": 605.9200000000001, "text": " It is Trick City Messenger.", "tokens": [467, 307, 43367, 4392, 34226, 13], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 107, "seek": 58484, "start": 605.9200000000001, "end": 611.0, "text": " This is just the view model representation of a messenger.", "tokens": [639, 307, 445, 264, 1910, 2316, 10290, 295, 257, 26599, 13], "temperature": 0.0, "avg_logprob": -0.12707556997026717, "compression_ratio": 1.54337899543379, "no_speech_prob": 5.915373913012445e-05}, {"id": 108, "seek": 61100, "start": 611.0, "end": 617.84, "text": " So you only have to implement a thin UI layer, where when the user clicks the button, the", "tokens": [407, 291, 787, 362, 281, 4445, 257, 5862, 15682, 4583, 11, 689, 562, 264, 4195, 18521, 264, 2960, 11, 264], "temperature": 0.0, "avg_logprob": -0.16531721353530884, "compression_ratio": 1.538860103626943, "no_speech_prob": 4.828642340726219e-05}, {"id": 109, "seek": 61100, "start": 617.84, "end": 625.88, "text": " UI sends this to the view model, and the view model says, OK, send a message, or go to", "tokens": [15682, 14790, 341, 281, 264, 1910, 2316, 11, 293, 264, 1910, 2316, 1619, 11, 2264, 11, 2845, 257, 3636, 11, 420, 352, 281], "temperature": 0.0, "avg_logprob": -0.16531721353530884, "compression_ratio": 1.538860103626943, "no_speech_prob": 4.828642340726219e-05}, {"id": 110, "seek": 61100, "start": 625.88, "end": 629.12, "text": " this room, or any other stuff.", "tokens": [341, 1808, 11, 420, 604, 661, 1507, 13], "temperature": 0.0, "avg_logprob": -0.16531721353530884, "compression_ratio": 1.538860103626943, "no_speech_prob": 4.828642340726219e-05}, {"id": 111, "seek": 61100, "start": 629.12, "end": 640.68, "text": " And with this approach, we have implemented an iOS client in a few weeks with one person.", "tokens": [400, 365, 341, 3109, 11, 321, 362, 12270, 364, 17430, 6423, 294, 257, 1326, 3259, 365, 472, 954, 13], "temperature": 0.0, "avg_logprob": -0.16531721353530884, "compression_ratio": 1.538860103626943, "no_speech_prob": 4.828642340726219e-05}, {"id": 112, "seek": 64068, "start": 640.68, "end": 646.0, "text": " So Trick City Client allows you to implement a fully-featured Matrix client or bot.", "tokens": [407, 43367, 4392, 2033, 1196, 4045, 291, 281, 4445, 257, 4498, 12, 2106, 1503, 67, 36274, 6423, 420, 10592, 13], "temperature": 0.0, "avg_logprob": -0.15214488817297894, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.0039853047346696e-05}, {"id": 113, "seek": 64068, "start": 646.0, "end": 655.92, "text": " So if you were at the Matrix Rust SDK talk, you can just use their representation and", "tokens": [407, 498, 291, 645, 412, 264, 36274, 34952, 37135, 751, 11, 291, 393, 445, 764, 641, 10290, 293], "temperature": 0.0, "avg_logprob": -0.15214488817297894, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.0039853047346696e-05}, {"id": 114, "seek": 64068, "start": 655.92, "end": 658.7199999999999, "text": " instead of Rust, you write Kotlin.", "tokens": [2602, 295, 34952, 11, 291, 2464, 30123, 5045, 13], "temperature": 0.0, "avg_logprob": -0.15214488817297894, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.0039853047346696e-05}, {"id": 115, "seek": 64068, "start": 658.7199999999999, "end": 665.5999999999999, "text": " So everything that Matrix Rust SDK does also does Trick City.", "tokens": [407, 1203, 300, 36274, 34952, 37135, 775, 611, 775, 43367, 4392, 13], "temperature": 0.0, "avg_logprob": -0.15214488817297894, "compression_ratio": 1.5465116279069768, "no_speech_prob": 4.0039853047346696e-05}, {"id": 116, "seek": 66560, "start": 665.6, "end": 673.0400000000001, "text": " Some features like Sliding Zinc aren't there, because we want to follow the stable Matrix", "tokens": [2188, 4122, 411, 6187, 2819, 1176, 4647, 3212, 380, 456, 11, 570, 321, 528, 281, 1524, 264, 8351, 36274], "temperature": 0.0, "avg_logprob": -0.2275618314743042, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.748065349413082e-05}, {"id": 117, "seek": 66560, "start": 673.0400000000001, "end": 679.24, "text": " specs, so we don't implement any MSCs.", "tokens": [27911, 11, 370, 321, 500, 380, 4445, 604, 7395, 33290, 13], "temperature": 0.0, "avg_logprob": -0.2275618314743042, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.748065349413082e-05}, {"id": 118, "seek": 66560, "start": 679.24, "end": 686.5600000000001, "text": " So we have all the E2E features, the exchangeability data stores and media stores, we have reactive", "tokens": [407, 321, 362, 439, 264, 462, 17, 36, 4122, 11, 264, 7742, 2310, 1412, 9512, 293, 3021, 9512, 11, 321, 362, 28897], "temperature": 0.0, "avg_logprob": -0.2275618314743042, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.748065349413082e-05}, {"id": 119, "seek": 66560, "start": 686.5600000000001, "end": 692.44, "text": " cache on top of that, notification, thumbnail generation, all that stuff you need to implement", "tokens": [19459, 322, 1192, 295, 300, 11, 11554, 11, 26746, 5125, 11, 439, 300, 1507, 291, 643, 281, 4445], "temperature": 0.0, "avg_logprob": -0.2275618314743042, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.748065349413082e-05}, {"id": 120, "seek": 69244, "start": 692.44, "end": 696.08, "text": " the client.", "tokens": [264, 6423, 13], "temperature": 0.0, "avg_logprob": -0.3056765556335449, "compression_ratio": 1.3941176470588235, "no_speech_prob": 7.719725545030087e-05}, {"id": 121, "seek": 69244, "start": 696.08, "end": 705.36, "text": " There are already some media store wrappers that we implemented for all targets, targets", "tokens": [821, 366, 1217, 512, 3021, 3531, 7843, 15226, 300, 321, 12270, 337, 439, 12911, 11, 12911], "temperature": 0.0, "avg_logprob": -0.3056765556335449, "compression_ratio": 1.3941176470588235, "no_speech_prob": 7.719725545030087e-05}, {"id": 122, "seek": 69244, "start": 705.36, "end": 714.9200000000001, "text": " expect browsers, we just use the FI system and on browsers we use indexDB.", "tokens": [2066, 36069, 11, 321, 445, 764, 264, 479, 40, 1185, 293, 322, 36069, 321, 764, 8186, 27735, 13], "temperature": 0.0, "avg_logprob": -0.3056765556335449, "compression_ratio": 1.3941176470588235, "no_speech_prob": 7.719725545030087e-05}, {"id": 123, "seek": 69244, "start": 714.9200000000001, "end": 719.44, "text": " Next I want to talk about how I accidentally created a cache.", "tokens": [3087, 286, 528, 281, 751, 466, 577, 286, 15715, 2942, 257, 19459, 13], "temperature": 0.0, "avg_logprob": -0.3056765556335449, "compression_ratio": 1.3941176470588235, "no_speech_prob": 7.719725545030087e-05}, {"id": 124, "seek": 71944, "start": 719.44, "end": 725.0400000000001, "text": " So on the left side you see the relation between the UI, Trick City and the storage", "tokens": [407, 322, 264, 1411, 1252, 291, 536, 264, 9721, 1296, 264, 15682, 11, 43367, 4392, 293, 264, 6725], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 125, "seek": 71944, "start": 725.0400000000001, "end": 726.72, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 126, "seek": 71944, "start": 726.72, "end": 731.1600000000001, "text": " And because reactive UIs are really common, I wanted Trick City to give the UI access", "tokens": [400, 570, 28897, 624, 6802, 366, 534, 2689, 11, 286, 1415, 43367, 4392, 281, 976, 264, 15682, 2105], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 127, "seek": 71944, "start": 731.1600000000001, "end": 733.84, "text": " to the data in a reactive way.", "tokens": [281, 264, 1412, 294, 257, 28897, 636, 13], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 128, "seek": 71944, "start": 733.84, "end": 740.5600000000001, "text": " So if anything changes, the UI should immediately know about this.", "tokens": [407, 498, 1340, 2962, 11, 264, 15682, 820, 4258, 458, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 129, "seek": 71944, "start": 740.5600000000001, "end": 742.32, "text": " But the question is how?", "tokens": [583, 264, 1168, 307, 577, 30], "temperature": 0.0, "avg_logprob": -0.14530109453804885, "compression_ratio": 1.541237113402062, "no_speech_prob": 6.200138886924833e-05}, {"id": 130, "seek": 74232, "start": 742.32, "end": 750.7600000000001, "text": " On the one hand there are a few databases which support listeners to react to changes", "tokens": [1282, 264, 472, 1011, 456, 366, 257, 1326, 22380, 597, 1406, 23274, 281, 4515, 281, 2962], "temperature": 0.0, "avg_logprob": -0.11827456061519794, "compression_ratio": 1.668421052631579, "no_speech_prob": 0.00014399943756870925}, {"id": 131, "seek": 74232, "start": 750.7600000000001, "end": 758.72, "text": " to the database, but on the other hand this would limit support for multiple supported", "tokens": [281, 264, 8149, 11, 457, 322, 264, 661, 1011, 341, 576, 4948, 1406, 337, 3866, 8104], "temperature": 0.0, "avg_logprob": -0.11827456061519794, "compression_ratio": 1.668421052631579, "no_speech_prob": 0.00014399943756870925}, {"id": 132, "seek": 74232, "start": 758.72, "end": 764.08, "text": " databases because finding a common interface for listeners would be hard.", "tokens": [22380, 570, 5006, 257, 2689, 9226, 337, 23274, 576, 312, 1152, 13], "temperature": 0.0, "avg_logprob": -0.11827456061519794, "compression_ratio": 1.668421052631579, "no_speech_prob": 0.00014399943756870925}, {"id": 133, "seek": 74232, "start": 764.08, "end": 770.96, "text": " So I started implementing an intermediate layer based on Kotlin flows.", "tokens": [407, 286, 1409, 18114, 364, 19376, 4583, 2361, 322, 30123, 5045, 12867, 13], "temperature": 0.0, "avg_logprob": -0.11827456061519794, "compression_ratio": 1.668421052631579, "no_speech_prob": 0.00014399943756870925}, {"id": 134, "seek": 77096, "start": 770.96, "end": 774.52, "text": " The flow in Kotlin is a reactive data structure.", "tokens": [440, 3095, 294, 30123, 5045, 307, 257, 28897, 1412, 3877, 13], "temperature": 0.0, "avg_logprob": -0.1569946152823312, "compression_ratio": 1.6032608695652173, "no_speech_prob": 3.115669096587226e-05}, {"id": 135, "seek": 77096, "start": 774.52, "end": 779.64, "text": " So you have a producer on the one side and a consumer on the other side.", "tokens": [407, 291, 362, 257, 12314, 322, 264, 472, 1252, 293, 257, 9711, 322, 264, 661, 1252, 13], "temperature": 0.0, "avg_logprob": -0.1569946152823312, "compression_ratio": 1.6032608695652173, "no_speech_prob": 3.115669096587226e-05}, {"id": 136, "seek": 77096, "start": 779.64, "end": 788.8000000000001, "text": " So if the producer changes anything, the consumers immediately know about that.", "tokens": [407, 498, 264, 12314, 2962, 1340, 11, 264, 11883, 4258, 458, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1569946152823312, "compression_ratio": 1.6032608695652173, "no_speech_prob": 3.115669096587226e-05}, {"id": 137, "seek": 77096, "start": 788.8000000000001, "end": 796.2800000000001, "text": " And what does the intermediate layer, it talks to a very thin database layer which only knows", "tokens": [400, 437, 775, 264, 19376, 4583, 11, 309, 6686, 281, 257, 588, 5862, 8149, 4583, 597, 787, 3255], "temperature": 0.0, "avg_logprob": -0.1569946152823312, "compression_ratio": 1.6032608695652173, "no_speech_prob": 3.115669096587226e-05}, {"id": 138, "seek": 79628, "start": 796.28, "end": 801.4399999999999, "text": " about save, read and delete data.", "tokens": [466, 3155, 11, 1401, 293, 12097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11714966156903435, "compression_ratio": 1.7973856209150327, "no_speech_prob": 7.955361797939986e-05}, {"id": 139, "seek": 79628, "start": 801.4399999999999, "end": 809.36, "text": " And if someone wants data from this layer, it just reads it from the database or if someone", "tokens": [400, 498, 1580, 2738, 1412, 490, 341, 4583, 11, 309, 445, 15700, 309, 490, 264, 8149, 420, 498, 1580], "temperature": 0.0, "avg_logprob": -0.11714966156903435, "compression_ratio": 1.7973856209150327, "no_speech_prob": 7.955361797939986e-05}, {"id": 140, "seek": 79628, "start": 809.36, "end": 816.0, "text": " changes something in this layer, it just writes it to the database.", "tokens": [2962, 746, 294, 341, 4583, 11, 309, 445, 13657, 309, 281, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.11714966156903435, "compression_ratio": 1.7973856209150327, "no_speech_prob": 7.955361797939986e-05}, {"id": 141, "seek": 79628, "start": 816.0, "end": 821.64, "text": " And the values are kept in this layer as long as they are subscribed from anyone.", "tokens": [400, 264, 4190, 366, 4305, 294, 341, 4583, 382, 938, 382, 436, 366, 16665, 490, 2878, 13], "temperature": 0.0, "avg_logprob": -0.11714966156903435, "compression_ratio": 1.7973856209150327, "no_speech_prob": 7.955361797939986e-05}, {"id": 142, "seek": 82164, "start": 821.64, "end": 828.6, "text": " So this means that if anyone else subscribes to a value, he will immediately get the current", "tokens": [407, 341, 1355, 300, 498, 2878, 1646, 2325, 6446, 281, 257, 2158, 11, 415, 486, 4258, 483, 264, 2190], "temperature": 0.0, "avg_logprob": -0.09635091145833333, "compression_ratio": 1.6237113402061856, "no_speech_prob": 8.205424819607288e-05}, {"id": 143, "seek": 82164, "start": 828.6, "end": 833.64, "text": " value because there is no additional database call needed because it is persisted in the", "tokens": [2158, 570, 456, 307, 572, 4497, 8149, 818, 2978, 570, 309, 307, 13233, 292, 294, 264], "temperature": 0.0, "avg_logprob": -0.09635091145833333, "compression_ratio": 1.6237113402061856, "no_speech_prob": 8.205424819607288e-05}, {"id": 144, "seek": 82164, "start": 833.64, "end": 836.0, "text": " intermediate layer.", "tokens": [19376, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09635091145833333, "compression_ratio": 1.6237113402061856, "no_speech_prob": 8.205424819607288e-05}, {"id": 145, "seek": 82164, "start": 836.0, "end": 841.4, "text": " This goes so far that even if there are no subscribers anymore, I just keep the value", "tokens": [639, 1709, 370, 1400, 300, 754, 498, 456, 366, 572, 11092, 3602, 11, 286, 445, 1066, 264, 2158], "temperature": 0.0, "avg_logprob": -0.09635091145833333, "compression_ratio": 1.6237113402061856, "no_speech_prob": 8.205424819607288e-05}, {"id": 146, "seek": 82164, "start": 841.4, "end": 843.68, "text": " a bit longer in this layer.", "tokens": [257, 857, 2854, 294, 341, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09635091145833333, "compression_ratio": 1.6237113402061856, "no_speech_prob": 8.205424819607288e-05}, {"id": 147, "seek": 84368, "start": 843.68, "end": 851.52, "text": " So if someone asks for a value for example 10 seconds later and the value is still stored,", "tokens": [407, 498, 1580, 8962, 337, 257, 2158, 337, 1365, 1266, 3949, 1780, 293, 264, 2158, 307, 920, 12187, 11], "temperature": 0.0, "avg_logprob": -0.19198202751052212, "compression_ratio": 1.4808743169398908, "no_speech_prob": 3.586133971111849e-05}, {"id": 148, "seek": 84368, "start": 851.52, "end": 860.92, "text": " he gets the value and there is no database call needed.", "tokens": [415, 2170, 264, 2158, 293, 456, 307, 572, 8149, 818, 2978, 13], "temperature": 0.0, "avg_logprob": -0.19198202751052212, "compression_ratio": 1.4808743169398908, "no_speech_prob": 3.586133971111849e-05}, {"id": 149, "seek": 84368, "start": 860.92, "end": 868.0, "text": " And you can now guess what I implemented, it's just cache.", "tokens": [400, 291, 393, 586, 2041, 437, 286, 12270, 11, 309, 311, 445, 19459, 13], "temperature": 0.0, "avg_logprob": -0.19198202751052212, "compression_ratio": 1.4808743169398908, "no_speech_prob": 3.586133971111849e-05}, {"id": 150, "seek": 84368, "start": 868.0, "end": 872.76, "text": " So as you see with this cache, everything in Trixity is reactive.", "tokens": [407, 382, 291, 536, 365, 341, 19459, 11, 1203, 294, 314, 6579, 507, 307, 28897, 13], "temperature": 0.0, "avg_logprob": -0.19198202751052212, "compression_ratio": 1.4808743169398908, "no_speech_prob": 3.586133971111849e-05}, {"id": 151, "seek": 87276, "start": 872.76, "end": 878.88, "text": " These are just a few examples, so you can just get all users or check if a user can", "tokens": [1981, 366, 445, 257, 1326, 5110, 11, 370, 291, 393, 445, 483, 439, 5022, 420, 1520, 498, 257, 4195, 393], "temperature": 0.0, "avg_logprob": -0.14224831532623808, "compression_ratio": 1.5393939393939393, "no_speech_prob": 6.498344737337902e-05}, {"id": 152, "seek": 87276, "start": 878.88, "end": 890.92, "text": " invite another one, you immediately get the notification if anything has changed.", "tokens": [7980, 1071, 472, 11, 291, 4258, 483, 264, 11554, 498, 1340, 575, 3105, 13], "temperature": 0.0, "avg_logprob": -0.14224831532623808, "compression_ratio": 1.5393939393939393, "no_speech_prob": 6.498344737337902e-05}, {"id": 153, "seek": 87276, "start": 890.92, "end": 897.72, "text": " As mentioned, the database layer is really thin, so we implemented many database layers.", "tokens": [1018, 2835, 11, 264, 8149, 4583, 307, 534, 5862, 11, 370, 321, 12270, 867, 8149, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14224831532623808, "compression_ratio": 1.5393939393939393, "no_speech_prob": 6.498344737337902e-05}, {"id": 154, "seek": 89772, "start": 897.72, "end": 904.0400000000001, "text": " So SQL-based one, we are via exposed for JVM-based targets.", "tokens": [407, 19200, 12, 6032, 472, 11, 321, 366, 5766, 9495, 337, 508, 53, 44, 12, 6032, 12911, 13], "temperature": 0.0, "avg_logprob": -0.28766005039215087, "compression_ratio": 1.6, "no_speech_prob": 3.216287586838007e-05}, {"id": 155, "seek": 89772, "start": 904.0400000000001, "end": 911.44, "text": " We implemented one with Realm that can be also used on native targets like iOS and for", "tokens": [492, 12270, 472, 365, 44723, 300, 393, 312, 611, 1143, 322, 8470, 12911, 411, 17430, 293, 337], "temperature": 0.0, "avg_logprob": -0.28766005039215087, "compression_ratio": 1.6, "no_speech_prob": 3.216287586838007e-05}, {"id": 156, "seek": 89772, "start": 911.44, "end": 916.72, "text": " browsers we have IndexedDB.", "tokens": [36069, 321, 362, 33552, 292, 27735, 13], "temperature": 0.0, "avg_logprob": -0.28766005039215087, "compression_ratio": 1.6, "no_speech_prob": 3.216287586838007e-05}, {"id": 157, "seek": 89772, "start": 916.72, "end": 923.96, "text": " So most of the data changes, when a Zinc is processed, most of the data changes when a", "tokens": [407, 881, 295, 264, 1412, 2962, 11, 562, 257, 1176, 4647, 307, 18846, 11, 881, 295, 264, 1412, 2962, 562, 257], "temperature": 0.0, "avg_logprob": -0.28766005039215087, "compression_ratio": 1.6, "no_speech_prob": 3.216287586838007e-05}, {"id": 158, "seek": 89772, "start": 923.96, "end": 925.24, "text": " Zinc is processed.", "tokens": [1176, 4647, 307, 18846, 13], "temperature": 0.0, "avg_logprob": -0.28766005039215087, "compression_ratio": 1.6, "no_speech_prob": 3.216287586838007e-05}, {"id": 159, "seek": 92524, "start": 925.24, "end": 933.8, "text": " So it is way more performant to make a large transaction around the Zinc.", "tokens": [407, 309, 307, 636, 544, 2042, 394, 281, 652, 257, 2416, 14425, 926, 264, 1176, 4647, 13], "temperature": 0.0, "avg_logprob": -0.18861838952818913, "compression_ratio": 1.675, "no_speech_prob": 9.591288107912987e-05}, {"id": 160, "seek": 92524, "start": 933.8, "end": 940.6, "text": " So you don't have a transaction, every time the cache writes something into the database,", "tokens": [407, 291, 500, 380, 362, 257, 14425, 11, 633, 565, 264, 19459, 13657, 746, 666, 264, 8149, 11], "temperature": 0.0, "avg_logprob": -0.18861838952818913, "compression_ratio": 1.675, "no_speech_prob": 9.591288107912987e-05}, {"id": 161, "seek": 92524, "start": 940.6, "end": 945.8, "text": " Trixity just spends a large transaction around Zinc, so you have thousands of writes in", "tokens": [314, 6579, 507, 445, 25620, 257, 2416, 14425, 926, 1176, 4647, 11, 370, 291, 362, 5383, 295, 13657, 294], "temperature": 0.0, "avg_logprob": -0.18861838952818913, "compression_ratio": 1.675, "no_speech_prob": 9.591288107912987e-05}, {"id": 162, "seek": 92524, "start": 945.8, "end": 948.52, "text": " one transaction.", "tokens": [472, 14425, 13], "temperature": 0.0, "avg_logprob": -0.18861838952818913, "compression_ratio": 1.675, "no_speech_prob": 9.591288107912987e-05}, {"id": 163, "seek": 94852, "start": 948.52, "end": 956.8, "text": " So everything fine, no, then there was Realm, and Realm is just a really fast database,", "tokens": [407, 1203, 2489, 11, 572, 11, 550, 456, 390, 44723, 11, 293, 44723, 307, 445, 257, 534, 2370, 8149, 11], "temperature": 0.0, "avg_logprob": -0.12808698992575368, "compression_ratio": 1.64, "no_speech_prob": 3.643945819931105e-05}, {"id": 164, "seek": 94852, "start": 956.8, "end": 962.64, "text": " but Realm only allows one write transaction at the time.", "tokens": [457, 44723, 787, 4045, 472, 2464, 14425, 412, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.12808698992575368, "compression_ratio": 1.64, "no_speech_prob": 3.643945819931105e-05}, {"id": 165, "seek": 94852, "start": 962.64, "end": 968.92, "text": " So if another one wants to write to the database, he needs to wait until the first transaction", "tokens": [407, 498, 1071, 472, 2738, 281, 2464, 281, 264, 8149, 11, 415, 2203, 281, 1699, 1826, 264, 700, 14425], "temperature": 0.0, "avg_logprob": -0.12808698992575368, "compression_ratio": 1.64, "no_speech_prob": 3.643945819931105e-05}, {"id": 166, "seek": 94852, "start": 968.92, "end": 971.16, "text": " ended.", "tokens": [4590, 13], "temperature": 0.0, "avg_logprob": -0.12808698992575368, "compression_ratio": 1.64, "no_speech_prob": 3.643945819931105e-05}, {"id": 167, "seek": 97116, "start": 971.16, "end": 979.9599999999999, "text": " And the problem is that while the Zinc is running, it may be needed that we have to", "tokens": [400, 264, 1154, 307, 300, 1339, 264, 1176, 4647, 307, 2614, 11, 309, 815, 312, 2978, 300, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.13317993565609582, "compression_ratio": 1.7525773195876289, "no_speech_prob": 2.5044961148523726e-05}, {"id": 168, "seek": 97116, "start": 979.9599999999999, "end": 985.16, "text": " wait for outdated keys to be updated to decrypt on stuff.", "tokens": [1699, 337, 36313, 9317, 281, 312, 10588, 281, 979, 627, 662, 322, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13317993565609582, "compression_ratio": 1.7525773195876289, "no_speech_prob": 2.5044961148523726e-05}, {"id": 169, "seek": 97116, "start": 985.16, "end": 991.76, "text": " So if the outdated keys part of Trixity want to write something in the database, he needs", "tokens": [407, 498, 264, 36313, 9317, 644, 295, 314, 6579, 507, 528, 281, 2464, 746, 294, 264, 8149, 11, 415, 2203], "temperature": 0.0, "avg_logprob": -0.13317993565609582, "compression_ratio": 1.7525773195876289, "no_speech_prob": 2.5044961148523726e-05}, {"id": 170, "seek": 97116, "start": 991.76, "end": 996.52, "text": " to wait until the Zinc is ended, but the Zinc waits for the keys to be updated, so we have", "tokens": [281, 1699, 1826, 264, 1176, 4647, 307, 4590, 11, 457, 264, 1176, 4647, 40597, 337, 264, 9317, 281, 312, 10588, 11, 370, 321, 362], "temperature": 0.0, "avg_logprob": -0.13317993565609582, "compression_ratio": 1.7525773195876289, "no_speech_prob": 2.5044961148523726e-05}, {"id": 171, "seek": 97116, "start": 996.52, "end": 1001.0, "text": " a deadlock there.", "tokens": [257, 3116, 4102, 456, 13], "temperature": 0.0, "avg_logprob": -0.13317993565609582, "compression_ratio": 1.7525773195876289, "no_speech_prob": 2.5044961148523726e-05}, {"id": 172, "seek": 100100, "start": 1001.0, "end": 1006.44, "text": " This is one of the reasons why I introduced Azure Zinc transactions.", "tokens": [639, 307, 472, 295, 264, 4112, 983, 286, 7268, 11969, 1176, 4647, 16856, 13], "temperature": 0.0, "avg_logprob": -0.11961539773380055, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.424382521188818e-05}, {"id": 173, "seek": 100100, "start": 1006.44, "end": 1012.64, "text": " The other reason was that most of the time the Zinc processing, as I find out with some", "tokens": [440, 661, 1778, 390, 300, 881, 295, 264, 565, 264, 1176, 4647, 9007, 11, 382, 286, 915, 484, 365, 512], "temperature": 0.0, "avg_logprob": -0.11961539773380055, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.424382521188818e-05}, {"id": 174, "seek": 100100, "start": 1012.64, "end": 1017.16, "text": " benchmark, was wasted due to writing to the database.", "tokens": [18927, 11, 390, 19496, 3462, 281, 3579, 281, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.11961539773380055, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.424382521188818e-05}, {"id": 175, "seek": 100100, "start": 1017.16, "end": 1024.64, "text": " So processing a Zinc takes a long time because there are so many IO operations that the user", "tokens": [407, 9007, 257, 1176, 4647, 2516, 257, 938, 565, 570, 456, 366, 370, 867, 39839, 7705, 300, 264, 4195], "temperature": 0.0, "avg_logprob": -0.11961539773380055, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.424382521188818e-05}, {"id": 176, "seek": 100100, "start": 1024.64, "end": 1030.34, "text": " have to wait until all operations are done.", "tokens": [362, 281, 1699, 1826, 439, 7705, 366, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11961539773380055, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.424382521188818e-05}, {"id": 177, "seek": 103034, "start": 1030.34, "end": 1038.1999999999998, "text": " So what does a Zinc transaction in Trixity mean that all changes to the database are", "tokens": [407, 437, 775, 257, 1176, 4647, 14425, 294, 314, 6579, 507, 914, 300, 439, 2962, 281, 264, 8149, 366], "temperature": 0.0, "avg_logprob": -0.17519289255142212, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.319679672131315e-05}, {"id": 178, "seek": 103034, "start": 1038.1999999999998, "end": 1041.84, "text": " collected and processed in the background?", "tokens": [11087, 293, 18846, 294, 264, 3678, 30], "temperature": 0.0, "avg_logprob": -0.17519289255142212, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.319679672131315e-05}, {"id": 179, "seek": 103034, "start": 1041.84, "end": 1049.84, "text": " So database operations are decoupled from the cache layer, and they are just written", "tokens": [407, 8149, 7705, 366, 979, 263, 15551, 490, 264, 19459, 4583, 11, 293, 436, 366, 445, 3720], "temperature": 0.0, "avg_logprob": -0.17519289255142212, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.319679672131315e-05}, {"id": 180, "seek": 103034, "start": 1049.84, "end": 1051.1999999999998, "text": " in the background.", "tokens": [294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.17519289255142212, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.319679672131315e-05}, {"id": 181, "seek": 103034, "start": 1051.1999999999998, "end": 1057.08, "text": " If everything fails, it is war-backed, but that's irrelevant in the normal use case.", "tokens": [759, 1203, 18199, 11, 309, 307, 1516, 12, 3207, 292, 11, 457, 300, 311, 28682, 294, 264, 2710, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17519289255142212, "compression_ratio": 1.5566502463054188, "no_speech_prob": 3.319679672131315e-05}, {"id": 182, "seek": 105708, "start": 1057.08, "end": 1065.12, "text": " So we can process even more Zincs at the same time as if we would wait that the transaction", "tokens": [407, 321, 393, 1399, 754, 544, 1176, 4647, 82, 412, 264, 912, 565, 382, 498, 321, 576, 1699, 300, 264, 14425], "temperature": 0.0, "avg_logprob": -0.21580281522538927, "compression_ratio": 1.3968253968253967, "no_speech_prob": 7.128724246285856e-05}, {"id": 183, "seek": 105708, "start": 1065.12, "end": 1068.0, "text": " has finished.", "tokens": [575, 4335, 13], "temperature": 0.0, "avg_logprob": -0.21580281522538927, "compression_ratio": 1.3968253968253967, "no_speech_prob": 7.128724246285856e-05}, {"id": 184, "seek": 105708, "start": 1068.0, "end": 1071.0, "text": " And this gave Trixity a huge performance boost.", "tokens": [400, 341, 2729, 314, 6579, 507, 257, 2603, 3389, 9194, 13], "temperature": 0.0, "avg_logprob": -0.21580281522538927, "compression_ratio": 1.3968253968253967, "no_speech_prob": 7.128724246285856e-05}, {"id": 185, "seek": 105708, "start": 1071.0, "end": 1081.1999999999998, "text": " Actually I released it last week, and I've wrote an integration test which just fails", "tokens": [5135, 286, 4736, 309, 1036, 1243, 11, 293, 286, 600, 4114, 364, 10980, 1500, 597, 445, 18199], "temperature": 0.0, "avg_logprob": -0.21580281522538927, "compression_ratio": 1.3968253968253967, "no_speech_prob": 7.128724246285856e-05}, {"id": 186, "seek": 105708, "start": 1081.1999999999998, "end": 1084.6799999999998, "text": " if it is not 50% faster.", "tokens": [498, 309, 307, 406, 2625, 4, 4663, 13], "temperature": 0.0, "avg_logprob": -0.21580281522538927, "compression_ratio": 1.3968253968253967, "no_speech_prob": 7.128724246285856e-05}, {"id": 187, "seek": 108468, "start": 1084.68, "end": 1091.1200000000001, "text": " So it is always green, I don't know.", "tokens": [407, 309, 307, 1009, 3092, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1459441609901957, "compression_ratio": 1.6372093023255814, "no_speech_prob": 8.335707389051095e-05}, {"id": 188, "seek": 108468, "start": 1091.1200000000001, "end": 1095.72, "text": " The next thing I did completely different in Trixity are timelines.", "tokens": [440, 958, 551, 286, 630, 2584, 819, 294, 314, 6579, 507, 366, 45886, 13], "temperature": 0.0, "avg_logprob": -0.1459441609901957, "compression_ratio": 1.6372093023255814, "no_speech_prob": 8.335707389051095e-05}, {"id": 189, "seek": 108468, "start": 1095.72, "end": 1102.04, "text": " So normally Zincs are sent as fragment from the server to the client.", "tokens": [407, 5646, 1176, 4647, 82, 366, 2279, 382, 26424, 490, 264, 7154, 281, 264, 6423, 13], "temperature": 0.0, "avg_logprob": -0.1459441609901957, "compression_ratio": 1.6372093023255814, "no_speech_prob": 8.335707389051095e-05}, {"id": 190, "seek": 108468, "start": 1102.04, "end": 1109.28, "text": " So one fragment contains a few timeline events, and if there's a gap, you get a token.", "tokens": [407, 472, 26424, 8306, 257, 1326, 12933, 3931, 11, 293, 498, 456, 311, 257, 7417, 11, 291, 483, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1459441609901957, "compression_ratio": 1.6372093023255814, "no_speech_prob": 8.335707389051095e-05}, {"id": 191, "seek": 108468, "start": 1109.28, "end": 1114.28, "text": " So you know as a client, okay, there's a gap, I need to fetch to fill that gap, and so on.", "tokens": [407, 291, 458, 382, 257, 6423, 11, 1392, 11, 456, 311, 257, 7417, 11, 286, 643, 281, 23673, 281, 2836, 300, 7417, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1459441609901957, "compression_ratio": 1.6372093023255814, "no_speech_prob": 8.335707389051095e-05}, {"id": 192, "seek": 111428, "start": 1114.28, "end": 1121.16, "text": " And these fragments normally are saved as is to the database in clients.", "tokens": [400, 613, 29197, 5646, 366, 6624, 382, 307, 281, 264, 8149, 294, 6982, 13], "temperature": 0.0, "avg_logprob": -0.13599474955413302, "compression_ratio": 1.4736842105263157, "no_speech_prob": 6.705216219415888e-05}, {"id": 193, "seek": 111428, "start": 1121.16, "end": 1125.76, "text": " In Trixity, I use another approach.", "tokens": [682, 314, 6579, 507, 11, 286, 764, 1071, 3109, 13], "temperature": 0.0, "avg_logprob": -0.13599474955413302, "compression_ratio": 1.4736842105263157, "no_speech_prob": 6.705216219415888e-05}, {"id": 194, "seek": 111428, "start": 1125.76, "end": 1130.24, "text": " There I have each timeline event pointing to each other.", "tokens": [821, 286, 362, 1184, 12933, 2280, 12166, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.13599474955413302, "compression_ratio": 1.4736842105263157, "no_speech_prob": 6.705216219415888e-05}, {"id": 195, "seek": 111428, "start": 1130.24, "end": 1136.28, "text": " And if there's a gap, the timeline event knows about this.", "tokens": [400, 498, 456, 311, 257, 7417, 11, 264, 12933, 2280, 3255, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.13599474955413302, "compression_ratio": 1.4736842105263157, "no_speech_prob": 6.705216219415888e-05}, {"id": 196, "seek": 113628, "start": 1136.28, "end": 1145.2, "text": " So this allows Trixity to again benefit from content flows.", "tokens": [407, 341, 4045, 314, 6579, 507, 281, 797, 5121, 490, 2701, 12867, 13], "temperature": 0.0, "avg_logprob": -0.16252523228742075, "compression_ratio": 1.691860465116279, "no_speech_prob": 1.2988355592824519e-05}, {"id": 197, "seek": 113628, "start": 1145.2, "end": 1151.8799999999999, "text": " So we have a producer that is the room starting from a timeline event, and a subscriber who", "tokens": [407, 321, 362, 257, 12314, 300, 307, 264, 1808, 2891, 490, 257, 12933, 2280, 11, 293, 257, 26122, 567], "temperature": 0.0, "avg_logprob": -0.16252523228742075, "compression_ratio": 1.691860465116279, "no_speech_prob": 1.2988355592824519e-05}, {"id": 198, "seek": 113628, "start": 1151.8799999999999, "end": 1155.32, "text": " wants the next timeline event to fill its timeline.", "tokens": [2738, 264, 958, 12933, 2280, 281, 2836, 1080, 12933, 13], "temperature": 0.0, "avg_logprob": -0.16252523228742075, "compression_ratio": 1.691860465116279, "no_speech_prob": 1.2988355592824519e-05}, {"id": 199, "seek": 113628, "start": 1155.32, "end": 1164.16, "text": " So this allows us to go really fast through the timeline and bid the timeline under the", "tokens": [407, 341, 4045, 505, 281, 352, 534, 2370, 807, 264, 12933, 293, 12957, 264, 12933, 833, 264], "temperature": 0.0, "avg_logprob": -0.16252523228742075, "compression_ratio": 1.691860465116279, "no_speech_prob": 1.2988355592824519e-05}, {"id": 200, "seek": 116416, "start": 1164.16, "end": 1171.96, "text": " top, and it makes it easier to fill the gaps, because we don't have another layer, fragments,", "tokens": [1192, 11, 293, 309, 1669, 309, 3571, 281, 2836, 264, 15031, 11, 570, 321, 500, 380, 362, 1071, 4583, 11, 29197, 11], "temperature": 0.0, "avg_logprob": -0.14987871226142435, "compression_ratio": 1.4213483146067416, "no_speech_prob": 9.449395292904228e-05}, {"id": 201, "seek": 116416, "start": 1171.96, "end": 1177.44, "text": " we just have timeline events.", "tokens": [321, 445, 362, 12933, 3931, 13], "temperature": 0.0, "avg_logprob": -0.14987871226142435, "compression_ratio": 1.4213483146067416, "no_speech_prob": 9.449395292904228e-05}, {"id": 202, "seek": 116416, "start": 1177.44, "end": 1182.44, "text": " And this way, it's also possible to very easy connect upgraded rooms.", "tokens": [400, 341, 636, 11, 309, 311, 611, 1944, 281, 588, 1858, 1745, 24133, 9396, 13], "temperature": 0.0, "avg_logprob": -0.14987871226142435, "compression_ratio": 1.4213483146067416, "no_speech_prob": 9.449395292904228e-05}, {"id": 203, "seek": 116416, "start": 1182.44, "end": 1189.3200000000002, "text": " So that one I released yesterday, I think, or two days ago.", "tokens": [407, 300, 472, 286, 4736, 5186, 11, 286, 519, 11, 420, 732, 1708, 2057, 13], "temperature": 0.0, "avg_logprob": -0.14987871226142435, "compression_ratio": 1.4213483146067416, "no_speech_prob": 9.449395292904228e-05}, {"id": 204, "seek": 118932, "start": 1189.32, "end": 1195.36, "text": " So the timeline event just shows to another timeline in another room.", "tokens": [407, 264, 12933, 2280, 445, 3110, 281, 1071, 12933, 294, 1071, 1808, 13], "temperature": 0.0, "avg_logprob": -0.1665026694536209, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.592672742845025e-05}, {"id": 205, "seek": 118932, "start": 1195.36, "end": 1201.6399999999999, "text": " So timelines with room upgrades are invisible for users of Trixity.", "tokens": [407, 45886, 365, 1808, 24868, 366, 14603, 337, 5022, 295, 314, 6579, 507, 13], "temperature": 0.0, "avg_logprob": -0.1665026694536209, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.592672742845025e-05}, {"id": 206, "seek": 118932, "start": 1201.6399999999999, "end": 1207.08, "text": " You just get an infinite timeline until you reach the oldest room and the first timeline", "tokens": [509, 445, 483, 364, 13785, 12933, 1826, 291, 2524, 264, 14026, 1808, 293, 264, 700, 12933], "temperature": 0.0, "avg_logprob": -0.1665026694536209, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.592672742845025e-05}, {"id": 207, "seek": 118932, "start": 1207.08, "end": 1210.84, "text": " event.", "tokens": [2280, 13], "temperature": 0.0, "avg_logprob": -0.1665026694536209, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.592672742845025e-05}, {"id": 208, "seek": 118932, "start": 1210.84, "end": 1214.12, "text": " And finally, a small example.", "tokens": [400, 2721, 11, 257, 1359, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1665026694536209, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.592672742845025e-05}, {"id": 209, "seek": 121412, "start": 1214.12, "end": 1220.32, "text": " So if you want to write a bot, that's a good start to use Trixity just to get a feeling", "tokens": [407, 498, 291, 528, 281, 2464, 257, 10592, 11, 300, 311, 257, 665, 722, 281, 764, 314, 6579, 507, 445, 281, 483, 257, 2633], "temperature": 0.0, "avg_logprob": -0.13103798602489716, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.951335980265867e-05}, {"id": 210, "seek": 121412, "start": 1220.32, "end": 1222.84, "text": " about it, how it works.", "tokens": [466, 309, 11, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13103798602489716, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.951335980265867e-05}, {"id": 211, "seek": 121412, "start": 1222.84, "end": 1226.4799999999998, "text": " You can just call get timeline events from now on.", "tokens": [509, 393, 445, 818, 483, 12933, 3931, 490, 586, 322, 13], "temperature": 0.0, "avg_logprob": -0.13103798602489716, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.951335980265867e-05}, {"id": 212, "seek": 121412, "start": 1226.4799999999998, "end": 1234.4399999999998, "text": " And what this does is it subscribes to the flow that I mentioned, which built the timeline,", "tokens": [400, 437, 341, 775, 307, 309, 2325, 6446, 281, 264, 3095, 300, 286, 2835, 11, 597, 3094, 264, 12933, 11], "temperature": 0.0, "avg_logprob": -0.13103798602489716, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.951335980265867e-05}, {"id": 213, "seek": 121412, "start": 1234.4399999999998, "end": 1239.1999999999998, "text": " and rates until the timeline event is decrypted, because the timeline itself also is a flow.", "tokens": [293, 6846, 1826, 264, 12933, 2280, 307, 979, 627, 25383, 11, 570, 264, 12933, 2564, 611, 307, 257, 3095, 13], "temperature": 0.0, "avg_logprob": -0.13103798602489716, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.951335980265867e-05}, {"id": 214, "seek": 123920, "start": 1239.2, "end": 1246.64, "text": " So if everything changes, it is redacted, or there's a reaction, or a replacement, the", "tokens": [407, 498, 1203, 2962, 11, 309, 307, 2182, 578, 292, 11, 420, 456, 311, 257, 5480, 11, 420, 257, 14419, 11, 264], "temperature": 0.0, "avg_logprob": -0.14808398976045498, "compression_ratio": 1.6612903225806452, "no_speech_prob": 6.49650683044456e-05}, {"id": 215, "seek": 123920, "start": 1246.64, "end": 1248.92, "text": " timeline event flow changes.", "tokens": [12933, 2280, 3095, 2962, 13], "temperature": 0.0, "avg_logprob": -0.14808398976045498, "compression_ratio": 1.6612903225806452, "no_speech_prob": 6.49650683044456e-05}, {"id": 216, "seek": 123920, "start": 1248.92, "end": 1256.2, "text": " So this get timeline events from now on just wraps it down, so you get a timeline event", "tokens": [407, 341, 483, 12933, 3931, 490, 586, 322, 445, 25831, 309, 760, 11, 370, 291, 483, 257, 12933, 2280], "temperature": 0.0, "avg_logprob": -0.14808398976045498, "compression_ratio": 1.6612903225806452, "no_speech_prob": 6.49650683044456e-05}, {"id": 217, "seek": 123920, "start": 1256.2, "end": 1258.28, "text": " that is decrypted.", "tokens": [300, 307, 979, 627, 25383, 13], "temperature": 0.0, "avg_logprob": -0.14808398976045498, "compression_ratio": 1.6612903225806452, "no_speech_prob": 6.49650683044456e-05}, {"id": 218, "seek": 123920, "start": 1258.28, "end": 1264.48, "text": " And you can see we can just check what type it has, and when we have checked the type,", "tokens": [400, 291, 393, 536, 321, 393, 445, 1520, 437, 2010, 309, 575, 11, 293, 562, 321, 362, 10033, 264, 2010, 11], "temperature": 0.0, "avg_logprob": -0.14808398976045498, "compression_ratio": 1.6612903225806452, "no_speech_prob": 6.49650683044456e-05}, {"id": 219, "seek": 126448, "start": 1264.48, "end": 1270.84, "text": " we have access to body, and then we have send message.", "tokens": [321, 362, 2105, 281, 1772, 11, 293, 550, 321, 362, 2845, 3636, 13], "temperature": 0.0, "avg_logprob": -0.13090027295626128, "compression_ratio": 1.6022727272727273, "no_speech_prob": 7.477647886844352e-05}, {"id": 220, "seek": 126448, "start": 1270.84, "end": 1275.2, "text": " So when you call send message, you don't have to worry about if the room is encrypted", "tokens": [407, 562, 291, 818, 2845, 3636, 11, 291, 500, 380, 362, 281, 3292, 466, 498, 264, 1808, 307, 36663], "temperature": 0.0, "avg_logprob": -0.13090027295626128, "compression_ratio": 1.6022727272727273, "no_speech_prob": 7.477647886844352e-05}, {"id": 221, "seek": 126448, "start": 1275.2, "end": 1276.2, "text": " or not.", "tokens": [420, 406, 13], "temperature": 0.0, "avg_logprob": -0.13090027295626128, "compression_ratio": 1.6022727272727273, "no_speech_prob": 7.477647886844352e-05}, {"id": 222, "seek": 126448, "start": 1276.2, "end": 1282.64, "text": " You can just use the DSL that I created to write text messages, image messages, and so", "tokens": [509, 393, 445, 764, 264, 15816, 43, 300, 286, 2942, 281, 2464, 2487, 7897, 11, 3256, 7897, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.13090027295626128, "compression_ratio": 1.6022727272727273, "no_speech_prob": 7.477647886844352e-05}, {"id": 223, "seek": 126448, "start": 1282.64, "end": 1286.68, "text": " on, and you can also form relations with that.", "tokens": [322, 11, 293, 291, 393, 611, 1254, 2299, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.13090027295626128, "compression_ratio": 1.6022727272727273, "no_speech_prob": 7.477647886844352e-05}, {"id": 224, "seek": 128668, "start": 1286.68, "end": 1294.52, "text": " So you can say like here, yeah, this is a reply to the timeline event I just got.", "tokens": [407, 291, 393, 584, 411, 510, 11, 1338, 11, 341, 307, 257, 16972, 281, 264, 12933, 2280, 286, 445, 658, 13], "temperature": 0.0, "avg_logprob": -0.1225909495699233, "compression_ratio": 1.4942528735632183, "no_speech_prob": 5.825056723551825e-05}, {"id": 225, "seek": 128668, "start": 1294.52, "end": 1300.1200000000001, "text": " And this has extensible events in mind, so if in the future there are other content blocks", "tokens": [400, 341, 575, 1279, 30633, 3931, 294, 1575, 11, 370, 498, 294, 264, 2027, 456, 366, 661, 2701, 8474], "temperature": 0.0, "avg_logprob": -0.1225909495699233, "compression_ratio": 1.4942528735632183, "no_speech_prob": 5.825056723551825e-05}, {"id": 226, "seek": 128668, "start": 1300.1200000000001, "end": 1309.92, "text": " that are added, we can just extend the DSL, and you can very, very intuitive write your", "tokens": [300, 366, 3869, 11, 321, 393, 445, 10101, 264, 15816, 43, 11, 293, 291, 393, 588, 11, 588, 21769, 2464, 428], "temperature": 0.0, "avg_logprob": -0.1225909495699233, "compression_ratio": 1.4942528735632183, "no_speech_prob": 5.825056723551825e-05}, {"id": 227, "seek": 130992, "start": 1309.92, "end": 1317.52, "text": " content with Trixity into an event, into an extensible event.", "tokens": [2701, 365, 314, 6579, 507, 666, 364, 2280, 11, 666, 364, 1279, 30633, 2280, 13], "temperature": 0.0, "avg_logprob": -0.1692517843001928, "compression_ratio": 1.6023391812865497, "no_speech_prob": 5.062040509073995e-05}, {"id": 228, "seek": 130992, "start": 1317.52, "end": 1322.96, "text": " So here are some projects that are using Trixity and that I know about.", "tokens": [407, 510, 366, 512, 4455, 300, 366, 1228, 314, 6579, 507, 293, 300, 286, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.1692517843001928, "compression_ratio": 1.6023391812865497, "no_speech_prob": 5.062040509073995e-05}, {"id": 229, "seek": 130992, "start": 1322.96, "end": 1327.44, "text": " There is a Spotify bot, a Mensa bot.", "tokens": [821, 307, 257, 29036, 10592, 11, 257, 7364, 64, 10592, 13], "temperature": 0.0, "avg_logprob": -0.1692517843001928, "compression_ratio": 1.6023391812865497, "no_speech_prob": 5.062040509073995e-05}, {"id": 230, "seek": 130992, "start": 1327.44, "end": 1333.48, "text": " Someone has created some extensions to better use it for bots and so on.", "tokens": [8734, 575, 2942, 512, 25129, 281, 1101, 764, 309, 337, 35410, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1692517843001928, "compression_ratio": 1.6023391812865497, "no_speech_prob": 5.062040509073995e-05}, {"id": 231, "seek": 130992, "start": 1333.48, "end": 1336.48, "text": " And there is Trixity examples.", "tokens": [400, 456, 307, 314, 6579, 507, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1692517843001928, "compression_ratio": 1.6023391812865497, "no_speech_prob": 5.062040509073995e-05}, {"id": 232, "seek": 133648, "start": 1336.48, "end": 1343.16, "text": " That is from me, this is just a ping bot, part of it you saw here.", "tokens": [663, 307, 490, 385, 11, 341, 307, 445, 257, 26151, 10592, 11, 644, 295, 309, 291, 1866, 510, 13], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 233, "seek": 133648, "start": 1343.16, "end": 1349.52, "text": " It is E2E enabled, you can just run it in your browser, on your Linux machine, or on", "tokens": [467, 307, 462, 17, 36, 15172, 11, 291, 393, 445, 1190, 309, 294, 428, 11185, 11, 322, 428, 18734, 3479, 11, 420, 322], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 234, "seek": 133648, "start": 1349.52, "end": 1355.44, "text": " your iOS client, or via the JVM on Android.", "tokens": [428, 17430, 6423, 11, 420, 5766, 264, 508, 53, 44, 322, 8853, 13], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 235, "seek": 133648, "start": 1355.44, "end": 1359.92, "text": " And there is also Timmy Messenger, that's our messenger from our company, but it's not", "tokens": [400, 456, 307, 611, 7172, 2226, 34226, 11, 300, 311, 527, 26599, 490, 527, 2237, 11, 457, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 236, "seek": 133648, "start": 1359.92, "end": 1360.92, "text": " open source yet.", "tokens": [1269, 4009, 1939, 13], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 237, "seek": 133648, "start": 1360.92, "end": 1365.2, "text": " We plan to, but we don't know how, because licensing.", "tokens": [492, 1393, 281, 11, 457, 321, 500, 380, 458, 577, 11, 570, 29759, 13], "temperature": 0.0, "avg_logprob": -0.22333293094813267, "compression_ratio": 1.5021276595744681, "no_speech_prob": 0.00019058160251006484}, {"id": 238, "seek": 136520, "start": 1365.2, "end": 1374.04, "text": " Yeah, just try it out and come to me if you have questions.", "tokens": [865, 11, 445, 853, 309, 484, 293, 808, 281, 385, 498, 291, 362, 1651, 13], "temperature": 0.0, "avg_logprob": -0.23540697733561197, "compression_ratio": 1.4491017964071857, "no_speech_prob": 0.00026445870753377676}, {"id": 239, "seek": 136520, "start": 1374.04, "end": 1375.48, "text": " I'm a bit around.", "tokens": [286, 478, 257, 857, 926, 13], "temperature": 0.0, "avg_logprob": -0.23540697733561197, "compression_ratio": 1.4491017964071857, "no_speech_prob": 0.00026445870753377676}, {"id": 240, "seek": 136520, "start": 1375.48, "end": 1379.96, "text": " This is the matrix room, this is my matrix ID.", "tokens": [639, 307, 264, 8141, 1808, 11, 341, 307, 452, 8141, 7348, 13], "temperature": 0.0, "avg_logprob": -0.23540697733561197, "compression_ratio": 1.4491017964071857, "no_speech_prob": 0.00026445870753377676}, {"id": 241, "seek": 136520, "start": 1379.96, "end": 1386.1200000000001, "text": " And if we have a bit of time, I just can show you a small demo, I think.", "tokens": [400, 498, 321, 362, 257, 857, 295, 565, 11, 286, 445, 393, 855, 291, 257, 1359, 10723, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.23540697733561197, "compression_ratio": 1.4491017964071857, "no_speech_prob": 0.00026445870753377676}, {"id": 242, "seek": 136520, "start": 1386.1200000000001, "end": 1391.88, "text": " Yeah, I made a small performance comparison.", "tokens": [865, 11, 286, 1027, 257, 1359, 3389, 9660, 13], "temperature": 0.0, "avg_logprob": -0.23540697733561197, "compression_ratio": 1.4491017964071857, "no_speech_prob": 0.00026445870753377676}, {"id": 243, "seek": 139188, "start": 1391.88, "end": 1398.6000000000001, "text": " It's not representative, because it just runs once on my machine, and there was no warm", "tokens": [467, 311, 406, 12424, 11, 570, 309, 445, 6676, 1564, 322, 452, 3479, 11, 293, 456, 390, 572, 4561], "temperature": 0.0, "avg_logprob": -0.2434817632039388, "compression_ratio": 1.5, "no_speech_prob": 0.00013077928451821208}, {"id": 244, "seek": 139188, "start": 1398.6000000000001, "end": 1402.1200000000001, "text": " up or multiple runs.", "tokens": [493, 420, 3866, 6676, 13], "temperature": 0.0, "avg_logprob": -0.2434817632039388, "compression_ratio": 1.5, "no_speech_prob": 0.00013077928451821208}, {"id": 245, "seek": 139188, "start": 1402.1200000000001, "end": 1409.3600000000001, "text": " Yeah, on the left side you see our Timmy client, but basically it's just using Trixity.", "tokens": [865, 11, 322, 264, 1411, 1252, 291, 536, 527, 7172, 2226, 6423, 11, 457, 1936, 309, 311, 445, 1228, 314, 6579, 507, 13], "temperature": 0.0, "avg_logprob": -0.2434817632039388, "compression_ratio": 1.5, "no_speech_prob": 0.00013077928451821208}, {"id": 246, "seek": 139188, "start": 1409.3600000000001, "end": 1413.92, "text": " On the right side there is Element, and in the middle is Fluffy Chat.", "tokens": [1282, 264, 558, 1252, 456, 307, 20900, 11, 293, 294, 264, 2808, 307, 3235, 14297, 27503, 13], "temperature": 0.0, "avg_logprob": -0.2434817632039388, "compression_ratio": 1.5, "no_speech_prob": 0.00013077928451821208}, {"id": 247, "seek": 139188, "start": 1413.92, "end": 1418.1200000000001, "text": " And now you can give me your bets, who is the fastest.", "tokens": [400, 586, 291, 393, 976, 385, 428, 39922, 11, 567, 307, 264, 14573, 13], "temperature": 0.0, "avg_logprob": -0.2434817632039388, "compression_ratio": 1.5, "no_speech_prob": 0.00013077928451821208}, {"id": 248, "seek": 141812, "start": 1418.12, "end": 1426.04, "text": " Yeah, let's see, when the red zoom comes, the response from the server reached the client.", "tokens": [865, 11, 718, 311, 536, 11, 562, 264, 2182, 8863, 1487, 11, 264, 4134, 490, 264, 7154, 6488, 264, 6423, 13], "temperature": 0.0, "avg_logprob": -0.17723453746122472, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00018486518820282072}, {"id": 249, "seek": 141812, "start": 1426.04, "end": 1431.3999999999999, "text": " So I just looked into the Synapse logs when the response was sent.", "tokens": [407, 286, 445, 2956, 666, 264, 26155, 11145, 20820, 562, 264, 4134, 390, 2279, 13], "temperature": 0.0, "avg_logprob": -0.17723453746122472, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00018486518820282072}, {"id": 250, "seek": 141812, "start": 1431.3999999999999, "end": 1437.12, "text": " So we just wait a few seconds, and then we see who is first.", "tokens": [407, 321, 445, 1699, 257, 1326, 3949, 11, 293, 550, 321, 536, 567, 307, 700, 13], "temperature": 0.0, "avg_logprob": -0.17723453746122472, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00018486518820282072}, {"id": 251, "seek": 141812, "start": 1437.12, "end": 1441.6399999999999, "text": " And you can look into opening rooms, because we have this caching, it is very fast in our", "tokens": [400, 291, 393, 574, 666, 5193, 9396, 11, 570, 321, 362, 341, 269, 2834, 11, 309, 307, 588, 2370, 294, 527], "temperature": 0.0, "avg_logprob": -0.17723453746122472, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00018486518820282072}, {"id": 252, "seek": 141812, "start": 1441.6399999999999, "end": 1447.1999999999998, "text": " client, but I must say Fluffy Chat is also very fast regarding opening rooms.", "tokens": [6423, 11, 457, 286, 1633, 584, 3235, 14297, 27503, 307, 611, 588, 2370, 8595, 5193, 9396, 13], "temperature": 0.0, "avg_logprob": -0.17723453746122472, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.00018486518820282072}, {"id": 253, "seek": 144720, "start": 1447.2, "end": 1454.2, "text": " So, oh, Trixity was the fastest.", "tokens": [407, 11, 1954, 11, 314, 6579, 507, 390, 264, 14573, 13], "temperature": 0.0, "avg_logprob": -0.20020821287825302, "compression_ratio": 1.5576923076923077, "no_speech_prob": 6.795151421101764e-05}, {"id": 254, "seek": 144720, "start": 1454.2, "end": 1461.16, "text": " And we can open rooms, and you see open rooms is also a lot faster than on Element.", "tokens": [400, 321, 393, 1269, 9396, 11, 293, 291, 536, 1269, 9396, 307, 611, 257, 688, 4663, 813, 322, 20900, 13], "temperature": 0.0, "avg_logprob": -0.20020821287825302, "compression_ratio": 1.5576923076923077, "no_speech_prob": 6.795151421101764e-05}, {"id": 255, "seek": 144720, "start": 1461.16, "end": 1468.52, "text": " And there was Fluffy Chat, and Fluffy Chat also is very fast.", "tokens": [400, 456, 390, 3235, 14297, 27503, 11, 293, 3235, 14297, 27503, 611, 307, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.20020821287825302, "compression_ratio": 1.5576923076923077, "no_speech_prob": 6.795151421101764e-05}, {"id": 256, "seek": 144720, "start": 1468.52, "end": 1476.8400000000001, "text": " Yeah, I also have a desktop demo, but there Neko is the fastest.", "tokens": [865, 11, 286, 611, 362, 257, 14502, 10723, 11, 457, 456, 426, 34241, 307, 264, 14573, 13], "temperature": 0.0, "avg_logprob": -0.20020821287825302, "compression_ratio": 1.5576923076923077, "no_speech_prob": 6.795151421101764e-05}, {"id": 257, "seek": 147684, "start": 1476.84, "end": 1479.84, "text": " This is Neko, this is Timmy, Element on the web.", "tokens": [639, 307, 426, 34241, 11, 341, 307, 7172, 2226, 11, 20900, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.22346438918002817, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00010358474537497386}, {"id": 258, "seek": 147684, "start": 1479.84, "end": 1487.12, "text": " It's a bit hard, this comparison, because Element runs in the web and does not have the", "tokens": [467, 311, 257, 857, 1152, 11, 341, 9660, 11, 570, 20900, 6676, 294, 264, 3670, 293, 775, 406, 362, 264], "temperature": 0.0, "avg_logprob": -0.22346438918002817, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00010358474537497386}, {"id": 259, "seek": 147684, "start": 1487.12, "end": 1489.1599999999999, "text": " multi-threading other clients have.", "tokens": [4825, 12, 392, 35908, 661, 6982, 362, 13], "temperature": 0.0, "avg_logprob": -0.22346438918002817, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00010358474537497386}, {"id": 260, "seek": 147684, "start": 1489.1599999999999, "end": 1493.32, "text": " So Neko, just three seconds.", "tokens": [407, 426, 34241, 11, 445, 1045, 3949, 13], "temperature": 0.0, "avg_logprob": -0.22346438918002817, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00010358474537497386}, {"id": 261, "seek": 147684, "start": 1493.32, "end": 1501.8799999999999, "text": " I can just chat around, and the next is Timmy on the left, top left side, also very fast", "tokens": [286, 393, 445, 5081, 926, 11, 293, 264, 958, 307, 7172, 2226, 322, 264, 1411, 11, 1192, 1411, 1252, 11, 611, 588, 2370], "temperature": 0.0, "avg_logprob": -0.22346438918002817, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00010358474537497386}, {"id": 262, "seek": 150188, "start": 1501.88, "end": 1507.0, "text": " opening rooms, and switching rooms, because it is cached all the time in events.", "tokens": [5193, 9396, 11, 293, 16493, 9396, 11, 570, 309, 307, 269, 15095, 439, 264, 565, 294, 3931, 13], "temperature": 0.0, "avg_logprob": -0.34921172569537984, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.00020499083620961756}, {"id": 263, "seek": 150188, "start": 1507.0, "end": 1514.8000000000002, "text": " Then there was Element, and I think also Fluffy Chat, yeah, Fluffy Chat also.", "tokens": [1396, 456, 390, 20900, 11, 293, 286, 519, 611, 3235, 14297, 27503, 11, 1338, 11, 3235, 14297, 27503, 611, 13], "temperature": 0.0, "avg_logprob": -0.34921172569537984, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.00020499083620961756}, {"id": 264, "seek": 150188, "start": 1514.8000000000002, "end": 1521.8000000000002, "text": " Yeah, okay, that was my talk, thank you.", "tokens": [865, 11, 1392, 11, 300, 390, 452, 751, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.34921172569537984, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.00020499083620961756}, {"id": 265, "seek": 152180, "start": 1521.8, "end": 1532.2, "text": " Questions?", "tokens": [27738, 30], "temperature": 0.0, "avg_logprob": -0.29690381458827425, "compression_ratio": 1.6, "no_speech_prob": 0.00031728961039334536}, {"id": 266, "seek": 152180, "start": 1532.2, "end": 1536.72, "text": " How do you prevent data loss with your async transactions?", "tokens": [1012, 360, 291, 4871, 1412, 4470, 365, 428, 382, 34015, 16856, 30], "temperature": 0.0, "avg_logprob": -0.29690381458827425, "compression_ratio": 1.6, "no_speech_prob": 0.00031728961039334536}, {"id": 267, "seek": 152180, "start": 1536.72, "end": 1539.8799999999999, "text": " The transactions are run each after.", "tokens": [440, 16856, 366, 1190, 1184, 934, 13], "temperature": 0.0, "avg_logprob": -0.29690381458827425, "compression_ratio": 1.6, "no_speech_prob": 0.00031728961039334536}, {"id": 268, "seek": 152180, "start": 1539.8799999999999, "end": 1546.76, "text": " So if one transaction fails, the other transactions are just run, and the next one starts with", "tokens": [407, 498, 472, 14425, 18199, 11, 264, 661, 16856, 366, 445, 1190, 11, 293, 264, 958, 472, 3719, 365], "temperature": 0.0, "avg_logprob": -0.29690381458827425, "compression_ratio": 1.6, "no_speech_prob": 0.00031728961039334536}, {"id": 269, "seek": 152180, "start": 1546.76, "end": 1547.76, "text": " the alt token.", "tokens": [264, 4955, 14862, 13], "temperature": 0.0, "avg_logprob": -0.29690381458827425, "compression_ratio": 1.6, "no_speech_prob": 0.00031728961039334536}, {"id": 270, "seek": 154776, "start": 1547.76, "end": 1553.76, "text": " What happens if, say, your battery runs out whilst a bunch of transactions are queued?", "tokens": [708, 2314, 498, 11, 584, 11, 428, 5809, 6676, 484, 18534, 257, 3840, 295, 16856, 366, 631, 5827, 30], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 271, "seek": 154776, "start": 1553.76, "end": 1559.2, "text": " If your battery runs out when all those transactions are queued, so they haven't been written to", "tokens": [759, 428, 5809, 6676, 484, 562, 439, 729, 16856, 366, 631, 5827, 11, 370, 436, 2378, 380, 668, 3720, 281], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 272, "seek": 154776, "start": 1559.2, "end": 1560.2, "text": " the database.", "tokens": [264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 273, "seek": 154776, "start": 1560.2, "end": 1562.0, "text": " Yeah, then they are gone.", "tokens": [865, 11, 550, 436, 366, 2780, 13], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 274, "seek": 154776, "start": 1562.0, "end": 1565.92, "text": " Your client has to do the work again, but mostly this doesn't happen.", "tokens": [2260, 6423, 575, 281, 360, 264, 589, 797, 11, 457, 5240, 341, 1177, 380, 1051, 13], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 275, "seek": 154776, "start": 1565.92, "end": 1572.4, "text": " If you close your client, all transactions are written that are just opened, but it depends", "tokens": [759, 291, 1998, 428, 6423, 11, 439, 16856, 366, 3720, 300, 366, 445, 5625, 11, 457, 309, 5946], "temperature": 0.0, "avg_logprob": -0.2421795953180372, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.0004398278542794287}, {"id": 276, "seek": 157240, "start": 1572.4, "end": 1578.2800000000002, "text": " on your platform if it is killed hardly, or a trick city have a bit of time to write", "tokens": [322, 428, 3663, 498, 309, 307, 4652, 13572, 11, 420, 257, 4282, 2307, 362, 257, 857, 295, 565, 281, 2464], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 277, "seek": 157240, "start": 1578.2800000000002, "end": 1580.3600000000001, "text": " the transactions back to the database.", "tokens": [264, 16856, 646, 281, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 278, "seek": 157240, "start": 1580.3600000000001, "end": 1588.2, "text": " But it's still very fast to write, so it's just a bit snappier on mobile devices, which", "tokens": [583, 309, 311, 920, 588, 2370, 281, 2464, 11, 370, 309, 311, 445, 257, 857, 14528, 427, 811, 322, 6013, 5759, 11, 597], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 279, "seek": 157240, "start": 1588.2, "end": 1589.96, "text": " are not that fast.", "tokens": [366, 406, 300, 2370, 13], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 280, "seek": 157240, "start": 1589.96, "end": 1597.68, "text": " Like my smartphone from 2016, Element, I can't run Element on that, because it's too slow,", "tokens": [1743, 452, 13307, 490, 6549, 11, 20900, 11, 286, 393, 380, 1190, 20900, 322, 300, 11, 570, 309, 311, 886, 2964, 11], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 281, "seek": 157240, "start": 1597.68, "end": 1602.3600000000001, "text": " and sending messages, 10 seconds later, the message is, oh, okay, yes, yes, now.", "tokens": [293, 7750, 7897, 11, 1266, 3949, 1780, 11, 264, 3636, 307, 11, 1954, 11, 1392, 11, 2086, 11, 2086, 11, 586, 13], "temperature": 0.0, "avg_logprob": -0.22630600380686533, "compression_ratio": 1.5521235521235521, "no_speech_prob": 7.59140239097178e-05}, {"id": 282, "seek": 160236, "start": 1602.36, "end": 1603.36, "text": " Now we send the message.", "tokens": [823, 321, 2845, 264, 3636, 13], "temperature": 0.0, "avg_logprob": -0.25788125991821287, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.217545109800994e-05}, {"id": 283, "seek": 160236, "start": 1603.36, "end": 1609.9599999999998, "text": " I don't have this problem, because zooms are just faster than the slow I owe writing to", "tokens": [286, 500, 380, 362, 341, 1154, 11, 570, 5721, 4785, 366, 445, 4663, 813, 264, 2964, 286, 16655, 3579, 281], "temperature": 0.0, "avg_logprob": -0.25788125991821287, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.217545109800994e-05}, {"id": 284, "seek": 160236, "start": 1609.9599999999998, "end": 1617.4799999999998, "text": " the database we have on old smartphones, for example.", "tokens": [264, 8149, 321, 362, 322, 1331, 26782, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.25788125991821287, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.217545109800994e-05}, {"id": 285, "seek": 160236, "start": 1617.4799999999998, "end": 1618.4799999999998, "text": " Another question.", "tokens": [3996, 1168, 13], "temperature": 0.0, "avg_logprob": -0.25788125991821287, "compression_ratio": 1.3333333333333333, "no_speech_prob": 8.217545109800994e-05}, {"id": 286, "seek": 161848, "start": 1618.48, "end": 1632.48, "text": " It's nice to write.", "tokens": [467, 311, 1481, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.25776827335357666, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0005865693092346191}, {"id": 287, "seek": 161848, "start": 1632.48, "end": 1634.48, "text": " I like DSLs.", "tokens": [286, 411, 15816, 43, 82, 13], "temperature": 0.0, "avg_logprob": -0.25776827335357666, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0005865693092346191}, {"id": 288, "seek": 161848, "start": 1634.48, "end": 1642.32, "text": " In Kotlin, we have them all over the language, and it feels very intuitive, because your", "tokens": [682, 30123, 5045, 11, 321, 362, 552, 439, 670, 264, 2856, 11, 293, 309, 3417, 588, 21769, 11, 570, 428], "temperature": 0.0, "avg_logprob": -0.25776827335357666, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0005865693092346191}, {"id": 289, "seek": 164232, "start": 1642.32, "end": 1655.4399999999998, "text": " IDE gives you suggestions, what methods they are, and it's a lot easier to read, I think.", "tokens": [40930, 2709, 291, 13396, 11, 437, 7150, 436, 366, 11, 293, 309, 311, 257, 688, 3571, 281, 1401, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.34517688017625076, "compression_ratio": 1.0595238095238095, "no_speech_prob": 0.00025522138457745314}, {"id": 290, "seek": 165544, "start": 1655.44, "end": 1673.3200000000002, "text": " There's Rust, and there's Kotlin, but is there any way to realize the amount of things that", "tokens": [821, 311, 34952, 11, 293, 456, 311, 30123, 5045, 11, 457, 307, 456, 604, 636, 281, 4325, 264, 2372, 295, 721, 300], "temperature": 0.0, "avg_logprob": -0.4211446724685968, "compression_ratio": 1.3984962406015038, "no_speech_prob": 9.943593613570556e-05}, {"id": 291, "seek": 165544, "start": 1673.3200000000002, "end": 1674.3200000000002, "text": " the user has to learn to use all these things?", "tokens": [264, 4195, 575, 281, 1466, 281, 764, 439, 613, 721, 30], "temperature": 0.0, "avg_logprob": -0.4211446724685968, "compression_ratio": 1.3984962406015038, "no_speech_prob": 9.943593613570556e-05}, {"id": 292, "seek": 165544, "start": 1674.3200000000002, "end": 1675.3200000000002, "text": " I didn't understand the question, acoustically.", "tokens": [286, 994, 380, 1223, 264, 1168, 11, 22740, 984, 13], "temperature": 0.0, "avg_logprob": -0.4211446724685968, "compression_ratio": 1.3984962406015038, "no_speech_prob": 9.943593613570556e-05}, {"id": 293, "seek": 167532, "start": 1675.32, "end": 1686.0, "text": " There's a lot of language learning to make any progress.", "tokens": [821, 311, 257, 688, 295, 2856, 2539, 281, 652, 604, 4205, 13], "temperature": 0.0, "avg_logprob": -0.256616193856766, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0001709642237983644}, {"id": 294, "seek": 167532, "start": 1686.0, "end": 1692.08, "text": " Is there any effort to unify this, or towards Rust, maybe?", "tokens": [1119, 456, 604, 4630, 281, 517, 2505, 341, 11, 420, 3030, 34952, 11, 1310, 30], "temperature": 0.0, "avg_logprob": -0.256616193856766, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0001709642237983644}, {"id": 295, "seek": 167532, "start": 1692.08, "end": 1696.36, "text": " To be honest, I don't like Rust.", "tokens": [1407, 312, 3245, 11, 286, 500, 380, 411, 34952, 13], "temperature": 0.0, "avg_logprob": -0.256616193856766, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0001709642237983644}, {"id": 296, "seek": 167532, "start": 1696.36, "end": 1703.32, "text": " I just like a higher level of implementing stuff, so we didn't spoke, I didn't spoke", "tokens": [286, 445, 411, 257, 2946, 1496, 295, 18114, 1507, 11, 370, 321, 994, 380, 7179, 11, 286, 994, 380, 7179], "temperature": 0.0, "avg_logprob": -0.256616193856766, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0001709642237983644}, {"id": 297, "seek": 170332, "start": 1703.32, "end": 1705.32, "text": " with the Matrix Rust team.", "tokens": [365, 264, 36274, 34952, 1469, 13], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 298, "seek": 170332, "start": 1705.32, "end": 1710.32, "text": " I think we are done with the time, and the last question from the audience would be that", "tokens": [286, 519, 321, 366, 1096, 365, 264, 565, 11, 293, 264, 1036, 1168, 490, 264, 4034, 576, 312, 300], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 299, "seek": 170332, "start": 1710.32, "end": 1716.32, "text": " we can open the windows and the doors a bit to get more air in.", "tokens": [321, 393, 1269, 264, 9309, 293, 264, 8077, 257, 857, 281, 483, 544, 1988, 294, 13], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 300, "seek": 170332, "start": 1716.32, "end": 1717.32, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 301, "seek": 170332, "start": 1717.32, "end": 1718.32, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 302, "seek": 170332, "start": 1718.32, "end": 1719.32, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.4211789885563637, "compression_ratio": 1.516778523489933, "no_speech_prob": 0.0005173045792616904}, {"id": 303, "seek": 171932, "start": 1719.32, "end": 1736.1599999999999, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 1.0, "avg_logprob": -1.5128475427627563, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0007824868080206215}], "language": "en"}