{"text": " Welcome our next speakers and give them a round of applause. Can you hear me? I guess you can. So yeah, hello, everyone, and welcome to the session about how we can use open telemetry on Kubernetes to collect traces, metrics, and logs. So my name is Pavel, I'm software engineer at Red Hat, I contribute and I'm a contributor and maintainer of Open Telemetry Operator and Yeager project. Yeah, my name is Bine, and I'm also working on the Open Telemetry Operator and spent most of the time on Open Telemetry. And so as I mentioned on today's agenda, there is the Open Telemetry Operator. We will show how you can use it to deploy the collector, how you can as well use it to instrument your workloads on Kubernetes. And after this brief introduction, we will walk you three use cases, how you can use it to collect traces, metrics, and logs. However, I will start with the history of open source observability. I'm doing this because I believe that if we understand the history, maybe we will better understand where we as industry are going. So on this slide, you essentially see a timeline of the, with the open source projects. And it's divided into the, the upper and bottom parts. In the bottom, you see the open source projects or platforms that you can deploy and they provide you with a storage and visualization capabilities for, for the observability data. Most of them work with distributed traces, however, some of them, like the Apache skywalking hyper-tracing signals, those are more like end-to-end platforms that can show traces, metrics, and logs. I would like to focus on the upper part that shows you the open source data collection kind of frameworks. And what we see there with, especially with open sensors and open telemetry is that it's becoming more important that these frameworks kind of work with all the signals. For me, the, the data collection, especially for tracing started with Zipkin project. It gave us a stable data model that we, as developers, could use to export traces into Zipkin, but as well to many other kind of platforms that adopted Zipkin project. As a developer, when we wanted to use Zipkin clients, because the ecosystem hosted client libraries as well, it was a bit problematic in polyglot environments because those clients were using kind of inconsistent APIs, there was no standardization. And so this problem then was partially solved with open tracing. The scope of the project was a bit wider, there was a specification, there was a document that defines which data should be collected and as well how the API in those languages should look like. This enabled us to build reusable instrumentation libraries. And then, even later, the open sensors project started with slightly different approach. There was no specification, there was no API, but there was SDK that everybody could use and a collector. So with open tracing, the approach was that developers would use the API and then at the build time provide the SDK from a vendor. With open sensors, everybody would use the SDK and then in the collector decide where the data should be sent. Those two projects were kind of competing and then finally they merged into open telemetry in 2019. So the hotel, it adopted all the pieces from open tracing and open sensors, but kind of the biggest innovation in hotel is the, at least in my view, is the auto instrumentation libraries or the agents. Those agents are production ready, most of them, because they were donated by one of the observability vendors, so they are, you know, production tested. So when we kind of summarize what happened is that we started with some instrumentation libraries, you know, with Zipkin project, then since we have some kind of standardization, we could build reusable instrumentation libraries and kind of create more sophisticated instrumentations for runtimes. And now we are in an age that we have available in open source agents or auto instrumentation libraries that we can just grab, put into our platforms, and we will get telemetry data almost for free. And I think, you know, so where are we going? I think we are going into an era where we, as developers, we won't have to care about how the telemetry is created for us. We will be, the instrumentation will become maybe the feature of the platform where we deploy the application. So this is one way to look at it. The other way might be that the observability will shift left, and since we have this data, we will start utilizing it for other use cases, probably like testing and security. So with that, I would like to move to the open telemetry, and it's obviously open source project hosted in the cognitive computing foundation, and its main goal is to provide the vendor or neutral telemetry data collection. It's the second most active project in CNCF after Kubernetes, so it's quite large. And there are several independent components that we can use. There is a specification that defines what data should be collected and how the API should look like, and obviously then there is the implementation of the API, the SDK and the standard data model called OTLP or open telemetry protocol. These four pieces are meant to be used primarily by instrumentation authors or the people that work on the observability systems. And last two components, the auto instrumentation or agent and collector are meant to be used by end users to kind of roll out observability in their organization. To facilitate open telemetry deployment on Kubernetes, there is a Helm chart and Kubernetes operator. What I would like to stress is that open telemetry is only about how we collect and create telemetry data. It's not a platform that you can deploy, it doesn't provide any storage or query APIs. So now let's go to the main part, the Kubernetes operator. The operator itself, it's a Golang application, it uses QBuilder and operator SDK, and it has three primary use cases. It can deploy the open telemetry collector as a deployment, demon set, stateful set. It can as well inject the collector as a side card to your workload. The second use case is that it can instrument your workloads running on Kubernetes by using those instrumentation libraries or agents from open telemetry. And last but not least, it integrates with Prometheus ecosystem. It can read the service and pod monitors, get the scraped targets, and split them across the collector instances that you have deployed. To enable this functionality, the operator provides two CRDs, one for the collector that is used to deploy the collector and integrate the Prometheus. And the second one is the instrumentation CRD, where you define how the applications should be instrumented. The operator itself then can be deployed through manifest files, home chart, or OLM. So what we see here is the Kubernetes cluster. There are three workloads, pod one, pod two, and pod three. The first workload is instrumented with the hotel SDK directly, so when we were building this application, we pulled in the hotel dependency and we compiled it against it and used those APIs directly in our business code and in the middlewares that we are using. The second pod is using the auto instrumentation libraries that were injected by the operator through the Venetian webhook. And the third pod is using Zipkin instrumentation and Prometheus instrumentation libraries, and it has the collector sidecar as well injected by the operator. So essentially the operator there, it reconciles three open telemetry CRs, two for the collector, and one instrumentation. And then all these workloads, they send data to the collector deployed, probably as a demon set, and then this collector then does some data normalization and sends finally data into platform of your choice, which can be Prometheus for metrics, Yeager for traces. With that, I would like to move to the second part, explaining the CRDs in more detail. Yep. The microphone should work. Yeah, so with the CRDs for today, we wanted to show both of them, and we start with the collector one. The collector CRD is a bit loaded, so therefore we picked a few things here, which I would say are the most used or important. So as Pawe mentioned, there are different deployment modes, different use cases for the open telemetry collector, and in the specification, we can go to the mode and just specify it there. There's a handy thing, which is the sidecar, we will see it afterwards. And if we want to use it, we only go to the part definition of our deployment and inject the annotation we see on the top right. And if we go with the deployment mode or something like this, and we want to expose it for collecting metrics, locks, and traces from a different system, for example, we can use the Ingress type, we can set there a lot of more, we configure there a lot of more like also the annotations, your Ingress class. But yeah, mainly the operator takes care of everything, creating services, also is able to balance your load there. And yeah, the last thing here is then the image section, which is also important. With the open telemetry operator, it usually ships the core distribution of open telemetry by default. So in open telemetry, the collector is split into two repositories when you go up and look at GitHub. So in core, you will find OTP, a logging exporter, so some basic stuff. And in Contrip, you find basically everything. So if you want to send your traces to some proprietary vendor or to J\u00e4ger, you probably need to look there. Okay, the next thing is then the configuration. The configuration for the open telemetry collector is here provided like it's usually done for the collector itself. So it's passed directly forward. It's split it into three parts here. We see the receiving part there. We specify our OTP receiver. Here it's accepts GRPC on a specific board. It could also be there that we specify a prometers receiver, which is then scraping something. Then the optional part is basically the processing part. We might want to save some resources and we batch them our telemetry data. And yeah, there are other useful things. And on the exporter section, here we use the logging exporter, which is part of the core distribution, but you can configure whatever you like. You can also have multiple exporters for one resource. There is one thing. On the right side, we see the extensions. It didn't fit on the slide, so it's there in this box. This is then used if you have, for example, an exporter, which needs some additional headers. Yeah, you want to set a barrier token or something else. You can do it there. And then finally, we go to the service section where we have different pipelines for each signal. And then we can then configure a processor and receiver and exporter in the way we wanted. So then there is another CD, which is used for the auto instrumentation. And it looks slightly different. So here we have also the, in the specification, we have the exporter. And the exporter only exports OTP, so which means if we want to export it to some, yeah, back end of our choice, we usually instrument our application directly then forward this traces to a collector instance, which is running next to it. And yeah, we can use the power of these processors. Yeah, then we can configure some other useful things like how the context is propagated and the sample rate. And to use it, it's also quite easy. So we have our deployment. In this case, we can, it can choose from this list of supported languages. We might use Java and we only set this annotation on the port level and it will take care of adding the SDK and also setting and configuring the environment variables. If we use something like Rust, we can also use the inject SDK annotation to configure then, yeah, just the destination because then SDK should be there. And if we have a setup where there is, let's say, some proxy in front, like Envoy, we can then just skip the, yeah, adding the auto instrumentation there by only configuring the container names we want to instrument. And we will see this in a minute, a bit more in detail. So this is then basically what we would need to do. So we create this instrumentation, we add this annotation on the left. We see the pot, there is our application. And in this gray box, you see what automatically is added. And this is then forwarded in this example to a collector. And yeah, how does this work? So the operator in that mission web hook, he will add this in its container. On the top left, we see how the container looks before. So there are no environment variables. It's just a plain application. And in the command section, there is then the copy, which copies the Java agent to our original container. And on the right side, we see the final result. We see the Java tool options where the container is loaded, and then we see all this environment variables to configure our SDK. And finally, what we have seen also in the presentation from Nicholas previously, we have here the Yeager output. So we can see the resource attributes and all the beautiful stuff that comes with it. So next, we have, we can have a look on metrics. So there is the open telemetry SDK. So if you want to go with open telemetry metrics, but I assume a lot of people have already some perimeter stuff in place. And the open telemetry operator also helps us with this. So we can, I might we look first on the receiver part on the bottom. We see there, we configure the perimeter's receiver, which has a scrape configuration, and there we can, for example, add some static targets. So we assume we add there three different scrape endpoints, then afterwards, if the target allocator is enabled, this will then take these scrape targets and divide, well, spread these targets across our replicas, which are then responsible for getting the metrics. And yeah, that's basically how it works. There's also an option to enable perimeter CRs, so we can then forward to this one. And the target allocator, which is an extra instance created by the Yeager, by the open telemetry operator, will then, yeah, get the targets from there. So we see this here in this graphic, quite good. On the left side, we see part one, which is using open telemetry, and it's pushing the information telemetry data it gets directly to a collector. And in this gray box, we see there, we have two instances running Prometoys, running instruments with Prometoys, and the collector one and collector two are pulling the information from there. So this is all managed then by the operator. We have seen the replicas, this is basically collector one and collector two, and since we enable the target allocator, we get the targets from there, so which is then coming from the port monitor. And finally, we send the information somewhere. So the last thing here, the last signal are then locks. So for locks, there are different options. So the first one would be to use the open telemetry SDK, what we might don't want right now because we need to do some work, but if we directly want to go ahead, there is the philoc receiver, we can configure it to get the information from this, and yeah, it's available in the conflict repository, and we have different parsers there which help us to move the locks into the OTP format. We will see in a minute how this looks like. And there are other options if you want to integrate with FluentBit, so there is a forwarder, so you can use it as a kind of a gateway then. And yeah, the only thing we need to do then is we can configure it as a demon set. We need to pass our information there, and the philoc receiver, for example, can then get all the locks. And how does this look like at the end? So this is when we exported the locks to the logging output, so standard out. We see that we have the resource attributes which are added automatically, and yeah, we see then the lock information, and on the bottom the trace ID and span ID which are not given if we read it just from disk, but that's it. Yeah, then we are almost at the end. Yeah, thanks a lot for the interesting talk. Does anyone have questions? Any questions? Raise your hand. Question? Yeah, there we go. For the logging part, would you suggest to replace any kind of cluster logging like with Fluent Bit, or that's like sending it off to Loki or something with an open telemetry log scraping, or is that complementary? I'm not sure if I fully got it. So you want to, yeah, in this case it's just another way, but the useful thing is if you have the open telemetry SDK, it will automatically add then the trace ID to it, and then you can correlate your signals. Sorry. So I'm super newbie to this, so I failed to understand how the, if the open telemetry is trying to replace, for example, the log parsers like the telegraph, for example, which is able to generate prometheus metrics by log scraping, or also how Zipkin, which is the tracing thing, fits in the metric collection of all this picture. So I'm not trying to understand how you cobble together all these sources and how open telemetry either replaces or either makes it easier to use all these technologies together. Thank you. So maybe on this slide you see that the third port is using the Zipkin and prometheus, and the collector can receive data in Zipkin format, it can scrape prometheus metrics, then transform this data into OTLT or the Zipkin as well, and then send it to the other collector. So the collector essentially can receive data in multiple formats, transform them to the format of your choice, and then use that format to send it to other systems. Hello and thanks for the talk. I'm just wondering, what's your strategy of filtering health check requests, for example, or the life probes request that you get in the pod? Health checks, like to avoid generating traces for health checks? Sorry? To avoid generating traces for health check endpoints? Yeah. That's a very good question. So you could maybe configure the collector to drop the data, but I think the best way would be to tell the instrumentation to skip instrumenting those endpoints. To be honest, I'm not sure if this is implemented in OTL agents, but I saw a lot of discussions around this problem, so probably there will be some solution. We have time for one last question, if there is any. No? Okay. Oh, no. And thanks a lot.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.04, "text": " Welcome our next speakers and give them a round of applause.", "tokens": [4027, 527, 958, 9518, 293, 976, 552, 257, 3098, 295, 9969, 13], "temperature": 0.0, "avg_logprob": -0.25470915707674896, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.14919371902942657}, {"id": 1, "seek": 0, "start": 16.04, "end": 17.04, "text": " Can you hear me?", "tokens": [1664, 291, 1568, 385, 30], "temperature": 0.0, "avg_logprob": -0.25470915707674896, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.14919371902942657}, {"id": 2, "seek": 0, "start": 17.04, "end": 19.28, "text": " I guess you can.", "tokens": [286, 2041, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.25470915707674896, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.14919371902942657}, {"id": 3, "seek": 0, "start": 19.28, "end": 23.72, "text": " So yeah, hello, everyone, and welcome to the session about how we can use open telemetry", "tokens": [407, 1338, 11, 7751, 11, 1518, 11, 293, 2928, 281, 264, 5481, 466, 577, 321, 393, 764, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.25470915707674896, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.14919371902942657}, {"id": 4, "seek": 0, "start": 23.72, "end": 29.36, "text": " on Kubernetes to collect traces, metrics, and logs.", "tokens": [322, 23145, 281, 2500, 26076, 11, 16367, 11, 293, 20820, 13], "temperature": 0.0, "avg_logprob": -0.25470915707674896, "compression_ratio": 1.4329268292682926, "no_speech_prob": 0.14919371902942657}, {"id": 5, "seek": 2936, "start": 29.36, "end": 34.88, "text": " So my name is Pavel, I'm software engineer at Red Hat, I contribute and I'm a contributor", "tokens": [407, 452, 1315, 307, 3426, 779, 11, 286, 478, 4722, 11403, 412, 4477, 15867, 11, 286, 10586, 293, 286, 478, 257, 42859], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 6, "seek": 2936, "start": 34.88, "end": 38.36, "text": " and maintainer of Open Telemetry Operator and Yeager project.", "tokens": [293, 6909, 260, 295, 7238, 1989, 306, 5537, 627, 12480, 1639, 293, 835, 3557, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 7, "seek": 2936, "start": 38.36, "end": 44.64, "text": " Yeah, my name is Bine, and I'm also working on the Open Telemetry Operator and spent most", "tokens": [865, 11, 452, 1315, 307, 363, 533, 11, 293, 286, 478, 611, 1364, 322, 264, 7238, 1989, 306, 5537, 627, 12480, 1639, 293, 4418, 881], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 8, "seek": 2936, "start": 44.64, "end": 47.92, "text": " of the time on Open Telemetry.", "tokens": [295, 264, 565, 322, 7238, 1989, 306, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 9, "seek": 2936, "start": 47.92, "end": 52.68, "text": " And so as I mentioned on today's agenda, there is the Open Telemetry Operator.", "tokens": [400, 370, 382, 286, 2835, 322, 965, 311, 9829, 11, 456, 307, 264, 7238, 1989, 306, 5537, 627, 12480, 1639, 13], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 10, "seek": 2936, "start": 52.68, "end": 56.84, "text": " We will show how you can use it to deploy the collector, how you can as well use it", "tokens": [492, 486, 855, 577, 291, 393, 764, 309, 281, 7274, 264, 23960, 11, 577, 291, 393, 382, 731, 764, 309], "temperature": 0.0, "avg_logprob": -0.2102222591638565, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.0008083066204562783}, {"id": 11, "seek": 5684, "start": 56.84, "end": 59.96, "text": " to instrument your workloads on Kubernetes.", "tokens": [281, 7198, 428, 32452, 322, 23145, 13], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 12, "seek": 5684, "start": 59.96, "end": 63.56, "text": " And after this brief introduction, we will walk you three use cases, how you can use", "tokens": [400, 934, 341, 5353, 9339, 11, 321, 486, 1792, 291, 1045, 764, 3331, 11, 577, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 13, "seek": 5684, "start": 63.56, "end": 66.24000000000001, "text": " it to collect traces, metrics, and logs.", "tokens": [309, 281, 2500, 26076, 11, 16367, 11, 293, 20820, 13], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 14, "seek": 5684, "start": 66.24000000000001, "end": 72.04, "text": " However, I will start with the history of open source observability.", "tokens": [2908, 11, 286, 486, 722, 365, 264, 2503, 295, 1269, 4009, 9951, 2310, 13], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 15, "seek": 5684, "start": 72.04, "end": 76.64, "text": " I'm doing this because I believe that if we understand the history, maybe we will better", "tokens": [286, 478, 884, 341, 570, 286, 1697, 300, 498, 321, 1223, 264, 2503, 11, 1310, 321, 486, 1101], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 16, "seek": 5684, "start": 76.64, "end": 82.28, "text": " understand where we as industry are going.", "tokens": [1223, 689, 321, 382, 3518, 366, 516, 13], "temperature": 0.0, "avg_logprob": -0.15530358003766348, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.00013710382336284965}, {"id": 17, "seek": 8228, "start": 82.28, "end": 87.64, "text": " So on this slide, you essentially see a timeline of the, with the open source projects.", "tokens": [407, 322, 341, 4137, 11, 291, 4476, 536, 257, 12933, 295, 264, 11, 365, 264, 1269, 4009, 4455, 13], "temperature": 0.0, "avg_logprob": -0.20205400427993464, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.00015577654994558543}, {"id": 18, "seek": 8228, "start": 87.64, "end": 91.08, "text": " And it's divided into the, the upper and bottom parts.", "tokens": [400, 309, 311, 6666, 666, 264, 11, 264, 6597, 293, 2767, 3166, 13], "temperature": 0.0, "avg_logprob": -0.20205400427993464, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.00015577654994558543}, {"id": 19, "seek": 8228, "start": 91.08, "end": 96.8, "text": " In the bottom, you see the open source projects or platforms that you can deploy and they", "tokens": [682, 264, 2767, 11, 291, 536, 264, 1269, 4009, 4455, 420, 9473, 300, 291, 393, 7274, 293, 436], "temperature": 0.0, "avg_logprob": -0.20205400427993464, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.00015577654994558543}, {"id": 20, "seek": 8228, "start": 96.8, "end": 104.24000000000001, "text": " provide you with a storage and visualization capabilities for, for the observability data.", "tokens": [2893, 291, 365, 257, 6725, 293, 25801, 10862, 337, 11, 337, 264, 9951, 2310, 1412, 13], "temperature": 0.0, "avg_logprob": -0.20205400427993464, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.00015577654994558543}, {"id": 21, "seek": 8228, "start": 104.24000000000001, "end": 110.08, "text": " Most of them work with distributed traces, however, some of them, like the Apache skywalking", "tokens": [4534, 295, 552, 589, 365, 12631, 26076, 11, 4461, 11, 512, 295, 552, 11, 411, 264, 46597, 5443, 12490, 278], "temperature": 0.0, "avg_logprob": -0.20205400427993464, "compression_ratio": 1.7049180327868851, "no_speech_prob": 0.00015577654994558543}, {"id": 22, "seek": 11008, "start": 110.08, "end": 116.16, "text": " hyper-tracing signals, those are more like end-to-end platforms that can show traces,", "tokens": [9848, 12, 6903, 5615, 12354, 11, 729, 366, 544, 411, 917, 12, 1353, 12, 521, 9473, 300, 393, 855, 26076, 11], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 23, "seek": 11008, "start": 116.16, "end": 117.64, "text": " metrics, and logs.", "tokens": [16367, 11, 293, 20820, 13], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 24, "seek": 11008, "start": 117.64, "end": 122.52, "text": " I would like to focus on the upper part that shows you the open source data collection", "tokens": [286, 576, 411, 281, 1879, 322, 264, 6597, 644, 300, 3110, 291, 264, 1269, 4009, 1412, 5765], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 25, "seek": 11008, "start": 122.52, "end": 125.36, "text": " kind of frameworks.", "tokens": [733, 295, 29834, 13], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 26, "seek": 11008, "start": 125.36, "end": 130.68, "text": " And what we see there with, especially with open sensors and open telemetry is that it's", "tokens": [400, 437, 321, 536, 456, 365, 11, 2318, 365, 1269, 14840, 293, 1269, 4304, 5537, 627, 307, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 27, "seek": 11008, "start": 130.68, "end": 136.48, "text": " becoming more important that these frameworks kind of work with all the signals.", "tokens": [5617, 544, 1021, 300, 613, 29834, 733, 295, 589, 365, 439, 264, 12354, 13], "temperature": 0.0, "avg_logprob": -0.1630763806794819, "compression_ratio": 1.6637554585152838, "no_speech_prob": 4.162410550634377e-05}, {"id": 28, "seek": 13648, "start": 136.48, "end": 143.6, "text": " For me, the, the data collection, especially for tracing started with Zipkin project.", "tokens": [1171, 385, 11, 264, 11, 264, 1412, 5765, 11, 2318, 337, 25262, 1409, 365, 1176, 647, 5843, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1223292350769043, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.6266856619040482e-05}, {"id": 29, "seek": 13648, "start": 143.6, "end": 148.44, "text": " It gave us a stable data model that we, as developers, could use to export traces into", "tokens": [467, 2729, 505, 257, 8351, 1412, 2316, 300, 321, 11, 382, 8849, 11, 727, 764, 281, 10725, 26076, 666], "temperature": 0.0, "avg_logprob": -0.1223292350769043, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.6266856619040482e-05}, {"id": 30, "seek": 13648, "start": 148.44, "end": 154.44, "text": " Zipkin, but as well to many other kind of platforms that adopted Zipkin project.", "tokens": [1176, 647, 5843, 11, 457, 382, 731, 281, 867, 661, 733, 295, 9473, 300, 12175, 1176, 647, 5843, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1223292350769043, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.6266856619040482e-05}, {"id": 31, "seek": 13648, "start": 154.44, "end": 160.83999999999997, "text": " As a developer, when we wanted to use Zipkin clients, because the ecosystem hosted client", "tokens": [1018, 257, 10754, 11, 562, 321, 1415, 281, 764, 1176, 647, 5843, 6982, 11, 570, 264, 11311, 19204, 6423], "temperature": 0.0, "avg_logprob": -0.1223292350769043, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.6266856619040482e-05}, {"id": 32, "seek": 13648, "start": 160.83999999999997, "end": 164.95999999999998, "text": " libraries as well, it was a bit problematic in polyglot environments because those clients", "tokens": [15148, 382, 731, 11, 309, 390, 257, 857, 19011, 294, 6754, 7191, 310, 12388, 570, 729, 6982], "temperature": 0.0, "avg_logprob": -0.1223292350769043, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.6266856619040482e-05}, {"id": 33, "seek": 16496, "start": 164.96, "end": 171.56, "text": " were using kind of inconsistent APIs, there was no standardization.", "tokens": [645, 1228, 733, 295, 36891, 21445, 11, 456, 390, 572, 3832, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15850370808651573, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.093846680712886e-05}, {"id": 34, "seek": 16496, "start": 171.56, "end": 176.12, "text": " And so this problem then was partially solved with open tracing.", "tokens": [400, 370, 341, 1154, 550, 390, 18886, 13041, 365, 1269, 25262, 13], "temperature": 0.0, "avg_logprob": -0.15850370808651573, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.093846680712886e-05}, {"id": 35, "seek": 16496, "start": 176.12, "end": 181.24, "text": " The scope of the project was a bit wider, there was a specification, there was a document", "tokens": [440, 11923, 295, 264, 1716, 390, 257, 857, 11842, 11, 456, 390, 257, 31256, 11, 456, 390, 257, 4166], "temperature": 0.0, "avg_logprob": -0.15850370808651573, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.093846680712886e-05}, {"id": 36, "seek": 16496, "start": 181.24, "end": 188.68, "text": " that defines which data should be collected and as well how the API in those languages", "tokens": [300, 23122, 597, 1412, 820, 312, 11087, 293, 382, 731, 577, 264, 9362, 294, 729, 8650], "temperature": 0.0, "avg_logprob": -0.15850370808651573, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.093846680712886e-05}, {"id": 37, "seek": 16496, "start": 188.68, "end": 190.44, "text": " should look like.", "tokens": [820, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.15850370808651573, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.093846680712886e-05}, {"id": 38, "seek": 19044, "start": 190.44, "end": 194.92, "text": " This enabled us to build reusable instrumentation libraries.", "tokens": [639, 15172, 505, 281, 1322, 41807, 7198, 399, 15148, 13], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 39, "seek": 19044, "start": 194.92, "end": 199.92, "text": " And then, even later, the open sensors project started with slightly different approach.", "tokens": [400, 550, 11, 754, 1780, 11, 264, 1269, 14840, 1716, 1409, 365, 4748, 819, 3109, 13], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 40, "seek": 19044, "start": 199.92, "end": 206.07999999999998, "text": " There was no specification, there was no API, but there was SDK that everybody could use", "tokens": [821, 390, 572, 31256, 11, 456, 390, 572, 9362, 11, 457, 456, 390, 37135, 300, 2201, 727, 764], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 41, "seek": 19044, "start": 206.07999999999998, "end": 208.64, "text": " and a collector.", "tokens": [293, 257, 23960, 13], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 42, "seek": 19044, "start": 208.64, "end": 214.68, "text": " So with open tracing, the approach was that developers would use the API and then at the", "tokens": [407, 365, 1269, 25262, 11, 264, 3109, 390, 300, 8849, 576, 764, 264, 9362, 293, 550, 412, 264], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 43, "seek": 19044, "start": 214.68, "end": 218.2, "text": " build time provide the SDK from a vendor.", "tokens": [1322, 565, 2893, 264, 37135, 490, 257, 24321, 13], "temperature": 0.0, "avg_logprob": -0.17341059781192394, "compression_ratio": 1.6425531914893616, "no_speech_prob": 3.177297912770882e-05}, {"id": 44, "seek": 21820, "start": 218.2, "end": 223.39999999999998, "text": " With open sensors, everybody would use the SDK and then in the collector decide where", "tokens": [2022, 1269, 14840, 11, 2201, 576, 764, 264, 37135, 293, 550, 294, 264, 23960, 4536, 689], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 45, "seek": 21820, "start": 223.39999999999998, "end": 225.51999999999998, "text": " the data should be sent.", "tokens": [264, 1412, 820, 312, 2279, 13], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 46, "seek": 21820, "start": 225.51999999999998, "end": 229.76, "text": " Those two projects were kind of competing and then finally they merged into open telemetry", "tokens": [3950, 732, 4455, 645, 733, 295, 15439, 293, 550, 2721, 436, 36427, 666, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 47, "seek": 21820, "start": 229.76, "end": 231.83999999999997, "text": " in 2019.", "tokens": [294, 6071, 13], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 48, "seek": 21820, "start": 231.83999999999997, "end": 238.39999999999998, "text": " So the hotel, it adopted all the pieces from open tracing and open sensors, but kind of", "tokens": [407, 264, 7622, 11, 309, 12175, 439, 264, 3755, 490, 1269, 25262, 293, 1269, 14840, 11, 457, 733, 295], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 49, "seek": 21820, "start": 238.39999999999998, "end": 243.32, "text": " the biggest innovation in hotel is the, at least in my view, is the auto instrumentation", "tokens": [264, 3880, 8504, 294, 7622, 307, 264, 11, 412, 1935, 294, 452, 1910, 11, 307, 264, 8399, 7198, 399], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 50, "seek": 21820, "start": 243.32, "end": 246.6, "text": " libraries or the agents.", "tokens": [15148, 420, 264, 12554, 13], "temperature": 0.0, "avg_logprob": -0.1555604368153185, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.194953387719579e-05}, {"id": 51, "seek": 24660, "start": 246.6, "end": 251.0, "text": " Those agents are production ready, most of them, because they were donated by one of", "tokens": [3950, 12554, 366, 4265, 1919, 11, 881, 295, 552, 11, 570, 436, 645, 23723, 538, 472, 295], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 52, "seek": 24660, "start": 251.0, "end": 257.4, "text": " the observability vendors, so they are, you know, production tested.", "tokens": [264, 9951, 2310, 22056, 11, 370, 436, 366, 11, 291, 458, 11, 4265, 8246, 13], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 53, "seek": 24660, "start": 257.4, "end": 261.48, "text": " So when we kind of summarize what happened is that we started with some instrumentation", "tokens": [407, 562, 321, 733, 295, 20858, 437, 2011, 307, 300, 321, 1409, 365, 512, 7198, 399], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 54, "seek": 24660, "start": 261.48, "end": 267.12, "text": " libraries, you know, with Zipkin project, then since we have some kind of standardization,", "tokens": [15148, 11, 291, 458, 11, 365, 1176, 647, 5843, 1716, 11, 550, 1670, 321, 362, 512, 733, 295, 3832, 2144, 11], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 55, "seek": 24660, "start": 267.12, "end": 272.68, "text": " we could build reusable instrumentation libraries and kind of create more sophisticated instrumentations", "tokens": [321, 727, 1322, 41807, 7198, 399, 15148, 293, 733, 295, 1884, 544, 16950, 7198, 763], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 56, "seek": 24660, "start": 272.68, "end": 274.28, "text": " for runtimes.", "tokens": [337, 49435, 1532, 13], "temperature": 0.0, "avg_logprob": -0.17081376618030025, "compression_ratio": 1.7755905511811023, "no_speech_prob": 0.00018990246462635696}, {"id": 57, "seek": 27428, "start": 274.28, "end": 279.79999999999995, "text": " And now we are in an age that we have available in open source agents or auto instrumentation", "tokens": [400, 586, 321, 366, 294, 364, 3205, 300, 321, 362, 2435, 294, 1269, 4009, 12554, 420, 8399, 7198, 399], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 58, "seek": 27428, "start": 279.79999999999995, "end": 284.96, "text": " libraries that we can just grab, put into our platforms, and we will get telemetry data", "tokens": [15148, 300, 321, 393, 445, 4444, 11, 829, 666, 527, 9473, 11, 293, 321, 486, 483, 4304, 5537, 627, 1412], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 59, "seek": 27428, "start": 284.96, "end": 286.88, "text": " almost for free.", "tokens": [1920, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 60, "seek": 27428, "start": 286.88, "end": 290.64, "text": " And I think, you know, so where are we going?", "tokens": [400, 286, 519, 11, 291, 458, 11, 370, 689, 366, 321, 516, 30], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 61, "seek": 27428, "start": 290.64, "end": 296.4, "text": " I think we are going into an era where we, as developers, we won't have to care about", "tokens": [286, 519, 321, 366, 516, 666, 364, 4249, 689, 321, 11, 382, 8849, 11, 321, 1582, 380, 362, 281, 1127, 466], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 62, "seek": 27428, "start": 296.4, "end": 298.4, "text": " how the telemetry is created for us.", "tokens": [577, 264, 4304, 5537, 627, 307, 2942, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.12947712322272878, "compression_ratio": 1.6238938053097345, "no_speech_prob": 4.597601218847558e-05}, {"id": 63, "seek": 29840, "start": 298.4, "end": 305.64, "text": " We will be, the instrumentation will become maybe the feature of the platform where we", "tokens": [492, 486, 312, 11, 264, 7198, 399, 486, 1813, 1310, 264, 4111, 295, 264, 3663, 689, 321], "temperature": 0.0, "avg_logprob": -0.13337068919894063, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.400892819627188e-05}, {"id": 64, "seek": 29840, "start": 305.64, "end": 307.64, "text": " deploy the application.", "tokens": [7274, 264, 3861, 13], "temperature": 0.0, "avg_logprob": -0.13337068919894063, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.400892819627188e-05}, {"id": 65, "seek": 29840, "start": 307.64, "end": 308.79999999999995, "text": " So this is one way to look at it.", "tokens": [407, 341, 307, 472, 636, 281, 574, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.13337068919894063, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.400892819627188e-05}, {"id": 66, "seek": 29840, "start": 308.79999999999995, "end": 314.15999999999997, "text": " The other way might be that the observability will shift left, and since we have this data,", "tokens": [440, 661, 636, 1062, 312, 300, 264, 9951, 2310, 486, 5513, 1411, 11, 293, 1670, 321, 362, 341, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13337068919894063, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.400892819627188e-05}, {"id": 67, "seek": 29840, "start": 314.15999999999997, "end": 322.52, "text": " we will start utilizing it for other use cases, probably like testing and security.", "tokens": [321, 486, 722, 26775, 309, 337, 661, 764, 3331, 11, 1391, 411, 4997, 293, 3825, 13], "temperature": 0.0, "avg_logprob": -0.13337068919894063, "compression_ratio": 1.6161616161616161, "no_speech_prob": 3.400892819627188e-05}, {"id": 68, "seek": 32252, "start": 322.52, "end": 329.0, "text": " So with that, I would like to move to the open telemetry, and it's obviously open source", "tokens": [407, 365, 300, 11, 286, 576, 411, 281, 1286, 281, 264, 1269, 4304, 5537, 627, 11, 293, 309, 311, 2745, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 69, "seek": 32252, "start": 329.0, "end": 333.79999999999995, "text": " project hosted in the cognitive computing foundation, and its main goal is to provide", "tokens": [1716, 19204, 294, 264, 15605, 15866, 7030, 11, 293, 1080, 2135, 3387, 307, 281, 2893], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 70, "seek": 32252, "start": 333.79999999999995, "end": 337.0, "text": " the vendor or neutral telemetry data collection.", "tokens": [264, 24321, 420, 10598, 4304, 5537, 627, 1412, 5765, 13], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 71, "seek": 32252, "start": 337.0, "end": 343.47999999999996, "text": " It's the second most active project in CNCF after Kubernetes, so it's quite large.", "tokens": [467, 311, 264, 1150, 881, 4967, 1716, 294, 48714, 37, 934, 23145, 11, 370, 309, 311, 1596, 2416, 13], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 72, "seek": 32252, "start": 343.47999999999996, "end": 346.56, "text": " And there are several independent components that we can use.", "tokens": [400, 456, 366, 2940, 6695, 6677, 300, 321, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 73, "seek": 32252, "start": 346.56, "end": 352.44, "text": " There is a specification that defines what data should be collected and how the API should", "tokens": [821, 307, 257, 31256, 300, 23122, 437, 1412, 820, 312, 11087, 293, 577, 264, 9362, 820], "temperature": 0.0, "avg_logprob": -0.1858317651481272, "compression_ratio": 1.6392857142857142, "no_speech_prob": 6.94108457537368e-05}, {"id": 74, "seek": 35244, "start": 352.44, "end": 358.44, "text": " look like, and obviously then there is the implementation of the API, the SDK and the", "tokens": [574, 411, 11, 293, 2745, 550, 456, 307, 264, 11420, 295, 264, 9362, 11, 264, 37135, 293, 264], "temperature": 0.0, "avg_logprob": -0.1522116829367245, "compression_ratio": 1.618421052631579, "no_speech_prob": 8.700363105162978e-05}, {"id": 75, "seek": 35244, "start": 358.44, "end": 362.88, "text": " standard data model called OTLP or open telemetry protocol.", "tokens": [3832, 1412, 2316, 1219, 38617, 45196, 420, 1269, 4304, 5537, 627, 10336, 13], "temperature": 0.0, "avg_logprob": -0.1522116829367245, "compression_ratio": 1.618421052631579, "no_speech_prob": 8.700363105162978e-05}, {"id": 76, "seek": 35244, "start": 362.88, "end": 368.32, "text": " These four pieces are meant to be used primarily by instrumentation authors or the people that", "tokens": [1981, 1451, 3755, 366, 4140, 281, 312, 1143, 10029, 538, 7198, 399, 16552, 420, 264, 561, 300], "temperature": 0.0, "avg_logprob": -0.1522116829367245, "compression_ratio": 1.618421052631579, "no_speech_prob": 8.700363105162978e-05}, {"id": 77, "seek": 35244, "start": 368.32, "end": 371.56, "text": " work on the observability systems.", "tokens": [589, 322, 264, 9951, 2310, 3652, 13], "temperature": 0.0, "avg_logprob": -0.1522116829367245, "compression_ratio": 1.618421052631579, "no_speech_prob": 8.700363105162978e-05}, {"id": 78, "seek": 35244, "start": 371.56, "end": 377.28, "text": " And last two components, the auto instrumentation or agent and collector are meant to be used", "tokens": [400, 1036, 732, 6677, 11, 264, 8399, 7198, 399, 420, 9461, 293, 23960, 366, 4140, 281, 312, 1143], "temperature": 0.0, "avg_logprob": -0.1522116829367245, "compression_ratio": 1.618421052631579, "no_speech_prob": 8.700363105162978e-05}, {"id": 79, "seek": 37728, "start": 377.28, "end": 382.59999999999997, "text": " by end users to kind of roll out observability in their organization.", "tokens": [538, 917, 5022, 281, 733, 295, 3373, 484, 9951, 2310, 294, 641, 4475, 13], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 80, "seek": 37728, "start": 382.59999999999997, "end": 386.47999999999996, "text": " To facilitate open telemetry deployment on Kubernetes, there is a Helm chart and Kubernetes", "tokens": [1407, 20207, 1269, 4304, 5537, 627, 19317, 322, 23145, 11, 456, 307, 257, 6128, 76, 6927, 293, 23145], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 81, "seek": 37728, "start": 386.47999999999996, "end": 391.0, "text": " operator.", "tokens": [12973, 13], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 82, "seek": 37728, "start": 391.0, "end": 396.15999999999997, "text": " What I would like to stress is that open telemetry is only about how we collect and create telemetry", "tokens": [708, 286, 576, 411, 281, 4244, 307, 300, 1269, 4304, 5537, 627, 307, 787, 466, 577, 321, 2500, 293, 1884, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 83, "seek": 37728, "start": 396.15999999999997, "end": 397.15999999999997, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 84, "seek": 37728, "start": 397.15999999999997, "end": 405.88, "text": " It's not a platform that you can deploy, it doesn't provide any storage or query APIs.", "tokens": [467, 311, 406, 257, 3663, 300, 291, 393, 7274, 11, 309, 1177, 380, 2893, 604, 6725, 420, 14581, 21445, 13], "temperature": 0.0, "avg_logprob": -0.13570971130042947, "compression_ratio": 1.5732758620689655, "no_speech_prob": 2.252528429380618e-05}, {"id": 85, "seek": 40588, "start": 405.88, "end": 409.56, "text": " So now let's go to the main part, the Kubernetes operator.", "tokens": [407, 586, 718, 311, 352, 281, 264, 2135, 644, 11, 264, 23145, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 86, "seek": 40588, "start": 409.56, "end": 416.36, "text": " The operator itself, it's a Golang application, it uses QBuilder and operator SDK, and it", "tokens": [440, 12973, 2564, 11, 309, 311, 257, 36319, 656, 3861, 11, 309, 4960, 1249, 28110, 793, 260, 293, 12973, 37135, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 87, "seek": 40588, "start": 416.36, "end": 418.88, "text": " has three primary use cases.", "tokens": [575, 1045, 6194, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 88, "seek": 40588, "start": 418.88, "end": 424.2, "text": " It can deploy the open telemetry collector as a deployment, demon set, stateful set.", "tokens": [467, 393, 7274, 264, 1269, 4304, 5537, 627, 23960, 382, 257, 19317, 11, 14283, 992, 11, 1785, 906, 992, 13], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 89, "seek": 40588, "start": 424.2, "end": 429.2, "text": " It can as well inject the collector as a side card to your workload.", "tokens": [467, 393, 382, 731, 10711, 264, 23960, 382, 257, 1252, 2920, 281, 428, 20139, 13], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 90, "seek": 40588, "start": 429.2, "end": 434.76, "text": " The second use case is that it can instrument your workloads running on Kubernetes by using", "tokens": [440, 1150, 764, 1389, 307, 300, 309, 393, 7198, 428, 32452, 2614, 322, 23145, 538, 1228], "temperature": 0.0, "avg_logprob": -0.1705945509451407, "compression_ratio": 1.7125506072874495, "no_speech_prob": 2.6097273803316057e-05}, {"id": 91, "seek": 43476, "start": 434.76, "end": 439.4, "text": " those instrumentation libraries or agents from open telemetry.", "tokens": [729, 7198, 399, 15148, 420, 12554, 490, 1269, 4304, 5537, 627, 13], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 92, "seek": 43476, "start": 439.4, "end": 443.68, "text": " And last but not least, it integrates with Prometheus ecosystem.", "tokens": [400, 1036, 457, 406, 1935, 11, 309, 3572, 1024, 365, 2114, 649, 42209, 11311, 13], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 93, "seek": 43476, "start": 443.68, "end": 449.28, "text": " It can read the service and pod monitors, get the scraped targets, and split them across", "tokens": [467, 393, 1401, 264, 2643, 293, 2497, 26518, 11, 483, 264, 13943, 3452, 12911, 11, 293, 7472, 552, 2108], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 94, "seek": 43476, "start": 449.28, "end": 453.36, "text": " the collector instances that you have deployed.", "tokens": [264, 23960, 14519, 300, 291, 362, 17826, 13], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 95, "seek": 43476, "start": 453.36, "end": 458.48, "text": " To enable this functionality, the operator provides two CRDs, one for the collector that", "tokens": [1407, 9528, 341, 14980, 11, 264, 12973, 6417, 732, 14123, 35, 82, 11, 472, 337, 264, 23960, 300], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 96, "seek": 43476, "start": 458.48, "end": 462.2, "text": " is used to deploy the collector and integrate the Prometheus.", "tokens": [307, 1143, 281, 7274, 264, 23960, 293, 13365, 264, 2114, 649, 42209, 13], "temperature": 0.0, "avg_logprob": -0.1336420810583866, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.1113442168571055e-05}, {"id": 97, "seek": 46220, "start": 462.2, "end": 467.24, "text": " And the second one is the instrumentation CRD, where you define how the applications", "tokens": [400, 264, 1150, 472, 307, 264, 7198, 399, 14123, 35, 11, 689, 291, 6964, 577, 264, 5821], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 98, "seek": 46220, "start": 467.24, "end": 469.92, "text": " should be instrumented.", "tokens": [820, 312, 7198, 292, 13], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 99, "seek": 46220, "start": 469.92, "end": 478.96, "text": " The operator itself then can be deployed through manifest files, home chart, or OLM.", "tokens": [440, 12973, 2564, 550, 393, 312, 17826, 807, 10067, 7098, 11, 1280, 6927, 11, 420, 422, 43, 44, 13], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 100, "seek": 46220, "start": 478.96, "end": 481.88, "text": " So what we see here is the Kubernetes cluster.", "tokens": [407, 437, 321, 536, 510, 307, 264, 23145, 13630, 13], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 101, "seek": 46220, "start": 481.88, "end": 486.15999999999997, "text": " There are three workloads, pod one, pod two, and pod three.", "tokens": [821, 366, 1045, 32452, 11, 2497, 472, 11, 2497, 732, 11, 293, 2497, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 102, "seek": 46220, "start": 486.15999999999997, "end": 491.59999999999997, "text": " The first workload is instrumented with the hotel SDK directly, so when we were building", "tokens": [440, 700, 20139, 307, 7198, 292, 365, 264, 7622, 37135, 3838, 11, 370, 562, 321, 645, 2390], "temperature": 0.0, "avg_logprob": -0.1724103750641813, "compression_ratio": 1.6074380165289257, "no_speech_prob": 3.7699381209677085e-05}, {"id": 103, "seek": 49160, "start": 491.6, "end": 498.44, "text": " this application, we pulled in the hotel dependency and we compiled it against it and used those", "tokens": [341, 3861, 11, 321, 7373, 294, 264, 7622, 33621, 293, 321, 36548, 309, 1970, 309, 293, 1143, 729], "temperature": 0.0, "avg_logprob": -0.18029338710910672, "compression_ratio": 1.7522522522522523, "no_speech_prob": 5.470678297569975e-05}, {"id": 104, "seek": 49160, "start": 498.44, "end": 504.24, "text": " APIs directly in our business code and in the middlewares that we are using.", "tokens": [21445, 3838, 294, 527, 1606, 3089, 293, 294, 264, 2808, 4151, 495, 300, 321, 366, 1228, 13], "temperature": 0.0, "avg_logprob": -0.18029338710910672, "compression_ratio": 1.7522522522522523, "no_speech_prob": 5.470678297569975e-05}, {"id": 105, "seek": 49160, "start": 504.24, "end": 510.36, "text": " The second pod is using the auto instrumentation libraries that were injected by the operator", "tokens": [440, 1150, 2497, 307, 1228, 264, 8399, 7198, 399, 15148, 300, 645, 36967, 538, 264, 12973], "temperature": 0.0, "avg_logprob": -0.18029338710910672, "compression_ratio": 1.7522522522522523, "no_speech_prob": 5.470678297569975e-05}, {"id": 106, "seek": 49160, "start": 510.36, "end": 514.52, "text": " through the Venetian webhook.", "tokens": [807, 264, 11182, 302, 952, 3670, 71, 1212, 13], "temperature": 0.0, "avg_logprob": -0.18029338710910672, "compression_ratio": 1.7522522522522523, "no_speech_prob": 5.470678297569975e-05}, {"id": 107, "seek": 49160, "start": 514.52, "end": 519.96, "text": " And the third pod is using Zipkin instrumentation and Prometheus instrumentation libraries,", "tokens": [400, 264, 2636, 2497, 307, 1228, 1176, 647, 5843, 7198, 399, 293, 2114, 649, 42209, 7198, 399, 15148, 11], "temperature": 0.0, "avg_logprob": -0.18029338710910672, "compression_ratio": 1.7522522522522523, "no_speech_prob": 5.470678297569975e-05}, {"id": 108, "seek": 51996, "start": 519.96, "end": 527.44, "text": " and it has the collector sidecar as well injected by the operator.", "tokens": [293, 309, 575, 264, 23960, 1252, 6166, 382, 731, 36967, 538, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.1571877978064797, "compression_ratio": 1.7230046948356808, "no_speech_prob": 4.0757782699074596e-05}, {"id": 109, "seek": 51996, "start": 527.44, "end": 535.84, "text": " So essentially the operator there, it reconciles three open telemetry CRs, two for the collector,", "tokens": [407, 4476, 264, 12973, 456, 11, 309, 9993, 3208, 279, 1045, 1269, 4304, 5537, 627, 14123, 82, 11, 732, 337, 264, 23960, 11], "temperature": 0.0, "avg_logprob": -0.1571877978064797, "compression_ratio": 1.7230046948356808, "no_speech_prob": 4.0757782699074596e-05}, {"id": 110, "seek": 51996, "start": 535.84, "end": 538.12, "text": " and one instrumentation.", "tokens": [293, 472, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.1571877978064797, "compression_ratio": 1.7230046948356808, "no_speech_prob": 4.0757782699074596e-05}, {"id": 111, "seek": 51996, "start": 538.12, "end": 542.76, "text": " And then all these workloads, they send data to the collector deployed, probably as a demon", "tokens": [400, 550, 439, 613, 32452, 11, 436, 2845, 1412, 281, 264, 23960, 17826, 11, 1391, 382, 257, 14283], "temperature": 0.0, "avg_logprob": -0.1571877978064797, "compression_ratio": 1.7230046948356808, "no_speech_prob": 4.0757782699074596e-05}, {"id": 112, "seek": 51996, "start": 542.76, "end": 548.44, "text": " set, and then this collector then does some data normalization and sends finally data", "tokens": [992, 11, 293, 550, 341, 23960, 550, 775, 512, 1412, 2710, 2144, 293, 14790, 2721, 1412], "temperature": 0.0, "avg_logprob": -0.1571877978064797, "compression_ratio": 1.7230046948356808, "no_speech_prob": 4.0757782699074596e-05}, {"id": 113, "seek": 54844, "start": 548.44, "end": 557.6, "text": " into platform of your choice, which can be Prometheus for metrics, Yeager for traces.", "tokens": [666, 3663, 295, 428, 3922, 11, 597, 393, 312, 2114, 649, 42209, 337, 16367, 11, 835, 3557, 337, 26076, 13], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 114, "seek": 54844, "start": 557.6, "end": 562.96, "text": " With that, I would like to move to the second part, explaining the CRDs in more detail.", "tokens": [2022, 300, 11, 286, 576, 411, 281, 1286, 281, 264, 1150, 644, 11, 13468, 264, 14123, 35, 82, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 115, "seek": 54844, "start": 562.96, "end": 563.96, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 116, "seek": 54844, "start": 563.96, "end": 566.36, "text": " The microphone should work.", "tokens": [440, 10952, 820, 589, 13], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 117, "seek": 54844, "start": 566.36, "end": 572.96, "text": " Yeah, so with the CRDs for today, we wanted to show both of them, and we start with the", "tokens": [865, 11, 370, 365, 264, 14123, 35, 82, 337, 965, 11, 321, 1415, 281, 855, 1293, 295, 552, 11, 293, 321, 722, 365, 264], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 118, "seek": 54844, "start": 572.96, "end": 574.46, "text": " collector one.", "tokens": [23960, 472, 13], "temperature": 0.0, "avg_logprob": -0.2744468265109592, "compression_ratio": 1.4927536231884058, "no_speech_prob": 3.607170583563857e-05}, {"id": 119, "seek": 57446, "start": 574.46, "end": 580.2, "text": " The collector CRD is a bit loaded, so therefore we picked a few things here, which I would", "tokens": [440, 23960, 14123, 35, 307, 257, 857, 13210, 11, 370, 4412, 321, 6183, 257, 1326, 721, 510, 11, 597, 286, 576], "temperature": 0.0, "avg_logprob": -0.15782747365007496, "compression_ratio": 1.6398305084745763, "no_speech_prob": 7.580652163596824e-05}, {"id": 120, "seek": 57446, "start": 580.2, "end": 585.12, "text": " say are the most used or important.", "tokens": [584, 366, 264, 881, 1143, 420, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15782747365007496, "compression_ratio": 1.6398305084745763, "no_speech_prob": 7.580652163596824e-05}, {"id": 121, "seek": 57446, "start": 585.12, "end": 588.64, "text": " So as Pawe mentioned, there are different deployment modes, different use cases for", "tokens": [407, 382, 3426, 826, 2835, 11, 456, 366, 819, 19317, 14068, 11, 819, 764, 3331, 337], "temperature": 0.0, "avg_logprob": -0.15782747365007496, "compression_ratio": 1.6398305084745763, "no_speech_prob": 7.580652163596824e-05}, {"id": 122, "seek": 57446, "start": 588.64, "end": 596.64, "text": " the open telemetry collector, and in the specification, we can go to the mode and just specify it there.", "tokens": [264, 1269, 4304, 5537, 627, 23960, 11, 293, 294, 264, 31256, 11, 321, 393, 352, 281, 264, 4391, 293, 445, 16500, 309, 456, 13], "temperature": 0.0, "avg_logprob": -0.15782747365007496, "compression_ratio": 1.6398305084745763, "no_speech_prob": 7.580652163596824e-05}, {"id": 123, "seek": 57446, "start": 596.64, "end": 601.6, "text": " There's a handy thing, which is the sidecar, we will see it afterwards.", "tokens": [821, 311, 257, 13239, 551, 11, 597, 307, 264, 1252, 6166, 11, 321, 486, 536, 309, 10543, 13], "temperature": 0.0, "avg_logprob": -0.15782747365007496, "compression_ratio": 1.6398305084745763, "no_speech_prob": 7.580652163596824e-05}, {"id": 124, "seek": 60160, "start": 601.6, "end": 606.48, "text": " And if we want to use it, we only go to the part definition of our deployment and inject", "tokens": [400, 498, 321, 528, 281, 764, 309, 11, 321, 787, 352, 281, 264, 644, 7123, 295, 527, 19317, 293, 10711], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 125, "seek": 60160, "start": 606.48, "end": 610.28, "text": " the annotation we see on the top right.", "tokens": [264, 48654, 321, 536, 322, 264, 1192, 558, 13], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 126, "seek": 60160, "start": 610.28, "end": 616.28, "text": " And if we go with the deployment mode or something like this, and we want to expose it for collecting", "tokens": [400, 498, 321, 352, 365, 264, 19317, 4391, 420, 746, 411, 341, 11, 293, 321, 528, 281, 19219, 309, 337, 12510], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 127, "seek": 60160, "start": 616.28, "end": 621.08, "text": " metrics, locks, and traces from a different system, for example, we can use the Ingress", "tokens": [16367, 11, 20703, 11, 293, 26076, 490, 257, 819, 1185, 11, 337, 1365, 11, 321, 393, 764, 264, 682, 3091], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 128, "seek": 60160, "start": 621.08, "end": 627.48, "text": " type, we can set there a lot of more, we configure there a lot of more like also the annotations,", "tokens": [2010, 11, 321, 393, 992, 456, 257, 688, 295, 544, 11, 321, 22162, 456, 257, 688, 295, 544, 411, 611, 264, 25339, 763, 11], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 129, "seek": 60160, "start": 627.48, "end": 630.0, "text": " your Ingress class.", "tokens": [428, 682, 3091, 1508, 13], "temperature": 0.0, "avg_logprob": -0.17317652069361864, "compression_ratio": 1.847457627118644, "no_speech_prob": 8.590170182287693e-05}, {"id": 130, "seek": 63000, "start": 630.0, "end": 634.84, "text": " But yeah, mainly the operator takes care of everything, creating services, also is able", "tokens": [583, 1338, 11, 8704, 264, 12973, 2516, 1127, 295, 1203, 11, 4084, 3328, 11, 611, 307, 1075], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 131, "seek": 63000, "start": 634.84, "end": 637.56, "text": " to balance your load there.", "tokens": [281, 4772, 428, 3677, 456, 13], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 132, "seek": 63000, "start": 637.56, "end": 643.92, "text": " And yeah, the last thing here is then the image section, which is also important.", "tokens": [400, 1338, 11, 264, 1036, 551, 510, 307, 550, 264, 3256, 3541, 11, 597, 307, 611, 1021, 13], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 133, "seek": 63000, "start": 643.92, "end": 648.96, "text": " With the open telemetry operator, it usually ships the core distribution of open telemetry", "tokens": [2022, 264, 1269, 4304, 5537, 627, 12973, 11, 309, 2673, 11434, 264, 4965, 7316, 295, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 134, "seek": 63000, "start": 648.96, "end": 649.96, "text": " by default.", "tokens": [538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 135, "seek": 63000, "start": 649.96, "end": 655.64, "text": " So in open telemetry, the collector is split into two repositories when you go up and look", "tokens": [407, 294, 1269, 4304, 5537, 627, 11, 264, 23960, 307, 7472, 666, 732, 22283, 2083, 562, 291, 352, 493, 293, 574], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 136, "seek": 63000, "start": 655.64, "end": 657.64, "text": " at GitHub.", "tokens": [412, 23331, 13], "temperature": 0.0, "avg_logprob": -0.14087910328096556, "compression_ratio": 1.6611570247933884, "no_speech_prob": 6.185289385030046e-05}, {"id": 137, "seek": 65764, "start": 657.64, "end": 664.24, "text": " So in core, you will find OTP, a logging exporter, so some basic stuff.", "tokens": [407, 294, 4965, 11, 291, 486, 915, 422, 16804, 11, 257, 27991, 1278, 6122, 11, 370, 512, 3875, 1507, 13], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 138, "seek": 65764, "start": 664.24, "end": 667.28, "text": " And in Contrip, you find basically everything.", "tokens": [400, 294, 4839, 8400, 11, 291, 915, 1936, 1203, 13], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 139, "seek": 65764, "start": 667.28, "end": 674.12, "text": " So if you want to send your traces to some proprietary vendor or to J\u00e4ger, you probably", "tokens": [407, 498, 291, 528, 281, 2845, 428, 26076, 281, 512, 38992, 24321, 420, 281, 508, 737, 1321, 11, 291, 1391], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 140, "seek": 65764, "start": 674.12, "end": 676.52, "text": " need to look there.", "tokens": [643, 281, 574, 456, 13], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 141, "seek": 65764, "start": 676.52, "end": 679.52, "text": " Okay, the next thing is then the configuration.", "tokens": [1033, 11, 264, 958, 551, 307, 550, 264, 11694, 13], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 142, "seek": 65764, "start": 679.52, "end": 684.72, "text": " The configuration for the open telemetry collector is here provided like it's usually done for", "tokens": [440, 11694, 337, 264, 1269, 4304, 5537, 627, 23960, 307, 510, 5649, 411, 309, 311, 2673, 1096, 337], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 143, "seek": 65764, "start": 684.72, "end": 686.04, "text": " the collector itself.", "tokens": [264, 23960, 2564, 13], "temperature": 0.0, "avg_logprob": -0.20802746004271275, "compression_ratio": 1.5934959349593496, "no_speech_prob": 5.055745714344084e-05}, {"id": 144, "seek": 68604, "start": 686.04, "end": 688.56, "text": " So it's passed directly forward.", "tokens": [407, 309, 311, 4678, 3838, 2128, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 145, "seek": 68604, "start": 688.56, "end": 690.5999999999999, "text": " It's split it into three parts here.", "tokens": [467, 311, 7472, 309, 666, 1045, 3166, 510, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 146, "seek": 68604, "start": 690.5999999999999, "end": 691.9599999999999, "text": " We see the receiving part there.", "tokens": [492, 536, 264, 10040, 644, 456, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 147, "seek": 68604, "start": 691.9599999999999, "end": 694.8, "text": " We specify our OTP receiver.", "tokens": [492, 16500, 527, 422, 16804, 20086, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 148, "seek": 68604, "start": 694.8, "end": 697.64, "text": " Here it's accepts GRPC on a specific board.", "tokens": [1692, 309, 311, 33538, 10903, 12986, 322, 257, 2685, 3150, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 149, "seek": 68604, "start": 697.64, "end": 703.4, "text": " It could also be there that we specify a prometers receiver, which is then scraping something.", "tokens": [467, 727, 611, 312, 456, 300, 321, 16500, 257, 37786, 433, 20086, 11, 597, 307, 550, 43738, 746, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 150, "seek": 68604, "start": 703.4, "end": 707.8, "text": " Then the optional part is basically the processing part.", "tokens": [1396, 264, 17312, 644, 307, 1936, 264, 9007, 644, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 151, "seek": 68604, "start": 707.8, "end": 713.3199999999999, "text": " We might want to save some resources and we batch them our telemetry data.", "tokens": [492, 1062, 528, 281, 3155, 512, 3593, 293, 321, 15245, 552, 527, 4304, 5537, 627, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18864014035179502, "compression_ratio": 1.654320987654321, "no_speech_prob": 2.5445509891142137e-05}, {"id": 152, "seek": 71332, "start": 713.32, "end": 716.9200000000001, "text": " And yeah, there are other useful things.", "tokens": [400, 1338, 11, 456, 366, 661, 4420, 721, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 153, "seek": 71332, "start": 716.9200000000001, "end": 720.5200000000001, "text": " And on the exporter section, here we use the logging exporter, which is part of the", "tokens": [400, 322, 264, 1278, 6122, 3541, 11, 510, 321, 764, 264, 27991, 1278, 6122, 11, 597, 307, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 154, "seek": 71332, "start": 720.5200000000001, "end": 724.2800000000001, "text": " core distribution, but you can configure whatever you like.", "tokens": [4965, 7316, 11, 457, 291, 393, 22162, 2035, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 155, "seek": 71332, "start": 724.2800000000001, "end": 728.7600000000001, "text": " You can also have multiple exporters for one resource.", "tokens": [509, 393, 611, 362, 3866, 1278, 12168, 337, 472, 7684, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 156, "seek": 71332, "start": 728.7600000000001, "end": 729.7600000000001, "text": " There is one thing.", "tokens": [821, 307, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 157, "seek": 71332, "start": 729.7600000000001, "end": 731.44, "text": " On the right side, we see the extensions.", "tokens": [1282, 264, 558, 1252, 11, 321, 536, 264, 25129, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 158, "seek": 71332, "start": 731.44, "end": 735.44, "text": " It didn't fit on the slide, so it's there in this box.", "tokens": [467, 994, 380, 3318, 322, 264, 4137, 11, 370, 309, 311, 456, 294, 341, 2424, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 159, "seek": 71332, "start": 735.44, "end": 739.9200000000001, "text": " This is then used if you have, for example, an exporter, which needs some additional", "tokens": [639, 307, 550, 1143, 498, 291, 362, 11, 337, 1365, 11, 364, 1278, 6122, 11, 597, 2203, 512, 4497], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 160, "seek": 71332, "start": 739.9200000000001, "end": 740.9200000000001, "text": " headers.", "tokens": [45101, 13], "temperature": 0.0, "avg_logprob": -0.14347139994303384, "compression_ratio": 1.7110266159695817, "no_speech_prob": 9.008237248053774e-05}, {"id": 161, "seek": 74092, "start": 740.92, "end": 744.28, "text": " Yeah, you want to set a barrier token or something else.", "tokens": [865, 11, 291, 528, 281, 992, 257, 13357, 14862, 420, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 162, "seek": 74092, "start": 744.28, "end": 746.1999999999999, "text": " You can do it there.", "tokens": [509, 393, 360, 309, 456, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 163, "seek": 74092, "start": 746.1999999999999, "end": 750.12, "text": " And then finally, we go to the service section where we have different pipelines for each", "tokens": [400, 550, 2721, 11, 321, 352, 281, 264, 2643, 3541, 689, 321, 362, 819, 40168, 337, 1184], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 164, "seek": 74092, "start": 750.12, "end": 751.5999999999999, "text": " signal.", "tokens": [6358, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 165, "seek": 74092, "start": 751.5999999999999, "end": 761.24, "text": " And then we can then configure a processor and receiver and exporter in the way we wanted.", "tokens": [400, 550, 321, 393, 550, 22162, 257, 15321, 293, 20086, 293, 1278, 6122, 294, 264, 636, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 166, "seek": 74092, "start": 761.24, "end": 766.56, "text": " So then there is another CD, which is used for the auto instrumentation.", "tokens": [407, 550, 456, 307, 1071, 6743, 11, 597, 307, 1143, 337, 264, 8399, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 167, "seek": 74092, "start": 766.56, "end": 769.28, "text": " And it looks slightly different.", "tokens": [400, 309, 1542, 4748, 819, 13], "temperature": 0.0, "avg_logprob": -0.20786229183799343, "compression_ratio": 1.6244541484716157, "no_speech_prob": 3.319227107567713e-05}, {"id": 168, "seek": 76928, "start": 769.28, "end": 773.0799999999999, "text": " So here we have also the, in the specification, we have the exporter.", "tokens": [407, 510, 321, 362, 611, 264, 11, 294, 264, 31256, 11, 321, 362, 264, 1278, 6122, 13], "temperature": 0.0, "avg_logprob": -0.20979188836139182, "compression_ratio": 1.6589861751152073, "no_speech_prob": 2.6651645384845324e-05}, {"id": 169, "seek": 76928, "start": 773.0799999999999, "end": 781.3199999999999, "text": " And the exporter only exports OTP, so which means if we want to export it to some, yeah,", "tokens": [400, 264, 1278, 6122, 787, 31428, 422, 16804, 11, 370, 597, 1355, 498, 321, 528, 281, 10725, 309, 281, 512, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.20979188836139182, "compression_ratio": 1.6589861751152073, "no_speech_prob": 2.6651645384845324e-05}, {"id": 170, "seek": 76928, "start": 781.3199999999999, "end": 786.4, "text": " back end of our choice, we usually instrument our application directly then forward this", "tokens": [646, 917, 295, 527, 3922, 11, 321, 2673, 7198, 527, 3861, 3838, 550, 2128, 341], "temperature": 0.0, "avg_logprob": -0.20979188836139182, "compression_ratio": 1.6589861751152073, "no_speech_prob": 2.6651645384845324e-05}, {"id": 171, "seek": 76928, "start": 786.4, "end": 790.6, "text": " traces to a collector instance, which is running next to it.", "tokens": [26076, 281, 257, 23960, 5197, 11, 597, 307, 2614, 958, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.20979188836139182, "compression_ratio": 1.6589861751152073, "no_speech_prob": 2.6651645384845324e-05}, {"id": 172, "seek": 76928, "start": 790.6, "end": 795.0, "text": " And yeah, we can use the power of these processors.", "tokens": [400, 1338, 11, 321, 393, 764, 264, 1347, 295, 613, 27751, 13], "temperature": 0.0, "avg_logprob": -0.20979188836139182, "compression_ratio": 1.6589861751152073, "no_speech_prob": 2.6651645384845324e-05}, {"id": 173, "seek": 79500, "start": 795.0, "end": 800.84, "text": " Yeah, then we can configure some other useful things like how the context is propagated", "tokens": [865, 11, 550, 321, 393, 22162, 512, 661, 4420, 721, 411, 577, 264, 4319, 307, 12425, 770], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 174, "seek": 79500, "start": 800.84, "end": 802.76, "text": " and the sample rate.", "tokens": [293, 264, 6889, 3314, 13], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 175, "seek": 79500, "start": 802.76, "end": 805.08, "text": " And to use it, it's also quite easy.", "tokens": [400, 281, 764, 309, 11, 309, 311, 611, 1596, 1858, 13], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 176, "seek": 79500, "start": 805.08, "end": 806.36, "text": " So we have our deployment.", "tokens": [407, 321, 362, 527, 19317, 13], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 177, "seek": 79500, "start": 806.36, "end": 811.24, "text": " In this case, we can, it can choose from this list of supported languages.", "tokens": [682, 341, 1389, 11, 321, 393, 11, 309, 393, 2826, 490, 341, 1329, 295, 8104, 8650, 13], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 178, "seek": 79500, "start": 811.24, "end": 817.88, "text": " We might use Java and we only set this annotation on the port level and it will take care of", "tokens": [492, 1062, 764, 10745, 293, 321, 787, 992, 341, 48654, 322, 264, 2436, 1496, 293, 309, 486, 747, 1127, 295], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 179, "seek": 79500, "start": 817.88, "end": 824.28, "text": " adding the SDK and also setting and configuring the environment variables.", "tokens": [5127, 264, 37135, 293, 611, 3287, 293, 6662, 1345, 264, 2823, 9102, 13], "temperature": 0.0, "avg_logprob": -0.14443245842343286, "compression_ratio": 1.6147859922178989, "no_speech_prob": 7.358328002737835e-05}, {"id": 180, "seek": 82428, "start": 824.28, "end": 831.64, "text": " If we use something like Rust, we can also use the inject SDK annotation to configure", "tokens": [759, 321, 764, 746, 411, 34952, 11, 321, 393, 611, 764, 264, 10711, 37135, 48654, 281, 22162], "temperature": 0.0, "avg_logprob": -0.17132735532872817, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.00010035618470283225}, {"id": 181, "seek": 82428, "start": 831.64, "end": 837.8399999999999, "text": " then, yeah, just the destination because then SDK should be there.", "tokens": [550, 11, 1338, 11, 445, 264, 12236, 570, 550, 37135, 820, 312, 456, 13], "temperature": 0.0, "avg_logprob": -0.17132735532872817, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.00010035618470283225}, {"id": 182, "seek": 82428, "start": 837.8399999999999, "end": 843.6, "text": " And if we have a setup where there is, let's say, some proxy in front, like Envoy, we can", "tokens": [400, 498, 321, 362, 257, 8657, 689, 456, 307, 11, 718, 311, 584, 11, 512, 29690, 294, 1868, 11, 411, 2193, 35176, 11, 321, 393], "temperature": 0.0, "avg_logprob": -0.17132735532872817, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.00010035618470283225}, {"id": 183, "seek": 82428, "start": 843.6, "end": 852.8, "text": " then just skip the, yeah, adding the auto instrumentation there by only configuring the container", "tokens": [550, 445, 10023, 264, 11, 1338, 11, 5127, 264, 8399, 7198, 399, 456, 538, 787, 6662, 1345, 264, 10129], "temperature": 0.0, "avg_logprob": -0.17132735532872817, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.00010035618470283225}, {"id": 184, "seek": 85280, "start": 852.8, "end": 856.3599999999999, "text": " names we want to instrument.", "tokens": [5288, 321, 528, 281, 7198, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 185, "seek": 85280, "start": 856.3599999999999, "end": 860.4399999999999, "text": " And we will see this in a minute, a bit more in detail.", "tokens": [400, 321, 486, 536, 341, 294, 257, 3456, 11, 257, 857, 544, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 186, "seek": 85280, "start": 860.4399999999999, "end": 863.12, "text": " So this is then basically what we would need to do.", "tokens": [407, 341, 307, 550, 1936, 437, 321, 576, 643, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 187, "seek": 85280, "start": 863.12, "end": 867.68, "text": " So we create this instrumentation, we add this annotation on the left.", "tokens": [407, 321, 1884, 341, 7198, 399, 11, 321, 909, 341, 48654, 322, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 188, "seek": 85280, "start": 867.68, "end": 871.16, "text": " We see the pot, there is our application.", "tokens": [492, 536, 264, 1847, 11, 456, 307, 527, 3861, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 189, "seek": 85280, "start": 871.16, "end": 876.4399999999999, "text": " And in this gray box, you see what automatically is added.", "tokens": [400, 294, 341, 10855, 2424, 11, 291, 536, 437, 6772, 307, 3869, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 190, "seek": 85280, "start": 876.4399999999999, "end": 882.0, "text": " And this is then forwarded in this example to a collector.", "tokens": [400, 341, 307, 550, 2128, 292, 294, 341, 1365, 281, 257, 23960, 13], "temperature": 0.0, "avg_logprob": -0.17419782638549805, "compression_ratio": 1.7311320754716981, "no_speech_prob": 4.7440098569495603e-05}, {"id": 191, "seek": 88200, "start": 882.0, "end": 883.64, "text": " And yeah, how does this work?", "tokens": [400, 1338, 11, 577, 775, 341, 589, 30], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 192, "seek": 88200, "start": 883.64, "end": 890.24, "text": " So the operator in that mission web hook, he will add this in its container.", "tokens": [407, 264, 12973, 294, 300, 4447, 3670, 6328, 11, 415, 486, 909, 341, 294, 1080, 10129, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 193, "seek": 88200, "start": 890.24, "end": 892.6, "text": " On the top left, we see how the container looks before.", "tokens": [1282, 264, 1192, 1411, 11, 321, 536, 577, 264, 10129, 1542, 949, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 194, "seek": 88200, "start": 892.6, "end": 894.4, "text": " So there are no environment variables.", "tokens": [407, 456, 366, 572, 2823, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 195, "seek": 88200, "start": 894.4, "end": 898.48, "text": " It's just a plain application.", "tokens": [467, 311, 445, 257, 11121, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 196, "seek": 88200, "start": 898.48, "end": 903.32, "text": " And in the command section, there is then the copy, which copies the Java agent to our", "tokens": [400, 294, 264, 5622, 3541, 11, 456, 307, 550, 264, 5055, 11, 597, 14341, 264, 10745, 9461, 281, 527], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 197, "seek": 88200, "start": 903.32, "end": 904.6, "text": " original container.", "tokens": [3380, 10129, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 198, "seek": 88200, "start": 904.6, "end": 907.0, "text": " And on the right side, we see the final result.", "tokens": [400, 322, 264, 558, 1252, 11, 321, 536, 264, 2572, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1539337268242469, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.5306271456647664e-05}, {"id": 199, "seek": 90700, "start": 907.0, "end": 913.96, "text": " We see the Java tool options where the container is loaded, and then we see all this environment", "tokens": [492, 536, 264, 10745, 2290, 3956, 689, 264, 10129, 307, 13210, 11, 293, 550, 321, 536, 439, 341, 2823], "temperature": 0.0, "avg_logprob": -0.16626194545200892, "compression_ratio": 1.5471698113207548, "no_speech_prob": 5.213209806242958e-05}, {"id": 200, "seek": 90700, "start": 913.96, "end": 918.6, "text": " variables to configure our SDK.", "tokens": [9102, 281, 22162, 527, 37135, 13], "temperature": 0.0, "avg_logprob": -0.16626194545200892, "compression_ratio": 1.5471698113207548, "no_speech_prob": 5.213209806242958e-05}, {"id": 201, "seek": 90700, "start": 918.6, "end": 924.08, "text": " And finally, what we have seen also in the presentation from Nicholas previously, we", "tokens": [400, 2721, 11, 437, 321, 362, 1612, 611, 294, 264, 5860, 490, 22924, 8046, 11, 321], "temperature": 0.0, "avg_logprob": -0.16626194545200892, "compression_ratio": 1.5471698113207548, "no_speech_prob": 5.213209806242958e-05}, {"id": 202, "seek": 90700, "start": 924.08, "end": 928.2, "text": " have here the Yeager output.", "tokens": [362, 510, 264, 835, 3557, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16626194545200892, "compression_ratio": 1.5471698113207548, "no_speech_prob": 5.213209806242958e-05}, {"id": 203, "seek": 90700, "start": 928.2, "end": 935.34, "text": " So we can see the resource attributes and all the beautiful stuff that comes with it.", "tokens": [407, 321, 393, 536, 264, 7684, 17212, 293, 439, 264, 2238, 1507, 300, 1487, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.16626194545200892, "compression_ratio": 1.5471698113207548, "no_speech_prob": 5.213209806242958e-05}, {"id": 204, "seek": 93534, "start": 935.34, "end": 939.5600000000001, "text": " So next, we have, we can have a look on metrics.", "tokens": [407, 958, 11, 321, 362, 11, 321, 393, 362, 257, 574, 322, 16367, 13], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 205, "seek": 93534, "start": 939.5600000000001, "end": 943.36, "text": " So there is the open telemetry SDK.", "tokens": [407, 456, 307, 264, 1269, 4304, 5537, 627, 37135, 13], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 206, "seek": 93534, "start": 943.36, "end": 948.84, "text": " So if you want to go with open telemetry metrics, but I assume a lot of people have already", "tokens": [407, 498, 291, 528, 281, 352, 365, 1269, 4304, 5537, 627, 16367, 11, 457, 286, 6552, 257, 688, 295, 561, 362, 1217], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 207, "seek": 93534, "start": 948.84, "end": 951.36, "text": " some perimeter stuff in place.", "tokens": [512, 32404, 1507, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 208, "seek": 93534, "start": 951.36, "end": 956.6, "text": " And the open telemetry operator also helps us with this.", "tokens": [400, 264, 1269, 4304, 5537, 627, 12973, 611, 3665, 505, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 209, "seek": 93534, "start": 956.6, "end": 962.2800000000001, "text": " So we can, I might we look first on the receiver part on the bottom.", "tokens": [407, 321, 393, 11, 286, 1062, 321, 574, 700, 322, 264, 20086, 644, 322, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.15481109420458475, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.734618313610554e-05}, {"id": 210, "seek": 96228, "start": 962.28, "end": 966.92, "text": " We see there, we configure the perimeter's receiver, which has a scrape configuration,", "tokens": [492, 536, 456, 11, 321, 22162, 264, 32404, 311, 20086, 11, 597, 575, 257, 32827, 11694, 11], "temperature": 0.0, "avg_logprob": -0.1813370319122964, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.005723844282329e-05}, {"id": 211, "seek": 96228, "start": 966.92, "end": 970.24, "text": " and there we can, for example, add some static targets.", "tokens": [293, 456, 321, 393, 11, 337, 1365, 11, 909, 512, 13437, 12911, 13], "temperature": 0.0, "avg_logprob": -0.1813370319122964, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.005723844282329e-05}, {"id": 212, "seek": 96228, "start": 970.24, "end": 976.72, "text": " So we assume we add there three different scrape endpoints, then afterwards, if the target", "tokens": [407, 321, 6552, 321, 909, 456, 1045, 819, 32827, 917, 20552, 11, 550, 10543, 11, 498, 264, 3779], "temperature": 0.0, "avg_logprob": -0.1813370319122964, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.005723844282329e-05}, {"id": 213, "seek": 96228, "start": 976.72, "end": 983.48, "text": " allocator is enabled, this will then take these scrape targets and divide, well, spread", "tokens": [12660, 1639, 307, 15172, 11, 341, 486, 550, 747, 613, 32827, 12911, 293, 9845, 11, 731, 11, 3974], "temperature": 0.0, "avg_logprob": -0.1813370319122964, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.005723844282329e-05}, {"id": 214, "seek": 96228, "start": 983.48, "end": 989.4399999999999, "text": " these targets across our replicas, which are then responsible for getting the metrics.", "tokens": [613, 12911, 2108, 527, 3248, 9150, 11, 597, 366, 550, 6250, 337, 1242, 264, 16367, 13], "temperature": 0.0, "avg_logprob": -0.1813370319122964, "compression_ratio": 1.7586206896551724, "no_speech_prob": 6.005723844282329e-05}, {"id": 215, "seek": 98944, "start": 989.44, "end": 992.84, "text": " And yeah, that's basically how it works.", "tokens": [400, 1338, 11, 300, 311, 1936, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 216, "seek": 98944, "start": 992.84, "end": 998.8000000000001, "text": " There's also an option to enable perimeter CRs, so we can then forward to this one.", "tokens": [821, 311, 611, 364, 3614, 281, 9528, 32404, 14123, 82, 11, 370, 321, 393, 550, 2128, 281, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 217, "seek": 98944, "start": 998.8000000000001, "end": 1002.72, "text": " And the target allocator, which is an extra instance created by the Yeager, by the open", "tokens": [400, 264, 3779, 12660, 1639, 11, 597, 307, 364, 2857, 5197, 2942, 538, 264, 835, 3557, 11, 538, 264, 1269], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 218, "seek": 98944, "start": 1002.72, "end": 1009.1600000000001, "text": " telemetry operator, will then, yeah, get the targets from there.", "tokens": [4304, 5537, 627, 12973, 11, 486, 550, 11, 1338, 11, 483, 264, 12911, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 219, "seek": 98944, "start": 1009.1600000000001, "end": 1012.4000000000001, "text": " So we see this here in this graphic, quite good.", "tokens": [407, 321, 536, 341, 510, 294, 341, 14089, 11, 1596, 665, 13], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 220, "seek": 98944, "start": 1012.4000000000001, "end": 1017.4000000000001, "text": " On the left side, we see part one, which is using open telemetry, and it's pushing the", "tokens": [1282, 264, 1411, 1252, 11, 321, 536, 644, 472, 11, 597, 307, 1228, 1269, 4304, 5537, 627, 11, 293, 309, 311, 7380, 264], "temperature": 0.0, "avg_logprob": -0.16034424823263418, "compression_ratio": 1.6788617886178863, "no_speech_prob": 4.391571565065533e-05}, {"id": 221, "seek": 101740, "start": 1017.4, "end": 1022.0, "text": " information telemetry data it gets directly to a collector.", "tokens": [1589, 4304, 5537, 627, 1412, 309, 2170, 3838, 281, 257, 23960, 13], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 222, "seek": 101740, "start": 1022.0, "end": 1028.6399999999999, "text": " And in this gray box, we see there, we have two instances running Prometoys, running instruments", "tokens": [400, 294, 341, 10855, 2424, 11, 321, 536, 456, 11, 321, 362, 732, 14519, 2614, 2114, 649, 78, 749, 11, 2614, 12190], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 223, "seek": 101740, "start": 1028.6399999999999, "end": 1033.84, "text": " with Prometoys, and the collector one and collector two are pulling the information", "tokens": [365, 2114, 649, 78, 749, 11, 293, 264, 23960, 472, 293, 23960, 732, 366, 8407, 264, 1589], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 224, "seek": 101740, "start": 1033.84, "end": 1034.84, "text": " from there.", "tokens": [490, 456, 13], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 225, "seek": 101740, "start": 1034.84, "end": 1037.16, "text": " So this is all managed then by the operator.", "tokens": [407, 341, 307, 439, 6453, 550, 538, 264, 12973, 13], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 226, "seek": 101740, "start": 1037.16, "end": 1041.2, "text": " We have seen the replicas, this is basically collector one and collector two, and since", "tokens": [492, 362, 1612, 264, 3248, 9150, 11, 341, 307, 1936, 23960, 472, 293, 23960, 732, 11, 293, 1670], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 227, "seek": 101740, "start": 1041.2, "end": 1045.6399999999999, "text": " we enable the target allocator, we get the targets from there, so which is then coming", "tokens": [321, 9528, 264, 3779, 12660, 1639, 11, 321, 483, 264, 12911, 490, 456, 11, 370, 597, 307, 550, 1348], "temperature": 0.0, "avg_logprob": -0.16096284654405382, "compression_ratio": 1.926530612244898, "no_speech_prob": 9.006195614347234e-05}, {"id": 228, "seek": 104564, "start": 1045.64, "end": 1047.76, "text": " from the port monitor.", "tokens": [490, 264, 2436, 6002, 13], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 229, "seek": 104564, "start": 1047.76, "end": 1052.44, "text": " And finally, we send the information somewhere.", "tokens": [400, 2721, 11, 321, 2845, 264, 1589, 4079, 13], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 230, "seek": 104564, "start": 1052.44, "end": 1056.68, "text": " So the last thing here, the last signal are then locks.", "tokens": [407, 264, 1036, 551, 510, 11, 264, 1036, 6358, 366, 550, 20703, 13], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 231, "seek": 104564, "start": 1056.68, "end": 1058.68, "text": " So for locks, there are different options.", "tokens": [407, 337, 20703, 11, 456, 366, 819, 3956, 13], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 232, "seek": 104564, "start": 1058.68, "end": 1066.8000000000002, "text": " So the first one would be to use the open telemetry SDK, what we might don't want right", "tokens": [407, 264, 700, 472, 576, 312, 281, 764, 264, 1269, 4304, 5537, 627, 37135, 11, 437, 321, 1062, 500, 380, 528, 558], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 233, "seek": 104564, "start": 1066.8000000000002, "end": 1070.92, "text": " now because we need to do some work, but if we directly want to go ahead, there is the", "tokens": [586, 570, 321, 643, 281, 360, 512, 589, 11, 457, 498, 321, 3838, 528, 281, 352, 2286, 11, 456, 307, 264], "temperature": 0.0, "avg_logprob": -0.13906780365974672, "compression_ratio": 1.6303317535545023, "no_speech_prob": 4.1908486309694126e-05}, {"id": 234, "seek": 107092, "start": 1070.92, "end": 1077.4, "text": " philoc receiver, we can configure it to get the information from this, and yeah, it's", "tokens": [903, 388, 905, 20086, 11, 321, 393, 22162, 309, 281, 483, 264, 1589, 490, 341, 11, 293, 1338, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.24695006749963247, "compression_ratio": 1.5394736842105263, "no_speech_prob": 8.458175580017269e-05}, {"id": 235, "seek": 107092, "start": 1077.4, "end": 1082.04, "text": " available in the conflict repository, and we have different parsers there which help", "tokens": [2435, 294, 264, 6596, 25841, 11, 293, 321, 362, 819, 21156, 433, 456, 597, 854], "temperature": 0.0, "avg_logprob": -0.24695006749963247, "compression_ratio": 1.5394736842105263, "no_speech_prob": 8.458175580017269e-05}, {"id": 236, "seek": 107092, "start": 1082.04, "end": 1088.16, "text": " us to move the locks into the OTP format.", "tokens": [505, 281, 1286, 264, 20703, 666, 264, 422, 16804, 7877, 13], "temperature": 0.0, "avg_logprob": -0.24695006749963247, "compression_ratio": 1.5394736842105263, "no_speech_prob": 8.458175580017269e-05}, {"id": 237, "seek": 107092, "start": 1088.16, "end": 1091.28, "text": " We will see in a minute how this looks like.", "tokens": [492, 486, 536, 294, 257, 3456, 577, 341, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.24695006749963247, "compression_ratio": 1.5394736842105263, "no_speech_prob": 8.458175580017269e-05}, {"id": 238, "seek": 107092, "start": 1091.28, "end": 1097.92, "text": " And there are other options if you want to integrate with FluentBit, so there is a forwarder,", "tokens": [400, 456, 366, 661, 3956, 498, 291, 528, 281, 13365, 365, 33612, 317, 33, 270, 11, 370, 456, 307, 257, 2128, 260, 11], "temperature": 0.0, "avg_logprob": -0.24695006749963247, "compression_ratio": 1.5394736842105263, "no_speech_prob": 8.458175580017269e-05}, {"id": 239, "seek": 109792, "start": 1097.92, "end": 1102.24, "text": " so you can use it as a kind of a gateway then.", "tokens": [370, 291, 393, 764, 309, 382, 257, 733, 295, 257, 28532, 550, 13], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 240, "seek": 109792, "start": 1102.24, "end": 1109.1200000000001, "text": " And yeah, the only thing we need to do then is we can configure it as a demon set.", "tokens": [400, 1338, 11, 264, 787, 551, 321, 643, 281, 360, 550, 307, 321, 393, 22162, 309, 382, 257, 14283, 992, 13], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 241, "seek": 109792, "start": 1109.1200000000001, "end": 1114.8400000000001, "text": " We need to pass our information there, and the philoc receiver, for example, can then", "tokens": [492, 643, 281, 1320, 527, 1589, 456, 11, 293, 264, 903, 388, 905, 20086, 11, 337, 1365, 11, 393, 550], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 242, "seek": 109792, "start": 1114.8400000000001, "end": 1118.52, "text": " get all the locks.", "tokens": [483, 439, 264, 20703, 13], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 243, "seek": 109792, "start": 1118.52, "end": 1120.64, "text": " And how does this look like at the end?", "tokens": [400, 577, 775, 341, 574, 411, 412, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 244, "seek": 109792, "start": 1120.64, "end": 1127.64, "text": " So this is when we exported the locks to the logging output, so standard out.", "tokens": [407, 341, 307, 562, 321, 42055, 264, 20703, 281, 264, 27991, 5598, 11, 370, 3832, 484, 13], "temperature": 0.0, "avg_logprob": -0.1511363410949707, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.597815132001415e-05}, {"id": 245, "seek": 112764, "start": 1127.64, "end": 1133.5200000000002, "text": " We see that we have the resource attributes which are added automatically, and yeah, we", "tokens": [492, 536, 300, 321, 362, 264, 7684, 17212, 597, 366, 3869, 6772, 11, 293, 1338, 11, 321], "temperature": 0.0, "avg_logprob": -0.2530378886631557, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.463493966497481e-05}, {"id": 246, "seek": 112764, "start": 1133.5200000000002, "end": 1138.5600000000002, "text": " see then the lock information, and on the bottom the trace ID and span ID which are", "tokens": [536, 550, 264, 4017, 1589, 11, 293, 322, 264, 2767, 264, 13508, 7348, 293, 16174, 7348, 597, 366], "temperature": 0.0, "avg_logprob": -0.2530378886631557, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.463493966497481e-05}, {"id": 247, "seek": 112764, "start": 1138.5600000000002, "end": 1143.0800000000002, "text": " not given if we read it just from disk, but that's it.", "tokens": [406, 2212, 498, 321, 1401, 309, 445, 490, 12355, 11, 457, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.2530378886631557, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.463493966497481e-05}, {"id": 248, "seek": 112764, "start": 1143.0800000000002, "end": 1149.16, "text": " Yeah, then we are almost at the end.", "tokens": [865, 11, 550, 321, 366, 1920, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.2530378886631557, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.463493966497481e-05}, {"id": 249, "seek": 114916, "start": 1149.16, "end": 1159.3200000000002, "text": " Yeah, thanks a lot for the interesting talk.", "tokens": [865, 11, 3231, 257, 688, 337, 264, 1880, 751, 13], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 250, "seek": 114916, "start": 1159.3200000000002, "end": 1163.2, "text": " Does anyone have questions?", "tokens": [4402, 2878, 362, 1651, 30], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 251, "seek": 114916, "start": 1163.2, "end": 1164.2, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 252, "seek": 114916, "start": 1164.2, "end": 1166.2, "text": " Raise your hand.", "tokens": [30062, 428, 1011, 13], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 253, "seek": 114916, "start": 1166.2, "end": 1168.2, "text": " Question?", "tokens": [14464, 30], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 254, "seek": 114916, "start": 1168.2, "end": 1176.0800000000002, "text": " Yeah, there we go.", "tokens": [865, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.26017323407259857, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.00036533287493512034}, {"id": 255, "seek": 117608, "start": 1176.08, "end": 1182.32, "text": " For the logging part, would you suggest to replace any kind of cluster logging like with", "tokens": [1171, 264, 27991, 644, 11, 576, 291, 3402, 281, 7406, 604, 733, 295, 13630, 27991, 411, 365], "temperature": 0.0, "avg_logprob": -0.23195343306570343, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0005570174544118345}, {"id": 256, "seek": 117608, "start": 1182.32, "end": 1191.76, "text": " Fluent Bit, or that's like sending it off to Loki or something with an open telemetry", "tokens": [33612, 317, 9101, 11, 420, 300, 311, 411, 7750, 309, 766, 281, 37940, 420, 746, 365, 364, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.23195343306570343, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0005570174544118345}, {"id": 257, "seek": 117608, "start": 1191.76, "end": 1199.28, "text": " log scraping, or is that complementary?", "tokens": [3565, 43738, 11, 420, 307, 300, 40705, 30], "temperature": 0.0, "avg_logprob": -0.23195343306570343, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0005570174544118345}, {"id": 258, "seek": 117608, "start": 1199.28, "end": 1200.8799999999999, "text": " I'm not sure if I fully got it.", "tokens": [286, 478, 406, 988, 498, 286, 4498, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.23195343306570343, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0005570174544118345}, {"id": 259, "seek": 120088, "start": 1200.88, "end": 1216.48, "text": " So you want to, yeah, in this case it's just another way, but the useful thing is if you", "tokens": [407, 291, 528, 281, 11, 1338, 11, 294, 341, 1389, 309, 311, 445, 1071, 636, 11, 457, 264, 4420, 551, 307, 498, 291], "temperature": 0.0, "avg_logprob": -0.15512907738779105, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016028559184633195}, {"id": 260, "seek": 120088, "start": 1216.48, "end": 1224.92, "text": " have the open telemetry SDK, it will automatically add then the trace ID to it, and then you", "tokens": [362, 264, 1269, 4304, 5537, 627, 37135, 11, 309, 486, 6772, 909, 550, 264, 13508, 7348, 281, 309, 11, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.15512907738779105, "compression_ratio": 1.3507462686567164, "no_speech_prob": 0.00016028559184633195}, {"id": 261, "seek": 122492, "start": 1224.92, "end": 1231.24, "text": " can correlate your signals.", "tokens": [393, 48742, 428, 12354, 13], "temperature": 0.0, "avg_logprob": -0.2085083508100666, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0013089284766465425}, {"id": 262, "seek": 122492, "start": 1231.24, "end": 1235.24, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2085083508100666, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0013089284766465425}, {"id": 263, "seek": 122492, "start": 1235.24, "end": 1244.04, "text": " So I'm super newbie to this, so I failed to understand how the, if the open telemetry", "tokens": [407, 286, 478, 1687, 777, 7392, 281, 341, 11, 370, 286, 7612, 281, 1223, 577, 264, 11, 498, 264, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.2085083508100666, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0013089284766465425}, {"id": 264, "seek": 122492, "start": 1244.04, "end": 1250.96, "text": " is trying to replace, for example, the log parsers like the telegraph, for example, which", "tokens": [307, 1382, 281, 7406, 11, 337, 1365, 11, 264, 3565, 21156, 433, 411, 264, 4304, 34091, 11, 337, 1365, 11, 597], "temperature": 0.0, "avg_logprob": -0.2085083508100666, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0013089284766465425}, {"id": 265, "seek": 125096, "start": 1250.96, "end": 1258.88, "text": " is able to generate prometheus metrics by log scraping, or also how Zipkin, which is the", "tokens": [307, 1075, 281, 8460, 37786, 42209, 16367, 538, 3565, 43738, 11, 420, 611, 577, 1176, 647, 5843, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.15623675720601143, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0010528035927563906}, {"id": 266, "seek": 125096, "start": 1258.88, "end": 1263.4, "text": " tracing thing, fits in the metric collection of all this picture.", "tokens": [25262, 551, 11, 9001, 294, 264, 20678, 5765, 295, 439, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.15623675720601143, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0010528035927563906}, {"id": 267, "seek": 125096, "start": 1263.4, "end": 1270.92, "text": " So I'm not trying to understand how you cobble together all these sources and how open telemetry", "tokens": [407, 286, 478, 406, 1382, 281, 1223, 577, 291, 598, 10387, 1214, 439, 613, 7139, 293, 577, 1269, 4304, 5537, 627], "temperature": 0.0, "avg_logprob": -0.15623675720601143, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0010528035927563906}, {"id": 268, "seek": 125096, "start": 1270.92, "end": 1277.0, "text": " either replaces or either makes it easier to use all these technologies together.", "tokens": [2139, 46734, 420, 2139, 1669, 309, 3571, 281, 764, 439, 613, 7943, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15623675720601143, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0010528035927563906}, {"id": 269, "seek": 127700, "start": 1277.0, "end": 1281.84, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1648269800039438, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.001374645042233169}, {"id": 270, "seek": 127700, "start": 1281.84, "end": 1289.88, "text": " So maybe on this slide you see that the third port is using the Zipkin and prometheus, and", "tokens": [407, 1310, 322, 341, 4137, 291, 536, 300, 264, 2636, 2436, 307, 1228, 264, 1176, 647, 5843, 293, 37786, 42209, 11, 293], "temperature": 0.0, "avg_logprob": -0.1648269800039438, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.001374645042233169}, {"id": 271, "seek": 127700, "start": 1289.88, "end": 1297.32, "text": " the collector can receive data in Zipkin format, it can scrape prometheus metrics, then transform", "tokens": [264, 23960, 393, 4774, 1412, 294, 1176, 647, 5843, 7877, 11, 309, 393, 32827, 37786, 42209, 16367, 11, 550, 4088], "temperature": 0.0, "avg_logprob": -0.1648269800039438, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.001374645042233169}, {"id": 272, "seek": 127700, "start": 1297.32, "end": 1305.64, "text": " this data into OTLT or the Zipkin as well, and then send it to the other collector.", "tokens": [341, 1412, 666, 38617, 43, 51, 420, 264, 1176, 647, 5843, 382, 731, 11, 293, 550, 2845, 309, 281, 264, 661, 23960, 13], "temperature": 0.0, "avg_logprob": -0.1648269800039438, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.001374645042233169}, {"id": 273, "seek": 130564, "start": 1305.64, "end": 1310.2800000000002, "text": " So the collector essentially can receive data in multiple formats, transform them to the", "tokens": [407, 264, 23960, 4476, 393, 4774, 1412, 294, 3866, 25879, 11, 4088, 552, 281, 264], "temperature": 0.0, "avg_logprob": -0.23024309442398397, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002589358715340495}, {"id": 274, "seek": 130564, "start": 1310.2800000000002, "end": 1326.0800000000002, "text": " format of your choice, and then use that format to send it to other systems.", "tokens": [7877, 295, 428, 3922, 11, 293, 550, 764, 300, 7877, 281, 2845, 309, 281, 661, 3652, 13], "temperature": 0.0, "avg_logprob": -0.23024309442398397, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002589358715340495}, {"id": 275, "seek": 130564, "start": 1326.0800000000002, "end": 1328.5600000000002, "text": " Hello and thanks for the talk.", "tokens": [2425, 293, 3231, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.23024309442398397, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.002589358715340495}, {"id": 276, "seek": 132856, "start": 1328.56, "end": 1335.6799999999998, "text": " I'm just wondering, what's your strategy of filtering health check requests, for example,", "tokens": [286, 478, 445, 6359, 11, 437, 311, 428, 5206, 295, 30822, 1585, 1520, 12475, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 277, "seek": 132856, "start": 1335.6799999999998, "end": 1340.28, "text": " or the life probes request that you get in the pod?", "tokens": [420, 264, 993, 1239, 279, 5308, 300, 291, 483, 294, 264, 2497, 30], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 278, "seek": 132856, "start": 1340.28, "end": 1344.72, "text": " Health checks, like to avoid generating traces for health checks?", "tokens": [5912, 13834, 11, 411, 281, 5042, 17746, 26076, 337, 1585, 13834, 30], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 279, "seek": 132856, "start": 1344.72, "end": 1345.72, "text": " Sorry?", "tokens": [4919, 30], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 280, "seek": 132856, "start": 1345.72, "end": 1348.8799999999999, "text": " To avoid generating traces for health check endpoints?", "tokens": [1407, 5042, 17746, 26076, 337, 1585, 1520, 917, 20552, 30], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 281, "seek": 132856, "start": 1348.8799999999999, "end": 1349.8799999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 282, "seek": 132856, "start": 1349.8799999999999, "end": 1354.24, "text": " That's a very good question.", "tokens": [663, 311, 257, 588, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.28937973976135256, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0038909984286874533}, {"id": 283, "seek": 135424, "start": 1354.24, "end": 1362.72, "text": " So you could maybe configure the collector to drop the data, but I think the best way", "tokens": [407, 291, 727, 1310, 22162, 264, 23960, 281, 3270, 264, 1412, 11, 457, 286, 519, 264, 1151, 636], "temperature": 0.0, "avg_logprob": -0.09761825708242564, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0008865793352015316}, {"id": 284, "seek": 135424, "start": 1362.72, "end": 1369.08, "text": " would be to tell the instrumentation to skip instrumenting those endpoints.", "tokens": [576, 312, 281, 980, 264, 7198, 399, 281, 10023, 7198, 278, 729, 917, 20552, 13], "temperature": 0.0, "avg_logprob": -0.09761825708242564, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0008865793352015316}, {"id": 285, "seek": 135424, "start": 1369.08, "end": 1376.0, "text": " To be honest, I'm not sure if this is implemented in OTL agents, but I saw a lot of discussions", "tokens": [1407, 312, 3245, 11, 286, 478, 406, 988, 498, 341, 307, 12270, 294, 38617, 43, 12554, 11, 457, 286, 1866, 257, 688, 295, 11088], "temperature": 0.0, "avg_logprob": -0.09761825708242564, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0008865793352015316}, {"id": 286, "seek": 137600, "start": 1376.0, "end": 1385.32, "text": " around this problem, so probably there will be some solution.", "tokens": [926, 341, 1154, 11, 370, 1391, 456, 486, 312, 512, 3827, 13], "temperature": 0.0, "avg_logprob": -0.2803060681212182, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0003161658241879195}, {"id": 287, "seek": 137600, "start": 1385.32, "end": 1389.12, "text": " We have time for one last question, if there is any.", "tokens": [492, 362, 565, 337, 472, 1036, 1168, 11, 498, 456, 307, 604, 13], "temperature": 0.0, "avg_logprob": -0.2803060681212182, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0003161658241879195}, {"id": 288, "seek": 137600, "start": 1389.12, "end": 1390.12, "text": " No?", "tokens": [883, 30], "temperature": 0.0, "avg_logprob": -0.2803060681212182, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0003161658241879195}, {"id": 289, "seek": 137600, "start": 1390.12, "end": 1391.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2803060681212182, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0003161658241879195}, {"id": 290, "seek": 137600, "start": 1391.12, "end": 1392.32, "text": " Oh, no.", "tokens": [876, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.2803060681212182, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0003161658241879195}, {"id": 291, "seek": 139232, "start": 1392.32, "end": 1408.36, "text": " And thanks a lot.", "tokens": [50364, 400, 3231, 257, 688, 13, 51166], "temperature": 0.0, "avg_logprob": -0.6442431211471558, "compression_ratio": 0.68, "no_speech_prob": 0.0005273406277410686}], "language": "en"}