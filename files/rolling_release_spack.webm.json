{"text": " All right, so I'm Todd Gamble, and I'm from Lawrence Livermore National Laboratory. Normally I would give an intro of what Livermore is, but who's been hearing about Livermore in the news lately? The people heard about the fusion ignition over in the US, that's our lab. So I'm from there. I work in the HPC area at Livermore, and so we have a big supercomputing center. And the HPC ecosystem is a pretty complex place. People distribute software, mostly as source. You build lots of different variants of the package. Users typically don't have root on the machine when they install software, and so they're building from source in their home directory or installing something in their home directory. And you want the code to be optimized for fancy machines like these ones over here. So you're trying to build software that supports a really broad set of environments, including like Power, ARM, AMD, Intel, and then also GPU architectures. So things like NVIDIA and now AMD GPUs are showing up, and we've even got a machine coming all out at Argonne. This is near Chicago with Intel Panaveco GPUs. On top of all that, the ecosystem has C, C++, Fortran, Python, other languages, Lua, all linked together in the same app. And so we want a distribution that can support this type of environment. And so SPAC is a package manager that enables software distribution for HPC, given that set of constraints. Packages are not quite like the build specs that you would see in your standard RPM or Deb-based distribution. They're really parameterized Python recipes for how to build that package on lots of different architectures. And it has a DSL for doing that. I'm not going to get into that today. But the end user can essentially take one package and install it lots of different ways. So you could say, I want to install HDF5 at a particular version. I want to install it with Clang, not GCC. I want to have the thread safe option on, or I want to inject some flags in the build and have an entirely different version of it that's built with a different set of flags, or that's targeted at a particular micro-architecture, or that maybe uses a particular dependency. So you can build the same package with two versions of MPI. So we're trying to provide the ease of use of mainstream tools with the flexibility needed for HPC so that we can get the performance everyone. And it builds from source, but you can also install relocatable build caches in SPAC, much like you would with, say, Nix or Geeks. They're not relocatable because they're not really targeting the sort of home directory use case, but it's the same sort of build cache model. It's not a typical binary distribution. The whole project has a fairly large community of contributors, or at least maybe not large by some of the other distribution standards, but we have 1100-plus contributors. We maintain the core tool, and then there's a whole bunch of people who work on package recipes. So in some ways, it looks a lot like Homebrew or a project like that. And then there's a whole bunch of infrastructure behind the scenes to keep all this working, and all these things together enable people to build lots of different software stacks. And so there's like an extreme-scale software stack that's maintained by the US Exascale Project. AWS has a stack that they use on their parallel cluster product internally, and also for users. Livermore has its internal software deployment. There are some math library stacks, VizTools, things like that. And every application, really, in HPC is kind of its own software stack. So you heard about flat packs and snaps in the last session, well, really, making apps more mindful of how their software is actually a distribution is something that we've been pushing for a long time within HPC. The GitHub is a pretty busy place. We merge 300 to 500 PRs per month, and it's like something like 411 commits or more. And so managing that is kind of painful. And we're trying very hard to reduce downstream work, which is actually difficult for a source-based distribution. If you think about how SPAC is structured, there's this mainline develop branch that actually most people use. They'll just clone it straight from the repo, build from that, kind of like you do with mixed packages or something. External contributors contribute there. And we cut a release every once in a while where we stabilize the packages and keep them sort of fixed so that you don't have a lot of version churn in the repo. And then to actually integrate with the HPC facilities, all the places that are deploying supercomputers, we have this E4S software distribution where they end up doing a whole bunch of downstream integration at the site, where they're basically building the whole thing from source, essentially in a new environment. And there's a whole lot of debugging that takes place there that we would really like to be able to move upstream. The applications, likewise, they are not necessarily using what the facility deploys. Some of them do. Some of them don't. They pull from basically all of these places. They might get a math solver library from the facility. They might get something else installed from SPAC mainline built the way that they want. And they may pull stuff off of release branches too, all to assemble an application and have it built. And so this is a lot of porting at the lowest end, and what we'd really like to do is take that software integration and move it upstream and get to a point where we can have these types of environments building NCI all the time in sort of a rolling release and do binary deploys on the supercomputers with actual optimized binaries. So that's what we're trying to get to. So we set out to make a binary distribution with a bunch of different goals. The main one, and the one that's pretty key to our whole ecosystem, is it has to be sustainable. We don't have that many maintainers. And they currently, their workflow is basically to work with people who are making contributions, on pull requests, help them get them merged, and then move on to the next one. And we don't want them to have to sit around and babysit builds on, say, a release integration branch all the time. We want a rolling release because people do tend to use the develop branch. And so we want that to be up to date with pretty current binaries all the time. But some people do fix themselves to releases, and so we want sort of snapshots for those releases as well. We need to be able to support, at least eventually, all the packages that are in SPAC. And it still has to be source-buildable around those binaries. So if you want to build a component and rely on binaries for some other component, we want to support that. And then finally, people trust sources. They can check some of them. You can download the tarball. You can usually check some of them, except for when GitHub changes the hashes. But we want to ensure that the binaries that we're generating are just as trustworthy as the sources. So we've taken some steps to ensure that. So SPAC is a little different from your standard distro if you haven't gathered already. If you think about a traditional package manager, you have a sort of a recipe per configuration. And so that's like your RPM build spec or dev spec or whatever. It goes into a build farm, and you produce packages, at least for one platform, in sort of a one-to-one relationship with those specs, actually. There's templating and things that goes on to reduce that. But you're typically maintaining one software stack that gets updated over time. In SPAC, what we're trying to do is we have these parameterized package recipes that go into build farm, but it's really the same recipe that's being used across different architectures. We force the contributors to work on the same package so that essentially you're modeling all the different ways the software can be used, and we try to get a lot of reuse out of the recipes across platforms. Those go into the build farm, and you can use the same recipes to produce optimized binaries for lots of different platforms. So you could get a graviton, arm build, you could get a Skylake binary, you could get a GPU build, and so on. And then you could do that for many different software stacks for different use cases. And then we want you to be able to build from source on top of that. So that's what we're trying to do. We put a CI architecture together that is sort of based around this. Like I said, we want to be sustainable, we want to maintain the workflow that we already have on the project, and so we want people, we want basically GitHub to be the center of the distribution. What goes into develop is really maintaining the distribution as well as contributing to the project. And so we have a bunch of infrastructure currently stood up in AWS to support this. So the binaries themselves and the sources are all distributed through S3 and CloudFront. We set up a big Kubernetes cluster to support autoscaling runners, and we're using high availability GitLab in there to drive the CI. GitLab may seem like a strange choice for maintaining a distribution, but the motivation behind that is really that all of the HPC centers also have internal GitLabs, and so do a lot of universities and other sites. And so the goal is really for all of this automation and tooling to be usable not just in the cloud for the large distribution of SPAC, but also for people's personal software stacks locally. And so the idea is that we're generating GitLab CI configuration, and you can use that either for this or internally or in an air gap network somewhere. So we're leveraging Carpenter on the backend for just-in-time instances for runner pools. That's a tool for AWS, it's open source, you can find it on GitHub. It essentially lets you make requests for nodes with certain amounts of memory, certain target architectures, and so on, and it manages containers on the instances for you on the backend and sort of moves work around so that you can have an efficient build pool in Kubernetes. We also have some bare-metal runners at the University of Oregon with more exotic architectures than you can maybe find in the cloud. So like there's an AMD MI 200 GPU builder in there, there's A64FX, which is what runs on Sugaku, it's the ARM architecture with vector instructions, Power9, and so on. And so we are able to do runs there for architectures that aren't supported in the cloud. There's some monitoring thrown in. We haven't really leveraged it in a smart way yet, but we are collecting a lot of data about our builds. And then there's a bot that helps sort of coordinate between GitHub and GitLab. And so we have sort of a sync script that allows us to build off of forks and things like that in GitLab over this whole setup. So it's fairly custom, but at least the GitLab component is recyclable internally. And we would like to be able to support more runners in the future, like if maybe we want to work with Azure on their HPC setup and they want to provide runners for the project or if other universities and places want to provide runners, we want to leave that open. For maintaining the stacks themselves, we made it possible to sort of instantiate a new stack in a pull request. And so we have this directory full of the sort of 16 stacks that we currently build in CI. You can see them there. Each one of those is some targeted software stack for some type of machine or some group. Each of those contains sort of a YAML file with configuration for the stack in it. And so the YAML file itself is fairly simple. It has a list of packages that you want to build, and so this is the machine learning one for CUDA. Those are all the names of the stack recipes that you're building here. And then some configuration up here. And so for this particular stack, you're saying, I want to build for x8664v3, which is AVX2. And I want to disable Rockum and enable CUDA, except on LLVM because there's some weird bug with the CUDA support there, at least in our stack. And so you can see it's fairly concise. You make a list of packages. You say, here's the configuration I want, and you can go and take this thing and build a bunch of packages. We make it easy to change sort of low-level stack-wide parameters. So the parameterized packages in stack, you can tell it to build with a different compiler. And so we had essentially this large E4S stack with maybe 600 packages working in standard environments. We wanted to support the one API compilers from Intel. And so that's Intel's new optimizing compilers. It is unlikely that anyone has ever run this much open source through a proprietary vendor compiler like that, but it is client-based. And so we were able to throw one API into the config by just saying, here's where one API lives, and make all packages require one API. And so the build system swaps in the one API compiler through some wrappers that are at the lower level. And we were able to get that stack working in a week or two, despite the fact that we've never built a lot of these packages with one API before. So I think that's actually pretty cool. In a lot of cases, it's not worth it to use a vendor compiler because there's so many bugs and issues with software that's never been built. But here, we're just really throwing sort of a bunch of open source packages through, and it helped us communicate with Intel. We were able to say, hey, here are bugs that we're seeing with your compiler. We can link you directly to the build log for the build that failed. And that helps them patch up the compiler, and it continues to help them ensure that it can build everything it needs to. In SPAC, you don't. So like I said, the recipes are these parameterized things, and so there's actually a solving step to these stacks. You saw sort of the requirements in the YAML file that said what I want to build. We run that through our packet solver to get sort of a fully resolved graph of all the things that need to be built in a stack. And then that is used to generate a GitLab CI YAML. And then for one of the problems that we have to solve there is mapping builds to runners. So once the whole thing is concrete, and we've said here's all the dependencies, these are all the exact build configurations we want to make, we have to say how that should be mapped to particular runners. And so we don't currently support things like cross builds. So if you want to build for AVX 512 or the more fancy vector instructions on newer Intel CPUs, you need to make sure that you get one of those CPUs in the build environment. And so we say, if you match AVX 512, give me an AVX 512 runner. If you match one of these somewhat atrocious, hard to build packages up here like LLVM and PyTorch, give me a gigantic runner with lots of memory, things like that. And essentially what this is doing is it's just saying, here's the package properties up at the top, here are the tags that should be on the runner, make sure that I get a runner with those capabilities. And we haven't got a schema for all the tags yet, but I think we could standardize this and make it easy for someone to plug in runners at their own site for this sort of thing. All right. So one of the things that we did here to ensure trust is we have essentially a build environment going on in pull requests. If you trust back, you're basically trusting the maintainers. We want to ensure that the binaries are things that are approved by the maintainers. And so we can't just distribute binaries that got built in pull requests. So when contributors submit package changes, we go and we have private buckets for every PR that we're supporting where we're doing the builds. The maintainers come along and say, oh, it worked. They review the code. And then they say, okay, we can merge that and rebuild everything on develop and sign. So essentially everything in the main release is getting built from only approved recipes. It's not using any binaries that were built in the PR. All right. The pull request integration, yeah, definitely makes things easy for contributors. And we were able to take the system and announce our public binary cache last June with something like 4600 builds in CI. And so it's mostly easy for contributors. They get a status update on their pull request. And mostly easy for users. They can just say, hey, use the binary mirror. So there are some problems. One issue is that build caches are a lot different from RPMs and devs. In most distributions, you would have sort of a stable ABI for your build cache. Your rebuild package, you can throw it in the mix with the others. Here if you modify one package, you really do have to rebuild all the dependents. And so if you modify XZ here, then you have to build everything that depends on it again in the build cache. And so what that can mean is if you have a gigantic software stack like this one and you modify, say, package conf at the bottom of it, it can trigger a massive rebuild of everything in the stack. And so that's one of the scalability problems that I think we're going to have to deal with in the long term is that you can get these really long-running pipelines. Caches like Visit and PyTorch and so on will build forever, and it frustrates contributors. The other sort of thing that happens is if you think about how the release works on develop, you're picking a commit every once in a while and building it. And if you have a PR that is sort of based behind the last develop build, that's OK. Although GitHub typically wants to merge that with head, which means that you'll build a lot of redundant things in your build environment. We can be picky and merge it with the last develop build to ensure that we get a lot of cache reuse in the build environment. But what that means is if we get a PR that's out ahead of the last develop build and say D up there is in progress, if you merge that second PR with D, you're basically going to be doing the same builds that D is doing but in a PR environment. And so if you have a bunch of those, we've brought GitLab down before by accidentally building all of those PRs that are not caught up with the latest or for which develop has not caught up with them. And so we have to be picky and hold back these guys until there's a build ahead of them so that we get enough reuse out of the cache to support this. So the other problem with long pipelines is that they, depending on how reliable your infrastructure is, the more things that you build in a pipeline, the more likely you already get a build failure somewhere. And so because we're building this cone of destruction in our pipelines, we are sort of subject to system failures happening in the pipeline somewhere. And so users have to kind of babysit and restart builds that have nothing to do with what they're contributing. So we're looking for ways that we could make that better. One issue that we have is consistency. So when you test on PRs, it's not always sufficient to ensure that your develop branch is working. So you may have this initial package state, a PR gets submitted, you test with new B. Another PR gets submitted, you test with new package C. If you take those and you don't require your PRs to be up to date with develop, when they both get merged, the state that's in develop is something that you've never tested because you have basically new versions of those two packages together now. And so there are ways to get around this. One of them is merge queues. So we're looking at merge queues as a way to scale this pipeline out. They essentially allow you to have pull requests with a small amount of testing where you then enqueue them in your sort of merge queue up there, that's the gray stuff. And they are sort of serialized for commit to develop. If they succeed, then they're merged directly in a fast forward fashion. And then basically the full testing is only done on the merge queue. And you always are assured that the thing that you tested is the thing that gets merged into develop. So we're looking very much forward to GitHub making merge queue available in the next couple of weeks. The other thing we think that could do is allow us to sort of stage the work on PRs. So we're looking at ways we could scale this out. Right now, for a relatively small number of packages, 4,600, we're able to build this, these massive rebuilds on PRs. But we need the stage to see how to scale it out further, so that's what we're looking at now. We might build only the package or only the package and direct dependence on PRs and maybe phase how much work we do on the develop builds as well. But we do need to do a full build every once in a while so that there's a consistent state in the build cache. So that's where we're at. Thanks. Thank you very much for the presentation. You mentioned quite a bit of other technologies, like Nix, Gwix, Dab, RPM. You could have mentioned Ombru as well, or maybe you did. And Docker. And it feels like all these tools could help you. Yeah. And it feels like you are building everything on your own. So is there a reason not to leverage any of these technologies? Which technologies do you mean? Yeah. So we are leveraging a lot of technologies, right? I guess which ones do you think we should? Nix, for example. So we don't. So Nix has essentially one version of everything in the mainline, right? And in the HPC environment, what we want you to be able to do is not build that one thing that's in the mainline, but to be able to build a one-off very easily. So the whole point of SPAC is think of it as Nix with a solver, right? It's Nix where you can say, actually, no, build this version of this thing with this build option for that GPU, and it will take the recipe and reuse it for that purpose. Whereas in Nix, it's much harder to have package variants like that. So that's really the power of SPAC. And so we're combinatorial Nix. You can think of it that way. Well, wouldn't you be able to leverage Nix and describe all these differences instead of redoing it? No. The Nix packages don't do that.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.44, "text": " All right, so I'm Todd Gamble, and I'm from Lawrence Livermore National Laboratory.", "tokens": [1057, 558, 11, 370, 286, 478, 21488, 24723, 638, 11, 293, 286, 478, 490, 22787, 28010, 3138, 4862, 40824, 13], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 1, "seek": 0, "start": 8.44, "end": 13.16, "text": " Normally I would give an intro of what Livermore is, but who's been hearing about Livermore", "tokens": [17424, 286, 576, 976, 364, 12897, 295, 437, 28010, 3138, 307, 11, 457, 567, 311, 668, 4763, 466, 28010, 3138], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 2, "seek": 0, "start": 13.16, "end": 14.88, "text": " in the news lately?", "tokens": [294, 264, 2583, 12881, 30], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 3, "seek": 0, "start": 14.88, "end": 18.72, "text": " The people heard about the fusion ignition over in the US, that's our lab.", "tokens": [440, 561, 2198, 466, 264, 23100, 37031, 670, 294, 264, 2546, 11, 300, 311, 527, 2715, 13], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 4, "seek": 0, "start": 18.72, "end": 21.080000000000002, "text": " So I'm from there.", "tokens": [407, 286, 478, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 5, "seek": 0, "start": 21.080000000000002, "end": 26.36, "text": " I work in the HPC area at Livermore, and so we have a big supercomputing center.", "tokens": [286, 589, 294, 264, 12557, 34, 1859, 412, 28010, 3138, 11, 293, 370, 321, 362, 257, 955, 27839, 2582, 278, 3056, 13], "temperature": 0.0, "avg_logprob": -0.19146358049832857, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.05408809706568718}, {"id": 6, "seek": 2636, "start": 26.36, "end": 30.5, "text": " And the HPC ecosystem is a pretty complex place.", "tokens": [400, 264, 12557, 34, 11311, 307, 257, 1238, 3997, 1081, 13], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 7, "seek": 2636, "start": 30.5, "end": 33.2, "text": " People distribute software, mostly as source.", "tokens": [3432, 20594, 4722, 11, 5240, 382, 4009, 13], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 8, "seek": 2636, "start": 33.2, "end": 36.480000000000004, "text": " You build lots of different variants of the package.", "tokens": [509, 1322, 3195, 295, 819, 21669, 295, 264, 7372, 13], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 9, "seek": 2636, "start": 36.480000000000004, "end": 39.68, "text": " Users typically don't have root on the machine when they install software, and so they're", "tokens": [47092, 5850, 500, 380, 362, 5593, 322, 264, 3479, 562, 436, 3625, 4722, 11, 293, 370, 436, 434], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 10, "seek": 2636, "start": 39.68, "end": 43.72, "text": " building from source in their home directory or installing something in their home directory.", "tokens": [2390, 490, 4009, 294, 641, 1280, 21120, 420, 20762, 746, 294, 641, 1280, 21120, 13], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 11, "seek": 2636, "start": 43.72, "end": 47.760000000000005, "text": " And you want the code to be optimized for fancy machines like these ones over here.", "tokens": [400, 291, 528, 264, 3089, 281, 312, 26941, 337, 10247, 8379, 411, 613, 2306, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 12, "seek": 2636, "start": 47.760000000000005, "end": 53.0, "text": " So you're trying to build software that supports a really broad set of environments, including", "tokens": [407, 291, 434, 1382, 281, 1322, 4722, 300, 9346, 257, 534, 4152, 992, 295, 12388, 11, 3009], "temperature": 0.0, "avg_logprob": -0.11841238396508354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.769257768406533e-06}, {"id": 13, "seek": 5300, "start": 53.0, "end": 58.56, "text": " like Power, ARM, AMD, Intel, and then also GPU architectures.", "tokens": [411, 7086, 11, 45209, 11, 34808, 11, 19762, 11, 293, 550, 611, 18407, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 14, "seek": 5300, "start": 58.56, "end": 63.519999999999996, "text": " So things like NVIDIA and now AMD GPUs are showing up, and we've even got a machine coming", "tokens": [407, 721, 411, 426, 3958, 6914, 293, 586, 34808, 18407, 82, 366, 4099, 493, 11, 293, 321, 600, 754, 658, 257, 3479, 1348], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 15, "seek": 5300, "start": 63.519999999999996, "end": 65.52, "text": " all out at Argonne.", "tokens": [439, 484, 412, 1587, 10660, 716, 13], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 16, "seek": 5300, "start": 65.52, "end": 69.84, "text": " This is near Chicago with Intel Panaveco GPUs.", "tokens": [639, 307, 2651, 9525, 365, 19762, 7557, 946, 1291, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 17, "seek": 5300, "start": 69.84, "end": 75.68, "text": " On top of all that, the ecosystem has C, C++, Fortran, Python, other languages, Lua, all", "tokens": [1282, 1192, 295, 439, 300, 11, 264, 11311, 575, 383, 11, 383, 25472, 11, 11002, 4257, 11, 15329, 11, 661, 8650, 11, 441, 4398, 11, 439], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 18, "seek": 5300, "start": 75.68, "end": 77.6, "text": " linked together in the same app.", "tokens": [9408, 1214, 294, 264, 912, 724, 13], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 19, "seek": 5300, "start": 77.6, "end": 81.62, "text": " And so we want a distribution that can support this type of environment.", "tokens": [400, 370, 321, 528, 257, 7316, 300, 393, 1406, 341, 2010, 295, 2823, 13], "temperature": 0.0, "avg_logprob": -0.17694574151157347, "compression_ratio": 1.4945848375451263, "no_speech_prob": 2.506154851289466e-05}, {"id": 20, "seek": 8162, "start": 81.62, "end": 85.80000000000001, "text": " And so SPAC is a package manager that enables software distribution for HPC, given that", "tokens": [400, 370, 8420, 4378, 307, 257, 7372, 6598, 300, 17077, 4722, 7316, 337, 12557, 34, 11, 2212, 300], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 21, "seek": 8162, "start": 85.80000000000001, "end": 88.60000000000001, "text": " set of constraints.", "tokens": [992, 295, 18491, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 22, "seek": 8162, "start": 88.60000000000001, "end": 92.64, "text": " Packages are not quite like the build specs that you would see in your standard RPM or", "tokens": [18466, 1660, 366, 406, 1596, 411, 264, 1322, 27911, 300, 291, 576, 536, 294, 428, 3832, 37389, 420], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 23, "seek": 8162, "start": 92.64, "end": 93.92, "text": " Deb-based distribution.", "tokens": [27347, 12, 6032, 7316, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 24, "seek": 8162, "start": 93.92, "end": 98.76, "text": " They're really parameterized Python recipes for how to build that package on lots of different", "tokens": [814, 434, 534, 13075, 1602, 15329, 13035, 337, 577, 281, 1322, 300, 7372, 322, 3195, 295, 819], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 25, "seek": 8162, "start": 98.76, "end": 99.76, "text": " architectures.", "tokens": [6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 26, "seek": 8162, "start": 99.76, "end": 101.08000000000001, "text": " And it has a DSL for doing that.", "tokens": [400, 309, 575, 257, 15816, 43, 337, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 27, "seek": 8162, "start": 101.08000000000001, "end": 103.12, "text": " I'm not going to get into that today.", "tokens": [286, 478, 406, 516, 281, 483, 666, 300, 965, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 28, "seek": 8162, "start": 103.12, "end": 107.80000000000001, "text": " But the end user can essentially take one package and install it lots of different ways.", "tokens": [583, 264, 917, 4195, 393, 4476, 747, 472, 7372, 293, 3625, 309, 3195, 295, 819, 2098, 13], "temperature": 0.0, "avg_logprob": -0.15119749600770044, "compression_ratio": 1.6375838926174497, "no_speech_prob": 4.565352810459444e-06}, {"id": 29, "seek": 10780, "start": 107.8, "end": 111.32, "text": " So you could say, I want to install HDF5 at a particular version.", "tokens": [407, 291, 727, 584, 11, 286, 528, 281, 3625, 12149, 37, 20, 412, 257, 1729, 3037, 13], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 30, "seek": 10780, "start": 111.32, "end": 114.32, "text": " I want to install it with Clang, not GCC.", "tokens": [286, 528, 281, 3625, 309, 365, 2033, 656, 11, 406, 460, 11717, 13], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 31, "seek": 10780, "start": 114.32, "end": 119.08, "text": " I want to have the thread safe option on, or I want to inject some flags in the build", "tokens": [286, 528, 281, 362, 264, 7207, 3273, 3614, 322, 11, 420, 286, 528, 281, 10711, 512, 23265, 294, 264, 1322], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 32, "seek": 10780, "start": 119.08, "end": 122.92, "text": " and have an entirely different version of it that's built with a different set of flags,", "tokens": [293, 362, 364, 7696, 819, 3037, 295, 309, 300, 311, 3094, 365, 257, 819, 992, 295, 23265, 11], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 33, "seek": 10780, "start": 122.92, "end": 127.64, "text": " or that's targeted at a particular micro-architecture, or that maybe uses a particular dependency.", "tokens": [420, 300, 311, 15045, 412, 257, 1729, 4532, 12, 1178, 5739, 540, 11, 420, 300, 1310, 4960, 257, 1729, 33621, 13], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 34, "seek": 10780, "start": 127.64, "end": 132.2, "text": " So you can build the same package with two versions of MPI.", "tokens": [407, 291, 393, 1322, 264, 912, 7372, 365, 732, 9606, 295, 14146, 40, 13], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 35, "seek": 10780, "start": 132.2, "end": 136.72, "text": " So we're trying to provide the ease of use of mainstream tools with the flexibility needed", "tokens": [407, 321, 434, 1382, 281, 2893, 264, 12708, 295, 764, 295, 15960, 3873, 365, 264, 12635, 2978], "temperature": 0.0, "avg_logprob": -0.12170753759496353, "compression_ratio": 1.8156996587030716, "no_speech_prob": 3.7847148632863536e-06}, {"id": 36, "seek": 13672, "start": 136.72, "end": 139.68, "text": " for HPC so that we can get the performance everyone.", "tokens": [337, 12557, 34, 370, 300, 321, 393, 483, 264, 3389, 1518, 13], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 37, "seek": 13672, "start": 139.68, "end": 145.64, "text": " And it builds from source, but you can also install relocatable build caches in SPAC, much", "tokens": [400, 309, 15182, 490, 4009, 11, 457, 291, 393, 611, 3625, 26981, 31415, 1322, 269, 13272, 294, 8420, 4378, 11, 709], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 38, "seek": 13672, "start": 145.64, "end": 149.36, "text": " like you would with, say, Nix or Geeks.", "tokens": [411, 291, 576, 365, 11, 584, 11, 426, 970, 420, 2876, 24785, 13], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 39, "seek": 13672, "start": 149.36, "end": 151.92, "text": " They're not relocatable because they're not really targeting the sort of home directory", "tokens": [814, 434, 406, 26981, 31415, 570, 436, 434, 406, 534, 17918, 264, 1333, 295, 1280, 21120], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 40, "seek": 13672, "start": 151.92, "end": 155.0, "text": " use case, but it's the same sort of build cache model.", "tokens": [764, 1389, 11, 457, 309, 311, 264, 912, 1333, 295, 1322, 19459, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 41, "seek": 13672, "start": 155.0, "end": 157.8, "text": " It's not a typical binary distribution.", "tokens": [467, 311, 406, 257, 7476, 17434, 7316, 13], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 42, "seek": 13672, "start": 157.8, "end": 161.52, "text": " The whole project has a fairly large community of contributors, or at least maybe not large", "tokens": [440, 1379, 1716, 575, 257, 6457, 2416, 1768, 295, 45627, 11, 420, 412, 1935, 1310, 406, 2416], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 43, "seek": 13672, "start": 161.52, "end": 166.26, "text": " by some of the other distribution standards, but we have 1100-plus contributors.", "tokens": [538, 512, 295, 264, 661, 7316, 7787, 11, 457, 321, 362, 2975, 628, 12, 18954, 45627, 13], "temperature": 0.0, "avg_logprob": -0.14997400956995346, "compression_ratio": 1.7003154574132493, "no_speech_prob": 1.2409786904754583e-05}, {"id": 44, "seek": 16626, "start": 166.26, "end": 169.56, "text": " We maintain the core tool, and then there's a whole bunch of people who work on package", "tokens": [492, 6909, 264, 4965, 2290, 11, 293, 550, 456, 311, 257, 1379, 3840, 295, 561, 567, 589, 322, 7372], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 45, "seek": 16626, "start": 169.56, "end": 170.56, "text": " recipes.", "tokens": [13035, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 46, "seek": 16626, "start": 170.56, "end": 173.64, "text": " So in some ways, it looks a lot like Homebrew or a project like that.", "tokens": [407, 294, 512, 2098, 11, 309, 1542, 257, 688, 411, 8719, 65, 2236, 420, 257, 1716, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 47, "seek": 16626, "start": 173.64, "end": 177.35999999999999, "text": " And then there's a whole bunch of infrastructure behind the scenes to keep all this working,", "tokens": [400, 550, 456, 311, 257, 1379, 3840, 295, 6896, 2261, 264, 8026, 281, 1066, 439, 341, 1364, 11], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 48, "seek": 16626, "start": 177.35999999999999, "end": 180.76, "text": " and all these things together enable people to build lots of different software stacks.", "tokens": [293, 439, 613, 721, 1214, 9528, 561, 281, 1322, 3195, 295, 819, 4722, 30792, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 49, "seek": 16626, "start": 180.76, "end": 185.84, "text": " And so there's like an extreme-scale software stack that's maintained by the US Exascale", "tokens": [400, 370, 456, 311, 411, 364, 8084, 12, 20033, 4722, 8630, 300, 311, 17578, 538, 264, 2546, 2111, 4806, 1220], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 50, "seek": 16626, "start": 185.84, "end": 186.84, "text": " Project.", "tokens": [9849, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 51, "seek": 16626, "start": 186.84, "end": 192.44, "text": " AWS has a stack that they use on their parallel cluster product internally, and also for users.", "tokens": [17650, 575, 257, 8630, 300, 436, 764, 322, 641, 8952, 13630, 1674, 19501, 11, 293, 611, 337, 5022, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 52, "seek": 16626, "start": 192.44, "end": 194.28, "text": " Livermore has its internal software deployment.", "tokens": [28010, 3138, 575, 1080, 6920, 4722, 19317, 13], "temperature": 0.0, "avg_logprob": -0.13856850207691462, "compression_ratio": 1.8123076923076924, "no_speech_prob": 1.0449404726387002e-05}, {"id": 53, "seek": 19428, "start": 194.28, "end": 198.48, "text": " There are some math library stacks, VizTools, things like that.", "tokens": [821, 366, 512, 5221, 6405, 30792, 11, 691, 590, 51, 29298, 11, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 54, "seek": 19428, "start": 198.48, "end": 201.6, "text": " And every application, really, in HPC is kind of its own software stack.", "tokens": [400, 633, 3861, 11, 534, 11, 294, 12557, 34, 307, 733, 295, 1080, 1065, 4722, 8630, 13], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 55, "seek": 19428, "start": 201.6, "end": 209.56, "text": " So you heard about flat packs and snaps in the last session, well, really, making apps", "tokens": [407, 291, 2198, 466, 4962, 19403, 293, 19206, 294, 264, 1036, 5481, 11, 731, 11, 534, 11, 1455, 7733], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 56, "seek": 19428, "start": 209.56, "end": 214.64, "text": " more mindful of how their software is actually a distribution is something that we've been", "tokens": [544, 14618, 295, 577, 641, 4722, 307, 767, 257, 7316, 307, 746, 300, 321, 600, 668], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 57, "seek": 19428, "start": 214.64, "end": 218.96, "text": " pushing for a long time within HPC.", "tokens": [7380, 337, 257, 938, 565, 1951, 12557, 34, 13], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 58, "seek": 19428, "start": 218.96, "end": 220.92000000000002, "text": " The GitHub is a pretty busy place.", "tokens": [440, 23331, 307, 257, 1238, 5856, 1081, 13], "temperature": 0.0, "avg_logprob": -0.17628236250443893, "compression_ratio": 1.5587044534412955, "no_speech_prob": 8.663803782837931e-06}, {"id": 59, "seek": 22092, "start": 220.92, "end": 226.55999999999997, "text": " We merge 300 to 500 PRs per month, and it's like something like 411 commits or more.", "tokens": [492, 22183, 6641, 281, 5923, 11568, 82, 680, 1618, 11, 293, 309, 311, 411, 746, 411, 1017, 5348, 48311, 420, 544, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 60, "seek": 22092, "start": 226.55999999999997, "end": 231.04, "text": " And so managing that is kind of painful.", "tokens": [400, 370, 11642, 300, 307, 733, 295, 11697, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 61, "seek": 22092, "start": 231.04, "end": 236.51999999999998, "text": " And we're trying very hard to reduce downstream work, which is actually difficult for a source-based", "tokens": [400, 321, 434, 1382, 588, 1152, 281, 5407, 30621, 589, 11, 597, 307, 767, 2252, 337, 257, 4009, 12, 6032], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 62, "seek": 22092, "start": 236.51999999999998, "end": 238.23999999999998, "text": " distribution.", "tokens": [7316, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 63, "seek": 22092, "start": 238.23999999999998, "end": 241.6, "text": " If you think about how SPAC is structured, there's this mainline develop branch that", "tokens": [759, 291, 519, 466, 577, 8420, 4378, 307, 18519, 11, 456, 311, 341, 2135, 1889, 1499, 9819, 300], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 64, "seek": 22092, "start": 241.6, "end": 242.6, "text": " actually most people use.", "tokens": [767, 881, 561, 764, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 65, "seek": 22092, "start": 242.6, "end": 245.39999999999998, "text": " They'll just clone it straight from the repo, build from that, kind of like you do with", "tokens": [814, 603, 445, 26506, 309, 2997, 490, 264, 49040, 11, 1322, 490, 300, 11, 733, 295, 411, 291, 360, 365], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 66, "seek": 22092, "start": 245.39999999999998, "end": 247.6, "text": " mixed packages or something.", "tokens": [7467, 17401, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 67, "seek": 22092, "start": 247.6, "end": 249.64, "text": " External contributors contribute there.", "tokens": [48277, 45627, 10586, 456, 13], "temperature": 0.0, "avg_logprob": -0.1513683076888796, "compression_ratio": 1.5727554179566563, "no_speech_prob": 8.137825716403313e-06}, {"id": 68, "seek": 24964, "start": 249.64, "end": 254.07999999999998, "text": " And we cut a release every once in a while where we stabilize the packages and keep them", "tokens": [400, 321, 1723, 257, 4374, 633, 1564, 294, 257, 1339, 689, 321, 31870, 264, 17401, 293, 1066, 552], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 69, "seek": 24964, "start": 254.07999999999998, "end": 259.56, "text": " sort of fixed so that you don't have a lot of version churn in the repo.", "tokens": [1333, 295, 6806, 370, 300, 291, 500, 380, 362, 257, 688, 295, 3037, 417, 925, 294, 264, 49040, 13], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 70, "seek": 24964, "start": 259.56, "end": 263.56, "text": " And then to actually integrate with the HPC facilities, all the places that are deploying", "tokens": [400, 550, 281, 767, 13365, 365, 264, 12557, 34, 9406, 11, 439, 264, 3190, 300, 366, 34198], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 71, "seek": 24964, "start": 263.56, "end": 269.64, "text": " supercomputers, we have this E4S software distribution where they end up doing a whole", "tokens": [27839, 2582, 433, 11, 321, 362, 341, 462, 19, 50, 4722, 7316, 689, 436, 917, 493, 884, 257, 1379], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 72, "seek": 24964, "start": 269.64, "end": 273.12, "text": " bunch of downstream integration at the site, where they're basically building the whole", "tokens": [3840, 295, 30621, 10980, 412, 264, 3621, 11, 689, 436, 434, 1936, 2390, 264, 1379], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 73, "seek": 24964, "start": 273.12, "end": 276.52, "text": " thing from source, essentially in a new environment.", "tokens": [551, 490, 4009, 11, 4476, 294, 257, 777, 2823, 13], "temperature": 0.0, "avg_logprob": -0.1258045264652797, "compression_ratio": 1.623728813559322, "no_speech_prob": 3.611307874962222e-06}, {"id": 74, "seek": 27652, "start": 276.52, "end": 279.68, "text": " And there's a whole lot of debugging that takes place there that we would really like", "tokens": [400, 456, 311, 257, 1379, 688, 295, 45592, 300, 2516, 1081, 456, 300, 321, 576, 534, 411], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 75, "seek": 27652, "start": 279.68, "end": 282.2, "text": " to be able to move upstream.", "tokens": [281, 312, 1075, 281, 1286, 33915, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 76, "seek": 27652, "start": 282.2, "end": 286.59999999999997, "text": " The applications, likewise, they are not necessarily using what the facility deploys.", "tokens": [440, 5821, 11, 32407, 11, 436, 366, 406, 4725, 1228, 437, 264, 8973, 368, 49522, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 77, "seek": 27652, "start": 286.59999999999997, "end": 287.59999999999997, "text": " Some of them do.", "tokens": [2188, 295, 552, 360, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 78, "seek": 27652, "start": 287.59999999999997, "end": 288.68, "text": " Some of them don't.", "tokens": [2188, 295, 552, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 79, "seek": 27652, "start": 288.68, "end": 290.52, "text": " They pull from basically all of these places.", "tokens": [814, 2235, 490, 1936, 439, 295, 613, 3190, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 80, "seek": 27652, "start": 290.52, "end": 294.08, "text": " They might get a math solver library from the facility.", "tokens": [814, 1062, 483, 257, 5221, 1404, 331, 6405, 490, 264, 8973, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 81, "seek": 27652, "start": 294.08, "end": 298.76, "text": " They might get something else installed from SPAC mainline built the way that they want.", "tokens": [814, 1062, 483, 746, 1646, 8899, 490, 8420, 4378, 2135, 1889, 3094, 264, 636, 300, 436, 528, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 82, "seek": 27652, "start": 298.76, "end": 302.96, "text": " And they may pull stuff off of release branches too, all to assemble an application and have", "tokens": [400, 436, 815, 2235, 1507, 766, 295, 4374, 14770, 886, 11, 439, 281, 22364, 364, 3861, 293, 362], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 83, "seek": 27652, "start": 302.96, "end": 304.2, "text": " it built.", "tokens": [309, 3094, 13], "temperature": 0.0, "avg_logprob": -0.11028048149625161, "compression_ratio": 1.7818791946308725, "no_speech_prob": 2.6839240945264464e-06}, {"id": 84, "seek": 30420, "start": 304.2, "end": 309.28, "text": " And so this is a lot of porting at the lowest end, and what we'd really like to do is take", "tokens": [400, 370, 341, 307, 257, 688, 295, 2436, 278, 412, 264, 12437, 917, 11, 293, 437, 321, 1116, 534, 411, 281, 360, 307, 747], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 85, "seek": 30420, "start": 309.28, "end": 316.0, "text": " that software integration and move it upstream and get to a point where we can have these", "tokens": [300, 4722, 10980, 293, 1286, 309, 33915, 293, 483, 281, 257, 935, 689, 321, 393, 362, 613], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 86, "seek": 30420, "start": 316.0, "end": 322.15999999999997, "text": " types of environments building NCI all the time in sort of a rolling release and do binary", "tokens": [3467, 295, 12388, 2390, 20786, 40, 439, 264, 565, 294, 1333, 295, 257, 9439, 4374, 293, 360, 17434], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 87, "seek": 30420, "start": 322.15999999999997, "end": 325.91999999999996, "text": " deploys on the supercomputers with actual optimized binaries.", "tokens": [368, 49522, 322, 264, 27839, 2582, 433, 365, 3539, 26941, 5171, 4889, 13], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 88, "seek": 30420, "start": 325.91999999999996, "end": 327.28, "text": " So that's what we're trying to get to.", "tokens": [407, 300, 311, 437, 321, 434, 1382, 281, 483, 281, 13], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 89, "seek": 30420, "start": 327.28, "end": 330.91999999999996, "text": " So we set out to make a binary distribution with a bunch of different goals.", "tokens": [407, 321, 992, 484, 281, 652, 257, 17434, 7316, 365, 257, 3840, 295, 819, 5493, 13], "temperature": 0.0, "avg_logprob": -0.09470347176610897, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.3285327895573573e-06}, {"id": 90, "seek": 33092, "start": 330.92, "end": 337.04, "text": " The main one, and the one that's pretty key to our whole ecosystem, is it has to be sustainable.", "tokens": [440, 2135, 472, 11, 293, 264, 472, 300, 311, 1238, 2141, 281, 527, 1379, 11311, 11, 307, 309, 575, 281, 312, 11235, 13], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 91, "seek": 33092, "start": 337.04, "end": 339.36, "text": " We don't have that many maintainers.", "tokens": [492, 500, 380, 362, 300, 867, 6909, 433, 13], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 92, "seek": 33092, "start": 339.36, "end": 343.76, "text": " And they currently, their workflow is basically to work with people who are making contributions,", "tokens": [400, 436, 4362, 11, 641, 20993, 307, 1936, 281, 589, 365, 561, 567, 366, 1455, 15725, 11], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 93, "seek": 33092, "start": 343.76, "end": 347.28000000000003, "text": " on pull requests, help them get them merged, and then move on to the next one.", "tokens": [322, 2235, 12475, 11, 854, 552, 483, 552, 36427, 11, 293, 550, 1286, 322, 281, 264, 958, 472, 13], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 94, "seek": 33092, "start": 347.28000000000003, "end": 351.56, "text": " And we don't want them to have to sit around and babysit builds on, say, a release integration", "tokens": [400, 321, 500, 380, 528, 552, 281, 362, 281, 1394, 926, 293, 39764, 270, 15182, 322, 11, 584, 11, 257, 4374, 10980], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 95, "seek": 33092, "start": 351.56, "end": 354.08000000000004, "text": " branch all the time.", "tokens": [9819, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 96, "seek": 33092, "start": 354.08000000000004, "end": 357.96000000000004, "text": " We want a rolling release because people do tend to use the develop branch.", "tokens": [492, 528, 257, 9439, 4374, 570, 561, 360, 3928, 281, 764, 264, 1499, 9819, 13], "temperature": 0.0, "avg_logprob": -0.11754045032319568, "compression_ratio": 1.7370242214532872, "no_speech_prob": 1.6794808743725298e-06}, {"id": 97, "seek": 35796, "start": 357.96, "end": 361.88, "text": " And so we want that to be up to date with pretty current binaries all the time.", "tokens": [400, 370, 321, 528, 300, 281, 312, 493, 281, 4002, 365, 1238, 2190, 5171, 4889, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 98, "seek": 35796, "start": 361.88, "end": 366.44, "text": " But some people do fix themselves to releases, and so we want sort of snapshots for those", "tokens": [583, 512, 561, 360, 3191, 2969, 281, 16952, 11, 293, 370, 321, 528, 1333, 295, 19206, 27495, 337, 729], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 99, "seek": 35796, "start": 366.44, "end": 368.03999999999996, "text": " releases as well.", "tokens": [16952, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 100, "seek": 35796, "start": 368.03999999999996, "end": 372.35999999999996, "text": " We need to be able to support, at least eventually, all the packages that are in SPAC.", "tokens": [492, 643, 281, 312, 1075, 281, 1406, 11, 412, 1935, 4728, 11, 439, 264, 17401, 300, 366, 294, 8420, 4378, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 101, "seek": 35796, "start": 372.35999999999996, "end": 375.32, "text": " And it still has to be source-buildable around those binaries.", "tokens": [400, 309, 920, 575, 281, 312, 4009, 12, 11516, 712, 926, 729, 5171, 4889, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 102, "seek": 35796, "start": 375.32, "end": 379.15999999999997, "text": " So if you want to build a component and rely on binaries for some other component, we want", "tokens": [407, 498, 291, 528, 281, 1322, 257, 6542, 293, 10687, 322, 5171, 4889, 337, 512, 661, 6542, 11, 321, 528], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 103, "seek": 35796, "start": 379.15999999999997, "end": 380.52, "text": " to support that.", "tokens": [281, 1406, 300, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 104, "seek": 35796, "start": 380.52, "end": 383.59999999999997, "text": " And then finally, people trust sources.", "tokens": [400, 550, 2721, 11, 561, 3361, 7139, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 105, "seek": 35796, "start": 383.59999999999997, "end": 384.59999999999997, "text": " They can check some of them.", "tokens": [814, 393, 1520, 512, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 106, "seek": 35796, "start": 384.59999999999997, "end": 385.59999999999997, "text": " You can download the tarball.", "tokens": [509, 393, 5484, 264, 3112, 3129, 13], "temperature": 0.0, "avg_logprob": -0.11727481998809397, "compression_ratio": 1.7953795379537953, "no_speech_prob": 2.8569650112331146e-06}, {"id": 107, "seek": 38560, "start": 385.6, "end": 389.52000000000004, "text": " You can usually check some of them, except for when GitHub changes the hashes.", "tokens": [509, 393, 2673, 1520, 512, 295, 552, 11, 3993, 337, 562, 23331, 2962, 264, 575, 8076, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 108, "seek": 38560, "start": 389.52000000000004, "end": 394.16, "text": " But we want to ensure that the binaries that we're generating are just as trustworthy as", "tokens": [583, 321, 528, 281, 5586, 300, 264, 5171, 4889, 300, 321, 434, 17746, 366, 445, 382, 39714, 382], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 109, "seek": 38560, "start": 394.16, "end": 395.16, "text": " the sources.", "tokens": [264, 7139, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 110, "seek": 38560, "start": 395.16, "end": 399.24, "text": " So we've taken some steps to ensure that.", "tokens": [407, 321, 600, 2726, 512, 4439, 281, 5586, 300, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 111, "seek": 38560, "start": 399.24, "end": 404.16, "text": " So SPAC is a little different from your standard distro if you haven't gathered already.", "tokens": [407, 8420, 4378, 307, 257, 707, 819, 490, 428, 3832, 1483, 340, 498, 291, 2378, 380, 13032, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 112, "seek": 38560, "start": 404.16, "end": 409.40000000000003, "text": " If you think about a traditional package manager, you have a sort of a recipe per configuration.", "tokens": [759, 291, 519, 466, 257, 5164, 7372, 6598, 11, 291, 362, 257, 1333, 295, 257, 6782, 680, 11694, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 113, "seek": 38560, "start": 409.40000000000003, "end": 412.24, "text": " And so that's like your RPM build spec or dev spec or whatever.", "tokens": [400, 370, 300, 311, 411, 428, 37389, 1322, 1608, 420, 1905, 1608, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1794222155187884, "compression_ratio": 1.5892255892255893, "no_speech_prob": 1.1477628504508175e-05}, {"id": 114, "seek": 41224, "start": 412.24, "end": 416.04, "text": " It goes into a build farm, and you produce packages, at least for one platform, in sort", "tokens": [467, 1709, 666, 257, 1322, 5421, 11, 293, 291, 5258, 17401, 11, 412, 1935, 337, 472, 3663, 11, 294, 1333], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 115, "seek": 41224, "start": 416.04, "end": 420.6, "text": " of a one-to-one relationship with those specs, actually.", "tokens": [295, 257, 472, 12, 1353, 12, 546, 2480, 365, 729, 27911, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 116, "seek": 41224, "start": 420.6, "end": 423.64, "text": " There's templating and things that goes on to reduce that.", "tokens": [821, 311, 9100, 990, 293, 721, 300, 1709, 322, 281, 5407, 300, 13], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 117, "seek": 41224, "start": 423.64, "end": 427.8, "text": " But you're typically maintaining one software stack that gets updated over time.", "tokens": [583, 291, 434, 5850, 14916, 472, 4722, 8630, 300, 2170, 10588, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 118, "seek": 41224, "start": 427.8, "end": 431.84000000000003, "text": " In SPAC, what we're trying to do is we have these parameterized package recipes that go", "tokens": [682, 8420, 4378, 11, 437, 321, 434, 1382, 281, 360, 307, 321, 362, 613, 13075, 1602, 7372, 13035, 300, 352], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 119, "seek": 41224, "start": 431.84000000000003, "end": 435.2, "text": " into build farm, but it's really the same recipe that's being used across different", "tokens": [666, 1322, 5421, 11, 457, 309, 311, 534, 264, 912, 6782, 300, 311, 885, 1143, 2108, 819], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 120, "seek": 41224, "start": 435.2, "end": 436.2, "text": " architectures.", "tokens": [6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 121, "seek": 41224, "start": 436.2, "end": 439.88, "text": " We force the contributors to work on the same package so that essentially you're modeling", "tokens": [492, 3464, 264, 45627, 281, 589, 322, 264, 912, 7372, 370, 300, 4476, 291, 434, 15983], "temperature": 0.0, "avg_logprob": -0.1504010518391927, "compression_ratio": 1.7208588957055215, "no_speech_prob": 4.222214101901045e-06}, {"id": 122, "seek": 43988, "start": 439.88, "end": 443.2, "text": " all the different ways the software can be used, and we try to get a lot of reuse out", "tokens": [439, 264, 819, 2098, 264, 4722, 393, 312, 1143, 11, 293, 321, 853, 281, 483, 257, 688, 295, 26225, 484], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 123, "seek": 43988, "start": 443.2, "end": 445.56, "text": " of the recipes across platforms.", "tokens": [295, 264, 13035, 2108, 9473, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 124, "seek": 43988, "start": 445.56, "end": 448.92, "text": " Those go into the build farm, and you can use the same recipes to produce optimized binaries", "tokens": [3950, 352, 666, 264, 1322, 5421, 11, 293, 291, 393, 764, 264, 912, 13035, 281, 5258, 26941, 5171, 4889], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 125, "seek": 43988, "start": 448.92, "end": 449.92, "text": " for lots of different platforms.", "tokens": [337, 3195, 295, 819, 9473, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 126, "seek": 43988, "start": 449.92, "end": 454.04, "text": " So you could get a graviton, arm build, you could get a Skylake binary, you could get", "tokens": [407, 291, 727, 483, 257, 26048, 266, 11, 3726, 1322, 11, 291, 727, 483, 257, 9879, 75, 619, 17434, 11, 291, 727, 483], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 127, "seek": 43988, "start": 454.04, "end": 455.92, "text": " a GPU build, and so on.", "tokens": [257, 18407, 1322, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 128, "seek": 43988, "start": 455.92, "end": 460.24, "text": " And then you could do that for many different software stacks for different use cases.", "tokens": [400, 550, 291, 727, 360, 300, 337, 867, 819, 4722, 30792, 337, 819, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 129, "seek": 43988, "start": 460.24, "end": 463.32, "text": " And then we want you to be able to build from source on top of that.", "tokens": [400, 550, 321, 528, 291, 281, 312, 1075, 281, 1322, 490, 4009, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 130, "seek": 43988, "start": 463.32, "end": 465.44, "text": " So that's what we're trying to do.", "tokens": [407, 300, 311, 437, 321, 434, 1382, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12237050286654769, "compression_ratio": 1.9257950530035335, "no_speech_prob": 6.53957249596715e-06}, {"id": 131, "seek": 46544, "start": 465.44, "end": 470.52, "text": " We put a CI architecture together that is sort of based around this.", "tokens": [492, 829, 257, 37777, 9482, 1214, 300, 307, 1333, 295, 2361, 926, 341, 13], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 132, "seek": 46544, "start": 470.52, "end": 473.28, "text": " Like I said, we want to be sustainable, we want to maintain the workflow that we already", "tokens": [1743, 286, 848, 11, 321, 528, 281, 312, 11235, 11, 321, 528, 281, 6909, 264, 20993, 300, 321, 1217], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 133, "seek": 46544, "start": 473.28, "end": 476.68, "text": " have on the project, and so we want people, we want basically GitHub to be the center", "tokens": [362, 322, 264, 1716, 11, 293, 370, 321, 528, 561, 11, 321, 528, 1936, 23331, 281, 312, 264, 3056], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 134, "seek": 46544, "start": 476.68, "end": 478.28, "text": " of the distribution.", "tokens": [295, 264, 7316, 13], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 135, "seek": 46544, "start": 478.28, "end": 481.76, "text": " What goes into develop is really maintaining the distribution as well as contributing to", "tokens": [708, 1709, 666, 1499, 307, 534, 14916, 264, 7316, 382, 731, 382, 19270, 281], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 136, "seek": 46544, "start": 481.76, "end": 482.88, "text": " the project.", "tokens": [264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 137, "seek": 46544, "start": 482.88, "end": 487.96, "text": " And so we have a bunch of infrastructure currently stood up in AWS to support this.", "tokens": [400, 370, 321, 362, 257, 3840, 295, 6896, 4362, 9371, 493, 294, 17650, 281, 1406, 341, 13], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 138, "seek": 46544, "start": 487.96, "end": 492.36, "text": " So the binaries themselves and the sources are all distributed through S3 and CloudFront.", "tokens": [407, 264, 5171, 4889, 2969, 293, 264, 7139, 366, 439, 12631, 807, 318, 18, 293, 8061, 37, 10001, 13], "temperature": 0.0, "avg_logprob": -0.10033452792430488, "compression_ratio": 1.7647058823529411, "no_speech_prob": 3.668246336019365e-06}, {"id": 139, "seek": 49236, "start": 492.36, "end": 498.52000000000004, "text": " We set up a big Kubernetes cluster to support autoscaling runners, and we're using high", "tokens": [492, 992, 493, 257, 955, 23145, 13630, 281, 1406, 1476, 10466, 4270, 33892, 11, 293, 321, 434, 1228, 1090], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 140, "seek": 49236, "start": 498.52000000000004, "end": 502.8, "text": " availability GitLab in there to drive the CI.", "tokens": [17945, 16939, 37880, 294, 456, 281, 3332, 264, 37777, 13], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 141, "seek": 49236, "start": 502.8, "end": 506.0, "text": " GitLab may seem like a strange choice for maintaining a distribution, but the motivation", "tokens": [16939, 37880, 815, 1643, 411, 257, 5861, 3922, 337, 14916, 257, 7316, 11, 457, 264, 12335], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 142, "seek": 49236, "start": 506.0, "end": 510.32, "text": " behind that is really that all of the HPC centers also have internal GitLabs, and so", "tokens": [2261, 300, 307, 534, 300, 439, 295, 264, 12557, 34, 10898, 611, 362, 6920, 16939, 43, 17243, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 143, "seek": 49236, "start": 510.32, "end": 512.48, "text": " do a lot of universities and other sites.", "tokens": [360, 257, 688, 295, 11779, 293, 661, 7533, 13], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 144, "seek": 49236, "start": 512.48, "end": 517.36, "text": " And so the goal is really for all of this automation and tooling to be usable not just", "tokens": [400, 370, 264, 3387, 307, 534, 337, 439, 295, 341, 17769, 293, 46593, 281, 312, 29975, 406, 445], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 145, "seek": 49236, "start": 517.36, "end": 521.84, "text": " in the cloud for the large distribution of SPAC, but also for people's personal software", "tokens": [294, 264, 4588, 337, 264, 2416, 7316, 295, 8420, 4378, 11, 457, 611, 337, 561, 311, 2973, 4722], "temperature": 0.0, "avg_logprob": -0.11832061646476624, "compression_ratio": 1.6935483870967742, "no_speech_prob": 6.048150680726394e-06}, {"id": 146, "seek": 52184, "start": 521.84, "end": 523.2800000000001, "text": " stacks locally.", "tokens": [30792, 16143, 13], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 147, "seek": 52184, "start": 523.2800000000001, "end": 528.5600000000001, "text": " And so the idea is that we're generating GitLab CI configuration, and you can use that either", "tokens": [400, 370, 264, 1558, 307, 300, 321, 434, 17746, 16939, 37880, 37777, 11694, 11, 293, 291, 393, 764, 300, 2139], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 148, "seek": 52184, "start": 528.5600000000001, "end": 534.32, "text": " for this or internally or in an air gap network somewhere.", "tokens": [337, 341, 420, 19501, 420, 294, 364, 1988, 7417, 3209, 4079, 13], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 149, "seek": 52184, "start": 534.32, "end": 539.76, "text": " So we're leveraging Carpenter on the backend for just-in-time instances for runner pools.", "tokens": [407, 321, 434, 32666, 2741, 79, 14278, 322, 264, 38087, 337, 445, 12, 259, 12, 3766, 14519, 337, 24376, 28688, 13], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 150, "seek": 52184, "start": 539.76, "end": 544.44, "text": " That's a tool for AWS, it's open source, you can find it on GitHub.", "tokens": [663, 311, 257, 2290, 337, 17650, 11, 309, 311, 1269, 4009, 11, 291, 393, 915, 309, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 151, "seek": 52184, "start": 544.44, "end": 549.1600000000001, "text": " It essentially lets you make requests for nodes with certain amounts of memory, certain target", "tokens": [467, 4476, 6653, 291, 652, 12475, 337, 13891, 365, 1629, 11663, 295, 4675, 11, 1629, 3779], "temperature": 0.0, "avg_logprob": -0.12858158293224517, "compression_ratio": 1.5767790262172285, "no_speech_prob": 9.664856406743638e-06}, {"id": 152, "seek": 54916, "start": 549.16, "end": 553.7199999999999, "text": " architectures, and so on, and it manages containers on the instances for you on the", "tokens": [6331, 1303, 11, 293, 370, 322, 11, 293, 309, 22489, 17089, 322, 264, 14519, 337, 291, 322, 264], "temperature": 0.0, "avg_logprob": -0.15708292373503097, "compression_ratio": 1.5482625482625483, "no_speech_prob": 1.8337421352043748e-05}, {"id": 153, "seek": 54916, "start": 553.7199999999999, "end": 562.56, "text": " backend and sort of moves work around so that you can have an efficient build pool in Kubernetes.", "tokens": [38087, 293, 1333, 295, 6067, 589, 926, 370, 300, 291, 393, 362, 364, 7148, 1322, 7005, 294, 23145, 13], "temperature": 0.0, "avg_logprob": -0.15708292373503097, "compression_ratio": 1.5482625482625483, "no_speech_prob": 1.8337421352043748e-05}, {"id": 154, "seek": 54916, "start": 562.56, "end": 567.6, "text": " We also have some bare-metal runners at the University of Oregon with more exotic architectures", "tokens": [492, 611, 362, 512, 6949, 12, 39857, 33892, 412, 264, 3535, 295, 18664, 365, 544, 27063, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.15708292373503097, "compression_ratio": 1.5482625482625483, "no_speech_prob": 1.8337421352043748e-05}, {"id": 155, "seek": 54916, "start": 567.6, "end": 569.8, "text": " than you can maybe find in the cloud.", "tokens": [813, 291, 393, 1310, 915, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.15708292373503097, "compression_ratio": 1.5482625482625483, "no_speech_prob": 1.8337421352043748e-05}, {"id": 156, "seek": 54916, "start": 569.8, "end": 575.64, "text": " So like there's an AMD MI 200 GPU builder in there, there's A64FX, which is what runs", "tokens": [407, 411, 456, 311, 364, 34808, 13696, 2331, 18407, 27377, 294, 456, 11, 456, 311, 316, 19395, 36092, 11, 597, 307, 437, 6676], "temperature": 0.0, "avg_logprob": -0.15708292373503097, "compression_ratio": 1.5482625482625483, "no_speech_prob": 1.8337421352043748e-05}, {"id": 157, "seek": 57564, "start": 575.64, "end": 581.28, "text": " on Sugaku, it's the ARM architecture with vector instructions, Power9, and so on.", "tokens": [322, 39131, 15803, 11, 309, 311, 264, 45209, 9482, 365, 8062, 9415, 11, 7086, 24, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 158, "seek": 57564, "start": 581.28, "end": 586.12, "text": " And so we are able to do runs there for architectures that aren't supported in the cloud.", "tokens": [400, 370, 321, 366, 1075, 281, 360, 6676, 456, 337, 6331, 1303, 300, 3212, 380, 8104, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 159, "seek": 57564, "start": 586.12, "end": 588.28, "text": " There's some monitoring thrown in.", "tokens": [821, 311, 512, 11028, 11732, 294, 13], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 160, "seek": 57564, "start": 588.28, "end": 592.08, "text": " We haven't really leveraged it in a smart way yet, but we are collecting a lot of data", "tokens": [492, 2378, 380, 534, 12451, 2980, 309, 294, 257, 4069, 636, 1939, 11, 457, 321, 366, 12510, 257, 688, 295, 1412], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 161, "seek": 57564, "start": 592.08, "end": 593.36, "text": " about our builds.", "tokens": [466, 527, 15182, 13], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 162, "seek": 57564, "start": 593.36, "end": 598.4399999999999, "text": " And then there's a bot that helps sort of coordinate between GitHub and GitLab.", "tokens": [400, 550, 456, 311, 257, 10592, 300, 3665, 1333, 295, 15670, 1296, 23331, 293, 16939, 37880, 13], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 163, "seek": 57564, "start": 598.4399999999999, "end": 603.96, "text": " And so we have sort of a sync script that allows us to build off of forks and things", "tokens": [400, 370, 321, 362, 1333, 295, 257, 20271, 5755, 300, 4045, 505, 281, 1322, 766, 295, 337, 1694, 293, 721], "temperature": 0.0, "avg_logprob": -0.11111380004882812, "compression_ratio": 1.6527777777777777, "no_speech_prob": 5.17325406690361e-06}, {"id": 164, "seek": 60396, "start": 603.96, "end": 606.8000000000001, "text": " like that in GitLab over this whole setup.", "tokens": [411, 300, 294, 16939, 37880, 670, 341, 1379, 8657, 13], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 165, "seek": 60396, "start": 606.8000000000001, "end": 612.08, "text": " So it's fairly custom, but at least the GitLab component is recyclable internally.", "tokens": [407, 309, 311, 6457, 2375, 11, 457, 412, 1935, 264, 16939, 37880, 6542, 307, 12036, 3474, 712, 19501, 13], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 166, "seek": 60396, "start": 612.08, "end": 615.2, "text": " And we would like to be able to support more runners in the future, like if maybe we want", "tokens": [400, 321, 576, 411, 281, 312, 1075, 281, 1406, 544, 33892, 294, 264, 2027, 11, 411, 498, 1310, 321, 528], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 167, "seek": 60396, "start": 615.2, "end": 619.08, "text": " to work with Azure on their HPC setup and they want to provide runners for the project", "tokens": [281, 589, 365, 11969, 322, 641, 12557, 34, 8657, 293, 436, 528, 281, 2893, 33892, 337, 264, 1716], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 168, "seek": 60396, "start": 619.08, "end": 624.1600000000001, "text": " or if other universities and places want to provide runners, we want to leave that open.", "tokens": [420, 498, 661, 11779, 293, 3190, 528, 281, 2893, 33892, 11, 321, 528, 281, 1856, 300, 1269, 13], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 169, "seek": 60396, "start": 624.1600000000001, "end": 630.0400000000001, "text": " For maintaining the stacks themselves, we made it possible to sort of instantiate a new", "tokens": [1171, 14916, 264, 30792, 2969, 11, 321, 1027, 309, 1944, 281, 1333, 295, 9836, 13024, 257, 777], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 170, "seek": 60396, "start": 630.0400000000001, "end": 631.88, "text": " stack in a pull request.", "tokens": [8630, 294, 257, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.10294881943733461, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.853561899333727e-06}, {"id": 171, "seek": 63188, "start": 631.88, "end": 635.88, "text": " And so we have this directory full of the sort of 16 stacks that we currently build", "tokens": [400, 370, 321, 362, 341, 21120, 1577, 295, 264, 1333, 295, 3165, 30792, 300, 321, 4362, 1322], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 172, "seek": 63188, "start": 635.88, "end": 636.88, "text": " in CI.", "tokens": [294, 37777, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 173, "seek": 63188, "start": 636.88, "end": 638.04, "text": " You can see them there.", "tokens": [509, 393, 536, 552, 456, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 174, "seek": 63188, "start": 638.04, "end": 644.92, "text": " Each one of those is some targeted software stack for some type of machine or some group.", "tokens": [6947, 472, 295, 729, 307, 512, 15045, 4722, 8630, 337, 512, 2010, 295, 3479, 420, 512, 1594, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 175, "seek": 63188, "start": 644.92, "end": 649.8, "text": " Each of those contains sort of a YAML file with configuration for the stack in it.", "tokens": [6947, 295, 729, 8306, 1333, 295, 257, 398, 2865, 43, 3991, 365, 11694, 337, 264, 8630, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 176, "seek": 63188, "start": 649.8, "end": 652.6, "text": " And so the YAML file itself is fairly simple.", "tokens": [400, 370, 264, 398, 2865, 43, 3991, 2564, 307, 6457, 2199, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 177, "seek": 63188, "start": 652.6, "end": 656.52, "text": " It has a list of packages that you want to build, and so this is the machine learning", "tokens": [467, 575, 257, 1329, 295, 17401, 300, 291, 528, 281, 1322, 11, 293, 370, 341, 307, 264, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 178, "seek": 63188, "start": 656.52, "end": 658.28, "text": " one for CUDA.", "tokens": [472, 337, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.08430511115962623, "compression_ratio": 1.69140625, "no_speech_prob": 3.555566991053638e-06}, {"id": 179, "seek": 65828, "start": 658.28, "end": 662.16, "text": " Those are all the names of the stack recipes that you're building here.", "tokens": [3950, 366, 439, 264, 5288, 295, 264, 8630, 13035, 300, 291, 434, 2390, 510, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 180, "seek": 65828, "start": 662.16, "end": 664.04, "text": " And then some configuration up here.", "tokens": [400, 550, 512, 11694, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 181, "seek": 65828, "start": 664.04, "end": 668.76, "text": " And so for this particular stack, you're saying, I want to build for x8664v3, which", "tokens": [400, 370, 337, 341, 1729, 8630, 11, 291, 434, 1566, 11, 286, 528, 281, 1322, 337, 2031, 22193, 19395, 85, 18, 11, 597], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 182, "seek": 65828, "start": 668.76, "end": 671.1999999999999, "text": " is AVX2.", "tokens": [307, 30198, 55, 17, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 183, "seek": 65828, "start": 671.1999999999999, "end": 675.92, "text": " And I want to disable Rockum and enable CUDA, except on LLVM because there's some weird", "tokens": [400, 286, 528, 281, 28362, 6922, 449, 293, 9528, 29777, 7509, 11, 3993, 322, 441, 43, 53, 44, 570, 456, 311, 512, 3657], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 184, "seek": 65828, "start": 675.92, "end": 679.92, "text": " bug with the CUDA support there, at least in our stack.", "tokens": [7426, 365, 264, 29777, 7509, 1406, 456, 11, 412, 1935, 294, 527, 8630, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 185, "seek": 65828, "start": 679.92, "end": 681.76, "text": " And so you can see it's fairly concise.", "tokens": [400, 370, 291, 393, 536, 309, 311, 6457, 44882, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 186, "seek": 65828, "start": 681.76, "end": 682.8399999999999, "text": " You make a list of packages.", "tokens": [509, 652, 257, 1329, 295, 17401, 13], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 187, "seek": 65828, "start": 682.8399999999999, "end": 687.6, "text": " You say, here's the configuration I want, and you can go and take this thing and build", "tokens": [509, 584, 11, 510, 311, 264, 11694, 286, 528, 11, 293, 291, 393, 352, 293, 747, 341, 551, 293, 1322], "temperature": 0.0, "avg_logprob": -0.15323293209075928, "compression_ratio": 1.6589403973509933, "no_speech_prob": 4.63749756818288e-06}, {"id": 188, "seek": 68760, "start": 687.6, "end": 690.28, "text": " a bunch of packages.", "tokens": [257, 3840, 295, 17401, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 189, "seek": 68760, "start": 690.28, "end": 694.0400000000001, "text": " We make it easy to change sort of low-level stack-wide parameters.", "tokens": [492, 652, 309, 1858, 281, 1319, 1333, 295, 2295, 12, 12418, 8630, 12, 7990, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 190, "seek": 68760, "start": 694.0400000000001, "end": 698.96, "text": " So the parameterized packages in stack, you can tell it to build with a different compiler.", "tokens": [407, 264, 13075, 1602, 17401, 294, 8630, 11, 291, 393, 980, 309, 281, 1322, 365, 257, 819, 31958, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 191, "seek": 68760, "start": 698.96, "end": 706.24, "text": " And so we had essentially this large E4S stack with maybe 600 packages working in standard", "tokens": [400, 370, 321, 632, 4476, 341, 2416, 462, 19, 50, 8630, 365, 1310, 11849, 17401, 1364, 294, 3832], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 192, "seek": 68760, "start": 706.24, "end": 707.24, "text": " environments.", "tokens": [12388, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 193, "seek": 68760, "start": 707.24, "end": 709.52, "text": " We wanted to support the one API compilers from Intel.", "tokens": [492, 1415, 281, 1406, 264, 472, 9362, 715, 388, 433, 490, 19762, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 194, "seek": 68760, "start": 709.52, "end": 712.8000000000001, "text": " And so that's Intel's new optimizing compilers.", "tokens": [400, 370, 300, 311, 19762, 311, 777, 40425, 715, 388, 433, 13], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 195, "seek": 68760, "start": 712.8000000000001, "end": 717.12, "text": " It is unlikely that anyone has ever run this much open source through a proprietary vendor", "tokens": [467, 307, 17518, 300, 2878, 575, 1562, 1190, 341, 709, 1269, 4009, 807, 257, 38992, 24321], "temperature": 0.0, "avg_logprob": -0.12552722962964483, "compression_ratio": 1.6313993174061434, "no_speech_prob": 6.43846215098165e-06}, {"id": 196, "seek": 71712, "start": 717.12, "end": 719.92, "text": " compiler like that, but it is client-based.", "tokens": [31958, 411, 300, 11, 457, 309, 307, 6423, 12, 6032, 13], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 197, "seek": 71712, "start": 719.92, "end": 723.28, "text": " And so we were able to throw one API into the config by just saying, here's where one", "tokens": [400, 370, 321, 645, 1075, 281, 3507, 472, 9362, 666, 264, 6662, 538, 445, 1566, 11, 510, 311, 689, 472], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 198, "seek": 71712, "start": 723.28, "end": 728.96, "text": " API lives, and make all packages require one API.", "tokens": [9362, 2909, 11, 293, 652, 439, 17401, 3651, 472, 9362, 13], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 199, "seek": 71712, "start": 728.96, "end": 732.88, "text": " And so the build system swaps in the one API compiler through some wrappers that are at", "tokens": [400, 370, 264, 1322, 1185, 1693, 2382, 294, 264, 472, 9362, 31958, 807, 512, 7843, 15226, 300, 366, 412], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 200, "seek": 71712, "start": 732.88, "end": 733.88, "text": " the lower level.", "tokens": [264, 3126, 1496, 13], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 201, "seek": 71712, "start": 733.88, "end": 737.32, "text": " And we were able to get that stack working in a week or two, despite the fact that we've", "tokens": [400, 321, 645, 1075, 281, 483, 300, 8630, 1364, 294, 257, 1243, 420, 732, 11, 7228, 264, 1186, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 202, "seek": 71712, "start": 737.32, "end": 740.52, "text": " never built a lot of these packages with one API before.", "tokens": [1128, 3094, 257, 688, 295, 613, 17401, 365, 472, 9362, 949, 13], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 203, "seek": 71712, "start": 740.52, "end": 742.92, "text": " So I think that's actually pretty cool.", "tokens": [407, 286, 519, 300, 311, 767, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 204, "seek": 71712, "start": 742.92, "end": 746.52, "text": " In a lot of cases, it's not worth it to use a vendor compiler because there's so many", "tokens": [682, 257, 688, 295, 3331, 11, 309, 311, 406, 3163, 309, 281, 764, 257, 24321, 31958, 570, 456, 311, 370, 867], "temperature": 0.0, "avg_logprob": -0.12713586961900866, "compression_ratio": 1.793548387096774, "no_speech_prob": 7.071542313497048e-06}, {"id": 205, "seek": 74652, "start": 746.52, "end": 749.88, "text": " bugs and issues with software that's never been built.", "tokens": [15120, 293, 2663, 365, 4722, 300, 311, 1128, 668, 3094, 13], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 206, "seek": 74652, "start": 749.88, "end": 754.88, "text": " But here, we're just really throwing sort of a bunch of open source packages through,", "tokens": [583, 510, 11, 321, 434, 445, 534, 10238, 1333, 295, 257, 3840, 295, 1269, 4009, 17401, 807, 11], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 207, "seek": 74652, "start": 754.88, "end": 757.12, "text": " and it helped us communicate with Intel.", "tokens": [293, 309, 4254, 505, 7890, 365, 19762, 13], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 208, "seek": 74652, "start": 757.12, "end": 760.4399999999999, "text": " We were able to say, hey, here are bugs that we're seeing with your compiler.", "tokens": [492, 645, 1075, 281, 584, 11, 4177, 11, 510, 366, 15120, 300, 321, 434, 2577, 365, 428, 31958, 13], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 209, "seek": 74652, "start": 760.4399999999999, "end": 764.92, "text": " We can link you directly to the build log for the build that failed.", "tokens": [492, 393, 2113, 291, 3838, 281, 264, 1322, 3565, 337, 264, 1322, 300, 7612, 13], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 210, "seek": 74652, "start": 764.92, "end": 769.0, "text": " And that helps them patch up the compiler, and it continues to help them ensure that", "tokens": [400, 300, 3665, 552, 9972, 493, 264, 31958, 11, 293, 309, 6515, 281, 854, 552, 5586, 300], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 211, "seek": 74652, "start": 769.0, "end": 774.16, "text": " it can build everything it needs to.", "tokens": [309, 393, 1322, 1203, 309, 2203, 281, 13], "temperature": 0.0, "avg_logprob": -0.11714916569846016, "compression_ratio": 1.717557251908397, "no_speech_prob": 9.514677003608085e-06}, {"id": 212, "seek": 77416, "start": 774.16, "end": 776.76, "text": " In SPAC, you don't.", "tokens": [682, 8420, 4378, 11, 291, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 213, "seek": 77416, "start": 776.76, "end": 780.68, "text": " So like I said, the recipes are these parameterized things, and so there's actually a solving", "tokens": [407, 411, 286, 848, 11, 264, 13035, 366, 613, 13075, 1602, 721, 11, 293, 370, 456, 311, 767, 257, 12606], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 214, "seek": 77416, "start": 780.68, "end": 782.9599999999999, "text": " step to these stacks.", "tokens": [1823, 281, 613, 30792, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 215, "seek": 77416, "start": 782.9599999999999, "end": 787.28, "text": " You saw sort of the requirements in the YAML file that said what I want to build.", "tokens": [509, 1866, 1333, 295, 264, 7728, 294, 264, 398, 2865, 43, 3991, 300, 848, 437, 286, 528, 281, 1322, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 216, "seek": 77416, "start": 787.28, "end": 791.76, "text": " We run that through our packet solver to get sort of a fully resolved graph of all the", "tokens": [492, 1190, 300, 807, 527, 20300, 1404, 331, 281, 483, 1333, 295, 257, 4498, 20772, 4295, 295, 439, 264], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 217, "seek": 77416, "start": 791.76, "end": 794.4, "text": " things that need to be built in a stack.", "tokens": [721, 300, 643, 281, 312, 3094, 294, 257, 8630, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 218, "seek": 77416, "start": 794.4, "end": 797.56, "text": " And then that is used to generate a GitLab CI YAML.", "tokens": [400, 550, 300, 307, 1143, 281, 8460, 257, 16939, 37880, 37777, 398, 2865, 43, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 219, "seek": 77416, "start": 797.56, "end": 802.0799999999999, "text": " And then for one of the problems that we have to solve there is mapping builds to runners.", "tokens": [400, 550, 337, 472, 295, 264, 2740, 300, 321, 362, 281, 5039, 456, 307, 18350, 15182, 281, 33892, 13], "temperature": 0.0, "avg_logprob": -0.11571154665591112, "compression_ratio": 1.6885813148788926, "no_speech_prob": 9.971587132895365e-06}, {"id": 220, "seek": 80208, "start": 802.08, "end": 805.1600000000001, "text": " So once the whole thing is concrete, and we've said here's all the dependencies, these are", "tokens": [407, 1564, 264, 1379, 551, 307, 9859, 11, 293, 321, 600, 848, 510, 311, 439, 264, 36606, 11, 613, 366], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 221, "seek": 80208, "start": 805.1600000000001, "end": 809.76, "text": " all the exact build configurations we want to make, we have to say how that should be", "tokens": [439, 264, 1900, 1322, 31493, 321, 528, 281, 652, 11, 321, 362, 281, 584, 577, 300, 820, 312], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 222, "seek": 80208, "start": 809.76, "end": 811.8000000000001, "text": " mapped to particular runners.", "tokens": [33318, 281, 1729, 33892, 13], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 223, "seek": 80208, "start": 811.8000000000001, "end": 816.0, "text": " And so we don't currently support things like cross builds.", "tokens": [400, 370, 321, 500, 380, 4362, 1406, 721, 411, 3278, 15182, 13], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 224, "seek": 80208, "start": 816.0, "end": 821.0400000000001, "text": " So if you want to build for AVX 512 or the more fancy vector instructions on newer Intel", "tokens": [407, 498, 291, 528, 281, 1322, 337, 30198, 55, 1025, 4762, 420, 264, 544, 10247, 8062, 9415, 322, 17628, 19762], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 225, "seek": 80208, "start": 821.0400000000001, "end": 825.24, "text": " CPUs, you need to make sure that you get one of those CPUs in the build environment.", "tokens": [13199, 82, 11, 291, 643, 281, 652, 988, 300, 291, 483, 472, 295, 729, 13199, 82, 294, 264, 1322, 2823, 13], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 226, "seek": 80208, "start": 825.24, "end": 829.72, "text": " And so we say, if you match AVX 512, give me an AVX 512 runner.", "tokens": [400, 370, 321, 584, 11, 498, 291, 2995, 30198, 55, 1025, 4762, 11, 976, 385, 364, 30198, 55, 1025, 4762, 24376, 13], "temperature": 0.0, "avg_logprob": -0.11745827945310679, "compression_ratio": 1.6744186046511629, "no_speech_prob": 5.954453172307694e-06}, {"id": 227, "seek": 82972, "start": 829.72, "end": 834.6, "text": " If you match one of these somewhat atrocious, hard to build packages up here like LLVM and", "tokens": [759, 291, 2995, 472, 295, 613, 8344, 412, 340, 4139, 11, 1152, 281, 1322, 17401, 493, 510, 411, 441, 43, 53, 44, 293], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 228, "seek": 82972, "start": 834.6, "end": 839.1600000000001, "text": " PyTorch, give me a gigantic runner with lots of memory, things like that.", "tokens": [9953, 51, 284, 339, 11, 976, 385, 257, 26800, 24376, 365, 3195, 295, 4675, 11, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 229, "seek": 82972, "start": 839.1600000000001, "end": 842.28, "text": " And essentially what this is doing is it's just saying, here's the package properties", "tokens": [400, 4476, 437, 341, 307, 884, 307, 309, 311, 445, 1566, 11, 510, 311, 264, 7372, 7221], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 230, "seek": 82972, "start": 842.28, "end": 845.76, "text": " up at the top, here are the tags that should be on the runner, make sure that I get a runner", "tokens": [493, 412, 264, 1192, 11, 510, 366, 264, 18632, 300, 820, 312, 322, 264, 24376, 11, 652, 988, 300, 286, 483, 257, 24376], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 231, "seek": 82972, "start": 845.76, "end": 848.6, "text": " with those capabilities.", "tokens": [365, 729, 10862, 13], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 232, "seek": 82972, "start": 848.6, "end": 853.6, "text": " And we haven't got a schema for all the tags yet, but I think we could standardize this", "tokens": [400, 321, 2378, 380, 658, 257, 34078, 337, 439, 264, 18632, 1939, 11, 457, 286, 519, 321, 727, 3832, 1125, 341], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 233, "seek": 82972, "start": 853.6, "end": 859.0, "text": " and make it easy for someone to plug in runners at their own site for this sort of thing.", "tokens": [293, 652, 309, 1858, 337, 1580, 281, 5452, 294, 33892, 412, 641, 1065, 3621, 337, 341, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.13460886228334654, "compression_ratio": 1.70625, "no_speech_prob": 3.08919061353663e-06}, {"id": 234, "seek": 85900, "start": 859.0, "end": 861.2, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 235, "seek": 85900, "start": 861.2, "end": 867.2, "text": " So one of the things that we did here to ensure trust is we have essentially a build environment", "tokens": [407, 472, 295, 264, 721, 300, 321, 630, 510, 281, 5586, 3361, 307, 321, 362, 4476, 257, 1322, 2823], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 236, "seek": 85900, "start": 867.2, "end": 870.08, "text": " going on in pull requests.", "tokens": [516, 322, 294, 2235, 12475, 13], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 237, "seek": 85900, "start": 870.08, "end": 872.32, "text": " If you trust back, you're basically trusting the maintainers.", "tokens": [759, 291, 3361, 646, 11, 291, 434, 1936, 28235, 264, 6909, 433, 13], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 238, "seek": 85900, "start": 872.32, "end": 876.88, "text": " We want to ensure that the binaries are things that are approved by the maintainers.", "tokens": [492, 528, 281, 5586, 300, 264, 5171, 4889, 366, 721, 300, 366, 10826, 538, 264, 6909, 433, 13], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 239, "seek": 85900, "start": 876.88, "end": 882.04, "text": " And so we can't just distribute binaries that got built in pull requests.", "tokens": [400, 370, 321, 393, 380, 445, 20594, 5171, 4889, 300, 658, 3094, 294, 2235, 12475, 13], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 240, "seek": 85900, "start": 882.04, "end": 886.12, "text": " So when contributors submit package changes, we go and we have private buckets for every", "tokens": [407, 562, 45627, 10315, 7372, 2962, 11, 321, 352, 293, 321, 362, 4551, 32191, 337, 633], "temperature": 0.0, "avg_logprob": -0.10840784946334696, "compression_ratio": 1.7549407114624507, "no_speech_prob": 9.816783858695999e-06}, {"id": 241, "seek": 88612, "start": 886.12, "end": 889.32, "text": " PR that we're supporting where we're doing the builds.", "tokens": [11568, 300, 321, 434, 7231, 689, 321, 434, 884, 264, 15182, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 242, "seek": 88612, "start": 889.32, "end": 891.2, "text": " The maintainers come along and say, oh, it worked.", "tokens": [440, 6909, 433, 808, 2051, 293, 584, 11, 1954, 11, 309, 2732, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 243, "seek": 88612, "start": 891.2, "end": 892.2, "text": " They review the code.", "tokens": [814, 3131, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 244, "seek": 88612, "start": 892.2, "end": 895.92, "text": " And then they say, okay, we can merge that and rebuild everything on develop and sign.", "tokens": [400, 550, 436, 584, 11, 1392, 11, 321, 393, 22183, 300, 293, 16877, 1203, 322, 1499, 293, 1465, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 245, "seek": 88612, "start": 895.92, "end": 899.96, "text": " So essentially everything in the main release is getting built from only approved recipes.", "tokens": [407, 4476, 1203, 294, 264, 2135, 4374, 307, 1242, 3094, 490, 787, 10826, 13035, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 246, "seek": 88612, "start": 899.96, "end": 903.48, "text": " It's not using any binaries that were built in the PR.", "tokens": [467, 311, 406, 1228, 604, 5171, 4889, 300, 645, 3094, 294, 264, 11568, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 247, "seek": 88612, "start": 903.48, "end": 906.12, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 248, "seek": 88612, "start": 906.12, "end": 913.6800000000001, "text": " The pull request integration, yeah, definitely makes things easy for contributors.", "tokens": [440, 2235, 5308, 10980, 11, 1338, 11, 2138, 1669, 721, 1858, 337, 45627, 13], "temperature": 0.0, "avg_logprob": -0.19662828361038612, "compression_ratio": 1.6691176470588236, "no_speech_prob": 1.4737998753844295e-05}, {"id": 249, "seek": 91368, "start": 913.68, "end": 918.7199999999999, "text": " And we were able to take the system and announce our public binary cache last June with something", "tokens": [400, 321, 645, 1075, 281, 747, 264, 1185, 293, 7478, 527, 1908, 17434, 19459, 1036, 6928, 365, 746], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 250, "seek": 91368, "start": 918.7199999999999, "end": 921.7199999999999, "text": " like 4600 builds in CI.", "tokens": [411, 1017, 15707, 15182, 294, 37777, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 251, "seek": 91368, "start": 921.7199999999999, "end": 923.1999999999999, "text": " And so it's mostly easy for contributors.", "tokens": [400, 370, 309, 311, 5240, 1858, 337, 45627, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 252, "seek": 91368, "start": 923.1999999999999, "end": 925.3599999999999, "text": " They get a status update on their pull request.", "tokens": [814, 483, 257, 6558, 5623, 322, 641, 2235, 5308, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 253, "seek": 91368, "start": 925.3599999999999, "end": 926.3599999999999, "text": " And mostly easy for users.", "tokens": [400, 5240, 1858, 337, 5022, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 254, "seek": 91368, "start": 926.3599999999999, "end": 930.3599999999999, "text": " They can just say, hey, use the binary mirror.", "tokens": [814, 393, 445, 584, 11, 4177, 11, 764, 264, 17434, 8013, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 255, "seek": 91368, "start": 930.3599999999999, "end": 932.3599999999999, "text": " So there are some problems.", "tokens": [407, 456, 366, 512, 2740, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 256, "seek": 91368, "start": 932.3599999999999, "end": 936.3199999999999, "text": " One issue is that build caches are a lot different from RPMs and devs.", "tokens": [1485, 2734, 307, 300, 1322, 269, 13272, 366, 257, 688, 819, 490, 14105, 26386, 293, 1905, 82, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 257, "seek": 91368, "start": 936.3199999999999, "end": 939.76, "text": " In most distributions, you would have sort of a stable ABI for your build cache.", "tokens": [682, 881, 37870, 11, 291, 576, 362, 1333, 295, 257, 8351, 316, 11291, 337, 428, 1322, 19459, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 258, "seek": 91368, "start": 939.76, "end": 942.4799999999999, "text": " Your rebuild package, you can throw it in the mix with the others.", "tokens": [2260, 16877, 7372, 11, 291, 393, 3507, 309, 294, 264, 2890, 365, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.14933941888471022, "compression_ratio": 1.6521739130434783, "no_speech_prob": 7.526957233494613e-06}, {"id": 259, "seek": 94248, "start": 942.48, "end": 946.44, "text": " Here if you modify one package, you really do have to rebuild all the dependents.", "tokens": [1692, 498, 291, 16927, 472, 7372, 11, 291, 534, 360, 362, 281, 16877, 439, 264, 5672, 791, 13], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 260, "seek": 94248, "start": 946.44, "end": 950.9200000000001, "text": " And so if you modify XZ here, then you have to build everything that depends on it again", "tokens": [400, 370, 498, 291, 16927, 1783, 57, 510, 11, 550, 291, 362, 281, 1322, 1203, 300, 5946, 322, 309, 797], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 261, "seek": 94248, "start": 950.9200000000001, "end": 952.36, "text": " in the build cache.", "tokens": [294, 264, 1322, 19459, 13], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 262, "seek": 94248, "start": 952.36, "end": 955.36, "text": " And so what that can mean is if you have a gigantic software stack like this one and", "tokens": [400, 370, 437, 300, 393, 914, 307, 498, 291, 362, 257, 26800, 4722, 8630, 411, 341, 472, 293], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 263, "seek": 94248, "start": 955.36, "end": 959.9200000000001, "text": " you modify, say, package conf at the bottom of it, it can trigger a massive rebuild of", "tokens": [291, 16927, 11, 584, 11, 7372, 1497, 412, 264, 2767, 295, 309, 11, 309, 393, 7875, 257, 5994, 16877, 295], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 264, "seek": 94248, "start": 959.9200000000001, "end": 962.32, "text": " everything in the stack.", "tokens": [1203, 294, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 265, "seek": 94248, "start": 962.32, "end": 965.8000000000001, "text": " And so that's one of the scalability problems that I think we're going to have to deal with", "tokens": [400, 370, 300, 311, 472, 295, 264, 15664, 2310, 2740, 300, 286, 519, 321, 434, 516, 281, 362, 281, 2028, 365], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 266, "seek": 94248, "start": 965.8000000000001, "end": 969.76, "text": " in the long term is that you can get these really long-running pipelines.", "tokens": [294, 264, 938, 1433, 307, 300, 291, 393, 483, 613, 534, 938, 12, 45482, 40168, 13], "temperature": 0.0, "avg_logprob": -0.1268314334517675, "compression_ratio": 1.8682432432432432, "no_speech_prob": 3.500635557429632e-06}, {"id": 267, "seek": 96976, "start": 969.76, "end": 976.36, "text": " Caches like Visit and PyTorch and so on will build forever, and it frustrates contributors.", "tokens": [383, 13272, 411, 24548, 293, 9953, 51, 284, 339, 293, 370, 322, 486, 1322, 5680, 11, 293, 309, 7454, 12507, 45627, 13], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 268, "seek": 96976, "start": 976.36, "end": 981.72, "text": " The other sort of thing that happens is if you think about how the release works on develop,", "tokens": [440, 661, 1333, 295, 551, 300, 2314, 307, 498, 291, 519, 466, 577, 264, 4374, 1985, 322, 1499, 11], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 269, "seek": 96976, "start": 981.72, "end": 985.52, "text": " you're picking a commit every once in a while and building it.", "tokens": [291, 434, 8867, 257, 5599, 633, 1564, 294, 257, 1339, 293, 2390, 309, 13], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 270, "seek": 96976, "start": 985.52, "end": 991.28, "text": " And if you have a PR that is sort of based behind the last develop build, that's OK.", "tokens": [400, 498, 291, 362, 257, 11568, 300, 307, 1333, 295, 2361, 2261, 264, 1036, 1499, 1322, 11, 300, 311, 2264, 13], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 271, "seek": 96976, "start": 991.28, "end": 994.64, "text": " Although GitHub typically wants to merge that with head, which means that you'll build a", "tokens": [5780, 23331, 5850, 2738, 281, 22183, 300, 365, 1378, 11, 597, 1355, 300, 291, 603, 1322, 257], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 272, "seek": 96976, "start": 994.64, "end": 997.6, "text": " lot of redundant things in your build environment.", "tokens": [688, 295, 40997, 721, 294, 428, 1322, 2823, 13], "temperature": 0.0, "avg_logprob": -0.13680900376418542, "compression_ratio": 1.6332179930795847, "no_speech_prob": 8.52862012834521e-06}, {"id": 273, "seek": 99760, "start": 997.6, "end": 1000.9200000000001, "text": " We can be picky and merge it with the last develop build to ensure that we get a lot", "tokens": [492, 393, 312, 41099, 293, 22183, 309, 365, 264, 1036, 1499, 1322, 281, 5586, 300, 321, 483, 257, 688], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 274, "seek": 99760, "start": 1000.9200000000001, "end": 1002.84, "text": " of cache reuse in the build environment.", "tokens": [295, 19459, 26225, 294, 264, 1322, 2823, 13], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 275, "seek": 99760, "start": 1002.84, "end": 1007.76, "text": " But what that means is if we get a PR that's out ahead of the last develop build and say", "tokens": [583, 437, 300, 1355, 307, 498, 321, 483, 257, 11568, 300, 311, 484, 2286, 295, 264, 1036, 1499, 1322, 293, 584], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 276, "seek": 99760, "start": 1007.76, "end": 1012.24, "text": " D up there is in progress, if you merge that second PR with D, you're basically going to", "tokens": [413, 493, 456, 307, 294, 4205, 11, 498, 291, 22183, 300, 1150, 11568, 365, 413, 11, 291, 434, 1936, 516, 281], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 277, "seek": 99760, "start": 1012.24, "end": 1016.0, "text": " be doing the same builds that D is doing but in a PR environment.", "tokens": [312, 884, 264, 912, 15182, 300, 413, 307, 884, 457, 294, 257, 11568, 2823, 13], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 278, "seek": 99760, "start": 1016.0, "end": 1021.52, "text": " And so if you have a bunch of those, we've brought GitLab down before by accidentally", "tokens": [400, 370, 498, 291, 362, 257, 3840, 295, 729, 11, 321, 600, 3038, 16939, 37880, 760, 949, 538, 15715], "temperature": 0.0, "avg_logprob": -0.10476044711903629, "compression_ratio": 1.7234848484848484, "no_speech_prob": 1.26772943076503e-06}, {"id": 279, "seek": 102152, "start": 1021.52, "end": 1027.8799999999999, "text": " building all of those PRs that are not caught up with the latest or for which develop has", "tokens": [2390, 439, 295, 729, 11568, 82, 300, 366, 406, 5415, 493, 365, 264, 6792, 420, 337, 597, 1499, 575], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 280, "seek": 102152, "start": 1027.8799999999999, "end": 1029.76, "text": " not caught up with them.", "tokens": [406, 5415, 493, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 281, "seek": 102152, "start": 1029.76, "end": 1034.36, "text": " And so we have to be picky and hold back these guys until there's a build ahead of them so", "tokens": [400, 370, 321, 362, 281, 312, 41099, 293, 1797, 646, 613, 1074, 1826, 456, 311, 257, 1322, 2286, 295, 552, 370], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 282, "seek": 102152, "start": 1034.36, "end": 1038.56, "text": " that we get enough reuse out of the cache to support this.", "tokens": [300, 321, 483, 1547, 26225, 484, 295, 264, 19459, 281, 1406, 341, 13], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 283, "seek": 102152, "start": 1038.56, "end": 1043.8, "text": " So the other problem with long pipelines is that they, depending on how reliable your", "tokens": [407, 264, 661, 1154, 365, 938, 40168, 307, 300, 436, 11, 5413, 322, 577, 12924, 428], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 284, "seek": 102152, "start": 1043.8, "end": 1048.08, "text": " infrastructure is, the more things that you build in a pipeline, the more likely you already", "tokens": [6896, 307, 11, 264, 544, 721, 300, 291, 1322, 294, 257, 15517, 11, 264, 544, 3700, 291, 1217], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 285, "seek": 102152, "start": 1048.08, "end": 1051.24, "text": " get a build failure somewhere.", "tokens": [483, 257, 1322, 7763, 4079, 13], "temperature": 0.0, "avg_logprob": -0.1004847816798998, "compression_ratio": 1.7426470588235294, "no_speech_prob": 2.6012833131972e-06}, {"id": 286, "seek": 105124, "start": 1051.24, "end": 1056.28, "text": " And so because we're building this cone of destruction in our pipelines, we are sort", "tokens": [400, 370, 570, 321, 434, 2390, 341, 19749, 295, 13563, 294, 527, 40168, 11, 321, 366, 1333], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 287, "seek": 105124, "start": 1056.28, "end": 1059.84, "text": " of subject to system failures happening in the pipeline somewhere.", "tokens": [295, 3983, 281, 1185, 20774, 2737, 294, 264, 15517, 4079, 13], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 288, "seek": 105124, "start": 1059.84, "end": 1063.6, "text": " And so users have to kind of babysit and restart builds that have nothing to do with what they're", "tokens": [400, 370, 5022, 362, 281, 733, 295, 39764, 270, 293, 21022, 15182, 300, 362, 1825, 281, 360, 365, 437, 436, 434], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 289, "seek": 105124, "start": 1063.6, "end": 1064.92, "text": " contributing.", "tokens": [19270, 13], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 290, "seek": 105124, "start": 1064.92, "end": 1068.8, "text": " So we're looking for ways that we could make that better.", "tokens": [407, 321, 434, 1237, 337, 2098, 300, 321, 727, 652, 300, 1101, 13], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 291, "seek": 105124, "start": 1068.8, "end": 1071.24, "text": " One issue that we have is consistency.", "tokens": [1485, 2734, 300, 321, 362, 307, 14416, 13], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 292, "seek": 105124, "start": 1071.24, "end": 1076.1200000000001, "text": " So when you test on PRs, it's not always sufficient to ensure that your develop branch is working.", "tokens": [407, 562, 291, 1500, 322, 11568, 82, 11, 309, 311, 406, 1009, 11563, 281, 5586, 300, 428, 1499, 9819, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.08463947122747248, "compression_ratio": 1.6875, "no_speech_prob": 6.746531653334387e-06}, {"id": 293, "seek": 107612, "start": 1076.12, "end": 1081.52, "text": " So you may have this initial package state, a PR gets submitted, you test with new B.", "tokens": [407, 291, 815, 362, 341, 5883, 7372, 1785, 11, 257, 11568, 2170, 14405, 11, 291, 1500, 365, 777, 363, 13], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 294, "seek": 107612, "start": 1081.52, "end": 1085.52, "text": " Another PR gets submitted, you test with new package C.", "tokens": [3996, 11568, 2170, 14405, 11, 291, 1500, 365, 777, 7372, 383, 13], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 295, "seek": 107612, "start": 1085.52, "end": 1089.3999999999999, "text": " If you take those and you don't require your PRs to be up to date with develop, when they", "tokens": [759, 291, 747, 729, 293, 291, 500, 380, 3651, 428, 11568, 82, 281, 312, 493, 281, 4002, 365, 1499, 11, 562, 436], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 296, "seek": 107612, "start": 1089.3999999999999, "end": 1094.8, "text": " both get merged, the state that's in develop is something that you've never tested because", "tokens": [1293, 483, 36427, 11, 264, 1785, 300, 311, 294, 1499, 307, 746, 300, 291, 600, 1128, 8246, 570], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 297, "seek": 107612, "start": 1094.8, "end": 1098.6399999999999, "text": " you have basically new versions of those two packages together now.", "tokens": [291, 362, 1936, 777, 9606, 295, 729, 732, 17401, 1214, 586, 13], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 298, "seek": 107612, "start": 1098.6399999999999, "end": 1101.2399999999998, "text": " And so there are ways to get around this.", "tokens": [400, 370, 456, 366, 2098, 281, 483, 926, 341, 13], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 299, "seek": 107612, "start": 1101.2399999999998, "end": 1102.6399999999999, "text": " One of them is merge queues.", "tokens": [1485, 295, 552, 307, 22183, 631, 1247, 13], "temperature": 0.0, "avg_logprob": -0.1085390478877698, "compression_ratio": 1.77992277992278, "no_speech_prob": 8.010898454813287e-06}, {"id": 300, "seek": 110264, "start": 1102.64, "end": 1106.2, "text": " So we're looking at merge queues as a way to scale this pipeline out.", "tokens": [407, 321, 434, 1237, 412, 22183, 631, 1247, 382, 257, 636, 281, 4373, 341, 15517, 484, 13], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 301, "seek": 110264, "start": 1106.2, "end": 1112.96, "text": " They essentially allow you to have pull requests with a small amount of testing where you then", "tokens": [814, 4476, 2089, 291, 281, 362, 2235, 12475, 365, 257, 1359, 2372, 295, 4997, 689, 291, 550], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 302, "seek": 110264, "start": 1112.96, "end": 1117.92, "text": " enqueue them in your sort of merge queue up there, that's the gray stuff.", "tokens": [465, 1077, 622, 552, 294, 428, 1333, 295, 22183, 18639, 493, 456, 11, 300, 311, 264, 10855, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 303, "seek": 110264, "start": 1117.92, "end": 1121.0800000000002, "text": " And they are sort of serialized for commit to develop.", "tokens": [400, 436, 366, 1333, 295, 17436, 1602, 337, 5599, 281, 1499, 13], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 304, "seek": 110264, "start": 1121.0800000000002, "end": 1125.8400000000001, "text": " If they succeed, then they're merged directly in a fast forward fashion.", "tokens": [759, 436, 7754, 11, 550, 436, 434, 36427, 3838, 294, 257, 2370, 2128, 6700, 13], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 305, "seek": 110264, "start": 1125.8400000000001, "end": 1130.64, "text": " And then basically the full testing is only done on the merge queue.", "tokens": [400, 550, 1936, 264, 1577, 4997, 307, 787, 1096, 322, 264, 22183, 18639, 13], "temperature": 0.0, "avg_logprob": -0.10438497861226399, "compression_ratio": 1.686046511627907, "no_speech_prob": 2.8568213110702345e-06}, {"id": 306, "seek": 113064, "start": 1130.64, "end": 1134.48, "text": " And you always are assured that the thing that you tested is the thing that gets merged", "tokens": [400, 291, 1009, 366, 23426, 300, 264, 551, 300, 291, 8246, 307, 264, 551, 300, 2170, 36427], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 307, "seek": 113064, "start": 1134.48, "end": 1135.6000000000001, "text": " into develop.", "tokens": [666, 1499, 13], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 308, "seek": 113064, "start": 1135.6000000000001, "end": 1141.44, "text": " So we're looking very much forward to GitHub making merge queue available in the next couple", "tokens": [407, 321, 434, 1237, 588, 709, 2128, 281, 23331, 1455, 22183, 18639, 2435, 294, 264, 958, 1916], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 309, "seek": 113064, "start": 1141.44, "end": 1142.44, "text": " of weeks.", "tokens": [295, 3259, 13], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 310, "seek": 113064, "start": 1142.44, "end": 1146.3600000000001, "text": " The other thing we think that could do is allow us to sort of stage the work on PRs.", "tokens": [440, 661, 551, 321, 519, 300, 727, 360, 307, 2089, 505, 281, 1333, 295, 3233, 264, 589, 322, 11568, 82, 13], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 311, "seek": 113064, "start": 1146.3600000000001, "end": 1149.2800000000002, "text": " So we're looking at ways we could scale this out.", "tokens": [407, 321, 434, 1237, 412, 2098, 321, 727, 4373, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 312, "seek": 113064, "start": 1149.2800000000002, "end": 1154.1200000000001, "text": " Right now, for a relatively small number of packages, 4,600, we're able to build this,", "tokens": [1779, 586, 11, 337, 257, 7226, 1359, 1230, 295, 17401, 11, 1017, 11, 15707, 11, 321, 434, 1075, 281, 1322, 341, 11], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 313, "seek": 113064, "start": 1154.1200000000001, "end": 1157.6000000000001, "text": " these massive rebuilds on PRs.", "tokens": [613, 5994, 16877, 82, 322, 11568, 82, 13], "temperature": 0.0, "avg_logprob": -0.1444198671451285, "compression_ratio": 1.6925925925925926, "no_speech_prob": 4.637304300558753e-06}, {"id": 314, "seek": 115760, "start": 1157.6, "end": 1161.1999999999998, "text": " But we need the stage to see how to scale it out further, so that's what we're looking", "tokens": [583, 321, 643, 264, 3233, 281, 536, 577, 281, 4373, 309, 484, 3052, 11, 370, 300, 311, 437, 321, 434, 1237], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 315, "seek": 115760, "start": 1161.1999999999998, "end": 1162.1999999999998, "text": " at now.", "tokens": [412, 586, 13], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 316, "seek": 115760, "start": 1162.1999999999998, "end": 1167.8799999999999, "text": " We might build only the package or only the package and direct dependence on PRs and maybe", "tokens": [492, 1062, 1322, 787, 264, 7372, 420, 787, 264, 7372, 293, 2047, 31704, 322, 11568, 82, 293, 1310], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 317, "seek": 115760, "start": 1167.8799999999999, "end": 1171.3999999999999, "text": " phase how much work we do on the develop builds as well.", "tokens": [5574, 577, 709, 589, 321, 360, 322, 264, 1499, 15182, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 318, "seek": 115760, "start": 1171.3999999999999, "end": 1175.24, "text": " But we do need to do a full build every once in a while so that there's a consistent state", "tokens": [583, 321, 360, 643, 281, 360, 257, 1577, 1322, 633, 1564, 294, 257, 1339, 370, 300, 456, 311, 257, 8398, 1785], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 319, "seek": 115760, "start": 1175.24, "end": 1176.24, "text": " in the build cache.", "tokens": [294, 264, 1322, 19459, 13], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 320, "seek": 115760, "start": 1176.24, "end": 1177.24, "text": " So that's where we're at.", "tokens": [407, 300, 311, 689, 321, 434, 412, 13], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 321, "seek": 115760, "start": 1177.24, "end": 1178.24, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.1445975697368657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 1.6173007679753937e-05}, {"id": 322, "seek": 117824, "start": 1178.24, "end": 1200.88, "text": " Thank you very much for the presentation.", "tokens": [1044, 291, 588, 709, 337, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.23222587505976358, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.003621679963544011}, {"id": 323, "seek": 120088, "start": 1200.88, "end": 1211.48, "text": " You mentioned quite a bit of other technologies, like Nix, Gwix, Dab, RPM.", "tokens": [509, 2835, 1596, 257, 857, 295, 661, 7943, 11, 411, 426, 970, 11, 460, 86, 970, 11, 413, 455, 11, 497, 18819, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 324, "seek": 120088, "start": 1211.48, "end": 1215.48, "text": " You could have mentioned Ombru as well, or maybe you did.", "tokens": [509, 727, 362, 2835, 422, 2504, 894, 382, 731, 11, 420, 1310, 291, 630, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 325, "seek": 120088, "start": 1215.48, "end": 1216.48, "text": " And Docker.", "tokens": [400, 33772, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 326, "seek": 120088, "start": 1216.48, "end": 1219.4, "text": " And it feels like all these tools could help you.", "tokens": [400, 309, 3417, 411, 439, 613, 3873, 727, 854, 291, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 327, "seek": 120088, "start": 1219.4, "end": 1220.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 328, "seek": 120088, "start": 1220.4, "end": 1223.8000000000002, "text": " And it feels like you are building everything on your own.", "tokens": [400, 309, 3417, 411, 291, 366, 2390, 1203, 322, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 329, "seek": 120088, "start": 1223.8000000000002, "end": 1229.1200000000001, "text": " So is there a reason not to leverage any of these technologies?", "tokens": [407, 307, 456, 257, 1778, 406, 281, 13982, 604, 295, 613, 7943, 30], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 330, "seek": 120088, "start": 1229.1200000000001, "end": 1230.1200000000001, "text": " Which technologies do you mean?", "tokens": [3013, 7943, 360, 291, 914, 30], "temperature": 0.0, "avg_logprob": -0.25964931376929423, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.002070835093036294}, {"id": 331, "seek": 123012, "start": 1230.12, "end": 1231.12, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 332, "seek": 123012, "start": 1231.12, "end": 1232.12, "text": " So we are leveraging a lot of technologies, right?", "tokens": [407, 321, 366, 32666, 257, 688, 295, 7943, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 333, "seek": 123012, "start": 1232.12, "end": 1233.56, "text": " I guess which ones do you think we should?", "tokens": [286, 2041, 597, 2306, 360, 291, 519, 321, 820, 30], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 334, "seek": 123012, "start": 1233.56, "end": 1234.9599999999998, "text": " Nix, for example.", "tokens": [426, 970, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 335, "seek": 123012, "start": 1234.9599999999998, "end": 1237.12, "text": " So we don't.", "tokens": [407, 321, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 336, "seek": 123012, "start": 1237.12, "end": 1241.3999999999999, "text": " So Nix has essentially one version of everything in the mainline, right?", "tokens": [407, 426, 970, 575, 4476, 472, 3037, 295, 1203, 294, 264, 2135, 1889, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 337, "seek": 123012, "start": 1241.3999999999999, "end": 1246.56, "text": " And in the HPC environment, what we want you to be able to do is not build that one thing", "tokens": [400, 294, 264, 12557, 34, 2823, 11, 437, 321, 528, 291, 281, 312, 1075, 281, 360, 307, 406, 1322, 300, 472, 551], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 338, "seek": 123012, "start": 1246.56, "end": 1250.08, "text": " that's in the mainline, but to be able to build a one-off very easily.", "tokens": [300, 311, 294, 264, 2135, 1889, 11, 457, 281, 312, 1075, 281, 1322, 257, 472, 12, 4506, 588, 3612, 13], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 339, "seek": 123012, "start": 1250.08, "end": 1254.3999999999999, "text": " So the whole point of SPAC is think of it as Nix with a solver, right?", "tokens": [407, 264, 1379, 935, 295, 8420, 4378, 307, 519, 295, 309, 382, 426, 970, 365, 257, 1404, 331, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 340, "seek": 123012, "start": 1254.3999999999999, "end": 1257.84, "text": " It's Nix where you can say, actually, no, build this version of this thing with this", "tokens": [467, 311, 426, 970, 689, 291, 393, 584, 11, 767, 11, 572, 11, 1322, 341, 3037, 295, 341, 551, 365, 341], "temperature": 0.0, "avg_logprob": -0.18639447138859674, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.0004320917068980634}, {"id": 341, "seek": 125784, "start": 1257.84, "end": 1262.36, "text": " build option for that GPU, and it will take the recipe and reuse it for that purpose.", "tokens": [1322, 3614, 337, 300, 18407, 11, 293, 309, 486, 747, 264, 6782, 293, 26225, 309, 337, 300, 4334, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 342, "seek": 125784, "start": 1262.36, "end": 1265.32, "text": " Whereas in Nix, it's much harder to have package variants like that.", "tokens": [13813, 294, 426, 970, 11, 309, 311, 709, 6081, 281, 362, 7372, 21669, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 343, "seek": 125784, "start": 1265.32, "end": 1267.9199999999998, "text": " So that's really the power of SPAC.", "tokens": [407, 300, 311, 534, 264, 1347, 295, 8420, 4378, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 344, "seek": 125784, "start": 1267.9199999999998, "end": 1270.12, "text": " And so we're combinatorial Nix.", "tokens": [400, 370, 321, 434, 2512, 31927, 831, 426, 970, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 345, "seek": 125784, "start": 1270.12, "end": 1271.6799999999998, "text": " You can think of it that way.", "tokens": [509, 393, 519, 295, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 346, "seek": 125784, "start": 1271.6799999999998, "end": 1277.32, "text": " Well, wouldn't you be able to leverage Nix and describe all these differences instead", "tokens": [1042, 11, 2759, 380, 291, 312, 1075, 281, 13982, 426, 970, 293, 6786, 439, 613, 7300, 2602], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 347, "seek": 125784, "start": 1277.32, "end": 1279.32, "text": " of redoing it?", "tokens": [295, 29956, 278, 309, 30], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 348, "seek": 125784, "start": 1279.32, "end": 1280.32, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.21535787167756454, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.00019275544036645442}, {"id": 349, "seek": 128032, "start": 1280.32, "end": 1288.32, "text": " The Nix packages don't do that.", "tokens": [50364, 440, 426, 970, 17401, 500, 380, 360, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.4009689490000407, "compression_ratio": 0.8378378378378378, "no_speech_prob": 0.00010248226317344233}], "language": "en"}