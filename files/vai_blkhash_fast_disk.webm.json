{"text": " Thank you. So welcome everyone. We are going to talk about fast disk images checksums. So who am I? I'm a long time contributor to this of the project, and I worked for Red Hat more than nine years on over storage. Who knows, obviously. And I focused on incremental backup, image transfer, and NBD tools. And this project is continuous, the work on fast disk checksum in Ovid that is available for Ovid. So we're going to talk about why do we need disk image checksums, and why we cannot use the standard tools for disk images. And we'll introduce the blockchain command, which is optimized for disk images. And the block cache library, which is used by the blockchain command. And you can also use it in your program. And a good example of this usage is the new checksum command in QMMA image, which is work in progress using the block cache library. Then we see a live demo. We'll play with the tools. And finally, the most important stuff, you can contribute to this project. So what is the issue with disk images? Why the additional than standard images? Let's say we want to upload a disk to, we have this QGAP2 image, usually it's compressed because it's the best format to publish images. And we want to upload it to Ovid, or maybe to OKD. Maybe Simon wants to upload it to OKD, so what we have to do is get the data from the disk image, and we need to upload it to some server on the virtualization system, and this server we write in with some disk. Now the disk can be many things. It can be just NFS server with, and we'll have there look a file similar to the file we uploaded, but not the same. It can be raw sparse image, and it will not be compressed while we have QGAP2 compressed image. Or it can be a small block device if it's Ovid, it can be a small block device just large enough to fit the guest data that we uploaded on some escasi server, or it can be SAS image stored in many nodes, in a cluster, in many disks. So we have very different shapes on the two sides, like disk image on one side, something completely different on the other side, different format, different way of storage. One thing must be the same. The bits must be the same. The guest data must be the same. So if we start the guest with the disk image, or with the virtual disk, we must get the same bits. So we can verify this operation by creating a checksum of the disk image, the guest data inside the disk image, and the guest data inside the virtual disk, whatever the format and shape, and the checksum must be the same. The logic image is mostly the same problem. We have some shape of disk on one side, some shape of disk on the other side, different formats, but the guest data must be the same, and the checksum must be the same. Another interesting problem is incremental backup. In this case, the backup system will only copy, only change blocks on each backup if it wants to be efficient. So let's say two days ago we did a full backup, and we stored it on this full QR2, and this is just one way that we can store the backups. It can be many other things. And yesterday we did another backup, and we stored it in one file, which is sitting on top of the full QR2, and this is the backing file of this file. So we created a chain, and today we did another backup. We copied more data from the virtual disk. We created another layer. So also in this case, the guest data inside the virtual disk must be the same as the guest data inside this entire chain. So if we know how to read the guest data from the entire chain, like a guest does, we can create a checksum, and we can compare it to the checksum of the virtual disk at the time of the backup, and we know if our backup is correct. So if we will restore this chain to another virtual disk, we will get the same data. So what is the issue with the standard tools? Can we use SHA-SUM to create a checksum of this chain? So first we have the issue of image format. Standard tools do not understand image format. So if we have the raw image, everything is fine. But if we have QCAP2 image, which is identical, and here I compare the images, QM image compare, which access the guest data and compare it bit by bit, so the images are identical, but we get different checksum from SHA-SUM. Image compression, even with the same format, both images are QCAP, but one of the compressed will get different checksum, because the host clusters are compressed, and SHA-SUM is looking at the host data, not at the guest data. Even if we have the same image format without compression, everything is the same, right? I just converted one image to the other image, and the images are identical, but we get different checksum. Why? I use the dash w flag, and this allows an order rights. So the order of the cluster on the host can be different. The guest data is the same. Finally, the issue of sparseness. Standard tools do not understand sparseness, so here we have six gigabyte image, but only little more than one gigabyte of data. But SHA-SUM must read the entire image, so it will read this one gigabyte of data, complete a hash from this one gigabyte, and then read almost five gigabyte of zeros, because anything which is not allocated is read the zeros. So it must do a lot of work, which is pretty slow. For example, if we take a bigger image, here I created 100 gigabyte image, but there is no data in this image. It's completely empty, just a big role, and completing your checksum took more than three minutes. So do we really want to use this for one terabyte image? It's not the best tool for this job. And let's introduce the blockchain command, which is optimized for this case. So first it looks just like the standard tools. If you know to use standard tools, you know to use it. Just run it and you get the checksum. It understands your image format, so if you use identical image, you will get the same checksum. Although the images are different. The size is different. They do not look the same on the host. Of course, it supports compressed QCaul, because it reads the QCaul image, which the compressor data, and it gets the actual guest data, so we get the same checksum. And it also supports snapshot. So if I create a snapshot, here I created a snapshot, this snapshot QCaul, on top of the Fedora35 image. Fedora35 is the backing file of your snapshot. And if I compute a checksum of the snapshot, I actually compute a snapshot of the guest data inside the entire chain, not of the tiny snapshot file, which has no data yet. And we also support NVD URL. For example, if I start NVD server, this QMNVD is NVD server. Here I started it, exposing this QCaul2 file using this NVD URL. And if you compute a checksum with the URL, we access QMNVD. I will get the guest data and compute a checksum. And actually, this is the way we always access images. Under the hood, we always run QMNVD and use NVD URL internally to access images. This is the reason it works. We also support reading for pipe, like the standard tools, but in this case, we cannot support any format, just raw format. And this is less efficient, because we must read the entire image. In other cases, we can read only the data. But it works. So it's not enough to create tools to get correct snapshot checksums. We want it to be much faster than a standard tool, because we are dealing with huge images, which are usually empty. Usually, when you start to use an image, it's completely empty. Then you install operating system, add some files. Everything starts really empty, and then goes. So here we tested this 6-gigabyte image with about 1 gigabit of data. And in this case, Blocksum was about 16 times faster. And another example, can we compute a checksum for 8 terabyte image? Is it practical? It is. It took only 2.5 or 2.6 seconds. And if we do it with checksum, it's not practical to actually do it. So I tested 100 gigabyte image. It took about 200 seconds. So the estimated time is 4 hours and 29 minutes. It means, in this case, we have 6,000 sign faster. And of course, we create a different checksum. It's probably obvious, but any tool exists on checksum because they use different algorithms. So Blocksum is using, under the hood, some cryptographic hash function, but it's a different construction. So we don't get the same checksum. Now, it's not available everywhere, anywhere. But I build it in copper. So if you have Fedora or CentOS or L-System, you can enable the copper repo, and then you can install the package and play with the tool. So the block hash library. Basically, Blocksum is just using the library to compute the hash. So you can also use the library to integrate this functionality in your program. The library is very simple. This is the entire API. It gives you the standard interface to create a new hash, to update it, and to get the result, the final interface, and of course, to free the resources used. So if you use any cryptographic hash libraries, maybe Hashlib or OpenSSL, you know these interfaces. Now the important difference is that when you update, when you give it a buffer with data, this API will detect zeros in the data. And if you find the zero block, we will hash it much, much faster. So this increases the throughput by one order of magnitude or something like this. Even if you read from the file, you give it a buffer with zeros, we can hash it much faster. But the most important API is the zero API, because new API, that no other library supports. So if you know that a range in a file is not allocated, let's say empty 8TB image, you check the file, you see that you have an 8TB hole. So you can just input this range to the library, and it will hash it really, really quickly, much, much faster than any other way. And you don't have to read anything for this. So how fast it is? For data, we can process about two gigabytes of data. If you give it a buffer with zeros, we can process almost 50 gigabytes per second. And the BLCache zero API can process almost three terabytes per second. And this is on this laptop. If you try on a Peler one, and this is the first one, like from two years ago, it's almost three times faster for data, and almost five times faster for zero, up to 13 gigabytes per second. And I didn't try the newer M1s or M2s. So if you want to use this library, you install the developer package, you install the headers, and the library package, the Libs package, and your application will depend on the Libs package. And everything should be automatic using RPM. Now, the most important step is integrating this into your image, because this is the natural tool to consume this functionality. So I have patches in review for adding this new command. It's pretty simple. You give it the fine name. You have bogus. You can control caching, and you can enforce the image format. And with this, you can compute a checksum of anything that your image can access. You get the same checksum. It uses the block hash library using the same configuration. So both tools are compatible. You can check my QMFork if you want to see the details. So what is the difference if you compare to a block sum? Usually, it runs almost the same. A little faster, maybe five percent faster. In some cases, it can be 45 percent faster, like in this special case, the image full of zeros. I think it, because QMFork image is closer to the data, it does not have to copy the data to QMFBD and then read it over the socket. So this is really the best place to use this technology. So let's see a small live demo. So we have several images. Let's try to look at the third of 35 images. So we have a 60-gigabit image, a little more than 1 gigabit of data, and we have this Q2 image, again similar size, and we can compare them. And of course they are identical, and we can create checksum with bilkash, bilkasum, and we'll get the same checksum pretty quickly. So let's try a bigger image, and this time we'll use the progress flag to make it more fun. This is a 60-gigabit file with 24 gigabit of data, so it will take some time to compute it. You can see that the progress jumps really quickly when we find a big hole. So it took 12 seconds, and we will not try to use Chassum now. And let's say the 8-terabyte image, which is really fast, and let's say the same is QMFBD with the new checksum command. Okay, takes more time to type than to compute it. Okay, so this is all for the demo, and the last part is how you can contribute to the project. So the easiest stuff is just install it, and play with it, and test it, and if you find an issue, report it. If you have interesting machines, benchmark it, and send the results to the project. We have some results in the readme now, probably should be in a better place. To run the benchmark, you need to build it on your system. It should be easy enough. The most important stuff is packaging and hardware. Packaging for Fedora and Centro Centrail is mostly done. I just need to get this into Fedora. There is some review process that I probably need some help on this. Other Linux resources, if you want to package it for Debian or whatever arch, please do, and contribute to the project. I like to keep all packaging upstream, so I will be very happy to add whatever you need to make it transparent. So it will not break when I change something upstream. Mac OS and 3BSD, I tested it on this platform without LibNBD, because we don't have LibNBD there, and we can also cannot port it before we port LibNBD and package it. But we can package only the library part, which will be useful for QML. Missing features, there are a lot of stuff that can be added. You can look at the site, we have an issue tracker, a lot of issues, a lot of stuff that we can add, like supporting any image format, because we can't support only QCAP 2.0, checksum, multiple image, use the file to verify checksum using what we recorded before, and stuff like this, improve the CI. And even more important, but of course much more work integrates into your system. So Ovid already supports checksum API using older implementation, Ovid's REST project uses this API to verify criminal backups. It can be upgraded to the new tool. Ovid can use it. For example, when you import an image to using CDI importer, or mainly for storage operation, Ovid can verify operation with this tool. For example, running a port connected to the disk and reading the image and verifying it. Maybe other systems, I don't know, if you like other systems and think that it can be useful there, you can contact me to discuss it. If I want to see the project links, we have the project in GitLab, also the issue tracker in the project, and the copy that I showed before. So that's all. How much time? How much time do we have? Five minutes per question. Okay. Yes. I have a question about the special case with block hash, blocks, I'm sorry, zero. Yes. How do you handle it under the hood? I see you specify the chat with the six, but how's that done? Okay. Okay. So how do we handle zeros? How do we do it efficiently? So I have a bonus section at the end of the slides that you can get from the site, and basically the idea is that we split the image to blocks using fixed size. The last block can be smaller, but it doesn't matter. And then we compute the hash for each block, but for zero blocks, we don't need to compute anything because we know that, like, we compute basically when you start operation, we compute the hash of the zero block based on the configuration. And then each time we see a zero block, we can use the pre-computed hash, so it's cost nothing. And then we compute a hash from the list of hashes. This is called a hash list. It's not really new. And now this costs something. You need to pay something for computing the hashes, but they are much, much smaller, like one of those magnitude smaller than the actual data. And to make it even faster, we also use multiple threads. So this, what I, the previous slide show actually what's done in each worker. So we map the blocks, when you write something, we split the blocks and we map them to multiple workers at the same time and send them to the worker queue and the others go and compute this combined hash, this block hash. And finally, we create a hash from the worker hashes. Yes. So, oh, how hard is it for you to add a new checksum algorithm instead of Shah? How hard is it to use? So how, can we use another checksum algorithm? Yes, in Blocksum, you have the parameter, you can specify another algorithm, you can use Shah1 or MD5 or whatever, but anything that OpenSSL supports, it also supports. I'm not sure if this is the best option, maybe we limit it because Shah1 is considered broken. But currently, this is what we support. Anything that OpenSSL provides. Yes. How do you identify zero blocks? What? How do you identify zero blocks? We, how do we identify zero blocks? So we use the very popular method that somebody ought to block about it, someone from the kernel a few years ago, you can use MenComp to, with an offset to compare the file against itself. So you check the first 16 bytes and then you can just check the two pointers, the start of the image and the start of the image plus 16. And it, it can process up to 50 gigabytes per second on this machine. Very efficient. Yes. Okay. I did get the question. Yes. Did we try on cryptographic algorithm? We didn't try, because we try to use something secure, like we try to get something which is secure as the original hash function, but we can try other algorithms. It's interesting stuff that we can try. Thank you. We're going to use the mic.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.52, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.49796072642008465, "compression_ratio": 1.075, "no_speech_prob": 0.3071903586387634}, {"id": 1, "seek": 0, "start": 16.52, "end": 17.52, "text": " So welcome everyone.", "tokens": [407, 2928, 1518, 13], "temperature": 0.0, "avg_logprob": -0.49796072642008465, "compression_ratio": 1.075, "no_speech_prob": 0.3071903586387634}, {"id": 2, "seek": 0, "start": 17.52, "end": 22.72, "text": " We are going to talk about fast disk images checksums.", "tokens": [492, 366, 516, 281, 751, 466, 2370, 12355, 5267, 13834, 8099, 13], "temperature": 0.0, "avg_logprob": -0.49796072642008465, "compression_ratio": 1.075, "no_speech_prob": 0.3071903586387634}, {"id": 3, "seek": 2272, "start": 22.72, "end": 30.72, "text": " So who am I? I'm a long time contributor to this of the project, and I worked for Red Hat", "tokens": [407, 567, 669, 286, 30, 286, 478, 257, 938, 565, 42859, 281, 341, 295, 264, 1716, 11, 293, 286, 2732, 337, 4477, 15867], "temperature": 0.0, "avg_logprob": -0.3412693871392144, "compression_ratio": 1.4433962264150944, "no_speech_prob": 0.0001104214316001162}, {"id": 4, "seek": 2272, "start": 30.72, "end": 39.28, "text": " more than nine years on over storage. Who knows, obviously.", "tokens": [544, 813, 4949, 924, 322, 670, 6725, 13, 2102, 3255, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.3412693871392144, "compression_ratio": 1.4433962264150944, "no_speech_prob": 0.0001104214316001162}, {"id": 5, "seek": 2272, "start": 39.28, "end": 44.36, "text": " And I focused on incremental backup, image transfer, and NBD tools.", "tokens": [400, 286, 5178, 322, 35759, 14807, 11, 3256, 5003, 11, 293, 426, 33, 35, 3873, 13], "temperature": 0.0, "avg_logprob": -0.3412693871392144, "compression_ratio": 1.4433962264150944, "no_speech_prob": 0.0001104214316001162}, {"id": 6, "seek": 2272, "start": 44.36, "end": 50.32, "text": " And this project is continuous, the work on fast disk checksum in Ovid that is available", "tokens": [400, 341, 1716, 307, 10957, 11, 264, 589, 322, 2370, 12355, 13834, 449, 294, 422, 6833, 300, 307, 2435], "temperature": 0.0, "avg_logprob": -0.3412693871392144, "compression_ratio": 1.4433962264150944, "no_speech_prob": 0.0001104214316001162}, {"id": 7, "seek": 5032, "start": 50.32, "end": 57.68, "text": " for Ovid. So we're going to talk about why do we need disk image checksums, and why we", "tokens": [337, 422, 6833, 13, 407, 321, 434, 516, 281, 751, 466, 983, 360, 321, 643, 12355, 3256, 13834, 8099, 11, 293, 983, 321], "temperature": 0.0, "avg_logprob": -0.1794082272437311, "compression_ratio": 1.7116279069767442, "no_speech_prob": 3.0341623642016202e-05}, {"id": 8, "seek": 5032, "start": 57.68, "end": 61.44, "text": " cannot use the standard tools for disk images.", "tokens": [2644, 764, 264, 3832, 3873, 337, 12355, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1794082272437311, "compression_ratio": 1.7116279069767442, "no_speech_prob": 3.0341623642016202e-05}, {"id": 9, "seek": 5032, "start": 61.44, "end": 66.76, "text": " And we'll introduce the blockchain command, which is optimized for disk images.", "tokens": [400, 321, 603, 5366, 264, 17176, 5622, 11, 597, 307, 26941, 337, 12355, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1794082272437311, "compression_ratio": 1.7116279069767442, "no_speech_prob": 3.0341623642016202e-05}, {"id": 10, "seek": 5032, "start": 66.76, "end": 71.32, "text": " And the block cache library, which is used by the blockchain command.", "tokens": [400, 264, 3461, 19459, 6405, 11, 597, 307, 1143, 538, 264, 17176, 5622, 13], "temperature": 0.0, "avg_logprob": -0.1794082272437311, "compression_ratio": 1.7116279069767442, "no_speech_prob": 3.0341623642016202e-05}, {"id": 11, "seek": 5032, "start": 71.32, "end": 77.96000000000001, "text": " And you can also use it in your program. And a good example of this usage is the new", "tokens": [400, 291, 393, 611, 764, 309, 294, 428, 1461, 13, 400, 257, 665, 1365, 295, 341, 14924, 307, 264, 777], "temperature": 0.0, "avg_logprob": -0.1794082272437311, "compression_ratio": 1.7116279069767442, "no_speech_prob": 3.0341623642016202e-05}, {"id": 12, "seek": 7796, "start": 77.96, "end": 83.91999999999999, "text": " checksum command in QMMA image, which is work in progress using the block cache library.", "tokens": [13834, 449, 5622, 294, 1249, 44, 9998, 3256, 11, 597, 307, 589, 294, 4205, 1228, 264, 3461, 19459, 6405, 13], "temperature": 0.0, "avg_logprob": -0.27242318710478225, "compression_ratio": 1.5, "no_speech_prob": 5.794257958768867e-05}, {"id": 13, "seek": 7796, "start": 83.91999999999999, "end": 88.24, "text": " Then we see a live demo. We'll play with the tools.", "tokens": [1396, 321, 536, 257, 1621, 10723, 13, 492, 603, 862, 365, 264, 3873, 13], "temperature": 0.0, "avg_logprob": -0.27242318710478225, "compression_ratio": 1.5, "no_speech_prob": 5.794257958768867e-05}, {"id": 14, "seek": 7796, "start": 88.24, "end": 93.88, "text": " And finally, the most important stuff, you can contribute to this project.", "tokens": [400, 2721, 11, 264, 881, 1021, 1507, 11, 291, 393, 10586, 281, 341, 1716, 13], "temperature": 0.0, "avg_logprob": -0.27242318710478225, "compression_ratio": 1.5, "no_speech_prob": 5.794257958768867e-05}, {"id": 15, "seek": 7796, "start": 93.88, "end": 100.91999999999999, "text": " So what is the issue with disk images? Why the additional than standard images?", "tokens": [407, 437, 307, 264, 2734, 365, 12355, 5267, 30, 1545, 264, 4497, 813, 3832, 5267, 30], "temperature": 0.0, "avg_logprob": -0.27242318710478225, "compression_ratio": 1.5, "no_speech_prob": 5.794257958768867e-05}, {"id": 16, "seek": 7796, "start": 100.91999999999999, "end": 107.36, "text": " Let's say we want to upload a disk to, we have this QGAP2 image, usually it's compressed", "tokens": [961, 311, 584, 321, 528, 281, 6580, 257, 12355, 281, 11, 321, 362, 341, 1249, 38, 4715, 17, 3256, 11, 2673, 309, 311, 30353], "temperature": 0.0, "avg_logprob": -0.27242318710478225, "compression_ratio": 1.5, "no_speech_prob": 5.794257958768867e-05}, {"id": 17, "seek": 10736, "start": 107.36, "end": 111.56, "text": " because it's the best format to publish images.", "tokens": [570, 309, 311, 264, 1151, 7877, 281, 11374, 5267, 13], "temperature": 0.0, "avg_logprob": -0.2890574137369792, "compression_ratio": 1.6490384615384615, "no_speech_prob": 7.498108607251197e-05}, {"id": 18, "seek": 10736, "start": 111.56, "end": 115.24, "text": " And we want to upload it to Ovid, or maybe to OKD.", "tokens": [400, 321, 528, 281, 6580, 309, 281, 422, 6833, 11, 420, 1310, 281, 2264, 35, 13], "temperature": 0.0, "avg_logprob": -0.2890574137369792, "compression_ratio": 1.6490384615384615, "no_speech_prob": 7.498108607251197e-05}, {"id": 19, "seek": 10736, "start": 115.24, "end": 121.84, "text": " Maybe Simon wants to upload it to OKD, so what we have to do is get the data from the", "tokens": [2704, 13193, 2738, 281, 6580, 309, 281, 2264, 35, 11, 370, 437, 321, 362, 281, 360, 307, 483, 264, 1412, 490, 264], "temperature": 0.0, "avg_logprob": -0.2890574137369792, "compression_ratio": 1.6490384615384615, "no_speech_prob": 7.498108607251197e-05}, {"id": 20, "seek": 10736, "start": 121.84, "end": 127.96000000000001, "text": " disk image, and we need to upload it to some server on the virtualization system, and this", "tokens": [12355, 3256, 11, 293, 321, 643, 281, 6580, 309, 281, 512, 7154, 322, 264, 6374, 2144, 1185, 11, 293, 341], "temperature": 0.0, "avg_logprob": -0.2890574137369792, "compression_ratio": 1.6490384615384615, "no_speech_prob": 7.498108607251197e-05}, {"id": 21, "seek": 10736, "start": 127.96000000000001, "end": 134.64, "text": " server we write in with some disk. Now the disk can be many things.", "tokens": [7154, 321, 2464, 294, 365, 512, 12355, 13, 823, 264, 12355, 393, 312, 867, 721, 13], "temperature": 0.0, "avg_logprob": -0.2890574137369792, "compression_ratio": 1.6490384615384615, "no_speech_prob": 7.498108607251197e-05}, {"id": 22, "seek": 13464, "start": 134.64, "end": 142.2, "text": " It can be just NFS server with, and we'll have there look a file similar to the file", "tokens": [467, 393, 312, 445, 13576, 50, 7154, 365, 11, 293, 321, 603, 362, 456, 574, 257, 3991, 2531, 281, 264, 3991], "temperature": 0.0, "avg_logprob": -0.2515070715615916, "compression_ratio": 1.627027027027027, "no_speech_prob": 5.804064858239144e-05}, {"id": 23, "seek": 13464, "start": 142.2, "end": 149.64, "text": " we uploaded, but not the same. It can be raw sparse image, and it will not be compressed", "tokens": [321, 17135, 11, 457, 406, 264, 912, 13, 467, 393, 312, 8936, 637, 11668, 3256, 11, 293, 309, 486, 406, 312, 30353], "temperature": 0.0, "avg_logprob": -0.2515070715615916, "compression_ratio": 1.627027027027027, "no_speech_prob": 5.804064858239144e-05}, {"id": 24, "seek": 13464, "start": 149.64, "end": 152.64, "text": " while we have QGAP2 compressed image.", "tokens": [1339, 321, 362, 1249, 38, 4715, 17, 30353, 3256, 13], "temperature": 0.0, "avg_logprob": -0.2515070715615916, "compression_ratio": 1.627027027027027, "no_speech_prob": 5.804064858239144e-05}, {"id": 25, "seek": 13464, "start": 152.64, "end": 159.67999999999998, "text": " Or it can be a small block device if it's Ovid, it can be a small block device just large", "tokens": [1610, 309, 393, 312, 257, 1359, 3461, 4302, 498, 309, 311, 422, 6833, 11, 309, 393, 312, 257, 1359, 3461, 4302, 445, 2416], "temperature": 0.0, "avg_logprob": -0.2515070715615916, "compression_ratio": 1.627027027027027, "no_speech_prob": 5.804064858239144e-05}, {"id": 26, "seek": 15968, "start": 159.68, "end": 171.76000000000002, "text": " enough to fit the guest data that we uploaded on some escasi server, or it can be SAS image", "tokens": [1547, 281, 3318, 264, 8341, 1412, 300, 321, 17135, 322, 512, 4721, 8483, 7154, 11, 420, 309, 393, 312, 33441, 3256], "temperature": 0.0, "avg_logprob": -0.29519143651743407, "compression_ratio": 1.5064935064935066, "no_speech_prob": 5.310906544764293e-06}, {"id": 27, "seek": 15968, "start": 171.76000000000002, "end": 178.64000000000001, "text": " stored in many nodes, in a cluster, in many disks.", "tokens": [12187, 294, 867, 13891, 11, 294, 257, 13630, 11, 294, 867, 41617, 13], "temperature": 0.0, "avg_logprob": -0.29519143651743407, "compression_ratio": 1.5064935064935066, "no_speech_prob": 5.310906544764293e-06}, {"id": 28, "seek": 15968, "start": 178.64000000000001, "end": 184.24, "text": " So we have very different shapes on the two sides, like disk image on one side, something", "tokens": [407, 321, 362, 588, 819, 10854, 322, 264, 732, 4881, 11, 411, 12355, 3256, 322, 472, 1252, 11, 746], "temperature": 0.0, "avg_logprob": -0.29519143651743407, "compression_ratio": 1.5064935064935066, "no_speech_prob": 5.310906544764293e-06}, {"id": 29, "seek": 18424, "start": 184.24, "end": 191.48000000000002, "text": " completely different on the other side, different format, different way of storage.", "tokens": [2584, 819, 322, 264, 661, 1252, 11, 819, 7877, 11, 819, 636, 295, 6725, 13], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 30, "seek": 18424, "start": 191.48000000000002, "end": 195.88, "text": " One thing must be the same. The bits must be the same. The guest data must be the same.", "tokens": [1485, 551, 1633, 312, 264, 912, 13, 440, 9239, 1633, 312, 264, 912, 13, 440, 8341, 1412, 1633, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 31, "seek": 18424, "start": 195.88, "end": 200.32000000000002, "text": " So if we start the guest with the disk image, or with the virtual disk, we must get the", "tokens": [407, 498, 321, 722, 264, 8341, 365, 264, 12355, 3256, 11, 420, 365, 264, 6374, 12355, 11, 321, 1633, 483, 264], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 32, "seek": 18424, "start": 200.32000000000002, "end": 201.96, "text": " same bits.", "tokens": [912, 9239, 13], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 33, "seek": 18424, "start": 201.96, "end": 208.28, "text": " So we can verify this operation by creating a checksum of the disk image, the guest data", "tokens": [407, 321, 393, 16888, 341, 6916, 538, 4084, 257, 13834, 449, 295, 264, 12355, 3256, 11, 264, 8341, 1412], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 34, "seek": 18424, "start": 208.28, "end": 213.68, "text": " inside the disk image, and the guest data inside the virtual disk, whatever the format", "tokens": [1854, 264, 12355, 3256, 11, 293, 264, 8341, 1412, 1854, 264, 6374, 12355, 11, 2035, 264, 7877], "temperature": 0.0, "avg_logprob": -0.16171111716880454, "compression_ratio": 2.1339712918660285, "no_speech_prob": 3.4644504921743646e-05}, {"id": 35, "seek": 21368, "start": 213.68, "end": 219.08, "text": " and shape, and the checksum must be the same.", "tokens": [293, 3909, 11, 293, 264, 13834, 449, 1633, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.18419155207547275, "compression_ratio": 1.8210526315789475, "no_speech_prob": 6.240428774617612e-05}, {"id": 36, "seek": 21368, "start": 219.08, "end": 226.28, "text": " The logic image is mostly the same problem. We have some shape of disk on one side, some", "tokens": [440, 9952, 3256, 307, 5240, 264, 912, 1154, 13, 492, 362, 512, 3909, 295, 12355, 322, 472, 1252, 11, 512], "temperature": 0.0, "avg_logprob": -0.18419155207547275, "compression_ratio": 1.8210526315789475, "no_speech_prob": 6.240428774617612e-05}, {"id": 37, "seek": 21368, "start": 226.28, "end": 231.52, "text": " shape of disk on the other side, different formats, but the guest data must be the same,", "tokens": [3909, 295, 12355, 322, 264, 661, 1252, 11, 819, 25879, 11, 457, 264, 8341, 1412, 1633, 312, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.18419155207547275, "compression_ratio": 1.8210526315789475, "no_speech_prob": 6.240428774617612e-05}, {"id": 38, "seek": 21368, "start": 231.52, "end": 235.52, "text": " and the checksum must be the same.", "tokens": [293, 264, 13834, 449, 1633, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.18419155207547275, "compression_ratio": 1.8210526315789475, "no_speech_prob": 6.240428774617612e-05}, {"id": 39, "seek": 21368, "start": 235.52, "end": 240.0, "text": " Another interesting problem is incremental backup. In this case, the backup system will", "tokens": [3996, 1880, 1154, 307, 35759, 14807, 13, 682, 341, 1389, 11, 264, 14807, 1185, 486], "temperature": 0.0, "avg_logprob": -0.18419155207547275, "compression_ratio": 1.8210526315789475, "no_speech_prob": 6.240428774617612e-05}, {"id": 40, "seek": 24000, "start": 240.0, "end": 246.04, "text": " only copy, only change blocks on each backup if it wants to be efficient.", "tokens": [787, 5055, 11, 787, 1319, 8474, 322, 1184, 14807, 498, 309, 2738, 281, 312, 7148, 13], "temperature": 0.0, "avg_logprob": -0.185492664302161, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00015031018119771034}, {"id": 41, "seek": 24000, "start": 246.04, "end": 252.68, "text": " So let's say two days ago we did a full backup, and we stored it on this full QR2, and this", "tokens": [407, 718, 311, 584, 732, 1708, 2057, 321, 630, 257, 1577, 14807, 11, 293, 321, 12187, 309, 322, 341, 1577, 1249, 49, 17, 11, 293, 341], "temperature": 0.0, "avg_logprob": -0.185492664302161, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00015031018119771034}, {"id": 42, "seek": 24000, "start": 252.68, "end": 257.64, "text": " is just one way that we can store the backups. It can be many other things.", "tokens": [307, 445, 472, 636, 300, 321, 393, 3531, 264, 50160, 13, 467, 393, 312, 867, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.185492664302161, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00015031018119771034}, {"id": 43, "seek": 24000, "start": 257.64, "end": 263.0, "text": " And yesterday we did another backup, and we stored it in one file, which is sitting on", "tokens": [400, 5186, 321, 630, 1071, 14807, 11, 293, 321, 12187, 309, 294, 472, 3991, 11, 597, 307, 3798, 322], "temperature": 0.0, "avg_logprob": -0.185492664302161, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00015031018119771034}, {"id": 44, "seek": 24000, "start": 263.0, "end": 268.32, "text": " top of the full QR2, and this is the backing file of this file.", "tokens": [1192, 295, 264, 1577, 1249, 49, 17, 11, 293, 341, 307, 264, 19373, 3991, 295, 341, 3991, 13], "temperature": 0.0, "avg_logprob": -0.185492664302161, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00015031018119771034}, {"id": 45, "seek": 26832, "start": 268.32, "end": 272.64, "text": " So we created a chain, and today we did another backup. We copied more data from the virtual", "tokens": [407, 321, 2942, 257, 5021, 11, 293, 965, 321, 630, 1071, 14807, 13, 492, 25365, 544, 1412, 490, 264, 6374], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 46, "seek": 26832, "start": 272.64, "end": 276.52, "text": " disk. We created another layer.", "tokens": [12355, 13, 492, 2942, 1071, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 47, "seek": 26832, "start": 276.52, "end": 281.96, "text": " So also in this case, the guest data inside the virtual disk must be the same as the guest", "tokens": [407, 611, 294, 341, 1389, 11, 264, 8341, 1412, 1854, 264, 6374, 12355, 1633, 312, 264, 912, 382, 264, 8341], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 48, "seek": 26832, "start": 281.96, "end": 286.96, "text": " data inside this entire chain. So if we know how to read the guest data from the entire", "tokens": [1412, 1854, 341, 2302, 5021, 13, 407, 498, 321, 458, 577, 281, 1401, 264, 8341, 1412, 490, 264, 2302], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 49, "seek": 26832, "start": 286.96, "end": 292.2, "text": " chain, like a guest does, we can create a checksum, and we can compare it to the checksum", "tokens": [5021, 11, 411, 257, 8341, 775, 11, 321, 393, 1884, 257, 13834, 449, 11, 293, 321, 393, 6794, 309, 281, 264, 13834, 449], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 50, "seek": 26832, "start": 292.2, "end": 296.92, "text": " of the virtual disk at the time of the backup, and we know if our backup is correct.", "tokens": [295, 264, 6374, 12355, 412, 264, 565, 295, 264, 14807, 11, 293, 321, 458, 498, 527, 14807, 307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.1527274061993855, "compression_ratio": 2.069264069264069, "no_speech_prob": 5.0866965466411784e-05}, {"id": 51, "seek": 29692, "start": 296.92, "end": 304.92, "text": " So if we will restore this chain to another virtual disk, we will get the same data.", "tokens": [407, 498, 321, 486, 15227, 341, 5021, 281, 1071, 6374, 12355, 11, 321, 486, 483, 264, 912, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19624633018416587, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.0354960977565497e-05}, {"id": 52, "seek": 29692, "start": 304.92, "end": 311.0, "text": " So what is the issue with the standard tools? Can we use SHA-SUM to create a checksum of", "tokens": [407, 437, 307, 264, 2734, 365, 264, 3832, 3873, 30, 1664, 321, 764, 38820, 12, 50, 14340, 281, 1884, 257, 13834, 449, 295], "temperature": 0.0, "avg_logprob": -0.19624633018416587, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.0354960977565497e-05}, {"id": 53, "seek": 29692, "start": 311.0, "end": 314.76, "text": " this chain?", "tokens": [341, 5021, 30], "temperature": 0.0, "avg_logprob": -0.19624633018416587, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.0354960977565497e-05}, {"id": 54, "seek": 29692, "start": 314.76, "end": 318.72, "text": " So first we have the issue of image format. Standard tools do not understand image format.", "tokens": [407, 700, 321, 362, 264, 2734, 295, 3256, 7877, 13, 21298, 3873, 360, 406, 1223, 3256, 7877, 13], "temperature": 0.0, "avg_logprob": -0.19624633018416587, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.0354960977565497e-05}, {"id": 55, "seek": 29692, "start": 318.72, "end": 324.76, "text": " So if we have the raw image, everything is fine. But if we have QCAP2 image, which is", "tokens": [407, 498, 321, 362, 264, 8936, 3256, 11, 1203, 307, 2489, 13, 583, 498, 321, 362, 1249, 34, 4715, 17, 3256, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.19624633018416587, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.0354960977565497e-05}, {"id": 56, "seek": 32476, "start": 324.76, "end": 332.15999999999997, "text": " identical, and here I compare the images, QM image compare, which access the guest data", "tokens": [14800, 11, 293, 510, 286, 6794, 264, 5267, 11, 1249, 44, 3256, 6794, 11, 597, 2105, 264, 8341, 1412], "temperature": 0.0, "avg_logprob": -0.22344160615728142, "compression_ratio": 1.7365853658536585, "no_speech_prob": 4.647840614779852e-05}, {"id": 57, "seek": 32476, "start": 332.15999999999997, "end": 338.28, "text": " and compare it bit by bit, so the images are identical, but we get different checksum from", "tokens": [293, 6794, 309, 857, 538, 857, 11, 370, 264, 5267, 366, 14800, 11, 457, 321, 483, 819, 13834, 449, 490], "temperature": 0.0, "avg_logprob": -0.22344160615728142, "compression_ratio": 1.7365853658536585, "no_speech_prob": 4.647840614779852e-05}, {"id": 58, "seek": 32476, "start": 338.28, "end": 346.4, "text": " SHA-SUM. Image compression, even with the same format, both images are QCAP, but one", "tokens": [38820, 12, 50, 14340, 13, 29903, 19355, 11, 754, 365, 264, 912, 7877, 11, 1293, 5267, 366, 1249, 34, 4715, 11, 457, 472], "temperature": 0.0, "avg_logprob": -0.22344160615728142, "compression_ratio": 1.7365853658536585, "no_speech_prob": 4.647840614779852e-05}, {"id": 59, "seek": 32476, "start": 346.4, "end": 353.84, "text": " of the compressed will get different checksum, because the host clusters are compressed, and", "tokens": [295, 264, 30353, 486, 483, 819, 13834, 449, 11, 570, 264, 3975, 23313, 366, 30353, 11, 293], "temperature": 0.0, "avg_logprob": -0.22344160615728142, "compression_ratio": 1.7365853658536585, "no_speech_prob": 4.647840614779852e-05}, {"id": 60, "seek": 35384, "start": 353.84, "end": 359.91999999999996, "text": " SHA-SUM is looking at the host data, not at the guest data. Even if we have the same image", "tokens": [38820, 12, 50, 14340, 307, 1237, 412, 264, 3975, 1412, 11, 406, 412, 264, 8341, 1412, 13, 2754, 498, 321, 362, 264, 912, 3256], "temperature": 0.0, "avg_logprob": -0.18982721411663553, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.1471303878352046e-05}, {"id": 61, "seek": 35384, "start": 359.91999999999996, "end": 365.4, "text": " format without compression, everything is the same, right? I just converted one image", "tokens": [7877, 1553, 19355, 11, 1203, 307, 264, 912, 11, 558, 30, 286, 445, 16424, 472, 3256], "temperature": 0.0, "avg_logprob": -0.18982721411663553, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.1471303878352046e-05}, {"id": 62, "seek": 35384, "start": 365.4, "end": 373.35999999999996, "text": " to the other image, and the images are identical, but we get different checksum. Why? I use", "tokens": [281, 264, 661, 3256, 11, 293, 264, 5267, 366, 14800, 11, 457, 321, 483, 819, 13834, 449, 13, 1545, 30, 286, 764], "temperature": 0.0, "avg_logprob": -0.18982721411663553, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.1471303878352046e-05}, {"id": 63, "seek": 35384, "start": 373.35999999999996, "end": 379.91999999999996, "text": " the dash w flag, and this allows an order rights. So the order of the cluster on the", "tokens": [264, 8240, 261, 7166, 11, 293, 341, 4045, 364, 1668, 4601, 13, 407, 264, 1668, 295, 264, 13630, 322, 264], "temperature": 0.0, "avg_logprob": -0.18982721411663553, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.1471303878352046e-05}, {"id": 64, "seek": 37992, "start": 379.92, "end": 388.2, "text": " host can be different. The guest data is the same. Finally, the issue of sparseness. Standard", "tokens": [3975, 393, 312, 819, 13, 440, 8341, 1412, 307, 264, 912, 13, 6288, 11, 264, 2734, 295, 637, 685, 15264, 13, 21298], "temperature": 0.0, "avg_logprob": -0.15115922626696135, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.690015420143027e-06}, {"id": 65, "seek": 37992, "start": 388.2, "end": 394.64000000000004, "text": " tools do not understand sparseness, so here we have six gigabyte image, but only little", "tokens": [3873, 360, 406, 1223, 637, 685, 15264, 11, 370, 510, 321, 362, 2309, 8741, 34529, 3256, 11, 457, 787, 707], "temperature": 0.0, "avg_logprob": -0.15115922626696135, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.690015420143027e-06}, {"id": 66, "seek": 37992, "start": 394.64000000000004, "end": 402.40000000000003, "text": " more than one gigabyte of data. But SHA-SUM must read the entire image, so it will read", "tokens": [544, 813, 472, 8741, 34529, 295, 1412, 13, 583, 38820, 12, 50, 14340, 1633, 1401, 264, 2302, 3256, 11, 370, 309, 486, 1401], "temperature": 0.0, "avg_logprob": -0.15115922626696135, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.690015420143027e-06}, {"id": 67, "seek": 37992, "start": 402.40000000000003, "end": 407.76, "text": " this one gigabyte of data, complete a hash from this one gigabyte, and then read almost", "tokens": [341, 472, 8741, 34529, 295, 1412, 11, 3566, 257, 22019, 490, 341, 472, 8741, 34529, 11, 293, 550, 1401, 1920], "temperature": 0.0, "avg_logprob": -0.15115922626696135, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.690015420143027e-06}, {"id": 68, "seek": 40776, "start": 407.76, "end": 413.88, "text": " five gigabyte of zeros, because anything which is not allocated is read the zeros. So it", "tokens": [1732, 8741, 34529, 295, 35193, 11, 570, 1340, 597, 307, 406, 29772, 307, 1401, 264, 35193, 13, 407, 309], "temperature": 0.0, "avg_logprob": -0.20271159756568172, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.0001093772953026928}, {"id": 69, "seek": 40776, "start": 413.88, "end": 420.52, "text": " must do a lot of work, which is pretty slow. For example, if we take a bigger image, here", "tokens": [1633, 360, 257, 688, 295, 589, 11, 597, 307, 1238, 2964, 13, 1171, 1365, 11, 498, 321, 747, 257, 3801, 3256, 11, 510], "temperature": 0.0, "avg_logprob": -0.20271159756568172, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.0001093772953026928}, {"id": 70, "seek": 40776, "start": 420.52, "end": 427.48, "text": " I created 100 gigabyte image, but there is no data in this image. It's completely empty,", "tokens": [286, 2942, 2319, 8741, 34529, 3256, 11, 457, 456, 307, 572, 1412, 294, 341, 3256, 13, 467, 311, 2584, 6707, 11], "temperature": 0.0, "avg_logprob": -0.20271159756568172, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.0001093772953026928}, {"id": 71, "seek": 40776, "start": 427.48, "end": 434.76, "text": " just a big role, and completing your checksum took more than three minutes. So do we really", "tokens": [445, 257, 955, 3090, 11, 293, 19472, 428, 13834, 449, 1890, 544, 813, 1045, 2077, 13, 407, 360, 321, 534], "temperature": 0.0, "avg_logprob": -0.20271159756568172, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.0001093772953026928}, {"id": 72, "seek": 43476, "start": 434.76, "end": 444.52, "text": " want to use this for one terabyte image? It's not the best tool for this job. And let's", "tokens": [528, 281, 764, 341, 337, 472, 1796, 34529, 3256, 30, 467, 311, 406, 264, 1151, 2290, 337, 341, 1691, 13, 400, 718, 311], "temperature": 0.0, "avg_logprob": -0.1873482174343533, "compression_ratio": 1.6777251184834123, "no_speech_prob": 7.322876626858488e-06}, {"id": 73, "seek": 43476, "start": 444.52, "end": 450.24, "text": " introduce the blockchain command, which is optimized for this case. So first it looks", "tokens": [5366, 264, 17176, 5622, 11, 597, 307, 26941, 337, 341, 1389, 13, 407, 700, 309, 1542], "temperature": 0.0, "avg_logprob": -0.1873482174343533, "compression_ratio": 1.6777251184834123, "no_speech_prob": 7.322876626858488e-06}, {"id": 74, "seek": 43476, "start": 450.24, "end": 455.64, "text": " just like the standard tools. If you know to use standard tools, you know to use it.", "tokens": [445, 411, 264, 3832, 3873, 13, 759, 291, 458, 281, 764, 3832, 3873, 11, 291, 458, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1873482174343533, "compression_ratio": 1.6777251184834123, "no_speech_prob": 7.322876626858488e-06}, {"id": 75, "seek": 43476, "start": 455.64, "end": 462.59999999999997, "text": " Just run it and you get the checksum. It understands your image format, so if you use identical", "tokens": [1449, 1190, 309, 293, 291, 483, 264, 13834, 449, 13, 467, 15146, 428, 3256, 7877, 11, 370, 498, 291, 764, 14800], "temperature": 0.0, "avg_logprob": -0.1873482174343533, "compression_ratio": 1.6777251184834123, "no_speech_prob": 7.322876626858488e-06}, {"id": 76, "seek": 46260, "start": 462.6, "end": 467.64000000000004, "text": " image, you will get the same checksum. Although the images are different. The size is different.", "tokens": [3256, 11, 291, 486, 483, 264, 912, 13834, 449, 13, 5780, 264, 5267, 366, 819, 13, 440, 2744, 307, 819, 13], "temperature": 0.0, "avg_logprob": -0.24813726759448493, "compression_ratio": 1.7333333333333334, "no_speech_prob": 8.159798744600266e-05}, {"id": 77, "seek": 46260, "start": 467.64000000000004, "end": 473.28000000000003, "text": " They do not look the same on the host. Of course, it supports compressed QCaul, because", "tokens": [814, 360, 406, 574, 264, 912, 322, 264, 3975, 13, 2720, 1164, 11, 309, 9346, 30353, 1249, 34, 64, 425, 11, 570], "temperature": 0.0, "avg_logprob": -0.24813726759448493, "compression_ratio": 1.7333333333333334, "no_speech_prob": 8.159798744600266e-05}, {"id": 78, "seek": 46260, "start": 473.28000000000003, "end": 479.12, "text": " it reads the QCaul image, which the compressor data, and it gets the actual guest data, so", "tokens": [309, 15700, 264, 1249, 34, 64, 425, 3256, 11, 597, 264, 28765, 1412, 11, 293, 309, 2170, 264, 3539, 8341, 1412, 11, 370], "temperature": 0.0, "avg_logprob": -0.24813726759448493, "compression_ratio": 1.7333333333333334, "no_speech_prob": 8.159798744600266e-05}, {"id": 79, "seek": 46260, "start": 479.12, "end": 486.68, "text": " we get the same checksum. And it also supports snapshot. So if I create a snapshot, here", "tokens": [321, 483, 264, 912, 13834, 449, 13, 400, 309, 611, 9346, 30163, 13, 407, 498, 286, 1884, 257, 30163, 11, 510], "temperature": 0.0, "avg_logprob": -0.24813726759448493, "compression_ratio": 1.7333333333333334, "no_speech_prob": 8.159798744600266e-05}, {"id": 80, "seek": 48668, "start": 486.68, "end": 493.52, "text": " I created a snapshot, this snapshot QCaul, on top of the Fedora35 image. Fedora35 is", "tokens": [286, 2942, 257, 30163, 11, 341, 30163, 1249, 34, 64, 425, 11, 322, 1192, 295, 264, 7772, 3252, 8794, 3256, 13, 7772, 3252, 8794, 307], "temperature": 0.0, "avg_logprob": -0.24514528002057756, "compression_ratio": 1.6687898089171975, "no_speech_prob": 9.287674038205296e-05}, {"id": 81, "seek": 48668, "start": 493.52, "end": 501.32, "text": " the backing file of your snapshot. And if I compute a checksum of the snapshot, I actually", "tokens": [264, 19373, 3991, 295, 428, 30163, 13, 400, 498, 286, 14722, 257, 13834, 449, 295, 264, 30163, 11, 286, 767], "temperature": 0.0, "avg_logprob": -0.24514528002057756, "compression_ratio": 1.6687898089171975, "no_speech_prob": 9.287674038205296e-05}, {"id": 82, "seek": 48668, "start": 501.32, "end": 506.48, "text": " compute a snapshot of the guest data inside the entire chain, not of the tiny snapshot", "tokens": [14722, 257, 30163, 295, 264, 8341, 1412, 1854, 264, 2302, 5021, 11, 406, 295, 264, 5870, 30163], "temperature": 0.0, "avg_logprob": -0.24514528002057756, "compression_ratio": 1.6687898089171975, "no_speech_prob": 9.287674038205296e-05}, {"id": 83, "seek": 50648, "start": 506.48, "end": 517.5600000000001, "text": " file, which has no data yet. And we also support NVD URL. For example, if I start NVD server,", "tokens": [3991, 11, 597, 575, 572, 1412, 1939, 13, 400, 321, 611, 1406, 46512, 35, 12905, 13, 1171, 1365, 11, 498, 286, 722, 46512, 35, 7154, 11], "temperature": 0.0, "avg_logprob": -0.21132579716769131, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.2965535461262334e-05}, {"id": 84, "seek": 50648, "start": 517.5600000000001, "end": 526.36, "text": " this QMNVD is NVD server. Here I started it, exposing this QCaul2 file using this NVD", "tokens": [341, 1249, 44, 45, 53, 35, 307, 46512, 35, 7154, 13, 1692, 286, 1409, 309, 11, 33178, 341, 1249, 34, 64, 425, 17, 3991, 1228, 341, 46512, 35], "temperature": 0.0, "avg_logprob": -0.21132579716769131, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.2965535461262334e-05}, {"id": 85, "seek": 50648, "start": 526.36, "end": 531.84, "text": " URL. And if you compute a checksum with the URL, we access QMNVD. I will get the guest", "tokens": [12905, 13, 400, 498, 291, 14722, 257, 13834, 449, 365, 264, 12905, 11, 321, 2105, 1249, 44, 45, 53, 35, 13, 286, 486, 483, 264, 8341], "temperature": 0.0, "avg_logprob": -0.21132579716769131, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.2965535461262334e-05}, {"id": 86, "seek": 53184, "start": 531.84, "end": 539.1600000000001, "text": " data and compute a checksum. And actually, this is the way we always access images. Under", "tokens": [1412, 293, 14722, 257, 13834, 449, 13, 400, 767, 11, 341, 307, 264, 636, 321, 1009, 2105, 5267, 13, 6974], "temperature": 0.0, "avg_logprob": -0.16402573170869247, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.0574774427805096e-06}, {"id": 87, "seek": 53184, "start": 539.1600000000001, "end": 545.1600000000001, "text": " the hood, we always run QMNVD and use NVD URL internally to access images. This is the", "tokens": [264, 13376, 11, 321, 1009, 1190, 1249, 44, 45, 53, 35, 293, 764, 46512, 35, 12905, 19501, 281, 2105, 5267, 13, 639, 307, 264], "temperature": 0.0, "avg_logprob": -0.16402573170869247, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.0574774427805096e-06}, {"id": 88, "seek": 53184, "start": 545.1600000000001, "end": 554.0, "text": " reason it works. We also support reading for pipe, like the standard tools, but in this", "tokens": [1778, 309, 1985, 13, 492, 611, 1406, 3760, 337, 11240, 11, 411, 264, 3832, 3873, 11, 457, 294, 341], "temperature": 0.0, "avg_logprob": -0.16402573170869247, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.0574774427805096e-06}, {"id": 89, "seek": 53184, "start": 554.0, "end": 559.24, "text": " case, we cannot support any format, just raw format. And this is less efficient, because", "tokens": [1389, 11, 321, 2644, 1406, 604, 7877, 11, 445, 8936, 7877, 13, 400, 341, 307, 1570, 7148, 11, 570], "temperature": 0.0, "avg_logprob": -0.16402573170869247, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.0574774427805096e-06}, {"id": 90, "seek": 55924, "start": 559.24, "end": 570.28, "text": " we must read the entire image. In other cases, we can read only the data. But it works. So", "tokens": [321, 1633, 1401, 264, 2302, 3256, 13, 682, 661, 3331, 11, 321, 393, 1401, 787, 264, 1412, 13, 583, 309, 1985, 13, 407], "temperature": 0.0, "avg_logprob": -0.20140522128933078, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.626737012993544e-05}, {"id": 91, "seek": 55924, "start": 570.28, "end": 577.4, "text": " it's not enough to create tools to get correct snapshot checksums. We want it to be much", "tokens": [309, 311, 406, 1547, 281, 1884, 3873, 281, 483, 3006, 30163, 13834, 8099, 13, 492, 528, 309, 281, 312, 709], "temperature": 0.0, "avg_logprob": -0.20140522128933078, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.626737012993544e-05}, {"id": 92, "seek": 55924, "start": 577.4, "end": 581.5600000000001, "text": " faster than a standard tool, because we are dealing with huge images, which are usually", "tokens": [4663, 813, 257, 3832, 2290, 11, 570, 321, 366, 6260, 365, 2603, 5267, 11, 597, 366, 2673], "temperature": 0.0, "avg_logprob": -0.20140522128933078, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.626737012993544e-05}, {"id": 93, "seek": 55924, "start": 581.5600000000001, "end": 587.6800000000001, "text": " empty. Usually, when you start to use an image, it's completely empty. Then you install operating", "tokens": [6707, 13, 11419, 11, 562, 291, 722, 281, 764, 364, 3256, 11, 309, 311, 2584, 6707, 13, 1396, 291, 3625, 7447], "temperature": 0.0, "avg_logprob": -0.20140522128933078, "compression_ratio": 1.6150442477876106, "no_speech_prob": 6.626737012993544e-05}, {"id": 94, "seek": 58768, "start": 587.68, "end": 595.52, "text": " system, add some files. Everything starts really empty, and then goes. So here we tested", "tokens": [1185, 11, 909, 512, 7098, 13, 5471, 3719, 534, 6707, 11, 293, 550, 1709, 13, 407, 510, 321, 8246], "temperature": 0.0, "avg_logprob": -0.248771850376913, "compression_ratio": 1.4371584699453552, "no_speech_prob": 1.219590922119096e-05}, {"id": 95, "seek": 58768, "start": 595.52, "end": 603.9599999999999, "text": " this 6-gigabyte image with about 1 gigabit of data. And in this case, Blocksum was about", "tokens": [341, 1386, 12, 70, 328, 34529, 3256, 365, 466, 502, 8741, 455, 270, 295, 1412, 13, 400, 294, 341, 1389, 11, 9865, 2761, 449, 390, 466], "temperature": 0.0, "avg_logprob": -0.248771850376913, "compression_ratio": 1.4371584699453552, "no_speech_prob": 1.219590922119096e-05}, {"id": 96, "seek": 58768, "start": 603.9599999999999, "end": 612.88, "text": " 16 times faster. And another example, can we compute a checksum for 8 terabyte image?", "tokens": [3165, 1413, 4663, 13, 400, 1071, 1365, 11, 393, 321, 14722, 257, 13834, 449, 337, 1649, 1796, 34529, 3256, 30], "temperature": 0.0, "avg_logprob": -0.248771850376913, "compression_ratio": 1.4371584699453552, "no_speech_prob": 1.219590922119096e-05}, {"id": 97, "seek": 61288, "start": 612.88, "end": 623.72, "text": " Is it practical? It is. It took only 2.5 or 2.6 seconds. And if we do it with checksum,", "tokens": [1119, 309, 8496, 30, 467, 307, 13, 467, 1890, 787, 568, 13, 20, 420, 568, 13, 21, 3949, 13, 400, 498, 321, 360, 309, 365, 13834, 449, 11], "temperature": 0.0, "avg_logprob": -0.17871249027741262, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.00016677168605383486}, {"id": 98, "seek": 61288, "start": 623.72, "end": 630.4399999999999, "text": " it's not practical to actually do it. So I tested 100 gigabyte image. It took about 200", "tokens": [309, 311, 406, 8496, 281, 767, 360, 309, 13, 407, 286, 8246, 2319, 8741, 34529, 3256, 13, 467, 1890, 466, 2331], "temperature": 0.0, "avg_logprob": -0.17871249027741262, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.00016677168605383486}, {"id": 99, "seek": 61288, "start": 630.4399999999999, "end": 636.56, "text": " seconds. So the estimated time is 4 hours and 29 minutes. It means, in this case, we", "tokens": [3949, 13, 407, 264, 14109, 565, 307, 1017, 2496, 293, 9413, 2077, 13, 467, 1355, 11, 294, 341, 1389, 11, 321], "temperature": 0.0, "avg_logprob": -0.17871249027741262, "compression_ratio": 1.452513966480447, "no_speech_prob": 0.00016677168605383486}, {"id": 100, "seek": 63656, "start": 636.56, "end": 645.04, "text": " have 6,000 sign faster. And of course, we create a different checksum. It's probably", "tokens": [362, 1386, 11, 1360, 1465, 4663, 13, 400, 295, 1164, 11, 321, 1884, 257, 819, 13834, 449, 13, 467, 311, 1391], "temperature": 0.0, "avg_logprob": -0.2126942101646872, "compression_ratio": 1.4808743169398908, "no_speech_prob": 7.511323929065838e-05}, {"id": 101, "seek": 63656, "start": 645.04, "end": 652.88, "text": " obvious, but any tool exists on checksum because they use different algorithms. So Blocksum", "tokens": [6322, 11, 457, 604, 2290, 8198, 322, 13834, 449, 570, 436, 764, 819, 14642, 13, 407, 9865, 2761, 449], "temperature": 0.0, "avg_logprob": -0.2126942101646872, "compression_ratio": 1.4808743169398908, "no_speech_prob": 7.511323929065838e-05}, {"id": 102, "seek": 63656, "start": 652.88, "end": 659.8399999999999, "text": " is using, under the hood, some cryptographic hash function, but it's a different construction.", "tokens": [307, 1228, 11, 833, 264, 13376, 11, 512, 9844, 12295, 22019, 2445, 11, 457, 309, 311, 257, 819, 6435, 13], "temperature": 0.0, "avg_logprob": -0.2126942101646872, "compression_ratio": 1.4808743169398908, "no_speech_prob": 7.511323929065838e-05}, {"id": 103, "seek": 65984, "start": 659.84, "end": 670.2800000000001, "text": " So we don't get the same checksum. Now, it's not available everywhere, anywhere. But I", "tokens": [407, 321, 500, 380, 483, 264, 912, 13834, 449, 13, 823, 11, 309, 311, 406, 2435, 5315, 11, 4992, 13, 583, 286], "temperature": 0.0, "avg_logprob": -0.2359073999765757, "compression_ratio": 1.4316939890710383, "no_speech_prob": 2.332624717382714e-05}, {"id": 104, "seek": 65984, "start": 670.2800000000001, "end": 676.96, "text": " build it in copper. So if you have Fedora or CentOS or L-System, you can enable the copper", "tokens": [1322, 309, 294, 15007, 13, 407, 498, 291, 362, 7772, 3252, 420, 3408, 4367, 420, 441, 12, 50, 9321, 11, 291, 393, 9528, 264, 15007], "temperature": 0.0, "avg_logprob": -0.2359073999765757, "compression_ratio": 1.4316939890710383, "no_speech_prob": 2.332624717382714e-05}, {"id": 105, "seek": 65984, "start": 676.96, "end": 684.8000000000001, "text": " repo, and then you can install the package and play with the tool. So the block hash", "tokens": [49040, 11, 293, 550, 291, 393, 3625, 264, 7372, 293, 862, 365, 264, 2290, 13, 407, 264, 3461, 22019], "temperature": 0.0, "avg_logprob": -0.2359073999765757, "compression_ratio": 1.4316939890710383, "no_speech_prob": 2.332624717382714e-05}, {"id": 106, "seek": 68480, "start": 684.8, "end": 691.8, "text": " library. Basically, Blocksum is just using the library to compute the hash. So you can", "tokens": [6405, 13, 8537, 11, 9865, 2761, 449, 307, 445, 1228, 264, 6405, 281, 14722, 264, 22019, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.1386149830288357, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.0384512279415503e-05}, {"id": 107, "seek": 68480, "start": 691.8, "end": 698.24, "text": " also use the library to integrate this functionality in your program. The library is very simple.", "tokens": [611, 764, 264, 6405, 281, 13365, 341, 14980, 294, 428, 1461, 13, 440, 6405, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1386149830288357, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.0384512279415503e-05}, {"id": 108, "seek": 68480, "start": 698.24, "end": 706.28, "text": " This is the entire API. It gives you the standard interface to create a new hash, to update it,", "tokens": [639, 307, 264, 2302, 9362, 13, 467, 2709, 291, 264, 3832, 9226, 281, 1884, 257, 777, 22019, 11, 281, 5623, 309, 11], "temperature": 0.0, "avg_logprob": -0.1386149830288357, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.0384512279415503e-05}, {"id": 109, "seek": 68480, "start": 706.28, "end": 713.4, "text": " and to get the result, the final interface, and of course, to free the resources used.", "tokens": [293, 281, 483, 264, 1874, 11, 264, 2572, 9226, 11, 293, 295, 1164, 11, 281, 1737, 264, 3593, 1143, 13], "temperature": 0.0, "avg_logprob": -0.1386149830288357, "compression_ratio": 1.660633484162896, "no_speech_prob": 1.0384512279415503e-05}, {"id": 110, "seek": 71340, "start": 713.4, "end": 723.12, "text": " So if you use any cryptographic hash libraries, maybe Hashlib or OpenSSL, you know these interfaces.", "tokens": [407, 498, 291, 764, 604, 9844, 12295, 22019, 15148, 11, 1310, 30775, 38270, 420, 7238, 21929, 43, 11, 291, 458, 613, 28416, 13], "temperature": 0.0, "avg_logprob": -0.18065165110996792, "compression_ratio": 1.4680851063829787, "no_speech_prob": 5.0704147724900395e-06}, {"id": 111, "seek": 71340, "start": 723.12, "end": 729.9599999999999, "text": " Now the important difference is that when you update, when you give it a buffer with", "tokens": [823, 264, 1021, 2649, 307, 300, 562, 291, 5623, 11, 562, 291, 976, 309, 257, 21762, 365], "temperature": 0.0, "avg_logprob": -0.18065165110996792, "compression_ratio": 1.4680851063829787, "no_speech_prob": 5.0704147724900395e-06}, {"id": 112, "seek": 71340, "start": 729.9599999999999, "end": 736.64, "text": " data, this API will detect zeros in the data. And if you find the zero block, we will hash", "tokens": [1412, 11, 341, 9362, 486, 5531, 35193, 294, 264, 1412, 13, 400, 498, 291, 915, 264, 4018, 3461, 11, 321, 486, 22019], "temperature": 0.0, "avg_logprob": -0.18065165110996792, "compression_ratio": 1.4680851063829787, "no_speech_prob": 5.0704147724900395e-06}, {"id": 113, "seek": 73664, "start": 736.64, "end": 747.28, "text": " it much, much faster. So this increases the throughput by one order of magnitude or something", "tokens": [309, 709, 11, 709, 4663, 13, 407, 341, 8637, 264, 44629, 538, 472, 1668, 295, 15668, 420, 746], "temperature": 0.0, "avg_logprob": -0.2300389654496137, "compression_ratio": 1.4777777777777779, "no_speech_prob": 1.206095748784719e-05}, {"id": 114, "seek": 73664, "start": 747.28, "end": 755.68, "text": " like this. Even if you read from the file, you give it a buffer with zeros, we can hash", "tokens": [411, 341, 13, 2754, 498, 291, 1401, 490, 264, 3991, 11, 291, 976, 309, 257, 21762, 365, 35193, 11, 321, 393, 22019], "temperature": 0.0, "avg_logprob": -0.2300389654496137, "compression_ratio": 1.4777777777777779, "no_speech_prob": 1.206095748784719e-05}, {"id": 115, "seek": 73664, "start": 755.68, "end": 761.48, "text": " it much faster. But the most important API is the zero API, because new API, that no", "tokens": [309, 709, 4663, 13, 583, 264, 881, 1021, 9362, 307, 264, 4018, 9362, 11, 570, 777, 9362, 11, 300, 572], "temperature": 0.0, "avg_logprob": -0.2300389654496137, "compression_ratio": 1.4777777777777779, "no_speech_prob": 1.206095748784719e-05}, {"id": 116, "seek": 76148, "start": 761.48, "end": 767.2, "text": " other library supports. So if you know that a range in a file is not allocated, let's", "tokens": [661, 6405, 9346, 13, 407, 498, 291, 458, 300, 257, 3613, 294, 257, 3991, 307, 406, 29772, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.21021888182335294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.189898853226623e-06}, {"id": 117, "seek": 76148, "start": 767.2, "end": 775.24, "text": " say empty 8TB image, you check the file, you see that you have an 8TB hole. So you can", "tokens": [584, 6707, 1649, 51, 33, 3256, 11, 291, 1520, 264, 3991, 11, 291, 536, 300, 291, 362, 364, 1649, 51, 33, 5458, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.21021888182335294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.189898853226623e-06}, {"id": 118, "seek": 76148, "start": 775.24, "end": 782.76, "text": " just input this range to the library, and it will hash it really, really quickly, much,", "tokens": [445, 4846, 341, 3613, 281, 264, 6405, 11, 293, 309, 486, 22019, 309, 534, 11, 534, 2661, 11, 709, 11], "temperature": 0.0, "avg_logprob": -0.21021888182335294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.189898853226623e-06}, {"id": 119, "seek": 76148, "start": 782.76, "end": 789.32, "text": " much faster than any other way. And you don't have to read anything for this. So how fast", "tokens": [709, 4663, 813, 604, 661, 636, 13, 400, 291, 500, 380, 362, 281, 1401, 1340, 337, 341, 13, 407, 577, 2370], "temperature": 0.0, "avg_logprob": -0.21021888182335294, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.189898853226623e-06}, {"id": 120, "seek": 78932, "start": 789.32, "end": 799.0400000000001, "text": " it is? For data, we can process about two gigabytes of data. If you give it a buffer", "tokens": [309, 307, 30, 1171, 1412, 11, 321, 393, 1399, 466, 732, 42741, 295, 1412, 13, 759, 291, 976, 309, 257, 21762], "temperature": 0.0, "avg_logprob": -0.23512803691707246, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.9889262047363445e-06}, {"id": 121, "seek": 78932, "start": 799.0400000000001, "end": 806.6400000000001, "text": " with zeros, we can process almost 50 gigabytes per second. And the BLCache zero API can process", "tokens": [365, 35193, 11, 321, 393, 1399, 1920, 2625, 42741, 680, 1150, 13, 400, 264, 15132, 34, 6000, 4018, 9362, 393, 1399], "temperature": 0.0, "avg_logprob": -0.23512803691707246, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.9889262047363445e-06}, {"id": 122, "seek": 78932, "start": 806.6400000000001, "end": 814.8800000000001, "text": " almost three terabytes per second. And this is on this laptop. If you try on a Peler one,", "tokens": [1920, 1045, 1796, 24538, 680, 1150, 13, 400, 341, 307, 322, 341, 10732, 13, 759, 291, 853, 322, 257, 430, 6185, 472, 11], "temperature": 0.0, "avg_logprob": -0.23512803691707246, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.9889262047363445e-06}, {"id": 123, "seek": 81488, "start": 814.88, "end": 825.04, "text": " and this is the first one, like from two years ago, it's almost three times faster for data,", "tokens": [293, 341, 307, 264, 700, 472, 11, 411, 490, 732, 924, 2057, 11, 309, 311, 1920, 1045, 1413, 4663, 337, 1412, 11], "temperature": 0.0, "avg_logprob": -0.23992459615071615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.616205377620645e-06}, {"id": 124, "seek": 81488, "start": 825.04, "end": 832.6, "text": " and almost five times faster for zero, up to 13 gigabytes per second. And I didn't try", "tokens": [293, 1920, 1732, 1413, 4663, 337, 4018, 11, 493, 281, 3705, 42741, 680, 1150, 13, 400, 286, 994, 380, 853], "temperature": 0.0, "avg_logprob": -0.23992459615071615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.616205377620645e-06}, {"id": 125, "seek": 81488, "start": 832.6, "end": 841.52, "text": " the newer M1s or M2s. So if you want to use this library, you install the developer package,", "tokens": [264, 17628, 376, 16, 82, 420, 376, 17, 82, 13, 407, 498, 291, 528, 281, 764, 341, 6405, 11, 291, 3625, 264, 10754, 7372, 11], "temperature": 0.0, "avg_logprob": -0.23992459615071615, "compression_ratio": 1.4545454545454546, "no_speech_prob": 8.616205377620645e-06}, {"id": 126, "seek": 84152, "start": 841.52, "end": 847.24, "text": " you install the headers, and the library package, the Libs package, and your application will", "tokens": [291, 3625, 264, 45101, 11, 293, 264, 6405, 7372, 11, 264, 15834, 82, 7372, 11, 293, 428, 3861, 486], "temperature": 0.0, "avg_logprob": -0.20469571698096492, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.4186348380462732e-05}, {"id": 127, "seek": 84152, "start": 847.24, "end": 857.28, "text": " depend on the Libs package. And everything should be automatic using RPM. Now, the most", "tokens": [5672, 322, 264, 15834, 82, 7372, 13, 400, 1203, 820, 312, 12509, 1228, 37389, 13, 823, 11, 264, 881], "temperature": 0.0, "avg_logprob": -0.20469571698096492, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.4186348380462732e-05}, {"id": 128, "seek": 84152, "start": 857.28, "end": 862.3199999999999, "text": " important step is integrating this into your image, because this is the natural tool to", "tokens": [1021, 1823, 307, 26889, 341, 666, 428, 3256, 11, 570, 341, 307, 264, 3303, 2290, 281], "temperature": 0.0, "avg_logprob": -0.20469571698096492, "compression_ratio": 1.5549132947976878, "no_speech_prob": 1.4186348380462732e-05}, {"id": 129, "seek": 86232, "start": 862.32, "end": 872.0400000000001, "text": " consume this functionality. So I have patches in review for adding this new command. It's", "tokens": [14732, 341, 14980, 13, 407, 286, 362, 26531, 294, 3131, 337, 5127, 341, 777, 5622, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.19422629709993855, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.2795738257409539e-05}, {"id": 130, "seek": 86232, "start": 872.0400000000001, "end": 878.24, "text": " pretty simple. You give it the fine name. You have bogus. You can control caching, and", "tokens": [1238, 2199, 13, 509, 976, 309, 264, 2489, 1315, 13, 509, 362, 26132, 301, 13, 509, 393, 1969, 269, 2834, 11, 293], "temperature": 0.0, "avg_logprob": -0.19422629709993855, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.2795738257409539e-05}, {"id": 131, "seek": 86232, "start": 878.24, "end": 883.6400000000001, "text": " you can enforce the image format. And with this, you can compute a checksum of anything", "tokens": [291, 393, 24825, 264, 3256, 7877, 13, 400, 365, 341, 11, 291, 393, 14722, 257, 13834, 449, 295, 1340], "temperature": 0.0, "avg_logprob": -0.19422629709993855, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.2795738257409539e-05}, {"id": 132, "seek": 86232, "start": 883.6400000000001, "end": 891.2800000000001, "text": " that your image can access. You get the same checksum. It uses the block hash library using", "tokens": [300, 428, 3256, 393, 2105, 13, 509, 483, 264, 912, 13834, 449, 13, 467, 4960, 264, 3461, 22019, 6405, 1228], "temperature": 0.0, "avg_logprob": -0.19422629709993855, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.2795738257409539e-05}, {"id": 133, "seek": 89128, "start": 891.28, "end": 899.24, "text": " the same configuration. So both tools are compatible. You can check my QMFork if you", "tokens": [264, 912, 11694, 13, 407, 1293, 3873, 366, 18218, 13, 509, 393, 1520, 452, 1249, 44, 37, 1284, 498, 291], "temperature": 0.0, "avg_logprob": -0.20637565340314593, "compression_ratio": 1.4224598930481283, "no_speech_prob": 3.839240434899693e-06}, {"id": 134, "seek": 89128, "start": 899.24, "end": 907.1999999999999, "text": " want to see the details. So what is the difference if you compare to a block sum? Usually, it", "tokens": [528, 281, 536, 264, 4365, 13, 407, 437, 307, 264, 2649, 498, 291, 6794, 281, 257, 3461, 2408, 30, 11419, 11, 309], "temperature": 0.0, "avg_logprob": -0.20637565340314593, "compression_ratio": 1.4224598930481283, "no_speech_prob": 3.839240434899693e-06}, {"id": 135, "seek": 89128, "start": 907.1999999999999, "end": 915.52, "text": " runs almost the same. A little faster, maybe five percent faster. In some cases, it can", "tokens": [6676, 1920, 264, 912, 13, 316, 707, 4663, 11, 1310, 1732, 3043, 4663, 13, 682, 512, 3331, 11, 309, 393], "temperature": 0.0, "avg_logprob": -0.20637565340314593, "compression_ratio": 1.4224598930481283, "no_speech_prob": 3.839240434899693e-06}, {"id": 136, "seek": 91552, "start": 915.52, "end": 922.04, "text": " be 45 percent faster, like in this special case, the image full of zeros. I think it,", "tokens": [312, 6905, 3043, 4663, 11, 411, 294, 341, 2121, 1389, 11, 264, 3256, 1577, 295, 35193, 13, 286, 519, 309, 11], "temperature": 0.0, "avg_logprob": -0.1826467514038086, "compression_ratio": 1.461111111111111, "no_speech_prob": 1.946291195054073e-05}, {"id": 137, "seek": 91552, "start": 922.04, "end": 928.92, "text": " because QMFork image is closer to the data, it does not have to copy the data to QMFBD", "tokens": [570, 1249, 44, 37, 1284, 3256, 307, 4966, 281, 264, 1412, 11, 309, 775, 406, 362, 281, 5055, 264, 1412, 281, 1249, 44, 37, 33, 35], "temperature": 0.0, "avg_logprob": -0.1826467514038086, "compression_ratio": 1.461111111111111, "no_speech_prob": 1.946291195054073e-05}, {"id": 138, "seek": 91552, "start": 928.92, "end": 937.84, "text": " and then read it over the socket. So this is really the best place to use this technology.", "tokens": [293, 550, 1401, 309, 670, 264, 19741, 13, 407, 341, 307, 534, 264, 1151, 1081, 281, 764, 341, 2899, 13], "temperature": 0.0, "avg_logprob": -0.1826467514038086, "compression_ratio": 1.461111111111111, "no_speech_prob": 1.946291195054073e-05}, {"id": 139, "seek": 93784, "start": 937.84, "end": 954.6, "text": " So let's see a small live demo. So we have several images. Let's try to look at the third", "tokens": [407, 718, 311, 536, 257, 1359, 1621, 10723, 13, 407, 321, 362, 2940, 5267, 13, 961, 311, 853, 281, 574, 412, 264, 2636], "temperature": 0.0, "avg_logprob": -0.2791620537086769, "compression_ratio": 1.0987654320987654, "no_speech_prob": 5.661553132085828e-06}, {"id": 140, "seek": 95460, "start": 954.6, "end": 973.8000000000001, "text": " of 35 images. So we have a 60-gigabit image, a little more than 1 gigabit of data, and", "tokens": [295, 6976, 5267, 13, 407, 321, 362, 257, 4060, 12, 70, 328, 455, 270, 3256, 11, 257, 707, 544, 813, 502, 8741, 455, 270, 295, 1412, 11, 293], "temperature": 0.0, "avg_logprob": -0.27968427538871765, "compression_ratio": 1.075, "no_speech_prob": 1.8092488971888088e-05}, {"id": 141, "seek": 97380, "start": 973.8, "end": 1000.52, "text": " we have this Q2 image, again similar size, and we can compare them. And of course they", "tokens": [321, 362, 341, 1249, 17, 3256, 11, 797, 2531, 2744, 11, 293, 321, 393, 6794, 552, 13, 400, 295, 1164, 436], "temperature": 0.0, "avg_logprob": -0.2702832412719727, "compression_ratio": 1.036144578313253, "no_speech_prob": 0.0006049198564141989}, {"id": 142, "seek": 100052, "start": 1000.52, "end": 1017.1999999999999, "text": " are identical, and we can create checksum with bilkash, bilkasum, and we'll get the", "tokens": [366, 14800, 11, 293, 321, 393, 1884, 13834, 449, 365, 8588, 74, 1299, 11, 8588, 32876, 449, 11, 293, 321, 603, 483, 264], "temperature": 0.0, "avg_logprob": -0.2563320159912109, "compression_ratio": 1.4049586776859504, "no_speech_prob": 2.4517848942195997e-05}, {"id": 143, "seek": 100052, "start": 1017.1999999999999, "end": 1026.04, "text": " same checksum pretty quickly. So let's try a bigger image, and this time we'll use the", "tokens": [912, 13834, 449, 1238, 2661, 13, 407, 718, 311, 853, 257, 3801, 3256, 11, 293, 341, 565, 321, 603, 764, 264], "temperature": 0.0, "avg_logprob": -0.2563320159912109, "compression_ratio": 1.4049586776859504, "no_speech_prob": 2.4517848942195997e-05}, {"id": 144, "seek": 102604, "start": 1026.04, "end": 1035.96, "text": " progress flag to make it more fun. This is a 60-gigabit file with 24 gigabit of data,", "tokens": [4205, 7166, 281, 652, 309, 544, 1019, 13, 639, 307, 257, 4060, 12, 70, 328, 455, 270, 3991, 365, 4022, 8741, 455, 270, 295, 1412, 11], "temperature": 0.0, "avg_logprob": -0.1779790392108992, "compression_ratio": 1.3111111111111111, "no_speech_prob": 0.00016320038412231952}, {"id": 145, "seek": 102604, "start": 1035.96, "end": 1049.2, "text": " so it will take some time to compute it. You can see that the progress jumps really quickly", "tokens": [370, 309, 486, 747, 512, 565, 281, 14722, 309, 13, 509, 393, 536, 300, 264, 4205, 16704, 534, 2661], "temperature": 0.0, "avg_logprob": -0.1779790392108992, "compression_ratio": 1.3111111111111111, "no_speech_prob": 0.00016320038412231952}, {"id": 146, "seek": 104920, "start": 1049.2, "end": 1057.8, "text": " when we find a big hole. So it took 12 seconds, and we will not try to use Chassum now. And", "tokens": [562, 321, 915, 257, 955, 5458, 13, 407, 309, 1890, 2272, 3949, 11, 293, 321, 486, 406, 853, 281, 764, 761, 640, 449, 586, 13, 400], "temperature": 0.0, "avg_logprob": -0.30044387558759267, "compression_ratio": 1.2826086956521738, "no_speech_prob": 6.034274338162504e-05}, {"id": 147, "seek": 104920, "start": 1057.8, "end": 1067.4, "text": " let's say the 8-terabyte image, which is really fast, and let's say the same is QMFBD", "tokens": [718, 311, 584, 264, 1649, 12, 391, 34529, 3256, 11, 597, 307, 534, 2370, 11, 293, 718, 311, 584, 264, 912, 307, 1249, 44, 37, 33, 35], "temperature": 0.0, "avg_logprob": -0.30044387558759267, "compression_ratio": 1.2826086956521738, "no_speech_prob": 6.034274338162504e-05}, {"id": 148, "seek": 106740, "start": 1067.4, "end": 1097.2, "text": " with the new checksum command. Okay, takes more time to type than to compute it. Okay,", "tokens": [365, 264, 777, 13834, 449, 5622, 13, 1033, 11, 2516, 544, 565, 281, 2010, 813, 281, 14722, 309, 13, 1033, 11], "temperature": 0.0, "avg_logprob": -0.1834983253479004, "compression_ratio": 1.0886075949367089, "no_speech_prob": 0.0016975905746221542}, {"id": 149, "seek": 109720, "start": 1097.2, "end": 1103.76, "text": " so this is all for the demo, and the last part is how you can contribute to the project.", "tokens": [370, 341, 307, 439, 337, 264, 10723, 11, 293, 264, 1036, 644, 307, 577, 291, 393, 10586, 281, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.18395658161329187, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.0939244576729834e-05}, {"id": 150, "seek": 109720, "start": 1103.76, "end": 1108.44, "text": " So the easiest stuff is just install it, and play with it, and test it, and if you find", "tokens": [407, 264, 12889, 1507, 307, 445, 3625, 309, 11, 293, 862, 365, 309, 11, 293, 1500, 309, 11, 293, 498, 291, 915], "temperature": 0.0, "avg_logprob": -0.18395658161329187, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.0939244576729834e-05}, {"id": 151, "seek": 109720, "start": 1108.44, "end": 1116.24, "text": " an issue, report it. If you have interesting machines, benchmark it, and send the results", "tokens": [364, 2734, 11, 2275, 309, 13, 759, 291, 362, 1880, 8379, 11, 18927, 309, 11, 293, 2845, 264, 3542], "temperature": 0.0, "avg_logprob": -0.18395658161329187, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.0939244576729834e-05}, {"id": 152, "seek": 109720, "start": 1116.24, "end": 1121.32, "text": " to the project. We have some results in the readme now, probably should be in a better", "tokens": [281, 264, 1716, 13, 492, 362, 512, 3542, 294, 264, 1401, 1398, 586, 11, 1391, 820, 312, 294, 257, 1101], "temperature": 0.0, "avg_logprob": -0.18395658161329187, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.0939244576729834e-05}, {"id": 153, "seek": 112132, "start": 1121.32, "end": 1127.6799999999998, "text": " place. To run the benchmark, you need to build it on your system. It should be easy enough.", "tokens": [1081, 13, 1407, 1190, 264, 18927, 11, 291, 643, 281, 1322, 309, 322, 428, 1185, 13, 467, 820, 312, 1858, 1547, 13], "temperature": 0.0, "avg_logprob": -0.21420707283439216, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.350038504227996e-05}, {"id": 154, "seek": 112132, "start": 1127.6799999999998, "end": 1136.6799999999998, "text": " The most important stuff is packaging and hardware. Packaging for Fedora and Centro", "tokens": [440, 881, 1021, 1507, 307, 16836, 293, 8837, 13, 18466, 3568, 337, 7772, 3252, 293, 3408, 340], "temperature": 0.0, "avg_logprob": -0.21420707283439216, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.350038504227996e-05}, {"id": 155, "seek": 112132, "start": 1136.6799999999998, "end": 1141.9199999999998, "text": " Centrail is mostly done. I just need to get this into Fedora. There is some review process", "tokens": [3408, 424, 388, 307, 5240, 1096, 13, 286, 445, 643, 281, 483, 341, 666, 7772, 3252, 13, 821, 307, 512, 3131, 1399], "temperature": 0.0, "avg_logprob": -0.21420707283439216, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.350038504227996e-05}, {"id": 156, "seek": 112132, "start": 1141.9199999999998, "end": 1149.3999999999999, "text": " that I probably need some help on this. Other Linux resources, if you want to package it for", "tokens": [300, 286, 1391, 643, 512, 854, 322, 341, 13, 5358, 18734, 3593, 11, 498, 291, 528, 281, 7372, 309, 337], "temperature": 0.0, "avg_logprob": -0.21420707283439216, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.350038504227996e-05}, {"id": 157, "seek": 114940, "start": 1149.4, "end": 1156.5600000000002, "text": " Debian or whatever arch, please do, and contribute to the project. I like to keep all packaging", "tokens": [1346, 20196, 420, 2035, 3912, 11, 1767, 360, 11, 293, 10586, 281, 264, 1716, 13, 286, 411, 281, 1066, 439, 16836], "temperature": 0.0, "avg_logprob": -0.24146117626781194, "compression_ratio": 1.4607329842931938, "no_speech_prob": 3.736020516953431e-05}, {"id": 158, "seek": 114940, "start": 1156.5600000000002, "end": 1166.52, "text": " upstream, so I will be very happy to add whatever you need to make it transparent. So it will", "tokens": [33915, 11, 370, 286, 486, 312, 588, 2055, 281, 909, 2035, 291, 643, 281, 652, 309, 12737, 13, 407, 309, 486], "temperature": 0.0, "avg_logprob": -0.24146117626781194, "compression_ratio": 1.4607329842931938, "no_speech_prob": 3.736020516953431e-05}, {"id": 159, "seek": 114940, "start": 1166.52, "end": 1174.4, "text": " not break when I change something upstream. Mac OS and 3BSD, I tested it on this platform", "tokens": [406, 1821, 562, 286, 1319, 746, 33915, 13, 5707, 12731, 293, 805, 8176, 35, 11, 286, 8246, 309, 322, 341, 3663], "temperature": 0.0, "avg_logprob": -0.24146117626781194, "compression_ratio": 1.4607329842931938, "no_speech_prob": 3.736020516953431e-05}, {"id": 160, "seek": 117440, "start": 1174.4, "end": 1182.44, "text": " without LibNBD, because we don't have LibNBD there, and we can also cannot port it before", "tokens": [1553, 15834, 45, 33, 35, 11, 570, 321, 500, 380, 362, 15834, 45, 33, 35, 456, 11, 293, 321, 393, 611, 2644, 2436, 309, 949], "temperature": 0.0, "avg_logprob": -0.2299467416910025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012666158727370203}, {"id": 161, "seek": 117440, "start": 1182.44, "end": 1191.24, "text": " we port LibNBD and package it. But we can package only the library part, which will be useful", "tokens": [321, 2436, 15834, 45, 33, 35, 293, 7372, 309, 13, 583, 321, 393, 7372, 787, 264, 6405, 644, 11, 597, 486, 312, 4420], "temperature": 0.0, "avg_logprob": -0.2299467416910025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012666158727370203}, {"id": 162, "seek": 117440, "start": 1191.24, "end": 1198.44, "text": " for QML. Missing features, there are a lot of stuff that can be added. You can look at", "tokens": [337, 1249, 12683, 13, 5275, 278, 4122, 11, 456, 366, 257, 688, 295, 1507, 300, 393, 312, 3869, 13, 509, 393, 574, 412], "temperature": 0.0, "avg_logprob": -0.2299467416910025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012666158727370203}, {"id": 163, "seek": 117440, "start": 1198.44, "end": 1203.44, "text": " the site, we have an issue tracker, a lot of issues, a lot of stuff that we can add,", "tokens": [264, 3621, 11, 321, 362, 364, 2734, 37516, 11, 257, 688, 295, 2663, 11, 257, 688, 295, 1507, 300, 321, 393, 909, 11], "temperature": 0.0, "avg_logprob": -0.2299467416910025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012666158727370203}, {"id": 164, "seek": 120344, "start": 1203.44, "end": 1210.96, "text": " like supporting any image format, because we can't support only QCAP 2.0, checksum, multiple", "tokens": [411, 7231, 604, 3256, 7877, 11, 570, 321, 393, 380, 1406, 787, 1249, 34, 4715, 568, 13, 15, 11, 13834, 449, 11, 3866], "temperature": 0.0, "avg_logprob": -0.3718773978097098, "compression_ratio": 1.518987341772152, "no_speech_prob": 5.1721694035222754e-05}, {"id": 165, "seek": 120344, "start": 1210.96, "end": 1218.8, "text": " image, use the file to verify checksum using what we recorded before, and stuff like this,", "tokens": [3256, 11, 764, 264, 3991, 281, 16888, 13834, 449, 1228, 437, 321, 8287, 949, 11, 293, 1507, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.3718773978097098, "compression_ratio": 1.518987341772152, "no_speech_prob": 5.1721694035222754e-05}, {"id": 166, "seek": 120344, "start": 1218.8, "end": 1225.3600000000001, "text": " improve the CI. And even more important, but of course much more work integrates into your", "tokens": [3470, 264, 37777, 13, 400, 754, 544, 1021, 11, 457, 295, 1164, 709, 544, 589, 3572, 1024, 666, 428], "temperature": 0.0, "avg_logprob": -0.3718773978097098, "compression_ratio": 1.518987341772152, "no_speech_prob": 5.1721694035222754e-05}, {"id": 167, "seek": 120344, "start": 1225.3600000000001, "end": 1232.96, "text": " system. So Ovid already supports checksum API using older implementation, Ovid's REST", "tokens": [1185, 13, 407, 422, 6833, 1217, 9346, 13834, 449, 9362, 1228, 4906, 11420, 11, 422, 6833, 311, 497, 14497], "temperature": 0.0, "avg_logprob": -0.3718773978097098, "compression_ratio": 1.518987341772152, "no_speech_prob": 5.1721694035222754e-05}, {"id": 168, "seek": 123296, "start": 1232.96, "end": 1242.96, "text": " project uses this API to verify criminal backups. It can be upgraded to the new tool.", "tokens": [1716, 4960, 341, 9362, 281, 16888, 8628, 50160, 13, 467, 393, 312, 24133, 281, 264, 777, 2290, 13], "temperature": 0.0, "avg_logprob": -0.2431024102603688, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0008350053685717285}, {"id": 169, "seek": 123296, "start": 1242.96, "end": 1251.8400000000001, "text": " Ovid can use it. For example, when you import an image to using CDI importer, or mainly", "tokens": [422, 6833, 393, 764, 309, 13, 1171, 1365, 11, 562, 291, 974, 364, 3256, 281, 1228, 383, 3085, 704, 6122, 11, 420, 8704], "temperature": 0.0, "avg_logprob": -0.2431024102603688, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0008350053685717285}, {"id": 170, "seek": 123296, "start": 1251.8400000000001, "end": 1261.1200000000001, "text": " for storage operation, Ovid can verify operation with this tool. For example, running a port", "tokens": [337, 6725, 6916, 11, 422, 6833, 393, 16888, 6916, 365, 341, 2290, 13, 1171, 1365, 11, 2614, 257, 2436], "temperature": 0.0, "avg_logprob": -0.2431024102603688, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0008350053685717285}, {"id": 171, "seek": 126112, "start": 1261.12, "end": 1268.2399999999998, "text": " connected to the disk and reading the image and verifying it. Maybe other systems, I don't", "tokens": [4582, 281, 264, 12355, 293, 3760, 264, 3256, 293, 1306, 5489, 309, 13, 2704, 661, 3652, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.27083304268973213, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.900855027604848e-05}, {"id": 172, "seek": 126112, "start": 1268.2399999999998, "end": 1275.2399999999998, "text": " know, if you like other systems and think that it can be useful there, you can contact", "tokens": [458, 11, 498, 291, 411, 661, 3652, 293, 519, 300, 309, 393, 312, 4420, 456, 11, 291, 393, 3385], "temperature": 0.0, "avg_logprob": -0.27083304268973213, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.900855027604848e-05}, {"id": 173, "seek": 126112, "start": 1275.2399999999998, "end": 1283.12, "text": " me to discuss it. If I want to see the project links, we have the project in GitLab, also", "tokens": [385, 281, 2248, 309, 13, 759, 286, 528, 281, 536, 264, 1716, 6123, 11, 321, 362, 264, 1716, 294, 16939, 37880, 11, 611], "temperature": 0.0, "avg_logprob": -0.27083304268973213, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.900855027604848e-05}, {"id": 174, "seek": 128312, "start": 1283.12, "end": 1293.12, "text": " the issue tracker in the project, and the copy that I showed before. So that's all.", "tokens": [264, 2734, 37516, 294, 264, 1716, 11, 293, 264, 5055, 300, 286, 4712, 949, 13, 407, 300, 311, 439, 13], "temperature": 0.0, "avg_logprob": -0.3355201482772827, "compression_ratio": 1.0375, "no_speech_prob": 8.041180262807757e-05}, {"id": 175, "seek": 129312, "start": 1293.12, "end": 1319.12, "text": " How much time? How much time do we have? Five minutes per question. Okay. Yes. I have a question about the special case with block hash, blocks, I'm sorry, zero. Yes. How do you handle it under the hood? I see you specify the chat with the six, but how's that done?", "tokens": [1012, 709, 565, 30, 1012, 709, 565, 360, 321, 362, 30, 9436, 2077, 680, 1168, 13, 1033, 13, 1079, 13, 286, 362, 257, 1168, 466, 264, 2121, 1389, 365, 3461, 22019, 11, 8474, 11, 286, 478, 2597, 11, 4018, 13, 1079, 13, 1012, 360, 291, 4813, 309, 833, 264, 13376, 30, 286, 536, 291, 16500, 264, 5081, 365, 264, 2309, 11, 457, 577, 311, 300, 1096, 30], "temperature": 0.0, "avg_logprob": -0.48100388217979756, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.014012140221893787}, {"id": 176, "seek": 131912, "start": 1319.12, "end": 1343.12, "text": " Okay. Okay. So how do we handle zeros? How do we do it efficiently? So I have a bonus section at the end of the slides that you can get from the site, and basically the idea is that we split the image to blocks using fixed size.", "tokens": [1033, 13, 1033, 13, 407, 577, 360, 321, 4813, 35193, 30, 1012, 360, 321, 360, 309, 19621, 30, 407, 286, 362, 257, 10882, 3541, 412, 264, 917, 295, 264, 9788, 300, 291, 393, 483, 490, 264, 3621, 11, 293, 1936, 264, 1558, 307, 300, 321, 7472, 264, 3256, 281, 8474, 1228, 6806, 2744, 13], "temperature": 0.0, "avg_logprob": -0.14919473384988718, "compression_ratio": 1.4430379746835442, "no_speech_prob": 6.916917482158169e-05}, {"id": 177, "seek": 134312, "start": 1343.12, "end": 1358.12, "text": " The last block can be smaller, but it doesn't matter. And then we compute the hash for each block, but for zero blocks, we don't need to compute anything because we know that, like, we compute basically when you start operation, we compute the hash of the zero block based on the", "tokens": [440, 1036, 3461, 393, 312, 4356, 11, 457, 309, 1177, 380, 1871, 13, 400, 550, 321, 14722, 264, 22019, 337, 1184, 3461, 11, 457, 337, 4018, 8474, 11, 321, 500, 380, 643, 281, 14722, 1340, 570, 321, 458, 300, 11, 411, 11, 321, 14722, 1936, 562, 291, 722, 6916, 11, 321, 14722, 264, 22019, 295, 264, 4018, 3461, 2361, 322, 264], "temperature": 0.0, "avg_logprob": -0.13650663816011868, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.00011069131141994148}, {"id": 178, "seek": 135812, "start": 1358.12, "end": 1374.12, "text": " configuration. And then each time we see a zero block, we can use the pre-computed hash, so it's cost nothing. And then we compute a hash from the list of hashes. This is called a hash list. It's not really new.", "tokens": [11694, 13, 400, 550, 1184, 565, 321, 536, 257, 4018, 3461, 11, 321, 393, 764, 264, 659, 12, 1112, 2582, 292, 22019, 11, 370, 309, 311, 2063, 1825, 13, 400, 550, 321, 14722, 257, 22019, 490, 264, 1329, 295, 575, 8076, 13, 639, 307, 1219, 257, 22019, 1329, 13, 467, 311, 406, 534, 777, 13], "temperature": 0.0, "avg_logprob": -0.1451228351916297, "compression_ratio": 1.435374149659864, "no_speech_prob": 2.5876677682390437e-05}, {"id": 179, "seek": 137412, "start": 1374.12, "end": 1390.12, "text": " And now this costs something. You need to pay something for computing the hashes, but they are much, much smaller, like one of those magnitude smaller than the actual data. And to make it even faster, we also use multiple threads.", "tokens": [400, 586, 341, 5497, 746, 13, 509, 643, 281, 1689, 746, 337, 15866, 264, 575, 8076, 11, 457, 436, 366, 709, 11, 709, 4356, 11, 411, 472, 295, 729, 15668, 4356, 813, 264, 3539, 1412, 13, 400, 281, 652, 309, 754, 4663, 11, 321, 611, 764, 3866, 19314, 13], "temperature": 0.0, "avg_logprob": -0.1325424662176168, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.067390778393019e-05}, {"id": 180, "seek": 139012, "start": 1390.12, "end": 1407.12, "text": " So this, what I, the previous slide show actually what's done in each worker. So we map the blocks, when you write something, we split the blocks and we map them to multiple workers at the same time and send them to the worker queue and the", "tokens": [407, 341, 11, 437, 286, 11, 264, 3894, 4137, 855, 767, 437, 311, 1096, 294, 1184, 11346, 13, 407, 321, 4471, 264, 8474, 11, 562, 291, 2464, 746, 11, 321, 7472, 264, 8474, 293, 321, 4471, 552, 281, 3866, 5600, 412, 264, 912, 565, 293, 2845, 552, 281, 264, 11346, 18639, 293, 264], "temperature": 0.0, "avg_logprob": -0.1893235591419956, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00043316592928022146}, {"id": 181, "seek": 140712, "start": 1407.12, "end": 1421.12, "text": " others go and compute this combined hash, this block hash. And finally, we create a hash from the worker hashes. Yes.", "tokens": [2357, 352, 293, 14722, 341, 9354, 22019, 11, 341, 3461, 22019, 13, 400, 2721, 11, 321, 1884, 257, 22019, 490, 264, 11346, 575, 8076, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.2842886217178837, "compression_ratio": 1.21875, "no_speech_prob": 0.0003350604965817183}, {"id": 182, "seek": 142112, "start": 1421.12, "end": 1438.12, "text": " So, oh, how hard is it for you to add a new checksum algorithm instead of Shah? How hard is it to use? So how, can we use another checksum algorithm? Yes, in Blocksum, you have the parameter, you can specify another", "tokens": [407, 11, 1954, 11, 577, 1152, 307, 309, 337, 291, 281, 909, 257, 777, 13834, 449, 9284, 2602, 295, 21159, 30, 1012, 1152, 307, 309, 281, 764, 30, 407, 577, 11, 393, 321, 764, 1071, 13834, 449, 9284, 30, 1079, 11, 294, 9865, 2761, 449, 11, 291, 362, 264, 13075, 11, 291, 393, 16500, 1071], "temperature": 0.0, "avg_logprob": -0.27938668202545686, "compression_ratio": 1.5140845070422535, "no_speech_prob": 0.00014224635378923267}, {"id": 183, "seek": 143812, "start": 1438.12, "end": 1454.12, "text": " algorithm, you can use Shah1 or MD5 or whatever, but anything that OpenSSL supports, it also supports. I'm not sure if this is the best option, maybe we limit it because Shah1 is considered broken.", "tokens": [9284, 11, 291, 393, 764, 21159, 16, 420, 22521, 20, 420, 2035, 11, 457, 1340, 300, 7238, 21929, 43, 9346, 11, 309, 611, 9346, 13, 286, 478, 406, 988, 498, 341, 307, 264, 1151, 3614, 11, 1310, 321, 4948, 309, 570, 21159, 16, 307, 4888, 5463, 13], "temperature": 0.0, "avg_logprob": -0.22918617893272722, "compression_ratio": 1.481081081081081, "no_speech_prob": 2.5835885026026517e-05}, {"id": 184, "seek": 143812, "start": 1454.12, "end": 1460.12, "text": " But currently, this is what we support. Anything that OpenSSL provides. Yes.", "tokens": [583, 4362, 11, 341, 307, 437, 321, 1406, 13, 11998, 300, 7238, 21929, 43, 6417, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.22918617893272722, "compression_ratio": 1.481081081081081, "no_speech_prob": 2.5835885026026517e-05}, {"id": 185, "seek": 146012, "start": 1460.12, "end": 1477.12, "text": " How do you identify zero blocks? What? How do you identify zero blocks? We, how do we identify zero blocks? So we use the very popular method that somebody ought to block about it, someone from the kernel a few", "tokens": [1012, 360, 291, 5876, 4018, 8474, 30, 708, 30, 1012, 360, 291, 5876, 4018, 8474, 30, 492, 11, 577, 360, 321, 5876, 4018, 8474, 30, 407, 321, 764, 264, 588, 3743, 3170, 300, 2618, 13416, 281, 3461, 466, 309, 11, 1580, 490, 264, 28256, 257, 1326], "temperature": 0.0, "avg_logprob": -0.21273805618286132, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.000128238505567424}, {"id": 186, "seek": 147712, "start": 1477.12, "end": 1497.12, "text": " years ago, you can use MenComp to, with an offset to compare the file against itself. So you check the first 16 bytes and then you can just check the two pointers, the start of the image and the start of the image plus 16.", "tokens": [924, 2057, 11, 291, 393, 764, 6685, 34, 8586, 281, 11, 365, 364, 18687, 281, 6794, 264, 3991, 1970, 2564, 13, 407, 291, 1520, 264, 700, 3165, 36088, 293, 550, 291, 393, 445, 1520, 264, 732, 44548, 11, 264, 722, 295, 264, 3256, 293, 264, 722, 295, 264, 3256, 1804, 3165, 13], "temperature": 0.0, "avg_logprob": -0.15350197042737687, "compression_ratio": 1.5524475524475525, "no_speech_prob": 6.798092363169417e-05}, {"id": 187, "seek": 149712, "start": 1497.12, "end": 1508.12, "text": " And it, it can process up to 50 gigabytes per second on this machine. Very efficient. Yes.", "tokens": [400, 309, 11, 309, 393, 1399, 493, 281, 2625, 42741, 680, 1150, 322, 341, 3479, 13, 4372, 7148, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.2314542007446289, "compression_ratio": 1.0465116279069768, "no_speech_prob": 0.0011280262842774391}, {"id": 188, "seek": 150812, "start": 1508.12, "end": 1536.12, "text": " Okay. I did get the question. Yes.", "tokens": [1033, 13, 286, 630, 483, 264, 1168, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.45667154448372976, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.008146158419549465}, {"id": 189, "seek": 153612, "start": 1536.12, "end": 1552.12, "text": " Did we try on cryptographic algorithm? We didn't try, because we try to use something secure, like we try to get something which is secure as the original hash function, but we can try other algorithms.", "tokens": [2589, 321, 853, 322, 9844, 12295, 9284, 30, 492, 994, 380, 853, 11, 570, 321, 853, 281, 764, 746, 7144, 11, 411, 321, 853, 281, 483, 746, 597, 307, 7144, 382, 264, 3380, 22019, 2445, 11, 457, 321, 393, 853, 661, 14642, 13], "temperature": 0.0, "avg_logprob": -0.2102793083816278, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011028132867068052}, {"id": 190, "seek": 153612, "start": 1552.12, "end": 1565.12, "text": " It's interesting stuff that we can try. Thank you.", "tokens": [467, 311, 1880, 1507, 300, 321, 393, 853, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2102793083816278, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011028132867068052}, {"id": 191, "seek": 156512, "start": 1565.12, "end": 1570.12, "text": " We're going to use the mic.", "tokens": [50364, 492, 434, 516, 281, 764, 264, 3123, 13, 50614], "temperature": 0.0, "avg_logprob": -0.5928422320972789, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.014100758358836174}], "language": "en"}