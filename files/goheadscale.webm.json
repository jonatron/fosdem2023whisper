{"text": " Next up, we have two speakers for the prize of one. They are going to talk about everything to do with an open, open, more open source version of Talescale. So let's give an applause to Christopher H. Gouan. Hello. Hello. Okay. This is cool. Hello. My name is Christopher, and I'm going to, together with H. Gouan there, talk a bit about how we use integration testing to kind of reimplement the control panel or the control server of Talescale. So first a little bit about us. Juan Fontalonso is the creator of Talescale. He works for the European Space Agency on cloud and DevOps and infrastructure. He claims to have been my first manager, but I think that's incorrect. And he has the attention span of a goldfish. Which makes the whole collaboration very fun, and I'm here with Christopher. He's a top contributor of Talescale and one of the other maintainers alongside me. He's part of the technical staff at Talescale, and part of his time at Talescale is to work improving Talescale. I was his manager, at least from a hierarchical point of view. And one of the challenges we have is that he always finds these kind of super niche languages, like OCaml or things like that, where he tries to reimplement headscaling. But first of all, how many people here know Talescale and headscale? Good. That's pretty good. So for the people who don't know, we'll do like a quick tweak what is Talescale. So Talescale tries to solve this problem, where you basically sit and you want to connect your organization or home or something like this, and you have an old school or legacy VPN concentrator, where you connect into your kind of perimeter, you have access to absolutely everything, there's a single point of failure and a massive bottleneck. And it tries to do this by creating like a mesh VPN that uses direct connections wire guard and kind of facilitates this for you using techniques like natural and has a very, very powerful client that will make sure that you always reach what you're trying to get to, and it offers a lot of different kind of granular access, and you get a lot more power compared to your old school bottleneck, single point of failure VPN. And in Talescale, the clients are open source, at least for the open platforms, and what they have is a closed SAS. But still, they are quite open when it comes to explaining how the whole thing works. And in March 2020, they publish a blog post basically explaining how the whole thing worked, how they use these natural techniques so you don't have to open the ports in your router. And there was a phrase in this blog post that gathered my attention for a little bit, and was basically talking about a coordination server, that the clients talk to a coordination server, the core of this services service offering, which is essentially a shared drop box for this wire guard public keys. So I was pass up by that, and basically I took that open source clients and started reverse engineering, basically a lot of print apps to see what kind of payload were they sending, what kind of endpoints or protocol they were doing. And yeah, this was around April 2020, in June, I had a lot of free time at that time, and in June I did the initial release. I talked to my friend Christopher about tail scale, and he was very happy distributing wire guard keys with Thansible, which, yeah, so I kept doing my own thing for a while. Head scale gained a little bit of traction, and around mid 2021, he joined because he was quite curious about the whole thing. But he was afraid about breaking stuff, and that's why kind of we are here, although he was not afraid of making a logo, but I think it's super nice. So what I've learned doing this reverse engineering exercise is that the tail scale clients talk to what is basically a web service. This web service receives metadata from the clients, like the endpoints or the wire guard public keys that they use, and assigns them IP addresses, like you would having a classic traditional VPN service. As everybody knows about everything, you can establish this mesh network across the clients without interference because the data doesn't go through the web service. So we arrived to the initial stage of head scale, the illusion that everything works and kind of worked until it stopped doing. So we had this web service, we implemented the web service, the series of endpoints that we found in the reverse engineering exercise, and we were assigning IP address to the when a node arrives, and what happens when a second node arrives? Hey, we want to tell that I'm here and I want to find my friends and I want to communicate with them. So in order to handle that, and to handle the metadata that you need to establish the connections, we developed a little bit of a state machine that will handle, and you know has arrived, there's been a change in the map of the network, and we need to distribute the updated metadata that we have. However, at that time, I was kind of learning go, and we follow a little bit of a weird approach when handling concurrency, which was basically adding more Mutex every time we needed it. And this is a problem, because at the end we ended up with a great Mutex for this state machine, and this is a very big problem because the Python track is tomorrow, so the grid logs are over there. So what ended up happening inside the state machine, or what didn't end up happening, was that basically some of the failure modes we saw was that a new node trying to register, and then we burned a couple of CPU cycles trying to calculate some stuff, and then we did nothing. So no updates were sent out or anything. Sometimes we would have a new node joining, and we would compute everything, send some network traffic, we just omitted the new information, that was kind of crucial for everyone to know, so it ended up not working. And sometimes a new node joined, nothing really happened, but then eventually something happens and it sent out an update to everyone, and that was, you know, useful. And sometimes on the individual update channels for each node, some of these aforementioned Mutexes kind of deadlocked up the whole thread or the GoRoutine, and then we just never sent updates to particular nodes, and sometimes we just deadlocked the whole server and you kind of had to kick it to make it come back to life. But still there was kind of this notion that it did work pretty well eventually most of the time, and it gave this illusion of working, because what you often saw was that you had like three nodes, and only two of them actually talked together, and as long as those had received the updates they needed, you know, the user was happy and you're just like, ah, it works, so I'm going to press the star sign on GitHub and share it with my friends. So, but we figured that eventually this would like caught up with us and we're trying to get to this stage where we, you know, it works most of the time, so what we did have was a fair amount of unit tests, but the problem with unit tests is that we're trying to reverse engineering something, that we're also learning how it works, and what we spent a lot of time on was misunderstanding how it was supposed to work, writing wrong, well, writing unit tests that would pass, but they were wrong, so you kind of have like a, a passing test and it's an entirely wrong implementation, and 90% of what we were actually trying to do was integrate with a third party software, and this is where we get to actual integration tests, so what I started doing was I found this Docker test framework, which basically allows you like programmatically create Docker containers, so we started making tests that spun up a head scale container, it created a bunch of tail scale instances also running in Docker, and associated them with a couple of users and tried to like emulate the entire environment so you can test everyone to everyone. We had them join the head scale server, and since it takes a little bit of time for everyone to catch up with each other and, you know, send the updates and stuff, so we put a sleep of two minutes in front of the test, which is a terrible idea, but, you know, you learn, and then after that sleep runs out, presumably everyone is now up to date and can talk to each other, so we had a test, the most basic test is, is my network working, can everyone ping everyone? So we tried to do that, and of course that didn't work because of all of the errors we actually had in the code, and I ran some initial like, tried to make some statistics on my laptop and out of like 100 test runs we had 70 failures, that's pretty bad, but at this point we're starting to approach like, we have an actual goal that we can measure so we can improve on this, and quite rapidly we figured out that these two big blocks of problems that we have is associated with two things, so one of them is the being able to reliably send updates to all of our clients, which is the kind of deadlock problem that the update channels were just locking up and didn't really work, so we made a massive, massive rewrite PR that re-did the whole logic and made sure that we always were able to send an update to the client as long as it was connected, and then the other problem was the state machine that was very broken, and then we kind of figured out that we can make a global state, and we tried to simplify it initially and optimize later, so basically a global state, how can we determine if everyone is up to date, and make sure that we know when you last received the successful update, and if not we have to re-issue one to make sure that you know everything. However, changing the Rambo culture takes a little bit of time. We kept merging staff without proper integration testing, but as Christopher said, we didn't have the incentive, we didn't have the pressure because the thing really worked. It's not the same when you are in your home lab and you join a node than when you are joining 100 nodes within one second, so if you are slowly joining machines to your tail net, things were working. However, the project was gaining popularity, and we were increasing more and more in contributions in external PRs, and this was around August 2021, or September, something like that. So it was great, we were getting to a point where we could improve headscales with confidence. We had a point of view, given that the project started as a reverse engineering effort, we had a lot of staff that was not that great, we could improve or maintain the compatibility with these third-party external clients that we are using, and we could improve from a community point of view, I'm going to talk a little bit about this now. For starters, we could improve from a technical point of view, we could do massive refactoring within the project, or implementation of the second version of the headscale protocol without breaking the existing users, the only thing that breaks is probably the mental health of the reviewer that has to deal with 3,000 lines of code. But that's a different thing. Then as I said, we have this minor small detail that we completely depend on a third-party client, because we are using exactly the same official clients as a stale scale, however, we have a very good working relationship with them, and every time that they change something, we get a heads up. However, we keep within our integration tests quite a bit of commitment for support in this client. So we target the head of the repository, we target the unstable releases, and we target nine minor releases of the client to make sure that nothing breaks from their side or from ours, because I mean, it can happen. And then I think integration testing also helps the community, because we as maintainers can trust in a better way those random PRs from random unknown people that appear in GitHub, which is something that is not given. And in theory, or that's what one would think, is that by having integration tests, contributors, those external people that we don't know, should also feel more confident when submitting a PR. But that's a theory. So it does still come with some challenges. So one of the things that we see occasionally is that a PR comes in and it doesn't have a test, and then we ask nicely if they can add a test, and then the contributor disappears. So some of the times we're trying to improve on this thing and kind of like always get them in. So what we try to do is, if they truly disappear, we try to pick it up if it's a feature that we really want and we are bound with to do so. Once we try to reach out and kind of sit and help them write a test and kind of onboard them in this kind of things, one of the tests actually for our, there is an SSH feature. And the test for that, I knew the developer and he was also in Norway, so once I was dropping by Oslo, we sat down for an afternoon and we worked on them together and paired on them. That's not available for everyone, sadly. But you know, we always try to kind of like get this test message out there in a way. But there is a couple of other challenges as well, and that is that adding the test raises some sort of learning curve. So you know, you need to know go test, you need to understand our test framework, you need to have Docker and all of this kind of thing, whereas it's not writing tests that are a lot less code. And it's hard to convince people how awesome tests actually really are, that they're not really a chore and that you really, really thank yourself later for doing them. So some of the things we're trying to do to even make this barrier lower, since we're so heavily dependent on this for compatibility and everything, is that we're making like our own test framework, V2, because we depended on a lot of repeated and copied code and there was a really high bar for adding new tests and it was really hard to update and change and it did depend on time.sleep, which was, yeah, haunted me so many times and it couldn't really be ran in parallel for many of the previous reasons and the documentation wasn't really good, like I knew how to use it, one knew how to use it and then that was about it. So a couple of other people figured it out. So what we're trying to do is we're abstracting things a bit away, so we have this concept called control server, which is what essentially head scale is and the tail scale product, the software as a service and it's implemented as like head scale in container and it exposes convenient functions that now have Godox support and all of these things to make it easier for developers to actually use it and then we have the tail scale client, which is implemented at tail scale in container and it has the same type of convenience functions and what this allows us to do is previously the two files on the right here, sorry, on the left, is two different versions of the setup code for the tests because when you needed something that was slightly special, you had to copy the whole thing and then make a new file to be able to write a test case like you see on the other side here, but now after abstracting that away, making it a lot more configurable, we allow people to write more or less regular test cases, but you just set up what we call a scenario, which is a head scale with a given amount of tail scale nodes and then you let them ping each other or something like this. So what do we test right now? We tried to, we kept all of the original tests, so basically we make all nodes, join the network and we make them ping each other to verify that we have a fully functioning network both by IP and magic DNS, magic DNS is tail scales DNS system. We test tail drop, which is a file sharing features, a bit like Apple's airdrop and we send the files from every node to every node to make sure that they work. We test all our registration flows because we broken them a couple of times, so it was better to do it that way, which is pre-authored keys and web plus a command line flow and even open ID we currently have tests for. We try to isolate all of our network from the internet and test with our own embedded relay server because tail scale depends on some relay servers that we also embed in our binary and we have preliminary tests for the SSH features that we support, which is like authenticated by head scale so you can SSH into your machine and we test SSH all to all and we try to do negative tests. And also we test our CLI because if you may change something, you don't want to sit and type in every single command in a structured way manually because that's just painful. So in the future, we want to also kind of improve this granular access control that tail scale offer. Currently this is a very good example of where we have added a lot of unit tests and they all pass, but they're all wrong, so well, they're mostly wrong, so we have to kind of redo most of this into integration test first and then kind of backfill the unit test once we know how the implementation is actually supposed to work. And one of the things we've been dabbling with, especially for this ACL feature, is to use that control server abstraction we have before and use the tail scale product to test our tests because if they pass on the public server, we know they're correct and then we can use them to verify our thing. And then maybe run tail scale in the VM instead of Docker to test it properly, but that's more of a benefit for tail scale than it is for us. So if you're just here waiting for the next talk, a little bit of a TLDR is that, I mean, we cannot understate how important having this integration testing when we depend on an external party has been for the development of health scale. I reckon also the head, like the name, is also excellent, ponytail scale would have been worse. We have, I mean, with integration testing, we are able to maintain this compatibility with the client and we are able to take contributions from third party developers, otherwise it's a little bit more difficult to develop this trust across the internet, right? And even though the tests are not perfect and we still have to migrate unit tests towards integration tests, I think this is one of the keys for the success of the project. So some extra things, tail scale is hosting a happy hour at a brewdog by the station. This QR code takes you to a sign up form, I'll quickly switch back to this slide at the end, but I have like a question slide as well, so, you know, we go through this. Basically this is how to reach us, Github, we have a Discord community, and we're very happy to talk to anyone who wants to talk to us here at Fostem, so please feel free to reach out and I'll leave it at this one if anyone has any questions. We have some minutes, I think. Thank you. While I have your attention, we have a go for that lost there wallet, look to the left, look to the right, front and back, if you see a wallet that is not yours, please come right to the front, it will help this person a lot. Thank you. After you look for the wallet and you have a question, raise your hand and I'll try to come with this microphone. How come the tail scale guys are not mad at you, and not only are not mad at you, but they hurt you afterwards. I mean, part of it, is it working? Yeah. No? Okay. I think part of it is that they are quite chill, I mean, they could have, they are quite chill, they could have taken this way worse than they have, and I don't think we are competition. We are focused on self-hostors, on home labs, perhaps a little bit of a small company. And what usually happens is that people that use headscales at home, then they go to their companies and they talk about tail scale, and when you're in a company, you actually prefer to pay for the service. So it's like a way of... It's like a way of selling headscales, sorry, headscales also. Okay, thank you very much. Last round of applause. If you have any questions, you can card them in the hallway track.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Next up, we have two speakers for the prize of one.", "tokens": [3087, 493, 11, 321, 362, 732, 9518, 337, 264, 12818, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 1, "seek": 0, "start": 10.0, "end": 15.48, "text": " They are going to talk about everything to do with an open, open, more open source version", "tokens": [814, 366, 516, 281, 751, 466, 1203, 281, 360, 365, 364, 1269, 11, 1269, 11, 544, 1269, 4009, 3037], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 2, "seek": 0, "start": 15.48, "end": 16.48, "text": " of Talescale.", "tokens": [295, 50099, 37088, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 3, "seek": 0, "start": 16.48, "end": 21.0, "text": " So let's give an applause to Christopher H. Gouan.", "tokens": [407, 718, 311, 976, 364, 9969, 281, 20649, 389, 13, 460, 263, 282, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 4, "seek": 0, "start": 21.0, "end": 22.0, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 5, "seek": 0, "start": 22.0, "end": 23.0, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 6, "seek": 0, "start": 23.0, "end": 24.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 7, "seek": 0, "start": 24.0, "end": 25.0, "text": " This is cool.", "tokens": [639, 307, 1627, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 8, "seek": 0, "start": 25.0, "end": 26.0, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.4235990105605707, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.6168403625488281}, {"id": 9, "seek": 2600, "start": 26.0, "end": 31.68, "text": " My name is Christopher, and I'm going to, together with H. Gouan there, talk a bit about", "tokens": [1222, 1315, 307, 20649, 11, 293, 286, 478, 516, 281, 11, 1214, 365, 389, 13, 460, 263, 282, 456, 11, 751, 257, 857, 466], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 10, "seek": 2600, "start": 31.68, "end": 38.68, "text": " how we use integration testing to kind of reimplement the control panel or the control", "tokens": [577, 321, 764, 10980, 4997, 281, 733, 295, 33433, 43704, 264, 1969, 4831, 420, 264, 1969], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 11, "seek": 2600, "start": 38.68, "end": 40.8, "text": " server of Talescale.", "tokens": [7154, 295, 50099, 37088, 13], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 12, "seek": 2600, "start": 40.8, "end": 43.68, "text": " So first a little bit about us.", "tokens": [407, 700, 257, 707, 857, 466, 505, 13], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 13, "seek": 2600, "start": 43.68, "end": 46.120000000000005, "text": " Juan Fontalonso is the creator of Talescale.", "tokens": [17064, 43901, 304, 266, 539, 307, 264, 14181, 295, 50099, 37088, 13], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 14, "seek": 2600, "start": 46.120000000000005, "end": 50.6, "text": " He works for the European Space Agency on cloud and DevOps and infrastructure.", "tokens": [634, 1985, 337, 264, 6473, 8705, 21649, 322, 4588, 293, 43051, 293, 6896, 13], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 15, "seek": 2600, "start": 50.6, "end": 55.2, "text": " He claims to have been my first manager, but I think that's incorrect.", "tokens": [634, 9441, 281, 362, 668, 452, 700, 6598, 11, 457, 286, 519, 300, 311, 18424, 13], "temperature": 0.0, "avg_logprob": -0.16313704069670257, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0013777394779026508}, {"id": 16, "seek": 5520, "start": 55.2, "end": 58.68, "text": " And he has the attention span of a goldfish.", "tokens": [400, 415, 575, 264, 3202, 16174, 295, 257, 3821, 11608, 13], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 17, "seek": 5520, "start": 58.68, "end": 63.160000000000004, "text": " Which makes the whole collaboration very fun, and I'm here with Christopher.", "tokens": [3013, 1669, 264, 1379, 9363, 588, 1019, 11, 293, 286, 478, 510, 365, 20649, 13], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 18, "seek": 5520, "start": 63.160000000000004, "end": 69.8, "text": " He's a top contributor of Talescale and one of the other maintainers alongside me.", "tokens": [634, 311, 257, 1192, 42859, 295, 50099, 37088, 293, 472, 295, 264, 661, 6909, 433, 12385, 385, 13], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 19, "seek": 5520, "start": 69.8, "end": 75.32000000000001, "text": " He's part of the technical staff at Talescale, and part of his time at Talescale is to work", "tokens": [634, 311, 644, 295, 264, 6191, 3525, 412, 50099, 37088, 11, 293, 644, 295, 702, 565, 412, 50099, 37088, 307, 281, 589], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 20, "seek": 5520, "start": 75.32000000000001, "end": 77.12, "text": " improving Talescale.", "tokens": [11470, 50099, 37088, 13], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 21, "seek": 5520, "start": 77.12, "end": 81.96000000000001, "text": " I was his manager, at least from a hierarchical point of view.", "tokens": [286, 390, 702, 6598, 11, 412, 1935, 490, 257, 35250, 804, 935, 295, 1910, 13], "temperature": 0.0, "avg_logprob": -0.1860603371051827, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0008516273810528219}, {"id": 22, "seek": 8196, "start": 81.96, "end": 86.44, "text": " And one of the challenges we have is that he always finds these kind of super niche", "tokens": [400, 472, 295, 264, 4759, 321, 362, 307, 300, 415, 1009, 10704, 613, 733, 295, 1687, 19956], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 23, "seek": 8196, "start": 86.44, "end": 91.36, "text": " languages, like OCaml or things like that, where he tries to reimplement headscaling.", "tokens": [8650, 11, 411, 422, 31030, 75, 420, 721, 411, 300, 11, 689, 415, 9898, 281, 33433, 43704, 1378, 4417, 4270, 13], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 24, "seek": 8196, "start": 91.36, "end": 98.8, "text": " But first of all, how many people here know Talescale and headscale?", "tokens": [583, 700, 295, 439, 11, 577, 867, 561, 510, 458, 50099, 37088, 293, 1378, 4417, 1220, 30], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 25, "seek": 8196, "start": 98.8, "end": 99.8, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 26, "seek": 8196, "start": 99.8, "end": 103.63999999999999, "text": " That's pretty good.", "tokens": [663, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 27, "seek": 8196, "start": 103.63999999999999, "end": 109.63999999999999, "text": " So for the people who don't know, we'll do like a quick tweak what is Talescale.", "tokens": [407, 337, 264, 561, 567, 500, 380, 458, 11, 321, 603, 360, 411, 257, 1702, 29879, 437, 307, 50099, 37088, 13], "temperature": 0.0, "avg_logprob": -0.2606820765229845, "compression_ratio": 1.547085201793722, "no_speech_prob": 0.001877155271358788}, {"id": 28, "seek": 10964, "start": 109.64, "end": 116.08, "text": " So Talescale tries to solve this problem, where you basically sit and you want to connect", "tokens": [407, 50099, 37088, 9898, 281, 5039, 341, 1154, 11, 689, 291, 1936, 1394, 293, 291, 528, 281, 1745], "temperature": 0.0, "avg_logprob": -0.20213679778270233, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.002947368426248431}, {"id": 29, "seek": 10964, "start": 116.08, "end": 122.64, "text": " your organization or home or something like this, and you have an old school or legacy", "tokens": [428, 4475, 420, 1280, 420, 746, 411, 341, 11, 293, 291, 362, 364, 1331, 1395, 420, 11711], "temperature": 0.0, "avg_logprob": -0.20213679778270233, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.002947368426248431}, {"id": 30, "seek": 10964, "start": 122.64, "end": 129.16, "text": " VPN concentrator, where you connect into your kind of perimeter, you have access to absolutely", "tokens": [24512, 5512, 19802, 11, 689, 291, 1745, 666, 428, 733, 295, 32404, 11, 291, 362, 2105, 281, 3122], "temperature": 0.0, "avg_logprob": -0.20213679778270233, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.002947368426248431}, {"id": 31, "seek": 10964, "start": 129.16, "end": 133.4, "text": " everything, there's a single point of failure and a massive bottleneck.", "tokens": [1203, 11, 456, 311, 257, 2167, 935, 295, 7763, 293, 257, 5994, 44641, 547, 13], "temperature": 0.0, "avg_logprob": -0.20213679778270233, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.002947368426248431}, {"id": 32, "seek": 13340, "start": 133.4, "end": 142.04000000000002, "text": " And it tries to do this by creating like a mesh VPN that uses direct connections wire", "tokens": [400, 309, 9898, 281, 360, 341, 538, 4084, 411, 257, 17407, 24512, 300, 4960, 2047, 9271, 6234], "temperature": 0.0, "avg_logprob": -0.18101277002474156, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0012633864535018802}, {"id": 33, "seek": 13340, "start": 142.04000000000002, "end": 147.96, "text": " guard and kind of facilitates this for you using techniques like natural and has a very,", "tokens": [6290, 293, 733, 295, 10217, 30035, 341, 337, 291, 1228, 7512, 411, 3303, 293, 575, 257, 588, 11], "temperature": 0.0, "avg_logprob": -0.18101277002474156, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0012633864535018802}, {"id": 34, "seek": 13340, "start": 147.96, "end": 153.92000000000002, "text": " very powerful client that will make sure that you always reach what you're trying to get", "tokens": [588, 4005, 6423, 300, 486, 652, 988, 300, 291, 1009, 2524, 437, 291, 434, 1382, 281, 483], "temperature": 0.0, "avg_logprob": -0.18101277002474156, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0012633864535018802}, {"id": 35, "seek": 13340, "start": 153.92000000000002, "end": 160.44, "text": " to, and it offers a lot of different kind of granular access, and you get a lot more", "tokens": [281, 11, 293, 309, 7736, 257, 688, 295, 819, 733, 295, 39962, 2105, 11, 293, 291, 483, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.18101277002474156, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0012633864535018802}, {"id": 36, "seek": 16044, "start": 160.44, "end": 166.88, "text": " power compared to your old school bottleneck, single point of failure VPN.", "tokens": [1347, 5347, 281, 428, 1331, 1395, 44641, 547, 11, 2167, 935, 295, 7763, 24512, 13], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 37, "seek": 16044, "start": 166.88, "end": 171.0, "text": " And in Talescale, the clients are open source, at least for the open platforms, and what", "tokens": [400, 294, 50099, 37088, 11, 264, 6982, 366, 1269, 4009, 11, 412, 1935, 337, 264, 1269, 9473, 11, 293, 437], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 38, "seek": 16044, "start": 171.0, "end": 172.8, "text": " they have is a closed SAS.", "tokens": [436, 362, 307, 257, 5395, 33441, 13], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 39, "seek": 16044, "start": 172.8, "end": 177.28, "text": " But still, they are quite open when it comes to explaining how the whole thing works.", "tokens": [583, 920, 11, 436, 366, 1596, 1269, 562, 309, 1487, 281, 13468, 577, 264, 1379, 551, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 40, "seek": 16044, "start": 177.28, "end": 182.6, "text": " And in March 2020, they publish a blog post basically explaining how the whole thing worked,", "tokens": [400, 294, 6129, 4808, 11, 436, 11374, 257, 6968, 2183, 1936, 13468, 577, 264, 1379, 551, 2732, 11], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 41, "seek": 16044, "start": 182.6, "end": 189.2, "text": " how they use these natural techniques so you don't have to open the ports in your router.", "tokens": [577, 436, 764, 613, 3303, 7512, 370, 291, 500, 380, 362, 281, 1269, 264, 18160, 294, 428, 22492, 13], "temperature": 0.0, "avg_logprob": -0.1965958191467835, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.0008133652154356241}, {"id": 42, "seek": 18920, "start": 189.2, "end": 194.92, "text": " And there was a phrase in this blog post that gathered my attention for a little bit, and", "tokens": [400, 456, 390, 257, 9535, 294, 341, 6968, 2183, 300, 13032, 452, 3202, 337, 257, 707, 857, 11, 293], "temperature": 0.0, "avg_logprob": -0.23131773051093607, "compression_ratio": 1.6612021857923498, "no_speech_prob": 0.0005125044845044613}, {"id": 43, "seek": 18920, "start": 194.92, "end": 198.95999999999998, "text": " was basically talking about a coordination server, that the clients talk to a coordination", "tokens": [390, 1936, 1417, 466, 257, 21252, 7154, 11, 300, 264, 6982, 751, 281, 257, 21252], "temperature": 0.0, "avg_logprob": -0.23131773051093607, "compression_ratio": 1.6612021857923498, "no_speech_prob": 0.0005125044845044613}, {"id": 44, "seek": 18920, "start": 198.95999999999998, "end": 207.0, "text": " server, the core of this services service offering, which is essentially a shared drop", "tokens": [7154, 11, 264, 4965, 295, 341, 3328, 2643, 8745, 11, 597, 307, 4476, 257, 5507, 3270], "temperature": 0.0, "avg_logprob": -0.23131773051093607, "compression_ratio": 1.6612021857923498, "no_speech_prob": 0.0005125044845044613}, {"id": 45, "seek": 18920, "start": 207.0, "end": 211.56, "text": " box for this wire guard public keys.", "tokens": [2424, 337, 341, 6234, 6290, 1908, 9317, 13], "temperature": 0.0, "avg_logprob": -0.23131773051093607, "compression_ratio": 1.6612021857923498, "no_speech_prob": 0.0005125044845044613}, {"id": 46, "seek": 21156, "start": 211.56, "end": 219.28, "text": " So I was pass up by that, and basically I took that open source clients and started reverse", "tokens": [407, 286, 390, 1320, 493, 538, 300, 11, 293, 1936, 286, 1890, 300, 1269, 4009, 6982, 293, 1409, 9943], "temperature": 0.0, "avg_logprob": -0.2583881703818717, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00046546757221221924}, {"id": 47, "seek": 21156, "start": 219.28, "end": 227.08, "text": " engineering, basically a lot of print apps to see what kind of payload were they sending,", "tokens": [7043, 11, 1936, 257, 688, 295, 4482, 7733, 281, 536, 437, 733, 295, 30918, 645, 436, 7750, 11], "temperature": 0.0, "avg_logprob": -0.2583881703818717, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00046546757221221924}, {"id": 48, "seek": 21156, "start": 227.08, "end": 232.36, "text": " what kind of endpoints or protocol they were doing.", "tokens": [437, 733, 295, 917, 20552, 420, 10336, 436, 645, 884, 13], "temperature": 0.0, "avg_logprob": -0.2583881703818717, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00046546757221221924}, {"id": 49, "seek": 21156, "start": 232.36, "end": 240.28, "text": " And yeah, this was around April 2020, in June, I had a lot of free time at that time, and", "tokens": [400, 1338, 11, 341, 390, 926, 6929, 4808, 11, 294, 6928, 11, 286, 632, 257, 688, 295, 1737, 565, 412, 300, 565, 11, 293], "temperature": 0.0, "avg_logprob": -0.2583881703818717, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00046546757221221924}, {"id": 50, "seek": 24028, "start": 240.28, "end": 242.44, "text": " in June I did the initial release.", "tokens": [294, 6928, 286, 630, 264, 5883, 4374, 13], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 51, "seek": 24028, "start": 242.44, "end": 246.96, "text": " I talked to my friend Christopher about tail scale, and he was very happy distributing", "tokens": [286, 2825, 281, 452, 1277, 20649, 466, 6838, 4373, 11, 293, 415, 390, 588, 2055, 41406], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 52, "seek": 24028, "start": 246.96, "end": 253.32, "text": " wire guard keys with Thansible, which, yeah, so I kept doing my own thing for a while.", "tokens": [6234, 6290, 9317, 365, 334, 599, 964, 11, 597, 11, 1338, 11, 370, 286, 4305, 884, 452, 1065, 551, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 53, "seek": 24028, "start": 253.32, "end": 259.0, "text": " Head scale gained a little bit of traction, and around mid 2021, he joined because he", "tokens": [11398, 4373, 12634, 257, 707, 857, 295, 23558, 11, 293, 926, 2062, 7201, 11, 415, 6869, 570, 415], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 54, "seek": 24028, "start": 259.0, "end": 260.88, "text": " was quite curious about the whole thing.", "tokens": [390, 1596, 6369, 466, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 55, "seek": 24028, "start": 260.88, "end": 267.28, "text": " But he was afraid about breaking stuff, and that's why kind of we are here, although", "tokens": [583, 415, 390, 4638, 466, 7697, 1507, 11, 293, 300, 311, 983, 733, 295, 321, 366, 510, 11, 4878], "temperature": 0.0, "avg_logprob": -0.23073631862424454, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00047139180242083967}, {"id": 56, "seek": 26728, "start": 267.28, "end": 270.79999999999995, "text": " he was not afraid of making a logo, but I think it's super nice.", "tokens": [415, 390, 406, 4638, 295, 1455, 257, 9699, 11, 457, 286, 519, 309, 311, 1687, 1481, 13], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 57, "seek": 26728, "start": 270.79999999999995, "end": 278.35999999999996, "text": " So what I've learned doing this reverse engineering exercise is that the tail scale clients talk", "tokens": [407, 437, 286, 600, 3264, 884, 341, 9943, 7043, 5380, 307, 300, 264, 6838, 4373, 6982, 751], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 58, "seek": 26728, "start": 278.35999999999996, "end": 281.67999999999995, "text": " to what is basically a web service.", "tokens": [281, 437, 307, 1936, 257, 3670, 2643, 13], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 59, "seek": 26728, "start": 281.67999999999995, "end": 287.91999999999996, "text": " This web service receives metadata from the clients, like the endpoints or the wire guard", "tokens": [639, 3670, 2643, 20717, 26603, 490, 264, 6982, 11, 411, 264, 917, 20552, 420, 264, 6234, 6290], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 60, "seek": 26728, "start": 287.91999999999996, "end": 294.44, "text": " public keys that they use, and assigns them IP addresses, like you would having a classic", "tokens": [1908, 9317, 300, 436, 764, 11, 293, 6269, 82, 552, 8671, 16862, 11, 411, 291, 576, 1419, 257, 7230], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 61, "seek": 26728, "start": 294.44, "end": 296.47999999999996, "text": " traditional VPN service.", "tokens": [5164, 24512, 2643, 13], "temperature": 0.0, "avg_logprob": -0.19654669364293417, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.00010703602310968563}, {"id": 62, "seek": 29648, "start": 296.48, "end": 304.88, "text": " As everybody knows about everything, you can establish this mesh network across the clients", "tokens": [1018, 2201, 3255, 466, 1203, 11, 291, 393, 8327, 341, 17407, 3209, 2108, 264, 6982], "temperature": 0.0, "avg_logprob": -0.19967125122805676, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00026906278799287975}, {"id": 63, "seek": 29648, "start": 304.88, "end": 309.6, "text": " without interference because the data doesn't go through the web service.", "tokens": [1553, 24497, 570, 264, 1412, 1177, 380, 352, 807, 264, 3670, 2643, 13], "temperature": 0.0, "avg_logprob": -0.19967125122805676, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00026906278799287975}, {"id": 64, "seek": 29648, "start": 309.6, "end": 313.84000000000003, "text": " So we arrived to the initial stage of head scale, the illusion that everything works", "tokens": [407, 321, 6678, 281, 264, 5883, 3233, 295, 1378, 4373, 11, 264, 18854, 300, 1203, 1985], "temperature": 0.0, "avg_logprob": -0.19967125122805676, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00026906278799287975}, {"id": 65, "seek": 29648, "start": 313.84000000000003, "end": 317.48, "text": " and kind of worked until it stopped doing.", "tokens": [293, 733, 295, 2732, 1826, 309, 5936, 884, 13], "temperature": 0.0, "avg_logprob": -0.19967125122805676, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00026906278799287975}, {"id": 66, "seek": 29648, "start": 317.48, "end": 323.24, "text": " So we had this web service, we implemented the web service, the series of endpoints", "tokens": [407, 321, 632, 341, 3670, 2643, 11, 321, 12270, 264, 3670, 2643, 11, 264, 2638, 295, 917, 20552], "temperature": 0.0, "avg_logprob": -0.19967125122805676, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00026906278799287975}, {"id": 67, "seek": 32324, "start": 323.24, "end": 328.40000000000003, "text": " that we found in the reverse engineering exercise, and we were assigning IP address", "tokens": [300, 321, 1352, 294, 264, 9943, 7043, 5380, 11, 293, 321, 645, 49602, 8671, 2985], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 68, "seek": 32324, "start": 328.40000000000003, "end": 334.08, "text": " to the when a node arrives, and what happens when a second node arrives?", "tokens": [281, 264, 562, 257, 9984, 20116, 11, 293, 437, 2314, 562, 257, 1150, 9984, 20116, 30], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 69, "seek": 32324, "start": 334.08, "end": 340.08, "text": " Hey, we want to tell that I'm here and I want to find my friends and I want to communicate", "tokens": [1911, 11, 321, 528, 281, 980, 300, 286, 478, 510, 293, 286, 528, 281, 915, 452, 1855, 293, 286, 528, 281, 7890], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 70, "seek": 32324, "start": 340.08, "end": 341.08, "text": " with them.", "tokens": [365, 552, 13], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 71, "seek": 32324, "start": 341.08, "end": 347.08, "text": " So in order to handle that, and to handle the metadata that you need to establish the", "tokens": [407, 294, 1668, 281, 4813, 300, 11, 293, 281, 4813, 264, 26603, 300, 291, 643, 281, 8327, 264], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 72, "seek": 32324, "start": 347.08, "end": 351.64, "text": " connections, we developed a little bit of a state machine that will handle, and you know", "tokens": [9271, 11, 321, 4743, 257, 707, 857, 295, 257, 1785, 3479, 300, 486, 4813, 11, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.2271334450199919, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0001707398914732039}, {"id": 73, "seek": 35164, "start": 351.64, "end": 358.68, "text": " has arrived, there's been a change in the map of the network, and we need to distribute", "tokens": [575, 6678, 11, 456, 311, 668, 257, 1319, 294, 264, 4471, 295, 264, 3209, 11, 293, 321, 643, 281, 20594], "temperature": 0.0, "avg_logprob": -0.21098377086498118, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007613605703227222}, {"id": 74, "seek": 35164, "start": 358.68, "end": 362.64, "text": " the updated metadata that we have.", "tokens": [264, 10588, 26603, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.21098377086498118, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007613605703227222}, {"id": 75, "seek": 35164, "start": 362.64, "end": 370.36, "text": " However, at that time, I was kind of learning go, and we follow a little bit of a weird", "tokens": [2908, 11, 412, 300, 565, 11, 286, 390, 733, 295, 2539, 352, 11, 293, 321, 1524, 257, 707, 857, 295, 257, 3657], "temperature": 0.0, "avg_logprob": -0.21098377086498118, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007613605703227222}, {"id": 76, "seek": 35164, "start": 370.36, "end": 378.44, "text": " approach when handling concurrency, which was basically adding more Mutex every time", "tokens": [3109, 562, 13175, 23702, 10457, 11, 597, 390, 1936, 5127, 544, 376, 1169, 87, 633, 565], "temperature": 0.0, "avg_logprob": -0.21098377086498118, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007613605703227222}, {"id": 77, "seek": 35164, "start": 378.44, "end": 379.44, "text": " we needed it.", "tokens": [321, 2978, 309, 13], "temperature": 0.0, "avg_logprob": -0.21098377086498118, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007613605703227222}, {"id": 78, "seek": 37944, "start": 379.44, "end": 385.48, "text": " And this is a problem, because at the end we ended up with a great Mutex for this state", "tokens": [400, 341, 307, 257, 1154, 11, 570, 412, 264, 917, 321, 4590, 493, 365, 257, 869, 376, 1169, 87, 337, 341, 1785], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 79, "seek": 37944, "start": 385.48, "end": 390.28, "text": " machine, and this is a very big problem because the Python track is tomorrow, so the grid", "tokens": [3479, 11, 293, 341, 307, 257, 588, 955, 1154, 570, 264, 15329, 2837, 307, 4153, 11, 370, 264, 10748], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 80, "seek": 37944, "start": 390.28, "end": 392.64, "text": " logs are over there.", "tokens": [20820, 366, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 81, "seek": 37944, "start": 392.64, "end": 397.24, "text": " So what ended up happening inside the state machine, or what didn't end up happening,", "tokens": [407, 437, 4590, 493, 2737, 1854, 264, 1785, 3479, 11, 420, 437, 994, 380, 917, 493, 2737, 11], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 82, "seek": 37944, "start": 397.24, "end": 402.88, "text": " was that basically some of the failure modes we saw was that a new node trying to register,", "tokens": [390, 300, 1936, 512, 295, 264, 7763, 14068, 321, 1866, 390, 300, 257, 777, 9984, 1382, 281, 7280, 11], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 83, "seek": 37944, "start": 402.88, "end": 406.92, "text": " and then we burned a couple of CPU cycles trying to calculate some stuff, and then we", "tokens": [293, 550, 321, 13490, 257, 1916, 295, 13199, 17796, 1382, 281, 8873, 512, 1507, 11, 293, 550, 321], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 84, "seek": 37944, "start": 406.92, "end": 408.12, "text": " did nothing.", "tokens": [630, 1825, 13], "temperature": 0.0, "avg_logprob": -0.15364561080932618, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.001592022366821766}, {"id": 85, "seek": 40812, "start": 408.12, "end": 411.28000000000003, "text": " So no updates were sent out or anything.", "tokens": [407, 572, 9205, 645, 2279, 484, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 86, "seek": 40812, "start": 411.28000000000003, "end": 416.32, "text": " Sometimes we would have a new node joining, and we would compute everything, send some", "tokens": [4803, 321, 576, 362, 257, 777, 9984, 5549, 11, 293, 321, 576, 14722, 1203, 11, 2845, 512], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 87, "seek": 40812, "start": 416.32, "end": 421.08, "text": " network traffic, we just omitted the new information, that was kind of crucial for everyone to know,", "tokens": [3209, 6419, 11, 321, 445, 3406, 3944, 264, 777, 1589, 11, 300, 390, 733, 295, 11462, 337, 1518, 281, 458, 11], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 88, "seek": 40812, "start": 421.08, "end": 423.32, "text": " so it ended up not working.", "tokens": [370, 309, 4590, 493, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 89, "seek": 40812, "start": 423.32, "end": 428.32, "text": " And sometimes a new node joined, nothing really happened, but then eventually something", "tokens": [400, 2171, 257, 777, 9984, 6869, 11, 1825, 534, 2011, 11, 457, 550, 4728, 746], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 90, "seek": 40812, "start": 428.32, "end": 433.76, "text": " happens and it sent out an update to everyone, and that was, you know, useful.", "tokens": [2314, 293, 309, 2279, 484, 364, 5623, 281, 1518, 11, 293, 300, 390, 11, 291, 458, 11, 4420, 13], "temperature": 0.0, "avg_logprob": -0.10765537561154832, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0012770965695381165}, {"id": 91, "seek": 43376, "start": 433.76, "end": 440.44, "text": " And sometimes on the individual update channels for each node, some of these aforementioned", "tokens": [400, 2171, 322, 264, 2609, 5623, 9235, 337, 1184, 9984, 11, 512, 295, 613, 48927, 46842], "temperature": 0.0, "avg_logprob": -0.1439153003692627, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0006685801781713963}, {"id": 92, "seek": 43376, "start": 440.44, "end": 445.52, "text": " Mutexes kind of deadlocked up the whole thread or the GoRoutine, and then we just never sent", "tokens": [376, 1169, 47047, 733, 295, 3116, 4102, 292, 493, 264, 1379, 7207, 420, 264, 1037, 49, 45075, 11, 293, 550, 321, 445, 1128, 2279], "temperature": 0.0, "avg_logprob": -0.1439153003692627, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0006685801781713963}, {"id": 93, "seek": 43376, "start": 445.52, "end": 449.88, "text": " updates to particular nodes, and sometimes we just deadlocked the whole server and you", "tokens": [9205, 281, 1729, 13891, 11, 293, 2171, 321, 445, 3116, 4102, 292, 264, 1379, 7154, 293, 291], "temperature": 0.0, "avg_logprob": -0.1439153003692627, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0006685801781713963}, {"id": 94, "seek": 43376, "start": 449.88, "end": 454.88, "text": " kind of had to kick it to make it come back to life.", "tokens": [733, 295, 632, 281, 4437, 309, 281, 652, 309, 808, 646, 281, 993, 13], "temperature": 0.0, "avg_logprob": -0.1439153003692627, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0006685801781713963}, {"id": 95, "seek": 43376, "start": 454.88, "end": 460.8, "text": " But still there was kind of this notion that it did work pretty well eventually most of", "tokens": [583, 920, 456, 390, 733, 295, 341, 10710, 300, 309, 630, 589, 1238, 731, 4728, 881, 295], "temperature": 0.0, "avg_logprob": -0.1439153003692627, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0006685801781713963}, {"id": 96, "seek": 46080, "start": 460.8, "end": 468.04, "text": " the time, and it gave this illusion of working, because what you often saw was that you had", "tokens": [264, 565, 11, 293, 309, 2729, 341, 18854, 295, 1364, 11, 570, 437, 291, 2049, 1866, 390, 300, 291, 632], "temperature": 0.0, "avg_logprob": -0.18670096764197716, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0017170559149235487}, {"id": 97, "seek": 46080, "start": 468.04, "end": 473.12, "text": " like three nodes, and only two of them actually talked together, and as long as those had", "tokens": [411, 1045, 13891, 11, 293, 787, 732, 295, 552, 767, 2825, 1214, 11, 293, 382, 938, 382, 729, 632], "temperature": 0.0, "avg_logprob": -0.18670096764197716, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0017170559149235487}, {"id": 98, "seek": 46080, "start": 473.12, "end": 478.0, "text": " received the updates they needed, you know, the user was happy and you're just like, ah,", "tokens": [4613, 264, 9205, 436, 2978, 11, 291, 458, 11, 264, 4195, 390, 2055, 293, 291, 434, 445, 411, 11, 3716, 11], "temperature": 0.0, "avg_logprob": -0.18670096764197716, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0017170559149235487}, {"id": 99, "seek": 46080, "start": 478.0, "end": 484.68, "text": " it works, so I'm going to press the star sign on GitHub and share it with my friends.", "tokens": [309, 1985, 11, 370, 286, 478, 516, 281, 1886, 264, 3543, 1465, 322, 23331, 293, 2073, 309, 365, 452, 1855, 13], "temperature": 0.0, "avg_logprob": -0.18670096764197716, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0017170559149235487}, {"id": 100, "seek": 48468, "start": 484.68, "end": 490.88, "text": " So, but we figured that eventually this would like caught up with us and we're trying to", "tokens": [407, 11, 457, 321, 8932, 300, 4728, 341, 576, 411, 5415, 493, 365, 505, 293, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.11823518426568659, "compression_ratio": 1.804, "no_speech_prob": 0.0008964849985204637}, {"id": 101, "seek": 48468, "start": 490.88, "end": 495.8, "text": " get to this stage where we, you know, it works most of the time, so what we did have was a", "tokens": [483, 281, 341, 3233, 689, 321, 11, 291, 458, 11, 309, 1985, 881, 295, 264, 565, 11, 370, 437, 321, 630, 362, 390, 257], "temperature": 0.0, "avg_logprob": -0.11823518426568659, "compression_ratio": 1.804, "no_speech_prob": 0.0008964849985204637}, {"id": 102, "seek": 48468, "start": 495.8, "end": 501.12, "text": " fair amount of unit tests, but the problem with unit tests is that we're trying to reverse", "tokens": [3143, 2372, 295, 4985, 6921, 11, 457, 264, 1154, 365, 4985, 6921, 307, 300, 321, 434, 1382, 281, 9943], "temperature": 0.0, "avg_logprob": -0.11823518426568659, "compression_ratio": 1.804, "no_speech_prob": 0.0008964849985204637}, {"id": 103, "seek": 48468, "start": 501.12, "end": 506.68, "text": " engineering something, that we're also learning how it works, and what we spent a lot of time", "tokens": [7043, 746, 11, 300, 321, 434, 611, 2539, 577, 309, 1985, 11, 293, 437, 321, 4418, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.11823518426568659, "compression_ratio": 1.804, "no_speech_prob": 0.0008964849985204637}, {"id": 104, "seek": 48468, "start": 506.68, "end": 511.8, "text": " on was misunderstanding how it was supposed to work, writing wrong, well, writing unit", "tokens": [322, 390, 29227, 577, 309, 390, 3442, 281, 589, 11, 3579, 2085, 11, 731, 11, 3579, 4985], "temperature": 0.0, "avg_logprob": -0.11823518426568659, "compression_ratio": 1.804, "no_speech_prob": 0.0008964849985204637}, {"id": 105, "seek": 51180, "start": 511.8, "end": 517.08, "text": " tests that would pass, but they were wrong, so you kind of have like a, a passing test", "tokens": [6921, 300, 576, 1320, 11, 457, 436, 645, 2085, 11, 370, 291, 733, 295, 362, 411, 257, 11, 257, 8437, 1500], "temperature": 0.0, "avg_logprob": -0.18448049715249845, "compression_ratio": 1.7372549019607844, "no_speech_prob": 0.00026539326063357294}, {"id": 106, "seek": 51180, "start": 517.08, "end": 521.6800000000001, "text": " and it's an entirely wrong implementation, and 90% of what we were actually trying to", "tokens": [293, 309, 311, 364, 7696, 2085, 11420, 11, 293, 4289, 4, 295, 437, 321, 645, 767, 1382, 281], "temperature": 0.0, "avg_logprob": -0.18448049715249845, "compression_ratio": 1.7372549019607844, "no_speech_prob": 0.00026539326063357294}, {"id": 107, "seek": 51180, "start": 521.6800000000001, "end": 528.12, "text": " do was integrate with a third party software, and this is where we get to actual integration", "tokens": [360, 390, 13365, 365, 257, 2636, 3595, 4722, 11, 293, 341, 307, 689, 321, 483, 281, 3539, 10980], "temperature": 0.0, "avg_logprob": -0.18448049715249845, "compression_ratio": 1.7372549019607844, "no_speech_prob": 0.00026539326063357294}, {"id": 108, "seek": 51180, "start": 528.12, "end": 533.24, "text": " tests, so what I started doing was I found this Docker test framework, which basically", "tokens": [6921, 11, 370, 437, 286, 1409, 884, 390, 286, 1352, 341, 33772, 1500, 8388, 11, 597, 1936], "temperature": 0.0, "avg_logprob": -0.18448049715249845, "compression_ratio": 1.7372549019607844, "no_speech_prob": 0.00026539326063357294}, {"id": 109, "seek": 51180, "start": 533.24, "end": 539.8, "text": " allows you like programmatically create Docker containers, so we started making tests that", "tokens": [4045, 291, 411, 37648, 5030, 1884, 33772, 17089, 11, 370, 321, 1409, 1455, 6921, 300], "temperature": 0.0, "avg_logprob": -0.18448049715249845, "compression_ratio": 1.7372549019607844, "no_speech_prob": 0.00026539326063357294}, {"id": 110, "seek": 53980, "start": 539.8, "end": 547.1999999999999, "text": " spun up a head scale container, it created a bunch of tail scale instances also running", "tokens": [37038, 493, 257, 1378, 4373, 10129, 11, 309, 2942, 257, 3840, 295, 6838, 4373, 14519, 611, 2614], "temperature": 0.0, "avg_logprob": -0.16099401396148058, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0006424094317480922}, {"id": 111, "seek": 53980, "start": 547.1999999999999, "end": 552.04, "text": " in Docker, and associated them with a couple of users and tried to like emulate the entire", "tokens": [294, 33772, 11, 293, 6615, 552, 365, 257, 1916, 295, 5022, 293, 3031, 281, 411, 45497, 264, 2302], "temperature": 0.0, "avg_logprob": -0.16099401396148058, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0006424094317480922}, {"id": 112, "seek": 53980, "start": 552.04, "end": 555.24, "text": " environment so you can test everyone to everyone.", "tokens": [2823, 370, 291, 393, 1500, 1518, 281, 1518, 13], "temperature": 0.0, "avg_logprob": -0.16099401396148058, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0006424094317480922}, {"id": 113, "seek": 53980, "start": 555.24, "end": 561.3599999999999, "text": " We had them join the head scale server, and since it takes a little bit of time for everyone", "tokens": [492, 632, 552, 3917, 264, 1378, 4373, 7154, 11, 293, 1670, 309, 2516, 257, 707, 857, 295, 565, 337, 1518], "temperature": 0.0, "avg_logprob": -0.16099401396148058, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0006424094317480922}, {"id": 114, "seek": 53980, "start": 561.3599999999999, "end": 565.68, "text": " to catch up with each other and, you know, send the updates and stuff, so we put a sleep", "tokens": [281, 3745, 493, 365, 1184, 661, 293, 11, 291, 458, 11, 2845, 264, 9205, 293, 1507, 11, 370, 321, 829, 257, 2817], "temperature": 0.0, "avg_logprob": -0.16099401396148058, "compression_ratio": 1.7012448132780082, "no_speech_prob": 0.0006424094317480922}, {"id": 115, "seek": 56568, "start": 565.68, "end": 573.4799999999999, "text": " of two minutes in front of the test, which is a terrible idea, but, you know, you learn,", "tokens": [295, 732, 2077, 294, 1868, 295, 264, 1500, 11, 597, 307, 257, 6237, 1558, 11, 457, 11, 291, 458, 11, 291, 1466, 11], "temperature": 0.0, "avg_logprob": -0.11758979797363281, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.0006394227966666222}, {"id": 116, "seek": 56568, "start": 573.4799999999999, "end": 578.1999999999999, "text": " and then after that sleep runs out, presumably everyone is now up to date and can talk to", "tokens": [293, 550, 934, 300, 2817, 6676, 484, 11, 26742, 1518, 307, 586, 493, 281, 4002, 293, 393, 751, 281], "temperature": 0.0, "avg_logprob": -0.11758979797363281, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.0006394227966666222}, {"id": 117, "seek": 56568, "start": 578.1999999999999, "end": 582.52, "text": " each other, so we had a test, the most basic test is, is my network working, can everyone", "tokens": [1184, 661, 11, 370, 321, 632, 257, 1500, 11, 264, 881, 3875, 1500, 307, 11, 307, 452, 3209, 1364, 11, 393, 1518], "temperature": 0.0, "avg_logprob": -0.11758979797363281, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.0006394227966666222}, {"id": 118, "seek": 56568, "start": 582.52, "end": 583.7199999999999, "text": " ping everyone?", "tokens": [26151, 1518, 30], "temperature": 0.0, "avg_logprob": -0.11758979797363281, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.0006394227966666222}, {"id": 119, "seek": 56568, "start": 583.7199999999999, "end": 590.12, "text": " So we tried to do that, and of course that didn't work because of all of the errors we", "tokens": [407, 321, 3031, 281, 360, 300, 11, 293, 295, 1164, 300, 994, 380, 589, 570, 295, 439, 295, 264, 13603, 321], "temperature": 0.0, "avg_logprob": -0.11758979797363281, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.0006394227966666222}, {"id": 120, "seek": 59012, "start": 590.12, "end": 596.16, "text": " actually had in the code, and I ran some initial like, tried to make some statistics on my", "tokens": [767, 632, 294, 264, 3089, 11, 293, 286, 5872, 512, 5883, 411, 11, 3031, 281, 652, 512, 12523, 322, 452], "temperature": 0.0, "avg_logprob": -0.134819021931401, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.001651954255066812}, {"id": 121, "seek": 59012, "start": 596.16, "end": 602.48, "text": " laptop and out of like 100 test runs we had 70 failures, that's pretty bad, but at this", "tokens": [10732, 293, 484, 295, 411, 2319, 1500, 6676, 321, 632, 5285, 20774, 11, 300, 311, 1238, 1578, 11, 457, 412, 341], "temperature": 0.0, "avg_logprob": -0.134819021931401, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.001651954255066812}, {"id": 122, "seek": 59012, "start": 602.48, "end": 606.8, "text": " point we're starting to approach like, we have an actual goal that we can measure so", "tokens": [935, 321, 434, 2891, 281, 3109, 411, 11, 321, 362, 364, 3539, 3387, 300, 321, 393, 3481, 370], "temperature": 0.0, "avg_logprob": -0.134819021931401, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.001651954255066812}, {"id": 123, "seek": 59012, "start": 606.8, "end": 613.96, "text": " we can improve on this, and quite rapidly we figured out that these two big blocks of", "tokens": [321, 393, 3470, 322, 341, 11, 293, 1596, 12910, 321, 8932, 484, 300, 613, 732, 955, 8474, 295], "temperature": 0.0, "avg_logprob": -0.134819021931401, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.001651954255066812}, {"id": 124, "seek": 59012, "start": 613.96, "end": 619.24, "text": " problems that we have is associated with two things, so one of them is the being able to", "tokens": [2740, 300, 321, 362, 307, 6615, 365, 732, 721, 11, 370, 472, 295, 552, 307, 264, 885, 1075, 281], "temperature": 0.0, "avg_logprob": -0.134819021931401, "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.001651954255066812}, {"id": 125, "seek": 61924, "start": 619.24, "end": 624.96, "text": " reliably send updates to all of our clients, which is the kind of deadlock problem that", "tokens": [49927, 2845, 9205, 281, 439, 295, 527, 6982, 11, 597, 307, 264, 733, 295, 3116, 4102, 1154, 300], "temperature": 0.0, "avg_logprob": -0.12871375310988653, "compression_ratio": 1.779591836734694, "no_speech_prob": 0.0010440319310873747}, {"id": 126, "seek": 61924, "start": 624.96, "end": 629.36, "text": " the update channels were just locking up and didn't really work, so we made a massive,", "tokens": [264, 5623, 9235, 645, 445, 23954, 493, 293, 994, 380, 534, 589, 11, 370, 321, 1027, 257, 5994, 11], "temperature": 0.0, "avg_logprob": -0.12871375310988653, "compression_ratio": 1.779591836734694, "no_speech_prob": 0.0010440319310873747}, {"id": 127, "seek": 61924, "start": 629.36, "end": 633.84, "text": " massive rewrite PR that re-did the whole logic and made sure that we always were able", "tokens": [5994, 28132, 11568, 300, 319, 12, 38169, 264, 1379, 9952, 293, 1027, 988, 300, 321, 1009, 645, 1075], "temperature": 0.0, "avg_logprob": -0.12871375310988653, "compression_ratio": 1.779591836734694, "no_speech_prob": 0.0010440319310873747}, {"id": 128, "seek": 61924, "start": 633.84, "end": 638.48, "text": " to send an update to the client as long as it was connected, and then the other problem", "tokens": [281, 2845, 364, 5623, 281, 264, 6423, 382, 938, 382, 309, 390, 4582, 11, 293, 550, 264, 661, 1154], "temperature": 0.0, "avg_logprob": -0.12871375310988653, "compression_ratio": 1.779591836734694, "no_speech_prob": 0.0010440319310873747}, {"id": 129, "seek": 61924, "start": 638.48, "end": 643.4, "text": " was the state machine that was very broken, and then we kind of figured out that we can", "tokens": [390, 264, 1785, 3479, 300, 390, 588, 5463, 11, 293, 550, 321, 733, 295, 8932, 484, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.12871375310988653, "compression_ratio": 1.779591836734694, "no_speech_prob": 0.0010440319310873747}, {"id": 130, "seek": 64340, "start": 643.4, "end": 650.16, "text": " make a global state, and we tried to simplify it initially and optimize later, so basically", "tokens": [652, 257, 4338, 1785, 11, 293, 321, 3031, 281, 20460, 309, 9105, 293, 19719, 1780, 11, 370, 1936], "temperature": 0.0, "avg_logprob": -0.1669458928315536, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0006788234459236264}, {"id": 131, "seek": 64340, "start": 650.16, "end": 657.4399999999999, "text": " a global state, how can we determine if everyone is up to date, and make sure that we know", "tokens": [257, 4338, 1785, 11, 577, 393, 321, 6997, 498, 1518, 307, 493, 281, 4002, 11, 293, 652, 988, 300, 321, 458], "temperature": 0.0, "avg_logprob": -0.1669458928315536, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0006788234459236264}, {"id": 132, "seek": 64340, "start": 657.4399999999999, "end": 661.72, "text": " when you last received the successful update, and if not we have to re-issue one to make", "tokens": [562, 291, 1036, 4613, 264, 4406, 5623, 11, 293, 498, 406, 321, 362, 281, 319, 12, 891, 622, 472, 281, 652], "temperature": 0.0, "avg_logprob": -0.1669458928315536, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0006788234459236264}, {"id": 133, "seek": 64340, "start": 661.72, "end": 663.8, "text": " sure that you know everything.", "tokens": [988, 300, 291, 458, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1669458928315536, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0006788234459236264}, {"id": 134, "seek": 64340, "start": 663.8, "end": 669.3199999999999, "text": " However, changing the Rambo culture takes a little bit of time.", "tokens": [2908, 11, 4473, 264, 9078, 1763, 3713, 2516, 257, 707, 857, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.1669458928315536, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0006788234459236264}, {"id": 135, "seek": 66932, "start": 669.32, "end": 675.5200000000001, "text": " We kept merging staff without proper integration testing, but as Christopher said, we didn't", "tokens": [492, 4305, 44559, 3525, 1553, 2296, 10980, 4997, 11, 457, 382, 20649, 848, 11, 321, 994, 380], "temperature": 0.0, "avg_logprob": -0.17248231909248266, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.00020816987671423703}, {"id": 136, "seek": 66932, "start": 675.5200000000001, "end": 680.8000000000001, "text": " have the incentive, we didn't have the pressure because the thing really worked.", "tokens": [362, 264, 22346, 11, 321, 994, 380, 362, 264, 3321, 570, 264, 551, 534, 2732, 13], "temperature": 0.0, "avg_logprob": -0.17248231909248266, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.00020816987671423703}, {"id": 137, "seek": 66932, "start": 680.8000000000001, "end": 686.0400000000001, "text": " It's not the same when you are in your home lab and you join a node than when you are", "tokens": [467, 311, 406, 264, 912, 562, 291, 366, 294, 428, 1280, 2715, 293, 291, 3917, 257, 9984, 813, 562, 291, 366], "temperature": 0.0, "avg_logprob": -0.17248231909248266, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.00020816987671423703}, {"id": 138, "seek": 66932, "start": 686.0400000000001, "end": 692.12, "text": " joining 100 nodes within one second, so if you are slowly joining machines to your tail", "tokens": [5549, 2319, 13891, 1951, 472, 1150, 11, 370, 498, 291, 366, 5692, 5549, 8379, 281, 428, 6838], "temperature": 0.0, "avg_logprob": -0.17248231909248266, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.00020816987671423703}, {"id": 139, "seek": 66932, "start": 692.12, "end": 694.72, "text": " net, things were working.", "tokens": [2533, 11, 721, 645, 1364, 13], "temperature": 0.0, "avg_logprob": -0.17248231909248266, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.00020816987671423703}, {"id": 140, "seek": 69472, "start": 694.72, "end": 702.1600000000001, "text": " However, the project was gaining popularity, and we were increasing more and more in contributions", "tokens": [2908, 11, 264, 1716, 390, 19752, 19301, 11, 293, 321, 645, 5662, 544, 293, 544, 294, 15725], "temperature": 0.0, "avg_logprob": -0.2505374321570763, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.00029132995405234396}, {"id": 141, "seek": 69472, "start": 702.1600000000001, "end": 709.36, "text": " in external PRs, and this was around August 2021, or September, something like that.", "tokens": [294, 8320, 11568, 82, 11, 293, 341, 390, 926, 6897, 7201, 11, 420, 7216, 11, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2505374321570763, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.00029132995405234396}, {"id": 142, "seek": 69472, "start": 709.36, "end": 716.88, "text": " So it was great, we were getting to a point where we could improve headscales with confidence.", "tokens": [407, 309, 390, 869, 11, 321, 645, 1242, 281, 257, 935, 689, 321, 727, 3470, 1378, 4417, 4229, 365, 6687, 13], "temperature": 0.0, "avg_logprob": -0.2505374321570763, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.00029132995405234396}, {"id": 143, "seek": 71688, "start": 716.88, "end": 728.0, "text": " We had a point of view, given that the project started as a reverse engineering effort, we", "tokens": [492, 632, 257, 935, 295, 1910, 11, 2212, 300, 264, 1716, 1409, 382, 257, 9943, 7043, 4630, 11, 321], "temperature": 0.0, "avg_logprob": -0.21204778883192274, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00029579680995084345}, {"id": 144, "seek": 71688, "start": 728.0, "end": 733.88, "text": " had a lot of staff that was not that great, we could improve or maintain the compatibility", "tokens": [632, 257, 688, 295, 3525, 300, 390, 406, 300, 869, 11, 321, 727, 3470, 420, 6909, 264, 34237], "temperature": 0.0, "avg_logprob": -0.21204778883192274, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00029579680995084345}, {"id": 145, "seek": 71688, "start": 733.88, "end": 740.8, "text": " with these third-party external clients that we are using, and we could improve from a", "tokens": [365, 613, 2636, 12, 23409, 8320, 6982, 300, 321, 366, 1228, 11, 293, 321, 727, 3470, 490, 257], "temperature": 0.0, "avg_logprob": -0.21204778883192274, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00029579680995084345}, {"id": 146, "seek": 74080, "start": 740.8, "end": 747.5999999999999, "text": " community point of view, I'm going to talk a little bit about this now.", "tokens": [1768, 935, 295, 1910, 11, 286, 478, 516, 281, 751, 257, 707, 857, 466, 341, 586, 13], "temperature": 0.0, "avg_logprob": -0.18012088107079574, "compression_ratio": 1.6804979253112033, "no_speech_prob": 0.00032440893119201064}, {"id": 147, "seek": 74080, "start": 747.5999999999999, "end": 752.92, "text": " For starters, we could improve from a technical point of view, we could do massive refactoring", "tokens": [1171, 35131, 11, 321, 727, 3470, 490, 257, 6191, 935, 295, 1910, 11, 321, 727, 360, 5994, 1895, 578, 3662], "temperature": 0.0, "avg_logprob": -0.18012088107079574, "compression_ratio": 1.6804979253112033, "no_speech_prob": 0.00032440893119201064}, {"id": 148, "seek": 74080, "start": 752.92, "end": 759.68, "text": " within the project, or implementation of the second version of the headscale protocol", "tokens": [1951, 264, 1716, 11, 420, 11420, 295, 264, 1150, 3037, 295, 264, 1378, 4417, 1220, 10336], "temperature": 0.0, "avg_logprob": -0.18012088107079574, "compression_ratio": 1.6804979253112033, "no_speech_prob": 0.00032440893119201064}, {"id": 149, "seek": 74080, "start": 759.68, "end": 765.9599999999999, "text": " without breaking the existing users, the only thing that breaks is probably the mental health", "tokens": [1553, 7697, 264, 6741, 5022, 11, 264, 787, 551, 300, 9857, 307, 1391, 264, 4973, 1585], "temperature": 0.0, "avg_logprob": -0.18012088107079574, "compression_ratio": 1.6804979253112033, "no_speech_prob": 0.00032440893119201064}, {"id": 150, "seek": 74080, "start": 765.9599999999999, "end": 769.56, "text": " of the reviewer that has to deal with 3,000 lines of code.", "tokens": [295, 264, 3131, 260, 300, 575, 281, 2028, 365, 805, 11, 1360, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18012088107079574, "compression_ratio": 1.6804979253112033, "no_speech_prob": 0.00032440893119201064}, {"id": 151, "seek": 76956, "start": 769.56, "end": 773.0799999999999, "text": " But that's a different thing.", "tokens": [583, 300, 311, 257, 819, 551, 13], "temperature": 0.0, "avg_logprob": -0.26581608376851895, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.0006234179600141943}, {"id": 152, "seek": 76956, "start": 773.0799999999999, "end": 780.4799999999999, "text": " Then as I said, we have this minor small detail that we completely depend on a third-party", "tokens": [1396, 382, 286, 848, 11, 321, 362, 341, 6696, 1359, 2607, 300, 321, 2584, 5672, 322, 257, 2636, 12, 23409], "temperature": 0.0, "avg_logprob": -0.26581608376851895, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.0006234179600141943}, {"id": 153, "seek": 76956, "start": 780.4799999999999, "end": 787.04, "text": " client, because we are using exactly the same official clients as a stale scale, however,", "tokens": [6423, 11, 570, 321, 366, 1228, 2293, 264, 912, 4783, 6982, 382, 257, 342, 1220, 4373, 11, 4461, 11], "temperature": 0.0, "avg_logprob": -0.26581608376851895, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.0006234179600141943}, {"id": 154, "seek": 76956, "start": 787.04, "end": 792.4, "text": " we have a very good working relationship with them, and every time that they change something,", "tokens": [321, 362, 257, 588, 665, 1364, 2480, 365, 552, 11, 293, 633, 565, 300, 436, 1319, 746, 11], "temperature": 0.0, "avg_logprob": -0.26581608376851895, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.0006234179600141943}, {"id": 155, "seek": 76956, "start": 792.4, "end": 793.9599999999999, "text": " we get a heads up.", "tokens": [321, 483, 257, 8050, 493, 13], "temperature": 0.0, "avg_logprob": -0.26581608376851895, "compression_ratio": 1.5502392344497609, "no_speech_prob": 0.0006234179600141943}, {"id": 156, "seek": 79396, "start": 793.96, "end": 801.96, "text": " However, we keep within our integration tests quite a bit of commitment for support in this", "tokens": [2908, 11, 321, 1066, 1951, 527, 10980, 6921, 1596, 257, 857, 295, 8371, 337, 1406, 294, 341], "temperature": 0.0, "avg_logprob": -0.22623119856181897, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0001645799056859687}, {"id": 157, "seek": 79396, "start": 801.96, "end": 802.96, "text": " client.", "tokens": [6423, 13], "temperature": 0.0, "avg_logprob": -0.22623119856181897, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0001645799056859687}, {"id": 158, "seek": 79396, "start": 802.96, "end": 808.2800000000001, "text": " So we target the head of the repository, we target the unstable releases, and we target", "tokens": [407, 321, 3779, 264, 1378, 295, 264, 25841, 11, 321, 3779, 264, 23742, 16952, 11, 293, 321, 3779], "temperature": 0.0, "avg_logprob": -0.22623119856181897, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0001645799056859687}, {"id": 159, "seek": 79396, "start": 808.2800000000001, "end": 815.0400000000001, "text": " nine minor releases of the client to make sure that nothing breaks from their side or from", "tokens": [4949, 6696, 16952, 295, 264, 6423, 281, 652, 988, 300, 1825, 9857, 490, 641, 1252, 420, 490], "temperature": 0.0, "avg_logprob": -0.22623119856181897, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0001645799056859687}, {"id": 160, "seek": 79396, "start": 815.0400000000001, "end": 818.36, "text": " ours, because I mean, it can happen.", "tokens": [11896, 11, 570, 286, 914, 11, 309, 393, 1051, 13], "temperature": 0.0, "avg_logprob": -0.22623119856181897, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0001645799056859687}, {"id": 161, "seek": 81836, "start": 818.36, "end": 828.04, "text": " And then I think integration testing also helps the community, because we as maintainers", "tokens": [400, 550, 286, 519, 10980, 4997, 611, 3665, 264, 1768, 11, 570, 321, 382, 6909, 433], "temperature": 0.0, "avg_logprob": -0.19037803949094287, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0015750419115647674}, {"id": 162, "seek": 81836, "start": 828.04, "end": 834.88, "text": " can trust in a better way those random PRs from random unknown people that appear in", "tokens": [393, 3361, 294, 257, 1101, 636, 729, 4974, 11568, 82, 490, 4974, 9841, 561, 300, 4204, 294], "temperature": 0.0, "avg_logprob": -0.19037803949094287, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0015750419115647674}, {"id": 163, "seek": 81836, "start": 834.88, "end": 841.64, "text": " GitHub, which is something that is not given.", "tokens": [23331, 11, 597, 307, 746, 300, 307, 406, 2212, 13], "temperature": 0.0, "avg_logprob": -0.19037803949094287, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.0015750419115647674}, {"id": 164, "seek": 84164, "start": 841.64, "end": 848.56, "text": " And in theory, or that's what one would think, is that by having integration tests, contributors,", "tokens": [400, 294, 5261, 11, 420, 300, 311, 437, 472, 576, 519, 11, 307, 300, 538, 1419, 10980, 6921, 11, 45627, 11], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 165, "seek": 84164, "start": 848.56, "end": 855.08, "text": " those external people that we don't know, should also feel more confident when submitting", "tokens": [729, 8320, 561, 300, 321, 500, 380, 458, 11, 820, 611, 841, 544, 6679, 562, 31836], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 166, "seek": 84164, "start": 855.08, "end": 856.08, "text": " a PR.", "tokens": [257, 11568, 13], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 167, "seek": 84164, "start": 856.08, "end": 858.68, "text": " But that's a theory.", "tokens": [583, 300, 311, 257, 5261, 13], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 168, "seek": 84164, "start": 858.68, "end": 861.48, "text": " So it does still come with some challenges.", "tokens": [407, 309, 775, 920, 808, 365, 512, 4759, 13], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 169, "seek": 84164, "start": 861.48, "end": 866.76, "text": " So one of the things that we see occasionally is that a PR comes in and it doesn't have", "tokens": [407, 472, 295, 264, 721, 300, 321, 536, 16895, 307, 300, 257, 11568, 1487, 294, 293, 309, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.1554286399584138, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0006543257622979581}, {"id": 170, "seek": 86676, "start": 866.76, "end": 873.3199999999999, "text": " a test, and then we ask nicely if they can add a test, and then the contributor disappears.", "tokens": [257, 1500, 11, 293, 550, 321, 1029, 9594, 498, 436, 393, 909, 257, 1500, 11, 293, 550, 264, 42859, 25527, 13], "temperature": 0.0, "avg_logprob": -0.18242272086765454, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.001221960410475731}, {"id": 171, "seek": 86676, "start": 873.3199999999999, "end": 882.0, "text": " So some of the times we're trying to improve on this thing and kind of like always get", "tokens": [407, 512, 295, 264, 1413, 321, 434, 1382, 281, 3470, 322, 341, 551, 293, 733, 295, 411, 1009, 483], "temperature": 0.0, "avg_logprob": -0.18242272086765454, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.001221960410475731}, {"id": 172, "seek": 86676, "start": 882.0, "end": 884.0, "text": " them in.", "tokens": [552, 294, 13], "temperature": 0.0, "avg_logprob": -0.18242272086765454, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.001221960410475731}, {"id": 173, "seek": 86676, "start": 884.0, "end": 889.08, "text": " So what we try to do is, if they truly disappear, we try to pick it up if it's a feature that", "tokens": [407, 437, 321, 853, 281, 360, 307, 11, 498, 436, 4908, 11596, 11, 321, 853, 281, 1888, 309, 493, 498, 309, 311, 257, 4111, 300], "temperature": 0.0, "avg_logprob": -0.18242272086765454, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.001221960410475731}, {"id": 174, "seek": 86676, "start": 889.08, "end": 891.84, "text": " we really want and we are bound with to do so.", "tokens": [321, 534, 528, 293, 321, 366, 5472, 365, 281, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.18242272086765454, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.001221960410475731}, {"id": 175, "seek": 89184, "start": 891.84, "end": 898.64, "text": " Once we try to reach out and kind of sit and help them write a test and kind of onboard", "tokens": [3443, 321, 853, 281, 2524, 484, 293, 733, 295, 1394, 293, 854, 552, 2464, 257, 1500, 293, 733, 295, 24033], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 176, "seek": 89184, "start": 898.64, "end": 905.52, "text": " them in this kind of things, one of the tests actually for our, there is an SSH feature.", "tokens": [552, 294, 341, 733, 295, 721, 11, 472, 295, 264, 6921, 767, 337, 527, 11, 456, 307, 364, 12238, 39, 4111, 13], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 177, "seek": 89184, "start": 905.52, "end": 911.6800000000001, "text": " And the test for that, I knew the developer and he was also in Norway, so once I was", "tokens": [400, 264, 1500, 337, 300, 11, 286, 2586, 264, 10754, 293, 415, 390, 611, 294, 24354, 11, 370, 1564, 286, 390], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 178, "seek": 89184, "start": 911.6800000000001, "end": 916.44, "text": " dropping by Oslo, we sat down for an afternoon and we worked on them together and paired", "tokens": [13601, 538, 8875, 752, 11, 321, 3227, 760, 337, 364, 6499, 293, 321, 2732, 322, 552, 1214, 293, 25699], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 179, "seek": 89184, "start": 916.44, "end": 917.44, "text": " on them.", "tokens": [322, 552, 13], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 180, "seek": 89184, "start": 917.44, "end": 919.36, "text": " That's not available for everyone, sadly.", "tokens": [663, 311, 406, 2435, 337, 1518, 11, 22023, 13], "temperature": 0.0, "avg_logprob": -0.15427668889363608, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.0015819695545360446}, {"id": 181, "seek": 91936, "start": 919.36, "end": 926.88, "text": " But you know, we always try to kind of like get this test message out there in a way.", "tokens": [583, 291, 458, 11, 321, 1009, 853, 281, 733, 295, 411, 483, 341, 1500, 3636, 484, 456, 294, 257, 636, 13], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 182, "seek": 91936, "start": 926.88, "end": 932.08, "text": " But there is a couple of other challenges as well, and that is that adding the test", "tokens": [583, 456, 307, 257, 1916, 295, 661, 4759, 382, 731, 11, 293, 300, 307, 300, 5127, 264, 1500], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 183, "seek": 91936, "start": 932.08, "end": 933.84, "text": " raises some sort of learning curve.", "tokens": [19658, 512, 1333, 295, 2539, 7605, 13], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 184, "seek": 91936, "start": 933.84, "end": 938.0, "text": " So you know, you need to know go test, you need to understand our test framework, you", "tokens": [407, 291, 458, 11, 291, 643, 281, 458, 352, 1500, 11, 291, 643, 281, 1223, 527, 1500, 8388, 11, 291], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 185, "seek": 91936, "start": 938.0, "end": 941.2, "text": " need to have Docker and all of this kind of thing, whereas it's not writing tests that", "tokens": [643, 281, 362, 33772, 293, 439, 295, 341, 733, 295, 551, 11, 9735, 309, 311, 406, 3579, 6921, 300], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 186, "seek": 91936, "start": 941.2, "end": 943.0, "text": " are a lot less code.", "tokens": [366, 257, 688, 1570, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 187, "seek": 91936, "start": 943.0, "end": 948.0, "text": " And it's hard to convince people how awesome tests actually really are, that they're not", "tokens": [400, 309, 311, 1152, 281, 13447, 561, 577, 3476, 6921, 767, 534, 366, 11, 300, 436, 434, 406], "temperature": 0.0, "avg_logprob": -0.19095062255859374, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0009353591594845057}, {"id": 188, "seek": 94800, "start": 948.0, "end": 954.68, "text": " really a chore and that you really, really thank yourself later for doing them.", "tokens": [534, 257, 14625, 293, 300, 291, 534, 11, 534, 1309, 1803, 1780, 337, 884, 552, 13], "temperature": 0.0, "avg_logprob": -0.14749450866992658, "compression_ratio": 1.744, "no_speech_prob": 0.0009782884735614061}, {"id": 189, "seek": 94800, "start": 954.68, "end": 958.44, "text": " So some of the things we're trying to do to even make this barrier lower, since we're", "tokens": [407, 512, 295, 264, 721, 321, 434, 1382, 281, 360, 281, 754, 652, 341, 13357, 3126, 11, 1670, 321, 434], "temperature": 0.0, "avg_logprob": -0.14749450866992658, "compression_ratio": 1.744, "no_speech_prob": 0.0009782884735614061}, {"id": 190, "seek": 94800, "start": 958.44, "end": 963.08, "text": " so heavily dependent on this for compatibility and everything, is that we're making like", "tokens": [370, 10950, 12334, 322, 341, 337, 34237, 293, 1203, 11, 307, 300, 321, 434, 1455, 411], "temperature": 0.0, "avg_logprob": -0.14749450866992658, "compression_ratio": 1.744, "no_speech_prob": 0.0009782884735614061}, {"id": 191, "seek": 94800, "start": 963.08, "end": 968.84, "text": " our own test framework, V2, because we depended on a lot of repeated and copied code and there", "tokens": [527, 1065, 1500, 8388, 11, 691, 17, 11, 570, 321, 1367, 3502, 322, 257, 688, 295, 10477, 293, 25365, 3089, 293, 456], "temperature": 0.0, "avg_logprob": -0.14749450866992658, "compression_ratio": 1.744, "no_speech_prob": 0.0009782884735614061}, {"id": 192, "seek": 94800, "start": 968.84, "end": 973.92, "text": " was a really high bar for adding new tests and it was really hard to update and change", "tokens": [390, 257, 534, 1090, 2159, 337, 5127, 777, 6921, 293, 309, 390, 534, 1152, 281, 5623, 293, 1319], "temperature": 0.0, "avg_logprob": -0.14749450866992658, "compression_ratio": 1.744, "no_speech_prob": 0.0009782884735614061}, {"id": 193, "seek": 97392, "start": 973.92, "end": 980.76, "text": " and it did depend on time.sleep, which was, yeah, haunted me so many times and it couldn't", "tokens": [293, 309, 630, 5672, 322, 565, 13, 82, 7927, 11, 597, 390, 11, 1338, 11, 24878, 385, 370, 867, 1413, 293, 309, 2809, 380], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 194, "seek": 97392, "start": 980.76, "end": 987.12, "text": " really be ran in parallel for many of the previous reasons and the documentation wasn't", "tokens": [534, 312, 5872, 294, 8952, 337, 867, 295, 264, 3894, 4112, 293, 264, 14333, 2067, 380], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 195, "seek": 97392, "start": 987.12, "end": 991.56, "text": " really good, like I knew how to use it, one knew how to use it and then that was about", "tokens": [534, 665, 11, 411, 286, 2586, 577, 281, 764, 309, 11, 472, 2586, 577, 281, 764, 309, 293, 550, 300, 390, 466], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 196, "seek": 97392, "start": 991.56, "end": 992.56, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 197, "seek": 97392, "start": 992.56, "end": 995.4799999999999, "text": " So a couple of other people figured it out.", "tokens": [407, 257, 1916, 295, 661, 561, 8932, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 198, "seek": 97392, "start": 995.4799999999999, "end": 999.36, "text": " So what we're trying to do is we're abstracting things a bit away, so we have this concept", "tokens": [407, 437, 321, 434, 1382, 281, 360, 307, 321, 434, 12649, 278, 721, 257, 857, 1314, 11, 370, 321, 362, 341, 3410], "temperature": 0.0, "avg_logprob": -0.1351854757829146, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0006057842401787639}, {"id": 199, "seek": 99936, "start": 999.36, "end": 1005.96, "text": " called control server, which is what essentially head scale is and the tail scale product,", "tokens": [1219, 1969, 7154, 11, 597, 307, 437, 4476, 1378, 4373, 307, 293, 264, 6838, 4373, 1674, 11], "temperature": 0.0, "avg_logprob": -0.16876779421411378, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0011862949468195438}, {"id": 200, "seek": 99936, "start": 1005.96, "end": 1010.52, "text": " the software as a service and it's implemented as like head scale in container and it exposes", "tokens": [264, 4722, 382, 257, 2643, 293, 309, 311, 12270, 382, 411, 1378, 4373, 294, 10129, 293, 309, 1278, 4201], "temperature": 0.0, "avg_logprob": -0.16876779421411378, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0011862949468195438}, {"id": 201, "seek": 99936, "start": 1010.52, "end": 1016.24, "text": " convenient functions that now have Godox support and all of these things to make it easier", "tokens": [10851, 6828, 300, 586, 362, 1265, 5230, 1406, 293, 439, 295, 613, 721, 281, 652, 309, 3571], "temperature": 0.0, "avg_logprob": -0.16876779421411378, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0011862949468195438}, {"id": 202, "seek": 99936, "start": 1016.24, "end": 1020.6800000000001, "text": " for developers to actually use it and then we have the tail scale client, which is implemented", "tokens": [337, 8849, 281, 767, 764, 309, 293, 550, 321, 362, 264, 6838, 4373, 6423, 11, 597, 307, 12270], "temperature": 0.0, "avg_logprob": -0.16876779421411378, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0011862949468195438}, {"id": 203, "seek": 99936, "start": 1020.6800000000001, "end": 1025.04, "text": " at tail scale in container and it has the same type of convenience functions and what", "tokens": [412, 6838, 4373, 294, 10129, 293, 309, 575, 264, 912, 2010, 295, 19283, 6828, 293, 437], "temperature": 0.0, "avg_logprob": -0.16876779421411378, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0011862949468195438}, {"id": 204, "seek": 102504, "start": 1025.04, "end": 1031.96, "text": " this allows us to do is previously the two files on the right here, sorry, on the left,", "tokens": [341, 4045, 505, 281, 360, 307, 8046, 264, 732, 7098, 322, 264, 558, 510, 11, 2597, 11, 322, 264, 1411, 11], "temperature": 0.0, "avg_logprob": -0.139684257157352, "compression_ratio": 1.7578125, "no_speech_prob": 0.0005896330112591386}, {"id": 205, "seek": 102504, "start": 1031.96, "end": 1038.36, "text": " is two different versions of the setup code for the tests because when you needed something", "tokens": [307, 732, 819, 9606, 295, 264, 8657, 3089, 337, 264, 6921, 570, 562, 291, 2978, 746], "temperature": 0.0, "avg_logprob": -0.139684257157352, "compression_ratio": 1.7578125, "no_speech_prob": 0.0005896330112591386}, {"id": 206, "seek": 102504, "start": 1038.36, "end": 1043.8, "text": " that was slightly special, you had to copy the whole thing and then make a new file to", "tokens": [300, 390, 4748, 2121, 11, 291, 632, 281, 5055, 264, 1379, 551, 293, 550, 652, 257, 777, 3991, 281], "temperature": 0.0, "avg_logprob": -0.139684257157352, "compression_ratio": 1.7578125, "no_speech_prob": 0.0005896330112591386}, {"id": 207, "seek": 102504, "start": 1043.8, "end": 1048.28, "text": " be able to write a test case like you see on the other side here, but now after abstracting", "tokens": [312, 1075, 281, 2464, 257, 1500, 1389, 411, 291, 536, 322, 264, 661, 1252, 510, 11, 457, 586, 934, 12649, 278], "temperature": 0.0, "avg_logprob": -0.139684257157352, "compression_ratio": 1.7578125, "no_speech_prob": 0.0005896330112591386}, {"id": 208, "seek": 102504, "start": 1048.28, "end": 1053.48, "text": " that away, making it a lot more configurable, we allow people to write more or less regular", "tokens": [300, 1314, 11, 1455, 309, 257, 688, 544, 22192, 712, 11, 321, 2089, 561, 281, 2464, 544, 420, 1570, 3890], "temperature": 0.0, "avg_logprob": -0.139684257157352, "compression_ratio": 1.7578125, "no_speech_prob": 0.0005896330112591386}, {"id": 209, "seek": 105348, "start": 1053.48, "end": 1058.52, "text": " test cases, but you just set up what we call a scenario, which is a head scale with a given", "tokens": [1500, 3331, 11, 457, 291, 445, 992, 493, 437, 321, 818, 257, 9005, 11, 597, 307, 257, 1378, 4373, 365, 257, 2212], "temperature": 0.0, "avg_logprob": -0.11306784851382477, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017208774806931615}, {"id": 210, "seek": 105348, "start": 1058.52, "end": 1064.56, "text": " amount of tail scale nodes and then you let them ping each other or something like this.", "tokens": [2372, 295, 6838, 4373, 13891, 293, 550, 291, 718, 552, 26151, 1184, 661, 420, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11306784851382477, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017208774806931615}, {"id": 211, "seek": 105348, "start": 1064.56, "end": 1067.08, "text": " So what do we test right now?", "tokens": [407, 437, 360, 321, 1500, 558, 586, 30], "temperature": 0.0, "avg_logprob": -0.11306784851382477, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017208774806931615}, {"id": 212, "seek": 105348, "start": 1067.08, "end": 1073.6, "text": " We tried to, we kept all of the original tests, so basically we make all nodes, join the network", "tokens": [492, 3031, 281, 11, 321, 4305, 439, 295, 264, 3380, 6921, 11, 370, 1936, 321, 652, 439, 13891, 11, 3917, 264, 3209], "temperature": 0.0, "avg_logprob": -0.11306784851382477, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017208774806931615}, {"id": 213, "seek": 105348, "start": 1073.6, "end": 1077.6, "text": " and we make them ping each other to verify that we have a fully functioning network both", "tokens": [293, 321, 652, 552, 26151, 1184, 661, 281, 16888, 300, 321, 362, 257, 4498, 18483, 3209, 1293], "temperature": 0.0, "avg_logprob": -0.11306784851382477, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017208774806931615}, {"id": 214, "seek": 107760, "start": 1077.6, "end": 1084.0, "text": " by IP and magic DNS, magic DNS is tail scales DNS system.", "tokens": [538, 8671, 293, 5585, 35153, 11, 5585, 35153, 307, 6838, 17408, 35153, 1185, 13], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 215, "seek": 107760, "start": 1084.0, "end": 1088.8799999999999, "text": " We test tail drop, which is a file sharing features, a bit like Apple's airdrop and we", "tokens": [492, 1500, 6838, 3270, 11, 597, 307, 257, 3991, 5414, 4122, 11, 257, 857, 411, 6373, 311, 257, 1271, 1513, 293, 321], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 216, "seek": 107760, "start": 1088.8799999999999, "end": 1093.56, "text": " send the files from every node to every node to make sure that they work.", "tokens": [2845, 264, 7098, 490, 633, 9984, 281, 633, 9984, 281, 652, 988, 300, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 217, "seek": 107760, "start": 1093.56, "end": 1097.84, "text": " We test all our registration flows because we broken them a couple of times, so it was", "tokens": [492, 1500, 439, 527, 16847, 12867, 570, 321, 5463, 552, 257, 1916, 295, 1413, 11, 370, 309, 390], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 218, "seek": 107760, "start": 1097.84, "end": 1103.56, "text": " better to do it that way, which is pre-authored keys and web plus a command line flow and", "tokens": [1101, 281, 360, 309, 300, 636, 11, 597, 307, 659, 12, 40198, 2769, 9317, 293, 3670, 1804, 257, 5622, 1622, 3095, 293], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 219, "seek": 107760, "start": 1103.56, "end": 1106.8, "text": " even open ID we currently have tests for.", "tokens": [754, 1269, 7348, 321, 4362, 362, 6921, 337, 13], "temperature": 0.0, "avg_logprob": -0.15084760914678158, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.00036595031269825995}, {"id": 220, "seek": 110680, "start": 1106.8, "end": 1111.52, "text": " We try to isolate all of our network from the internet and test with our own embedded", "tokens": [492, 853, 281, 25660, 439, 295, 527, 3209, 490, 264, 4705, 293, 1500, 365, 527, 1065, 16741], "temperature": 0.0, "avg_logprob": -0.16648848160453464, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0006336519145406783}, {"id": 221, "seek": 110680, "start": 1111.52, "end": 1116.76, "text": " relay server because tail scale depends on some relay servers that we also embed in", "tokens": [24214, 7154, 570, 6838, 4373, 5946, 322, 512, 24214, 15909, 300, 321, 611, 12240, 294], "temperature": 0.0, "avg_logprob": -0.16648848160453464, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0006336519145406783}, {"id": 222, "seek": 110680, "start": 1116.76, "end": 1125.0, "text": " our binary and we have preliminary tests for the SSH features that we support, which is", "tokens": [527, 17434, 293, 321, 362, 28817, 6921, 337, 264, 12238, 39, 4122, 300, 321, 1406, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.16648848160453464, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0006336519145406783}, {"id": 223, "seek": 110680, "start": 1125.0, "end": 1130.8, "text": " like authenticated by head scale so you can SSH into your machine and we test SSH all", "tokens": [411, 9214, 3587, 538, 1378, 4373, 370, 291, 393, 12238, 39, 666, 428, 3479, 293, 321, 1500, 12238, 39, 439], "temperature": 0.0, "avg_logprob": -0.16648848160453464, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0006336519145406783}, {"id": 224, "seek": 110680, "start": 1130.8, "end": 1133.08, "text": " to all and we try to do negative tests.", "tokens": [281, 439, 293, 321, 853, 281, 360, 3671, 6921, 13], "temperature": 0.0, "avg_logprob": -0.16648848160453464, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0006336519145406783}, {"id": 225, "seek": 113308, "start": 1133.08, "end": 1138.76, "text": " And also we test our CLI because if you may change something, you don't want to sit and", "tokens": [400, 611, 321, 1500, 527, 12855, 40, 570, 498, 291, 815, 1319, 746, 11, 291, 500, 380, 528, 281, 1394, 293], "temperature": 0.0, "avg_logprob": -0.1383223051435492, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.00050436268793419}, {"id": 226, "seek": 113308, "start": 1138.76, "end": 1144.76, "text": " type in every single command in a structured way manually because that's just painful.", "tokens": [2010, 294, 633, 2167, 5622, 294, 257, 18519, 636, 16945, 570, 300, 311, 445, 11697, 13], "temperature": 0.0, "avg_logprob": -0.1383223051435492, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.00050436268793419}, {"id": 227, "seek": 113308, "start": 1144.76, "end": 1149.48, "text": " So in the future, we want to also kind of improve this granular access control that", "tokens": [407, 294, 264, 2027, 11, 321, 528, 281, 611, 733, 295, 3470, 341, 39962, 2105, 1969, 300], "temperature": 0.0, "avg_logprob": -0.1383223051435492, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.00050436268793419}, {"id": 228, "seek": 113308, "start": 1149.48, "end": 1151.12, "text": " tail scale offer.", "tokens": [6838, 4373, 2626, 13], "temperature": 0.0, "avg_logprob": -0.1383223051435492, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.00050436268793419}, {"id": 229, "seek": 113308, "start": 1151.12, "end": 1157.48, "text": " Currently this is a very good example of where we have added a lot of unit tests and they", "tokens": [19964, 341, 307, 257, 588, 665, 1365, 295, 689, 321, 362, 3869, 257, 688, 295, 4985, 6921, 293, 436], "temperature": 0.0, "avg_logprob": -0.1383223051435492, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.00050436268793419}, {"id": 230, "seek": 115748, "start": 1157.48, "end": 1164.32, "text": " all pass, but they're all wrong, so well, they're mostly wrong, so we have to kind of", "tokens": [439, 1320, 11, 457, 436, 434, 439, 2085, 11, 370, 731, 11, 436, 434, 5240, 2085, 11, 370, 321, 362, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.14525733331237176, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.0006722795660607517}, {"id": 231, "seek": 115748, "start": 1164.32, "end": 1170.56, "text": " redo most of this into integration test first and then kind of backfill the unit test once", "tokens": [29956, 881, 295, 341, 666, 10980, 1500, 700, 293, 550, 733, 295, 646, 31072, 264, 4985, 1500, 1564], "temperature": 0.0, "avg_logprob": -0.14525733331237176, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.0006722795660607517}, {"id": 232, "seek": 115748, "start": 1170.56, "end": 1174.0, "text": " we know how the implementation is actually supposed to work.", "tokens": [321, 458, 577, 264, 11420, 307, 767, 3442, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.14525733331237176, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.0006722795660607517}, {"id": 233, "seek": 115748, "start": 1174.0, "end": 1178.0, "text": " And one of the things we've been dabbling with, especially for this ACL feature, is", "tokens": [400, 472, 295, 264, 721, 321, 600, 668, 274, 10797, 1688, 365, 11, 2318, 337, 341, 43873, 4111, 11, 307], "temperature": 0.0, "avg_logprob": -0.14525733331237176, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.0006722795660607517}, {"id": 234, "seek": 115748, "start": 1178.0, "end": 1183.2, "text": " to use that control server abstraction we have before and use the tail scale product", "tokens": [281, 764, 300, 1969, 7154, 37765, 321, 362, 949, 293, 764, 264, 6838, 4373, 1674], "temperature": 0.0, "avg_logprob": -0.14525733331237176, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.0006722795660607517}, {"id": 235, "seek": 118320, "start": 1183.2, "end": 1188.2, "text": " to test our tests because if they pass on the public server, we know they're correct", "tokens": [281, 1500, 527, 6921, 570, 498, 436, 1320, 322, 264, 1908, 7154, 11, 321, 458, 436, 434, 3006], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 236, "seek": 118320, "start": 1188.2, "end": 1190.8400000000001, "text": " and then we can use them to verify our thing.", "tokens": [293, 550, 321, 393, 764, 552, 281, 16888, 527, 551, 13], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 237, "seek": 118320, "start": 1190.8400000000001, "end": 1194.64, "text": " And then maybe run tail scale in the VM instead of Docker to test it properly, but that's", "tokens": [400, 550, 1310, 1190, 6838, 4373, 294, 264, 18038, 2602, 295, 33772, 281, 1500, 309, 6108, 11, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 238, "seek": 118320, "start": 1194.64, "end": 1199.1200000000001, "text": " more of a benefit for tail scale than it is for us.", "tokens": [544, 295, 257, 5121, 337, 6838, 4373, 813, 309, 307, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 239, "seek": 118320, "start": 1199.1200000000001, "end": 1205.3600000000001, "text": " So if you're just here waiting for the next talk, a little bit of a TLDR is that, I mean,", "tokens": [407, 498, 291, 434, 445, 510, 3806, 337, 264, 958, 751, 11, 257, 707, 857, 295, 257, 40277, 9301, 307, 300, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 240, "seek": 118320, "start": 1205.3600000000001, "end": 1209.88, "text": " we cannot understate how important having this integration testing when we depend on", "tokens": [321, 2644, 833, 15406, 577, 1021, 1419, 341, 10980, 4997, 562, 321, 5672, 322], "temperature": 0.0, "avg_logprob": -0.1431573950726053, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0005701452610082924}, {"id": 241, "seek": 120988, "start": 1209.88, "end": 1215.44, "text": " an external party has been for the development of health scale.", "tokens": [364, 8320, 3595, 575, 668, 337, 264, 3250, 295, 1585, 4373, 13], "temperature": 0.0, "avg_logprob": -0.3178644895553589, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00041188683826476336}, {"id": 242, "seek": 120988, "start": 1215.44, "end": 1220.88, "text": " I reckon also the head, like the name, is also excellent, ponytail scale would have", "tokens": [286, 29548, 611, 264, 1378, 11, 411, 264, 1315, 11, 307, 611, 7103, 11, 49138, 4373, 576, 362], "temperature": 0.0, "avg_logprob": -0.3178644895553589, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00041188683826476336}, {"id": 243, "seek": 120988, "start": 1220.88, "end": 1224.44, "text": " been worse.", "tokens": [668, 5324, 13], "temperature": 0.0, "avg_logprob": -0.3178644895553589, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00041188683826476336}, {"id": 244, "seek": 120988, "start": 1224.44, "end": 1228.96, "text": " We have, I mean, with integration testing, we are able to maintain this compatibility", "tokens": [492, 362, 11, 286, 914, 11, 365, 10980, 4997, 11, 321, 366, 1075, 281, 6909, 341, 34237], "temperature": 0.0, "avg_logprob": -0.3178644895553589, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00041188683826476336}, {"id": 245, "seek": 120988, "start": 1228.96, "end": 1237.0, "text": " with the client and we are able to take contributions from third party developers, otherwise it's", "tokens": [365, 264, 6423, 293, 321, 366, 1075, 281, 747, 15725, 490, 2636, 3595, 8849, 11, 5911, 309, 311], "temperature": 0.0, "avg_logprob": -0.3178644895553589, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00041188683826476336}, {"id": 246, "seek": 123700, "start": 1237.0, "end": 1241.76, "text": " a little bit more difficult to develop this trust across the internet, right?", "tokens": [257, 707, 857, 544, 2252, 281, 1499, 341, 3361, 2108, 264, 4705, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21760718822479247, "compression_ratio": 1.5627906976744186, "no_speech_prob": 0.0009763730340637267}, {"id": 247, "seek": 123700, "start": 1241.76, "end": 1246.88, "text": " And even though the tests are not perfect and we still have to migrate unit tests towards", "tokens": [400, 754, 1673, 264, 6921, 366, 406, 2176, 293, 321, 920, 362, 281, 31821, 4985, 6921, 3030], "temperature": 0.0, "avg_logprob": -0.21760718822479247, "compression_ratio": 1.5627906976744186, "no_speech_prob": 0.0009763730340637267}, {"id": 248, "seek": 123700, "start": 1246.88, "end": 1256.28, "text": " integration tests, I think this is one of the keys for the success of the project.", "tokens": [10980, 6921, 11, 286, 519, 341, 307, 472, 295, 264, 9317, 337, 264, 2245, 295, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.21760718822479247, "compression_ratio": 1.5627906976744186, "no_speech_prob": 0.0009763730340637267}, {"id": 249, "seek": 123700, "start": 1256.28, "end": 1263.0, "text": " So some extra things, tail scale is hosting a happy hour at a brewdog by the station.", "tokens": [407, 512, 2857, 721, 11, 6838, 4373, 307, 16058, 257, 2055, 1773, 412, 257, 34619, 14833, 538, 264, 5214, 13], "temperature": 0.0, "avg_logprob": -0.21760718822479247, "compression_ratio": 1.5627906976744186, "no_speech_prob": 0.0009763730340637267}, {"id": 250, "seek": 126300, "start": 1263.0, "end": 1267.72, "text": " This QR code takes you to a sign up form, I'll quickly switch back to this slide at", "tokens": [639, 32784, 3089, 2516, 291, 281, 257, 1465, 493, 1254, 11, 286, 603, 2661, 3679, 646, 281, 341, 4137, 412], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 251, "seek": 126300, "start": 1267.72, "end": 1272.32, "text": " the end, but I have like a question slide as well, so, you know, we go through this.", "tokens": [264, 917, 11, 457, 286, 362, 411, 257, 1168, 4137, 382, 731, 11, 370, 11, 291, 458, 11, 321, 352, 807, 341, 13], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 252, "seek": 126300, "start": 1272.32, "end": 1277.72, "text": " Basically this is how to reach us, Github, we have a Discord community, and we're very", "tokens": [8537, 341, 307, 577, 281, 2524, 505, 11, 460, 355, 836, 11, 321, 362, 257, 32623, 1768, 11, 293, 321, 434, 588], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 253, "seek": 126300, "start": 1277.72, "end": 1281.6, "text": " happy to talk to anyone who wants to talk to us here at Fostem, so please feel free", "tokens": [2055, 281, 751, 281, 2878, 567, 2738, 281, 751, 281, 505, 510, 412, 479, 555, 443, 11, 370, 1767, 841, 1737], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 254, "seek": 126300, "start": 1281.6, "end": 1286.72, "text": " to reach out and I'll leave it at this one if anyone has any questions.", "tokens": [281, 2524, 484, 293, 286, 603, 1856, 309, 412, 341, 472, 498, 2878, 575, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 255, "seek": 126300, "start": 1286.72, "end": 1290.24, "text": " We have some minutes, I think.", "tokens": [492, 362, 512, 2077, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.2187821502685547, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.00245461892336607}, {"id": 256, "seek": 129024, "start": 1290.24, "end": 1297.56, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 257, "seek": 129024, "start": 1297.56, "end": 1302.0, "text": " While I have your attention, we have a go for that lost there wallet, look to the left,", "tokens": [3987, 286, 362, 428, 3202, 11, 321, 362, 257, 352, 337, 300, 2731, 456, 16599, 11, 574, 281, 264, 1411, 11], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 258, "seek": 129024, "start": 1302.0, "end": 1306.32, "text": " look to the right, front and back, if you see a wallet that is not yours, please come", "tokens": [574, 281, 264, 558, 11, 1868, 293, 646, 11, 498, 291, 536, 257, 16599, 300, 307, 406, 6342, 11, 1767, 808], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 259, "seek": 129024, "start": 1306.32, "end": 1309.2, "text": " right to the front, it will help this person a lot.", "tokens": [558, 281, 264, 1868, 11, 309, 486, 854, 341, 954, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 260, "seek": 129024, "start": 1309.2, "end": 1310.2, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 261, "seek": 129024, "start": 1310.2, "end": 1316.92, "text": " After you look for the wallet and you have a question, raise your hand and I'll try", "tokens": [2381, 291, 574, 337, 264, 16599, 293, 291, 362, 257, 1168, 11, 5300, 428, 1011, 293, 286, 603, 853], "temperature": 0.0, "avg_logprob": -0.19858617985502203, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.005174328573048115}, {"id": 262, "seek": 131692, "start": 1316.92, "end": 1327.04, "text": " to come with this microphone.", "tokens": [281, 808, 365, 341, 10952, 13], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 263, "seek": 131692, "start": 1327.04, "end": 1331.72, "text": " How come the tail scale guys are not mad at you, and not only are not mad at you, but", "tokens": [1012, 808, 264, 6838, 4373, 1074, 366, 406, 5244, 412, 291, 11, 293, 406, 787, 366, 406, 5244, 412, 291, 11, 457], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 264, "seek": 131692, "start": 1331.72, "end": 1336.3200000000002, "text": " they hurt you afterwards.", "tokens": [436, 4607, 291, 10543, 13], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 265, "seek": 131692, "start": 1336.3200000000002, "end": 1339.04, "text": " I mean, part of it, is it working?", "tokens": [286, 914, 11, 644, 295, 309, 11, 307, 309, 1364, 30], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 266, "seek": 131692, "start": 1339.04, "end": 1340.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 267, "seek": 131692, "start": 1340.04, "end": 1341.04, "text": " No?", "tokens": [883, 30], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 268, "seek": 131692, "start": 1341.04, "end": 1342.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 269, "seek": 131692, "start": 1342.04, "end": 1345.44, "text": " I think part of it is that they are quite chill, I mean, they could have, they are quite", "tokens": [286, 519, 644, 295, 309, 307, 300, 436, 366, 1596, 11355, 11, 286, 914, 11, 436, 727, 362, 11, 436, 366, 1596], "temperature": 0.0, "avg_logprob": -0.3253164503309462, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.021614598110318184}, {"id": 270, "seek": 134544, "start": 1345.44, "end": 1351.64, "text": " chill, they could have taken this way worse than they have, and I don't think we are competition.", "tokens": [11355, 11, 436, 727, 362, 2726, 341, 636, 5324, 813, 436, 362, 11, 293, 286, 500, 380, 519, 321, 366, 6211, 13], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 271, "seek": 134544, "start": 1351.64, "end": 1358.0800000000002, "text": " We are focused on self-hostors, on home labs, perhaps a little bit of a small company.", "tokens": [492, 366, 5178, 322, 2698, 12, 6037, 830, 11, 322, 1280, 20339, 11, 4317, 257, 707, 857, 295, 257, 1359, 2237, 13], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 272, "seek": 134544, "start": 1358.0800000000002, "end": 1363.44, "text": " And what usually happens is that people that use headscales at home, then they go to their", "tokens": [400, 437, 2673, 2314, 307, 300, 561, 300, 764, 1378, 4417, 4229, 412, 1280, 11, 550, 436, 352, 281, 641], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 273, "seek": 134544, "start": 1363.44, "end": 1367.6000000000001, "text": " companies and they talk about tail scale, and when you're in a company, you actually", "tokens": [3431, 293, 436, 751, 466, 6838, 4373, 11, 293, 562, 291, 434, 294, 257, 2237, 11, 291, 767], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 274, "seek": 134544, "start": 1367.6000000000001, "end": 1370.64, "text": " prefer to pay for the service.", "tokens": [4382, 281, 1689, 337, 264, 2643, 13], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 275, "seek": 134544, "start": 1370.64, "end": 1373.0800000000002, "text": " So it's like a way of...", "tokens": [407, 309, 311, 411, 257, 636, 295, 485], "temperature": 0.0, "avg_logprob": -0.2991456384057397, "compression_ratio": 1.657370517928287, "no_speech_prob": 0.006884461268782616}, {"id": 276, "seek": 137308, "start": 1373.08, "end": 1386.28, "text": " It's like a way of selling headscales, sorry, headscales also.", "tokens": [467, 311, 411, 257, 636, 295, 6511, 1378, 4417, 4229, 11, 2597, 11, 1378, 4417, 4229, 611, 13], "temperature": 0.0, "avg_logprob": -0.3502613350197121, "compression_ratio": 1.3211678832116789, "no_speech_prob": 0.007822711020708084}, {"id": 277, "seek": 137308, "start": 1386.28, "end": 1387.28, "text": " Okay, thank you very much.", "tokens": [1033, 11, 1309, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.3502613350197121, "compression_ratio": 1.3211678832116789, "no_speech_prob": 0.007822711020708084}, {"id": 278, "seek": 137308, "start": 1387.28, "end": 1388.28, "text": " Last round of applause.", "tokens": [5264, 3098, 295, 9969, 13], "temperature": 0.0, "avg_logprob": -0.3502613350197121, "compression_ratio": 1.3211678832116789, "no_speech_prob": 0.007822711020708084}, {"id": 279, "seek": 138828, "start": 1388.28, "end": 1404.04, "text": " If you have any questions, you can card them in the hallway track.", "tokens": [759, 291, 362, 604, 1651, 11, 291, 393, 2920, 552, 294, 264, 23903, 2837, 13], "temperature": 0.0, "avg_logprob": -0.4362763856586657, "compression_ratio": 1.0, "no_speech_prob": 0.0052116247825324535}], "language": "en"}