{"text": " Okay. Hello, everyone, and welcome to our talk today. Yeah, so today we're going to talk about real-time analytics application with Apache Pino and Apache Pulsar and Pino. So here today too is myself. I'm Mary Grigleski. I'm a streaming developer advocate at Data Stacks. A company is actually primarily doing Apache Cassandra up to this point. And now we're going to be doing more Apache Pulsar. And it's a streaming event, streaming platform that's kind of optimized for the cloud-native platform. I'm based in Chicago. I'm also Java champion and president of Chicago Java user script too, and blah, blah, blah, all these things. I was a developer before too, just so you know, mostly in Java. So now we have Mark. So we have Mark introduce. Hello. And I do realize, Javier, that we've stolen your intro section. So we've got, we've gone straight and say, yeah, hi. I'm Mark. I work at Starchy. We do Apache Pino. I'm a developer advocate there. And so, yeah, like we kind of had on the first slide, we're going to be showing you how to, yeah, how to, I guess maybe more, how to build a real-time like analytics dashboard with Pulsar, Pino, and then Python dashboard library called Streamlit. So we're going to see half the talk will be that. And we're going to see how well does the Wi-Fi survive our attempts to use live data. So let's see. Let's hope the demo gods are in the room. So I guess first things to start with is to define like what exactly does this mean? What is real-time analytics? So we've seen lots of talks, right, showing streaming data. So obviously that's like a big part of it. But the real-time analytics bit is kind of this bit in highlighted. So the goal is we're trying to provide insights to make decisions quickly. And I guess the most important bit is that farsight. So like, it's cool that we've got the data. We can capture our IoT data. We can capture like orders coming from a website. We can capture the logs, but we want to do something, do something with it. And so if we move on from the definition, like all the talks we've seen so far, they focus on events. We've got events representing stuff. So someone is purchasing something. Someone wants to do a search query. Someone is taking a taxi ride like we've seen in Javier's talk. And those events are cool on their own, right? But we really want to get some insights from them. Like what can we do? And eventually that leads to, okay, we've got some insight and we do something as a result of knowing that this is happening. So like say, we know someone is searching for a pizza. Okay, let's get a pizza shop in front of them if we're doing Google Adverts. If someone's then going in and buying the pizza, okay, let's show them in real time what are the things that people have also been buying along with that pizza or that hamburger that we can suggest like in the flow. So we want to try and react to those events coming in. So we've seen like lots of tools. We've seen Beam. We've seen Flink. We've seen like lots of tools for getting the streaming data, but we want to do something with that data. Like that's the whole purpose, I guess, of all the applications that we're building. And in the real time analytics space, like this is like, you imagine the value of data over time in this world where we're trying to do something with that streaming data, the value of that data goes down over time. So if we know like today, like, hey, you made an order and something's gone wrong with it, I can like try and do something to make you happy, like give you a voucher or call you up and try and fix it. If I find out like when I batch process that tomorrow, it's like, oh, you already hate me. So it's too late. And so we're kind of focused on the left side of this diagram. So in terms of real time analytics, we want the data like close to when it's coming in, maybe not exactly like, not not exactly like when it comes in, like in the time just after that, that's the kind of region that we're living. And there can be lots of people who are interested in this data. So it could be the analysts inside a company. So maybe it's people like, in our imaginary pizza shop, like they're actually like running, running the, running the operations with the pizza shop. It might be the management. They're like, hey, I want to know what's going on now. What's the, what's the revenue that we're seeing like right now, like we're in the last 10 minutes or whatever it is. And it could be the users. And so that's, that's kind of the interesting thing that we, that there's sort of changed, I guess, from doing traditional analytics to doing this real time stuff is that the data is almost coming back that users are creating the data and then we're feeding it back to them in terms of products that they can then use. And yeah, I guess that's more or less what I wanted to say. And so when we're building these applications, there are kind of, I mean, they're not, they're not strictly like this, but there's sort of four obvious quadrants of applications that people build. So they go along, oh yes, I should show you, they go along two axes. So we've got human facing and machine facing along the Y and then internal and external on the X. So if we go in the top side, that would be like the observability area. And actually a lot of the time series querying would be in there. And this would be sort of the area of like data dogs. So hey, like I wanted to get the met, I've got like all this telemetry data coming in and I want to know what's going on, like what's going on in my, in my AWS cluster. What's happening like with all my Lambda functions? Like are there any that are suddenly really slow? Like can I, can I like figure that out? And maybe there's a machine that's actually interpreting that data rather than a human is looking at it and going, oh yeah, it's that one Lambda function there that's really slow. Probably be feeding it into like, like some other tool that's figuring it out. If we come down here, so this is where we're going to be today. So imagine this is a dashboard and obviously I'm sure you've seen loads of dashboards, you've probably seen Tableau, you've seen loads of BI tools and with a lot of them, the data that you're using is maybe like yesterday's, yesterday's data. And so what we're going to show you today is how could you build one that is like updating as, as new data is coming in. If we come up to the, to the top left, this is now the machine is processing the data, but it's for the users. So this would be like how Javier was talking. So we've got like the fraud detection system. So the data's coming in, it's being processed by something and then I'm seeing, I'm seeing the result, but maybe I'm not going and working out like with my own query. Oh, look, I wrote this really, really clever query. Here's the, here's the thing. Maybe there's some, some pre-processing happening by some sort of machine learning algorithm. And then if we come down into the bottom corner, be some sort of external service that could be, yeah, like in our piece, for example, like an order tracking service. Like, you know how on your phone you order something from, I don't know what's this food delivery service here just eats. Maybe you see like, Hey, look, I can see exactly where it is. How far away is it? Why on earth has the driver gone the wrong way to my house? You can see all that sort of, all that sort of information. Maybe too much. Maybe they should just show you. It's always coming towards your house. It never went like the absolute opposite way and got to your house an hour later. So just to show you some real time, real, real world examples of where this is, where this is used. So, so LinkedIn is one. So the, who viewed your profile, I guess most of you have probably seen this and you see like, I guess for people spying, spying on you. And if you, if you have, you can kind of see like all the people that look to you. And it's, it's very, it's very up to date, right? Like if I went and viewed one of your profile pages, you would see it straight away. Like, Hey, look, Mark looks at it. Use for that would be, Hey, maybe someone is in like, you often, I guess it's often recruiters, right? You're like, Oh, I wonder why that person is following them. I'm going to, I'm going to contact them. So it's almost like, is there a real time way of like interacting with someone? I don't know if you wanted to collaborate with them on something or yeah, I guess they got a job that's available. They use it in the news field as well. So I guess here is similar to what you would see in, I guess in Facebook, I guess even in TikTok and those sorts of tools. The goal is kind of make you interact with, with this product more like they want you to stay on it. So they need to show you what is happening now so that you're going to stay on it and not, and not, yeah, I guess to not go away and do something else, which potentially is more useful, but they want you to just stay on there. And then yeah, for LinkedIn, like this one's, I guess this one's like a little bit of a, a smaller user base, but yeah, for the recruiters, I can see like, okay, what is the trends of what is happening in terms of what jobs are available, what places those are in the world and so on. And then Uber Eats, let's do one more example. So Uber Eats is another one. So this one's kind of like what I was saying. So they've got a dash, this is a very, very much a dashboard approach. And this is for a restaurant manager. So if they were hosting the restaurant on there, and you can see like the things that are interesting are they would get like missed orders. So hey, we've made a mess of this order. Can we fix it like now rather than waiting till tomorrow? We've got this order that's gone wrong. Can we, can we go and fix it? It's almost like you're able to achieve like the customer service that you are in a restaurant where you can kind of see like with your eyes, okay, these people look really angry with me. It's like, hey, look, the data is being shown is almost giving you the equivalent of the, in the restaurant experience without being in the restaurant. Okay, so what, how do we go about, so those are some examples of people who have built those things and they're, they're a way more of them. Those are just some, some ones that are picked up. How do we, how do we build that? So there are some properties that we need to, we need to achieve. And some of these Javier was talking about in his talks. First one is we want to be able to get the data in quickly where it is in these applications that generally coming from a streaming data platform of, of some sorts. In our talk, it's going to be Pulsar and we need to be able to get the data into Pulsar and then into like wherever we want to get it to query it, in this case, into Pina, we need to get it in there very, very quickly. Once it's in there, we want to be able to query it very quickly as well. So one way of thinking about it is in these applications, we want to do OLTP type queries, like query speeds on OLAP data. So we want to be querying everything but getting like the results in, in like, imagine like a refresh on a web page or like on a page on a web, on a mobile app. So I don't want to, don't want to be sitting there waiting for five or 10 seconds for the results, right? And that, that, that particular requirement is a lot more the case when it's an external user, right? Like if it's inside a company, because it's fine, you can just go and get a coffee and wait for the results. But if it's outside, you're not going to, you're not going to do that. They're going to use, use another application instead. And then finally, we want to be able to scale it, right? So either it could be like one, one dashboard doing loads of different queries and kind of aggregate and bringing everything together into one view or it's a lot, maybe lots of users like concurrently doing it. But end result is lots of queries are coming in. We need to be able to handle those and it can't affect those other two things either. So we need to be able to still be like doing lots of those concurrent queries while ingesting big amounts of data very quickly. So how do we go about building one of those? So these are, we kind of got around the outside some of the properties that you would have. And then in the middle, we've got a couple of tools that can achieve this. So in this case, Pulsar and Pino. And so you can kind of see we want to achieve real-time ingestion. The data will often be like very wide, like lots of, lots of columns, lots of properties potentially nested. And then we've got to do something with it to figure out how we're going to get it into a structure that we can query it. And then yeah, you can kind of see some of them. So we need to be able to, the data needs to be fresh. We want to do thousands of queries a second. And then the latency needs to be like OLTP style. So I'm going to hand the microphone back to Mary now. Okay. Thank you. Okay. Thank you, Mark. So, okay. So now we're going to focus just couple minutes on Apache Pulsar. So Pulsar, right? How many of you actually real quickly have heard of Pulsar or working? Oh, working. Okay, cool. So some of you have, and then of course there are also folks using Kafka too. But here I'm wanting to say, to tell you why we want to use Pulsar, right? So right now in here too, for those of you who are new and essentially to Pulsar, there are like a couple components to it, but primarily to their brokers that are serverless Java runtime, right? And running, but it's very flexible too. So for clients, you are supposed to be writing your producer and consumer, it takes on a pop-up type of architectural pattern, right? So I won't have time to get into all the details, but just give you a highlight, right? Producer, consumer is what you write. You can write it in Java, in Go, in Python, in any kind of languages that are supported by the community too. And then multiple brokers that are running and also is optimized too to run in the cloud native environment. Also too, instead of it managing all of the huge amounts of log messages, it actually leveraged on Bookkey, which is a patchy bookkeeper project. And it's also like a high availability or fast read and fast write type of log, logging, distributed logging system. Then it also makes use of Zookeeper to help it to manage the cluster, that aspect of things. And now really quick to kind of give you an introduction. Pulsar was developed first by Yahoo back in 2013 or so, and it's basically recognizing we need an event streaming platform that can run very effectively in a cloud native environment. They contributed to Apache Software Foundation in 2016 and then very quickly became a top level project in 2018. And again, it's very cloud native in nature. It's already cluster based. Multitenancy is supported too. Again, I talked already about the simple client APIs that you can write in many different languages. And it separates, one of the strengths of Pulsar is that it separates out the compute and the storage. So Pulsar manage all of the message things and then have Bookkeeper manage all of the log messages and stuff. And then also Pulsar has guaranteed message delivery. And also it has a Pulsar function framework that's very lightweight too. So then you don't need to rely on any external libraries or vendors essentially to kind of do message transformation as you are constructing your data pipeline too. And also another feature about it is that it has tiered storage offload. So if there's messages that becomes cold, then you can move it off to, or actually if it kind of becomes cold, then it gets moved off to offline storage such as like S3 buckets and things like that. Okay, so just real quick slide just to show too like about streaming and not versus not streaming. So in modern day streaming, as you can see, we're ingesting data and this is what we'll be ingesting for like, you know, analytics software processing so like Pino, you ingest data and then without actually writing to disk like the traditional way of doing things. So then it speeds up to the whole process. And it processes the data in memory. And when you're done with processing, you can use Pulsar function to transform your data, whatever you need, and then you output your data to a sync. So there will be connectors that that helps you to do that. So the whole kind of pipeline is designed to be very efficient. That's what it is. So and now we back to Pino a little more and then with the demo too. All right, so what is what is Pino? So this diagram more or less explains it. So as I say, we've got data coming in from from sources. So in our case, it's going to be Pulsar, but you can kind of see there are lots of other ones. And what's kind of interesting is you could you could have in theory, you could load into the same tables streaming and batch data sources and query them query them both together. Once they come in, it's a it's a column store. So it's a store. And then you can kind of do some aggregation. There's all the different types of indexes on top of that. And you can do some prematerialization as well. And then on the right hand side, we've got a couple of the use cases. So just to quickly show you the architecture of how this how this works from the from the far side. So the data is coming in from Pulsar here. We've got there are we're going to be using three components. We've got a controller, we've got a server and then we'll have a broker, which will come up here. So the controller is the manager of the cluster. So it's taking care of a hey, where does this where does this data need to go and then and Pino uses a tool called Helix, which is on top of on top of zookeeper. And so it will then it breaks data out into segments. So the segments will in this particular case will map to the partitions coming from Pulsar. So what like each partition will be coming to a segment. So if we had, for example, four partitions, we get four, four different segments. And if you were doing it in the cluster, in our case, we'll just did on my machine. But if you were doing it in a cluster, it would then have decide where is it going to replicate each of the data coming from each of the partitions. I might do one, one is on server one and seven four, and two is on server two and seven three and so on at the servers in the data. So that's where the data is going. Remember, the controller manages everything. And then we have the broker is taking care of the querying. So the query comes in here. So for example, here we're counting from a table how many how many rows for the country us. And then the broker sends it out in a scatter gather type pattern out to the servers that it knows have the data that will map that might be appropriate. And then it will they will kind of send their result back and then the broker takes care of aggregating it and sending it back to the client. Okay, now let's have a look at a demo. Let's see if I can get this to. So what we're going to do is if in case you want to look at it, this is where the demo lives. Hopefully that QR code still works. I'll just let you take a picture of it. And what it what it is, we can kind of tell from the name. So it's a wiki demo. It's going to sit in as I say before, it's going to sit in this bottom corner. So real time dashboard. And this is the architecture of it. So we've got a streaming API. And the data is going to come in, we've got a Python application, it's going to process it, put it into Palsar, from Palsar into Pino. And then we're going to have a stream that dashboard on the end. We've got 15 minutes. So let's see how we go. So wiki media has a really nice recent changes feed. So this is capturing all the changes that are being done to different properties in wiki media. And they actually store it internally as Kafka, but then expose it using server side events protocols. This is a HTTP protocol, it basically just streams out loads and loads of it, it reflects from the infinite stream of these events. And this is what it looks like. So we get three properties to get an event, an ID and a data. The main bit that's interesting is data. So you can kind of see it's like a sort of nested JSON structure of stuff. And you can kind of, you can sort of see like, so we've got like the URL that's been changed, you've got a request ID, you got an ID. It says somewhere, yeah, like what was the title of the page? When was the time stamp? At the revision? Yeah, it's got lots of lots of kind of interesting stuff that we can, that we can pull out. Nope, not done yet. Okay, so now that would have been, that would have been amazing. So we have got, if I come, can you see? Yes. All right, great. How can I see if I can get this to stay on my, hopefully, is that good enough? Is that good enough? Yeah. All right, perfect. So we have a, let's see if I can type, pigmentize. Ah. Yeah, there we go. So we've got a script called wiki.pipe. So this is what it looks like. So I guess the red is not entirely readable. But we've got, this is, this is the, don't do that. This is the URL that we're going to be working with. So you can see these are, if I paste that into my browser. Oh, I don't know. What's that done? It's going to the wrong one. Come here. Hang on. I'll escape that. So if we paste that in here, this is what it, this is what it looks like. So you get some loads and loads of messages. Chrome will eventually get very, very angry with you if you leave this running forever. But you can kind of see the messages are coming through. And so we're going to be processing that. So you sort of see, we've got, we're just using the request library in Python to make a wrap around it. So that's this bit here. Wrap around that, that particular endpoint. And then it's going to stream the events. And we've got this SSE, the server side events client, Python client. Wrap that round and we get an infinite stream of messages. So if we were to run this, so let's just go here. And we'll pipe it into JQ just so it's a bit more readable. So you can kind of see like the messages are coming through. If I stop it and scroll up, so you can see we've got, this is what it looks like. So you got the schema, meta, ID type. You can kind of see like, oh, this is, this is, this is what, oh, this is Russian, Russian, someone's changing Russian Wikipedia at the moment apparently. There you go. Thanks. So we've got lots of different stuff. And so that's kind of the first bit, right? We wanted to get the stream into like a fashion that we've got it, right? So we've done that bit. Next bit is we want to get it into Pulsar. So let's have a look at our second script. So this one is to get it into Pulsar. Oh, hang on. It's a bit longer. So let's just pipe it into less first. So we're going to be using the Pulsar client. And the first bit is the same, right? So we still build our streaming client here, like for the, for the wiki. But then we also build a Pulsar client here. So we're creating a Pulsar client. We point it to localhost 665. This is the port. We're actually running all this in Docker, but if it exposes the ports out to the, to the host OS, we create a producer. So our topic is going to be called wiki events. And then if you scroll down here, this is still the same. So we're still looping through that stream of stuff. But this time we, we then call the Pulsar producer and we send it a message and we're sending it in async, which means we're not going to wait for the response, right? And then finally we flush, we flushed it every 100 messages. Anything else to add on the Pulsar production? That's good. All right, cool. So now let's call that one. So if we call Python wiki to Pulsar. So then I run and you can kind of see like it will sort of run away and everything is, everything is happy. I can never remember quite exactly what the commands are, but we can then use the Pulsar client to check that these messages are making their way in. So we can call this Pulsar client and we say we're going to actually, do you want to explain it? You're probably better than me. Yeah, consuming and the events and then also the subscription. You have a name to it. Okay. So it's going to pick up wherever it was the last time I did it. So let's see. So there we go. I mean, this is going way faster. I guess it's catching up the ones that we've done, we've done before. So, and then eventually we'll come to the same sort of speeds. So we can see these are, these are the messages coming through Pulsar. We've put it in JSON format, but it can handle multiple forms, right? If you wanted to. So yeah, sort of all the, all the typical ones you would expect. So I guess most people would be doing Avro with some sort of schema attached to it. But I mean, for, yeah, for simplicity in the demo, JSON is quite a, quite a nice tool. Okay. So we've got it that far. Next thing is we want to get it into our, into a way that we can query it from, let's see, where's my, right? So, so as I say, Pino, it's like model is table. So you create a table and you can query it. And the first thing is we need to have a schema for that table. So this is what the Wikipedia one looked like. So we're pulling out some of the fields. We've got ID, we've got Wiki, we've got user, we've got title, comment, stream, type, bots, and then we've specified a timestamp down to bottom. You notice maybe that this is, this is kind of using the, the language of data warehousing. So these are dimension fields. We don't have any metrics fields in because there aren't really anything that we can count in this dataset, but you could have, if you had something that you were counting, then you could put that as a metric field. And then finally we've got a date time field as well. Now in general, by default, whatever fields you put in here, it will map exactly. If there is a value in your source with this name, it will map it directly. So in ideal world, everything is flat and we just go map, map, map, map, map. In this case actually, it's not quite as simple as that, but so what we need to do, so I'll just show you the table conflict that goes with it. But the first thing to notice is we can do these transformation functions to get the data into our column. So ID is under meta.id. So we're using this JSON path function to pull stuff out. You could, if you wanted to, if you, if we had cleaned the data up before, we could have used Palsar's serverless functions to do the same thing. And then we would have it in a cleaner state and just go straight into Pina. So you can kind of choose which, which of those options. This is not a replacement for a stream processor, right? This is just a very, very tiny adjustments to the data, right? If it was like slightly, slightly wrong. And then the other thing was the timestamps in the wiki give you are in milliseconds, epoch seconds, and I need epoch milliseconds for, for Palsar. So I multiply by a thousand. Now let's, oh, sorry. Yeah, I forgot the top bit. So top bit, name of the table needs to match the name of the schema. So it's Wikipedia. We need to specify what's the timestamp. And then this is the config that's telling it, hey, I need to, actually, I'm looking, I'm looking at the wrong one even. I should be, I shouldn't be saying table Palsar. There we go. So, sorry. So you see here, we say, hey, I'm going to be pulling the data from the Palsar stream. This is my Palsar connection string. I then need to say, hey, I'm going to decode, I need, I'm going to tell it which factory to use. I need to tell it the Palsar factory. And then this is, yeah, I mean, this is not really necessary for, for now. And what we're going to do now is we're going to add our table. So what this is going to do is going to create the table and then immediately it's going to start consuming. If you didn't have any messages in there, it obviously wouldn't consume anything. But since we, we do, we should be able to see our table here, Wikipedia. And you can see we've got, the messages are kind of coming in. So you can see we've got, at the moment, 9,000 messages. We could write like a, so it's a sequel, sequel on top of it. So we can, thanks. We could say, okay, let's have a look which user is doing the most stuff. Let's hang on. Oh, forgot the group by group. By user. Oh, can't type. Order by, can't start sending. So we could do, yeah, we could do something. What people, where people are changing stuff. And then finally, yeah, let me just, let me just quickly show you what, what, what exactly what we're doing. Okay. They're back again. Should I put it back? Yeah. Yeah. Okay. All right, easy one, easy one. So, oh, sorry. So what we're doing, so it's all in Python. So we're using the Pena Python driver. We're connecting to Pena here. And then we're basically just running some queries. So the one that we're showing you, like the table on the top, this red is not entirely readable. Is that aggregation plus filtering. So kind of capturing what happened in the last minute versus, versus what happened a minute before. And then we're going to kind of just run the query and stick it into pandas. We do the same. We build some metrics. There's numbers on the top there called, string that calls the metrics. So we can pass in like a value and then we can build the dieters. And the dieters, this minute minus the previous minutes, you can kind of see the change. And instead of, yeah, I mean, I guess, yeah, that's probably enough on the code. But if you want to have a look at that, it's all in the, the repository. And in theory, like you've seen me literally just running the commands in there. It should just, should just work. So hopefully, hopefully you can, you can follow along. But I just came back to here to conclude. So yeah, lots of, lots of people doing stuff with, with parents or some of the, the users parcel as well. So lots of, lots of different, lots of people, people using it. Just a conclusion and then, and then one more slide. So I hope that you can kind of see what you're combining these different tools together. You can build some quite, some quite cool applications. In this case, I'm just going to show you what the action would be because all the users have built. But you could imagine, like if it was like, if you were looking at real wicked users, you maybe want to try and encourage the ones you see like are coming in new. We've built something that's fresh there, the fast gradient scale. And we've done it with a classroom. For me, I'm writing a book with an ex colleague of mine, I'm showing you how to build these type of things. If you're interested in that, then there's my contact on your left. And I let Mary conclude. Just, okay. Here, it's just how you can connect me with me and also Apache Pulsar. If you're interested, we have a Slack group and also a wiki page. Sorry, wiki page of Apache Pulsar neighborhood team that you can build up on more stuff on. So, okay, I think then this is pretty much it. And if you need more links to Pulsar, this is the page and I can share the slide that we can share the slide with you so you can have more. And we also have developer data stacks. We'll have it on YouTube in five minutes about Pulsar if you're interested in that. Also, we'll have also master data stacks developers that you can find examples to because today we can't get into a lot of these things. And myself too, I've actually have a Twitch stream every Wednesday afternoon, central time like Chicago, so there will be evening time here. So if you're interested, you can follow me on Twitch as well. And yeah, I think that's it. How did it become like this? Thank you. Thank you. Backup slides in case the camera gets disastrously. Two questions. Here's one. Yeah, quick one. As you know, the way Kafka has, you can have cluster of Kafka instance brokers and stuff like that. Do you have that same problem where like one goes down, the other one says going down? It's like zookeeper kind of causes that problem. Because when I say Kafka, it always seems to be a problem with speaking with each other. So that's more about, I can move it offline with you and if folks are interested too, but I actually also have another talk that's kind of deeper right into Pulsar. As well as tomorrow too, I actually have a talk at the open JDK room, but that's more focusing on JMS. So there will be open JDK room in building H. So if you want to do that. But as far as Pulsar is concerned, it is very cloud native by itself. So a lot of things who in fact I didn't get to talk about is very infrastructure aware. So things like, you know, you don't want to worry about how do you deal with offsets while in Kafka. You don't have such, you don't have such thing. And then there are other things too. You just don't have enough time. I think I was told this time's up. So we can move it offline and then follow me on my discord. Actually, I didn't even get my discord, but follow me somewhere. Twitter. And I'll be happy to answer any questions you have. Thank you. Okay. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.64, "text": " Okay. Hello, everyone, and welcome to our talk today. Yeah, so today we're going to", "tokens": [50364, 1033, 13, 2425, 11, 1518, 11, 293, 2928, 281, 527, 751, 965, 13, 865, 11, 370, 965, 321, 434, 516, 281, 50846], "temperature": 0.0, "avg_logprob": -0.24808887768817203, "compression_ratio": 1.5107296137339057, "no_speech_prob": 0.5588266253471375}, {"id": 1, "seek": 0, "start": 9.64, "end": 15.88, "text": " talk about real-time analytics application with Apache Pino and Apache Pulsar and Pino.", "tokens": [50846, 751, 466, 957, 12, 3766, 15370, 3861, 365, 46597, 430, 2982, 293, 46597, 430, 9468, 289, 293, 430, 2982, 13, 51158], "temperature": 0.0, "avg_logprob": -0.24808887768817203, "compression_ratio": 1.5107296137339057, "no_speech_prob": 0.5588266253471375}, {"id": 2, "seek": 0, "start": 15.88, "end": 21.64, "text": " So here today too is myself. I'm Mary Grigleski. I'm a streaming developer advocate at Data", "tokens": [51158, 407, 510, 965, 886, 307, 2059, 13, 286, 478, 6059, 2606, 328, 904, 2984, 13, 286, 478, 257, 11791, 10754, 14608, 412, 11888, 51446], "temperature": 0.0, "avg_logprob": -0.24808887768817203, "compression_ratio": 1.5107296137339057, "no_speech_prob": 0.5588266253471375}, {"id": 3, "seek": 0, "start": 21.64, "end": 26.96, "text": " Stacks. A company is actually primarily doing Apache Cassandra up to this point. And now", "tokens": [51446, 745, 7424, 13, 316, 2237, 307, 767, 10029, 884, 46597, 18208, 18401, 493, 281, 341, 935, 13, 400, 586, 51712], "temperature": 0.0, "avg_logprob": -0.24808887768817203, "compression_ratio": 1.5107296137339057, "no_speech_prob": 0.5588266253471375}, {"id": 4, "seek": 2696, "start": 26.96, "end": 31.48, "text": " we're going to be doing more Apache Pulsar. And it's a streaming event, streaming platform", "tokens": [50364, 321, 434, 516, 281, 312, 884, 544, 46597, 430, 9468, 289, 13, 400, 309, 311, 257, 11791, 2280, 11, 11791, 3663, 50590], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 5, "seek": 2696, "start": 31.48, "end": 36.08, "text": " that's kind of optimized for the cloud-native platform. I'm based in Chicago. I'm also", "tokens": [50590, 300, 311, 733, 295, 26941, 337, 264, 4588, 12, 77, 1166, 3663, 13, 286, 478, 2361, 294, 9525, 13, 286, 478, 611, 50820], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 6, "seek": 2696, "start": 36.08, "end": 40.8, "text": " Java champion and president of Chicago Java user script too, and blah, blah, blah, all", "tokens": [50820, 10745, 10971, 293, 3868, 295, 9525, 10745, 4195, 5755, 886, 11, 293, 12288, 11, 12288, 11, 12288, 11, 439, 51056], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 7, "seek": 2696, "start": 40.8, "end": 45.56, "text": " these things. I was a developer before too, just so you know, mostly in Java. So now we", "tokens": [51056, 613, 721, 13, 286, 390, 257, 10754, 949, 886, 11, 445, 370, 291, 458, 11, 5240, 294, 10745, 13, 407, 586, 321, 51294], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 8, "seek": 2696, "start": 45.56, "end": 50.6, "text": " have Mark. So we have Mark introduce. Hello. And I do realize, Javier, that we've stolen", "tokens": [51294, 362, 3934, 13, 407, 321, 362, 3934, 5366, 13, 2425, 13, 400, 286, 360, 4325, 11, 508, 25384, 11, 300, 321, 600, 15900, 51546], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 9, "seek": 2696, "start": 50.6, "end": 55.84, "text": " your intro section. So we've got, we've gone straight and say, yeah, hi. I'm Mark. I work", "tokens": [51546, 428, 12897, 3541, 13, 407, 321, 600, 658, 11, 321, 600, 2780, 2997, 293, 584, 11, 1338, 11, 4879, 13, 286, 478, 3934, 13, 286, 589, 51808], "temperature": 0.0, "avg_logprob": -0.23161038249528326, "compression_ratio": 1.707395498392283, "no_speech_prob": 0.029309524223208427}, {"id": 10, "seek": 5584, "start": 56.040000000000006, "end": 61.0, "text": " at Starchy. We do Apache Pino. I'm a developer advocate there. And so, yeah, like we kind", "tokens": [50374, 412, 745, 13988, 13, 492, 360, 46597, 430, 2982, 13, 286, 478, 257, 10754, 14608, 456, 13, 400, 370, 11, 1338, 11, 411, 321, 733, 50622], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 11, "seek": 5584, "start": 61.0, "end": 64.2, "text": " of had on the first slide, we're going to be showing you how to, yeah, how to, I guess", "tokens": [50622, 295, 632, 322, 264, 700, 4137, 11, 321, 434, 516, 281, 312, 4099, 291, 577, 281, 11, 1338, 11, 577, 281, 11, 286, 2041, 50782], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 12, "seek": 5584, "start": 64.2, "end": 70.2, "text": " maybe more, how to build a real-time like analytics dashboard with Pulsar, Pino, and", "tokens": [50782, 1310, 544, 11, 577, 281, 1322, 257, 957, 12, 3766, 411, 15370, 18342, 365, 430, 9468, 289, 11, 430, 2982, 11, 293, 51082], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 13, "seek": 5584, "start": 70.2, "end": 74.52000000000001, "text": " then Python dashboard library called Streamlit. So we're going to see half the talk will be", "tokens": [51082, 550, 15329, 18342, 6405, 1219, 24904, 23062, 13, 407, 321, 434, 516, 281, 536, 1922, 264, 751, 486, 312, 51298], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 14, "seek": 5584, "start": 74.52000000000001, "end": 78.56, "text": " that. And we're going to see how well does the Wi-Fi survive our attempts to use live", "tokens": [51298, 300, 13, 400, 321, 434, 516, 281, 536, 577, 731, 775, 264, 14035, 12, 13229, 7867, 527, 15257, 281, 764, 1621, 51500], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 15, "seek": 5584, "start": 78.56, "end": 84.92, "text": " data. So let's see. Let's hope the demo gods are in the room. So I guess first things to", "tokens": [51500, 1412, 13, 407, 718, 311, 536, 13, 961, 311, 1454, 264, 10723, 14049, 366, 294, 264, 1808, 13, 407, 286, 2041, 700, 721, 281, 51818], "temperature": 0.0, "avg_logprob": -0.19359833122099807, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0810239240527153}, {"id": 16, "seek": 8492, "start": 84.96000000000001, "end": 88.76, "text": " start with is to define like what exactly does this mean? What is real-time analytics?", "tokens": [50366, 722, 365, 307, 281, 6964, 411, 437, 2293, 775, 341, 914, 30, 708, 307, 957, 12, 3766, 15370, 30, 50556], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 17, "seek": 8492, "start": 88.76, "end": 92.08, "text": " So we've seen lots of talks, right, showing streaming data. So obviously that's like a", "tokens": [50556, 407, 321, 600, 1612, 3195, 295, 6686, 11, 558, 11, 4099, 11791, 1412, 13, 407, 2745, 300, 311, 411, 257, 50722], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 18, "seek": 8492, "start": 92.08, "end": 97.28, "text": " big part of it. But the real-time analytics bit is kind of this bit in highlighted. So", "tokens": [50722, 955, 644, 295, 309, 13, 583, 264, 957, 12, 3766, 15370, 857, 307, 733, 295, 341, 857, 294, 17173, 13, 407, 50982], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 19, "seek": 8492, "start": 97.28, "end": 102.16, "text": " the goal is we're trying to provide insights to make decisions quickly. And I guess the", "tokens": [50982, 264, 3387, 307, 321, 434, 1382, 281, 2893, 14310, 281, 652, 5327, 2661, 13, 400, 286, 2041, 264, 51226], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 20, "seek": 8492, "start": 102.16, "end": 105.76, "text": " most important bit is that farsight. So like, it's cool that we've got the data. We can", "tokens": [51226, 881, 1021, 857, 307, 300, 283, 685, 397, 13, 407, 411, 11, 309, 311, 1627, 300, 321, 600, 658, 264, 1412, 13, 492, 393, 51406], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 21, "seek": 8492, "start": 105.76, "end": 109.56, "text": " capture our IoT data. We can capture like orders coming from a website. We can capture", "tokens": [51406, 7983, 527, 30112, 1412, 13, 492, 393, 7983, 411, 9470, 1348, 490, 257, 3144, 13, 492, 393, 7983, 51596], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 22, "seek": 8492, "start": 109.56, "end": 114.44, "text": " the logs, but we want to do something, do something with it. And so if we move on from", "tokens": [51596, 264, 20820, 11, 457, 321, 528, 281, 360, 746, 11, 360, 746, 365, 309, 13, 400, 370, 498, 321, 1286, 322, 490, 51840], "temperature": 0.0, "avg_logprob": -0.19684754142278357, "compression_ratio": 1.8154761904761905, "no_speech_prob": 0.007586880587041378}, {"id": 23, "seek": 11444, "start": 114.48, "end": 118.48, "text": " the definition, like all the talks we've seen so far, they focus on events. We've got events", "tokens": [50366, 264, 7123, 11, 411, 439, 264, 6686, 321, 600, 1612, 370, 1400, 11, 436, 1879, 322, 3931, 13, 492, 600, 658, 3931, 50566], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 24, "seek": 11444, "start": 118.48, "end": 123.12, "text": " representing stuff. So someone is purchasing something. Someone wants to do a search query.", "tokens": [50566, 13460, 1507, 13, 407, 1580, 307, 20906, 746, 13, 8734, 2738, 281, 360, 257, 3164, 14581, 13, 50798], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 25, "seek": 11444, "start": 123.12, "end": 127.75999999999999, "text": " Someone is taking a taxi ride like we've seen in Javier's talk. And those events are cool", "tokens": [50798, 8734, 307, 1940, 257, 18984, 5077, 411, 321, 600, 1612, 294, 508, 25384, 311, 751, 13, 400, 729, 3931, 366, 1627, 51030], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 26, "seek": 11444, "start": 127.75999999999999, "end": 130.88, "text": " on their own, right? But we really want to get some insights from them. Like what can", "tokens": [51030, 322, 641, 1065, 11, 558, 30, 583, 321, 534, 528, 281, 483, 512, 14310, 490, 552, 13, 1743, 437, 393, 51186], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 27, "seek": 11444, "start": 130.88, "end": 134.24, "text": " we do? And eventually that leads to, okay, we've got some insight and we do something", "tokens": [51186, 321, 360, 30, 400, 4728, 300, 6689, 281, 11, 1392, 11, 321, 600, 658, 512, 11269, 293, 321, 360, 746, 51354], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 28, "seek": 11444, "start": 134.24, "end": 138.0, "text": " as a result of knowing that this is happening. So like say, we know someone is searching", "tokens": [51354, 382, 257, 1874, 295, 5276, 300, 341, 307, 2737, 13, 407, 411, 584, 11, 321, 458, 1580, 307, 10808, 51542], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 29, "seek": 11444, "start": 138.0, "end": 141.8, "text": " for a pizza. Okay, let's get a pizza shop in front of them if we're doing Google Adverts.", "tokens": [51542, 337, 257, 8298, 13, 1033, 11, 718, 311, 483, 257, 8298, 3945, 294, 1868, 295, 552, 498, 321, 434, 884, 3329, 1999, 36999, 13, 51732], "temperature": 0.0, "avg_logprob": -0.15778380819836502, "compression_ratio": 1.816860465116279, "no_speech_prob": 0.02498963288962841}, {"id": 30, "seek": 14180, "start": 141.8, "end": 145.16000000000003, "text": " If someone's then going in and buying the pizza, okay, let's show them in real time", "tokens": [50364, 759, 1580, 311, 550, 516, 294, 293, 6382, 264, 8298, 11, 1392, 11, 718, 311, 855, 552, 294, 957, 565, 50532], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 31, "seek": 14180, "start": 145.16000000000003, "end": 148.64000000000001, "text": " what are the things that people have also been buying along with that pizza or that", "tokens": [50532, 437, 366, 264, 721, 300, 561, 362, 611, 668, 6382, 2051, 365, 300, 8298, 420, 300, 50706], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 32, "seek": 14180, "start": 148.64000000000001, "end": 153.36, "text": " hamburger that we can suggest like in the flow. So we want to try and react to those", "tokens": [50706, 34575, 300, 321, 393, 3402, 411, 294, 264, 3095, 13, 407, 321, 528, 281, 853, 293, 4515, 281, 729, 50942], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 33, "seek": 14180, "start": 153.36, "end": 157.84, "text": " events coming in. So we've seen like lots of tools. We've seen Beam. We've seen Flink.", "tokens": [50942, 3931, 1348, 294, 13, 407, 321, 600, 1612, 411, 3195, 295, 3873, 13, 492, 600, 1612, 40916, 13, 492, 600, 1612, 3235, 475, 13, 51166], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 34, "seek": 14180, "start": 157.84, "end": 162.16000000000003, "text": " We've seen like lots of tools for getting the streaming data, but we want to do something", "tokens": [51166, 492, 600, 1612, 411, 3195, 295, 3873, 337, 1242, 264, 11791, 1412, 11, 457, 321, 528, 281, 360, 746, 51382], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 35, "seek": 14180, "start": 162.16000000000003, "end": 165.96, "text": " with that data. Like that's the whole purpose, I guess, of all the applications that we're", "tokens": [51382, 365, 300, 1412, 13, 1743, 300, 311, 264, 1379, 4334, 11, 286, 2041, 11, 295, 439, 264, 5821, 300, 321, 434, 51572], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 36, "seek": 14180, "start": 165.96, "end": 171.0, "text": " building. And in the real time analytics space, like this is like, you imagine the value of", "tokens": [51572, 2390, 13, 400, 294, 264, 957, 565, 15370, 1901, 11, 411, 341, 307, 411, 11, 291, 3811, 264, 2158, 295, 51824], "temperature": 0.0, "avg_logprob": -0.16734567457629787, "compression_ratio": 1.8601823708206686, "no_speech_prob": 0.009822412393987179}, {"id": 37, "seek": 17100, "start": 171.04, "end": 176.08, "text": " data over time in this world where we're trying to do something with that streaming data,", "tokens": [50366, 1412, 670, 565, 294, 341, 1002, 689, 321, 434, 1382, 281, 360, 746, 365, 300, 11791, 1412, 11, 50618], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 38, "seek": 17100, "start": 176.08, "end": 181.08, "text": " the value of that data goes down over time. So if we know like today, like, hey, you made", "tokens": [50618, 264, 2158, 295, 300, 1412, 1709, 760, 670, 565, 13, 407, 498, 321, 458, 411, 965, 11, 411, 11, 4177, 11, 291, 1027, 50868], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 39, "seek": 17100, "start": 181.08, "end": 184.88, "text": " an order and something's gone wrong with it, I can like try and do something to make you", "tokens": [50868, 364, 1668, 293, 746, 311, 2780, 2085, 365, 309, 11, 286, 393, 411, 853, 293, 360, 746, 281, 652, 291, 51058], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 40, "seek": 17100, "start": 184.88, "end": 188.64, "text": " happy, like give you a voucher or call you up and try and fix it. If I find out like", "tokens": [51058, 2055, 11, 411, 976, 291, 257, 31007, 260, 420, 818, 291, 493, 293, 853, 293, 3191, 309, 13, 759, 286, 915, 484, 411, 51246], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 41, "seek": 17100, "start": 188.64, "end": 193.32, "text": " when I batch process that tomorrow, it's like, oh, you already hate me. So it's too late.", "tokens": [51246, 562, 286, 15245, 1399, 300, 4153, 11, 309, 311, 411, 11, 1954, 11, 291, 1217, 4700, 385, 13, 407, 309, 311, 886, 3469, 13, 51480], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 42, "seek": 17100, "start": 193.32, "end": 197.2, "text": " And so we're kind of focused on the left side of this diagram. So in terms of real time", "tokens": [51480, 400, 370, 321, 434, 733, 295, 5178, 322, 264, 1411, 1252, 295, 341, 10686, 13, 407, 294, 2115, 295, 957, 565, 51674], "temperature": 0.0, "avg_logprob": -0.139340247307624, "compression_ratio": 1.8, "no_speech_prob": 0.04257854074239731}, {"id": 43, "seek": 19720, "start": 197.23999999999998, "end": 201.56, "text": " analytics, we want the data like close to when it's coming in, maybe not exactly like, not", "tokens": [50366, 15370, 11, 321, 528, 264, 1412, 411, 1998, 281, 562, 309, 311, 1348, 294, 11, 1310, 406, 2293, 411, 11, 406, 50582], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 44, "seek": 19720, "start": 201.56, "end": 204.95999999999998, "text": " not exactly like when it comes in, like in the time just after that, that's the kind", "tokens": [50582, 406, 2293, 411, 562, 309, 1487, 294, 11, 411, 294, 264, 565, 445, 934, 300, 11, 300, 311, 264, 733, 50752], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 45, "seek": 19720, "start": 204.95999999999998, "end": 208.51999999999998, "text": " of region that we're living. And there can be lots of people who are interested in this", "tokens": [50752, 295, 4458, 300, 321, 434, 2647, 13, 400, 456, 393, 312, 3195, 295, 561, 567, 366, 3102, 294, 341, 50930], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 46, "seek": 19720, "start": 208.51999999999998, "end": 213.04, "text": " data. So it could be the analysts inside a company. So maybe it's people like, in our", "tokens": [50930, 1412, 13, 407, 309, 727, 312, 264, 31388, 1854, 257, 2237, 13, 407, 1310, 309, 311, 561, 411, 11, 294, 527, 51156], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 47, "seek": 19720, "start": 213.04, "end": 217.07999999999998, "text": " imaginary pizza shop, like they're actually like running, running the, running the operations", "tokens": [51156, 26164, 8298, 3945, 11, 411, 436, 434, 767, 411, 2614, 11, 2614, 264, 11, 2614, 264, 7705, 51358], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 48, "seek": 19720, "start": 217.07999999999998, "end": 219.56, "text": " with the pizza shop. It might be the management. They're like, hey, I want to know what's going", "tokens": [51358, 365, 264, 8298, 3945, 13, 467, 1062, 312, 264, 4592, 13, 814, 434, 411, 11, 4177, 11, 286, 528, 281, 458, 437, 311, 516, 51482], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 49, "seek": 19720, "start": 219.56, "end": 222.67999999999998, "text": " on now. What's the, what's the revenue that we're seeing like right now, like we're in", "tokens": [51482, 322, 586, 13, 708, 311, 264, 11, 437, 311, 264, 9324, 300, 321, 434, 2577, 411, 558, 586, 11, 411, 321, 434, 294, 51638], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 50, "seek": 19720, "start": 222.67999999999998, "end": 226.16, "text": " the last 10 minutes or whatever it is. And it could be the users. And so that's, that's", "tokens": [51638, 264, 1036, 1266, 2077, 420, 2035, 309, 307, 13, 400, 309, 727, 312, 264, 5022, 13, 400, 370, 300, 311, 11, 300, 311, 51812], "temperature": 0.0, "avg_logprob": -0.18537806439143356, "compression_ratio": 1.9723756906077348, "no_speech_prob": 0.038197681307792664}, {"id": 51, "seek": 22616, "start": 226.2, "end": 229.84, "text": " kind of the interesting thing that we, that there's sort of changed, I guess, from doing", "tokens": [50366, 733, 295, 264, 1880, 551, 300, 321, 11, 300, 456, 311, 1333, 295, 3105, 11, 286, 2041, 11, 490, 884, 50548], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 52, "seek": 22616, "start": 229.84, "end": 233.76, "text": " traditional analytics to doing this real time stuff is that the data is almost coming back", "tokens": [50548, 5164, 15370, 281, 884, 341, 957, 565, 1507, 307, 300, 264, 1412, 307, 1920, 1348, 646, 50744], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 53, "seek": 22616, "start": 233.76, "end": 237.92, "text": " that users are creating the data and then we're feeding it back to them in terms of products", "tokens": [50744, 300, 5022, 366, 4084, 264, 1412, 293, 550, 321, 434, 12919, 309, 646, 281, 552, 294, 2115, 295, 3383, 50952], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 54, "seek": 22616, "start": 237.92, "end": 244.51999999999998, "text": " that they can then use. And yeah, I guess that's more or less what I wanted to say. And so when", "tokens": [50952, 300, 436, 393, 550, 764, 13, 400, 1338, 11, 286, 2041, 300, 311, 544, 420, 1570, 437, 286, 1415, 281, 584, 13, 400, 370, 562, 51282], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 55, "seek": 22616, "start": 244.51999999999998, "end": 247.51999999999998, "text": " we're building these applications, there are kind of, I mean, they're not, they're not strictly", "tokens": [51282, 321, 434, 2390, 613, 5821, 11, 456, 366, 733, 295, 11, 286, 914, 11, 436, 434, 406, 11, 436, 434, 406, 20792, 51432], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 56, "seek": 22616, "start": 247.51999999999998, "end": 251.2, "text": " like this, but there's sort of four obvious quadrants of applications that people build.", "tokens": [51432, 411, 341, 11, 457, 456, 311, 1333, 295, 1451, 6322, 10787, 10968, 295, 5821, 300, 561, 1322, 13, 51616], "temperature": 0.0, "avg_logprob": -0.16042319340492361, "compression_ratio": 1.9068965517241379, "no_speech_prob": 0.0018035024404525757}, {"id": 57, "seek": 25120, "start": 251.2, "end": 255.48, "text": " So they go along, oh yes, I should show you, they go along two axes. So we've got human facing", "tokens": [50364, 407, 436, 352, 2051, 11, 1954, 2086, 11, 286, 820, 855, 291, 11, 436, 352, 2051, 732, 35387, 13, 407, 321, 600, 658, 1952, 7170, 50578], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 58, "seek": 25120, "start": 255.48, "end": 260.76, "text": " and machine facing along the Y and then internal and external on the X. So if we go in the top", "tokens": [50578, 293, 3479, 7170, 2051, 264, 398, 293, 550, 6920, 293, 8320, 322, 264, 1783, 13, 407, 498, 321, 352, 294, 264, 1192, 50842], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 59, "seek": 25120, "start": 260.76, "end": 264.68, "text": " side, that would be like the observability area. And actually a lot of the time series", "tokens": [50842, 1252, 11, 300, 576, 312, 411, 264, 9951, 2310, 1859, 13, 400, 767, 257, 688, 295, 264, 565, 2638, 51038], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 60, "seek": 25120, "start": 264.68, "end": 268.12, "text": " querying would be in there. And this would be sort of the area of like data dogs. So", "tokens": [51038, 7083, 1840, 576, 312, 294, 456, 13, 400, 341, 576, 312, 1333, 295, 264, 1859, 295, 411, 1412, 7197, 13, 407, 51210], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 61, "seek": 25120, "start": 268.12, "end": 271.64, "text": " hey, like I wanted to get the met, I've got like all this telemetry data coming in and", "tokens": [51210, 4177, 11, 411, 286, 1415, 281, 483, 264, 1131, 11, 286, 600, 658, 411, 439, 341, 4304, 5537, 627, 1412, 1348, 294, 293, 51386], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 62, "seek": 25120, "start": 271.64, "end": 275.52, "text": " I want to know what's going on, like what's going on in my, in my AWS cluster. What's", "tokens": [51386, 286, 528, 281, 458, 437, 311, 516, 322, 11, 411, 437, 311, 516, 322, 294, 452, 11, 294, 452, 17650, 13630, 13, 708, 311, 51580], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 63, "seek": 25120, "start": 275.52, "end": 278.44, "text": " happening like with all my Lambda functions? Like are there any that are suddenly really", "tokens": [51580, 2737, 411, 365, 439, 452, 45691, 6828, 30, 1743, 366, 456, 604, 300, 366, 5800, 534, 51726], "temperature": 0.0, "avg_logprob": -0.17213973079819278, "compression_ratio": 1.8708708708708708, "no_speech_prob": 0.00713089806959033}, {"id": 64, "seek": 27844, "start": 278.44, "end": 283.4, "text": " slow? Like can I, can I like figure that out? And maybe there's a machine that's actually", "tokens": [50364, 2964, 30, 1743, 393, 286, 11, 393, 286, 411, 2573, 300, 484, 30, 400, 1310, 456, 311, 257, 3479, 300, 311, 767, 50612], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 65, "seek": 27844, "start": 283.4, "end": 286.44, "text": " interpreting that data rather than a human is looking at it and going, oh yeah, it's", "tokens": [50612, 37395, 300, 1412, 2831, 813, 257, 1952, 307, 1237, 412, 309, 293, 516, 11, 1954, 1338, 11, 309, 311, 50764], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 66, "seek": 27844, "start": 286.44, "end": 291.56, "text": " that one Lambda function there that's really slow. Probably be feeding it into like, like", "tokens": [50764, 300, 472, 45691, 2445, 456, 300, 311, 534, 2964, 13, 9210, 312, 12919, 309, 666, 411, 11, 411, 51020], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 67, "seek": 27844, "start": 291.56, "end": 295.04, "text": " some other tool that's figuring it out. If we come down here, so this is where we're", "tokens": [51020, 512, 661, 2290, 300, 311, 15213, 309, 484, 13, 759, 321, 808, 760, 510, 11, 370, 341, 307, 689, 321, 434, 51194], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 68, "seek": 27844, "start": 295.04, "end": 299.68, "text": " going to be today. So imagine this is a dashboard and obviously I'm sure you've seen loads of", "tokens": [51194, 516, 281, 312, 965, 13, 407, 3811, 341, 307, 257, 18342, 293, 2745, 286, 478, 988, 291, 600, 1612, 12668, 295, 51426], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 69, "seek": 27844, "start": 299.68, "end": 304.28, "text": " dashboards, you've probably seen Tableau, you've seen loads of BI tools and with a lot of them,", "tokens": [51426, 8240, 17228, 11, 291, 600, 1391, 1612, 25535, 1459, 11, 291, 600, 1612, 12668, 295, 23524, 3873, 293, 365, 257, 688, 295, 552, 11, 51656], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 70, "seek": 27844, "start": 304.28, "end": 307.48, "text": " the data that you're using is maybe like yesterday's, yesterday's data. And so what we're going", "tokens": [51656, 264, 1412, 300, 291, 434, 1228, 307, 1310, 411, 5186, 311, 11, 5186, 311, 1412, 13, 400, 370, 437, 321, 434, 516, 51816], "temperature": 0.0, "avg_logprob": -0.18359816147505872, "compression_ratio": 1.8567251461988303, "no_speech_prob": 0.08923263847827911}, {"id": 71, "seek": 30748, "start": 307.52000000000004, "end": 311.64000000000004, "text": " to show you today is how could you build one that is like updating as, as new data is coming", "tokens": [50366, 281, 855, 291, 965, 307, 577, 727, 291, 1322, 472, 300, 307, 411, 25113, 382, 11, 382, 777, 1412, 307, 1348, 50572], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 72, "seek": 30748, "start": 311.64000000000004, "end": 316.44, "text": " in. If we come up to the, to the top left, this is now the machine is processing the", "tokens": [50572, 294, 13, 759, 321, 808, 493, 281, 264, 11, 281, 264, 1192, 1411, 11, 341, 307, 586, 264, 3479, 307, 9007, 264, 50812], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 73, "seek": 30748, "start": 316.44, "end": 320.8, "text": " data, but it's for the users. So this would be like how Javier was talking. So we've got", "tokens": [50812, 1412, 11, 457, 309, 311, 337, 264, 5022, 13, 407, 341, 576, 312, 411, 577, 508, 25384, 390, 1417, 13, 407, 321, 600, 658, 51030], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 74, "seek": 30748, "start": 320.8, "end": 323.6, "text": " like the fraud detection system. So the data's coming in, it's being processed by something", "tokens": [51030, 411, 264, 14560, 17784, 1185, 13, 407, 264, 1412, 311, 1348, 294, 11, 309, 311, 885, 18846, 538, 746, 51170], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 75, "seek": 30748, "start": 323.6, "end": 327.04, "text": " and then I'm seeing, I'm seeing the result, but maybe I'm not going and working out like", "tokens": [51170, 293, 550, 286, 478, 2577, 11, 286, 478, 2577, 264, 1874, 11, 457, 1310, 286, 478, 406, 516, 293, 1364, 484, 411, 51342], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 76, "seek": 30748, "start": 327.04, "end": 330.56, "text": " with my own query. Oh, look, I wrote this really, really clever query. Here's the, here's", "tokens": [51342, 365, 452, 1065, 14581, 13, 876, 11, 574, 11, 286, 4114, 341, 534, 11, 534, 13494, 14581, 13, 1692, 311, 264, 11, 510, 311, 51518], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 77, "seek": 30748, "start": 330.56, "end": 334.6, "text": " the thing. Maybe there's some, some pre-processing happening by some sort of machine learning", "tokens": [51518, 264, 551, 13, 2704, 456, 311, 512, 11, 512, 659, 12, 41075, 278, 2737, 538, 512, 1333, 295, 3479, 2539, 51720], "temperature": 0.0, "avg_logprob": -0.15497348422095888, "compression_ratio": 1.8724035608308605, "no_speech_prob": 0.06497300416231155}, {"id": 78, "seek": 33460, "start": 334.64000000000004, "end": 338.96000000000004, "text": " algorithm. And then if we come down into the bottom corner, be some sort of external service", "tokens": [50366, 9284, 13, 400, 550, 498, 321, 808, 760, 666, 264, 2767, 4538, 11, 312, 512, 1333, 295, 8320, 2643, 50582], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 79, "seek": 33460, "start": 338.96000000000004, "end": 342.36, "text": " that could be, yeah, like in our piece, for example, like an order tracking service. Like,", "tokens": [50582, 300, 727, 312, 11, 1338, 11, 411, 294, 527, 2522, 11, 337, 1365, 11, 411, 364, 1668, 11603, 2643, 13, 1743, 11, 50752], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 80, "seek": 33460, "start": 342.36, "end": 346.28000000000003, "text": " you know how on your phone you order something from, I don't know what's this food delivery", "tokens": [50752, 291, 458, 577, 322, 428, 2593, 291, 1668, 746, 490, 11, 286, 500, 380, 458, 437, 311, 341, 1755, 8982, 50948], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 81, "seek": 33460, "start": 346.28000000000003, "end": 350.6, "text": " service here just eats. Maybe you see like, Hey, look, I can see exactly where it is. How", "tokens": [50948, 2643, 510, 445, 18109, 13, 2704, 291, 536, 411, 11, 1911, 11, 574, 11, 286, 393, 536, 2293, 689, 309, 307, 13, 1012, 51164], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 82, "seek": 33460, "start": 350.6, "end": 355.08000000000004, "text": " far away is it? Why on earth has the driver gone the wrong way to my house? You can see", "tokens": [51164, 1400, 1314, 307, 309, 30, 1545, 322, 4120, 575, 264, 6787, 2780, 264, 2085, 636, 281, 452, 1782, 30, 509, 393, 536, 51388], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 83, "seek": 33460, "start": 355.08000000000004, "end": 358.36, "text": " all that sort of, all that sort of information. Maybe too much. Maybe they should just show", "tokens": [51388, 439, 300, 1333, 295, 11, 439, 300, 1333, 295, 1589, 13, 2704, 886, 709, 13, 2704, 436, 820, 445, 855, 51552], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 84, "seek": 33460, "start": 358.36, "end": 361.96000000000004, "text": " you. It's always coming towards your house. It never went like the absolute opposite way", "tokens": [51552, 291, 13, 467, 311, 1009, 1348, 3030, 428, 1782, 13, 467, 1128, 1437, 411, 264, 8236, 6182, 636, 51732], "temperature": 0.0, "avg_logprob": -0.18593040704727173, "compression_ratio": 1.7562326869806095, "no_speech_prob": 0.023635325953364372}, {"id": 85, "seek": 36196, "start": 362.03999999999996, "end": 367.44, "text": " and got to your house an hour later. So just to show you some real time, real, real world", "tokens": [50368, 293, 658, 281, 428, 1782, 364, 1773, 1780, 13, 407, 445, 281, 855, 291, 512, 957, 565, 11, 957, 11, 957, 1002, 50638], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 86, "seek": 36196, "start": 367.44, "end": 372.28, "text": " examples of where this is, where this is used. So, so LinkedIn is one. So the, who viewed", "tokens": [50638, 5110, 295, 689, 341, 307, 11, 689, 341, 307, 1143, 13, 407, 11, 370, 20657, 307, 472, 13, 407, 264, 11, 567, 19174, 50880], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 87, "seek": 36196, "start": 372.28, "end": 376.28, "text": " your profile, I guess most of you have probably seen this and you see like, I guess for people", "tokens": [50880, 428, 7964, 11, 286, 2041, 881, 295, 291, 362, 1391, 1612, 341, 293, 291, 536, 411, 11, 286, 2041, 337, 561, 51080], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 88, "seek": 36196, "start": 376.28, "end": 379.79999999999995, "text": " spying, spying on you. And if you, if you have, you can kind of see like all the people that", "tokens": [51080, 637, 1840, 11, 637, 1840, 322, 291, 13, 400, 498, 291, 11, 498, 291, 362, 11, 291, 393, 733, 295, 536, 411, 439, 264, 561, 300, 51256], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 89, "seek": 36196, "start": 379.79999999999995, "end": 383.03999999999996, "text": " look to you. And it's, it's very, it's very up to date, right? Like if I went and viewed", "tokens": [51256, 574, 281, 291, 13, 400, 309, 311, 11, 309, 311, 588, 11, 309, 311, 588, 493, 281, 4002, 11, 558, 30, 1743, 498, 286, 1437, 293, 19174, 51418], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 90, "seek": 36196, "start": 383.03999999999996, "end": 385.71999999999997, "text": " one of your profile pages, you would see it straight away. Like, Hey, look, Mark looks", "tokens": [51418, 472, 295, 428, 7964, 7183, 11, 291, 576, 536, 309, 2997, 1314, 13, 1743, 11, 1911, 11, 574, 11, 3934, 1542, 51552], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 91, "seek": 36196, "start": 385.71999999999997, "end": 390.24, "text": " at it. Use for that would be, Hey, maybe someone is in like, you often, I guess it's often", "tokens": [51552, 412, 309, 13, 8278, 337, 300, 576, 312, 11, 1911, 11, 1310, 1580, 307, 294, 411, 11, 291, 2049, 11, 286, 2041, 309, 311, 2049, 51778], "temperature": 0.0, "avg_logprob": -0.17713407653471383, "compression_ratio": 1.8813056379821957, "no_speech_prob": 0.02514488808810711}, {"id": 92, "seek": 39024, "start": 390.28000000000003, "end": 392.76, "text": " recruiters, right? You're like, Oh, I wonder why that person is following them. I'm going", "tokens": [50366, 15119, 433, 11, 558, 30, 509, 434, 411, 11, 876, 11, 286, 2441, 983, 300, 954, 307, 3480, 552, 13, 286, 478, 516, 50490], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 93, "seek": 39024, "start": 392.76, "end": 396.48, "text": " to, I'm going to contact them. So it's almost like, is there a real time way of like interacting", "tokens": [50490, 281, 11, 286, 478, 516, 281, 3385, 552, 13, 407, 309, 311, 1920, 411, 11, 307, 456, 257, 957, 565, 636, 295, 411, 18017, 50676], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 94, "seek": 39024, "start": 396.48, "end": 399.72, "text": " with someone? I don't know if you wanted to collaborate with them on something or yeah,", "tokens": [50676, 365, 1580, 30, 286, 500, 380, 458, 498, 291, 1415, 281, 18338, 365, 552, 322, 746, 420, 1338, 11, 50838], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 95, "seek": 39024, "start": 399.72, "end": 404.2, "text": " I guess they got a job that's available. They use it in the news field as well. So I guess", "tokens": [50838, 286, 2041, 436, 658, 257, 1691, 300, 311, 2435, 13, 814, 764, 309, 294, 264, 2583, 2519, 382, 731, 13, 407, 286, 2041, 51062], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 96, "seek": 39024, "start": 404.2, "end": 408.56, "text": " here is similar to what you would see in, I guess in Facebook, I guess even in TikTok", "tokens": [51062, 510, 307, 2531, 281, 437, 291, 576, 536, 294, 11, 286, 2041, 294, 4384, 11, 286, 2041, 754, 294, 20211, 51280], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 97, "seek": 39024, "start": 408.56, "end": 413.08, "text": " and those sorts of tools. The goal is kind of make you interact with, with this product", "tokens": [51280, 293, 729, 7527, 295, 3873, 13, 440, 3387, 307, 733, 295, 652, 291, 4648, 365, 11, 365, 341, 1674, 51506], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 98, "seek": 39024, "start": 413.08, "end": 416.68, "text": " more like they want you to stay on it. So they need to show you what is happening now", "tokens": [51506, 544, 411, 436, 528, 291, 281, 1754, 322, 309, 13, 407, 436, 643, 281, 855, 291, 437, 307, 2737, 586, 51686], "temperature": 0.0, "avg_logprob": -0.20951064040021197, "compression_ratio": 1.7806267806267806, "no_speech_prob": 0.20683789253234863}, {"id": 99, "seek": 41668, "start": 416.84000000000003, "end": 420.52, "text": " so that you're going to stay on it and not, and not, yeah, I guess to not go away and do", "tokens": [50372, 370, 300, 291, 434, 516, 281, 1754, 322, 309, 293, 406, 11, 293, 406, 11, 1338, 11, 286, 2041, 281, 406, 352, 1314, 293, 360, 50556], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 100, "seek": 41668, "start": 420.52, "end": 424.04, "text": " something else, which potentially is more useful, but they want you to just stay on", "tokens": [50556, 746, 1646, 11, 597, 7263, 307, 544, 4420, 11, 457, 436, 528, 291, 281, 445, 1754, 322, 50732], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 101, "seek": 41668, "start": 424.04, "end": 427.84000000000003, "text": " there. And then yeah, for LinkedIn, like this one's, I guess this one's like a little bit", "tokens": [50732, 456, 13, 400, 550, 1338, 11, 337, 20657, 11, 411, 341, 472, 311, 11, 286, 2041, 341, 472, 311, 411, 257, 707, 857, 50922], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 102, "seek": 41668, "start": 427.84000000000003, "end": 432.44, "text": " of a, a smaller user base, but yeah, for the recruiters, I can see like, okay, what is", "tokens": [50922, 295, 257, 11, 257, 4356, 4195, 3096, 11, 457, 1338, 11, 337, 264, 15119, 433, 11, 286, 393, 536, 411, 11, 1392, 11, 437, 307, 51152], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 103, "seek": 41668, "start": 432.44, "end": 437.04, "text": " the trends of what is happening in terms of what jobs are available, what places those", "tokens": [51152, 264, 13892, 295, 437, 307, 2737, 294, 2115, 295, 437, 4782, 366, 2435, 11, 437, 3190, 729, 51382], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 104, "seek": 41668, "start": 437.04, "end": 440.84000000000003, "text": " are in the world and so on. And then Uber Eats, let's do one more example. So Uber Eats", "tokens": [51382, 366, 294, 264, 1002, 293, 370, 322, 13, 400, 550, 21839, 462, 1720, 11, 718, 311, 360, 472, 544, 1365, 13, 407, 21839, 462, 1720, 51572], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 105, "seek": 41668, "start": 440.84000000000003, "end": 443.36, "text": " is another one. So this one's kind of like what I was saying. So they've got a dash,", "tokens": [51572, 307, 1071, 472, 13, 407, 341, 472, 311, 733, 295, 411, 437, 286, 390, 1566, 13, 407, 436, 600, 658, 257, 8240, 11, 51698], "temperature": 0.0, "avg_logprob": -0.16206812719155472, "compression_ratio": 1.817910447761194, "no_speech_prob": 0.03417101502418518}, {"id": 106, "seek": 44336, "start": 443.40000000000003, "end": 447.32, "text": " this is a very, very much a dashboard approach. And this is for a restaurant manager. So if", "tokens": [50366, 341, 307, 257, 588, 11, 588, 709, 257, 18342, 3109, 13, 400, 341, 307, 337, 257, 6383, 6598, 13, 407, 498, 50562], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 107, "seek": 44336, "start": 447.32, "end": 450.68, "text": " they were hosting the restaurant on there, and you can see like the things that are", "tokens": [50562, 436, 645, 16058, 264, 6383, 322, 456, 11, 293, 291, 393, 536, 411, 264, 721, 300, 366, 50730], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 108, "seek": 44336, "start": 450.68, "end": 453.6, "text": " interesting are they would get like missed orders. So hey, we've made a mess of this", "tokens": [50730, 1880, 366, 436, 576, 483, 411, 6721, 9470, 13, 407, 4177, 11, 321, 600, 1027, 257, 2082, 295, 341, 50876], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 109, "seek": 44336, "start": 453.6, "end": 457.40000000000003, "text": " order. Can we fix it like now rather than waiting till tomorrow? We've got this order", "tokens": [50876, 1668, 13, 1664, 321, 3191, 309, 411, 586, 2831, 813, 3806, 4288, 4153, 30, 492, 600, 658, 341, 1668, 51066], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 110, "seek": 44336, "start": 457.40000000000003, "end": 460.92, "text": " that's gone wrong. Can we, can we go and fix it? It's almost like you're able to achieve", "tokens": [51066, 300, 311, 2780, 2085, 13, 1664, 321, 11, 393, 321, 352, 293, 3191, 309, 30, 467, 311, 1920, 411, 291, 434, 1075, 281, 4584, 51242], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 111, "seek": 44336, "start": 460.92, "end": 463.76, "text": " like the customer service that you are in a restaurant where you can kind of see like", "tokens": [51242, 411, 264, 5474, 2643, 300, 291, 366, 294, 257, 6383, 689, 291, 393, 733, 295, 536, 411, 51384], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 112, "seek": 44336, "start": 463.76, "end": 467.24, "text": " with your eyes, okay, these people look really angry with me. It's like, hey, look, the data", "tokens": [51384, 365, 428, 2575, 11, 1392, 11, 613, 561, 574, 534, 6884, 365, 385, 13, 467, 311, 411, 11, 4177, 11, 574, 11, 264, 1412, 51558], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 113, "seek": 44336, "start": 467.24, "end": 471.12, "text": " is being shown is almost giving you the equivalent of the, in the restaurant experience without", "tokens": [51558, 307, 885, 4898, 307, 1920, 2902, 291, 264, 10344, 295, 264, 11, 294, 264, 6383, 1752, 1553, 51752], "temperature": 0.0, "avg_logprob": -0.16172858801755038, "compression_ratio": 1.868421052631579, "no_speech_prob": 0.02820737473666668}, {"id": 114, "seek": 47112, "start": 471.16, "end": 475.52, "text": " being in the restaurant. Okay, so what, how do we go about, so those are some examples of", "tokens": [50366, 885, 294, 264, 6383, 13, 1033, 11, 370, 437, 11, 577, 360, 321, 352, 466, 11, 370, 729, 366, 512, 5110, 295, 50584], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 115, "seek": 47112, "start": 475.68, "end": 478.76, "text": " people who have built those things and they're, they're a way more of them. Those are just", "tokens": [50592, 561, 567, 362, 3094, 729, 721, 293, 436, 434, 11, 436, 434, 257, 636, 544, 295, 552, 13, 3950, 366, 445, 50746], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 116, "seek": 47112, "start": 478.76, "end": 482.0, "text": " some, some ones that are picked up. How do we, how do we build that? So there are some", "tokens": [50746, 512, 11, 512, 2306, 300, 366, 6183, 493, 13, 1012, 360, 321, 11, 577, 360, 321, 1322, 300, 30, 407, 456, 366, 512, 50908], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 117, "seek": 47112, "start": 482.0, "end": 486.64, "text": " properties that we need to, we need to achieve. And some of these Javier was talking about", "tokens": [50908, 7221, 300, 321, 643, 281, 11, 321, 643, 281, 4584, 13, 400, 512, 295, 613, 508, 25384, 390, 1417, 466, 51140], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 118, "seek": 47112, "start": 486.64, "end": 490.52, "text": " in his talks. First one is we want to be able to get the data in quickly where it is in", "tokens": [51140, 294, 702, 6686, 13, 2386, 472, 307, 321, 528, 281, 312, 1075, 281, 483, 264, 1412, 294, 2661, 689, 309, 307, 294, 51334], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 119, "seek": 47112, "start": 490.52, "end": 494.16, "text": " these applications that generally coming from a streaming data platform of, of some", "tokens": [51334, 613, 5821, 300, 5101, 1348, 490, 257, 11791, 1412, 3663, 295, 11, 295, 512, 51516], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 120, "seek": 47112, "start": 494.16, "end": 498.32, "text": " sorts. In our talk, it's going to be Pulsar and we need to be able to get the data into", "tokens": [51516, 7527, 13, 682, 527, 751, 11, 309, 311, 516, 281, 312, 430, 9468, 289, 293, 321, 643, 281, 312, 1075, 281, 483, 264, 1412, 666, 51724], "temperature": 0.0, "avg_logprob": -0.18231581471449027, "compression_ratio": 1.844776119402985, "no_speech_prob": 0.009126255288720131}, {"id": 121, "seek": 49832, "start": 498.36, "end": 502.71999999999997, "text": " Pulsar and then into like wherever we want to get it to query it, in this case, into", "tokens": [50366, 430, 9468, 289, 293, 550, 666, 411, 8660, 321, 528, 281, 483, 309, 281, 14581, 309, 11, 294, 341, 1389, 11, 666, 50584], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 122, "seek": 49832, "start": 502.71999999999997, "end": 507.08, "text": " Pina, we need to get it in there very, very quickly. Once it's in there, we want to be", "tokens": [50584, 430, 1426, 11, 321, 643, 281, 483, 309, 294, 456, 588, 11, 588, 2661, 13, 3443, 309, 311, 294, 456, 11, 321, 528, 281, 312, 50802], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 123, "seek": 49832, "start": 507.08, "end": 512.04, "text": " able to query it very quickly as well. So one way of thinking about it is in these applications,", "tokens": [50802, 1075, 281, 14581, 309, 588, 2661, 382, 731, 13, 407, 472, 636, 295, 1953, 466, 309, 307, 294, 613, 5821, 11, 51050], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 124, "seek": 49832, "start": 512.04, "end": 517.36, "text": " we want to do OLTP type queries, like query speeds on OLAP data. So we want to be querying", "tokens": [51050, 321, 528, 281, 360, 39191, 16804, 2010, 24109, 11, 411, 14581, 16411, 322, 39191, 4715, 1412, 13, 407, 321, 528, 281, 312, 7083, 1840, 51316], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 125, "seek": 49832, "start": 517.36, "end": 521.24, "text": " everything but getting like the results in, in like, imagine like a refresh on a web page", "tokens": [51316, 1203, 457, 1242, 411, 264, 3542, 294, 11, 294, 411, 11, 3811, 411, 257, 15134, 322, 257, 3670, 3028, 51510], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 126, "seek": 49832, "start": 521.24, "end": 524.64, "text": " or like on a page on a web, on a mobile app. So I don't want to, don't want to be sitting", "tokens": [51510, 420, 411, 322, 257, 3028, 322, 257, 3670, 11, 322, 257, 6013, 724, 13, 407, 286, 500, 380, 528, 281, 11, 500, 380, 528, 281, 312, 3798, 51680], "temperature": 0.0, "avg_logprob": -0.14866025824295848, "compression_ratio": 1.967153284671533, "no_speech_prob": 0.005840844474732876}, {"id": 127, "seek": 52464, "start": 524.72, "end": 529.08, "text": " there waiting for five or 10 seconds for the results, right? And that, that, that particular", "tokens": [50368, 456, 3806, 337, 1732, 420, 1266, 3949, 337, 264, 3542, 11, 558, 30, 400, 300, 11, 300, 11, 300, 1729, 50586], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 128, "seek": 52464, "start": 529.08, "end": 533.92, "text": " requirement is a lot more the case when it's an external user, right? Like if it's inside", "tokens": [50586, 11695, 307, 257, 688, 544, 264, 1389, 562, 309, 311, 364, 8320, 4195, 11, 558, 30, 1743, 498, 309, 311, 1854, 50828], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 129, "seek": 52464, "start": 533.92, "end": 536.64, "text": " a company, because it's fine, you can just go and get a coffee and wait for the results.", "tokens": [50828, 257, 2237, 11, 570, 309, 311, 2489, 11, 291, 393, 445, 352, 293, 483, 257, 4982, 293, 1699, 337, 264, 3542, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 130, "seek": 52464, "start": 536.64, "end": 538.84, "text": " But if it's outside, you're not going to, you're not going to do that. They're going", "tokens": [50964, 583, 498, 309, 311, 2380, 11, 291, 434, 406, 516, 281, 11, 291, 434, 406, 516, 281, 360, 300, 13, 814, 434, 516, 51074], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 131, "seek": 52464, "start": 538.84, "end": 542.88, "text": " to use, use another application instead. And then finally, we want to be able to scale it,", "tokens": [51074, 281, 764, 11, 764, 1071, 3861, 2602, 13, 400, 550, 2721, 11, 321, 528, 281, 312, 1075, 281, 4373, 309, 11, 51276], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 132, "seek": 52464, "start": 542.88, "end": 546.96, "text": " right? So either it could be like one, one dashboard doing loads of different queries", "tokens": [51276, 558, 30, 407, 2139, 309, 727, 312, 411, 472, 11, 472, 18342, 884, 12668, 295, 819, 24109, 51480], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 133, "seek": 52464, "start": 546.96, "end": 551.72, "text": " and kind of aggregate and bringing everything together into one view or it's a lot, maybe", "tokens": [51480, 293, 733, 295, 26118, 293, 5062, 1203, 1214, 666, 472, 1910, 420, 309, 311, 257, 688, 11, 1310, 51718], "temperature": 0.0, "avg_logprob": -0.17275163191783277, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.0455026775598526}, {"id": 134, "seek": 55172, "start": 551.76, "end": 556.12, "text": " lots of users like concurrently doing it. But end result is lots of queries are coming", "tokens": [50366, 3195, 295, 5022, 411, 37702, 356, 884, 309, 13, 583, 917, 1874, 307, 3195, 295, 24109, 366, 1348, 50584], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 135, "seek": 55172, "start": 556.12, "end": 559.28, "text": " in. We need to be able to handle those and it can't affect those other two things either.", "tokens": [50584, 294, 13, 492, 643, 281, 312, 1075, 281, 4813, 729, 293, 309, 393, 380, 3345, 729, 661, 732, 721, 2139, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 136, "seek": 55172, "start": 559.28, "end": 563.64, "text": " So we need to be able to still be like doing lots of those concurrent queries while ingesting", "tokens": [50742, 407, 321, 643, 281, 312, 1075, 281, 920, 312, 411, 884, 3195, 295, 729, 37702, 24109, 1339, 3957, 8714, 50960], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 137, "seek": 55172, "start": 563.64, "end": 568.4, "text": " big amounts of data very quickly. So how do we go about building one of those? So these", "tokens": [50960, 955, 11663, 295, 1412, 588, 2661, 13, 407, 577, 360, 321, 352, 466, 2390, 472, 295, 729, 30, 407, 613, 51198], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 138, "seek": 55172, "start": 568.4, "end": 571.5600000000001, "text": " are, we kind of got around the outside some of the properties that you would have. And", "tokens": [51198, 366, 11, 321, 733, 295, 658, 926, 264, 2380, 512, 295, 264, 7221, 300, 291, 576, 362, 13, 400, 51356], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 139, "seek": 55172, "start": 571.5600000000001, "end": 575.28, "text": " then in the middle, we've got a couple of tools that can achieve this. So in this case,", "tokens": [51356, 550, 294, 264, 2808, 11, 321, 600, 658, 257, 1916, 295, 3873, 300, 393, 4584, 341, 13, 407, 294, 341, 1389, 11, 51542], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 140, "seek": 55172, "start": 575.28, "end": 578.96, "text": " Pulsar and Pino. And so you can kind of see we want to achieve real-time ingestion. The", "tokens": [51542, 430, 9468, 289, 293, 430, 2982, 13, 400, 370, 291, 393, 733, 295, 536, 321, 528, 281, 4584, 957, 12, 3766, 3957, 31342, 13, 440, 51726], "temperature": 0.0, "avg_logprob": -0.1765448808670044, "compression_ratio": 1.8482142857142858, "no_speech_prob": 0.008077137172222137}, {"id": 141, "seek": 57896, "start": 578.96, "end": 582.6800000000001, "text": " data will often be like very wide, like lots of, lots of columns, lots of properties potentially", "tokens": [50364, 1412, 486, 2049, 312, 411, 588, 4874, 11, 411, 3195, 295, 11, 3195, 295, 13766, 11, 3195, 295, 7221, 7263, 50550], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 142, "seek": 57896, "start": 582.6800000000001, "end": 585.24, "text": " nested. And then we've got to do something with it to figure out how we're going to get", "tokens": [50550, 15646, 292, 13, 400, 550, 321, 600, 658, 281, 360, 746, 365, 309, 281, 2573, 484, 577, 321, 434, 516, 281, 483, 50678], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 143, "seek": 57896, "start": 585.24, "end": 589.08, "text": " it into a structure that we can query it. And then yeah, you can kind of see some of", "tokens": [50678, 309, 666, 257, 3877, 300, 321, 393, 14581, 309, 13, 400, 550, 1338, 11, 291, 393, 733, 295, 536, 512, 295, 50870], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 144, "seek": 57896, "start": 589.08, "end": 593.0, "text": " them. So we need to be able to, the data needs to be fresh. We want to do thousands of queries", "tokens": [50870, 552, 13, 407, 321, 643, 281, 312, 1075, 281, 11, 264, 1412, 2203, 281, 312, 4451, 13, 492, 528, 281, 360, 5383, 295, 24109, 51066], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 145, "seek": 57896, "start": 593.0, "end": 597.88, "text": " a second. And then the latency needs to be like OLTP style. So I'm going to hand the", "tokens": [51066, 257, 1150, 13, 400, 550, 264, 27043, 2203, 281, 312, 411, 39191, 16804, 3758, 13, 407, 286, 478, 516, 281, 1011, 264, 51310], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 146, "seek": 57896, "start": 597.88, "end": 598.88, "text": " microphone back to Mary now.", "tokens": [51310, 10952, 646, 281, 6059, 586, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 147, "seek": 57896, "start": 598.88, "end": 604.24, "text": " Okay. Thank you. Okay. Thank you, Mark. So, okay. So now we're going to focus just couple", "tokens": [51360, 1033, 13, 1044, 291, 13, 1033, 13, 1044, 291, 11, 3934, 13, 407, 11, 1392, 13, 407, 586, 321, 434, 516, 281, 1879, 445, 1916, 51628], "temperature": 0.0, "avg_logprob": -0.17129105788010818, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.007435754872858524}, {"id": 148, "seek": 60424, "start": 604.24, "end": 609.32, "text": " minutes on Apache Pulsar. So Pulsar, right? How many of you actually real quickly have", "tokens": [50364, 2077, 322, 46597, 430, 9468, 289, 13, 407, 430, 9468, 289, 11, 558, 30, 1012, 867, 295, 291, 767, 957, 2661, 362, 50618], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 149, "seek": 60424, "start": 609.32, "end": 614.0, "text": " heard of Pulsar or working? Oh, working. Okay, cool. So some of you have, and then of course", "tokens": [50618, 2198, 295, 430, 9468, 289, 420, 1364, 30, 876, 11, 1364, 13, 1033, 11, 1627, 13, 407, 512, 295, 291, 362, 11, 293, 550, 295, 1164, 50852], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 150, "seek": 60424, "start": 614.0, "end": 618.8, "text": " there are also folks using Kafka too. But here I'm wanting to say, to tell you why we", "tokens": [50852, 456, 366, 611, 4024, 1228, 47064, 886, 13, 583, 510, 286, 478, 7935, 281, 584, 11, 281, 980, 291, 983, 321, 51092], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 151, "seek": 60424, "start": 618.8, "end": 623.36, "text": " want to use Pulsar, right? So right now in here too, for those of you who are new and", "tokens": [51092, 528, 281, 764, 430, 9468, 289, 11, 558, 30, 407, 558, 586, 294, 510, 886, 11, 337, 729, 295, 291, 567, 366, 777, 293, 51320], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 152, "seek": 60424, "start": 623.36, "end": 628.5600000000001, "text": " essentially to Pulsar, there are like a couple components to it, but primarily to their brokers", "tokens": [51320, 4476, 281, 430, 9468, 289, 11, 456, 366, 411, 257, 1916, 6677, 281, 309, 11, 457, 10029, 281, 641, 47549, 51580], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 153, "seek": 60424, "start": 628.5600000000001, "end": 633.8, "text": " that are serverless Java runtime, right? And running, but it's very flexible too. So for", "tokens": [51580, 300, 366, 7154, 1832, 10745, 34474, 11, 558, 30, 400, 2614, 11, 457, 309, 311, 588, 11358, 886, 13, 407, 337, 51842], "temperature": 0.0, "avg_logprob": -0.18212309399166624, "compression_ratio": 1.745928338762215, "no_speech_prob": 0.4834393262863159}, {"id": 154, "seek": 63380, "start": 633.8399999999999, "end": 638.4799999999999, "text": " clients, you are supposed to be writing your producer and consumer, it takes on a pop-up", "tokens": [50366, 6982, 11, 291, 366, 3442, 281, 312, 3579, 428, 12314, 293, 9711, 11, 309, 2516, 322, 257, 1665, 12, 1010, 50598], "temperature": 0.0, "avg_logprob": -0.172832795551845, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.030570868402719498}, {"id": 155, "seek": 63380, "start": 638.4799999999999, "end": 643.24, "text": " type of architectural pattern, right? So I won't have time to get into all the details,", "tokens": [50598, 2010, 295, 26621, 5102, 11, 558, 30, 407, 286, 1582, 380, 362, 565, 281, 483, 666, 439, 264, 4365, 11, 50836], "temperature": 0.0, "avg_logprob": -0.172832795551845, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.030570868402719498}, {"id": 156, "seek": 63380, "start": 643.24, "end": 646.4799999999999, "text": " but just give you a highlight, right? Producer, consumer is what you write. You can write it", "tokens": [50836, 457, 445, 976, 291, 257, 5078, 11, 558, 30, 33034, 11, 9711, 307, 437, 291, 2464, 13, 509, 393, 2464, 309, 50998], "temperature": 0.0, "avg_logprob": -0.172832795551845, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.030570868402719498}, {"id": 157, "seek": 63380, "start": 646.4799999999999, "end": 651.92, "text": " in Java, in Go, in Python, in any kind of languages that are supported by the community", "tokens": [50998, 294, 10745, 11, 294, 1037, 11, 294, 15329, 11, 294, 604, 733, 295, 8650, 300, 366, 8104, 538, 264, 1768, 51270], "temperature": 0.0, "avg_logprob": -0.172832795551845, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.030570868402719498}, {"id": 158, "seek": 63380, "start": 651.92, "end": 657.4, "text": " too. And then multiple brokers that are running and also is optimized too to run in the cloud", "tokens": [51270, 886, 13, 400, 550, 3866, 47549, 300, 366, 2614, 293, 611, 307, 26941, 886, 281, 1190, 294, 264, 4588, 51544], "temperature": 0.0, "avg_logprob": -0.172832795551845, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.030570868402719498}, {"id": 159, "seek": 65740, "start": 657.4, "end": 664.52, "text": " native environment. Also too, instead of it managing all of the huge amounts of log messages,", "tokens": [50364, 8470, 2823, 13, 2743, 886, 11, 2602, 295, 309, 11642, 439, 295, 264, 2603, 11663, 295, 3565, 7897, 11, 50720], "temperature": 0.0, "avg_logprob": -0.2707545757293701, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.06446090340614319}, {"id": 160, "seek": 65740, "start": 664.52, "end": 669.0799999999999, "text": " it actually leveraged on Bookkey, which is a patchy bookkeeper project. And it's also", "tokens": [50720, 309, 767, 12451, 2980, 322, 9476, 4119, 11, 597, 307, 257, 9972, 88, 1446, 23083, 1716, 13, 400, 309, 311, 611, 50948], "temperature": 0.0, "avg_logprob": -0.2707545757293701, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.06446090340614319}, {"id": 161, "seek": 65740, "start": 669.0799999999999, "end": 676.4399999999999, "text": " like a high availability or fast read and fast write type of log, logging, distributed", "tokens": [50948, 411, 257, 1090, 17945, 420, 2370, 1401, 293, 2370, 2464, 2010, 295, 3565, 11, 27991, 11, 12631, 51316], "temperature": 0.0, "avg_logprob": -0.2707545757293701, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.06446090340614319}, {"id": 162, "seek": 65740, "start": 676.4399999999999, "end": 682.96, "text": " logging system. Then it also makes use of Zookeeper to help it to manage the cluster, that aspect", "tokens": [51316, 27991, 1185, 13, 1396, 309, 611, 1669, 764, 295, 34589, 23083, 281, 854, 309, 281, 3067, 264, 13630, 11, 300, 4171, 51642], "temperature": 0.0, "avg_logprob": -0.2707545757293701, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.06446090340614319}, {"id": 163, "seek": 68296, "start": 683.0, "end": 688.64, "text": " of things. And now really quick to kind of give you an introduction. Pulsar was developed", "tokens": [50366, 295, 721, 13, 400, 586, 534, 1702, 281, 733, 295, 976, 291, 364, 9339, 13, 430, 9468, 289, 390, 4743, 50648], "temperature": 0.0, "avg_logprob": -0.1654186248779297, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.008638432249426842}, {"id": 164, "seek": 68296, "start": 688.64, "end": 694.76, "text": " first by Yahoo back in 2013 or so, and it's basically recognizing we need an event streaming", "tokens": [50648, 700, 538, 41757, 646, 294, 9012, 420, 370, 11, 293, 309, 311, 1936, 18538, 321, 643, 364, 2280, 11791, 50954], "temperature": 0.0, "avg_logprob": -0.1654186248779297, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.008638432249426842}, {"id": 165, "seek": 68296, "start": 694.76, "end": 700.76, "text": " platform that can run very effectively in a cloud native environment. They contributed", "tokens": [50954, 3663, 300, 393, 1190, 588, 8659, 294, 257, 4588, 8470, 2823, 13, 814, 18434, 51254], "temperature": 0.0, "avg_logprob": -0.1654186248779297, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.008638432249426842}, {"id": 166, "seek": 68296, "start": 700.76, "end": 706.72, "text": " to Apache Software Foundation in 2016 and then very quickly became a top level project", "tokens": [51254, 281, 46597, 27428, 10335, 294, 6549, 293, 550, 588, 2661, 3062, 257, 1192, 1496, 1716, 51552], "temperature": 0.0, "avg_logprob": -0.1654186248779297, "compression_ratio": 1.4650205761316872, "no_speech_prob": 0.008638432249426842}, {"id": 167, "seek": 70672, "start": 706.72, "end": 713.44, "text": " in 2018. And again, it's very cloud native in nature. It's already cluster based. Multitenancy", "tokens": [50364, 294, 6096, 13, 400, 797, 11, 309, 311, 588, 4588, 8470, 294, 3687, 13, 467, 311, 1217, 13630, 2361, 13, 14665, 6009, 6717, 50700], "temperature": 0.0, "avg_logprob": -0.1561263225696705, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0187678299844265}, {"id": 168, "seek": 70672, "start": 713.44, "end": 717.76, "text": " is supported too. Again, I talked already about the simple client APIs that you can", "tokens": [50700, 307, 8104, 886, 13, 3764, 11, 286, 2825, 1217, 466, 264, 2199, 6423, 21445, 300, 291, 393, 50916], "temperature": 0.0, "avg_logprob": -0.1561263225696705, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0187678299844265}, {"id": 169, "seek": 70672, "start": 717.76, "end": 722.5600000000001, "text": " write in many different languages. And it separates, one of the strengths of Pulsar", "tokens": [50916, 2464, 294, 867, 819, 8650, 13, 400, 309, 34149, 11, 472, 295, 264, 16986, 295, 430, 9468, 289, 51156], "temperature": 0.0, "avg_logprob": -0.1561263225696705, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0187678299844265}, {"id": 170, "seek": 70672, "start": 722.5600000000001, "end": 727.52, "text": " is that it separates out the compute and the storage. So Pulsar manage all of the message", "tokens": [51156, 307, 300, 309, 34149, 484, 264, 14722, 293, 264, 6725, 13, 407, 430, 9468, 289, 3067, 439, 295, 264, 3636, 51404], "temperature": 0.0, "avg_logprob": -0.1561263225696705, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0187678299844265}, {"id": 171, "seek": 70672, "start": 727.52, "end": 732.84, "text": " things and then have Bookkeeper manage all of the log messages and stuff. And then also", "tokens": [51404, 721, 293, 550, 362, 9476, 23083, 3067, 439, 295, 264, 3565, 7897, 293, 1507, 13, 400, 550, 611, 51670], "temperature": 0.0, "avg_logprob": -0.1561263225696705, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.0187678299844265}, {"id": 172, "seek": 73284, "start": 732.9200000000001, "end": 738.24, "text": " Pulsar has guaranteed message delivery. And also it has a Pulsar function framework that's", "tokens": [50368, 430, 9468, 289, 575, 18031, 3636, 8982, 13, 400, 611, 309, 575, 257, 430, 9468, 289, 2445, 8388, 300, 311, 50634], "temperature": 0.0, "avg_logprob": -0.20548303310687727, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0009682598174549639}, {"id": 173, "seek": 73284, "start": 738.24, "end": 743.96, "text": " very lightweight too. So then you don't need to rely on any external libraries or vendors", "tokens": [50634, 588, 22052, 886, 13, 407, 550, 291, 500, 380, 643, 281, 10687, 322, 604, 8320, 15148, 420, 22056, 50920], "temperature": 0.0, "avg_logprob": -0.20548303310687727, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0009682598174549639}, {"id": 174, "seek": 73284, "start": 743.96, "end": 750.1600000000001, "text": " essentially to kind of do message transformation as you are constructing your data pipeline", "tokens": [50920, 4476, 281, 733, 295, 360, 3636, 9887, 382, 291, 366, 39969, 428, 1412, 15517, 51230], "temperature": 0.0, "avg_logprob": -0.20548303310687727, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0009682598174549639}, {"id": 175, "seek": 73284, "start": 750.1600000000001, "end": 755.76, "text": " too. And also another feature about it is that it has tiered storage offload. So if", "tokens": [51230, 886, 13, 400, 611, 1071, 4111, 466, 309, 307, 300, 309, 575, 12362, 292, 6725, 766, 2907, 13, 407, 498, 51510], "temperature": 0.0, "avg_logprob": -0.20548303310687727, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0009682598174549639}, {"id": 176, "seek": 73284, "start": 755.76, "end": 762.0400000000001, "text": " there's messages that becomes cold, then you can move it off to, or actually if it kind", "tokens": [51510, 456, 311, 7897, 300, 3643, 3554, 11, 550, 291, 393, 1286, 309, 766, 281, 11, 420, 767, 498, 309, 733, 51824], "temperature": 0.0, "avg_logprob": -0.20548303310687727, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0009682598174549639}, {"id": 177, "seek": 76204, "start": 762.04, "end": 766.5999999999999, "text": " of becomes cold, then it gets moved off to offline storage such as like S3 buckets and", "tokens": [50364, 295, 3643, 3554, 11, 550, 309, 2170, 4259, 766, 281, 21857, 6725, 1270, 382, 411, 318, 18, 32191, 293, 50592], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 178, "seek": 76204, "start": 766.5999999999999, "end": 770.88, "text": " things like that. Okay, so just real quick slide just to show too like about streaming", "tokens": [50592, 721, 411, 300, 13, 1033, 11, 370, 445, 957, 1702, 4137, 445, 281, 855, 886, 411, 466, 11791, 50806], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 179, "seek": 76204, "start": 770.88, "end": 775.5999999999999, "text": " and not versus not streaming. So in modern day streaming, as you can see, we're ingesting", "tokens": [50806, 293, 406, 5717, 406, 11791, 13, 407, 294, 4363, 786, 11791, 11, 382, 291, 393, 536, 11, 321, 434, 3957, 8714, 51042], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 180, "seek": 76204, "start": 775.5999999999999, "end": 781.9599999999999, "text": " data and this is what we'll be ingesting for like, you know, analytics software processing", "tokens": [51042, 1412, 293, 341, 307, 437, 321, 603, 312, 3957, 8714, 337, 411, 11, 291, 458, 11, 15370, 4722, 9007, 51360], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 181, "seek": 76204, "start": 781.9599999999999, "end": 786.8399999999999, "text": " so like Pino, you ingest data and then without actually writing to disk like the traditional", "tokens": [51360, 370, 411, 430, 2982, 11, 291, 3957, 377, 1412, 293, 550, 1553, 767, 3579, 281, 12355, 411, 264, 5164, 51604], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 182, "seek": 76204, "start": 786.8399999999999, "end": 791.3199999999999, "text": " way of doing things. So then it speeds up to the whole process. And it processes the", "tokens": [51604, 636, 295, 884, 721, 13, 407, 550, 309, 16411, 493, 281, 264, 1379, 1399, 13, 400, 309, 7555, 264, 51828], "temperature": 0.0, "avg_logprob": -0.16134702697280764, "compression_ratio": 1.7733333333333334, "no_speech_prob": 0.0028311789501458406}, {"id": 183, "seek": 79132, "start": 791.32, "end": 795.7600000000001, "text": " data in memory. And when you're done with processing, you can use Pulsar function to", "tokens": [50364, 1412, 294, 4675, 13, 400, 562, 291, 434, 1096, 365, 9007, 11, 291, 393, 764, 430, 9468, 289, 2445, 281, 50586], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 184, "seek": 79132, "start": 795.7600000000001, "end": 800.2, "text": " transform your data, whatever you need, and then you output your data to a sync. So there", "tokens": [50586, 4088, 428, 1412, 11, 2035, 291, 643, 11, 293, 550, 291, 5598, 428, 1412, 281, 257, 20271, 13, 407, 456, 50808], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 185, "seek": 79132, "start": 800.2, "end": 804.72, "text": " will be connectors that that helps you to do that. So the whole kind of pipeline is designed", "tokens": [50808, 486, 312, 31865, 300, 300, 3665, 291, 281, 360, 300, 13, 407, 264, 1379, 733, 295, 15517, 307, 4761, 51034], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 186, "seek": 79132, "start": 804.72, "end": 810.0400000000001, "text": " to be very efficient. That's what it is. So and now we back to Pino a little more and", "tokens": [51034, 281, 312, 588, 7148, 13, 663, 311, 437, 309, 307, 13, 407, 293, 586, 321, 646, 281, 430, 2982, 257, 707, 544, 293, 51300], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 187, "seek": 79132, "start": 810.0400000000001, "end": 815.12, "text": " then with the demo too. All right, so what is what is Pino? So this diagram more or less", "tokens": [51300, 550, 365, 264, 10723, 886, 13, 1057, 558, 11, 370, 437, 307, 437, 307, 430, 2982, 30, 407, 341, 10686, 544, 420, 1570, 51554], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 188, "seek": 79132, "start": 815.12, "end": 821.24, "text": " explains it. So as I say, we've got data coming in from from sources. So in our case, it's", "tokens": [51554, 13948, 309, 13, 407, 382, 286, 584, 11, 321, 600, 658, 1412, 1348, 294, 490, 490, 7139, 13, 407, 294, 527, 1389, 11, 309, 311, 51860], "temperature": 0.0, "avg_logprob": -0.14283699459499782, "compression_ratio": 1.7193548387096773, "no_speech_prob": 0.0036414116621017456}, {"id": 189, "seek": 82124, "start": 821.24, "end": 824.48, "text": " going to be Pulsar, but you can kind of see there are lots of other ones. And what's kind", "tokens": [50364, 516, 281, 312, 430, 9468, 289, 11, 457, 291, 393, 733, 295, 536, 456, 366, 3195, 295, 661, 2306, 13, 400, 437, 311, 733, 50526], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 190, "seek": 82124, "start": 824.48, "end": 828.04, "text": " of interesting is you could you could have in theory, you could load into the same tables", "tokens": [50526, 295, 1880, 307, 291, 727, 291, 727, 362, 294, 5261, 11, 291, 727, 3677, 666, 264, 912, 8020, 50704], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 191, "seek": 82124, "start": 828.04, "end": 832.44, "text": " streaming and batch data sources and query them query them both together. Once they come", "tokens": [50704, 11791, 293, 15245, 1412, 7139, 293, 14581, 552, 14581, 552, 1293, 1214, 13, 3443, 436, 808, 50924], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 192, "seek": 82124, "start": 832.44, "end": 837.5600000000001, "text": " in, it's a it's a column store. So it's a store. And then you can kind of do some aggregation.", "tokens": [50924, 294, 11, 309, 311, 257, 309, 311, 257, 7738, 3531, 13, 407, 309, 311, 257, 3531, 13, 400, 550, 291, 393, 733, 295, 360, 512, 16743, 399, 13, 51180], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 193, "seek": 82124, "start": 837.5600000000001, "end": 841.1, "text": " There's all the different types of indexes on top of that. And you can do some prematerialization", "tokens": [51180, 821, 311, 439, 264, 819, 3467, 295, 8186, 279, 322, 1192, 295, 300, 13, 400, 291, 393, 360, 512, 5624, 40364, 2144, 51357], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 194, "seek": 82124, "start": 841.1, "end": 845.24, "text": " as well. And then on the right hand side, we've got a couple of the use cases. So just", "tokens": [51357, 382, 731, 13, 400, 550, 322, 264, 558, 1011, 1252, 11, 321, 600, 658, 257, 1916, 295, 264, 764, 3331, 13, 407, 445, 51564], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 195, "seek": 82124, "start": 845.24, "end": 850.48, "text": " to quickly show you the architecture of how this how this works from the from the far side.", "tokens": [51564, 281, 2661, 855, 291, 264, 9482, 295, 577, 341, 577, 341, 1985, 490, 264, 490, 264, 1400, 1252, 13, 51826], "temperature": 0.0, "avg_logprob": -0.13214845542448111, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.004523200914263725}, {"id": 196, "seek": 85048, "start": 850.48, "end": 854.72, "text": " So the data is coming in from Pulsar here. We've got there are we're going to be using", "tokens": [50364, 407, 264, 1412, 307, 1348, 294, 490, 430, 9468, 289, 510, 13, 492, 600, 658, 456, 366, 321, 434, 516, 281, 312, 1228, 50576], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 197, "seek": 85048, "start": 854.72, "end": 857.24, "text": " three components. We've got a controller, we've got a server and then we'll have a broker,", "tokens": [50576, 1045, 6677, 13, 492, 600, 658, 257, 10561, 11, 321, 600, 658, 257, 7154, 293, 550, 321, 603, 362, 257, 26502, 11, 50702], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 198, "seek": 85048, "start": 857.24, "end": 861.08, "text": " which will come up here. So the controller is the manager of the cluster. So it's taking", "tokens": [50702, 597, 486, 808, 493, 510, 13, 407, 264, 10561, 307, 264, 6598, 295, 264, 13630, 13, 407, 309, 311, 1940, 50894], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 199, "seek": 85048, "start": 861.08, "end": 866.16, "text": " care of a hey, where does this where does this data need to go and then and Pino uses", "tokens": [50894, 1127, 295, 257, 4177, 11, 689, 775, 341, 689, 775, 341, 1412, 643, 281, 352, 293, 550, 293, 430, 2982, 4960, 51148], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 200, "seek": 85048, "start": 866.16, "end": 872.28, "text": " a tool called Helix, which is on top of on top of zookeeper. And so it will then it breaks", "tokens": [51148, 257, 2290, 1219, 6128, 970, 11, 597, 307, 322, 1192, 295, 322, 1192, 295, 25347, 23083, 13, 400, 370, 309, 486, 550, 309, 9857, 51454], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 201, "seek": 85048, "start": 872.28, "end": 878.08, "text": " data out into segments. So the segments will in this particular case will map to the partitions", "tokens": [51454, 1412, 484, 666, 19904, 13, 407, 264, 19904, 486, 294, 341, 1729, 1389, 486, 4471, 281, 264, 644, 2451, 51744], "temperature": 0.0, "avg_logprob": -0.1698286683409364, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.002235689200460911}, {"id": 202, "seek": 87808, "start": 878.08, "end": 882.8000000000001, "text": " coming from Pulsar. So what like each partition will be coming to a segment. So if we had,", "tokens": [50364, 1348, 490, 430, 9468, 289, 13, 407, 437, 411, 1184, 24808, 486, 312, 1348, 281, 257, 9469, 13, 407, 498, 321, 632, 11, 50600], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 203, "seek": 87808, "start": 882.8000000000001, "end": 886.1600000000001, "text": " for example, four partitions, we get four, four different segments. And if you were doing", "tokens": [50600, 337, 1365, 11, 1451, 644, 2451, 11, 321, 483, 1451, 11, 1451, 819, 19904, 13, 400, 498, 291, 645, 884, 50768], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 204, "seek": 87808, "start": 886.1600000000001, "end": 888.96, "text": " it in the cluster, in our case, we'll just did on my machine. But if you were doing it", "tokens": [50768, 309, 294, 264, 13630, 11, 294, 527, 1389, 11, 321, 603, 445, 630, 322, 452, 3479, 13, 583, 498, 291, 645, 884, 309, 50908], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 205, "seek": 87808, "start": 888.96, "end": 893.48, "text": " in a cluster, it would then have decide where is it going to replicate each of the data coming", "tokens": [50908, 294, 257, 13630, 11, 309, 576, 550, 362, 4536, 689, 307, 309, 516, 281, 25356, 1184, 295, 264, 1412, 1348, 51134], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 206, "seek": 87808, "start": 893.48, "end": 897.6800000000001, "text": " from each of the partitions. I might do one, one is on server one and seven four, and two", "tokens": [51134, 490, 1184, 295, 264, 644, 2451, 13, 286, 1062, 360, 472, 11, 472, 307, 322, 7154, 472, 293, 3407, 1451, 11, 293, 732, 51344], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 207, "seek": 87808, "start": 897.6800000000001, "end": 901.6800000000001, "text": " is on server two and seven three and so on at the servers in the data. So that's where", "tokens": [51344, 307, 322, 7154, 732, 293, 3407, 1045, 293, 370, 322, 412, 264, 15909, 294, 264, 1412, 13, 407, 300, 311, 689, 51544], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 208, "seek": 87808, "start": 901.6800000000001, "end": 905.48, "text": " the data is going. Remember, the controller manages everything. And then we have the broker", "tokens": [51544, 264, 1412, 307, 516, 13, 5459, 11, 264, 10561, 22489, 1203, 13, 400, 550, 321, 362, 264, 26502, 51734], "temperature": 0.0, "avg_logprob": -0.21781379420582841, "compression_ratio": 1.929663608562691, "no_speech_prob": 0.014840524643659592}, {"id": 209, "seek": 90548, "start": 905.52, "end": 909.24, "text": " is taking care of the querying. So the query comes in here. So for example, here we're", "tokens": [50366, 307, 1940, 1127, 295, 264, 7083, 1840, 13, 407, 264, 14581, 1487, 294, 510, 13, 407, 337, 1365, 11, 510, 321, 434, 50552], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 210, "seek": 90548, "start": 909.24, "end": 914.24, "text": " counting from a table how many how many rows for the country us. And then the broker sends", "tokens": [50552, 13251, 490, 257, 3199, 577, 867, 577, 867, 13241, 337, 264, 1941, 505, 13, 400, 550, 264, 26502, 14790, 50802], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 211, "seek": 90548, "start": 914.24, "end": 918.2, "text": " it out in a scatter gather type pattern out to the servers that it knows have the data", "tokens": [50802, 309, 484, 294, 257, 34951, 5448, 2010, 5102, 484, 281, 264, 15909, 300, 309, 3255, 362, 264, 1412, 51000], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 212, "seek": 90548, "start": 918.2, "end": 922.4, "text": " that will map that might be appropriate. And then it will they will kind of send their result", "tokens": [51000, 300, 486, 4471, 300, 1062, 312, 6854, 13, 400, 550, 309, 486, 436, 486, 733, 295, 2845, 641, 1874, 51210], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 213, "seek": 90548, "start": 922.4, "end": 928.04, "text": " back and then the broker takes care of aggregating it and sending it back to the client. Okay,", "tokens": [51210, 646, 293, 550, 264, 26502, 2516, 1127, 295, 16743, 990, 309, 293, 7750, 309, 646, 281, 264, 6423, 13, 1033, 11, 51492], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 214, "seek": 90548, "start": 928.04, "end": 932.72, "text": " now let's have a look at a demo. Let's see if I can get this to. So what we're going to", "tokens": [51492, 586, 718, 311, 362, 257, 574, 412, 257, 10723, 13, 961, 311, 536, 498, 286, 393, 483, 341, 281, 13, 407, 437, 321, 434, 516, 281, 51726], "temperature": 0.0, "avg_logprob": -0.17953847981185364, "compression_ratio": 1.8154362416107384, "no_speech_prob": 0.25292691588401794}, {"id": 215, "seek": 93272, "start": 932.76, "end": 938.44, "text": " do is if in case you want to look at it, this is where the demo lives. Hopefully that QR", "tokens": [50366, 360, 307, 498, 294, 1389, 291, 528, 281, 574, 412, 309, 11, 341, 307, 689, 264, 10723, 2909, 13, 10429, 300, 32784, 50650], "temperature": 0.0, "avg_logprob": -0.17101207309299046, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.02898300625383854}, {"id": 216, "seek": 93272, "start": 938.44, "end": 946.2, "text": " code still works. I'll just let you take a picture of it. And what it what it is, we can kind of", "tokens": [50650, 3089, 920, 1985, 13, 286, 603, 445, 718, 291, 747, 257, 3036, 295, 309, 13, 400, 437, 309, 437, 309, 307, 11, 321, 393, 733, 295, 51038], "temperature": 0.0, "avg_logprob": -0.17101207309299046, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.02898300625383854}, {"id": 217, "seek": 93272, "start": 946.2, "end": 951.64, "text": " tell from the name. So it's a wiki demo. It's going to sit in as I say before, it's going to sit", "tokens": [51038, 980, 490, 264, 1315, 13, 407, 309, 311, 257, 261, 9850, 10723, 13, 467, 311, 516, 281, 1394, 294, 382, 286, 584, 949, 11, 309, 311, 516, 281, 1394, 51310], "temperature": 0.0, "avg_logprob": -0.17101207309299046, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.02898300625383854}, {"id": 218, "seek": 93272, "start": 951.64, "end": 956.0, "text": " in this bottom corner. So real time dashboard. And this is the architecture of it. So we've got a", "tokens": [51310, 294, 341, 2767, 4538, 13, 407, 957, 565, 18342, 13, 400, 341, 307, 264, 9482, 295, 309, 13, 407, 321, 600, 658, 257, 51528], "temperature": 0.0, "avg_logprob": -0.17101207309299046, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.02898300625383854}, {"id": 219, "seek": 93272, "start": 956.0, "end": 959.6800000000001, "text": " streaming API. And the data is going to come in, we've got a Python application, it's going to", "tokens": [51528, 11791, 9362, 13, 400, 264, 1412, 307, 516, 281, 808, 294, 11, 321, 600, 658, 257, 15329, 3861, 11, 309, 311, 516, 281, 51712], "temperature": 0.0, "avg_logprob": -0.17101207309299046, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.02898300625383854}, {"id": 220, "seek": 95968, "start": 959.68, "end": 963.76, "text": " process it, put it into Palsar, from Palsar into Pino. And then we're going to have a stream", "tokens": [50364, 1399, 309, 11, 829, 309, 666, 430, 1124, 289, 11, 490, 430, 1124, 289, 666, 430, 2982, 13, 400, 550, 321, 434, 516, 281, 362, 257, 4309, 50568], "temperature": 0.0, "avg_logprob": -0.24359963156960227, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.014217904768884182}, {"id": 221, "seek": 95968, "start": 963.76, "end": 969.4399999999999, "text": " that dashboard on the end. We've got 15 minutes. So let's see how we go. So wiki media has a really", "tokens": [50568, 300, 18342, 322, 264, 917, 13, 492, 600, 658, 2119, 2077, 13, 407, 718, 311, 536, 577, 321, 352, 13, 407, 261, 9850, 3021, 575, 257, 534, 50852], "temperature": 0.0, "avg_logprob": -0.24359963156960227, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.014217904768884182}, {"id": 222, "seek": 95968, "start": 969.4399999999999, "end": 977.4399999999999, "text": " nice recent changes feed. So this is capturing all the changes that are being done to different", "tokens": [50852, 1481, 5162, 2962, 3154, 13, 407, 341, 307, 23384, 439, 264, 2962, 300, 366, 885, 1096, 281, 819, 51252], "temperature": 0.0, "avg_logprob": -0.24359963156960227, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.014217904768884182}, {"id": 223, "seek": 95968, "start": 977.4399999999999, "end": 982.56, "text": " properties in wiki media. And they actually store it internally as Kafka, but then expose it using", "tokens": [51252, 7221, 294, 261, 9850, 3021, 13, 400, 436, 767, 3531, 309, 19501, 382, 47064, 11, 457, 550, 19219, 309, 1228, 51508], "temperature": 0.0, "avg_logprob": -0.24359963156960227, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.014217904768884182}, {"id": 224, "seek": 95968, "start": 982.56, "end": 986.8, "text": " server side events protocols. This is a HTTP protocol, it basically just streams out loads", "tokens": [51508, 7154, 1252, 3931, 20618, 13, 639, 307, 257, 33283, 10336, 11, 309, 1936, 445, 15842, 484, 12668, 51720], "temperature": 0.0, "avg_logprob": -0.24359963156960227, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.014217904768884182}, {"id": 225, "seek": 98680, "start": 986.8, "end": 992.0, "text": " and loads of it, it reflects from the infinite stream of these events. And this is what it looks", "tokens": [50364, 293, 12668, 295, 309, 11, 309, 18926, 490, 264, 13785, 4309, 295, 613, 3931, 13, 400, 341, 307, 437, 309, 1542, 50624], "temperature": 0.0, "avg_logprob": -0.18583413057549056, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0064228796400129795}, {"id": 226, "seek": 98680, "start": 992.0, "end": 996.4, "text": " like. So we get three properties to get an event, an ID and a data. The main bit that's", "tokens": [50624, 411, 13, 407, 321, 483, 1045, 7221, 281, 483, 364, 2280, 11, 364, 7348, 293, 257, 1412, 13, 440, 2135, 857, 300, 311, 50844], "temperature": 0.0, "avg_logprob": -0.18583413057549056, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0064228796400129795}, {"id": 227, "seek": 98680, "start": 996.4, "end": 1000.56, "text": " interesting is data. So you can kind of see it's like a sort of nested JSON structure of stuff.", "tokens": [50844, 1880, 307, 1412, 13, 407, 291, 393, 733, 295, 536, 309, 311, 411, 257, 1333, 295, 15646, 292, 31828, 3877, 295, 1507, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18583413057549056, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0064228796400129795}, {"id": 228, "seek": 98680, "start": 1000.56, "end": 1006.56, "text": " And you can kind of, you can sort of see like, so we've got like the URL that's been changed,", "tokens": [51052, 400, 291, 393, 733, 295, 11, 291, 393, 1333, 295, 536, 411, 11, 370, 321, 600, 658, 411, 264, 12905, 300, 311, 668, 3105, 11, 51352], "temperature": 0.0, "avg_logprob": -0.18583413057549056, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0064228796400129795}, {"id": 229, "seek": 98680, "start": 1006.56, "end": 1011.4399999999999, "text": " you've got a request ID, you got an ID. It says somewhere, yeah, like what was the title of the", "tokens": [51352, 291, 600, 658, 257, 5308, 7348, 11, 291, 658, 364, 7348, 13, 467, 1619, 4079, 11, 1338, 11, 411, 437, 390, 264, 4876, 295, 264, 51596], "temperature": 0.0, "avg_logprob": -0.18583413057549056, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0064228796400129795}, {"id": 230, "seek": 101144, "start": 1011.44, "end": 1017.9200000000001, "text": " page? When was the time stamp? At the revision? Yeah, it's got lots of lots of kind of interesting", "tokens": [50364, 3028, 30, 1133, 390, 264, 565, 9921, 30, 1711, 264, 34218, 30, 865, 11, 309, 311, 658, 3195, 295, 3195, 295, 733, 295, 1880, 50688], "temperature": 0.0, "avg_logprob": -0.18974175369530394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.06187903881072998}, {"id": 231, "seek": 101144, "start": 1017.9200000000001, "end": 1022.8800000000001, "text": " stuff that we can, that we can pull out. Nope, not done yet. Okay, so now that would have been,", "tokens": [50688, 1507, 300, 321, 393, 11, 300, 321, 393, 2235, 484, 13, 12172, 11, 406, 1096, 1939, 13, 1033, 11, 370, 586, 300, 576, 362, 668, 11, 50936], "temperature": 0.0, "avg_logprob": -0.18974175369530394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.06187903881072998}, {"id": 232, "seek": 101144, "start": 1022.8800000000001, "end": 1029.76, "text": " that would have been amazing. So we have got, if I come, can you see? Yes. All right, great. How can I", "tokens": [50936, 300, 576, 362, 668, 2243, 13, 407, 321, 362, 658, 11, 498, 286, 808, 11, 393, 291, 536, 30, 1079, 13, 1057, 558, 11, 869, 13, 1012, 393, 286, 51280], "temperature": 0.0, "avg_logprob": -0.18974175369530394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.06187903881072998}, {"id": 233, "seek": 101144, "start": 1032.72, "end": 1037.8400000000001, "text": " see if I can get this to stay on my, hopefully, is that good enough? Is that good enough? Yeah.", "tokens": [51428, 536, 498, 286, 393, 483, 341, 281, 1754, 322, 452, 11, 4696, 11, 307, 300, 665, 1547, 30, 1119, 300, 665, 1547, 30, 865, 13, 51684], "temperature": 0.0, "avg_logprob": -0.18974175369530394, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.06187903881072998}, {"id": 234, "seek": 103784, "start": 1037.84, "end": 1044.8, "text": " All right, perfect. So we have a, let's see if I can type, pigmentize. Ah.", "tokens": [50364, 1057, 558, 11, 2176, 13, 407, 321, 362, 257, 11, 718, 311, 536, 498, 286, 393, 2010, 11, 8120, 518, 1125, 13, 2438, 13, 50712], "temperature": 0.0, "avg_logprob": -0.18885184671277197, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.002440725453197956}, {"id": 235, "seek": 103784, "start": 1048.1599999999999, "end": 1053.04, "text": " Yeah, there we go. So we've got a script called wiki.pipe. So this is what it looks like. So", "tokens": [50880, 865, 11, 456, 321, 352, 13, 407, 321, 600, 658, 257, 5755, 1219, 261, 9850, 13, 50042, 13, 407, 341, 307, 437, 309, 1542, 411, 13, 407, 51124], "temperature": 0.0, "avg_logprob": -0.18885184671277197, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.002440725453197956}, {"id": 236, "seek": 103784, "start": 1054.3999999999999, "end": 1058.08, "text": " I guess the red is not entirely readable. But we've got, this is, this is the,", "tokens": [51192, 286, 2041, 264, 2182, 307, 406, 7696, 49857, 13, 583, 321, 600, 658, 11, 341, 307, 11, 341, 307, 264, 11, 51376], "temperature": 0.0, "avg_logprob": -0.18885184671277197, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.002440725453197956}, {"id": 237, "seek": 103784, "start": 1058.9599999999998, "end": 1062.8, "text": " don't do that. This is the URL that we're going to be working with. So you can see these are,", "tokens": [51420, 500, 380, 360, 300, 13, 639, 307, 264, 12905, 300, 321, 434, 516, 281, 312, 1364, 365, 13, 407, 291, 393, 536, 613, 366, 11, 51612], "temperature": 0.0, "avg_logprob": -0.18885184671277197, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.002440725453197956}, {"id": 238, "seek": 106280, "start": 1063.76, "end": 1069.28, "text": " if I paste that into my browser. Oh, I don't know. What's that done? It's going to the wrong one.", "tokens": [50412, 498, 286, 9163, 300, 666, 452, 11185, 13, 876, 11, 286, 500, 380, 458, 13, 708, 311, 300, 1096, 30, 467, 311, 516, 281, 264, 2085, 472, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 239, "seek": 106280, "start": 1069.28, "end": 1075.6, "text": " Come here. Hang on. I'll escape that. So if we paste that in here, this is what it, this is what", "tokens": [50688, 2492, 510, 13, 14070, 322, 13, 286, 603, 7615, 300, 13, 407, 498, 321, 9163, 300, 294, 510, 11, 341, 307, 437, 309, 11, 341, 307, 437, 51004], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 240, "seek": 106280, "start": 1075.6, "end": 1079.36, "text": " it looks like. So you get some loads and loads of messages. Chrome will eventually get very,", "tokens": [51004, 309, 1542, 411, 13, 407, 291, 483, 512, 12668, 293, 12668, 295, 7897, 13, 15327, 486, 4728, 483, 588, 11, 51192], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 241, "seek": 106280, "start": 1079.36, "end": 1082.08, "text": " very angry with you if you leave this running forever. But you can kind of see the messages", "tokens": [51192, 588, 6884, 365, 291, 498, 291, 1856, 341, 2614, 5680, 13, 583, 291, 393, 733, 295, 536, 264, 7897, 51328], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 242, "seek": 106280, "start": 1082.08, "end": 1086.8799999999999, "text": " are coming through. And so we're going to be processing that. So you sort of see, we've got,", "tokens": [51328, 366, 1348, 807, 13, 400, 370, 321, 434, 516, 281, 312, 9007, 300, 13, 407, 291, 1333, 295, 536, 11, 321, 600, 658, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 243, "seek": 106280, "start": 1086.8799999999999, "end": 1091.68, "text": " we're just using the request library in Python to make a wrap around it. So that's this bit here.", "tokens": [51568, 321, 434, 445, 1228, 264, 5308, 6405, 294, 15329, 281, 652, 257, 7019, 926, 309, 13, 407, 300, 311, 341, 857, 510, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1499546912408644, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004522239323705435}, {"id": 244, "seek": 109168, "start": 1091.92, "end": 1095.92, "text": " Wrap around that, that particular endpoint. And then it's going to stream the events. And we've", "tokens": [50376, 41291, 926, 300, 11, 300, 1729, 35795, 13, 400, 550, 309, 311, 516, 281, 4309, 264, 3931, 13, 400, 321, 600, 50576], "temperature": 0.0, "avg_logprob": -0.14847145080566407, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.002055215882137418}, {"id": 245, "seek": 109168, "start": 1095.92, "end": 1101.04, "text": " got this SSE, the server side events client, Python client. Wrap that round and we get an", "tokens": [50576, 658, 341, 318, 5879, 11, 264, 7154, 1252, 3931, 6423, 11, 15329, 6423, 13, 41291, 300, 3098, 293, 321, 483, 364, 50832], "temperature": 0.0, "avg_logprob": -0.14847145080566407, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.002055215882137418}, {"id": 246, "seek": 109168, "start": 1101.04, "end": 1107.44, "text": " infinite stream of messages. So if we were to run this, so let's just go here. And we'll pipe it", "tokens": [50832, 13785, 4309, 295, 7897, 13, 407, 498, 321, 645, 281, 1190, 341, 11, 370, 718, 311, 445, 352, 510, 13, 400, 321, 603, 11240, 309, 51152], "temperature": 0.0, "avg_logprob": -0.14847145080566407, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.002055215882137418}, {"id": 247, "seek": 109168, "start": 1107.44, "end": 1111.92, "text": " into JQ just so it's a bit more readable. So you can kind of see like the messages are coming through.", "tokens": [51152, 666, 508, 48, 445, 370, 309, 311, 257, 857, 544, 49857, 13, 407, 291, 393, 733, 295, 536, 411, 264, 7897, 366, 1348, 807, 13, 51376], "temperature": 0.0, "avg_logprob": -0.14847145080566407, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.002055215882137418}, {"id": 248, "seek": 109168, "start": 1113.2, "end": 1117.28, "text": " If I stop it and scroll up, so you can see we've got, this is what it looks like. So you got", "tokens": [51440, 759, 286, 1590, 309, 293, 11369, 493, 11, 370, 291, 393, 536, 321, 600, 658, 11, 341, 307, 437, 309, 1542, 411, 13, 407, 291, 658, 51644], "temperature": 0.0, "avg_logprob": -0.14847145080566407, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.002055215882137418}, {"id": 249, "seek": 111728, "start": 1118.24, "end": 1124.0, "text": " the schema, meta, ID type. You can kind of see like, oh, this is, this is, this is what,", "tokens": [50412, 264, 34078, 11, 19616, 11, 7348, 2010, 13, 509, 393, 733, 295, 536, 411, 11, 1954, 11, 341, 307, 11, 341, 307, 11, 341, 307, 437, 11, 50700], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 250, "seek": 111728, "start": 1124.0, "end": 1127.44, "text": " oh, this is Russian, Russian, someone's changing Russian Wikipedia at the moment apparently.", "tokens": [50700, 1954, 11, 341, 307, 7220, 11, 7220, 11, 1580, 311, 4473, 7220, 28999, 412, 264, 1623, 7970, 13, 50872], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 251, "seek": 111728, "start": 1128.08, "end": 1132.8799999999999, "text": " There you go. Thanks. So we've got lots of different stuff. And so that's kind of the first", "tokens": [50904, 821, 291, 352, 13, 2561, 13, 407, 321, 600, 658, 3195, 295, 819, 1507, 13, 400, 370, 300, 311, 733, 295, 264, 700, 51144], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 252, "seek": 111728, "start": 1132.8799999999999, "end": 1137.04, "text": " bit, right? We wanted to get the stream into like a fashion that we've got it, right? So we've done", "tokens": [51144, 857, 11, 558, 30, 492, 1415, 281, 483, 264, 4309, 666, 411, 257, 6700, 300, 321, 600, 658, 309, 11, 558, 30, 407, 321, 600, 1096, 51352], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 253, "seek": 111728, "start": 1137.04, "end": 1141.12, "text": " that bit. Next bit is we want to get it into Pulsar. So let's have a look at our second script. So", "tokens": [51352, 300, 857, 13, 3087, 857, 307, 321, 528, 281, 483, 309, 666, 430, 9468, 289, 13, 407, 718, 311, 362, 257, 574, 412, 527, 1150, 5755, 13, 407, 51556], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 254, "seek": 111728, "start": 1141.68, "end": 1146.24, "text": " this one is to get it into Pulsar. Oh, hang on. It's a bit longer. So let's just pipe it into", "tokens": [51584, 341, 472, 307, 281, 483, 309, 666, 430, 9468, 289, 13, 876, 11, 3967, 322, 13, 467, 311, 257, 857, 2854, 13, 407, 718, 311, 445, 11240, 309, 666, 51812], "temperature": 0.0, "avg_logprob": -0.16417295282537286, "compression_ratio": 1.8557377049180328, "no_speech_prob": 0.005480948369950056}, {"id": 255, "seek": 114624, "start": 1146.24, "end": 1151.6, "text": " less first. So we're going to be using the Pulsar client. And the first bit is the same, right?", "tokens": [50364, 1570, 700, 13, 407, 321, 434, 516, 281, 312, 1228, 264, 430, 9468, 289, 6423, 13, 400, 264, 700, 857, 307, 264, 912, 11, 558, 30, 50632], "temperature": 0.0, "avg_logprob": -0.13433496275944495, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011859475634992123}, {"id": 256, "seek": 114624, "start": 1151.6, "end": 1158.48, "text": " So we still build our streaming client here, like for the, for the wiki. But then we also build a", "tokens": [50632, 407, 321, 920, 1322, 527, 11791, 6423, 510, 11, 411, 337, 264, 11, 337, 264, 261, 9850, 13, 583, 550, 321, 611, 1322, 257, 50976], "temperature": 0.0, "avg_logprob": -0.13433496275944495, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011859475634992123}, {"id": 257, "seek": 114624, "start": 1158.48, "end": 1163.68, "text": " Pulsar client here. So we're creating a Pulsar client. We point it to localhost 665. This is", "tokens": [50976, 430, 9468, 289, 6423, 510, 13, 407, 321, 434, 4084, 257, 430, 9468, 289, 6423, 13, 492, 935, 309, 281, 2654, 6037, 1386, 16824, 13, 639, 307, 51236], "temperature": 0.0, "avg_logprob": -0.13433496275944495, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011859475634992123}, {"id": 258, "seek": 114624, "start": 1163.68, "end": 1167.1200000000001, "text": " the port. We're actually running all this in Docker, but if it exposes the ports out to the,", "tokens": [51236, 264, 2436, 13, 492, 434, 767, 2614, 439, 341, 294, 33772, 11, 457, 498, 309, 1278, 4201, 264, 18160, 484, 281, 264, 11, 51408], "temperature": 0.0, "avg_logprob": -0.13433496275944495, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011859475634992123}, {"id": 259, "seek": 114624, "start": 1167.1200000000001, "end": 1173.36, "text": " to the host OS, we create a producer. So our topic is going to be called wiki events.", "tokens": [51408, 281, 264, 3975, 12731, 11, 321, 1884, 257, 12314, 13, 407, 527, 4829, 307, 516, 281, 312, 1219, 261, 9850, 3931, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13433496275944495, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011859475634992123}, {"id": 260, "seek": 117336, "start": 1173.36, "end": 1177.36, "text": " And then if you scroll down here, this is still the same. So we're still looping through that", "tokens": [50364, 400, 550, 498, 291, 11369, 760, 510, 11, 341, 307, 920, 264, 912, 13, 407, 321, 434, 920, 6367, 278, 807, 300, 50564], "temperature": 0.0, "avg_logprob": -0.12760183379406065, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0034698026720434427}, {"id": 261, "seek": 117336, "start": 1177.36, "end": 1183.12, "text": " stream of stuff. But this time we, we then call the Pulsar producer and we send it a message and", "tokens": [50564, 4309, 295, 1507, 13, 583, 341, 565, 321, 11, 321, 550, 818, 264, 430, 9468, 289, 12314, 293, 321, 2845, 309, 257, 3636, 293, 50852], "temperature": 0.0, "avg_logprob": -0.12760183379406065, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0034698026720434427}, {"id": 262, "seek": 117336, "start": 1183.12, "end": 1189.28, "text": " we're sending it in async, which means we're not going to wait for the response, right? And then", "tokens": [50852, 321, 434, 7750, 309, 294, 382, 34015, 11, 597, 1355, 321, 434, 406, 516, 281, 1699, 337, 264, 4134, 11, 558, 30, 400, 550, 51160], "temperature": 0.0, "avg_logprob": -0.12760183379406065, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0034698026720434427}, {"id": 263, "seek": 117336, "start": 1189.28, "end": 1195.04, "text": " finally we flush, we flushed it every 100 messages. Anything else to add on the Pulsar", "tokens": [51160, 2721, 321, 19568, 11, 321, 19568, 292, 309, 633, 2319, 7897, 13, 11998, 1646, 281, 909, 322, 264, 430, 9468, 289, 51448], "temperature": 0.0, "avg_logprob": -0.12760183379406065, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0034698026720434427}, {"id": 264, "seek": 117336, "start": 1195.04, "end": 1201.12, "text": " production? That's good. All right, cool. So now let's call that one. So if we call Python", "tokens": [51448, 4265, 30, 663, 311, 665, 13, 1057, 558, 11, 1627, 13, 407, 586, 718, 311, 818, 300, 472, 13, 407, 498, 321, 818, 15329, 51752], "temperature": 0.0, "avg_logprob": -0.12760183379406065, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0034698026720434427}, {"id": 265, "seek": 120112, "start": 1202.08, "end": 1208.7199999999998, "text": " wiki to Pulsar. So then I run and you can kind of see like it will sort of run away and everything", "tokens": [50412, 261, 9850, 281, 430, 9468, 289, 13, 407, 550, 286, 1190, 293, 291, 393, 733, 295, 536, 411, 309, 486, 1333, 295, 1190, 1314, 293, 1203, 50744], "temperature": 0.0, "avg_logprob": -0.15135339830742509, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004183879122138023}, {"id": 266, "seek": 120112, "start": 1208.7199999999998, "end": 1212.9599999999998, "text": " is, everything is happy. I can never remember quite exactly what the commands are, but we can", "tokens": [50744, 307, 11, 1203, 307, 2055, 13, 286, 393, 1128, 1604, 1596, 2293, 437, 264, 16901, 366, 11, 457, 321, 393, 50956], "temperature": 0.0, "avg_logprob": -0.15135339830742509, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004183879122138023}, {"id": 267, "seek": 120112, "start": 1212.9599999999998, "end": 1218.32, "text": " then use the Pulsar client to check that these messages are making their way in. So we can call", "tokens": [50956, 550, 764, 264, 430, 9468, 289, 6423, 281, 1520, 300, 613, 7897, 366, 1455, 641, 636, 294, 13, 407, 321, 393, 818, 51224], "temperature": 0.0, "avg_logprob": -0.15135339830742509, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004183879122138023}, {"id": 268, "seek": 120112, "start": 1218.32, "end": 1221.28, "text": " this Pulsar client and we say we're going to actually, do you want to explain it? You're", "tokens": [51224, 341, 430, 9468, 289, 6423, 293, 321, 584, 321, 434, 516, 281, 767, 11, 360, 291, 528, 281, 2903, 309, 30, 509, 434, 51372], "temperature": 0.0, "avg_logprob": -0.15135339830742509, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004183879122138023}, {"id": 269, "seek": 120112, "start": 1221.28, "end": 1227.4399999999998, "text": " probably better than me. Yeah, consuming and the events and then also the subscription. You have", "tokens": [51372, 1391, 1101, 813, 385, 13, 865, 11, 19867, 293, 264, 3931, 293, 550, 611, 264, 17231, 13, 509, 362, 51680], "temperature": 0.0, "avg_logprob": -0.15135339830742509, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.004183879122138023}, {"id": 270, "seek": 122744, "start": 1227.52, "end": 1232.48, "text": " a name to it. Okay. So it's going to pick up wherever it was the last time I did it. So let's", "tokens": [50368, 257, 1315, 281, 309, 13, 1033, 13, 407, 309, 311, 516, 281, 1888, 493, 8660, 309, 390, 264, 1036, 565, 286, 630, 309, 13, 407, 718, 311, 50616], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 271, "seek": 122744, "start": 1232.48, "end": 1236.48, "text": " see. So there we go. I mean, this is going way faster. I guess it's catching up the ones that", "tokens": [50616, 536, 13, 407, 456, 321, 352, 13, 286, 914, 11, 341, 307, 516, 636, 4663, 13, 286, 2041, 309, 311, 16124, 493, 264, 2306, 300, 50816], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 272, "seek": 122744, "start": 1236.48, "end": 1240.3200000000002, "text": " we've done, we've done before. So, and then eventually we'll come to the same sort of speeds.", "tokens": [50816, 321, 600, 1096, 11, 321, 600, 1096, 949, 13, 407, 11, 293, 550, 4728, 321, 603, 808, 281, 264, 912, 1333, 295, 16411, 13, 51008], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 273, "seek": 122744, "start": 1240.3200000000002, "end": 1244.8, "text": " So we can see these are, these are the messages coming through Pulsar. We've put it in JSON", "tokens": [51008, 407, 321, 393, 536, 613, 366, 11, 613, 366, 264, 7897, 1348, 807, 430, 9468, 289, 13, 492, 600, 829, 309, 294, 31828, 51232], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 274, "seek": 122744, "start": 1245.52, "end": 1250.0, "text": " format, but it can handle multiple forms, right? If you wanted to. So yeah, sort of all the, all", "tokens": [51268, 7877, 11, 457, 309, 393, 4813, 3866, 6422, 11, 558, 30, 759, 291, 1415, 281, 13, 407, 1338, 11, 1333, 295, 439, 264, 11, 439, 51492], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 275, "seek": 122744, "start": 1250.0, "end": 1253.3600000000001, "text": " the typical ones you would expect. So I guess most people would be doing Avro with some sort of", "tokens": [51492, 264, 7476, 2306, 291, 576, 2066, 13, 407, 286, 2041, 881, 561, 576, 312, 884, 11667, 340, 365, 512, 1333, 295, 51660], "temperature": 0.0, "avg_logprob": -0.13391572724348344, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.0020568983163684607}, {"id": 276, "seek": 125336, "start": 1254.0, "end": 1257.12, "text": " schema attached to it. But I mean, for, yeah, for simplicity in the demo,", "tokens": [50396, 34078, 8570, 281, 309, 13, 583, 286, 914, 11, 337, 11, 1338, 11, 337, 25632, 294, 264, 10723, 11, 50552], "temperature": 0.0, "avg_logprob": -0.15451973071996716, "compression_ratio": 1.6872727272727273, "no_speech_prob": 0.0027783007826656103}, {"id": 277, "seek": 125336, "start": 1257.12, "end": 1261.6799999999998, "text": " JSON is quite a, quite a nice tool. Okay. So we've got it that far. Next thing is we want to get it", "tokens": [50552, 31828, 307, 1596, 257, 11, 1596, 257, 1481, 2290, 13, 1033, 13, 407, 321, 600, 658, 309, 300, 1400, 13, 3087, 551, 307, 321, 528, 281, 483, 309, 50780], "temperature": 0.0, "avg_logprob": -0.15451973071996716, "compression_ratio": 1.6872727272727273, "no_speech_prob": 0.0027783007826656103}, {"id": 278, "seek": 125336, "start": 1261.6799999999998, "end": 1269.36, "text": " into our, into a way that we can query it from, let's see, where's my, right? So, so as I say,", "tokens": [50780, 666, 527, 11, 666, 257, 636, 300, 321, 393, 14581, 309, 490, 11, 718, 311, 536, 11, 689, 311, 452, 11, 558, 30, 407, 11, 370, 382, 286, 584, 11, 51164], "temperature": 0.0, "avg_logprob": -0.15451973071996716, "compression_ratio": 1.6872727272727273, "no_speech_prob": 0.0027783007826656103}, {"id": 279, "seek": 125336, "start": 1269.36, "end": 1275.4399999999998, "text": " Pino, it's like model is table. So you create a table and you can query it. And the first thing is", "tokens": [51164, 430, 2982, 11, 309, 311, 411, 2316, 307, 3199, 13, 407, 291, 1884, 257, 3199, 293, 291, 393, 14581, 309, 13, 400, 264, 700, 551, 307, 51468], "temperature": 0.0, "avg_logprob": -0.15451973071996716, "compression_ratio": 1.6872727272727273, "no_speech_prob": 0.0027783007826656103}, {"id": 280, "seek": 125336, "start": 1275.4399999999998, "end": 1281.52, "text": " we need to have a schema for that table. So this is what the Wikipedia one looked like. So we're", "tokens": [51468, 321, 643, 281, 362, 257, 34078, 337, 300, 3199, 13, 407, 341, 307, 437, 264, 28999, 472, 2956, 411, 13, 407, 321, 434, 51772], "temperature": 0.0, "avg_logprob": -0.15451973071996716, "compression_ratio": 1.6872727272727273, "no_speech_prob": 0.0027783007826656103}, {"id": 281, "seek": 128152, "start": 1281.52, "end": 1284.8799999999999, "text": " pulling out some of the fields. We've got ID, we've got Wiki, we've got user, we've got title,", "tokens": [50364, 8407, 484, 512, 295, 264, 7909, 13, 492, 600, 658, 7348, 11, 321, 600, 658, 35892, 11, 321, 600, 658, 4195, 11, 321, 600, 658, 4876, 11, 50532], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 282, "seek": 128152, "start": 1284.8799999999999, "end": 1290.72, "text": " comment, stream, type, bots, and then we've specified a timestamp down to bottom. You notice maybe", "tokens": [50532, 2871, 11, 4309, 11, 2010, 11, 35410, 11, 293, 550, 321, 600, 22206, 257, 49108, 1215, 760, 281, 2767, 13, 509, 3449, 1310, 50824], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 283, "seek": 128152, "start": 1291.6, "end": 1295.44, "text": " that this is, this is kind of using the, the language of data warehousing. So these are dimension", "tokens": [50868, 300, 341, 307, 11, 341, 307, 733, 295, 1228, 264, 11, 264, 2856, 295, 1412, 17464, 71, 24220, 13, 407, 613, 366, 10139, 51060], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 284, "seek": 128152, "start": 1295.44, "end": 1298.8799999999999, "text": " fields. We don't have any metrics fields in because there aren't really anything that we can count", "tokens": [51060, 7909, 13, 492, 500, 380, 362, 604, 16367, 7909, 294, 570, 456, 3212, 380, 534, 1340, 300, 321, 393, 1207, 51232], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 285, "seek": 128152, "start": 1298.8799999999999, "end": 1302.96, "text": " in this dataset, but you could have, if you had something that you were counting, then you could", "tokens": [51232, 294, 341, 28872, 11, 457, 291, 727, 362, 11, 498, 291, 632, 746, 300, 291, 645, 13251, 11, 550, 291, 727, 51436], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 286, "seek": 128152, "start": 1302.96, "end": 1308.4, "text": " put that as a metric field. And then finally we've got a date time field as well. Now in general,", "tokens": [51436, 829, 300, 382, 257, 20678, 2519, 13, 400, 550, 2721, 321, 600, 658, 257, 4002, 565, 2519, 382, 731, 13, 823, 294, 2674, 11, 51708], "temperature": 0.0, "avg_logprob": -0.10238476176010936, "compression_ratio": 1.828125, "no_speech_prob": 0.01110941544175148}, {"id": 287, "seek": 130840, "start": 1309.0400000000002, "end": 1314.0, "text": " by default, whatever fields you put in here, it will map exactly. If there is a value in your", "tokens": [50396, 538, 7576, 11, 2035, 7909, 291, 829, 294, 510, 11, 309, 486, 4471, 2293, 13, 759, 456, 307, 257, 2158, 294, 428, 50644], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 288, "seek": 130840, "start": 1314.0, "end": 1318.5600000000002, "text": " source with this name, it will map it directly. So in ideal world, everything is flat and we just", "tokens": [50644, 4009, 365, 341, 1315, 11, 309, 486, 4471, 309, 3838, 13, 407, 294, 7157, 1002, 11, 1203, 307, 4962, 293, 321, 445, 50872], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 289, "seek": 130840, "start": 1318.5600000000002, "end": 1324.96, "text": " go map, map, map, map, map. In this case actually, it's not quite as simple as that, but so what", "tokens": [50872, 352, 4471, 11, 4471, 11, 4471, 11, 4471, 11, 4471, 13, 682, 341, 1389, 767, 11, 309, 311, 406, 1596, 382, 2199, 382, 300, 11, 457, 370, 437, 51192], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 290, "seek": 130840, "start": 1324.96, "end": 1328.24, "text": " we need to do, so I'll just show you the table conflict that goes with it. But the first thing", "tokens": [51192, 321, 643, 281, 360, 11, 370, 286, 603, 445, 855, 291, 264, 3199, 6596, 300, 1709, 365, 309, 13, 583, 264, 700, 551, 51356], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 291, "seek": 130840, "start": 1328.24, "end": 1333.2, "text": " to notice is we can do these transformation functions to get the data into our column. So", "tokens": [51356, 281, 3449, 307, 321, 393, 360, 613, 9887, 6828, 281, 483, 264, 1412, 666, 527, 7738, 13, 407, 51604], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 292, "seek": 130840, "start": 1333.2, "end": 1337.8400000000001, "text": " ID is under meta.id. So we're using this JSON path function to pull stuff out. You could,", "tokens": [51604, 7348, 307, 833, 19616, 13, 327, 13, 407, 321, 434, 1228, 341, 31828, 3100, 2445, 281, 2235, 1507, 484, 13, 509, 727, 11, 51836], "temperature": 0.0, "avg_logprob": -0.10393360773722331, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.007317802868783474}, {"id": 293, "seek": 133784, "start": 1337.84, "end": 1341.9199999999998, "text": " if you wanted to, if you, if we had cleaned the data up before, we could have used Palsar's", "tokens": [50364, 498, 291, 1415, 281, 11, 498, 291, 11, 498, 321, 632, 16146, 264, 1412, 493, 949, 11, 321, 727, 362, 1143, 430, 1124, 289, 311, 50568], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 294, "seek": 133784, "start": 1341.9199999999998, "end": 1345.6799999999998, "text": " serverless functions to do the same thing. And then we would have it in a cleaner state and", "tokens": [50568, 7154, 1832, 6828, 281, 360, 264, 912, 551, 13, 400, 550, 321, 576, 362, 309, 294, 257, 16532, 1785, 293, 50756], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 295, "seek": 133784, "start": 1345.6799999999998, "end": 1350.72, "text": " just go straight into Pina. So you can kind of choose which, which of those options. This is not", "tokens": [50756, 445, 352, 2997, 666, 430, 1426, 13, 407, 291, 393, 733, 295, 2826, 597, 11, 597, 295, 729, 3956, 13, 639, 307, 406, 51008], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 296, "seek": 133784, "start": 1350.72, "end": 1356.0, "text": " a replacement for a stream processor, right? This is just a very, very tiny adjustments to the data,", "tokens": [51008, 257, 14419, 337, 257, 4309, 15321, 11, 558, 30, 639, 307, 445, 257, 588, 11, 588, 5870, 18624, 281, 264, 1412, 11, 51272], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 297, "seek": 133784, "start": 1356.0, "end": 1360.32, "text": " right? If it was like slightly, slightly wrong. And then the other thing was the timestamps in", "tokens": [51272, 558, 30, 759, 309, 390, 411, 4748, 11, 4748, 2085, 13, 400, 550, 264, 661, 551, 390, 264, 49108, 23150, 294, 51488], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 298, "seek": 133784, "start": 1360.9599999999998, "end": 1366.72, "text": " the wiki give you are in milliseconds, epoch seconds, and I need epoch milliseconds for,", "tokens": [51520, 264, 261, 9850, 976, 291, 366, 294, 34184, 11, 30992, 339, 3949, 11, 293, 286, 643, 30992, 339, 34184, 337, 11, 51808], "temperature": 0.0, "avg_logprob": -0.15115957390772153, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0026663828175514936}, {"id": 299, "seek": 136672, "start": 1366.8, "end": 1372.16, "text": " for Palsar. So I multiply by a thousand. Now let's, oh, sorry. Yeah, I forgot the top bit. So", "tokens": [50368, 337, 430, 1124, 289, 13, 407, 286, 12972, 538, 257, 4714, 13, 823, 718, 311, 11, 1954, 11, 2597, 13, 865, 11, 286, 5298, 264, 1192, 857, 13, 407, 50636], "temperature": 0.0, "avg_logprob": -0.1934419111772017, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0022752233780920506}, {"id": 300, "seek": 136672, "start": 1372.16, "end": 1376.08, "text": " top bit, name of the table needs to match the name of the schema. So it's Wikipedia.", "tokens": [50636, 1192, 857, 11, 1315, 295, 264, 3199, 2203, 281, 2995, 264, 1315, 295, 264, 34078, 13, 407, 309, 311, 28999, 13, 50832], "temperature": 0.0, "avg_logprob": -0.1934419111772017, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0022752233780920506}, {"id": 301, "seek": 136672, "start": 1376.8, "end": 1380.56, "text": " We need to specify what's the timestamp. And then this is the config that's telling it, hey, I need", "tokens": [50868, 492, 643, 281, 16500, 437, 311, 264, 49108, 1215, 13, 400, 550, 341, 307, 264, 6662, 300, 311, 3585, 309, 11, 4177, 11, 286, 643, 51056], "temperature": 0.0, "avg_logprob": -0.1934419111772017, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0022752233780920506}, {"id": 302, "seek": 136672, "start": 1380.56, "end": 1385.3600000000001, "text": " to, actually, I'm looking, I'm looking at the wrong one even. I should be, I shouldn't be saying", "tokens": [51056, 281, 11, 767, 11, 286, 478, 1237, 11, 286, 478, 1237, 412, 264, 2085, 472, 754, 13, 286, 820, 312, 11, 286, 4659, 380, 312, 1566, 51296], "temperature": 0.0, "avg_logprob": -0.1934419111772017, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0022752233780920506}, {"id": 303, "seek": 136672, "start": 1386.72, "end": 1394.56, "text": " table Palsar. There we go. So, sorry. So you see here, we say, hey, I'm going to be pulling the", "tokens": [51364, 3199, 430, 1124, 289, 13, 821, 321, 352, 13, 407, 11, 2597, 13, 407, 291, 536, 510, 11, 321, 584, 11, 4177, 11, 286, 478, 516, 281, 312, 8407, 264, 51756], "temperature": 0.0, "avg_logprob": -0.1934419111772017, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0022752233780920506}, {"id": 304, "seek": 139456, "start": 1394.56, "end": 1401.04, "text": " data from the Palsar stream. This is my Palsar connection string. I then need to say, hey,", "tokens": [50364, 1412, 490, 264, 430, 1124, 289, 4309, 13, 639, 307, 452, 430, 1124, 289, 4984, 6798, 13, 286, 550, 643, 281, 584, 11, 4177, 11, 50688], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 305, "seek": 139456, "start": 1401.04, "end": 1404.56, "text": " I'm going to decode, I need, I'm going to tell it which factory to use. I need to tell it the", "tokens": [50688, 286, 478, 516, 281, 979, 1429, 11, 286, 643, 11, 286, 478, 516, 281, 980, 309, 597, 9265, 281, 764, 13, 286, 643, 281, 980, 309, 264, 50864], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 306, "seek": 139456, "start": 1404.56, "end": 1409.28, "text": " Palsar factory. And then this is, yeah, I mean, this is not really necessary for, for now. And", "tokens": [50864, 430, 1124, 289, 9265, 13, 400, 550, 341, 307, 11, 1338, 11, 286, 914, 11, 341, 307, 406, 534, 4818, 337, 11, 337, 586, 13, 400, 51100], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 307, "seek": 139456, "start": 1409.28, "end": 1413.36, "text": " what we're going to do now is we're going to add our table. So what this is going to do is going", "tokens": [51100, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 909, 527, 3199, 13, 407, 437, 341, 307, 516, 281, 360, 307, 516, 51304], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 308, "seek": 139456, "start": 1413.36, "end": 1416.1599999999999, "text": " to create the table and then immediately it's going to start consuming. If you didn't have any", "tokens": [51304, 281, 1884, 264, 3199, 293, 550, 4258, 309, 311, 516, 281, 722, 19867, 13, 759, 291, 994, 380, 362, 604, 51444], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 309, "seek": 139456, "start": 1416.1599999999999, "end": 1420.8799999999999, "text": " messages in there, it obviously wouldn't consume anything. But since we, we do, we should be able", "tokens": [51444, 7897, 294, 456, 11, 309, 2745, 2759, 380, 14732, 1340, 13, 583, 1670, 321, 11, 321, 360, 11, 321, 820, 312, 1075, 51680], "temperature": 0.0, "avg_logprob": -0.07000837386029321, "compression_ratio": 1.9486301369863013, "no_speech_prob": 0.008132063783705235}, {"id": 310, "seek": 142088, "start": 1420.88, "end": 1426.16, "text": " to see our table here, Wikipedia. And you can see we've got, the messages are kind of coming in.", "tokens": [50364, 281, 536, 527, 3199, 510, 11, 28999, 13, 400, 291, 393, 536, 321, 600, 658, 11, 264, 7897, 366, 733, 295, 1348, 294, 13, 50628], "temperature": 0.0, "avg_logprob": -0.19610975570037587, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.009307224303483963}, {"id": 311, "seek": 142088, "start": 1426.16, "end": 1430.96, "text": " So you can see we've got, at the moment, 9,000 messages. We could write like a, so it's a sequel,", "tokens": [50628, 407, 291, 393, 536, 321, 600, 658, 11, 412, 264, 1623, 11, 1722, 11, 1360, 7897, 13, 492, 727, 2464, 411, 257, 11, 370, 309, 311, 257, 20622, 11, 50868], "temperature": 0.0, "avg_logprob": -0.19610975570037587, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.009307224303483963}, {"id": 312, "seek": 142088, "start": 1431.6000000000001, "end": 1436.48, "text": " sequel on top of it. So we can, thanks. We could say, okay, let's have a look which user is doing", "tokens": [50900, 20622, 322, 1192, 295, 309, 13, 407, 321, 393, 11, 3231, 13, 492, 727, 584, 11, 1392, 11, 718, 311, 362, 257, 574, 597, 4195, 307, 884, 51144], "temperature": 0.0, "avg_logprob": -0.19610975570037587, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.009307224303483963}, {"id": 313, "seek": 142088, "start": 1436.48, "end": 1445.5200000000002, "text": " the most stuff. Let's hang on. Oh, forgot the group by group. By user. Oh, can't type. Order by,", "tokens": [51144, 264, 881, 1507, 13, 961, 311, 3967, 322, 13, 876, 11, 5298, 264, 1594, 538, 1594, 13, 3146, 4195, 13, 876, 11, 393, 380, 2010, 13, 16321, 538, 11, 51596], "temperature": 0.0, "avg_logprob": -0.19610975570037587, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.009307224303483963}, {"id": 314, "seek": 144552, "start": 1445.52, "end": 1449.2, "text": " can't start sending. So we could do, yeah, we could do something.", "tokens": [50392, 393, 380, 722, 7750, 13, 407, 321, 727, 360, 11, 1338, 11, 321, 727, 360, 746, 13, 50548], "temperature": 0.0, "avg_logprob": -0.2475417137145996, "compression_ratio": 1.1016949152542372, "no_speech_prob": 0.09083423763513565}, {"id": 315, "seek": 162552, "start": 1625.52, "end": 1633.12, "text": " What people, where people are changing stuff. And then finally, yeah, let me just, let me just", "tokens": [50364, 708, 561, 11, 689, 561, 366, 4473, 1507, 13, 400, 550, 2721, 11, 1338, 11, 718, 385, 445, 11, 718, 385, 445, 50744], "temperature": 0.0, "avg_logprob": -0.35239421753656297, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.10007072985172272}, {"id": 316, "seek": 162552, "start": 1633.12, "end": 1641.04, "text": " quickly show you what, what, what exactly what we're doing. Okay. They're back again. Should I", "tokens": [50744, 2661, 855, 291, 437, 11, 437, 11, 437, 2293, 437, 321, 434, 884, 13, 1033, 13, 814, 434, 646, 797, 13, 6454, 286, 51140], "temperature": 0.0, "avg_logprob": -0.35239421753656297, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.10007072985172272}, {"id": 317, "seek": 162552, "start": 1641.04, "end": 1649.2, "text": " put it back? Yeah. Yeah. Okay. All right, easy one, easy one. So, oh, sorry. So what we're doing,", "tokens": [51140, 829, 309, 646, 30, 865, 13, 865, 13, 1033, 13, 1057, 558, 11, 1858, 472, 11, 1858, 472, 13, 407, 11, 1954, 11, 2597, 13, 407, 437, 321, 434, 884, 11, 51548], "temperature": 0.0, "avg_logprob": -0.35239421753656297, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.10007072985172272}, {"id": 318, "seek": 164920, "start": 1649.28, "end": 1654.56, "text": " so it's all in Python. So we're using the Pena Python driver. We're connecting to Pena here.", "tokens": [50368, 370, 309, 311, 439, 294, 15329, 13, 407, 321, 434, 1228, 264, 430, 4118, 15329, 6787, 13, 492, 434, 11015, 281, 430, 4118, 510, 13, 50632], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 319, "seek": 164920, "start": 1655.44, "end": 1658.24, "text": " And then we're basically just running some queries. So the one that we're showing you,", "tokens": [50676, 400, 550, 321, 434, 1936, 445, 2614, 512, 24109, 13, 407, 264, 472, 300, 321, 434, 4099, 291, 11, 50816], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 320, "seek": 164920, "start": 1658.24, "end": 1662.32, "text": " like the table on the top, this red is not entirely readable. Is that aggregation plus", "tokens": [50816, 411, 264, 3199, 322, 264, 1192, 11, 341, 2182, 307, 406, 7696, 49857, 13, 1119, 300, 16743, 399, 1804, 51020], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 321, "seek": 164920, "start": 1662.32, "end": 1667.04, "text": " filtering. So kind of capturing what happened in the last minute versus, versus what happened", "tokens": [51020, 30822, 13, 407, 733, 295, 23384, 437, 2011, 294, 264, 1036, 3456, 5717, 11, 5717, 437, 2011, 51256], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 322, "seek": 164920, "start": 1667.04, "end": 1671.2, "text": " a minute before. And then we're going to kind of just run the query and stick it into pandas.", "tokens": [51256, 257, 3456, 949, 13, 400, 550, 321, 434, 516, 281, 733, 295, 445, 1190, 264, 14581, 293, 2897, 309, 666, 4565, 296, 13, 51464], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 323, "seek": 164920, "start": 1671.2, "end": 1674.8, "text": " We do the same. We build some metrics. There's numbers on the top there called,", "tokens": [51464, 492, 360, 264, 912, 13, 492, 1322, 512, 16367, 13, 821, 311, 3547, 322, 264, 1192, 456, 1219, 11, 51644], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 324, "seek": 164920, "start": 1674.8, "end": 1678.56, "text": " string that calls the metrics. So we can pass in like a value and then we can build", "tokens": [51644, 6798, 300, 5498, 264, 16367, 13, 407, 321, 393, 1320, 294, 411, 257, 2158, 293, 550, 321, 393, 1322, 51832], "temperature": 0.0, "avg_logprob": -0.31489742789298864, "compression_ratio": 1.889908256880734, "no_speech_prob": 0.3927278518676758}, {"id": 325, "seek": 167856, "start": 1678.56, "end": 1681.84, "text": " the dieters. And the dieters, this minute minus the previous minutes, you can kind of see the", "tokens": [50364, 264, 1026, 6202, 13, 400, 264, 1026, 6202, 11, 341, 3456, 3175, 264, 3894, 2077, 11, 291, 393, 733, 295, 536, 264, 50528], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 326, "seek": 167856, "start": 1681.84, "end": 1687.28, "text": " change. And instead of, yeah, I mean, I guess, yeah, that's probably enough on the code. But", "tokens": [50528, 1319, 13, 400, 2602, 295, 11, 1338, 11, 286, 914, 11, 286, 2041, 11, 1338, 11, 300, 311, 1391, 1547, 322, 264, 3089, 13, 583, 50800], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 327, "seek": 167856, "start": 1687.28, "end": 1691.52, "text": " if you want to have a look at that, it's all in the, the repository. And in theory, like you've", "tokens": [50800, 498, 291, 528, 281, 362, 257, 574, 412, 300, 11, 309, 311, 439, 294, 264, 11, 264, 25841, 13, 400, 294, 5261, 11, 411, 291, 600, 51012], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 328, "seek": 167856, "start": 1691.52, "end": 1695.36, "text": " seen me literally just running the commands in there. It should just, should just work. So", "tokens": [51012, 1612, 385, 3736, 445, 2614, 264, 16901, 294, 456, 13, 467, 820, 445, 11, 820, 445, 589, 13, 407, 51204], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 329, "seek": 167856, "start": 1695.36, "end": 1699.76, "text": " hopefully, hopefully you can, you can follow along. But I just came back to here to conclude.", "tokens": [51204, 4696, 11, 4696, 291, 393, 11, 291, 393, 1524, 2051, 13, 583, 286, 445, 1361, 646, 281, 510, 281, 16886, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 330, "seek": 167856, "start": 1701.44, "end": 1706.32, "text": " So yeah, lots of, lots of people doing stuff with, with parents or some of the, the users", "tokens": [51508, 407, 1338, 11, 3195, 295, 11, 3195, 295, 561, 884, 1507, 365, 11, 365, 3152, 420, 512, 295, 264, 11, 264, 5022, 51752], "temperature": 0.0, "avg_logprob": -0.2661678903054871, "compression_ratio": 1.8262295081967213, "no_speech_prob": 0.00833184365183115}, {"id": 331, "seek": 170632, "start": 1707.2, "end": 1713.12, "text": " parcel as well. So lots of, lots of different, lots of people, people using it. Just a conclusion", "tokens": [50408, 34082, 382, 731, 13, 407, 3195, 295, 11, 3195, 295, 819, 11, 3195, 295, 561, 11, 561, 1228, 309, 13, 1449, 257, 10063, 50704], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 332, "seek": 170632, "start": 1713.12, "end": 1718.0, "text": " and then, and then one more slide. So I hope that you can kind of see what you're combining", "tokens": [50704, 293, 550, 11, 293, 550, 472, 544, 4137, 13, 407, 286, 1454, 300, 291, 393, 733, 295, 536, 437, 291, 434, 21928, 50948], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 333, "seek": 170632, "start": 1718.0, "end": 1723.36, "text": " these different tools together. You can build some quite, some quite cool applications. In this", "tokens": [50948, 613, 819, 3873, 1214, 13, 509, 393, 1322, 512, 1596, 11, 512, 1596, 1627, 5821, 13, 682, 341, 51216], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 334, "seek": 170632, "start": 1723.36, "end": 1726.48, "text": " case, I'm just going to show you what the action would be because all the users have built. But", "tokens": [51216, 1389, 11, 286, 478, 445, 516, 281, 855, 291, 437, 264, 3069, 576, 312, 570, 439, 264, 5022, 362, 3094, 13, 583, 51372], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 335, "seek": 170632, "start": 1726.48, "end": 1731.4399999999998, "text": " you could imagine, like if it was like, if you were looking at real wicked users, you maybe want", "tokens": [51372, 291, 727, 3811, 11, 411, 498, 309, 390, 411, 11, 498, 291, 645, 1237, 412, 957, 22663, 5022, 11, 291, 1310, 528, 51620], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 336, "seek": 170632, "start": 1731.4399999999998, "end": 1734.8799999999999, "text": " to try and encourage the ones you see like are coming in new. We've built something that's fresh", "tokens": [51620, 281, 853, 293, 5373, 264, 2306, 291, 536, 411, 366, 1348, 294, 777, 13, 492, 600, 3094, 746, 300, 311, 4451, 51792], "temperature": 0.0, "avg_logprob": -0.30971336364746094, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.012985171750187874}, {"id": 337, "seek": 173488, "start": 1734.96, "end": 1740.5600000000002, "text": " there, the fast gradient scale. And we've done it with a classroom. For me, I'm writing a book with", "tokens": [50368, 456, 11, 264, 2370, 16235, 4373, 13, 400, 321, 600, 1096, 309, 365, 257, 7419, 13, 1171, 385, 11, 286, 478, 3579, 257, 1446, 365, 50648], "temperature": 0.0, "avg_logprob": -0.323944091796875, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.01080132182687521}, {"id": 338, "seek": 173488, "start": 1740.5600000000002, "end": 1744.0, "text": " an ex colleague of mine, I'm showing you how to build these type of things. If you're interested", "tokens": [50648, 364, 454, 13532, 295, 3892, 11, 286, 478, 4099, 291, 577, 281, 1322, 613, 2010, 295, 721, 13, 759, 291, 434, 3102, 50820], "temperature": 0.0, "avg_logprob": -0.323944091796875, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.01080132182687521}, {"id": 339, "seek": 173488, "start": 1744.0, "end": 1750.3200000000002, "text": " in that, then there's my contact on your left. And I let Mary conclude. Just, okay. Here, it's just", "tokens": [50820, 294, 300, 11, 550, 456, 311, 452, 3385, 322, 428, 1411, 13, 400, 286, 718, 6059, 16886, 13, 1449, 11, 1392, 13, 1692, 11, 309, 311, 445, 51136], "temperature": 0.0, "avg_logprob": -0.323944091796875, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.01080132182687521}, {"id": 340, "seek": 173488, "start": 1750.3200000000002, "end": 1755.2800000000002, "text": " how you can connect me with me and also Apache Pulsar. If you're interested, we have a Slack group", "tokens": [51136, 577, 291, 393, 1745, 385, 365, 385, 293, 611, 46597, 430, 9468, 289, 13, 759, 291, 434, 3102, 11, 321, 362, 257, 37211, 1594, 51384], "temperature": 0.0, "avg_logprob": -0.323944091796875, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.01080132182687521}, {"id": 341, "seek": 173488, "start": 1755.2800000000002, "end": 1760.48, "text": " and also a wiki page. Sorry, wiki page of Apache Pulsar neighborhood team that you can", "tokens": [51384, 293, 611, 257, 261, 9850, 3028, 13, 4919, 11, 261, 9850, 3028, 295, 46597, 430, 9468, 289, 7630, 1469, 300, 291, 393, 51644], "temperature": 0.0, "avg_logprob": -0.323944091796875, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.01080132182687521}, {"id": 342, "seek": 176048, "start": 1761.04, "end": 1767.04, "text": " build up on more stuff on. So, okay, I think then this is pretty much it. And if you need more", "tokens": [50392, 1322, 493, 322, 544, 1507, 322, 13, 407, 11, 1392, 11, 286, 519, 550, 341, 307, 1238, 709, 309, 13, 400, 498, 291, 643, 544, 50692], "temperature": 0.0, "avg_logprob": -0.1887286247745637, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.010786851868033409}, {"id": 343, "seek": 176048, "start": 1767.92, "end": 1772.4, "text": " links to Pulsar, this is the page and I can share the slide that we can share the slide", "tokens": [50736, 6123, 281, 430, 9468, 289, 11, 341, 307, 264, 3028, 293, 286, 393, 2073, 264, 4137, 300, 321, 393, 2073, 264, 4137, 50960], "temperature": 0.0, "avg_logprob": -0.1887286247745637, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.010786851868033409}, {"id": 344, "seek": 176048, "start": 1772.4, "end": 1777.76, "text": " with you so you can have more. And we also have developer data stacks. We'll have it on YouTube", "tokens": [50960, 365, 291, 370, 291, 393, 362, 544, 13, 400, 321, 611, 362, 10754, 1412, 30792, 13, 492, 603, 362, 309, 322, 3088, 51228], "temperature": 0.0, "avg_logprob": -0.1887286247745637, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.010786851868033409}, {"id": 345, "seek": 176048, "start": 1777.76, "end": 1783.68, "text": " in five minutes about Pulsar if you're interested in that. Also, we'll have also master data stacks", "tokens": [51228, 294, 1732, 2077, 466, 430, 9468, 289, 498, 291, 434, 3102, 294, 300, 13, 2743, 11, 321, 603, 362, 611, 4505, 1412, 30792, 51524], "temperature": 0.0, "avg_logprob": -0.1887286247745637, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.010786851868033409}, {"id": 346, "seek": 176048, "start": 1783.68, "end": 1788.08, "text": " developers that you can find examples to because today we can't get into a lot of these things.", "tokens": [51524, 8849, 300, 291, 393, 915, 5110, 281, 570, 965, 321, 393, 380, 483, 666, 257, 688, 295, 613, 721, 13, 51744], "temperature": 0.0, "avg_logprob": -0.1887286247745637, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.010786851868033409}, {"id": 347, "seek": 178808, "start": 1788.08, "end": 1791.76, "text": " And myself too, I've actually have a Twitch stream every Wednesday afternoon,", "tokens": [50364, 400, 2059, 886, 11, 286, 600, 767, 362, 257, 22222, 4309, 633, 10579, 6499, 11, 50548], "temperature": 0.2, "avg_logprob": -0.29896502176920575, "compression_ratio": 1.461139896373057, "no_speech_prob": 0.011737960390746593}, {"id": 348, "seek": 178808, "start": 1792.48, "end": 1796.3999999999999, "text": " central time like Chicago, so there will be evening time here. So if you're interested,", "tokens": [50584, 5777, 565, 411, 9525, 11, 370, 456, 486, 312, 5634, 565, 510, 13, 407, 498, 291, 434, 3102, 11, 50780], "temperature": 0.2, "avg_logprob": -0.29896502176920575, "compression_ratio": 1.461139896373057, "no_speech_prob": 0.011737960390746593}, {"id": 349, "seek": 178808, "start": 1796.3999999999999, "end": 1803.12, "text": " you can follow me on Twitch as well. And yeah, I think that's it. How did it become like this?", "tokens": [50780, 291, 393, 1524, 385, 322, 22222, 382, 731, 13, 400, 1338, 11, 286, 519, 300, 311, 309, 13, 1012, 630, 309, 1813, 411, 341, 30, 51116], "temperature": 0.2, "avg_logprob": -0.29896502176920575, "compression_ratio": 1.461139896373057, "no_speech_prob": 0.011737960390746593}, {"id": 350, "seek": 178808, "start": 1803.12, "end": 1810.24, "text": " Thank you. Thank you.", "tokens": [51116, 1044, 291, 13, 1044, 291, 13, 51472], "temperature": 0.2, "avg_logprob": -0.29896502176920575, "compression_ratio": 1.461139896373057, "no_speech_prob": 0.011737960390746593}, {"id": 351, "seek": 181024, "start": 1810.24, "end": 1816.4, "text": " Backup slides in case the camera gets disastrously.", "tokens": [50364, 5833, 1010, 9788, 294, 1389, 264, 2799, 2170, 42103, 81, 5098, 13, 50672], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 352, "seek": 181024, "start": 1816.4, "end": 1818.4, "text": " Two questions.", "tokens": [50672, 4453, 1651, 13, 50772], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 353, "seek": 181024, "start": 1818.4, "end": 1822.4, "text": " Here's one.", "tokens": [50772, 1692, 311, 472, 13, 50972], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 354, "seek": 181024, "start": 1822.4, "end": 1827.84, "text": " Yeah, quick one. As you know, the way Kafka has, you can have cluster of Kafka instance", "tokens": [50972, 865, 11, 1702, 472, 13, 1018, 291, 458, 11, 264, 636, 47064, 575, 11, 291, 393, 362, 13630, 295, 47064, 5197, 51244], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 355, "seek": 181024, "start": 1827.84, "end": 1831.28, "text": " brokers and stuff like that. Do you have that same problem where like one goes down,", "tokens": [51244, 47549, 293, 1507, 411, 300, 13, 1144, 291, 362, 300, 912, 1154, 689, 411, 472, 1709, 760, 11, 51416], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 356, "seek": 181024, "start": 1831.28, "end": 1834.32, "text": " the other one says going down? It's like zookeeper kind of causes that problem.", "tokens": [51416, 264, 661, 472, 1619, 516, 760, 30, 467, 311, 411, 25347, 23083, 733, 295, 7700, 300, 1154, 13, 51568], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 357, "seek": 181024, "start": 1834.32, "end": 1838.48, "text": " Because when I say Kafka, it always seems to be a problem with speaking with each other.", "tokens": [51568, 1436, 562, 286, 584, 47064, 11, 309, 1009, 2544, 281, 312, 257, 1154, 365, 4124, 365, 1184, 661, 13, 51776], "temperature": 0.0, "avg_logprob": -0.38292412285332206, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.5510075688362122}, {"id": 358, "seek": 183848, "start": 1838.48, "end": 1846.88, "text": " So that's more about, I can move it offline with you and if folks are interested too,", "tokens": [50364, 407, 300, 311, 544, 466, 11, 286, 393, 1286, 309, 21857, 365, 291, 293, 498, 4024, 366, 3102, 886, 11, 50784], "temperature": 0.0, "avg_logprob": -0.19860804875691732, "compression_ratio": 1.65, "no_speech_prob": 0.019802000373601913}, {"id": 359, "seek": 183848, "start": 1846.88, "end": 1851.76, "text": " but I actually also have another talk that's kind of deeper right into Pulsar.", "tokens": [50784, 457, 286, 767, 611, 362, 1071, 751, 300, 311, 733, 295, 7731, 558, 666, 430, 9468, 289, 13, 51028], "temperature": 0.0, "avg_logprob": -0.19860804875691732, "compression_ratio": 1.65, "no_speech_prob": 0.019802000373601913}, {"id": 360, "seek": 183848, "start": 1851.76, "end": 1855.44, "text": " As well as tomorrow too, I actually have a talk at the open JDK room, but that's", "tokens": [51028, 1018, 731, 382, 4153, 886, 11, 286, 767, 362, 257, 751, 412, 264, 1269, 37082, 42, 1808, 11, 457, 300, 311, 51212], "temperature": 0.0, "avg_logprob": -0.19860804875691732, "compression_ratio": 1.65, "no_speech_prob": 0.019802000373601913}, {"id": 361, "seek": 183848, "start": 1855.44, "end": 1860.96, "text": " more focusing on JMS. So there will be open JDK room in building H. So if you want to do that.", "tokens": [51212, 544, 8416, 322, 508, 10288, 13, 407, 456, 486, 312, 1269, 37082, 42, 1808, 294, 2390, 389, 13, 407, 498, 291, 528, 281, 360, 300, 13, 51488], "temperature": 0.0, "avg_logprob": -0.19860804875691732, "compression_ratio": 1.65, "no_speech_prob": 0.019802000373601913}, {"id": 362, "seek": 183848, "start": 1860.96, "end": 1866.08, "text": " But as far as Pulsar is concerned, it is very cloud native by itself. So a lot of things", "tokens": [51488, 583, 382, 1400, 382, 430, 9468, 289, 307, 5922, 11, 309, 307, 588, 4588, 8470, 538, 2564, 13, 407, 257, 688, 295, 721, 51744], "temperature": 0.0, "avg_logprob": -0.19860804875691732, "compression_ratio": 1.65, "no_speech_prob": 0.019802000373601913}, {"id": 363, "seek": 186608, "start": 1866.08, "end": 1869.84, "text": " who in fact I didn't get to talk about is very infrastructure aware.", "tokens": [50364, 567, 294, 1186, 286, 994, 380, 483, 281, 751, 466, 307, 588, 6896, 3650, 13, 50552], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 364, "seek": 186608, "start": 1869.84, "end": 1873.28, "text": " So things like, you know, you don't want to worry about how do you deal with", "tokens": [50552, 407, 721, 411, 11, 291, 458, 11, 291, 500, 380, 528, 281, 3292, 466, 577, 360, 291, 2028, 365, 50724], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 365, "seek": 186608, "start": 1873.28, "end": 1876.6399999999999, "text": " offsets while in Kafka. You don't have such, you don't have such thing.", "tokens": [50724, 39457, 1385, 1339, 294, 47064, 13, 509, 500, 380, 362, 1270, 11, 291, 500, 380, 362, 1270, 551, 13, 50892], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 366, "seek": 186608, "start": 1876.6399999999999, "end": 1880.6399999999999, "text": " And then there are other things too. You just don't have enough time.", "tokens": [50892, 400, 550, 456, 366, 661, 721, 886, 13, 509, 445, 500, 380, 362, 1547, 565, 13, 51092], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 367, "seek": 186608, "start": 1880.6399999999999, "end": 1884.48, "text": " I think I was told this time's up. So we can move it offline and then follow me on", "tokens": [51092, 286, 519, 286, 390, 1907, 341, 565, 311, 493, 13, 407, 321, 393, 1286, 309, 21857, 293, 550, 1524, 385, 322, 51284], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 368, "seek": 186608, "start": 1884.48, "end": 1887.9199999999998, "text": " my discord. Actually, I didn't even get my discord, but follow me somewhere.", "tokens": [51284, 452, 32989, 13, 5135, 11, 286, 994, 380, 754, 483, 452, 32989, 11, 457, 1524, 385, 4079, 13, 51456], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 369, "seek": 186608, "start": 1888.48, "end": 1892.8799999999999, "text": " Twitter. And I'll be happy to answer any questions you have. Thank you.", "tokens": [51484, 5794, 13, 400, 286, 603, 312, 2055, 281, 1867, 604, 1651, 291, 362, 13, 1044, 291, 13, 51704], "temperature": 0.0, "avg_logprob": -0.204972801479042, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0323953814804554}, {"id": 370, "seek": 189288, "start": 1892.88, "end": 1899.68, "text": " Okay. Thank you. Thank you.", "tokens": [50364, 1033, 13, 1044, 291, 13, 1044, 291, 13, 50704], "temperature": 0.0, "avg_logprob": -0.36462371999567206, "compression_ratio": 1.08, "no_speech_prob": 0.26458337903022766}], "language": "en"}