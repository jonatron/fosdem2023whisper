{"text": " Hello, everyone. I am Bhavan. Yes, I'm here because I love software. Also, I really love talking. This is however the first time I'm giving a talk, so go easy on me. I work in Munich as a senior dev at this small startup called WorkerBase. I love DIY and I love making cocktails. That's me making one when I shouldn't be. I don't use a lot of social media, but you can find me on LinkedIn. Let's jump right into it and talk about some JavaScript architecture. I was secret to share with you. I've had the worst layover of my life at Berlin Airport and all I want to do right now is just sleep on a bench. You guys have to help me a little bit out here and make this more interactive so I don't go to sleep. Usually, it's the other way around. I had questions in there hoping that the audience will not go to sleep, but now it's on you guys. First thing, and this is a fairly easy and uncontroversial statement, I hope, right? The JavaScript engines are asynchronous. Is there anyone who disagrees with this? Do you all agree with it? Who agrees with it? Just raise your hand. It's fine. It's fine. Don't be ashamed. Just raise your hand. Okay. You agree with it? Three, four people who disagrees with it? Okay. Can one of you maybe tell me why you disagree? I mean, don't give a reason that because you're asking this question, obviously, the answer is no, but can you give a reason apart from that? Yes, exactly. So JavaScript engines are actually synchronous. Right? JavaScript and time environments are, in fact, asynchronous, right? And we'll talk about in this talk what's the difference between the two, right? But so far, does anyone get what I'm saying? There's the engine and then there's the runtime environment and there are two separate things, right? Final question. Is Node.js single-threaded? Yes. Anyone says no? Okay. One guy. Two. Good. I'll not bother you too much with this. There's a bit of a bad question, so to say, because I should have defined what do I mean by Node.js here? Do I mean the runtime or do I mean the whole ecosystem? But colloquially, when you say Node.js, you mean the whole thing, right? And that is not always single-threaded. There are parts of it that are actually multi-threaded and we will try and demystify some of these things, right? So this is what the JavaScript runtime environment, wow, that's a mouthful. This is what it looks like, right? Up there, you have your V8 engine, which is the JavaScript engine. I mean, it doesn't have to be V8. It's V8 for Chrome, SpiderMonkey, for Mozilla, so on and so forth. But that's the JavaScript engine, right? That's the thing that understands JavaScript and parses it and does a bunch of things, right? It reads and reads JavaScript, does stuff. And that thing, as we just mentioned, is synchronous. There's a few other things here that will actually give you the asynchronous part, right? But let's talk first only about V8, right? So what does V8? And when I say V8, I'm only using it as a placeholder for JavaScript engine, right? It could be any engine. It doesn't matter for the purpose of this talk. So what does it do? It does memory allocation. You have your heap, so it manages, it randomly allocates memory whenever it needs to store like a variable or something. It has the execution context, which is a fancy term for your call stack, right? And we'll talk about what call stack is in a slide or two. It is also single threaded, right? And synchronous. Okay. So yeah, I pretty much covered that, I guess. A quick intro to call stack. If you have ever seen an error message in JavaScript, what you see there is your call stack, right? It's a snapshot of your call stack when that error happens. That's basically what it is. And it's single threaded, which means that there is only one call stack in the JavaScript engine, right? So in other words, it can only do one thing at a time, right? If it has to do two things, it cannot. It has to first finish what it's doing and then do the next thing. Right? Let's look at a quick example for this, right? So what I have here is a simple pseudo code. Well, not pseudo code. It's a working code, right? So we have three functions here. It's pretty self-explanatory, right? I don't need to explain you what's going on here. What we will see on this side is what's happening with the call stack, right? So as the execution starts, you have your first function. Okay. First of all, you have sort of the general execution context, right? Which is sort of like the main equivalent of your JavaScript engine. Like if you've ever done like cc++ code, there's this main function, right? So that's this. You have a bunch of functions, nothing to do so far, nothing to execute. And then we actually come to a statement, right? And what do we want? We want to print greeting. So that adds something to the call stack, right? So now we are going to this function, print greeting. What do we do inside of print greeting? We call the greet function, right? And we call it with some value, but that's not important. So when we call the greet function, one more thing gets added to the call stack. And what are we doing in the greet function? We are calling the join words function, right? One more thing gets added to the call stack. And now you hit return, right? So now we actually have to return something. So something gets popped from the stack, right? So now you are out of the join function. You now, you go to the return statement of greet, you are out of greet. You go into the next statement of your print greeting. You do your console log. And there's no return statement here, but it's end of the function. So you're going out of this one as well, right? And you're back to your main thread. And that's it. That's how asynchronous, not asynchronous. That's how asynchronous JavaScript code runs, right? So far so good. Everyone with me? Great. Another example, like I was saying, if you've ever seen an error stack, essentially what you see is the call stack, right? You see a snapshot of the stack when the error happened, right? Or if you ever use the debug tool, for example, you are also seeing the call stack over there. Now let's look at something slightly different. What happens in this case, right? We all know what would happen without referring to the stack, right? It'll give hello, it'll give there, and then it'll give forcedM, right? But now there's sort of two things happening here. There's a mistake there. I forgot to add the time. It'll be there in the next slide. Just ignore that. But yeah. So what happens, right? Because our call stack now cannot do two things at a time, right? And we know from experience that setTimeout is going to sort of run parallelly while the stack moves on to the next thing, right? So that's where the other things that we had in that previous picture come into play, right? So you have mainly three things here. You have your web APIs. In the browser, you'll have web APIs. If you're doing Node.js, you'll have what's called libUV. And towards the end, I'll also talk a bit about libUV. But for now, let's assume we are in the browser, right? So we are on the client side. You have these web APIs here. You have your task queue. And you have the star of the stock, the rotating thing, what's called the event loop, right? And you will go through in more detail. But just to summarize, all the stuff that is sort of slower, right? That's not happening immediately. That's not being invoked immediately or running synchronously. That gets delegated to the web APIs, right? So here you have your DOM manipulation. You can make XHR calls. You can do setTimeout. If you were in Node.js, you could make a call to the database. Anything that's slow runs here, right? But the whole point of doing this is that after you run something, you want to do something back into your main thread, right? You are finally, you're running JavaScript. And you are making a call to some external system web or doing some delay. But finally, you then what, right? You need to do something. And that something is handled by the task queue, right? So take a simple example of a setTimeout. You have a callback and some delay, right? So the actual waiting for the time, say you put a delay of 300 milliseconds. So the actual waiting for 300 milliseconds is done by the web API. Then your callback goes to the task queue. And then it has to somehow go back to your main call stack and get executed, right? Because your callback is still JavaScript. I mean, we are assuming it's a simple callback here. Obviously, that itself could have another callback and then the process repeats, right? But let's assume for now we are just doing like a console or you know, callback, right? So that's pure JavaScript and it has to go back to the stack. And the event loop is what decides this part, right? So the event loop checks if the stack is empty, if the call stack is empty, as in if it's idle, then check the task queue. If there's something in the task queue, move it to the call stack, right? This is in a nutshell what's happening. This is in a nutshell how JavaScript manages to have asynchronous features, right? While still itself being single threaded and synchronous. So let's look at our function. And now we have, let's run this, right? So we have our main. We go to console log and we log hello. Okay. I don't know. We'll just, we'll not do full screen. Ah, I know what's not working. So that there's, there's a gif here of, of like a timer. So just, just assume that for now, right? But, but yeah, it's not important. We can still manage without it, right? So let's, let's go back. Let's start from the top, from the bottom, right? So you have your main, you have your main function, then you call your console log that gets executed. So it gets popped off the stack. You guys know this, but I'm doing this practice because I'm also going to ask you questions about this, right? So it's a nice thing to visualize for some examples, right? You have your set timeout, right? Set timeout is not on the stack. It gets delegated to the web API, right? Which starts a timer. And then it knows that it has to run a callback when the timer is finished, right? While this is running, we move on to the next thing that we can do, right? In the stack. And then we have another console log that gets executed. It goes away as in it's popped. And then the main thread is free, right? And then after some time, so, so you see that tick? There was supposed to be a loading thing there, right? To show that it's still counting. After some time, your two seconds are done. And then, so after some time, your two seconds are done. Your timer is over. That's also done. You move the callback to your task queue, right? And then the event loop checks if the call stack is free, which in this case it is free, right? It moves the callback function over to the stack and then it runs it, right? So there's again a console log that gets printed and you're done, right? So to summarize the JavaScript engine itself is synchronous and single-threaded. The Node.js runtime is asynchronous because it manages the asynchronous things outside of the JavaScript engine, right? In a separate thing, right? Which is usually written in C++, either by VPIs or LibUV. And the event loop is the glue that ties all of this together, right? So wherever your thing is running, the callback goes to the task queue and then the event loop decides when to run these callbacks. All right. Before I go there, does anyone have a question so far? Yeah? During that event loop, are they green threads that are created? How does that work from lower level? Sorry, can you repeat it? Are they green threads that are created then? How in terms of lower level, does it become multi-threaded at that moment? Yeah? You want me to give them the mic? Okay, yeah. So the question is, in the event loop, are there green threads that are created? The event loop does just one thing, right? It's a loop. It's basically a while too, right? And we will try and make a more complex model of it. But for now, for our understanding, it's doing just one thing. Go to the queue. If there's something in the queue and nothing in the stack, pull the first thing from the queue and put it in the stack, right? That's all it's doing. So there's no multi-threading. There's no nothing. It's just a while loop. There is multi-threading in the other part over here, but in Node.js, right? Not in the browser. All right. So, exercise time, right? This is a super simple function. Ignore all the boilerplate in the HTML. I was too lazy to remove it. All that that HTML is doing is it has a script.js and the script.js is over here, right? What is the script.js doing? Pretty simple to, pretty similar to what you guys saw earlier. It has two console logs, a set timeout in between, and another console log in the callback, right? So the first question is this, what's the output? Perfect, right? That's very simple. We just saw it. Second question, what will be the max number of tasks in the task queue? Right? This is a little bit tricky. One. Two. Who said two? Why? Yeah, but console log is just going to run on the stack. So, see, your answer is right, but the reason is wrong, right? So there will in fact be two things, right? Because reading the script tag itself is a task, right? In this case, the script tag is a different file, so it actually has to, I don't know how browsers internally do it, but actually has to go to the location and read the content. But even if the script tag is on the HTML, it's still a different task, right? And this is important to understand if you have to guess, even there's a race condition basically, right? And when we come to micro tasks, this will become a little bit more complex, and we have an example for that. But just keep in mind for now that the script tag is also a task, right? And that's the last part in the learning. So this talk, I don't know if anyone read the description, is in three parts, right? So there's intro to what happens in the browser, then there's a deep dive of the loop itself, or, well, the task queue, and then the third part is Node.js. And we are done with the first part, right? So everything looks good so far, right? It's a little bit janky, they did some weird things there, but we get what's happening now, right? Well, not exactly. I mean, we do, but there's more nuances, right? So let's do a deep dive into what's actually happening inside of the loop and how our different tasks handle, right? So as the line says, not all tasks are created equal. This is the model we had so far, right? That the task queue is a simple single queue, which had callback 1, callback 2, callback 3, whatever. And every time the event loop is a while loop, right? So in each cycle of the while loop, which is, by the way, called a tick, right? That's just the term. You might have heard this sometimes next tick, right? That's what it's talking about. Okay. I've been told I don't have a lot of time. Let's try to fit this because we are, like, barely halfway there. In reality, there are multiple queues inside of your task queue, right? So what I told you earlier was a bit of a lie. And actually JavaScript or, well, the JavaScript ecosystem does not handle each task equally, right? Some are given a higher priority than the others. Click events for example. And this varies a bit from browser to browser. It's different in Node.js. So don't take this as like Bible. This is just an example to show you, right? And it's also oversimplified, obviously. But click events are given a higher priority and then everything else, right? There is also something called a request animation frame called back queue, right? And what the F is that, right? So the browser, sorry, JavaScript and time is also responsible for rendering, right? It's also responsible for drawing things on the screen because the browser is doing that, right? And it doesn't do it on every tick, right? Because that would be wasteful, right? Because you have a tick happens roughly, let's say, one millisecond. Not really, but let's assume that. Whereas if you have a 60 hertz screen, you only need to refresh every 16 or 17 milliseconds, right? So it's smart enough to understand that I don't need to do this all the time, but it does need to do it at some point, right? And if you block the event loop, you're going to freeze your screen, right? That's the big take home message. So let's look super quickly at an example. We have three frames here, right? And there's a rendering step happening on each frame, right? And now what we want to do is we want to... So the time is up, but if you guys don't mind, I'm maybe going to take five more minutes. If you have questions, maybe hit me up later, but I at least want to finish this part, right? So you have the rendering step. And now say if you want to change some logic in the rendering step or related to rendering. You would just do like a timeout or something, right? Say, run this logic every, I don't know, at 60 hertz frequency. But that's not very good because as we saw in the set timeout zero example, the time you give in set timeout is not guaranteed, right? It's the minimum time that it'll wait. If your queue, if your stack is not empty, when your time is up, then it'll wait for the next take, it'll wait for the stack to be empty and only then will it do something, right? So if you just ran this as is in like a set timeout, that'd be a very bad user experience, right? Because you might skip a few frames, you might have visual artifacts, and all kinds of weird things can happen, right? So that's why we have a separate queue for this, right? And that queue can be accessed through what's called request animation frame. And that's this in the parlance of my coloring, right? So let's quickly summarize. The event loop is responsible for rendering frames, but not in every cycle. But when it does render something, it takes everything in the request animation frame queue and renders it altogether, right? So you have this, and then you have micro tasks. And micro tasks are basically promises, right? They're not really, but we are oversimplifying, and we are anyways out of time. So let's just stick with that for now, right? The big difference with micro tasks is that the queue has to be empty whenever it's run, right? Even if the micro task creates another task that also needs to get executed, you don't wait for the next time, right? So let's look at an example super quick. So in one tick, in one cycle, right, on which animation is also going to get rendered, you will pick up one of the regular events. You would do all the micro tasks, right? If you have more micro tasks, you will do all of them. Then you will go to the animation frame. If you have more tasks for animation, you will not do them, right? You will wait for the next animation cycle to do it. That's some big difference between these two. And again, as you can see, if you mess up micro tasks, you can end up freezing up your screen, right? You put a while loop. You do a task that calls itself, like a promise that calls itself, and then everything is going to get frozen. That's because of this. All right. I think we are out of time. This is maybe the one last thing I want to do, if you guys don't mind, and then we'll stop, right? So can someone tell me what would be the answer to this? Awesome. Can you explain it? Perfect. Yes. And also keep in mind that the script itself is a task, right? Because why not be otherwise, right? So why not just put the set time out in the queue and execute it? But you can't because script itself is in the queue, right? So the set time out will go afterwards. All right. That's about it. Thanks a lot. Hope you guys learned something. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.08, "text": " Hello, everyone. I am Bhavan. Yes, I'm here because I love software. Also, I really love", "tokens": [50364, 2425, 11, 1518, 13, 286, 669, 13550, 21071, 13, 1079, 11, 286, 478, 510, 570, 286, 959, 4722, 13, 2743, 11, 286, 534, 959, 51118], "temperature": 0.0, "avg_logprob": -0.2426337442900005, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.18480774760246277}, {"id": 1, "seek": 0, "start": 15.08, "end": 22.56, "text": " talking. This is however the first time I'm giving a talk, so go easy on me. I work in", "tokens": [51118, 1417, 13, 639, 307, 4461, 264, 700, 565, 286, 478, 2902, 257, 751, 11, 370, 352, 1858, 322, 385, 13, 286, 589, 294, 51492], "temperature": 0.0, "avg_logprob": -0.2426337442900005, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.18480774760246277}, {"id": 2, "seek": 0, "start": 22.56, "end": 29.84, "text": " Munich as a senior dev at this small startup called WorkerBase. I love DIY and I love", "tokens": [51492, 40601, 382, 257, 7965, 1905, 412, 341, 1359, 18578, 1219, 6603, 260, 33, 651, 13, 286, 959, 22194, 293, 286, 959, 51856], "temperature": 0.0, "avg_logprob": -0.2426337442900005, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.18480774760246277}, {"id": 3, "seek": 2984, "start": 29.84, "end": 36.480000000000004, "text": " making cocktails. That's me making one when I shouldn't be. I don't use a lot of social", "tokens": [50364, 1455, 49006, 13, 663, 311, 385, 1455, 472, 562, 286, 4659, 380, 312, 13, 286, 500, 380, 764, 257, 688, 295, 2093, 50696], "temperature": 0.0, "avg_logprob": -0.16566047929737665, "compression_ratio": 1.4278350515463918, "no_speech_prob": 0.04954161122441292}, {"id": 4, "seek": 2984, "start": 36.480000000000004, "end": 45.04, "text": " media, but you can find me on LinkedIn. Let's jump right into it and talk about some JavaScript", "tokens": [50696, 3021, 11, 457, 291, 393, 915, 385, 322, 20657, 13, 961, 311, 3012, 558, 666, 309, 293, 751, 466, 512, 15778, 51124], "temperature": 0.0, "avg_logprob": -0.16566047929737665, "compression_ratio": 1.4278350515463918, "no_speech_prob": 0.04954161122441292}, {"id": 5, "seek": 2984, "start": 45.04, "end": 54.760000000000005, "text": " architecture. I was secret to share with you. I've had the worst layover of my life at Berlin", "tokens": [51124, 9482, 13, 286, 390, 4054, 281, 2073, 365, 291, 13, 286, 600, 632, 264, 5855, 2360, 3570, 295, 452, 993, 412, 13848, 51610], "temperature": 0.0, "avg_logprob": -0.16566047929737665, "compression_ratio": 1.4278350515463918, "no_speech_prob": 0.04954161122441292}, {"id": 6, "seek": 5476, "start": 54.76, "end": 62.44, "text": " Airport and all I want to do right now is just sleep on a bench. You guys have to help me a", "tokens": [50364, 25784, 293, 439, 286, 528, 281, 360, 558, 586, 307, 445, 2817, 322, 257, 10638, 13, 509, 1074, 362, 281, 854, 385, 257, 50748], "temperature": 0.0, "avg_logprob": -0.1571382201544129, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.503741443157196}, {"id": 7, "seek": 5476, "start": 62.44, "end": 68.28, "text": " little bit out here and make this more interactive so I don't go to sleep. Usually, it's the other", "tokens": [50748, 707, 857, 484, 510, 293, 652, 341, 544, 15141, 370, 286, 500, 380, 352, 281, 2817, 13, 11419, 11, 309, 311, 264, 661, 51040], "temperature": 0.0, "avg_logprob": -0.1571382201544129, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.503741443157196}, {"id": 8, "seek": 5476, "start": 68.28, "end": 73.24, "text": " way around. I had questions in there hoping that the audience will not go to sleep, but now it's on", "tokens": [51040, 636, 926, 13, 286, 632, 1651, 294, 456, 7159, 300, 264, 4034, 486, 406, 352, 281, 2817, 11, 457, 586, 309, 311, 322, 51288], "temperature": 0.0, "avg_logprob": -0.1571382201544129, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.503741443157196}, {"id": 9, "seek": 5476, "start": 73.24, "end": 80.84, "text": " you guys. First thing, and this is a fairly easy and uncontroversial statement, I hope,", "tokens": [51288, 291, 1074, 13, 2386, 551, 11, 293, 341, 307, 257, 6457, 1858, 293, 36019, 340, 840, 831, 5629, 11, 286, 1454, 11, 51668], "temperature": 0.0, "avg_logprob": -0.1571382201544129, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.503741443157196}, {"id": 10, "seek": 8084, "start": 80.92, "end": 87.88000000000001, "text": " right? The JavaScript engines are asynchronous. Is there anyone who disagrees with this?", "tokens": [50368, 558, 30, 440, 15778, 12982, 366, 49174, 13, 1119, 456, 2878, 567, 10414, 4856, 365, 341, 30, 50716], "temperature": 0.0, "avg_logprob": -0.15710590134805708, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.04946596547961235}, {"id": 11, "seek": 8084, "start": 90.04, "end": 95.88000000000001, "text": " Do you all agree with it? Who agrees with it? Just raise your hand.", "tokens": [50824, 1144, 291, 439, 3986, 365, 309, 30, 2102, 26383, 365, 309, 30, 1449, 5300, 428, 1011, 13, 51116], "temperature": 0.0, "avg_logprob": -0.15710590134805708, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.04946596547961235}, {"id": 12, "seek": 8084, "start": 99.08000000000001, "end": 104.44, "text": " It's fine. It's fine. Don't be ashamed. Just raise your hand. Okay. You agree with it?", "tokens": [51276, 467, 311, 2489, 13, 467, 311, 2489, 13, 1468, 380, 312, 19489, 13, 1449, 5300, 428, 1011, 13, 1033, 13, 509, 3986, 365, 309, 30, 51544], "temperature": 0.0, "avg_logprob": -0.15710590134805708, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.04946596547961235}, {"id": 13, "seek": 10444, "start": 104.52, "end": 111.72, "text": " Three, four people who disagrees with it? Okay. Can one of you maybe tell me why you disagree?", "tokens": [50368, 6244, 11, 1451, 561, 567, 10414, 4856, 365, 309, 30, 1033, 13, 1664, 472, 295, 291, 1310, 980, 385, 983, 291, 14091, 30, 50728], "temperature": 0.0, "avg_logprob": -0.13885257770488788, "compression_ratio": 1.4466019417475728, "no_speech_prob": 0.0653555691242218}, {"id": 14, "seek": 10444, "start": 112.44, "end": 116.52, "text": " I mean, don't give a reason that because you're asking this question, obviously,", "tokens": [50764, 286, 914, 11, 500, 380, 976, 257, 1778, 300, 570, 291, 434, 3365, 341, 1168, 11, 2745, 11, 50968], "temperature": 0.0, "avg_logprob": -0.13885257770488788, "compression_ratio": 1.4466019417475728, "no_speech_prob": 0.0653555691242218}, {"id": 15, "seek": 10444, "start": 116.52, "end": 120.6, "text": " the answer is no, but can you give a reason apart from that?", "tokens": [50968, 264, 1867, 307, 572, 11, 457, 393, 291, 976, 257, 1778, 4936, 490, 300, 30, 51172], "temperature": 0.0, "avg_logprob": -0.13885257770488788, "compression_ratio": 1.4466019417475728, "no_speech_prob": 0.0653555691242218}, {"id": 16, "seek": 10444, "start": 124.84, "end": 130.76, "text": " Yes, exactly. So JavaScript engines are actually synchronous.", "tokens": [51384, 1079, 11, 2293, 13, 407, 15778, 12982, 366, 767, 44743, 13, 51680], "temperature": 0.0, "avg_logprob": -0.13885257770488788, "compression_ratio": 1.4466019417475728, "no_speech_prob": 0.0653555691242218}, {"id": 17, "seek": 13076, "start": 131.07999999999998, "end": 139.79999999999998, "text": " Right? JavaScript and time environments are, in fact, asynchronous, right? And we'll talk about", "tokens": [50380, 1779, 30, 15778, 293, 565, 12388, 366, 11, 294, 1186, 11, 49174, 11, 558, 30, 400, 321, 603, 751, 466, 50816], "temperature": 0.0, "avg_logprob": -0.2056701494299847, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.008678142912685871}, {"id": 18, "seek": 13076, "start": 139.79999999999998, "end": 146.28, "text": " in this talk what's the difference between the two, right? But so far, does anyone get what I'm", "tokens": [50816, 294, 341, 751, 437, 311, 264, 2649, 1296, 264, 732, 11, 558, 30, 583, 370, 1400, 11, 775, 2878, 483, 437, 286, 478, 51140], "temperature": 0.0, "avg_logprob": -0.2056701494299847, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.008678142912685871}, {"id": 19, "seek": 13076, "start": 146.28, "end": 149.88, "text": " saying? There's the engine and then there's the runtime environment and there are two separate", "tokens": [51140, 1566, 30, 821, 311, 264, 2848, 293, 550, 456, 311, 264, 34474, 2823, 293, 456, 366, 732, 4994, 51320], "temperature": 0.0, "avg_logprob": -0.2056701494299847, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.008678142912685871}, {"id": 20, "seek": 14988, "start": 149.88, "end": 161.48, "text": " things, right? Final question. Is Node.js single-threaded? Yes. Anyone says no?", "tokens": [50364, 721, 11, 558, 30, 13443, 1168, 13, 1119, 38640, 13, 25530, 2167, 12, 392, 2538, 292, 30, 1079, 13, 14643, 1619, 572, 30, 50944], "temperature": 0.0, "avg_logprob": -0.224754718312046, "compression_ratio": 1.2447552447552448, "no_speech_prob": 0.016639480367302895}, {"id": 21, "seek": 14988, "start": 163.88, "end": 176.84, "text": " Okay. One guy. Two. Good. I'll not bother you too much with this. There's a bit of a bad question,", "tokens": [51064, 1033, 13, 1485, 2146, 13, 4453, 13, 2205, 13, 286, 603, 406, 8677, 291, 886, 709, 365, 341, 13, 821, 311, 257, 857, 295, 257, 1578, 1168, 11, 51712], "temperature": 0.0, "avg_logprob": -0.224754718312046, "compression_ratio": 1.2447552447552448, "no_speech_prob": 0.016639480367302895}, {"id": 22, "seek": 17684, "start": 176.84, "end": 181.96, "text": " so to say, because I should have defined what do I mean by Node.js here? Do I mean the runtime or", "tokens": [50364, 370, 281, 584, 11, 570, 286, 820, 362, 7642, 437, 360, 286, 914, 538, 38640, 13, 25530, 510, 30, 1144, 286, 914, 264, 34474, 420, 50620], "temperature": 0.0, "avg_logprob": -0.09506557143737222, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.03298286348581314}, {"id": 23, "seek": 17684, "start": 181.96, "end": 187.08, "text": " do I mean the whole ecosystem? But colloquially, when you say Node.js, you mean the whole thing,", "tokens": [50620, 360, 286, 914, 264, 1379, 11311, 30, 583, 1263, 29826, 2270, 11, 562, 291, 584, 38640, 13, 25530, 11, 291, 914, 264, 1379, 551, 11, 50876], "temperature": 0.0, "avg_logprob": -0.09506557143737222, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.03298286348581314}, {"id": 24, "seek": 17684, "start": 187.08, "end": 194.44, "text": " right? And that is not always single-threaded. There are parts of it that are actually multi-threaded", "tokens": [50876, 558, 30, 400, 300, 307, 406, 1009, 2167, 12, 392, 2538, 292, 13, 821, 366, 3166, 295, 309, 300, 366, 767, 4825, 12, 392, 2538, 292, 51244], "temperature": 0.0, "avg_logprob": -0.09506557143737222, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.03298286348581314}, {"id": 25, "seek": 17684, "start": 195.0, "end": 203.72, "text": " and we will try and demystify some of these things, right? So this is what the JavaScript", "tokens": [51272, 293, 321, 486, 853, 293, 1371, 38593, 2505, 512, 295, 613, 721, 11, 558, 30, 407, 341, 307, 437, 264, 15778, 51708], "temperature": 0.0, "avg_logprob": -0.09506557143737222, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.03298286348581314}, {"id": 26, "seek": 20372, "start": 204.52, "end": 210.92, "text": " runtime environment, wow, that's a mouthful. This is what it looks like, right? Up there,", "tokens": [50404, 34474, 2823, 11, 6076, 11, 300, 311, 257, 4525, 906, 13, 639, 307, 437, 309, 1542, 411, 11, 558, 30, 5858, 456, 11, 50724], "temperature": 0.0, "avg_logprob": -0.11774505509270562, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.011301014572381973}, {"id": 27, "seek": 20372, "start": 210.92, "end": 217.32, "text": " you have your V8 engine, which is the JavaScript engine. I mean, it doesn't have to be V8. It's", "tokens": [50724, 291, 362, 428, 691, 23, 2848, 11, 597, 307, 264, 15778, 2848, 13, 286, 914, 11, 309, 1177, 380, 362, 281, 312, 691, 23, 13, 467, 311, 51044], "temperature": 0.0, "avg_logprob": -0.11774505509270562, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.011301014572381973}, {"id": 28, "seek": 20372, "start": 217.32, "end": 226.28, "text": " V8 for Chrome, SpiderMonkey, for Mozilla, so on and so forth. But that's the JavaScript engine,", "tokens": [51044, 691, 23, 337, 15327, 11, 17733, 32498, 4119, 11, 337, 3335, 26403, 11, 370, 322, 293, 370, 5220, 13, 583, 300, 311, 264, 15778, 2848, 11, 51492], "temperature": 0.0, "avg_logprob": -0.11774505509270562, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.011301014572381973}, {"id": 29, "seek": 20372, "start": 226.28, "end": 232.6, "text": " right? That's the thing that understands JavaScript and parses it and does a bunch of things, right?", "tokens": [51492, 558, 30, 663, 311, 264, 551, 300, 15146, 15778, 293, 21156, 279, 309, 293, 775, 257, 3840, 295, 721, 11, 558, 30, 51808], "temperature": 0.0, "avg_logprob": -0.11774505509270562, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.011301014572381973}, {"id": 30, "seek": 23260, "start": 232.6, "end": 240.6, "text": " It reads and reads JavaScript, does stuff. And that thing, as we just mentioned, is synchronous.", "tokens": [50364, 467, 15700, 293, 15700, 15778, 11, 775, 1507, 13, 400, 300, 551, 11, 382, 321, 445, 2835, 11, 307, 44743, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10302510887685448, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019543026573956013}, {"id": 31, "seek": 23260, "start": 241.4, "end": 249.0, "text": " There's a few other things here that will actually give you the asynchronous part, right?", "tokens": [50804, 821, 311, 257, 1326, 661, 721, 510, 300, 486, 767, 976, 291, 264, 49174, 644, 11, 558, 30, 51184], "temperature": 0.0, "avg_logprob": -0.10302510887685448, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019543026573956013}, {"id": 32, "seek": 23260, "start": 250.51999999999998, "end": 256.84, "text": " But let's talk first only about V8, right? So what does V8? And when I say V8, I'm only using it as", "tokens": [51260, 583, 718, 311, 751, 700, 787, 466, 691, 23, 11, 558, 30, 407, 437, 775, 691, 23, 30, 400, 562, 286, 584, 691, 23, 11, 286, 478, 787, 1228, 309, 382, 51576], "temperature": 0.0, "avg_logprob": -0.10302510887685448, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019543026573956013}, {"id": 33, "seek": 23260, "start": 256.84, "end": 260.84, "text": " a placeholder for JavaScript engine, right? It could be any engine. It doesn't matter", "tokens": [51576, 257, 1081, 20480, 337, 15778, 2848, 11, 558, 30, 467, 727, 312, 604, 2848, 13, 467, 1177, 380, 1871, 51776], "temperature": 0.0, "avg_logprob": -0.10302510887685448, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019543026573956013}, {"id": 34, "seek": 26084, "start": 260.84, "end": 267.15999999999997, "text": " for the purpose of this talk. So what does it do? It does memory allocation. You have your heap,", "tokens": [50364, 337, 264, 4334, 295, 341, 751, 13, 407, 437, 775, 309, 360, 30, 467, 775, 4675, 27599, 13, 509, 362, 428, 33591, 11, 50680], "temperature": 0.0, "avg_logprob": -0.11546142787149508, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.0033729977440088987}, {"id": 35, "seek": 26084, "start": 267.15999999999997, "end": 273.32, "text": " so it manages, it randomly allocates memory whenever it needs to store like a variable or something.", "tokens": [50680, 370, 309, 22489, 11, 309, 16979, 12660, 1024, 4675, 5699, 309, 2203, 281, 3531, 411, 257, 7006, 420, 746, 13, 50988], "temperature": 0.0, "avg_logprob": -0.11546142787149508, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.0033729977440088987}, {"id": 36, "seek": 26084, "start": 274.67999999999995, "end": 281.96, "text": " It has the execution context, which is a fancy term for your call stack, right? And we'll talk", "tokens": [51056, 467, 575, 264, 15058, 4319, 11, 597, 307, 257, 10247, 1433, 337, 428, 818, 8630, 11, 558, 30, 400, 321, 603, 751, 51420], "temperature": 0.0, "avg_logprob": -0.11546142787149508, "compression_ratio": 1.4747474747474747, "no_speech_prob": 0.0033729977440088987}, {"id": 37, "seek": 28196, "start": 281.96, "end": 292.03999999999996, "text": " about what call stack is in a slide or two. It is also single threaded, right? And synchronous.", "tokens": [50364, 466, 437, 818, 8630, 307, 294, 257, 4137, 420, 732, 13, 467, 307, 611, 2167, 47493, 11, 558, 30, 400, 44743, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13906213972303602, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.06359545141458511}, {"id": 38, "seek": 28196, "start": 296.52, "end": 304.91999999999996, "text": " Okay. So yeah, I pretty much covered that, I guess. A quick intro to call stack. If you have ever seen", "tokens": [51092, 1033, 13, 407, 1338, 11, 286, 1238, 709, 5343, 300, 11, 286, 2041, 13, 316, 1702, 12897, 281, 818, 8630, 13, 759, 291, 362, 1562, 1612, 51512], "temperature": 0.0, "avg_logprob": -0.13906213972303602, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.06359545141458511}, {"id": 39, "seek": 30492, "start": 305.0, "end": 311.8, "text": " an error message in JavaScript, what you see there is your call stack, right? It's a snapshot of", "tokens": [50368, 364, 6713, 3636, 294, 15778, 11, 437, 291, 536, 456, 307, 428, 818, 8630, 11, 558, 30, 467, 311, 257, 30163, 295, 50708], "temperature": 0.0, "avg_logprob": -0.0766171656156841, "compression_ratio": 1.8140495867768596, "no_speech_prob": 0.15950995683670044}, {"id": 40, "seek": 30492, "start": 311.8, "end": 319.16, "text": " your call stack when that error happens. That's basically what it is. And it's single threaded,", "tokens": [50708, 428, 818, 8630, 562, 300, 6713, 2314, 13, 663, 311, 1936, 437, 309, 307, 13, 400, 309, 311, 2167, 47493, 11, 51076], "temperature": 0.0, "avg_logprob": -0.0766171656156841, "compression_ratio": 1.8140495867768596, "no_speech_prob": 0.15950995683670044}, {"id": 41, "seek": 30492, "start": 319.16, "end": 325.08000000000004, "text": " which means that there is only one call stack in the JavaScript engine, right? So in other words,", "tokens": [51076, 597, 1355, 300, 456, 307, 787, 472, 818, 8630, 294, 264, 15778, 2848, 11, 558, 30, 407, 294, 661, 2283, 11, 51372], "temperature": 0.0, "avg_logprob": -0.0766171656156841, "compression_ratio": 1.8140495867768596, "no_speech_prob": 0.15950995683670044}, {"id": 42, "seek": 30492, "start": 325.08000000000004, "end": 329.56, "text": " it can only do one thing at a time, right? If it has to do two things,", "tokens": [51372, 309, 393, 787, 360, 472, 551, 412, 257, 565, 11, 558, 30, 759, 309, 575, 281, 360, 732, 721, 11, 51596], "temperature": 0.0, "avg_logprob": -0.0766171656156841, "compression_ratio": 1.8140495867768596, "no_speech_prob": 0.15950995683670044}, {"id": 43, "seek": 30492, "start": 331.16, "end": 334.36, "text": " it cannot. It has to first finish what it's doing and then do the next thing.", "tokens": [51676, 309, 2644, 13, 467, 575, 281, 700, 2413, 437, 309, 311, 884, 293, 550, 360, 264, 958, 551, 13, 51836], "temperature": 0.0, "avg_logprob": -0.0766171656156841, "compression_ratio": 1.8140495867768596, "no_speech_prob": 0.15950995683670044}, {"id": 44, "seek": 33492, "start": 335.48, "end": 343.32, "text": " Right? Let's look at a quick example for this, right? So what I have here is a simple pseudo", "tokens": [50392, 1779, 30, 961, 311, 574, 412, 257, 1702, 1365, 337, 341, 11, 558, 30, 407, 437, 286, 362, 510, 307, 257, 2199, 35899, 50784], "temperature": 0.0, "avg_logprob": -0.10744236964805454, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.00028228084556758404}, {"id": 45, "seek": 33492, "start": 343.32, "end": 350.84000000000003, "text": " code. Well, not pseudo code. It's a working code, right? So we have three functions here.", "tokens": [50784, 3089, 13, 1042, 11, 406, 35899, 3089, 13, 467, 311, 257, 1364, 3089, 11, 558, 30, 407, 321, 362, 1045, 6828, 510, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10744236964805454, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.00028228084556758404}, {"id": 46, "seek": 33492, "start": 351.96000000000004, "end": 355.40000000000003, "text": " It's pretty self-explanatory, right? I don't need to explain you what's going on here.", "tokens": [51216, 467, 311, 1238, 2698, 12, 3121, 16554, 4745, 11, 558, 30, 286, 500, 380, 643, 281, 2903, 291, 437, 311, 516, 322, 510, 13, 51388], "temperature": 0.0, "avg_logprob": -0.10744236964805454, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.00028228084556758404}, {"id": 47, "seek": 33492, "start": 357.48, "end": 364.28000000000003, "text": " What we will see on this side is what's happening with the call stack, right? So as the execution", "tokens": [51492, 708, 321, 486, 536, 322, 341, 1252, 307, 437, 311, 2737, 365, 264, 818, 8630, 11, 558, 30, 407, 382, 264, 15058, 51832], "temperature": 0.0, "avg_logprob": -0.10744236964805454, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.00028228084556758404}, {"id": 48, "seek": 36428, "start": 364.28, "end": 372.67999999999995, "text": " starts, you have your first function. Okay. First of all, you have sort of the general execution", "tokens": [50364, 3719, 11, 291, 362, 428, 700, 2445, 13, 1033, 13, 2386, 295, 439, 11, 291, 362, 1333, 295, 264, 2674, 15058, 50784], "temperature": 0.0, "avg_logprob": -0.09832911795758187, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0007913438021205366}, {"id": 49, "seek": 36428, "start": 372.67999999999995, "end": 379.23999999999995, "text": " context, right? Which is sort of like the main equivalent of your JavaScript engine. Like if", "tokens": [50784, 4319, 11, 558, 30, 3013, 307, 1333, 295, 411, 264, 2135, 10344, 295, 428, 15778, 2848, 13, 1743, 498, 51112], "temperature": 0.0, "avg_logprob": -0.09832911795758187, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0007913438021205366}, {"id": 50, "seek": 36428, "start": 379.23999999999995, "end": 383.79999999999995, "text": " you've ever done like cc++ code, there's this main function, right? So that's this.", "tokens": [51112, 291, 600, 1562, 1096, 411, 269, 66, 25472, 3089, 11, 456, 311, 341, 2135, 2445, 11, 558, 30, 407, 300, 311, 341, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09832911795758187, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0007913438021205366}, {"id": 51, "seek": 36428, "start": 386.35999999999996, "end": 392.91999999999996, "text": " You have a bunch of functions, nothing to do so far, nothing to execute. And then we actually", "tokens": [51468, 509, 362, 257, 3840, 295, 6828, 11, 1825, 281, 360, 370, 1400, 11, 1825, 281, 14483, 13, 400, 550, 321, 767, 51796], "temperature": 0.0, "avg_logprob": -0.09832911795758187, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0007913438021205366}, {"id": 52, "seek": 39292, "start": 392.92, "end": 400.44, "text": " come to a statement, right? And what do we want? We want to print greeting. So that adds something", "tokens": [50364, 808, 281, 257, 5629, 11, 558, 30, 400, 437, 360, 321, 528, 30, 492, 528, 281, 4482, 28174, 13, 407, 300, 10860, 746, 50740], "temperature": 0.0, "avg_logprob": -0.05410839812924163, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.002671239897608757}, {"id": 53, "seek": 39292, "start": 400.44, "end": 406.52000000000004, "text": " to the call stack, right? So now we are going to this function, print greeting. What do we do", "tokens": [50740, 281, 264, 818, 8630, 11, 558, 30, 407, 586, 321, 366, 516, 281, 341, 2445, 11, 4482, 28174, 13, 708, 360, 321, 360, 51044], "temperature": 0.0, "avg_logprob": -0.05410839812924163, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.002671239897608757}, {"id": 54, "seek": 39292, "start": 406.52000000000004, "end": 414.36, "text": " inside of print greeting? We call the greet function, right? And we call it with some value,", "tokens": [51044, 1854, 295, 4482, 28174, 30, 492, 818, 264, 12044, 2445, 11, 558, 30, 400, 321, 818, 309, 365, 512, 2158, 11, 51436], "temperature": 0.0, "avg_logprob": -0.05410839812924163, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.002671239897608757}, {"id": 55, "seek": 39292, "start": 414.36, "end": 420.20000000000005, "text": " but that's not important. So when we call the greet function, one more thing gets added to the call", "tokens": [51436, 457, 300, 311, 406, 1021, 13, 407, 562, 321, 818, 264, 12044, 2445, 11, 472, 544, 551, 2170, 3869, 281, 264, 818, 51728], "temperature": 0.0, "avg_logprob": -0.05410839812924163, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.002671239897608757}, {"id": 56, "seek": 42020, "start": 420.2, "end": 425.71999999999997, "text": " stack. And what are we doing in the greet function? We are calling the join words function,", "tokens": [50364, 8630, 13, 400, 437, 366, 321, 884, 294, 264, 12044, 2445, 30, 492, 366, 5141, 264, 3917, 2283, 2445, 11, 50640], "temperature": 0.0, "avg_logprob": -0.11978342135747273, "compression_ratio": 1.9, "no_speech_prob": 0.0012835641391575336}, {"id": 57, "seek": 42020, "start": 426.52, "end": 431.64, "text": " right? One more thing gets added to the call stack. And now you hit return, right? So now we", "tokens": [50680, 558, 30, 1485, 544, 551, 2170, 3869, 281, 264, 818, 8630, 13, 400, 586, 291, 2045, 2736, 11, 558, 30, 407, 586, 321, 50936], "temperature": 0.0, "avg_logprob": -0.11978342135747273, "compression_ratio": 1.9, "no_speech_prob": 0.0012835641391575336}, {"id": 58, "seek": 42020, "start": 431.64, "end": 440.03999999999996, "text": " actually have to return something. So something gets popped from the stack, right? So now you are", "tokens": [50936, 767, 362, 281, 2736, 746, 13, 407, 746, 2170, 21545, 490, 264, 8630, 11, 558, 30, 407, 586, 291, 366, 51356], "temperature": 0.0, "avg_logprob": -0.11978342135747273, "compression_ratio": 1.9, "no_speech_prob": 0.0012835641391575336}, {"id": 59, "seek": 42020, "start": 440.03999999999996, "end": 448.84, "text": " out of the join function. You now, you go to the return statement of greet, you are out of greet.", "tokens": [51356, 484, 295, 264, 3917, 2445, 13, 509, 586, 11, 291, 352, 281, 264, 2736, 5629, 295, 12044, 11, 291, 366, 484, 295, 12044, 13, 51796], "temperature": 0.0, "avg_logprob": -0.11978342135747273, "compression_ratio": 1.9, "no_speech_prob": 0.0012835641391575336}, {"id": 60, "seek": 45020, "start": 450.2, "end": 455.71999999999997, "text": " You go into the next statement of your print greeting. You do your console log.", "tokens": [50364, 509, 352, 666, 264, 958, 5629, 295, 428, 4482, 28174, 13, 509, 360, 428, 11076, 3565, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12203364786894424, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0023907532449811697}, {"id": 61, "seek": 45020, "start": 457.47999999999996, "end": 462.03999999999996, "text": " And there's no return statement here, but it's end of the function. So you're going out of this", "tokens": [50728, 400, 456, 311, 572, 2736, 5629, 510, 11, 457, 309, 311, 917, 295, 264, 2445, 13, 407, 291, 434, 516, 484, 295, 341, 50956], "temperature": 0.0, "avg_logprob": -0.12203364786894424, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0023907532449811697}, {"id": 62, "seek": 45020, "start": 462.03999999999996, "end": 469.96, "text": " one as well, right? And you're back to your main thread. And that's it. That's how asynchronous,", "tokens": [50956, 472, 382, 731, 11, 558, 30, 400, 291, 434, 646, 281, 428, 2135, 7207, 13, 400, 300, 311, 309, 13, 663, 311, 577, 49174, 11, 51352], "temperature": 0.0, "avg_logprob": -0.12203364786894424, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0023907532449811697}, {"id": 63, "seek": 45020, "start": 471.32, "end": 479.56, "text": " not asynchronous. That's how asynchronous JavaScript code runs, right? So far so good.", "tokens": [51420, 406, 49174, 13, 663, 311, 577, 49174, 15778, 3089, 6676, 11, 558, 30, 407, 1400, 370, 665, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12203364786894424, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.0023907532449811697}, {"id": 64, "seek": 48020, "start": 480.2, "end": 489.24, "text": " Everyone with me? Great. Another example, like I was saying, if you've ever seen an error stack,", "tokens": [50364, 5198, 365, 385, 30, 3769, 13, 3996, 1365, 11, 411, 286, 390, 1566, 11, 498, 291, 600, 1562, 1612, 364, 6713, 8630, 11, 50816], "temperature": 0.0, "avg_logprob": -0.0732716687520345, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.00045078102266415954}, {"id": 65, "seek": 48020, "start": 490.68, "end": 497.0, "text": " essentially what you see is the call stack, right? You see a snapshot of the stack when the error", "tokens": [50888, 4476, 437, 291, 536, 307, 264, 818, 8630, 11, 558, 30, 509, 536, 257, 30163, 295, 264, 8630, 562, 264, 6713, 51204], "temperature": 0.0, "avg_logprob": -0.0732716687520345, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.00045078102266415954}, {"id": 66, "seek": 48020, "start": 497.0, "end": 503.71999999999997, "text": " happened, right? Or if you ever use the debug tool, for example, you are also seeing the call stack", "tokens": [51204, 2011, 11, 558, 30, 1610, 498, 291, 1562, 764, 264, 24083, 2290, 11, 337, 1365, 11, 291, 366, 611, 2577, 264, 818, 8630, 51540], "temperature": 0.0, "avg_logprob": -0.0732716687520345, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.00045078102266415954}, {"id": 67, "seek": 50372, "start": 503.72, "end": 513.1600000000001, "text": " over there. Now let's look at something slightly different. What happens in this case, right?", "tokens": [50364, 670, 456, 13, 823, 718, 311, 574, 412, 746, 4748, 819, 13, 708, 2314, 294, 341, 1389, 11, 558, 30, 50836], "temperature": 0.0, "avg_logprob": -0.11818172454833985, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.005375399719923735}, {"id": 68, "seek": 50372, "start": 515.08, "end": 519.8000000000001, "text": " We all know what would happen without referring to the stack, right? It'll give hello,", "tokens": [50932, 492, 439, 458, 437, 576, 1051, 1553, 13761, 281, 264, 8630, 11, 558, 30, 467, 603, 976, 7751, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11818172454833985, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.005375399719923735}, {"id": 69, "seek": 50372, "start": 519.8000000000001, "end": 526.44, "text": " it'll give there, and then it'll give forcedM, right? But now there's sort of two things happening here.", "tokens": [51168, 309, 603, 976, 456, 11, 293, 550, 309, 603, 976, 7579, 44, 11, 558, 30, 583, 586, 456, 311, 1333, 295, 732, 721, 2737, 510, 13, 51500], "temperature": 0.0, "avg_logprob": -0.11818172454833985, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.005375399719923735}, {"id": 70, "seek": 50372, "start": 527.72, "end": 532.9200000000001, "text": " There's a mistake there. I forgot to add the time. It'll be there in the next slide. Just ignore that.", "tokens": [51564, 821, 311, 257, 6146, 456, 13, 286, 5298, 281, 909, 264, 565, 13, 467, 603, 312, 456, 294, 264, 958, 4137, 13, 1449, 11200, 300, 13, 51824], "temperature": 0.0, "avg_logprob": -0.11818172454833985, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.005375399719923735}, {"id": 71, "seek": 53372, "start": 534.12, "end": 541.4, "text": " But yeah. So what happens, right? Because our call stack now cannot do two things at a time,", "tokens": [50384, 583, 1338, 13, 407, 437, 2314, 11, 558, 30, 1436, 527, 818, 8630, 586, 2644, 360, 732, 721, 412, 257, 565, 11, 50748], "temperature": 0.0, "avg_logprob": -0.12345727752236758, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.0007904650992713869}, {"id": 72, "seek": 53372, "start": 541.4, "end": 548.0400000000001, "text": " right? And we know from experience that setTimeout is going to sort of run", "tokens": [50748, 558, 30, 400, 321, 458, 490, 1752, 300, 992, 22233, 346, 307, 516, 281, 1333, 295, 1190, 51080], "temperature": 0.0, "avg_logprob": -0.12345727752236758, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.0007904650992713869}, {"id": 73, "seek": 53372, "start": 548.0400000000001, "end": 557.08, "text": " parallelly while the stack moves on to the next thing, right? So that's where the other things", "tokens": [51080, 8069, 7442, 1339, 264, 8630, 6067, 322, 281, 264, 958, 551, 11, 558, 30, 407, 300, 311, 689, 264, 661, 721, 51532], "temperature": 0.0, "avg_logprob": -0.12345727752236758, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.0007904650992713869}, {"id": 74, "seek": 55708, "start": 557.08, "end": 564.36, "text": " that we had in that previous picture come into play, right? So you have mainly three things here.", "tokens": [50364, 300, 321, 632, 294, 300, 3894, 3036, 808, 666, 862, 11, 558, 30, 407, 291, 362, 8704, 1045, 721, 510, 13, 50728], "temperature": 0.0, "avg_logprob": -0.11074075612935934, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.024027148261666298}, {"id": 75, "seek": 55708, "start": 564.36, "end": 572.36, "text": " You have your web APIs. In the browser, you'll have web APIs. If you're doing Node.js, you'll have", "tokens": [50728, 509, 362, 428, 3670, 21445, 13, 682, 264, 11185, 11, 291, 603, 362, 3670, 21445, 13, 759, 291, 434, 884, 38640, 13, 25530, 11, 291, 603, 362, 51128], "temperature": 0.0, "avg_logprob": -0.11074075612935934, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.024027148261666298}, {"id": 76, "seek": 55708, "start": 572.36, "end": 578.76, "text": " what's called libUV. And towards the end, I'll also talk a bit about libUV. But for now, let's", "tokens": [51128, 437, 311, 1219, 22854, 52, 53, 13, 400, 3030, 264, 917, 11, 286, 603, 611, 751, 257, 857, 466, 22854, 52, 53, 13, 583, 337, 586, 11, 718, 311, 51448], "temperature": 0.0, "avg_logprob": -0.11074075612935934, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.024027148261666298}, {"id": 77, "seek": 55708, "start": 578.76, "end": 584.76, "text": " assume we are in the browser, right? So we are on the client side. You have these web APIs here.", "tokens": [51448, 6552, 321, 366, 294, 264, 11185, 11, 558, 30, 407, 321, 366, 322, 264, 6423, 1252, 13, 509, 362, 613, 3670, 21445, 510, 13, 51748], "temperature": 0.0, "avg_logprob": -0.11074075612935934, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.024027148261666298}, {"id": 78, "seek": 58476, "start": 585.08, "end": 593.72, "text": " You have your task queue. And you have the star of the stock, the rotating thing, what's called the event loop,", "tokens": [50380, 509, 362, 428, 5633, 18639, 13, 400, 291, 362, 264, 3543, 295, 264, 4127, 11, 264, 19627, 551, 11, 437, 311, 1219, 264, 2280, 6367, 11, 50812], "temperature": 0.0, "avg_logprob": -0.2267915826094778, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.001323993899859488}, {"id": 79, "seek": 58476, "start": 593.72, "end": 604.04, "text": " right? And you will go through in more detail. But just to summarize, all the stuff that is sort", "tokens": [50812, 558, 30, 400, 291, 486, 352, 807, 294, 544, 2607, 13, 583, 445, 281, 20858, 11, 439, 264, 1507, 300, 307, 1333, 51328], "temperature": 0.0, "avg_logprob": -0.2267915826094778, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.001323993899859488}, {"id": 80, "seek": 58476, "start": 604.04, "end": 610.68, "text": " of slower, right? That's not happening immediately. That's not being invoked immediately or running", "tokens": [51328, 295, 14009, 11, 558, 30, 663, 311, 406, 2737, 4258, 13, 663, 311, 406, 885, 1048, 9511, 4258, 420, 2614, 51660], "temperature": 0.0, "avg_logprob": -0.2267915826094778, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.001323993899859488}, {"id": 81, "seek": 61068, "start": 611.3199999999999, "end": 618.4399999999999, "text": " synchronously. That gets delegated to the web APIs, right? So here you have your DOM manipulation.", "tokens": [50396, 19331, 5098, 13, 663, 2170, 15824, 770, 281, 264, 3670, 21445, 11, 558, 30, 407, 510, 291, 362, 428, 35727, 26475, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1045074462890625, "compression_ratio": 1.445, "no_speech_prob": 0.003024537581950426}, {"id": 82, "seek": 61068, "start": 619.0799999999999, "end": 627.16, "text": " You can make XHR calls. You can do setTimeout. If you were in Node.js, you could make a call", "tokens": [50784, 509, 393, 652, 1783, 39, 49, 5498, 13, 509, 393, 360, 992, 22233, 346, 13, 759, 291, 645, 294, 38640, 13, 25530, 11, 291, 727, 652, 257, 818, 51188], "temperature": 0.0, "avg_logprob": -0.1045074462890625, "compression_ratio": 1.445, "no_speech_prob": 0.003024537581950426}, {"id": 83, "seek": 61068, "start": 627.16, "end": 636.5999999999999, "text": " to the database. Anything that's slow runs here, right? But the whole point of doing this is that", "tokens": [51188, 281, 264, 8149, 13, 11998, 300, 311, 2964, 6676, 510, 11, 558, 30, 583, 264, 1379, 935, 295, 884, 341, 307, 300, 51660], "temperature": 0.0, "avg_logprob": -0.1045074462890625, "compression_ratio": 1.445, "no_speech_prob": 0.003024537581950426}, {"id": 84, "seek": 63660, "start": 636.6, "end": 641.88, "text": " after you run something, you want to do something back into your main thread, right?", "tokens": [50364, 934, 291, 1190, 746, 11, 291, 528, 281, 360, 746, 646, 666, 428, 2135, 7207, 11, 558, 30, 50628], "temperature": 0.0, "avg_logprob": -0.1477731243594662, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.008173932321369648}, {"id": 85, "seek": 63660, "start": 641.88, "end": 647.8000000000001, "text": " You are finally, you're running JavaScript. And you are making a call to some external system", "tokens": [50628, 509, 366, 2721, 11, 291, 434, 2614, 15778, 13, 400, 291, 366, 1455, 257, 818, 281, 512, 8320, 1185, 50924], "temperature": 0.0, "avg_logprob": -0.1477731243594662, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.008173932321369648}, {"id": 86, "seek": 63660, "start": 647.8000000000001, "end": 653.88, "text": " web or doing some delay. But finally, you then what, right? You need to do something. And that", "tokens": [50924, 3670, 420, 884, 512, 8577, 13, 583, 2721, 11, 291, 550, 437, 11, 558, 30, 509, 643, 281, 360, 746, 13, 400, 300, 51228], "temperature": 0.0, "avg_logprob": -0.1477731243594662, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.008173932321369648}, {"id": 87, "seek": 63660, "start": 653.88, "end": 661.72, "text": " something is handled by the task queue, right? So take a simple example of a setTimeout.", "tokens": [51228, 746, 307, 18033, 538, 264, 5633, 18639, 11, 558, 30, 407, 747, 257, 2199, 1365, 295, 257, 992, 22233, 346, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1477731243594662, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.008173932321369648}, {"id": 88, "seek": 66172, "start": 662.44, "end": 671.88, "text": " You have a callback and some delay, right? So the actual waiting for the time, say you put a delay", "tokens": [50400, 509, 362, 257, 818, 3207, 293, 512, 8577, 11, 558, 30, 407, 264, 3539, 3806, 337, 264, 565, 11, 584, 291, 829, 257, 8577, 50872], "temperature": 0.0, "avg_logprob": -0.1105631496129411, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.014720839448273182}, {"id": 89, "seek": 66172, "start": 671.88, "end": 677.32, "text": " of 300 milliseconds. So the actual waiting for 300 milliseconds is done by the web API.", "tokens": [50872, 295, 6641, 34184, 13, 407, 264, 3539, 3806, 337, 6641, 34184, 307, 1096, 538, 264, 3670, 9362, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1105631496129411, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.014720839448273182}, {"id": 90, "seek": 66172, "start": 678.28, "end": 686.6800000000001, "text": " Then your callback goes to the task queue. And then it has to somehow go back to your main", "tokens": [51192, 1396, 428, 818, 3207, 1709, 281, 264, 5633, 18639, 13, 400, 550, 309, 575, 281, 6063, 352, 646, 281, 428, 2135, 51612], "temperature": 0.0, "avg_logprob": -0.1105631496129411, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.014720839448273182}, {"id": 91, "seek": 66172, "start": 686.6800000000001, "end": 690.6800000000001, "text": " call stack and get executed, right? Because your callback is still JavaScript.", "tokens": [51612, 818, 8630, 293, 483, 17577, 11, 558, 30, 1436, 428, 818, 3207, 307, 920, 15778, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1105631496129411, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.014720839448273182}, {"id": 92, "seek": 69068, "start": 690.68, "end": 695.3199999999999, "text": " I mean, we are assuming it's a simple callback here. Obviously, that itself could have another", "tokens": [50364, 286, 914, 11, 321, 366, 11926, 309, 311, 257, 2199, 818, 3207, 510, 13, 7580, 11, 300, 2564, 727, 362, 1071, 50596], "temperature": 0.0, "avg_logprob": -0.1491587475092724, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.00035670388024300337}, {"id": 93, "seek": 69068, "start": 695.3199999999999, "end": 699.0, "text": " callback and then the process repeats, right? But let's assume for now we are just doing like a", "tokens": [50596, 818, 3207, 293, 550, 264, 1399, 35038, 11, 558, 30, 583, 718, 311, 6552, 337, 586, 321, 366, 445, 884, 411, 257, 50780], "temperature": 0.0, "avg_logprob": -0.1491587475092724, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.00035670388024300337}, {"id": 94, "seek": 69068, "start": 699.0, "end": 703.16, "text": " console or you know, callback, right? So that's pure JavaScript and it has to go back to the stack.", "tokens": [50780, 11076, 420, 291, 458, 11, 818, 3207, 11, 558, 30, 407, 300, 311, 6075, 15778, 293, 309, 575, 281, 352, 646, 281, 264, 8630, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1491587475092724, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.00035670388024300337}, {"id": 95, "seek": 69068, "start": 704.4399999999999, "end": 715.16, "text": " And the event loop is what decides this part, right? So the event loop checks if the stack is", "tokens": [51052, 400, 264, 2280, 6367, 307, 437, 14898, 341, 644, 11, 558, 30, 407, 264, 2280, 6367, 13834, 498, 264, 8630, 307, 51588], "temperature": 0.0, "avg_logprob": -0.1491587475092724, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.00035670388024300337}, {"id": 96, "seek": 71516, "start": 715.16, "end": 723.56, "text": " empty, if the call stack is empty, as in if it's idle, then check the task queue. If there's", "tokens": [50364, 6707, 11, 498, 264, 818, 8630, 307, 6707, 11, 382, 294, 498, 309, 311, 30650, 11, 550, 1520, 264, 5633, 18639, 13, 759, 456, 311, 50784], "temperature": 0.0, "avg_logprob": -0.12051872357930223, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0043289558961987495}, {"id": 97, "seek": 71516, "start": 723.56, "end": 730.76, "text": " something in the task queue, move it to the call stack, right? This is in a nutshell what's happening.", "tokens": [50784, 746, 294, 264, 5633, 18639, 11, 1286, 309, 281, 264, 818, 8630, 11, 558, 30, 639, 307, 294, 257, 37711, 437, 311, 2737, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12051872357930223, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0043289558961987495}, {"id": 98, "seek": 71516, "start": 731.88, "end": 737.24, "text": " This is in a nutshell how JavaScript manages to have asynchronous features, right? While", "tokens": [51200, 639, 307, 294, 257, 37711, 577, 15778, 22489, 281, 362, 49174, 4122, 11, 558, 30, 3987, 51468], "temperature": 0.0, "avg_logprob": -0.12051872357930223, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0043289558961987495}, {"id": 99, "seek": 73724, "start": 737.24, "end": 739.64, "text": " still itself being single threaded and synchronous.", "tokens": [50364, 920, 2564, 885, 2167, 47493, 293, 44743, 13, 50484], "temperature": 0.0, "avg_logprob": -0.2211526393890381, "compression_ratio": 1.2543859649122806, "no_speech_prob": 0.0028859684243798256}, {"id": 100, "seek": 73724, "start": 742.76, "end": 752.6800000000001, "text": " So let's look at our function. And now we have, let's run this, right? So we have our main.", "tokens": [50640, 407, 718, 311, 574, 412, 527, 2445, 13, 400, 586, 321, 362, 11, 718, 311, 1190, 341, 11, 558, 30, 407, 321, 362, 527, 2135, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2211526393890381, "compression_ratio": 1.2543859649122806, "no_speech_prob": 0.0028859684243798256}, {"id": 101, "seek": 75268, "start": 752.68, "end": 772.76, "text": " We go to console log and we log hello. Okay. I don't know. We'll just, we'll not do full screen.", "tokens": [50364, 492, 352, 281, 11076, 3565, 293, 321, 3565, 7751, 13, 1033, 13, 286, 500, 380, 458, 13, 492, 603, 445, 11, 321, 603, 406, 360, 1577, 2568, 13, 51368], "temperature": 0.0, "avg_logprob": -0.31757423281669617, "compression_ratio": 1.1162790697674418, "no_speech_prob": 0.00842477660626173}, {"id": 102, "seek": 77276, "start": 773.64, "end": 784.36, "text": " Ah, I know what's not working. So that there's, there's a gif here of, of like a timer. So just,", "tokens": [50408, 2438, 11, 286, 458, 437, 311, 406, 1364, 13, 407, 300, 456, 311, 11, 456, 311, 257, 290, 351, 510, 295, 11, 295, 411, 257, 19247, 13, 407, 445, 11, 50944], "temperature": 0.0, "avg_logprob": -0.14772359000311958, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.014236402697861195}, {"id": 103, "seek": 77276, "start": 784.36, "end": 792.84, "text": " just assume that for now, right? But, but yeah, it's not important. We can still manage without", "tokens": [50944, 445, 6552, 300, 337, 586, 11, 558, 30, 583, 11, 457, 1338, 11, 309, 311, 406, 1021, 13, 492, 393, 920, 3067, 1553, 51368], "temperature": 0.0, "avg_logprob": -0.14772359000311958, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.014236402697861195}, {"id": 104, "seek": 77276, "start": 792.84, "end": 800.68, "text": " it, right? So let's, let's go back. Let's start from the top, from the bottom, right? So you have", "tokens": [51368, 309, 11, 558, 30, 407, 718, 311, 11, 718, 311, 352, 646, 13, 961, 311, 722, 490, 264, 1192, 11, 490, 264, 2767, 11, 558, 30, 407, 291, 362, 51760], "temperature": 0.0, "avg_logprob": -0.14772359000311958, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.014236402697861195}, {"id": 105, "seek": 80068, "start": 800.68, "end": 810.12, "text": " your main, you have your main function, then you call your console log that gets executed. So it gets", "tokens": [50364, 428, 2135, 11, 291, 362, 428, 2135, 2445, 11, 550, 291, 818, 428, 11076, 3565, 300, 2170, 17577, 13, 407, 309, 2170, 50836], "temperature": 0.0, "avg_logprob": -0.10399198532104492, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.004260942339897156}, {"id": 106, "seek": 80068, "start": 810.12, "end": 815.3199999999999, "text": " popped off the stack. You guys know this, but I'm doing this practice because I'm also going to", "tokens": [50836, 21545, 766, 264, 8630, 13, 509, 1074, 458, 341, 11, 457, 286, 478, 884, 341, 3124, 570, 286, 478, 611, 516, 281, 51096], "temperature": 0.0, "avg_logprob": -0.10399198532104492, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.004260942339897156}, {"id": 107, "seek": 80068, "start": 815.3199999999999, "end": 820.76, "text": " ask you questions about this, right? So it's a nice thing to visualize for some examples, right?", "tokens": [51096, 1029, 291, 1651, 466, 341, 11, 558, 30, 407, 309, 311, 257, 1481, 551, 281, 23273, 337, 512, 5110, 11, 558, 30, 51368], "temperature": 0.0, "avg_logprob": -0.10399198532104492, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.004260942339897156}, {"id": 108, "seek": 80068, "start": 821.9599999999999, "end": 828.4399999999999, "text": " You have your set timeout, right? Set timeout is not on the stack. It gets delegated to the", "tokens": [51428, 509, 362, 428, 992, 565, 346, 11, 558, 30, 8928, 565, 346, 307, 406, 322, 264, 8630, 13, 467, 2170, 15824, 770, 281, 264, 51752], "temperature": 0.0, "avg_logprob": -0.10399198532104492, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.004260942339897156}, {"id": 109, "seek": 82844, "start": 828.44, "end": 834.36, "text": " web API, right? Which starts a timer. And then it knows that it has to run a callback when the", "tokens": [50364, 3670, 9362, 11, 558, 30, 3013, 3719, 257, 19247, 13, 400, 550, 309, 3255, 300, 309, 575, 281, 1190, 257, 818, 3207, 562, 264, 50660], "temperature": 0.0, "avg_logprob": -0.11225027356828962, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0029332113917917013}, {"id": 110, "seek": 82844, "start": 834.36, "end": 841.32, "text": " timer is finished, right? While this is running, we move on to the next thing that we can do, right?", "tokens": [50660, 19247, 307, 4335, 11, 558, 30, 3987, 341, 307, 2614, 11, 321, 1286, 322, 281, 264, 958, 551, 300, 321, 393, 360, 11, 558, 30, 51008], "temperature": 0.0, "avg_logprob": -0.11225027356828962, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0029332113917917013}, {"id": 111, "seek": 82844, "start": 841.32, "end": 848.7600000000001, "text": " In the stack. And then we have another console log that gets executed. It goes away as in it's", "tokens": [51008, 682, 264, 8630, 13, 400, 550, 321, 362, 1071, 11076, 3565, 300, 2170, 17577, 13, 467, 1709, 1314, 382, 294, 309, 311, 51380], "temperature": 0.0, "avg_logprob": -0.11225027356828962, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0029332113917917013}, {"id": 112, "seek": 82844, "start": 848.7600000000001, "end": 856.36, "text": " popped. And then the main thread is free, right? And then after some time, so, so you see that", "tokens": [51380, 21545, 13, 400, 550, 264, 2135, 7207, 307, 1737, 11, 558, 30, 400, 550, 934, 512, 565, 11, 370, 11, 370, 291, 536, 300, 51760], "temperature": 0.0, "avg_logprob": -0.11225027356828962, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0029332113917917013}, {"id": 113, "seek": 85636, "start": 856.36, "end": 861.5600000000001, "text": " tick? There was supposed to be a loading thing there, right? To show that it's still counting.", "tokens": [50364, 5204, 30, 821, 390, 3442, 281, 312, 257, 15114, 551, 456, 11, 558, 30, 1407, 855, 300, 309, 311, 920, 13251, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1564648151397705, "compression_ratio": 1.606896551724138, "no_speech_prob": 0.002470030914992094}, {"id": 114, "seek": 85636, "start": 862.92, "end": 867.96, "text": " After some time, your two seconds are done. And then,", "tokens": [50692, 2381, 512, 565, 11, 428, 732, 3949, 366, 1096, 13, 400, 550, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1564648151397705, "compression_ratio": 1.606896551724138, "no_speech_prob": 0.002470030914992094}, {"id": 115, "seek": 85636, "start": 875.0, "end": 879.72, "text": " so after some time, your two seconds are done. Your timer is over. That's also done.", "tokens": [51296, 370, 934, 512, 565, 11, 428, 732, 3949, 366, 1096, 13, 2260, 19247, 307, 670, 13, 663, 311, 611, 1096, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1564648151397705, "compression_ratio": 1.606896551724138, "no_speech_prob": 0.002470030914992094}, {"id": 116, "seek": 87972, "start": 879.8000000000001, "end": 888.9200000000001, "text": " You move the callback to your task queue, right? And then the event loop checks if the call stack", "tokens": [50368, 509, 1286, 264, 818, 3207, 281, 428, 5633, 18639, 11, 558, 30, 400, 550, 264, 2280, 6367, 13834, 498, 264, 818, 8630, 50824], "temperature": 0.0, "avg_logprob": -0.08689589249460321, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0043970737606287}, {"id": 117, "seek": 87972, "start": 888.9200000000001, "end": 897.4, "text": " is free, which in this case it is free, right? It moves the callback function over to the stack", "tokens": [50824, 307, 1737, 11, 597, 294, 341, 1389, 309, 307, 1737, 11, 558, 30, 467, 6067, 264, 818, 3207, 2445, 670, 281, 264, 8630, 51248], "temperature": 0.0, "avg_logprob": -0.08689589249460321, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0043970737606287}, {"id": 118, "seek": 87972, "start": 898.44, "end": 905.5600000000001, "text": " and then it runs it, right? So there's again a console log that gets printed and you're done,", "tokens": [51300, 293, 550, 309, 6676, 309, 11, 558, 30, 407, 456, 311, 797, 257, 11076, 3565, 300, 2170, 13567, 293, 291, 434, 1096, 11, 51656], "temperature": 0.0, "avg_logprob": -0.08689589249460321, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.0043970737606287}, {"id": 119, "seek": 90556, "start": 906.28, "end": 915.88, "text": " right? So to summarize the JavaScript engine itself is synchronous and single-threaded.", "tokens": [50400, 558, 30, 407, 281, 20858, 264, 15778, 2848, 2564, 307, 44743, 293, 2167, 12, 392, 2538, 292, 13, 50880], "temperature": 0.0, "avg_logprob": -0.15268659591674805, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0017517592059448361}, {"id": 120, "seek": 90556, "start": 917.88, "end": 925.8, "text": " The Node.js runtime is asynchronous because it manages the asynchronous things outside of", "tokens": [50980, 440, 38640, 13, 25530, 34474, 307, 49174, 570, 309, 22489, 264, 49174, 721, 2380, 295, 51376], "temperature": 0.0, "avg_logprob": -0.15268659591674805, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0017517592059448361}, {"id": 121, "seek": 90556, "start": 925.8, "end": 932.1199999999999, "text": " the JavaScript engine, right? In a separate thing, right? Which is usually written in C++,", "tokens": [51376, 264, 15778, 2848, 11, 558, 30, 682, 257, 4994, 551, 11, 558, 30, 3013, 307, 2673, 3720, 294, 383, 25472, 11, 51692], "temperature": 0.0, "avg_logprob": -0.15268659591674805, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0017517592059448361}, {"id": 122, "seek": 93212, "start": 932.12, "end": 942.12, "text": " either by VPIs or LibUV. And the event loop is the glue that ties all of this together, right? So", "tokens": [50364, 2139, 538, 35812, 6802, 420, 15834, 52, 53, 13, 400, 264, 2280, 6367, 307, 264, 8998, 300, 14039, 439, 295, 341, 1214, 11, 558, 30, 407, 50864], "temperature": 0.0, "avg_logprob": -0.11724097019917257, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00406038761138916}, {"id": 123, "seek": 93212, "start": 942.12, "end": 948.36, "text": " wherever your thing is running, the callback goes to the task queue and then the event loop", "tokens": [50864, 8660, 428, 551, 307, 2614, 11, 264, 818, 3207, 1709, 281, 264, 5633, 18639, 293, 550, 264, 2280, 6367, 51176], "temperature": 0.0, "avg_logprob": -0.11724097019917257, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00406038761138916}, {"id": 124, "seek": 93212, "start": 948.36, "end": 959.08, "text": " decides when to run these callbacks. All right. Before I go there, does anyone have a question", "tokens": [51176, 14898, 562, 281, 1190, 613, 818, 17758, 13, 1057, 558, 13, 4546, 286, 352, 456, 11, 775, 2878, 362, 257, 1168, 51712], "temperature": 0.0, "avg_logprob": -0.11724097019917257, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00406038761138916}, {"id": 125, "seek": 95908, "start": 960.0400000000001, "end": 961.5600000000001, "text": " so far? Yeah?", "tokens": [50412, 370, 1400, 30, 865, 30, 50488], "temperature": 0.0, "avg_logprob": -0.28807476043701175, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.012130466289818287}, {"id": 126, "seek": 95908, "start": 963.88, "end": 970.6, "text": " During that event loop, are they green threads that are created? How does that work from", "tokens": [50604, 6842, 300, 2280, 6367, 11, 366, 436, 3092, 19314, 300, 366, 2942, 30, 1012, 775, 300, 589, 490, 50940], "temperature": 0.0, "avg_logprob": -0.28807476043701175, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.012130466289818287}, {"id": 127, "seek": 95908, "start": 971.88, "end": 977.48, "text": " lower level? Sorry, can you repeat it? Are they green threads that are created then?", "tokens": [51004, 3126, 1496, 30, 4919, 11, 393, 291, 7149, 309, 30, 2014, 436, 3092, 19314, 300, 366, 2942, 550, 30, 51284], "temperature": 0.0, "avg_logprob": -0.28807476043701175, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.012130466289818287}, {"id": 128, "seek": 97748, "start": 978.2, "end": 986.04, "text": " How in terms of lower level, does it become multi-threaded at that moment?", "tokens": [50400, 1012, 294, 2115, 295, 3126, 1496, 11, 775, 309, 1813, 4825, 12, 392, 2538, 292, 412, 300, 1623, 30, 50792], "temperature": 0.0, "avg_logprob": -0.2616837056477865, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.010406234301626682}, {"id": 129, "seek": 97748, "start": 988.6, "end": 996.52, "text": " Yeah? You want me to give them the mic? Okay, yeah. So the question is, in the event loop,", "tokens": [50920, 865, 30, 509, 528, 385, 281, 976, 552, 264, 3123, 30, 1033, 11, 1338, 13, 407, 264, 1168, 307, 11, 294, 264, 2280, 6367, 11, 51316], "temperature": 0.0, "avg_logprob": -0.2616837056477865, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.010406234301626682}, {"id": 130, "seek": 97748, "start": 998.04, "end": 1004.84, "text": " are there green threads that are created? The event loop does just one thing, right? It's a loop.", "tokens": [51392, 366, 456, 3092, 19314, 300, 366, 2942, 30, 440, 2280, 6367, 775, 445, 472, 551, 11, 558, 30, 467, 311, 257, 6367, 13, 51732], "temperature": 0.0, "avg_logprob": -0.2616837056477865, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.010406234301626682}, {"id": 131, "seek": 100484, "start": 1004.84, "end": 1015.0, "text": " It's basically a while too, right? And we will try and make a more complex model of it. But for", "tokens": [50364, 467, 311, 1936, 257, 1339, 886, 11, 558, 30, 400, 321, 486, 853, 293, 652, 257, 544, 3997, 2316, 295, 309, 13, 583, 337, 50872], "temperature": 0.0, "avg_logprob": -0.08157747359502883, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0076874359510838985}, {"id": 132, "seek": 100484, "start": 1015.0, "end": 1021.72, "text": " now, for our understanding, it's doing just one thing. Go to the queue. If there's something in", "tokens": [50872, 586, 11, 337, 527, 3701, 11, 309, 311, 884, 445, 472, 551, 13, 1037, 281, 264, 18639, 13, 759, 456, 311, 746, 294, 51208], "temperature": 0.0, "avg_logprob": -0.08157747359502883, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0076874359510838985}, {"id": 133, "seek": 100484, "start": 1021.72, "end": 1028.3600000000001, "text": " the queue and nothing in the stack, pull the first thing from the queue and put it in the stack,", "tokens": [51208, 264, 18639, 293, 1825, 294, 264, 8630, 11, 2235, 264, 700, 551, 490, 264, 18639, 293, 829, 309, 294, 264, 8630, 11, 51540], "temperature": 0.0, "avg_logprob": -0.08157747359502883, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0076874359510838985}, {"id": 134, "seek": 100484, "start": 1028.3600000000001, "end": 1032.3600000000001, "text": " right? That's all it's doing. So there's no multi-threading. There's no nothing. It's just", "tokens": [51540, 558, 30, 663, 311, 439, 309, 311, 884, 13, 407, 456, 311, 572, 4825, 12, 392, 35908, 13, 821, 311, 572, 1825, 13, 467, 311, 445, 51740], "temperature": 0.0, "avg_logprob": -0.08157747359502883, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0076874359510838985}, {"id": 135, "seek": 103236, "start": 1032.36, "end": 1041.9599999999998, "text": " a while loop. There is multi-threading in the other part over here, but in Node.js, right? Not in", "tokens": [50364, 257, 1339, 6367, 13, 821, 307, 4825, 12, 392, 35908, 294, 264, 661, 644, 670, 510, 11, 457, 294, 38640, 13, 25530, 11, 558, 30, 1726, 294, 50844], "temperature": 0.0, "avg_logprob": -0.1320656977201763, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.002799004316329956}, {"id": 136, "seek": 103236, "start": 1041.9599999999998, "end": 1056.6, "text": " the browser. All right. So, exercise time, right? This is a super simple function. Ignore all the", "tokens": [50844, 264, 11185, 13, 1057, 558, 13, 407, 11, 5380, 565, 11, 558, 30, 639, 307, 257, 1687, 2199, 2445, 13, 24754, 418, 439, 264, 51576], "temperature": 0.0, "avg_logprob": -0.1320656977201763, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.002799004316329956}, {"id": 137, "seek": 105660, "start": 1056.6799999999998, "end": 1065.08, "text": " boilerplate in the HTML. I was too lazy to remove it. All that that HTML is doing is it has a script.js", "tokens": [50368, 39228, 37008, 294, 264, 17995, 13, 286, 390, 886, 14847, 281, 4159, 309, 13, 1057, 300, 300, 17995, 307, 884, 307, 309, 575, 257, 5755, 13, 25530, 50788], "temperature": 0.0, "avg_logprob": -0.11440038681030273, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.02402576245367527}, {"id": 138, "seek": 105660, "start": 1065.08, "end": 1073.08, "text": " and the script.js is over here, right? What is the script.js doing? Pretty simple to, pretty", "tokens": [50788, 293, 264, 5755, 13, 25530, 307, 670, 510, 11, 558, 30, 708, 307, 264, 5755, 13, 25530, 884, 30, 10693, 2199, 281, 11, 1238, 51188], "temperature": 0.0, "avg_logprob": -0.11440038681030273, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.02402576245367527}, {"id": 139, "seek": 105660, "start": 1073.08, "end": 1078.04, "text": " similar to what you guys saw earlier. It has two console logs, a set timeout in between,", "tokens": [51188, 2531, 281, 437, 291, 1074, 1866, 3071, 13, 467, 575, 732, 11076, 20820, 11, 257, 992, 565, 346, 294, 1296, 11, 51436], "temperature": 0.0, "avg_logprob": -0.11440038681030273, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.02402576245367527}, {"id": 140, "seek": 105660, "start": 1078.84, "end": 1085.56, "text": " and another console log in the callback, right? So the first question is this, what's the output?", "tokens": [51476, 293, 1071, 11076, 3565, 294, 264, 818, 3207, 11, 558, 30, 407, 264, 700, 1168, 307, 341, 11, 437, 311, 264, 5598, 30, 51812], "temperature": 0.0, "avg_logprob": -0.11440038681030273, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.02402576245367527}, {"id": 141, "seek": 108660, "start": 1087.08, "end": 1096.12, "text": " Perfect, right? That's very simple. We just saw it. Second question, what will be the max", "tokens": [50388, 10246, 11, 558, 30, 663, 311, 588, 2199, 13, 492, 445, 1866, 309, 13, 5736, 1168, 11, 437, 486, 312, 264, 11469, 50840], "temperature": 0.0, "avg_logprob": -0.2114096093685069, "compression_ratio": 1.2595419847328244, "no_speech_prob": 0.0013029322726652026}, {"id": 142, "seek": 108660, "start": 1096.12, "end": 1105.1599999999999, "text": " number of tasks in the task queue? Right? This is a little bit tricky. One.", "tokens": [50840, 1230, 295, 9608, 294, 264, 5633, 18639, 30, 1779, 30, 639, 307, 257, 707, 857, 12414, 13, 1485, 13, 51292], "temperature": 0.0, "avg_logprob": -0.2114096093685069, "compression_ratio": 1.2595419847328244, "no_speech_prob": 0.0013029322726652026}, {"id": 143, "seek": 110516, "start": 1105.88, "end": 1111.0800000000002, "text": " Two. Who said two? Why?", "tokens": [50400, 4453, 13, 2102, 848, 732, 30, 1545, 30, 50660], "temperature": 0.0, "avg_logprob": -0.22105169296264648, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.013420063070952892}, {"id": 144, "seek": 110516, "start": 1118.44, "end": 1120.68, "text": " Yeah, but console log is just going to run on the stack.", "tokens": [51028, 865, 11, 457, 11076, 3565, 307, 445, 516, 281, 1190, 322, 264, 8630, 13, 51140], "temperature": 0.0, "avg_logprob": -0.22105169296264648, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.013420063070952892}, {"id": 145, "seek": 110516, "start": 1122.28, "end": 1128.8400000000001, "text": " So, see, your answer is right, but the reason is wrong, right? So there will in fact be two things,", "tokens": [51220, 407, 11, 536, 11, 428, 1867, 307, 558, 11, 457, 264, 1778, 307, 2085, 11, 558, 30, 407, 456, 486, 294, 1186, 312, 732, 721, 11, 51548], "temperature": 0.0, "avg_logprob": -0.22105169296264648, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.013420063070952892}, {"id": 146, "seek": 112884, "start": 1128.84, "end": 1139.0, "text": " right? Because reading the script tag itself is a task, right? In this case, the script tag", "tokens": [50364, 558, 30, 1436, 3760, 264, 5755, 6162, 2564, 307, 257, 5633, 11, 558, 30, 682, 341, 1389, 11, 264, 5755, 6162, 50872], "temperature": 0.0, "avg_logprob": -0.09791896217747738, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.013623343780636787}, {"id": 147, "seek": 112884, "start": 1139.0, "end": 1143.72, "text": " is a different file, so it actually has to, I don't know how browsers internally do it,", "tokens": [50872, 307, 257, 819, 3991, 11, 370, 309, 767, 575, 281, 11, 286, 500, 380, 458, 577, 36069, 19501, 360, 309, 11, 51108], "temperature": 0.0, "avg_logprob": -0.09791896217747738, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.013623343780636787}, {"id": 148, "seek": 112884, "start": 1143.72, "end": 1148.36, "text": " but actually has to go to the location and read the content. But even if the script tag is on", "tokens": [51108, 457, 767, 575, 281, 352, 281, 264, 4914, 293, 1401, 264, 2701, 13, 583, 754, 498, 264, 5755, 6162, 307, 322, 51340], "temperature": 0.0, "avg_logprob": -0.09791896217747738, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.013623343780636787}, {"id": 149, "seek": 112884, "start": 1148.36, "end": 1157.72, "text": " the HTML, it's still a different task, right? And this is important to understand if you have to", "tokens": [51340, 264, 17995, 11, 309, 311, 920, 257, 819, 5633, 11, 558, 30, 400, 341, 307, 1021, 281, 1223, 498, 291, 362, 281, 51808], "temperature": 0.0, "avg_logprob": -0.09791896217747738, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.013623343780636787}, {"id": 150, "seek": 115772, "start": 1157.72, "end": 1162.68, "text": " guess, even there's a race condition basically, right? And when we come to micro tasks,", "tokens": [50364, 2041, 11, 754, 456, 311, 257, 4569, 4188, 1936, 11, 558, 30, 400, 562, 321, 808, 281, 4532, 9608, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1086551382186565, "compression_ratio": 1.59375, "no_speech_prob": 0.004678809083998203}, {"id": 151, "seek": 115772, "start": 1162.68, "end": 1167.32, "text": " this will become a little bit more complex, and we have an example for that. But just", "tokens": [50612, 341, 486, 1813, 257, 707, 857, 544, 3997, 11, 293, 321, 362, 364, 1365, 337, 300, 13, 583, 445, 50844], "temperature": 0.0, "avg_logprob": -0.1086551382186565, "compression_ratio": 1.59375, "no_speech_prob": 0.004678809083998203}, {"id": 152, "seek": 115772, "start": 1167.32, "end": 1177.48, "text": " keep in mind for now that the script tag is also a task, right? And that's the last part", "tokens": [50844, 1066, 294, 1575, 337, 586, 300, 264, 5755, 6162, 307, 611, 257, 5633, 11, 558, 30, 400, 300, 311, 264, 1036, 644, 51352], "temperature": 0.0, "avg_logprob": -0.1086551382186565, "compression_ratio": 1.59375, "no_speech_prob": 0.004678809083998203}, {"id": 153, "seek": 115772, "start": 1178.28, "end": 1186.3600000000001, "text": " in the learning. So this talk, I don't know if anyone read the description, is in three parts,", "tokens": [51392, 294, 264, 2539, 13, 407, 341, 751, 11, 286, 500, 380, 458, 498, 2878, 1401, 264, 3855, 11, 307, 294, 1045, 3166, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1086551382186565, "compression_ratio": 1.59375, "no_speech_prob": 0.004678809083998203}, {"id": 154, "seek": 118636, "start": 1186.36, "end": 1192.04, "text": " right? So there's intro to what happens in the browser, then there's a deep dive of", "tokens": [50364, 558, 30, 407, 456, 311, 12897, 281, 437, 2314, 294, 264, 11185, 11, 550, 456, 311, 257, 2452, 9192, 295, 50648], "temperature": 0.0, "avg_logprob": -0.12620379000293966, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.007681156974285841}, {"id": 155, "seek": 118636, "start": 1192.9199999999998, "end": 1197.8799999999999, "text": " the loop itself, or, well, the task queue, and then the third part is Node.js. And we are done", "tokens": [50692, 264, 6367, 2564, 11, 420, 11, 731, 11, 264, 5633, 18639, 11, 293, 550, 264, 2636, 644, 307, 38640, 13, 25530, 13, 400, 321, 366, 1096, 50940], "temperature": 0.0, "avg_logprob": -0.12620379000293966, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.007681156974285841}, {"id": 156, "seek": 118636, "start": 1197.8799999999999, "end": 1204.6799999999998, "text": " with the first part, right? So everything looks good so far, right? It's a little bit janky,", "tokens": [50940, 365, 264, 700, 644, 11, 558, 30, 407, 1203, 1542, 665, 370, 1400, 11, 558, 30, 467, 311, 257, 707, 857, 361, 657, 88, 11, 51280], "temperature": 0.0, "avg_logprob": -0.12620379000293966, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.007681156974285841}, {"id": 157, "seek": 118636, "start": 1204.6799999999998, "end": 1208.4399999999998, "text": " they did some weird things there, but we get what's happening now, right?", "tokens": [51280, 436, 630, 512, 3657, 721, 456, 11, 457, 321, 483, 437, 311, 2737, 586, 11, 558, 30, 51468], "temperature": 0.0, "avg_logprob": -0.12620379000293966, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.007681156974285841}, {"id": 158, "seek": 120844, "start": 1209.4, "end": 1218.76, "text": " Well, not exactly. I mean, we do, but there's more nuances, right? So let's do a deep dive", "tokens": [50412, 1042, 11, 406, 2293, 13, 286, 914, 11, 321, 360, 11, 457, 456, 311, 544, 38775, 11, 558, 30, 407, 718, 311, 360, 257, 2452, 9192, 50880], "temperature": 0.0, "avg_logprob": -0.11918588688499049, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.004397057928144932}, {"id": 159, "seek": 120844, "start": 1218.76, "end": 1225.72, "text": " into what's actually happening inside of the loop and how our different tasks handle, right?", "tokens": [50880, 666, 437, 311, 767, 2737, 1854, 295, 264, 6367, 293, 577, 527, 819, 9608, 4813, 11, 558, 30, 51228], "temperature": 0.0, "avg_logprob": -0.11918588688499049, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.004397057928144932}, {"id": 160, "seek": 120844, "start": 1225.72, "end": 1233.0, "text": " So as the line says, not all tasks are created equal. This is the model we had so far, right?", "tokens": [51228, 407, 382, 264, 1622, 1619, 11, 406, 439, 9608, 366, 2942, 2681, 13, 639, 307, 264, 2316, 321, 632, 370, 1400, 11, 558, 30, 51592], "temperature": 0.0, "avg_logprob": -0.11918588688499049, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.004397057928144932}, {"id": 161, "seek": 123300, "start": 1233.0, "end": 1239.72, "text": " That the task queue is a simple single queue, which had callback 1, callback 2, callback 3,", "tokens": [50364, 663, 264, 5633, 18639, 307, 257, 2199, 2167, 18639, 11, 597, 632, 818, 3207, 502, 11, 818, 3207, 568, 11, 818, 3207, 805, 11, 50700], "temperature": 0.0, "avg_logprob": -0.12433213107990769, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017649522051215172}, {"id": 162, "seek": 123300, "start": 1239.72, "end": 1247.0, "text": " whatever. And every time the event loop is a while loop, right? So in each cycle of the", "tokens": [50700, 2035, 13, 400, 633, 565, 264, 2280, 6367, 307, 257, 1339, 6367, 11, 558, 30, 407, 294, 1184, 6586, 295, 264, 51064], "temperature": 0.0, "avg_logprob": -0.12433213107990769, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017649522051215172}, {"id": 163, "seek": 123300, "start": 1247.0, "end": 1251.64, "text": " while loop, which is, by the way, called a tick, right? That's just the term. You might have heard", "tokens": [51064, 1339, 6367, 11, 597, 307, 11, 538, 264, 636, 11, 1219, 257, 5204, 11, 558, 30, 663, 311, 445, 264, 1433, 13, 509, 1062, 362, 2198, 51296], "temperature": 0.0, "avg_logprob": -0.12433213107990769, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017649522051215172}, {"id": 164, "seek": 123300, "start": 1251.64, "end": 1259.08, "text": " this sometimes next tick, right? That's what it's talking about. Okay. I've been told I don't have", "tokens": [51296, 341, 2171, 958, 5204, 11, 558, 30, 663, 311, 437, 309, 311, 1417, 466, 13, 1033, 13, 286, 600, 668, 1907, 286, 500, 380, 362, 51668], "temperature": 0.0, "avg_logprob": -0.12433213107990769, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.017649522051215172}, {"id": 165, "seek": 125908, "start": 1259.08, "end": 1267.24, "text": " a lot of time. Let's try to fit this because we are, like, barely halfway there. In reality,", "tokens": [50364, 257, 688, 295, 565, 13, 961, 311, 853, 281, 3318, 341, 570, 321, 366, 11, 411, 11, 10268, 15461, 456, 13, 682, 4103, 11, 50772], "temperature": 0.0, "avg_logprob": -0.10878612570566674, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.025104966014623642}, {"id": 166, "seek": 125908, "start": 1268.04, "end": 1274.52, "text": " there are multiple queues inside of your task queue, right? So what I told you earlier was a bit of a", "tokens": [50812, 456, 366, 3866, 631, 1247, 1854, 295, 428, 5633, 18639, 11, 558, 30, 407, 437, 286, 1907, 291, 3071, 390, 257, 857, 295, 257, 51136], "temperature": 0.0, "avg_logprob": -0.10878612570566674, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.025104966014623642}, {"id": 167, "seek": 125908, "start": 1274.52, "end": 1282.9199999999998, "text": " lie. And actually JavaScript or, well, the JavaScript ecosystem does not handle each task", "tokens": [51136, 4544, 13, 400, 767, 15778, 420, 11, 731, 11, 264, 15778, 11311, 775, 406, 4813, 1184, 5633, 51556], "temperature": 0.0, "avg_logprob": -0.10878612570566674, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.025104966014623642}, {"id": 168, "seek": 128292, "start": 1282.92, "end": 1289.4, "text": " equally, right? Some are given a higher priority than the others. Click events for example. And", "tokens": [50364, 12309, 11, 558, 30, 2188, 366, 2212, 257, 2946, 9365, 813, 264, 2357, 13, 8230, 3931, 337, 1365, 13, 400, 50688], "temperature": 0.0, "avg_logprob": -0.11016392451460644, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03156339004635811}, {"id": 169, "seek": 128292, "start": 1289.4, "end": 1295.0, "text": " this varies a bit from browser to browser. It's different in Node.js. So don't take this as like", "tokens": [50688, 341, 21716, 257, 857, 490, 11185, 281, 11185, 13, 467, 311, 819, 294, 38640, 13, 25530, 13, 407, 500, 380, 747, 341, 382, 411, 50968], "temperature": 0.0, "avg_logprob": -0.11016392451460644, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03156339004635811}, {"id": 170, "seek": 128292, "start": 1295.0, "end": 1299.96, "text": " Bible. This is just an example to show you, right? And it's also oversimplified, obviously.", "tokens": [50968, 6544, 13, 639, 307, 445, 364, 1365, 281, 855, 291, 11, 558, 30, 400, 309, 311, 611, 15488, 332, 564, 2587, 11, 2745, 13, 51216], "temperature": 0.0, "avg_logprob": -0.11016392451460644, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03156339004635811}, {"id": 171, "seek": 128292, "start": 1301.48, "end": 1307.16, "text": " But click events are given a higher priority and then everything else, right?", "tokens": [51292, 583, 2052, 3931, 366, 2212, 257, 2946, 9365, 293, 550, 1203, 1646, 11, 558, 30, 51576], "temperature": 0.0, "avg_logprob": -0.11016392451460644, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03156339004635811}, {"id": 172, "seek": 130716, "start": 1307.72, "end": 1318.44, "text": " There is also something called a request animation frame called back queue, right? And what the F is", "tokens": [50392, 821, 307, 611, 746, 1219, 257, 5308, 9603, 3920, 1219, 646, 18639, 11, 558, 30, 400, 437, 264, 479, 307, 50928], "temperature": 0.0, "avg_logprob": -0.1594237159280216, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.007338249124586582}, {"id": 173, "seek": 130716, "start": 1318.44, "end": 1329.4, "text": " that, right? So the browser, sorry, JavaScript and time is also responsible for rendering, right?", "tokens": [50928, 300, 11, 558, 30, 407, 264, 11185, 11, 2597, 11, 15778, 293, 565, 307, 611, 6250, 337, 22407, 11, 558, 30, 51476], "temperature": 0.0, "avg_logprob": -0.1594237159280216, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.007338249124586582}, {"id": 174, "seek": 130716, "start": 1329.4, "end": 1334.0400000000002, "text": " It's also responsible for drawing things on the screen because the browser is doing that, right?", "tokens": [51476, 467, 311, 611, 6250, 337, 6316, 721, 322, 264, 2568, 570, 264, 11185, 307, 884, 300, 11, 558, 30, 51708], "temperature": 0.0, "avg_logprob": -0.1594237159280216, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.007338249124586582}, {"id": 175, "seek": 133404, "start": 1334.04, "end": 1341.24, "text": " And it doesn't do it on every tick, right? Because that would be wasteful, right? Because", "tokens": [50364, 400, 309, 1177, 380, 360, 309, 322, 633, 5204, 11, 558, 30, 1436, 300, 576, 312, 5964, 906, 11, 558, 30, 1436, 50724], "temperature": 0.0, "avg_logprob": -0.10418935850554821, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.002320418832823634}, {"id": 176, "seek": 133404, "start": 1341.24, "end": 1346.92, "text": " you have a tick happens roughly, let's say, one millisecond. Not really, but let's assume that.", "tokens": [50724, 291, 362, 257, 5204, 2314, 9810, 11, 718, 311, 584, 11, 472, 27940, 18882, 13, 1726, 534, 11, 457, 718, 311, 6552, 300, 13, 51008], "temperature": 0.0, "avg_logprob": -0.10418935850554821, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.002320418832823634}, {"id": 177, "seek": 133404, "start": 1346.92, "end": 1353.96, "text": " Whereas if you have a 60 hertz screen, you only need to refresh every 16 or 17 milliseconds, right?", "tokens": [51008, 13813, 498, 291, 362, 257, 4060, 45830, 2568, 11, 291, 787, 643, 281, 15134, 633, 3165, 420, 3282, 34184, 11, 558, 30, 51360], "temperature": 0.0, "avg_logprob": -0.10418935850554821, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.002320418832823634}, {"id": 178, "seek": 133404, "start": 1353.96, "end": 1358.84, "text": " So it's smart enough to understand that I don't need to do this all the time, but it does need to", "tokens": [51360, 407, 309, 311, 4069, 1547, 281, 1223, 300, 286, 500, 380, 643, 281, 360, 341, 439, 264, 565, 11, 457, 309, 775, 643, 281, 51604], "temperature": 0.0, "avg_logprob": -0.10418935850554821, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.002320418832823634}, {"id": 179, "seek": 135884, "start": 1358.84, "end": 1367.8799999999999, "text": " do it at some point, right? And if you block the event loop, you're going to freeze your screen,", "tokens": [50364, 360, 309, 412, 512, 935, 11, 558, 30, 400, 498, 291, 3461, 264, 2280, 6367, 11, 291, 434, 516, 281, 15959, 428, 2568, 11, 50816], "temperature": 0.0, "avg_logprob": -0.0664367283860298, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.008052971214056015}, {"id": 180, "seek": 135884, "start": 1367.8799999999999, "end": 1375.0, "text": " right? That's the big take home message. So let's look super quickly at an example.", "tokens": [50816, 558, 30, 663, 311, 264, 955, 747, 1280, 3636, 13, 407, 718, 311, 574, 1687, 2661, 412, 364, 1365, 13, 51172], "temperature": 0.0, "avg_logprob": -0.0664367283860298, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.008052971214056015}, {"id": 181, "seek": 135884, "start": 1375.0, "end": 1383.48, "text": " We have three frames here, right? And there's a rendering step happening on each frame, right?", "tokens": [51172, 492, 362, 1045, 12083, 510, 11, 558, 30, 400, 456, 311, 257, 22407, 1823, 2737, 322, 1184, 3920, 11, 558, 30, 51596], "temperature": 0.0, "avg_logprob": -0.0664367283860298, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.008052971214056015}, {"id": 182, "seek": 138348, "start": 1383.48, "end": 1392.1200000000001, "text": " And now what we want to do is we want to... So the time is up, but if you guys don't mind,", "tokens": [50364, 400, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 485, 407, 264, 565, 307, 493, 11, 457, 498, 291, 1074, 500, 380, 1575, 11, 50796], "temperature": 0.0, "avg_logprob": -0.105996162333387, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0060920920222997665}, {"id": 183, "seek": 138348, "start": 1392.1200000000001, "end": 1398.04, "text": " I'm maybe going to take five more minutes. If you have questions, maybe hit me up later,", "tokens": [50796, 286, 478, 1310, 516, 281, 747, 1732, 544, 2077, 13, 759, 291, 362, 1651, 11, 1310, 2045, 385, 493, 1780, 11, 51092], "temperature": 0.0, "avg_logprob": -0.105996162333387, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0060920920222997665}, {"id": 184, "seek": 138348, "start": 1398.04, "end": 1404.28, "text": " but I at least want to finish this part, right? So you have the rendering step.", "tokens": [51092, 457, 286, 412, 1935, 528, 281, 2413, 341, 644, 11, 558, 30, 407, 291, 362, 264, 22407, 1823, 13, 51404], "temperature": 0.0, "avg_logprob": -0.105996162333387, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0060920920222997665}, {"id": 185, "seek": 138348, "start": 1406.6, "end": 1413.08, "text": " And now say if you want to change some logic in the rendering step or related to rendering.", "tokens": [51520, 400, 586, 584, 498, 291, 528, 281, 1319, 512, 9952, 294, 264, 22407, 1823, 420, 4077, 281, 22407, 13, 51844], "temperature": 0.0, "avg_logprob": -0.105996162333387, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0060920920222997665}, {"id": 186, "seek": 141348, "start": 1413.64, "end": 1419.48, "text": " You would just do like a timeout or something, right? Say, run this logic every, I don't know,", "tokens": [50372, 509, 576, 445, 360, 411, 257, 565, 346, 420, 746, 11, 558, 30, 6463, 11, 1190, 341, 9952, 633, 11, 286, 500, 380, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12662784108575784, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0027116029523313046}, {"id": 187, "seek": 141348, "start": 1419.48, "end": 1426.84, "text": " at 60 hertz frequency. But that's not very good because as we saw in the set timeout zero example,", "tokens": [50664, 412, 4060, 45830, 7893, 13, 583, 300, 311, 406, 588, 665, 570, 382, 321, 1866, 294, 264, 992, 565, 346, 4018, 1365, 11, 51032], "temperature": 0.0, "avg_logprob": -0.12662784108575784, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0027116029523313046}, {"id": 188, "seek": 141348, "start": 1427.48, "end": 1433.4, "text": " the time you give in set timeout is not guaranteed, right? It's the minimum time that it'll wait.", "tokens": [51064, 264, 565, 291, 976, 294, 992, 565, 346, 307, 406, 18031, 11, 558, 30, 467, 311, 264, 7285, 565, 300, 309, 603, 1699, 13, 51360], "temperature": 0.0, "avg_logprob": -0.12662784108575784, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0027116029523313046}, {"id": 189, "seek": 141348, "start": 1433.4, "end": 1438.6, "text": " If your queue, if your stack is not empty, when your time is up, then it'll wait for the next", "tokens": [51360, 759, 428, 18639, 11, 498, 428, 8630, 307, 406, 6707, 11, 562, 428, 565, 307, 493, 11, 550, 309, 603, 1699, 337, 264, 958, 51620], "temperature": 0.0, "avg_logprob": -0.12662784108575784, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0027116029523313046}, {"id": 190, "seek": 143860, "start": 1438.6, "end": 1442.1999999999998, "text": " take, it'll wait for the stack to be empty and only then will it do something, right?", "tokens": [50364, 747, 11, 309, 603, 1699, 337, 264, 8630, 281, 312, 6707, 293, 787, 550, 486, 309, 360, 746, 11, 558, 30, 50544], "temperature": 0.0, "avg_logprob": -0.12016585043498448, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.004194838460534811}, {"id": 191, "seek": 143860, "start": 1442.9199999999998, "end": 1449.8799999999999, "text": " So if you just ran this as is in like a set timeout, that'd be a very bad user experience,", "tokens": [50580, 407, 498, 291, 445, 5872, 341, 382, 307, 294, 411, 257, 992, 565, 346, 11, 300, 1116, 312, 257, 588, 1578, 4195, 1752, 11, 50928], "temperature": 0.0, "avg_logprob": -0.12016585043498448, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.004194838460534811}, {"id": 192, "seek": 143860, "start": 1449.8799999999999, "end": 1454.6799999999998, "text": " right? Because you might skip a few frames, you might have visual artifacts, and all kinds", "tokens": [50928, 558, 30, 1436, 291, 1062, 10023, 257, 1326, 12083, 11, 291, 1062, 362, 5056, 24617, 11, 293, 439, 3685, 51168], "temperature": 0.0, "avg_logprob": -0.12016585043498448, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.004194838460534811}, {"id": 193, "seek": 143860, "start": 1454.6799999999998, "end": 1459.8799999999999, "text": " of weird things can happen, right? So that's why we have a separate queue for this, right?", "tokens": [51168, 295, 3657, 721, 393, 1051, 11, 558, 30, 407, 300, 311, 983, 321, 362, 257, 4994, 18639, 337, 341, 11, 558, 30, 51428], "temperature": 0.0, "avg_logprob": -0.12016585043498448, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.004194838460534811}, {"id": 194, "seek": 143860, "start": 1460.6799999999998, "end": 1464.04, "text": " And that queue can be accessed through what's called request animation frame.", "tokens": [51468, 400, 300, 18639, 393, 312, 34211, 807, 437, 311, 1219, 5308, 9603, 3920, 13, 51636], "temperature": 0.0, "avg_logprob": -0.12016585043498448, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.004194838460534811}, {"id": 195, "seek": 146404, "start": 1464.76, "end": 1476.6, "text": " And that's this in the parlance of my coloring, right? So let's quickly summarize. The event", "tokens": [50400, 400, 300, 311, 341, 294, 264, 13734, 719, 295, 452, 23198, 11, 558, 30, 407, 718, 311, 2661, 20858, 13, 440, 2280, 50992], "temperature": 0.0, "avg_logprob": -0.12352977581878206, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.002431541681289673}, {"id": 196, "seek": 146404, "start": 1476.6, "end": 1484.84, "text": " loop is responsible for rendering frames, but not in every cycle. But when it does render something,", "tokens": [50992, 6367, 307, 6250, 337, 22407, 12083, 11, 457, 406, 294, 633, 6586, 13, 583, 562, 309, 775, 15529, 746, 11, 51404], "temperature": 0.0, "avg_logprob": -0.12352977581878206, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.002431541681289673}, {"id": 197, "seek": 146404, "start": 1484.84, "end": 1492.84, "text": " it takes everything in the request animation frame queue and renders it altogether, right?", "tokens": [51404, 309, 2516, 1203, 294, 264, 5308, 9603, 3920, 18639, 293, 6125, 433, 309, 19051, 11, 558, 30, 51804], "temperature": 0.0, "avg_logprob": -0.12352977581878206, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.002431541681289673}, {"id": 198, "seek": 149284, "start": 1493.48, "end": 1497.9599999999998, "text": " So you have this, and then you have micro tasks.", "tokens": [50396, 407, 291, 362, 341, 11, 293, 550, 291, 362, 4532, 9608, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1122379976160386, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.001262676902115345}, {"id": 199, "seek": 149284, "start": 1500.36, "end": 1508.1999999999998, "text": " And micro tasks are basically promises, right? They're not really, but we are oversimplifying,", "tokens": [50740, 400, 4532, 9608, 366, 1936, 16403, 11, 558, 30, 814, 434, 406, 534, 11, 457, 321, 366, 15488, 332, 564, 5489, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1122379976160386, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.001262676902115345}, {"id": 200, "seek": 149284, "start": 1508.1999999999998, "end": 1511.8799999999999, "text": " and we are anyways out of time. So let's just stick with that for now, right?", "tokens": [51132, 293, 321, 366, 13448, 484, 295, 565, 13, 407, 718, 311, 445, 2897, 365, 300, 337, 586, 11, 558, 30, 51316], "temperature": 0.0, "avg_logprob": -0.1122379976160386, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.001262676902115345}, {"id": 201, "seek": 149284, "start": 1512.76, "end": 1520.76, "text": " The big difference with micro tasks is that the queue has to be empty whenever it's run, right?", "tokens": [51360, 440, 955, 2649, 365, 4532, 9608, 307, 300, 264, 18639, 575, 281, 312, 6707, 5699, 309, 311, 1190, 11, 558, 30, 51760], "temperature": 0.0, "avg_logprob": -0.1122379976160386, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.001262676902115345}, {"id": 202, "seek": 152076, "start": 1520.76, "end": 1525.24, "text": " Even if the micro task creates another task that also needs to get executed,", "tokens": [50364, 2754, 498, 264, 4532, 5633, 7829, 1071, 5633, 300, 611, 2203, 281, 483, 17577, 11, 50588], "temperature": 0.0, "avg_logprob": -0.08015188677557583, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0034251902252435684}, {"id": 203, "seek": 152076, "start": 1525.24, "end": 1532.2, "text": " you don't wait for the next time, right? So let's look at an example super quick. So", "tokens": [50588, 291, 500, 380, 1699, 337, 264, 958, 565, 11, 558, 30, 407, 718, 311, 574, 412, 364, 1365, 1687, 1702, 13, 407, 50936], "temperature": 0.0, "avg_logprob": -0.08015188677557583, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0034251902252435684}, {"id": 204, "seek": 152076, "start": 1532.2, "end": 1536.84, "text": " in one tick, in one cycle, right, on which animation is also going to get rendered,", "tokens": [50936, 294, 472, 5204, 11, 294, 472, 6586, 11, 558, 11, 322, 597, 9603, 307, 611, 516, 281, 483, 28748, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08015188677557583, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0034251902252435684}, {"id": 205, "seek": 152076, "start": 1537.72, "end": 1545.72, "text": " you will pick up one of the regular events. You would do all the micro tasks, right?", "tokens": [51212, 291, 486, 1888, 493, 472, 295, 264, 3890, 3931, 13, 509, 576, 360, 439, 264, 4532, 9608, 11, 558, 30, 51612], "temperature": 0.0, "avg_logprob": -0.08015188677557583, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0034251902252435684}, {"id": 206, "seek": 154572, "start": 1545.72, "end": 1554.2, "text": " If you have more micro tasks, you will do all of them. Then you will go to the animation frame.", "tokens": [50364, 759, 291, 362, 544, 4532, 9608, 11, 291, 486, 360, 439, 295, 552, 13, 1396, 291, 486, 352, 281, 264, 9603, 3920, 13, 50788], "temperature": 0.0, "avg_logprob": -0.09317413965861003, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006188736297190189}, {"id": 207, "seek": 154572, "start": 1554.84, "end": 1560.52, "text": " If you have more tasks for animation, you will not do them, right? You will wait for the next", "tokens": [50820, 759, 291, 362, 544, 9608, 337, 9603, 11, 291, 486, 406, 360, 552, 11, 558, 30, 509, 486, 1699, 337, 264, 958, 51104], "temperature": 0.0, "avg_logprob": -0.09317413965861003, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006188736297190189}, {"id": 208, "seek": 154572, "start": 1560.52, "end": 1566.44, "text": " animation cycle to do it. That's some big difference between these two. And again,", "tokens": [51104, 9603, 6586, 281, 360, 309, 13, 663, 311, 512, 955, 2649, 1296, 613, 732, 13, 400, 797, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09317413965861003, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006188736297190189}, {"id": 209, "seek": 154572, "start": 1566.44, "end": 1572.44, "text": " as you can see, if you mess up micro tasks, you can end up freezing up your screen, right?", "tokens": [51400, 382, 291, 393, 536, 11, 498, 291, 2082, 493, 4532, 9608, 11, 291, 393, 917, 493, 20200, 493, 428, 2568, 11, 558, 30, 51700], "temperature": 0.0, "avg_logprob": -0.09317413965861003, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.006188736297190189}, {"id": 210, "seek": 157244, "start": 1572.52, "end": 1577.4, "text": " You put a while loop. You do a task that calls itself, like a promise that calls itself,", "tokens": [50368, 509, 829, 257, 1339, 6367, 13, 509, 360, 257, 5633, 300, 5498, 2564, 11, 411, 257, 6228, 300, 5498, 2564, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08963905210080354, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.0065832240507006645}, {"id": 211, "seek": 157244, "start": 1577.4, "end": 1580.52, "text": " and then everything is going to get frozen. That's because of this.", "tokens": [50612, 293, 550, 1203, 307, 516, 281, 483, 12496, 13, 663, 311, 570, 295, 341, 13, 50768], "temperature": 0.0, "avg_logprob": -0.08963905210080354, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.0065832240507006645}, {"id": 212, "seek": 157244, "start": 1583.0, "end": 1591.96, "text": " All right. I think we are out of time. This is maybe the one last thing I want to do,", "tokens": [50892, 1057, 558, 13, 286, 519, 321, 366, 484, 295, 565, 13, 639, 307, 1310, 264, 472, 1036, 551, 286, 528, 281, 360, 11, 51340], "temperature": 0.0, "avg_logprob": -0.08963905210080354, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.0065832240507006645}, {"id": 213, "seek": 157244, "start": 1591.96, "end": 1598.28, "text": " if you guys don't mind, and then we'll stop, right? So can someone tell me what would be", "tokens": [51340, 498, 291, 1074, 500, 380, 1575, 11, 293, 550, 321, 603, 1590, 11, 558, 30, 407, 393, 1580, 980, 385, 437, 576, 312, 51656], "temperature": 0.0, "avg_logprob": -0.08963905210080354, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.0065832240507006645}, {"id": 214, "seek": 159828, "start": 1598.28, "end": 1602.52, "text": " the answer to this? Awesome. Can you explain it?", "tokens": [50364, 264, 1867, 281, 341, 30, 10391, 13, 1664, 291, 2903, 309, 30, 50576], "temperature": 0.0, "avg_logprob": -0.22011522146371695, "compression_ratio": 1.1810344827586208, "no_speech_prob": 0.021218398585915565}, {"id": 215, "seek": 159828, "start": 1620.52, "end": 1628.04, "text": " Perfect. Yes. And also keep in mind that the script itself is a task, right? Because why", "tokens": [51476, 10246, 13, 1079, 13, 400, 611, 1066, 294, 1575, 300, 264, 5755, 2564, 307, 257, 5633, 11, 558, 30, 1436, 983, 51852], "temperature": 0.0, "avg_logprob": -0.22011522146371695, "compression_ratio": 1.1810344827586208, "no_speech_prob": 0.021218398585915565}, {"id": 216, "seek": 162804, "start": 1628.04, "end": 1637.56, "text": " not be otherwise, right? So why not just put the set time out in the queue and execute it?", "tokens": [50364, 406, 312, 5911, 11, 558, 30, 407, 983, 406, 445, 829, 264, 992, 565, 484, 294, 264, 18639, 293, 14483, 309, 30, 50840], "temperature": 0.0, "avg_logprob": -0.11841179983956474, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.011768024414777756}, {"id": 217, "seek": 162804, "start": 1637.56, "end": 1643.96, "text": " But you can't because script itself is in the queue, right? So the set time out will go afterwards.", "tokens": [50840, 583, 291, 393, 380, 570, 5755, 2564, 307, 294, 264, 18639, 11, 558, 30, 407, 264, 992, 565, 484, 486, 352, 10543, 13, 51160], "temperature": 0.0, "avg_logprob": -0.11841179983956474, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.011768024414777756}, {"id": 218, "seek": 162804, "start": 1645.3999999999999, "end": 1656.28, "text": " All right. That's about it. Thanks a lot. Hope you guys learned something.", "tokens": [51232, 1057, 558, 13, 663, 311, 466, 309, 13, 2561, 257, 688, 13, 6483, 291, 1074, 3264, 746, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11841179983956474, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.011768024414777756}, {"id": 219, "seek": 165804, "start": 1658.04, "end": 1660.28, "text": " Thank you.", "tokens": [50376, 1044, 291, 13, 50476], "temperature": 0.0, "avg_logprob": -0.9520121415456136, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9218613505363464}], "language": "en"}