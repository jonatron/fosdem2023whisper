{"text": " All right. Well, hello everyone. You'll notice I'm not Ben. This is Ben. There's three of us here to talk to you about different things, all about improving clients and making them as fast as you would normally expect them to be. So, first of all, my name is Keegan. I'm going to be talking about Scyling Sync. And then you've got Ben, going to talk about Russ Eskay and Mara about Element X. So, first of all, Scyling Sync, and a bit about myself. I'm a staff software engineer at Element. And I've worked on many different projects over the years, and more recently working on things like Dendrite and Peer-to-Peer and Scyling Sync. But first of all, what even is Scyling Sync? So, for context, Scyling Sync, the current Scyling Sync mechanism in Matrix is really, really slow. So, if you go and open up your mobile app after a weekend away or something like that, it takes a little while to Scyling Sync. It could take 30 seconds or a minute, depending on how many rooms are on your account. And this is kind of bad, right? We'd like it to sync instantly. And the whole point of Scyling Sync is trying to make that happen, trying to make it sync instantly, or virtually instantly. There was a talk last year on the online TOS-DEM. If you want to know more information about the deep dive of how Scyling Sync works, and there's a QR code there. But I'm not going to be covering too much detail about how Scyling Sync works other than enough to kind of fill in the gaps if you have no idea what this is. So, at a high level, Scyling Sync works by sorting and filtering. So, you can see here you've got all the rooms on the user's account, and then you can filter that set of rooms down in some way. So, for example, you could filter it based on, like, I want encrypted rooms, or I want DM rooms, things like that. And then you can apply some sort of sorting operation to them. So, you might say, sort them by the room name, or you could say, sort by recency. So, like, the last timestamp in the room, or by the number of notification counts, number of unread messages and stuff that mention your name, and that sort of thing. And then you can request the first five rooms, 10 rooms, 20 rooms, and things like that. Also, the rooms themselves, you can filter the room state using Scyling Sync. So, in normal sync, you will go and get all of the room state, and if there's a lot of room state, that's not great. So, in Scyling Sync, you can specify, I'm only interested in, like, the topic of the room and whether it's encrypted or not, and that's it. This is a pretty big change to how Matrix works today. So, how is this actually going to, like, how are we actually going to do this in practice? So, in practice, there is a go process, which is the Scyling Sync proxy, which I've been working on for over a year now, which has a Postgres database attached, and it will go and do Sync v2 requests on your behalf to an upstream server. It could be Synapse, it could be Deadrite, whatever, it doesn't really matter, it could be Conjuret. And the important thing here is that this proxy exposes a Scyling Sync API. So, it exposes a new endpoint for a new Sync endpoint, and then a client can go and try the Scyling Sync API and see how it feels for them. So, they don't need to have a particular implementation on Synapse, they will wait for these implementations to land. You can try it on your own server if you run a proxy. In terms of a protocol level, what this looks like is you can see here you've got, like, some lists. It's a list subject, and then you can specify things like things we were talking about before. So, you can say you've got the ranges there, you've got, so that's how many, like, the top-end rooms that you want, the sort ordering that you want, as well as any filters you apply here. And here you can see we're filtering by IsDMTrue, and that's going to be used to populate the people tabs, say, on ElementWeb. You also have these things for room subscriptions. They are kind of like the room lists, but this is when you know the specific rim ID. So, if you follow a permalink, which may include the rim ID, or if you, you know, if you refresh the page and you know that, you know, this person was currently viewing this room, that room may not be in this list, right? So, you would need to subscribe to that room directly because you know the rim ID. And typically, as well, the kinds of information you want here is different. So, in here, we are requesting all the state in the room and a much higher timeline element because this is being used to populate the actual full room view. The response is very similar, as well. So, you have a list object here, and then you get a list of rim IDs that are used to populate the correct ordering here. And then you also have a top-level rims array, a rims object, and then that's just a key value map, where the keys are the rim IDs, and then the values are all the data that you requested. So, these all get aggregated together, which I will speak about a bit later. In terms of what's new, so if you followed SidingSync, then you might be like, okay, I know all this, but what's actually happened over the past year? We have clients now that run SidingSync. So, this is from Element Web. It's got a nice scary warning there. So, you know, it's great. It all works on Web, but also it actually works on mobile devices, as well, thanks to the Rust SDK, which I'll leave for Ben to talk about. So, there's also a whole bunch of new extension MSCs. So, extension MSCs are an idea of trying to break up the complexity of SidingSync because the sync API is by far one of the most, or not the most complicated part of the client's server API, and trying to put everything into one MSC is going to be doomed to failure. So, we're trying to specify a core part of the MSC, a core part of what is syncing, which is the syncing rims, and working out the sorts and the filter arguments, and then we're leaving to extensions all the extra stuff on top. And the idea is that you can opt into any of these things. So, if your client doesn't do receipts, then great. Don't subscribe to receipts. Don't even enable this extension. Briefly, how these extensions work. So, these two extensions go together because they're ultimately used to make encryption work in encrypted rooms. So, you can see, or actually you can't see at all. Here is an encrypted event. So, there's basically, you have to trust me, there's a ciphertext here with lots of gibberish effectively, and then you need to run keys to go decrypt it into your normal text. The way that works is that you need to get keys via your two device messages. That's why they go together. The other thing here is that it implements another MSC called dropping-sale-centered device messages. You can barely see it on here, but this is an output from Postgres, which is trying to work out how many unread or unacknowledged two device events are there for a given user device. And you might think that might be, say, 100, maybe 1,000 tops. It turns out this can be a lot. This is several hundred thousand unread or unacknowledged two device events. And it turns out when I analyzed a lot of this, this was almost entirely down to RIM keys being requested, and then either canceled or successfully sent. So, this MSC 3944 basically says, hey, if you request a RIM key and then you subsequently cancel that request, you're going to go and delete those two device messages, so they don't just keep stacking up in this way. And that obviously really helps reduce the amount of bandwidth for SignSync as well. The other thing we've got is account data. So, if you wonder what account data does, if you've ever used the breadcrumbs thing at the top here on Element Web, that's synchronized seasoning account data. Also, account data is really, really useful for working out accurate notification counts. So, at the bottom here, you can just about see that you've got some messages here. You've got a message and a timeline. This is encrypted. And it says here, notification count one. Notification counts are the gray numbers, and you've got a highlight count of zero, which is the red number. And yet, on the UI, you can see that it's a red number and it's gone to one. So, something's happened here where the client has worked out that, oh, I should use this as a highlight count, not a notification count. It's overwritten what the service told it. And what's happened here is that the client has decrypted the message, and then it's checked the message to say, hey, you know, is there any app mention or any specific keywords based on your push rules? And if that is true, then it knows, ah, okay, I need to actually make this a red highlight rather than just a normal gray and red count. And that's done using push rules. And push rules is done in, stored as an account data. Final two ones are receipts and typing. Thank you. So, hopefully, you know what receipts and typing notifications are. The main changes for Sliding Sync is that the receipts are lazily loaded. So, you might think, what does that mean exactly? Well, if you request a timeline limit of 10, then you will get those 10 events, and then you will get the receipts for those 10 events, and you won't get receipts for any other events. And you might think, shit, hasn't it always done this? Well, not really. So, here's some JQ for Matrix HQ, which is this room ID, and it's just pulling out the receipt EDU, and then kind of checking, like, roughly how many receipts there are. And, you know, Matrix HQ is quite a big room, so you might think, you know, 100,000. No, there's quite a lot of rooms, quite a lot of receipts in there. And this is not great from a bandwidth perspective, right? We don't want to be sending 53,000 read receipts, particularly for events which you are unlikely to ever view, right? Because these could be for events that occurred, like, a year ago. So, Sliding Sync also fixes that. So, with all these performance optimizations, a very large account with 4,000 rooms can take less than a second to actually sync, which is down from 15 minutes on Sync V2. So, very happy with that, but it's still not really good enough. We're, you know, we're trying to go big or go home kind of thing, so we want to make it even faster, so it is literally instant. You don't want to have to be waiting a couple of seconds. It should just kind of open up, just like most other messaging clients, so you can just open them up and they just work. The problem is that things are going to get a lot worse here, which I will talk about in a moment. So, we've added in a bunch of tracing to the proxy server. So, things like, this is runtime trace. So, you can see exactly the control flow. There's some spans there, and you can see various optimizations that were done. So, this is identifying so bits of code. Lots and lots and lots of commits. Sometimes it's just, you forgot to add an index. Sometimes you should be doing things in bulk instead of doing things sequentially. So, lots of work has gone into this. And also, if you're going for, you know, 100 milliseconds kind of, aiming for 100 milliseconds, the actual amount of data you send is important, because this starts to become quite a large factor in the total time it takes. We can do simple things like deduplication and enabling GZIP for large responses, which we now do. And as well as that, we can aggressively cache things in memory wherever possible. So, we don't have to query the database when clients send a request. So, there's three levels of caching involved at the proxy level, whereas a global cache which contains kind of information which doesn't change for any user, so it's a constant. So, things like the number of joined users in a room, it's the same if you're Alice or if you're Bob, it's always going to be the same. Whereas things like the user cache or things like, you know, what's the unread count for this room? Well, that's going to change depending on which user. And then the connections are things like which room subscriptions have you got, or which lists like what are your top end rooms or whatever your sliding window is. Interesting thing to note here is that the room name is actually not global data. The data that's used to calculate the room name is global data and is the same for everyone, but the room name itself isn't because of DMs. So, if you have a DM with Alice and Bob, then from Alice's point of view, the room name is Bob, but from Bob's point of view, the room name is Alice. So, lots and lots of optimizations have been done. So, with all of this, we're now getting less than 100 milliseconds, which is what we wanted, but it's still not good enough because things are going to get a lot worse because clients, really, it's all up to the clients because clients want offline support and they want instant access. You know, they don't want to have to be having to do a network request to, you know, when they click on a room, they want to just see the list. They don't want to see a spinner. And in the best case, you have a spinner for half a second, maybe, and then it loads, which is, you know, it's not great, but maybe acceptable. But then, if you're on a mobile app and you go into a tunnel, then it's just going to spin it forever and then you're sad. So, users expect these things to kind of work instantly. And you can kind of, you know, Sighting Sync has ways that you can kind of fix this. So, if you want to go and instantly see the room timeline, that's fine because we can pre-cache the room timeline, right? You can use a higher timeline limit and then you can go and pre-cache that. So, you see the room list, you click through and immediately you see all the events, at least a screen's worth of events. For the other thing, which is you want to scroll the room list instantly and smoothly, well, you can opt out of the sliding windows entirely and you can just request really small stub information like, I just want the avatar, I just want the room name and that's it. And then you'll know the position, the room name, the avatar, and then you can just request all the rooms entirely. So, that will scale with the number of rooms in the user's account, but it's, you know, it's possible. And you can use something like this. So, you say timeline limit of zero, but there's a problem here, right? Because you have a timeline limit of 20 on the first one, then a timeline limit of zero. So, you kind of want a bit of both. So, it turns out what clients really want is delayed data delivery. So, the API wasn't originally designed for that a year ago. So, we've made a lot of changes to support this kind of idea of delayed data delivery. So, one of the things is timeline trickling. So, what timeline trickling is, is that you can initially request a set of rooms and you can say, I want only the most recent message in this room. And then at a later point, you can say, okay, now I want the last 10 messages in this room. And then it will go and, and effectively backpaginate those messages for you. Likewise, the clients want all the rooms and the accounts. So, but they want maybe more detail on the rooms that are in the viewport. So, again, you can support this by having two lists effectively. You've got one list, which is just the visible rooms. That might have more accurate information for like room previews. So, you know, you've got room preview, you might have, you know, typing notifications, you might register for typing notifications in those rooms. But then you're not really interested in type notifications for rooms, you know, really far down the list. And then in the background, you can just have a separate list, which just kind of in the background goes and gets all the other rooms and all the core information that you need. So, this has kind of been a huge trade-off, right? On the one hand, you've got sync v2, which is getting everything and is super slow, but it's got fantastic offline support as a result of that. And on the other side, you've got sliding sync. It's super fast. You only literally getting the data that you need, but, you know, there's compromises to be made there because you have to do network requests all the time and things can be slower. There's only so fast you can do. There's only so much you can optimize a server. So, really, I think element is kind of aiming to do something like that. So, it's mostly kind of sliding sync, but there are compromises and trade-offs that are being made to try to give a really good offline experience as well. So, in terms of what's next, we need to add threads because there's no threading support at all in sliding sync. And threads, obviously, only reasonably recently landed and was enabled everywhere. Threads is complicated because threads are changes fundamental answers to questions like, is this room unread? Because normally you could just be like, well, what's your red marker? What's most recent event? Okay, it must be unread. Whereas now you could have scenarios where, you know, the most recent event in the room is on a thread. So, if you were just to click on a room and you see the timeline, they're all messages, but in a thread, you know, three days ago, there's actually a newer message. So, adding support for threads is going to be quite tricky to get right and we'll have to probably iterate on it quite a lot, but it is coming. The other thing we're going to be adding in is this concept called delta tokens, which unless you've read the MSC, you'll have no idea what it is. Ultimately, what delta tokens are is to, sliding sync has a problem at the moment, the proxy server, because it has amnesia. So, it will time out your connection if you don't use it for, say, half an hour. And it will clean up all that in memory state. All those caches and things get cleaned up. And the problem is, is that then when you reconnect, even though your client has stored those rooms and stored a lot of the timeline and stored a bunch of room state, the proxy server doesn't know this. So, it's going to resend that information to you. So, the point of delta tokens is to say, hey, I remember me, I've already stored, I already know about these events. And then those events aren't sent to the client again in duplicate. A few more API optimizations that we recently swapped to using lists as keys, which basically means that instead of representing the requests and response lists as an array of lists, they're now just a big key value map, which makes it easier because you can then reference an individual list by the list key name. So, for things like extensions, this is great because you could then have a way of expressing, I want typing notifications, but only on these named lists, whereas before that was very difficult to express. And we also really want to have comprehensive client support. It's getting reasonably stable now, and it's certainly very performant. And Element Web uses Slime Sync natively in the JS SDK, but obviously, that doesn't really work for mobile. And it would be nice to have some sort of SDK that could be used for Android and iOS, and maybe even web at some point. I think there is. Yes. Let's talk about the Rust SDK. So, this is, sorry, overall a very technical talk. You already noticed that. But I'm going to lighten up a little bit more. But first about me, so I'm Ben. Hi. My name is the only name in the presentation. I don't know why. These guys have more work and show more stuff. So, quick, yeah. I've led the Rust SDK team for the last year for Element, and I've been working in decentralization, decentralized tech for a couple years already. I worked at PariTech before, was leading the substrate client team there, if you know, blockchain. That's one of the most favorite blockchain building systems. I'm going to be working as a tech link for ActiveGlobal, where we're building, on top of the Rust SDK, an organizing app for NGOs and civil society. So, I've been working in this for over a decade. You might know me from almost not at all threatening talk I gave at Jason, like, 2017. That was already about, like, how do you do decentralized privacy-first technology? Enough about me. Let's talk about, let me tell you a little story. We're back in 2019, 2020, and it's the state of the clients. For the sake of argument, I'm talking about Element clients here, because I think there's exceptions to what I'm going to tell you. But let me tell you two truths in a lie, and you can tell me if you can spot the lie. So, truth number one, many clients out there don't actually implement end-to-end encryption, which is pretty sad, because it's a very fundamental part of what we're working on. That is mostly because it's hard. Even if you use the most widely used LibOm library, that is a seed library that is already slightly dated. There's, like, a lot of knowledge that has been built up that is not easy to ingrain in this existing library anymore. Clients usually implement the entire HTTP, or at least most of the state machine around room, room state, who's allowed to write, as well as the entire messaging mechanics themselves in their own language, in their own environment. Therefore, because we have that, clients are super fast, it is totally integrated into the system that they are, and it's just a smooth experience. I don't have to ask you, you know, which one of this is a lie, the cake is a lie. At this time, enter our hero. Our hero is Damir. Damir is working as a crypto dev for Element. He's a Rust into the S, and he knows the crypto in and out. He's intending to rewrite a plug-in that he's using for an ISC client, which is called WeChat, that connects to Matrix. Because of simple problems that are limitations in the Python implementation that WeChat offers, he wants to rewrite it in Rust. But he doesn't really find a good space to build it on. This is not an actual representation, but we're going to use it for now. So he goes out and says, OK, let's write this. How hard could it be? He quickly realizes, OK, so the crypto side with the C, I would like to have that in Rust. I'm going to get that Y in a second. And he pulls that out later, which is now called Votosimac. You might have heard about that, which is our crypto implementation that we're pushing forward as a live-all misdiplicated. But he figures out the stuff around that to make crypto work, not the encryption itself, but the entire thing of how do I know which messages to encrypt with what key in what room, what if a message comes in and I don't have the encryption key? All of that state management around that is actually as complicated and as problematic as the actual crypto. And that is why a lot of people try to use the crypto, but then fail in doing all of that, making it a really terrible experience. And then I drop it and say, oh, let's not do encryption. That's too hard. But he continues and pushes on because he really wants that for WeChat and starts out with what we know as the Rust metrics SDK. So why did he pick Rust? I'm not talking in his name, but I'm going to give you some reasons why. If you heard about Rust before, you probably heard about it because it's the most popular, most beloved language. Rust is running now on the Stack Overflow system. So who here has heard about Rust? All right. Who has used Rust? Keep your hands up. Okay. Okay. That's fairly, fairly good. And while that is definitely true to some degree, like there's a lot of log for that language, it's even bigger in crypto because encryption, building encryption and building that safely is really hard. At the same time, you're not, you can't really go for Python or that kind of stuff because it's, well, too inefficient. So most information used to you see Rust seemed like a such nice alternative. So inside crypto and encryption, Rust is already a big thing. So that's probably the main reason he chose it because he wanted to use it. But there's also a good amount of actual reasons why Rust makes sense to build this with. This is a screenshot of the website of Rustlang.org from yesterday. I'm going to break it down a little more because we have to understand one key thing. Rust was invented by Mozilla to build a new browser. They had Firefox 2010, 2011. They were like, there's so much C, C++ in here. It's so complicated. We barely know how we can change stuff ourselves. And it's like, it's still a Netscape code base in there, right? Like, it's like 20 years of stuff. So they were like, let's build a new browser and it's called the CERBO project as a recent research project. And through that, they realized like, there's certain things we'd like to have from new languages and they started building their own language to build a browser. That project still exists. It's CERBO.org today. Mozilla has handed off the management to the Linux foundation. It's still a research project. I recommend if you want to start with Rust. That is a really good community to start with. But the key point here is that it was a language built by practitioners for practitioners. They didn't set out to say like, hey, let's make a theoretically proven language. Let's make a really beautiful looking language. All of these ideals were not existing. They wanted a language that they can use, that they're more efficient in building a browser with, which is already quite hard. If you say like, I want to build a browser, that's a lot of stuff you have to do. And so they set out to build, this is the previous claim that Rust had, which is a type safe systems language, so systems language like level of C, C++, with zero cost abstractions. Well, by practitioner I said that. So it's a modern language. It reached 1.0 in 2015. It is as speedy as C and C++. Sometimes it's speedier. The most famous example is ripgrab. If you go for that, it's like 10 times faster than the next comparable implementation to grab over a lot of files. And it does all of that without any garbage collector or VM. Again, the goal is to have zero cost abstractions. Any abstraction that Rust gives you, and a lot of the abstractions that the community also gives you in their own crates, has the idea of we can lower that down at compile time to nothing. It doesn't actually exist. Therefore, garbage collector cycles, no. VM below that, no. It should work on an embedded system. That rules out a lot of places. But all of that without memory safety bugs. Just probably the biggest concern for any security researcher. Buffer overflows are nonexistent, effectively, in Rust. Very famously, a couple of days ago, Google announced that since they have been shipping Rust in Android, I think a third of the code that they ship in Android is now Rust, their amount of memory bugs has halved, even lesser than that. And that is their main concern so far. That most of that happens at compile time. So at compile time, the compiler is a little more annoying and telling you, like, you need to tell me where this memory is going to go. Is that in this thread or in that thread? But it also means that after it compiles, it runs. But again, because it's built from practitioners for practitioners, it's not just about the language. Like, you need to be able to actually work with that. That means that it's very famous for its very good tooling. It has a really nice compiler that very famously when people jump from other languages and they run through the first error, they see the compiler complain and they switch immediately back to look at the code. In Rust, you don't do that. The compiler is probably going to tell you what you need to change. Or at least what they, what it thinks you need to change to make that run. That is a completely behavioral change. The compiler is your friend telling you, look, you need to just tell me, is that in this thread or in that thread? This is what I assume you would want to do. It can be wrong, of course, because you have higher level abstraction that you need to work with. But overall, it's pretty good. The same for cargo, which is the package management system and build system, but also Rust up, which is the meta version of organizing your own Rust installation. And all of that, it provides with being built against the LLVM backend, which means it's more or less instantly portable. When you can run it and you don't have any specific architecture code for your Mac, it will compile for Windows as well. You basically just have to say there's another target. The way that LLVM works, it has an abstract syntax tree of its own in between. We compile, basically Rust compiles to that. And then everything that LLVM supports as a target, it can compile to you. And that is pretty amazing. That led to Rust being the very first language that had native support for WebAssembly as a target language. Because it was just switching on, oh, yeah, the target for that. At the same time, sorry, my voice is still a little sick, it allows you to have a C-compatible lip interface. And that makes it really nice to embed it into other stuff and use it as a library. All right, so that's Rust. What currently do we have in the Rust SDK now, a year later? The idea is essentially that everything you need to have to build a matrix client, it should be there. Batteries included. That specifically means we want, we have an async type safe API. Like requests you do, they're type safe. They come back, we check that the JSON that comes back is what it needs to be. It has a full featured room state. So for every room that you're in, it can tell you, can you write in that room? What kind of messages can you write? What are the other users in the room? What is their avatar? What other states do they have? All of that stuff, it is managing for you. You don't have to bother too much about that. It has a persistent storage layer support. So you don't have to worry about caching it or putting it somewhere locally. You can still do that on your own if you want. It is a pluggable interface, but it already comes with a native version which is kind of a deprecated slab which we intend to replace with the SQLite which is still partially there for crypto, but not from the other side yet. But it also has, for example, support for web, for indexeddb. So you can run it in the browser. One of the examples actually is an echo bot that runs in your browser in Wasm. It's pretty awesome. And for us, almost the most important part is that it has transparent end-to-end encryption support. When you're in a room and that room is encrypted, it's going to send the messages out to get the keys that you need to allow you to verify with a different device. But from the point that you join with a new device and you just say room send and you give it the message, it's going to send an encrypted message. That's it. For the most part of it, unless user interaction is required, you don't have to bother about that. It's going to store that information. It's going to make sure that when you start up the next time through the storage support that you have all the keys there, you don't have to bother about there being an additional end-to-end encryption that you have to take care with. I already mentioned that it has Wasm and web support. And because of the C layer, we're also able to offer support to different bindings out there. So we have two bindings that are used in the next generation of element apps. You're going to see that later for Kotlin and Swift through Uni-FFI. But there's also custom bindings for Node.js and for JS on the web as well. I think there's Python bindings out there, but they're not maintained by us. This all allows us to go beyond what we have so far. It allows us to ingrain more of the stuff that different clients and implementations have been using, but that has verily cross-pollinated. If you had a really clever way of managing your timeline in Android, the iOS people wouldn't know. That all converges into this singular place now. And that allows us to do a lot more things a lot quicker. One of the things that we currently do is we offer a new experimental timeline API that manages the state for you. Like back in 2018, 2019, editing messages came around and that fundamentally changed the idea of an event in Matrix. It's just not a stream of events anymore, but events acting upon other events. This changes a message from a previous thing. With a new timeline API, you don't have to bother. We're just going to tell you, oh, position 17. This is now this. The same is true for redactions. The same is true for reactions. All of these things ensune threads. I don't know how we're going to do threads yet, but that all is supposed to be right there. You don't have to bother about the state machine changes that this requires. It's just going to tell you, hey, you need to render a different thing now. The other thing that was mentioned before as well is support for sliding sync. Both of these are still experimental. You have to actively switch them on because it's interfaces that we're not confident with that are going to stick exactly the way they are, but there's implementations out there using that. All right, so does it work? Does it live up to the promise? Well, let's see. In order to build sliding sync, I built a small testing UI. With sliding sync right now, this is Mr. Big, my test account with, I don't know how many rooms, but usually loading it on an element web is like a minute for initials sync. With my timeline, with sliding sync up and this testing system, it's 200 milliseconds. It's 200 milliseconds to render the room. You can see this down here. And to pull up all other rooms, it's like another 30 milliseconds. So yeah, it's fast. It does what it's supposed to do. But is that actually true? I'm a core developer. Of course, the thing that I'm building here is hopefully going to work, but how plausible is that as a SDK? Like maybe I'm just building a lot of stuff. Okay, let's take a look at the thing itself. So the implementation on top of the Rust SDK for this UI is a whopping 2,000 lines. It's pretty small. And most of that is actually 2e realm stuff because actually 2e's in Rust are not that great, so you have to do a lot of state management. The actual implementation of managing the Rust SDK is less than 130 lines of code. Everything else you saw, including that this stores it on your hard drive, totally abstracted away. I don't have to bother about this from that perspective. So I would say, yeah, definitely. It does SDK. But again, I'm a core developer. Hopefully it's easy for me to build this. It should be fairly okay to build something as quick. But of course, it's supposed to be working for you. All right. All right. For that, we have also brushed up our game a little bit on documentation. And one thing I would like you to look at to check the time. It's all right. So we have reorganized the repo a little bit to make it a little cleaner. You can see there's a bunch of stuff around that. There's Xtas, which is our task manager, benchmarks, testing. That should be self-explanatory. We have the bindings and the Uni-FFI bind gen to organize bindings. We have the labs, which is also where you find the jack-in implementation if you're curious about this. But the main stuff lives in crates and contrap is other things built on hub. Do we have an examples folder exactly for this kind of stuff? So let me quickly, is that possible? Roughly. I put the slides into the dev room if you want to look at them. Quickly run through one, the SDK bot 101 thing. It allows you to directly use that from the repo with that command. What you see on the first screen is just the imports that we need. You see mostly Rust SDK stuff, some minor managing around that. If you're familiar with Rust, you know that binary has this main function. We use Tokyo here. It's an async function, right? Async API. Most of that is just parsing in a very ugly way, the command line, and then handing it over to Lock-in and Sync. This Lock-in and Sync sets up some minor stuff. You see that we have a lot of information about this in code comments for right here for you. It does even set up a slept store. You can call the Lock-in username. You can give it a name for the bot that is the device that you will see, and it locks you in. Going further, I don't have the time to go through the entire thing, but it explains everything right here. This bot does two things. For every room that you ask it to join, it will automatically join, which is this first event handler, and the second event handler is reacting on messages. An event handler in the client is basically just a callback that you can say, when these kind of events come in, please tell me, and then I react to this. Those themselves can be async again, pretty nice. Then it just starts syncing, and that's all it does, which means it's running the Sync loop. This does not, at this point, use sliding sync. As I told you, it's kind of experimental. Let's look at the room message. The un-room message, whenever we receive a message, we can again mention that before it's type safe. It's going to give us the actual room message in a typed format, so we can rely on the compiler here to make sure that things are as they should be. We make sure that we are in this room. We try to figure out if it's a text message. If it's a text message we check for, is it dollar bank party? If so, we're going to respond with a message, and that's all the thing does. In reality, it looks like this. I'm showing you this is just regular main at the moment. Then, if I run the bot, this is slightly capped, so you can't see my password. I'm here connected to that bot. You see that I'm in here. I had two more prints that are not in main right now to make it a little cleaner. I'm sending a message. We see that this message is ignored, but if I send bank party, you can see it's reacting, it's sending this. Most importantly, this is an encrypted room. I didn't have to do anything to build a bot that allows me to live and interact with encrypted room. That's an encrypted message. I didn't have to do anything. You saw that there was no setup. I hadn't had to manage anything. The Rust SDK did all of that for me. If you want to learn more, if you want to use this, you can find all of the code, of course, at metrics.metrics.usdk. You can join our developer and general talking about the Rust SDK room. The example you just saw is inside the examples folder getting started. Jack in. The other client you saw before is in labs. Jack in. All of that code, obviously. I really recommend going for the getting started. It has a lot of documentation. I also want to send an honorable mention to Benjamin who is working on Trinity, which is a built on top of the Rust SDK, a bot framework, I would say. It allows you to write some very small Rust that is compiled to Wasm that it runs in the client that can react to messages. You can write just the message part and say, like, I have a bot that reacts to messages. This is one. Oh, yeah. Element is hiring. If you are interested in working on this full-time Element IO careers, we're going to have time for questions later. We have to get through all of these first. Let's see what you can actually build with this. Thank you. That's a long one. So, hello, everyone. My name is Mauro. Honestly, my colleagues are at a slide where they presented themselves. I don't have such a thing. So, I have to be brief. I come from Italy, Naples. I'm a software engineer. I work at Element. I mostly work on the IO side of things and set up working on some Rust implementations. Today, I'm going to talk about the new client element tags. The new client is pretty much being built with the idea of both the fine goals. The first of them is pretty much user experience. The thing is that we really wanted to improve over the user experience of the current Element implementation. The thing is that Element started as pretty much a showcase for what Magics was capable of. So, it was a bit like an app made by engineers, for engineers. So, yeah, not everyone is into this kind of stuff. So, sometimes it's a bit hard to use for the average user, and we want to improve over this. Also, we want, of course, to have a performance to be another very important goal. Actually, just as important as UX, we're actually, thanks to the slide-in sync implementation on Element tags, we're aiming to actually launch the app in less than 100 milliseconds. That's pretty much the thing that we're aiming for. And, of course, also optimize the bandwidth usage. Also, we want to build an app that is reliable just from the start. So, testing code coverage is pretty much right from the start of the project, a Niagara priority. And also, we want to build the app in a way that is actually relying on shared components pretty much. Mattis Francis Decay is just one of them, but, of course, we're planning to build more components that will be shared across different implementation, across different platforms, different projects. So, not even necessarily Element tags. It is that we will be able to use them, and, of course, anyone in the open source community will be able to use them. So, why are we writing the Android and the iOS app? That's actually a good question, because some of these goals could also be achieved with a very big refactor. But, let's go more in depth on why we want to do our right. So, let's start with Element iOS. Element iOS, it's quite old. It started in 2015. Essentially, it was, as I said, pretty much a POC to showcase what Metrix was capable of. It started as being named the Metrix iOS console, in fact. Then it went through a bunch of identity crisis and changed name three times. I guess it was first console, then viator, then riot. Now, it's Element. Let's hope it's going to stick with that. So, and, yeah, it was built by engineers to showcase pretty much what Metrix was capable of. But the thing is that, first of all, as I said, the user experience was not great. Second, it was made with some very old components written on Objective-C that used some very old architectural pattern, like MVC, which should stand for a review controller, but it stands more for massive review controller, because you know, by doing this, but it's a very good controller, and it's just a huge mess, and you start looking at 60,000 lines of code in a controller, and you're like, oh, my God, why am I alive? So, yeah, you don't want to see that anymore, pretty much. We want to move to a newer architecture. Also, even if we did a lot of refactors on the Element iOS implementation, you essentially, yeah, we were essentially not able to change all the old implementation, since they were very hard to maintain, and we still relied on these components a lot. So, yeah, four components are still using these old implementations. Half of the code is still in Objective-C, and code coverage is quite low. So, we decided to experiment a bit in Q2 2022. We decided to pretty much build a minimum client using the Metrix for access decay, and pretty much the state-of-the-art frameworks provided by Apple, like SwiftUI, but not only that, like also AsyncAway and things like that. So, yeah, and we were actually able to build this new minimum client that had a room list timeline, and it was, let's just say, a technical success. It was super fast and amazing. So, we decided to build, on top of this second POC, by giving a more focus on the UX, because as I said, yeah, now we have a performance client, but now we need to have a simple client that anyone is able to use. So, element tax was, iOS was then born. On the Android side, things are slightly different, because technically speaking, the Android application already had a rewrite in 2019. So, we had two choices. We could essentially just take the Android SDK, put it on a side, and pretty much replace it with the Rust SDK, or maybe just rewrite it from scratch and using pretty much the state-of-the-art frameworks that Android provides right now, like for example Jetpack Compose. In the end, we decided to go for the latter, for two reasons. First of all, I mean, if we're building an application on iOS that uses the latest frameworks, why do you want to do the same for Android? And second, UX, as I said, UX was a very, very important concern. So, even if you wanted to rebuild the app from scratch or rewrite it, just pretty much change some stuff for the existing app, it would still require pretty much a huge UX overall, which in the end made the rewrite even more sense. So, pretty much obviously the architecture of element tax structured. Well, we have pretty much the backbone of the client. It's pretty much all sitting in the Magic Rust SDK. It's all there. And the Magic Rust SDK through Uni-SFI is able to expose with bindings and causing bindings. It's interesting because, as pretty much Ben said, it's exposing objects that are reactive, that the client is the only thing that it needs to care about, doesn't need to care about the events, all the events, the newer events. It just needs to know that the event has been changed and it's in that place. It doesn't need to know that it's a new event that came afterwards and so on. So, the idea is that these objects that the bindings expose are actually already ready to be displayed, essentially. So, you just need to render them on the UI and that makes the development way wazer. And of course, the sliding sink is pretty much a requirement on element tax in the sense that it's being built with the idea that the sliding sink will be pretty much next standard for the clients. And so, it will only work with servers that implement for now this sliding sink proxy, essentially. So, this is an example of how the code is pretty much translated from Rust into Zwift and Kotlin through Uni-SFI. As you can see, there is the timeline item. I would say it is pretty much an object that is pretty much like a view model. It's already ready to be displayed. It just pretty much need to take the presentation data from this object and render them on the UI and that's it. Which will make implementing clients for the future with the Matrix Rust SDK way wazer. So, the bindings are pretty much separate to repo. Anyone can download them as a file, a year file for Android or as a framework for Zwift implementations or you can just pretty much use a package manager like Maven Central on Android or Zwift package manager on, yeah, on Zwift implementations, essentially. I think it's Zwift implementations because actually it's interesting but the Matrix Rust SDK is scalable of running on any Apple system target. So, I really can't wait someone crazy enough to build a client for Apple Watch or Apple TV. I'm pretty sure that the 10 people in the world at Apple TV will be very pleased that there is a Matrix client on their Apple TV. But, of course, ElementX is going to share more than just the Rust SDK. We're pretty much trying to build other components that we hope to share just across ElementX but across multiple projects. For example, we want to build an OpenID Connect component, an Element call component. And, of course, since the two apps are pretty much the same up on different platforms, they're going to share translation, they're going to share design tokens. So, I mean, why don't we just pretty much make a component to share these elements already. And we're actually also building an interesting component which is called the Rich Text Editor, which is essentially an SDK of written Rust that then exposes these bindings in Zwift and Kotlin through edify and also in WebAssembly. And it's essentially a UI framework that you can import into your client to render rich text in what you see is what you get fashion, essentially. It's something that is going to come also into ElementX. So, keep an eye for it. But, hey, what's this like? Oh, actually, it's already there. Oh, but this is not ElementX. Actually, this element, the Rich Text Editor is already in Element right now. But in iOS, Android, and Web, you can enable it in labs. You can just go to labs, enable it, and test it. And, you know, if you're able to break it, just, you know, send us some feedback and we will try to fix it as soon as possible. It's a project that I've worked on. I'm very proud of it. I think we achieved something really great because it's a very simple way to pretty much, it's a way in which you can create rich text without need of using markdowns and see how they look like, which will make life easier for when you need to create something like this. Because I challenge everyone to make something like this with markdowns. I mean, you'd go crazy with that. So, yeah, the cool thing is that this Rich Text Editor SDK that we built, I mean, it's not just for metrics, so, metrics client. I mean, technically speaking, anyone could use this. Maybe you want to make a note app. You want to make, I don't know, like an app that is your diary, whatever you want. You can pretty much implement this. And, if you want to test it, you can scan the QR code. You will get pretty much to the latest main implementation on web. It's a test debug version there. The one on labs is more stable. This one is more to play around with it. It's cool because this one, it allows you to pretty much see how the rich text is transformed into a DOM representation, which is in Rust, and then transformed back into an HTML, which is the one that we are sending over the metrics lines, of course. So, of course, testing for reliability, another important keyword. It's something that we want to pretty much improve. And so, pretty much, we built a very, yeah, very stack test infrastructure that we hope is going to cover all these areas. It's already covering most of these areas. And, yeah, pretty much make the app more reliable, and the project way, way safer. So, yeah, ElementTax actually has come with a lot of benefits. First of all, on the tech side, it's way, way faster, both because the metrics for us has decayed. I mean, it's amazing. It makes things easier. Both from a development standpoint, because you just write it once and deploy it everywhere. But at the same time, the fact that you just have your models already ready to be displayed, it's amazing. And also, slide-in sync. And, of course, the use of declarative UIs like SwiftUI and Jetpack Compose makes development time actually faster. And actually, it's also easier to test, I would say. But also, the UI performance has been improved, actually. Also, sharing components is something that will benefit not only just ElementTax, but pretty much any client that wants to implement a metrics client. But actually, we hope that some of the sharing components that we're building will not just benefit the metrics community, but the overall open source community. So, yeah, but, yeah, the major benefit, actually, we should not focus just on the main benefit that we are offering on the tech side. We actually want to focus on the benefit we're really offering to the users, because in the end, the main focus ElementTax, yeah, its performance, its tech, its sharing components, but, of course, it's making the app more usable, more accessible, easier to use. We want to make an app that is not just... We want to make an app that, essentially, also can be used by your friends and family to chat with you, even casually, during the day. So, not just for people that, essentially, want to keep their conversation safe and secure for the metrics protocol. Roadmap. Pretty much this is the present of the future of ElementTax. For now, you can log in, check the room list, timeline, send messages, edit, reply, react. But there are some restrictions. First of all, of course, as I say, this lighting sync is required. So, if your server doesn't activate its lighting sync proxy, yeah, you can much, pretty much use the client on that server. Also, it only supports authentication, and authentication, it's only through the metrics protocol. We want to support also IDC and registration, but when we will build the OIDC component, we will support that. Device verification is there, but only for emojis, so, no QR verification yet, and also no messages through the description. Yeah, this is pretty much where you can find the ElementTax iOS version repo. There will be a public test flight coming soon, and actually, Matthew, will demo this in this afternoon? Yeah. Okay. That's the plan. And regarding ElementTax Android, it's a bit behind schedule, because, as I said, it was developed after ElementTax iOS, so it's more in a state of being set up. But of course, you can try to run it, check the state of the repo. You know, if you want to play around with it, this is pretty much where you can find the actual repo of ElementTax Android. This is pretty much the roadmap on what we plan to do. Actually, more than a plan, it's more like what we, let's say, it's more like, say, it's not a deadline, it's more like what we imagine we're able to achieve in these dates. And I was also told to be as vague as possible, so for the release date of the public launch, I will just say that it will come sometime in the future. All right. Okay. And that should deal with it. So, yeah, that's all. And we can do a, I think, a rapid QA session, right? Yes, yes. We have 10 minutes. Oh, okay. Nice. Right on schedule. Nice. Okay. Yeah, it's this around. Yeah. Please go ahead. If I remember correctly, the sliding sync option in ElementWeb said that you can't disable it in the warning, why is that? So, the question is, let me repeat it for the camera. Why can't you disable the sliding sync labs feature in the current version? Mostly because of end-to-end encrypted messages, you would risk being unable to decrypt your end-to-end encrypted messages in that session. So, the reason why is because when you log into the proxy, it's going to be syncing on your account, right? And it's going to sync forever. Well, until the access token gets invalidated, but it's going to be syncing on your behalf. If you toggled sliding sync on then off, if you turned it off, then your ElementWeb would be using the V2 sync as well as the proxy, because the proxy didn't know you toggled it off. So, that means you've got two sync loops for your account. And that's going to cause problems when it causes a race condition because two device messages, when they're acknowledged, and they get acknowledged by increasing the sync token, they get deleted on the server. So, if your ElementWeb was super, super fast and managed to race ahead slightly of the proxy, then it would go and get all the two device events, and the proxy would not, or vice versa. And vice versa is the problem that's trying to warn against. So, if the proxy was ahead, then you would not get a certain two device events, and therefore you may potentially lose room keys, and therefore may potentially be unable to decrypt messages. Hopefully that's clear. Do you have any data on whether sliding sync significantly impacts the server load? So, the question is, what about server load on sliding sync? Do we have any data? I need clarification, because do you mean at a proxy level, or do you mean in like a general sense for native implementations of the server? Does using sliding sync improve server performance? A native implementation, yes, it would. So, that's one of the reasons why the existing sync implementation is slow, is just because the servers have to do an awful lot of work. And obviously, I've been developing on dendrite, I know exactly what things are slow there. So, a lot of the API that's exposed to the clients are basically efficient ways that you can do it. So, you only get like the current state of rooms, you don't tend to need to go back in time, you don't need to remember all your synth tokens since the beginning of time. These are things that slow down the processing. So, yes, a native implementation, but a proxy implementation obviously is a sync loop that's going to be made, so that will increase load, right? Because that's going to be constantly syncing on your account. Okay, so that's an element X question, I guess. Wait, let me repeat it first. So, the question is about multi-user account support in the app. It's something that we're discussing, but for now, there is no definite plan, let's say. From the metrics SDK side, I can tell you that you can do it. That's not an issue. So, I think you were next. Saw you. So, two part question. One is how far out do you think Sliding Sync is from actually being like merged and finalized as a spec? And then second part to that is are there plans to do a native implementation for those APIs in Synapse? Yes, so the question is basically how long it's going to take for Sliding Sync to land and will we get native implementations in Synapse? You will get a native implementation in Synapse, I don't know when. And yes, we're going to try to merge and land it as soon as this is practically possible, which, you know, there's still things we need to fix, right? Like things like threading and stuff just doesn't work. And that's actually one of the biggest blockers at the moment from us trying out just defaulting element web to Sliding Sync on by default is that for compatible service, obviously, is the fact that we don't have threading support, so you wouldn't have feature parity. So, when we do a feature parity, then, you know, there could be element web clients which enable it by default. It won't be in labs, it will be enabled in labs by default. So, you know, we're getting there, but I can't give you a time, unfortunately. Thank you, next. So, the question is the authentication parts in the REST SDK. So, yes, we have login via username and password. We have implemented OIDC in general, but I don't think it's fully tested. And we have an SSO feature as well. So, we ask the server, the specification test, right? The server tells us what is possible, and then we allow you to use those. So, generally, yeah, if your server is SSO, you can use metrics SDK with it. Jan, here. Question from the Internet. Ooh, a question from the Internet. I heard about them. Are there any plans or what is the status of the matrix RTC in the REST SDK? Yeah, the question is about RTC in the REST SDK. If you followed the RTC talk before, you noticed that most of the RTC part of the RTC is actually offloaded to web RTC in the current implementation. So, going through a web view. For us, as REST, that means we don't have to bother about most of that. There's only some signaling that happens on the actual matrix protocol. So, we don't have at the moment the plan to implement an actual RTC our side. I wouldn't see where you would want to do that for other than that view. So, currently, it's not on the roadmap, at least for our side. Let me talk about IoT. Yeah. So, that's a common one as REST is very, so the question is about IoT devices. Could you do that with REST? I see you can. Yes. That is generally possible. We have, because of the storage systems and some other things in there, and because matrix itself is still quite heavy as an overall protocol. We have tried to get it into an actual embedded device. That is not at the moment possible. We would have to improve a lot on the way that we use REST. REST itself provides that, but we can't do that. But you can use it, for example, on an Android, not an Androidino, but a Raspberry Pi. We know of people that run Raspberry Pis that have signals coming in, and then they use the REST SDK to send it over into rooms. That is definitely possible, because it's more or less just a bot. From our perspective, it's just a bot. So, that is possible, but you still need a significant amount of memory at the moment, and that would make it not possible for actual embedded devices. Yet, if anybody wants to do that, come to me. I can show you and mentor you and help you, because it would be very exciting if we had the possibility to do that. Jan, another question from the internet? There's also a question about the element X. What you see is what you get, editor. Is it still possible to use just markdown if you want to just use knockdown? So, the question is about the element X. Where's your big editor? Can you still use markdown if you want to use markdown? Actually, even on the current element implementation that is on the client, you can actually also still use markdown. So, there's an option that allows you to turn off the rich text and turn back the simple text, and when the simple text is on, pretty much you can use markdowns. But, will it render in what you see, what you get, fashion? So, the question is about does the markdown then render in the WYSIWYG? No, when you're using the simple text version, it's rendering pretty much like a simple text with the markdowns. So, any plans? Or, naming it in the rich text without the markdowns? Currently not. We're pretty much trying to build the rich text editor, as it is with just the rich text using the formatting toolbar to be the most performant and good and simple to it as possible. But, it is something that for sure, when we have a very stable product, we will look into. No question. Will this finally unite the markdown syntaxes that you can use in different element clients? Will that finally reduce the amount of different markdown syntaxes that you can use in element clients? I'm not sure about the question actually. Will the WYSIWYG editor in simple text mode use one unified markdown implementation so you don't have to remember different variants of markdown and different clients? But you're talking if we are going in the future to support the markdowns inside the WYSIWYG directly without turning off the rich text? This is what you mean? In simple text mode, if you enter markdown, we would parse the same way on element iris, android and web. So, I think there's a confusion here. You switch on the WYSIWYG editor, then you get the WYSIWYG. If you turn it off, you have a simple text mode that you can do some markdown, but it's not going to be rendered inside of this. So, it's just going to fall back to the existing implementation. So, therefore, yeah, to answer your question, it's falling back to the existing implementation. So, no, they will still be incompatible. So, that we might switch to use the markdown for round-tripping, because at some point, I think this was the previous question, that people are going to want to round-trip between the markdown implementation and the WYSIWYG one. And to do that consistently, you're going to want to use the same library. You put that in the Rust layer. And finally, we get out to the nightmare of Common Mark versus GitHub, Flavint, markdown versus whatever random library the different element platforms have. I think Android is still out of sync with the others. Great. One last question. Where can we meet you today, or maybe later, if we have more questions? I think we're going to hang around here, right? Yeah, for sure. We'll be able to stand soon. We have to stand in K1. I'm just going to be around here, lurking, so just talk to me. Yeah, same for me. I'm going to be here. I'm the guy with that hat. All right. Thank you very much. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.4, "text": " All right. Well, hello everyone. You'll notice I'm not Ben. This is Ben. There's three of", "tokens": [1057, 558, 13, 1042, 11, 7751, 1518, 13, 509, 603, 3449, 286, 478, 406, 3964, 13, 639, 307, 3964, 13, 821, 311, 1045, 295], "temperature": 0.0, "avg_logprob": -0.23938132305534518, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.1274367868900299}, {"id": 1, "seek": 0, "start": 10.4, "end": 15.24, "text": " us here to talk to you about different things, all about improving clients and making them", "tokens": [505, 510, 281, 751, 281, 291, 466, 819, 721, 11, 439, 466, 11470, 6982, 293, 1455, 552], "temperature": 0.0, "avg_logprob": -0.23938132305534518, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.1274367868900299}, {"id": 2, "seek": 0, "start": 15.24, "end": 21.04, "text": " as fast as you would normally expect them to be. So, first of all, my name is Keegan.", "tokens": [382, 2370, 382, 291, 576, 5646, 2066, 552, 281, 312, 13, 407, 11, 700, 295, 439, 11, 452, 1315, 307, 3189, 43118, 13], "temperature": 0.0, "avg_logprob": -0.23938132305534518, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.1274367868900299}, {"id": 3, "seek": 0, "start": 21.04, "end": 25.32, "text": " I'm going to be talking about Scyling Sync. And then you've got Ben, going to talk about", "tokens": [286, 478, 516, 281, 312, 1417, 466, 318, 1344, 1688, 26155, 66, 13, 400, 550, 291, 600, 658, 3964, 11, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.23938132305534518, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.1274367868900299}, {"id": 4, "seek": 2532, "start": 25.32, "end": 31.880000000000003, "text": " Russ Eskay and Mara about Element X. So, first of all, Scyling Sync, and a bit about myself.", "tokens": [3878, 2313, 789, 293, 2039, 64, 466, 20900, 1783, 13, 407, 11, 700, 295, 439, 11, 318, 1344, 1688, 26155, 66, 11, 293, 257, 857, 466, 2059, 13], "temperature": 0.0, "avg_logprob": -0.2116299725453788, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.1583318812190555e-05}, {"id": 5, "seek": 2532, "start": 31.880000000000003, "end": 37.28, "text": " I'm a staff software engineer at Element. And I've worked on many different projects", "tokens": [286, 478, 257, 3525, 4722, 11403, 412, 20900, 13, 400, 286, 600, 2732, 322, 867, 819, 4455], "temperature": 0.0, "avg_logprob": -0.2116299725453788, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.1583318812190555e-05}, {"id": 6, "seek": 2532, "start": 37.28, "end": 41.760000000000005, "text": " over the years, and more recently working on things like Dendrite and Peer-to-Peer and", "tokens": [670, 264, 924, 11, 293, 544, 3938, 1364, 322, 721, 411, 413, 521, 35002, 293, 2396, 260, 12, 1353, 12, 20048, 260, 293], "temperature": 0.0, "avg_logprob": -0.2116299725453788, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.1583318812190555e-05}, {"id": 7, "seek": 2532, "start": 41.760000000000005, "end": 49.28, "text": " Scyling Sync. But first of all, what even is Scyling Sync? So, for context, Scyling Sync,", "tokens": [318, 1344, 1688, 26155, 66, 13, 583, 700, 295, 439, 11, 437, 754, 307, 318, 1344, 1688, 26155, 66, 30, 407, 11, 337, 4319, 11, 318, 1344, 1688, 26155, 66, 11], "temperature": 0.0, "avg_logprob": -0.2116299725453788, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.1583318812190555e-05}, {"id": 8, "seek": 4928, "start": 49.28, "end": 56.160000000000004, "text": " the current Scyling Sync mechanism in Matrix is really, really slow. So, if you go and open", "tokens": [264, 2190, 318, 1344, 1688, 26155, 66, 7513, 294, 36274, 307, 534, 11, 534, 2964, 13, 407, 11, 498, 291, 352, 293, 1269], "temperature": 0.0, "avg_logprob": -0.15278189061051708, "compression_ratio": 1.6553030303030303, "no_speech_prob": 7.96249969425844e-06}, {"id": 9, "seek": 4928, "start": 56.160000000000004, "end": 59.760000000000005, "text": " up your mobile app after a weekend away or something like that, it takes a little while", "tokens": [493, 428, 6013, 724, 934, 257, 6711, 1314, 420, 746, 411, 300, 11, 309, 2516, 257, 707, 1339], "temperature": 0.0, "avg_logprob": -0.15278189061051708, "compression_ratio": 1.6553030303030303, "no_speech_prob": 7.96249969425844e-06}, {"id": 10, "seek": 4928, "start": 59.760000000000005, "end": 63.24, "text": " to Scyling Sync. It could take 30 seconds or a minute, depending on how many rooms are", "tokens": [281, 318, 1344, 1688, 26155, 66, 13, 467, 727, 747, 2217, 3949, 420, 257, 3456, 11, 5413, 322, 577, 867, 9396, 366], "temperature": 0.0, "avg_logprob": -0.15278189061051708, "compression_ratio": 1.6553030303030303, "no_speech_prob": 7.96249969425844e-06}, {"id": 11, "seek": 4928, "start": 63.24, "end": 70.2, "text": " on your account. And this is kind of bad, right? We'd like it to sync instantly. And", "tokens": [322, 428, 2696, 13, 400, 341, 307, 733, 295, 1578, 11, 558, 30, 492, 1116, 411, 309, 281, 20271, 13518, 13, 400], "temperature": 0.0, "avg_logprob": -0.15278189061051708, "compression_ratio": 1.6553030303030303, "no_speech_prob": 7.96249969425844e-06}, {"id": 12, "seek": 4928, "start": 70.2, "end": 73.56, "text": " the whole point of Scyling Sync is trying to make that happen, trying to make it sync", "tokens": [264, 1379, 935, 295, 318, 1344, 1688, 26155, 66, 307, 1382, 281, 652, 300, 1051, 11, 1382, 281, 652, 309, 20271], "temperature": 0.0, "avg_logprob": -0.15278189061051708, "compression_ratio": 1.6553030303030303, "no_speech_prob": 7.96249969425844e-06}, {"id": 13, "seek": 7356, "start": 73.56, "end": 79.24000000000001, "text": " instantly, or virtually instantly. There was a talk last year on the online TOS-DEM. If", "tokens": [13518, 11, 420, 14103, 13518, 13, 821, 390, 257, 751, 1036, 1064, 322, 264, 2950, 314, 4367, 12, 35, 6683, 13, 759], "temperature": 0.0, "avg_logprob": -0.13751158142089845, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.4065329853328876e-05}, {"id": 14, "seek": 7356, "start": 79.24000000000001, "end": 82.76, "text": " you want to know more information about the deep dive of how Scyling Sync works, and there's", "tokens": [291, 528, 281, 458, 544, 1589, 466, 264, 2452, 9192, 295, 577, 318, 1344, 1688, 26155, 66, 1985, 11, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.13751158142089845, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.4065329853328876e-05}, {"id": 15, "seek": 7356, "start": 82.76, "end": 88.72, "text": " a QR code there. But I'm not going to be covering too much detail about how Scyling Sync works", "tokens": [257, 32784, 3089, 456, 13, 583, 286, 478, 406, 516, 281, 312, 10322, 886, 709, 2607, 466, 577, 318, 1344, 1688, 26155, 66, 1985], "temperature": 0.0, "avg_logprob": -0.13751158142089845, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.4065329853328876e-05}, {"id": 16, "seek": 7356, "start": 88.72, "end": 94.72, "text": " other than enough to kind of fill in the gaps if you have no idea what this is. So, at a", "tokens": [661, 813, 1547, 281, 733, 295, 2836, 294, 264, 15031, 498, 291, 362, 572, 1558, 437, 341, 307, 13, 407, 11, 412, 257], "temperature": 0.0, "avg_logprob": -0.13751158142089845, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.4065329853328876e-05}, {"id": 17, "seek": 7356, "start": 94.72, "end": 100.80000000000001, "text": " high level, Scyling Sync works by sorting and filtering. So, you can see here you've", "tokens": [1090, 1496, 11, 318, 1344, 1688, 26155, 66, 1985, 538, 32411, 293, 30822, 13, 407, 11, 291, 393, 536, 510, 291, 600], "temperature": 0.0, "avg_logprob": -0.13751158142089845, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.4065329853328876e-05}, {"id": 18, "seek": 10080, "start": 100.8, "end": 106.12, "text": " got all the rooms on the user's account, and then you can filter that set of rooms down", "tokens": [658, 439, 264, 9396, 322, 264, 4195, 311, 2696, 11, 293, 550, 291, 393, 6608, 300, 992, 295, 9396, 760], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 19, "seek": 10080, "start": 106.12, "end": 110.08, "text": " in some way. So, for example, you could filter it based on, like, I want encrypted rooms,", "tokens": [294, 512, 636, 13, 407, 11, 337, 1365, 11, 291, 727, 6608, 309, 2361, 322, 11, 411, 11, 286, 528, 36663, 9396, 11], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 20, "seek": 10080, "start": 110.08, "end": 114.75999999999999, "text": " or I want DM rooms, things like that. And then you can apply some sort of sorting operation", "tokens": [420, 286, 528, 15322, 9396, 11, 721, 411, 300, 13, 400, 550, 291, 393, 3079, 512, 1333, 295, 32411, 6916], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 21, "seek": 10080, "start": 114.75999999999999, "end": 120.24, "text": " to them. So, you might say, sort them by the room name, or you could say, sort by recency.", "tokens": [281, 552, 13, 407, 11, 291, 1062, 584, 11, 1333, 552, 538, 264, 1808, 1315, 11, 420, 291, 727, 584, 11, 1333, 538, 850, 3020, 13], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 22, "seek": 10080, "start": 120.24, "end": 126.0, "text": " So, like, the last timestamp in the room, or by the number of notification counts, number", "tokens": [407, 11, 411, 11, 264, 1036, 49108, 1215, 294, 264, 1808, 11, 420, 538, 264, 1230, 295, 11554, 14893, 11, 1230], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 23, "seek": 10080, "start": 126.0, "end": 130.04, "text": " of unread messages and stuff that mention your name, and that sort of thing. And then", "tokens": [295, 517, 2538, 7897, 293, 1507, 300, 2152, 428, 1315, 11, 293, 300, 1333, 295, 551, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1480658337786481, "compression_ratio": 1.9211469534050178, "no_speech_prob": 4.359702506917529e-05}, {"id": 24, "seek": 13004, "start": 130.04, "end": 136.16, "text": " you can request the first five rooms, 10 rooms, 20 rooms, and things like that. Also, the", "tokens": [291, 393, 5308, 264, 700, 1732, 9396, 11, 1266, 9396, 11, 945, 9396, 11, 293, 721, 411, 300, 13, 2743, 11, 264], "temperature": 0.0, "avg_logprob": -0.12365926156832477, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.777422014332842e-05}, {"id": 25, "seek": 13004, "start": 136.16, "end": 141.28, "text": " rooms themselves, you can filter the room state using Scyling Sync. So, in normal sync,", "tokens": [9396, 2969, 11, 291, 393, 6608, 264, 1808, 1785, 1228, 318, 1344, 1688, 26155, 66, 13, 407, 11, 294, 2710, 20271, 11], "temperature": 0.0, "avg_logprob": -0.12365926156832477, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.777422014332842e-05}, {"id": 26, "seek": 13004, "start": 141.28, "end": 144.68, "text": " you will go and get all of the room state, and if there's a lot of room state, that's", "tokens": [291, 486, 352, 293, 483, 439, 295, 264, 1808, 1785, 11, 293, 498, 456, 311, 257, 688, 295, 1808, 1785, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.12365926156832477, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.777422014332842e-05}, {"id": 27, "seek": 13004, "start": 144.68, "end": 150.12, "text": " not great. So, in Scyling Sync, you can specify, I'm only interested in, like, the topic of", "tokens": [406, 869, 13, 407, 11, 294, 318, 1344, 1688, 26155, 66, 11, 291, 393, 16500, 11, 286, 478, 787, 3102, 294, 11, 411, 11, 264, 4829, 295], "temperature": 0.0, "avg_logprob": -0.12365926156832477, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.777422014332842e-05}, {"id": 28, "seek": 13004, "start": 150.12, "end": 156.88, "text": " the room and whether it's encrypted or not, and that's it. This is a pretty big change", "tokens": [264, 1808, 293, 1968, 309, 311, 36663, 420, 406, 11, 293, 300, 311, 309, 13, 639, 307, 257, 1238, 955, 1319], "temperature": 0.0, "avg_logprob": -0.12365926156832477, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.777422014332842e-05}, {"id": 29, "seek": 15688, "start": 156.88, "end": 162.16, "text": " to how Matrix works today. So, how is this actually going to, like, how are we actually", "tokens": [281, 577, 36274, 1985, 965, 13, 407, 11, 577, 307, 341, 767, 516, 281, 11, 411, 11, 577, 366, 321, 767], "temperature": 0.0, "avg_logprob": -0.14920900344848634, "compression_ratio": 1.6106194690265487, "no_speech_prob": 4.674945375882089e-05}, {"id": 30, "seek": 15688, "start": 162.16, "end": 166.44, "text": " going to do this in practice? So, in practice, there is a go process, which is the Scyling", "tokens": [516, 281, 360, 341, 294, 3124, 30, 407, 11, 294, 3124, 11, 456, 307, 257, 352, 1399, 11, 597, 307, 264, 318, 1344, 1688], "temperature": 0.0, "avg_logprob": -0.14920900344848634, "compression_ratio": 1.6106194690265487, "no_speech_prob": 4.674945375882089e-05}, {"id": 31, "seek": 15688, "start": 166.44, "end": 172.79999999999998, "text": " Sync proxy, which I've been working on for over a year now, which has a Postgres database", "tokens": [26155, 66, 29690, 11, 597, 286, 600, 668, 1364, 322, 337, 670, 257, 1064, 586, 11, 597, 575, 257, 10223, 45189, 8149], "temperature": 0.0, "avg_logprob": -0.14920900344848634, "compression_ratio": 1.6106194690265487, "no_speech_prob": 4.674945375882089e-05}, {"id": 32, "seek": 15688, "start": 172.79999999999998, "end": 181.16, "text": " attached, and it will go and do Sync v2 requests on your behalf to an upstream server. It could", "tokens": [8570, 11, 293, 309, 486, 352, 293, 360, 26155, 66, 371, 17, 12475, 322, 428, 9490, 281, 364, 33915, 7154, 13, 467, 727], "temperature": 0.0, "avg_logprob": -0.14920900344848634, "compression_ratio": 1.6106194690265487, "no_speech_prob": 4.674945375882089e-05}, {"id": 33, "seek": 18116, "start": 181.16, "end": 186.32, "text": " be Synapse, it could be Deadrite, whatever, it doesn't really matter, it could be Conjuret.", "tokens": [312, 26155, 11145, 11, 309, 727, 312, 12550, 35002, 11, 2035, 11, 309, 1177, 380, 534, 1871, 11, 309, 727, 312, 2656, 73, 540, 83, 13], "temperature": 0.0, "avg_logprob": -0.17113065719604492, "compression_ratio": 1.7653846153846153, "no_speech_prob": 1.5275611076503992e-05}, {"id": 34, "seek": 18116, "start": 186.32, "end": 193.07999999999998, "text": " And the important thing here is that this proxy exposes a Scyling Sync API. So, it exposes", "tokens": [400, 264, 1021, 551, 510, 307, 300, 341, 29690, 1278, 4201, 257, 318, 1344, 1688, 26155, 66, 9362, 13, 407, 11, 309, 1278, 4201], "temperature": 0.0, "avg_logprob": -0.17113065719604492, "compression_ratio": 1.7653846153846153, "no_speech_prob": 1.5275611076503992e-05}, {"id": 35, "seek": 18116, "start": 193.07999999999998, "end": 199.6, "text": " a new endpoint for a new Sync endpoint, and then a client can go and try the Scyling Sync", "tokens": [257, 777, 35795, 337, 257, 777, 26155, 66, 35795, 11, 293, 550, 257, 6423, 393, 352, 293, 853, 264, 318, 1344, 1688, 26155, 66], "temperature": 0.0, "avg_logprob": -0.17113065719604492, "compression_ratio": 1.7653846153846153, "no_speech_prob": 1.5275611076503992e-05}, {"id": 36, "seek": 18116, "start": 199.6, "end": 204.8, "text": " API and see how it feels for them. So, they don't need to have a particular implementation", "tokens": [9362, 293, 536, 577, 309, 3417, 337, 552, 13, 407, 11, 436, 500, 380, 643, 281, 362, 257, 1729, 11420], "temperature": 0.0, "avg_logprob": -0.17113065719604492, "compression_ratio": 1.7653846153846153, "no_speech_prob": 1.5275611076503992e-05}, {"id": 37, "seek": 18116, "start": 204.8, "end": 209.35999999999999, "text": " on Synapse, they will wait for these implementations to land. You can try it on your own server", "tokens": [322, 26155, 11145, 11, 436, 486, 1699, 337, 613, 4445, 763, 281, 2117, 13, 509, 393, 853, 309, 322, 428, 1065, 7154], "temperature": 0.0, "avg_logprob": -0.17113065719604492, "compression_ratio": 1.7653846153846153, "no_speech_prob": 1.5275611076503992e-05}, {"id": 38, "seek": 20936, "start": 209.36, "end": 215.92000000000002, "text": " if you run a proxy. In terms of a protocol level, what this looks like is you can see", "tokens": [498, 291, 1190, 257, 29690, 13, 682, 2115, 295, 257, 10336, 1496, 11, 437, 341, 1542, 411, 307, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.1684585730234782, "compression_ratio": 1.8106995884773662, "no_speech_prob": 7.195324724307284e-05}, {"id": 39, "seek": 20936, "start": 215.92000000000002, "end": 220.36, "text": " here you've got, like, some lists. It's a list subject, and then you can specify things", "tokens": [510, 291, 600, 658, 11, 411, 11, 512, 14511, 13, 467, 311, 257, 1329, 3983, 11, 293, 550, 291, 393, 16500, 721], "temperature": 0.0, "avg_logprob": -0.1684585730234782, "compression_ratio": 1.8106995884773662, "no_speech_prob": 7.195324724307284e-05}, {"id": 40, "seek": 20936, "start": 220.36, "end": 224.20000000000002, "text": " like things we were talking about before. So, you can say you've got the ranges there,", "tokens": [411, 721, 321, 645, 1417, 466, 949, 13, 407, 11, 291, 393, 584, 291, 600, 658, 264, 22526, 456, 11], "temperature": 0.0, "avg_logprob": -0.1684585730234782, "compression_ratio": 1.8106995884773662, "no_speech_prob": 7.195324724307284e-05}, {"id": 41, "seek": 20936, "start": 224.20000000000002, "end": 230.04000000000002, "text": " you've got, so that's how many, like, the top-end rooms that you want, the sort ordering that", "tokens": [291, 600, 658, 11, 370, 300, 311, 577, 867, 11, 411, 11, 264, 1192, 12, 521, 9396, 300, 291, 528, 11, 264, 1333, 21739, 300], "temperature": 0.0, "avg_logprob": -0.1684585730234782, "compression_ratio": 1.8106995884773662, "no_speech_prob": 7.195324724307284e-05}, {"id": 42, "seek": 20936, "start": 230.04000000000002, "end": 233.96, "text": " you want, as well as any filters you apply here. And here you can see we're filtering", "tokens": [291, 528, 11, 382, 731, 382, 604, 15995, 291, 3079, 510, 13, 400, 510, 291, 393, 536, 321, 434, 30822], "temperature": 0.0, "avg_logprob": -0.1684585730234782, "compression_ratio": 1.8106995884773662, "no_speech_prob": 7.195324724307284e-05}, {"id": 43, "seek": 23396, "start": 233.96, "end": 239.84, "text": " by IsDMTrue, and that's going to be used to populate the people tabs, say, on ElementWeb.", "tokens": [538, 1119, 35, 44, 14252, 622, 11, 293, 300, 311, 516, 281, 312, 1143, 281, 1665, 5256, 264, 561, 20743, 11, 584, 11, 322, 20900, 4360, 65, 13], "temperature": 0.0, "avg_logprob": -0.17409183084964752, "compression_ratio": 1.6727941176470589, "no_speech_prob": 8.450410678051412e-05}, {"id": 44, "seek": 23396, "start": 239.84, "end": 244.68, "text": " You also have these things for room subscriptions. They are kind of like the room lists, but", "tokens": [509, 611, 362, 613, 721, 337, 1808, 44951, 13, 814, 366, 733, 295, 411, 264, 1808, 14511, 11, 457], "temperature": 0.0, "avg_logprob": -0.17409183084964752, "compression_ratio": 1.6727941176470589, "no_speech_prob": 8.450410678051412e-05}, {"id": 45, "seek": 23396, "start": 244.68, "end": 250.04000000000002, "text": " this is when you know the specific rim ID. So, if you follow a permalink, which may include", "tokens": [341, 307, 562, 291, 458, 264, 2685, 15982, 7348, 13, 407, 11, 498, 291, 1524, 257, 4784, 304, 475, 11, 597, 815, 4090], "temperature": 0.0, "avg_logprob": -0.17409183084964752, "compression_ratio": 1.6727941176470589, "no_speech_prob": 8.450410678051412e-05}, {"id": 46, "seek": 23396, "start": 250.04000000000002, "end": 255.4, "text": " the rim ID, or if you, you know, if you refresh the page and you know that, you know, this", "tokens": [264, 15982, 7348, 11, 420, 498, 291, 11, 291, 458, 11, 498, 291, 15134, 264, 3028, 293, 291, 458, 300, 11, 291, 458, 11, 341], "temperature": 0.0, "avg_logprob": -0.17409183084964752, "compression_ratio": 1.6727941176470589, "no_speech_prob": 8.450410678051412e-05}, {"id": 47, "seek": 23396, "start": 255.4, "end": 260.64, "text": " person was currently viewing this room, that room may not be in this list, right? So, you", "tokens": [954, 390, 4362, 17480, 341, 1808, 11, 300, 1808, 815, 406, 312, 294, 341, 1329, 11, 558, 30, 407, 11, 291], "temperature": 0.0, "avg_logprob": -0.17409183084964752, "compression_ratio": 1.6727941176470589, "no_speech_prob": 8.450410678051412e-05}, {"id": 48, "seek": 26064, "start": 260.64, "end": 264.91999999999996, "text": " would need to subscribe to that room directly because you know the rim ID. And typically,", "tokens": [576, 643, 281, 3022, 281, 300, 1808, 3838, 570, 291, 458, 264, 15982, 7348, 13, 400, 5850, 11], "temperature": 0.0, "avg_logprob": -0.1362452593716708, "compression_ratio": 1.6870229007633588, "no_speech_prob": 1.4964090951252729e-05}, {"id": 49, "seek": 26064, "start": 264.91999999999996, "end": 270.08, "text": " as well, the kinds of information you want here is different. So, in here, we are requesting", "tokens": [382, 731, 11, 264, 3685, 295, 1589, 291, 528, 510, 307, 819, 13, 407, 11, 294, 510, 11, 321, 366, 31937], "temperature": 0.0, "avg_logprob": -0.1362452593716708, "compression_ratio": 1.6870229007633588, "no_speech_prob": 1.4964090951252729e-05}, {"id": 50, "seek": 26064, "start": 270.08, "end": 275.0, "text": " all the state in the room and a much higher timeline element because this is being used", "tokens": [439, 264, 1785, 294, 264, 1808, 293, 257, 709, 2946, 12933, 4478, 570, 341, 307, 885, 1143], "temperature": 0.0, "avg_logprob": -0.1362452593716708, "compression_ratio": 1.6870229007633588, "no_speech_prob": 1.4964090951252729e-05}, {"id": 51, "seek": 26064, "start": 275.0, "end": 281.71999999999997, "text": " to populate the actual full room view. The response is very similar, as well. So, you", "tokens": [281, 1665, 5256, 264, 3539, 1577, 1808, 1910, 13, 440, 4134, 307, 588, 2531, 11, 382, 731, 13, 407, 11, 291], "temperature": 0.0, "avg_logprob": -0.1362452593716708, "compression_ratio": 1.6870229007633588, "no_speech_prob": 1.4964090951252729e-05}, {"id": 52, "seek": 26064, "start": 281.71999999999997, "end": 286.2, "text": " have a list object here, and then you get a list of rim IDs that are used to populate", "tokens": [362, 257, 1329, 2657, 510, 11, 293, 550, 291, 483, 257, 1329, 295, 15982, 48212, 300, 366, 1143, 281, 1665, 5256], "temperature": 0.0, "avg_logprob": -0.1362452593716708, "compression_ratio": 1.6870229007633588, "no_speech_prob": 1.4964090951252729e-05}, {"id": 53, "seek": 28620, "start": 286.2, "end": 292.15999999999997, "text": " the correct ordering here. And then you also have a top-level rims array, a rims object,", "tokens": [264, 3006, 21739, 510, 13, 400, 550, 291, 611, 362, 257, 1192, 12, 12418, 15982, 82, 10225, 11, 257, 15982, 82, 2657, 11], "temperature": 0.0, "avg_logprob": -0.18862005139960616, "compression_ratio": 1.6654135338345866, "no_speech_prob": 1.6522590158274397e-05}, {"id": 54, "seek": 28620, "start": 292.15999999999997, "end": 295.8, "text": " and then that's just a key value map, where the keys are the rim IDs, and then the values", "tokens": [293, 550, 300, 311, 445, 257, 2141, 2158, 4471, 11, 689, 264, 9317, 366, 264, 15982, 48212, 11, 293, 550, 264, 4190], "temperature": 0.0, "avg_logprob": -0.18862005139960616, "compression_ratio": 1.6654135338345866, "no_speech_prob": 1.6522590158274397e-05}, {"id": 55, "seek": 28620, "start": 295.8, "end": 302.48, "text": " are all the data that you requested. So, these all get aggregated together, which I will", "tokens": [366, 439, 264, 1412, 300, 291, 16436, 13, 407, 11, 613, 439, 483, 16743, 770, 1214, 11, 597, 286, 486], "temperature": 0.0, "avg_logprob": -0.18862005139960616, "compression_ratio": 1.6654135338345866, "no_speech_prob": 1.6522590158274397e-05}, {"id": 56, "seek": 28620, "start": 302.48, "end": 308.0, "text": " speak about a bit later. In terms of what's new, so if you followed SidingSync, then you", "tokens": [1710, 466, 257, 857, 1780, 13, 682, 2115, 295, 437, 311, 777, 11, 370, 498, 291, 6263, 318, 2819, 50, 34015, 11, 550, 291], "temperature": 0.0, "avg_logprob": -0.18862005139960616, "compression_ratio": 1.6654135338345866, "no_speech_prob": 1.6522590158274397e-05}, {"id": 57, "seek": 28620, "start": 308.0, "end": 312.48, "text": " might be like, okay, I know all this, but what's actually happened over the past year?", "tokens": [1062, 312, 411, 11, 1392, 11, 286, 458, 439, 341, 11, 457, 437, 311, 767, 2011, 670, 264, 1791, 1064, 30], "temperature": 0.0, "avg_logprob": -0.18862005139960616, "compression_ratio": 1.6654135338345866, "no_speech_prob": 1.6522590158274397e-05}, {"id": 58, "seek": 31248, "start": 312.48, "end": 316.88, "text": " We have clients now that run SidingSync. So, this is from Element Web. It's got a nice", "tokens": [492, 362, 6982, 586, 300, 1190, 318, 2819, 50, 34015, 13, 407, 11, 341, 307, 490, 20900, 9573, 13, 467, 311, 658, 257, 1481], "temperature": 0.0, "avg_logprob": -0.15609382813976658, "compression_ratio": 1.6117216117216118, "no_speech_prob": 2.3739576135994866e-05}, {"id": 59, "seek": 31248, "start": 316.88, "end": 321.44, "text": " scary warning there. So, you know, it's great. It all works on Web, but also it actually", "tokens": [6958, 9164, 456, 13, 407, 11, 291, 458, 11, 309, 311, 869, 13, 467, 439, 1985, 322, 9573, 11, 457, 611, 309, 767], "temperature": 0.0, "avg_logprob": -0.15609382813976658, "compression_ratio": 1.6117216117216118, "no_speech_prob": 2.3739576135994866e-05}, {"id": 60, "seek": 31248, "start": 321.44, "end": 326.72, "text": " works on mobile devices, as well, thanks to the Rust SDK, which I'll leave for Ben to", "tokens": [1985, 322, 6013, 5759, 11, 382, 731, 11, 3231, 281, 264, 34952, 37135, 11, 597, 286, 603, 1856, 337, 3964, 281], "temperature": 0.0, "avg_logprob": -0.15609382813976658, "compression_ratio": 1.6117216117216118, "no_speech_prob": 2.3739576135994866e-05}, {"id": 61, "seek": 31248, "start": 326.72, "end": 333.12, "text": " talk about. So, there's also a whole bunch of new extension MSCs. So, extension MSCs", "tokens": [751, 466, 13, 407, 11, 456, 311, 611, 257, 1379, 3840, 295, 777, 10320, 7395, 33290, 13, 407, 11, 10320, 7395, 33290], "temperature": 0.0, "avg_logprob": -0.15609382813976658, "compression_ratio": 1.6117216117216118, "no_speech_prob": 2.3739576135994866e-05}, {"id": 62, "seek": 31248, "start": 333.12, "end": 339.08000000000004, "text": " are an idea of trying to break up the complexity of SidingSync because the sync API is by far", "tokens": [366, 364, 1558, 295, 1382, 281, 1821, 493, 264, 14024, 295, 318, 2819, 50, 34015, 570, 264, 20271, 9362, 307, 538, 1400], "temperature": 0.0, "avg_logprob": -0.15609382813976658, "compression_ratio": 1.6117216117216118, "no_speech_prob": 2.3739576135994866e-05}, {"id": 63, "seek": 33908, "start": 339.08, "end": 343.71999999999997, "text": " one of the most, or not the most complicated part of the client's server API, and trying", "tokens": [472, 295, 264, 881, 11, 420, 406, 264, 881, 6179, 644, 295, 264, 6423, 311, 7154, 9362, 11, 293, 1382], "temperature": 0.0, "avg_logprob": -0.15305277926862731, "compression_ratio": 1.779527559055118, "no_speech_prob": 5.409900768427178e-05}, {"id": 64, "seek": 33908, "start": 343.71999999999997, "end": 348.2, "text": " to put everything into one MSC is going to be doomed to failure. So, we're trying to", "tokens": [281, 829, 1203, 666, 472, 7395, 34, 307, 516, 281, 312, 33847, 281, 7763, 13, 407, 11, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.15305277926862731, "compression_ratio": 1.779527559055118, "no_speech_prob": 5.409900768427178e-05}, {"id": 65, "seek": 33908, "start": 348.2, "end": 355.03999999999996, "text": " specify a core part of the MSC, a core part of what is syncing, which is the syncing rims,", "tokens": [16500, 257, 4965, 644, 295, 264, 7395, 34, 11, 257, 4965, 644, 295, 437, 307, 5451, 2175, 11, 597, 307, 264, 5451, 2175, 15982, 82, 11], "temperature": 0.0, "avg_logprob": -0.15305277926862731, "compression_ratio": 1.779527559055118, "no_speech_prob": 5.409900768427178e-05}, {"id": 66, "seek": 33908, "start": 355.03999999999996, "end": 360.36, "text": " and working out the sorts and the filter arguments, and then we're leaving to extensions all the", "tokens": [293, 1364, 484, 264, 7527, 293, 264, 6608, 12869, 11, 293, 550, 321, 434, 5012, 281, 25129, 439, 264], "temperature": 0.0, "avg_logprob": -0.15305277926862731, "compression_ratio": 1.779527559055118, "no_speech_prob": 5.409900768427178e-05}, {"id": 67, "seek": 33908, "start": 360.36, "end": 365.32, "text": " extra stuff on top. And the idea is that you can opt into any of these things. So, if your", "tokens": [2857, 1507, 322, 1192, 13, 400, 264, 1558, 307, 300, 291, 393, 2427, 666, 604, 295, 613, 721, 13, 407, 11, 498, 428], "temperature": 0.0, "avg_logprob": -0.15305277926862731, "compression_ratio": 1.779527559055118, "no_speech_prob": 5.409900768427178e-05}, {"id": 68, "seek": 36532, "start": 365.32, "end": 369.92, "text": " client doesn't do receipts, then great. Don't subscribe to receipts. Don't even enable this", "tokens": [6423, 1177, 380, 360, 2268, 48908, 11, 550, 869, 13, 1468, 380, 3022, 281, 2268, 48908, 13, 1468, 380, 754, 9528, 341], "temperature": 0.0, "avg_logprob": -0.2201824528830392, "compression_ratio": 1.7569721115537849, "no_speech_prob": 7.049628038657829e-05}, {"id": 69, "seek": 36532, "start": 369.92, "end": 375.68, "text": " extension. Briefly, how these extensions work. So, these two extensions go together because", "tokens": [10320, 13, 39805, 356, 11, 577, 613, 25129, 589, 13, 407, 11, 613, 732, 25129, 352, 1214, 570], "temperature": 0.0, "avg_logprob": -0.2201824528830392, "compression_ratio": 1.7569721115537849, "no_speech_prob": 7.049628038657829e-05}, {"id": 70, "seek": 36532, "start": 375.68, "end": 380.04, "text": " they're ultimately used to make encryption work in encrypted rooms. So, you can see,", "tokens": [436, 434, 6284, 1143, 281, 652, 29575, 589, 294, 36663, 9396, 13, 407, 11, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.2201824528830392, "compression_ratio": 1.7569721115537849, "no_speech_prob": 7.049628038657829e-05}, {"id": 71, "seek": 36532, "start": 380.04, "end": 385.56, "text": " or actually you can't see at all. Here is an encrypted event. So, there's basically,", "tokens": [420, 767, 291, 393, 380, 536, 412, 439, 13, 1692, 307, 364, 36663, 2280, 13, 407, 11, 456, 311, 1936, 11], "temperature": 0.0, "avg_logprob": -0.2201824528830392, "compression_ratio": 1.7569721115537849, "no_speech_prob": 7.049628038657829e-05}, {"id": 72, "seek": 36532, "start": 385.56, "end": 390.28, "text": " you have to trust me, there's a ciphertext here with lots of gibberish effectively, and", "tokens": [291, 362, 281, 3361, 385, 11, 456, 311, 257, 269, 21240, 25111, 510, 365, 3195, 295, 4553, 43189, 8659, 11, 293], "temperature": 0.0, "avg_logprob": -0.2201824528830392, "compression_ratio": 1.7569721115537849, "no_speech_prob": 7.049628038657829e-05}, {"id": 73, "seek": 39028, "start": 390.28, "end": 398.15999999999997, "text": " then you need to run keys to go decrypt it into your normal text. The way that works", "tokens": [550, 291, 643, 281, 1190, 9317, 281, 352, 979, 627, 662, 309, 666, 428, 2710, 2487, 13, 440, 636, 300, 1985], "temperature": 0.0, "avg_logprob": -0.1508011923895942, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9126782717648894e-05}, {"id": 74, "seek": 39028, "start": 398.15999999999997, "end": 404.67999999999995, "text": " is that you need to get keys via your two device messages. That's why they go together.", "tokens": [307, 300, 291, 643, 281, 483, 9317, 5766, 428, 732, 4302, 7897, 13, 663, 311, 983, 436, 352, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1508011923895942, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9126782717648894e-05}, {"id": 75, "seek": 39028, "start": 404.67999999999995, "end": 411.23999999999995, "text": " The other thing here is that it implements another MSC called dropping-sale-centered", "tokens": [440, 661, 551, 510, 307, 300, 309, 704, 17988, 1071, 7395, 34, 1219, 13601, 12, 82, 1220, 12, 36814], "temperature": 0.0, "avg_logprob": -0.1508011923895942, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9126782717648894e-05}, {"id": 76, "seek": 39028, "start": 411.23999999999995, "end": 415.64, "text": " device messages. You can barely see it on here, but this is an output from Postgres,", "tokens": [4302, 7897, 13, 509, 393, 10268, 536, 309, 322, 510, 11, 457, 341, 307, 364, 5598, 490, 10223, 45189, 11], "temperature": 0.0, "avg_logprob": -0.1508011923895942, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9126782717648894e-05}, {"id": 77, "seek": 41564, "start": 415.64, "end": 421.32, "text": " which is trying to work out how many unread or unacknowledged two device events are there", "tokens": [597, 307, 1382, 281, 589, 484, 577, 867, 517, 2538, 420, 517, 501, 3785, 1493, 3004, 732, 4302, 3931, 366, 456], "temperature": 0.0, "avg_logprob": -0.1169915685848314, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0001031919164233841}, {"id": 78, "seek": 41564, "start": 421.32, "end": 427.84, "text": " for a given user device. And you might think that might be, say, 100, maybe 1,000 tops.", "tokens": [337, 257, 2212, 4195, 4302, 13, 400, 291, 1062, 519, 300, 1062, 312, 11, 584, 11, 2319, 11, 1310, 502, 11, 1360, 22836, 13], "temperature": 0.0, "avg_logprob": -0.1169915685848314, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0001031919164233841}, {"id": 79, "seek": 41564, "start": 427.84, "end": 434.36, "text": " It turns out this can be a lot. This is several hundred thousand unread or unacknowledged two", "tokens": [467, 4523, 484, 341, 393, 312, 257, 688, 13, 639, 307, 2940, 3262, 4714, 517, 2538, 420, 517, 501, 3785, 1493, 3004, 732], "temperature": 0.0, "avg_logprob": -0.1169915685848314, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0001031919164233841}, {"id": 80, "seek": 41564, "start": 434.36, "end": 439.68, "text": " device events. And it turns out when I analyzed a lot of this, this was almost entirely down", "tokens": [4302, 3931, 13, 400, 309, 4523, 484, 562, 286, 28181, 257, 688, 295, 341, 11, 341, 390, 1920, 7696, 760], "temperature": 0.0, "avg_logprob": -0.1169915685848314, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0001031919164233841}, {"id": 81, "seek": 43968, "start": 439.68, "end": 448.72, "text": " to RIM keys being requested, and then either canceled or successfully sent. So, this MSC", "tokens": [281, 497, 6324, 9317, 885, 16436, 11, 293, 550, 2139, 24839, 420, 10727, 2279, 13, 407, 11, 341, 7395, 34], "temperature": 0.0, "avg_logprob": -0.16140011624173, "compression_ratio": 1.5935251798561152, "no_speech_prob": 6.541042239405215e-05}, {"id": 82, "seek": 43968, "start": 448.72, "end": 454.32, "text": " 3944 basically says, hey, if you request a RIM key and then you subsequently cancel", "tokens": [15238, 13912, 1936, 1619, 11, 4177, 11, 498, 291, 5308, 257, 497, 6324, 2141, 293, 550, 291, 26514, 10373], "temperature": 0.0, "avg_logprob": -0.16140011624173, "compression_ratio": 1.5935251798561152, "no_speech_prob": 6.541042239405215e-05}, {"id": 83, "seek": 43968, "start": 454.32, "end": 459.4, "text": " that request, you're going to go and delete those two device messages, so they don't just", "tokens": [300, 5308, 11, 291, 434, 516, 281, 352, 293, 12097, 729, 732, 4302, 7897, 11, 370, 436, 500, 380, 445], "temperature": 0.0, "avg_logprob": -0.16140011624173, "compression_ratio": 1.5935251798561152, "no_speech_prob": 6.541042239405215e-05}, {"id": 84, "seek": 43968, "start": 459.4, "end": 463.8, "text": " keep stacking up in this way. And that obviously really helps reduce the amount of bandwidth", "tokens": [1066, 41376, 493, 294, 341, 636, 13, 400, 300, 2745, 534, 3665, 5407, 264, 2372, 295, 23647], "temperature": 0.0, "avg_logprob": -0.16140011624173, "compression_ratio": 1.5935251798561152, "no_speech_prob": 6.541042239405215e-05}, {"id": 85, "seek": 43968, "start": 463.8, "end": 469.6, "text": " for SignSync as well. The other thing we've got is account data. So, if you wonder what", "tokens": [337, 13515, 50, 34015, 382, 731, 13, 440, 661, 551, 321, 600, 658, 307, 2696, 1412, 13, 407, 11, 498, 291, 2441, 437], "temperature": 0.0, "avg_logprob": -0.16140011624173, "compression_ratio": 1.5935251798561152, "no_speech_prob": 6.541042239405215e-05}, {"id": 86, "seek": 46960, "start": 469.6, "end": 474.32000000000005, "text": " account data does, if you've ever used the breadcrumbs thing at the top here on Element", "tokens": [2696, 1412, 775, 11, 498, 291, 600, 1562, 1143, 264, 5961, 66, 6247, 929, 551, 412, 264, 1192, 510, 322, 20900], "temperature": 0.0, "avg_logprob": -0.16756009609899788, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.1758776711067185e-05}, {"id": 87, "seek": 46960, "start": 474.32000000000005, "end": 479.36, "text": " Web, that's synchronized seasoning account data. Also, account data is really, really", "tokens": [9573, 11, 300, 311, 19331, 1602, 23421, 2696, 1412, 13, 2743, 11, 2696, 1412, 307, 534, 11, 534], "temperature": 0.0, "avg_logprob": -0.16756009609899788, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.1758776711067185e-05}, {"id": 88, "seek": 46960, "start": 479.36, "end": 485.84000000000003, "text": " useful for working out accurate notification counts. So, at the bottom here, you can just", "tokens": [4420, 337, 1364, 484, 8559, 11554, 14893, 13, 407, 11, 412, 264, 2767, 510, 11, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.16756009609899788, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.1758776711067185e-05}, {"id": 89, "seek": 46960, "start": 485.84000000000003, "end": 490.40000000000003, "text": " about see that you've got some messages here. You've got a message and a timeline. This", "tokens": [466, 536, 300, 291, 600, 658, 512, 7897, 510, 13, 509, 600, 658, 257, 3636, 293, 257, 12933, 13, 639], "temperature": 0.0, "avg_logprob": -0.16756009609899788, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.1758776711067185e-05}, {"id": 90, "seek": 46960, "start": 490.40000000000003, "end": 495.36, "text": " is encrypted. And it says here, notification count one. Notification counts are the gray", "tokens": [307, 36663, 13, 400, 309, 1619, 510, 11, 11554, 1207, 472, 13, 1726, 3774, 14893, 366, 264, 10855], "temperature": 0.0, "avg_logprob": -0.16756009609899788, "compression_ratio": 1.752988047808765, "no_speech_prob": 2.1758776711067185e-05}, {"id": 91, "seek": 49536, "start": 495.36, "end": 499.96000000000004, "text": " numbers, and you've got a highlight count of zero, which is the red number. And yet,", "tokens": [3547, 11, 293, 291, 600, 658, 257, 5078, 1207, 295, 4018, 11, 597, 307, 264, 2182, 1230, 13, 400, 1939, 11], "temperature": 0.0, "avg_logprob": -0.13385221053814067, "compression_ratio": 1.7877551020408162, "no_speech_prob": 1.5354024071712047e-05}, {"id": 92, "seek": 49536, "start": 499.96000000000004, "end": 505.64, "text": " on the UI, you can see that it's a red number and it's gone to one. So, something's happened", "tokens": [322, 264, 15682, 11, 291, 393, 536, 300, 309, 311, 257, 2182, 1230, 293, 309, 311, 2780, 281, 472, 13, 407, 11, 746, 311, 2011], "temperature": 0.0, "avg_logprob": -0.13385221053814067, "compression_ratio": 1.7877551020408162, "no_speech_prob": 1.5354024071712047e-05}, {"id": 93, "seek": 49536, "start": 505.64, "end": 512.36, "text": " here where the client has worked out that, oh, I should use this as a highlight count,", "tokens": [510, 689, 264, 6423, 575, 2732, 484, 300, 11, 1954, 11, 286, 820, 764, 341, 382, 257, 5078, 1207, 11], "temperature": 0.0, "avg_logprob": -0.13385221053814067, "compression_ratio": 1.7877551020408162, "no_speech_prob": 1.5354024071712047e-05}, {"id": 94, "seek": 49536, "start": 512.36, "end": 517.04, "text": " not a notification count. It's overwritten what the service told it. And what's happened", "tokens": [406, 257, 11554, 1207, 13, 467, 311, 670, 26859, 437, 264, 2643, 1907, 309, 13, 400, 437, 311, 2011], "temperature": 0.0, "avg_logprob": -0.13385221053814067, "compression_ratio": 1.7877551020408162, "no_speech_prob": 1.5354024071712047e-05}, {"id": 95, "seek": 49536, "start": 517.04, "end": 521.5600000000001, "text": " here is that the client has decrypted the message, and then it's checked the message", "tokens": [510, 307, 300, 264, 6423, 575, 979, 627, 25383, 264, 3636, 11, 293, 550, 309, 311, 10033, 264, 3636], "temperature": 0.0, "avg_logprob": -0.13385221053814067, "compression_ratio": 1.7877551020408162, "no_speech_prob": 1.5354024071712047e-05}, {"id": 96, "seek": 52156, "start": 521.56, "end": 528.16, "text": " to say, hey, you know, is there any app mention or any specific keywords based on your push", "tokens": [281, 584, 11, 4177, 11, 291, 458, 11, 307, 456, 604, 724, 2152, 420, 604, 2685, 21009, 2361, 322, 428, 2944], "temperature": 0.0, "avg_logprob": -0.1913138230641683, "compression_ratio": 1.5991189427312775, "no_speech_prob": 9.232477168552577e-06}, {"id": 97, "seek": 52156, "start": 528.16, "end": 533.4799999999999, "text": " rules? And if that is true, then it knows, ah, okay, I need to actually make this a red", "tokens": [4474, 30, 400, 498, 300, 307, 2074, 11, 550, 309, 3255, 11, 3716, 11, 1392, 11, 286, 643, 281, 767, 652, 341, 257, 2182], "temperature": 0.0, "avg_logprob": -0.1913138230641683, "compression_ratio": 1.5991189427312775, "no_speech_prob": 9.232477168552577e-06}, {"id": 98, "seek": 52156, "start": 533.4799999999999, "end": 539.28, "text": " highlight rather than just a normal gray and red count. And that's done using push rules.", "tokens": [5078, 2831, 813, 445, 257, 2710, 10855, 293, 2182, 1207, 13, 400, 300, 311, 1096, 1228, 2944, 4474, 13], "temperature": 0.0, "avg_logprob": -0.1913138230641683, "compression_ratio": 1.5991189427312775, "no_speech_prob": 9.232477168552577e-06}, {"id": 99, "seek": 52156, "start": 539.28, "end": 546.16, "text": " And push rules is done in, stored as an account data. Final two ones are receipts and typing.", "tokens": [400, 2944, 4474, 307, 1096, 294, 11, 12187, 382, 364, 2696, 1412, 13, 13443, 732, 2306, 366, 2268, 48908, 293, 18444, 13], "temperature": 0.0, "avg_logprob": -0.1913138230641683, "compression_ratio": 1.5991189427312775, "no_speech_prob": 9.232477168552577e-06}, {"id": 100, "seek": 54616, "start": 546.16, "end": 553.4399999999999, "text": " Thank you. So, hopefully, you know what receipts and typing notifications are. The main changes", "tokens": [1044, 291, 13, 407, 11, 4696, 11, 291, 458, 437, 2268, 48908, 293, 18444, 13426, 366, 13, 440, 2135, 2962], "temperature": 0.0, "avg_logprob": -0.1449939707914988, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.818386150873266e-05}, {"id": 101, "seek": 54616, "start": 553.4399999999999, "end": 560.1999999999999, "text": " for Sliding Sync is that the receipts are lazily loaded. So, you might think, what does", "tokens": [337, 6187, 2819, 26155, 66, 307, 300, 264, 2268, 48908, 366, 19320, 953, 13210, 13, 407, 11, 291, 1062, 519, 11, 437, 775], "temperature": 0.0, "avg_logprob": -0.1449939707914988, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.818386150873266e-05}, {"id": 102, "seek": 54616, "start": 560.1999999999999, "end": 565.4399999999999, "text": " that mean exactly? Well, if you request a timeline limit of 10, then you will get those", "tokens": [300, 914, 2293, 30, 1042, 11, 498, 291, 5308, 257, 12933, 4948, 295, 1266, 11, 550, 291, 486, 483, 729], "temperature": 0.0, "avg_logprob": -0.1449939707914988, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.818386150873266e-05}, {"id": 103, "seek": 54616, "start": 565.4399999999999, "end": 571.64, "text": " 10 events, and then you will get the receipts for those 10 events, and you won't get receipts", "tokens": [1266, 3931, 11, 293, 550, 291, 486, 483, 264, 2268, 48908, 337, 729, 1266, 3931, 11, 293, 291, 1582, 380, 483, 2268, 48908], "temperature": 0.0, "avg_logprob": -0.1449939707914988, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.818386150873266e-05}, {"id": 104, "seek": 57164, "start": 571.64, "end": 577.0, "text": " for any other events. And you might think, shit, hasn't it always done this? Well, not", "tokens": [337, 604, 661, 3931, 13, 400, 291, 1062, 519, 11, 4611, 11, 6132, 380, 309, 1009, 1096, 341, 30, 1042, 11, 406], "temperature": 0.0, "avg_logprob": -0.16022701411284218, "compression_ratio": 1.6441947565543071, "no_speech_prob": 4.325801637605764e-05}, {"id": 105, "seek": 57164, "start": 577.0, "end": 581.96, "text": " really. So, here's some JQ for Matrix HQ, which is this room ID, and it's just pulling", "tokens": [534, 13, 407, 11, 510, 311, 512, 508, 48, 337, 36274, 43209, 11, 597, 307, 341, 1808, 7348, 11, 293, 309, 311, 445, 8407], "temperature": 0.0, "avg_logprob": -0.16022701411284218, "compression_ratio": 1.6441947565543071, "no_speech_prob": 4.325801637605764e-05}, {"id": 106, "seek": 57164, "start": 581.96, "end": 588.4, "text": " out the receipt EDU, and then kind of checking, like, roughly how many receipts there are.", "tokens": [484, 264, 33882, 18050, 52, 11, 293, 550, 733, 295, 8568, 11, 411, 11, 9810, 577, 867, 2268, 48908, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.16022701411284218, "compression_ratio": 1.6441947565543071, "no_speech_prob": 4.325801637605764e-05}, {"id": 107, "seek": 57164, "start": 588.4, "end": 593.12, "text": " And, you know, Matrix HQ is quite a big room, so you might think, you know, 100,000. No,", "tokens": [400, 11, 291, 458, 11, 36274, 43209, 307, 1596, 257, 955, 1808, 11, 370, 291, 1062, 519, 11, 291, 458, 11, 2319, 11, 1360, 13, 883, 11], "temperature": 0.0, "avg_logprob": -0.16022701411284218, "compression_ratio": 1.6441947565543071, "no_speech_prob": 4.325801637605764e-05}, {"id": 108, "seek": 57164, "start": 593.12, "end": 597.84, "text": " there's quite a lot of rooms, quite a lot of receipts in there. And this is not great", "tokens": [456, 311, 1596, 257, 688, 295, 9396, 11, 1596, 257, 688, 295, 2268, 48908, 294, 456, 13, 400, 341, 307, 406, 869], "temperature": 0.0, "avg_logprob": -0.16022701411284218, "compression_ratio": 1.6441947565543071, "no_speech_prob": 4.325801637605764e-05}, {"id": 109, "seek": 59784, "start": 597.84, "end": 603.6800000000001, "text": " from a bandwidth perspective, right? We don't want to be sending 53,000 read receipts, particularly", "tokens": [490, 257, 23647, 4585, 11, 558, 30, 492, 500, 380, 528, 281, 312, 7750, 21860, 11, 1360, 1401, 2268, 48908, 11, 4098], "temperature": 0.0, "avg_logprob": -0.12879938774920524, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.206766167655587e-05}, {"id": 110, "seek": 59784, "start": 603.6800000000001, "end": 609.0400000000001, "text": " for events which you are unlikely to ever view, right? Because these could be for events", "tokens": [337, 3931, 597, 291, 366, 17518, 281, 1562, 1910, 11, 558, 30, 1436, 613, 727, 312, 337, 3931], "temperature": 0.0, "avg_logprob": -0.12879938774920524, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.206766167655587e-05}, {"id": 111, "seek": 59784, "start": 609.0400000000001, "end": 615.8000000000001, "text": " that occurred, like, a year ago. So, Sliding Sync also fixes that. So, with all these performance", "tokens": [300, 11068, 11, 411, 11, 257, 1064, 2057, 13, 407, 11, 6187, 2819, 26155, 66, 611, 32539, 300, 13, 407, 11, 365, 439, 613, 3389], "temperature": 0.0, "avg_logprob": -0.12879938774920524, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.206766167655587e-05}, {"id": 112, "seek": 59784, "start": 615.8000000000001, "end": 623.8000000000001, "text": " optimizations, a very large account with 4,000 rooms can take less than a second to", "tokens": [5028, 14455, 11, 257, 588, 2416, 2696, 365, 1017, 11, 1360, 9396, 393, 747, 1570, 813, 257, 1150, 281], "temperature": 0.0, "avg_logprob": -0.12879938774920524, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.206766167655587e-05}, {"id": 113, "seek": 62380, "start": 623.8, "end": 629.12, "text": " actually sync, which is down from 15 minutes on Sync V2. So, very happy with that, but", "tokens": [767, 20271, 11, 597, 307, 760, 490, 2119, 2077, 322, 26155, 66, 691, 17, 13, 407, 11, 588, 2055, 365, 300, 11, 457], "temperature": 0.0, "avg_logprob": -0.14976604779561362, "compression_ratio": 1.5863309352517985, "no_speech_prob": 5.564286402659491e-05}, {"id": 114, "seek": 62380, "start": 629.12, "end": 632.4399999999999, "text": " it's still not really good enough. We're, you know, we're trying to go big or go home", "tokens": [309, 311, 920, 406, 534, 665, 1547, 13, 492, 434, 11, 291, 458, 11, 321, 434, 1382, 281, 352, 955, 420, 352, 1280], "temperature": 0.0, "avg_logprob": -0.14976604779561362, "compression_ratio": 1.5863309352517985, "no_speech_prob": 5.564286402659491e-05}, {"id": 115, "seek": 62380, "start": 632.4399999999999, "end": 639.3599999999999, "text": " kind of thing, so we want to make it even faster, so it is literally instant. You don't", "tokens": [733, 295, 551, 11, 370, 321, 528, 281, 652, 309, 754, 4663, 11, 370, 309, 307, 3736, 9836, 13, 509, 500, 380], "temperature": 0.0, "avg_logprob": -0.14976604779561362, "compression_ratio": 1.5863309352517985, "no_speech_prob": 5.564286402659491e-05}, {"id": 116, "seek": 62380, "start": 639.3599999999999, "end": 643.3599999999999, "text": " want to have to be waiting a couple of seconds. It should just kind of open up, just like", "tokens": [528, 281, 362, 281, 312, 3806, 257, 1916, 295, 3949, 13, 467, 820, 445, 733, 295, 1269, 493, 11, 445, 411], "temperature": 0.0, "avg_logprob": -0.14976604779561362, "compression_ratio": 1.5863309352517985, "no_speech_prob": 5.564286402659491e-05}, {"id": 117, "seek": 62380, "start": 643.3599999999999, "end": 648.52, "text": " most other messaging clients, so you can just open them up and they just work. The problem", "tokens": [881, 661, 21812, 6982, 11, 370, 291, 393, 445, 1269, 552, 493, 293, 436, 445, 589, 13, 440, 1154], "temperature": 0.0, "avg_logprob": -0.14976604779561362, "compression_ratio": 1.5863309352517985, "no_speech_prob": 5.564286402659491e-05}, {"id": 118, "seek": 64852, "start": 648.52, "end": 654.3199999999999, "text": " is that things are going to get a lot worse here, which I will talk about in a moment.", "tokens": [307, 300, 721, 366, 516, 281, 483, 257, 688, 5324, 510, 11, 597, 286, 486, 751, 466, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.18037195408597906, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.4923699382052291e-05}, {"id": 119, "seek": 64852, "start": 654.3199999999999, "end": 663.88, "text": " So, we've added in a bunch of tracing to the proxy server. So, things like, this is runtime", "tokens": [407, 11, 321, 600, 3869, 294, 257, 3840, 295, 25262, 281, 264, 29690, 7154, 13, 407, 11, 721, 411, 11, 341, 307, 34474], "temperature": 0.0, "avg_logprob": -0.18037195408597906, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.4923699382052291e-05}, {"id": 120, "seek": 64852, "start": 663.88, "end": 667.84, "text": " trace. So, you can see exactly the control flow. There's some spans there, and you can", "tokens": [13508, 13, 407, 11, 291, 393, 536, 2293, 264, 1969, 3095, 13, 821, 311, 512, 44086, 456, 11, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.18037195408597906, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.4923699382052291e-05}, {"id": 121, "seek": 64852, "start": 667.84, "end": 675.28, "text": " see various optimizations that were done. So, this is identifying so bits of code. Lots", "tokens": [536, 3683, 5028, 14455, 300, 645, 1096, 13, 407, 11, 341, 307, 16696, 370, 9239, 295, 3089, 13, 15908], "temperature": 0.0, "avg_logprob": -0.18037195408597906, "compression_ratio": 1.5972850678733033, "no_speech_prob": 1.4923699382052291e-05}, {"id": 122, "seek": 67528, "start": 675.28, "end": 679.3199999999999, "text": " and lots and lots of commits. Sometimes it's just, you forgot to add an index. Sometimes", "tokens": [293, 3195, 293, 3195, 295, 48311, 13, 4803, 309, 311, 445, 11, 291, 5298, 281, 909, 364, 8186, 13, 4803], "temperature": 0.0, "avg_logprob": -0.16074645077740704, "compression_ratio": 1.6884615384615385, "no_speech_prob": 2.8247251975699328e-05}, {"id": 123, "seek": 67528, "start": 679.3199999999999, "end": 684.56, "text": " you should be doing things in bulk instead of doing things sequentially. So, lots of", "tokens": [291, 820, 312, 884, 721, 294, 16139, 2602, 295, 884, 721, 5123, 3137, 13, 407, 11, 3195, 295], "temperature": 0.0, "avg_logprob": -0.16074645077740704, "compression_ratio": 1.6884615384615385, "no_speech_prob": 2.8247251975699328e-05}, {"id": 124, "seek": 67528, "start": 684.56, "end": 689.68, "text": " work has gone into this. And also, if you're going for, you know, 100 milliseconds kind", "tokens": [589, 575, 2780, 666, 341, 13, 400, 611, 11, 498, 291, 434, 516, 337, 11, 291, 458, 11, 2319, 34184, 733], "temperature": 0.0, "avg_logprob": -0.16074645077740704, "compression_ratio": 1.6884615384615385, "no_speech_prob": 2.8247251975699328e-05}, {"id": 125, "seek": 67528, "start": 689.68, "end": 694.0, "text": " of, aiming for 100 milliseconds, the actual amount of data you send is important, because", "tokens": [295, 11, 20253, 337, 2319, 34184, 11, 264, 3539, 2372, 295, 1412, 291, 2845, 307, 1021, 11, 570], "temperature": 0.0, "avg_logprob": -0.16074645077740704, "compression_ratio": 1.6884615384615385, "no_speech_prob": 2.8247251975699328e-05}, {"id": 126, "seek": 67528, "start": 694.0, "end": 699.9599999999999, "text": " this starts to become quite a large factor in the total time it takes. We can do simple", "tokens": [341, 3719, 281, 1813, 1596, 257, 2416, 5952, 294, 264, 3217, 565, 309, 2516, 13, 492, 393, 360, 2199], "temperature": 0.0, "avg_logprob": -0.16074645077740704, "compression_ratio": 1.6884615384615385, "no_speech_prob": 2.8247251975699328e-05}, {"id": 127, "seek": 69996, "start": 699.96, "end": 706.96, "text": " things like deduplication and enabling GZIP for large responses, which we now do. And", "tokens": [721, 411, 4172, 84, 4770, 399, 293, 23148, 460, 57, 9139, 337, 2416, 13019, 11, 597, 321, 586, 360, 13, 400], "temperature": 0.0, "avg_logprob": -0.1453000445698583, "compression_ratio": 1.5411255411255411, "no_speech_prob": 2.8079502953914925e-05}, {"id": 128, "seek": 69996, "start": 706.96, "end": 712.52, "text": " as well as that, we can aggressively cache things in memory wherever possible. So, we", "tokens": [382, 731, 382, 300, 11, 321, 393, 32024, 19459, 721, 294, 4675, 8660, 1944, 13, 407, 11, 321], "temperature": 0.0, "avg_logprob": -0.1453000445698583, "compression_ratio": 1.5411255411255411, "no_speech_prob": 2.8079502953914925e-05}, {"id": 129, "seek": 69996, "start": 712.52, "end": 716.64, "text": " don't have to query the database when clients send a request. So, there's three levels", "tokens": [500, 380, 362, 281, 14581, 264, 8149, 562, 6982, 2845, 257, 5308, 13, 407, 11, 456, 311, 1045, 4358], "temperature": 0.0, "avg_logprob": -0.1453000445698583, "compression_ratio": 1.5411255411255411, "no_speech_prob": 2.8079502953914925e-05}, {"id": 130, "seek": 69996, "start": 716.64, "end": 723.44, "text": " of caching involved at the proxy level, whereas a global cache which contains kind of information", "tokens": [295, 269, 2834, 3288, 412, 264, 29690, 1496, 11, 9735, 257, 4338, 19459, 597, 8306, 733, 295, 1589], "temperature": 0.0, "avg_logprob": -0.1453000445698583, "compression_ratio": 1.5411255411255411, "no_speech_prob": 2.8079502953914925e-05}, {"id": 131, "seek": 72344, "start": 723.44, "end": 730.8000000000001, "text": " which doesn't change for any user, so it's a constant. So, things like the number of", "tokens": [597, 1177, 380, 1319, 337, 604, 4195, 11, 370, 309, 311, 257, 5754, 13, 407, 11, 721, 411, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.16345815491258053, "compression_ratio": 1.7991803278688525, "no_speech_prob": 4.044958404847421e-05}, {"id": 132, "seek": 72344, "start": 730.8000000000001, "end": 736.7600000000001, "text": " joined users in a room, it's the same if you're Alice or if you're Bob, it's always going", "tokens": [6869, 5022, 294, 257, 1808, 11, 309, 311, 264, 912, 498, 291, 434, 16004, 420, 498, 291, 434, 6085, 11, 309, 311, 1009, 516], "temperature": 0.0, "avg_logprob": -0.16345815491258053, "compression_ratio": 1.7991803278688525, "no_speech_prob": 4.044958404847421e-05}, {"id": 133, "seek": 72344, "start": 736.7600000000001, "end": 740.84, "text": " to be the same. Whereas things like the user cache or things like, you know, what's the", "tokens": [281, 312, 264, 912, 13, 13813, 721, 411, 264, 4195, 19459, 420, 721, 411, 11, 291, 458, 11, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.16345815491258053, "compression_ratio": 1.7991803278688525, "no_speech_prob": 4.044958404847421e-05}, {"id": 134, "seek": 72344, "start": 740.84, "end": 744.5200000000001, "text": " unread count for this room? Well, that's going to change depending on which user. And then", "tokens": [517, 2538, 1207, 337, 341, 1808, 30, 1042, 11, 300, 311, 516, 281, 1319, 5413, 322, 597, 4195, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.16345815491258053, "compression_ratio": 1.7991803278688525, "no_speech_prob": 4.044958404847421e-05}, {"id": 135, "seek": 72344, "start": 744.5200000000001, "end": 749.5200000000001, "text": " the connections are things like which room subscriptions have you got, or which lists", "tokens": [264, 9271, 366, 721, 411, 597, 1808, 44951, 362, 291, 658, 11, 420, 597, 14511], "temperature": 0.0, "avg_logprob": -0.16345815491258053, "compression_ratio": 1.7991803278688525, "no_speech_prob": 4.044958404847421e-05}, {"id": 136, "seek": 74952, "start": 749.52, "end": 753.84, "text": " like what are your top end rooms or whatever your sliding window is. Interesting thing", "tokens": [411, 437, 366, 428, 1192, 917, 9396, 420, 2035, 428, 21169, 4910, 307, 13, 14711, 551], "temperature": 0.0, "avg_logprob": -0.1535454967565704, "compression_ratio": 1.9047619047619047, "no_speech_prob": 1.947188502526842e-05}, {"id": 137, "seek": 74952, "start": 753.84, "end": 759.56, "text": " to note here is that the room name is actually not global data. The data that's used to", "tokens": [281, 3637, 510, 307, 300, 264, 1808, 1315, 307, 767, 406, 4338, 1412, 13, 440, 1412, 300, 311, 1143, 281], "temperature": 0.0, "avg_logprob": -0.1535454967565704, "compression_ratio": 1.9047619047619047, "no_speech_prob": 1.947188502526842e-05}, {"id": 138, "seek": 74952, "start": 759.56, "end": 765.56, "text": " calculate the room name is global data and is the same for everyone, but the room name", "tokens": [8873, 264, 1808, 1315, 307, 4338, 1412, 293, 307, 264, 912, 337, 1518, 11, 457, 264, 1808, 1315], "temperature": 0.0, "avg_logprob": -0.1535454967565704, "compression_ratio": 1.9047619047619047, "no_speech_prob": 1.947188502526842e-05}, {"id": 139, "seek": 74952, "start": 765.56, "end": 770.3199999999999, "text": " itself isn't because of DMs. So, if you have a DM with Alice and Bob, then from Alice's", "tokens": [2564, 1943, 380, 570, 295, 15322, 82, 13, 407, 11, 498, 291, 362, 257, 15322, 365, 16004, 293, 6085, 11, 550, 490, 16004, 311], "temperature": 0.0, "avg_logprob": -0.1535454967565704, "compression_ratio": 1.9047619047619047, "no_speech_prob": 1.947188502526842e-05}, {"id": 140, "seek": 74952, "start": 770.3199999999999, "end": 774.16, "text": " point of view, the room name is Bob, but from Bob's point of view, the room name is Alice.", "tokens": [935, 295, 1910, 11, 264, 1808, 1315, 307, 6085, 11, 457, 490, 6085, 311, 935, 295, 1910, 11, 264, 1808, 1315, 307, 16004, 13], "temperature": 0.0, "avg_logprob": -0.1535454967565704, "compression_ratio": 1.9047619047619047, "no_speech_prob": 1.947188502526842e-05}, {"id": 141, "seek": 77416, "start": 774.16, "end": 781.92, "text": " So, lots and lots of optimizations have been done. So, with all of this, we're now getting", "tokens": [407, 11, 3195, 293, 3195, 295, 5028, 14455, 362, 668, 1096, 13, 407, 11, 365, 439, 295, 341, 11, 321, 434, 586, 1242], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 142, "seek": 77416, "start": 781.92, "end": 785.6, "text": " less than 100 milliseconds, which is what we wanted, but it's still not good enough", "tokens": [1570, 813, 2319, 34184, 11, 597, 307, 437, 321, 1415, 11, 457, 309, 311, 920, 406, 665, 1547], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 143, "seek": 77416, "start": 785.6, "end": 791.9599999999999, "text": " because things are going to get a lot worse because clients, really, it's all up to the", "tokens": [570, 721, 366, 516, 281, 483, 257, 688, 5324, 570, 6982, 11, 534, 11, 309, 311, 439, 493, 281, 264], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 144, "seek": 77416, "start": 791.9599999999999, "end": 795.7199999999999, "text": " clients because clients want offline support and they want instant access. You know, they", "tokens": [6982, 570, 6982, 528, 21857, 1406, 293, 436, 528, 9836, 2105, 13, 509, 458, 11, 436], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 145, "seek": 77416, "start": 795.7199999999999, "end": 800.16, "text": " don't want to have to be having to do a network request to, you know, when they click on a", "tokens": [500, 380, 528, 281, 362, 281, 312, 1419, 281, 360, 257, 3209, 5308, 281, 11, 291, 458, 11, 562, 436, 2052, 322, 257], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 146, "seek": 77416, "start": 800.16, "end": 803.24, "text": " room, they want to just see the list. They don't want to see a spinner. And in the best", "tokens": [1808, 11, 436, 528, 281, 445, 536, 264, 1329, 13, 814, 500, 380, 528, 281, 536, 257, 44849, 13, 400, 294, 264, 1151], "temperature": 0.0, "avg_logprob": -0.15608396321317575, "compression_ratio": 1.7818791946308725, "no_speech_prob": 3.3818185329437256e-05}, {"id": 147, "seek": 80324, "start": 803.24, "end": 808.44, "text": " case, you have a spinner for half a second, maybe, and then it loads, which is, you know,", "tokens": [1389, 11, 291, 362, 257, 44849, 337, 1922, 257, 1150, 11, 1310, 11, 293, 550, 309, 12668, 11, 597, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 148, "seek": 80324, "start": 808.44, "end": 812.4, "text": " it's not great, but maybe acceptable. But then, if you're on a mobile app and you go", "tokens": [309, 311, 406, 869, 11, 457, 1310, 15513, 13, 583, 550, 11, 498, 291, 434, 322, 257, 6013, 724, 293, 291, 352], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 149, "seek": 80324, "start": 812.4, "end": 817.76, "text": " into a tunnel, then it's just going to spin it forever and then you're sad. So, users", "tokens": [666, 257, 13186, 11, 550, 309, 311, 445, 516, 281, 6060, 309, 5680, 293, 550, 291, 434, 4227, 13, 407, 11, 5022], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 150, "seek": 80324, "start": 817.76, "end": 822.36, "text": " expect these things to kind of work instantly. And you can kind of, you know, Sighting Sync", "tokens": [2066, 613, 721, 281, 733, 295, 589, 13518, 13, 400, 291, 393, 733, 295, 11, 291, 458, 11, 318, 397, 278, 26155, 66], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 151, "seek": 80324, "start": 822.36, "end": 826.92, "text": " has ways that you can kind of fix this. So, if you want to go and instantly see the room", "tokens": [575, 2098, 300, 291, 393, 733, 295, 3191, 341, 13, 407, 11, 498, 291, 528, 281, 352, 293, 13518, 536, 264, 1808], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 152, "seek": 80324, "start": 826.92, "end": 830.72, "text": " timeline, that's fine because we can pre-cache the room timeline, right? You can use a higher", "tokens": [12933, 11, 300, 311, 2489, 570, 321, 393, 659, 12, 66, 6000, 264, 1808, 12933, 11, 558, 30, 509, 393, 764, 257, 2946], "temperature": 0.0, "avg_logprob": -0.13169787089029947, "compression_ratio": 1.7892976588628762, "no_speech_prob": 3.697700230986811e-05}, {"id": 153, "seek": 83072, "start": 830.72, "end": 835.32, "text": " timeline limit and then you can go and pre-cache that. So, you see the room list, you click", "tokens": [12933, 4948, 293, 550, 291, 393, 352, 293, 659, 12, 66, 6000, 300, 13, 407, 11, 291, 536, 264, 1808, 1329, 11, 291, 2052], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 154, "seek": 83072, "start": 835.32, "end": 842.32, "text": " through and immediately you see all the events, at least a screen's worth of events. For the", "tokens": [807, 293, 4258, 291, 536, 439, 264, 3931, 11, 412, 1935, 257, 2568, 311, 3163, 295, 3931, 13, 1171, 264], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 155, "seek": 83072, "start": 842.32, "end": 846.08, "text": " other thing, which is you want to scroll the room list instantly and smoothly, well, you", "tokens": [661, 551, 11, 597, 307, 291, 528, 281, 11369, 264, 1808, 1329, 13518, 293, 19565, 11, 731, 11, 291], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 156, "seek": 83072, "start": 846.08, "end": 850.9200000000001, "text": " can opt out of the sliding windows entirely and you can just request really small stub", "tokens": [393, 2427, 484, 295, 264, 21169, 9309, 7696, 293, 291, 393, 445, 5308, 534, 1359, 20266], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 157, "seek": 83072, "start": 850.9200000000001, "end": 854.52, "text": " information like, I just want the avatar, I just want the room name and that's it. And", "tokens": [1589, 411, 11, 286, 445, 528, 264, 36205, 11, 286, 445, 528, 264, 1808, 1315, 293, 300, 311, 309, 13, 400], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 158, "seek": 83072, "start": 854.52, "end": 859.48, "text": " then you'll know the position, the room name, the avatar, and then you can just request", "tokens": [550, 291, 603, 458, 264, 2535, 11, 264, 1808, 1315, 11, 264, 36205, 11, 293, 550, 291, 393, 445, 5308], "temperature": 0.0, "avg_logprob": -0.15621151141266323, "compression_ratio": 1.917562724014337, "no_speech_prob": 2.683595994312782e-05}, {"id": 159, "seek": 85948, "start": 859.48, "end": 863.72, "text": " all the rooms entirely. So, that will scale with the number of rooms in the user's account,", "tokens": [439, 264, 9396, 7696, 13, 407, 11, 300, 486, 4373, 365, 264, 1230, 295, 9396, 294, 264, 4195, 311, 2696, 11], "temperature": 0.0, "avg_logprob": -0.1752496356806479, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.0265336641168687e-05}, {"id": 160, "seek": 85948, "start": 863.72, "end": 870.4, "text": " but it's, you know, it's possible. And you can use something like this. So, you say timeline", "tokens": [457, 309, 311, 11, 291, 458, 11, 309, 311, 1944, 13, 400, 291, 393, 764, 746, 411, 341, 13, 407, 11, 291, 584, 12933], "temperature": 0.0, "avg_logprob": -0.1752496356806479, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.0265336641168687e-05}, {"id": 161, "seek": 85948, "start": 870.4, "end": 875.0, "text": " limit of zero, but there's a problem here, right? Because you have a timeline limit of", "tokens": [4948, 295, 4018, 11, 457, 456, 311, 257, 1154, 510, 11, 558, 30, 1436, 291, 362, 257, 12933, 4948, 295], "temperature": 0.0, "avg_logprob": -0.1752496356806479, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.0265336641168687e-05}, {"id": 162, "seek": 85948, "start": 875.0, "end": 879.36, "text": " 20 on the first one, then a timeline limit of zero. So, you kind of want a bit of both.", "tokens": [945, 322, 264, 700, 472, 11, 550, 257, 12933, 4948, 295, 4018, 13, 407, 11, 291, 733, 295, 528, 257, 857, 295, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1752496356806479, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.0265336641168687e-05}, {"id": 163, "seek": 85948, "start": 879.36, "end": 886.6800000000001, "text": " So, it turns out what clients really want is delayed data delivery. So, the API wasn't", "tokens": [407, 11, 309, 4523, 484, 437, 6982, 534, 528, 307, 20268, 1412, 8982, 13, 407, 11, 264, 9362, 2067, 380], "temperature": 0.0, "avg_logprob": -0.1752496356806479, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.0265336641168687e-05}, {"id": 164, "seek": 88668, "start": 886.68, "end": 890.88, "text": " originally designed for that a year ago. So, we've made a lot of changes to support this", "tokens": [7993, 4761, 337, 300, 257, 1064, 2057, 13, 407, 11, 321, 600, 1027, 257, 688, 295, 2962, 281, 1406, 341], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 165, "seek": 88668, "start": 890.88, "end": 895.56, "text": " kind of idea of delayed data delivery. So, one of the things is timeline trickling. So,", "tokens": [733, 295, 1558, 295, 20268, 1412, 8982, 13, 407, 11, 472, 295, 264, 721, 307, 12933, 4282, 1688, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 166, "seek": 88668, "start": 895.56, "end": 899.7199999999999, "text": " what timeline trickling is, is that you can initially request a set of rooms and you can", "tokens": [437, 12933, 4282, 1688, 307, 11, 307, 300, 291, 393, 9105, 5308, 257, 992, 295, 9396, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 167, "seek": 88668, "start": 899.7199999999999, "end": 904.4, "text": " say, I want only the most recent message in this room. And then at a later point, you", "tokens": [584, 11, 286, 528, 787, 264, 881, 5162, 3636, 294, 341, 1808, 13, 400, 550, 412, 257, 1780, 935, 11, 291], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 168, "seek": 88668, "start": 904.4, "end": 909.3199999999999, "text": " can say, okay, now I want the last 10 messages in this room. And then it will go and, and", "tokens": [393, 584, 11, 1392, 11, 586, 286, 528, 264, 1036, 1266, 7897, 294, 341, 1808, 13, 400, 550, 309, 486, 352, 293, 11, 293], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 169, "seek": 88668, "start": 909.3199999999999, "end": 916.4799999999999, "text": " effectively backpaginate those messages for you. Likewise, the clients want all the rooms", "tokens": [8659, 646, 79, 559, 13923, 729, 7897, 337, 291, 13, 30269, 11, 264, 6982, 528, 439, 264, 9396], "temperature": 0.0, "avg_logprob": -0.14572062805621294, "compression_ratio": 1.824742268041237, "no_speech_prob": 2.0378252884256653e-05}, {"id": 170, "seek": 91648, "start": 916.48, "end": 922.6, "text": " and the accounts. So, but they want maybe more detail on the rooms that are in the viewport.", "tokens": [293, 264, 9402, 13, 407, 11, 457, 436, 528, 1310, 544, 2607, 322, 264, 9396, 300, 366, 294, 264, 1910, 2707, 13], "temperature": 0.0, "avg_logprob": -0.1833817568692294, "compression_ratio": 1.8559670781893005, "no_speech_prob": 5.1017559599131346e-05}, {"id": 171, "seek": 91648, "start": 922.6, "end": 928.6800000000001, "text": " So, again, you can support this by having two lists effectively. You've got one list,", "tokens": [407, 11, 797, 11, 291, 393, 1406, 341, 538, 1419, 732, 14511, 8659, 13, 509, 600, 658, 472, 1329, 11], "temperature": 0.0, "avg_logprob": -0.1833817568692294, "compression_ratio": 1.8559670781893005, "no_speech_prob": 5.1017559599131346e-05}, {"id": 172, "seek": 91648, "start": 928.6800000000001, "end": 933.6, "text": " which is just the visible rooms. That might have more accurate information for like room", "tokens": [597, 307, 445, 264, 8974, 9396, 13, 663, 1062, 362, 544, 8559, 1589, 337, 411, 1808], "temperature": 0.0, "avg_logprob": -0.1833817568692294, "compression_ratio": 1.8559670781893005, "no_speech_prob": 5.1017559599131346e-05}, {"id": 173, "seek": 91648, "start": 933.6, "end": 938.2, "text": " previews. So, you know, you've got room preview, you might have, you know, typing notifications,", "tokens": [14281, 82, 13, 407, 11, 291, 458, 11, 291, 600, 658, 1808, 14281, 11, 291, 1062, 362, 11, 291, 458, 11, 18444, 13426, 11], "temperature": 0.0, "avg_logprob": -0.1833817568692294, "compression_ratio": 1.8559670781893005, "no_speech_prob": 5.1017559599131346e-05}, {"id": 174, "seek": 91648, "start": 938.2, "end": 942.44, "text": " you might register for typing notifications in those rooms. But then you're not really", "tokens": [291, 1062, 7280, 337, 18444, 13426, 294, 729, 9396, 13, 583, 550, 291, 434, 406, 534], "temperature": 0.0, "avg_logprob": -0.1833817568692294, "compression_ratio": 1.8559670781893005, "no_speech_prob": 5.1017559599131346e-05}, {"id": 175, "seek": 94244, "start": 942.44, "end": 947.1600000000001, "text": " interested in type notifications for rooms, you know, really far down the list. And then", "tokens": [3102, 294, 2010, 13426, 337, 9396, 11, 291, 458, 11, 534, 1400, 760, 264, 1329, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 176, "seek": 94244, "start": 947.1600000000001, "end": 950.48, "text": " in the background, you can just have a separate list, which just kind of in the background", "tokens": [294, 264, 3678, 11, 291, 393, 445, 362, 257, 4994, 1329, 11, 597, 445, 733, 295, 294, 264, 3678], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 177, "seek": 94244, "start": 950.48, "end": 957.8800000000001, "text": " goes and gets all the other rooms and all the core information that you need. So, this", "tokens": [1709, 293, 2170, 439, 264, 661, 9396, 293, 439, 264, 4965, 1589, 300, 291, 643, 13, 407, 11, 341], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 178, "seek": 94244, "start": 957.8800000000001, "end": 963.08, "text": " has kind of been a huge trade-off, right? On the one hand, you've got sync v2, which", "tokens": [575, 733, 295, 668, 257, 2603, 4923, 12, 4506, 11, 558, 30, 1282, 264, 472, 1011, 11, 291, 600, 658, 20271, 371, 17, 11, 597], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 179, "seek": 94244, "start": 963.08, "end": 968.2800000000001, "text": " is getting everything and is super slow, but it's got fantastic offline support as a result", "tokens": [307, 1242, 1203, 293, 307, 1687, 2964, 11, 457, 309, 311, 658, 5456, 21857, 1406, 382, 257, 1874], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 180, "seek": 94244, "start": 968.2800000000001, "end": 972.36, "text": " of that. And on the other side, you've got sliding sync. It's super fast. You only literally", "tokens": [295, 300, 13, 400, 322, 264, 661, 1252, 11, 291, 600, 658, 21169, 20271, 13, 467, 311, 1687, 2370, 13, 509, 787, 3736], "temperature": 0.0, "avg_logprob": -0.13658976554870605, "compression_ratio": 1.7402597402597402, "no_speech_prob": 9.381991731061134e-06}, {"id": 181, "seek": 97236, "start": 972.36, "end": 976.8000000000001, "text": " getting the data that you need, but, you know, there's compromises to be made there because", "tokens": [1242, 264, 1412, 300, 291, 643, 11, 457, 11, 291, 458, 11, 456, 311, 11482, 3598, 281, 312, 1027, 456, 570], "temperature": 0.0, "avg_logprob": -0.12423831538150185, "compression_ratio": 1.7701612903225807, "no_speech_prob": 1.1241432730457745e-05}, {"id": 182, "seek": 97236, "start": 976.8000000000001, "end": 981.44, "text": " you have to do network requests all the time and things can be slower. There's only so", "tokens": [291, 362, 281, 360, 3209, 12475, 439, 264, 565, 293, 721, 393, 312, 14009, 13, 821, 311, 787, 370], "temperature": 0.0, "avg_logprob": -0.12423831538150185, "compression_ratio": 1.7701612903225807, "no_speech_prob": 1.1241432730457745e-05}, {"id": 183, "seek": 97236, "start": 981.44, "end": 986.0, "text": " fast you can do. There's only so much you can optimize a server. So, really, I think", "tokens": [2370, 291, 393, 360, 13, 821, 311, 787, 370, 709, 291, 393, 19719, 257, 7154, 13, 407, 11, 534, 11, 286, 519], "temperature": 0.0, "avg_logprob": -0.12423831538150185, "compression_ratio": 1.7701612903225807, "no_speech_prob": 1.1241432730457745e-05}, {"id": 184, "seek": 97236, "start": 986.0, "end": 990.88, "text": " element is kind of aiming to do something like that. So, it's mostly kind of sliding", "tokens": [4478, 307, 733, 295, 20253, 281, 360, 746, 411, 300, 13, 407, 11, 309, 311, 5240, 733, 295, 21169], "temperature": 0.0, "avg_logprob": -0.12423831538150185, "compression_ratio": 1.7701612903225807, "no_speech_prob": 1.1241432730457745e-05}, {"id": 185, "seek": 97236, "start": 990.88, "end": 996.08, "text": " sync, but there are compromises and trade-offs that are being made to try to give a really", "tokens": [20271, 11, 457, 456, 366, 11482, 3598, 293, 4923, 12, 19231, 300, 366, 885, 1027, 281, 853, 281, 976, 257, 534], "temperature": 0.0, "avg_logprob": -0.12423831538150185, "compression_ratio": 1.7701612903225807, "no_speech_prob": 1.1241432730457745e-05}, {"id": 186, "seek": 99608, "start": 996.08, "end": 1003.32, "text": " good offline experience as well. So, in terms of what's next, we need to add threads because", "tokens": [665, 21857, 1752, 382, 731, 13, 407, 11, 294, 2115, 295, 437, 311, 958, 11, 321, 643, 281, 909, 19314, 570], "temperature": 0.0, "avg_logprob": -0.13992645626976377, "compression_ratio": 1.6397058823529411, "no_speech_prob": 2.4398541427217424e-05}, {"id": 187, "seek": 99608, "start": 1003.32, "end": 1007.6, "text": " there's no threading support at all in sliding sync. And threads, obviously, only reasonably", "tokens": [456, 311, 572, 7207, 278, 1406, 412, 439, 294, 21169, 20271, 13, 400, 19314, 11, 2745, 11, 787, 23551], "temperature": 0.0, "avg_logprob": -0.13992645626976377, "compression_ratio": 1.6397058823529411, "no_speech_prob": 2.4398541427217424e-05}, {"id": 188, "seek": 99608, "start": 1007.6, "end": 1014.24, "text": " recently landed and was enabled everywhere. Threads is complicated because threads are", "tokens": [3938, 15336, 293, 390, 15172, 5315, 13, 334, 2538, 82, 307, 6179, 570, 19314, 366], "temperature": 0.0, "avg_logprob": -0.13992645626976377, "compression_ratio": 1.6397058823529411, "no_speech_prob": 2.4398541427217424e-05}, {"id": 189, "seek": 99608, "start": 1014.24, "end": 1020.24, "text": " changes fundamental answers to questions like, is this room unread? Because normally you", "tokens": [2962, 8088, 6338, 281, 1651, 411, 11, 307, 341, 1808, 517, 2538, 30, 1436, 5646, 291], "temperature": 0.0, "avg_logprob": -0.13992645626976377, "compression_ratio": 1.6397058823529411, "no_speech_prob": 2.4398541427217424e-05}, {"id": 190, "seek": 99608, "start": 1020.24, "end": 1024.6000000000001, "text": " could just be like, well, what's your red marker? What's most recent event? Okay, it", "tokens": [727, 445, 312, 411, 11, 731, 11, 437, 311, 428, 2182, 15247, 30, 708, 311, 881, 5162, 2280, 30, 1033, 11, 309], "temperature": 0.0, "avg_logprob": -0.13992645626976377, "compression_ratio": 1.6397058823529411, "no_speech_prob": 2.4398541427217424e-05}, {"id": 191, "seek": 102460, "start": 1024.6, "end": 1030.12, "text": " must be unread. Whereas now you could have scenarios where, you know, the most recent", "tokens": [1633, 312, 517, 2538, 13, 13813, 586, 291, 727, 362, 15077, 689, 11, 291, 458, 11, 264, 881, 5162], "temperature": 0.0, "avg_logprob": -0.1297925580449465, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.1282068044238258e-05}, {"id": 192, "seek": 102460, "start": 1030.12, "end": 1035.04, "text": " event in the room is on a thread. So, if you were just to click on a room and you see the", "tokens": [2280, 294, 264, 1808, 307, 322, 257, 7207, 13, 407, 11, 498, 291, 645, 445, 281, 2052, 322, 257, 1808, 293, 291, 536, 264], "temperature": 0.0, "avg_logprob": -0.1297925580449465, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.1282068044238258e-05}, {"id": 193, "seek": 102460, "start": 1035.04, "end": 1039.6, "text": " timeline, they're all messages, but in a thread, you know, three days ago, there's actually", "tokens": [12933, 11, 436, 434, 439, 7897, 11, 457, 294, 257, 7207, 11, 291, 458, 11, 1045, 1708, 2057, 11, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.1297925580449465, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.1282068044238258e-05}, {"id": 194, "seek": 102460, "start": 1039.6, "end": 1047.04, "text": " a newer message. So, adding support for threads is going to be quite tricky to get right and", "tokens": [257, 17628, 3636, 13, 407, 11, 5127, 1406, 337, 19314, 307, 516, 281, 312, 1596, 12414, 281, 483, 558, 293], "temperature": 0.0, "avg_logprob": -0.1297925580449465, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.1282068044238258e-05}, {"id": 195, "seek": 102460, "start": 1047.04, "end": 1051.9599999999998, "text": " we'll have to probably iterate on it quite a lot, but it is coming. The other thing we're", "tokens": [321, 603, 362, 281, 1391, 44497, 322, 309, 1596, 257, 688, 11, 457, 309, 307, 1348, 13, 440, 661, 551, 321, 434], "temperature": 0.0, "avg_logprob": -0.1297925580449465, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.1282068044238258e-05}, {"id": 196, "seek": 105196, "start": 1051.96, "end": 1055.96, "text": " going to be adding in is this concept called delta tokens, which unless you've read the", "tokens": [516, 281, 312, 5127, 294, 307, 341, 3410, 1219, 8289, 22667, 11, 597, 5969, 291, 600, 1401, 264], "temperature": 0.0, "avg_logprob": -0.1266429286357785, "compression_ratio": 1.662962962962963, "no_speech_prob": 1.4202579222910572e-05}, {"id": 197, "seek": 105196, "start": 1055.96, "end": 1062.8400000000001, "text": " MSC, you'll have no idea what it is. Ultimately, what delta tokens are is to, sliding sync", "tokens": [7395, 34, 11, 291, 603, 362, 572, 1558, 437, 309, 307, 13, 23921, 11, 437, 8289, 22667, 366, 307, 281, 11, 21169, 20271], "temperature": 0.0, "avg_logprob": -0.1266429286357785, "compression_ratio": 1.662962962962963, "no_speech_prob": 1.4202579222910572e-05}, {"id": 198, "seek": 105196, "start": 1062.8400000000001, "end": 1067.56, "text": " has a problem at the moment, the proxy server, because it has amnesia. So, it will time out", "tokens": [575, 257, 1154, 412, 264, 1623, 11, 264, 29690, 7154, 11, 570, 309, 575, 669, 4081, 654, 13, 407, 11, 309, 486, 565, 484], "temperature": 0.0, "avg_logprob": -0.1266429286357785, "compression_ratio": 1.662962962962963, "no_speech_prob": 1.4202579222910572e-05}, {"id": 199, "seek": 105196, "start": 1067.56, "end": 1071.6000000000001, "text": " your connection if you don't use it for, say, half an hour. And it will clean up all that", "tokens": [428, 4984, 498, 291, 500, 380, 764, 309, 337, 11, 584, 11, 1922, 364, 1773, 13, 400, 309, 486, 2541, 493, 439, 300], "temperature": 0.0, "avg_logprob": -0.1266429286357785, "compression_ratio": 1.662962962962963, "no_speech_prob": 1.4202579222910572e-05}, {"id": 200, "seek": 105196, "start": 1071.6000000000001, "end": 1078.1200000000001, "text": " in memory state. All those caches and things get cleaned up. And the problem is, is that", "tokens": [294, 4675, 1785, 13, 1057, 729, 269, 13272, 293, 721, 483, 16146, 493, 13, 400, 264, 1154, 307, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.1266429286357785, "compression_ratio": 1.662962962962963, "no_speech_prob": 1.4202579222910572e-05}, {"id": 201, "seek": 107812, "start": 1078.12, "end": 1082.6399999999999, "text": " then when you reconnect, even though your client has stored those rooms and stored a lot of", "tokens": [550, 562, 291, 30095, 11, 754, 1673, 428, 6423, 575, 12187, 729, 9396, 293, 12187, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1385858586404176, "compression_ratio": 1.691449814126394, "no_speech_prob": 8.046741641010158e-06}, {"id": 202, "seek": 107812, "start": 1082.6399999999999, "end": 1087.6799999999998, "text": " the timeline and stored a bunch of room state, the proxy server doesn't know this. So, it's", "tokens": [264, 12933, 293, 12187, 257, 3840, 295, 1808, 1785, 11, 264, 29690, 7154, 1177, 380, 458, 341, 13, 407, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1385858586404176, "compression_ratio": 1.691449814126394, "no_speech_prob": 8.046741641010158e-06}, {"id": 203, "seek": 107812, "start": 1087.6799999999998, "end": 1092.8, "text": " going to resend that information to you. So, the point of delta tokens is to say, hey,", "tokens": [516, 281, 725, 521, 300, 1589, 281, 291, 13, 407, 11, 264, 935, 295, 8289, 22667, 307, 281, 584, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.1385858586404176, "compression_ratio": 1.691449814126394, "no_speech_prob": 8.046741641010158e-06}, {"id": 204, "seek": 107812, "start": 1092.8, "end": 1099.52, "text": " I remember me, I've already stored, I already know about these events. And then those events", "tokens": [286, 1604, 385, 11, 286, 600, 1217, 12187, 11, 286, 1217, 458, 466, 613, 3931, 13, 400, 550, 729, 3931], "temperature": 0.0, "avg_logprob": -0.1385858586404176, "compression_ratio": 1.691449814126394, "no_speech_prob": 8.046741641010158e-06}, {"id": 205, "seek": 107812, "start": 1099.52, "end": 1105.52, "text": " aren't sent to the client again in duplicate. A few more API optimizations that we recently", "tokens": [3212, 380, 2279, 281, 264, 6423, 797, 294, 23976, 13, 316, 1326, 544, 9362, 5028, 14455, 300, 321, 3938], "temperature": 0.0, "avg_logprob": -0.1385858586404176, "compression_ratio": 1.691449814126394, "no_speech_prob": 8.046741641010158e-06}, {"id": 206, "seek": 110552, "start": 1105.52, "end": 1112.24, "text": " swapped to using lists as keys, which basically means that instead of representing the requests", "tokens": [50011, 281, 1228, 14511, 382, 9317, 11, 597, 1936, 1355, 300, 2602, 295, 13460, 264, 12475], "temperature": 0.0, "avg_logprob": -0.14408625808416628, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.839096022304147e-05}, {"id": 207, "seek": 110552, "start": 1112.24, "end": 1118.44, "text": " and response lists as an array of lists, they're now just a big key value map, which makes", "tokens": [293, 4134, 14511, 382, 364, 10225, 295, 14511, 11, 436, 434, 586, 445, 257, 955, 2141, 2158, 4471, 11, 597, 1669], "temperature": 0.0, "avg_logprob": -0.14408625808416628, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.839096022304147e-05}, {"id": 208, "seek": 110552, "start": 1118.44, "end": 1123.92, "text": " it easier because you can then reference an individual list by the list key name. So,", "tokens": [309, 3571, 570, 291, 393, 550, 6408, 364, 2609, 1329, 538, 264, 1329, 2141, 1315, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.14408625808416628, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.839096022304147e-05}, {"id": 209, "seek": 110552, "start": 1123.92, "end": 1128.92, "text": " for things like extensions, this is great because you could then have a way of expressing,", "tokens": [337, 721, 411, 25129, 11, 341, 307, 869, 570, 291, 727, 550, 362, 257, 636, 295, 22171, 11], "temperature": 0.0, "avg_logprob": -0.14408625808416628, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.839096022304147e-05}, {"id": 210, "seek": 110552, "start": 1128.92, "end": 1134.16, "text": " I want typing notifications, but only on these named lists, whereas before that was very", "tokens": [286, 528, 18444, 13426, 11, 457, 787, 322, 613, 4926, 14511, 11, 9735, 949, 300, 390, 588], "temperature": 0.0, "avg_logprob": -0.14408625808416628, "compression_ratio": 1.6802973977695168, "no_speech_prob": 6.839096022304147e-05}, {"id": 211, "seek": 113416, "start": 1134.16, "end": 1139.8400000000001, "text": " difficult to express. And we also really want to have comprehensive client support. It's", "tokens": [2252, 281, 5109, 13, 400, 321, 611, 534, 528, 281, 362, 13914, 6423, 1406, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.1791552738709883, "compression_ratio": 1.5407725321888412, "no_speech_prob": 4.5191314711701125e-05}, {"id": 212, "seek": 113416, "start": 1139.8400000000001, "end": 1145.96, "text": " getting reasonably stable now, and it's certainly very performant. And Element Web uses Slime", "tokens": [1242, 23551, 8351, 586, 11, 293, 309, 311, 3297, 588, 2042, 394, 13, 400, 20900, 9573, 4960, 6187, 1312], "temperature": 0.0, "avg_logprob": -0.1791552738709883, "compression_ratio": 1.5407725321888412, "no_speech_prob": 4.5191314711701125e-05}, {"id": 213, "seek": 113416, "start": 1145.96, "end": 1152.16, "text": " Sync natively in the JS SDK, but obviously, that doesn't really work for mobile. And it", "tokens": [26155, 66, 8470, 356, 294, 264, 33063, 37135, 11, 457, 2745, 11, 300, 1177, 380, 534, 589, 337, 6013, 13, 400, 309], "temperature": 0.0, "avg_logprob": -0.1791552738709883, "compression_ratio": 1.5407725321888412, "no_speech_prob": 4.5191314711701125e-05}, {"id": 214, "seek": 113416, "start": 1152.16, "end": 1159.1200000000001, "text": " would be nice to have some sort of SDK that could be used for Android and iOS, and maybe", "tokens": [576, 312, 1481, 281, 362, 512, 1333, 295, 37135, 300, 727, 312, 1143, 337, 8853, 293, 17430, 11, 293, 1310], "temperature": 0.0, "avg_logprob": -0.1791552738709883, "compression_ratio": 1.5407725321888412, "no_speech_prob": 4.5191314711701125e-05}, {"id": 215, "seek": 115912, "start": 1159.12, "end": 1166.12, "text": " even web at some point. I think there is.", "tokens": [754, 3670, 412, 512, 935, 13, 286, 519, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.2837497166224888, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.8376200134516694e-05}, {"id": 216, "seek": 115912, "start": 1166.12, "end": 1183.12, "text": " Yes. Let's talk about the Rust SDK. So, this is, sorry, overall a very technical talk.", "tokens": [1079, 13, 961, 311, 751, 466, 264, 34952, 37135, 13, 407, 11, 341, 307, 11, 2597, 11, 4787, 257, 588, 6191, 751, 13], "temperature": 0.0, "avg_logprob": -0.2837497166224888, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.8376200134516694e-05}, {"id": 217, "seek": 115912, "start": 1183.12, "end": 1188.1999999999998, "text": " You already noticed that. But I'm going to lighten up a little bit more. But first about", "tokens": [509, 1217, 5694, 300, 13, 583, 286, 478, 516, 281, 1442, 268, 493, 257, 707, 857, 544, 13, 583, 700, 466], "temperature": 0.0, "avg_logprob": -0.2837497166224888, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.8376200134516694e-05}, {"id": 218, "seek": 118820, "start": 1188.2, "end": 1193.28, "text": " me, so I'm Ben. Hi. My name is the only name in the presentation. I don't know why. These", "tokens": [385, 11, 370, 286, 478, 3964, 13, 2421, 13, 1222, 1315, 307, 264, 787, 1315, 294, 264, 5860, 13, 286, 500, 380, 458, 983, 13, 1981], "temperature": 0.0, "avg_logprob": -0.23697217305501303, "compression_ratio": 1.6366906474820144, "no_speech_prob": 6.495594425359741e-05}, {"id": 219, "seek": 118820, "start": 1193.28, "end": 1200.76, "text": " guys have more work and show more stuff. So, quick, yeah. I've led the Rust SDK team for", "tokens": [1074, 362, 544, 589, 293, 855, 544, 1507, 13, 407, 11, 1702, 11, 1338, 13, 286, 600, 4684, 264, 34952, 37135, 1469, 337], "temperature": 0.0, "avg_logprob": -0.23697217305501303, "compression_ratio": 1.6366906474820144, "no_speech_prob": 6.495594425359741e-05}, {"id": 220, "seek": 118820, "start": 1200.76, "end": 1206.16, "text": " the last year for Element, and I've been working in decentralization, decentralized tech for", "tokens": [264, 1036, 1064, 337, 20900, 11, 293, 286, 600, 668, 1364, 294, 26515, 2144, 11, 32870, 7553, 337], "temperature": 0.0, "avg_logprob": -0.23697217305501303, "compression_ratio": 1.6366906474820144, "no_speech_prob": 6.495594425359741e-05}, {"id": 221, "seek": 118820, "start": 1206.16, "end": 1211.0800000000002, "text": " a couple years already. I worked at PariTech before, was leading the substrate client team", "tokens": [257, 1916, 924, 1217, 13, 286, 2732, 412, 3457, 72, 36050, 949, 11, 390, 5775, 264, 27585, 6423, 1469], "temperature": 0.0, "avg_logprob": -0.23697217305501303, "compression_ratio": 1.6366906474820144, "no_speech_prob": 6.495594425359741e-05}, {"id": 222, "seek": 118820, "start": 1211.0800000000002, "end": 1216.3600000000001, "text": " there, if you know, blockchain. That's one of the most favorite blockchain building systems.", "tokens": [456, 11, 498, 291, 458, 11, 17176, 13, 663, 311, 472, 295, 264, 881, 2954, 17176, 2390, 3652, 13], "temperature": 0.0, "avg_logprob": -0.23697217305501303, "compression_ratio": 1.6366906474820144, "no_speech_prob": 6.495594425359741e-05}, {"id": 223, "seek": 121636, "start": 1216.36, "end": 1220.6799999999998, "text": " I'm going to be working as a tech link for ActiveGlobal, where we're building, on top", "tokens": [286, 478, 516, 281, 312, 1364, 382, 257, 7553, 2113, 337, 26635, 38, 752, 2645, 11, 689, 321, 434, 2390, 11, 322, 1192], "temperature": 0.0, "avg_logprob": -0.23059045892012747, "compression_ratio": 1.407114624505929, "no_speech_prob": 4.130433080717921e-05}, {"id": 224, "seek": 121636, "start": 1220.6799999999998, "end": 1227.84, "text": " of the Rust SDK, an organizing app for NGOs and civil society. So, I've been working in", "tokens": [295, 264, 34952, 37135, 11, 364, 17608, 724, 337, 46454, 293, 5605, 4086, 13, 407, 11, 286, 600, 668, 1364, 294], "temperature": 0.0, "avg_logprob": -0.23059045892012747, "compression_ratio": 1.407114624505929, "no_speech_prob": 4.130433080717921e-05}, {"id": 225, "seek": 121636, "start": 1227.84, "end": 1234.0, "text": " this for over a decade. You might know me from almost not at all threatening talk I gave", "tokens": [341, 337, 670, 257, 10378, 13, 509, 1062, 458, 385, 490, 1920, 406, 412, 439, 20768, 751, 286, 2729], "temperature": 0.0, "avg_logprob": -0.23059045892012747, "compression_ratio": 1.407114624505929, "no_speech_prob": 4.130433080717921e-05}, {"id": 226, "seek": 121636, "start": 1234.0, "end": 1241.1999999999998, "text": " at Jason, like, 2017. That was already about, like, how do you do decentralized privacy-first", "tokens": [412, 11181, 11, 411, 11, 6591, 13, 663, 390, 1217, 466, 11, 411, 11, 577, 360, 291, 360, 32870, 11427, 12, 29581], "temperature": 0.0, "avg_logprob": -0.23059045892012747, "compression_ratio": 1.407114624505929, "no_speech_prob": 4.130433080717921e-05}, {"id": 227, "seek": 124120, "start": 1241.2, "end": 1249.04, "text": " technology? Enough about me. Let's talk about, let me tell you a little story. We're back", "tokens": [2899, 30, 19401, 466, 385, 13, 961, 311, 751, 466, 11, 718, 385, 980, 291, 257, 707, 1657, 13, 492, 434, 646], "temperature": 0.0, "avg_logprob": -0.15911730324349752, "compression_ratio": 1.6690909090909092, "no_speech_prob": 2.0455705453059636e-05}, {"id": 228, "seek": 124120, "start": 1249.04, "end": 1256.52, "text": " in 2019, 2020, and it's the state of the clients. For the sake of argument, I'm talking about", "tokens": [294, 6071, 11, 4808, 11, 293, 309, 311, 264, 1785, 295, 264, 6982, 13, 1171, 264, 9717, 295, 6770, 11, 286, 478, 1417, 466], "temperature": 0.0, "avg_logprob": -0.15911730324349752, "compression_ratio": 1.6690909090909092, "no_speech_prob": 2.0455705453059636e-05}, {"id": 229, "seek": 124120, "start": 1256.52, "end": 1261.4, "text": " Element clients here, because I think there's exceptions to what I'm going to tell you. But", "tokens": [20900, 6982, 510, 11, 570, 286, 519, 456, 311, 22847, 281, 437, 286, 478, 516, 281, 980, 291, 13, 583], "temperature": 0.0, "avg_logprob": -0.15911730324349752, "compression_ratio": 1.6690909090909092, "no_speech_prob": 2.0455705453059636e-05}, {"id": 230, "seek": 124120, "start": 1261.4, "end": 1265.1200000000001, "text": " let me tell you two truths in a lie, and you can tell me if you can spot the lie. So, truth", "tokens": [718, 385, 980, 291, 732, 30079, 294, 257, 4544, 11, 293, 291, 393, 980, 385, 498, 291, 393, 4008, 264, 4544, 13, 407, 11, 3494], "temperature": 0.0, "avg_logprob": -0.15911730324349752, "compression_ratio": 1.6690909090909092, "no_speech_prob": 2.0455705453059636e-05}, {"id": 231, "seek": 124120, "start": 1265.1200000000001, "end": 1271.04, "text": " number one, many clients out there don't actually implement end-to-end encryption, which is", "tokens": [1230, 472, 11, 867, 6982, 484, 456, 500, 380, 767, 4445, 917, 12, 1353, 12, 521, 29575, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.15911730324349752, "compression_ratio": 1.6690909090909092, "no_speech_prob": 2.0455705453059636e-05}, {"id": 232, "seek": 127104, "start": 1271.04, "end": 1275.8799999999999, "text": " pretty sad, because it's a very fundamental part of what we're working on. That is mostly", "tokens": [1238, 4227, 11, 570, 309, 311, 257, 588, 8088, 644, 295, 437, 321, 434, 1364, 322, 13, 663, 307, 5240], "temperature": 0.0, "avg_logprob": -0.17777330534798758, "compression_ratio": 1.6277372262773722, "no_speech_prob": 1.9219865862396546e-05}, {"id": 233, "seek": 127104, "start": 1275.8799999999999, "end": 1281.52, "text": " because it's hard. Even if you use the most widely used LibOm library, that is a seed", "tokens": [570, 309, 311, 1152, 13, 2754, 498, 291, 764, 264, 881, 13371, 1143, 15834, 46, 76, 6405, 11, 300, 307, 257, 8871], "temperature": 0.0, "avg_logprob": -0.17777330534798758, "compression_ratio": 1.6277372262773722, "no_speech_prob": 1.9219865862396546e-05}, {"id": 234, "seek": 127104, "start": 1281.52, "end": 1284.96, "text": " library that is already slightly dated. There's, like, a lot of knowledge that has been built", "tokens": [6405, 300, 307, 1217, 4748, 23804, 13, 821, 311, 11, 411, 11, 257, 688, 295, 3601, 300, 575, 668, 3094], "temperature": 0.0, "avg_logprob": -0.17777330534798758, "compression_ratio": 1.6277372262773722, "no_speech_prob": 1.9219865862396546e-05}, {"id": 235, "seek": 127104, "start": 1284.96, "end": 1292.56, "text": " up that is not easy to ingrain in this existing library anymore. Clients usually implement", "tokens": [493, 300, 307, 406, 1858, 281, 3957, 7146, 294, 341, 6741, 6405, 3602, 13, 2033, 2448, 2673, 4445], "temperature": 0.0, "avg_logprob": -0.17777330534798758, "compression_ratio": 1.6277372262773722, "no_speech_prob": 1.9219865862396546e-05}, {"id": 236, "seek": 127104, "start": 1292.56, "end": 1297.8799999999999, "text": " the entire HTTP, or at least most of the state machine around room, room state, who's", "tokens": [264, 2302, 33283, 11, 420, 412, 1935, 881, 295, 264, 1785, 3479, 926, 1808, 11, 1808, 1785, 11, 567, 311], "temperature": 0.0, "avg_logprob": -0.17777330534798758, "compression_ratio": 1.6277372262773722, "no_speech_prob": 1.9219865862396546e-05}, {"id": 237, "seek": 129788, "start": 1297.88, "end": 1303.6000000000001, "text": " allowed to write, as well as the entire messaging mechanics themselves in their own language,", "tokens": [4350, 281, 2464, 11, 382, 731, 382, 264, 2302, 21812, 12939, 2969, 294, 641, 1065, 2856, 11], "temperature": 0.0, "avg_logprob": -0.15850698030911958, "compression_ratio": 1.6209386281588447, "no_speech_prob": 2.3550384867121466e-05}, {"id": 238, "seek": 129788, "start": 1303.6000000000001, "end": 1308.64, "text": " in their own environment. Therefore, because we have that, clients are super fast, it is", "tokens": [294, 641, 1065, 2823, 13, 7504, 11, 570, 321, 362, 300, 11, 6982, 366, 1687, 2370, 11, 309, 307], "temperature": 0.0, "avg_logprob": -0.15850698030911958, "compression_ratio": 1.6209386281588447, "no_speech_prob": 2.3550384867121466e-05}, {"id": 239, "seek": 129788, "start": 1308.64, "end": 1315.88, "text": " totally integrated into the system that they are, and it's just a smooth experience. I", "tokens": [3879, 10919, 666, 264, 1185, 300, 436, 366, 11, 293, 309, 311, 445, 257, 5508, 1752, 13, 286], "temperature": 0.0, "avg_logprob": -0.15850698030911958, "compression_ratio": 1.6209386281588447, "no_speech_prob": 2.3550384867121466e-05}, {"id": 240, "seek": 129788, "start": 1315.88, "end": 1320.92, "text": " don't have to ask you, you know, which one of this is a lie, the cake is a lie. At this", "tokens": [500, 380, 362, 281, 1029, 291, 11, 291, 458, 11, 597, 472, 295, 341, 307, 257, 4544, 11, 264, 5908, 307, 257, 4544, 13, 1711, 341], "temperature": 0.0, "avg_logprob": -0.15850698030911958, "compression_ratio": 1.6209386281588447, "no_speech_prob": 2.3550384867121466e-05}, {"id": 241, "seek": 129788, "start": 1320.92, "end": 1327.4, "text": " time, enter our hero. Our hero is Damir. Damir is working as a crypto dev for Element. He's", "tokens": [565, 11, 3242, 527, 5316, 13, 2621, 5316, 307, 5885, 347, 13, 5885, 347, 307, 1364, 382, 257, 17240, 1905, 337, 20900, 13, 634, 311], "temperature": 0.0, "avg_logprob": -0.15850698030911958, "compression_ratio": 1.6209386281588447, "no_speech_prob": 2.3550384867121466e-05}, {"id": 242, "seek": 132740, "start": 1327.4, "end": 1335.0, "text": " a Rust into the S, and he knows the crypto in and out. He's intending to rewrite a plug-in", "tokens": [257, 34952, 666, 264, 318, 11, 293, 415, 3255, 264, 17240, 294, 293, 484, 13, 634, 311, 560, 2029, 281, 28132, 257, 5452, 12, 259], "temperature": 0.0, "avg_logprob": -0.1518567457037457, "compression_ratio": 1.6107142857142858, "no_speech_prob": 4.830881516681984e-05}, {"id": 243, "seek": 132740, "start": 1335.0, "end": 1340.3600000000001, "text": " that he's using for an ISC client, which is called WeChat, that connects to Matrix. Because", "tokens": [300, 415, 311, 1228, 337, 364, 6205, 34, 6423, 11, 597, 307, 1219, 492, 41683, 11, 300, 16967, 281, 36274, 13, 1436], "temperature": 0.0, "avg_logprob": -0.1518567457037457, "compression_ratio": 1.6107142857142858, "no_speech_prob": 4.830881516681984e-05}, {"id": 244, "seek": 132740, "start": 1340.3600000000001, "end": 1345.96, "text": " of simple problems that are limitations in the Python implementation that WeChat offers,", "tokens": [295, 2199, 2740, 300, 366, 15705, 294, 264, 15329, 11420, 300, 492, 41683, 7736, 11], "temperature": 0.0, "avg_logprob": -0.1518567457037457, "compression_ratio": 1.6107142857142858, "no_speech_prob": 4.830881516681984e-05}, {"id": 245, "seek": 132740, "start": 1345.96, "end": 1351.64, "text": " he wants to rewrite it in Rust. But he doesn't really find a good space to build it on. This", "tokens": [415, 2738, 281, 28132, 309, 294, 34952, 13, 583, 415, 1177, 380, 534, 915, 257, 665, 1901, 281, 1322, 309, 322, 13, 639], "temperature": 0.0, "avg_logprob": -0.1518567457037457, "compression_ratio": 1.6107142857142858, "no_speech_prob": 4.830881516681984e-05}, {"id": 246, "seek": 132740, "start": 1351.64, "end": 1356.1200000000001, "text": " is not an actual representation, but we're going to use it for now. So he goes out and", "tokens": [307, 406, 364, 3539, 10290, 11, 457, 321, 434, 516, 281, 764, 309, 337, 586, 13, 407, 415, 1709, 484, 293], "temperature": 0.0, "avg_logprob": -0.1518567457037457, "compression_ratio": 1.6107142857142858, "no_speech_prob": 4.830881516681984e-05}, {"id": 247, "seek": 135612, "start": 1356.12, "end": 1361.76, "text": " says, OK, let's write this. How hard could it be? He quickly realizes, OK, so the crypto", "tokens": [1619, 11, 2264, 11, 718, 311, 2464, 341, 13, 1012, 1152, 727, 309, 312, 30, 634, 2661, 29316, 11, 2264, 11, 370, 264, 17240], "temperature": 0.0, "avg_logprob": -0.19957032242441566, "compression_ratio": 1.5901060070671378, "no_speech_prob": 4.610755786416121e-05}, {"id": 248, "seek": 135612, "start": 1361.76, "end": 1368.6, "text": " side with the C, I would like to have that in Rust. I'm going to get that Y in a second.", "tokens": [1252, 365, 264, 383, 11, 286, 576, 411, 281, 362, 300, 294, 34952, 13, 286, 478, 516, 281, 483, 300, 398, 294, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.19957032242441566, "compression_ratio": 1.5901060070671378, "no_speech_prob": 4.610755786416121e-05}, {"id": 249, "seek": 135612, "start": 1368.6, "end": 1372.28, "text": " And he pulls that out later, which is now called Votosimac. You might have heard about", "tokens": [400, 415, 16982, 300, 484, 1780, 11, 597, 307, 586, 1219, 691, 310, 329, 332, 326, 13, 509, 1062, 362, 2198, 466], "temperature": 0.0, "avg_logprob": -0.19957032242441566, "compression_ratio": 1.5901060070671378, "no_speech_prob": 4.610755786416121e-05}, {"id": 250, "seek": 135612, "start": 1372.28, "end": 1379.8, "text": " that, which is our crypto implementation that we're pushing forward as a live-all misdiplicated.", "tokens": [300, 11, 597, 307, 527, 17240, 11420, 300, 321, 434, 7380, 2128, 382, 257, 1621, 12, 336, 3346, 4504, 564, 3587, 13], "temperature": 0.0, "avg_logprob": -0.19957032242441566, "compression_ratio": 1.5901060070671378, "no_speech_prob": 4.610755786416121e-05}, {"id": 251, "seek": 135612, "start": 1379.8, "end": 1383.9599999999998, "text": " But he figures out the stuff around that to make crypto work, not the encryption itself,", "tokens": [583, 415, 9624, 484, 264, 1507, 926, 300, 281, 652, 17240, 589, 11, 406, 264, 29575, 2564, 11], "temperature": 0.0, "avg_logprob": -0.19957032242441566, "compression_ratio": 1.5901060070671378, "no_speech_prob": 4.610755786416121e-05}, {"id": 252, "seek": 138396, "start": 1383.96, "end": 1390.52, "text": " but the entire thing of how do I know which messages to encrypt with what key in what room,", "tokens": [457, 264, 2302, 551, 295, 577, 360, 286, 458, 597, 7897, 281, 17972, 662, 365, 437, 2141, 294, 437, 1808, 11], "temperature": 0.0, "avg_logprob": -0.13168652289736588, "compression_ratio": 1.751937984496124, "no_speech_prob": 7.2956254371092655e-06}, {"id": 253, "seek": 138396, "start": 1390.52, "end": 1394.04, "text": " what if a message comes in and I don't have the encryption key? All of that state management", "tokens": [437, 498, 257, 3636, 1487, 294, 293, 286, 500, 380, 362, 264, 29575, 2141, 30, 1057, 295, 300, 1785, 4592], "temperature": 0.0, "avg_logprob": -0.13168652289736588, "compression_ratio": 1.751937984496124, "no_speech_prob": 7.2956254371092655e-06}, {"id": 254, "seek": 138396, "start": 1394.04, "end": 1399.8400000000001, "text": " around that is actually as complicated and as problematic as the actual crypto. And that", "tokens": [926, 300, 307, 767, 382, 6179, 293, 382, 19011, 382, 264, 3539, 17240, 13, 400, 300], "temperature": 0.0, "avg_logprob": -0.13168652289736588, "compression_ratio": 1.751937984496124, "no_speech_prob": 7.2956254371092655e-06}, {"id": 255, "seek": 138396, "start": 1399.8400000000001, "end": 1404.24, "text": " is why a lot of people try to use the crypto, but then fail in doing all of that, making", "tokens": [307, 983, 257, 688, 295, 561, 853, 281, 764, 264, 17240, 11, 457, 550, 3061, 294, 884, 439, 295, 300, 11, 1455], "temperature": 0.0, "avg_logprob": -0.13168652289736588, "compression_ratio": 1.751937984496124, "no_speech_prob": 7.2956254371092655e-06}, {"id": 256, "seek": 138396, "start": 1404.24, "end": 1408.3600000000001, "text": " it a really terrible experience. And then I drop it and say, oh, let's not do encryption.", "tokens": [309, 257, 534, 6237, 1752, 13, 400, 550, 286, 3270, 309, 293, 584, 11, 1954, 11, 718, 311, 406, 360, 29575, 13], "temperature": 0.0, "avg_logprob": -0.13168652289736588, "compression_ratio": 1.751937984496124, "no_speech_prob": 7.2956254371092655e-06}, {"id": 257, "seek": 140836, "start": 1408.36, "end": 1414.3999999999999, "text": " That's too hard. But he continues and pushes on because he really wants that for WeChat", "tokens": [663, 311, 886, 1152, 13, 583, 415, 6515, 293, 21020, 322, 570, 415, 534, 2738, 300, 337, 492, 41683], "temperature": 0.0, "avg_logprob": -0.13109346536489633, "compression_ratio": 1.5387931034482758, "no_speech_prob": 4.468878614716232e-05}, {"id": 258, "seek": 140836, "start": 1414.3999999999999, "end": 1423.4399999999998, "text": " and starts out with what we know as the Rust metrics SDK. So why did he pick Rust? I'm", "tokens": [293, 3719, 484, 365, 437, 321, 458, 382, 264, 34952, 16367, 37135, 13, 407, 983, 630, 415, 1888, 34952, 30, 286, 478], "temperature": 0.0, "avg_logprob": -0.13109346536489633, "compression_ratio": 1.5387931034482758, "no_speech_prob": 4.468878614716232e-05}, {"id": 259, "seek": 140836, "start": 1423.4399999999998, "end": 1428.12, "text": " not talking in his name, but I'm going to give you some reasons why. If you heard about", "tokens": [406, 1417, 294, 702, 1315, 11, 457, 286, 478, 516, 281, 976, 291, 512, 4112, 983, 13, 759, 291, 2198, 466], "temperature": 0.0, "avg_logprob": -0.13109346536489633, "compression_ratio": 1.5387931034482758, "no_speech_prob": 4.468878614716232e-05}, {"id": 260, "seek": 140836, "start": 1428.12, "end": 1436.08, "text": " Rust before, you probably heard about it because it's the most popular, most beloved language.", "tokens": [34952, 949, 11, 291, 1391, 2198, 466, 309, 570, 309, 311, 264, 881, 3743, 11, 881, 14553, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13109346536489633, "compression_ratio": 1.5387931034482758, "no_speech_prob": 4.468878614716232e-05}, {"id": 261, "seek": 143608, "start": 1436.08, "end": 1443.6, "text": " Rust is running now on the Stack Overflow system. So who here has heard about Rust? All right.", "tokens": [34952, 307, 2614, 586, 322, 264, 37649, 4886, 10565, 1185, 13, 407, 567, 510, 575, 2198, 466, 34952, 30, 1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.18081725045536343, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.828449866385199e-05}, {"id": 262, "seek": 143608, "start": 1443.6, "end": 1450.9199999999998, "text": " Who has used Rust? Keep your hands up. Okay. Okay. That's fairly, fairly good. And while", "tokens": [2102, 575, 1143, 34952, 30, 5527, 428, 2377, 493, 13, 1033, 13, 1033, 13, 663, 311, 6457, 11, 6457, 665, 13, 400, 1339], "temperature": 0.0, "avg_logprob": -0.18081725045536343, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.828449866385199e-05}, {"id": 263, "seek": 143608, "start": 1450.9199999999998, "end": 1457.56, "text": " that is definitely true to some degree, like there's a lot of log for that language, it's", "tokens": [300, 307, 2138, 2074, 281, 512, 4314, 11, 411, 456, 311, 257, 688, 295, 3565, 337, 300, 2856, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.18081725045536343, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.828449866385199e-05}, {"id": 264, "seek": 143608, "start": 1457.56, "end": 1463.32, "text": " even bigger in crypto because encryption, building encryption and building that safely", "tokens": [754, 3801, 294, 17240, 570, 29575, 11, 2390, 29575, 293, 2390, 300, 11750], "temperature": 0.0, "avg_logprob": -0.18081725045536343, "compression_ratio": 1.5517241379310345, "no_speech_prob": 5.828449866385199e-05}, {"id": 265, "seek": 146332, "start": 1463.32, "end": 1467.2, "text": " is really hard. At the same time, you're not, you can't really go for Python or that kind", "tokens": [307, 534, 1152, 13, 1711, 264, 912, 565, 11, 291, 434, 406, 11, 291, 393, 380, 534, 352, 337, 15329, 420, 300, 733], "temperature": 0.0, "avg_logprob": -0.12207143496623081, "compression_ratio": 1.64, "no_speech_prob": 3.590933920349926e-05}, {"id": 266, "seek": 146332, "start": 1467.2, "end": 1473.04, "text": " of stuff because it's, well, too inefficient. So most information used to you see Rust seemed", "tokens": [295, 1507, 570, 309, 311, 11, 731, 11, 886, 43495, 13, 407, 881, 1589, 1143, 281, 291, 536, 34952, 6576], "temperature": 0.0, "avg_logprob": -0.12207143496623081, "compression_ratio": 1.64, "no_speech_prob": 3.590933920349926e-05}, {"id": 267, "seek": 146332, "start": 1473.04, "end": 1477.84, "text": " like a such nice alternative. So inside crypto and encryption, Rust is already a big thing.", "tokens": [411, 257, 1270, 1481, 8535, 13, 407, 1854, 17240, 293, 29575, 11, 34952, 307, 1217, 257, 955, 551, 13], "temperature": 0.0, "avg_logprob": -0.12207143496623081, "compression_ratio": 1.64, "no_speech_prob": 3.590933920349926e-05}, {"id": 268, "seek": 146332, "start": 1477.84, "end": 1482.32, "text": " So that's probably the main reason he chose it because he wanted to use it. But there's", "tokens": [407, 300, 311, 1391, 264, 2135, 1778, 415, 5111, 309, 570, 415, 1415, 281, 764, 309, 13, 583, 456, 311], "temperature": 0.0, "avg_logprob": -0.12207143496623081, "compression_ratio": 1.64, "no_speech_prob": 3.590933920349926e-05}, {"id": 269, "seek": 146332, "start": 1482.32, "end": 1489.3999999999999, "text": " also a good amount of actual reasons why Rust makes sense to build this with. This is a", "tokens": [611, 257, 665, 2372, 295, 3539, 4112, 983, 34952, 1669, 2020, 281, 1322, 341, 365, 13, 639, 307, 257], "temperature": 0.0, "avg_logprob": -0.12207143496623081, "compression_ratio": 1.64, "no_speech_prob": 3.590933920349926e-05}, {"id": 270, "seek": 148940, "start": 1489.4, "end": 1494.8000000000002, "text": " screenshot of the website of Rustlang.org from yesterday. I'm going to break it down", "tokens": [27712, 295, 264, 3144, 295, 34952, 25241, 13, 4646, 490, 5186, 13, 286, 478, 516, 281, 1821, 309, 760], "temperature": 0.0, "avg_logprob": -0.14858555537398144, "compression_ratio": 1.4176706827309238, "no_speech_prob": 4.538376015261747e-05}, {"id": 271, "seek": 148940, "start": 1494.8000000000002, "end": 1501.3600000000001, "text": " a little more because we have to understand one key thing. Rust was invented by Mozilla", "tokens": [257, 707, 544, 570, 321, 362, 281, 1223, 472, 2141, 551, 13, 34952, 390, 14479, 538, 3335, 26403], "temperature": 0.0, "avg_logprob": -0.14858555537398144, "compression_ratio": 1.4176706827309238, "no_speech_prob": 4.538376015261747e-05}, {"id": 272, "seek": 148940, "start": 1501.3600000000001, "end": 1508.3200000000002, "text": " to build a new browser. They had Firefox 2010, 2011. They were like, there's so much C, C++", "tokens": [281, 1322, 257, 777, 11185, 13, 814, 632, 46613, 9657, 11, 10154, 13, 814, 645, 411, 11, 456, 311, 370, 709, 383, 11, 383, 25472], "temperature": 0.0, "avg_logprob": -0.14858555537398144, "compression_ratio": 1.4176706827309238, "no_speech_prob": 4.538376015261747e-05}, {"id": 273, "seek": 148940, "start": 1508.3200000000002, "end": 1514.2, "text": " in here. It's so complicated. We barely know how we can change stuff ourselves. And it's", "tokens": [294, 510, 13, 467, 311, 370, 6179, 13, 492, 10268, 458, 577, 321, 393, 1319, 1507, 4175, 13, 400, 309, 311], "temperature": 0.0, "avg_logprob": -0.14858555537398144, "compression_ratio": 1.4176706827309238, "no_speech_prob": 4.538376015261747e-05}, {"id": 274, "seek": 151420, "start": 1514.2, "end": 1520.24, "text": " like, it's still a Netscape code base in there, right? Like, it's like 20 years of stuff.", "tokens": [411, 11, 309, 311, 920, 257, 426, 1385, 4747, 3089, 3096, 294, 456, 11, 558, 30, 1743, 11, 309, 311, 411, 945, 924, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1930117406764952, "compression_ratio": 1.6605839416058394, "no_speech_prob": 1.4508311323879752e-05}, {"id": 275, "seek": 151420, "start": 1520.24, "end": 1524.24, "text": " So they were like, let's build a new browser and it's called the CERBO project as a recent", "tokens": [407, 436, 645, 411, 11, 718, 311, 1322, 257, 777, 11185, 293, 309, 311, 1219, 264, 383, 1598, 15893, 1716, 382, 257, 5162], "temperature": 0.0, "avg_logprob": -0.1930117406764952, "compression_ratio": 1.6605839416058394, "no_speech_prob": 1.4508311323879752e-05}, {"id": 276, "seek": 151420, "start": 1524.24, "end": 1529.28, "text": " research project. And through that, they realized like, there's certain things we'd like to", "tokens": [2132, 1716, 13, 400, 807, 300, 11, 436, 5334, 411, 11, 456, 311, 1629, 721, 321, 1116, 411, 281], "temperature": 0.0, "avg_logprob": -0.1930117406764952, "compression_ratio": 1.6605839416058394, "no_speech_prob": 1.4508311323879752e-05}, {"id": 277, "seek": 151420, "start": 1529.28, "end": 1535.04, "text": " have from new languages and they started building their own language to build a browser. That", "tokens": [362, 490, 777, 8650, 293, 436, 1409, 2390, 641, 1065, 2856, 281, 1322, 257, 11185, 13, 663], "temperature": 0.0, "avg_logprob": -0.1930117406764952, "compression_ratio": 1.6605839416058394, "no_speech_prob": 1.4508311323879752e-05}, {"id": 278, "seek": 151420, "start": 1535.04, "end": 1541.28, "text": " project still exists. It's CERBO.org today. Mozilla has handed off the management to the", "tokens": [1716, 920, 8198, 13, 467, 311, 383, 1598, 15893, 13, 4646, 965, 13, 3335, 26403, 575, 16013, 766, 264, 4592, 281, 264], "temperature": 0.0, "avg_logprob": -0.1930117406764952, "compression_ratio": 1.6605839416058394, "no_speech_prob": 1.4508311323879752e-05}, {"id": 279, "seek": 154128, "start": 1541.28, "end": 1545.8, "text": " Linux foundation. It's still a research project. I recommend if you want to start with Rust.", "tokens": [18734, 7030, 13, 467, 311, 920, 257, 2132, 1716, 13, 286, 2748, 498, 291, 528, 281, 722, 365, 34952, 13], "temperature": 0.0, "avg_logprob": -0.12433631397853388, "compression_ratio": 1.7461538461538462, "no_speech_prob": 8.01193618826801e-06}, {"id": 280, "seek": 154128, "start": 1545.8, "end": 1556.12, "text": " That is a really good community to start with. But the key point here is that it was a language", "tokens": [663, 307, 257, 534, 665, 1768, 281, 722, 365, 13, 583, 264, 2141, 935, 510, 307, 300, 309, 390, 257, 2856], "temperature": 0.0, "avg_logprob": -0.12433631397853388, "compression_ratio": 1.7461538461538462, "no_speech_prob": 8.01193618826801e-06}, {"id": 281, "seek": 154128, "start": 1556.12, "end": 1560.76, "text": " built by practitioners for practitioners. They didn't set out to say like, hey, let's", "tokens": [3094, 538, 25742, 337, 25742, 13, 814, 994, 380, 992, 484, 281, 584, 411, 11, 4177, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.12433631397853388, "compression_ratio": 1.7461538461538462, "no_speech_prob": 8.01193618826801e-06}, {"id": 282, "seek": 154128, "start": 1560.76, "end": 1566.76, "text": " make a theoretically proven language. Let's make a really beautiful looking language. All", "tokens": [652, 257, 29400, 12785, 2856, 13, 961, 311, 652, 257, 534, 2238, 1237, 2856, 13, 1057], "temperature": 0.0, "avg_logprob": -0.12433631397853388, "compression_ratio": 1.7461538461538462, "no_speech_prob": 8.01193618826801e-06}, {"id": 283, "seek": 154128, "start": 1566.76, "end": 1570.76, "text": " of these ideals were not existing. They wanted a language that they can use, that they're", "tokens": [295, 613, 30956, 645, 406, 6741, 13, 814, 1415, 257, 2856, 300, 436, 393, 764, 11, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.12433631397853388, "compression_ratio": 1.7461538461538462, "no_speech_prob": 8.01193618826801e-06}, {"id": 284, "seek": 157076, "start": 1570.76, "end": 1574.32, "text": " more efficient in building a browser with, which is already quite hard. If you say like,", "tokens": [544, 7148, 294, 2390, 257, 11185, 365, 11, 597, 307, 1217, 1596, 1152, 13, 759, 291, 584, 411, 11], "temperature": 0.0, "avg_logprob": -0.21628638236753403, "compression_ratio": 1.5866666666666667, "no_speech_prob": 3.5910230508306995e-05}, {"id": 285, "seek": 157076, "start": 1574.32, "end": 1578.24, "text": " I want to build a browser, that's a lot of stuff you have to do. And so they set out", "tokens": [286, 528, 281, 1322, 257, 11185, 11, 300, 311, 257, 688, 295, 1507, 291, 362, 281, 360, 13, 400, 370, 436, 992, 484], "temperature": 0.0, "avg_logprob": -0.21628638236753403, "compression_ratio": 1.5866666666666667, "no_speech_prob": 3.5910230508306995e-05}, {"id": 286, "seek": 157076, "start": 1578.24, "end": 1585.0, "text": " to build, this is the previous claim that Rust had, which is a type safe systems language,", "tokens": [281, 1322, 11, 341, 307, 264, 3894, 3932, 300, 34952, 632, 11, 597, 307, 257, 2010, 3273, 3652, 2856, 11], "temperature": 0.0, "avg_logprob": -0.21628638236753403, "compression_ratio": 1.5866666666666667, "no_speech_prob": 3.5910230508306995e-05}, {"id": 287, "seek": 157076, "start": 1585.0, "end": 1593.4, "text": " so systems language like level of C, C++, with zero cost abstractions. Well, by practitioner", "tokens": [370, 3652, 2856, 411, 1496, 295, 383, 11, 383, 25472, 11, 365, 4018, 2063, 12649, 626, 13, 1042, 11, 538, 32125], "temperature": 0.0, "avg_logprob": -0.21628638236753403, "compression_ratio": 1.5866666666666667, "no_speech_prob": 3.5910230508306995e-05}, {"id": 288, "seek": 159340, "start": 1593.4, "end": 1601.0, "text": " I said that. So it's a modern language. It reached 1.0 in 2015. It is as speedy as C and", "tokens": [286, 848, 300, 13, 407, 309, 311, 257, 4363, 2856, 13, 467, 6488, 502, 13, 15, 294, 7546, 13, 467, 307, 382, 3073, 88, 382, 383, 293], "temperature": 0.0, "avg_logprob": -0.1673344698819247, "compression_ratio": 1.6057347670250897, "no_speech_prob": 2.710666558414232e-05}, {"id": 289, "seek": 159340, "start": 1601.0, "end": 1605.2800000000002, "text": " C++. Sometimes it's speedier. The most famous example is ripgrab. If you go for that, it's", "tokens": [383, 25472, 13, 4803, 309, 311, 3073, 811, 13, 440, 881, 4618, 1365, 307, 12782, 70, 5305, 13, 759, 291, 352, 337, 300, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1673344698819247, "compression_ratio": 1.6057347670250897, "no_speech_prob": 2.710666558414232e-05}, {"id": 290, "seek": 159340, "start": 1605.2800000000002, "end": 1612.8000000000002, "text": " like 10 times faster than the next comparable implementation to grab over a lot of files.", "tokens": [411, 1266, 1413, 4663, 813, 264, 958, 25323, 11420, 281, 4444, 670, 257, 688, 295, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1673344698819247, "compression_ratio": 1.6057347670250897, "no_speech_prob": 2.710666558414232e-05}, {"id": 291, "seek": 159340, "start": 1612.8000000000002, "end": 1619.1200000000001, "text": " And it does all of that without any garbage collector or VM. Again, the goal is to have", "tokens": [400, 309, 775, 439, 295, 300, 1553, 604, 14150, 23960, 420, 18038, 13, 3764, 11, 264, 3387, 307, 281, 362], "temperature": 0.0, "avg_logprob": -0.1673344698819247, "compression_ratio": 1.6057347670250897, "no_speech_prob": 2.710666558414232e-05}, {"id": 292, "seek": 159340, "start": 1619.1200000000001, "end": 1623.1200000000001, "text": " zero cost abstractions. Any abstraction that Rust gives you, and a lot of the abstractions", "tokens": [4018, 2063, 12649, 626, 13, 2639, 37765, 300, 34952, 2709, 291, 11, 293, 257, 688, 295, 264, 12649, 626], "temperature": 0.0, "avg_logprob": -0.1673344698819247, "compression_ratio": 1.6057347670250897, "no_speech_prob": 2.710666558414232e-05}, {"id": 293, "seek": 162312, "start": 1623.12, "end": 1628.4799999999998, "text": " that the community also gives you in their own crates, has the idea of we can lower that", "tokens": [300, 264, 1768, 611, 2709, 291, 294, 641, 1065, 941, 1024, 11, 575, 264, 1558, 295, 321, 393, 3126, 300], "temperature": 0.0, "avg_logprob": -0.1397106346042677, "compression_ratio": 1.5148936170212766, "no_speech_prob": 9.817388672672678e-06}, {"id": 294, "seek": 162312, "start": 1628.4799999999998, "end": 1633.4399999999998, "text": " down at compile time to nothing. It doesn't actually exist. Therefore, garbage collector", "tokens": [760, 412, 31413, 565, 281, 1825, 13, 467, 1177, 380, 767, 2514, 13, 7504, 11, 14150, 23960], "temperature": 0.0, "avg_logprob": -0.1397106346042677, "compression_ratio": 1.5148936170212766, "no_speech_prob": 9.817388672672678e-06}, {"id": 295, "seek": 162312, "start": 1633.4399999999998, "end": 1640.12, "text": " cycles, no. VM below that, no. It should work on an embedded system. That rules out a lot", "tokens": [17796, 11, 572, 13, 18038, 2507, 300, 11, 572, 13, 467, 820, 589, 322, 364, 16741, 1185, 13, 663, 4474, 484, 257, 688], "temperature": 0.0, "avg_logprob": -0.1397106346042677, "compression_ratio": 1.5148936170212766, "no_speech_prob": 9.817388672672678e-06}, {"id": 296, "seek": 162312, "start": 1640.12, "end": 1649.08, "text": " of places. But all of that without memory safety bugs. Just probably the biggest concern", "tokens": [295, 3190, 13, 583, 439, 295, 300, 1553, 4675, 4514, 15120, 13, 1449, 1391, 264, 3880, 3136], "temperature": 0.0, "avg_logprob": -0.1397106346042677, "compression_ratio": 1.5148936170212766, "no_speech_prob": 9.817388672672678e-06}, {"id": 297, "seek": 164908, "start": 1649.08, "end": 1656.6799999999998, "text": " for any security researcher. Buffer overflows are nonexistent, effectively, in Rust. Very", "tokens": [337, 604, 3825, 21751, 13, 20254, 260, 670, 33229, 366, 6022, 87, 25367, 11, 8659, 11, 294, 34952, 13, 4372], "temperature": 0.0, "avg_logprob": -0.20762903040105646, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.5860215828288347e-05}, {"id": 298, "seek": 164908, "start": 1656.6799999999998, "end": 1663.76, "text": " famously, a couple of days ago, Google announced that since they have been shipping Rust in", "tokens": [34360, 11, 257, 1916, 295, 1708, 2057, 11, 3329, 7548, 300, 1670, 436, 362, 668, 14122, 34952, 294], "temperature": 0.0, "avg_logprob": -0.20762903040105646, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.5860215828288347e-05}, {"id": 299, "seek": 164908, "start": 1663.76, "end": 1670.6799999999998, "text": " Android, I think a third of the code that they ship in Android is now Rust, their amount", "tokens": [8853, 11, 286, 519, 257, 2636, 295, 264, 3089, 300, 436, 5374, 294, 8853, 307, 586, 34952, 11, 641, 2372], "temperature": 0.0, "avg_logprob": -0.20762903040105646, "compression_ratio": 1.4516129032258065, "no_speech_prob": 2.5860215828288347e-05}, {"id": 300, "seek": 167068, "start": 1670.68, "end": 1679.28, "text": " of memory bugs has halved, even lesser than that. And that is their main concern so far.", "tokens": [295, 4675, 15120, 575, 7523, 937, 11, 754, 22043, 813, 300, 13, 400, 300, 307, 641, 2135, 3136, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.17892428561373874, "compression_ratio": 1.7550200803212852, "no_speech_prob": 3.373191793798469e-05}, {"id": 301, "seek": 167068, "start": 1679.28, "end": 1683.48, "text": " That most of that happens at compile time. So at compile time, the compiler is a little", "tokens": [663, 881, 295, 300, 2314, 412, 31413, 565, 13, 407, 412, 31413, 565, 11, 264, 31958, 307, 257, 707], "temperature": 0.0, "avg_logprob": -0.17892428561373874, "compression_ratio": 1.7550200803212852, "no_speech_prob": 3.373191793798469e-05}, {"id": 302, "seek": 167068, "start": 1683.48, "end": 1687.2, "text": " more annoying and telling you, like, you need to tell me where this memory is going to go.", "tokens": [544, 11304, 293, 3585, 291, 11, 411, 11, 291, 643, 281, 980, 385, 689, 341, 4675, 307, 516, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.17892428561373874, "compression_ratio": 1.7550200803212852, "no_speech_prob": 3.373191793798469e-05}, {"id": 303, "seek": 167068, "start": 1687.2, "end": 1691.0, "text": " Is that in this thread or in that thread? But it also means that after it compiles,", "tokens": [1119, 300, 294, 341, 7207, 420, 294, 300, 7207, 30, 583, 309, 611, 1355, 300, 934, 309, 715, 4680, 11], "temperature": 0.0, "avg_logprob": -0.17892428561373874, "compression_ratio": 1.7550200803212852, "no_speech_prob": 3.373191793798469e-05}, {"id": 304, "seek": 167068, "start": 1691.0, "end": 1698.04, "text": " it runs. But again, because it's built from practitioners for practitioners, it's not", "tokens": [309, 6676, 13, 583, 797, 11, 570, 309, 311, 3094, 490, 25742, 337, 25742, 11, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.17892428561373874, "compression_ratio": 1.7550200803212852, "no_speech_prob": 3.373191793798469e-05}, {"id": 305, "seek": 169804, "start": 1698.04, "end": 1702.84, "text": " just about the language. Like, you need to be able to actually work with that. That means", "tokens": [445, 466, 264, 2856, 13, 1743, 11, 291, 643, 281, 312, 1075, 281, 767, 589, 365, 300, 13, 663, 1355], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 306, "seek": 169804, "start": 1702.84, "end": 1710.28, "text": " that it's very famous for its very good tooling. It has a really nice compiler that very famously", "tokens": [300, 309, 311, 588, 4618, 337, 1080, 588, 665, 46593, 13, 467, 575, 257, 534, 1481, 31958, 300, 588, 34360], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 307, "seek": 169804, "start": 1710.28, "end": 1714.2, "text": " when people jump from other languages and they run through the first error, they see", "tokens": [562, 561, 3012, 490, 661, 8650, 293, 436, 1190, 807, 264, 700, 6713, 11, 436, 536], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 308, "seek": 169804, "start": 1714.2, "end": 1717.8, "text": " the compiler complain and they switch immediately back to look at the code. In Rust, you don't", "tokens": [264, 31958, 11024, 293, 436, 3679, 4258, 646, 281, 574, 412, 264, 3089, 13, 682, 34952, 11, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 309, "seek": 169804, "start": 1717.8, "end": 1721.36, "text": " do that. The compiler is probably going to tell you what you need to change. Or at least", "tokens": [360, 300, 13, 440, 31958, 307, 1391, 516, 281, 980, 291, 437, 291, 643, 281, 1319, 13, 1610, 412, 1935], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 310, "seek": 169804, "start": 1721.36, "end": 1726.44, "text": " what they, what it thinks you need to change to make that run. That is a completely behavioral", "tokens": [437, 436, 11, 437, 309, 7309, 291, 643, 281, 1319, 281, 652, 300, 1190, 13, 663, 307, 257, 2584, 19124], "temperature": 0.0, "avg_logprob": -0.1599023232093224, "compression_ratio": 1.8366666666666667, "no_speech_prob": 2.0141420463914983e-05}, {"id": 311, "seek": 172644, "start": 1726.44, "end": 1731.16, "text": " change. The compiler is your friend telling you, look, you need to just tell me, is that", "tokens": [1319, 13, 440, 31958, 307, 428, 1277, 3585, 291, 11, 574, 11, 291, 643, 281, 445, 980, 385, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.1543136943470348, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.5452967747696675e-05}, {"id": 312, "seek": 172644, "start": 1731.16, "end": 1735.6000000000001, "text": " in this thread or in that thread? This is what I assume you would want to do. It can", "tokens": [294, 341, 7207, 420, 294, 300, 7207, 30, 639, 307, 437, 286, 6552, 291, 576, 528, 281, 360, 13, 467, 393], "temperature": 0.0, "avg_logprob": -0.1543136943470348, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.5452967747696675e-05}, {"id": 313, "seek": 172644, "start": 1735.6000000000001, "end": 1739.76, "text": " be wrong, of course, because you have higher level abstraction that you need to work with.", "tokens": [312, 2085, 11, 295, 1164, 11, 570, 291, 362, 2946, 1496, 37765, 300, 291, 643, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.1543136943470348, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.5452967747696675e-05}, {"id": 314, "seek": 172644, "start": 1739.76, "end": 1745.8, "text": " But overall, it's pretty good. The same for cargo, which is the package management system", "tokens": [583, 4787, 11, 309, 311, 1238, 665, 13, 440, 912, 337, 19449, 11, 597, 307, 264, 7372, 4592, 1185], "temperature": 0.0, "avg_logprob": -0.1543136943470348, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.5452967747696675e-05}, {"id": 315, "seek": 172644, "start": 1745.8, "end": 1751.0800000000002, "text": " and build system, but also Rust up, which is the meta version of organizing your own", "tokens": [293, 1322, 1185, 11, 457, 611, 34952, 493, 11, 597, 307, 264, 19616, 3037, 295, 17608, 428, 1065], "temperature": 0.0, "avg_logprob": -0.1543136943470348, "compression_ratio": 1.6319702602230484, "no_speech_prob": 2.5452967747696675e-05}, {"id": 316, "seek": 175108, "start": 1751.08, "end": 1764.72, "text": " Rust installation. And all of that, it provides with being built against the LLVM backend,", "tokens": [34952, 13260, 13, 400, 439, 295, 300, 11, 309, 6417, 365, 885, 3094, 1970, 264, 441, 43, 53, 44, 38087, 11], "temperature": 0.0, "avg_logprob": -0.1109904673562121, "compression_ratio": 1.403061224489796, "no_speech_prob": 2.0455685444176197e-05}, {"id": 317, "seek": 175108, "start": 1764.72, "end": 1769.6399999999999, "text": " which means it's more or less instantly portable. When you can run it and you don't have any", "tokens": [597, 1355, 309, 311, 544, 420, 1570, 13518, 21800, 13, 1133, 291, 393, 1190, 309, 293, 291, 500, 380, 362, 604], "temperature": 0.0, "avg_logprob": -0.1109904673562121, "compression_ratio": 1.403061224489796, "no_speech_prob": 2.0455685444176197e-05}, {"id": 318, "seek": 175108, "start": 1769.6399999999999, "end": 1776.52, "text": " specific architecture code for your Mac, it will compile for Windows as well. You basically", "tokens": [2685, 9482, 3089, 337, 428, 5707, 11, 309, 486, 31413, 337, 8591, 382, 731, 13, 509, 1936], "temperature": 0.0, "avg_logprob": -0.1109904673562121, "compression_ratio": 1.403061224489796, "no_speech_prob": 2.0455685444176197e-05}, {"id": 319, "seek": 177652, "start": 1776.52, "end": 1781.48, "text": " just have to say there's another target. The way that LLVM works, it has an abstract syntax", "tokens": [445, 362, 281, 584, 456, 311, 1071, 3779, 13, 440, 636, 300, 441, 43, 53, 44, 1985, 11, 309, 575, 364, 12649, 28431], "temperature": 0.0, "avg_logprob": -0.15289088427010228, "compression_ratio": 1.7159090909090908, "no_speech_prob": 2.014301389863249e-05}, {"id": 320, "seek": 177652, "start": 1781.48, "end": 1786.04, "text": " tree of its own in between. We compile, basically Rust compiles to that. And then everything", "tokens": [4230, 295, 1080, 1065, 294, 1296, 13, 492, 31413, 11, 1936, 34952, 715, 4680, 281, 300, 13, 400, 550, 1203], "temperature": 0.0, "avg_logprob": -0.15289088427010228, "compression_ratio": 1.7159090909090908, "no_speech_prob": 2.014301389863249e-05}, {"id": 321, "seek": 177652, "start": 1786.04, "end": 1791.48, "text": " that LLVM supports as a target, it can compile to you. And that is pretty amazing. That led", "tokens": [300, 441, 43, 53, 44, 9346, 382, 257, 3779, 11, 309, 393, 31413, 281, 291, 13, 400, 300, 307, 1238, 2243, 13, 663, 4684], "temperature": 0.0, "avg_logprob": -0.15289088427010228, "compression_ratio": 1.7159090909090908, "no_speech_prob": 2.014301389863249e-05}, {"id": 322, "seek": 177652, "start": 1791.48, "end": 1799.12, "text": " to Rust being the very first language that had native support for WebAssembly as a target", "tokens": [281, 34952, 885, 264, 588, 700, 2856, 300, 632, 8470, 1406, 337, 9573, 10884, 19160, 382, 257, 3779], "temperature": 0.0, "avg_logprob": -0.15289088427010228, "compression_ratio": 1.7159090909090908, "no_speech_prob": 2.014301389863249e-05}, {"id": 323, "seek": 177652, "start": 1799.12, "end": 1804.12, "text": " language. Because it was just switching on, oh, yeah, the target for that. At the same", "tokens": [2856, 13, 1436, 309, 390, 445, 16493, 322, 11, 1954, 11, 1338, 11, 264, 3779, 337, 300, 13, 1711, 264, 912], "temperature": 0.0, "avg_logprob": -0.15289088427010228, "compression_ratio": 1.7159090909090908, "no_speech_prob": 2.014301389863249e-05}, {"id": 324, "seek": 180412, "start": 1804.12, "end": 1811.9599999999998, "text": " time, sorry, my voice is still a little sick, it allows you to have a C-compatible lip interface.", "tokens": [565, 11, 2597, 11, 452, 3177, 307, 920, 257, 707, 4998, 11, 309, 4045, 291, 281, 362, 257, 383, 12, 1112, 11584, 964, 8280, 9226, 13], "temperature": 0.0, "avg_logprob": -0.2198514185453716, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.4967999049986247e-05}, {"id": 325, "seek": 180412, "start": 1811.9599999999998, "end": 1820.84, "text": " And that makes it really nice to embed it into other stuff and use it as a library.", "tokens": [400, 300, 1669, 309, 534, 1481, 281, 12240, 309, 666, 661, 1507, 293, 764, 309, 382, 257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2198514185453716, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.4967999049986247e-05}, {"id": 326, "seek": 180412, "start": 1820.84, "end": 1829.9199999999998, "text": " All right, so that's Rust. What currently do we have in the Rust SDK now, a year later?", "tokens": [1057, 558, 11, 370, 300, 311, 34952, 13, 708, 4362, 360, 321, 362, 294, 264, 34952, 37135, 586, 11, 257, 1064, 1780, 30], "temperature": 0.0, "avg_logprob": -0.2198514185453716, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.4967999049986247e-05}, {"id": 327, "seek": 182992, "start": 1829.92, "end": 1837.2, "text": " The idea is essentially that everything you need to have to build a matrix client, it", "tokens": [440, 1558, 307, 4476, 300, 1203, 291, 643, 281, 362, 281, 1322, 257, 8141, 6423, 11, 309], "temperature": 0.0, "avg_logprob": -0.17022999657524956, "compression_ratio": 1.5301724137931034, "no_speech_prob": 3.2883413041417953e-06}, {"id": 328, "seek": 182992, "start": 1837.2, "end": 1844.28, "text": " should be there. Batteries included. That specifically means we want, we have an async", "tokens": [820, 312, 456, 13, 33066, 530, 5556, 13, 663, 4682, 1355, 321, 528, 11, 321, 362, 364, 382, 34015], "temperature": 0.0, "avg_logprob": -0.17022999657524956, "compression_ratio": 1.5301724137931034, "no_speech_prob": 3.2883413041417953e-06}, {"id": 329, "seek": 182992, "start": 1844.28, "end": 1849.28, "text": " type safe API. Like requests you do, they're type safe. They come back, we check that the", "tokens": [2010, 3273, 9362, 13, 1743, 12475, 291, 360, 11, 436, 434, 2010, 3273, 13, 814, 808, 646, 11, 321, 1520, 300, 264], "temperature": 0.0, "avg_logprob": -0.17022999657524956, "compression_ratio": 1.5301724137931034, "no_speech_prob": 3.2883413041417953e-06}, {"id": 330, "seek": 182992, "start": 1849.28, "end": 1858.0, "text": " JSON that comes back is what it needs to be. It has a full featured room state. So for every", "tokens": [31828, 300, 1487, 646, 307, 437, 309, 2203, 281, 312, 13, 467, 575, 257, 1577, 13822, 1808, 1785, 13, 407, 337, 633], "temperature": 0.0, "avg_logprob": -0.17022999657524956, "compression_ratio": 1.5301724137931034, "no_speech_prob": 3.2883413041417953e-06}, {"id": 331, "seek": 185800, "start": 1858.0, "end": 1861.8, "text": " room that you're in, it can tell you, can you write in that room? What kind of messages", "tokens": [1808, 300, 291, 434, 294, 11, 309, 393, 980, 291, 11, 393, 291, 2464, 294, 300, 1808, 30, 708, 733, 295, 7897], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 332, "seek": 185800, "start": 1861.8, "end": 1866.12, "text": " can you write? What are the other users in the room? What is their avatar? What other", "tokens": [393, 291, 2464, 30, 708, 366, 264, 661, 5022, 294, 264, 1808, 30, 708, 307, 641, 36205, 30, 708, 661], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 333, "seek": 185800, "start": 1866.12, "end": 1872.0, "text": " states do they have? All of that stuff, it is managing for you. You don't have to bother", "tokens": [4368, 360, 436, 362, 30, 1057, 295, 300, 1507, 11, 309, 307, 11642, 337, 291, 13, 509, 500, 380, 362, 281, 8677], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 334, "seek": 185800, "start": 1872.0, "end": 1878.0, "text": " too much about that. It has a persistent storage layer support. So you don't have to worry about", "tokens": [886, 709, 466, 300, 13, 467, 575, 257, 24315, 6725, 4583, 1406, 13, 407, 291, 500, 380, 362, 281, 3292, 466], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 335, "seek": 185800, "start": 1878.0, "end": 1882.56, "text": " caching it or putting it somewhere locally. You can still do that on your own if you want.", "tokens": [269, 2834, 309, 420, 3372, 309, 4079, 16143, 13, 509, 393, 920, 360, 300, 322, 428, 1065, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 336, "seek": 185800, "start": 1882.56, "end": 1886.56, "text": " It is a pluggable interface, but it already comes with a native version which is kind", "tokens": [467, 307, 257, 499, 3562, 712, 9226, 11, 457, 309, 1217, 1487, 365, 257, 8470, 3037, 597, 307, 733], "temperature": 0.0, "avg_logprob": -0.12024108282953715, "compression_ratio": 1.7986577181208054, "no_speech_prob": 1.892376531031914e-05}, {"id": 337, "seek": 188656, "start": 1886.56, "end": 1892.04, "text": " of a deprecated slab which we intend to replace with the SQLite which is still partially there", "tokens": [295, 257, 1367, 13867, 770, 38616, 597, 321, 19759, 281, 7406, 365, 264, 19200, 642, 597, 307, 920, 18886, 456], "temperature": 0.0, "avg_logprob": -0.22953977423199154, "compression_ratio": 1.635036496350365, "no_speech_prob": 3.320947507745586e-05}, {"id": 338, "seek": 188656, "start": 1892.04, "end": 1896.56, "text": " for crypto, but not from the other side yet. But it also has, for example, support for", "tokens": [337, 17240, 11, 457, 406, 490, 264, 661, 1252, 1939, 13, 583, 309, 611, 575, 11, 337, 1365, 11, 1406, 337], "temperature": 0.0, "avg_logprob": -0.22953977423199154, "compression_ratio": 1.635036496350365, "no_speech_prob": 3.320947507745586e-05}, {"id": 339, "seek": 188656, "start": 1896.56, "end": 1902.2, "text": " web, for indexeddb. So you can run it in the browser. One of the examples actually is an", "tokens": [3670, 11, 337, 8186, 292, 67, 65, 13, 407, 291, 393, 1190, 309, 294, 264, 11185, 13, 1485, 295, 264, 5110, 767, 307, 364], "temperature": 0.0, "avg_logprob": -0.22953977423199154, "compression_ratio": 1.635036496350365, "no_speech_prob": 3.320947507745586e-05}, {"id": 340, "seek": 188656, "start": 1902.2, "end": 1910.28, "text": " echo bot that runs in your browser in Wasm. It's pretty awesome. And for us, almost the", "tokens": [14300, 10592, 300, 6676, 294, 428, 11185, 294, 3027, 76, 13, 467, 311, 1238, 3476, 13, 400, 337, 505, 11, 1920, 264], "temperature": 0.0, "avg_logprob": -0.22953977423199154, "compression_ratio": 1.635036496350365, "no_speech_prob": 3.320947507745586e-05}, {"id": 341, "seek": 188656, "start": 1910.28, "end": 1914.8799999999999, "text": " most important part is that it has transparent end-to-end encryption support. When you're", "tokens": [881, 1021, 644, 307, 300, 309, 575, 12737, 917, 12, 1353, 12, 521, 29575, 1406, 13, 1133, 291, 434], "temperature": 0.0, "avg_logprob": -0.22953977423199154, "compression_ratio": 1.635036496350365, "no_speech_prob": 3.320947507745586e-05}, {"id": 342, "seek": 191488, "start": 1914.88, "end": 1919.64, "text": " in a room and that room is encrypted, it's going to send the messages out to get the", "tokens": [294, 257, 1808, 293, 300, 1808, 307, 36663, 11, 309, 311, 516, 281, 2845, 264, 7897, 484, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.115592315133694, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.6469209589995444e-05}, {"id": 343, "seek": 191488, "start": 1919.64, "end": 1925.2800000000002, "text": " keys that you need to allow you to verify with a different device. But from the point", "tokens": [9317, 300, 291, 643, 281, 2089, 291, 281, 16888, 365, 257, 819, 4302, 13, 583, 490, 264, 935], "temperature": 0.0, "avg_logprob": -0.115592315133694, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.6469209589995444e-05}, {"id": 344, "seek": 191488, "start": 1925.2800000000002, "end": 1931.0800000000002, "text": " that you join with a new device and you just say room send and you give it the message,", "tokens": [300, 291, 3917, 365, 257, 777, 4302, 293, 291, 445, 584, 1808, 2845, 293, 291, 976, 309, 264, 3636, 11], "temperature": 0.0, "avg_logprob": -0.115592315133694, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.6469209589995444e-05}, {"id": 345, "seek": 191488, "start": 1931.0800000000002, "end": 1937.64, "text": " it's going to send an encrypted message. That's it. For the most part of it, unless user interaction", "tokens": [309, 311, 516, 281, 2845, 364, 36663, 3636, 13, 663, 311, 309, 13, 1171, 264, 881, 644, 295, 309, 11, 5969, 4195, 9285], "temperature": 0.0, "avg_logprob": -0.115592315133694, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.6469209589995444e-05}, {"id": 346, "seek": 191488, "start": 1937.64, "end": 1941.7600000000002, "text": " is required, you don't have to bother about that. It's going to store that information.", "tokens": [307, 4739, 11, 291, 500, 380, 362, 281, 8677, 466, 300, 13, 467, 311, 516, 281, 3531, 300, 1589, 13], "temperature": 0.0, "avg_logprob": -0.115592315133694, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.6469209589995444e-05}, {"id": 347, "seek": 194176, "start": 1941.76, "end": 1945.44, "text": " It's going to make sure that when you start up the next time through the storage support", "tokens": [467, 311, 516, 281, 652, 988, 300, 562, 291, 722, 493, 264, 958, 565, 807, 264, 6725, 1406], "temperature": 0.0, "avg_logprob": -0.09683269535729644, "compression_ratio": 1.7440944881889764, "no_speech_prob": 1.8056409317068756e-05}, {"id": 348, "seek": 194176, "start": 1945.44, "end": 1950.72, "text": " that you have all the keys there, you don't have to bother about there being an additional", "tokens": [300, 291, 362, 439, 264, 9317, 456, 11, 291, 500, 380, 362, 281, 8677, 466, 456, 885, 364, 4497], "temperature": 0.0, "avg_logprob": -0.09683269535729644, "compression_ratio": 1.7440944881889764, "no_speech_prob": 1.8056409317068756e-05}, {"id": 349, "seek": 194176, "start": 1950.72, "end": 1954.4, "text": " end-to-end encryption that you have to take care with. I already mentioned that it has", "tokens": [917, 12, 1353, 12, 521, 29575, 300, 291, 362, 281, 747, 1127, 365, 13, 286, 1217, 2835, 300, 309, 575], "temperature": 0.0, "avg_logprob": -0.09683269535729644, "compression_ratio": 1.7440944881889764, "no_speech_prob": 1.8056409317068756e-05}, {"id": 350, "seek": 194176, "start": 1954.4, "end": 1960.68, "text": " Wasm and web support. And because of the C layer, we're also able to offer support", "tokens": [3027, 76, 293, 3670, 1406, 13, 400, 570, 295, 264, 383, 4583, 11, 321, 434, 611, 1075, 281, 2626, 1406], "temperature": 0.0, "avg_logprob": -0.09683269535729644, "compression_ratio": 1.7440944881889764, "no_speech_prob": 1.8056409317068756e-05}, {"id": 351, "seek": 194176, "start": 1960.68, "end": 1967.24, "text": " to different bindings out there. So we have two bindings that are used in the next generation", "tokens": [281, 819, 14786, 1109, 484, 456, 13, 407, 321, 362, 732, 14786, 1109, 300, 366, 1143, 294, 264, 958, 5125], "temperature": 0.0, "avg_logprob": -0.09683269535729644, "compression_ratio": 1.7440944881889764, "no_speech_prob": 1.8056409317068756e-05}, {"id": 352, "seek": 196724, "start": 1967.24, "end": 1972.1200000000001, "text": " of element apps. You're going to see that later for Kotlin and Swift through Uni-FFI.", "tokens": [295, 4478, 7733, 13, 509, 434, 516, 281, 536, 300, 1780, 337, 30123, 5045, 293, 25539, 807, 35191, 12, 6345, 40, 13], "temperature": 0.0, "avg_logprob": -0.20935829564144737, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.9770635592285544e-05}, {"id": 353, "seek": 196724, "start": 1972.1200000000001, "end": 1978.76, "text": " But there's also custom bindings for Node.js and for JS on the web as well. I think there's", "tokens": [583, 456, 311, 611, 2375, 14786, 1109, 337, 38640, 13, 25530, 293, 337, 33063, 322, 264, 3670, 382, 731, 13, 286, 519, 456, 311], "temperature": 0.0, "avg_logprob": -0.20935829564144737, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.9770635592285544e-05}, {"id": 354, "seek": 196724, "start": 1978.76, "end": 1985.36, "text": " Python bindings out there, but they're not maintained by us. This all allows us to go", "tokens": [15329, 14786, 1109, 484, 456, 11, 457, 436, 434, 406, 17578, 538, 505, 13, 639, 439, 4045, 505, 281, 352], "temperature": 0.0, "avg_logprob": -0.20935829564144737, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.9770635592285544e-05}, {"id": 355, "seek": 196724, "start": 1985.36, "end": 1993.32, "text": " beyond what we have so far. It allows us to ingrain more of the stuff that different", "tokens": [4399, 437, 321, 362, 370, 1400, 13, 467, 4045, 505, 281, 3957, 7146, 544, 295, 264, 1507, 300, 819], "temperature": 0.0, "avg_logprob": -0.20935829564144737, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.9770635592285544e-05}, {"id": 356, "seek": 199332, "start": 1993.32, "end": 1998.76, "text": " clients and implementations have been using, but that has verily cross-pollinated. If you", "tokens": [6982, 293, 4445, 763, 362, 668, 1228, 11, 457, 300, 575, 1306, 953, 3278, 12, 79, 1833, 5410, 13, 759, 291], "temperature": 0.0, "avg_logprob": -0.1225486928766424, "compression_ratio": 1.5650684931506849, "no_speech_prob": 1.618531132407952e-05}, {"id": 357, "seek": 199332, "start": 1998.76, "end": 2004.56, "text": " had a really clever way of managing your timeline in Android, the iOS people wouldn't know. That", "tokens": [632, 257, 534, 13494, 636, 295, 11642, 428, 12933, 294, 8853, 11, 264, 17430, 561, 2759, 380, 458, 13, 663], "temperature": 0.0, "avg_logprob": -0.1225486928766424, "compression_ratio": 1.5650684931506849, "no_speech_prob": 1.618531132407952e-05}, {"id": 358, "seek": 199332, "start": 2004.56, "end": 2009.52, "text": " all converges into this singular place now. And that allows us to do a lot more things", "tokens": [439, 9652, 2880, 666, 341, 20010, 1081, 586, 13, 400, 300, 4045, 505, 281, 360, 257, 688, 544, 721], "temperature": 0.0, "avg_logprob": -0.1225486928766424, "compression_ratio": 1.5650684931506849, "no_speech_prob": 1.618531132407952e-05}, {"id": 359, "seek": 199332, "start": 2009.52, "end": 2015.6799999999998, "text": " a lot quicker. One of the things that we currently do is we offer a new experimental timeline", "tokens": [257, 688, 16255, 13, 1485, 295, 264, 721, 300, 321, 4362, 360, 307, 321, 2626, 257, 777, 17069, 12933], "temperature": 0.0, "avg_logprob": -0.1225486928766424, "compression_ratio": 1.5650684931506849, "no_speech_prob": 1.618531132407952e-05}, {"id": 360, "seek": 199332, "start": 2015.6799999999998, "end": 2022.56, "text": " API that manages the state for you. Like back in 2018, 2019, editing messages came around", "tokens": [9362, 300, 22489, 264, 1785, 337, 291, 13, 1743, 646, 294, 6096, 11, 6071, 11, 10000, 7897, 1361, 926], "temperature": 0.0, "avg_logprob": -0.1225486928766424, "compression_ratio": 1.5650684931506849, "no_speech_prob": 1.618531132407952e-05}, {"id": 361, "seek": 202256, "start": 2022.56, "end": 2027.8, "text": " and that fundamentally changed the idea of an event in Matrix. It's just not a stream", "tokens": [293, 300, 17879, 3105, 264, 1558, 295, 364, 2280, 294, 36274, 13, 467, 311, 445, 406, 257, 4309], "temperature": 0.0, "avg_logprob": -0.1404248901775905, "compression_ratio": 1.6980392156862745, "no_speech_prob": 2.429164851491805e-05}, {"id": 362, "seek": 202256, "start": 2027.8, "end": 2032.9199999999998, "text": " of events anymore, but events acting upon other events. This changes a message from", "tokens": [295, 3931, 3602, 11, 457, 3931, 6577, 3564, 661, 3931, 13, 639, 2962, 257, 3636, 490], "temperature": 0.0, "avg_logprob": -0.1404248901775905, "compression_ratio": 1.6980392156862745, "no_speech_prob": 2.429164851491805e-05}, {"id": 363, "seek": 202256, "start": 2032.9199999999998, "end": 2037.1599999999999, "text": " a previous thing. With a new timeline API, you don't have to bother. We're just going", "tokens": [257, 3894, 551, 13, 2022, 257, 777, 12933, 9362, 11, 291, 500, 380, 362, 281, 8677, 13, 492, 434, 445, 516], "temperature": 0.0, "avg_logprob": -0.1404248901775905, "compression_ratio": 1.6980392156862745, "no_speech_prob": 2.429164851491805e-05}, {"id": 364, "seek": 202256, "start": 2037.1599999999999, "end": 2042.8, "text": " to tell you, oh, position 17. This is now this. The same is true for redactions. The", "tokens": [281, 980, 291, 11, 1954, 11, 2535, 3282, 13, 639, 307, 586, 341, 13, 440, 912, 307, 2074, 337, 2182, 12299, 13, 440], "temperature": 0.0, "avg_logprob": -0.1404248901775905, "compression_ratio": 1.6980392156862745, "no_speech_prob": 2.429164851491805e-05}, {"id": 365, "seek": 202256, "start": 2042.8, "end": 2048.6, "text": " same is true for reactions. All of these things ensune threads. I don't know how we're going", "tokens": [912, 307, 2074, 337, 12215, 13, 1057, 295, 613, 721, 3489, 2613, 19314, 13, 286, 500, 380, 458, 577, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1404248901775905, "compression_ratio": 1.6980392156862745, "no_speech_prob": 2.429164851491805e-05}, {"id": 366, "seek": 204860, "start": 2048.6, "end": 2053.44, "text": " to do threads yet, but that all is supposed to be right there. You don't have to bother", "tokens": [281, 360, 19314, 1939, 11, 457, 300, 439, 307, 3442, 281, 312, 558, 456, 13, 509, 500, 380, 362, 281, 8677], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 367, "seek": 204860, "start": 2053.44, "end": 2056.72, "text": " about the state machine changes that this requires. It's just going to tell you, hey,", "tokens": [466, 264, 1785, 3479, 2962, 300, 341, 7029, 13, 467, 311, 445, 516, 281, 980, 291, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 368, "seek": 204860, "start": 2056.72, "end": 2061.7999999999997, "text": " you need to render a different thing now. The other thing that was mentioned before", "tokens": [291, 643, 281, 15529, 257, 819, 551, 586, 13, 440, 661, 551, 300, 390, 2835, 949], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 369, "seek": 204860, "start": 2061.7999999999997, "end": 2066.48, "text": " as well is support for sliding sync. Both of these are still experimental. You have to", "tokens": [382, 731, 307, 1406, 337, 21169, 20271, 13, 6767, 295, 613, 366, 920, 17069, 13, 509, 362, 281], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 370, "seek": 204860, "start": 2066.48, "end": 2070.72, "text": " actively switch them on because it's interfaces that we're not confident with that are going", "tokens": [13022, 3679, 552, 322, 570, 309, 311, 28416, 300, 321, 434, 406, 6679, 365, 300, 366, 516], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 371, "seek": 204860, "start": 2070.72, "end": 2075.08, "text": " to stick exactly the way they are, but there's implementations out there using that.", "tokens": [281, 2897, 2293, 264, 636, 436, 366, 11, 457, 456, 311, 4445, 763, 484, 456, 1228, 300, 13], "temperature": 0.0, "avg_logprob": -0.13030146777145263, "compression_ratio": 1.769491525423729, "no_speech_prob": 1.3845128705725074e-05}, {"id": 372, "seek": 207508, "start": 2075.08, "end": 2083.36, "text": " All right, so does it work? Does it live up to the promise? Well, let's see. In order", "tokens": [1057, 558, 11, 370, 775, 309, 589, 30, 4402, 309, 1621, 493, 281, 264, 6228, 30, 1042, 11, 718, 311, 536, 13, 682, 1668], "temperature": 0.0, "avg_logprob": -0.18613684907251474, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.985349369235337e-05}, {"id": 373, "seek": 207508, "start": 2083.36, "end": 2091.84, "text": " to build sliding sync, I built a small testing UI. With sliding sync right now, this is Mr.", "tokens": [281, 1322, 21169, 20271, 11, 286, 3094, 257, 1359, 4997, 15682, 13, 2022, 21169, 20271, 558, 586, 11, 341, 307, 2221, 13], "temperature": 0.0, "avg_logprob": -0.18613684907251474, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.985349369235337e-05}, {"id": 374, "seek": 207508, "start": 2091.84, "end": 2096.3199999999997, "text": " Big, my test account with, I don't know how many rooms, but usually loading it on an element", "tokens": [5429, 11, 452, 1500, 2696, 365, 11, 286, 500, 380, 458, 577, 867, 9396, 11, 457, 2673, 15114, 309, 322, 364, 4478], "temperature": 0.0, "avg_logprob": -0.18613684907251474, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.985349369235337e-05}, {"id": 375, "seek": 207508, "start": 2096.3199999999997, "end": 2104.2, "text": " web is like a minute for initials sync. With my timeline, with sliding sync up and this", "tokens": [3670, 307, 411, 257, 3456, 337, 5883, 82, 20271, 13, 2022, 452, 12933, 11, 365, 21169, 20271, 493, 293, 341], "temperature": 0.0, "avg_logprob": -0.18613684907251474, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.985349369235337e-05}, {"id": 376, "seek": 210420, "start": 2104.2, "end": 2110.7599999999998, "text": " testing system, it's 200 milliseconds. It's 200 milliseconds to render the room. You can", "tokens": [4997, 1185, 11, 309, 311, 2331, 34184, 13, 467, 311, 2331, 34184, 281, 15529, 264, 1808, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.15741923451423645, "compression_ratio": 1.6211453744493391, "no_speech_prob": 4.356611498224083e-06}, {"id": 377, "seek": 210420, "start": 2110.7599999999998, "end": 2117.3599999999997, "text": " see this down here. And to pull up all other rooms, it's like another 30 milliseconds.", "tokens": [536, 341, 760, 510, 13, 400, 281, 2235, 493, 439, 661, 9396, 11, 309, 311, 411, 1071, 2217, 34184, 13], "temperature": 0.0, "avg_logprob": -0.15741923451423645, "compression_ratio": 1.6211453744493391, "no_speech_prob": 4.356611498224083e-06}, {"id": 378, "seek": 210420, "start": 2117.3599999999997, "end": 2127.3999999999996, "text": " So yeah, it's fast. It does what it's supposed to do. But is that actually true? I'm a core", "tokens": [407, 1338, 11, 309, 311, 2370, 13, 467, 775, 437, 309, 311, 3442, 281, 360, 13, 583, 307, 300, 767, 2074, 30, 286, 478, 257, 4965], "temperature": 0.0, "avg_logprob": -0.15741923451423645, "compression_ratio": 1.6211453744493391, "no_speech_prob": 4.356611498224083e-06}, {"id": 379, "seek": 210420, "start": 2127.3999999999996, "end": 2134.04, "text": " developer. Of course, the thing that I'm building here is hopefully going to work, but how plausible", "tokens": [10754, 13, 2720, 1164, 11, 264, 551, 300, 286, 478, 2390, 510, 307, 4696, 516, 281, 589, 11, 457, 577, 39925], "temperature": 0.0, "avg_logprob": -0.15741923451423645, "compression_ratio": 1.6211453744493391, "no_speech_prob": 4.356611498224083e-06}, {"id": 380, "seek": 213404, "start": 2134.04, "end": 2139.68, "text": " is that as a SDK? Like maybe I'm just building a lot of stuff. Okay, let's take a look at", "tokens": [307, 300, 382, 257, 37135, 30, 1743, 1310, 286, 478, 445, 2390, 257, 688, 295, 1507, 13, 1033, 11, 718, 311, 747, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.16822606867009943, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.892250838864129e-05}, {"id": 381, "seek": 213404, "start": 2139.68, "end": 2145.88, "text": " the thing itself. So the implementation on top of the Rust SDK for this UI is a whopping", "tokens": [264, 551, 2564, 13, 407, 264, 11420, 322, 1192, 295, 264, 34952, 37135, 337, 341, 15682, 307, 257, 50043], "temperature": 0.0, "avg_logprob": -0.16822606867009943, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.892250838864129e-05}, {"id": 382, "seek": 213404, "start": 2145.88, "end": 2152.96, "text": " 2,000 lines. It's pretty small. And most of that is actually 2e realm stuff because actually", "tokens": [568, 11, 1360, 3876, 13, 467, 311, 1238, 1359, 13, 400, 881, 295, 300, 307, 767, 568, 68, 15355, 1507, 570, 767], "temperature": 0.0, "avg_logprob": -0.16822606867009943, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.892250838864129e-05}, {"id": 383, "seek": 213404, "start": 2152.96, "end": 2156.8, "text": " 2e's in Rust are not that great, so you have to do a lot of state management. The actual", "tokens": [568, 68, 311, 294, 34952, 366, 406, 300, 869, 11, 370, 291, 362, 281, 360, 257, 688, 295, 1785, 4592, 13, 440, 3539], "temperature": 0.0, "avg_logprob": -0.16822606867009943, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.892250838864129e-05}, {"id": 384, "seek": 215680, "start": 2156.8, "end": 2164.6000000000004, "text": " implementation of managing the Rust SDK is less than 130 lines of code. Everything else", "tokens": [11420, 295, 11642, 264, 34952, 37135, 307, 1570, 813, 19966, 3876, 295, 3089, 13, 5471, 1646], "temperature": 0.0, "avg_logprob": -0.16136368838223544, "compression_ratio": 1.4767932489451476, "no_speech_prob": 5.506973593583098e-06}, {"id": 385, "seek": 215680, "start": 2164.6000000000004, "end": 2169.32, "text": " you saw, including that this stores it on your hard drive, totally abstracted away. I", "tokens": [291, 1866, 11, 3009, 300, 341, 9512, 309, 322, 428, 1152, 3332, 11, 3879, 12649, 292, 1314, 13, 286], "temperature": 0.0, "avg_logprob": -0.16136368838223544, "compression_ratio": 1.4767932489451476, "no_speech_prob": 5.506973593583098e-06}, {"id": 386, "seek": 215680, "start": 2169.32, "end": 2176.7200000000003, "text": " don't have to bother about this from that perspective. So I would say, yeah, definitely.", "tokens": [500, 380, 362, 281, 8677, 466, 341, 490, 300, 4585, 13, 407, 286, 576, 584, 11, 1338, 11, 2138, 13], "temperature": 0.0, "avg_logprob": -0.16136368838223544, "compression_ratio": 1.4767932489451476, "no_speech_prob": 5.506973593583098e-06}, {"id": 387, "seek": 215680, "start": 2176.7200000000003, "end": 2184.36, "text": " It does SDK. But again, I'm a core developer. Hopefully it's easy for me to build this.", "tokens": [467, 775, 37135, 13, 583, 797, 11, 286, 478, 257, 4965, 10754, 13, 10429, 309, 311, 1858, 337, 385, 281, 1322, 341, 13], "temperature": 0.0, "avg_logprob": -0.16136368838223544, "compression_ratio": 1.4767932489451476, "no_speech_prob": 5.506973593583098e-06}, {"id": 388, "seek": 218436, "start": 2184.36, "end": 2189.88, "text": " It should be fairly okay to build something as quick. But of course, it's supposed to", "tokens": [467, 820, 312, 6457, 1392, 281, 1322, 746, 382, 1702, 13, 583, 295, 1164, 11, 309, 311, 3442, 281], "temperature": 0.0, "avg_logprob": -0.18175586732495733, "compression_ratio": 1.6468401486988848, "no_speech_prob": 2.4675740860402584e-05}, {"id": 389, "seek": 218436, "start": 2189.88, "end": 2196.2400000000002, "text": " be working for you. All right. All right. For that, we have also brushed up our game", "tokens": [312, 1364, 337, 291, 13, 1057, 558, 13, 1057, 558, 13, 1171, 300, 11, 321, 362, 611, 40694, 493, 527, 1216], "temperature": 0.0, "avg_logprob": -0.18175586732495733, "compression_ratio": 1.6468401486988848, "no_speech_prob": 2.4675740860402584e-05}, {"id": 390, "seek": 218436, "start": 2196.2400000000002, "end": 2202.8, "text": " a little bit on documentation. And one thing I would like you to look at to check the time.", "tokens": [257, 707, 857, 322, 14333, 13, 400, 472, 551, 286, 576, 411, 291, 281, 574, 412, 281, 1520, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.18175586732495733, "compression_ratio": 1.6468401486988848, "no_speech_prob": 2.4675740860402584e-05}, {"id": 391, "seek": 218436, "start": 2202.8, "end": 2208.88, "text": " It's all right. So we have reorganized the repo a little bit to make it a little cleaner.", "tokens": [467, 311, 439, 558, 13, 407, 321, 362, 41203, 1602, 264, 49040, 257, 707, 857, 281, 652, 309, 257, 707, 16532, 13], "temperature": 0.0, "avg_logprob": -0.18175586732495733, "compression_ratio": 1.6468401486988848, "no_speech_prob": 2.4675740860402584e-05}, {"id": 392, "seek": 218436, "start": 2208.88, "end": 2214.04, "text": " You can see there's a bunch of stuff around that. There's Xtas, which is our task manager,", "tokens": [509, 393, 536, 456, 311, 257, 3840, 295, 1507, 926, 300, 13, 821, 311, 1783, 83, 296, 11, 597, 307, 527, 5633, 6598, 11], "temperature": 0.0, "avg_logprob": -0.18175586732495733, "compression_ratio": 1.6468401486988848, "no_speech_prob": 2.4675740860402584e-05}, {"id": 393, "seek": 221404, "start": 2214.04, "end": 2219.52, "text": " benchmarks, testing. That should be self-explanatory. We have the bindings and the Uni-FFI bind", "tokens": [43751, 11, 4997, 13, 663, 820, 312, 2698, 12, 3121, 16554, 4745, 13, 492, 362, 264, 14786, 1109, 293, 264, 35191, 12, 6345, 40, 14786], "temperature": 0.0, "avg_logprob": -0.22234408594980962, "compression_ratio": 1.6085409252669038, "no_speech_prob": 1.7773703802959062e-05}, {"id": 394, "seek": 221404, "start": 2219.52, "end": 2224.8, "text": " gen to organize bindings. We have the labs, which is also where you find the jack-in implementation", "tokens": [1049, 281, 13859, 14786, 1109, 13, 492, 362, 264, 20339, 11, 597, 307, 611, 689, 291, 915, 264, 7109, 12, 259, 11420], "temperature": 0.0, "avg_logprob": -0.22234408594980962, "compression_ratio": 1.6085409252669038, "no_speech_prob": 1.7773703802959062e-05}, {"id": 395, "seek": 221404, "start": 2224.8, "end": 2230.92, "text": " if you're curious about this. But the main stuff lives in crates and contrap is other", "tokens": [498, 291, 434, 6369, 466, 341, 13, 583, 264, 2135, 1507, 2909, 294, 941, 1024, 293, 660, 4007, 307, 661], "temperature": 0.0, "avg_logprob": -0.22234408594980962, "compression_ratio": 1.6085409252669038, "no_speech_prob": 1.7773703802959062e-05}, {"id": 396, "seek": 221404, "start": 2230.92, "end": 2235.16, "text": " things built on hub. Do we have an examples folder exactly for this kind of stuff? So", "tokens": [721, 3094, 322, 11838, 13, 1144, 321, 362, 364, 5110, 10820, 2293, 337, 341, 733, 295, 1507, 30, 407], "temperature": 0.0, "avg_logprob": -0.22234408594980962, "compression_ratio": 1.6085409252669038, "no_speech_prob": 1.7773703802959062e-05}, {"id": 397, "seek": 221404, "start": 2235.16, "end": 2241.88, "text": " let me quickly, is that possible? Roughly. I put the slides into the dev room if you", "tokens": [718, 385, 2661, 11, 307, 300, 1944, 30, 42791, 356, 13, 286, 829, 264, 9788, 666, 264, 1905, 1808, 498, 291], "temperature": 0.0, "avg_logprob": -0.22234408594980962, "compression_ratio": 1.6085409252669038, "no_speech_prob": 1.7773703802959062e-05}, {"id": 398, "seek": 224188, "start": 2241.88, "end": 2250.0, "text": " want to look at them. Quickly run through one, the SDK bot 101 thing. It allows you", "tokens": [528, 281, 574, 412, 552, 13, 31800, 1190, 807, 472, 11, 264, 37135, 10592, 21055, 551, 13, 467, 4045, 291], "temperature": 0.0, "avg_logprob": -0.13465798321892233, "compression_ratio": 1.5244444444444445, "no_speech_prob": 3.763207496376708e-05}, {"id": 399, "seek": 224188, "start": 2250.0, "end": 2255.2400000000002, "text": " to directly use that from the repo with that command. What you see on the first screen", "tokens": [281, 3838, 764, 300, 490, 264, 49040, 365, 300, 5622, 13, 708, 291, 536, 322, 264, 700, 2568], "temperature": 0.0, "avg_logprob": -0.13465798321892233, "compression_ratio": 1.5244444444444445, "no_speech_prob": 3.763207496376708e-05}, {"id": 400, "seek": 224188, "start": 2255.2400000000002, "end": 2261.76, "text": " is just the imports that we need. You see mostly Rust SDK stuff, some minor managing", "tokens": [307, 445, 264, 41596, 300, 321, 643, 13, 509, 536, 5240, 34952, 37135, 1507, 11, 512, 6696, 11642], "temperature": 0.0, "avg_logprob": -0.13465798321892233, "compression_ratio": 1.5244444444444445, "no_speech_prob": 3.763207496376708e-05}, {"id": 401, "seek": 224188, "start": 2261.76, "end": 2268.56, "text": " around that. If you're familiar with Rust, you know that binary has this main function.", "tokens": [926, 300, 13, 759, 291, 434, 4963, 365, 34952, 11, 291, 458, 300, 17434, 575, 341, 2135, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13465798321892233, "compression_ratio": 1.5244444444444445, "no_speech_prob": 3.763207496376708e-05}, {"id": 402, "seek": 226856, "start": 2268.56, "end": 2274.0, "text": " We use Tokyo here. It's an async function, right? Async API. Most of that is just parsing", "tokens": [492, 764, 15147, 510, 13, 467, 311, 364, 382, 34015, 2445, 11, 558, 30, 1018, 34015, 9362, 13, 4534, 295, 300, 307, 445, 21156, 278], "temperature": 0.0, "avg_logprob": -0.20352779388427733, "compression_ratio": 1.5283842794759825, "no_speech_prob": 4.539164001471363e-05}, {"id": 403, "seek": 226856, "start": 2274.0, "end": 2281.0, "text": " in a very ugly way, the command line, and then handing it over to Lock-in and Sync.", "tokens": [294, 257, 588, 12246, 636, 11, 264, 5622, 1622, 11, 293, 550, 34774, 309, 670, 281, 16736, 12, 259, 293, 26155, 66, 13], "temperature": 0.0, "avg_logprob": -0.20352779388427733, "compression_ratio": 1.5283842794759825, "no_speech_prob": 4.539164001471363e-05}, {"id": 404, "seek": 226856, "start": 2281.0, "end": 2286.32, "text": " This Lock-in and Sync sets up some minor stuff. You see that we have a lot of information", "tokens": [639, 16736, 12, 259, 293, 26155, 66, 6352, 493, 512, 6696, 1507, 13, 509, 536, 300, 321, 362, 257, 688, 295, 1589], "temperature": 0.0, "avg_logprob": -0.20352779388427733, "compression_ratio": 1.5283842794759825, "no_speech_prob": 4.539164001471363e-05}, {"id": 405, "seek": 226856, "start": 2286.32, "end": 2293.48, "text": " about this in code comments for right here for you. It does even set up a slept store.", "tokens": [466, 341, 294, 3089, 3053, 337, 558, 510, 337, 291, 13, 467, 775, 754, 992, 493, 257, 17400, 3531, 13], "temperature": 0.0, "avg_logprob": -0.20352779388427733, "compression_ratio": 1.5283842794759825, "no_speech_prob": 4.539164001471363e-05}, {"id": 406, "seek": 229348, "start": 2293.48, "end": 2299.64, "text": " You can call the Lock-in username. You can give it a name for the bot that is the device", "tokens": [509, 393, 818, 264, 16736, 12, 259, 30351, 13, 509, 393, 976, 309, 257, 1315, 337, 264, 10592, 300, 307, 264, 4302], "temperature": 0.0, "avg_logprob": -0.16457820975262186, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.8850381568190642e-05}, {"id": 407, "seek": 229348, "start": 2299.64, "end": 2309.56, "text": " that you will see, and it locks you in. Going further, I don't have the time to go through", "tokens": [300, 291, 486, 536, 11, 293, 309, 20703, 291, 294, 13, 10963, 3052, 11, 286, 500, 380, 362, 264, 565, 281, 352, 807], "temperature": 0.0, "avg_logprob": -0.16457820975262186, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.8850381568190642e-05}, {"id": 408, "seek": 229348, "start": 2309.56, "end": 2315.0, "text": " the entire thing, but it explains everything right here. This bot does two things. For", "tokens": [264, 2302, 551, 11, 457, 309, 13948, 1203, 558, 510, 13, 639, 10592, 775, 732, 721, 13, 1171], "temperature": 0.0, "avg_logprob": -0.16457820975262186, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.8850381568190642e-05}, {"id": 409, "seek": 229348, "start": 2315.0, "end": 2320.04, "text": " every room that you ask it to join, it will automatically join, which is this first event", "tokens": [633, 1808, 300, 291, 1029, 309, 281, 3917, 11, 309, 486, 6772, 3917, 11, 597, 307, 341, 700, 2280], "temperature": 0.0, "avg_logprob": -0.16457820975262186, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.8850381568190642e-05}, {"id": 410, "seek": 232004, "start": 2320.04, "end": 2325.16, "text": " handler, and the second event handler is reacting on messages. An event handler in the client", "tokens": [41967, 11, 293, 264, 1150, 2280, 41967, 307, 25817, 322, 7897, 13, 1107, 2280, 41967, 294, 264, 6423], "temperature": 0.0, "avg_logprob": -0.17914450963338216, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.917605085414834e-05}, {"id": 411, "seek": 232004, "start": 2325.16, "end": 2329.88, "text": " is basically just a callback that you can say, when these kind of events come in, please", "tokens": [307, 1936, 445, 257, 818, 3207, 300, 291, 393, 584, 11, 562, 613, 733, 295, 3931, 808, 294, 11, 1767], "temperature": 0.0, "avg_logprob": -0.17914450963338216, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.917605085414834e-05}, {"id": 412, "seek": 232004, "start": 2329.88, "end": 2336.52, "text": " tell me, and then I react to this. Those themselves can be async again, pretty nice. Then it just", "tokens": [980, 385, 11, 293, 550, 286, 4515, 281, 341, 13, 3950, 2969, 393, 312, 382, 34015, 797, 11, 1238, 1481, 13, 1396, 309, 445], "temperature": 0.0, "avg_logprob": -0.17914450963338216, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.917605085414834e-05}, {"id": 413, "seek": 232004, "start": 2336.52, "end": 2342.4, "text": " starts syncing, and that's all it does, which means it's running the Sync loop. This does", "tokens": [3719, 5451, 2175, 11, 293, 300, 311, 439, 309, 775, 11, 597, 1355, 309, 311, 2614, 264, 26155, 66, 6367, 13, 639, 775], "temperature": 0.0, "avg_logprob": -0.17914450963338216, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.917605085414834e-05}, {"id": 414, "seek": 232004, "start": 2342.4, "end": 2347.72, "text": " not, at this point, use sliding sync. As I told you, it's kind of experimental. Let's", "tokens": [406, 11, 412, 341, 935, 11, 764, 21169, 20271, 13, 1018, 286, 1907, 291, 11, 309, 311, 733, 295, 17069, 13, 961, 311], "temperature": 0.0, "avg_logprob": -0.17914450963338216, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.917605085414834e-05}, {"id": 415, "seek": 234772, "start": 2347.72, "end": 2354.9199999999996, "text": " look at the room message. The un-room message, whenever we receive a message, we can again", "tokens": [574, 412, 264, 1808, 3636, 13, 440, 517, 12, 2861, 3636, 11, 5699, 321, 4774, 257, 3636, 11, 321, 393, 797], "temperature": 0.0, "avg_logprob": -0.16861501015907476, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.943720003007911e-05}, {"id": 416, "seek": 234772, "start": 2354.9199999999996, "end": 2358.56, "text": " mention that before it's type safe. It's going to give us the actual room message in a typed", "tokens": [2152, 300, 949, 309, 311, 2010, 3273, 13, 467, 311, 516, 281, 976, 505, 264, 3539, 1808, 3636, 294, 257, 33941], "temperature": 0.0, "avg_logprob": -0.16861501015907476, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.943720003007911e-05}, {"id": 417, "seek": 234772, "start": 2358.56, "end": 2364.2, "text": " format, so we can rely on the compiler here to make sure that things are as they should", "tokens": [7877, 11, 370, 321, 393, 10687, 322, 264, 31958, 510, 281, 652, 988, 300, 721, 366, 382, 436, 820], "temperature": 0.0, "avg_logprob": -0.16861501015907476, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.943720003007911e-05}, {"id": 418, "seek": 234772, "start": 2364.2, "end": 2370.2, "text": " be. We make sure that we are in this room. We try to figure out if it's a text message.", "tokens": [312, 13, 492, 652, 988, 300, 321, 366, 294, 341, 1808, 13, 492, 853, 281, 2573, 484, 498, 309, 311, 257, 2487, 3636, 13], "temperature": 0.0, "avg_logprob": -0.16861501015907476, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.943720003007911e-05}, {"id": 419, "seek": 234772, "start": 2370.2, "end": 2376.4399999999996, "text": " If it's a text message we check for, is it dollar bank party? If so, we're going to respond", "tokens": [759, 309, 311, 257, 2487, 3636, 321, 1520, 337, 11, 307, 309, 7241, 3765, 3595, 30, 759, 370, 11, 321, 434, 516, 281, 4196], "temperature": 0.0, "avg_logprob": -0.16861501015907476, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.943720003007911e-05}, {"id": 420, "seek": 237644, "start": 2376.44, "end": 2383.16, "text": " with a message, and that's all the thing does. In reality, it looks like this. I'm showing", "tokens": [365, 257, 3636, 11, 293, 300, 311, 439, 264, 551, 775, 13, 682, 4103, 11, 309, 1542, 411, 341, 13, 286, 478, 4099], "temperature": 0.0, "avg_logprob": -0.15166065993818265, "compression_ratio": 1.6123348017621146, "no_speech_prob": 1.9832215912174433e-05}, {"id": 421, "seek": 237644, "start": 2383.16, "end": 2390.6, "text": " you this is just regular main at the moment. Then, if I run the bot, this is slightly capped,", "tokens": [291, 341, 307, 445, 3890, 2135, 412, 264, 1623, 13, 1396, 11, 498, 286, 1190, 264, 10592, 11, 341, 307, 4748, 1335, 3320, 11], "temperature": 0.0, "avg_logprob": -0.15166065993818265, "compression_ratio": 1.6123348017621146, "no_speech_prob": 1.9832215912174433e-05}, {"id": 422, "seek": 237644, "start": 2390.6, "end": 2397.28, "text": " so you can't see my password. I'm here connected to that bot. You see that I'm in here. I had", "tokens": [370, 291, 393, 380, 536, 452, 11524, 13, 286, 478, 510, 4582, 281, 300, 10592, 13, 509, 536, 300, 286, 478, 294, 510, 13, 286, 632], "temperature": 0.0, "avg_logprob": -0.15166065993818265, "compression_ratio": 1.6123348017621146, "no_speech_prob": 1.9832215912174433e-05}, {"id": 423, "seek": 237644, "start": 2397.28, "end": 2401.7200000000003, "text": " two more prints that are not in main right now to make it a little cleaner. I'm sending", "tokens": [732, 544, 22305, 300, 366, 406, 294, 2135, 558, 586, 281, 652, 309, 257, 707, 16532, 13, 286, 478, 7750], "temperature": 0.0, "avg_logprob": -0.15166065993818265, "compression_ratio": 1.6123348017621146, "no_speech_prob": 1.9832215912174433e-05}, {"id": 424, "seek": 240172, "start": 2401.72, "end": 2408.3199999999997, "text": " a message. We see that this message is ignored, but if I send bank party, you can see it's", "tokens": [257, 3636, 13, 492, 536, 300, 341, 3636, 307, 19735, 11, 457, 498, 286, 2845, 3765, 3595, 11, 291, 393, 536, 309, 311], "temperature": 0.0, "avg_logprob": -0.13679135839144388, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.0783196305274032e-05}, {"id": 425, "seek": 240172, "start": 2408.3199999999997, "end": 2414.9599999999996, "text": " reacting, it's sending this. Most importantly, this is an encrypted room. I didn't have to", "tokens": [25817, 11, 309, 311, 7750, 341, 13, 4534, 8906, 11, 341, 307, 364, 36663, 1808, 13, 286, 994, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.13679135839144388, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.0783196305274032e-05}, {"id": 426, "seek": 240172, "start": 2414.9599999999996, "end": 2421.68, "text": " do anything to build a bot that allows me to live and interact with encrypted room. That's", "tokens": [360, 1340, 281, 1322, 257, 10592, 300, 4045, 385, 281, 1621, 293, 4648, 365, 36663, 1808, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.13679135839144388, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.0783196305274032e-05}, {"id": 427, "seek": 240172, "start": 2421.68, "end": 2426.3999999999996, "text": " an encrypted message. I didn't have to do anything. You saw that there was no setup. I hadn't", "tokens": [364, 36663, 3636, 13, 286, 994, 380, 362, 281, 360, 1340, 13, 509, 1866, 300, 456, 390, 572, 8657, 13, 286, 8782, 380], "temperature": 0.0, "avg_logprob": -0.13679135839144388, "compression_ratio": 1.7428571428571429, "no_speech_prob": 2.0783196305274032e-05}, {"id": 428, "seek": 242640, "start": 2426.4, "end": 2434.32, "text": " had to manage anything. The Rust SDK did all of that for me. If you want to learn more,", "tokens": [632, 281, 3067, 1340, 13, 440, 34952, 37135, 630, 439, 295, 300, 337, 385, 13, 759, 291, 528, 281, 1466, 544, 11], "temperature": 0.0, "avg_logprob": -0.17842493559184827, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.0450406080053654e-05}, {"id": 429, "seek": 242640, "start": 2434.32, "end": 2439.76, "text": " if you want to use this, you can find all of the code, of course, at metrics.metrics.usdk.", "tokens": [498, 291, 528, 281, 764, 341, 11, 291, 393, 915, 439, 295, 264, 3089, 11, 295, 1164, 11, 412, 16367, 13, 5537, 10716, 13, 301, 67, 74, 13], "temperature": 0.0, "avg_logprob": -0.17842493559184827, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.0450406080053654e-05}, {"id": 430, "seek": 242640, "start": 2439.76, "end": 2446.7200000000003, "text": " You can join our developer and general talking about the Rust SDK room. The example you just", "tokens": [509, 393, 3917, 527, 10754, 293, 2674, 1417, 466, 264, 34952, 37135, 1808, 13, 440, 1365, 291, 445], "temperature": 0.0, "avg_logprob": -0.17842493559184827, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.0450406080053654e-05}, {"id": 431, "seek": 242640, "start": 2446.7200000000003, "end": 2451.28, "text": " saw is inside the examples folder getting started. Jack in. The other client you saw", "tokens": [1866, 307, 1854, 264, 5110, 10820, 1242, 1409, 13, 4718, 294, 13, 440, 661, 6423, 291, 1866], "temperature": 0.0, "avg_logprob": -0.17842493559184827, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.0450406080053654e-05}, {"id": 432, "seek": 245128, "start": 2451.28, "end": 2457.2400000000002, "text": " before is in labs. Jack in. All of that code, obviously. I really recommend going for the", "tokens": [949, 307, 294, 20339, 13, 4718, 294, 13, 1057, 295, 300, 3089, 11, 2745, 13, 286, 534, 2748, 516, 337, 264], "temperature": 0.0, "avg_logprob": -0.22736234250275986, "compression_ratio": 1.5338983050847457, "no_speech_prob": 2.5860852474579588e-05}, {"id": 433, "seek": 245128, "start": 2457.2400000000002, "end": 2464.0800000000004, "text": " getting started. It has a lot of documentation. I also want to send an honorable mention to", "tokens": [1242, 1409, 13, 467, 575, 257, 688, 295, 14333, 13, 286, 611, 528, 281, 2845, 364, 36322, 2152, 281], "temperature": 0.0, "avg_logprob": -0.22736234250275986, "compression_ratio": 1.5338983050847457, "no_speech_prob": 2.5860852474579588e-05}, {"id": 434, "seek": 245128, "start": 2464.0800000000004, "end": 2471.84, "text": " Benjamin who is working on Trinity, which is a built on top of the Rust SDK, a bot framework,", "tokens": [22231, 567, 307, 1364, 322, 33121, 11, 597, 307, 257, 3094, 322, 1192, 295, 264, 34952, 37135, 11, 257, 10592, 8388, 11], "temperature": 0.0, "avg_logprob": -0.22736234250275986, "compression_ratio": 1.5338983050847457, "no_speech_prob": 2.5860852474579588e-05}, {"id": 435, "seek": 245128, "start": 2471.84, "end": 2477.1600000000003, "text": " I would say. It allows you to write some very small Rust that is compiled to Wasm that", "tokens": [286, 576, 584, 13, 467, 4045, 291, 281, 2464, 512, 588, 1359, 34952, 300, 307, 36548, 281, 3027, 76, 300], "temperature": 0.0, "avg_logprob": -0.22736234250275986, "compression_ratio": 1.5338983050847457, "no_speech_prob": 2.5860852474579588e-05}, {"id": 436, "seek": 247716, "start": 2477.16, "end": 2482.3999999999996, "text": " it runs in the client that can react to messages. You can write just the message part and say,", "tokens": [309, 6676, 294, 264, 6423, 300, 393, 4515, 281, 7897, 13, 509, 393, 2464, 445, 264, 3636, 644, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.18030118440326892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.5932528185658157e-05}, {"id": 437, "seek": 247716, "start": 2482.3999999999996, "end": 2489.6, "text": " like, I have a bot that reacts to messages. This is one. Oh, yeah. Element is hiring.", "tokens": [411, 11, 286, 362, 257, 10592, 300, 33305, 281, 7897, 13, 639, 307, 472, 13, 876, 11, 1338, 13, 20900, 307, 15335, 13], "temperature": 0.0, "avg_logprob": -0.18030118440326892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.5932528185658157e-05}, {"id": 438, "seek": 247716, "start": 2489.6, "end": 2499.3599999999997, "text": " If you are interested in working on this full-time Element IO careers, we're going to have time", "tokens": [759, 291, 366, 3102, 294, 1364, 322, 341, 1577, 12, 3766, 20900, 39839, 16409, 11, 321, 434, 516, 281, 362, 565], "temperature": 0.0, "avg_logprob": -0.18030118440326892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.5932528185658157e-05}, {"id": 439, "seek": 247716, "start": 2499.3599999999997, "end": 2505.08, "text": " for questions later. We have to get through all of these first. Let's see what you can", "tokens": [337, 1651, 1780, 13, 492, 362, 281, 483, 807, 439, 295, 613, 700, 13, 961, 311, 536, 437, 291, 393], "temperature": 0.0, "avg_logprob": -0.18030118440326892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.5932528185658157e-05}, {"id": 440, "seek": 250508, "start": 2505.08, "end": 2512.08, "text": " actually build with this. Thank you. That's a long one.", "tokens": [767, 1322, 365, 341, 13, 1044, 291, 13, 663, 311, 257, 938, 472, 13], "temperature": 0.0, "avg_logprob": -0.33398904934735363, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.0011884839041158557}, {"id": 441, "seek": 250508, "start": 2512.08, "end": 2528.08, "text": " So, hello, everyone. My name is Mauro. Honestly, my colleagues are at a slide where they presented", "tokens": [407, 11, 7751, 11, 1518, 13, 1222, 1315, 307, 32858, 340, 13, 12348, 11, 452, 7734, 366, 412, 257, 4137, 689, 436, 8212], "temperature": 0.0, "avg_logprob": -0.33398904934735363, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.0011884839041158557}, {"id": 442, "seek": 250508, "start": 2528.08, "end": 2534.3199999999997, "text": " themselves. I don't have such a thing. So, I have to be brief. I come from Italy, Naples.", "tokens": [2969, 13, 286, 500, 380, 362, 1270, 257, 551, 13, 407, 11, 286, 362, 281, 312, 5353, 13, 286, 808, 490, 10705, 11, 6056, 2622, 13], "temperature": 0.0, "avg_logprob": -0.33398904934735363, "compression_ratio": 1.3942857142857144, "no_speech_prob": 0.0011884839041158557}, {"id": 443, "seek": 253432, "start": 2534.32, "end": 2539.0800000000004, "text": " I'm a software engineer. I work at Element. I mostly work on the IO side of things and", "tokens": [286, 478, 257, 4722, 11403, 13, 286, 589, 412, 20900, 13, 286, 5240, 589, 322, 264, 39839, 1252, 295, 721, 293], "temperature": 0.0, "avg_logprob": -0.2493290534386268, "compression_ratio": 1.8451882845188285, "no_speech_prob": 0.0003718410443980247}, {"id": 444, "seek": 253432, "start": 2539.0800000000004, "end": 2546.1200000000003, "text": " set up working on some Rust implementations. Today, I'm going to talk about the new client", "tokens": [992, 493, 1364, 322, 512, 34952, 4445, 763, 13, 2692, 11, 286, 478, 516, 281, 751, 466, 264, 777, 6423], "temperature": 0.0, "avg_logprob": -0.2493290534386268, "compression_ratio": 1.8451882845188285, "no_speech_prob": 0.0003718410443980247}, {"id": 445, "seek": 253432, "start": 2546.1200000000003, "end": 2553.32, "text": " element tags. The new client is pretty much being built with the idea of both the fine", "tokens": [4478, 18632, 13, 440, 777, 6423, 307, 1238, 709, 885, 3094, 365, 264, 1558, 295, 1293, 264, 2489], "temperature": 0.0, "avg_logprob": -0.2493290534386268, "compression_ratio": 1.8451882845188285, "no_speech_prob": 0.0003718410443980247}, {"id": 446, "seek": 253432, "start": 2553.32, "end": 2557.92, "text": " goals. The first of them is pretty much user experience. The thing is that we really wanted", "tokens": [5493, 13, 440, 700, 295, 552, 307, 1238, 709, 4195, 1752, 13, 440, 551, 307, 300, 321, 534, 1415], "temperature": 0.0, "avg_logprob": -0.2493290534386268, "compression_ratio": 1.8451882845188285, "no_speech_prob": 0.0003718410443980247}, {"id": 447, "seek": 253432, "start": 2557.92, "end": 2563.1600000000003, "text": " to improve over the user experience of the current Element implementation. The thing", "tokens": [281, 3470, 670, 264, 4195, 1752, 295, 264, 2190, 20900, 11420, 13, 440, 551], "temperature": 0.0, "avg_logprob": -0.2493290534386268, "compression_ratio": 1.8451882845188285, "no_speech_prob": 0.0003718410443980247}, {"id": 448, "seek": 256316, "start": 2563.16, "end": 2571.0, "text": " is that Element started as pretty much a showcase for what Magics was capable of. So, it was", "tokens": [307, 300, 20900, 1409, 382, 1238, 709, 257, 20388, 337, 437, 6395, 1167, 390, 8189, 295, 13, 407, 11, 309, 390], "temperature": 0.0, "avg_logprob": -0.17093747854232788, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.00013729897909797728}, {"id": 449, "seek": 256316, "start": 2571.0, "end": 2580.0, "text": " a bit like an app made by engineers, for engineers. So, yeah, not everyone is into this kind of", "tokens": [257, 857, 411, 364, 724, 1027, 538, 11955, 11, 337, 11955, 13, 407, 11, 1338, 11, 406, 1518, 307, 666, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.17093747854232788, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.00013729897909797728}, {"id": 450, "seek": 256316, "start": 2580.0, "end": 2584.0, "text": " stuff. So, sometimes it's a bit hard to use for the average user, and we want to improve", "tokens": [1507, 13, 407, 11, 2171, 309, 311, 257, 857, 1152, 281, 764, 337, 264, 4274, 4195, 11, 293, 321, 528, 281, 3470], "temperature": 0.0, "avg_logprob": -0.17093747854232788, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.00013729897909797728}, {"id": 451, "seek": 256316, "start": 2584.0, "end": 2588.8799999999997, "text": " over this. Also, we want, of course, to have a performance to be another very important", "tokens": [670, 341, 13, 2743, 11, 321, 528, 11, 295, 1164, 11, 281, 362, 257, 3389, 281, 312, 1071, 588, 1021], "temperature": 0.0, "avg_logprob": -0.17093747854232788, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.00013729897909797728}, {"id": 452, "seek": 258888, "start": 2588.88, "end": 2594.76, "text": " goal. Actually, just as important as UX, we're actually, thanks to the slide-in sync implementation", "tokens": [3387, 13, 5135, 11, 445, 382, 1021, 382, 40176, 11, 321, 434, 767, 11, 3231, 281, 264, 4137, 12, 259, 20271, 11420], "temperature": 0.0, "avg_logprob": -0.16945711771647134, "compression_ratio": 1.6826568265682658, "no_speech_prob": 7.625801663380116e-05}, {"id": 453, "seek": 258888, "start": 2594.76, "end": 2599.6400000000003, "text": " on Element tags, we're aiming to actually launch the app in less than 100 milliseconds.", "tokens": [322, 20900, 18632, 11, 321, 434, 20253, 281, 767, 4025, 264, 724, 294, 1570, 813, 2319, 34184, 13], "temperature": 0.0, "avg_logprob": -0.16945711771647134, "compression_ratio": 1.6826568265682658, "no_speech_prob": 7.625801663380116e-05}, {"id": 454, "seek": 258888, "start": 2599.6400000000003, "end": 2604.56, "text": " That's pretty much the thing that we're aiming for. And, of course, also optimize the bandwidth", "tokens": [663, 311, 1238, 709, 264, 551, 300, 321, 434, 20253, 337, 13, 400, 11, 295, 1164, 11, 611, 19719, 264, 23647], "temperature": 0.0, "avg_logprob": -0.16945711771647134, "compression_ratio": 1.6826568265682658, "no_speech_prob": 7.625801663380116e-05}, {"id": 455, "seek": 258888, "start": 2604.56, "end": 2611.2400000000002, "text": " usage. Also, we want to build an app that is reliable just from the start. So, testing", "tokens": [14924, 13, 2743, 11, 321, 528, 281, 1322, 364, 724, 300, 307, 12924, 445, 490, 264, 722, 13, 407, 11, 4997], "temperature": 0.0, "avg_logprob": -0.16945711771647134, "compression_ratio": 1.6826568265682658, "no_speech_prob": 7.625801663380116e-05}, {"id": 456, "seek": 258888, "start": 2611.2400000000002, "end": 2616.96, "text": " code coverage is pretty much right from the start of the project, a Niagara priority.", "tokens": [3089, 9645, 307, 1238, 709, 558, 490, 264, 722, 295, 264, 1716, 11, 257, 45123, 9365, 13], "temperature": 0.0, "avg_logprob": -0.16945711771647134, "compression_ratio": 1.6826568265682658, "no_speech_prob": 7.625801663380116e-05}, {"id": 457, "seek": 261696, "start": 2616.96, "end": 2621.0, "text": " And also, we want to build the app in a way that is actually relying on shared components", "tokens": [400, 611, 11, 321, 528, 281, 1322, 264, 724, 294, 257, 636, 300, 307, 767, 24140, 322, 5507, 6677], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 458, "seek": 261696, "start": 2621.0, "end": 2626.6, "text": " pretty much. Mattis Francis Decay is just one of them, but, of course, we're planning", "tokens": [1238, 709, 13, 7397, 271, 19648, 12427, 320, 307, 445, 472, 295, 552, 11, 457, 11, 295, 1164, 11, 321, 434, 5038], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 459, "seek": 261696, "start": 2626.6, "end": 2631.6, "text": " to build more components that will be shared across different implementation, across different", "tokens": [281, 1322, 544, 6677, 300, 486, 312, 5507, 2108, 819, 11420, 11, 2108, 819], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 460, "seek": 261696, "start": 2631.6, "end": 2636.4, "text": " platforms, different projects. So, not even necessarily Element tags. It is that we will", "tokens": [9473, 11, 819, 4455, 13, 407, 11, 406, 754, 4725, 20900, 18632, 13, 467, 307, 300, 321, 486], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 461, "seek": 261696, "start": 2636.4, "end": 2640.2400000000002, "text": " be able to use them, and, of course, anyone in the open source community will be able", "tokens": [312, 1075, 281, 764, 552, 11, 293, 11, 295, 1164, 11, 2878, 294, 264, 1269, 4009, 1768, 486, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 462, "seek": 261696, "start": 2640.2400000000002, "end": 2646.92, "text": " to use them. So, why are we writing the Android and the iOS app? That's actually a good question,", "tokens": [281, 764, 552, 13, 407, 11, 983, 366, 321, 3579, 264, 8853, 293, 264, 17430, 724, 30, 663, 311, 767, 257, 665, 1168, 11], "temperature": 0.0, "avg_logprob": -0.1998616720883901, "compression_ratio": 1.7980132450331126, "no_speech_prob": 0.00012823825818486512}, {"id": 463, "seek": 264692, "start": 2646.92, "end": 2653.04, "text": " because some of these goals could also be achieved with a very big refactor. But, let's", "tokens": [570, 512, 295, 613, 5493, 727, 611, 312, 11042, 365, 257, 588, 955, 1895, 15104, 13, 583, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.18789051152482816, "compression_ratio": 1.4191919191919191, "no_speech_prob": 0.0004238385008648038}, {"id": 464, "seek": 264692, "start": 2653.04, "end": 2659.28, "text": " go more in depth on why we want to do our right. So, let's start with Element iOS. Element", "tokens": [352, 544, 294, 7161, 322, 983, 321, 528, 281, 360, 527, 558, 13, 407, 11, 718, 311, 722, 365, 20900, 17430, 13, 20900], "temperature": 0.0, "avg_logprob": -0.18789051152482816, "compression_ratio": 1.4191919191919191, "no_speech_prob": 0.0004238385008648038}, {"id": 465, "seek": 264692, "start": 2659.28, "end": 2668.7200000000003, "text": " iOS, it's quite old. It started in 2015. Essentially, it was, as I said, pretty much a POC to showcase", "tokens": [17430, 11, 309, 311, 1596, 1331, 13, 467, 1409, 294, 7546, 13, 23596, 11, 309, 390, 11, 382, 286, 848, 11, 1238, 709, 257, 22299, 34, 281, 20388], "temperature": 0.0, "avg_logprob": -0.18789051152482816, "compression_ratio": 1.4191919191919191, "no_speech_prob": 0.0004238385008648038}, {"id": 466, "seek": 266872, "start": 2668.72, "end": 2677.3199999999997, "text": " what Metrix was capable of. It started as being named the Metrix iOS console, in fact.", "tokens": [437, 6377, 6579, 390, 8189, 295, 13, 467, 1409, 382, 885, 4926, 264, 6377, 6579, 17430, 11076, 11, 294, 1186, 13], "temperature": 0.0, "avg_logprob": -0.26632985566791734, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.00035065284464508295}, {"id": 467, "seek": 266872, "start": 2677.3199999999997, "end": 2684.08, "text": " Then it went through a bunch of identity crisis and changed name three times. I guess it was", "tokens": [1396, 309, 1437, 807, 257, 3840, 295, 6575, 5869, 293, 3105, 1315, 1045, 1413, 13, 286, 2041, 309, 390], "temperature": 0.0, "avg_logprob": -0.26632985566791734, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.00035065284464508295}, {"id": 468, "seek": 266872, "start": 2684.08, "end": 2689.3199999999997, "text": " first console, then viator, then riot. Now, it's Element. Let's hope it's going to stick", "tokens": [700, 11076, 11, 550, 1932, 1639, 11, 550, 32211, 13, 823, 11, 309, 311, 20900, 13, 961, 311, 1454, 309, 311, 516, 281, 2897], "temperature": 0.0, "avg_logprob": -0.26632985566791734, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.00035065284464508295}, {"id": 469, "seek": 266872, "start": 2689.3199999999997, "end": 2697.04, "text": " with that. So, and, yeah, it was built by engineers to showcase pretty much what Metrix", "tokens": [365, 300, 13, 407, 11, 293, 11, 1338, 11, 309, 390, 3094, 538, 11955, 281, 20388, 1238, 709, 437, 6377, 6579], "temperature": 0.0, "avg_logprob": -0.26632985566791734, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.00035065284464508295}, {"id": 470, "seek": 269704, "start": 2697.04, "end": 2702.52, "text": " was capable of. But the thing is that, first of all, as I said, the user experience was", "tokens": [390, 8189, 295, 13, 583, 264, 551, 307, 300, 11, 700, 295, 439, 11, 382, 286, 848, 11, 264, 4195, 1752, 390], "temperature": 0.0, "avg_logprob": -0.33992767333984375, "compression_ratio": 1.635379061371841, "no_speech_prob": 0.0005294050206430256}, {"id": 471, "seek": 269704, "start": 2702.52, "end": 2710.08, "text": " not great. Second, it was made with some very old components written on Objective-C that", "tokens": [406, 869, 13, 5736, 11, 309, 390, 1027, 365, 512, 588, 1331, 6677, 3720, 322, 24753, 488, 12, 34, 300], "temperature": 0.0, "avg_logprob": -0.33992767333984375, "compression_ratio": 1.635379061371841, "no_speech_prob": 0.0005294050206430256}, {"id": 472, "seek": 269704, "start": 2710.08, "end": 2718.16, "text": " used some very old architectural pattern, like MVC, which should stand for a review controller,", "tokens": [1143, 512, 588, 1331, 26621, 5102, 11, 411, 17663, 34, 11, 597, 820, 1463, 337, 257, 3131, 10561, 11], "temperature": 0.0, "avg_logprob": -0.33992767333984375, "compression_ratio": 1.635379061371841, "no_speech_prob": 0.0005294050206430256}, {"id": 473, "seek": 269704, "start": 2718.16, "end": 2721.72, "text": " but it stands more for massive review controller, because you know, by doing this, but it's", "tokens": [457, 309, 7382, 544, 337, 5994, 3131, 10561, 11, 570, 291, 458, 11, 538, 884, 341, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.33992767333984375, "compression_ratio": 1.635379061371841, "no_speech_prob": 0.0005294050206430256}, {"id": 474, "seek": 269704, "start": 2721.72, "end": 2726.2, "text": " a very good controller, and it's just a huge mess, and you start looking at 60,000 lines", "tokens": [257, 588, 665, 10561, 11, 293, 309, 311, 445, 257, 2603, 2082, 11, 293, 291, 722, 1237, 412, 4060, 11, 1360, 3876], "temperature": 0.0, "avg_logprob": -0.33992767333984375, "compression_ratio": 1.635379061371841, "no_speech_prob": 0.0005294050206430256}, {"id": 475, "seek": 272620, "start": 2726.2, "end": 2733.16, "text": " of code in a controller, and you're like, oh, my God, why am I alive? So, yeah, you don't", "tokens": [295, 3089, 294, 257, 10561, 11, 293, 291, 434, 411, 11, 1954, 11, 452, 1265, 11, 983, 669, 286, 5465, 30, 407, 11, 1338, 11, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.20072824578536183, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0002736309834290296}, {"id": 476, "seek": 272620, "start": 2733.16, "end": 2738.64, "text": " want to see that anymore, pretty much. We want to move to a newer architecture. Also,", "tokens": [528, 281, 536, 300, 3602, 11, 1238, 709, 13, 492, 528, 281, 1286, 281, 257, 17628, 9482, 13, 2743, 11], "temperature": 0.0, "avg_logprob": -0.20072824578536183, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0002736309834290296}, {"id": 477, "seek": 272620, "start": 2738.64, "end": 2744.48, "text": " even if we did a lot of refactors on the Element iOS implementation, you essentially, yeah,", "tokens": [754, 498, 321, 630, 257, 688, 295, 1895, 578, 830, 322, 264, 20900, 17430, 11420, 11, 291, 4476, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.20072824578536183, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0002736309834290296}, {"id": 478, "seek": 272620, "start": 2744.48, "end": 2749.4399999999996, "text": " we were essentially not able to change all the old implementation, since they were very", "tokens": [321, 645, 4476, 406, 1075, 281, 1319, 439, 264, 1331, 11420, 11, 1670, 436, 645, 588], "temperature": 0.0, "avg_logprob": -0.20072824578536183, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0002736309834290296}, {"id": 479, "seek": 274944, "start": 2749.44, "end": 2756.44, "text": " hard to maintain, and we still relied on these components a lot. So, yeah, four components", "tokens": [1152, 281, 6909, 11, 293, 321, 920, 35463, 322, 613, 6677, 257, 688, 13, 407, 11, 1338, 11, 1451, 6677], "temperature": 0.0, "avg_logprob": -0.1790610935377038, "compression_ratio": 1.6227106227106227, "no_speech_prob": 6.509276136057451e-05}, {"id": 480, "seek": 274944, "start": 2756.44, "end": 2760.8, "text": " are still using these old implementations. Half of the code is still in Objective-C,", "tokens": [366, 920, 1228, 613, 1331, 4445, 763, 13, 15917, 295, 264, 3089, 307, 920, 294, 24753, 488, 12, 34, 11], "temperature": 0.0, "avg_logprob": -0.1790610935377038, "compression_ratio": 1.6227106227106227, "no_speech_prob": 6.509276136057451e-05}, {"id": 481, "seek": 274944, "start": 2760.8, "end": 2767.4, "text": " and code coverage is quite low. So, we decided to experiment a bit in Q2 2022. We decided", "tokens": [293, 3089, 9645, 307, 1596, 2295, 13, 407, 11, 321, 3047, 281, 5120, 257, 857, 294, 1249, 17, 20229, 13, 492, 3047], "temperature": 0.0, "avg_logprob": -0.1790610935377038, "compression_ratio": 1.6227106227106227, "no_speech_prob": 6.509276136057451e-05}, {"id": 482, "seek": 274944, "start": 2767.4, "end": 2773.28, "text": " to pretty much build a minimum client using the Metrix for access decay, and pretty much", "tokens": [281, 1238, 709, 1322, 257, 7285, 6423, 1228, 264, 6377, 6579, 337, 2105, 21039, 11, 293, 1238, 709], "temperature": 0.0, "avg_logprob": -0.1790610935377038, "compression_ratio": 1.6227106227106227, "no_speech_prob": 6.509276136057451e-05}, {"id": 483, "seek": 274944, "start": 2773.28, "end": 2778.16, "text": " the state-of-the-art frameworks provided by Apple, like SwiftUI, but not only that, like", "tokens": [264, 1785, 12, 2670, 12, 3322, 12, 446, 29834, 5649, 538, 6373, 11, 411, 25539, 46324, 11, 457, 406, 787, 300, 11, 411], "temperature": 0.0, "avg_logprob": -0.1790610935377038, "compression_ratio": 1.6227106227106227, "no_speech_prob": 6.509276136057451e-05}, {"id": 484, "seek": 277816, "start": 2778.16, "end": 2784.52, "text": " also AsyncAway and things like that. So, yeah, and we were actually able to build this new", "tokens": [611, 1018, 34015, 32, 676, 293, 721, 411, 300, 13, 407, 11, 1338, 11, 293, 321, 645, 767, 1075, 281, 1322, 341, 777], "temperature": 0.0, "avg_logprob": -0.23072522229487352, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.00015655900642741472}, {"id": 485, "seek": 277816, "start": 2784.52, "end": 2790.6, "text": " minimum client that had a room list timeline, and it was, let's just say, a technical success.", "tokens": [7285, 6423, 300, 632, 257, 1808, 1329, 12933, 11, 293, 309, 390, 11, 718, 311, 445, 584, 11, 257, 6191, 2245, 13], "temperature": 0.0, "avg_logprob": -0.23072522229487352, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.00015655900642741472}, {"id": 486, "seek": 277816, "start": 2790.6, "end": 2797.7999999999997, "text": " It was super fast and amazing. So, we decided to build, on top of this second POC, by giving", "tokens": [467, 390, 1687, 2370, 293, 2243, 13, 407, 11, 321, 3047, 281, 1322, 11, 322, 1192, 295, 341, 1150, 22299, 34, 11, 538, 2902], "temperature": 0.0, "avg_logprob": -0.23072522229487352, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.00015655900642741472}, {"id": 487, "seek": 277816, "start": 2797.7999999999997, "end": 2804.68, "text": " a more focus on the UX, because as I said, yeah, now we have a performance client, but", "tokens": [257, 544, 1879, 322, 264, 40176, 11, 570, 382, 286, 848, 11, 1338, 11, 586, 321, 362, 257, 3389, 6423, 11, 457], "temperature": 0.0, "avg_logprob": -0.23072522229487352, "compression_ratio": 1.5336134453781514, "no_speech_prob": 0.00015655900642741472}, {"id": 488, "seek": 280468, "start": 2804.68, "end": 2810.72, "text": " now we need to have a simple client that anyone is able to use. So, element tax was, iOS was", "tokens": [586, 321, 643, 281, 362, 257, 2199, 6423, 300, 2878, 307, 1075, 281, 764, 13, 407, 11, 4478, 3366, 390, 11, 17430, 390], "temperature": 0.0, "avg_logprob": -0.2259850087373153, "compression_ratio": 1.5537190082644627, "no_speech_prob": 9.732911712490022e-05}, {"id": 489, "seek": 280468, "start": 2810.72, "end": 2815.56, "text": " then born. On the Android side, things are slightly different, because technically speaking,", "tokens": [550, 4232, 13, 1282, 264, 8853, 1252, 11, 721, 366, 4748, 819, 11, 570, 12120, 4124, 11], "temperature": 0.0, "avg_logprob": -0.2259850087373153, "compression_ratio": 1.5537190082644627, "no_speech_prob": 9.732911712490022e-05}, {"id": 490, "seek": 280468, "start": 2815.56, "end": 2822.7599999999998, "text": " the Android application already had a rewrite in 2019. So, we had two choices. We could essentially", "tokens": [264, 8853, 3861, 1217, 632, 257, 28132, 294, 6071, 13, 407, 11, 321, 632, 732, 7994, 13, 492, 727, 4476], "temperature": 0.0, "avg_logprob": -0.2259850087373153, "compression_ratio": 1.5537190082644627, "no_speech_prob": 9.732911712490022e-05}, {"id": 491, "seek": 280468, "start": 2822.7599999999998, "end": 2831.04, "text": " just take the Android SDK, put it on a side, and pretty much replace it with the Rust SDK,", "tokens": [445, 747, 264, 8853, 37135, 11, 829, 309, 322, 257, 1252, 11, 293, 1238, 709, 7406, 309, 365, 264, 34952, 37135, 11], "temperature": 0.0, "avg_logprob": -0.2259850087373153, "compression_ratio": 1.5537190082644627, "no_speech_prob": 9.732911712490022e-05}, {"id": 492, "seek": 283104, "start": 2831.04, "end": 2837.84, "text": " or maybe just rewrite it from scratch and using pretty much the state-of-the-art frameworks", "tokens": [420, 1310, 445, 28132, 309, 490, 8459, 293, 1228, 1238, 709, 264, 1785, 12, 2670, 12, 3322, 12, 446, 29834], "temperature": 0.0, "avg_logprob": -0.15437176601945862, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.0002037389058386907}, {"id": 493, "seek": 283104, "start": 2837.84, "end": 2842.44, "text": " that Android provides right now, like for example Jetpack Compose. In the end, we decided to go", "tokens": [300, 8853, 6417, 558, 586, 11, 411, 337, 1365, 28730, 9539, 6620, 541, 13, 682, 264, 917, 11, 321, 3047, 281, 352], "temperature": 0.0, "avg_logprob": -0.15437176601945862, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.0002037389058386907}, {"id": 494, "seek": 283104, "start": 2842.44, "end": 2847.24, "text": " for the latter, for two reasons. First of all, I mean, if we're building an application on iOS", "tokens": [337, 264, 18481, 11, 337, 732, 4112, 13, 2386, 295, 439, 11, 286, 914, 11, 498, 321, 434, 2390, 364, 3861, 322, 17430], "temperature": 0.0, "avg_logprob": -0.15437176601945862, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.0002037389058386907}, {"id": 495, "seek": 283104, "start": 2847.24, "end": 2851.84, "text": " that uses the latest frameworks, why do you want to do the same for Android? And second,", "tokens": [300, 4960, 264, 6792, 29834, 11, 983, 360, 291, 528, 281, 360, 264, 912, 337, 8853, 30, 400, 1150, 11], "temperature": 0.0, "avg_logprob": -0.15437176601945862, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.0002037389058386907}, {"id": 496, "seek": 283104, "start": 2851.84, "end": 2857.84, "text": " UX, as I said, UX was a very, very important concern. So, even if you wanted to rebuild the", "tokens": [40176, 11, 382, 286, 848, 11, 40176, 390, 257, 588, 11, 588, 1021, 3136, 13, 407, 11, 754, 498, 291, 1415, 281, 16877, 264], "temperature": 0.0, "avg_logprob": -0.15437176601945862, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.0002037389058386907}, {"id": 497, "seek": 285784, "start": 2857.84, "end": 2862.4, "text": " app from scratch or rewrite it, just pretty much change some stuff for the existing app,", "tokens": [724, 490, 8459, 420, 28132, 309, 11, 445, 1238, 709, 1319, 512, 1507, 337, 264, 6741, 724, 11], "temperature": 0.0, "avg_logprob": -0.14662114585318217, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.00035864365054294467}, {"id": 498, "seek": 285784, "start": 2862.4, "end": 2868.88, "text": " it would still require pretty much a huge UX overall, which in the end made the rewrite even", "tokens": [309, 576, 920, 3651, 1238, 709, 257, 2603, 40176, 4787, 11, 597, 294, 264, 917, 1027, 264, 28132, 754], "temperature": 0.0, "avg_logprob": -0.14662114585318217, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.00035864365054294467}, {"id": 499, "seek": 285784, "start": 2868.88, "end": 2876.44, "text": " more sense. So, pretty much obviously the architecture of element tax structured. Well,", "tokens": [544, 2020, 13, 407, 11, 1238, 709, 2745, 264, 9482, 295, 4478, 3366, 18519, 13, 1042, 11], "temperature": 0.0, "avg_logprob": -0.14662114585318217, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.00035864365054294467}, {"id": 500, "seek": 285784, "start": 2876.44, "end": 2880.7200000000003, "text": " we have pretty much the backbone of the client. It's pretty much all sitting in the", "tokens": [321, 362, 1238, 709, 264, 34889, 295, 264, 6423, 13, 467, 311, 1238, 709, 439, 3798, 294, 264], "temperature": 0.0, "avg_logprob": -0.14662114585318217, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.00035864365054294467}, {"id": 501, "seek": 288072, "start": 2880.72, "end": 2887.52, "text": " Magic Rust SDK. It's all there. And the Magic Rust SDK through Uni-SFI is able to expose with", "tokens": [16154, 34952, 37135, 13, 467, 311, 439, 456, 13, 400, 264, 16154, 34952, 37135, 807, 35191, 12, 50, 38568, 307, 1075, 281, 19219, 365], "temperature": 0.0, "avg_logprob": -0.22680567131667842, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0005837455973960459}, {"id": 502, "seek": 288072, "start": 2887.52, "end": 2893.8799999999997, "text": " bindings and causing bindings. It's interesting because, as pretty much Ben said, it's exposing", "tokens": [14786, 1109, 293, 9853, 14786, 1109, 13, 467, 311, 1880, 570, 11, 382, 1238, 709, 3964, 848, 11, 309, 311, 33178], "temperature": 0.0, "avg_logprob": -0.22680567131667842, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0005837455973960459}, {"id": 503, "seek": 288072, "start": 2893.8799999999997, "end": 2900.2799999999997, "text": " objects that are reactive, that the client is the only thing that it needs to care about,", "tokens": [6565, 300, 366, 28897, 11, 300, 264, 6423, 307, 264, 787, 551, 300, 309, 2203, 281, 1127, 466, 11], "temperature": 0.0, "avg_logprob": -0.22680567131667842, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0005837455973960459}, {"id": 504, "seek": 288072, "start": 2900.2799999999997, "end": 2904.72, "text": " doesn't need to care about the events, all the events, the newer events. It just needs to know", "tokens": [1177, 380, 643, 281, 1127, 466, 264, 3931, 11, 439, 264, 3931, 11, 264, 17628, 3931, 13, 467, 445, 2203, 281, 458], "temperature": 0.0, "avg_logprob": -0.22680567131667842, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0005837455973960459}, {"id": 505, "seek": 288072, "start": 2904.72, "end": 2909.0, "text": " that the event has been changed and it's in that place. It doesn't need to know that it's a new", "tokens": [300, 264, 2280, 575, 668, 3105, 293, 309, 311, 294, 300, 1081, 13, 467, 1177, 380, 643, 281, 458, 300, 309, 311, 257, 777], "temperature": 0.0, "avg_logprob": -0.22680567131667842, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0005837455973960459}, {"id": 506, "seek": 290900, "start": 2909.0, "end": 2914.24, "text": " event that came afterwards and so on. So, the idea is that these objects that the bindings expose", "tokens": [2280, 300, 1361, 10543, 293, 370, 322, 13, 407, 11, 264, 1558, 307, 300, 613, 6565, 300, 264, 14786, 1109, 19219], "temperature": 0.0, "avg_logprob": -0.19989936692374094, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.00012548382801469415}, {"id": 507, "seek": 290900, "start": 2914.24, "end": 2920.56, "text": " are actually already ready to be displayed, essentially. So, you just need to render them", "tokens": [366, 767, 1217, 1919, 281, 312, 16372, 11, 4476, 13, 407, 11, 291, 445, 643, 281, 15529, 552], "temperature": 0.0, "avg_logprob": -0.19989936692374094, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.00012548382801469415}, {"id": 508, "seek": 290900, "start": 2920.56, "end": 2927.76, "text": " on the UI and that makes the development way wazer. And of course, the sliding sink is pretty", "tokens": [322, 264, 15682, 293, 300, 1669, 264, 3250, 636, 261, 921, 260, 13, 400, 295, 1164, 11, 264, 21169, 9500, 307, 1238], "temperature": 0.0, "avg_logprob": -0.19989936692374094, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.00012548382801469415}, {"id": 509, "seek": 290900, "start": 2927.76, "end": 2932.12, "text": " much a requirement on element tax in the sense that it's being built with the idea that the", "tokens": [709, 257, 11695, 322, 4478, 3366, 294, 264, 2020, 300, 309, 311, 885, 3094, 365, 264, 1558, 300, 264], "temperature": 0.0, "avg_logprob": -0.19989936692374094, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.00012548382801469415}, {"id": 510, "seek": 290900, "start": 2932.12, "end": 2938.24, "text": " sliding sink will be pretty much next standard for the clients. And so, it will only work with", "tokens": [21169, 9500, 486, 312, 1238, 709, 958, 3832, 337, 264, 6982, 13, 400, 370, 11, 309, 486, 787, 589, 365], "temperature": 0.0, "avg_logprob": -0.19989936692374094, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.00012548382801469415}, {"id": 511, "seek": 293824, "start": 2938.24, "end": 2942.9599999999996, "text": " servers that implement for now this sliding sink proxy, essentially. So, this is an example of", "tokens": [15909, 300, 4445, 337, 586, 341, 21169, 9500, 29690, 11, 4476, 13, 407, 11, 341, 307, 364, 1365, 295], "temperature": 0.0, "avg_logprob": -0.17368145646720096, "compression_ratio": 1.6491228070175439, "no_speech_prob": 9.237299673259258e-05}, {"id": 512, "seek": 293824, "start": 2942.9599999999996, "end": 2950.64, "text": " how the code is pretty much translated from Rust into Zwift and Kotlin through Uni-SFI. As you", "tokens": [577, 264, 3089, 307, 1238, 709, 16805, 490, 34952, 666, 29385, 2008, 293, 30123, 5045, 807, 35191, 12, 50, 38568, 13, 1018, 291], "temperature": 0.0, "avg_logprob": -0.17368145646720096, "compression_ratio": 1.6491228070175439, "no_speech_prob": 9.237299673259258e-05}, {"id": 513, "seek": 293824, "start": 2950.64, "end": 2955.56, "text": " can see, there is the timeline item. I would say it is pretty much an object that is pretty much", "tokens": [393, 536, 11, 456, 307, 264, 12933, 3174, 13, 286, 576, 584, 309, 307, 1238, 709, 364, 2657, 300, 307, 1238, 709], "temperature": 0.0, "avg_logprob": -0.17368145646720096, "compression_ratio": 1.6491228070175439, "no_speech_prob": 9.237299673259258e-05}, {"id": 514, "seek": 293824, "start": 2955.56, "end": 2961.2, "text": " like a view model. It's already ready to be displayed. It just pretty much need to take the", "tokens": [411, 257, 1910, 2316, 13, 467, 311, 1217, 1919, 281, 312, 16372, 13, 467, 445, 1238, 709, 643, 281, 747, 264], "temperature": 0.0, "avg_logprob": -0.17368145646720096, "compression_ratio": 1.6491228070175439, "no_speech_prob": 9.237299673259258e-05}, {"id": 515, "seek": 293824, "start": 2961.2, "end": 2966.4399999999996, "text": " presentation data from this object and render them on the UI and that's it. Which will make", "tokens": [5860, 1412, 490, 341, 2657, 293, 15529, 552, 322, 264, 15682, 293, 300, 311, 309, 13, 3013, 486, 652], "temperature": 0.0, "avg_logprob": -0.17368145646720096, "compression_ratio": 1.6491228070175439, "no_speech_prob": 9.237299673259258e-05}, {"id": 516, "seek": 296644, "start": 2966.44, "end": 2973.6, "text": " implementing clients for the future with the Matrix Rust SDK way wazer. So, the bindings are", "tokens": [18114, 6982, 337, 264, 2027, 365, 264, 36274, 34952, 37135, 636, 261, 921, 260, 13, 407, 11, 264, 14786, 1109, 366], "temperature": 0.0, "avg_logprob": -0.24901628056797412, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.00011499185347929597}, {"id": 517, "seek": 296644, "start": 2973.6, "end": 2981.36, "text": " pretty much separate to repo. Anyone can download them as a file, a year file for Android or as", "tokens": [1238, 709, 4994, 281, 49040, 13, 14643, 393, 5484, 552, 382, 257, 3991, 11, 257, 1064, 3991, 337, 8853, 420, 382], "temperature": 0.0, "avg_logprob": -0.24901628056797412, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.00011499185347929597}, {"id": 518, "seek": 296644, "start": 2981.36, "end": 2986.2000000000003, "text": " a framework for Zwift implementations or you can just pretty much use a package manager like", "tokens": [257, 8388, 337, 29385, 2008, 4445, 763, 420, 291, 393, 445, 1238, 709, 764, 257, 7372, 6598, 411], "temperature": 0.0, "avg_logprob": -0.24901628056797412, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.00011499185347929597}, {"id": 519, "seek": 296644, "start": 2986.2000000000003, "end": 2992.32, "text": " Maven Central on Android or Zwift package manager on, yeah, on Zwift implementations,", "tokens": [4042, 553, 9701, 322, 8853, 420, 29385, 2008, 7372, 6598, 322, 11, 1338, 11, 322, 29385, 2008, 4445, 763, 11], "temperature": 0.0, "avg_logprob": -0.24901628056797412, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.00011499185347929597}, {"id": 520, "seek": 296644, "start": 2992.32, "end": 2996.0, "text": " essentially. I think it's Zwift implementations because actually it's interesting but the", "tokens": [4476, 13, 286, 519, 309, 311, 29385, 2008, 4445, 763, 570, 767, 309, 311, 1880, 457, 264], "temperature": 0.0, "avg_logprob": -0.24901628056797412, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.00011499185347929597}, {"id": 521, "seek": 299600, "start": 2996.0, "end": 3000.56, "text": " Matrix Rust SDK is scalable of running on any Apple system target. So, I really can't wait", "tokens": [36274, 34952, 37135, 307, 38481, 295, 2614, 322, 604, 6373, 1185, 3779, 13, 407, 11, 286, 534, 393, 380, 1699], "temperature": 0.0, "avg_logprob": -0.2109938479484396, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.00014382315566763282}, {"id": 522, "seek": 299600, "start": 3000.56, "end": 3006.92, "text": " someone crazy enough to build a client for Apple Watch or Apple TV. I'm pretty sure that the", "tokens": [1580, 3219, 1547, 281, 1322, 257, 6423, 337, 6373, 7277, 420, 6373, 3558, 13, 286, 478, 1238, 988, 300, 264], "temperature": 0.0, "avg_logprob": -0.2109938479484396, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.00014382315566763282}, {"id": 523, "seek": 299600, "start": 3006.92, "end": 3014.44, "text": " 10 people in the world at Apple TV will be very pleased that there is a Matrix client on their", "tokens": [1266, 561, 294, 264, 1002, 412, 6373, 3558, 486, 312, 588, 10587, 300, 456, 307, 257, 36274, 6423, 322, 641], "temperature": 0.0, "avg_logprob": -0.2109938479484396, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.00014382315566763282}, {"id": 524, "seek": 299600, "start": 3014.44, "end": 3022.8, "text": " Apple TV. But, of course, ElementX is going to share more than just the Rust SDK. We're pretty", "tokens": [6373, 3558, 13, 583, 11, 295, 1164, 11, 20900, 55, 307, 516, 281, 2073, 544, 813, 445, 264, 34952, 37135, 13, 492, 434, 1238], "temperature": 0.0, "avg_logprob": -0.2109938479484396, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.00014382315566763282}, {"id": 525, "seek": 302280, "start": 3022.8, "end": 3029.88, "text": " much trying to build other components that we hope to share just across ElementX but across", "tokens": [709, 1382, 281, 1322, 661, 6677, 300, 321, 1454, 281, 2073, 445, 2108, 20900, 55, 457, 2108], "temperature": 0.0, "avg_logprob": -0.20037846951871305, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00012632877042051405}, {"id": 526, "seek": 302280, "start": 3029.88, "end": 3034.8, "text": " multiple projects. For example, we want to build an OpenID Connect component, an Element call", "tokens": [3866, 4455, 13, 1171, 1365, 11, 321, 528, 281, 1322, 364, 7238, 2777, 11653, 6542, 11, 364, 20900, 818], "temperature": 0.0, "avg_logprob": -0.20037846951871305, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00012632877042051405}, {"id": 527, "seek": 302280, "start": 3034.8, "end": 3039.32, "text": " component. And, of course, since the two apps are pretty much the same up on different platforms,", "tokens": [6542, 13, 400, 11, 295, 1164, 11, 1670, 264, 732, 7733, 366, 1238, 709, 264, 912, 493, 322, 819, 9473, 11], "temperature": 0.0, "avg_logprob": -0.20037846951871305, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00012632877042051405}, {"id": 528, "seek": 302280, "start": 3039.32, "end": 3043.96, "text": " they're going to share translation, they're going to share design tokens. So, I mean, why don't we", "tokens": [436, 434, 516, 281, 2073, 12853, 11, 436, 434, 516, 281, 2073, 1715, 22667, 13, 407, 11, 286, 914, 11, 983, 500, 380, 321], "temperature": 0.0, "avg_logprob": -0.20037846951871305, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00012632877042051405}, {"id": 529, "seek": 302280, "start": 3043.96, "end": 3052.1200000000003, "text": " just pretty much make a component to share these elements already. And we're actually also building", "tokens": [445, 1238, 709, 652, 257, 6542, 281, 2073, 613, 4959, 1217, 13, 400, 321, 434, 767, 611, 2390], "temperature": 0.0, "avg_logprob": -0.20037846951871305, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00012632877042051405}, {"id": 530, "seek": 305212, "start": 3052.12, "end": 3060.92, "text": " an interesting component which is called the Rich Text Editor, which is essentially an SDK of", "tokens": [364, 1880, 6542, 597, 307, 1219, 264, 6781, 18643, 24281, 11, 597, 307, 4476, 364, 37135, 295], "temperature": 0.0, "avg_logprob": -0.21716822999896426, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.00041035329923033714}, {"id": 531, "seek": 305212, "start": 3060.92, "end": 3068.68, "text": " written Rust that then exposes these bindings in Zwift and Kotlin through edify and also in", "tokens": [3720, 34952, 300, 550, 1278, 4201, 613, 14786, 1109, 294, 29385, 2008, 293, 30123, 5045, 807, 1257, 2505, 293, 611, 294], "temperature": 0.0, "avg_logprob": -0.21716822999896426, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.00041035329923033714}, {"id": 532, "seek": 305212, "start": 3068.68, "end": 3079.52, "text": " WebAssembly. And it's essentially a UI framework that you can import into your client to render", "tokens": [9573, 10884, 19160, 13, 400, 309, 311, 4476, 257, 15682, 8388, 300, 291, 393, 974, 666, 428, 6423, 281, 15529], "temperature": 0.0, "avg_logprob": -0.21716822999896426, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.00041035329923033714}, {"id": 533, "seek": 307952, "start": 3079.52, "end": 3085.84, "text": " rich text in what you see is what you get fashion, essentially. It's something that is going to", "tokens": [4593, 2487, 294, 437, 291, 536, 307, 437, 291, 483, 6700, 11, 4476, 13, 467, 311, 746, 300, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.21317465951509565, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00017503350682090968}, {"id": 534, "seek": 307952, "start": 3085.84, "end": 3092.84, "text": " come also into ElementX. So, keep an eye for it. But, hey, what's this like? Oh, actually,", "tokens": [808, 611, 666, 20900, 55, 13, 407, 11, 1066, 364, 3313, 337, 309, 13, 583, 11, 4177, 11, 437, 311, 341, 411, 30, 876, 11, 767, 11], "temperature": 0.0, "avg_logprob": -0.21317465951509565, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00017503350682090968}, {"id": 535, "seek": 307952, "start": 3092.84, "end": 3098.96, "text": " it's already there. Oh, but this is not ElementX. Actually, this element, the Rich Text Editor is", "tokens": [309, 311, 1217, 456, 13, 876, 11, 457, 341, 307, 406, 20900, 55, 13, 5135, 11, 341, 4478, 11, 264, 6781, 18643, 24281, 307], "temperature": 0.0, "avg_logprob": -0.21317465951509565, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00017503350682090968}, {"id": 536, "seek": 307952, "start": 3098.96, "end": 3105.84, "text": " already in Element right now. But in iOS, Android, and Web, you can enable it in labs. You can just", "tokens": [1217, 294, 20900, 558, 586, 13, 583, 294, 17430, 11, 8853, 11, 293, 9573, 11, 291, 393, 9528, 309, 294, 20339, 13, 509, 393, 445], "temperature": 0.0, "avg_logprob": -0.21317465951509565, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00017503350682090968}, {"id": 537, "seek": 310584, "start": 3105.84, "end": 3111.1200000000003, "text": " go to labs, enable it, and test it. And, you know, if you're able to break it, just, you know,", "tokens": [352, 281, 20339, 11, 9528, 309, 11, 293, 1500, 309, 13, 400, 11, 291, 458, 11, 498, 291, 434, 1075, 281, 1821, 309, 11, 445, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15672369301319122, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0003250444424338639}, {"id": 538, "seek": 310584, "start": 3111.1200000000003, "end": 3115.96, "text": " send us some feedback and we will try to fix it as soon as possible. It's a project that I've", "tokens": [2845, 505, 512, 5824, 293, 321, 486, 853, 281, 3191, 309, 382, 2321, 382, 1944, 13, 467, 311, 257, 1716, 300, 286, 600], "temperature": 0.0, "avg_logprob": -0.15672369301319122, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0003250444424338639}, {"id": 539, "seek": 310584, "start": 3115.96, "end": 3121.6000000000004, "text": " worked on. I'm very proud of it. I think we achieved something really great because it's a very", "tokens": [2732, 322, 13, 286, 478, 588, 4570, 295, 309, 13, 286, 519, 321, 11042, 746, 534, 869, 570, 309, 311, 257, 588], "temperature": 0.0, "avg_logprob": -0.15672369301319122, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0003250444424338639}, {"id": 540, "seek": 310584, "start": 3121.6000000000004, "end": 3126.8, "text": " simple way to pretty much, it's a way in which you can create rich text without need of using", "tokens": [2199, 636, 281, 1238, 709, 11, 309, 311, 257, 636, 294, 597, 291, 393, 1884, 4593, 2487, 1553, 643, 295, 1228], "temperature": 0.0, "avg_logprob": -0.15672369301319122, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0003250444424338639}, {"id": 541, "seek": 310584, "start": 3126.8, "end": 3131.96, "text": " markdowns and see how they look like, which will make life easier for when you need to create", "tokens": [1491, 5093, 82, 293, 536, 577, 436, 574, 411, 11, 597, 486, 652, 993, 3571, 337, 562, 291, 643, 281, 1884], "temperature": 0.0, "avg_logprob": -0.15672369301319122, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0003250444424338639}, {"id": 542, "seek": 313196, "start": 3131.96, "end": 3136.92, "text": " something like this. Because I challenge everyone to make something like this with markdowns. I", "tokens": [746, 411, 341, 13, 1436, 286, 3430, 1518, 281, 652, 746, 411, 341, 365, 1491, 5093, 82, 13, 286], "temperature": 0.0, "avg_logprob": -0.17273971438407898, "compression_ratio": 1.743682310469314, "no_speech_prob": 3.679705332615413e-05}, {"id": 543, "seek": 313196, "start": 3136.92, "end": 3144.2400000000002, "text": " mean, you'd go crazy with that. So, yeah, the cool thing is that this Rich Text Editor SDK that", "tokens": [914, 11, 291, 1116, 352, 3219, 365, 300, 13, 407, 11, 1338, 11, 264, 1627, 551, 307, 300, 341, 6781, 18643, 24281, 37135, 300], "temperature": 0.0, "avg_logprob": -0.17273971438407898, "compression_ratio": 1.743682310469314, "no_speech_prob": 3.679705332615413e-05}, {"id": 544, "seek": 313196, "start": 3144.2400000000002, "end": 3148.68, "text": " we built, I mean, it's not just for metrics, so, metrics client. I mean, technically speaking,", "tokens": [321, 3094, 11, 286, 914, 11, 309, 311, 406, 445, 337, 16367, 11, 370, 11, 16367, 6423, 13, 286, 914, 11, 12120, 4124, 11], "temperature": 0.0, "avg_logprob": -0.17273971438407898, "compression_ratio": 1.743682310469314, "no_speech_prob": 3.679705332615413e-05}, {"id": 545, "seek": 313196, "start": 3148.68, "end": 3155.56, "text": " anyone could use this. Maybe you want to make a note app. You want to make, I don't know, like an", "tokens": [2878, 727, 764, 341, 13, 2704, 291, 528, 281, 652, 257, 3637, 724, 13, 509, 528, 281, 652, 11, 286, 500, 380, 458, 11, 411, 364], "temperature": 0.0, "avg_logprob": -0.17273971438407898, "compression_ratio": 1.743682310469314, "no_speech_prob": 3.679705332615413e-05}, {"id": 546, "seek": 313196, "start": 3155.56, "end": 3160.84, "text": " app that is your diary, whatever you want. You can pretty much implement this. And, if you want to", "tokens": [724, 300, 307, 428, 26492, 11, 2035, 291, 528, 13, 509, 393, 1238, 709, 4445, 341, 13, 400, 11, 498, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.17273971438407898, "compression_ratio": 1.743682310469314, "no_speech_prob": 3.679705332615413e-05}, {"id": 547, "seek": 316084, "start": 3160.84, "end": 3166.32, "text": " test it, you can scan the QR code. You will get pretty much to the latest main implementation on", "tokens": [1500, 309, 11, 291, 393, 11049, 264, 32784, 3089, 13, 509, 486, 483, 1238, 709, 281, 264, 6792, 2135, 11420, 322], "temperature": 0.0, "avg_logprob": -0.1468021746763249, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00011397211346775293}, {"id": 548, "seek": 316084, "start": 3166.32, "end": 3172.36, "text": " web. It's a test debug version there. The one on labs is more stable. This one is more to play", "tokens": [3670, 13, 467, 311, 257, 1500, 24083, 3037, 456, 13, 440, 472, 322, 20339, 307, 544, 8351, 13, 639, 472, 307, 544, 281, 862], "temperature": 0.0, "avg_logprob": -0.1468021746763249, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00011397211346775293}, {"id": 549, "seek": 316084, "start": 3172.36, "end": 3179.2400000000002, "text": " around with it. It's cool because this one, it allows you to pretty much see how the rich text is", "tokens": [926, 365, 309, 13, 467, 311, 1627, 570, 341, 472, 11, 309, 4045, 291, 281, 1238, 709, 536, 577, 264, 4593, 2487, 307], "temperature": 0.0, "avg_logprob": -0.1468021746763249, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00011397211346775293}, {"id": 550, "seek": 316084, "start": 3179.2400000000002, "end": 3187.1200000000003, "text": " transformed into a DOM representation, which is in Rust, and then transformed back into an HTML,", "tokens": [16894, 666, 257, 35727, 10290, 11, 597, 307, 294, 34952, 11, 293, 550, 16894, 646, 666, 364, 17995, 11], "temperature": 0.0, "avg_logprob": -0.1468021746763249, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.00011397211346775293}, {"id": 551, "seek": 318712, "start": 3187.12, "end": 3192.7599999999998, "text": " which is the one that we are sending over the metrics lines, of course. So, of course, testing", "tokens": [597, 307, 264, 472, 300, 321, 366, 7750, 670, 264, 16367, 3876, 11, 295, 1164, 13, 407, 11, 295, 1164, 11, 4997], "temperature": 0.0, "avg_logprob": -0.24649340980931334, "compression_ratio": 1.7321428571428572, "no_speech_prob": 6.325345020741224e-05}, {"id": 552, "seek": 318712, "start": 3192.7599999999998, "end": 3197.24, "text": " for reliability, another important keyword. It's something that we want to pretty much improve.", "tokens": [337, 24550, 11, 1071, 1021, 20428, 13, 467, 311, 746, 300, 321, 528, 281, 1238, 709, 3470, 13], "temperature": 0.0, "avg_logprob": -0.24649340980931334, "compression_ratio": 1.7321428571428572, "no_speech_prob": 6.325345020741224e-05}, {"id": 553, "seek": 318712, "start": 3197.24, "end": 3204.64, "text": " And so, pretty much, we built a very, yeah, very stack test infrastructure that we hope is going", "tokens": [400, 370, 11, 1238, 709, 11, 321, 3094, 257, 588, 11, 1338, 11, 588, 8630, 1500, 6896, 300, 321, 1454, 307, 516], "temperature": 0.0, "avg_logprob": -0.24649340980931334, "compression_ratio": 1.7321428571428572, "no_speech_prob": 6.325345020741224e-05}, {"id": 554, "seek": 318712, "start": 3204.64, "end": 3211.04, "text": " to cover all these areas. It's already covering most of these areas. And, yeah, pretty much make the", "tokens": [281, 2060, 439, 613, 3179, 13, 467, 311, 1217, 10322, 881, 295, 613, 3179, 13, 400, 11, 1338, 11, 1238, 709, 652, 264], "temperature": 0.0, "avg_logprob": -0.24649340980931334, "compression_ratio": 1.7321428571428572, "no_speech_prob": 6.325345020741224e-05}, {"id": 555, "seek": 321104, "start": 3211.04, "end": 3218.36, "text": " app more reliable, and the project way, way safer. So, yeah, ElementTax actually has come with a lot", "tokens": [724, 544, 12924, 11, 293, 264, 1716, 636, 11, 636, 15856, 13, 407, 11, 1338, 11, 20900, 51, 2797, 767, 575, 808, 365, 257, 688], "temperature": 0.0, "avg_logprob": -0.2620527417052026, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.00035491317976266146}, {"id": 556, "seek": 321104, "start": 3218.36, "end": 3224.72, "text": " of benefits. First of all, on the tech side, it's way, way faster, both because the metrics", "tokens": [295, 5311, 13, 2386, 295, 439, 11, 322, 264, 7553, 1252, 11, 309, 311, 636, 11, 636, 4663, 11, 1293, 570, 264, 16367], "temperature": 0.0, "avg_logprob": -0.2620527417052026, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.00035491317976266146}, {"id": 557, "seek": 321104, "start": 3224.72, "end": 3229.64, "text": " for us has decayed. I mean, it's amazing. It makes things easier. Both from a development standpoint,", "tokens": [337, 505, 575, 21039, 292, 13, 286, 914, 11, 309, 311, 2243, 13, 467, 1669, 721, 3571, 13, 6767, 490, 257, 3250, 15827, 11], "temperature": 0.0, "avg_logprob": -0.2620527417052026, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.00035491317976266146}, {"id": 558, "seek": 321104, "start": 3229.64, "end": 3235.48, "text": " because you just write it once and deploy it everywhere. But at the same time, the fact that", "tokens": [570, 291, 445, 2464, 309, 1564, 293, 7274, 309, 5315, 13, 583, 412, 264, 912, 565, 11, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.2620527417052026, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.00035491317976266146}, {"id": 559, "seek": 323548, "start": 3235.48, "end": 3242.8, "text": " you just have your models already ready to be displayed, it's amazing. And also, slide-in sync.", "tokens": [291, 445, 362, 428, 5245, 1217, 1919, 281, 312, 16372, 11, 309, 311, 2243, 13, 400, 611, 11, 4137, 12, 259, 20271, 13], "temperature": 0.0, "avg_logprob": -0.23356503312305738, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.0001684822200331837}, {"id": 560, "seek": 323548, "start": 3242.8, "end": 3249.04, "text": " And, of course, the use of declarative UIs like SwiftUI and Jetpack Compose makes development", "tokens": [400, 11, 295, 1164, 11, 264, 764, 295, 16694, 1166, 624, 6802, 411, 25539, 46324, 293, 28730, 9539, 6620, 541, 1669, 3250], "temperature": 0.0, "avg_logprob": -0.23356503312305738, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.0001684822200331837}, {"id": 561, "seek": 323548, "start": 3249.04, "end": 3259.0, "text": " time actually faster. And actually, it's also easier to test, I would say. But also, the UI", "tokens": [565, 767, 4663, 13, 400, 767, 11, 309, 311, 611, 3571, 281, 1500, 11, 286, 576, 584, 13, 583, 611, 11, 264, 15682], "temperature": 0.0, "avg_logprob": -0.23356503312305738, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.0001684822200331837}, {"id": 562, "seek": 323548, "start": 3259.0, "end": 3265.2400000000002, "text": " performance has been improved, actually. Also, sharing components is something that will", "tokens": [3389, 575, 668, 9689, 11, 767, 13, 2743, 11, 5414, 6677, 307, 746, 300, 486], "temperature": 0.0, "avg_logprob": -0.23356503312305738, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.0001684822200331837}, {"id": 563, "seek": 326524, "start": 3265.24, "end": 3271.4799999999996, "text": " benefit not only just ElementTax, but pretty much any client that wants to implement a metrics", "tokens": [5121, 406, 787, 445, 20900, 51, 2797, 11, 457, 1238, 709, 604, 6423, 300, 2738, 281, 4445, 257, 16367], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 564, "seek": 326524, "start": 3271.4799999999996, "end": 3275.0, "text": " client. But actually, we hope that some of the sharing components that we're building will not", "tokens": [6423, 13, 583, 767, 11, 321, 1454, 300, 512, 295, 264, 5414, 6677, 300, 321, 434, 2390, 486, 406], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 565, "seek": 326524, "start": 3275.0, "end": 3280.9199999999996, "text": " just benefit the metrics community, but the overall open source community. So, yeah, but, yeah,", "tokens": [445, 5121, 264, 16367, 1768, 11, 457, 264, 4787, 1269, 4009, 1768, 13, 407, 11, 1338, 11, 457, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 566, "seek": 326524, "start": 3280.9199999999996, "end": 3284.3599999999997, "text": " the major benefit, actually, we should not focus just on the main benefit that we are", "tokens": [264, 2563, 5121, 11, 767, 11, 321, 820, 406, 1879, 445, 322, 264, 2135, 5121, 300, 321, 366], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 567, "seek": 326524, "start": 3284.3599999999997, "end": 3288.3599999999997, "text": " offering on the tech side. We actually want to focus on the benefit we're really offering to", "tokens": [8745, 322, 264, 7553, 1252, 13, 492, 767, 528, 281, 1879, 322, 264, 5121, 321, 434, 534, 8745, 281], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 568, "seek": 326524, "start": 3288.3599999999997, "end": 3293.16, "text": " the users, because in the end, the main focus ElementTax, yeah, its performance, its tech,", "tokens": [264, 5022, 11, 570, 294, 264, 917, 11, 264, 2135, 1879, 20900, 51, 2797, 11, 1338, 11, 1080, 3389, 11, 1080, 7553, 11], "temperature": 0.0, "avg_logprob": -0.13276088685917675, "compression_ratio": 1.9750889679715302, "no_speech_prob": 0.0005120585556142032}, {"id": 569, "seek": 329316, "start": 3293.16, "end": 3298.12, "text": " its sharing components, but, of course, it's making the app more usable, more accessible,", "tokens": [1080, 5414, 6677, 11, 457, 11, 295, 1164, 11, 309, 311, 1455, 264, 724, 544, 29975, 11, 544, 9515, 11], "temperature": 0.0, "avg_logprob": -0.09101422916759144, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.00019092483853455633}, {"id": 570, "seek": 329316, "start": 3298.12, "end": 3303.56, "text": " easier to use. We want to make an app that is not just... We want to make an app that,", "tokens": [3571, 281, 764, 13, 492, 528, 281, 652, 364, 724, 300, 307, 406, 445, 485, 492, 528, 281, 652, 364, 724, 300, 11], "temperature": 0.0, "avg_logprob": -0.09101422916759144, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.00019092483853455633}, {"id": 571, "seek": 329316, "start": 3303.56, "end": 3309.3999999999996, "text": " essentially, also can be used by your friends and family to chat with you, even casually,", "tokens": [4476, 11, 611, 393, 312, 1143, 538, 428, 1855, 293, 1605, 281, 5081, 365, 291, 11, 754, 34872, 11], "temperature": 0.0, "avg_logprob": -0.09101422916759144, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.00019092483853455633}, {"id": 572, "seek": 329316, "start": 3309.3999999999996, "end": 3312.8399999999997, "text": " during the day. So, not just for people that, essentially, want to keep their conversation", "tokens": [1830, 264, 786, 13, 407, 11, 406, 445, 337, 561, 300, 11, 4476, 11, 528, 281, 1066, 641, 3761], "temperature": 0.0, "avg_logprob": -0.09101422916759144, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.00019092483853455633}, {"id": 573, "seek": 329316, "start": 3312.8399999999997, "end": 3320.68, "text": " safe and secure for the metrics protocol. Roadmap. Pretty much this is the present", "tokens": [3273, 293, 7144, 337, 264, 16367, 10336, 13, 11507, 24223, 13, 10693, 709, 341, 307, 264, 1974], "temperature": 0.0, "avg_logprob": -0.09101422916759144, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.00019092483853455633}, {"id": 574, "seek": 332068, "start": 3320.68, "end": 3327.8799999999997, "text": " of the future of ElementTax. For now, you can log in, check the room list, timeline,", "tokens": [295, 264, 2027, 295, 20900, 51, 2797, 13, 1171, 586, 11, 291, 393, 3565, 294, 11, 1520, 264, 1808, 1329, 11, 12933, 11], "temperature": 0.0, "avg_logprob": -0.1862124034336635, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0004210805636830628}, {"id": 575, "seek": 332068, "start": 3327.8799999999997, "end": 3333.7999999999997, "text": " send messages, edit, reply, react. But there are some restrictions. First of all, of course,", "tokens": [2845, 7897, 11, 8129, 11, 16972, 11, 4515, 13, 583, 456, 366, 512, 14191, 13, 2386, 295, 439, 11, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.1862124034336635, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0004210805636830628}, {"id": 576, "seek": 332068, "start": 3333.7999999999997, "end": 3337.3199999999997, "text": " as I say, this lighting sync is required. So, if your server doesn't activate its lighting sync", "tokens": [382, 286, 584, 11, 341, 9577, 20271, 307, 4739, 13, 407, 11, 498, 428, 7154, 1177, 380, 13615, 1080, 9577, 20271], "temperature": 0.0, "avg_logprob": -0.1862124034336635, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0004210805636830628}, {"id": 577, "seek": 332068, "start": 3337.3199999999997, "end": 3343.3999999999996, "text": " proxy, yeah, you can much, pretty much use the client on that server. Also, it only supports", "tokens": [29690, 11, 1338, 11, 291, 393, 709, 11, 1238, 709, 764, 264, 6423, 322, 300, 7154, 13, 2743, 11, 309, 787, 9346], "temperature": 0.0, "avg_logprob": -0.1862124034336635, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0004210805636830628}, {"id": 578, "seek": 334340, "start": 3343.4, "end": 3350.76, "text": " authentication, and authentication, it's only through the metrics protocol. We want to support", "tokens": [26643, 11, 293, 26643, 11, 309, 311, 787, 807, 264, 16367, 10336, 13, 492, 528, 281, 1406], "temperature": 0.0, "avg_logprob": -0.16057139855844002, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.00029637207626365125}, {"id": 579, "seek": 334340, "start": 3350.76, "end": 3356.2000000000003, "text": " also IDC and registration, but when we will build the OIDC component, we will support that.", "tokens": [611, 7348, 34, 293, 16847, 11, 457, 562, 321, 486, 1322, 264, 422, 2777, 34, 6542, 11, 321, 486, 1406, 300, 13], "temperature": 0.0, "avg_logprob": -0.16057139855844002, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.00029637207626365125}, {"id": 580, "seek": 334340, "start": 3356.2000000000003, "end": 3360.6, "text": " Device verification is there, but only for emojis, so, no QR verification yet,", "tokens": [50140, 30206, 307, 456, 11, 457, 787, 337, 19611, 40371, 11, 370, 11, 572, 32784, 30206, 1939, 11], "temperature": 0.0, "avg_logprob": -0.16057139855844002, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.00029637207626365125}, {"id": 581, "seek": 334340, "start": 3360.6, "end": 3365.96, "text": " and also no messages through the description. Yeah, this is pretty much where you can find the", "tokens": [293, 611, 572, 7897, 807, 264, 3855, 13, 865, 11, 341, 307, 1238, 709, 689, 291, 393, 915, 264], "temperature": 0.0, "avg_logprob": -0.16057139855844002, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.00029637207626365125}, {"id": 582, "seek": 334340, "start": 3365.96, "end": 3372.76, "text": " ElementTax iOS version repo. There will be a public test flight coming soon, and actually,", "tokens": [20900, 51, 2797, 17430, 3037, 49040, 13, 821, 486, 312, 257, 1908, 1500, 7018, 1348, 2321, 11, 293, 767, 11], "temperature": 0.0, "avg_logprob": -0.16057139855844002, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.00029637207626365125}, {"id": 583, "seek": 337276, "start": 3372.76, "end": 3380.0400000000004, "text": " Matthew, will demo this in this afternoon? Yeah. Okay. That's the plan. And regarding", "tokens": [12434, 11, 486, 10723, 341, 294, 341, 6499, 30, 865, 13, 1033, 13, 663, 311, 264, 1393, 13, 400, 8595], "temperature": 0.0, "avg_logprob": -0.11963764953613282, "compression_ratio": 1.6875, "no_speech_prob": 0.0005115859676152468}, {"id": 584, "seek": 337276, "start": 3380.0400000000004, "end": 3385.1600000000003, "text": " ElementTax Android, it's a bit behind schedule, because, as I said, it was developed after", "tokens": [20900, 51, 2797, 8853, 11, 309, 311, 257, 857, 2261, 7567, 11, 570, 11, 382, 286, 848, 11, 309, 390, 4743, 934], "temperature": 0.0, "avg_logprob": -0.11963764953613282, "compression_ratio": 1.6875, "no_speech_prob": 0.0005115859676152468}, {"id": 585, "seek": 337276, "start": 3385.1600000000003, "end": 3390.84, "text": " ElementTax iOS, so it's more in a state of being set up. But of course, you can try to run it,", "tokens": [20900, 51, 2797, 17430, 11, 370, 309, 311, 544, 294, 257, 1785, 295, 885, 992, 493, 13, 583, 295, 1164, 11, 291, 393, 853, 281, 1190, 309, 11], "temperature": 0.0, "avg_logprob": -0.11963764953613282, "compression_ratio": 1.6875, "no_speech_prob": 0.0005115859676152468}, {"id": 586, "seek": 337276, "start": 3390.84, "end": 3394.84, "text": " check the state of the repo. You know, if you want to play around with it, this is pretty much", "tokens": [1520, 264, 1785, 295, 264, 49040, 13, 509, 458, 11, 498, 291, 528, 281, 862, 926, 365, 309, 11, 341, 307, 1238, 709], "temperature": 0.0, "avg_logprob": -0.11963764953613282, "compression_ratio": 1.6875, "no_speech_prob": 0.0005115859676152468}, {"id": 587, "seek": 337276, "start": 3394.84, "end": 3401.7200000000003, "text": " where you can find the actual repo of ElementTax Android. This is pretty much the roadmap on", "tokens": [689, 291, 393, 915, 264, 3539, 49040, 295, 20900, 51, 2797, 8853, 13, 639, 307, 1238, 709, 264, 35738, 322], "temperature": 0.0, "avg_logprob": -0.11963764953613282, "compression_ratio": 1.6875, "no_speech_prob": 0.0005115859676152468}, {"id": 588, "seek": 340172, "start": 3401.72, "end": 3406.3599999999997, "text": " what we plan to do. Actually, more than a plan, it's more like what we, let's say,", "tokens": [437, 321, 1393, 281, 360, 13, 5135, 11, 544, 813, 257, 1393, 11, 309, 311, 544, 411, 437, 321, 11, 718, 311, 584, 11], "temperature": 0.0, "avg_logprob": -0.1710797627766927, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0004313003446441144}, {"id": 589, "seek": 340172, "start": 3407.64, "end": 3411.9599999999996, "text": " it's more like, say, it's not a deadline, it's more like what we imagine we're able to achieve", "tokens": [309, 311, 544, 411, 11, 584, 11, 309, 311, 406, 257, 20615, 11, 309, 311, 544, 411, 437, 321, 3811, 321, 434, 1075, 281, 4584], "temperature": 0.0, "avg_logprob": -0.1710797627766927, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0004313003446441144}, {"id": 590, "seek": 340172, "start": 3411.9599999999996, "end": 3419.08, "text": " in these dates. And I was also told to be as vague as possible, so for the release date of the", "tokens": [294, 613, 11691, 13, 400, 286, 390, 611, 1907, 281, 312, 382, 24247, 382, 1944, 11, 370, 337, 264, 4374, 4002, 295, 264], "temperature": 0.0, "avg_logprob": -0.1710797627766927, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0004313003446441144}, {"id": 591, "seek": 340172, "start": 3419.08, "end": 3426.52, "text": " public launch, I will just say that it will come sometime in the future. All right. Okay. And that", "tokens": [1908, 4025, 11, 286, 486, 445, 584, 300, 309, 486, 808, 15053, 294, 264, 2027, 13, 1057, 558, 13, 1033, 13, 400, 300], "temperature": 0.0, "avg_logprob": -0.1710797627766927, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0004313003446441144}, {"id": 592, "seek": 342652, "start": 3426.52, "end": 3433.88, "text": " should deal with it. So, yeah, that's all. And we can do a, I think, a rapid QA session, right?", "tokens": [820, 2028, 365, 309, 13, 407, 11, 1338, 11, 300, 311, 439, 13, 400, 321, 393, 360, 257, 11, 286, 519, 11, 257, 7558, 1249, 32, 5481, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.27674911572383, "compression_ratio": 1.4430379746835442, "no_speech_prob": 0.0010380089515820146}, {"id": 593, "seek": 342652, "start": 3434.44, "end": 3438.44, "text": " Yes, yes. We have 10 minutes. Oh, okay. Nice. Right on schedule. Nice.", "tokens": [1079, 11, 2086, 13, 492, 362, 1266, 2077, 13, 876, 11, 1392, 13, 5490, 13, 1779, 322, 7567, 13, 5490, 13], "temperature": 0.0, "avg_logprob": -0.27674911572383, "compression_ratio": 1.4430379746835442, "no_speech_prob": 0.0010380089515820146}, {"id": 594, "seek": 342652, "start": 3440.68, "end": 3448.92, "text": " Okay. Yeah, it's this around. Yeah. Please go ahead. If I remember correctly, the sliding sync", "tokens": [1033, 13, 865, 11, 309, 311, 341, 926, 13, 865, 13, 2555, 352, 2286, 13, 759, 286, 1604, 8944, 11, 264, 21169, 20271], "temperature": 0.0, "avg_logprob": -0.27674911572383, "compression_ratio": 1.4430379746835442, "no_speech_prob": 0.0010380089515820146}, {"id": 595, "seek": 342652, "start": 3450.2, "end": 3455.4, "text": " option in ElementWeb said that you can't disable it in the warning, why is that?", "tokens": [3614, 294, 20900, 4360, 65, 848, 300, 291, 393, 380, 28362, 309, 294, 264, 9164, 11, 983, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.27674911572383, "compression_ratio": 1.4430379746835442, "no_speech_prob": 0.0010380089515820146}, {"id": 596, "seek": 345540, "start": 3455.4, "end": 3461.88, "text": " So, the question is, let me repeat it for the camera. Why can't you disable the sliding sync", "tokens": [407, 11, 264, 1168, 307, 11, 718, 385, 7149, 309, 337, 264, 2799, 13, 1545, 393, 380, 291, 28362, 264, 21169, 20271], "temperature": 0.0, "avg_logprob": -0.11346998810768127, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00013314060925040394}, {"id": 597, "seek": 345540, "start": 3461.88, "end": 3468.2000000000003, "text": " labs feature in the current version? Mostly because of end-to-end encrypted messages,", "tokens": [20339, 4111, 294, 264, 2190, 3037, 30, 29035, 570, 295, 917, 12, 1353, 12, 521, 36663, 7897, 11], "temperature": 0.0, "avg_logprob": -0.11346998810768127, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00013314060925040394}, {"id": 598, "seek": 345540, "start": 3468.2000000000003, "end": 3473.2400000000002, "text": " you would risk being unable to decrypt your end-to-end encrypted messages in that session.", "tokens": [291, 576, 3148, 885, 11299, 281, 979, 627, 662, 428, 917, 12, 1353, 12, 521, 36663, 7897, 294, 300, 5481, 13], "temperature": 0.0, "avg_logprob": -0.11346998810768127, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00013314060925040394}, {"id": 599, "seek": 345540, "start": 3473.88, "end": 3482.28, "text": " So, the reason why is because when you log into the proxy, it's going to be syncing on your account,", "tokens": [407, 11, 264, 1778, 983, 307, 570, 562, 291, 3565, 666, 264, 29690, 11, 309, 311, 516, 281, 312, 5451, 2175, 322, 428, 2696, 11], "temperature": 0.0, "avg_logprob": -0.11346998810768127, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.00013314060925040394}, {"id": 600, "seek": 348228, "start": 3482.28, "end": 3486.6800000000003, "text": " right? And it's going to sync forever. Well, until the access token gets invalidated,", "tokens": [558, 30, 400, 309, 311, 516, 281, 20271, 5680, 13, 1042, 11, 1826, 264, 2105, 14862, 2170, 34702, 770, 11], "temperature": 0.0, "avg_logprob": -0.09058014551798503, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.00012413260992616415}, {"id": 601, "seek": 348228, "start": 3486.6800000000003, "end": 3492.0400000000004, "text": " but it's going to be syncing on your behalf. If you toggled sliding sync on then off, if you", "tokens": [457, 309, 311, 516, 281, 312, 5451, 2175, 322, 428, 9490, 13, 759, 291, 26911, 1493, 21169, 20271, 322, 550, 766, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.09058014551798503, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.00012413260992616415}, {"id": 602, "seek": 348228, "start": 3492.0400000000004, "end": 3498.36, "text": " turned it off, then your ElementWeb would be using the V2 sync as well as the proxy,", "tokens": [3574, 309, 766, 11, 550, 428, 20900, 4360, 65, 576, 312, 1228, 264, 691, 17, 20271, 382, 731, 382, 264, 29690, 11], "temperature": 0.0, "avg_logprob": -0.09058014551798503, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.00012413260992616415}, {"id": 603, "seek": 348228, "start": 3498.36, "end": 3502.36, "text": " because the proxy didn't know you toggled it off. So, that means you've got two sync loops for your", "tokens": [570, 264, 29690, 994, 380, 458, 291, 26911, 1493, 309, 766, 13, 407, 11, 300, 1355, 291, 600, 658, 732, 20271, 16121, 337, 428], "temperature": 0.0, "avg_logprob": -0.09058014551798503, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.00012413260992616415}, {"id": 604, "seek": 348228, "start": 3502.36, "end": 3509.0, "text": " account. And that's going to cause problems when it causes a race condition because two device", "tokens": [2696, 13, 400, 300, 311, 516, 281, 3082, 2740, 562, 309, 7700, 257, 4569, 4188, 570, 732, 4302], "temperature": 0.0, "avg_logprob": -0.09058014551798503, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.00012413260992616415}, {"id": 605, "seek": 350900, "start": 3509.0, "end": 3513.72, "text": " messages, when they're acknowledged, and they get acknowledged by increasing the sync token,", "tokens": [7897, 11, 562, 436, 434, 27262, 11, 293, 436, 483, 27262, 538, 5662, 264, 20271, 14862, 11], "temperature": 0.0, "avg_logprob": -0.11310509272984096, "compression_ratio": 1.8636363636363635, "no_speech_prob": 9.107937512453645e-05}, {"id": 606, "seek": 350900, "start": 3514.44, "end": 3519.32, "text": " they get deleted on the server. So, if your ElementWeb was super, super fast and managed", "tokens": [436, 483, 22981, 322, 264, 7154, 13, 407, 11, 498, 428, 20900, 4360, 65, 390, 1687, 11, 1687, 2370, 293, 6453], "temperature": 0.0, "avg_logprob": -0.11310509272984096, "compression_ratio": 1.8636363636363635, "no_speech_prob": 9.107937512453645e-05}, {"id": 607, "seek": 350900, "start": 3519.32, "end": 3526.44, "text": " to race ahead slightly of the proxy, then it would go and get all the two device events,", "tokens": [281, 4569, 2286, 4748, 295, 264, 29690, 11, 550, 309, 576, 352, 293, 483, 439, 264, 732, 4302, 3931, 11], "temperature": 0.0, "avg_logprob": -0.11310509272984096, "compression_ratio": 1.8636363636363635, "no_speech_prob": 9.107937512453645e-05}, {"id": 608, "seek": 350900, "start": 3526.44, "end": 3532.44, "text": " and the proxy would not, or vice versa. And vice versa is the problem that's trying to warn against.", "tokens": [293, 264, 29690, 576, 406, 11, 420, 11964, 25650, 13, 400, 11964, 25650, 307, 264, 1154, 300, 311, 1382, 281, 12286, 1970, 13], "temperature": 0.0, "avg_logprob": -0.11310509272984096, "compression_ratio": 1.8636363636363635, "no_speech_prob": 9.107937512453645e-05}, {"id": 609, "seek": 350900, "start": 3532.44, "end": 3538.44, "text": " So, if the proxy was ahead, then you would not get a certain two device events,", "tokens": [407, 11, 498, 264, 29690, 390, 2286, 11, 550, 291, 576, 406, 483, 257, 1629, 732, 4302, 3931, 11], "temperature": 0.0, "avg_logprob": -0.11310509272984096, "compression_ratio": 1.8636363636363635, "no_speech_prob": 9.107937512453645e-05}, {"id": 610, "seek": 353844, "start": 3538.44, "end": 3542.76, "text": " and therefore you may potentially lose room keys, and therefore may potentially be unable to decrypt", "tokens": [293, 4412, 291, 815, 7263, 3624, 1808, 9317, 11, 293, 4412, 815, 7263, 312, 11299, 281, 979, 627, 662], "temperature": 0.0, "avg_logprob": -0.14555906987452244, "compression_ratio": 1.71875, "no_speech_prob": 0.0002751218562480062}, {"id": 611, "seek": 353844, "start": 3542.76, "end": 3550.68, "text": " messages. Hopefully that's clear. Do you have any data on whether sliding sync significantly", "tokens": [7897, 13, 10429, 300, 311, 1850, 13, 1144, 291, 362, 604, 1412, 322, 1968, 21169, 20271, 10591], "temperature": 0.0, "avg_logprob": -0.14555906987452244, "compression_ratio": 1.71875, "no_speech_prob": 0.0002751218562480062}, {"id": 612, "seek": 353844, "start": 3550.68, "end": 3556.36, "text": " impacts the server load? So, the question is, what about server load on sliding sync? Do we have", "tokens": [11606, 264, 7154, 3677, 30, 407, 11, 264, 1168, 307, 11, 437, 466, 7154, 3677, 322, 21169, 20271, 30, 1144, 321, 362], "temperature": 0.0, "avg_logprob": -0.14555906987452244, "compression_ratio": 1.71875, "no_speech_prob": 0.0002751218562480062}, {"id": 613, "seek": 353844, "start": 3556.36, "end": 3562.6, "text": " any data? I need clarification, because do you mean at a proxy level, or do you mean in like a", "tokens": [604, 1412, 30, 286, 643, 34449, 11, 570, 360, 291, 914, 412, 257, 29690, 1496, 11, 420, 360, 291, 914, 294, 411, 257], "temperature": 0.0, "avg_logprob": -0.14555906987452244, "compression_ratio": 1.71875, "no_speech_prob": 0.0002751218562480062}, {"id": 614, "seek": 356260, "start": 3562.6, "end": 3569.0, "text": " general sense for native implementations of the server? Does using sliding sync improve", "tokens": [2674, 2020, 337, 8470, 4445, 763, 295, 264, 7154, 30, 4402, 1228, 21169, 20271, 3470], "temperature": 0.0, "avg_logprob": -0.12674194032495673, "compression_ratio": 1.6563876651982379, "no_speech_prob": 6.795670924475417e-05}, {"id": 615, "seek": 356260, "start": 3569.0, "end": 3575.0, "text": " server performance? A native implementation, yes, it would. So, that's one of the reasons why the", "tokens": [7154, 3389, 30, 316, 8470, 11420, 11, 2086, 11, 309, 576, 13, 407, 11, 300, 311, 472, 295, 264, 4112, 983, 264], "temperature": 0.0, "avg_logprob": -0.12674194032495673, "compression_ratio": 1.6563876651982379, "no_speech_prob": 6.795670924475417e-05}, {"id": 616, "seek": 356260, "start": 3575.0, "end": 3579.16, "text": " existing sync implementation is slow, is just because the servers have to do an awful lot of", "tokens": [6741, 20271, 11420, 307, 2964, 11, 307, 445, 570, 264, 15909, 362, 281, 360, 364, 11232, 688, 295], "temperature": 0.0, "avg_logprob": -0.12674194032495673, "compression_ratio": 1.6563876651982379, "no_speech_prob": 6.795670924475417e-05}, {"id": 617, "seek": 356260, "start": 3579.16, "end": 3584.52, "text": " work. And obviously, I've been developing on dendrite, I know exactly what things are slow there.", "tokens": [589, 13, 400, 2745, 11, 286, 600, 668, 6416, 322, 274, 521, 35002, 11, 286, 458, 2293, 437, 721, 366, 2964, 456, 13], "temperature": 0.0, "avg_logprob": -0.12674194032495673, "compression_ratio": 1.6563876651982379, "no_speech_prob": 6.795670924475417e-05}, {"id": 618, "seek": 358452, "start": 3584.52, "end": 3592.7599999999998, "text": " So, a lot of the API that's exposed to the clients are basically efficient ways that you", "tokens": [407, 11, 257, 688, 295, 264, 9362, 300, 311, 9495, 281, 264, 6982, 366, 1936, 7148, 2098, 300, 291], "temperature": 0.0, "avg_logprob": -0.09322659308169068, "compression_ratio": 1.698581560283688, "no_speech_prob": 2.37434414884774e-05}, {"id": 619, "seek": 358452, "start": 3592.7599999999998, "end": 3597.08, "text": " can do it. So, you only get like the current state of rooms, you don't tend to need to go back in", "tokens": [393, 360, 309, 13, 407, 11, 291, 787, 483, 411, 264, 2190, 1785, 295, 9396, 11, 291, 500, 380, 3928, 281, 643, 281, 352, 646, 294], "temperature": 0.0, "avg_logprob": -0.09322659308169068, "compression_ratio": 1.698581560283688, "no_speech_prob": 2.37434414884774e-05}, {"id": 620, "seek": 358452, "start": 3597.08, "end": 3600.7599999999998, "text": " time, you don't need to remember all your synth tokens since the beginning of time. These are", "tokens": [565, 11, 291, 500, 380, 643, 281, 1604, 439, 428, 10657, 22667, 1670, 264, 2863, 295, 565, 13, 1981, 366], "temperature": 0.0, "avg_logprob": -0.09322659308169068, "compression_ratio": 1.698581560283688, "no_speech_prob": 2.37434414884774e-05}, {"id": 621, "seek": 358452, "start": 3600.7599999999998, "end": 3605.64, "text": " things that slow down the processing. So, yes, a native implementation, but a proxy implementation", "tokens": [721, 300, 2964, 760, 264, 9007, 13, 407, 11, 2086, 11, 257, 8470, 11420, 11, 457, 257, 29690, 11420], "temperature": 0.0, "avg_logprob": -0.09322659308169068, "compression_ratio": 1.698581560283688, "no_speech_prob": 2.37434414884774e-05}, {"id": 622, "seek": 358452, "start": 3605.64, "end": 3611.16, "text": " obviously is a sync loop that's going to be made, so that will increase load, right? Because that's", "tokens": [2745, 307, 257, 20271, 6367, 300, 311, 516, 281, 312, 1027, 11, 370, 300, 486, 3488, 3677, 11, 558, 30, 1436, 300, 311], "temperature": 0.0, "avg_logprob": -0.09322659308169068, "compression_ratio": 1.698581560283688, "no_speech_prob": 2.37434414884774e-05}, {"id": 623, "seek": 361116, "start": 3611.16, "end": 3617.24, "text": " going to be constantly syncing on your account.", "tokens": [516, 281, 312, 6460, 5451, 2175, 322, 428, 2696, 13], "temperature": 0.0, "avg_logprob": -0.21938353997689705, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.000236667794524692}, {"id": 624, "seek": 361116, "start": 3628.3599999999997, "end": 3633.16, "text": " Okay, so that's an element X question, I guess. Wait, let me repeat it first. So,", "tokens": [1033, 11, 370, 300, 311, 364, 4478, 1783, 1168, 11, 286, 2041, 13, 3802, 11, 718, 385, 7149, 309, 700, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.21938353997689705, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.000236667794524692}, {"id": 625, "seek": 361116, "start": 3633.16, "end": 3635.7999999999997, "text": " the question is about multi-user account support in the app.", "tokens": [264, 1168, 307, 466, 4825, 12, 18088, 2696, 1406, 294, 264, 724, 13], "temperature": 0.0, "avg_logprob": -0.21938353997689705, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.000236667794524692}, {"id": 626, "seek": 363580, "start": 3635.8, "end": 3643.32, "text": " It's something that we're discussing, but for now, there is no definite plan, let's say.", "tokens": [467, 311, 746, 300, 321, 434, 10850, 11, 457, 337, 586, 11, 456, 307, 572, 25131, 1393, 11, 718, 311, 584, 13], "temperature": 0.0, "avg_logprob": -0.2040828647035541, "compression_ratio": 1.3935483870967742, "no_speech_prob": 0.0006687869317829609}, {"id": 627, "seek": 363580, "start": 3649.0800000000004, "end": 3653.7200000000003, "text": " From the metrics SDK side, I can tell you that you can do it. That's not an issue.", "tokens": [3358, 264, 16367, 37135, 1252, 11, 286, 393, 980, 291, 300, 291, 393, 360, 309, 13, 663, 311, 406, 364, 2734, 13], "temperature": 0.0, "avg_logprob": -0.2040828647035541, "compression_ratio": 1.3935483870967742, "no_speech_prob": 0.0006687869317829609}, {"id": 628, "seek": 365372, "start": 3653.72, "end": 3671.72, "text": " So, I think you were next. Saw you. So, two part question. One is how far out do you think Sliding Sync is from actually being like merged and finalized as a spec? And then second part to that is are there plans to do a native implementation for those APIs in Synapse?", "tokens": [407, 11, 286, 519, 291, 645, 958, 13, 27307, 291, 13, 407, 11, 732, 644, 1168, 13, 1485, 307, 577, 1400, 484, 360, 291, 519, 6187, 2819, 26155, 66, 307, 490, 767, 885, 411, 36427, 293, 2572, 1602, 382, 257, 1608, 30, 400, 550, 1150, 644, 281, 300, 307, 366, 456, 5482, 281, 360, 257, 8470, 11420, 337, 729, 21445, 294, 26155, 11145, 30], "temperature": 0.0, "avg_logprob": -0.2504466000725241, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.00019050922128371894}, {"id": 629, "seek": 365372, "start": 3675.16, "end": 3680.8399999999997, "text": " Yes, so the question is basically how long it's going to take for Sliding Sync to land and will we get native implementations in Synapse?", "tokens": [1079, 11, 370, 264, 1168, 307, 1936, 577, 938, 309, 311, 516, 281, 747, 337, 6187, 2819, 26155, 66, 281, 2117, 293, 486, 321, 483, 8470, 4445, 763, 294, 26155, 11145, 30], "temperature": 0.0, "avg_logprob": -0.2504466000725241, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.00019050922128371894}, {"id": 630, "seek": 368084, "start": 3680.84, "end": 3708.84, "text": " You will get a native implementation in Synapse, I don't know when. And yes, we're going to try to merge and land it as soon as this is practically possible, which, you know, there's still things we need to fix, right? Like things like threading and stuff just doesn't work. And that's actually one of the biggest blockers at the moment from us trying out just defaulting element web to Sliding Sync on by default is that for compatible service, obviously, is the fact that we don't have threading support, so you wouldn't have feature parity.", "tokens": [509, 486, 483, 257, 8470, 11420, 294, 26155, 11145, 11, 286, 500, 380, 458, 562, 13, 400, 2086, 11, 321, 434, 516, 281, 853, 281, 22183, 293, 2117, 309, 382, 2321, 382, 341, 307, 15667, 1944, 11, 597, 11, 291, 458, 11, 456, 311, 920, 721, 321, 643, 281, 3191, 11, 558, 30, 1743, 721, 411, 7207, 278, 293, 1507, 445, 1177, 380, 589, 13, 400, 300, 311, 767, 472, 295, 264, 3880, 3461, 433, 412, 264, 1623, 490, 505, 1382, 484, 445, 7576, 278, 4478, 3670, 281, 6187, 2819, 26155, 66, 322, 538, 7576, 307, 300, 337, 18218, 2643, 11, 2745, 11, 307, 264, 1186, 300, 321, 500, 380, 362, 7207, 278, 1406, 11, 370, 291, 2759, 380, 362, 4111, 44747, 13], "temperature": 0.0, "avg_logprob": -0.17254742299477885, "compression_ratio": 1.665644171779141, "no_speech_prob": 0.00013737518747802824}, {"id": 631, "seek": 370884, "start": 3708.84, "end": 3722.84, "text": " So, when we do a feature parity, then, you know, there could be element web clients which enable it by default. It won't be in labs, it will be enabled in labs by default. So, you know, we're getting there, but I can't give you a time, unfortunately.", "tokens": [407, 11, 562, 321, 360, 257, 4111, 44747, 11, 550, 11, 291, 458, 11, 456, 727, 312, 4478, 3670, 6982, 597, 9528, 309, 538, 7576, 13, 467, 1582, 380, 312, 294, 20339, 11, 309, 486, 312, 15172, 294, 20339, 538, 7576, 13, 407, 11, 291, 458, 11, 321, 434, 1242, 456, 11, 457, 286, 393, 380, 976, 291, 257, 565, 11, 7015, 13], "temperature": 0.0, "avg_logprob": -0.1629602802333547, "compression_ratio": 1.5527950310559007, "no_speech_prob": 0.0002967074979096651}, {"id": 632, "seek": 372284, "start": 3722.84, "end": 3738.84, "text": " Thank you, next.", "tokens": [1044, 291, 11, 958, 13], "temperature": 0.0, "avg_logprob": -0.36838176515367294, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.00021655841555912048}, {"id": 633, "seek": 373884, "start": 3738.84, "end": 3766.84, "text": " So, the question is the authentication parts in the REST SDK. So, yes, we have login via username and password. We have implemented OIDC in general, but I don't think it's fully tested. And we have an SSO feature as well. So, we ask the server, the specification test, right? The server tells us what is possible, and then we allow you to use those.", "tokens": [407, 11, 264, 1168, 307, 264, 26643, 3166, 294, 264, 497, 14497, 37135, 13, 407, 11, 2086, 11, 321, 362, 24276, 5766, 30351, 293, 11524, 13, 492, 362, 12270, 422, 2777, 34, 294, 2674, 11, 457, 286, 500, 380, 519, 309, 311, 4498, 8246, 13, 400, 321, 362, 364, 12238, 46, 4111, 382, 731, 13, 407, 11, 321, 1029, 264, 7154, 11, 264, 31256, 1500, 11, 558, 30, 440, 7154, 5112, 505, 437, 307, 1944, 11, 293, 550, 321, 2089, 291, 281, 764, 729, 13], "temperature": 0.0, "avg_logprob": -0.17021953925657807, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.00018476051627658308}, {"id": 634, "seek": 376684, "start": 3766.84, "end": 3772.84, "text": " So, generally, yeah, if your server is SSO, you can use metrics SDK with it. Jan, here.", "tokens": [407, 11, 5101, 11, 1338, 11, 498, 428, 7154, 307, 12238, 46, 11, 291, 393, 764, 16367, 37135, 365, 309, 13, 4956, 11, 510, 13], "temperature": 0.0, "avg_logprob": -0.24179343654684823, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.943807577248663e-05}, {"id": 635, "seek": 376684, "start": 3772.84, "end": 3774.84, "text": " Question from the Internet.", "tokens": [14464, 490, 264, 7703, 13], "temperature": 0.0, "avg_logprob": -0.24179343654684823, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.943807577248663e-05}, {"id": 636, "seek": 376684, "start": 3774.84, "end": 3776.84, "text": " Ooh, a question from the Internet. I heard about them.", "tokens": [7951, 11, 257, 1168, 490, 264, 7703, 13, 286, 2198, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.24179343654684823, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.943807577248663e-05}, {"id": 637, "seek": 376684, "start": 3776.84, "end": 3782.84, "text": " Are there any plans or what is the status of the matrix RTC in the REST SDK?", "tokens": [2014, 456, 604, 5482, 420, 437, 307, 264, 6558, 295, 264, 8141, 497, 18238, 294, 264, 497, 14497, 37135, 30], "temperature": 0.0, "avg_logprob": -0.24179343654684823, "compression_ratio": 1.4529411764705882, "no_speech_prob": 7.943807577248663e-05}, {"id": 638, "seek": 378284, "start": 3782.84, "end": 3798.84, "text": " Yeah, the question is about RTC in the REST SDK. If you followed the RTC talk before, you noticed that most of the RTC part of the RTC is actually offloaded to web RTC in the current implementation. So, going through a web view.", "tokens": [865, 11, 264, 1168, 307, 466, 497, 18238, 294, 264, 497, 14497, 37135, 13, 759, 291, 6263, 264, 497, 18238, 751, 949, 11, 291, 5694, 300, 881, 295, 264, 497, 18238, 644, 295, 264, 497, 18238, 307, 767, 766, 2907, 292, 281, 3670, 497, 18238, 294, 264, 2190, 11420, 13, 407, 11, 516, 807, 257, 3670, 1910, 13], "temperature": 0.0, "avg_logprob": -0.12881070567715552, "compression_ratio": 1.4522292993630572, "no_speech_prob": 3.1180661608232185e-05}, {"id": 639, "seek": 379884, "start": 3798.84, "end": 3820.84, "text": " For us, as REST, that means we don't have to bother about most of that. There's only some signaling that happens on the actual matrix protocol. So, we don't have at the moment the plan to implement an actual RTC our side. I wouldn't see where you would want to do that for other than that view.", "tokens": [1171, 505, 11, 382, 497, 14497, 11, 300, 1355, 321, 500, 380, 362, 281, 8677, 466, 881, 295, 300, 13, 821, 311, 787, 512, 38639, 300, 2314, 322, 264, 3539, 8141, 10336, 13, 407, 11, 321, 500, 380, 362, 412, 264, 1623, 264, 1393, 281, 4445, 364, 3539, 497, 18238, 527, 1252, 13, 286, 2759, 380, 536, 689, 291, 576, 528, 281, 360, 300, 337, 661, 813, 300, 1910, 13], "temperature": 0.0, "avg_logprob": -0.14853742045740928, "compression_ratio": 1.6527777777777777, "no_speech_prob": 8.468485611956567e-05}, {"id": 640, "seek": 379884, "start": 3820.84, "end": 3826.84, "text": " So, currently, it's not on the roadmap, at least for our side.", "tokens": [407, 11, 4362, 11, 309, 311, 406, 322, 264, 35738, 11, 412, 1935, 337, 527, 1252, 13], "temperature": 0.0, "avg_logprob": -0.14853742045740928, "compression_ratio": 1.6527777777777777, "no_speech_prob": 8.468485611956567e-05}, {"id": 641, "seek": 382684, "start": 3826.84, "end": 3836.84, "text": " Let me talk about IoT.", "tokens": [961, 385, 751, 466, 30112, 13], "temperature": 0.0, "avg_logprob": -0.27957908312479657, "compression_ratio": 1.2377049180327868, "no_speech_prob": 8.869505836628377e-05}, {"id": 642, "seek": 382684, "start": 3836.84, "end": 3848.84, "text": " Yeah. So, that's a common one as REST is very, so the question is about IoT devices. Could you do that with REST? I see you can.", "tokens": [865, 13, 407, 11, 300, 311, 257, 2689, 472, 382, 497, 14497, 307, 588, 11, 370, 264, 1168, 307, 466, 30112, 5759, 13, 7497, 291, 360, 300, 365, 497, 14497, 30, 286, 536, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.27957908312479657, "compression_ratio": 1.2377049180327868, "no_speech_prob": 8.869505836628377e-05}, {"id": 643, "seek": 384884, "start": 3848.84, "end": 3860.84, "text": " Yes. That is generally possible. We have, because of the storage systems and some other things in there, and because matrix itself is still quite heavy as an overall protocol.", "tokens": [1079, 13, 663, 307, 5101, 1944, 13, 492, 362, 11, 570, 295, 264, 6725, 3652, 293, 512, 661, 721, 294, 456, 11, 293, 570, 8141, 2564, 307, 920, 1596, 4676, 382, 364, 4787, 10336, 13], "temperature": 0.0, "avg_logprob": -0.06937348577711318, "compression_ratio": 1.6563876651982379, "no_speech_prob": 2.9755077775917016e-05}, {"id": 644, "seek": 384884, "start": 3860.84, "end": 3871.84, "text": " We have tried to get it into an actual embedded device. That is not at the moment possible. We would have to improve a lot on the way that we use REST. REST itself provides that, but we can't do that.", "tokens": [492, 362, 3031, 281, 483, 309, 666, 364, 3539, 16741, 4302, 13, 663, 307, 406, 412, 264, 1623, 1944, 13, 492, 576, 362, 281, 3470, 257, 688, 322, 264, 636, 300, 321, 764, 497, 14497, 13, 497, 14497, 2564, 6417, 300, 11, 457, 321, 393, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.06937348577711318, "compression_ratio": 1.6563876651982379, "no_speech_prob": 2.9755077775917016e-05}, {"id": 645, "seek": 387184, "start": 3871.84, "end": 3884.84, "text": " But you can use it, for example, on an Android, not an Androidino, but a Raspberry Pi. We know of people that run Raspberry Pis that have signals coming in, and then they use the REST SDK to send it over into rooms.", "tokens": [583, 291, 393, 764, 309, 11, 337, 1365, 11, 322, 364, 8853, 11, 406, 364, 8853, 2982, 11, 457, 257, 41154, 17741, 13, 492, 458, 295, 561, 300, 1190, 41154, 43263, 300, 362, 12354, 1348, 294, 11, 293, 550, 436, 764, 264, 497, 14497, 37135, 281, 2845, 309, 670, 666, 9396, 13], "temperature": 0.0, "avg_logprob": -0.1122341741595352, "compression_ratio": 1.6596491228070176, "no_speech_prob": 3.966171789215878e-06}, {"id": 646, "seek": 387184, "start": 3884.84, "end": 3899.84, "text": " That is definitely possible, because it's more or less just a bot. From our perspective, it's just a bot. So, that is possible, but you still need a significant amount of memory at the moment, and that would make it not possible for actual embedded devices.", "tokens": [663, 307, 2138, 1944, 11, 570, 309, 311, 544, 420, 1570, 445, 257, 10592, 13, 3358, 527, 4585, 11, 309, 311, 445, 257, 10592, 13, 407, 11, 300, 307, 1944, 11, 457, 291, 920, 643, 257, 4776, 2372, 295, 4675, 412, 264, 1623, 11, 293, 300, 576, 652, 309, 406, 1944, 337, 3539, 16741, 5759, 13], "temperature": 0.0, "avg_logprob": -0.1122341741595352, "compression_ratio": 1.6596491228070176, "no_speech_prob": 3.966171789215878e-06}, {"id": 647, "seek": 389984, "start": 3899.84, "end": 3908.84, "text": " Yet, if anybody wants to do that, come to me. I can show you and mentor you and help you, because it would be very exciting if we had the possibility to do that.", "tokens": [10890, 11, 498, 4472, 2738, 281, 360, 300, 11, 808, 281, 385, 13, 286, 393, 855, 291, 293, 14478, 291, 293, 854, 291, 11, 570, 309, 576, 312, 588, 4670, 498, 321, 632, 264, 7959, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15311784362792968, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.00017098757962230593}, {"id": 648, "seek": 389984, "start": 3908.84, "end": 3910.84, "text": " Jan, another question from the internet?", "tokens": [4956, 11, 1071, 1168, 490, 264, 4705, 30], "temperature": 0.0, "avg_logprob": -0.15311784362792968, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.00017098757962230593}, {"id": 649, "seek": 389984, "start": 3910.84, "end": 3921.84, "text": " There's also a question about the element X. What you see is what you get, editor. Is it still possible to use just markdown if you want to just use knockdown?", "tokens": [821, 311, 611, 257, 1168, 466, 264, 4478, 1783, 13, 708, 291, 536, 307, 437, 291, 483, 11, 9839, 13, 1119, 309, 920, 1944, 281, 764, 445, 1491, 5093, 498, 291, 528, 281, 445, 764, 6728, 5093, 30], "temperature": 0.0, "avg_logprob": -0.15311784362792968, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.00017098757962230593}, {"id": 650, "seek": 389984, "start": 3921.84, "end": 3928.84, "text": " So, the question is about the element X. Where's your big editor? Can you still use markdown if you want to use markdown?", "tokens": [407, 11, 264, 1168, 307, 466, 264, 4478, 1783, 13, 2305, 311, 428, 955, 9839, 30, 1664, 291, 920, 764, 1491, 5093, 498, 291, 528, 281, 764, 1491, 5093, 30], "temperature": 0.0, "avg_logprob": -0.15311784362792968, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.00017098757962230593}, {"id": 651, "seek": 392884, "start": 3928.84, "end": 3935.84, "text": " Actually, even on the current element implementation that is on the client, you can actually also still use markdown.", "tokens": [5135, 11, 754, 322, 264, 2190, 4478, 11420, 300, 307, 322, 264, 6423, 11, 291, 393, 767, 611, 920, 764, 1491, 5093, 13], "temperature": 0.0, "avg_logprob": -0.1811894487451624, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.0009617492323741317}, {"id": 652, "seek": 392884, "start": 3935.84, "end": 3946.84, "text": " So, there's an option that allows you to turn off the rich text and turn back the simple text, and when the simple text is on, pretty much you can use markdowns.", "tokens": [407, 11, 456, 311, 364, 3614, 300, 4045, 291, 281, 1261, 766, 264, 4593, 2487, 293, 1261, 646, 264, 2199, 2487, 11, 293, 562, 264, 2199, 2487, 307, 322, 11, 1238, 709, 291, 393, 764, 1491, 5093, 82, 13], "temperature": 0.0, "avg_logprob": -0.1811894487451624, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.0009617492323741317}, {"id": 653, "seek": 392884, "start": 3946.84, "end": 3951.84, "text": " But, will it render in what you see, what you get, fashion?", "tokens": [583, 11, 486, 309, 15529, 294, 437, 291, 536, 11, 437, 291, 483, 11, 6700, 30], "temperature": 0.0, "avg_logprob": -0.1811894487451624, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.0009617492323741317}, {"id": 654, "seek": 392884, "start": 3951.84, "end": 3955.84, "text": " So, the question is about does the markdown then render in the WYSIWYG?", "tokens": [407, 11, 264, 1168, 307, 466, 775, 264, 1491, 5093, 550, 15529, 294, 264, 46410, 20262, 54, 56, 38, 30], "temperature": 0.0, "avg_logprob": -0.1811894487451624, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.0009617492323741317}, {"id": 655, "seek": 395584, "start": 3955.84, "end": 3961.84, "text": " No, when you're using the simple text version, it's rendering pretty much like a simple text with the markdowns.", "tokens": [883, 11, 562, 291, 434, 1228, 264, 2199, 2487, 3037, 11, 309, 311, 22407, 1238, 709, 411, 257, 2199, 2487, 365, 264, 1491, 5093, 82, 13], "temperature": 0.0, "avg_logprob": -0.16247123529103177, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0018021782161667943}, {"id": 656, "seek": 395584, "start": 3961.84, "end": 3963.84, "text": " So, any plans?", "tokens": [407, 11, 604, 5482, 30], "temperature": 0.0, "avg_logprob": -0.16247123529103177, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0018021782161667943}, {"id": 657, "seek": 395584, "start": 3963.84, "end": 3967.84, "text": " Or, naming it in the rich text without the markdowns?", "tokens": [1610, 11, 25290, 309, 294, 264, 4593, 2487, 1553, 264, 1491, 5093, 82, 30], "temperature": 0.0, "avg_logprob": -0.16247123529103177, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0018021782161667943}, {"id": 658, "seek": 395584, "start": 3967.84, "end": 3977.84, "text": " Currently not. We're pretty much trying to build the rich text editor, as it is with just the rich text using the formatting toolbar to be the most performant and good and simple to it as possible.", "tokens": [19964, 406, 13, 492, 434, 1238, 709, 1382, 281, 1322, 264, 4593, 2487, 9839, 11, 382, 309, 307, 365, 445, 264, 4593, 2487, 1228, 264, 39366, 47715, 281, 312, 264, 881, 2042, 394, 293, 665, 293, 2199, 281, 309, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.16247123529103177, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0018021782161667943}, {"id": 659, "seek": 395584, "start": 3977.84, "end": 3984.84, "text": " But, it is something that for sure, when we have a very stable product, we will look into.", "tokens": [583, 11, 309, 307, 746, 300, 337, 988, 11, 562, 321, 362, 257, 588, 8351, 1674, 11, 321, 486, 574, 666, 13], "temperature": 0.0, "avg_logprob": -0.16247123529103177, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0018021782161667943}, {"id": 660, "seek": 398484, "start": 3984.84, "end": 3986.84, "text": " No question.", "tokens": [883, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1745913028717041, "compression_ratio": 1.91324200913242, "no_speech_prob": 0.0007842892082408071}, {"id": 661, "seek": 398484, "start": 3986.84, "end": 3991.84, "text": " Will this finally unite the markdown syntaxes that you can use in different element clients?", "tokens": [3099, 341, 2721, 29320, 264, 1491, 5093, 28431, 279, 300, 291, 393, 764, 294, 819, 4478, 6982, 30], "temperature": 0.0, "avg_logprob": -0.1745913028717041, "compression_ratio": 1.91324200913242, "no_speech_prob": 0.0007842892082408071}, {"id": 662, "seek": 398484, "start": 3991.84, "end": 3999.84, "text": " Will that finally reduce the amount of different markdown syntaxes that you can use in element clients?", "tokens": [3099, 300, 2721, 5407, 264, 2372, 295, 819, 1491, 5093, 28431, 279, 300, 291, 393, 764, 294, 4478, 6982, 30], "temperature": 0.0, "avg_logprob": -0.1745913028717041, "compression_ratio": 1.91324200913242, "no_speech_prob": 0.0007842892082408071}, {"id": 663, "seek": 398484, "start": 3999.84, "end": 4001.84, "text": " I'm not sure about the question actually.", "tokens": [286, 478, 406, 988, 466, 264, 1168, 767, 13], "temperature": 0.0, "avg_logprob": -0.1745913028717041, "compression_ratio": 1.91324200913242, "no_speech_prob": 0.0007842892082408071}, {"id": 664, "seek": 398484, "start": 4001.84, "end": 4013.84, "text": " Will the WYSIWYG editor in simple text mode use one unified markdown implementation so you don't have to remember different variants of markdown and different clients?", "tokens": [3099, 264, 46410, 20262, 54, 56, 38, 9839, 294, 2199, 2487, 4391, 764, 472, 26787, 1491, 5093, 11420, 370, 291, 500, 380, 362, 281, 1604, 819, 21669, 295, 1491, 5093, 293, 819, 6982, 30], "temperature": 0.0, "avg_logprob": -0.1745913028717041, "compression_ratio": 1.91324200913242, "no_speech_prob": 0.0007842892082408071}, {"id": 665, "seek": 401384, "start": 4013.84, "end": 4022.84, "text": " But you're talking if we are going in the future to support the markdowns inside the WYSIWYG directly without turning off the rich text?", "tokens": [583, 291, 434, 1417, 498, 321, 366, 516, 294, 264, 2027, 281, 1406, 264, 1491, 5093, 82, 1854, 264, 46410, 20262, 54, 56, 38, 3838, 1553, 6246, 766, 264, 4593, 2487, 30], "temperature": 0.0, "avg_logprob": -0.12853277509457597, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.0006261521484702826}, {"id": 666, "seek": 401384, "start": 4022.84, "end": 4023.84, "text": " This is what you mean?", "tokens": [639, 307, 437, 291, 914, 30], "temperature": 0.0, "avg_logprob": -0.12853277509457597, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.0006261521484702826}, {"id": 667, "seek": 401384, "start": 4023.84, "end": 4033.84, "text": " In simple text mode, if you enter markdown, we would parse the same way on element iris, android and web.", "tokens": [682, 2199, 2487, 4391, 11, 498, 291, 3242, 1491, 5093, 11, 321, 576, 48377, 264, 912, 636, 322, 4478, 3418, 271, 11, 36157, 293, 3670, 13], "temperature": 0.0, "avg_logprob": -0.12853277509457597, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.0006261521484702826}, {"id": 668, "seek": 401384, "start": 4033.84, "end": 4035.84, "text": " So, I think there's a confusion here.", "tokens": [407, 11, 286, 519, 456, 311, 257, 15075, 510, 13], "temperature": 0.0, "avg_logprob": -0.12853277509457597, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.0006261521484702826}, {"id": 669, "seek": 401384, "start": 4035.84, "end": 4039.84, "text": " You switch on the WYSIWYG editor, then you get the WYSIWYG.", "tokens": [509, 3679, 322, 264, 46410, 20262, 54, 56, 38, 9839, 11, 550, 291, 483, 264, 46410, 20262, 54, 56, 38, 13], "temperature": 0.0, "avg_logprob": -0.12853277509457597, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.0006261521484702826}, {"id": 670, "seek": 403984, "start": 4039.84, "end": 4046.84, "text": " If you turn it off, you have a simple text mode that you can do some markdown, but it's not going to be rendered inside of this.", "tokens": [759, 291, 1261, 309, 766, 11, 291, 362, 257, 2199, 2487, 4391, 300, 291, 393, 360, 512, 1491, 5093, 11, 457, 309, 311, 406, 516, 281, 312, 28748, 1854, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1776859543540261, "compression_ratio": 1.7277486910994764, "no_speech_prob": 0.00012049867655150592}, {"id": 671, "seek": 403984, "start": 4046.84, "end": 4051.84, "text": " So, it's just going to fall back to the existing implementation.", "tokens": [407, 11, 309, 311, 445, 516, 281, 2100, 646, 281, 264, 6741, 11420, 13], "temperature": 0.0, "avg_logprob": -0.1776859543540261, "compression_ratio": 1.7277486910994764, "no_speech_prob": 0.00012049867655150592}, {"id": 672, "seek": 403984, "start": 4051.84, "end": 4055.84, "text": " So, therefore, yeah, to answer your question, it's falling back to the existing implementation.", "tokens": [407, 11, 4412, 11, 1338, 11, 281, 1867, 428, 1168, 11, 309, 311, 7440, 646, 281, 264, 6741, 11420, 13], "temperature": 0.0, "avg_logprob": -0.1776859543540261, "compression_ratio": 1.7277486910994764, "no_speech_prob": 0.00012049867655150592}, {"id": 673, "seek": 403984, "start": 4055.84, "end": 4057.84, "text": " So, no, they will still be incompatible.", "tokens": [407, 11, 572, 11, 436, 486, 920, 312, 40393, 267, 964, 13], "temperature": 0.0, "avg_logprob": -0.1776859543540261, "compression_ratio": 1.7277486910994764, "no_speech_prob": 0.00012049867655150592}, {"id": 674, "seek": 405784, "start": 4057.84, "end": 4070.84, "text": " So, that we might switch to use the markdown for round-tripping, because at some point, I think this was the previous question, that people are going to want to round-trip between the markdown implementation and the WYSIWYG one.", "tokens": [407, 11, 300, 321, 1062, 3679, 281, 764, 264, 1491, 5093, 337, 3098, 12, 83, 470, 3759, 11, 570, 412, 512, 935, 11, 286, 519, 341, 390, 264, 3894, 1168, 11, 300, 561, 366, 516, 281, 528, 281, 3098, 12, 83, 8400, 1296, 264, 1491, 5093, 11420, 293, 264, 46410, 20262, 54, 56, 38, 472, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 675, "seek": 405784, "start": 4070.84, "end": 4073.84, "text": " And to do that consistently, you're going to want to use the same library.", "tokens": [400, 281, 360, 300, 14961, 11, 291, 434, 516, 281, 528, 281, 764, 264, 912, 6405, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 676, "seek": 405784, "start": 4073.84, "end": 4074.84, "text": " You put that in the Rust layer.", "tokens": [509, 829, 300, 294, 264, 34952, 4583, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 677, "seek": 405784, "start": 4074.84, "end": 4082.84, "text": " And finally, we get out to the nightmare of Common Mark versus GitHub, Flavint, markdown versus whatever random library the different element platforms have.", "tokens": [400, 2721, 11, 321, 483, 484, 281, 264, 18724, 295, 18235, 3934, 5717, 23331, 11, 3235, 706, 686, 11, 1491, 5093, 5717, 2035, 4974, 6405, 264, 819, 4478, 9473, 362, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 678, "seek": 405784, "start": 4082.84, "end": 4084.84, "text": " I think Android is still out of sync with the others.", "tokens": [286, 519, 8853, 307, 920, 484, 295, 20271, 365, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 679, "seek": 405784, "start": 4084.84, "end": 4085.84, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.18297198159354075, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.00017794256564229727}, {"id": 680, "seek": 408584, "start": 4085.84, "end": 4087.84, "text": " One last question.", "tokens": [1485, 1036, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 681, "seek": 408584, "start": 4087.84, "end": 4091.84, "text": " Where can we meet you today, or maybe later, if we have more questions?", "tokens": [2305, 393, 321, 1677, 291, 965, 11, 420, 1310, 1780, 11, 498, 321, 362, 544, 1651, 30], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 682, "seek": 408584, "start": 4091.84, "end": 4093.84, "text": " I think we're going to hang around here, right?", "tokens": [286, 519, 321, 434, 516, 281, 3967, 926, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 683, "seek": 408584, "start": 4093.84, "end": 4094.84, "text": " Yeah, for sure.", "tokens": [865, 11, 337, 988, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 684, "seek": 408584, "start": 4094.84, "end": 4095.84, "text": " We'll be able to stand soon.", "tokens": [492, 603, 312, 1075, 281, 1463, 2321, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 685, "seek": 408584, "start": 4095.84, "end": 4097.84, "text": " We have to stand in K1.", "tokens": [492, 362, 281, 1463, 294, 591, 16, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 686, "seek": 408584, "start": 4097.84, "end": 4100.84, "text": " I'm just going to be around here, lurking, so just talk to me.", "tokens": [286, 478, 445, 516, 281, 312, 926, 510, 11, 35583, 5092, 11, 370, 445, 751, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 687, "seek": 408584, "start": 4100.84, "end": 4101.84, "text": " Yeah, same for me.", "tokens": [865, 11, 912, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 688, "seek": 408584, "start": 4101.84, "end": 4102.84, "text": " I'm going to be here.", "tokens": [286, 478, 516, 281, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 689, "seek": 408584, "start": 4102.84, "end": 4103.84, "text": " I'm the guy with that hat.", "tokens": [286, 478, 264, 2146, 365, 300, 2385, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 690, "seek": 408584, "start": 4103.84, "end": 4104.84, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 691, "seek": 408584, "start": 4104.84, "end": 4105.84, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 692, "seek": 408584, "start": 4105.84, "end": 4106.84, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1196311025908499, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0006146503146737814}, {"id": 693, "seek": 410684, "start": 4106.84, "end": 4115.84, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.45370642344156903, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0003263668331783265}], "language": "en"}