{"text": " Okay, this speaker claims that in 20 minutes, Robert is going to build an event-driven application. Well to be kind, I gave him 25 minutes. So start your countdown clocks. Hello. So my name is Robert, and yes, I would like to show you today that we can build an event-driven application in Go, and it can be as simple as building a simple HTTP server. And I actually decided to put the bar a bit higher. I think that I can do it within 15 minutes. All right, at the beginning, a couple of words about myself. So during the day, I work in a company named SlashID, so I work there as a principal engineer, and we are creating some identity and we're onboarding from a solution that is a bit more frictionless than a solution available now on the market. And during the night, I'm blogging at 3.0.tech blog, where we are writing some blog posts that are covering how to create Go applications that are business applications, but are also maintainable in the long term. I know maybe some of you had a chance to read at least one article there, there are some people. Nice. I will have something special for you later. You can find me on Twitter, GitHub, Mastodon, there's also my email if you would like to write to me and ask about something, but what's the most important for today? I'm Oselton of Watermill Library, and how everything started with Watermill, because I think that this is pretty important context. So a couple of years ago, I worked in a company where we are creating products that were not doing something super unusual, but the idea was that each user was able to add some content and he should be able to, we were storing it to MySQL, plus we wanted to have some more advanced search, plus have ability to create fit for other users with some magic machine learning models that they were doing personalization. And usually if you are building such kind of system in a synchronous way, there's one problem. So this part may be sometimes slow, because elastic search is under high load, or magic machine learning model so that this day it will not work. Not nice, but it happens in work, unfortunately. And yeah, or even worse, for example, some part is not working, and it's not best user experience if it's working slowly, or it's, so for example, you can imagine that you're adding some tweet and you're waiting for 10 seconds, because I know elastic search need to index something or machine learning model is working slowly, or even you are not able to add this content. And it doesn't make sense, because everything what is done on this other part of the diagram could be done asynchronously, because okay, it's not a problem if, for example, the search, some content that was added cannot be searched, for example, for one minute after it's added, if something is done. It's much better than not allowing people to add anything. So by the book, the default solution for such problems is using some kind of pop-up and doing it asynchronously. So in this case, we decided to use Kafka, because it's scalable, it's nice, but as usually with some concepts that you're reading in the books or listening on the conferences, it's not that simple in practice. And it was also the case here. The first problem was that the big part of the team wasn't actually working in asynchronous architectures earlier. That kind of makes sense, because if you're starting to learn to code, you're not starting with building some event-driven application, you're rather creating some REST API or website. So it makes sense that it was a big entry point for people that didn't use that. And it was not the only problem, because event-driven architecture has a lot of concepts that you need to know, like customer groups, partitioning, message ordering, at least one's delivery, acknowledge negative, acknowledge poison queue. And with all of that, you need to be sure that you didn't miss an event. And it's pretty important in some domains. In some cases, okay, it's fine, you're missing some event and okay. But for example, I used to work in the financial domain, and losing one event may, for example, mean that somebody will be not paid out. Not nice. In general, I believe that as engineers, we should be responsible, because sometimes the code that we are building has a really big impact to the real life. And after thinking for a while, I actually started to wonder, is it maybe something that I can do to making, to building some kind of applications in Go simpler? And here we are. This is how WaterMill was created. So far, we have more than 5,000 stars in the Github. We have more than 50 contributors across multiple WaterMill repositories. We are supporting 12 different PubSupp implementations, like Kafka, like Google Cloud PubSupp, like NATS JetStream, Rabbit and Q, but we have also some more strange implementations, like MySQL, for example, if you don't have infrastructure for some real PubSupp, or for example, would like to avoid to face commits problem. If you are doing some more fun projects, you can have just Go channel implementation or BoldDB, for example. But there is one more important thing than that, WaterMill has logo, and it is a logo with Go for Vomiting to Gobernati's logo, not as Muai. And you can think about WaterMill, like, so let's go back to this HTTP server example. So you can think about WaterMill, like something that makes your life simpler, like standard library for HTTP. So, for example, if you are implementing an HTTP server, you don't care about TLS, layers of network, you can start connection pooling and all this stuff, you are just implementing the logic in most cases. Sometimes, of course, you may have some specific scenarios that you care about that, but in most cases, you should just implement your handlers and don't care about everything around. And as you already, some of you shown, so I sometimes wrote the article that I think that frameworks are probably not working best in Go, and WaterMill is also, for example, that's the case why WaterMill is actually a library. And it's pretty good to upside. So the first one is that if you already have some system and you would like to migrate to WaterMill, it's kind of simple, because WaterMill doesn't add anything super custom and it can be integrated with any existing system, and vice versa. See, for example, for some reason, you decide that you don't like WaterMill, but you will not. So you can migrate from WaterMill to some different library. So this is the good thing. And I think what's pretty important, so how everything is done, because, okay, in theory it may sound nice, but it's helping, but how WaterMill is built. And in the heart of WaterMill, I would say that you can see in multiple places something that is named UNIX philosophy. And it's kind of old philosophy, because it's from 1978. And it's saying us to write programs that do one thing and do it well, write programs to work together, and write programs to handle, in our case, message. Because that is a universal interface. And some small question now. Do you know who's that? So it's Ken Thompson. So he's the author of this philosophy. And what's also interesting, he's one of the authors of Go programming language. Actually it makes sense, because if you look on the Go, for example, to IO Reader or our writer, this is pretty nicely visible there. And I know that for a lot of people didn't know about UNIX philosophy. And sometimes when I have too much time to think, I have some impression that, no, sometimes we forgot about some good old ideas and we're trying to reinvent the wheel, even if some problems were already solved. And you know, it's maybe something like in Dark Ages that it was some old nice ideas, but it was a bit forgotten. And OK, maybe I'm thinking too much. Let's go back to the watermill. So there are a couple important times in watermill. So the first one is message. So if you compare it to HTTP server, so it's something similar to HTTP request. So in message we have UID, that is pretty useful for debugging. We have metadata. So metadata is something like headers request plus payload. So this is the place where you are storing your event, for example. The two next important parts of watermill are publisher and subscriber. So publisher, you can publish those messages. And with subscriber, you're right. You can subscribe for those messages from the provided topic and receive that by the channel. You usually are not using these interfaces because it's used somewhere internally in watermill. But for example, if you would like to add a new implementation of PubSub, this is something that you're implementing. And each PubSub implementation is implementing this interface. That's why I actually pretty like this interface. Because it's making some constraint on the implementers that, OK, they need to implement that in that way. But it's also not good because it's making each of them pretty compatible with themselves. And the last but not least type is hender function. Hender function is something like HTTP handler that you are implementing in your HTTP server with the small difference that instead of receiving HTTP request, you are receiving a message. And optionally, you can receive the message. So the idea is that you can react on some message, do something, and emit some other messages so you can do some kind of changing later. I will show shortly an example. And everything is magically connected, sorry, it may be small, but you need to trust me that in the middle there is a router here. And this is connecting everything. So the message is going from some publisher, it doesn't need to be WaterMill, it's going to the queue by subscriber, the router. Router is passing it through middleware. Middleware works in WaterMill like HTTP, so another thing that is pretty similar. And it's processed by handlers. And later, if we want, we can publish some other messages. Not super complex. So do you know the first rule of live coding? Don't do live coding. So do live coding. What can go wrong? All right. Like to change sharing settings, so on second, it's probably not this one. This is why you are not doing live coding. Yes. Okay. So something does work, that's good, but I'm not really like, I want it. This is something that I wanted to have. So I prepared a simple application here. And what does application does? So if you're not from Brussels, so this may be something familiar to you. So it allows you to book a room in hotel. So you can provide room ID, pass guest counts, and let's see if it works. Okay. It seems that it's not working sometimes. Sometimes it's working. Sometimes it's not working. Sometimes it's working slowly, slowly, slowly, slowly. Sometimes it's even not working slowly. So it's even worse. So let's check the source code of that application. So okay. So here we are running HTTP, so boring, signals handling boring, but this is probably not boring. This is usually when the most interesting part of the application lives. Let's check our handler. So okay, so we are unmartialing stuff, to book room request, we have some advanced algorithm of calculation of room price, and we are taking payment. What can go wrong here? And okay, as we can see, our payment provider, it's not super stable, but okay, I don't know, let's imagine that it's our boss colleague and we cannot change that, no, politics. It happens. It's okay. What we can do? We can do like that, go, fang, okay, done, it works now, but it's one problem with that. So if our server will die, there is a chance that we'll not take payment, and it doesn't like that as the best idea. So what will be my idea? So instead of doing it synchronously with this HTTP handler, I would like to emit some event, listen to that event, and take payment asynchronously. So let's do that, and let's do that with watermill, of course. So at the beginning, we need to get rid of that, and we need to have our publisher here. Message publisher, so this is the interface that you should remember, all right. And I also can prepare some code snippets to not lose time on some boring stuff like room booked. Well, we have our event, so room booked, all right, guest count, and price, room, price. All right, now we need to marshal that, because we are sending bytes between our processes through our PAPS app, so JSON, because JSON is kind of common and it's pretty easy to debug. So let's marshal that, payload error, room booked. Don't do such error handling at home, please. And now let's publish that. The H publisher, publish topic, so let's use bookings, and we need our message. Let's remember we need to have UID, so it doesn't matter actually what format of UID it can be, I know, it can be even empty for some plantations, but good luck with debugging, and room booked payloads. All right, and it returns error, so we need to handle that in not a nice way, but it's live coding, so it's fine. All right, so we have the first part. So we have our room booked event, we're publishing that to the topic bookings, and, okay, so we just need to inject now the publisher. So let's check where it's created, okay, we no longer need payments. I heard that Kafka is nice and scalable, so let's use Kafka. I have also snippet for that, it's nothing magical here, it's just this and the water mid-documentation, and let's use this publisher. We don't need subscriber yet, but probably we'll need it later. All right, by the way, I'm running some nice Docker Compos under the hood that is recompiling the project each time when I'm putting changes there. At the end of the presentation, I will give you materials with all the source code, and with the description of how it's done, that it's automatically reloading after each change. All right, so we have our publisher, we are publishing our event, so let's check if it works. Hopefully it will work, okay, so you can see that our API is pretty stable, and let's check if our event is really published. So we'll use mule tool, so mule is part of water mule, as you can guess, and we'll consume from bookings from Kafka. Mule is allowing you to consume messages from multiple Pub-Sub types that are supported in water mule. I know that there is tool for that in Kafka, but it's not mine, so. And yeah, with mule, you can use multiple Pub-Sub types, and okay, as you can see, now we have event here, so it seems to work. Okay, so done, thank you. Not really. We are not taking payments, so probably if our company will go bankrupt pretty quickly, so we'll need to start to take payments. So for that, we already have our subscriber, that's good, so let's uncomment that, okay. We need to have water mule router, so message router error, router config, water mule logger, router handling, and now we need to add a handler. So we'll use addHander, so we'll need to provide handler name, so it will be payments. It doesn't matter really what is the handler name, but again, pretty useful for debugging. Subscribe topic. So we're subscribing to the topic that we published this message, so this is bookings. Bookings, we need to use subscriber, and we need to publish the topic. So we'll publish event when we succeed to take payments, so payments, publisher, and handler function. So hopefully you remember handler function signature, so yeah, we are receiving message and we are returning message, but we'll do it in a bit more fancy way, payments handler, because we can inject some dependencies earlier, I need to fix that, and that, all right. So we have our payments handler, so we'll receive message, and we'll take payment and emit some event. So we need to have our payment provider, and what? We need to have room booked, we need to have our shoulder, so message payload to room booked. And compared to standard library HTTP handler, you can return errors from a water new handler, so I don't need to panic. And all right, so we should have the payload that we published here, so that's good, so we can now use that to take payment for room booked price, great, great. And as I said, so I would like to also, I need some event, so it may be useful, so if you're an intimate event that we took the payment, we can have some BI or we can, I don't know, do something else, I mean, I don't know, we can send beer to this person after he booked room, because why not? And, okay, so we need the second event, payment taken, payment taken, filled, filled, room booked, room booked as well as price, and we need to marshal it again to JSON. Error. Cool, okay, and the last thing that we need to do is returning message, message as new, message new, UID new string, and payment taken payload. I hope that I'm not writing too fast or too slow, all right, so in there, there is a chance that it may work, so what we are doing, so we are receiving our room booked event, we are marshaling that, we are taking payment, and when we succeed, we are emitting another event. Sounds like a done, so the only thing that we need to do is to reuse that handler, so we have that one, and handler, cool, let's check if it compiles, it even compiles, so let's check if it's working, so let's book a couple rooms, and the idea is that by default WaterMe handler will try if the payment provider failed, so in there we should see some information that payment was taken, and we don't see that, I don't, I know why we don't see that, because we didn't start at router, run, context, error, it's a bit naive implementation because it's not really graceful shutdown, but what in the documentation, as I remember, we have examples with real graceful shutdown, so, okay, and let's see, okay, so we have some random error, and you can see payment taken, hooray, our company is saved, all right, so this is working, but there's one problem with that, so now we figure out that, okay, actually Kafka is a bit hard to run, and we are on GCP, so maybe we can just Google it, so I think that I can change Kafka implementation to Google, it pops up in one minute, I'm rewriting the bar today, hi, but I think that I can do that, let's start the timer, one, two, three, okay, let's check, I think I did that, so let's book, and okay, payment taken, we can double check, so let's use meal, and let's consume bookings, you see, it works, all right, so it will be that from live coding, one last thing that I would like to show you, because you may notice that, okay, it's a lot of boring JSON there, et cetera, et cetera, you may notice that I don't like boring stuff, because probably there are more interesting things to do than marshalling to JSON, so that's because of that we created a component that is named CQRS component, and the idea is that instead of doing this JSON-marshall and all that stuff, you can provide configuration to which format you would like to marshall everything, and under the hood it would be done, so you can use JSON, you can use Protobuf, Avro, I don't know, even something custom if you really want, the idea is that you're only implementing this interface, so you're providing the name of the handler, you are providing the event that you are expecting to receive, so in that case it will be room-booked, and you may notice that it was pre-generic, so we have the interface here, but we are working on the newer version, and you are just receiving this event, zero, un-marshalling, or whatever, and the same is going when you are publishing an event, so you are just providing the struct and watermill under the hood is doing all the marshalling stuff. Okay, so I think that will be all for live coding, it looks that I was lucky this time that everything worked, and yeah, of course it's still not production-grade implementation, I mean it's even hard to create a production-grade implementation of HTTP server, so it's more kind of inspiration to look deeper and see that, okay, it's not that scary, but you need to take into consideration that there are things like Kafka and Google Cloud pops-up internals, what is once delivery, actually shown the secure component, but I didn't call that, but it's helping a bit, so where you should start, because okay, it may be a lot of sources for you, and a lot of stuff to check, so I heard that we have pretty nice documentation, so we don't have any consulting or whatever for watermill, so we kind of don't care to have bad documentation, so yeah, I heard that we have pretty good documentation, so at the end of the presentation it will be in the link, what else, we have also a lot of examples in watermill, so I will encourage you to, it's black, oh, live coding, okay, it's not live coding, not only live coding, it's risky, so yeah, we have a lot of examples that probably you cannot see because it's on the black, but you need to believe me that this is on the watermill repository, at this point I wanted to say a big thank you to all watermill contributors, because without you it wouldn't be like it's now, and it's not an announcement that we actually released watermill 1.2 after having too many release candidates, so yeah, finally it's released, and you are all invited to an online release party, and we will say what are the new features, and it will be on March 1st, on the last link it will be also linked for that, and I think that will be also, this is the, again it's not working, oh, yeah, so this is the link that I promised to give you, the bonus that I have, I have super fancy holographic sticker notes, I'm sure that you don't have sticker notes, laptop stickers, so I'm sure that you don't have holographic ones, so if you don't have, so I have a lot of them, and yeah, I think that would be all, so thank you very much for your attention. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.72, "text": " Okay, this speaker claims that in 20 minutes, Robert is going to build an event-driven application.", "tokens": [1033, 11, 341, 8145, 9441, 300, 294, 945, 2077, 11, 7977, 307, 516, 281, 1322, 364, 2280, 12, 25456, 3861, 13], "temperature": 0.0, "avg_logprob": -0.4188558197021484, "compression_ratio": 1.2535211267605635, "no_speech_prob": 0.4693520963191986}, {"id": 1, "seek": 0, "start": 15.72, "end": 18.16, "text": " Well to be kind, I gave him 25 minutes.", "tokens": [1042, 281, 312, 733, 11, 286, 2729, 796, 3552, 2077, 13], "temperature": 0.0, "avg_logprob": -0.4188558197021484, "compression_ratio": 1.2535211267605635, "no_speech_prob": 0.4693520963191986}, {"id": 2, "seek": 0, "start": 18.16, "end": 20.2, "text": " So start your countdown clocks.", "tokens": [407, 722, 428, 35985, 41528, 13], "temperature": 0.0, "avg_logprob": -0.4188558197021484, "compression_ratio": 1.2535211267605635, "no_speech_prob": 0.4693520963191986}, {"id": 3, "seek": 0, "start": 20.2, "end": 21.2, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.4188558197021484, "compression_ratio": 1.2535211267605635, "no_speech_prob": 0.4693520963191986}, {"id": 4, "seek": 2120, "start": 21.2, "end": 33.2, "text": " So my name is Robert, and yes, I would like to show you today that we can build an event-driven", "tokens": [407, 452, 1315, 307, 7977, 11, 293, 2086, 11, 286, 576, 411, 281, 855, 291, 965, 300, 321, 393, 1322, 364, 2280, 12, 25456], "temperature": 0.0, "avg_logprob": -0.21591863116702517, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.0004719539429061115}, {"id": 5, "seek": 2120, "start": 33.2, "end": 38.32, "text": " application in Go, and it can be as simple as building a simple HTTP server.", "tokens": [3861, 294, 1037, 11, 293, 309, 393, 312, 382, 2199, 382, 2390, 257, 2199, 33283, 7154, 13], "temperature": 0.0, "avg_logprob": -0.21591863116702517, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.0004719539429061115}, {"id": 6, "seek": 2120, "start": 38.32, "end": 41.16, "text": " And I actually decided to put the bar a bit higher.", "tokens": [400, 286, 767, 3047, 281, 829, 264, 2159, 257, 857, 2946, 13], "temperature": 0.0, "avg_logprob": -0.21591863116702517, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.0004719539429061115}, {"id": 7, "seek": 2120, "start": 41.16, "end": 47.44, "text": " I think that I can do it within 15 minutes.", "tokens": [286, 519, 300, 286, 393, 360, 309, 1951, 2119, 2077, 13], "temperature": 0.0, "avg_logprob": -0.21591863116702517, "compression_ratio": 1.425531914893617, "no_speech_prob": 0.0004719539429061115}, {"id": 8, "seek": 4744, "start": 47.44, "end": 53.48, "text": " All right, at the beginning, a couple of words about myself.", "tokens": [1057, 558, 11, 412, 264, 2863, 11, 257, 1916, 295, 2283, 466, 2059, 13], "temperature": 0.0, "avg_logprob": -0.20298116023723894, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.0003958672459702939}, {"id": 9, "seek": 4744, "start": 53.48, "end": 58.32, "text": " So during the day, I work in a company named SlashID, so I work there as a principal engineer,", "tokens": [407, 1830, 264, 786, 11, 286, 589, 294, 257, 2237, 4926, 6187, 1299, 2777, 11, 370, 286, 589, 456, 382, 257, 9716, 11403, 11], "temperature": 0.0, "avg_logprob": -0.20298116023723894, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.0003958672459702939}, {"id": 10, "seek": 4744, "start": 58.32, "end": 63.28, "text": " and we are creating some identity and we're onboarding from a solution that is a bit more", "tokens": [293, 321, 366, 4084, 512, 6575, 293, 321, 434, 24033, 278, 490, 257, 3827, 300, 307, 257, 857, 544], "temperature": 0.0, "avg_logprob": -0.20298116023723894, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.0003958672459702939}, {"id": 11, "seek": 4744, "start": 63.28, "end": 67.47999999999999, "text": " frictionless than a solution available now on the market.", "tokens": [17710, 1832, 813, 257, 3827, 2435, 586, 322, 264, 2142, 13], "temperature": 0.0, "avg_logprob": -0.20298116023723894, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.0003958672459702939}, {"id": 12, "seek": 4744, "start": 67.47999999999999, "end": 72.84, "text": " And during the night, I'm blogging at 3.0.tech blog, where we are writing some blog posts", "tokens": [400, 1830, 264, 1818, 11, 286, 478, 6968, 3249, 412, 805, 13, 15, 13, 25970, 6968, 11, 689, 321, 366, 3579, 512, 6968, 12300], "temperature": 0.0, "avg_logprob": -0.20298116023723894, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.0003958672459702939}, {"id": 13, "seek": 7284, "start": 72.84, "end": 79.60000000000001, "text": " that are covering how to create Go applications that are business applications, but are also", "tokens": [300, 366, 10322, 577, 281, 1884, 1037, 5821, 300, 366, 1606, 5821, 11, 457, 366, 611], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 14, "seek": 7284, "start": 79.60000000000001, "end": 81.92, "text": " maintainable in the long term.", "tokens": [6909, 712, 294, 264, 938, 1433, 13], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 15, "seek": 7284, "start": 81.92, "end": 86.16, "text": " I know maybe some of you had a chance to read at least one article there, there are some", "tokens": [286, 458, 1310, 512, 295, 291, 632, 257, 2931, 281, 1401, 412, 1935, 472, 7222, 456, 11, 456, 366, 512], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 16, "seek": 7284, "start": 86.16, "end": 87.16, "text": " people.", "tokens": [561, 13], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 17, "seek": 7284, "start": 87.16, "end": 88.16, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 18, "seek": 7284, "start": 88.16, "end": 91.16, "text": " I will have something special for you later.", "tokens": [286, 486, 362, 746, 2121, 337, 291, 1780, 13], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 19, "seek": 7284, "start": 91.16, "end": 96.44, "text": " You can find me on Twitter, GitHub, Mastodon, there's also my email if you would like to", "tokens": [509, 393, 915, 385, 322, 5794, 11, 23331, 11, 376, 525, 378, 266, 11, 456, 311, 611, 452, 3796, 498, 291, 576, 411, 281], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 20, "seek": 7284, "start": 96.44, "end": 100.60000000000001, "text": " write to me and ask about something, but what's the most important for today?", "tokens": [2464, 281, 385, 293, 1029, 466, 746, 11, 457, 437, 311, 264, 881, 1021, 337, 965, 30], "temperature": 0.0, "avg_logprob": -0.2091131127398947, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.000693748181220144}, {"id": 21, "seek": 10060, "start": 100.6, "end": 104.72, "text": " I'm Oselton of Watermill Library, and how everything started with Watermill, because", "tokens": [286, 478, 422, 790, 1756, 295, 8772, 18841, 12806, 11, 293, 577, 1203, 1409, 365, 8772, 18841, 11, 570], "temperature": 0.0, "avg_logprob": -0.21759958267211915, "compression_ratio": 1.616, "no_speech_prob": 0.001055163680575788}, {"id": 22, "seek": 10060, "start": 104.72, "end": 107.55999999999999, "text": " I think that this is pretty important context.", "tokens": [286, 519, 300, 341, 307, 1238, 1021, 4319, 13], "temperature": 0.0, "avg_logprob": -0.21759958267211915, "compression_ratio": 1.616, "no_speech_prob": 0.001055163680575788}, {"id": 23, "seek": 10060, "start": 107.55999999999999, "end": 113.8, "text": " So a couple of years ago, I worked in a company where we are creating products that were not", "tokens": [407, 257, 1916, 295, 924, 2057, 11, 286, 2732, 294, 257, 2237, 689, 321, 366, 4084, 3383, 300, 645, 406], "temperature": 0.0, "avg_logprob": -0.21759958267211915, "compression_ratio": 1.616, "no_speech_prob": 0.001055163680575788}, {"id": 24, "seek": 10060, "start": 113.8, "end": 120.08, "text": " doing something super unusual, but the idea was that each user was able to add some content", "tokens": [884, 746, 1687, 10901, 11, 457, 264, 1558, 390, 300, 1184, 4195, 390, 1075, 281, 909, 512, 2701], "temperature": 0.0, "avg_logprob": -0.21759958267211915, "compression_ratio": 1.616, "no_speech_prob": 0.001055163680575788}, {"id": 25, "seek": 10060, "start": 120.08, "end": 126.28, "text": " and he should be able to, we were storing it to MySQL, plus we wanted to have some more", "tokens": [293, 415, 820, 312, 1075, 281, 11, 321, 645, 26085, 309, 281, 1222, 39934, 11, 1804, 321, 1415, 281, 362, 512, 544], "temperature": 0.0, "avg_logprob": -0.21759958267211915, "compression_ratio": 1.616, "no_speech_prob": 0.001055163680575788}, {"id": 26, "seek": 12628, "start": 126.28, "end": 134.32, "text": " advanced search, plus have ability to create fit for other users with some magic machine", "tokens": [7339, 3164, 11, 1804, 362, 3485, 281, 1884, 3318, 337, 661, 5022, 365, 512, 5585, 3479], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 27, "seek": 12628, "start": 134.32, "end": 137.28, "text": " learning models that they were doing personalization.", "tokens": [2539, 5245, 300, 436, 645, 884, 2973, 2144, 13], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 28, "seek": 12628, "start": 137.28, "end": 141.24, "text": " And usually if you are building such kind of system in a synchronous way, there's one", "tokens": [400, 2673, 498, 291, 366, 2390, 1270, 733, 295, 1185, 294, 257, 44743, 636, 11, 456, 311, 472], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 29, "seek": 12628, "start": 141.24, "end": 142.24, "text": " problem.", "tokens": [1154, 13], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 30, "seek": 12628, "start": 142.24, "end": 148.4, "text": " So this part may be sometimes slow, because elastic search is under high load, or magic", "tokens": [407, 341, 644, 815, 312, 2171, 2964, 11, 570, 17115, 3164, 307, 833, 1090, 3677, 11, 420, 5585], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 31, "seek": 12628, "start": 148.4, "end": 152.6, "text": " machine learning model so that this day it will not work.", "tokens": [3479, 2539, 2316, 370, 300, 341, 786, 309, 486, 406, 589, 13], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 32, "seek": 12628, "start": 152.6, "end": 156.12, "text": " Not nice, but it happens in work, unfortunately.", "tokens": [1726, 1481, 11, 457, 309, 2314, 294, 589, 11, 7015, 13], "temperature": 0.0, "avg_logprob": -0.2763374553007238, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0002640783495735377}, {"id": 33, "seek": 15612, "start": 156.12, "end": 162.04, "text": " And yeah, or even worse, for example, some part is not working, and it's not best user", "tokens": [400, 1338, 11, 420, 754, 5324, 11, 337, 1365, 11, 512, 644, 307, 406, 1364, 11, 293, 309, 311, 406, 1151, 4195], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 34, "seek": 15612, "start": 162.04, "end": 166.56, "text": " experience if it's working slowly, or it's, so for example, you can imagine that you're", "tokens": [1752, 498, 309, 311, 1364, 5692, 11, 420, 309, 311, 11, 370, 337, 1365, 11, 291, 393, 3811, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 35, "seek": 15612, "start": 166.56, "end": 171.16, "text": " adding some tweet and you're waiting for 10 seconds, because I know elastic search need", "tokens": [5127, 512, 15258, 293, 291, 434, 3806, 337, 1266, 3949, 11, 570, 286, 458, 17115, 3164, 643], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 36, "seek": 15612, "start": 171.16, "end": 176.20000000000002, "text": " to index something or machine learning model is working slowly, or even you are not able", "tokens": [281, 8186, 746, 420, 3479, 2539, 2316, 307, 1364, 5692, 11, 420, 754, 291, 366, 406, 1075], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 37, "seek": 15612, "start": 176.20000000000002, "end": 177.68, "text": " to add this content.", "tokens": [281, 909, 341, 2701, 13], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 38, "seek": 15612, "start": 177.68, "end": 183.92000000000002, "text": " And it doesn't make sense, because everything what is done on this other part of the diagram", "tokens": [400, 309, 1177, 380, 652, 2020, 11, 570, 1203, 437, 307, 1096, 322, 341, 661, 644, 295, 264, 10686], "temperature": 0.0, "avg_logprob": -0.2147282310154127, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00044561398681253195}, {"id": 39, "seek": 18392, "start": 183.92, "end": 189.28, "text": " could be done asynchronously, because okay, it's not a problem if, for example, the search,", "tokens": [727, 312, 1096, 42642, 5098, 11, 570, 1392, 11, 309, 311, 406, 257, 1154, 498, 11, 337, 1365, 11, 264, 3164, 11], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 40, "seek": 18392, "start": 189.28, "end": 194.44, "text": " some content that was added cannot be searched, for example, for one minute after it's added,", "tokens": [512, 2701, 300, 390, 3869, 2644, 312, 22961, 11, 337, 1365, 11, 337, 472, 3456, 934, 309, 311, 3869, 11], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 41, "seek": 18392, "start": 194.44, "end": 195.44, "text": " if something is done.", "tokens": [498, 746, 307, 1096, 13], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 42, "seek": 18392, "start": 195.44, "end": 198.95999999999998, "text": " It's much better than not allowing people to add anything.", "tokens": [467, 311, 709, 1101, 813, 406, 8293, 561, 281, 909, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 43, "seek": 18392, "start": 198.95999999999998, "end": 206.88, "text": " So by the book, the default solution for such problems is using some kind of pop-up and", "tokens": [407, 538, 264, 1446, 11, 264, 7576, 3827, 337, 1270, 2740, 307, 1228, 512, 733, 295, 1665, 12, 1010, 293], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 44, "seek": 18392, "start": 206.88, "end": 208.56, "text": " doing it asynchronously.", "tokens": [884, 309, 42642, 5098, 13], "temperature": 0.0, "avg_logprob": -0.2034122505966498, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0003040791198145598}, {"id": 45, "seek": 20856, "start": 208.56, "end": 215.16, "text": " So in this case, we decided to use Kafka, because it's scalable, it's nice, but as usually", "tokens": [407, 294, 341, 1389, 11, 321, 3047, 281, 764, 47064, 11, 570, 309, 311, 38481, 11, 309, 311, 1481, 11, 457, 382, 2673], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 46, "seek": 20856, "start": 215.16, "end": 219.4, "text": " with some concepts that you're reading in the books or listening on the conferences,", "tokens": [365, 512, 10392, 300, 291, 434, 3760, 294, 264, 3642, 420, 4764, 322, 264, 22032, 11], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 47, "seek": 20856, "start": 219.4, "end": 221.52, "text": " it's not that simple in practice.", "tokens": [309, 311, 406, 300, 2199, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 48, "seek": 20856, "start": 221.52, "end": 224.0, "text": " And it was also the case here.", "tokens": [400, 309, 390, 611, 264, 1389, 510, 13], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 49, "seek": 20856, "start": 224.0, "end": 228.88, "text": " The first problem was that the big part of the team wasn't actually working in asynchronous", "tokens": [440, 700, 1154, 390, 300, 264, 955, 644, 295, 264, 1469, 2067, 380, 767, 1364, 294, 49174], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 50, "seek": 20856, "start": 228.88, "end": 230.04, "text": " architectures earlier.", "tokens": [6331, 1303, 3071, 13], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 51, "seek": 20856, "start": 230.04, "end": 234.24, "text": " That kind of makes sense, because if you're starting to learn to code, you're not starting", "tokens": [663, 733, 295, 1669, 2020, 11, 570, 498, 291, 434, 2891, 281, 1466, 281, 3089, 11, 291, 434, 406, 2891], "temperature": 0.0, "avg_logprob": -0.17023040567125594, "compression_ratio": 1.7286821705426356, "no_speech_prob": 6.016627958160825e-05}, {"id": 52, "seek": 23424, "start": 234.24, "end": 239.52, "text": " with building some event-driven application, you're rather creating some REST API or website.", "tokens": [365, 2390, 512, 2280, 12, 25456, 3861, 11, 291, 434, 2831, 4084, 512, 497, 14497, 9362, 420, 3144, 13], "temperature": 0.0, "avg_logprob": -0.2132112763144753, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.00016627732838969678}, {"id": 53, "seek": 23424, "start": 239.52, "end": 249.84, "text": " So it makes sense that it was a big entry point for people that didn't use that.", "tokens": [407, 309, 1669, 2020, 300, 309, 390, 257, 955, 8729, 935, 337, 561, 300, 994, 380, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.2132112763144753, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.00016627732838969678}, {"id": 54, "seek": 23424, "start": 249.84, "end": 256.28000000000003, "text": " And it was not the only problem, because event-driven architecture has a lot of concepts that you", "tokens": [400, 309, 390, 406, 264, 787, 1154, 11, 570, 2280, 12, 25456, 9482, 575, 257, 688, 295, 10392, 300, 291], "temperature": 0.0, "avg_logprob": -0.2132112763144753, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.00016627732838969678}, {"id": 55, "seek": 23424, "start": 256.28000000000003, "end": 263.84000000000003, "text": " need to know, like customer groups, partitioning, message ordering, at least one's delivery,", "tokens": [643, 281, 458, 11, 411, 5474, 3935, 11, 24808, 278, 11, 3636, 21739, 11, 412, 1935, 472, 311, 8982, 11], "temperature": 0.0, "avg_logprob": -0.2132112763144753, "compression_ratio": 1.5466101694915255, "no_speech_prob": 0.00016627732838969678}, {"id": 56, "seek": 26384, "start": 263.84, "end": 266.4, "text": " acknowledge negative, acknowledge poison queue.", "tokens": [10692, 3671, 11, 10692, 10836, 18639, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 57, "seek": 26384, "start": 266.4, "end": 269.32, "text": " And with all of that, you need to be sure that you didn't miss an event.", "tokens": [400, 365, 439, 295, 300, 11, 291, 643, 281, 312, 988, 300, 291, 994, 380, 1713, 364, 2280, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 58, "seek": 26384, "start": 269.32, "end": 271.4, "text": " And it's pretty important in some domains.", "tokens": [400, 309, 311, 1238, 1021, 294, 512, 25514, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 59, "seek": 26384, "start": 271.4, "end": 274.96, "text": " In some cases, okay, it's fine, you're missing some event and okay.", "tokens": [682, 512, 3331, 11, 1392, 11, 309, 311, 2489, 11, 291, 434, 5361, 512, 2280, 293, 1392, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 60, "seek": 26384, "start": 274.96, "end": 279.71999999999997, "text": " But for example, I used to work in the financial domain, and losing one event may, for example,", "tokens": [583, 337, 1365, 11, 286, 1143, 281, 589, 294, 264, 4669, 9274, 11, 293, 7027, 472, 2280, 815, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 61, "seek": 26384, "start": 279.71999999999997, "end": 281.79999999999995, "text": " mean that somebody will be not paid out.", "tokens": [914, 300, 2618, 486, 312, 406, 4835, 484, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 62, "seek": 26384, "start": 281.79999999999995, "end": 282.79999999999995, "text": " Not nice.", "tokens": [1726, 1481, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 63, "seek": 26384, "start": 282.79999999999995, "end": 287.64, "text": " In general, I believe that as engineers, we should be responsible, because sometimes the", "tokens": [682, 2674, 11, 286, 1697, 300, 382, 11955, 11, 321, 820, 312, 6250, 11, 570, 2171, 264], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 64, "seek": 26384, "start": 287.64, "end": 292.64, "text": " code that we are building has a really big impact to the real life.", "tokens": [3089, 300, 321, 366, 2390, 575, 257, 534, 955, 2712, 281, 264, 957, 993, 13], "temperature": 0.0, "avg_logprob": -0.21352614258690703, "compression_ratio": 1.742671009771987, "no_speech_prob": 0.0003240231890231371}, {"id": 65, "seek": 29264, "start": 292.64, "end": 297.52, "text": " And after thinking for a while, I actually started to wonder, is it maybe something that", "tokens": [400, 934, 1953, 337, 257, 1339, 11, 286, 767, 1409, 281, 2441, 11, 307, 309, 1310, 746, 300], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 66, "seek": 29264, "start": 297.52, "end": 302.96, "text": " I can do to making, to building some kind of applications in Go simpler?", "tokens": [286, 393, 360, 281, 1455, 11, 281, 2390, 512, 733, 295, 5821, 294, 1037, 18587, 30], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 67, "seek": 29264, "start": 302.96, "end": 303.96, "text": " And here we are.", "tokens": [400, 510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 68, "seek": 29264, "start": 303.96, "end": 306.15999999999997, "text": " This is how WaterMill was created.", "tokens": [639, 307, 577, 8772, 44, 373, 390, 2942, 13], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 69, "seek": 29264, "start": 306.15999999999997, "end": 310.88, "text": " So far, we have more than 5,000 stars in the Github.", "tokens": [407, 1400, 11, 321, 362, 544, 813, 1025, 11, 1360, 6105, 294, 264, 460, 355, 836, 13], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 70, "seek": 29264, "start": 310.88, "end": 314.91999999999996, "text": " We have more than 50 contributors across multiple WaterMill repositories.", "tokens": [492, 362, 544, 813, 2625, 45627, 2108, 3866, 8772, 44, 373, 22283, 2083, 13], "temperature": 0.0, "avg_logprob": -0.23762274301180275, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0005882320110686123}, {"id": 71, "seek": 31492, "start": 314.92, "end": 324.64000000000004, "text": " We are supporting 12 different PubSupp implementations, like Kafka, like Google Cloud PubSupp, like", "tokens": [492, 366, 7231, 2272, 819, 21808, 50, 10504, 4445, 763, 11, 411, 47064, 11, 411, 3329, 8061, 21808, 50, 10504, 11, 411], "temperature": 0.0, "avg_logprob": -0.28091010680565465, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0003094393468927592}, {"id": 72, "seek": 31492, "start": 324.64000000000004, "end": 330.32, "text": " NATS JetStream, Rabbit and Q, but we have also some more strange implementations, like", "tokens": [426, 2218, 50, 28730, 4520, 1572, 11, 42092, 293, 1249, 11, 457, 321, 362, 611, 512, 544, 5861, 4445, 763, 11, 411], "temperature": 0.0, "avg_logprob": -0.28091010680565465, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0003094393468927592}, {"id": 73, "seek": 31492, "start": 330.32, "end": 335.24, "text": " MySQL, for example, if you don't have infrastructure for some real PubSupp, or for example, would", "tokens": [1222, 39934, 11, 337, 1365, 11, 498, 291, 500, 380, 362, 6896, 337, 512, 957, 21808, 50, 10504, 11, 420, 337, 1365, 11, 576], "temperature": 0.0, "avg_logprob": -0.28091010680565465, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0003094393468927592}, {"id": 74, "seek": 31492, "start": 335.24, "end": 338.84000000000003, "text": " like to avoid to face commits problem.", "tokens": [411, 281, 5042, 281, 1851, 48311, 1154, 13], "temperature": 0.0, "avg_logprob": -0.28091010680565465, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0003094393468927592}, {"id": 75, "seek": 31492, "start": 338.84000000000003, "end": 344.48, "text": " If you are doing some more fun projects, you can have just Go channel implementation", "tokens": [759, 291, 366, 884, 512, 544, 1019, 4455, 11, 291, 393, 362, 445, 1037, 2269, 11420], "temperature": 0.0, "avg_logprob": -0.28091010680565465, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0003094393468927592}, {"id": 76, "seek": 34448, "start": 344.48, "end": 346.48, "text": " or BoldDB, for example.", "tokens": [420, 48954, 27735, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 77, "seek": 34448, "start": 346.48, "end": 351.08000000000004, "text": " But there is one more important thing than that, WaterMill has logo, and it is a logo", "tokens": [583, 456, 307, 472, 544, 1021, 551, 813, 300, 11, 8772, 44, 373, 575, 9699, 11, 293, 309, 307, 257, 9699], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 78, "seek": 34448, "start": 351.08000000000004, "end": 357.68, "text": " with Go for Vomiting to Gobernati's logo, not as Muai.", "tokens": [365, 1037, 337, 691, 298, 1748, 281, 1037, 26848, 6908, 311, 9699, 11, 406, 382, 15601, 1301, 13], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 79, "seek": 34448, "start": 357.68, "end": 363.72, "text": " And you can think about WaterMill, like, so let's go back to this HTTP server example.", "tokens": [400, 291, 393, 519, 466, 8772, 44, 373, 11, 411, 11, 370, 718, 311, 352, 646, 281, 341, 33283, 7154, 1365, 13], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 80, "seek": 34448, "start": 363.72, "end": 368.6, "text": " So you can think about WaterMill, like something that makes your life simpler, like standard", "tokens": [407, 291, 393, 519, 466, 8772, 44, 373, 11, 411, 746, 300, 1669, 428, 993, 18587, 11, 411, 3832], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 81, "seek": 34448, "start": 368.6, "end": 369.6, "text": " library for HTTP.", "tokens": [6405, 337, 33283, 13], "temperature": 0.0, "avg_logprob": -0.3470765613374256, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.0001048096310114488}, {"id": 82, "seek": 36960, "start": 369.6, "end": 374.6, "text": " So, for example, if you are implementing an HTTP server, you don't care about TLS, layers", "tokens": [407, 11, 337, 1365, 11, 498, 291, 366, 18114, 364, 33283, 7154, 11, 291, 500, 380, 1127, 466, 314, 19198, 11, 7914], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 83, "seek": 36960, "start": 374.6, "end": 378.88, "text": " of network, you can start connection pooling and all this stuff, you are just implementing", "tokens": [295, 3209, 11, 291, 393, 722, 4984, 7005, 278, 293, 439, 341, 1507, 11, 291, 366, 445, 18114], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 84, "seek": 36960, "start": 378.88, "end": 379.88, "text": " the logic in most cases.", "tokens": [264, 9952, 294, 881, 3331, 13], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 85, "seek": 36960, "start": 379.88, "end": 384.12, "text": " Sometimes, of course, you may have some specific scenarios that you care about that, but in", "tokens": [4803, 11, 295, 1164, 11, 291, 815, 362, 512, 2685, 15077, 300, 291, 1127, 466, 300, 11, 457, 294], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 86, "seek": 36960, "start": 384.12, "end": 389.36, "text": " most cases, you should just implement your handlers and don't care about everything around.", "tokens": [881, 3331, 11, 291, 820, 445, 4445, 428, 1011, 11977, 293, 500, 380, 1127, 466, 1203, 926, 13], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 87, "seek": 36960, "start": 389.36, "end": 397.32000000000005, "text": " And as you already, some of you shown, so I sometimes wrote the article that I think", "tokens": [400, 382, 291, 1217, 11, 512, 295, 291, 4898, 11, 370, 286, 2171, 4114, 264, 7222, 300, 286, 519], "temperature": 0.0, "avg_logprob": -0.23309138725543843, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.00040769376209937036}, {"id": 88, "seek": 39732, "start": 397.32, "end": 403.08, "text": " that frameworks are probably not working best in Go, and WaterMill is also, for example,", "tokens": [300, 29834, 366, 1391, 406, 1364, 1151, 294, 1037, 11, 293, 8772, 44, 373, 307, 611, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 89, "seek": 39732, "start": 403.08, "end": 405.76, "text": " that's the case why WaterMill is actually a library.", "tokens": [300, 311, 264, 1389, 983, 8772, 44, 373, 307, 767, 257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 90, "seek": 39732, "start": 405.76, "end": 408.96, "text": " And it's pretty good to upside.", "tokens": [400, 309, 311, 1238, 665, 281, 14119, 13], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 91, "seek": 39732, "start": 408.96, "end": 412.92, "text": " So the first one is that if you already have some system and you would like to migrate", "tokens": [407, 264, 700, 472, 307, 300, 498, 291, 1217, 362, 512, 1185, 293, 291, 576, 411, 281, 31821], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 92, "seek": 39732, "start": 412.92, "end": 417.76, "text": " to WaterMill, it's kind of simple, because WaterMill doesn't add anything super custom", "tokens": [281, 8772, 44, 373, 11, 309, 311, 733, 295, 2199, 11, 570, 8772, 44, 373, 1177, 380, 909, 1340, 1687, 2375], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 93, "seek": 39732, "start": 417.76, "end": 422.76, "text": " and it can be integrated with any existing system, and vice versa.", "tokens": [293, 309, 393, 312, 10919, 365, 604, 6741, 1185, 11, 293, 11964, 25650, 13], "temperature": 0.0, "avg_logprob": -0.23629963839495624, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00018334644846618176}, {"id": 94, "seek": 42276, "start": 422.76, "end": 427.56, "text": " See, for example, for some reason, you decide that you don't like WaterMill, but you will", "tokens": [3008, 11, 337, 1365, 11, 337, 512, 1778, 11, 291, 4536, 300, 291, 500, 380, 411, 8772, 44, 373, 11, 457, 291, 486], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 95, "seek": 42276, "start": 427.56, "end": 428.56, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 96, "seek": 42276, "start": 428.56, "end": 431.56, "text": " So you can migrate from WaterMill to some different library.", "tokens": [407, 291, 393, 31821, 490, 8772, 44, 373, 281, 512, 819, 6405, 13], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 97, "seek": 42276, "start": 431.56, "end": 432.71999999999997, "text": " So this is the good thing.", "tokens": [407, 341, 307, 264, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 98, "seek": 42276, "start": 432.71999999999997, "end": 436.88, "text": " And I think what's pretty important, so how everything is done, because, okay, in theory", "tokens": [400, 286, 519, 437, 311, 1238, 1021, 11, 370, 577, 1203, 307, 1096, 11, 570, 11, 1392, 11, 294, 5261], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 99, "seek": 42276, "start": 436.88, "end": 442.48, "text": " it may sound nice, but it's helping, but how WaterMill is built.", "tokens": [309, 815, 1626, 1481, 11, 457, 309, 311, 4315, 11, 457, 577, 8772, 44, 373, 307, 3094, 13], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 100, "seek": 42276, "start": 442.48, "end": 449.2, "text": " And in the heart of WaterMill, I would say that you can see in multiple places something", "tokens": [400, 294, 264, 1917, 295, 8772, 44, 373, 11, 286, 576, 584, 300, 291, 393, 536, 294, 3866, 3190, 746], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 101, "seek": 42276, "start": 449.2, "end": 450.44, "text": " that is named UNIX philosophy.", "tokens": [300, 307, 4926, 8229, 21124, 10675, 13], "temperature": 0.0, "avg_logprob": -0.20998187363147736, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00040410293149761856}, {"id": 102, "seek": 45044, "start": 450.44, "end": 454.6, "text": " And it's kind of old philosophy, because it's from 1978.", "tokens": [400, 309, 311, 733, 295, 1331, 10675, 11, 570, 309, 311, 490, 33191, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 103, "seek": 45044, "start": 454.6, "end": 458.96, "text": " And it's saying us to write programs that do one thing and do it well, write programs", "tokens": [400, 309, 311, 1566, 505, 281, 2464, 4268, 300, 360, 472, 551, 293, 360, 309, 731, 11, 2464, 4268], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 104, "seek": 45044, "start": 458.96, "end": 464.32, "text": " to work together, and write programs to handle, in our case, message.", "tokens": [281, 589, 1214, 11, 293, 2464, 4268, 281, 4813, 11, 294, 527, 1389, 11, 3636, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 105, "seek": 45044, "start": 464.32, "end": 466.64, "text": " Because that is a universal interface.", "tokens": [1436, 300, 307, 257, 11455, 9226, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 106, "seek": 45044, "start": 466.64, "end": 468.64, "text": " And some small question now.", "tokens": [400, 512, 1359, 1168, 586, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 107, "seek": 45044, "start": 468.64, "end": 473.64, "text": " Do you know who's that?", "tokens": [1144, 291, 458, 567, 311, 300, 30], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 108, "seek": 45044, "start": 473.64, "end": 474.64, "text": " So it's Ken Thompson.", "tokens": [407, 309, 311, 8273, 23460, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 109, "seek": 45044, "start": 474.64, "end": 477.0, "text": " So he's the author of this philosophy.", "tokens": [407, 415, 311, 264, 3793, 295, 341, 10675, 13], "temperature": 0.0, "avg_logprob": -0.20433549319996552, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00046431238297373056}, {"id": 110, "seek": 47700, "start": 477.0, "end": 481.8, "text": " And what's also interesting, he's one of the authors of Go programming language.", "tokens": [400, 437, 311, 611, 1880, 11, 415, 311, 472, 295, 264, 16552, 295, 1037, 9410, 2856, 13], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 111, "seek": 47700, "start": 481.8, "end": 486.44, "text": " Actually it makes sense, because if you look on the Go, for example, to IO Reader or our", "tokens": [5135, 309, 1669, 2020, 11, 570, 498, 291, 574, 322, 264, 1037, 11, 337, 1365, 11, 281, 286, 46, 1300, 8312, 420, 527], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 112, "seek": 47700, "start": 486.44, "end": 489.24, "text": " writer, this is pretty nicely visible there.", "tokens": [9936, 11, 341, 307, 1238, 9594, 8974, 456, 13], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 113, "seek": 47700, "start": 489.24, "end": 493.88, "text": " And I know that for a lot of people didn't know about UNIX philosophy.", "tokens": [400, 286, 458, 300, 337, 257, 688, 295, 561, 994, 380, 458, 466, 8229, 21124, 10675, 13], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 114, "seek": 47700, "start": 493.88, "end": 498.08, "text": " And sometimes when I have too much time to think, I have some impression that, no, sometimes", "tokens": [400, 2171, 562, 286, 362, 886, 709, 565, 281, 519, 11, 286, 362, 512, 9995, 300, 11, 572, 11, 2171], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 115, "seek": 47700, "start": 498.08, "end": 503.88, "text": " we forgot about some good old ideas and we're trying to reinvent the wheel, even if some", "tokens": [321, 5298, 466, 512, 665, 1331, 3487, 293, 321, 434, 1382, 281, 33477, 264, 5589, 11, 754, 498, 512], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 116, "seek": 47700, "start": 503.88, "end": 506.56, "text": " problems were already solved.", "tokens": [2740, 645, 1217, 13041, 13], "temperature": 0.0, "avg_logprob": -0.24874874902149988, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.0004612038901541382}, {"id": 117, "seek": 50656, "start": 506.56, "end": 512.0, "text": " And you know, it's maybe something like in Dark Ages that it was some old nice ideas,", "tokens": [400, 291, 458, 11, 309, 311, 1310, 746, 411, 294, 9563, 37362, 300, 309, 390, 512, 1331, 1481, 3487, 11], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 118, "seek": 50656, "start": 512.0, "end": 514.04, "text": " but it was a bit forgotten.", "tokens": [457, 309, 390, 257, 857, 11832, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 119, "seek": 50656, "start": 514.04, "end": 516.2, "text": " And OK, maybe I'm thinking too much.", "tokens": [400, 2264, 11, 1310, 286, 478, 1953, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 120, "seek": 50656, "start": 516.2, "end": 518.4, "text": " Let's go back to the watermill.", "tokens": [961, 311, 352, 646, 281, 264, 1281, 18841, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 121, "seek": 50656, "start": 518.4, "end": 522.84, "text": " So there are a couple important times in watermill.", "tokens": [407, 456, 366, 257, 1916, 1021, 1413, 294, 1281, 18841, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 122, "seek": 50656, "start": 522.84, "end": 524.6, "text": " So the first one is message.", "tokens": [407, 264, 700, 472, 307, 3636, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 123, "seek": 50656, "start": 524.6, "end": 528.52, "text": " So if you compare it to HTTP server, so it's something similar to HTTP request.", "tokens": [407, 498, 291, 6794, 309, 281, 33283, 7154, 11, 370, 309, 311, 746, 2531, 281, 33283, 5308, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 124, "seek": 50656, "start": 528.52, "end": 531.84, "text": " So in message we have UID, that is pretty useful for debugging.", "tokens": [407, 294, 3636, 321, 362, 624, 2777, 11, 300, 307, 1238, 4420, 337, 45592, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 125, "seek": 50656, "start": 531.84, "end": 532.84, "text": " We have metadata.", "tokens": [492, 362, 26603, 13], "temperature": 0.0, "avg_logprob": -0.2516905571803574, "compression_ratio": 1.6098484848484849, "no_speech_prob": 0.0002179159491788596}, {"id": 126, "seek": 53284, "start": 532.84, "end": 536.8000000000001, "text": " So metadata is something like headers request plus payload.", "tokens": [407, 26603, 307, 746, 411, 45101, 5308, 1804, 30918, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 127, "seek": 53284, "start": 536.8000000000001, "end": 541.6800000000001, "text": " So this is the place where you are storing your event, for example.", "tokens": [407, 341, 307, 264, 1081, 689, 291, 366, 26085, 428, 2280, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 128, "seek": 53284, "start": 541.6800000000001, "end": 547.1600000000001, "text": " The two next important parts of watermill are publisher and subscriber.", "tokens": [440, 732, 958, 1021, 3166, 295, 1281, 18841, 366, 25088, 293, 26122, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 129, "seek": 53284, "start": 547.1600000000001, "end": 549.9200000000001, "text": " So publisher, you can publish those messages.", "tokens": [407, 25088, 11, 291, 393, 11374, 729, 7897, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 130, "seek": 53284, "start": 549.9200000000001, "end": 552.0, "text": " And with subscriber, you're right.", "tokens": [400, 365, 26122, 11, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 131, "seek": 53284, "start": 552.0, "end": 557.1600000000001, "text": " You can subscribe for those messages from the provided topic and receive that by the", "tokens": [509, 393, 3022, 337, 729, 7897, 490, 264, 5649, 4829, 293, 4774, 300, 538, 264], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 132, "seek": 53284, "start": 557.1600000000001, "end": 558.1600000000001, "text": " channel.", "tokens": [2269, 13], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 133, "seek": 53284, "start": 558.1600000000001, "end": 561.36, "text": " You usually are not using these interfaces because it's used somewhere internally in", "tokens": [509, 2673, 366, 406, 1228, 613, 28416, 570, 309, 311, 1143, 4079, 19501, 294], "temperature": 0.0, "avg_logprob": -0.21537025158221906, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.00042914444929920137}, {"id": 134, "seek": 56136, "start": 561.36, "end": 563.0, "text": " watermill.", "tokens": [1281, 18841, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 135, "seek": 56136, "start": 563.0, "end": 567.0, "text": " But for example, if you would like to add a new implementation of PubSub, this is something", "tokens": [583, 337, 1365, 11, 498, 291, 576, 411, 281, 909, 257, 777, 11420, 295, 21808, 39582, 11, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 136, "seek": 56136, "start": 567.0, "end": 568.0, "text": " that you're implementing.", "tokens": [300, 291, 434, 18114, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 137, "seek": 56136, "start": 568.0, "end": 570.84, "text": " And each PubSub implementation is implementing this interface.", "tokens": [400, 1184, 21808, 39582, 11420, 307, 18114, 341, 9226, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 138, "seek": 56136, "start": 570.84, "end": 572.5600000000001, "text": " That's why I actually pretty like this interface.", "tokens": [663, 311, 983, 286, 767, 1238, 411, 341, 9226, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 139, "seek": 56136, "start": 572.5600000000001, "end": 578.16, "text": " Because it's making some constraint on the implementers that, OK, they need to implement", "tokens": [1436, 309, 311, 1455, 512, 25534, 322, 264, 4445, 433, 300, 11, 2264, 11, 436, 643, 281, 4445], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 140, "seek": 56136, "start": 578.16, "end": 579.6, "text": " that in that way.", "tokens": [300, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 141, "seek": 56136, "start": 579.6, "end": 587.6, "text": " But it's also not good because it's making each of them pretty compatible with themselves.", "tokens": [583, 309, 311, 611, 406, 665, 570, 309, 311, 1455, 1184, 295, 552, 1238, 18218, 365, 2969, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 142, "seek": 56136, "start": 587.6, "end": 591.04, "text": " And the last but not least type is hender function.", "tokens": [400, 264, 1036, 457, 406, 1935, 2010, 307, 276, 3216, 2445, 13], "temperature": 0.0, "avg_logprob": -0.176589918530677, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0003727908479049802}, {"id": 143, "seek": 59104, "start": 591.04, "end": 596.76, "text": " Hender function is something like HTTP handler that you are implementing in your HTTP server", "tokens": [389, 3216, 2445, 307, 746, 411, 33283, 41967, 300, 291, 366, 18114, 294, 428, 33283, 7154], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 144, "seek": 59104, "start": 596.76, "end": 600.3199999999999, "text": " with the small difference that instead of receiving HTTP request, you are receiving", "tokens": [365, 264, 1359, 2649, 300, 2602, 295, 10040, 33283, 5308, 11, 291, 366, 10040], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 145, "seek": 59104, "start": 600.3199999999999, "end": 601.3199999999999, "text": " a message.", "tokens": [257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 146, "seek": 59104, "start": 601.3199999999999, "end": 603.0, "text": " And optionally, you can receive the message.", "tokens": [400, 3614, 379, 11, 291, 393, 4774, 264, 3636, 13], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 147, "seek": 59104, "start": 603.0, "end": 609.5999999999999, "text": " So the idea is that you can react on some message, do something, and emit some other", "tokens": [407, 264, 1558, 307, 300, 291, 393, 4515, 322, 512, 3636, 11, 360, 746, 11, 293, 32084, 512, 661], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 148, "seek": 59104, "start": 609.5999999999999, "end": 611.64, "text": " messages so you can do some kind of changing later.", "tokens": [7897, 370, 291, 393, 360, 512, 733, 295, 4473, 1780, 13], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 149, "seek": 59104, "start": 611.64, "end": 614.5999999999999, "text": " I will show shortly an example.", "tokens": [286, 486, 855, 13392, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 150, "seek": 59104, "start": 614.5999999999999, "end": 619.04, "text": " And everything is magically connected, sorry, it may be small, but you need to trust me", "tokens": [400, 1203, 307, 39763, 4582, 11, 2597, 11, 309, 815, 312, 1359, 11, 457, 291, 643, 281, 3361, 385], "temperature": 0.0, "avg_logprob": -0.21695606525127703, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.00022397027350962162}, {"id": 151, "seek": 61904, "start": 619.04, "end": 622.0, "text": " that in the middle there is a router here.", "tokens": [300, 294, 264, 2808, 456, 307, 257, 22492, 510, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 152, "seek": 61904, "start": 622.0, "end": 623.64, "text": " And this is connecting everything.", "tokens": [400, 341, 307, 11015, 1203, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 153, "seek": 61904, "start": 623.64, "end": 628.8399999999999, "text": " So the message is going from some publisher, it doesn't need to be WaterMill, it's going", "tokens": [407, 264, 3636, 307, 516, 490, 512, 25088, 11, 309, 1177, 380, 643, 281, 312, 8772, 44, 373, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 154, "seek": 61904, "start": 628.8399999999999, "end": 633.8, "text": " to the queue by subscriber, the router.", "tokens": [281, 264, 18639, 538, 26122, 11, 264, 22492, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 155, "seek": 61904, "start": 633.8, "end": 635.7199999999999, "text": " Router is passing it through middleware.", "tokens": [497, 23985, 307, 8437, 309, 807, 2808, 3039, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 156, "seek": 61904, "start": 635.7199999999999, "end": 640.8399999999999, "text": " Middleware works in WaterMill like HTTP, so another thing that is pretty similar.", "tokens": [10775, 3039, 1985, 294, 8772, 44, 373, 411, 33283, 11, 370, 1071, 551, 300, 307, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 157, "seek": 61904, "start": 640.8399999999999, "end": 643.4, "text": " And it's processed by handlers.", "tokens": [400, 309, 311, 18846, 538, 1011, 11977, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 158, "seek": 61904, "start": 643.4, "end": 648.0, "text": " And later, if we want, we can publish some other messages.", "tokens": [400, 1780, 11, 498, 321, 528, 11, 321, 393, 11374, 512, 661, 7897, 13], "temperature": 0.0, "avg_logprob": -0.2691325304801004, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00022296053066384047}, {"id": 159, "seek": 64800, "start": 648.0, "end": 649.6, "text": " Not super complex.", "tokens": [1726, 1687, 3997, 13], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 160, "seek": 64800, "start": 649.6, "end": 654.48, "text": " So do you know the first rule of live coding?", "tokens": [407, 360, 291, 458, 264, 700, 4978, 295, 1621, 17720, 30], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 161, "seek": 64800, "start": 654.48, "end": 655.48, "text": " Don't do live coding.", "tokens": [1468, 380, 360, 1621, 17720, 13], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 162, "seek": 64800, "start": 655.48, "end": 657.48, "text": " So do live coding.", "tokens": [407, 360, 1621, 17720, 13], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 163, "seek": 64800, "start": 657.48, "end": 660.48, "text": " What can go wrong?", "tokens": [708, 393, 352, 2085, 30], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 164, "seek": 64800, "start": 660.48, "end": 661.48, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.343114177385966, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.000967463303823024}, {"id": 165, "seek": 66148, "start": 661.48, "end": 688.12, "text": " Like to change sharing settings, so on second, it's probably not this one.", "tokens": [1743, 281, 1319, 5414, 6257, 11, 370, 322, 1150, 11, 309, 311, 1391, 406, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.5014231091453916, "compression_ratio": 1.0, "no_speech_prob": 0.0038708001375198364}, {"id": 166, "seek": 68812, "start": 688.12, "end": 694.12, "text": " This is why you are not doing live coding.", "tokens": [639, 307, 983, 291, 366, 406, 884, 1621, 17720, 13], "temperature": 0.0, "avg_logprob": -0.5747878334738992, "compression_ratio": 1.1454545454545455, "no_speech_prob": 0.0008938307873904705}, {"id": 167, "seek": 68812, "start": 694.12, "end": 696.12, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.5747878334738992, "compression_ratio": 1.1454545454545455, "no_speech_prob": 0.0008938307873904705}, {"id": 168, "seek": 68812, "start": 696.12, "end": 698.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5747878334738992, "compression_ratio": 1.1454545454545455, "no_speech_prob": 0.0008938307873904705}, {"id": 169, "seek": 68812, "start": 698.12, "end": 717.52, "text": " So something does work, that's good, but I'm not really like, I want it.", "tokens": [407, 746, 775, 589, 11, 300, 311, 665, 11, 457, 286, 478, 406, 534, 411, 11, 286, 528, 309, 13], "temperature": 0.0, "avg_logprob": -0.5747878334738992, "compression_ratio": 1.1454545454545455, "no_speech_prob": 0.0008938307873904705}, {"id": 170, "seek": 71752, "start": 717.52, "end": 719.8, "text": " This is something that I wanted to have.", "tokens": [639, 307, 746, 300, 286, 1415, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 171, "seek": 71752, "start": 719.8, "end": 723.28, "text": " So I prepared a simple application here.", "tokens": [407, 286, 4927, 257, 2199, 3861, 510, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 172, "seek": 71752, "start": 723.28, "end": 724.84, "text": " And what does application does?", "tokens": [400, 437, 775, 3861, 775, 30], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 173, "seek": 71752, "start": 724.84, "end": 729.28, "text": " So if you're not from Brussels, so this may be something familiar to you.", "tokens": [407, 498, 291, 434, 406, 490, 38717, 11, 370, 341, 815, 312, 746, 4963, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 174, "seek": 71752, "start": 729.28, "end": 732.48, "text": " So it allows you to book a room in hotel.", "tokens": [407, 309, 4045, 291, 281, 1446, 257, 1808, 294, 7622, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 175, "seek": 71752, "start": 732.48, "end": 738.16, "text": " So you can provide room ID, pass guest counts, and let's see if it works.", "tokens": [407, 291, 393, 2893, 1808, 7348, 11, 1320, 8341, 14893, 11, 293, 718, 311, 536, 498, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 176, "seek": 71752, "start": 738.16, "end": 739.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 177, "seek": 71752, "start": 739.16, "end": 741.12, "text": " It seems that it's not working sometimes.", "tokens": [467, 2544, 300, 309, 311, 406, 1364, 2171, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 178, "seek": 71752, "start": 741.12, "end": 742.8, "text": " Sometimes it's working.", "tokens": [4803, 309, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 179, "seek": 71752, "start": 742.8, "end": 745.88, "text": " Sometimes it's not working.", "tokens": [4803, 309, 311, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.28311337922748764, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.0017983034485951066}, {"id": 180, "seek": 74588, "start": 745.88, "end": 750.84, "text": " Sometimes it's working slowly, slowly, slowly, slowly.", "tokens": [4803, 309, 311, 1364, 5692, 11, 5692, 11, 5692, 11, 5692, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 181, "seek": 74588, "start": 750.84, "end": 752.24, "text": " Sometimes it's even not working slowly.", "tokens": [4803, 309, 311, 754, 406, 1364, 5692, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 182, "seek": 74588, "start": 752.24, "end": 753.6, "text": " So it's even worse.", "tokens": [407, 309, 311, 754, 5324, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 183, "seek": 74588, "start": 753.6, "end": 758.6, "text": " So let's check the source code of that application.", "tokens": [407, 718, 311, 1520, 264, 4009, 3089, 295, 300, 3861, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 184, "seek": 74588, "start": 758.6, "end": 759.6, "text": " So okay.", "tokens": [407, 1392, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 185, "seek": 74588, "start": 759.6, "end": 766.4, "text": " So here we are running HTTP, so boring, signals handling boring, but this is probably not", "tokens": [407, 510, 321, 366, 2614, 33283, 11, 370, 9989, 11, 12354, 13175, 9989, 11, 457, 341, 307, 1391, 406], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 186, "seek": 74588, "start": 766.4, "end": 767.4, "text": " boring.", "tokens": [9989, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 187, "seek": 74588, "start": 767.4, "end": 771.28, "text": " This is usually when the most interesting part of the application lives.", "tokens": [639, 307, 2673, 562, 264, 881, 1880, 644, 295, 264, 3861, 2909, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 188, "seek": 74588, "start": 771.28, "end": 772.28, "text": " Let's check our handler.", "tokens": [961, 311, 1520, 527, 41967, 13], "temperature": 0.0, "avg_logprob": -0.21549427032470703, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.00022466093651019037}, {"id": 189, "seek": 77228, "start": 772.28, "end": 779.0, "text": " So okay, so we are unmartialing stuff, to book room request, we have some advanced algorithm", "tokens": [407, 1392, 11, 370, 321, 366, 517, 18696, 831, 278, 1507, 11, 281, 1446, 1808, 5308, 11, 321, 362, 512, 7339, 9284], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 190, "seek": 77228, "start": 779.0, "end": 782.68, "text": " of calculation of room price, and we are taking payment.", "tokens": [295, 17108, 295, 1808, 3218, 11, 293, 321, 366, 1940, 10224, 13], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 191, "seek": 77228, "start": 782.68, "end": 784.8, "text": " What can go wrong here?", "tokens": [708, 393, 352, 2085, 510, 30], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 192, "seek": 77228, "start": 784.8, "end": 790.8399999999999, "text": " And okay, as we can see, our payment provider, it's not super stable, but okay, I don't know,", "tokens": [400, 1392, 11, 382, 321, 393, 536, 11, 527, 10224, 12398, 11, 309, 311, 406, 1687, 8351, 11, 457, 1392, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 193, "seek": 77228, "start": 790.8399999999999, "end": 795.4399999999999, "text": " let's imagine that it's our boss colleague and we cannot change that, no, politics.", "tokens": [718, 311, 3811, 300, 309, 311, 527, 5741, 13532, 293, 321, 2644, 1319, 300, 11, 572, 11, 7341, 13], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 194, "seek": 77228, "start": 795.4399999999999, "end": 796.4399999999999, "text": " It happens.", "tokens": [467, 2314, 13], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 195, "seek": 77228, "start": 796.4399999999999, "end": 797.4399999999999, "text": " It's okay.", "tokens": [467, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 196, "seek": 77228, "start": 797.4399999999999, "end": 798.4399999999999, "text": " What we can do?", "tokens": [708, 321, 393, 360, 30], "temperature": 0.0, "avg_logprob": -0.22175508582073708, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.0005528447800315917}, {"id": 197, "seek": 79844, "start": 798.44, "end": 805.2800000000001, "text": " We can do like that, go, fang, okay, done, it works now, but it's one problem with that.", "tokens": [492, 393, 360, 411, 300, 11, 352, 11, 283, 656, 11, 1392, 11, 1096, 11, 309, 1985, 586, 11, 457, 309, 311, 472, 1154, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 198, "seek": 79844, "start": 805.2800000000001, "end": 811.72, "text": " So if our server will die, there is a chance that we'll not take payment, and it doesn't", "tokens": [407, 498, 527, 7154, 486, 978, 11, 456, 307, 257, 2931, 300, 321, 603, 406, 747, 10224, 11, 293, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 199, "seek": 79844, "start": 811.72, "end": 813.0, "text": " like that as the best idea.", "tokens": [411, 300, 382, 264, 1151, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 200, "seek": 79844, "start": 813.0, "end": 814.8000000000001, "text": " So what will be my idea?", "tokens": [407, 437, 486, 312, 452, 1558, 30], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 201, "seek": 79844, "start": 814.8000000000001, "end": 820.44, "text": " So instead of doing it synchronously with this HTTP handler, I would like to emit some", "tokens": [407, 2602, 295, 884, 309, 19331, 5098, 365, 341, 33283, 41967, 11, 286, 576, 411, 281, 32084, 512], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 202, "seek": 79844, "start": 820.44, "end": 826.4000000000001, "text": " event, listen to that event, and take payment asynchronously.", "tokens": [2280, 11, 2140, 281, 300, 2280, 11, 293, 747, 10224, 42642, 5098, 13], "temperature": 0.0, "avg_logprob": -0.2764198161937572, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.00036415114300325513}, {"id": 203, "seek": 82640, "start": 826.4, "end": 829.12, "text": " So let's do that, and let's do that with watermill, of course.", "tokens": [407, 718, 311, 360, 300, 11, 293, 718, 311, 360, 300, 365, 1281, 18841, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.3075638674618153, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.000958436110522598}, {"id": 204, "seek": 82640, "start": 829.12, "end": 836.12, "text": " So at the beginning, we need to get rid of that, and we need to have our publisher here.", "tokens": [407, 412, 264, 2863, 11, 321, 643, 281, 483, 3973, 295, 300, 11, 293, 321, 643, 281, 362, 527, 25088, 510, 13], "temperature": 0.0, "avg_logprob": -0.3075638674618153, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.000958436110522598}, {"id": 205, "seek": 82640, "start": 836.12, "end": 844.16, "text": " Message publisher, so this is the interface that you should remember, all right.", "tokens": [45947, 25088, 11, 370, 341, 307, 264, 9226, 300, 291, 820, 1604, 11, 439, 558, 13], "temperature": 0.0, "avg_logprob": -0.3075638674618153, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.000958436110522598}, {"id": 206, "seek": 82640, "start": 844.16, "end": 849.04, "text": " And I also can prepare some code snippets to not lose time on some boring stuff like", "tokens": [400, 286, 611, 393, 5940, 512, 3089, 35623, 1385, 281, 406, 3624, 565, 322, 512, 9989, 1507, 411], "temperature": 0.0, "avg_logprob": -0.3075638674618153, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.000958436110522598}, {"id": 207, "seek": 82640, "start": 849.04, "end": 850.04, "text": " room booked.", "tokens": [1808, 26735, 13], "temperature": 0.0, "avg_logprob": -0.3075638674618153, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.000958436110522598}, {"id": 208, "seek": 85004, "start": 850.04, "end": 868.0799999999999, "text": " Well, we have our event, so room booked, all right, guest count, and price, room, price.", "tokens": [1042, 11, 321, 362, 527, 2280, 11, 370, 1808, 26735, 11, 439, 558, 11, 8341, 1207, 11, 293, 3218, 11, 1808, 11, 3218, 13], "temperature": 0.0, "avg_logprob": -0.32400750494622566, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.000847065297421068}, {"id": 209, "seek": 85004, "start": 868.0799999999999, "end": 872.8399999999999, "text": " All right, now we need to marshal that, because we are sending bytes between our processes", "tokens": [1057, 558, 11, 586, 321, 643, 281, 30517, 4947, 300, 11, 570, 321, 366, 7750, 36088, 1296, 527, 7555], "temperature": 0.0, "avg_logprob": -0.32400750494622566, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.000847065297421068}, {"id": 210, "seek": 85004, "start": 872.8399999999999, "end": 878.04, "text": " through our PAPS app, so JSON, because JSON is kind of common and it's pretty easy to", "tokens": [807, 527, 430, 4715, 50, 724, 11, 370, 31828, 11, 570, 31828, 307, 733, 295, 2689, 293, 309, 311, 1238, 1858, 281], "temperature": 0.0, "avg_logprob": -0.32400750494622566, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.000847065297421068}, {"id": 211, "seek": 85004, "start": 878.04, "end": 879.04, "text": " debug.", "tokens": [24083, 13], "temperature": 0.0, "avg_logprob": -0.32400750494622566, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.000847065297421068}, {"id": 212, "seek": 87904, "start": 879.04, "end": 890.24, "text": " So let's marshal that, payload error, room booked.", "tokens": [407, 718, 311, 30517, 4947, 300, 11, 30918, 6713, 11, 1808, 26735, 13], "temperature": 0.0, "avg_logprob": -0.3809753075624124, "compression_ratio": 1.180952380952381, "no_speech_prob": 0.0015991240506991744}, {"id": 213, "seek": 87904, "start": 890.24, "end": 898.8, "text": " Don't do such error handling at home, please.", "tokens": [1468, 380, 360, 1270, 6713, 13175, 412, 1280, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.3809753075624124, "compression_ratio": 1.180952380952381, "no_speech_prob": 0.0015991240506991744}, {"id": 214, "seek": 87904, "start": 898.8, "end": 901.12, "text": " And now let's publish that.", "tokens": [400, 586, 718, 311, 11374, 300, 13], "temperature": 0.0, "avg_logprob": -0.3809753075624124, "compression_ratio": 1.180952380952381, "no_speech_prob": 0.0015991240506991744}, {"id": 215, "seek": 90112, "start": 901.12, "end": 909.32, "text": " The H publisher, publish topic, so let's use bookings, and we need our message.", "tokens": [440, 389, 25088, 11, 11374, 4829, 11, 370, 718, 311, 764, 1446, 1109, 11, 293, 321, 643, 527, 3636, 13], "temperature": 0.0, "avg_logprob": -0.31836390495300293, "compression_ratio": 1.46875, "no_speech_prob": 0.0007618269883096218}, {"id": 216, "seek": 90112, "start": 909.32, "end": 917.0, "text": " Let's remember we need to have UID, so it doesn't matter actually what format of UID", "tokens": [961, 311, 1604, 321, 643, 281, 362, 624, 2777, 11, 370, 309, 1177, 380, 1871, 767, 437, 7877, 295, 624, 2777], "temperature": 0.0, "avg_logprob": -0.31836390495300293, "compression_ratio": 1.46875, "no_speech_prob": 0.0007618269883096218}, {"id": 217, "seek": 90112, "start": 917.0, "end": 923.5600000000001, "text": " it can be, I know, it can be even empty for some plantations, but good luck with debugging,", "tokens": [309, 393, 312, 11, 286, 458, 11, 309, 393, 312, 754, 6707, 337, 512, 3709, 763, 11, 457, 665, 3668, 365, 45592, 11], "temperature": 0.0, "avg_logprob": -0.31836390495300293, "compression_ratio": 1.46875, "no_speech_prob": 0.0007618269883096218}, {"id": 218, "seek": 90112, "start": 923.5600000000001, "end": 925.4, "text": " and room booked payloads.", "tokens": [293, 1808, 26735, 30918, 82, 13], "temperature": 0.0, "avg_logprob": -0.31836390495300293, "compression_ratio": 1.46875, "no_speech_prob": 0.0007618269883096218}, {"id": 219, "seek": 92540, "start": 925.4, "end": 933.64, "text": " All right, and it returns error, so we need to handle that in not a nice way, but it's", "tokens": [1057, 558, 11, 293, 309, 11247, 6713, 11, 370, 321, 643, 281, 4813, 300, 294, 406, 257, 1481, 636, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 220, "seek": 92540, "start": 933.64, "end": 935.04, "text": " live coding, so it's fine.", "tokens": [1621, 17720, 11, 370, 309, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 221, "seek": 92540, "start": 935.04, "end": 937.0799999999999, "text": " All right, so we have the first part.", "tokens": [1057, 558, 11, 370, 321, 362, 264, 700, 644, 13], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 222, "seek": 92540, "start": 937.0799999999999, "end": 944.28, "text": " So we have our room booked event, we're publishing that to the topic bookings, and, okay, so", "tokens": [407, 321, 362, 527, 1808, 26735, 2280, 11, 321, 434, 17832, 300, 281, 264, 4829, 1446, 1109, 11, 293, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 223, "seek": 92540, "start": 944.28, "end": 946.28, "text": " we just need to inject now the publisher.", "tokens": [321, 445, 643, 281, 10711, 586, 264, 25088, 13], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 224, "seek": 92540, "start": 946.28, "end": 950.64, "text": " So let's check where it's created, okay, we no longer need payments.", "tokens": [407, 718, 311, 1520, 689, 309, 311, 2942, 11, 1392, 11, 321, 572, 2854, 643, 14348, 13], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 225, "seek": 92540, "start": 950.64, "end": 953.48, "text": " I heard that Kafka is nice and scalable, so let's use Kafka.", "tokens": [286, 2198, 300, 47064, 307, 1481, 293, 38481, 11, 370, 718, 311, 764, 47064, 13], "temperature": 0.0, "avg_logprob": -0.20942401097825736, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0006866350304335356}, {"id": 226, "seek": 95348, "start": 953.48, "end": 958.76, "text": " I have also snippet for that, it's nothing magical here, it's just this and the water", "tokens": [286, 362, 611, 35623, 302, 337, 300, 11, 309, 311, 1825, 12066, 510, 11, 309, 311, 445, 341, 293, 264, 1281], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 227, "seek": 95348, "start": 958.76, "end": 962.16, "text": " mid-documentation, and let's use this publisher.", "tokens": [2062, 12, 67, 30439, 399, 11, 293, 718, 311, 764, 341, 25088, 13], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 228, "seek": 95348, "start": 962.16, "end": 966.16, "text": " We don't need subscriber yet, but probably we'll need it later.", "tokens": [492, 500, 380, 643, 26122, 1939, 11, 457, 1391, 321, 603, 643, 309, 1780, 13], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 229, "seek": 95348, "start": 966.16, "end": 972.48, "text": " All right, by the way, I'm running some nice Docker Compos under the hood that is recompiling", "tokens": [1057, 558, 11, 538, 264, 636, 11, 286, 478, 2614, 512, 1481, 33772, 6620, 329, 833, 264, 13376, 300, 307, 48000, 4883], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 230, "seek": 95348, "start": 972.48, "end": 975.5600000000001, "text": " the project each time when I'm putting changes there.", "tokens": [264, 1716, 1184, 565, 562, 286, 478, 3372, 2962, 456, 13], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 231, "seek": 95348, "start": 975.5600000000001, "end": 980.32, "text": " At the end of the presentation, I will give you materials with all the source code, and", "tokens": [1711, 264, 917, 295, 264, 5860, 11, 286, 486, 976, 291, 5319, 365, 439, 264, 4009, 3089, 11, 293], "temperature": 0.0, "avg_logprob": -0.23168301789656928, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.0005404028343036771}, {"id": 232, "seek": 98032, "start": 980.32, "end": 985.2, "text": " with the description of how it's done, that it's automatically reloading after each change.", "tokens": [365, 264, 3855, 295, 577, 309, 311, 1096, 11, 300, 309, 311, 6772, 25628, 278, 934, 1184, 1319, 13], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 233, "seek": 98032, "start": 985.2, "end": 989.96, "text": " All right, so we have our publisher, we are publishing our event, so let's check if it", "tokens": [1057, 558, 11, 370, 321, 362, 527, 25088, 11, 321, 366, 17832, 527, 2280, 11, 370, 718, 311, 1520, 498, 309], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 234, "seek": 98032, "start": 989.96, "end": 990.96, "text": " works.", "tokens": [1985, 13], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 235, "seek": 98032, "start": 990.96, "end": 997.0, "text": " Hopefully it will work, okay, so you can see that our API is pretty stable, and let's check", "tokens": [10429, 309, 486, 589, 11, 1392, 11, 370, 291, 393, 536, 300, 527, 9362, 307, 1238, 8351, 11, 293, 718, 311, 1520], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 236, "seek": 98032, "start": 997.0, "end": 1000.5600000000001, "text": " if our event is really published.", "tokens": [498, 527, 2280, 307, 534, 6572, 13], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 237, "seek": 98032, "start": 1000.5600000000001, "end": 1006.6800000000001, "text": " So we'll use mule tool, so mule is part of water mule, as you can guess, and we'll consume", "tokens": [407, 321, 603, 764, 275, 2271, 2290, 11, 370, 275, 2271, 307, 644, 295, 1281, 275, 2271, 11, 382, 291, 393, 2041, 11, 293, 321, 603, 14732], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 238, "seek": 98032, "start": 1006.6800000000001, "end": 1008.4000000000001, "text": " from bookings from Kafka.", "tokens": [490, 1446, 1109, 490, 47064, 13], "temperature": 0.0, "avg_logprob": -0.2336038112640381, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.00014858212671242654}, {"id": 239, "seek": 100840, "start": 1008.4, "end": 1013.52, "text": " Mule is allowing you to consume messages from multiple Pub-Sub types that are supported", "tokens": [376, 2271, 307, 8293, 291, 281, 14732, 7897, 490, 3866, 21808, 12, 39582, 3467, 300, 366, 8104], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 240, "seek": 100840, "start": 1013.52, "end": 1014.52, "text": " in water mule.", "tokens": [294, 1281, 275, 2271, 13], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 241, "seek": 100840, "start": 1014.52, "end": 1020.3199999999999, "text": " I know that there is tool for that in Kafka, but it's not mine, so.", "tokens": [286, 458, 300, 456, 307, 2290, 337, 300, 294, 47064, 11, 457, 309, 311, 406, 3892, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 242, "seek": 100840, "start": 1020.3199999999999, "end": 1024.0, "text": " And yeah, with mule, you can use multiple Pub-Sub types, and okay, as you can see, now", "tokens": [400, 1338, 11, 365, 275, 2271, 11, 291, 393, 764, 3866, 21808, 12, 39582, 3467, 11, 293, 1392, 11, 382, 291, 393, 536, 11, 586], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 243, "seek": 100840, "start": 1024.0, "end": 1026.4, "text": " we have event here, so it seems to work.", "tokens": [321, 362, 2280, 510, 11, 370, 309, 2544, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 244, "seek": 100840, "start": 1026.4, "end": 1028.56, "text": " Okay, so done, thank you.", "tokens": [1033, 11, 370, 1096, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 245, "seek": 100840, "start": 1028.56, "end": 1029.56, "text": " Not really.", "tokens": [1726, 534, 13], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 246, "seek": 100840, "start": 1029.56, "end": 1036.56, "text": " We are not taking payments, so probably if our company will go bankrupt pretty quickly,", "tokens": [492, 366, 406, 1940, 14348, 11, 370, 1391, 498, 527, 2237, 486, 352, 21780, 1238, 2661, 11], "temperature": 0.0, "avg_logprob": -0.24514930228876874, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005776484031230211}, {"id": 247, "seek": 103656, "start": 1036.56, "end": 1039.3999999999999, "text": " so we'll need to start to take payments.", "tokens": [370, 321, 603, 643, 281, 722, 281, 747, 14348, 13], "temperature": 0.0, "avg_logprob": -0.26496779918670654, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.000864293600898236}, {"id": 248, "seek": 103656, "start": 1039.3999999999999, "end": 1049.9199999999998, "text": " So for that, we already have our subscriber, that's good, so let's uncomment that, okay.", "tokens": [407, 337, 300, 11, 321, 1217, 362, 527, 26122, 11, 300, 311, 665, 11, 370, 718, 311, 8585, 518, 300, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.26496779918670654, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.000864293600898236}, {"id": 249, "seek": 103656, "start": 1049.9199999999998, "end": 1064.08, "text": " We need to have water mule router, so message router error, router config, water mule logger,", "tokens": [492, 643, 281, 362, 1281, 275, 2271, 22492, 11, 370, 3636, 22492, 6713, 11, 22492, 6662, 11, 1281, 275, 2271, 3565, 1321, 11], "temperature": 0.0, "avg_logprob": -0.26496779918670654, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.000864293600898236}, {"id": 250, "seek": 106408, "start": 1064.08, "end": 1067.12, "text": " router handling, and now we need to add a handler.", "tokens": [22492, 13175, 11, 293, 586, 321, 643, 281, 909, 257, 41967, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 251, "seek": 106408, "start": 1067.12, "end": 1073.48, "text": " So we'll use addHander, so we'll need to provide handler name, so it will be payments.", "tokens": [407, 321, 603, 764, 909, 39, 4483, 11, 370, 321, 603, 643, 281, 2893, 41967, 1315, 11, 370, 309, 486, 312, 14348, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 252, "seek": 106408, "start": 1073.48, "end": 1078.96, "text": " It doesn't matter really what is the handler name, but again, pretty useful for debugging.", "tokens": [467, 1177, 380, 1871, 534, 437, 307, 264, 41967, 1315, 11, 457, 797, 11, 1238, 4420, 337, 45592, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 253, "seek": 106408, "start": 1078.96, "end": 1079.96, "text": " Subscribe topic.", "tokens": [10611, 4829, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 254, "seek": 106408, "start": 1079.96, "end": 1087.04, "text": " So we're subscribing to the topic that we published this message, so this is bookings.", "tokens": [407, 321, 434, 19981, 281, 264, 4829, 300, 321, 6572, 341, 3636, 11, 370, 341, 307, 1446, 1109, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 255, "seek": 106408, "start": 1087.04, "end": 1092.6399999999999, "text": " Bookings, we need to use subscriber, and we need to publish the topic.", "tokens": [9476, 1109, 11, 321, 643, 281, 764, 26122, 11, 293, 321, 643, 281, 11374, 264, 4829, 13], "temperature": 0.0, "avg_logprob": -0.20326136651440202, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.0005742911598645151}, {"id": 256, "seek": 109264, "start": 1092.64, "end": 1105.96, "text": " So we'll publish event when we succeed to take payments, so payments, publisher, and", "tokens": [407, 321, 603, 11374, 2280, 562, 321, 7754, 281, 747, 14348, 11, 370, 14348, 11, 25088, 11, 293], "temperature": 0.0, "avg_logprob": -0.21970068518795186, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.00022084935335442424}, {"id": 257, "seek": 109264, "start": 1105.96, "end": 1106.96, "text": " handler function.", "tokens": [41967, 2445, 13], "temperature": 0.0, "avg_logprob": -0.21970068518795186, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.00022084935335442424}, {"id": 258, "seek": 109264, "start": 1106.96, "end": 1111.48, "text": " So hopefully you remember handler function signature, so yeah, we are receiving message", "tokens": [407, 4696, 291, 1604, 41967, 2445, 13397, 11, 370, 1338, 11, 321, 366, 10040, 3636], "temperature": 0.0, "avg_logprob": -0.21970068518795186, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.00022084935335442424}, {"id": 259, "seek": 109264, "start": 1111.48, "end": 1118.4, "text": " and we are returning message, but we'll do it in a bit more fancy way, payments handler,", "tokens": [293, 321, 366, 12678, 3636, 11, 457, 321, 603, 360, 309, 294, 257, 857, 544, 10247, 636, 11, 14348, 41967, 11], "temperature": 0.0, "avg_logprob": -0.21970068518795186, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.00022084935335442424}, {"id": 260, "seek": 111840, "start": 1118.4, "end": 1126.24, "text": " because we can inject some dependencies earlier, I need to fix that, and that, all right.", "tokens": [570, 321, 393, 10711, 512, 36606, 3071, 11, 286, 643, 281, 3191, 300, 11, 293, 300, 11, 439, 558, 13], "temperature": 0.0, "avg_logprob": -0.3015318943903996, "compression_ratio": 1.62, "no_speech_prob": 0.0008625498157925904}, {"id": 261, "seek": 111840, "start": 1126.24, "end": 1132.0400000000002, "text": " So we have our payments handler, so we'll receive message, and we'll take payment and", "tokens": [407, 321, 362, 527, 14348, 41967, 11, 370, 321, 603, 4774, 3636, 11, 293, 321, 603, 747, 10224, 293], "temperature": 0.0, "avg_logprob": -0.3015318943903996, "compression_ratio": 1.62, "no_speech_prob": 0.0008625498157925904}, {"id": 262, "seek": 111840, "start": 1132.0400000000002, "end": 1133.4, "text": " emit some event.", "tokens": [32084, 512, 2280, 13], "temperature": 0.0, "avg_logprob": -0.3015318943903996, "compression_ratio": 1.62, "no_speech_prob": 0.0008625498157925904}, {"id": 263, "seek": 111840, "start": 1133.4, "end": 1141.8400000000001, "text": " So we need to have our payment provider, and what?", "tokens": [407, 321, 643, 281, 362, 527, 10224, 12398, 11, 293, 437, 30], "temperature": 0.0, "avg_logprob": -0.3015318943903996, "compression_ratio": 1.62, "no_speech_prob": 0.0008625498157925904}, {"id": 264, "seek": 114184, "start": 1141.84, "end": 1160.08, "text": " We need to have room booked, we need to have our shoulder, so message payload to room booked.", "tokens": [492, 643, 281, 362, 1808, 26735, 11, 321, 643, 281, 362, 527, 7948, 11, 370, 3636, 30918, 281, 1808, 26735, 13], "temperature": 0.0, "avg_logprob": -0.3582658594304865, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0023760658223181963}, {"id": 265, "seek": 114184, "start": 1160.08, "end": 1166.84, "text": " And compared to standard library HTTP handler, you can return errors from a water new handler,", "tokens": [400, 5347, 281, 3832, 6405, 33283, 41967, 11, 291, 393, 2736, 13603, 490, 257, 1281, 777, 41967, 11], "temperature": 0.0, "avg_logprob": -0.3582658594304865, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0023760658223181963}, {"id": 266, "seek": 114184, "start": 1166.84, "end": 1170.24, "text": " so I don't need to panic.", "tokens": [370, 286, 500, 380, 643, 281, 14783, 13], "temperature": 0.0, "avg_logprob": -0.3582658594304865, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0023760658223181963}, {"id": 267, "seek": 117024, "start": 1170.24, "end": 1175.76, "text": " And all right, so we should have the payload that we published here, so that's good, so", "tokens": [400, 439, 558, 11, 370, 321, 820, 362, 264, 30918, 300, 321, 6572, 510, 11, 370, 300, 311, 665, 11, 370], "temperature": 0.0, "avg_logprob": -0.2217562058392693, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0010474310256540775}, {"id": 268, "seek": 117024, "start": 1175.76, "end": 1186.32, "text": " we can now use that to take payment for room booked price, great, great.", "tokens": [321, 393, 586, 764, 300, 281, 747, 10224, 337, 1808, 26735, 3218, 11, 869, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.2217562058392693, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0010474310256540775}, {"id": 269, "seek": 117024, "start": 1186.32, "end": 1189.72, "text": " And as I said, so I would like to also, I need some event, so it may be useful, so if", "tokens": [400, 382, 286, 848, 11, 370, 286, 576, 411, 281, 611, 11, 286, 643, 512, 2280, 11, 370, 309, 815, 312, 4420, 11, 370, 498], "temperature": 0.0, "avg_logprob": -0.2217562058392693, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0010474310256540775}, {"id": 270, "seek": 117024, "start": 1189.72, "end": 1194.36, "text": " you're an intimate event that we took the payment, we can have some BI or we can, I", "tokens": [291, 434, 364, 20215, 2280, 300, 321, 1890, 264, 10224, 11, 321, 393, 362, 512, 23524, 420, 321, 393, 11, 286], "temperature": 0.0, "avg_logprob": -0.2217562058392693, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0010474310256540775}, {"id": 271, "seek": 117024, "start": 1194.36, "end": 1197.76, "text": " don't know, do something else, I mean, I don't know, we can send beer to this person", "tokens": [500, 380, 458, 11, 360, 746, 1646, 11, 286, 914, 11, 286, 500, 380, 458, 11, 321, 393, 2845, 8795, 281, 341, 954], "temperature": 0.0, "avg_logprob": -0.2217562058392693, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0010474310256540775}, {"id": 272, "seek": 119776, "start": 1197.76, "end": 1201.12, "text": " after he booked room, because why not?", "tokens": [934, 415, 26735, 1808, 11, 570, 983, 406, 30], "temperature": 0.0, "avg_logprob": -0.3196390442929025, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.001696147141046822}, {"id": 273, "seek": 119776, "start": 1201.12, "end": 1218.84, "text": " And, okay, so we need the second event, payment taken, payment taken, filled, filled, room", "tokens": [400, 11, 1392, 11, 370, 321, 643, 264, 1150, 2280, 11, 10224, 2726, 11, 10224, 2726, 11, 6412, 11, 6412, 11, 1808], "temperature": 0.0, "avg_logprob": -0.3196390442929025, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.001696147141046822}, {"id": 274, "seek": 119776, "start": 1218.84, "end": 1227.64, "text": " booked, room booked as well as price, and we need to marshal it again to JSON.", "tokens": [26735, 11, 1808, 26735, 382, 731, 382, 3218, 11, 293, 321, 643, 281, 30517, 4947, 309, 797, 281, 31828, 13], "temperature": 0.0, "avg_logprob": -0.3196390442929025, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.001696147141046822}, {"id": 275, "seek": 122764, "start": 1227.64, "end": 1228.64, "text": " Error.", "tokens": [3300, 2874, 13], "temperature": 0.0, "avg_logprob": -0.38724195692274305, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001307064201682806}, {"id": 276, "seek": 122764, "start": 1228.64, "end": 1247.16, "text": " Cool, okay, and the last thing that we need to do is returning message, message as new,", "tokens": [8561, 11, 1392, 11, 293, 264, 1036, 551, 300, 321, 643, 281, 360, 307, 12678, 3636, 11, 3636, 382, 777, 11], "temperature": 0.0, "avg_logprob": -0.38724195692274305, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001307064201682806}, {"id": 277, "seek": 122764, "start": 1247.16, "end": 1254.8400000000001, "text": " message new, UID new string, and payment taken payload.", "tokens": [3636, 777, 11, 624, 2777, 777, 6798, 11, 293, 10224, 2726, 30918, 13], "temperature": 0.0, "avg_logprob": -0.38724195692274305, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001307064201682806}, {"id": 278, "seek": 125484, "start": 1254.84, "end": 1261.32, "text": " I hope that I'm not writing too fast or too slow, all right, so in there, there is a chance", "tokens": [286, 1454, 300, 286, 478, 406, 3579, 886, 2370, 420, 886, 2964, 11, 439, 558, 11, 370, 294, 456, 11, 456, 307, 257, 2931], "temperature": 0.0, "avg_logprob": -0.23722763816909034, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0009183519287034869}, {"id": 279, "seek": 125484, "start": 1261.32, "end": 1265.9199999999998, "text": " that it may work, so what we are doing, so we are receiving our room booked event, we", "tokens": [300, 309, 815, 589, 11, 370, 437, 321, 366, 884, 11, 370, 321, 366, 10040, 527, 1808, 26735, 2280, 11, 321], "temperature": 0.0, "avg_logprob": -0.23722763816909034, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0009183519287034869}, {"id": 280, "seek": 125484, "start": 1265.9199999999998, "end": 1271.72, "text": " are marshaling that, we are taking payment, and when we succeed, we are emitting another", "tokens": [366, 30517, 4947, 278, 300, 11, 321, 366, 1940, 10224, 11, 293, 562, 321, 7754, 11, 321, 366, 846, 2414, 1071], "temperature": 0.0, "avg_logprob": -0.23722763816909034, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0009183519287034869}, {"id": 281, "seek": 125484, "start": 1271.72, "end": 1272.72, "text": " event.", "tokens": [2280, 13], "temperature": 0.0, "avg_logprob": -0.23722763816909034, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0009183519287034869}, {"id": 282, "seek": 125484, "start": 1272.72, "end": 1278.56, "text": " Sounds like a done, so the only thing that we need to do is to reuse that handler, so", "tokens": [14576, 411, 257, 1096, 11, 370, 264, 787, 551, 300, 321, 643, 281, 360, 307, 281, 26225, 300, 41967, 11, 370], "temperature": 0.0, "avg_logprob": -0.23722763816909034, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0009183519287034869}, {"id": 283, "seek": 127856, "start": 1278.56, "end": 1289.6399999999999, "text": " we have that one, and handler, cool, let's check if it compiles, it even compiles, so", "tokens": [321, 362, 300, 472, 11, 293, 41967, 11, 1627, 11, 718, 311, 1520, 498, 309, 715, 4680, 11, 309, 754, 715, 4680, 11, 370], "temperature": 0.0, "avg_logprob": -0.23619528074522275, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0008452632464468479}, {"id": 284, "seek": 127856, "start": 1289.6399999999999, "end": 1296.2, "text": " let's check if it's working, so let's book a couple rooms, and the idea is that by default", "tokens": [718, 311, 1520, 498, 309, 311, 1364, 11, 370, 718, 311, 1446, 257, 1916, 9396, 11, 293, 264, 1558, 307, 300, 538, 7576], "temperature": 0.0, "avg_logprob": -0.23619528074522275, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0008452632464468479}, {"id": 285, "seek": 127856, "start": 1296.2, "end": 1308.52, "text": " WaterMe handler will try if the payment provider failed, so in there we should see some information", "tokens": [8772, 12671, 41967, 486, 853, 498, 264, 10224, 12398, 7612, 11, 370, 294, 456, 321, 820, 536, 512, 1589], "temperature": 0.0, "avg_logprob": -0.23619528074522275, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0008452632464468479}, {"id": 286, "seek": 130852, "start": 1308.52, "end": 1314.52, "text": " that payment was taken, and we don't see that, I don't, I know why we don't see that, because", "tokens": [300, 10224, 390, 2726, 11, 293, 321, 500, 380, 536, 300, 11, 286, 500, 380, 11, 286, 458, 983, 321, 500, 380, 536, 300, 11, 570], "temperature": 0.0, "avg_logprob": -0.2378831554103542, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00043375417590141296}, {"id": 287, "seek": 130852, "start": 1314.52, "end": 1326.28, "text": " we didn't start at router, run, context, error, it's a bit naive implementation because it's", "tokens": [321, 994, 380, 722, 412, 22492, 11, 1190, 11, 4319, 11, 6713, 11, 309, 311, 257, 857, 29052, 11420, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.2378831554103542, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00043375417590141296}, {"id": 288, "seek": 130852, "start": 1326.28, "end": 1332.12, "text": " not really graceful shutdown, but what in the documentation, as I remember, we have", "tokens": [406, 534, 10042, 906, 34927, 11, 457, 437, 294, 264, 14333, 11, 382, 286, 1604, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.2378831554103542, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00043375417590141296}, {"id": 289, "seek": 133212, "start": 1332.12, "end": 1338.84, "text": " examples with real graceful shutdown, so, okay, and let's see, okay, so we have some", "tokens": [5110, 365, 957, 10042, 906, 34927, 11, 370, 11, 1392, 11, 293, 718, 311, 536, 11, 1392, 11, 370, 321, 362, 512], "temperature": 0.0, "avg_logprob": -0.21624460407331877, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.00036813964834436774}, {"id": 290, "seek": 133212, "start": 1338.84, "end": 1348.3999999999999, "text": " random error, and you can see payment taken, hooray, our company is saved, all right, so", "tokens": [4974, 6713, 11, 293, 291, 393, 536, 10224, 2726, 11, 43330, 320, 11, 527, 2237, 307, 6624, 11, 439, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.21624460407331877, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.00036813964834436774}, {"id": 291, "seek": 133212, "start": 1348.3999999999999, "end": 1354.56, "text": " this is working, but there's one problem with that, so now we figure out that, okay, actually", "tokens": [341, 307, 1364, 11, 457, 456, 311, 472, 1154, 365, 300, 11, 370, 586, 321, 2573, 484, 300, 11, 1392, 11, 767], "temperature": 0.0, "avg_logprob": -0.21624460407331877, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.00036813964834436774}, {"id": 292, "seek": 133212, "start": 1354.56, "end": 1360.76, "text": " Kafka is a bit hard to run, and we are on GCP, so maybe we can just Google it, so I think", "tokens": [47064, 307, 257, 857, 1152, 281, 1190, 11, 293, 321, 366, 322, 460, 20049, 11, 370, 1310, 321, 393, 445, 3329, 309, 11, 370, 286, 519], "temperature": 0.0, "avg_logprob": -0.21624460407331877, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.00036813964834436774}, {"id": 293, "seek": 136076, "start": 1360.76, "end": 1365.84, "text": " that I can change Kafka implementation to Google, it pops up in one minute, I'm rewriting", "tokens": [300, 286, 393, 1319, 47064, 11420, 281, 3329, 11, 309, 16795, 493, 294, 472, 3456, 11, 286, 478, 319, 19868], "temperature": 0.0, "avg_logprob": -0.2552671806485045, "compression_ratio": 1.3410852713178294, "no_speech_prob": 0.001170682837255299}, {"id": 294, "seek": 136076, "start": 1365.84, "end": 1378.76, "text": " the bar today, hi, but I think that I can do that, let's start the timer, one, two,", "tokens": [264, 2159, 965, 11, 4879, 11, 457, 286, 519, 300, 286, 393, 360, 300, 11, 718, 311, 722, 264, 19247, 11, 472, 11, 732, 11], "temperature": 0.0, "avg_logprob": -0.2552671806485045, "compression_ratio": 1.3410852713178294, "no_speech_prob": 0.001170682837255299}, {"id": 295, "seek": 137876, "start": 1378.76, "end": 1401.72, "text": " three, okay, let's check, I think I did that, so let's book, and okay, payment taken, we", "tokens": [1045, 11, 1392, 11, 718, 311, 1520, 11, 286, 519, 286, 630, 300, 11, 370, 718, 311, 1446, 11, 293, 1392, 11, 10224, 2726, 11, 321], "temperature": 0.0, "avg_logprob": -0.2348015308380127, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.0012403822038322687}, {"id": 296, "seek": 140172, "start": 1401.72, "end": 1418.0, "text": " can double check, so let's use meal, and let's consume bookings, you see, it works, all right,", "tokens": [393, 3834, 1520, 11, 370, 718, 311, 764, 6791, 11, 293, 718, 311, 14732, 1446, 1109, 11, 291, 536, 11, 309, 1985, 11, 439, 558, 11], "temperature": 0.0, "avg_logprob": -0.18438340440581116, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.00024700272479094565}, {"id": 297, "seek": 140172, "start": 1418.0, "end": 1424.2, "text": " so it will be that from live coding, one last thing that I would like to show you, because", "tokens": [370, 309, 486, 312, 300, 490, 1621, 17720, 11, 472, 1036, 551, 300, 286, 576, 411, 281, 855, 291, 11, 570], "temperature": 0.0, "avg_logprob": -0.18438340440581116, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.00024700272479094565}, {"id": 298, "seek": 140172, "start": 1424.2, "end": 1429.4, "text": " you may notice that, okay, it's a lot of boring JSON there, et cetera, et cetera, you may", "tokens": [291, 815, 3449, 300, 11, 1392, 11, 309, 311, 257, 688, 295, 9989, 31828, 456, 11, 1030, 11458, 11, 1030, 11458, 11, 291, 815], "temperature": 0.0, "avg_logprob": -0.18438340440581116, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.00024700272479094565}, {"id": 299, "seek": 142940, "start": 1429.4, "end": 1433.8000000000002, "text": " notice that I don't like boring stuff, because probably there are more interesting things", "tokens": [3449, 300, 286, 500, 380, 411, 9989, 1507, 11, 570, 1391, 456, 366, 544, 1880, 721], "temperature": 0.0, "avg_logprob": -0.20543527181169627, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0007496040198020637}, {"id": 300, "seek": 142940, "start": 1433.8000000000002, "end": 1440.8400000000001, "text": " to do than marshalling to JSON, so that's because of that we created a component that", "tokens": [281, 360, 813, 21653, 24021, 281, 31828, 11, 370, 300, 311, 570, 295, 300, 321, 2942, 257, 6542, 300], "temperature": 0.0, "avg_logprob": -0.20543527181169627, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0007496040198020637}, {"id": 301, "seek": 142940, "start": 1440.8400000000001, "end": 1447.48, "text": " is named CQRS component, and the idea is that instead of doing this JSON-marshall and all", "tokens": [307, 4926, 383, 48, 43580, 6542, 11, 293, 264, 1558, 307, 300, 2602, 295, 884, 341, 31828, 12, 76, 7064, 336, 293, 439], "temperature": 0.0, "avg_logprob": -0.20543527181169627, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0007496040198020637}, {"id": 302, "seek": 142940, "start": 1447.48, "end": 1452.6000000000001, "text": " that stuff, you can provide configuration to which format you would like to marshall everything,", "tokens": [300, 1507, 11, 291, 393, 2893, 11694, 281, 597, 7877, 291, 576, 411, 281, 21653, 336, 1203, 11], "temperature": 0.0, "avg_logprob": -0.20543527181169627, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0007496040198020637}, {"id": 303, "seek": 142940, "start": 1452.6000000000001, "end": 1457.68, "text": " and under the hood it would be done, so you can use JSON, you can use Protobuf, Avro,", "tokens": [293, 833, 264, 13376, 309, 576, 312, 1096, 11, 370, 291, 393, 764, 31828, 11, 291, 393, 764, 10019, 996, 2947, 11, 11667, 340, 11], "temperature": 0.0, "avg_logprob": -0.20543527181169627, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0007496040198020637}, {"id": 304, "seek": 145768, "start": 1457.68, "end": 1462.1200000000001, "text": " I don't know, even something custom if you really want, the idea is that you're only", "tokens": [286, 500, 380, 458, 11, 754, 746, 2375, 498, 291, 534, 528, 11, 264, 1558, 307, 300, 291, 434, 787], "temperature": 0.0, "avg_logprob": -0.21228760939378005, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.00044499413343146443}, {"id": 305, "seek": 145768, "start": 1462.1200000000001, "end": 1467.0, "text": " implementing this interface, so you're providing the name of the handler, you are providing", "tokens": [18114, 341, 9226, 11, 370, 291, 434, 6530, 264, 1315, 295, 264, 41967, 11, 291, 366, 6530], "temperature": 0.0, "avg_logprob": -0.21228760939378005, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.00044499413343146443}, {"id": 306, "seek": 145768, "start": 1467.0, "end": 1473.5600000000002, "text": " the event that you are expecting to receive, so in that case it will be room-booked, and", "tokens": [264, 2280, 300, 291, 366, 9650, 281, 4774, 11, 370, 294, 300, 1389, 309, 486, 312, 1808, 12, 2939, 292, 11, 293], "temperature": 0.0, "avg_logprob": -0.21228760939378005, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.00044499413343146443}, {"id": 307, "seek": 145768, "start": 1473.5600000000002, "end": 1478.44, "text": " you may notice that it was pre-generic, so we have the interface here, but we are working", "tokens": [291, 815, 3449, 300, 309, 390, 659, 12, 21848, 299, 11, 370, 321, 362, 264, 9226, 510, 11, 457, 321, 366, 1364], "temperature": 0.0, "avg_logprob": -0.21228760939378005, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.00044499413343146443}, {"id": 308, "seek": 145768, "start": 1478.44, "end": 1485.72, "text": " on the newer version, and you are just receiving this event, zero, un-marshalling, or whatever,", "tokens": [322, 264, 17628, 3037, 11, 293, 291, 366, 445, 10040, 341, 2280, 11, 4018, 11, 517, 12, 76, 7064, 24021, 11, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.21228760939378005, "compression_ratio": 1.7896825396825398, "no_speech_prob": 0.00044499413343146443}, {"id": 309, "seek": 148572, "start": 1485.72, "end": 1491.8, "text": " and the same is going when you are publishing an event, so you are just providing the struct", "tokens": [293, 264, 912, 307, 516, 562, 291, 366, 17832, 364, 2280, 11, 370, 291, 366, 445, 6530, 264, 6594], "temperature": 0.0, "avg_logprob": -0.17116799127487908, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00044573473860509694}, {"id": 310, "seek": 148572, "start": 1491.8, "end": 1495.88, "text": " and watermill under the hood is doing all the marshalling stuff.", "tokens": [293, 1281, 18841, 833, 264, 13376, 307, 884, 439, 264, 21653, 24021, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17116799127487908, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00044573473860509694}, {"id": 311, "seek": 148572, "start": 1495.88, "end": 1503.08, "text": " Okay, so I think that will be all for live coding, it looks that I was lucky this time", "tokens": [1033, 11, 370, 286, 519, 300, 486, 312, 439, 337, 1621, 17720, 11, 309, 1542, 300, 286, 390, 6356, 341, 565], "temperature": 0.0, "avg_logprob": -0.17116799127487908, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00044573473860509694}, {"id": 312, "seek": 148572, "start": 1503.08, "end": 1509.2, "text": " that everything worked, and yeah, of course it's still not production-grade implementation,", "tokens": [300, 1203, 2732, 11, 293, 1338, 11, 295, 1164, 309, 311, 920, 406, 4265, 12, 8692, 11420, 11], "temperature": 0.0, "avg_logprob": -0.17116799127487908, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00044573473860509694}, {"id": 313, "seek": 148572, "start": 1509.2, "end": 1514.72, "text": " I mean it's even hard to create a production-grade implementation of HTTP server, so it's more", "tokens": [286, 914, 309, 311, 754, 1152, 281, 1884, 257, 4265, 12, 8692, 11420, 295, 33283, 7154, 11, 370, 309, 311, 544], "temperature": 0.0, "avg_logprob": -0.17116799127487908, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00044573473860509694}, {"id": 314, "seek": 151472, "start": 1514.72, "end": 1519.44, "text": " kind of inspiration to look deeper and see that, okay, it's not that scary, but you need", "tokens": [733, 295, 10249, 281, 574, 7731, 293, 536, 300, 11, 1392, 11, 309, 311, 406, 300, 6958, 11, 457, 291, 643], "temperature": 0.0, "avg_logprob": -0.2646608182362148, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.0004008801479358226}, {"id": 315, "seek": 151472, "start": 1519.44, "end": 1524.3600000000001, "text": " to take into consideration that there are things like Kafka and Google Cloud pops-up", "tokens": [281, 747, 666, 12381, 300, 456, 366, 721, 411, 47064, 293, 3329, 8061, 16795, 12, 1010], "temperature": 0.0, "avg_logprob": -0.2646608182362148, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.0004008801479358226}, {"id": 316, "seek": 151472, "start": 1524.3600000000001, "end": 1529.2, "text": " internals, what is once delivery, actually shown the secure component, but I didn't", "tokens": [2154, 1124, 11, 437, 307, 1564, 8982, 11, 767, 4898, 264, 7144, 6542, 11, 457, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.2646608182362148, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.0004008801479358226}, {"id": 317, "seek": 151472, "start": 1529.2, "end": 1535.92, "text": " call that, but it's helping a bit, so where you should start, because okay, it may be", "tokens": [818, 300, 11, 457, 309, 311, 4315, 257, 857, 11, 370, 689, 291, 820, 722, 11, 570, 1392, 11, 309, 815, 312], "temperature": 0.0, "avg_logprob": -0.2646608182362148, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.0004008801479358226}, {"id": 318, "seek": 151472, "start": 1535.92, "end": 1540.76, "text": " a lot of sources for you, and a lot of stuff to check, so I heard that we have pretty nice", "tokens": [257, 688, 295, 7139, 337, 291, 11, 293, 257, 688, 295, 1507, 281, 1520, 11, 370, 286, 2198, 300, 321, 362, 1238, 1481], "temperature": 0.0, "avg_logprob": -0.2646608182362148, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.0004008801479358226}, {"id": 319, "seek": 154076, "start": 1540.76, "end": 1546.72, "text": " documentation, so we don't have any consulting or whatever for watermill, so we kind of don't", "tokens": [14333, 11, 370, 321, 500, 380, 362, 604, 23682, 420, 2035, 337, 1281, 18841, 11, 370, 321, 733, 295, 500, 380], "temperature": 0.0, "avg_logprob": -0.21532805593390214, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.0005035740905441344}, {"id": 320, "seek": 154076, "start": 1546.72, "end": 1552.24, "text": " care to have bad documentation, so yeah, I heard that we have pretty good documentation,", "tokens": [1127, 281, 362, 1578, 14333, 11, 370, 1338, 11, 286, 2198, 300, 321, 362, 1238, 665, 14333, 11], "temperature": 0.0, "avg_logprob": -0.21532805593390214, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.0005035740905441344}, {"id": 321, "seek": 154076, "start": 1552.24, "end": 1560.2, "text": " so at the end of the presentation it will be in the link, what else, we have also a", "tokens": [370, 412, 264, 917, 295, 264, 5860, 309, 486, 312, 294, 264, 2113, 11, 437, 1646, 11, 321, 362, 611, 257], "temperature": 0.0, "avg_logprob": -0.21532805593390214, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.0005035740905441344}, {"id": 322, "seek": 154076, "start": 1560.2, "end": 1569.6, "text": " lot of examples in watermill, so I will encourage you to, it's black, oh, live coding, okay,", "tokens": [688, 295, 5110, 294, 1281, 18841, 11, 370, 286, 486, 5373, 291, 281, 11, 309, 311, 2211, 11, 1954, 11, 1621, 17720, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.21532805593390214, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.0005035740905441344}, {"id": 323, "seek": 156960, "start": 1569.6, "end": 1574.3999999999999, "text": " it's not live coding, not only live coding, it's risky, so yeah, we have a lot of examples", "tokens": [309, 311, 406, 1621, 17720, 11, 406, 787, 1621, 17720, 11, 309, 311, 21137, 11, 370, 1338, 11, 321, 362, 257, 688, 295, 5110], "temperature": 0.0, "avg_logprob": -0.20361135687146867, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.0003440151340328157}, {"id": 324, "seek": 156960, "start": 1574.3999999999999, "end": 1578.3999999999999, "text": " that probably you cannot see because it's on the black, but you need to believe me that", "tokens": [300, 1391, 291, 2644, 536, 570, 309, 311, 322, 264, 2211, 11, 457, 291, 643, 281, 1697, 385, 300], "temperature": 0.0, "avg_logprob": -0.20361135687146867, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.0003440151340328157}, {"id": 325, "seek": 156960, "start": 1578.3999999999999, "end": 1583.84, "text": " this is on the watermill repository, at this point I wanted to say a big thank you to all", "tokens": [341, 307, 322, 264, 1281, 18841, 25841, 11, 412, 341, 935, 286, 1415, 281, 584, 257, 955, 1309, 291, 281, 439], "temperature": 0.0, "avg_logprob": -0.20361135687146867, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.0003440151340328157}, {"id": 326, "seek": 156960, "start": 1583.84, "end": 1590.04, "text": " watermill contributors, because without you it wouldn't be like it's now, and it's not", "tokens": [1281, 18841, 45627, 11, 570, 1553, 291, 309, 2759, 380, 312, 411, 309, 311, 586, 11, 293, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.20361135687146867, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.0003440151340328157}, {"id": 327, "seek": 156960, "start": 1590.04, "end": 1595.12, "text": " an announcement that we actually released watermill 1.2 after having too many release", "tokens": [364, 12847, 300, 321, 767, 4736, 1281, 18841, 502, 13, 17, 934, 1419, 886, 867, 4374], "temperature": 0.0, "avg_logprob": -0.20361135687146867, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.0003440151340328157}, {"id": 328, "seek": 159512, "start": 1595.12, "end": 1599.6399999999999, "text": " candidates, so yeah, finally it's released, and you are all invited to an online release", "tokens": [11255, 11, 370, 1338, 11, 2721, 309, 311, 4736, 11, 293, 291, 366, 439, 9185, 281, 364, 2950, 4374], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 329, "seek": 159512, "start": 1599.6399999999999, "end": 1605.1999999999998, "text": " party, and we will say what are the new features, and it will be on March 1st, on the last link", "tokens": [3595, 11, 293, 321, 486, 584, 437, 366, 264, 777, 4122, 11, 293, 309, 486, 312, 322, 6129, 502, 372, 11, 322, 264, 1036, 2113], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 330, "seek": 159512, "start": 1605.1999999999998, "end": 1610.4399999999998, "text": " it will be also linked for that, and I think that will be also, this is the, again it's", "tokens": [309, 486, 312, 611, 9408, 337, 300, 11, 293, 286, 519, 300, 486, 312, 611, 11, 341, 307, 264, 11, 797, 309, 311], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 331, "seek": 159512, "start": 1610.4399999999998, "end": 1614.8799999999999, "text": " not working, oh, yeah, so this is the link that I promised to give you, the bonus that", "tokens": [406, 1364, 11, 1954, 11, 1338, 11, 370, 341, 307, 264, 2113, 300, 286, 10768, 281, 976, 291, 11, 264, 10882, 300], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 332, "seek": 159512, "start": 1614.8799999999999, "end": 1620.6, "text": " I have, I have super fancy holographic sticker notes, I'm sure that you don't have sticker", "tokens": [286, 362, 11, 286, 362, 1687, 10247, 38541, 2662, 299, 20400, 5570, 11, 286, 478, 988, 300, 291, 500, 380, 362, 20400], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 333, "seek": 159512, "start": 1620.6, "end": 1624.36, "text": " notes, laptop stickers, so I'm sure that you don't have holographic ones, so if you don't", "tokens": [5570, 11, 10732, 21019, 11, 370, 286, 478, 988, 300, 291, 500, 380, 362, 38541, 2662, 299, 2306, 11, 370, 498, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.16987165668666762, "compression_ratio": 1.9852941176470589, "no_speech_prob": 0.0005853140028193593}, {"id": 334, "seek": 162436, "start": 1624.36, "end": 1629.32, "text": " have, so I have a lot of them, and yeah, I think that would be all, so thank you very", "tokens": [362, 11, 370, 286, 362, 257, 688, 295, 552, 11, 293, 1338, 11, 286, 519, 300, 576, 312, 439, 11, 370, 1309, 291, 588], "temperature": 0.0, "avg_logprob": -0.25623551282015716, "compression_ratio": 1.32, "no_speech_prob": 0.002341768704354763}, {"id": 335, "seek": 162436, "start": 1629.32, "end": 1630.32, "text": " much for your attention.", "tokens": [709, 337, 428, 3202, 13], "temperature": 0.0, "avg_logprob": -0.25623551282015716, "compression_ratio": 1.32, "no_speech_prob": 0.002341768704354763}, {"id": 336, "seek": 162436, "start": 1630.32, "end": 1631.32, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.25623551282015716, "compression_ratio": 1.32, "no_speech_prob": 0.002341768704354763}, {"id": 337, "seek": 163132, "start": 1631.32, "end": 1657.32, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.864386967250279, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.003208072856068611}], "language": "en"}