{"text": " Hello, everybody, and welcome to the second talk of the Python Dev Room, where Vadim Markovstev will be talking to you about accelerating object serialization using constraints. I'm really happy to welcome him here, and I hope you will enjoy this talk. Thank you. So let's start with a short bio. I'm a backend developer. I'm also a machine learning engineer. Used to be a Google developer expert in machine learning before the COVID. I call it in quite a few languages in the last 12 months, mostly in Python and Go, a bit of cc++. And I work in Athenian. This is a startup. It does a software service product for engineering leaders. So the talk will be divided into two parts. The first will be about some custom binary serialization that I had to do in my daily job. And the second will be about speeding up JSON dump. So the first is about how I coded something that was working faster than people by many, many times. And why I had to do that? Well, apparently because I had a use case that didn't really fit well into regular, pickling into regular serialization of pandas data frames. And it's not necessarily a best practice to materialize a huge pandas data frame while you're serving a request. But nevertheless, I had to do that. And it was really big, a few megabytes at least. And I noticed that according to the traces, picking it just took too much time. And we really needed to put it into distributed cache. Otherwise, you would spend extra time recomputing it in every request. And that data frame was quite special. I mean, it's not a usual bunch of integers and floating points. No, it was quite complex. It contained strings, it contained daytime columns, could be numpy daytime, could be just regular Python objects, contained nested lists and digs, and even nested numpy arrays. So it was quite complex. And this was mostly the reason why regular serialization, arrow and everything worked too slow in serializing it. So I came up with a custom binary format. And although it's not really open, it's also public and everyone can study the source code. It's not universal. Really, it only supports the types inside the pandas data frame that we had to serialize at work. It's not backward compatible. It's not portable at all. It doesn't fit well into different CPU architectures or it always assumes Linux and it always assumes CPython 3.11. And this is quite important. I'll talk about it a little bit later. So this is generally a really, really bad idea. And you only do that when you don't have any other options. And you need to squeeze a lot of performance in the backend. However, on the bright side, it was quite compact code. Cytan, because performance. It's a single pass serializer really, really, really fast. And also it releases Jill. This is something that regular people or arrow cannot do for the constraint that they had to be universal and pickle everything, any object. As soon as you have an assumption of what kind of object you can serialize, you can call it in Cytan in such a way that you release Jill and it works even faster. And also other code to execute in parallel. And releasing Jill is probably the hardest feature in this serializer because it was more labor intensive that I initially thought. You see Cytan offers C API wrapper for a lot of APIs inside CPython API. And they are great. However, as soon as you release the Jill, you cannot use them. Just Cytan doesn't compile it. And this is a good thing because to use these wrappers without Jill, you need to be absolutely sure that you are doing the right thing, that you are not using the heap, that you're not writing to any Python objects, also that those Python objects are not mutated somewhere else. This is all true for our backend, but it can be different in other use cases. So this is what happens. And what if there is no C API at all or some C API wrapper missing? You have to re-inclimate it from scratch in Cytan. And this is what we had to do quite much. Final note is that, for example, PyPy has a garbage collector that can relocate objects from time to time. So the pointer to the object can change. And this is why this serializer just doesn't work with PyPy even in Siri because it always assumes that as soon as you have a pointer to Python object, it never changes. Anyway, this is how the high-level serialization of pandas works. And this is not really specific to my serializer or your pickling or this suggests how it works. So every pandas data frame currently in 1.x has a block manager that maps column names to different blocks. Every block has the same data type. And this is a two-dimensional non-PRA underneath. This 2D non-PRA doesn't know anything about what columns are, what are they named. It's just the internal storage, let's say. And whenever you reach a column in the in the data frame, you use the block manager to access them and do some operations. It works fast, yes, and it's also memory efficient. Every operation on a single column executes on a non-PRA underneath, so it's really efficient. And it also allows us to read this pandas data frame and serialize it as a bunch of non-PRAs without Jill. So this is great for us. One thing that we had to re-sync properly is how do we serialize object data type in non-PRAs. And we, again, don't support all use cases, only those that we know exist in our backend. This is some site on pseudo code that shows some general idea how we do this serialization. There is a function called PyArrayData that returns you a row pointer to underlying data in the non-PRA. Then you iterate by lengths. We only support one-dimensional arrays so far, like every column, and we serialize each PyObject independently. And this PyObject can be in Siri arbitrarily, but, again, in our backend, there is a predefined list of things that can be stored like a string, a daytime, an integer, and so on. When we have to iterate standard library containers, such as a list or dictionary, it turned out, again, quite simple. You use PyListGetSize or PyListGetItem. For Dict, you use PyDictNextIterator. This is so great. If you didn't have to release the JIL, you would just use list or Dict type and site. And site would generate somewhat similar code, also efficient. But, again, since we released the JIL, we have to do, let's say, heavy lifting from scratch. For serializing integers and floats, you just convert PyObject to C type, like double or long. If it's a non-P scalar, a non-P scalar is usually a structure, you use a special non-PC API for extracting the scalar value, and you just memory copy it to your output stream. So, this is why we are not portable in the CPU, for example, because the order of bytes in the integer, this endianness, can be whatever. For Intel, it's little endian. For ARM, it can be little or big. So, since our backend always works on the same kind of CPU, and we are really sure about that, we can do these things without caring about endianness and converting to some other order, network order, and so on. And there's also the reason why it works so fast. To serialize a string, it's also nothing special. I have to say a few words about how strings are stored in C Python internally. It's quite smart, and I'm honestly really impressed how C Python does it. It stores strings in three different ways, in one byte encoding, in two byte encoding, and four byte encoding. And it dynamically chooses the encoding based on the maximum number of the character that has to be stored in the string. So, let's say if you only have ASCII characters, you choose one byte encoding and you're super memory efficient. If you have some crazy emojis, then you choose four byte encoding. And yes, if there are other characters, it's not so memory efficient. At the same time, when you address a string by an index, it works super fast because every character is guaranteed to have the same number of bytes underneath. Anyway, to copy a string to binary output format, you just take the row pointer to the string contents. And for the size, you get the number of bytes per character and multiply it by the length of the string. So, again, nothing special in Siri, but you have to be aware of the internals. To serialize daytime and time delta, C Python provides a special C API as well. It's worth mentioning that internal representation of daytime and time delta in Python is not a timestamp. It's not a single integer like in other programming languages. It's really a struct that contains the number of year, the number of months, the number of days, seconds, and so on. So, you just use these getters and you serialize each integer to the output. Worse fast. The same for time delta, you get days and seconds. All of that allowed us to speed up pandas array, pandas data frame serialization by roughly 20x. And, of course, the speed up can be really different depending on the nature of the data frame. If it only contains integers and floating points, the speed up will be marginal. Pickle actually is working quite fast, so it works well for strongly typed numpy arrays, actually. So, does it mean that we kind of managed to beat Pickle? Well, yes and no, because Pickle is universal and it can do all things at once. And it's way harder to be fast at all possible use cases than be super fast at one particular use case. So, I don't know how to say. Maybe yes, maybe no. But, however, there is also an elephant in the room. Can you see it? So far, I only talked about how fast we were at serializing objects, serializing data frames. But supposedly, this is not what you do. Only that you do, you also need to deserialize them back. Otherwise, that will be this anecdote for a compressor that compresses everything to one byte, just kind of decompress back. And decompressing back from this format is also complex, but since I don't have much time to talk about it, I'd rather leave it for next time. But in brief, this is the place where I had to do some extra actions to stay performing. So, yeah. The second part of the talk, how we solved a similar problem, but for Jason's serialization in the backend, this is what we had. A lot of models like a data class with some fields inside, strings, floating points, nested lists, nested models. It could be several levels deep. If you work with a first API that can seem familiar, if I'm not mistaken, by-identic, can offer similar structure. So, it's not really about using data class or something else. A lot of us saw that. And the problem was we had thousands of such objects. And we needed to cache them and surf pagination. Again, this is probably an anti-pattern in a way, because you would ideally push down all the filters in the backend so that you don't have to materialize the whole list of objects to return. But in our case, we really had to do that. So, we loaded all the models from cache. Then we deserialized the bytes to the actual Python objects, these data classes. We took on a dose that corresponded to the pagination and we converted them to Jason's string. So, two things went wrong here. Deserializing everything was super slow and also converting to Jason was also slow. Our first attempt to fix it was actually lightweight. Let's just pre-convert all data classes to atomic objects, like dictionaries and lists. C Python is really, really, really fast. It is working with Dx and lists. So, we assumed that that would help. Then you just store the cache, those atomic basic objects, and you return those that's requested in the pagination. So, that was for the first call, where you create the cache. For the next call, you just serve those objects that are corresponding to the pagination. However, it was still slow. It was slow because conversion from data classes to basic objects through data classes to dict was really slow and painfully slow. And serializing basic objects to Jason was also kind of slow. And just for the subsequent calls, we had problems with deserializing all the objects. It was not great. Materializing a lot of Python objects requires you to do a lot of rev count increments and it just cannot work fast. There is no way it can work fast. So, we had to be inventive and add this to Jason extra value function that pre-serializes all the objects to Jason and also produces the index where each object begins. And this table of contents can be used in the future. When you have pagination calls, you just select the part of this huge Jason blob corresponding to the pagination and you return it. And since you only work with strings, this works fast. However, it really depends on the performance of this function that converts data classes to this huge Jason string with a table of contents. And, yeah, this can be skipped really. This function had to be implemented from scratch, unfortunately, because Jason dumps doesn't specify the table of contents and it cannot do that. Also, to convert data classes to Jason, you still need to convert them to basic objects first using 2D. Jason dumps cannot convert data classes directly. And the only way for us to move on was to really write our own serializer. And this serializer works using a so-called specification of the serialization. The thing is these data classes are typed and you can scan these types to build some plan, how you should perform this serialization, how you should iterate the objects and how you should write them to Jason. Apart from that, we had to implement iterating lists and dicks. Well, we kind of already covered that. Converting integers and floating points to a string. This is really basic. Into stir, float to stir and others. We just don't think about it when you work with them in Python. You just convert them to string, problem solved. However, since it works with HIP and it touches the internal sort of Python, you cannot use it inside and you have to reimplement it from scratch. The same is daytime. Finally, for strings, you need to escape them. If you have a double column inside a string, you need to escape it, the same about new line. And since Jason is UTF-8 and internal binary encoding of Python strings can be one byte, two byte, four byte, we also need to do this conversion. So this is quite interesting. So the main trick to serialize is if you have slots in your data class, these slots are stored as pointers inside the PyObject structure. You take the type of data class, you get the slot members through C API, and each member contains the byte offset where the pointers exist. You just read these pointers and you serialize the objects. Therefore, the serialization spec is recursive. You have the data type that you need to convert to JSON and you have a list of nested specifications with some slot offset. We only support predefined types like lists, floating points, strings, boolean values, times, not everything. We are not universally have constraints, but it allows us to perform faster. Oh, the code listing didn't load for some reason. Anyway, I don't have time for that. This serializer appeared to be 100x faster, really, really, really fast, so fast that we just don't see it in our traces and profile anymore. So the goal was achieved. And it can be improved in the future by pre-computing the serialization spec inside the data class member. Final advice is doing less is doing more. I think it's also mentioned in Zen of Python, probably. Know your tools. It's always a great idea to know your tools and learn the internals, how they work, because it will allow you to write more performance programs. Going low level when you learn the internals is your super weapon that allows you to do many things. You can break the rules and you can do magic. However, you should only do that if you've got your architecture right. Otherwise, it's just stupid. You optimize the place that shouldn't be optimized in the first place. And also, with great power comes consequences, because when we release the GIL, we cannot use almost all the standard library and we have to implement crazy things like UTF conversions. This is annoying and you need to be prepared for that. Thanks. you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.48, "text": " Hello, everybody, and welcome to the second talk of the Python Dev Room, where Vadim", "tokens": [50364, 2425, 11, 2201, 11, 293, 2928, 281, 264, 1150, 751, 295, 264, 15329, 9096, 19190, 11, 689, 24788, 332, 50988], "temperature": 0.0, "avg_logprob": -0.3782358169555664, "compression_ratio": 1.4, "no_speech_prob": 0.33812257647514343}, {"id": 1, "seek": 0, "start": 12.48, "end": 18.16, "text": " Markovstev will be talking to you about accelerating object serialization using constraints.", "tokens": [50988, 3934, 5179, 2941, 85, 486, 312, 1417, 281, 291, 466, 34391, 2657, 17436, 2144, 1228, 18491, 13, 51272], "temperature": 0.0, "avg_logprob": -0.3782358169555664, "compression_ratio": 1.4, "no_speech_prob": 0.33812257647514343}, {"id": 2, "seek": 0, "start": 18.16, "end": 25.28, "text": " I'm really happy to welcome him here, and I hope you will enjoy this talk.", "tokens": [51272, 286, 478, 534, 2055, 281, 2928, 796, 510, 11, 293, 286, 1454, 291, 486, 2103, 341, 751, 13, 51628], "temperature": 0.0, "avg_logprob": -0.3782358169555664, "compression_ratio": 1.4, "no_speech_prob": 0.33812257647514343}, {"id": 3, "seek": 2528, "start": 25.52, "end": 29.520000000000003, "text": " Thank you.", "tokens": [50376, 1044, 291, 13, 50576], "temperature": 0.0, "avg_logprob": -0.24343094268402496, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.11769065260887146}, {"id": 4, "seek": 2528, "start": 32.56, "end": 39.120000000000005, "text": " So let's start with a short bio. I'm a backend developer. I'm also a machine learning engineer.", "tokens": [50728, 407, 718, 311, 722, 365, 257, 2099, 12198, 13, 286, 478, 257, 38087, 10754, 13, 286, 478, 611, 257, 3479, 2539, 11403, 13, 51056], "temperature": 0.0, "avg_logprob": -0.24343094268402496, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.11769065260887146}, {"id": 5, "seek": 2528, "start": 39.120000000000005, "end": 43.040000000000006, "text": " Used to be a Google developer expert in machine learning before the COVID.", "tokens": [51056, 43237, 281, 312, 257, 3329, 10754, 5844, 294, 3479, 2539, 949, 264, 4566, 13, 51252], "temperature": 0.0, "avg_logprob": -0.24343094268402496, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.11769065260887146}, {"id": 6, "seek": 2528, "start": 44.16, "end": 50.32, "text": " I call it in quite a few languages in the last 12 months, mostly in Python and Go, a bit of cc++.", "tokens": [51308, 286, 818, 309, 294, 1596, 257, 1326, 8650, 294, 264, 1036, 2272, 2493, 11, 5240, 294, 15329, 293, 1037, 11, 257, 857, 295, 269, 66, 25472, 13, 51616], "temperature": 0.0, "avg_logprob": -0.24343094268402496, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.11769065260887146}, {"id": 7, "seek": 5032, "start": 51.04, "end": 57.2, "text": " And I work in Athenian. This is a startup. It does a software service product for engineering", "tokens": [50400, 400, 286, 589, 294, 16487, 268, 952, 13, 639, 307, 257, 18578, 13, 467, 775, 257, 4722, 2643, 1674, 337, 7043, 50708], "temperature": 0.0, "avg_logprob": -0.16690603892008463, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0782846212387085}, {"id": 8, "seek": 5032, "start": 57.2, "end": 65.12, "text": " leaders. So the talk will be divided into two parts. The first will be about some custom binary", "tokens": [50708, 3523, 13, 407, 264, 751, 486, 312, 6666, 666, 732, 3166, 13, 440, 700, 486, 312, 466, 512, 2375, 17434, 51104], "temperature": 0.0, "avg_logprob": -0.16690603892008463, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0782846212387085}, {"id": 9, "seek": 5032, "start": 65.12, "end": 72.0, "text": " serialization that I had to do in my daily job. And the second will be about speeding up JSON dump.", "tokens": [51104, 17436, 2144, 300, 286, 632, 281, 360, 294, 452, 5212, 1691, 13, 400, 264, 1150, 486, 312, 466, 35593, 493, 31828, 11430, 13, 51448], "temperature": 0.0, "avg_logprob": -0.16690603892008463, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0782846212387085}, {"id": 10, "seek": 5032, "start": 72.96000000000001, "end": 79.76, "text": " So the first is about how I coded something that was working faster than people by", "tokens": [51496, 407, 264, 700, 307, 466, 577, 286, 34874, 746, 300, 390, 1364, 4663, 813, 561, 538, 51836], "temperature": 0.0, "avg_logprob": -0.16690603892008463, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0782846212387085}, {"id": 11, "seek": 7976, "start": 79.76, "end": 86.88000000000001, "text": " many, many times. And why I had to do that? Well, apparently because I had a use case that", "tokens": [50364, 867, 11, 867, 1413, 13, 400, 983, 286, 632, 281, 360, 300, 30, 1042, 11, 7970, 570, 286, 632, 257, 764, 1389, 300, 50720], "temperature": 0.0, "avg_logprob": -0.15662542633388354, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.02025732584297657}, {"id": 12, "seek": 7976, "start": 87.52000000000001, "end": 93.76, "text": " didn't really fit well into regular, pickling into regular serialization of pandas data frames.", "tokens": [50752, 994, 380, 534, 3318, 731, 666, 3890, 11, 1888, 1688, 666, 3890, 17436, 2144, 295, 4565, 296, 1412, 12083, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15662542633388354, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.02025732584297657}, {"id": 13, "seek": 7976, "start": 94.48, "end": 100.64, "text": " And it's not necessarily a best practice to materialize a huge pandas data frame", "tokens": [51100, 400, 309, 311, 406, 4725, 257, 1151, 3124, 281, 2527, 1125, 257, 2603, 4565, 296, 1412, 3920, 51408], "temperature": 0.0, "avg_logprob": -0.15662542633388354, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.02025732584297657}, {"id": 14, "seek": 7976, "start": 100.64, "end": 106.88000000000001, "text": " while you're serving a request. But nevertheless, I had to do that. And it was really big,", "tokens": [51408, 1339, 291, 434, 8148, 257, 5308, 13, 583, 26924, 11, 286, 632, 281, 360, 300, 13, 400, 309, 390, 534, 955, 11, 51720], "temperature": 0.0, "avg_logprob": -0.15662542633388354, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.02025732584297657}, {"id": 15, "seek": 10688, "start": 106.88, "end": 113.92, "text": " a few megabytes at least. And I noticed that according to the traces, picking it just took", "tokens": [50364, 257, 1326, 10816, 24538, 412, 1935, 13, 400, 286, 5694, 300, 4650, 281, 264, 26076, 11, 8867, 309, 445, 1890, 50716], "temperature": 0.0, "avg_logprob": -0.08781540935689752, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.012075601145625114}, {"id": 16, "seek": 10688, "start": 113.92, "end": 119.03999999999999, "text": " too much time. And we really needed to put it into distributed cache. Otherwise,", "tokens": [50716, 886, 709, 565, 13, 400, 321, 534, 2978, 281, 829, 309, 666, 12631, 19459, 13, 10328, 11, 50972], "temperature": 0.0, "avg_logprob": -0.08781540935689752, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.012075601145625114}, {"id": 17, "seek": 10688, "start": 119.03999999999999, "end": 125.75999999999999, "text": " you would spend extra time recomputing it in every request. And that data frame was quite", "tokens": [50972, 291, 576, 3496, 2857, 565, 23334, 2582, 278, 309, 294, 633, 5308, 13, 400, 300, 1412, 3920, 390, 1596, 51308], "temperature": 0.0, "avg_logprob": -0.08781540935689752, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.012075601145625114}, {"id": 18, "seek": 10688, "start": 125.75999999999999, "end": 132.48, "text": " special. I mean, it's not a usual bunch of integers and floating points. No, it was quite", "tokens": [51308, 2121, 13, 286, 914, 11, 309, 311, 406, 257, 7713, 3840, 295, 41674, 293, 12607, 2793, 13, 883, 11, 309, 390, 1596, 51644], "temperature": 0.0, "avg_logprob": -0.08781540935689752, "compression_ratio": 1.5462555066079295, "no_speech_prob": 0.012075601145625114}, {"id": 19, "seek": 13248, "start": 132.48, "end": 139.28, "text": " complex. It contained strings, it contained daytime columns, could be numpy daytime,", "tokens": [50364, 3997, 13, 467, 16212, 13985, 11, 309, 16212, 31908, 13766, 11, 727, 312, 1031, 8200, 31908, 11, 50704], "temperature": 0.0, "avg_logprob": -0.19073291208552218, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.015244871377944946}, {"id": 20, "seek": 13248, "start": 139.28, "end": 147.28, "text": " could be just regular Python objects, contained nested lists and digs, and even nested numpy", "tokens": [50704, 727, 312, 445, 3890, 15329, 6565, 11, 16212, 15646, 292, 14511, 293, 2528, 82, 11, 293, 754, 15646, 292, 1031, 8200, 51104], "temperature": 0.0, "avg_logprob": -0.19073291208552218, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.015244871377944946}, {"id": 21, "seek": 13248, "start": 147.28, "end": 153.44, "text": " arrays. So it was quite complex. And this was mostly the reason why regular serialization,", "tokens": [51104, 41011, 13, 407, 309, 390, 1596, 3997, 13, 400, 341, 390, 5240, 264, 1778, 983, 3890, 17436, 2144, 11, 51412], "temperature": 0.0, "avg_logprob": -0.19073291208552218, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.015244871377944946}, {"id": 22, "seek": 13248, "start": 153.44, "end": 162.0, "text": " arrow and everything worked too slow in serializing it. So I came up with a custom binary format.", "tokens": [51412, 11610, 293, 1203, 2732, 886, 2964, 294, 17436, 3319, 309, 13, 407, 286, 1361, 493, 365, 257, 2375, 17434, 7877, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19073291208552218, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.015244871377944946}, {"id": 23, "seek": 16248, "start": 163.04, "end": 170.48, "text": " And although it's not really open, it's also public and everyone can study the source code.", "tokens": [50392, 400, 4878, 309, 311, 406, 534, 1269, 11, 309, 311, 611, 1908, 293, 1518, 393, 2979, 264, 4009, 3089, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10088967193256725, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.005784892942756414}, {"id": 24, "seek": 16248, "start": 170.48, "end": 176.95999999999998, "text": " It's not universal. Really, it only supports the types inside the pandas data frame that", "tokens": [50764, 467, 311, 406, 11455, 13, 4083, 11, 309, 787, 9346, 264, 3467, 1854, 264, 4565, 296, 1412, 3920, 300, 51088], "temperature": 0.0, "avg_logprob": -0.10088967193256725, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.005784892942756414}, {"id": 25, "seek": 16248, "start": 177.6, "end": 184.79999999999998, "text": " we had to serialize at work. It's not backward compatible. It's not portable at all. It doesn't", "tokens": [51120, 321, 632, 281, 17436, 1125, 412, 589, 13, 467, 311, 406, 23897, 18218, 13, 467, 311, 406, 21800, 412, 439, 13, 467, 1177, 380, 51480], "temperature": 0.0, "avg_logprob": -0.10088967193256725, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.005784892942756414}, {"id": 26, "seek": 16248, "start": 184.79999999999998, "end": 190.64, "text": " fit well into different CPU architectures or it always assumes Linux and it always", "tokens": [51480, 3318, 731, 666, 819, 13199, 6331, 1303, 420, 309, 1009, 37808, 18734, 293, 309, 1009, 51772], "temperature": 0.0, "avg_logprob": -0.10088967193256725, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.005784892942756414}, {"id": 27, "seek": 19064, "start": 190.64, "end": 196.64, "text": " assumes CPython 3.11. And this is quite important. I'll talk about it a little bit later.", "tokens": [50364, 37808, 22431, 88, 11943, 805, 13, 5348, 13, 400, 341, 307, 1596, 1021, 13, 286, 603, 751, 466, 309, 257, 707, 857, 1780, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15295307082359236, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.014591767452657223}, {"id": 28, "seek": 19064, "start": 197.35999999999999, "end": 203.04, "text": " So this is generally a really, really bad idea. And you only do that when you don't have any other", "tokens": [50700, 407, 341, 307, 5101, 257, 534, 11, 534, 1578, 1558, 13, 400, 291, 787, 360, 300, 562, 291, 500, 380, 362, 604, 661, 50984], "temperature": 0.0, "avg_logprob": -0.15295307082359236, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.014591767452657223}, {"id": 29, "seek": 19064, "start": 203.04, "end": 210.32, "text": " options. And you need to squeeze a lot of performance in the backend. However, on the bright side,", "tokens": [50984, 3956, 13, 400, 291, 643, 281, 13578, 257, 688, 295, 3389, 294, 264, 38087, 13, 2908, 11, 322, 264, 4730, 1252, 11, 51348], "temperature": 0.0, "avg_logprob": -0.15295307082359236, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.014591767452657223}, {"id": 30, "seek": 19064, "start": 211.35999999999999, "end": 218.79999999999998, "text": " it was quite compact code. Cytan, because performance. It's a single pass serializer", "tokens": [51400, 309, 390, 1596, 14679, 3089, 13, 383, 4328, 282, 11, 570, 3389, 13, 467, 311, 257, 2167, 1320, 17436, 6545, 51772], "temperature": 0.0, "avg_logprob": -0.15295307082359236, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.014591767452657223}, {"id": 31, "seek": 21880, "start": 219.68, "end": 225.44, "text": " really, really, really fast. And also it releases Jill. This is something that regular people or", "tokens": [50408, 534, 11, 534, 11, 534, 2370, 13, 400, 611, 309, 16952, 24690, 13, 639, 307, 746, 300, 3890, 561, 420, 50696], "temperature": 0.0, "avg_logprob": -0.125038818879561, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.007920755073428154}, {"id": 32, "seek": 21880, "start": 225.44, "end": 234.48000000000002, "text": " arrow cannot do for the constraint that they had to be universal and pickle everything, any object.", "tokens": [50696, 11610, 2644, 360, 337, 264, 25534, 300, 436, 632, 281, 312, 11455, 293, 31433, 1203, 11, 604, 2657, 13, 51148], "temperature": 0.0, "avg_logprob": -0.125038818879561, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.007920755073428154}, {"id": 33, "seek": 21880, "start": 235.92000000000002, "end": 240.0, "text": " As soon as you have an assumption of what kind of object you can serialize,", "tokens": [51220, 1018, 2321, 382, 291, 362, 364, 15302, 295, 437, 733, 295, 2657, 291, 393, 17436, 1125, 11, 51424], "temperature": 0.0, "avg_logprob": -0.125038818879561, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.007920755073428154}, {"id": 34, "seek": 21880, "start": 240.0, "end": 244.96, "text": " you can call it in Cytan in such a way that you release Jill and it works even faster.", "tokens": [51424, 291, 393, 818, 309, 294, 383, 4328, 282, 294, 1270, 257, 636, 300, 291, 4374, 24690, 293, 309, 1985, 754, 4663, 13, 51672], "temperature": 0.0, "avg_logprob": -0.125038818879561, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.007920755073428154}, {"id": 35, "seek": 24496, "start": 245.68, "end": 252.72, "text": " And also other code to execute in parallel. And releasing Jill is probably the hardest", "tokens": [50400, 400, 611, 661, 3089, 281, 14483, 294, 8952, 13, 400, 16327, 24690, 307, 1391, 264, 13158, 50752], "temperature": 0.0, "avg_logprob": -0.17972516279954176, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.021090975031256676}, {"id": 36, "seek": 24496, "start": 252.72, "end": 259.84000000000003, "text": " feature in this serializer because it was more labor intensive that I initially thought.", "tokens": [50752, 4111, 294, 341, 17436, 6545, 570, 309, 390, 544, 5938, 18957, 300, 286, 9105, 1194, 13, 51108], "temperature": 0.0, "avg_logprob": -0.17972516279954176, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.021090975031256676}, {"id": 37, "seek": 24496, "start": 259.84000000000003, "end": 270.64, "text": " You see Cytan offers C API wrapper for a lot of APIs inside CPython API. And they are great.", "tokens": [51108, 509, 536, 383, 4328, 282, 7736, 383, 9362, 46906, 337, 257, 688, 295, 21445, 1854, 22431, 88, 11943, 9362, 13, 400, 436, 366, 869, 13, 51648], "temperature": 0.0, "avg_logprob": -0.17972516279954176, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.021090975031256676}, {"id": 38, "seek": 27064, "start": 270.64, "end": 276.15999999999997, "text": " However, as soon as you release the Jill, you cannot use them. Just Cytan doesn't compile it.", "tokens": [50364, 2908, 11, 382, 2321, 382, 291, 4374, 264, 24690, 11, 291, 2644, 764, 552, 13, 1449, 383, 4328, 282, 1177, 380, 31413, 309, 13, 50640], "temperature": 0.0, "avg_logprob": -0.07836499967073139, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.030076144263148308}, {"id": 39, "seek": 27064, "start": 276.15999999999997, "end": 283.28, "text": " And this is a good thing because to use these wrappers without Jill, you need to be absolutely", "tokens": [50640, 400, 341, 307, 257, 665, 551, 570, 281, 764, 613, 7843, 15226, 1553, 24690, 11, 291, 643, 281, 312, 3122, 50996], "temperature": 0.0, "avg_logprob": -0.07836499967073139, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.030076144263148308}, {"id": 40, "seek": 27064, "start": 283.28, "end": 287.52, "text": " sure that you are doing the right thing, that you are not using the heap, that you're not writing", "tokens": [50996, 988, 300, 291, 366, 884, 264, 558, 551, 11, 300, 291, 366, 406, 1228, 264, 33591, 11, 300, 291, 434, 406, 3579, 51208], "temperature": 0.0, "avg_logprob": -0.07836499967073139, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.030076144263148308}, {"id": 41, "seek": 27064, "start": 287.52, "end": 293.76, "text": " to any Python objects, also that those Python objects are not mutated somewhere else. This is", "tokens": [51208, 281, 604, 15329, 6565, 11, 611, 300, 729, 15329, 6565, 366, 406, 5839, 770, 4079, 1646, 13, 639, 307, 51520], "temperature": 0.0, "avg_logprob": -0.07836499967073139, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.030076144263148308}, {"id": 42, "seek": 29376, "start": 293.76, "end": 301.84, "text": " all true for our backend, but it can be different in other use cases. So this is what happens.", "tokens": [50364, 439, 2074, 337, 527, 38087, 11, 457, 309, 393, 312, 819, 294, 661, 764, 3331, 13, 407, 341, 307, 437, 2314, 13, 50768], "temperature": 0.0, "avg_logprob": -0.13598872519828178, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.02733534388244152}, {"id": 43, "seek": 29376, "start": 302.71999999999997, "end": 310.08, "text": " And what if there is no C API at all or some C API wrapper missing?", "tokens": [50812, 400, 437, 498, 456, 307, 572, 383, 9362, 412, 439, 420, 512, 383, 9362, 46906, 5361, 30, 51180], "temperature": 0.0, "avg_logprob": -0.13598872519828178, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.02733534388244152}, {"id": 44, "seek": 29376, "start": 311.36, "end": 318.32, "text": " You have to re-inclimate it from scratch in Cytan. And this is what we had to do quite much.", "tokens": [51244, 509, 362, 281, 319, 12, 259, 3474, 2905, 309, 490, 8459, 294, 383, 4328, 282, 13, 400, 341, 307, 437, 321, 632, 281, 360, 1596, 709, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13598872519828178, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.02733534388244152}, {"id": 45, "seek": 31832, "start": 319.28, "end": 327.12, "text": " Final note is that, for example, PyPy has a garbage collector that can relocate objects", "tokens": [50412, 13443, 3637, 307, 300, 11, 337, 1365, 11, 9953, 47, 88, 575, 257, 14150, 23960, 300, 393, 26981, 473, 6565, 50804], "temperature": 0.0, "avg_logprob": -0.11469062169392903, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.013069773092865944}, {"id": 46, "seek": 31832, "start": 327.12, "end": 334.56, "text": " from time to time. So the pointer to the object can change. And this is why this serializer just", "tokens": [50804, 490, 565, 281, 565, 13, 407, 264, 23918, 281, 264, 2657, 393, 1319, 13, 400, 341, 307, 983, 341, 17436, 6545, 445, 51176], "temperature": 0.0, "avg_logprob": -0.11469062169392903, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.013069773092865944}, {"id": 47, "seek": 31832, "start": 334.56, "end": 340.15999999999997, "text": " doesn't work with PyPy even in Siri because it always assumes that as soon as you have a pointer", "tokens": [51176, 1177, 380, 589, 365, 9953, 47, 88, 754, 294, 33682, 570, 309, 1009, 37808, 300, 382, 2321, 382, 291, 362, 257, 23918, 51456], "temperature": 0.0, "avg_logprob": -0.11469062169392903, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.013069773092865944}, {"id": 48, "seek": 34016, "start": 340.16, "end": 350.08000000000004, "text": " to Python object, it never changes. Anyway, this is how the high-level serialization of", "tokens": [50364, 281, 15329, 2657, 11, 309, 1128, 2962, 13, 5684, 11, 341, 307, 577, 264, 1090, 12, 12418, 17436, 2144, 295, 50860], "temperature": 0.0, "avg_logprob": -0.16537903376988003, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.02380005642771721}, {"id": 49, "seek": 34016, "start": 350.08000000000004, "end": 358.32000000000005, "text": " pandas works. And this is not really specific to my serializer or your pickling or this", "tokens": [50860, 4565, 296, 1985, 13, 400, 341, 307, 406, 534, 2685, 281, 452, 17436, 6545, 420, 428, 1888, 1688, 420, 341, 51272], "temperature": 0.0, "avg_logprob": -0.16537903376988003, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.02380005642771721}, {"id": 50, "seek": 34016, "start": 358.32000000000005, "end": 365.28000000000003, "text": " suggests how it works. So every pandas data frame currently in 1.x has a block manager that maps", "tokens": [51272, 13409, 577, 309, 1985, 13, 407, 633, 4565, 296, 1412, 3920, 4362, 294, 502, 13, 87, 575, 257, 3461, 6598, 300, 11317, 51620], "temperature": 0.0, "avg_logprob": -0.16537903376988003, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.02380005642771721}, {"id": 51, "seek": 36528, "start": 365.28, "end": 372.88, "text": " column names to different blocks. Every block has the same data type. And this is a two-dimensional", "tokens": [50364, 7738, 5288, 281, 819, 8474, 13, 2048, 3461, 575, 264, 912, 1412, 2010, 13, 400, 341, 307, 257, 732, 12, 18759, 50744], "temperature": 0.0, "avg_logprob": -0.13759077707926431, "compression_ratio": 1.4416243654822336, "no_speech_prob": 0.055324964225292206}, {"id": 52, "seek": 36528, "start": 372.88, "end": 381.76, "text": " non-PRA underneath. This 2D non-PRA doesn't know anything about what columns are, what are they", "tokens": [50744, 2107, 12, 47, 3750, 7223, 13, 639, 568, 35, 2107, 12, 47, 3750, 1177, 380, 458, 1340, 466, 437, 13766, 366, 11, 437, 366, 436, 51188], "temperature": 0.0, "avg_logprob": -0.13759077707926431, "compression_ratio": 1.4416243654822336, "no_speech_prob": 0.055324964225292206}, {"id": 53, "seek": 36528, "start": 381.76, "end": 390.64, "text": " named. It's just the internal storage, let's say. And whenever you reach a column in the", "tokens": [51188, 4926, 13, 467, 311, 445, 264, 6920, 6725, 11, 718, 311, 584, 13, 400, 5699, 291, 2524, 257, 7738, 294, 264, 51632], "temperature": 0.0, "avg_logprob": -0.13759077707926431, "compression_ratio": 1.4416243654822336, "no_speech_prob": 0.055324964225292206}, {"id": 54, "seek": 39064, "start": 391.59999999999997, "end": 397.44, "text": " in the data frame, you use the block manager to access them and do some operations. It works", "tokens": [50412, 294, 264, 1412, 3920, 11, 291, 764, 264, 3461, 6598, 281, 2105, 552, 293, 360, 512, 7705, 13, 467, 1985, 50704], "temperature": 0.0, "avg_logprob": -0.10842700137032403, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.011049208231270313}, {"id": 55, "seek": 39064, "start": 397.44, "end": 405.36, "text": " fast, yes, and it's also memory efficient. Every operation on a single column executes", "tokens": [50704, 2370, 11, 2086, 11, 293, 309, 311, 611, 4675, 7148, 13, 2048, 6916, 322, 257, 2167, 7738, 4454, 1819, 51100], "temperature": 0.0, "avg_logprob": -0.10842700137032403, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.011049208231270313}, {"id": 56, "seek": 39064, "start": 405.36, "end": 415.2, "text": " on a non-PRA underneath, so it's really efficient. And it also allows us to read this pandas data", "tokens": [51100, 322, 257, 2107, 12, 47, 3750, 7223, 11, 370, 309, 311, 534, 7148, 13, 400, 309, 611, 4045, 505, 281, 1401, 341, 4565, 296, 1412, 51592], "temperature": 0.0, "avg_logprob": -0.10842700137032403, "compression_ratio": 1.5054347826086956, "no_speech_prob": 0.011049208231270313}, {"id": 57, "seek": 41520, "start": 415.2, "end": 421.84, "text": " frame and serialize it as a bunch of non-PRAs without Jill. So this is great for us. One thing", "tokens": [50364, 3920, 293, 17436, 1125, 309, 382, 257, 3840, 295, 2107, 12, 47, 3750, 82, 1553, 24690, 13, 407, 341, 307, 869, 337, 505, 13, 1485, 551, 50696], "temperature": 0.0, "avg_logprob": -0.15601209231785365, "compression_ratio": 1.4536082474226804, "no_speech_prob": 0.03243846446275711}, {"id": 58, "seek": 41520, "start": 421.84, "end": 433.2, "text": " that we had to re-sync properly is how do we serialize object data type in non-PRAs. And we,", "tokens": [50696, 300, 321, 632, 281, 319, 12, 82, 34015, 6108, 307, 577, 360, 321, 17436, 1125, 2657, 1412, 2010, 294, 2107, 12, 47, 3750, 82, 13, 400, 321, 11, 51264], "temperature": 0.0, "avg_logprob": -0.15601209231785365, "compression_ratio": 1.4536082474226804, "no_speech_prob": 0.03243846446275711}, {"id": 59, "seek": 41520, "start": 433.2, "end": 440.32, "text": " again, don't support all use cases, only those that we know exist in our backend. This is some", "tokens": [51264, 797, 11, 500, 380, 1406, 439, 764, 3331, 11, 787, 729, 300, 321, 458, 2514, 294, 527, 38087, 13, 639, 307, 512, 51620], "temperature": 0.0, "avg_logprob": -0.15601209231785365, "compression_ratio": 1.4536082474226804, "no_speech_prob": 0.03243846446275711}, {"id": 60, "seek": 44032, "start": 440.32, "end": 450.08, "text": " site on pseudo code that shows some general idea how we do this serialization. There is a", "tokens": [50364, 3621, 322, 35899, 3089, 300, 3110, 512, 2674, 1558, 577, 321, 360, 341, 17436, 2144, 13, 821, 307, 257, 50852], "temperature": 0.0, "avg_logprob": -0.1814598764692034, "compression_ratio": 1.4329896907216495, "no_speech_prob": 0.019466858357191086}, {"id": 61, "seek": 44032, "start": 450.88, "end": 457.28, "text": " function called PyArrayData that returns you a row pointer to underlying data in the non-PRA.", "tokens": [50892, 2445, 1219, 9953, 10683, 3458, 35, 3274, 300, 11247, 291, 257, 5386, 23918, 281, 14217, 1412, 294, 264, 2107, 12, 47, 3750, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1814598764692034, "compression_ratio": 1.4329896907216495, "no_speech_prob": 0.019466858357191086}, {"id": 62, "seek": 44032, "start": 457.92, "end": 464.56, "text": " Then you iterate by lengths. We only support one-dimensional arrays so far, like every column,", "tokens": [51244, 1396, 291, 44497, 538, 26329, 13, 492, 787, 1406, 472, 12, 18759, 41011, 370, 1400, 11, 411, 633, 7738, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1814598764692034, "compression_ratio": 1.4329896907216495, "no_speech_prob": 0.019466858357191086}, {"id": 63, "seek": 46456, "start": 465.36, "end": 472.4, "text": " and we serialize each PyObject independently. And this PyObject can be in Siri arbitrarily,", "tokens": [50404, 293, 321, 17436, 1125, 1184, 9953, 45483, 1020, 21761, 13, 400, 341, 9953, 45483, 1020, 393, 312, 294, 33682, 19071, 3289, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1599482536315918, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.011759663932025433}, {"id": 64, "seek": 46456, "start": 472.4, "end": 478.64, "text": " but, again, in our backend, there is a predefined list of things that can be stored like a string,", "tokens": [50756, 457, 11, 797, 11, 294, 527, 38087, 11, 456, 307, 257, 659, 37716, 1329, 295, 721, 300, 393, 312, 12187, 411, 257, 6798, 11, 51068], "temperature": 0.0, "avg_logprob": -0.1599482536315918, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.011759663932025433}, {"id": 65, "seek": 46456, "start": 478.64, "end": 487.2, "text": " a daytime, an integer, and so on. When we have to iterate standard library containers,", "tokens": [51068, 257, 31908, 11, 364, 24922, 11, 293, 370, 322, 13, 1133, 321, 362, 281, 44497, 3832, 6405, 17089, 11, 51496], "temperature": 0.0, "avg_logprob": -0.1599482536315918, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.011759663932025433}, {"id": 66, "seek": 46456, "start": 487.2, "end": 494.16, "text": " such as a list or dictionary, it turned out, again, quite simple. You use PyListGetSize or", "tokens": [51496, 1270, 382, 257, 1329, 420, 25890, 11, 309, 3574, 484, 11, 797, 11, 1596, 2199, 13, 509, 764, 9953, 43, 468, 18133, 50, 1125, 420, 51844], "temperature": 0.0, "avg_logprob": -0.1599482536315918, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.011759663932025433}, {"id": 67, "seek": 49416, "start": 494.24, "end": 505.28000000000003, "text": " PyListGetItem. For Dict, you use PyDictNextIterator. This is so great. If you didn't have to release", "tokens": [50368, 9953, 43, 468, 18133, 3522, 443, 13, 1171, 413, 985, 11, 291, 764, 9953, 35, 985, 31002, 40, 391, 1639, 13, 639, 307, 370, 869, 13, 759, 291, 994, 380, 362, 281, 4374, 50920], "temperature": 0.0, "avg_logprob": -0.24101127277721057, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.006256563123315573}, {"id": 68, "seek": 49416, "start": 505.28000000000003, "end": 513.9200000000001, "text": " the JIL, you would just use list or Dict type and site. And site would generate somewhat similar", "tokens": [50920, 264, 508, 4620, 11, 291, 576, 445, 764, 1329, 420, 413, 985, 2010, 293, 3621, 13, 400, 3621, 576, 8460, 8344, 2531, 51352], "temperature": 0.0, "avg_logprob": -0.24101127277721057, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.006256563123315573}, {"id": 69, "seek": 49416, "start": 513.9200000000001, "end": 518.8000000000001, "text": " code, also efficient. But, again, since we released the JIL, we have to do, let's say,", "tokens": [51352, 3089, 11, 611, 7148, 13, 583, 11, 797, 11, 1670, 321, 4736, 264, 508, 4620, 11, 321, 362, 281, 360, 11, 718, 311, 584, 11, 51596], "temperature": 0.0, "avg_logprob": -0.24101127277721057, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.006256563123315573}, {"id": 70, "seek": 51880, "start": 518.8, "end": 526.56, "text": " heavy lifting from scratch. For serializing integers and floats, you just convert PyObject to", "tokens": [50364, 4676, 15798, 490, 8459, 13, 1171, 17436, 3319, 41674, 293, 37878, 11, 291, 445, 7620, 9953, 45483, 1020, 281, 50752], "temperature": 0.0, "avg_logprob": -0.16824905292407885, "compression_ratio": 1.4473684210526316, "no_speech_prob": 0.0040391902439296246}, {"id": 71, "seek": 51880, "start": 527.3599999999999, "end": 534.64, "text": " C type, like double or long. If it's a non-P scalar, a non-P scalar is usually a structure,", "tokens": [50792, 383, 2010, 11, 411, 3834, 420, 938, 13, 759, 309, 311, 257, 2107, 12, 47, 39684, 11, 257, 2107, 12, 47, 39684, 307, 2673, 257, 3877, 11, 51156], "temperature": 0.0, "avg_logprob": -0.16824905292407885, "compression_ratio": 1.4473684210526316, "no_speech_prob": 0.0040391902439296246}, {"id": 72, "seek": 51880, "start": 534.64, "end": 542.4799999999999, "text": " you use a special non-PC API for extracting the scalar value, and you just memory copy it", "tokens": [51156, 291, 764, 257, 2121, 2107, 12, 12986, 9362, 337, 49844, 264, 39684, 2158, 11, 293, 291, 445, 4675, 5055, 309, 51548], "temperature": 0.0, "avg_logprob": -0.16824905292407885, "compression_ratio": 1.4473684210526316, "no_speech_prob": 0.0040391902439296246}, {"id": 73, "seek": 54248, "start": 542.48, "end": 548.88, "text": " to your output stream. So, this is why we are not portable in the CPU, for example, because", "tokens": [50364, 281, 428, 5598, 4309, 13, 407, 11, 341, 307, 983, 321, 366, 406, 21800, 294, 264, 13199, 11, 337, 1365, 11, 570, 50684], "temperature": 0.0, "avg_logprob": -0.14925765056236118, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.029793204739689827}, {"id": 74, "seek": 54248, "start": 549.76, "end": 556.64, "text": " the order of bytes in the integer, this endianness, can be whatever. For Intel, it's little endian.", "tokens": [50728, 264, 1668, 295, 36088, 294, 264, 24922, 11, 341, 917, 952, 1287, 11, 393, 312, 2035, 13, 1171, 19762, 11, 309, 311, 707, 917, 952, 13, 51072], "temperature": 0.0, "avg_logprob": -0.14925765056236118, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.029793204739689827}, {"id": 75, "seek": 54248, "start": 556.64, "end": 564.08, "text": " For ARM, it can be little or big. So, since our backend always works on the same kind of CPU,", "tokens": [51072, 1171, 45209, 11, 309, 393, 312, 707, 420, 955, 13, 407, 11, 1670, 527, 38087, 1009, 1985, 322, 264, 912, 733, 295, 13199, 11, 51444], "temperature": 0.0, "avg_logprob": -0.14925765056236118, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.029793204739689827}, {"id": 76, "seek": 54248, "start": 564.08, "end": 571.2, "text": " and we are really sure about that, we can do these things without caring about endianness and", "tokens": [51444, 293, 321, 366, 534, 988, 466, 300, 11, 321, 393, 360, 613, 721, 1553, 15365, 466, 917, 952, 1287, 293, 51800], "temperature": 0.0, "avg_logprob": -0.14925765056236118, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.029793204739689827}, {"id": 77, "seek": 57120, "start": 571.2, "end": 576.6400000000001, "text": " converting to some other order, network order, and so on. And there's also the reason why it", "tokens": [50364, 29942, 281, 512, 661, 1668, 11, 3209, 1668, 11, 293, 370, 322, 13, 400, 456, 311, 611, 264, 1778, 983, 309, 50636], "temperature": 0.0, "avg_logprob": -0.10898357548125803, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.007526192348450422}, {"id": 78, "seek": 57120, "start": 576.6400000000001, "end": 586.32, "text": " works so fast. To serialize a string, it's also nothing special. I have to say a few words about", "tokens": [50636, 1985, 370, 2370, 13, 1407, 17436, 1125, 257, 6798, 11, 309, 311, 611, 1825, 2121, 13, 286, 362, 281, 584, 257, 1326, 2283, 466, 51120], "temperature": 0.0, "avg_logprob": -0.10898357548125803, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.007526192348450422}, {"id": 79, "seek": 57120, "start": 586.32, "end": 592.96, "text": " how strings are stored in C Python internally. It's quite smart, and I'm honestly really impressed", "tokens": [51120, 577, 13985, 366, 12187, 294, 383, 15329, 19501, 13, 467, 311, 1596, 4069, 11, 293, 286, 478, 6095, 534, 11679, 51452], "temperature": 0.0, "avg_logprob": -0.10898357548125803, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.007526192348450422}, {"id": 80, "seek": 59296, "start": 593.0400000000001, "end": 603.44, "text": " how C Python does it. It stores strings in three different ways, in one byte encoding,", "tokens": [50368, 577, 383, 15329, 775, 309, 13, 467, 9512, 13985, 294, 1045, 819, 2098, 11, 294, 472, 40846, 43430, 11, 50888], "temperature": 0.0, "avg_logprob": -0.12657702100145948, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.04452972859144211}, {"id": 81, "seek": 59296, "start": 603.44, "end": 608.32, "text": " in two byte encoding, and four byte encoding. And it dynamically chooses the encoding based on the", "tokens": [50888, 294, 732, 40846, 43430, 11, 293, 1451, 40846, 43430, 13, 400, 309, 43492, 25963, 264, 43430, 2361, 322, 264, 51132], "temperature": 0.0, "avg_logprob": -0.12657702100145948, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.04452972859144211}, {"id": 82, "seek": 59296, "start": 608.32, "end": 615.12, "text": " maximum number of the character that has to be stored in the string. So, let's say if you only", "tokens": [51132, 6674, 1230, 295, 264, 2517, 300, 575, 281, 312, 12187, 294, 264, 6798, 13, 407, 11, 718, 311, 584, 498, 291, 787, 51472], "temperature": 0.0, "avg_logprob": -0.12657702100145948, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.04452972859144211}, {"id": 83, "seek": 59296, "start": 615.12, "end": 621.0400000000001, "text": " have ASCII characters, you choose one byte encoding and you're super memory efficient. If you have", "tokens": [51472, 362, 7469, 34, 9503, 4342, 11, 291, 2826, 472, 40846, 43430, 293, 291, 434, 1687, 4675, 7148, 13, 759, 291, 362, 51768], "temperature": 0.0, "avg_logprob": -0.12657702100145948, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.04452972859144211}, {"id": 84, "seek": 62104, "start": 621.04, "end": 628.4, "text": " some crazy emojis, then you choose four byte encoding. And yes, if there are other characters,", "tokens": [50364, 512, 3219, 19611, 40371, 11, 550, 291, 2826, 1451, 40846, 43430, 13, 400, 2086, 11, 498, 456, 366, 661, 4342, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08479139539930555, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.022633146494627}, {"id": 85, "seek": 62104, "start": 628.4, "end": 634.4, "text": " it's not so memory efficient. At the same time, when you address a string by an index, it works", "tokens": [50732, 309, 311, 406, 370, 4675, 7148, 13, 1711, 264, 912, 565, 11, 562, 291, 2985, 257, 6798, 538, 364, 8186, 11, 309, 1985, 51032], "temperature": 0.0, "avg_logprob": -0.08479139539930555, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.022633146494627}, {"id": 86, "seek": 62104, "start": 634.4, "end": 639.1999999999999, "text": " super fast because every character is guaranteed to have the same number of bytes underneath.", "tokens": [51032, 1687, 2370, 570, 633, 2517, 307, 18031, 281, 362, 264, 912, 1230, 295, 36088, 7223, 13, 51272], "temperature": 0.0, "avg_logprob": -0.08479139539930555, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.022633146494627}, {"id": 87, "seek": 62104, "start": 640.0, "end": 649.68, "text": " Anyway, to copy a string to binary output format, you just take the row pointer to the string", "tokens": [51312, 5684, 11, 281, 5055, 257, 6798, 281, 17434, 5598, 7877, 11, 291, 445, 747, 264, 5386, 23918, 281, 264, 6798, 51796], "temperature": 0.0, "avg_logprob": -0.08479139539930555, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.022633146494627}, {"id": 88, "seek": 64968, "start": 649.68, "end": 656.4, "text": " contents. And for the size, you get the number of bytes per character and multiply it by the", "tokens": [50364, 15768, 13, 400, 337, 264, 2744, 11, 291, 483, 264, 1230, 295, 36088, 680, 2517, 293, 12972, 309, 538, 264, 50700], "temperature": 0.0, "avg_logprob": -0.16462293831077782, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.00816685426980257}, {"id": 89, "seek": 64968, "start": 656.4, "end": 664.0, "text": " length of the string. So, again, nothing special in Siri, but you have to be aware of the internals.", "tokens": [50700, 4641, 295, 264, 6798, 13, 407, 11, 797, 11, 1825, 2121, 294, 33682, 11, 457, 291, 362, 281, 312, 3650, 295, 264, 2154, 1124, 13, 51080], "temperature": 0.0, "avg_logprob": -0.16462293831077782, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.00816685426980257}, {"id": 90, "seek": 64968, "start": 664.7199999999999, "end": 674.64, "text": " To serialize daytime and time delta, C Python provides a special C API as well. It's worth", "tokens": [51116, 1407, 17436, 1125, 31908, 293, 565, 8289, 11, 383, 15329, 6417, 257, 2121, 383, 9362, 382, 731, 13, 467, 311, 3163, 51612], "temperature": 0.0, "avg_logprob": -0.16462293831077782, "compression_ratio": 1.4564102564102563, "no_speech_prob": 0.00816685426980257}, {"id": 91, "seek": 67464, "start": 674.64, "end": 680.56, "text": " mentioning that internal representation of daytime and time delta in Python is not a timestamp.", "tokens": [50364, 18315, 300, 6920, 10290, 295, 31908, 293, 565, 8289, 294, 15329, 307, 406, 257, 49108, 1215, 13, 50660], "temperature": 0.0, "avg_logprob": -0.11997121509752776, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0515347383916378}, {"id": 92, "seek": 67464, "start": 680.56, "end": 686.96, "text": " It's not a single integer like in other programming languages. It's really a struct that contains the", "tokens": [50660, 467, 311, 406, 257, 2167, 24922, 411, 294, 661, 9410, 8650, 13, 467, 311, 534, 257, 6594, 300, 8306, 264, 50980], "temperature": 0.0, "avg_logprob": -0.11997121509752776, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0515347383916378}, {"id": 93, "seek": 67464, "start": 686.96, "end": 695.04, "text": " number of year, the number of months, the number of days, seconds, and so on. So, you just use", "tokens": [50980, 1230, 295, 1064, 11, 264, 1230, 295, 2493, 11, 264, 1230, 295, 1708, 11, 3949, 11, 293, 370, 322, 13, 407, 11, 291, 445, 764, 51384], "temperature": 0.0, "avg_logprob": -0.11997121509752776, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0515347383916378}, {"id": 94, "seek": 67464, "start": 695.04, "end": 702.48, "text": " these getters and you serialize each integer to the output. Worse fast. The same for time delta,", "tokens": [51384, 613, 483, 1559, 293, 291, 17436, 1125, 1184, 24922, 281, 264, 5598, 13, 343, 18699, 2370, 13, 440, 912, 337, 565, 8289, 11, 51756], "temperature": 0.0, "avg_logprob": -0.11997121509752776, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0515347383916378}, {"id": 95, "seek": 70248, "start": 702.48, "end": 711.28, "text": " you get days and seconds. All of that allowed us to speed up pandas array, pandas data frame", "tokens": [50364, 291, 483, 1708, 293, 3949, 13, 1057, 295, 300, 4350, 505, 281, 3073, 493, 4565, 296, 10225, 11, 4565, 296, 1412, 3920, 50804], "temperature": 0.0, "avg_logprob": -0.13603910258118534, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.010236582718789577}, {"id": 96, "seek": 70248, "start": 711.28, "end": 718.88, "text": " serialization by roughly 20x. And, of course, the speed up can be really different depending on the", "tokens": [50804, 17436, 2144, 538, 9810, 945, 87, 13, 400, 11, 295, 1164, 11, 264, 3073, 493, 393, 312, 534, 819, 5413, 322, 264, 51184], "temperature": 0.0, "avg_logprob": -0.13603910258118534, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.010236582718789577}, {"id": 97, "seek": 70248, "start": 718.88, "end": 725.04, "text": " nature of the data frame. If it only contains integers and floating points, the speed up will", "tokens": [51184, 3687, 295, 264, 1412, 3920, 13, 759, 309, 787, 8306, 41674, 293, 12607, 2793, 11, 264, 3073, 493, 486, 51492], "temperature": 0.0, "avg_logprob": -0.13603910258118534, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.010236582718789577}, {"id": 98, "seek": 72504, "start": 725.04, "end": 734.0, "text": " be marginal. Pickle actually is working quite fast, so it works well for strongly typed numpy", "tokens": [50364, 312, 16885, 13, 14129, 306, 767, 307, 1364, 1596, 2370, 11, 370, 309, 1985, 731, 337, 10613, 33941, 1031, 8200, 50812], "temperature": 0.0, "avg_logprob": -0.1627465532971667, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.047462914139032364}, {"id": 99, "seek": 72504, "start": 734.0, "end": 742.24, "text": " arrays, actually. So, does it mean that we kind of managed to beat Pickle? Well, yes and no, because", "tokens": [50812, 41011, 11, 767, 13, 407, 11, 775, 309, 914, 300, 321, 733, 295, 6453, 281, 4224, 14129, 306, 30, 1042, 11, 2086, 293, 572, 11, 570, 51224], "temperature": 0.0, "avg_logprob": -0.1627465532971667, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.047462914139032364}, {"id": 100, "seek": 72504, "start": 742.9599999999999, "end": 749.04, "text": " Pickle is universal and it can do all things at once. And it's way harder to be fast at all", "tokens": [51260, 14129, 306, 307, 11455, 293, 309, 393, 360, 439, 721, 412, 1564, 13, 400, 309, 311, 636, 6081, 281, 312, 2370, 412, 439, 51564], "temperature": 0.0, "avg_logprob": -0.1627465532971667, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.047462914139032364}, {"id": 101, "seek": 74904, "start": 749.04, "end": 755.12, "text": " possible use cases than be super fast at one particular use case. So, I don't know how to say.", "tokens": [50364, 1944, 764, 3331, 813, 312, 1687, 2370, 412, 472, 1729, 764, 1389, 13, 407, 11, 286, 500, 380, 458, 577, 281, 584, 13, 50668], "temperature": 0.0, "avg_logprob": -0.08553001755162289, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.03696240484714508}, {"id": 102, "seek": 74904, "start": 755.12, "end": 762.64, "text": " Maybe yes, maybe no. But, however, there is also an elephant in the room. Can you see it?", "tokens": [50668, 2704, 2086, 11, 1310, 572, 13, 583, 11, 4461, 11, 456, 307, 611, 364, 19791, 294, 264, 1808, 13, 1664, 291, 536, 309, 30, 51044], "temperature": 0.0, "avg_logprob": -0.08553001755162289, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.03696240484714508}, {"id": 103, "seek": 74904, "start": 766.64, "end": 773.28, "text": " So far, I only talked about how fast we were at serializing objects, serializing data frames.", "tokens": [51244, 407, 1400, 11, 286, 787, 2825, 466, 577, 2370, 321, 645, 412, 17436, 3319, 6565, 11, 17436, 3319, 1412, 12083, 13, 51576], "temperature": 0.0, "avg_logprob": -0.08553001755162289, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.03696240484714508}, {"id": 104, "seek": 77328, "start": 773.28, "end": 780.0, "text": " But supposedly, this is not what you do. Only that you do, you also need to deserialize them back.", "tokens": [50364, 583, 20581, 11, 341, 307, 406, 437, 291, 360, 13, 5686, 300, 291, 360, 11, 291, 611, 643, 281, 730, 260, 831, 1125, 552, 646, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1901983552508884, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.04755353555083275}, {"id": 105, "seek": 77328, "start": 781.04, "end": 786.8, "text": " Otherwise, that will be this anecdote for a compressor that compresses everything to one byte,", "tokens": [50752, 10328, 11, 300, 486, 312, 341, 49845, 337, 257, 28765, 300, 14778, 279, 1203, 281, 472, 40846, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1901983552508884, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.04755353555083275}, {"id": 106, "seek": 77328, "start": 786.8, "end": 794.3199999999999, "text": " just kind of decompress back. And decompressing back from this format is also complex, but since", "tokens": [51040, 445, 733, 295, 22867, 735, 646, 13, 400, 22867, 18605, 646, 490, 341, 7877, 307, 611, 3997, 11, 457, 1670, 51416], "temperature": 0.0, "avg_logprob": -0.1901983552508884, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.04755353555083275}, {"id": 107, "seek": 79432, "start": 795.2, "end": 804.0, "text": " I don't have much time to talk about it, I'd rather leave it for next time. But in brief,", "tokens": [50408, 286, 500, 380, 362, 709, 565, 281, 751, 466, 309, 11, 286, 1116, 2831, 1856, 309, 337, 958, 565, 13, 583, 294, 5353, 11, 50848], "temperature": 0.0, "avg_logprob": -0.15264462100134957, "compression_ratio": 1.4224598930481283, "no_speech_prob": 0.036781854927539825}, {"id": 108, "seek": 79432, "start": 804.0, "end": 815.5200000000001, "text": " this is the place where I had to do some extra actions to stay performing. So, yeah. The second", "tokens": [50848, 341, 307, 264, 1081, 689, 286, 632, 281, 360, 512, 2857, 5909, 281, 1754, 10205, 13, 407, 11, 1338, 13, 440, 1150, 51424], "temperature": 0.0, "avg_logprob": -0.15264462100134957, "compression_ratio": 1.4224598930481283, "no_speech_prob": 0.036781854927539825}, {"id": 109, "seek": 79432, "start": 815.5200000000001, "end": 821.44, "text": " part of the talk, how we solved a similar problem, but for Jason's serialization", "tokens": [51424, 644, 295, 264, 751, 11, 577, 321, 13041, 257, 2531, 1154, 11, 457, 337, 11181, 311, 17436, 2144, 51720], "temperature": 0.0, "avg_logprob": -0.15264462100134957, "compression_ratio": 1.4224598930481283, "no_speech_prob": 0.036781854927539825}, {"id": 110, "seek": 82144, "start": 822.4000000000001, "end": 830.96, "text": " in the backend, this is what we had. A lot of models like a data class with some fields inside,", "tokens": [50412, 294, 264, 38087, 11, 341, 307, 437, 321, 632, 13, 316, 688, 295, 5245, 411, 257, 1412, 1508, 365, 512, 7909, 1854, 11, 50840], "temperature": 0.0, "avg_logprob": -0.2085508296364232, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.018697589635849}, {"id": 111, "seek": 82144, "start": 832.0, "end": 840.6400000000001, "text": " strings, floating points, nested lists, nested models. It could be several levels deep. If you", "tokens": [50892, 13985, 11, 12607, 2793, 11, 15646, 292, 14511, 11, 15646, 292, 5245, 13, 467, 727, 312, 2940, 4358, 2452, 13, 759, 291, 51324], "temperature": 0.0, "avg_logprob": -0.2085508296364232, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.018697589635849}, {"id": 112, "seek": 82144, "start": 840.6400000000001, "end": 848.1600000000001, "text": " work with a first API that can seem familiar, if I'm not mistaken, by-identic, can offer", "tokens": [51324, 589, 365, 257, 700, 9362, 300, 393, 1643, 4963, 11, 498, 286, 478, 406, 21333, 11, 538, 12, 1078, 299, 11, 393, 2626, 51700], "temperature": 0.0, "avg_logprob": -0.2085508296364232, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.018697589635849}, {"id": 113, "seek": 84816, "start": 848.24, "end": 852.3199999999999, "text": " similar structure. So, it's not really about using data class or something else.", "tokens": [50368, 2531, 3877, 13, 407, 11, 309, 311, 406, 534, 466, 1228, 1412, 1508, 420, 746, 1646, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13138912148671608, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.016423529013991356}, {"id": 114, "seek": 84816, "start": 855.28, "end": 863.4399999999999, "text": " A lot of us saw that. And the problem was we had thousands of such objects. And we needed to cache", "tokens": [50720, 316, 688, 295, 505, 1866, 300, 13, 400, 264, 1154, 390, 321, 632, 5383, 295, 1270, 6565, 13, 400, 321, 2978, 281, 19459, 51128], "temperature": 0.0, "avg_logprob": -0.13138912148671608, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.016423529013991356}, {"id": 115, "seek": 84816, "start": 863.4399999999999, "end": 873.1999999999999, "text": " them and surf pagination. Again, this is probably an anti-pattern in a way, because you would ideally", "tokens": [51128, 552, 293, 9684, 11812, 2486, 13, 3764, 11, 341, 307, 1391, 364, 6061, 12, 79, 1161, 77, 294, 257, 636, 11, 570, 291, 576, 22915, 51616], "temperature": 0.0, "avg_logprob": -0.13138912148671608, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.016423529013991356}, {"id": 116, "seek": 87320, "start": 873.2, "end": 878.4000000000001, "text": " push down all the filters in the backend so that you don't have to materialize the whole", "tokens": [50364, 2944, 760, 439, 264, 15995, 294, 264, 38087, 370, 300, 291, 500, 380, 362, 281, 2527, 1125, 264, 1379, 50624], "temperature": 0.0, "avg_logprob": -0.10588333051498622, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.023320676758885384}, {"id": 117, "seek": 87320, "start": 878.4000000000001, "end": 888.88, "text": " list of objects to return. But in our case, we really had to do that. So, we loaded all the models", "tokens": [50624, 1329, 295, 6565, 281, 2736, 13, 583, 294, 527, 1389, 11, 321, 534, 632, 281, 360, 300, 13, 407, 11, 321, 13210, 439, 264, 5245, 51148], "temperature": 0.0, "avg_logprob": -0.10588333051498622, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.023320676758885384}, {"id": 118, "seek": 87320, "start": 888.88, "end": 895.76, "text": " from cache. Then we deserialized the bytes to the actual Python objects, these data classes.", "tokens": [51148, 490, 19459, 13, 1396, 321, 730, 260, 831, 1602, 264, 36088, 281, 264, 3539, 15329, 6565, 11, 613, 1412, 5359, 13, 51492], "temperature": 0.0, "avg_logprob": -0.10588333051498622, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.023320676758885384}, {"id": 119, "seek": 89576, "start": 896.56, "end": 903.4399999999999, "text": " We took on a dose that corresponded to the pagination and we converted them to Jason's string.", "tokens": [50404, 492, 1890, 322, 257, 14041, 300, 6805, 292, 281, 264, 11812, 2486, 293, 321, 16424, 552, 281, 11181, 311, 6798, 13, 50748], "temperature": 0.0, "avg_logprob": -0.12798389036263993, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.013554646633565426}, {"id": 120, "seek": 89576, "start": 903.4399999999999, "end": 911.36, "text": " So, two things went wrong here. Deserializing everything was super slow and also converting", "tokens": [50748, 407, 11, 732, 721, 1437, 2085, 510, 13, 3885, 260, 831, 3319, 1203, 390, 1687, 2964, 293, 611, 29942, 51144], "temperature": 0.0, "avg_logprob": -0.12798389036263993, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.013554646633565426}, {"id": 121, "seek": 89576, "start": 911.36, "end": 919.6, "text": " to Jason was also slow. Our first attempt to fix it was actually lightweight. Let's just", "tokens": [51144, 281, 11181, 390, 611, 2964, 13, 2621, 700, 5217, 281, 3191, 309, 390, 767, 22052, 13, 961, 311, 445, 51556], "temperature": 0.0, "avg_logprob": -0.12798389036263993, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.013554646633565426}, {"id": 122, "seek": 91960, "start": 920.4, "end": 932.0, "text": " pre-convert all data classes to atomic objects, like dictionaries and lists. C Python is really,", "tokens": [50404, 659, 12, 1671, 3281, 439, 1412, 5359, 281, 22275, 6565, 11, 411, 22352, 4889, 293, 14511, 13, 383, 15329, 307, 534, 11, 50984], "temperature": 0.0, "avg_logprob": -0.22147745481679137, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.020872673019766808}, {"id": 123, "seek": 91960, "start": 932.0, "end": 939.12, "text": " really, really fast. It is working with Dx and lists. So, we assumed that that would help.", "tokens": [50984, 534, 11, 534, 2370, 13, 467, 307, 1364, 365, 413, 87, 293, 14511, 13, 407, 11, 321, 15895, 300, 300, 576, 854, 13, 51340], "temperature": 0.0, "avg_logprob": -0.22147745481679137, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.020872673019766808}, {"id": 124, "seek": 91960, "start": 940.0, "end": 947.36, "text": " Then you just store the cache, those atomic basic objects, and you return those that's", "tokens": [51384, 1396, 291, 445, 3531, 264, 19459, 11, 729, 22275, 3875, 6565, 11, 293, 291, 2736, 729, 300, 311, 51752], "temperature": 0.0, "avg_logprob": -0.22147745481679137, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.020872673019766808}, {"id": 125, "seek": 94736, "start": 947.36, "end": 953.12, "text": " requested in the pagination. So, that was for the first call, where you create the cache.", "tokens": [50364, 16436, 294, 264, 11812, 2486, 13, 407, 11, 300, 390, 337, 264, 700, 818, 11, 689, 291, 1884, 264, 19459, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12254049777984619, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.008248486556112766}, {"id": 126, "seek": 94736, "start": 953.76, "end": 959.44, "text": " For the next call, you just serve those objects that are corresponding to the pagination.", "tokens": [50684, 1171, 264, 958, 818, 11, 291, 445, 4596, 729, 6565, 300, 366, 11760, 281, 264, 11812, 2486, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12254049777984619, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.008248486556112766}, {"id": 127, "seek": 94736, "start": 961.12, "end": 966.5600000000001, "text": " However, it was still slow. It was slow because conversion from data classes to", "tokens": [51052, 2908, 11, 309, 390, 920, 2964, 13, 467, 390, 2964, 570, 14298, 490, 1412, 5359, 281, 51324], "temperature": 0.0, "avg_logprob": -0.12254049777984619, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.008248486556112766}, {"id": 128, "seek": 94736, "start": 967.92, "end": 973.6, "text": " basic objects through data classes to dict was really slow and painfully slow.", "tokens": [51392, 3875, 6565, 807, 1412, 5359, 281, 12569, 390, 534, 2964, 293, 1822, 2277, 2964, 13, 51676], "temperature": 0.0, "avg_logprob": -0.12254049777984619, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.008248486556112766}, {"id": 129, "seek": 97360, "start": 974.5600000000001, "end": 979.84, "text": " And serializing basic objects to Jason was also kind of slow.", "tokens": [50412, 400, 17436, 3319, 3875, 6565, 281, 11181, 390, 611, 733, 295, 2964, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1371627030549226, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.011924752034246922}, {"id": 130, "seek": 97360, "start": 981.52, "end": 990.72, "text": " And just for the subsequent calls, we had problems with deserializing all the objects.", "tokens": [50760, 400, 445, 337, 264, 19962, 5498, 11, 321, 632, 2740, 365, 730, 260, 831, 3319, 439, 264, 6565, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1371627030549226, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.011924752034246922}, {"id": 131, "seek": 97360, "start": 990.72, "end": 997.0400000000001, "text": " It was not great. Materializing a lot of Python objects requires you to do a lot of rev count", "tokens": [51220, 467, 390, 406, 869, 13, 29160, 3319, 257, 688, 295, 15329, 6565, 7029, 291, 281, 360, 257, 688, 295, 3698, 1207, 51536], "temperature": 0.0, "avg_logprob": -0.1371627030549226, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.011924752034246922}, {"id": 132, "seek": 97360, "start": 997.0400000000001, "end": 1002.08, "text": " increments and it just cannot work fast. There is no way it can work fast.", "tokens": [51536, 1946, 1117, 293, 309, 445, 2644, 589, 2370, 13, 821, 307, 572, 636, 309, 393, 589, 2370, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1371627030549226, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.011924752034246922}, {"id": 133, "seek": 100208, "start": 1003.0400000000001, "end": 1009.12, "text": " So, we had to be inventive and add this to Jason extra value function", "tokens": [50412, 407, 11, 321, 632, 281, 312, 7962, 488, 293, 909, 341, 281, 11181, 2857, 2158, 2445, 50716], "temperature": 0.0, "avg_logprob": -0.17437787489457565, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.010848470963537693}, {"id": 134, "seek": 100208, "start": 1009.9200000000001, "end": 1018.96, "text": " that pre-serializes all the objects to Jason and also produces the index where each object begins.", "tokens": [50756, 300, 659, 12, 12484, 831, 5660, 439, 264, 6565, 281, 11181, 293, 611, 14725, 264, 8186, 689, 1184, 2657, 7338, 13, 51208], "temperature": 0.0, "avg_logprob": -0.17437787489457565, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.010848470963537693}, {"id": 135, "seek": 100208, "start": 1020.48, "end": 1027.92, "text": " And this table of contents can be used in the future. When you have pagination calls, you just", "tokens": [51284, 400, 341, 3199, 295, 15768, 393, 312, 1143, 294, 264, 2027, 13, 1133, 291, 362, 11812, 2486, 5498, 11, 291, 445, 51656], "temperature": 0.0, "avg_logprob": -0.17437787489457565, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.010848470963537693}, {"id": 136, "seek": 102792, "start": 1028.64, "end": 1035.76, "text": " select the part of this huge Jason blob corresponding to the pagination and you return", "tokens": [50400, 3048, 264, 644, 295, 341, 2603, 11181, 46115, 11760, 281, 264, 11812, 2486, 293, 291, 2736, 50756], "temperature": 0.0, "avg_logprob": -0.10228473632062068, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.02477179653942585}, {"id": 137, "seek": 102792, "start": 1035.76, "end": 1045.44, "text": " it. And since you only work with strings, this works fast. However, it really depends on the", "tokens": [50756, 309, 13, 400, 1670, 291, 787, 589, 365, 13985, 11, 341, 1985, 2370, 13, 2908, 11, 309, 534, 5946, 322, 264, 51240], "temperature": 0.0, "avg_logprob": -0.10228473632062068, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.02477179653942585}, {"id": 138, "seek": 102792, "start": 1045.44, "end": 1051.44, "text": " performance of this function that converts data classes to this huge Jason string with a table", "tokens": [51240, 3389, 295, 341, 2445, 300, 38874, 1412, 5359, 281, 341, 2603, 11181, 6798, 365, 257, 3199, 51540], "temperature": 0.0, "avg_logprob": -0.10228473632062068, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.02477179653942585}, {"id": 139, "seek": 105144, "start": 1051.44, "end": 1060.96, "text": " of contents. And, yeah, this can be skipped really. This function had to be implemented", "tokens": [50364, 295, 15768, 13, 400, 11, 1338, 11, 341, 393, 312, 30193, 534, 13, 639, 2445, 632, 281, 312, 12270, 50840], "temperature": 0.0, "avg_logprob": -0.12018081323424382, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031114770099520683}, {"id": 140, "seek": 105144, "start": 1060.96, "end": 1068.0, "text": " from scratch, unfortunately, because Jason dumps doesn't specify the table of contents and it cannot", "tokens": [50840, 490, 8459, 11, 7015, 11, 570, 11181, 11430, 82, 1177, 380, 16500, 264, 3199, 295, 15768, 293, 309, 2644, 51192], "temperature": 0.0, "avg_logprob": -0.12018081323424382, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031114770099520683}, {"id": 141, "seek": 105144, "start": 1068.0, "end": 1073.3600000000001, "text": " do that. Also, to convert data classes to Jason, you still need to convert them to basic objects", "tokens": [51192, 360, 300, 13, 2743, 11, 281, 7620, 1412, 5359, 281, 11181, 11, 291, 920, 643, 281, 7620, 552, 281, 3875, 6565, 51460], "temperature": 0.0, "avg_logprob": -0.12018081323424382, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031114770099520683}, {"id": 142, "seek": 107336, "start": 1073.36, "end": 1081.76, "text": " first using 2D. Jason dumps cannot convert data classes directly. And the only way for us to", "tokens": [50364, 700, 1228, 568, 35, 13, 11181, 11430, 82, 2644, 7620, 1412, 5359, 3838, 13, 400, 264, 787, 636, 337, 505, 281, 50784], "temperature": 0.0, "avg_logprob": -0.14193621803732479, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.045448604971170425}, {"id": 143, "seek": 107336, "start": 1081.76, "end": 1090.1599999999999, "text": " move on was to really write our own serializer. And this serializer works using a so-called", "tokens": [50784, 1286, 322, 390, 281, 534, 2464, 527, 1065, 17436, 6545, 13, 400, 341, 17436, 6545, 1985, 1228, 257, 370, 12, 11880, 51204], "temperature": 0.0, "avg_logprob": -0.14193621803732479, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.045448604971170425}, {"id": 144, "seek": 107336, "start": 1090.1599999999999, "end": 1098.6399999999999, "text": " specification of the serialization. The thing is these data classes are typed and you can scan", "tokens": [51204, 31256, 295, 264, 17436, 2144, 13, 440, 551, 307, 613, 1412, 5359, 366, 33941, 293, 291, 393, 11049, 51628], "temperature": 0.0, "avg_logprob": -0.14193621803732479, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.045448604971170425}, {"id": 145, "seek": 109864, "start": 1098.64, "end": 1104.5600000000002, "text": " these types to build some plan, how you should perform this serialization, how you should iterate", "tokens": [50364, 613, 3467, 281, 1322, 512, 1393, 11, 577, 291, 820, 2042, 341, 17436, 2144, 11, 577, 291, 820, 44497, 50660], "temperature": 0.0, "avg_logprob": -0.18994011417511972, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.017321914434432983}, {"id": 146, "seek": 109864, "start": 1104.5600000000002, "end": 1111.44, "text": " the objects and how you should write them to Jason. Apart from that, we had to implement", "tokens": [50660, 264, 6565, 293, 577, 291, 820, 2464, 552, 281, 11181, 13, 24111, 490, 300, 11, 321, 632, 281, 4445, 51004], "temperature": 0.0, "avg_logprob": -0.18994011417511972, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.017321914434432983}, {"id": 147, "seek": 109864, "start": 1112.3200000000002, "end": 1117.5200000000002, "text": " iterating lists and dicks. Well, we kind of already covered that. Converting integers and", "tokens": [51048, 17138, 990, 14511, 293, 274, 7663, 13, 1042, 11, 321, 733, 295, 1217, 5343, 300, 13, 2656, 331, 783, 41674, 293, 51308], "temperature": 0.0, "avg_logprob": -0.18994011417511972, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.017321914434432983}, {"id": 148, "seek": 109864, "start": 1117.5200000000002, "end": 1125.2, "text": " floating points to a string. This is really basic. Into stir, float to stir and others. We just don't", "tokens": [51308, 12607, 2793, 281, 257, 6798, 13, 639, 307, 534, 3875, 13, 23373, 8946, 11, 15706, 281, 8946, 293, 2357, 13, 492, 445, 500, 380, 51692], "temperature": 0.0, "avg_logprob": -0.18994011417511972, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.017321914434432983}, {"id": 149, "seek": 112520, "start": 1125.28, "end": 1130.0, "text": " think about it when you work with them in Python. You just convert them to string, problem solved.", "tokens": [50368, 519, 466, 309, 562, 291, 589, 365, 552, 294, 15329, 13, 509, 445, 7620, 552, 281, 6798, 11, 1154, 13041, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1407990907367907, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.07676272839307785}, {"id": 150, "seek": 112520, "start": 1130.56, "end": 1139.6000000000001, "text": " However, since it works with HIP and it touches the internal sort of Python, you cannot use it", "tokens": [50632, 2908, 11, 1670, 309, 1985, 365, 389, 9139, 293, 309, 17431, 264, 6920, 1333, 295, 15329, 11, 291, 2644, 764, 309, 51084], "temperature": 0.0, "avg_logprob": -0.1407990907367907, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.07676272839307785}, {"id": 151, "seek": 112520, "start": 1139.6000000000001, "end": 1146.24, "text": " inside and you have to reimplement it from scratch. The same is daytime. Finally, for strings,", "tokens": [51084, 1854, 293, 291, 362, 281, 33433, 43704, 309, 490, 8459, 13, 440, 912, 307, 31908, 13, 6288, 11, 337, 13985, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1407990907367907, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.07676272839307785}, {"id": 152, "seek": 112520, "start": 1146.24, "end": 1152.24, "text": " you need to escape them. If you have a double column inside a string, you need to escape it,", "tokens": [51416, 291, 643, 281, 7615, 552, 13, 759, 291, 362, 257, 3834, 7738, 1854, 257, 6798, 11, 291, 643, 281, 7615, 309, 11, 51716], "temperature": 0.0, "avg_logprob": -0.1407990907367907, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.07676272839307785}, {"id": 153, "seek": 115224, "start": 1152.24, "end": 1162.4, "text": " the same about new line. And since Jason is UTF-8 and internal binary encoding of Python strings", "tokens": [50364, 264, 912, 466, 777, 1622, 13, 400, 1670, 11181, 307, 624, 20527, 12, 23, 293, 6920, 17434, 43430, 295, 15329, 13985, 50872], "temperature": 0.0, "avg_logprob": -0.12475336225409257, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.01143152080476284}, {"id": 154, "seek": 115224, "start": 1162.4, "end": 1170.88, "text": " can be one byte, two byte, four byte, we also need to do this conversion. So this is quite interesting.", "tokens": [50872, 393, 312, 472, 40846, 11, 732, 40846, 11, 1451, 40846, 11, 321, 611, 643, 281, 360, 341, 14298, 13, 407, 341, 307, 1596, 1880, 13, 51296], "temperature": 0.0, "avg_logprob": -0.12475336225409257, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.01143152080476284}, {"id": 155, "seek": 115224, "start": 1171.6, "end": 1180.08, "text": " So the main trick to serialize is if you have slots in your data class, these slots are stored as", "tokens": [51332, 407, 264, 2135, 4282, 281, 17436, 1125, 307, 498, 291, 362, 24266, 294, 428, 1412, 1508, 11, 613, 24266, 366, 12187, 382, 51756], "temperature": 0.0, "avg_logprob": -0.12475336225409257, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.01143152080476284}, {"id": 156, "seek": 118008, "start": 1180.1599999999999, "end": 1188.6399999999999, "text": " pointers inside the PyObject structure. You take the type of data class, you get the slot members", "tokens": [50368, 44548, 1854, 264, 9953, 45483, 1020, 3877, 13, 509, 747, 264, 2010, 295, 1412, 1508, 11, 291, 483, 264, 14747, 2679, 50792], "temperature": 0.0, "avg_logprob": -0.1212612972703091, "compression_ratio": 1.65, "no_speech_prob": 0.04206385090947151}, {"id": 157, "seek": 118008, "start": 1188.6399999999999, "end": 1194.24, "text": " through C API, and each member contains the byte offset where the pointers exist.", "tokens": [50792, 807, 383, 9362, 11, 293, 1184, 4006, 8306, 264, 40846, 18687, 689, 264, 44548, 2514, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1212612972703091, "compression_ratio": 1.65, "no_speech_prob": 0.04206385090947151}, {"id": 158, "seek": 118008, "start": 1195.04, "end": 1202.32, "text": " You just read these pointers and you serialize the objects. Therefore, the serialization spec", "tokens": [51112, 509, 445, 1401, 613, 44548, 293, 291, 17436, 1125, 264, 6565, 13, 7504, 11, 264, 17436, 2144, 1608, 51476], "temperature": 0.0, "avg_logprob": -0.1212612972703091, "compression_ratio": 1.65, "no_speech_prob": 0.04206385090947151}, {"id": 159, "seek": 118008, "start": 1202.32, "end": 1208.1599999999999, "text": " is recursive. You have the data type that you need to convert to JSON and you have a list", "tokens": [51476, 307, 20560, 488, 13, 509, 362, 264, 1412, 2010, 300, 291, 643, 281, 7620, 281, 31828, 293, 291, 362, 257, 1329, 51768], "temperature": 0.0, "avg_logprob": -0.1212612972703091, "compression_ratio": 1.65, "no_speech_prob": 0.04206385090947151}, {"id": 160, "seek": 120816, "start": 1209.1200000000001, "end": 1218.8000000000002, "text": " of nested specifications with some slot offset. We only support predefined types like lists,", "tokens": [50412, 295, 15646, 292, 29448, 365, 512, 14747, 18687, 13, 492, 787, 1406, 659, 37716, 3467, 411, 14511, 11, 50896], "temperature": 0.0, "avg_logprob": -0.17824588102452896, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.00919342041015625}, {"id": 161, "seek": 120816, "start": 1218.8000000000002, "end": 1225.0400000000002, "text": " floating points, strings, boolean values, times, not everything. We are not universally have", "tokens": [50896, 12607, 2793, 11, 13985, 11, 748, 4812, 282, 4190, 11, 1413, 11, 406, 1203, 13, 492, 366, 406, 43995, 362, 51208], "temperature": 0.0, "avg_logprob": -0.17824588102452896, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.00919342041015625}, {"id": 162, "seek": 120816, "start": 1225.0400000000002, "end": 1231.2, "text": " constraints, but it allows us to perform faster. Oh, the code listing didn't load for some reason.", "tokens": [51208, 18491, 11, 457, 309, 4045, 505, 281, 2042, 4663, 13, 876, 11, 264, 3089, 22161, 994, 380, 3677, 337, 512, 1778, 13, 51516], "temperature": 0.0, "avg_logprob": -0.17824588102452896, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.00919342041015625}, {"id": 163, "seek": 123120, "start": 1231.76, "end": 1238.0, "text": " Anyway, I don't have time for that. This serializer appeared to be 100x faster,", "tokens": [50392, 5684, 11, 286, 500, 380, 362, 565, 337, 300, 13, 639, 17436, 6545, 8516, 281, 312, 2319, 87, 4663, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1307336621814304, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.010147673077881336}, {"id": 164, "seek": 123120, "start": 1238.0, "end": 1243.68, "text": " really, really, really fast, so fast that we just don't see it in our traces and profile", "tokens": [50704, 534, 11, 534, 11, 534, 2370, 11, 370, 2370, 300, 321, 445, 500, 380, 536, 309, 294, 527, 26076, 293, 7964, 50988], "temperature": 0.0, "avg_logprob": -0.1307336621814304, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.010147673077881336}, {"id": 165, "seek": 123120, "start": 1243.68, "end": 1249.52, "text": " anymore. So the goal was achieved. And it can be improved in the future by pre-computing the", "tokens": [50988, 3602, 13, 407, 264, 3387, 390, 11042, 13, 400, 309, 393, 312, 9689, 294, 264, 2027, 538, 659, 12, 1112, 2582, 278, 264, 51280], "temperature": 0.0, "avg_logprob": -0.1307336621814304, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.010147673077881336}, {"id": 166, "seek": 124952, "start": 1250.08, "end": 1262.24, "text": " serialization spec inside the data class member. Final advice is doing less is doing more. I think", "tokens": [50392, 17436, 2144, 1608, 1854, 264, 1412, 1508, 4006, 13, 13443, 5192, 307, 884, 1570, 307, 884, 544, 13, 286, 519, 51000], "temperature": 0.0, "avg_logprob": -0.1214998926435198, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.07464837282896042}, {"id": 167, "seek": 124952, "start": 1262.24, "end": 1268.6399999999999, "text": " it's also mentioned in Zen of Python, probably. Know your tools. It's always a great idea to", "tokens": [51000, 309, 311, 611, 2835, 294, 22387, 295, 15329, 11, 1391, 13, 10265, 428, 3873, 13, 467, 311, 1009, 257, 869, 1558, 281, 51320], "temperature": 0.0, "avg_logprob": -0.1214998926435198, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.07464837282896042}, {"id": 168, "seek": 124952, "start": 1268.6399999999999, "end": 1275.04, "text": " know your tools and learn the internals, how they work, because it will allow you to write", "tokens": [51320, 458, 428, 3873, 293, 1466, 264, 2154, 1124, 11, 577, 436, 589, 11, 570, 309, 486, 2089, 291, 281, 2464, 51640], "temperature": 0.0, "avg_logprob": -0.1214998926435198, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.07464837282896042}, {"id": 169, "seek": 127504, "start": 1275.04, "end": 1281.12, "text": " more performance programs. Going low level when you learn the internals is your super", "tokens": [50364, 544, 3389, 4268, 13, 10963, 2295, 1496, 562, 291, 1466, 264, 2154, 1124, 307, 428, 1687, 50668], "temperature": 0.0, "avg_logprob": -0.10460574206183938, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.029984930530190468}, {"id": 170, "seek": 127504, "start": 1281.12, "end": 1287.28, "text": " weapon that allows you to do many things. You can break the rules and you can do magic. However,", "tokens": [50668, 7463, 300, 4045, 291, 281, 360, 867, 721, 13, 509, 393, 1821, 264, 4474, 293, 291, 393, 360, 5585, 13, 2908, 11, 50976], "temperature": 0.0, "avg_logprob": -0.10460574206183938, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.029984930530190468}, {"id": 171, "seek": 127504, "start": 1287.28, "end": 1292.96, "text": " you should only do that if you've got your architecture right. Otherwise, it's just stupid.", "tokens": [50976, 291, 820, 787, 360, 300, 498, 291, 600, 658, 428, 9482, 558, 13, 10328, 11, 309, 311, 445, 6631, 13, 51260], "temperature": 0.0, "avg_logprob": -0.10460574206183938, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.029984930530190468}, {"id": 172, "seek": 127504, "start": 1292.96, "end": 1298.48, "text": " You optimize the place that shouldn't be optimized in the first place. And also,", "tokens": [51260, 509, 19719, 264, 1081, 300, 4659, 380, 312, 26941, 294, 264, 700, 1081, 13, 400, 611, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10460574206183938, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.029984930530190468}, {"id": 173, "seek": 129848, "start": 1298.56, "end": 1304.56, "text": " with great power comes consequences, because when we release the GIL, we cannot use", "tokens": [50368, 365, 869, 1347, 1487, 10098, 11, 570, 562, 321, 4374, 264, 460, 4620, 11, 321, 2644, 764, 50668], "temperature": 0.0, "avg_logprob": -0.2570960825139826, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.07617215067148209}, {"id": 174, "seek": 129848, "start": 1305.3600000000001, "end": 1309.44, "text": " almost all the standard library and we have to implement crazy things like", "tokens": [50708, 1920, 439, 264, 3832, 6405, 293, 321, 362, 281, 4445, 3219, 721, 411, 50912], "temperature": 0.0, "avg_logprob": -0.2570960825139826, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.07617215067148209}, {"id": 175, "seek": 129848, "start": 1309.44, "end": 1325.44, "text": " UTF conversions. This is annoying and you need to be prepared for that. Thanks.", "tokens": [50912, 624, 20527, 42256, 13, 639, 307, 11304, 293, 291, 643, 281, 312, 4927, 337, 300, 13, 2561, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2570960825139826, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.07617215067148209}, {"id": 176, "seek": 132848, "start": 1328.48, "end": 1329.8600000000001, "text": " you", "tokens": [50404, 291, 50433], "temperature": 0.0, "avg_logprob": -0.9989107251167297, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.8156115412712097}], "language": "en"}