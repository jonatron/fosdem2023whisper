{"text": " Yeah. Hi, everyone. My name is Christian Simon, and I'm going to be talking about continuous profiling. So we heard a lot about observability today already, and I'm going to want to introduce maybe an additional signal there. So maybe quickly about me. So I'm working at Grafana Labs. I'm a software engineer there, and worked on our databases for observability. So I worked on Cortex slash Mamiir now. I worked on Loki, and most recently I switched to the flat team, and I'm 50% of the flat team, and we kind of work on continuous profiling database. There's not going to be a particular focus on flat today. So basically what I want to talk today is kind of introduce how it's measured, what you can achieve with it, and then maybe as I learn more, at the next first time I can go more into detail and look at very specific languages there. So when we talk about observability, what are our common goals? Obviously we want to ensure that the user journeys of our users are successful, that we maybe even can be proactively find problems before a user notices it. And basically we want to be as quickly as possible when those problems happen to disrupt less of those user journeys. And observability provides us like an objective way of getting some insights into the state of our system in production. And even after a certain event has happened, we found the right way, reboot, and it's all up again. I think it might be able to help us understanding what exactly happened when we want to figure out the root cause of something. So one of the I guess easiest and probably oldest observability signals is logs. So like I guess it starts with a kind of print hello somewhere in your code. And I guess you probably don't need a show of hands who's using it. Like I guess everyone somehow uses logs or is asleep if they don't show a hand. So basically your application doesn't need any specific SDK. It can probably just log based on the standard library of your languages. One of the challenges, most of the time the format is rather varied. So like even in terms of timestamps, it can be really, really hard to get a common understanding of your log lines there. And also like when you then want to aggregate them, they are quite costly. So like it spides, you need to kind of convert them to in floats and so on. And also that something that can happen is that like you have so many logs that you can't find the ones that you're actually interested in. So like grab error, for example, could be, yeah, could be maybe helpful, but also like there might be just too many errors. And you kind of lose the important ones. So also like if you want to learn more about logs, my colleagues, Oven and Kavi, they're going to speak about Loki. So definitely stay in the room. And I'm going to move on to the next signal. So metrics, I'm also assuming pretty much everyone has come across them and is using them. So in this case, you kind of avoid that problem I mentioned before. You have the actual integers exposed. You maybe know about those integers that you care about them. So to get a metric, most of the time you have to do some kind of define a list of metrics you care about and then you can collect them. So it might be, you might be having kind of an outage and didn't have that metric that you care about. And so you need to kind of constantly improve on the exposure of the metrics. Obviously, like Peruvius is the kind of main tool in that space. And very often we talk about web services, I guess, when we think about those applications. So the red method, so like get the rates of your requests, get the error rate of your request, and get the latency duration of the request can already cover quite a lot of cases. And obviously, like as it's kind of just integers or floats, you can aggregate them quite efficiently across like, yeah, a multitude of pods or like a really huge set up of services. And then if you get more into that kind of microservices architecture that has kind of evolved over the last couple of years, you will find yourself kind of having a really complex composition of services being involved in answering requests. And so like, you might even struggle to understand what's slowing you down or where the error is coming from, why do I have this time out here? And distributed tracing can help you a lot with kind of getting an understanding what your service is doing. It also might be that like, maybe the service is actually doing way too much and you're calculating things over and over again. So that is super helpful to get a bit more like the kind of flow of the data in your system. So like the challenge there might be like, you might have a lot of requests and that's, while it's somewhat cheap to get the tracing, you might not cover all the requests. So for example, now production system, I think maybe someone needs to correct me if I'm wrong, but like when we receive a Grafana cloud like logs and metric data, we only cover 1% of those with traces while we cover 100% of our queries. So like, basically you need to make a selective decision if it's worth investing that. Obviously logs data looks always the same, it comes every second and so on. So like, we see more value in having all of those queries where there's a complex kind of caching and all sorts of systems interacting with it and so that allows us, yeah, to look a bit deeper and even then find that one service that maybe is the bottleneck there. So maybe looking at a bit of a real problem, so I'm having an online shop, I'm selling socks and a user is complaining about getting some time out when wanting to check out. That's obviously not great because I'm not selling the socks, but at least the user got some trace ID and is complaining to our customer service. Then starting from there, I'm figuring out it's the location service that actually was the one that cost the time out in the end. And then looking at the metrics of the location service, I might find, oh, there's actually 5% of the requests timing out, so maybe 5% of my users are not able to buy their socks monthly or whatever. So what are the next steps? I guess scaling up is always good. Maybe the service is just overloaded. The person that wrote it left years ago, so we have no idea. So we just scale it up. Obviously, it comes with a cost and so I think always the first thing would be fixing the bleed, making sure there are no more timeouts. So scaling up is definitely the right option here. But then if you do that over years, you might suddenly find yourself having a lot of extra costs attached to that location service. And so that's kind of where I think we need another signal. And I think that signal should be profiling. So I guess most people might have come across profiling. And it basically measures your code and how long it executes, for example, or what kind of bytes it allocates in memory. And it basically helps you maybe understand your program even more or someone else's program in the location service case. And that eventually can translate in cost savings if you find out where the problem lies, like you can maybe fix it or can get some ideas. Maybe you can also look at the fix and see if it's gotten actually worse or better. And yeah, like that basically gives you a better understanding of how your code behaves. And so now the question is, what is actually measured in a profile? So I created a bit of a program. I don't know. I hope everyone can see it. So it's basically like a program that has a main function and then calls out other functions. So you can see there's a do a lot and there's a do little function. And both of them then call prepare. And obviously in the comments, there's some work going on. And obviously the work could be allocating memory, like using the CPU, something like that. So let's say we use CPU. So when the function starts, like we are first going to do something within the main, let's say we spend three CPU cycles, which is not a lot, but that then gets recorded like yes, we took us three CPU cycles in main. We then go into the prepare method through do a lot. Then we spend another five CPU cycles. And those kind of stack traces then they are recorded in the profile. And going through the program, like we will end up with that kind of measurement of stack traces. And while it kind of works with ten lines of codes, you can maybe spot where the problem is. Like there's the 20 and do a lot. It definitely kind of breaks down when you're speaking about like a lot of services or like a lot of code base that is happened or happens to be hot and actively used. And so like there are a couple of ways of visualizing them. I think one of the first things you would find is kind of a top table. So like in that table, you can order it by different values. Like so this is kind of an example from P prof, like which is kind of the go tool. And you can see kind of clearly do a lot is the method that comes out on top. And like there are now different ways how you can look at the value. So you have the flat count, which is the function itself only. So you can see the 20 that we had before, 20 CPU cycles. But we also have the cumulative measurement, which also includes the prepare that is going to get called from do a lot. And so like we already can see we spend 52% of our program and do a lot. So maybe we already can stop looking at the table and just look at do a lot. Because if we fix do a lot or get rid of it, we need half of the CPU less. And that's that kind of, it's kind of represented by the sum. So the sum will always change depending on how you order the table in that particular example. So in this case, like we have 100% already at row number four, because we only have four functions. And to get a bit more of a visual sense of what's going on, there are the so-called flame graphs. And I think the most confusing thing for me about them was the coloring. So obviously like red is always not great. Should we look at main? No, we shouldn't. So basically like the coloring I think is random or uses some kind of hashing. And basically it's only meant to look like a flame. So like the red here doesn't mean anything. So like if you're colorblind, that's perfect for flame graphs. So what we actually want to look at is this kind of like at the leaf end is basically where the program would spend things. Like you can see the three CPU cycles main here. So there's nothing below. So main uses 100% through methods that are called by main. And then there's nothing beyond this here. So like here we spend something in main. And in the same way in do little, we can see the five. And in do a lot, we can see the 20 quite wide. And then the prepares with five each as well. And now obviously if you look across a really huge program, you basically like can can spot kind of what's going on quite quickly. And then if you have like similar with like route in main, like you basically can ignore that, but it helps you maybe locate which component of your of your program you want to look at because like maybe you're not good at naming and you call everything prepare in util and and it would still tell you roughly where it gets called and and how the program gets there. So how do we get that profile? And that can be kind of quite that can be quite a lot of challenges how to get that. So I think I would say like there's like roughly two ways like either your ecosystem supports kind of profiles fairly natively. And then you instrument the profile, you added maybe a library and SDK. And like basically like the runtime within your environment will maybe expose the information. So like it's not available for all languages. There's I guess a lot of work going on that it becomes more and more available. And kind of other approaches more like through an agent and EPPF has been quite hyped. I'm I'm not very familiar with the EPPF myself. I have used the agents but haven't written any code. But basically what it would use it would use an outside view of it. So you wouldn't need to change the binary running really like you would just kind of look at the information you get from the Linux kernel like you hook into, I don't know, often enough when the CPU runs to then find out what is currently running. So there are different languages like for example in a compiled language. You would be having a lot more information. The memory addresses are the same. You can kind of use the information within the simple table to figure out where your program is and what is currently running. In like I don't know like an interpreted language like Ruby, Python, this might be a bit harder and that information might not be accessible to the kernel without further work. Like it also when you compile you might drop those simple tables like so that you really need to kind of be preparing your application a bit for that. I want to look into the kind of prime example I'm most familiar with. I'm mostly a go developer over the last couple of years. Go has quite a kind of mature set of tools in that area. So basically the standard library allows you to expose that information. It supports like CPU and memory. And especially garbage collected languages. Memory is quite a thing to also non-garbage collected languages. But memory is really important to understand the usage there. I have a quick example of a program like so you basically like just expose an HTTP port where you can download the profile whenever you want and you have that P prof tool that you can point to it. So like in that kind of first line example you would just get a two second long profile. So CPU profile that looks at the CPU for two seconds and basically records like whatever is running how long on the CPU and then you get the file and P prof will allow you to visualize it through that top table for example. So what I forgot to mention as well. So later you can maybe go to that URL and look at that profile that I had as an example. And in the same way you can get the memory allocations and P prof also allows you to launch an HTTP server to be a bit more interactive and select certain code paths. So that is like quite a lot in the go docs, go.dev about profiling. So I definitely leave it there. So you can also look at kind of maybe if you are a go developer like use that and play around yourself. But now I want to speak about why profiling might be actually quite difficult. So the example I had like I had three CPU cycles and if you think about that is not very much. So and just to record what the program was doing in those three CPU cycle probably takes I have no idea about thousands of CPU cycles. And so you really want to be careful what you want to record. So if you really would record all of that like your program would probably have like a massive overhead would slow down by profiling behave completely different and you also would have a lot more data to store to analyze. And then if you think about micro services and replica count 500 you might get quite a lot of data that is actually not that useful to you because are you really caring about three CPU cycles? Probably not. And because of that to allow continuous profiling so to do that in production across like a wide set of deployments like I think Google were the first ones to do that and they were starting to sample those profiling. So instead of looking at really every code that runs go for example looks 100 times a second what is currently running on the CPU and then records it and obviously maybe like integer adder will not be on the CPU if you don't run it all the time and so you get a really accurate representation what is really taking your CPU time. And the way that works you also need to be kind of aware that like some things the actual program might not be on the CPU because it might be waiting for IO and so like when you kind of collect a profile and the profile is not having that many seconds you really need to think about is this really what I want to optimize or maybe I'm not seeing what I actually want to see. With that kind of statistical approach like I don't have any kind of sources to say but like I think generally you say that it's like a two to three percent overhead that gets added on top of your program execution so that's I guess a lot more reasonable than the full approach with recording everything. And so what do you generally kind of would do obviously if you first need to ship your application somewhere and run it then you can look at the profiles and yeah think about it look at it like maybe you are the owner of that code maybe you have a bit more understanding and those profiles maybe can give you a bit more of an idea of what you're actually doing there or how the system is reacting to your code. And so basically like for that green box multiple solutions exist so I'm obviously a bit biased but I also have to say our project is fairly young and evolving. So for example there's like CNCF Pixie, EBPF based, there's Polar Signals Parker like people are in the room, Pyroscope and kind of our solution. I think they're all great like you can all use them and start using them and exploring like maybe just your benchmarks for a start and then as you get more familiar with it like you might kind of discover more and more of the value there. So I'm still going to use Flare now for my quick demo. So let me just see. So I guess most of you are kind of familiar with Grafana and Explore. Why is it so huge? And so basically that's kind of the entry point you're going to see in the Explore. You have the kind of typical time range selection so let's say we want to see the last 15 minutes now and here we can see the profile types collected. So that's just a Docker compose running locally on my laptop, hopefully running locally on my laptop since I started to talk. And for example we can here look at the CPU. So that's kind of the nanoseconds band on the CPU and you can kind of see the flame graph from earlier and maybe some bug. I don't know. It looks a bit bigger than it usually should be. But we can see that kind of top table. We can see the aggregation of all of the services. So I'm running like five pods or something like that, different languages. So you can see like for example this here is like a Python main module where it's doing some prime numbers. So what I first want to kind of break down here is by label. And that's really the only kind of functionality that we have in terms of querying. So here we would look at the different instances and we kind of see the CPU time spent like, I don't know, there's like a Rust port and they are both blue so I don't know which switch, but I guess Flare is doing more. So that might be the Flare one. And for my example now I want to look at just like a small program that I wrote to show like how like the aspect. So like here we can see kind of the timeline. So this is like a profile gets collected I think every 15 seconds and that's basically a dot. And then the flame graph and the top table below would kind of just aggregate that. So like there's no time component in here. That's also important to understand. And so like while we were looking at memory I'm now going to kind of switch to the allocated space. And oh no. And here we have some label selection like that you might be familiar. And this random port here you can see like the allocation so the amount of memory that gets allocated is like around six megabytes. But then every couple of every five minutes roughly you can see like some peak. And so if you already look in the flame graph there's already some kind of big red box and the colors don't matter. But basically like you can see this this kind of piece of code is doing kind of a majority of the allocations. And now you could even kind of zoom in here if you really want to figure out and then it even gets bigger and you can see some more what's going on. And so now if you actually want to look at the kind of code for this. And if flare is maybe in version 0.6 we could even see the line of code that we should look at for now you can. But basically like it would show us allocations in line 21. And I guess most of you can see what this kind of program is doing so every five minutes it will kind of have some peak of allocations. And you only see that kind of because you have the time component you can select and then see the flame graph aggregation. Cool. Yeah that was almost my talk. Like I have one more slide that I should just quickly want to. So in the latest version 120 there is profile guided optimizations and I think that might be a really big topic. So what it does it looks at kind of your profile and that can come from production from benchmarks from whatever and tries to do the better decisions during compile time of what things to do with your code like for example think the only thing that it does right now is making inlining decisions. But basically like if it sees this method is called a lot and is in the hot path it would then make the decisions to inline the method maybe if it's a bit colder it would not do it and you can be a lot more accurate as a compiler if you have that kind of data if you know that method is in the hot path or not. Okay that was it. Thank you. Thanks a lot that was awesome. Questions. Thank you. Thank you for the talk. I'm just wondering how would the profiling work with very multi-threaded code. Is there ability to drill down into that level. Yeah so like maybe so like in terms of multi-threading like obviously we only have the main method in that example. So you can see rude and then mine is 100% and like if it's multi-threaded you would have kind of maybe more so it's basically all only the stack trace that gets recorded like you would not see kind of maybe the connections where the thread where it's threading off and things like that. You would get the stack trace. Cold stack. Have you looked into any other profiling formats than B prof ingestion. I know open telemetry has been doing some stuff about introducing a profiling format that people can standardize on but I don't know if you've looked at that at all. Yes. Can you like I haven't seen you like sorry can you repeat like I struggled to. So I was wondering if you've looked at any other profiling ingestion formats other than B prof. No I like I or so like right now we use P prof personally with the player. So I think there's a lot of kind of improvements to be had over over the format there and that's like as far as I know some active work around open telemetry to to come to I guess a better format in the sense to not send symbols over and over again and reduce interest but not no it's the the accurate and short answer. Okay so thank you for the talk and my question is that looking at the flare architecture it's currently pool models so the flare agent is scraping the profiling data from the applications that they configure it to scrape. My question is is there an eventual plan to also add maybe a push gateway or similar API for applications where this might be suitable. Yeah like definitely like I think I also can see kind of the push use case for maybe if you want to get your micro benchmarks from CI CD in so like the API in theory allows it but tooling is missing but I definitely think it's a valid like push use case as well. I think in terms of scalability I think pool will be better but yeah I agree. Thanks for the talk. I have a small question. Did you try to implement this tooling in the end of the CI CD and CO continuous optimization? No like so we're not using it yet for that. I think it's it's definitely a super useful thing because like yeah like you want to see maybe how a pool request behaves like maybe how your application allocates more or less in different parts and and if the trade-offs are right there but yeah I think it definitely can and should be used for that but no no tooling right now. Yeah no I fully agree as well yeah. Hello thank you. So if I understand correctly profiles such as traces combined with OS metrics right so at a concrete specific time you can see how much CPU you used and so on right. Yeah I guess it looks a bit more at the actual line of code rather than I don't know like I don't know I haven't used like tracing where it automatically finds the function maybe that also tells you the line of code but yeah like it definitely adds some metrics to it like without you doing much I guess other than making sure it can read the symbol tables and the function names. Yeah so so I just had like a dumb question or like dumb idea couldn't you just combine for example you already have node exporter which exposes metrics at all times so you have OS metrics and you have traces for example so couldn't you just have some kind of integration graph on or or somewhere else that just combines traces with metrics. Yeah so like I think that was also the like like people that work longer at continuous profiling software that they try to kind of reuse kind of Prometheus and I think where you end up kind of in it's just a very high cardinality it's too many lines of codes and and that's kind of where it stops but like in theory like I guess most promql constructs and functions are maybe something we need to implement on top of that in a similar way because in the end you just get metrics out of it and so basically the problem was too many lines of code too much changing over time and like you just get too much serious turn through that. So thanks a lot. Yeah thank you for coming.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.200000000000001, "text": " Yeah. Hi, everyone. My name is Christian Simon, and I'm going to be talking about continuous", "tokens": [865, 13, 2421, 11, 1518, 13, 1222, 1315, 307, 5778, 13193, 11, 293, 286, 478, 516, 281, 312, 1417, 466, 10957], "temperature": 0.0, "avg_logprob": -0.24927472095100248, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.22389687597751617}, {"id": 1, "seek": 0, "start": 15.200000000000001, "end": 23.36, "text": " profiling. So we heard a lot about observability today already, and I'm going to want to introduce", "tokens": [1740, 4883, 13, 407, 321, 2198, 257, 688, 466, 9951, 2310, 965, 1217, 11, 293, 286, 478, 516, 281, 528, 281, 5366], "temperature": 0.0, "avg_logprob": -0.24927472095100248, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.22389687597751617}, {"id": 2, "seek": 2336, "start": 23.36, "end": 30.72, "text": " maybe an additional signal there. So maybe quickly about me. So I'm working at Grafana", "tokens": [1310, 364, 4497, 6358, 456, 13, 407, 1310, 2661, 466, 385, 13, 407, 286, 478, 1364, 412, 8985, 69, 2095], "temperature": 0.0, "avg_logprob": -0.24374661380297516, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011496536899358034}, {"id": 3, "seek": 2336, "start": 30.72, "end": 37.24, "text": " Labs. I'm a software engineer there, and worked on our databases for observability. So I worked", "tokens": [40047, 13, 286, 478, 257, 4722, 11403, 456, 11, 293, 2732, 322, 527, 22380, 337, 9951, 2310, 13, 407, 286, 2732], "temperature": 0.0, "avg_logprob": -0.24374661380297516, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011496536899358034}, {"id": 4, "seek": 2336, "start": 37.24, "end": 45.32, "text": " on Cortex slash Mamiir now. I worked on Loki, and most recently I switched to the flat team,", "tokens": [322, 28522, 3121, 17330, 376, 4526, 347, 586, 13, 286, 2732, 322, 37940, 11, 293, 881, 3938, 286, 16858, 281, 264, 4962, 1469, 11], "temperature": 0.0, "avg_logprob": -0.24374661380297516, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.00011496536899358034}, {"id": 5, "seek": 4532, "start": 45.32, "end": 53.68, "text": " and I'm 50% of the flat team, and we kind of work on continuous profiling database. There's", "tokens": [293, 286, 478, 2625, 4, 295, 264, 4962, 1469, 11, 293, 321, 733, 295, 589, 322, 10957, 1740, 4883, 8149, 13, 821, 311], "temperature": 0.0, "avg_logprob": -0.16370330686154572, "compression_ratio": 1.5387931034482758, "no_speech_prob": 8.067311136983335e-05}, {"id": 6, "seek": 4532, "start": 53.68, "end": 59.44, "text": " not going to be a particular focus on flat today. So basically what I want to talk today", "tokens": [406, 516, 281, 312, 257, 1729, 1879, 322, 4962, 965, 13, 407, 1936, 437, 286, 528, 281, 751, 965], "temperature": 0.0, "avg_logprob": -0.16370330686154572, "compression_ratio": 1.5387931034482758, "no_speech_prob": 8.067311136983335e-05}, {"id": 7, "seek": 4532, "start": 59.44, "end": 64.92, "text": " is kind of introduce how it's measured, what you can achieve with it, and then maybe as", "tokens": [307, 733, 295, 5366, 577, 309, 311, 12690, 11, 437, 291, 393, 4584, 365, 309, 11, 293, 550, 1310, 382], "temperature": 0.0, "avg_logprob": -0.16370330686154572, "compression_ratio": 1.5387931034482758, "no_speech_prob": 8.067311136983335e-05}, {"id": 8, "seek": 4532, "start": 64.92, "end": 72.8, "text": " I learn more, at the next first time I can go more into detail and look at very specific", "tokens": [286, 1466, 544, 11, 412, 264, 958, 700, 565, 286, 393, 352, 544, 666, 2607, 293, 574, 412, 588, 2685], "temperature": 0.0, "avg_logprob": -0.16370330686154572, "compression_ratio": 1.5387931034482758, "no_speech_prob": 8.067311136983335e-05}, {"id": 9, "seek": 7280, "start": 72.8, "end": 82.6, "text": " languages there. So when we talk about observability, what are our common goals? Obviously we want", "tokens": [8650, 456, 13, 407, 562, 321, 751, 466, 9951, 2310, 11, 437, 366, 527, 2689, 5493, 30, 7580, 321, 528], "temperature": 0.0, "avg_logprob": -0.16267172992229462, "compression_ratio": 1.489247311827957, "no_speech_prob": 8.091499330475926e-05}, {"id": 10, "seek": 7280, "start": 82.6, "end": 88.67999999999999, "text": " to ensure that the user journeys of our users are successful, that we maybe even can be", "tokens": [281, 5586, 300, 264, 4195, 36736, 295, 527, 5022, 366, 4406, 11, 300, 321, 1310, 754, 393, 312], "temperature": 0.0, "avg_logprob": -0.16267172992229462, "compression_ratio": 1.489247311827957, "no_speech_prob": 8.091499330475926e-05}, {"id": 11, "seek": 7280, "start": 88.67999999999999, "end": 100.72, "text": " proactively find problems before a user notices it. And basically we want to be as quickly", "tokens": [447, 45679, 915, 2740, 949, 257, 4195, 32978, 309, 13, 400, 1936, 321, 528, 281, 312, 382, 2661], "temperature": 0.0, "avg_logprob": -0.16267172992229462, "compression_ratio": 1.489247311827957, "no_speech_prob": 8.091499330475926e-05}, {"id": 12, "seek": 10072, "start": 100.72, "end": 107.08, "text": " as possible when those problems happen to disrupt less of those user journeys. And observability", "tokens": [382, 1944, 562, 729, 2740, 1051, 281, 14124, 1570, 295, 729, 4195, 36736, 13, 400, 9951, 2310], "temperature": 0.0, "avg_logprob": -0.1151052102809999, "compression_ratio": 1.567099567099567, "no_speech_prob": 5.49799224245362e-05}, {"id": 13, "seek": 10072, "start": 107.08, "end": 112.84, "text": " provides us like an objective way of getting some insights into the state of our system", "tokens": [6417, 505, 411, 364, 10024, 636, 295, 1242, 512, 14310, 666, 264, 1785, 295, 527, 1185], "temperature": 0.0, "avg_logprob": -0.1151052102809999, "compression_ratio": 1.567099567099567, "no_speech_prob": 5.49799224245362e-05}, {"id": 14, "seek": 10072, "start": 112.84, "end": 121.32, "text": " in production. And even after a certain event has happened, we found the right way, reboot,", "tokens": [294, 4265, 13, 400, 754, 934, 257, 1629, 2280, 575, 2011, 11, 321, 1352, 264, 558, 636, 11, 33818, 11], "temperature": 0.0, "avg_logprob": -0.1151052102809999, "compression_ratio": 1.567099567099567, "no_speech_prob": 5.49799224245362e-05}, {"id": 15, "seek": 10072, "start": 121.32, "end": 125.36, "text": " and it's all up again. I think it might be able to help us understanding what exactly", "tokens": [293, 309, 311, 439, 493, 797, 13, 286, 519, 309, 1062, 312, 1075, 281, 854, 505, 3701, 437, 2293], "temperature": 0.0, "avg_logprob": -0.1151052102809999, "compression_ratio": 1.567099567099567, "no_speech_prob": 5.49799224245362e-05}, {"id": 16, "seek": 12536, "start": 125.36, "end": 133.48, "text": " happened when we want to figure out the root cause of something. So one of the I guess", "tokens": [2011, 562, 321, 528, 281, 2573, 484, 264, 5593, 3082, 295, 746, 13, 407, 472, 295, 264, 286, 2041], "temperature": 0.0, "avg_logprob": -0.1803086939312163, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00012418086407706141}, {"id": 17, "seek": 12536, "start": 133.48, "end": 141.52, "text": " easiest and probably oldest observability signals is logs. So like I guess it starts", "tokens": [12889, 293, 1391, 14026, 9951, 2310, 12354, 307, 20820, 13, 407, 411, 286, 2041, 309, 3719], "temperature": 0.0, "avg_logprob": -0.1803086939312163, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00012418086407706141}, {"id": 18, "seek": 12536, "start": 141.52, "end": 147.04, "text": " with a kind of print hello somewhere in your code. And I guess you probably don't need", "tokens": [365, 257, 733, 295, 4482, 7751, 4079, 294, 428, 3089, 13, 400, 286, 2041, 291, 1391, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.1803086939312163, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00012418086407706141}, {"id": 19, "seek": 12536, "start": 147.04, "end": 152.84, "text": " a show of hands who's using it. Like I guess everyone somehow uses logs or is asleep if", "tokens": [257, 855, 295, 2377, 567, 311, 1228, 309, 13, 1743, 286, 2041, 1518, 6063, 4960, 20820, 420, 307, 11039, 498], "temperature": 0.0, "avg_logprob": -0.1803086939312163, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00012418086407706141}, {"id": 20, "seek": 15284, "start": 152.84, "end": 160.32, "text": " they don't show a hand. So basically your application doesn't need any specific SDK.", "tokens": [436, 500, 380, 855, 257, 1011, 13, 407, 1936, 428, 3861, 1177, 380, 643, 604, 2685, 37135, 13], "temperature": 0.0, "avg_logprob": -0.14903627742420544, "compression_ratio": 1.5527426160337552, "no_speech_prob": 8.043913112487644e-05}, {"id": 21, "seek": 15284, "start": 160.32, "end": 167.52, "text": " It can probably just log based on the standard library of your languages. One of the challenges,", "tokens": [467, 393, 1391, 445, 3565, 2361, 322, 264, 3832, 6405, 295, 428, 8650, 13, 1485, 295, 264, 4759, 11], "temperature": 0.0, "avg_logprob": -0.14903627742420544, "compression_ratio": 1.5527426160337552, "no_speech_prob": 8.043913112487644e-05}, {"id": 22, "seek": 15284, "start": 167.52, "end": 174.12, "text": " most of the time the format is rather varied. So like even in terms of timestamps, it can", "tokens": [881, 295, 264, 565, 264, 7877, 307, 2831, 22877, 13, 407, 411, 754, 294, 2115, 295, 49108, 23150, 11, 309, 393], "temperature": 0.0, "avg_logprob": -0.14903627742420544, "compression_ratio": 1.5527426160337552, "no_speech_prob": 8.043913112487644e-05}, {"id": 23, "seek": 15284, "start": 174.12, "end": 181.92000000000002, "text": " be really, really hard to get a common understanding of your log lines there. And also like when", "tokens": [312, 534, 11, 534, 1152, 281, 483, 257, 2689, 3701, 295, 428, 3565, 3876, 456, 13, 400, 611, 411, 562], "temperature": 0.0, "avg_logprob": -0.14903627742420544, "compression_ratio": 1.5527426160337552, "no_speech_prob": 8.043913112487644e-05}, {"id": 24, "seek": 18192, "start": 181.92, "end": 185.92, "text": " you then want to aggregate them, they are quite costly. So like it spides, you need", "tokens": [291, 550, 528, 281, 26118, 552, 11, 436, 366, 1596, 28328, 13, 407, 411, 309, 637, 1875, 11, 291, 643], "temperature": 0.0, "avg_logprob": -0.22381640004587697, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.00015604797226842493}, {"id": 25, "seek": 18192, "start": 185.92, "end": 193.07999999999998, "text": " to kind of convert them to in floats and so on. And also that something that can happen", "tokens": [281, 733, 295, 7620, 552, 281, 294, 37878, 293, 370, 322, 13, 400, 611, 300, 746, 300, 393, 1051], "temperature": 0.0, "avg_logprob": -0.22381640004587697, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.00015604797226842493}, {"id": 26, "seek": 18192, "start": 193.07999999999998, "end": 198.0, "text": " is that like you have so many logs that you can't find the ones that you're actually interested", "tokens": [307, 300, 411, 291, 362, 370, 867, 20820, 300, 291, 393, 380, 915, 264, 2306, 300, 291, 434, 767, 3102], "temperature": 0.0, "avg_logprob": -0.22381640004587697, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.00015604797226842493}, {"id": 27, "seek": 18192, "start": 198.0, "end": 205.88, "text": " in. So like grab error, for example, could be, yeah, could be maybe helpful, but also", "tokens": [294, 13, 407, 411, 4444, 6713, 11, 337, 1365, 11, 727, 312, 11, 1338, 11, 727, 312, 1310, 4961, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.22381640004587697, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.00015604797226842493}, {"id": 28, "seek": 20588, "start": 205.88, "end": 213.32, "text": " like there might be just too many errors. And you kind of lose the important ones. So", "tokens": [411, 456, 1062, 312, 445, 886, 867, 13603, 13, 400, 291, 733, 295, 3624, 264, 1021, 2306, 13, 407], "temperature": 0.0, "avg_logprob": -0.17280197143554688, "compression_ratio": 1.6194029850746268, "no_speech_prob": 3.847299740300514e-05}, {"id": 29, "seek": 20588, "start": 213.32, "end": 218.56, "text": " also like if you want to learn more about logs, my colleagues, Oven and Kavi, they're", "tokens": [611, 411, 498, 291, 528, 281, 1466, 544, 466, 20820, 11, 452, 7734, 11, 422, 553, 293, 591, 18442, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.17280197143554688, "compression_ratio": 1.6194029850746268, "no_speech_prob": 3.847299740300514e-05}, {"id": 30, "seek": 20588, "start": 218.56, "end": 224.16, "text": " going to speak about Loki. So definitely stay in the room. And I'm going to move on", "tokens": [516, 281, 1710, 466, 37940, 13, 407, 2138, 1754, 294, 264, 1808, 13, 400, 286, 478, 516, 281, 1286, 322], "temperature": 0.0, "avg_logprob": -0.17280197143554688, "compression_ratio": 1.6194029850746268, "no_speech_prob": 3.847299740300514e-05}, {"id": 31, "seek": 20588, "start": 224.16, "end": 229.12, "text": " to the next signal. So metrics, I'm also assuming pretty much everyone has come across them", "tokens": [281, 264, 958, 6358, 13, 407, 16367, 11, 286, 478, 611, 11926, 1238, 709, 1518, 575, 808, 2108, 552], "temperature": 0.0, "avg_logprob": -0.17280197143554688, "compression_ratio": 1.6194029850746268, "no_speech_prob": 3.847299740300514e-05}, {"id": 32, "seek": 20588, "start": 229.12, "end": 235.04, "text": " and is using them. So in this case, you kind of avoid that problem I mentioned before.", "tokens": [293, 307, 1228, 552, 13, 407, 294, 341, 1389, 11, 291, 733, 295, 5042, 300, 1154, 286, 2835, 949, 13], "temperature": 0.0, "avg_logprob": -0.17280197143554688, "compression_ratio": 1.6194029850746268, "no_speech_prob": 3.847299740300514e-05}, {"id": 33, "seek": 23504, "start": 235.04, "end": 239.84, "text": " You have the actual integers exposed. You maybe know about those integers that you care", "tokens": [509, 362, 264, 3539, 41674, 9495, 13, 509, 1310, 458, 466, 729, 41674, 300, 291, 1127], "temperature": 0.0, "avg_logprob": -0.12708070928400214, "compression_ratio": 1.8451882845188285, "no_speech_prob": 8.915133366826922e-05}, {"id": 34, "seek": 23504, "start": 239.84, "end": 247.23999999999998, "text": " about them. So to get a metric, most of the time you have to do some kind of define a", "tokens": [466, 552, 13, 407, 281, 483, 257, 20678, 11, 881, 295, 264, 565, 291, 362, 281, 360, 512, 733, 295, 6964, 257], "temperature": 0.0, "avg_logprob": -0.12708070928400214, "compression_ratio": 1.8451882845188285, "no_speech_prob": 8.915133366826922e-05}, {"id": 35, "seek": 23504, "start": 247.23999999999998, "end": 252.6, "text": " list of metrics you care about and then you can collect them. So it might be, you might", "tokens": [1329, 295, 16367, 291, 1127, 466, 293, 550, 291, 393, 2500, 552, 13, 407, 309, 1062, 312, 11, 291, 1062], "temperature": 0.0, "avg_logprob": -0.12708070928400214, "compression_ratio": 1.8451882845188285, "no_speech_prob": 8.915133366826922e-05}, {"id": 36, "seek": 23504, "start": 252.6, "end": 256.59999999999997, "text": " be having kind of an outage and didn't have that metric that you care about. And so you", "tokens": [312, 1419, 733, 295, 364, 484, 609, 293, 994, 380, 362, 300, 20678, 300, 291, 1127, 466, 13, 400, 370, 291], "temperature": 0.0, "avg_logprob": -0.12708070928400214, "compression_ratio": 1.8451882845188285, "no_speech_prob": 8.915133366826922e-05}, {"id": 37, "seek": 23504, "start": 256.59999999999997, "end": 262.28, "text": " need to kind of constantly improve on the exposure of the metrics. Obviously, like Peruvius", "tokens": [643, 281, 733, 295, 6460, 3470, 322, 264, 10420, 295, 264, 16367, 13, 7580, 11, 411, 3026, 9350, 4872], "temperature": 0.0, "avg_logprob": -0.12708070928400214, "compression_ratio": 1.8451882845188285, "no_speech_prob": 8.915133366826922e-05}, {"id": 38, "seek": 26228, "start": 262.28, "end": 271.08, "text": " is the kind of main tool in that space. And very often we talk about web services, I guess,", "tokens": [307, 264, 733, 295, 2135, 2290, 294, 300, 1901, 13, 400, 588, 2049, 321, 751, 466, 3670, 3328, 11, 286, 2041, 11], "temperature": 0.0, "avg_logprob": -0.14898736289377962, "compression_ratio": 1.6822429906542056, "no_speech_prob": 5.28437813045457e-05}, {"id": 39, "seek": 26228, "start": 271.08, "end": 276.52, "text": " when we think about those applications. So the red method, so like get the rates of your", "tokens": [562, 321, 519, 466, 729, 5821, 13, 407, 264, 2182, 3170, 11, 370, 411, 483, 264, 6846, 295, 428], "temperature": 0.0, "avg_logprob": -0.14898736289377962, "compression_ratio": 1.6822429906542056, "no_speech_prob": 5.28437813045457e-05}, {"id": 40, "seek": 26228, "start": 276.52, "end": 283.67999999999995, "text": " requests, get the error rate of your request, and get the latency duration of the request", "tokens": [12475, 11, 483, 264, 6713, 3314, 295, 428, 5308, 11, 293, 483, 264, 27043, 16365, 295, 264, 5308], "temperature": 0.0, "avg_logprob": -0.14898736289377962, "compression_ratio": 1.6822429906542056, "no_speech_prob": 5.28437813045457e-05}, {"id": 41, "seek": 26228, "start": 283.67999999999995, "end": 289.32, "text": " can already cover quite a lot of cases. And obviously, like as it's kind of just integers", "tokens": [393, 1217, 2060, 1596, 257, 688, 295, 3331, 13, 400, 2745, 11, 411, 382, 309, 311, 733, 295, 445, 41674], "temperature": 0.0, "avg_logprob": -0.14898736289377962, "compression_ratio": 1.6822429906542056, "no_speech_prob": 5.28437813045457e-05}, {"id": 42, "seek": 28932, "start": 289.32, "end": 296.56, "text": " or floats, you can aggregate them quite efficiently across like, yeah, a multitude of pods or", "tokens": [420, 37878, 11, 291, 393, 26118, 552, 1596, 19621, 2108, 411, 11, 1338, 11, 257, 36358, 295, 31925, 420], "temperature": 0.0, "avg_logprob": -0.11889627503185737, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.025577279913705e-05}, {"id": 43, "seek": 28932, "start": 296.56, "end": 306.44, "text": " like a really huge set up of services. And then if you get more into that kind of microservices", "tokens": [411, 257, 534, 2603, 992, 493, 295, 3328, 13, 400, 550, 498, 291, 483, 544, 666, 300, 733, 295, 15547, 47480], "temperature": 0.0, "avg_logprob": -0.11889627503185737, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.025577279913705e-05}, {"id": 44, "seek": 28932, "start": 306.44, "end": 311.4, "text": " architecture that has kind of evolved over the last couple of years, you will find yourself", "tokens": [9482, 300, 575, 733, 295, 14178, 670, 264, 1036, 1916, 295, 924, 11, 291, 486, 915, 1803], "temperature": 0.0, "avg_logprob": -0.11889627503185737, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.025577279913705e-05}, {"id": 45, "seek": 28932, "start": 311.4, "end": 318.68, "text": " kind of having a really complex composition of services being involved in answering requests.", "tokens": [733, 295, 1419, 257, 534, 3997, 12686, 295, 3328, 885, 3288, 294, 13430, 12475, 13], "temperature": 0.0, "avg_logprob": -0.11889627503185737, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.025577279913705e-05}, {"id": 46, "seek": 31868, "start": 318.68, "end": 323.44, "text": " And so like, you might even struggle to understand what's slowing you down or where the error", "tokens": [400, 370, 411, 11, 291, 1062, 754, 7799, 281, 1223, 437, 311, 26958, 291, 760, 420, 689, 264, 6713], "temperature": 0.0, "avg_logprob": -0.15552887549767128, "compression_ratio": 1.6793893129770991, "no_speech_prob": 3.2110856409417465e-05}, {"id": 47, "seek": 31868, "start": 323.44, "end": 329.72, "text": " is coming from, why do I have this time out here? And distributed tracing can help you", "tokens": [307, 1348, 490, 11, 983, 360, 286, 362, 341, 565, 484, 510, 30, 400, 12631, 25262, 393, 854, 291], "temperature": 0.0, "avg_logprob": -0.15552887549767128, "compression_ratio": 1.6793893129770991, "no_speech_prob": 3.2110856409417465e-05}, {"id": 48, "seek": 31868, "start": 329.72, "end": 334.36, "text": " a lot with kind of getting an understanding what your service is doing. It also might", "tokens": [257, 688, 365, 733, 295, 1242, 364, 3701, 437, 428, 2643, 307, 884, 13, 467, 611, 1062], "temperature": 0.0, "avg_logprob": -0.15552887549767128, "compression_ratio": 1.6793893129770991, "no_speech_prob": 3.2110856409417465e-05}, {"id": 49, "seek": 31868, "start": 334.36, "end": 338.8, "text": " be that like, maybe the service is actually doing way too much and you're calculating", "tokens": [312, 300, 411, 11, 1310, 264, 2643, 307, 767, 884, 636, 886, 709, 293, 291, 434, 28258], "temperature": 0.0, "avg_logprob": -0.15552887549767128, "compression_ratio": 1.6793893129770991, "no_speech_prob": 3.2110856409417465e-05}, {"id": 50, "seek": 31868, "start": 338.8, "end": 345.32, "text": " things over and over again. So that is super helpful to get a bit more like the kind of", "tokens": [721, 670, 293, 670, 797, 13, 407, 300, 307, 1687, 4961, 281, 483, 257, 857, 544, 411, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.15552887549767128, "compression_ratio": 1.6793893129770991, "no_speech_prob": 3.2110856409417465e-05}, {"id": 51, "seek": 34532, "start": 345.32, "end": 353.04, "text": " flow of the data in your system. So like the challenge there might be like, you might have", "tokens": [3095, 295, 264, 1412, 294, 428, 1185, 13, 407, 411, 264, 3430, 456, 1062, 312, 411, 11, 291, 1062, 362], "temperature": 0.0, "avg_logprob": -0.17637781943044356, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.792790448642336e-05}, {"id": 52, "seek": 34532, "start": 353.04, "end": 359.52, "text": " a lot of requests and that's, while it's somewhat cheap to get the tracing, you might not cover", "tokens": [257, 688, 295, 12475, 293, 300, 311, 11, 1339, 309, 311, 8344, 7084, 281, 483, 264, 25262, 11, 291, 1062, 406, 2060], "temperature": 0.0, "avg_logprob": -0.17637781943044356, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.792790448642336e-05}, {"id": 53, "seek": 34532, "start": 359.52, "end": 364.4, "text": " all the requests. So for example, now production system, I think maybe someone needs to correct", "tokens": [439, 264, 12475, 13, 407, 337, 1365, 11, 586, 4265, 1185, 11, 286, 519, 1310, 1580, 2203, 281, 3006], "temperature": 0.0, "avg_logprob": -0.17637781943044356, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.792790448642336e-05}, {"id": 54, "seek": 34532, "start": 364.4, "end": 371.88, "text": " me if I'm wrong, but like when we receive a Grafana cloud like logs and metric data,", "tokens": [385, 498, 286, 478, 2085, 11, 457, 411, 562, 321, 4774, 257, 8985, 69, 2095, 4588, 411, 20820, 293, 20678, 1412, 11], "temperature": 0.0, "avg_logprob": -0.17637781943044356, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.792790448642336e-05}, {"id": 55, "seek": 37188, "start": 371.88, "end": 381.48, "text": " we only cover 1% of those with traces while we cover 100% of our queries. So like, basically", "tokens": [321, 787, 2060, 502, 4, 295, 729, 365, 26076, 1339, 321, 2060, 2319, 4, 295, 527, 24109, 13, 407, 411, 11, 1936], "temperature": 0.0, "avg_logprob": -0.16423009236653646, "compression_ratio": 1.578723404255319, "no_speech_prob": 1.2185579180368222e-05}, {"id": 56, "seek": 37188, "start": 381.48, "end": 386.92, "text": " you need to make a selective decision if it's worth investing that. Obviously logs data", "tokens": [291, 643, 281, 652, 257, 33930, 3537, 498, 309, 311, 3163, 10978, 300, 13, 7580, 20820, 1412], "temperature": 0.0, "avg_logprob": -0.16423009236653646, "compression_ratio": 1.578723404255319, "no_speech_prob": 1.2185579180368222e-05}, {"id": 57, "seek": 37188, "start": 386.92, "end": 392.71999999999997, "text": " looks always the same, it comes every second and so on. So like, we see more value in having", "tokens": [1542, 1009, 264, 912, 11, 309, 1487, 633, 1150, 293, 370, 322, 13, 407, 411, 11, 321, 536, 544, 2158, 294, 1419], "temperature": 0.0, "avg_logprob": -0.16423009236653646, "compression_ratio": 1.578723404255319, "no_speech_prob": 1.2185579180368222e-05}, {"id": 58, "seek": 37188, "start": 392.71999999999997, "end": 398.71999999999997, "text": " all of those queries where there's a complex kind of caching and all sorts of systems interacting", "tokens": [439, 295, 729, 24109, 689, 456, 311, 257, 3997, 733, 295, 269, 2834, 293, 439, 7527, 295, 3652, 18017], "temperature": 0.0, "avg_logprob": -0.16423009236653646, "compression_ratio": 1.578723404255319, "no_speech_prob": 1.2185579180368222e-05}, {"id": 59, "seek": 39872, "start": 398.72, "end": 406.64000000000004, "text": " with it and so that allows us, yeah, to look a bit deeper and even then find that one service", "tokens": [365, 309, 293, 370, 300, 4045, 505, 11, 1338, 11, 281, 574, 257, 857, 7731, 293, 754, 550, 915, 300, 472, 2643], "temperature": 0.0, "avg_logprob": -0.1274774178214695, "compression_ratio": 1.6515837104072397, "no_speech_prob": 2.8634274713112973e-05}, {"id": 60, "seek": 39872, "start": 406.64000000000004, "end": 413.04, "text": " that maybe is the bottleneck there. So maybe looking at a bit of a real problem, so I'm", "tokens": [300, 1310, 307, 264, 44641, 547, 456, 13, 407, 1310, 1237, 412, 257, 857, 295, 257, 957, 1154, 11, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.1274774178214695, "compression_ratio": 1.6515837104072397, "no_speech_prob": 2.8634274713112973e-05}, {"id": 61, "seek": 39872, "start": 413.04, "end": 419.8, "text": " having an online shop, I'm selling socks and a user is complaining about getting some time", "tokens": [1419, 364, 2950, 3945, 11, 286, 478, 6511, 17564, 293, 257, 4195, 307, 20740, 466, 1242, 512, 565], "temperature": 0.0, "avg_logprob": -0.1274774178214695, "compression_ratio": 1.6515837104072397, "no_speech_prob": 2.8634274713112973e-05}, {"id": 62, "seek": 39872, "start": 419.8, "end": 426.68, "text": " out when wanting to check out. That's obviously not great because I'm not selling the socks,", "tokens": [484, 562, 7935, 281, 1520, 484, 13, 663, 311, 2745, 406, 869, 570, 286, 478, 406, 6511, 264, 17564, 11], "temperature": 0.0, "avg_logprob": -0.1274774178214695, "compression_ratio": 1.6515837104072397, "no_speech_prob": 2.8634274713112973e-05}, {"id": 63, "seek": 42668, "start": 426.68, "end": 432.8, "text": " but at least the user got some trace ID and is complaining to our customer service. Then", "tokens": [457, 412, 1935, 264, 4195, 658, 512, 13508, 7348, 293, 307, 20740, 281, 527, 5474, 2643, 13, 1396], "temperature": 0.0, "avg_logprob": -0.2072949025822782, "compression_ratio": 1.6571428571428573, "no_speech_prob": 4.08615633205045e-05}, {"id": 64, "seek": 42668, "start": 432.8, "end": 438.84000000000003, "text": " starting from there, I'm figuring out it's the location service that actually was the", "tokens": [2891, 490, 456, 11, 286, 478, 15213, 484, 309, 311, 264, 4914, 2643, 300, 767, 390, 264], "temperature": 0.0, "avg_logprob": -0.2072949025822782, "compression_ratio": 1.6571428571428573, "no_speech_prob": 4.08615633205045e-05}, {"id": 65, "seek": 42668, "start": 438.84000000000003, "end": 444.88, "text": " one that cost the time out in the end. And then looking at the metrics of the location", "tokens": [472, 300, 2063, 264, 565, 484, 294, 264, 917, 13, 400, 550, 1237, 412, 264, 16367, 295, 264, 4914], "temperature": 0.0, "avg_logprob": -0.2072949025822782, "compression_ratio": 1.6571428571428573, "no_speech_prob": 4.08615633205045e-05}, {"id": 66, "seek": 42668, "start": 444.88, "end": 450.52, "text": " service, I might find, oh, there's actually 5% of the requests timing out, so maybe 5%", "tokens": [2643, 11, 286, 1062, 915, 11, 1954, 11, 456, 311, 767, 1025, 4, 295, 264, 12475, 10822, 484, 11, 370, 1310, 1025, 4], "temperature": 0.0, "avg_logprob": -0.2072949025822782, "compression_ratio": 1.6571428571428573, "no_speech_prob": 4.08615633205045e-05}, {"id": 67, "seek": 45052, "start": 450.52, "end": 458.44, "text": " of my users are not able to buy their socks monthly or whatever. So what are the next steps?", "tokens": [295, 452, 5022, 366, 406, 1075, 281, 2256, 641, 17564, 12878, 420, 2035, 13, 407, 437, 366, 264, 958, 4439, 30], "temperature": 0.0, "avg_logprob": -0.15537281658338464, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.00010349384683649987}, {"id": 68, "seek": 45052, "start": 458.44, "end": 464.91999999999996, "text": " I guess scaling up is always good. Maybe the service is just overloaded. The person that", "tokens": [286, 2041, 21589, 493, 307, 1009, 665, 13, 2704, 264, 2643, 307, 445, 28777, 292, 13, 440, 954, 300], "temperature": 0.0, "avg_logprob": -0.15537281658338464, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.00010349384683649987}, {"id": 69, "seek": 45052, "start": 464.91999999999996, "end": 470.08, "text": " wrote it left years ago, so we have no idea. So we just scale it up. Obviously, it comes", "tokens": [4114, 309, 1411, 924, 2057, 11, 370, 321, 362, 572, 1558, 13, 407, 321, 445, 4373, 309, 493, 13, 7580, 11, 309, 1487], "temperature": 0.0, "avg_logprob": -0.15537281658338464, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.00010349384683649987}, {"id": 70, "seek": 45052, "start": 470.08, "end": 477.52, "text": " with a cost and so I think always the first thing would be fixing the bleed, making sure", "tokens": [365, 257, 2063, 293, 370, 286, 519, 1009, 264, 700, 551, 576, 312, 19442, 264, 28385, 11, 1455, 988], "temperature": 0.0, "avg_logprob": -0.15537281658338464, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.00010349384683649987}, {"id": 71, "seek": 47752, "start": 477.52, "end": 481.59999999999997, "text": " there are no more timeouts. So scaling up is definitely the right option here. But then", "tokens": [456, 366, 572, 544, 565, 7711, 13, 407, 21589, 493, 307, 2138, 264, 558, 3614, 510, 13, 583, 550], "temperature": 0.0, "avg_logprob": -0.16411557651701428, "compression_ratio": 1.5863636363636364, "no_speech_prob": 3.769432078115642e-05}, {"id": 72, "seek": 47752, "start": 481.59999999999997, "end": 486.52, "text": " if you do that over years, you might suddenly find yourself having a lot of extra costs", "tokens": [498, 291, 360, 300, 670, 924, 11, 291, 1062, 5800, 915, 1803, 1419, 257, 688, 295, 2857, 5497], "temperature": 0.0, "avg_logprob": -0.16411557651701428, "compression_ratio": 1.5863636363636364, "no_speech_prob": 3.769432078115642e-05}, {"id": 73, "seek": 47752, "start": 486.52, "end": 493.12, "text": " attached to that location service. And so that's kind of where I think we need another", "tokens": [8570, 281, 300, 4914, 2643, 13, 400, 370, 300, 311, 733, 295, 689, 286, 519, 321, 643, 1071], "temperature": 0.0, "avg_logprob": -0.16411557651701428, "compression_ratio": 1.5863636363636364, "no_speech_prob": 3.769432078115642e-05}, {"id": 74, "seek": 47752, "start": 493.12, "end": 499.15999999999997, "text": " signal. And I think that signal should be profiling. So I guess most people might have", "tokens": [6358, 13, 400, 286, 519, 300, 6358, 820, 312, 1740, 4883, 13, 407, 286, 2041, 881, 561, 1062, 362], "temperature": 0.0, "avg_logprob": -0.16411557651701428, "compression_ratio": 1.5863636363636364, "no_speech_prob": 3.769432078115642e-05}, {"id": 75, "seek": 49916, "start": 499.16, "end": 509.88000000000005, "text": " come across profiling. And it basically measures your code and how long it executes, for example,", "tokens": [808, 2108, 1740, 4883, 13, 400, 309, 1936, 8000, 428, 3089, 293, 577, 938, 309, 4454, 1819, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1365025887122521, "compression_ratio": 1.5888888888888888, "no_speech_prob": 3.322025804664008e-05}, {"id": 76, "seek": 49916, "start": 509.88000000000005, "end": 517.76, "text": " or what kind of bytes it allocates in memory. And it basically helps you maybe understand", "tokens": [420, 437, 733, 295, 36088, 309, 12660, 1024, 294, 4675, 13, 400, 309, 1936, 3665, 291, 1310, 1223], "temperature": 0.0, "avg_logprob": -0.1365025887122521, "compression_ratio": 1.5888888888888888, "no_speech_prob": 3.322025804664008e-05}, {"id": 77, "seek": 49916, "start": 517.76, "end": 525.44, "text": " your program even more or someone else's program in the location service case. And that eventually", "tokens": [428, 1461, 754, 544, 420, 1580, 1646, 311, 1461, 294, 264, 4914, 2643, 1389, 13, 400, 300, 4728], "temperature": 0.0, "avg_logprob": -0.1365025887122521, "compression_ratio": 1.5888888888888888, "no_speech_prob": 3.322025804664008e-05}, {"id": 78, "seek": 52544, "start": 525.44, "end": 530.6, "text": " can translate in cost savings if you find out where the problem lies, like you can maybe", "tokens": [393, 13799, 294, 2063, 13454, 498, 291, 915, 484, 689, 264, 1154, 9134, 11, 411, 291, 393, 1310], "temperature": 0.0, "avg_logprob": -0.16276426424925355, "compression_ratio": 1.609865470852018, "no_speech_prob": 3.1367297196993604e-05}, {"id": 79, "seek": 52544, "start": 530.6, "end": 536.24, "text": " fix it or can get some ideas. Maybe you can also look at the fix and see if it's gotten", "tokens": [3191, 309, 420, 393, 483, 512, 3487, 13, 2704, 291, 393, 611, 574, 412, 264, 3191, 293, 536, 498, 309, 311, 5768], "temperature": 0.0, "avg_logprob": -0.16276426424925355, "compression_ratio": 1.609865470852018, "no_speech_prob": 3.1367297196993604e-05}, {"id": 80, "seek": 52544, "start": 536.24, "end": 544.72, "text": " actually worse or better. And yeah, like that basically gives you a better understanding", "tokens": [767, 5324, 420, 1101, 13, 400, 1338, 11, 411, 300, 1936, 2709, 291, 257, 1101, 3701], "temperature": 0.0, "avg_logprob": -0.16276426424925355, "compression_ratio": 1.609865470852018, "no_speech_prob": 3.1367297196993604e-05}, {"id": 81, "seek": 52544, "start": 544.72, "end": 553.7600000000001, "text": " of how your code behaves. And so now the question is, what is actually measured in a profile?", "tokens": [295, 577, 428, 3089, 36896, 13, 400, 370, 586, 264, 1168, 307, 11, 437, 307, 767, 12690, 294, 257, 7964, 30], "temperature": 0.0, "avg_logprob": -0.16276426424925355, "compression_ratio": 1.609865470852018, "no_speech_prob": 3.1367297196993604e-05}, {"id": 82, "seek": 55376, "start": 553.76, "end": 560.2, "text": " So I created a bit of a program. I don't know. I hope everyone can see it. So it's basically", "tokens": [407, 286, 2942, 257, 857, 295, 257, 1461, 13, 286, 500, 380, 458, 13, 286, 1454, 1518, 393, 536, 309, 13, 407, 309, 311, 1936], "temperature": 0.0, "avg_logprob": -0.14526762860886594, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.4715129509568214e-05}, {"id": 83, "seek": 55376, "start": 560.2, "end": 564.68, "text": " like a program that has a main function and then calls out other functions. So you can", "tokens": [411, 257, 1461, 300, 575, 257, 2135, 2445, 293, 550, 5498, 484, 661, 6828, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.14526762860886594, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.4715129509568214e-05}, {"id": 84, "seek": 55376, "start": 564.68, "end": 571.56, "text": " see there's a do a lot and there's a do little function. And both of them then call prepare.", "tokens": [536, 456, 311, 257, 360, 257, 688, 293, 456, 311, 257, 360, 707, 2445, 13, 400, 1293, 295, 552, 550, 818, 5940, 13], "temperature": 0.0, "avg_logprob": -0.14526762860886594, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.4715129509568214e-05}, {"id": 85, "seek": 55376, "start": 571.56, "end": 575.12, "text": " And obviously in the comments, there's some work going on. And obviously the work could", "tokens": [400, 2745, 294, 264, 3053, 11, 456, 311, 512, 589, 516, 322, 13, 400, 2745, 264, 589, 727], "temperature": 0.0, "avg_logprob": -0.14526762860886594, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.4715129509568214e-05}, {"id": 86, "seek": 57512, "start": 575.12, "end": 583.72, "text": " be allocating memory, like using the CPU, something like that. So let's say we use CPU.", "tokens": [312, 12660, 990, 4675, 11, 411, 1228, 264, 13199, 11, 746, 411, 300, 13, 407, 718, 311, 584, 321, 764, 13199, 13], "temperature": 0.0, "avg_logprob": -0.1604601778882615, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.008944829867687e-05}, {"id": 87, "seek": 57512, "start": 583.72, "end": 591.28, "text": " So when the function starts, like we are first going to do something within the main, let's", "tokens": [407, 562, 264, 2445, 3719, 11, 411, 321, 366, 700, 516, 281, 360, 746, 1951, 264, 2135, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.1604601778882615, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.008944829867687e-05}, {"id": 88, "seek": 57512, "start": 591.28, "end": 596.24, "text": " say we spend three CPU cycles, which is not a lot, but that then gets recorded like yes,", "tokens": [584, 321, 3496, 1045, 13199, 17796, 11, 597, 307, 406, 257, 688, 11, 457, 300, 550, 2170, 8287, 411, 2086, 11], "temperature": 0.0, "avg_logprob": -0.1604601778882615, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.008944829867687e-05}, {"id": 89, "seek": 57512, "start": 596.24, "end": 603.84, "text": " we took us three CPU cycles in main. We then go into the prepare method through do a lot.", "tokens": [321, 1890, 505, 1045, 13199, 17796, 294, 2135, 13, 492, 550, 352, 666, 264, 5940, 3170, 807, 360, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1604601778882615, "compression_ratio": 1.7211538461538463, "no_speech_prob": 2.008944829867687e-05}, {"id": 90, "seek": 60384, "start": 603.84, "end": 608.84, "text": " Then we spend another five CPU cycles. And those kind of stack traces then they are recorded", "tokens": [1396, 321, 3496, 1071, 1732, 13199, 17796, 13, 400, 729, 733, 295, 8630, 26076, 550, 436, 366, 8287], "temperature": 0.0, "avg_logprob": -0.16564265164462003, "compression_ratio": 1.587719298245614, "no_speech_prob": 2.7537907953956164e-05}, {"id": 91, "seek": 60384, "start": 608.84, "end": 616.52, "text": " in the profile. And going through the program, like we will end up with that kind of measurement", "tokens": [294, 264, 7964, 13, 400, 516, 807, 264, 1461, 11, 411, 321, 486, 917, 493, 365, 300, 733, 295, 13160], "temperature": 0.0, "avg_logprob": -0.16564265164462003, "compression_ratio": 1.587719298245614, "no_speech_prob": 2.7537907953956164e-05}, {"id": 92, "seek": 60384, "start": 616.52, "end": 624.4, "text": " of stack traces. And while it kind of works with ten lines of codes, you can maybe spot", "tokens": [295, 8630, 26076, 13, 400, 1339, 309, 733, 295, 1985, 365, 2064, 3876, 295, 14211, 11, 291, 393, 1310, 4008], "temperature": 0.0, "avg_logprob": -0.16564265164462003, "compression_ratio": 1.587719298245614, "no_speech_prob": 2.7537907953956164e-05}, {"id": 93, "seek": 60384, "start": 624.4, "end": 628.8000000000001, "text": " where the problem is. Like there's the 20 and do a lot. It definitely kind of breaks", "tokens": [689, 264, 1154, 307, 13, 1743, 456, 311, 264, 945, 293, 360, 257, 688, 13, 467, 2138, 733, 295, 9857], "temperature": 0.0, "avg_logprob": -0.16564265164462003, "compression_ratio": 1.587719298245614, "no_speech_prob": 2.7537907953956164e-05}, {"id": 94, "seek": 62880, "start": 628.8, "end": 635.52, "text": " down when you're speaking about like a lot of services or like a lot of code base that", "tokens": [760, 562, 291, 434, 4124, 466, 411, 257, 688, 295, 3328, 420, 411, 257, 688, 295, 3089, 3096, 300], "temperature": 0.0, "avg_logprob": -0.16722210684975425, "compression_ratio": 1.6238532110091743, "no_speech_prob": 4.153765621595085e-05}, {"id": 95, "seek": 62880, "start": 635.52, "end": 641.12, "text": " is happened or happens to be hot and actively used. And so like there are a couple of ways", "tokens": [307, 2011, 420, 2314, 281, 312, 2368, 293, 13022, 1143, 13, 400, 370, 411, 456, 366, 257, 1916, 295, 2098], "temperature": 0.0, "avg_logprob": -0.16722210684975425, "compression_ratio": 1.6238532110091743, "no_speech_prob": 4.153765621595085e-05}, {"id": 96, "seek": 62880, "start": 641.12, "end": 647.9599999999999, "text": " of visualizing them. I think one of the first things you would find is kind of a top table.", "tokens": [295, 5056, 3319, 552, 13, 286, 519, 472, 295, 264, 700, 721, 291, 576, 915, 307, 733, 295, 257, 1192, 3199, 13], "temperature": 0.0, "avg_logprob": -0.16722210684975425, "compression_ratio": 1.6238532110091743, "no_speech_prob": 4.153765621595085e-05}, {"id": 97, "seek": 62880, "start": 647.9599999999999, "end": 656.3599999999999, "text": " So like in that table, you can order it by different values. Like so this is kind of", "tokens": [407, 411, 294, 300, 3199, 11, 291, 393, 1668, 309, 538, 819, 4190, 13, 1743, 370, 341, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.16722210684975425, "compression_ratio": 1.6238532110091743, "no_speech_prob": 4.153765621595085e-05}, {"id": 98, "seek": 65636, "start": 656.36, "end": 661.76, "text": " an example from P prof, like which is kind of the go tool. And you can see kind of clearly", "tokens": [364, 1365, 490, 430, 1740, 11, 411, 597, 307, 733, 295, 264, 352, 2290, 13, 400, 291, 393, 536, 733, 295, 4448], "temperature": 0.0, "avg_logprob": -0.18125051133176115, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.6192858058493584e-05}, {"id": 99, "seek": 65636, "start": 661.76, "end": 670.4, "text": " do a lot is the method that comes out on top. And like there are now different ways how", "tokens": [360, 257, 688, 307, 264, 3170, 300, 1487, 484, 322, 1192, 13, 400, 411, 456, 366, 586, 819, 2098, 577], "temperature": 0.0, "avg_logprob": -0.18125051133176115, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.6192858058493584e-05}, {"id": 100, "seek": 65636, "start": 670.4, "end": 675.16, "text": " you can look at the value. So you have the flat count, which is the function itself only.", "tokens": [291, 393, 574, 412, 264, 2158, 13, 407, 291, 362, 264, 4962, 1207, 11, 597, 307, 264, 2445, 2564, 787, 13], "temperature": 0.0, "avg_logprob": -0.18125051133176115, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.6192858058493584e-05}, {"id": 101, "seek": 65636, "start": 675.16, "end": 680.28, "text": " So you can see the 20 that we had before, 20 CPU cycles. But we also have the cumulative", "tokens": [407, 291, 393, 536, 264, 945, 300, 321, 632, 949, 11, 945, 13199, 17796, 13, 583, 321, 611, 362, 264, 38379], "temperature": 0.0, "avg_logprob": -0.18125051133176115, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.6192858058493584e-05}, {"id": 102, "seek": 68028, "start": 680.28, "end": 687.28, "text": " measurement, which also includes the prepare that is going to get called from do a lot.", "tokens": [13160, 11, 597, 611, 5974, 264, 5940, 300, 307, 516, 281, 483, 1219, 490, 360, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.13700558958935136, "compression_ratio": 1.7007575757575757, "no_speech_prob": 1.1835393706860486e-05}, {"id": 103, "seek": 68028, "start": 687.28, "end": 695.04, "text": " And so like we already can see we spend 52% of our program and do a lot. So maybe we already", "tokens": [400, 370, 411, 321, 1217, 393, 536, 321, 3496, 18079, 4, 295, 527, 1461, 293, 360, 257, 688, 13, 407, 1310, 321, 1217], "temperature": 0.0, "avg_logprob": -0.13700558958935136, "compression_ratio": 1.7007575757575757, "no_speech_prob": 1.1835393706860486e-05}, {"id": 104, "seek": 68028, "start": 695.04, "end": 699.0, "text": " can stop looking at the table and just look at do a lot. Because if we fix do a lot or", "tokens": [393, 1590, 1237, 412, 264, 3199, 293, 445, 574, 412, 360, 257, 688, 13, 1436, 498, 321, 3191, 360, 257, 688, 420], "temperature": 0.0, "avg_logprob": -0.13700558958935136, "compression_ratio": 1.7007575757575757, "no_speech_prob": 1.1835393706860486e-05}, {"id": 105, "seek": 68028, "start": 699.0, "end": 705.4, "text": " get rid of it, we need half of the CPU less. And that's that kind of, it's kind of represented", "tokens": [483, 3973, 295, 309, 11, 321, 643, 1922, 295, 264, 13199, 1570, 13, 400, 300, 311, 300, 733, 295, 11, 309, 311, 733, 295, 10379], "temperature": 0.0, "avg_logprob": -0.13700558958935136, "compression_ratio": 1.7007575757575757, "no_speech_prob": 1.1835393706860486e-05}, {"id": 106, "seek": 68028, "start": 705.4, "end": 709.4, "text": " by the sum. So the sum will always change depending on how you order the table in that", "tokens": [538, 264, 2408, 13, 407, 264, 2408, 486, 1009, 1319, 5413, 322, 577, 291, 1668, 264, 3199, 294, 300], "temperature": 0.0, "avg_logprob": -0.13700558958935136, "compression_ratio": 1.7007575757575757, "no_speech_prob": 1.1835393706860486e-05}, {"id": 107, "seek": 70940, "start": 709.4, "end": 716.4, "text": " particular example. So in this case, like we have 100% already at row number four, because", "tokens": [1729, 1365, 13, 407, 294, 341, 1389, 11, 411, 321, 362, 2319, 4, 1217, 412, 5386, 1230, 1451, 11, 570], "temperature": 0.0, "avg_logprob": -0.15842719511552292, "compression_ratio": 1.5, "no_speech_prob": 7.465392991434783e-05}, {"id": 108, "seek": 70940, "start": 716.4, "end": 724.76, "text": " we only have four functions. And to get a bit more of a visual sense of what's going", "tokens": [321, 787, 362, 1451, 6828, 13, 400, 281, 483, 257, 857, 544, 295, 257, 5056, 2020, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.15842719511552292, "compression_ratio": 1.5, "no_speech_prob": 7.465392991434783e-05}, {"id": 109, "seek": 70940, "start": 724.76, "end": 731.0799999999999, "text": " on, there are the so-called flame graphs. And I think the most confusing thing for me", "tokens": [322, 11, 456, 366, 264, 370, 12, 11880, 13287, 24877, 13, 400, 286, 519, 264, 881, 13181, 551, 337, 385], "temperature": 0.0, "avg_logprob": -0.15842719511552292, "compression_ratio": 1.5, "no_speech_prob": 7.465392991434783e-05}, {"id": 110, "seek": 70940, "start": 731.0799999999999, "end": 736.96, "text": " about them was the coloring. So obviously like red is always not great. Should we look", "tokens": [466, 552, 390, 264, 23198, 13, 407, 2745, 411, 2182, 307, 1009, 406, 869, 13, 6454, 321, 574], "temperature": 0.0, "avg_logprob": -0.15842719511552292, "compression_ratio": 1.5, "no_speech_prob": 7.465392991434783e-05}, {"id": 111, "seek": 73696, "start": 736.96, "end": 745.9200000000001, "text": " at main? No, we shouldn't. So basically like the coloring I think is random or uses some", "tokens": [412, 2135, 30, 883, 11, 321, 4659, 380, 13, 407, 1936, 411, 264, 23198, 286, 519, 307, 4974, 420, 4960, 512], "temperature": 0.0, "avg_logprob": -0.160570606432463, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.0235545889590867e-05}, {"id": 112, "seek": 73696, "start": 745.9200000000001, "end": 750.84, "text": " kind of hashing. And basically it's only meant to look like a flame. So like the red here", "tokens": [733, 295, 575, 571, 13, 400, 1936, 309, 311, 787, 4140, 281, 574, 411, 257, 13287, 13, 407, 411, 264, 2182, 510], "temperature": 0.0, "avg_logprob": -0.160570606432463, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.0235545889590867e-05}, {"id": 113, "seek": 73696, "start": 750.84, "end": 758.12, "text": " doesn't mean anything. So like if you're colorblind, that's perfect for flame graphs. So what we", "tokens": [1177, 380, 914, 1340, 13, 407, 411, 498, 291, 434, 2017, 47494, 11, 300, 311, 2176, 337, 13287, 24877, 13, 407, 437, 321], "temperature": 0.0, "avg_logprob": -0.160570606432463, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.0235545889590867e-05}, {"id": 114, "seek": 73696, "start": 758.12, "end": 765.0400000000001, "text": " actually want to look at is this kind of like at the leaf end is basically where the program", "tokens": [767, 528, 281, 574, 412, 307, 341, 733, 295, 411, 412, 264, 10871, 917, 307, 1936, 689, 264, 1461], "temperature": 0.0, "avg_logprob": -0.160570606432463, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.0235545889590867e-05}, {"id": 115, "seek": 76504, "start": 765.04, "end": 768.88, "text": " would spend things. Like you can see the three CPU cycles main here. So there's nothing", "tokens": [576, 3496, 721, 13, 1743, 291, 393, 536, 264, 1045, 13199, 17796, 2135, 510, 13, 407, 456, 311, 1825], "temperature": 0.0, "avg_logprob": -0.15288347946970085, "compression_ratio": 1.7783251231527093, "no_speech_prob": 6.338542880257592e-05}, {"id": 116, "seek": 76504, "start": 768.88, "end": 776.56, "text": " below. So main uses 100% through methods that are called by main. And then there's nothing", "tokens": [2507, 13, 407, 2135, 4960, 2319, 4, 807, 7150, 300, 366, 1219, 538, 2135, 13, 400, 550, 456, 311, 1825], "temperature": 0.0, "avg_logprob": -0.15288347946970085, "compression_ratio": 1.7783251231527093, "no_speech_prob": 6.338542880257592e-05}, {"id": 117, "seek": 76504, "start": 776.56, "end": 782.56, "text": " beyond this here. So like here we spend something in main. And in the same way in do little,", "tokens": [4399, 341, 510, 13, 407, 411, 510, 321, 3496, 746, 294, 2135, 13, 400, 294, 264, 912, 636, 294, 360, 707, 11], "temperature": 0.0, "avg_logprob": -0.15288347946970085, "compression_ratio": 1.7783251231527093, "no_speech_prob": 6.338542880257592e-05}, {"id": 118, "seek": 76504, "start": 782.56, "end": 788.16, "text": " we can see the five. And in do a lot, we can see the 20 quite wide. And then the prepares", "tokens": [321, 393, 536, 264, 1732, 13, 400, 294, 360, 257, 688, 11, 321, 393, 536, 264, 945, 1596, 4874, 13, 400, 550, 264, 39418], "temperature": 0.0, "avg_logprob": -0.15288347946970085, "compression_ratio": 1.7783251231527093, "no_speech_prob": 6.338542880257592e-05}, {"id": 119, "seek": 78816, "start": 788.16, "end": 795.16, "text": " with five each as well. And now obviously if you look across a really huge program, you", "tokens": [365, 1732, 1184, 382, 731, 13, 400, 586, 2745, 498, 291, 574, 2108, 257, 534, 2603, 1461, 11, 291], "temperature": 0.0, "avg_logprob": -0.17292970769545613, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00012340227840468287}, {"id": 120, "seek": 78816, "start": 795.16, "end": 801.0, "text": " basically like can can spot kind of what's going on quite quickly. And then if you have", "tokens": [1936, 411, 393, 393, 4008, 733, 295, 437, 311, 516, 322, 1596, 2661, 13, 400, 550, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.17292970769545613, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00012340227840468287}, {"id": 121, "seek": 78816, "start": 801.0, "end": 805.88, "text": " like similar with like route in main, like you basically can ignore that, but it helps", "tokens": [411, 2531, 365, 411, 7955, 294, 2135, 11, 411, 291, 1936, 393, 11200, 300, 11, 457, 309, 3665], "temperature": 0.0, "avg_logprob": -0.17292970769545613, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00012340227840468287}, {"id": 122, "seek": 78816, "start": 805.88, "end": 810.8, "text": " you maybe locate which component of your of your program you want to look at because", "tokens": [291, 1310, 22370, 597, 6542, 295, 428, 295, 428, 1461, 291, 528, 281, 574, 412, 570], "temperature": 0.0, "avg_logprob": -0.17292970769545613, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00012340227840468287}, {"id": 123, "seek": 78816, "start": 810.8, "end": 816.36, "text": " like maybe you're not good at naming and you call everything prepare in util and and it", "tokens": [411, 1310, 291, 434, 406, 665, 412, 25290, 293, 291, 818, 1203, 5940, 294, 4976, 293, 293, 309], "temperature": 0.0, "avg_logprob": -0.17292970769545613, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00012340227840468287}, {"id": 124, "seek": 81636, "start": 816.36, "end": 825.5600000000001, "text": " would still tell you roughly where it gets called and and how the program gets there.", "tokens": [576, 920, 980, 291, 9810, 689, 309, 2170, 1219, 293, 293, 577, 264, 1461, 2170, 456, 13], "temperature": 0.0, "avg_logprob": -0.1807155833524816, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.863425834220834e-05}, {"id": 125, "seek": 81636, "start": 825.5600000000001, "end": 831.28, "text": " So how do we get that profile? And that can be kind of quite that can be quite a lot of", "tokens": [407, 577, 360, 321, 483, 300, 7964, 30, 400, 300, 393, 312, 733, 295, 1596, 300, 393, 312, 1596, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1807155833524816, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.863425834220834e-05}, {"id": 126, "seek": 81636, "start": 831.28, "end": 838.28, "text": " challenges how to get that. So I think I would say like there's like roughly two ways like", "tokens": [4759, 577, 281, 483, 300, 13, 407, 286, 519, 286, 576, 584, 411, 456, 311, 411, 9810, 732, 2098, 411], "temperature": 0.0, "avg_logprob": -0.1807155833524816, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.863425834220834e-05}, {"id": 127, "seek": 81636, "start": 838.28, "end": 845.1800000000001, "text": " either your ecosystem supports kind of profiles fairly natively. And then you instrument the", "tokens": [2139, 428, 11311, 9346, 733, 295, 23693, 6457, 8470, 356, 13, 400, 550, 291, 7198, 264], "temperature": 0.0, "avg_logprob": -0.1807155833524816, "compression_ratio": 1.7246376811594204, "no_speech_prob": 2.863425834220834e-05}, {"id": 128, "seek": 84518, "start": 845.18, "end": 853.7199999999999, "text": " profile, you added maybe a library and SDK. And like basically like the runtime within", "tokens": [7964, 11, 291, 3869, 1310, 257, 6405, 293, 37135, 13, 400, 411, 1936, 411, 264, 34474, 1951], "temperature": 0.0, "avg_logprob": -0.25921633886912515, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.00011640515003819019}, {"id": 129, "seek": 84518, "start": 853.7199999999999, "end": 862.3599999999999, "text": " your environment will maybe expose the information. So like it's not available for all languages.", "tokens": [428, 2823, 486, 1310, 19219, 264, 1589, 13, 407, 411, 309, 311, 406, 2435, 337, 439, 8650, 13], "temperature": 0.0, "avg_logprob": -0.25921633886912515, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.00011640515003819019}, {"id": 130, "seek": 84518, "start": 862.3599999999999, "end": 870.4399999999999, "text": " There's I guess a lot of work going on that it becomes more and more available. And kind", "tokens": [821, 311, 286, 2041, 257, 688, 295, 589, 516, 322, 300, 309, 3643, 544, 293, 544, 2435, 13, 400, 733], "temperature": 0.0, "avg_logprob": -0.25921633886912515, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.00011640515003819019}, {"id": 131, "seek": 87044, "start": 870.44, "end": 875.5600000000001, "text": " of other approaches more like through an agent and EPPF has been quite hyped. I'm I'm not", "tokens": [295, 661, 11587, 544, 411, 807, 364, 9461, 293, 25330, 47, 37, 575, 668, 1596, 43172, 13, 286, 478, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.2307929649009361, "compression_ratio": 1.6213235294117647, "no_speech_prob": 5.098782276036218e-05}, {"id": 132, "seek": 87044, "start": 875.5600000000001, "end": 881.2, "text": " very familiar with the EPPF myself. I have used the agents but haven't written any code.", "tokens": [588, 4963, 365, 264, 25330, 47, 37, 2059, 13, 286, 362, 1143, 264, 12554, 457, 2378, 380, 3720, 604, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2307929649009361, "compression_ratio": 1.6213235294117647, "no_speech_prob": 5.098782276036218e-05}, {"id": 133, "seek": 87044, "start": 881.2, "end": 884.96, "text": " But basically what it would use it would use an outside view of it. So you wouldn't need", "tokens": [583, 1936, 437, 309, 576, 764, 309, 576, 764, 364, 2380, 1910, 295, 309, 13, 407, 291, 2759, 380, 643], "temperature": 0.0, "avg_logprob": -0.2307929649009361, "compression_ratio": 1.6213235294117647, "no_speech_prob": 5.098782276036218e-05}, {"id": 134, "seek": 87044, "start": 884.96, "end": 891.2800000000001, "text": " to change the binary running really like you would just kind of look at the information", "tokens": [281, 1319, 264, 17434, 2614, 534, 411, 291, 576, 445, 733, 295, 574, 412, 264, 1589], "temperature": 0.0, "avg_logprob": -0.2307929649009361, "compression_ratio": 1.6213235294117647, "no_speech_prob": 5.098782276036218e-05}, {"id": 135, "seek": 87044, "start": 891.2800000000001, "end": 898.0, "text": " you get from the Linux kernel like you hook into, I don't know, often enough when the", "tokens": [291, 483, 490, 264, 18734, 28256, 411, 291, 6328, 666, 11, 286, 500, 380, 458, 11, 2049, 1547, 562, 264], "temperature": 0.0, "avg_logprob": -0.2307929649009361, "compression_ratio": 1.6213235294117647, "no_speech_prob": 5.098782276036218e-05}, {"id": 136, "seek": 89800, "start": 898.0, "end": 906.0, "text": " CPU runs to then find out what is currently running. So there are different languages", "tokens": [13199, 6676, 281, 550, 915, 484, 437, 307, 4362, 2614, 13, 407, 456, 366, 819, 8650], "temperature": 0.0, "avg_logprob": -0.1556851625442505, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.347461799625307e-05}, {"id": 137, "seek": 89800, "start": 906.0, "end": 912.48, "text": " like for example in a compiled language. You would be having a lot more information. The", "tokens": [411, 337, 1365, 294, 257, 36548, 2856, 13, 509, 576, 312, 1419, 257, 688, 544, 1589, 13, 440], "temperature": 0.0, "avg_logprob": -0.1556851625442505, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.347461799625307e-05}, {"id": 138, "seek": 89800, "start": 912.48, "end": 918.84, "text": " memory addresses are the same. You can kind of use the information within the simple table", "tokens": [4675, 16862, 366, 264, 912, 13, 509, 393, 733, 295, 764, 264, 1589, 1951, 264, 2199, 3199], "temperature": 0.0, "avg_logprob": -0.1556851625442505, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.347461799625307e-05}, {"id": 139, "seek": 89800, "start": 918.84, "end": 925.64, "text": " to figure out where your program is and what is currently running. In like I don't know", "tokens": [281, 2573, 484, 689, 428, 1461, 307, 293, 437, 307, 4362, 2614, 13, 682, 411, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.1556851625442505, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.347461799625307e-05}, {"id": 140, "seek": 92564, "start": 925.64, "end": 930.28, "text": " like an interpreted language like Ruby, Python, this might be a bit harder and that information", "tokens": [411, 364, 26749, 2856, 411, 19907, 11, 15329, 11, 341, 1062, 312, 257, 857, 6081, 293, 300, 1589], "temperature": 0.0, "avg_logprob": -0.1670761267344157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 6.97169016348198e-05}, {"id": 141, "seek": 92564, "start": 930.28, "end": 936.3199999999999, "text": " might not be accessible to the kernel without further work. Like it also when you compile", "tokens": [1062, 406, 312, 9515, 281, 264, 28256, 1553, 3052, 589, 13, 1743, 309, 611, 562, 291, 31413], "temperature": 0.0, "avg_logprob": -0.1670761267344157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 6.97169016348198e-05}, {"id": 142, "seek": 92564, "start": 936.3199999999999, "end": 943.24, "text": " you might drop those simple tables like so that you really need to kind of be preparing", "tokens": [291, 1062, 3270, 729, 2199, 8020, 411, 370, 300, 291, 534, 643, 281, 733, 295, 312, 10075], "temperature": 0.0, "avg_logprob": -0.1670761267344157, "compression_ratio": 1.5166666666666666, "no_speech_prob": 6.97169016348198e-05}, {"id": 143, "seek": 94324, "start": 943.24, "end": 956.76, "text": " your application a bit for that. I want to look into the kind of prime example I'm most", "tokens": [428, 3861, 257, 857, 337, 300, 13, 286, 528, 281, 574, 666, 264, 733, 295, 5835, 1365, 286, 478, 881], "temperature": 0.0, "avg_logprob": -0.2082671407443374, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.3298627133481205e-05}, {"id": 144, "seek": 94324, "start": 956.76, "end": 964.92, "text": " familiar with. I'm mostly a go developer over the last couple of years. Go has quite a kind", "tokens": [4963, 365, 13, 286, 478, 5240, 257, 352, 10754, 670, 264, 1036, 1916, 295, 924, 13, 1037, 575, 1596, 257, 733], "temperature": 0.0, "avg_logprob": -0.2082671407443374, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.3298627133481205e-05}, {"id": 145, "seek": 94324, "start": 964.92, "end": 971.64, "text": " of mature set of tools in that area. So basically the standard library allows you to expose", "tokens": [295, 14442, 992, 295, 3873, 294, 300, 1859, 13, 407, 1936, 264, 3832, 6405, 4045, 291, 281, 19219], "temperature": 0.0, "avg_logprob": -0.2082671407443374, "compression_ratio": 1.4808743169398908, "no_speech_prob": 4.3298627133481205e-05}, {"id": 146, "seek": 97164, "start": 971.64, "end": 978.04, "text": " that information. It supports like CPU and memory. And especially garbage collected", "tokens": [300, 1589, 13, 467, 9346, 411, 13199, 293, 4675, 13, 400, 2318, 14150, 11087], "temperature": 0.0, "avg_logprob": -0.20226243336995442, "compression_ratio": 1.6, "no_speech_prob": 1.846182749432046e-05}, {"id": 147, "seek": 97164, "start": 978.04, "end": 984.84, "text": " languages. Memory is quite a thing to also non-garbage collected languages. But memory", "tokens": [8650, 13, 38203, 307, 1596, 257, 551, 281, 611, 2107, 12, 2976, 9742, 11087, 8650, 13, 583, 4675], "temperature": 0.0, "avg_logprob": -0.20226243336995442, "compression_ratio": 1.6, "no_speech_prob": 1.846182749432046e-05}, {"id": 148, "seek": 97164, "start": 984.84, "end": 993.48, "text": " is really important to understand the usage there. I have a quick example of a program", "tokens": [307, 534, 1021, 281, 1223, 264, 14924, 456, 13, 286, 362, 257, 1702, 1365, 295, 257, 1461], "temperature": 0.0, "avg_logprob": -0.20226243336995442, "compression_ratio": 1.6, "no_speech_prob": 1.846182749432046e-05}, {"id": 149, "seek": 97164, "start": 993.48, "end": 997.72, "text": " like so you basically like just expose an HTTP port where you can download the profile", "tokens": [411, 370, 291, 1936, 411, 445, 19219, 364, 33283, 2436, 689, 291, 393, 5484, 264, 7964], "temperature": 0.0, "avg_logprob": -0.20226243336995442, "compression_ratio": 1.6, "no_speech_prob": 1.846182749432046e-05}, {"id": 150, "seek": 99772, "start": 997.72, "end": 1005.2, "text": " whenever you want and you have that P prof tool that you can point to it. So like in that", "tokens": [5699, 291, 528, 293, 291, 362, 300, 430, 1740, 2290, 300, 291, 393, 935, 281, 309, 13, 407, 411, 294, 300], "temperature": 0.0, "avg_logprob": -0.21518873326918658, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.930849536322057e-05}, {"id": 151, "seek": 99772, "start": 1005.2, "end": 1010.48, "text": " kind of first line example you would just get a two second long profile. So CPU profile", "tokens": [733, 295, 700, 1622, 1365, 291, 576, 445, 483, 257, 732, 1150, 938, 7964, 13, 407, 13199, 7964], "temperature": 0.0, "avg_logprob": -0.21518873326918658, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.930849536322057e-05}, {"id": 152, "seek": 99772, "start": 1010.48, "end": 1016.96, "text": " that looks at the CPU for two seconds and basically records like whatever is running", "tokens": [300, 1542, 412, 264, 13199, 337, 732, 3949, 293, 1936, 7724, 411, 2035, 307, 2614], "temperature": 0.0, "avg_logprob": -0.21518873326918658, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.930849536322057e-05}, {"id": 153, "seek": 99772, "start": 1016.96, "end": 1022.36, "text": " how long on the CPU and then you get the file and P prof will allow you to visualize it through", "tokens": [577, 938, 322, 264, 13199, 293, 550, 291, 483, 264, 3991, 293, 430, 1740, 486, 2089, 291, 281, 23273, 309, 807], "temperature": 0.0, "avg_logprob": -0.21518873326918658, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.930849536322057e-05}, {"id": 154, "seek": 102236, "start": 1022.36, "end": 1029.64, "text": " that top table for example. So what I forgot to mention as well. So later you can maybe", "tokens": [300, 1192, 3199, 337, 1365, 13, 407, 437, 286, 5298, 281, 2152, 382, 731, 13, 407, 1780, 291, 393, 1310], "temperature": 0.0, "avg_logprob": -0.16179756557240205, "compression_ratio": 1.5562130177514792, "no_speech_prob": 1.1759356311813463e-05}, {"id": 155, "seek": 102236, "start": 1029.64, "end": 1038.68, "text": " go to that URL and look at that profile that I had as an example. And in the same way you", "tokens": [352, 281, 300, 12905, 293, 574, 412, 300, 7964, 300, 286, 632, 382, 364, 1365, 13, 400, 294, 264, 912, 636, 291], "temperature": 0.0, "avg_logprob": -0.16179756557240205, "compression_ratio": 1.5562130177514792, "no_speech_prob": 1.1759356311813463e-05}, {"id": 156, "seek": 102236, "start": 1038.68, "end": 1045.1200000000001, "text": " can get the memory allocations and P prof also allows you to launch an HTTP server to", "tokens": [393, 483, 264, 4675, 12660, 763, 293, 430, 1740, 611, 4045, 291, 281, 4025, 364, 33283, 7154, 281], "temperature": 0.0, "avg_logprob": -0.16179756557240205, "compression_ratio": 1.5562130177514792, "no_speech_prob": 1.1759356311813463e-05}, {"id": 157, "seek": 104512, "start": 1045.12, "end": 1052.6399999999999, "text": " be a bit more interactive and select certain code paths. So that is like quite a lot in", "tokens": [312, 257, 857, 544, 15141, 293, 3048, 1629, 3089, 14518, 13, 407, 300, 307, 411, 1596, 257, 688, 294], "temperature": 0.0, "avg_logprob": -0.1603519996900237, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.0359924747026525e-05}, {"id": 158, "seek": 104512, "start": 1052.6399999999999, "end": 1061.7199999999998, "text": " the go docs, go.dev about profiling. So I definitely leave it there. So you can also", "tokens": [264, 352, 45623, 11, 352, 13, 40343, 466, 1740, 4883, 13, 407, 286, 2138, 1856, 309, 456, 13, 407, 291, 393, 611], "temperature": 0.0, "avg_logprob": -0.1603519996900237, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.0359924747026525e-05}, {"id": 159, "seek": 104512, "start": 1061.7199999999998, "end": 1067.36, "text": " look at kind of maybe if you are a go developer like use that and play around yourself. But", "tokens": [574, 412, 733, 295, 1310, 498, 291, 366, 257, 352, 10754, 411, 764, 300, 293, 862, 926, 1803, 13, 583], "temperature": 0.0, "avg_logprob": -0.1603519996900237, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.0359924747026525e-05}, {"id": 160, "seek": 104512, "start": 1067.36, "end": 1074.0, "text": " now I want to speak about why profiling might be actually quite difficult. So the example", "tokens": [586, 286, 528, 281, 1710, 466, 983, 1740, 4883, 1062, 312, 767, 1596, 2252, 13, 407, 264, 1365], "temperature": 0.0, "avg_logprob": -0.1603519996900237, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.0359924747026525e-05}, {"id": 161, "seek": 107400, "start": 1074.0, "end": 1080.88, "text": " I had like I had three CPU cycles and if you think about that is not very much. So and", "tokens": [286, 632, 411, 286, 632, 1045, 13199, 17796, 293, 498, 291, 519, 466, 300, 307, 406, 588, 709, 13, 407, 293], "temperature": 0.0, "avg_logprob": -0.18395133972167968, "compression_ratio": 1.790983606557377, "no_speech_prob": 5.9506699471967295e-05}, {"id": 162, "seek": 107400, "start": 1080.88, "end": 1085.48, "text": " just to record what the program was doing in those three CPU cycle probably takes I", "tokens": [445, 281, 2136, 437, 264, 1461, 390, 884, 294, 729, 1045, 13199, 6586, 1391, 2516, 286], "temperature": 0.0, "avg_logprob": -0.18395133972167968, "compression_ratio": 1.790983606557377, "no_speech_prob": 5.9506699471967295e-05}, {"id": 163, "seek": 107400, "start": 1085.48, "end": 1091.12, "text": " have no idea about thousands of CPU cycles. And so you really want to be careful what", "tokens": [362, 572, 1558, 466, 5383, 295, 13199, 17796, 13, 400, 370, 291, 534, 528, 281, 312, 5026, 437], "temperature": 0.0, "avg_logprob": -0.18395133972167968, "compression_ratio": 1.790983606557377, "no_speech_prob": 5.9506699471967295e-05}, {"id": 164, "seek": 107400, "start": 1091.12, "end": 1097.64, "text": " you want to record. So if you really would record all of that like your program would", "tokens": [291, 528, 281, 2136, 13, 407, 498, 291, 534, 576, 2136, 439, 295, 300, 411, 428, 1461, 576], "temperature": 0.0, "avg_logprob": -0.18395133972167968, "compression_ratio": 1.790983606557377, "no_speech_prob": 5.9506699471967295e-05}, {"id": 165, "seek": 107400, "start": 1097.64, "end": 1103.52, "text": " probably have like a massive overhead would slow down by profiling behave completely different", "tokens": [1391, 362, 411, 257, 5994, 19922, 576, 2964, 760, 538, 1740, 4883, 15158, 2584, 819], "temperature": 0.0, "avg_logprob": -0.18395133972167968, "compression_ratio": 1.790983606557377, "no_speech_prob": 5.9506699471967295e-05}, {"id": 166, "seek": 110352, "start": 1103.52, "end": 1109.32, "text": " and you also would have a lot more data to store to analyze. And then if you think about", "tokens": [293, 291, 611, 576, 362, 257, 688, 544, 1412, 281, 3531, 281, 12477, 13, 400, 550, 498, 291, 519, 466], "temperature": 0.0, "avg_logprob": -0.17889698442206325, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.4880038836272433e-05}, {"id": 167, "seek": 110352, "start": 1109.32, "end": 1117.16, "text": " micro services and replica count 500 you might get quite a lot of data that is actually", "tokens": [4532, 3328, 293, 35456, 1207, 5923, 291, 1062, 483, 1596, 257, 688, 295, 1412, 300, 307, 767], "temperature": 0.0, "avg_logprob": -0.17889698442206325, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.4880038836272433e-05}, {"id": 168, "seek": 110352, "start": 1117.16, "end": 1125.48, "text": " not that useful to you because are you really caring about three CPU cycles? Probably not.", "tokens": [406, 300, 4420, 281, 291, 570, 366, 291, 534, 15365, 466, 1045, 13199, 17796, 30, 9210, 406, 13], "temperature": 0.0, "avg_logprob": -0.17889698442206325, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.4880038836272433e-05}, {"id": 169, "seek": 110352, "start": 1125.48, "end": 1131.44, "text": " And because of that to allow continuous profiling so to do that in production across like a", "tokens": [400, 570, 295, 300, 281, 2089, 10957, 1740, 4883, 370, 281, 360, 300, 294, 4265, 2108, 411, 257], "temperature": 0.0, "avg_logprob": -0.17889698442206325, "compression_ratio": 1.5884955752212389, "no_speech_prob": 2.4880038836272433e-05}, {"id": 170, "seek": 113144, "start": 1131.44, "end": 1140.16, "text": " wide set of deployments like I think Google were the first ones to do that and they were", "tokens": [4874, 992, 295, 7274, 1117, 411, 286, 519, 3329, 645, 264, 700, 2306, 281, 360, 300, 293, 436, 645], "temperature": 0.0, "avg_logprob": -0.18684648332141696, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0001143186236731708}, {"id": 171, "seek": 113144, "start": 1140.16, "end": 1146.0800000000002, "text": " starting to sample those profiling. So instead of looking at really every code that runs", "tokens": [2891, 281, 6889, 729, 1740, 4883, 13, 407, 2602, 295, 1237, 412, 534, 633, 3089, 300, 6676], "temperature": 0.0, "avg_logprob": -0.18684648332141696, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0001143186236731708}, {"id": 172, "seek": 113144, "start": 1146.0800000000002, "end": 1153.04, "text": " go for example looks 100 times a second what is currently running on the CPU and then records", "tokens": [352, 337, 1365, 1542, 2319, 1413, 257, 1150, 437, 307, 4362, 2614, 322, 264, 13199, 293, 550, 7724], "temperature": 0.0, "avg_logprob": -0.18684648332141696, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0001143186236731708}, {"id": 173, "seek": 113144, "start": 1153.04, "end": 1160.72, "text": " it and obviously maybe like integer adder will not be on the CPU if you don't run it", "tokens": [309, 293, 2745, 1310, 411, 24922, 909, 260, 486, 406, 312, 322, 264, 13199, 498, 291, 500, 380, 1190, 309], "temperature": 0.0, "avg_logprob": -0.18684648332141696, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0001143186236731708}, {"id": 174, "seek": 116072, "start": 1160.72, "end": 1166.92, "text": " all the time and so you get a really accurate representation what is really taking your", "tokens": [439, 264, 565, 293, 370, 291, 483, 257, 534, 8559, 10290, 437, 307, 534, 1940, 428], "temperature": 0.0, "avg_logprob": -0.1353558105172463, "compression_ratio": 1.7509881422924902, "no_speech_prob": 5.025534119340591e-05}, {"id": 175, "seek": 116072, "start": 1166.92, "end": 1172.28, "text": " CPU time. And the way that works you also need to be kind of aware that like some things", "tokens": [13199, 565, 13, 400, 264, 636, 300, 1985, 291, 611, 643, 281, 312, 733, 295, 3650, 300, 411, 512, 721], "temperature": 0.0, "avg_logprob": -0.1353558105172463, "compression_ratio": 1.7509881422924902, "no_speech_prob": 5.025534119340591e-05}, {"id": 176, "seek": 116072, "start": 1172.28, "end": 1178.04, "text": " the actual program might not be on the CPU because it might be waiting for IO and so", "tokens": [264, 3539, 1461, 1062, 406, 312, 322, 264, 13199, 570, 309, 1062, 312, 3806, 337, 39839, 293, 370], "temperature": 0.0, "avg_logprob": -0.1353558105172463, "compression_ratio": 1.7509881422924902, "no_speech_prob": 5.025534119340591e-05}, {"id": 177, "seek": 116072, "start": 1178.04, "end": 1183.2, "text": " like when you kind of collect a profile and the profile is not having that many seconds", "tokens": [411, 562, 291, 733, 295, 2500, 257, 7964, 293, 264, 7964, 307, 406, 1419, 300, 867, 3949], "temperature": 0.0, "avg_logprob": -0.1353558105172463, "compression_ratio": 1.7509881422924902, "no_speech_prob": 5.025534119340591e-05}, {"id": 178, "seek": 116072, "start": 1183.2, "end": 1188.16, "text": " you really need to think about is this really what I want to optimize or maybe I'm not seeing", "tokens": [291, 534, 643, 281, 519, 466, 307, 341, 534, 437, 286, 528, 281, 19719, 420, 1310, 286, 478, 406, 2577], "temperature": 0.0, "avg_logprob": -0.1353558105172463, "compression_ratio": 1.7509881422924902, "no_speech_prob": 5.025534119340591e-05}, {"id": 179, "seek": 118816, "start": 1188.16, "end": 1198.88, "text": " what I actually want to see. With that kind of statistical approach like I don't have", "tokens": [437, 286, 767, 528, 281, 536, 13, 2022, 300, 733, 295, 22820, 3109, 411, 286, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.1701592810360002, "compression_ratio": 1.6186046511627907, "no_speech_prob": 4.6925739297876135e-05}, {"id": 180, "seek": 118816, "start": 1198.88, "end": 1204.24, "text": " any kind of sources to say but like I think generally you say that it's like a two to", "tokens": [604, 733, 295, 7139, 281, 584, 457, 411, 286, 519, 5101, 291, 584, 300, 309, 311, 411, 257, 732, 281], "temperature": 0.0, "avg_logprob": -0.1701592810360002, "compression_ratio": 1.6186046511627907, "no_speech_prob": 4.6925739297876135e-05}, {"id": 181, "seek": 118816, "start": 1204.24, "end": 1209.16, "text": " three percent overhead that gets added on top of your program execution so that's I guess", "tokens": [1045, 3043, 19922, 300, 2170, 3869, 322, 1192, 295, 428, 1461, 15058, 370, 300, 311, 286, 2041], "temperature": 0.0, "avg_logprob": -0.1701592810360002, "compression_ratio": 1.6186046511627907, "no_speech_prob": 4.6925739297876135e-05}, {"id": 182, "seek": 118816, "start": 1209.16, "end": 1217.76, "text": " a lot more reasonable than the full approach with recording everything. And so what do", "tokens": [257, 688, 544, 10585, 813, 264, 1577, 3109, 365, 6613, 1203, 13, 400, 370, 437, 360], "temperature": 0.0, "avg_logprob": -0.1701592810360002, "compression_ratio": 1.6186046511627907, "no_speech_prob": 4.6925739297876135e-05}, {"id": 183, "seek": 121776, "start": 1217.76, "end": 1221.28, "text": " you generally kind of would do obviously if you first need to ship your application somewhere", "tokens": [291, 5101, 733, 295, 576, 360, 2745, 498, 291, 700, 643, 281, 5374, 428, 3861, 4079], "temperature": 0.0, "avg_logprob": -0.12595697477752088, "compression_ratio": 1.7913385826771653, "no_speech_prob": 4.385469947010279e-05}, {"id": 184, "seek": 121776, "start": 1221.28, "end": 1228.68, "text": " and run it then you can look at the profiles and yeah think about it look at it like maybe", "tokens": [293, 1190, 309, 550, 291, 393, 574, 412, 264, 23693, 293, 1338, 519, 466, 309, 574, 412, 309, 411, 1310], "temperature": 0.0, "avg_logprob": -0.12595697477752088, "compression_ratio": 1.7913385826771653, "no_speech_prob": 4.385469947010279e-05}, {"id": 185, "seek": 121776, "start": 1228.68, "end": 1234.2, "text": " you are the owner of that code maybe you have a bit more understanding and those profiles", "tokens": [291, 366, 264, 7289, 295, 300, 3089, 1310, 291, 362, 257, 857, 544, 3701, 293, 729, 23693], "temperature": 0.0, "avg_logprob": -0.12595697477752088, "compression_ratio": 1.7913385826771653, "no_speech_prob": 4.385469947010279e-05}, {"id": 186, "seek": 121776, "start": 1234.2, "end": 1239.04, "text": " maybe can give you a bit more of an idea of what you're actually doing there or how the", "tokens": [1310, 393, 976, 291, 257, 857, 544, 295, 364, 1558, 295, 437, 291, 434, 767, 884, 456, 420, 577, 264], "temperature": 0.0, "avg_logprob": -0.12595697477752088, "compression_ratio": 1.7913385826771653, "no_speech_prob": 4.385469947010279e-05}, {"id": 187, "seek": 121776, "start": 1239.04, "end": 1246.32, "text": " system is reacting to your code. And so basically like for that green box multiple solutions", "tokens": [1185, 307, 25817, 281, 428, 3089, 13, 400, 370, 1936, 411, 337, 300, 3092, 2424, 3866, 6547], "temperature": 0.0, "avg_logprob": -0.12595697477752088, "compression_ratio": 1.7913385826771653, "no_speech_prob": 4.385469947010279e-05}, {"id": 188, "seek": 124632, "start": 1246.32, "end": 1251.6799999999998, "text": " exist so I'm obviously a bit biased but I also have to say our project is fairly young", "tokens": [2514, 370, 286, 478, 2745, 257, 857, 28035, 457, 286, 611, 362, 281, 584, 527, 1716, 307, 6457, 2037], "temperature": 0.0, "avg_logprob": -0.1781285398749895, "compression_ratio": 1.5128205128205128, "no_speech_prob": 8.938603423302993e-05}, {"id": 189, "seek": 124632, "start": 1251.6799999999998, "end": 1261.56, "text": " and evolving. So for example there's like CNCF Pixie, EBPF based, there's Polar Signals", "tokens": [293, 21085, 13, 407, 337, 1365, 456, 311, 411, 48714, 37, 18652, 414, 11, 50148, 47, 37, 2361, 11, 456, 311, 3635, 289, 13515, 1124], "temperature": 0.0, "avg_logprob": -0.1781285398749895, "compression_ratio": 1.5128205128205128, "no_speech_prob": 8.938603423302993e-05}, {"id": 190, "seek": 124632, "start": 1261.56, "end": 1268.72, "text": " Parker like people are in the room, Pyroscope and kind of our solution. I think they're", "tokens": [20155, 411, 561, 366, 294, 264, 1808, 11, 9953, 2635, 13960, 293, 733, 295, 527, 3827, 13, 286, 519, 436, 434], "temperature": 0.0, "avg_logprob": -0.1781285398749895, "compression_ratio": 1.5128205128205128, "no_speech_prob": 8.938603423302993e-05}, {"id": 191, "seek": 124632, "start": 1268.72, "end": 1274.56, "text": " all great like you can all use them and start using them and exploring like maybe just your", "tokens": [439, 869, 411, 291, 393, 439, 764, 552, 293, 722, 1228, 552, 293, 12736, 411, 1310, 445, 428], "temperature": 0.0, "avg_logprob": -0.1781285398749895, "compression_ratio": 1.5128205128205128, "no_speech_prob": 8.938603423302993e-05}, {"id": 192, "seek": 127456, "start": 1274.56, "end": 1279.84, "text": " benchmarks for a start and then as you get more familiar with it like you might kind", "tokens": [43751, 337, 257, 722, 293, 550, 382, 291, 483, 544, 4963, 365, 309, 411, 291, 1062, 733], "temperature": 0.0, "avg_logprob": -0.13483938489641462, "compression_ratio": 1.5411764705882354, "no_speech_prob": 8.282859198516235e-05}, {"id": 193, "seek": 127456, "start": 1279.84, "end": 1288.32, "text": " of discover more and more of the value there. So I'm still going to use Flare now for my", "tokens": [295, 4411, 544, 293, 544, 295, 264, 2158, 456, 13, 407, 286, 478, 920, 516, 281, 764, 3235, 543, 586, 337, 452], "temperature": 0.0, "avg_logprob": -0.13483938489641462, "compression_ratio": 1.5411764705882354, "no_speech_prob": 8.282859198516235e-05}, {"id": 194, "seek": 127456, "start": 1288.32, "end": 1304.1599999999999, "text": " quick demo. So let me just see. So I guess most of you are kind of familiar with Grafana", "tokens": [1702, 10723, 13, 407, 718, 385, 445, 536, 13, 407, 286, 2041, 881, 295, 291, 366, 733, 295, 4963, 365, 8985, 69, 2095], "temperature": 0.0, "avg_logprob": -0.13483938489641462, "compression_ratio": 1.5411764705882354, "no_speech_prob": 8.282859198516235e-05}, {"id": 195, "seek": 130416, "start": 1304.16, "end": 1317.8000000000002, "text": " and Explore. Why is it so huge? And so basically that's kind of the entry point you're going", "tokens": [293, 12514, 418, 13, 1545, 307, 309, 370, 2603, 30, 400, 370, 1936, 300, 311, 733, 295, 264, 8729, 935, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.20774852207728794, "compression_ratio": 1.52, "no_speech_prob": 4.201271804049611e-05}, {"id": 196, "seek": 130416, "start": 1317.8000000000002, "end": 1324.8000000000002, "text": " to see in the Explore. You have the kind of typical time range selection so let's say", "tokens": [281, 536, 294, 264, 12514, 418, 13, 509, 362, 264, 733, 295, 7476, 565, 3613, 9450, 370, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.20774852207728794, "compression_ratio": 1.52, "no_speech_prob": 4.201271804049611e-05}, {"id": 197, "seek": 130416, "start": 1324.8000000000002, "end": 1329.24, "text": " we want to see the last 15 minutes now and here we can see the profile types collected.", "tokens": [321, 528, 281, 536, 264, 1036, 2119, 2077, 586, 293, 510, 321, 393, 536, 264, 7964, 3467, 11087, 13], "temperature": 0.0, "avg_logprob": -0.20774852207728794, "compression_ratio": 1.52, "no_speech_prob": 4.201271804049611e-05}, {"id": 198, "seek": 132924, "start": 1329.24, "end": 1334.24, "text": " So that's just a Docker compose running locally on my laptop, hopefully running locally on", "tokens": [407, 300, 311, 445, 257, 33772, 35925, 2614, 16143, 322, 452, 10732, 11, 4696, 2614, 16143, 322], "temperature": 0.0, "avg_logprob": -0.20613566688869311, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.24273135245312e-05}, {"id": 199, "seek": 132924, "start": 1334.24, "end": 1341.2, "text": " my laptop since I started to talk. And for example we can here look at the CPU. So that's", "tokens": [452, 10732, 1670, 286, 1409, 281, 751, 13, 400, 337, 1365, 321, 393, 510, 574, 412, 264, 13199, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.20613566688869311, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.24273135245312e-05}, {"id": 200, "seek": 132924, "start": 1341.2, "end": 1349.6, "text": " kind of the nanoseconds band on the CPU and you can kind of see the flame graph from earlier", "tokens": [733, 295, 264, 14067, 541, 28750, 4116, 322, 264, 13199, 293, 291, 393, 733, 295, 536, 264, 13287, 4295, 490, 3071], "temperature": 0.0, "avg_logprob": -0.20613566688869311, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.24273135245312e-05}, {"id": 201, "seek": 132924, "start": 1349.6, "end": 1355.16, "text": " and maybe some bug. I don't know. It looks a bit bigger than it usually should be. But", "tokens": [293, 1310, 512, 7426, 13, 286, 500, 380, 458, 13, 467, 1542, 257, 857, 3801, 813, 309, 2673, 820, 312, 13, 583], "temperature": 0.0, "avg_logprob": -0.20613566688869311, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.24273135245312e-05}, {"id": 202, "seek": 135516, "start": 1355.16, "end": 1359.48, "text": " we can see that kind of top table. We can see the aggregation of all of the services.", "tokens": [321, 393, 536, 300, 733, 295, 1192, 3199, 13, 492, 393, 536, 264, 16743, 399, 295, 439, 295, 264, 3328, 13], "temperature": 0.0, "avg_logprob": -0.11596656490016628, "compression_ratio": 1.6934865900383143, "no_speech_prob": 2.342558764212299e-05}, {"id": 203, "seek": 135516, "start": 1359.48, "end": 1364.68, "text": " So I'm running like five pods or something like that, different languages. So you can", "tokens": [407, 286, 478, 2614, 411, 1732, 31925, 420, 746, 411, 300, 11, 819, 8650, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.11596656490016628, "compression_ratio": 1.6934865900383143, "no_speech_prob": 2.342558764212299e-05}, {"id": 204, "seek": 135516, "start": 1364.68, "end": 1372.2, "text": " see like for example this here is like a Python main module where it's doing some prime numbers.", "tokens": [536, 411, 337, 1365, 341, 510, 307, 411, 257, 15329, 2135, 10088, 689, 309, 311, 884, 512, 5835, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11596656490016628, "compression_ratio": 1.6934865900383143, "no_speech_prob": 2.342558764212299e-05}, {"id": 205, "seek": 135516, "start": 1372.2, "end": 1378.0, "text": " So what I first want to kind of break down here is by label. And that's really the only", "tokens": [407, 437, 286, 700, 528, 281, 733, 295, 1821, 760, 510, 307, 538, 7645, 13, 400, 300, 311, 534, 264, 787], "temperature": 0.0, "avg_logprob": -0.11596656490016628, "compression_ratio": 1.6934865900383143, "no_speech_prob": 2.342558764212299e-05}, {"id": 206, "seek": 135516, "start": 1378.0, "end": 1381.8000000000002, "text": " kind of functionality that we have in terms of querying. So here we would look at the", "tokens": [733, 295, 14980, 300, 321, 362, 294, 2115, 295, 7083, 1840, 13, 407, 510, 321, 576, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.11596656490016628, "compression_ratio": 1.6934865900383143, "no_speech_prob": 2.342558764212299e-05}, {"id": 207, "seek": 138180, "start": 1381.8, "end": 1391.28, "text": " different instances and we kind of see the CPU time spent like, I don't know, there's", "tokens": [819, 14519, 293, 321, 733, 295, 536, 264, 13199, 565, 4418, 411, 11, 286, 500, 380, 458, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.23492325676812065, "compression_ratio": 1.441340782122905, "no_speech_prob": 2.6925419660983607e-05}, {"id": 208, "seek": 138180, "start": 1391.28, "end": 1395.28, "text": " like a Rust port and they are both blue so I don't know which switch, but I guess Flare", "tokens": [411, 257, 34952, 2436, 293, 436, 366, 1293, 3344, 370, 286, 500, 380, 458, 597, 3679, 11, 457, 286, 2041, 3235, 543], "temperature": 0.0, "avg_logprob": -0.23492325676812065, "compression_ratio": 1.441340782122905, "no_speech_prob": 2.6925419660983607e-05}, {"id": 209, "seek": 138180, "start": 1395.28, "end": 1403.96, "text": " is doing more. So that might be the Flare one. And for my example now I want to look", "tokens": [307, 884, 544, 13, 407, 300, 1062, 312, 264, 3235, 543, 472, 13, 400, 337, 452, 1365, 586, 286, 528, 281, 574], "temperature": 0.0, "avg_logprob": -0.23492325676812065, "compression_ratio": 1.441340782122905, "no_speech_prob": 2.6925419660983607e-05}, {"id": 210, "seek": 140396, "start": 1403.96, "end": 1414.08, "text": " at just like a small program that I wrote to show like how like the aspect. So like here", "tokens": [412, 445, 411, 257, 1359, 1461, 300, 286, 4114, 281, 855, 411, 577, 411, 264, 4171, 13, 407, 411, 510], "temperature": 0.0, "avg_logprob": -0.1541122896917935, "compression_ratio": 1.6157407407407407, "no_speech_prob": 8.511869054927956e-06}, {"id": 211, "seek": 140396, "start": 1414.08, "end": 1418.76, "text": " we can see kind of the timeline. So this is like a profile gets collected I think every", "tokens": [321, 393, 536, 733, 295, 264, 12933, 13, 407, 341, 307, 411, 257, 7964, 2170, 11087, 286, 519, 633], "temperature": 0.0, "avg_logprob": -0.1541122896917935, "compression_ratio": 1.6157407407407407, "no_speech_prob": 8.511869054927956e-06}, {"id": 212, "seek": 140396, "start": 1418.76, "end": 1424.68, "text": " 15 seconds and that's basically a dot. And then the flame graph and the top table below", "tokens": [2119, 3949, 293, 300, 311, 1936, 257, 5893, 13, 400, 550, 264, 13287, 4295, 293, 264, 1192, 3199, 2507], "temperature": 0.0, "avg_logprob": -0.1541122896917935, "compression_ratio": 1.6157407407407407, "no_speech_prob": 8.511869054927956e-06}, {"id": 213, "seek": 140396, "start": 1424.68, "end": 1428.4, "text": " would kind of just aggregate that. So like there's no time component in here. That's", "tokens": [576, 733, 295, 445, 26118, 300, 13, 407, 411, 456, 311, 572, 565, 6542, 294, 510, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.1541122896917935, "compression_ratio": 1.6157407407407407, "no_speech_prob": 8.511869054927956e-06}, {"id": 214, "seek": 142840, "start": 1428.4, "end": 1436.92, "text": " also important to understand. And so like while we were looking at memory I'm now going", "tokens": [611, 1021, 281, 1223, 13, 400, 370, 411, 1339, 321, 645, 1237, 412, 4675, 286, 478, 586, 516], "temperature": 0.0, "avg_logprob": -0.16249047219753265, "compression_ratio": 1.5823529411764705, "no_speech_prob": 7.398262823699042e-06}, {"id": 215, "seek": 142840, "start": 1436.92, "end": 1448.6000000000001, "text": " to kind of switch to the allocated space. And oh no. And here we have some label selection", "tokens": [281, 733, 295, 3679, 281, 264, 29772, 1901, 13, 400, 1954, 572, 13, 400, 510, 321, 362, 512, 7645, 9450], "temperature": 0.0, "avg_logprob": -0.16249047219753265, "compression_ratio": 1.5823529411764705, "no_speech_prob": 7.398262823699042e-06}, {"id": 216, "seek": 142840, "start": 1448.6000000000001, "end": 1455.48, "text": " like that you might be familiar. And this random port here you can see like the allocation", "tokens": [411, 300, 291, 1062, 312, 4963, 13, 400, 341, 4974, 2436, 510, 291, 393, 536, 411, 264, 27599], "temperature": 0.0, "avg_logprob": -0.16249047219753265, "compression_ratio": 1.5823529411764705, "no_speech_prob": 7.398262823699042e-06}, {"id": 217, "seek": 145548, "start": 1455.48, "end": 1462.4, "text": " so the amount of memory that gets allocated is like around six megabytes. But then every", "tokens": [370, 264, 2372, 295, 4675, 300, 2170, 29772, 307, 411, 926, 2309, 10816, 24538, 13, 583, 550, 633], "temperature": 0.0, "avg_logprob": -0.1554949246920072, "compression_ratio": 1.5197740112994351, "no_speech_prob": 2.1651532733812928e-05}, {"id": 218, "seek": 145548, "start": 1462.4, "end": 1470.92, "text": " couple of every five minutes roughly you can see like some peak. And so if you already", "tokens": [1916, 295, 633, 1732, 2077, 9810, 291, 393, 536, 411, 512, 10651, 13, 400, 370, 498, 291, 1217], "temperature": 0.0, "avg_logprob": -0.1554949246920072, "compression_ratio": 1.5197740112994351, "no_speech_prob": 2.1651532733812928e-05}, {"id": 219, "seek": 145548, "start": 1470.92, "end": 1480.1200000000001, "text": " look in the flame graph there's already some kind of big red box and the colors don't matter.", "tokens": [574, 294, 264, 13287, 4295, 456, 311, 1217, 512, 733, 295, 955, 2182, 2424, 293, 264, 4577, 500, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.1554949246920072, "compression_ratio": 1.5197740112994351, "no_speech_prob": 2.1651532733812928e-05}, {"id": 220, "seek": 148012, "start": 1480.12, "end": 1485.0, "text": " But basically like you can see this this kind of piece of code is doing kind of a majority", "tokens": [583, 1936, 411, 291, 393, 536, 341, 341, 733, 295, 2522, 295, 3089, 307, 884, 733, 295, 257, 6286], "temperature": 0.0, "avg_logprob": -0.1274269587958037, "compression_ratio": 1.6134969325153374, "no_speech_prob": 4.8056088417069986e-05}, {"id": 221, "seek": 148012, "start": 1485.0, "end": 1489.52, "text": " of the allocations. And now you could even kind of zoom in here if you really want to", "tokens": [295, 264, 12660, 763, 13, 400, 586, 291, 727, 754, 733, 295, 8863, 294, 510, 498, 291, 534, 528, 281], "temperature": 0.0, "avg_logprob": -0.1274269587958037, "compression_ratio": 1.6134969325153374, "no_speech_prob": 4.8056088417069986e-05}, {"id": 222, "seek": 148012, "start": 1489.52, "end": 1496.6, "text": " figure out and then it even gets bigger and you can see some more what's going on. And", "tokens": [2573, 484, 293, 550, 309, 754, 2170, 3801, 293, 291, 393, 536, 512, 544, 437, 311, 516, 322, 13, 400], "temperature": 0.0, "avg_logprob": -0.1274269587958037, "compression_ratio": 1.6134969325153374, "no_speech_prob": 4.8056088417069986e-05}, {"id": 223, "seek": 149660, "start": 1496.6, "end": 1512.76, "text": " so now if you actually want to look at the kind of code for this. And if flare is maybe", "tokens": [370, 586, 498, 291, 767, 528, 281, 574, 412, 264, 733, 295, 3089, 337, 341, 13, 400, 498, 32446, 307, 1310], "temperature": 0.0, "avg_logprob": -0.18220599710124813, "compression_ratio": 1.5549132947976878, "no_speech_prob": 7.914720299595501e-06}, {"id": 224, "seek": 149660, "start": 1512.76, "end": 1516.7199999999998, "text": " in version 0.6 we could even see the line of code that we should look at for now you", "tokens": [294, 3037, 1958, 13, 21, 321, 727, 754, 536, 264, 1622, 295, 3089, 300, 321, 820, 574, 412, 337, 586, 291], "temperature": 0.0, "avg_logprob": -0.18220599710124813, "compression_ratio": 1.5549132947976878, "no_speech_prob": 7.914720299595501e-06}, {"id": 225, "seek": 149660, "start": 1516.7199999999998, "end": 1524.56, "text": " can. But basically like it would show us allocations in line 21. And I guess most of you can see", "tokens": [393, 13, 583, 1936, 411, 309, 576, 855, 505, 12660, 763, 294, 1622, 5080, 13, 400, 286, 2041, 881, 295, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.18220599710124813, "compression_ratio": 1.5549132947976878, "no_speech_prob": 7.914720299595501e-06}, {"id": 226, "seek": 152456, "start": 1524.56, "end": 1527.96, "text": " what this kind of program is doing so every five minutes it will kind of have some peak", "tokens": [437, 341, 733, 295, 1461, 307, 884, 370, 633, 1732, 2077, 309, 486, 733, 295, 362, 512, 10651], "temperature": 0.0, "avg_logprob": -0.18772366403163165, "compression_ratio": 1.5517241379310345, "no_speech_prob": 6.789308099541813e-05}, {"id": 227, "seek": 152456, "start": 1527.96, "end": 1534.3999999999999, "text": " of allocations. And you only see that kind of because you have the time component you", "tokens": [295, 12660, 763, 13, 400, 291, 787, 536, 300, 733, 295, 570, 291, 362, 264, 565, 6542, 291], "temperature": 0.0, "avg_logprob": -0.18772366403163165, "compression_ratio": 1.5517241379310345, "no_speech_prob": 6.789308099541813e-05}, {"id": 228, "seek": 152456, "start": 1534.3999999999999, "end": 1543.84, "text": " can select and then see the flame graph aggregation. Cool. Yeah that was almost my talk. Like I", "tokens": [393, 3048, 293, 550, 536, 264, 13287, 4295, 16743, 399, 13, 8561, 13, 865, 300, 390, 1920, 452, 751, 13, 1743, 286], "temperature": 0.0, "avg_logprob": -0.18772366403163165, "compression_ratio": 1.5517241379310345, "no_speech_prob": 6.789308099541813e-05}, {"id": 229, "seek": 152456, "start": 1543.84, "end": 1549.32, "text": " have one more slide that I should just quickly want to. So in the latest version 120 there", "tokens": [362, 472, 544, 4137, 300, 286, 820, 445, 2661, 528, 281, 13, 407, 294, 264, 6792, 3037, 10411, 456], "temperature": 0.0, "avg_logprob": -0.18772366403163165, "compression_ratio": 1.5517241379310345, "no_speech_prob": 6.789308099541813e-05}, {"id": 230, "seek": 154932, "start": 1549.32, "end": 1555.1599999999999, "text": " is profile guided optimizations and I think that might be a really big topic. So what", "tokens": [307, 7964, 19663, 5028, 14455, 293, 286, 519, 300, 1062, 312, 257, 534, 955, 4829, 13, 407, 437], "temperature": 0.0, "avg_logprob": -0.15452607164105164, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.00012267848069313914}, {"id": 231, "seek": 154932, "start": 1555.1599999999999, "end": 1559.84, "text": " it does it looks at kind of your profile and that can come from production from benchmarks", "tokens": [309, 775, 309, 1542, 412, 733, 295, 428, 7964, 293, 300, 393, 808, 490, 4265, 490, 43751], "temperature": 0.0, "avg_logprob": -0.15452607164105164, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.00012267848069313914}, {"id": 232, "seek": 154932, "start": 1559.84, "end": 1567.2, "text": " from whatever and tries to do the better decisions during compile time of what things to do with", "tokens": [490, 2035, 293, 9898, 281, 360, 264, 1101, 5327, 1830, 31413, 565, 295, 437, 721, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.15452607164105164, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.00012267848069313914}, {"id": 233, "seek": 154932, "start": 1567.2, "end": 1571.36, "text": " your code like for example think the only thing that it does right now is making inlining", "tokens": [428, 3089, 411, 337, 1365, 519, 264, 787, 551, 300, 309, 775, 558, 586, 307, 1455, 294, 31079], "temperature": 0.0, "avg_logprob": -0.15452607164105164, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.00012267848069313914}, {"id": 234, "seek": 154932, "start": 1571.36, "end": 1577.1599999999999, "text": " decisions. But basically like if it sees this method is called a lot and is in the hot path", "tokens": [5327, 13, 583, 1936, 411, 498, 309, 8194, 341, 3170, 307, 1219, 257, 688, 293, 307, 294, 264, 2368, 3100], "temperature": 0.0, "avg_logprob": -0.15452607164105164, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.00012267848069313914}, {"id": 235, "seek": 157716, "start": 1577.16, "end": 1581.8400000000001, "text": " it would then make the decisions to inline the method maybe if it's a bit colder it would", "tokens": [309, 576, 550, 652, 264, 5327, 281, 294, 1889, 264, 3170, 1310, 498, 309, 311, 257, 857, 31020, 309, 576], "temperature": 0.0, "avg_logprob": -0.1870138168334961, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00013338819553609937}, {"id": 236, "seek": 157716, "start": 1581.8400000000001, "end": 1587.5600000000002, "text": " not do it and you can be a lot more accurate as a compiler if you have that kind of data", "tokens": [406, 360, 309, 293, 291, 393, 312, 257, 688, 544, 8559, 382, 257, 31958, 498, 291, 362, 300, 733, 295, 1412], "temperature": 0.0, "avg_logprob": -0.1870138168334961, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00013338819553609937}, {"id": 237, "seek": 157716, "start": 1587.5600000000002, "end": 1600.92, "text": " if you know that method is in the hot path or not. Okay that was it. Thank you.", "tokens": [498, 291, 458, 300, 3170, 307, 294, 264, 2368, 3100, 420, 406, 13, 1033, 300, 390, 309, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1870138168334961, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00013338819553609937}, {"id": 238, "seek": 160092, "start": 1600.92, "end": 1612.04, "text": " Thanks a lot that was awesome. Questions. Thank you. Thank you for the talk. I'm just", "tokens": [2561, 257, 688, 300, 390, 3476, 13, 27738, 13, 1044, 291, 13, 1044, 291, 337, 264, 751, 13, 286, 478, 445], "temperature": 0.0, "avg_logprob": -0.18040470594770452, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0002049729082500562}, {"id": 239, "seek": 160092, "start": 1612.04, "end": 1617.52, "text": " wondering how would the profiling work with very multi-threaded code. Is there ability", "tokens": [6359, 577, 576, 264, 1740, 4883, 589, 365, 588, 4825, 12, 392, 2538, 292, 3089, 13, 1119, 456, 3485], "temperature": 0.0, "avg_logprob": -0.18040470594770452, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0002049729082500562}, {"id": 240, "seek": 160092, "start": 1617.52, "end": 1623.48, "text": " to drill down into that level. Yeah so like maybe so like in terms of multi-threading", "tokens": [281, 11392, 760, 666, 300, 1496, 13, 865, 370, 411, 1310, 370, 411, 294, 2115, 295, 4825, 12, 392, 35908], "temperature": 0.0, "avg_logprob": -0.18040470594770452, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0002049729082500562}, {"id": 241, "seek": 160092, "start": 1623.48, "end": 1628.4, "text": " like obviously we only have the main method in that example. So you can see rude and then", "tokens": [411, 2745, 321, 787, 362, 264, 2135, 3170, 294, 300, 1365, 13, 407, 291, 393, 536, 18895, 293, 550], "temperature": 0.0, "avg_logprob": -0.18040470594770452, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0002049729082500562}, {"id": 242, "seek": 162840, "start": 1628.4, "end": 1633.96, "text": " mine is 100% and like if it's multi-threaded you would have kind of maybe more so it's", "tokens": [3892, 307, 2319, 4, 293, 411, 498, 309, 311, 4825, 12, 392, 2538, 292, 291, 576, 362, 733, 295, 1310, 544, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.21238662456643992, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0001660004345467314}, {"id": 243, "seek": 162840, "start": 1633.96, "end": 1639.16, "text": " basically all only the stack trace that gets recorded like you would not see kind of maybe", "tokens": [1936, 439, 787, 264, 8630, 13508, 300, 2170, 8287, 411, 291, 576, 406, 536, 733, 295, 1310], "temperature": 0.0, "avg_logprob": -0.21238662456643992, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0001660004345467314}, {"id": 244, "seek": 162840, "start": 1639.16, "end": 1643.48, "text": " the connections where the thread where it's threading off and things like that. You would", "tokens": [264, 9271, 689, 264, 7207, 689, 309, 311, 7207, 278, 766, 293, 721, 411, 300, 13, 509, 576], "temperature": 0.0, "avg_logprob": -0.21238662456643992, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0001660004345467314}, {"id": 245, "seek": 162840, "start": 1643.48, "end": 1651.44, "text": " get the stack trace. Cold stack. Have you looked into any other profiling formats than", "tokens": [483, 264, 8630, 13508, 13, 16918, 8630, 13, 3560, 291, 2956, 666, 604, 661, 1740, 4883, 25879, 813], "temperature": 0.0, "avg_logprob": -0.21238662456643992, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0001660004345467314}, {"id": 246, "seek": 165144, "start": 1651.44, "end": 1658.8, "text": " B prof ingestion. I know open telemetry has been doing some stuff about introducing a profiling", "tokens": [363, 1740, 3957, 31342, 13, 286, 458, 1269, 4304, 5537, 627, 575, 668, 884, 512, 1507, 466, 15424, 257, 1740, 4883], "temperature": 0.0, "avg_logprob": -0.2366928948296441, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.00044646667083725333}, {"id": 247, "seek": 165144, "start": 1658.8, "end": 1664.44, "text": " format that people can standardize on but I don't know if you've looked at that at all.", "tokens": [7877, 300, 561, 393, 3832, 1125, 322, 457, 286, 500, 380, 458, 498, 291, 600, 2956, 412, 300, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2366928948296441, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.00044646667083725333}, {"id": 248, "seek": 165144, "start": 1664.44, "end": 1674.28, "text": " Yes. Can you like I haven't seen you like sorry can you repeat like I struggled to.", "tokens": [1079, 13, 1664, 291, 411, 286, 2378, 380, 1612, 291, 411, 2597, 393, 291, 7149, 411, 286, 19023, 281, 13], "temperature": 0.0, "avg_logprob": -0.2366928948296441, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.00044646667083725333}, {"id": 249, "seek": 165144, "start": 1674.28, "end": 1678.3200000000002, "text": " So I was wondering if you've looked at any other profiling ingestion formats other than", "tokens": [407, 286, 390, 6359, 498, 291, 600, 2956, 412, 604, 661, 1740, 4883, 3957, 31342, 25879, 661, 813], "temperature": 0.0, "avg_logprob": -0.2366928948296441, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.00044646667083725333}, {"id": 250, "seek": 167832, "start": 1678.32, "end": 1685.72, "text": " B prof. No I like I or so like right now we use P prof personally with the player. So", "tokens": [363, 1740, 13, 883, 286, 411, 286, 420, 370, 411, 558, 586, 321, 764, 430, 1740, 5665, 365, 264, 4256, 13, 407], "temperature": 0.0, "avg_logprob": -0.2163090291230575, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00014256408030632883}, {"id": 251, "seek": 167832, "start": 1685.72, "end": 1691.8799999999999, "text": " I think there's a lot of kind of improvements to be had over over the format there and that's", "tokens": [286, 519, 456, 311, 257, 688, 295, 733, 295, 13797, 281, 312, 632, 670, 670, 264, 7877, 456, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.2163090291230575, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00014256408030632883}, {"id": 252, "seek": 167832, "start": 1691.8799999999999, "end": 1698.1599999999999, "text": " like as far as I know some active work around open telemetry to to come to I guess a better", "tokens": [411, 382, 1400, 382, 286, 458, 512, 4967, 589, 926, 1269, 4304, 5537, 627, 281, 281, 808, 281, 286, 2041, 257, 1101], "temperature": 0.0, "avg_logprob": -0.2163090291230575, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00014256408030632883}, {"id": 253, "seek": 167832, "start": 1698.1599999999999, "end": 1703.12, "text": " format in the sense to not send symbols over and over again and reduce interest but not", "tokens": [7877, 294, 264, 2020, 281, 406, 2845, 16944, 670, 293, 670, 797, 293, 5407, 1179, 457, 406], "temperature": 0.0, "avg_logprob": -0.2163090291230575, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00014256408030632883}, {"id": 254, "seek": 170312, "start": 1703.12, "end": 1712.8, "text": " no it's the the accurate and short answer. Okay so thank you for the talk and my question", "tokens": [572, 309, 311, 264, 264, 8559, 293, 2099, 1867, 13, 1033, 370, 1309, 291, 337, 264, 751, 293, 452, 1168], "temperature": 0.0, "avg_logprob": -0.22342027150667632, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0002916670055128634}, {"id": 255, "seek": 170312, "start": 1712.8, "end": 1716.56, "text": " is that looking at the flare architecture it's currently pool models so the flare agent", "tokens": [307, 300, 1237, 412, 264, 32446, 9482, 309, 311, 4362, 7005, 5245, 370, 264, 32446, 9461], "temperature": 0.0, "avg_logprob": -0.22342027150667632, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0002916670055128634}, {"id": 256, "seek": 170312, "start": 1716.56, "end": 1720.7199999999998, "text": " is scraping the profiling data from the applications that they configure it to scrape. My question", "tokens": [307, 43738, 264, 1740, 4883, 1412, 490, 264, 5821, 300, 436, 22162, 309, 281, 32827, 13, 1222, 1168], "temperature": 0.0, "avg_logprob": -0.22342027150667632, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0002916670055128634}, {"id": 257, "seek": 170312, "start": 1720.7199999999998, "end": 1725.84, "text": " is is there an eventual plan to also add maybe a push gateway or similar API for applications", "tokens": [307, 307, 456, 364, 33160, 1393, 281, 611, 909, 1310, 257, 2944, 28532, 420, 2531, 9362, 337, 5821], "temperature": 0.0, "avg_logprob": -0.22342027150667632, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0002916670055128634}, {"id": 258, "seek": 170312, "start": 1725.84, "end": 1731.8, "text": " where this might be suitable. Yeah like definitely like I think I also can see kind of the push", "tokens": [689, 341, 1062, 312, 12873, 13, 865, 411, 2138, 411, 286, 519, 286, 611, 393, 536, 733, 295, 264, 2944], "temperature": 0.0, "avg_logprob": -0.22342027150667632, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0002916670055128634}, {"id": 259, "seek": 173180, "start": 1731.8, "end": 1737.3999999999999, "text": " use case for maybe if you want to get your micro benchmarks from CI CD in so like the", "tokens": [764, 1389, 337, 1310, 498, 291, 528, 281, 483, 428, 4532, 43751, 490, 37777, 6743, 294, 370, 411, 264], "temperature": 0.0, "avg_logprob": -0.15310002755427707, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.00022882611665409058}, {"id": 260, "seek": 173180, "start": 1737.3999999999999, "end": 1743.9199999999998, "text": " API in theory allows it but tooling is missing but I definitely think it's a valid like push", "tokens": [9362, 294, 5261, 4045, 309, 457, 46593, 307, 5361, 457, 286, 2138, 519, 309, 311, 257, 7363, 411, 2944], "temperature": 0.0, "avg_logprob": -0.15310002755427707, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.00022882611665409058}, {"id": 261, "seek": 173180, "start": 1743.9199999999998, "end": 1754.32, "text": " use case as well. I think in terms of scalability I think pool will be better but yeah I agree.", "tokens": [764, 1389, 382, 731, 13, 286, 519, 294, 2115, 295, 15664, 2310, 286, 519, 7005, 486, 312, 1101, 457, 1338, 286, 3986, 13], "temperature": 0.0, "avg_logprob": -0.15310002755427707, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.00022882611665409058}, {"id": 262, "seek": 175432, "start": 1754.32, "end": 1764.04, "text": " Thanks for the talk. I have a small question. Did you try to implement this tooling in the", "tokens": [2561, 337, 264, 751, 13, 286, 362, 257, 1359, 1168, 13, 2589, 291, 853, 281, 4445, 341, 46593, 294, 264], "temperature": 0.0, "avg_logprob": -0.1631904032038546, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00035016564652323723}, {"id": 263, "seek": 175432, "start": 1764.04, "end": 1772.72, "text": " end of the CI CD and CO continuous optimization? No like so we're not using it yet for that.", "tokens": [917, 295, 264, 37777, 6743, 293, 3002, 10957, 19618, 30, 883, 411, 370, 321, 434, 406, 1228, 309, 1939, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1631904032038546, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00035016564652323723}, {"id": 264, "seek": 175432, "start": 1772.72, "end": 1778.8, "text": " I think it's it's definitely a super useful thing because like yeah like you want to see", "tokens": [286, 519, 309, 311, 309, 311, 2138, 257, 1687, 4420, 551, 570, 411, 1338, 411, 291, 528, 281, 536], "temperature": 0.0, "avg_logprob": -0.1631904032038546, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00035016564652323723}, {"id": 265, "seek": 175432, "start": 1778.8, "end": 1782.72, "text": " maybe how a pool request behaves like maybe how your application allocates more or less", "tokens": [1310, 577, 257, 7005, 5308, 36896, 411, 1310, 577, 428, 3861, 12660, 1024, 544, 420, 1570], "temperature": 0.0, "avg_logprob": -0.1631904032038546, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00035016564652323723}, {"id": 266, "seek": 178272, "start": 1782.72, "end": 1787.64, "text": " in different parts and and if the trade-offs are right there but yeah I think it definitely", "tokens": [294, 819, 3166, 293, 293, 498, 264, 4923, 12, 19231, 366, 558, 456, 457, 1338, 286, 519, 309, 2138], "temperature": 0.0, "avg_logprob": -0.19853916764259338, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00023205003526527435}, {"id": 267, "seek": 178272, "start": 1787.64, "end": 1804.08, "text": " can and should be used for that but no no tooling right now. Yeah no I fully agree as", "tokens": [393, 293, 820, 312, 1143, 337, 300, 457, 572, 572, 46593, 558, 586, 13, 865, 572, 286, 4498, 3986, 382], "temperature": 0.0, "avg_logprob": -0.19853916764259338, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00023205003526527435}, {"id": 268, "seek": 178272, "start": 1804.08, "end": 1812.4, "text": " well yeah. Hello thank you. So if I understand correctly profiles such as traces combined", "tokens": [731, 1338, 13, 2425, 1309, 291, 13, 407, 498, 286, 1223, 8944, 23693, 1270, 382, 26076, 9354], "temperature": 0.0, "avg_logprob": -0.19853916764259338, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.00023205003526527435}, {"id": 269, "seek": 181240, "start": 1812.4, "end": 1819.96, "text": " with OS metrics right so at a concrete specific time you can see how much CPU you used and", "tokens": [365, 12731, 16367, 558, 370, 412, 257, 9859, 2685, 565, 291, 393, 536, 577, 709, 13199, 291, 1143, 293], "temperature": 0.0, "avg_logprob": -0.14263525776479435, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00015358613745775074}, {"id": 270, "seek": 181240, "start": 1819.96, "end": 1826.88, "text": " so on right. Yeah I guess it looks a bit more at the actual line of code rather than I don't", "tokens": [370, 322, 558, 13, 865, 286, 2041, 309, 1542, 257, 857, 544, 412, 264, 3539, 1622, 295, 3089, 2831, 813, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.14263525776479435, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00015358613745775074}, {"id": 271, "seek": 181240, "start": 1826.88, "end": 1830.88, "text": " know like I don't know I haven't used like tracing where it automatically finds the function", "tokens": [458, 411, 286, 500, 380, 458, 286, 2378, 380, 1143, 411, 25262, 689, 309, 6772, 10704, 264, 2445], "temperature": 0.0, "avg_logprob": -0.14263525776479435, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00015358613745775074}, {"id": 272, "seek": 181240, "start": 1830.88, "end": 1838.5600000000002, "text": " maybe that also tells you the line of code but yeah like it definitely adds some metrics", "tokens": [1310, 300, 611, 5112, 291, 264, 1622, 295, 3089, 457, 1338, 411, 309, 2138, 10860, 512, 16367], "temperature": 0.0, "avg_logprob": -0.14263525776479435, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.00015358613745775074}, {"id": 273, "seek": 183856, "start": 1838.56, "end": 1843.44, "text": " to it like without you doing much I guess other than making sure it can read the symbol", "tokens": [281, 309, 411, 1553, 291, 884, 709, 286, 2041, 661, 813, 1455, 988, 309, 393, 1401, 264, 5986], "temperature": 0.0, "avg_logprob": -0.1637265396118164, "compression_ratio": 1.7661290322580645, "no_speech_prob": 4.589353557094e-05}, {"id": 274, "seek": 183856, "start": 1843.44, "end": 1847.96, "text": " tables and the function names. Yeah so so I just had like a dumb question or like dumb", "tokens": [8020, 293, 264, 2445, 5288, 13, 865, 370, 370, 286, 445, 632, 411, 257, 10316, 1168, 420, 411, 10316], "temperature": 0.0, "avg_logprob": -0.1637265396118164, "compression_ratio": 1.7661290322580645, "no_speech_prob": 4.589353557094e-05}, {"id": 275, "seek": 183856, "start": 1847.96, "end": 1854.44, "text": " idea couldn't you just combine for example you already have node exporter which exposes", "tokens": [1558, 2809, 380, 291, 445, 10432, 337, 1365, 291, 1217, 362, 9984, 1278, 6122, 597, 1278, 4201], "temperature": 0.0, "avg_logprob": -0.1637265396118164, "compression_ratio": 1.7661290322580645, "no_speech_prob": 4.589353557094e-05}, {"id": 276, "seek": 183856, "start": 1854.44, "end": 1862.12, "text": " metrics at all times so you have OS metrics and you have traces for example so couldn't", "tokens": [16367, 412, 439, 1413, 370, 291, 362, 12731, 16367, 293, 291, 362, 26076, 337, 1365, 370, 2809, 380], "temperature": 0.0, "avg_logprob": -0.1637265396118164, "compression_ratio": 1.7661290322580645, "no_speech_prob": 4.589353557094e-05}, {"id": 277, "seek": 183856, "start": 1862.12, "end": 1867.0, "text": " you just have some kind of integration graph on or or somewhere else that just combines", "tokens": [291, 445, 362, 512, 733, 295, 10980, 4295, 322, 420, 420, 4079, 1646, 300, 445, 29520], "temperature": 0.0, "avg_logprob": -0.1637265396118164, "compression_ratio": 1.7661290322580645, "no_speech_prob": 4.589353557094e-05}, {"id": 278, "seek": 186700, "start": 1867.0, "end": 1870.76, "text": " traces with metrics. Yeah so like I think that was also the like like people that work", "tokens": [26076, 365, 16367, 13, 865, 370, 411, 286, 519, 300, 390, 611, 264, 411, 411, 561, 300, 589], "temperature": 0.0, "avg_logprob": -0.16274154733080382, "compression_ratio": 1.7198443579766538, "no_speech_prob": 7.791759708197787e-05}, {"id": 279, "seek": 186700, "start": 1870.76, "end": 1875.48, "text": " longer at continuous profiling software that they try to kind of reuse kind of Prometheus", "tokens": [2854, 412, 10957, 1740, 4883, 4722, 300, 436, 853, 281, 733, 295, 26225, 733, 295, 2114, 649, 42209], "temperature": 0.0, "avg_logprob": -0.16274154733080382, "compression_ratio": 1.7198443579766538, "no_speech_prob": 7.791759708197787e-05}, {"id": 280, "seek": 186700, "start": 1875.48, "end": 1881.4, "text": " and I think where you end up kind of in it's just a very high cardinality it's too many", "tokens": [293, 286, 519, 689, 291, 917, 493, 733, 295, 294, 309, 311, 445, 257, 588, 1090, 2920, 259, 1860, 309, 311, 886, 867], "temperature": 0.0, "avg_logprob": -0.16274154733080382, "compression_ratio": 1.7198443579766538, "no_speech_prob": 7.791759708197787e-05}, {"id": 281, "seek": 186700, "start": 1881.4, "end": 1886.04, "text": " lines of codes and and that's kind of where it stops but like in theory like I guess most", "tokens": [3876, 295, 14211, 293, 293, 300, 311, 733, 295, 689, 309, 10094, 457, 411, 294, 5261, 411, 286, 2041, 881], "temperature": 0.0, "avg_logprob": -0.16274154733080382, "compression_ratio": 1.7198443579766538, "no_speech_prob": 7.791759708197787e-05}, {"id": 282, "seek": 186700, "start": 1886.04, "end": 1892.4, "text": " promql constructs and functions are maybe something we need to implement on top of that", "tokens": [2234, 80, 75, 7690, 82, 293, 6828, 366, 1310, 746, 321, 643, 281, 4445, 322, 1192, 295, 300], "temperature": 0.0, "avg_logprob": -0.16274154733080382, "compression_ratio": 1.7198443579766538, "no_speech_prob": 7.791759708197787e-05}, {"id": 283, "seek": 189240, "start": 1892.4, "end": 1899.0, "text": " in a similar way because in the end you just get metrics out of it and so basically the", "tokens": [294, 257, 2531, 636, 570, 294, 264, 917, 291, 445, 483, 16367, 484, 295, 309, 293, 370, 1936, 264], "temperature": 0.0, "avg_logprob": -0.20379429176205494, "compression_ratio": 1.5521472392638036, "no_speech_prob": 6.359568214975297e-05}, {"id": 284, "seek": 189240, "start": 1899.0, "end": 1903.76, "text": " problem was too many lines of code too much changing over time and like you just get too", "tokens": [1154, 390, 886, 867, 3876, 295, 3089, 886, 709, 4473, 670, 565, 293, 411, 291, 445, 483, 886], "temperature": 0.0, "avg_logprob": -0.20379429176205494, "compression_ratio": 1.5521472392638036, "no_speech_prob": 6.359568214975297e-05}, {"id": 285, "seek": 190376, "start": 1903.76, "end": 1922.68, "text": " much serious turn through that. So thanks a lot. Yeah thank you for coming.", "tokens": [50364, 709, 3156, 1261, 807, 300, 13, 407, 3231, 257, 688, 13, 865, 1309, 291, 337, 1348, 13, 51310], "temperature": 0.0, "avg_logprob": -0.3558505535125732, "compression_ratio": 1.0273972602739727, "no_speech_prob": 0.0002470256295055151}], "language": "en"}