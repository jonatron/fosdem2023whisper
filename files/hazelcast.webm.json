{"text": " The next session is a very important one around streaming and Java. Of course, streaming is an increasingly, or has been for years, important popular topic. And Thibautz is going to tell us more about it. Yes. Thank you. You have to talk loudly. Yes. Yeah. So, welcome everyone. So, this session is mainly about three-time stream processing. So, what I'm planning to do today, because it's Sunday and early morning, is to make it as easy as possible. And the fact is, I don't know your background, so I'm not sure how much you know about real-time stream processing. So, I will take it from scratch, basically, to get up to speed everyone. And I will also show you, demo how you can basically use real-time stream processing in your work as well. So, before we start, anyone recognize these guys on the screen here? On the left. Details. Yes, that's correct. So, these are the details. On the right side is the Liverpool Football Cup. That's where I came from. So, I wanted to highlight these two images here, because I wanted to say, you know, real-time stream processing is not about domain specific. So, it could be anywhere. So, it doesn't have to be, like, for example, in financial institutions, or machine learning, or IT or IT. It could be, for example, in sports, or music, or any domain, basically. The fact is, you're using real-time stream processing in every single day. So, just to give you an idea what real-time means, and how you can actually approach it. So, anyone can guess how long it takes for an IT blink? The question is a second. So, yeah, sub-milli-seconds are roughly one-third of a second. So, that's pretty fast. So, the same thing applies if you want to clap hands, or if you want to take a photo as well. So, we're not talking about minutes here. We're not talking about days or weeks, which what batch system is all about. We were talking about, like, sub-milli-seconds, how you can process it in real-time. So, as you can see, it's everywhere. So, it's not domain specific. And some of you who work already with real-time know that, basically, you have some kind of events coming into this moment, and you try to make sense out of it. So, looking at it from user perspective, what you want is to make sure that you have some kind of secret source, or key element, when it comes to real-time stream processing. So, I've seen it so many times. People approach it from the wrong angle. So, they try, basically, to read the data in real-time, and they try, basically, to provide some kind of meaning of this data. So, I'll give you a demo today for logs, so this should be easy to follow. But the secret source here is to kind of combine new data, real-time data with historical data. So, what we mean by real-time data is this data is coming this moment, and you read it in this moment. Obviously, you want to make sense out of it. You want to understand what's going on here. And the historical data is normal data. We know about, like, stored somewhere on physical drive, for example, or database, whatever. So, you want to make sure, basically, to have these two types of data at the same speed. So, now we're talking about two types of data, but how many, you know, is too many, basically? What size we're talking about here? So, for some might be, like, a few thousand, others might be millions, others might be billions. So, essentially, what we're trying to do here is not taking, like, a small data set and trying to process it, because that's, you know, easy to do. But we're talking about, like, over a billion, or over, like, ten billion of seconds in transactions per second. So, the idea here is to take a huge amount of data and, you know, trying to find some kind of trains and alerts from this data. So, I'll give you an example here, so you can start now to work on what's going on here. So, imagine, basically, you write a Java program, and you obviously have some kind of logging mechanism in your application. So, in order to understand what's going on with your log system, essentially, what you want is to have some kind of platform to allow you to actually, you know, analyze it, but also, like, at the same time, we're not talking about logs from yesterday. So, for example, events happened yesterday, you want to make sure that, for example, you actually do alerts or trains in the same moment. So, same thing for trains as well. So, if you want to know if your application is going to crash or not, what you want is to kind of have a platform or solution where it is easy for you to actually look at the data in this moment and say, hey, something is going wrong here. I need to basically define trains out of it and do some alerts. Now, for manual work, this is kind of like painful because you need to go through loops, for example, and you want to make sure that you know how to scale it and also kind of like, you know, knowing exactly where your data is stored because your enemy, when it comes to real-time scene processing, is latency. So, you want to make sure your application is as low as latency when it comes to delay, basically. Obviously, the scaling is bottleneck. And now, if you look at platforms, now, you might have heard of some of these. So, the easiest way is to split these platforms into various categories. So, on this one, here, you can see, you can have open source solutions or you can have hybrid, which is mixed between open source and the managed service. And on the horizontal, as you can see, you need to capture your data. Obviously, in real-time, you need to do some kind of transport as well as some kind of transformation and processing as well. So, you can split it into 12 squares and it becomes like, you know, easier to understand which tool you need to use. But obviously, this is still a bit complex because the area for real-time scene processing is mainly about two different subjects. So, it's not only, you know, capture or transport because you have to do all of these at the same time. It's kind of like, if you want, you need basically to decide if you're going to use swim processing engines from one side or you want to have some kind of fast data storage from the other side. So, swim processing engines are pretty good in handling data coming in real-time, which is like, for example, Kafka or TX equals and so on. Or from the far right side, you can see fast data storage, which is kind of like essentially caching solutions to your application. So, for example, MongoDB, Redis and so on. So, if you want to apply this solution, essentially you need one tool from the left and one tool from the right, which means that it adds more work on your side. So, what you want is kind of looking at it in this way and say, hey, I want one solution for you, and that's where Hazercast comes into place. Obviously, I work for Hazercast and the Hazercast as itself is built on top of the Java virtual machine. So, it's Java based and it's open source. So, this is the platform here. It's kind of like a A to Z solution. So, what you want is to catch your data, capture your data. It could be coming from Apache, for example, Apache Kafka and from IoT devices. It could be coming from some kind of custom connectors because it's open source, which means feel free to contribute to this project or if it comes from file watch up, for example, or from work suffix. So, once you have this data, free time data ingested into the platform, platform itself has two main components. So, the first one is the jet engine. So, this is the engine for scene processing and the demo I will show you how to use it and also the fast data storage or fast data management. So, this is essentially a component which allows you to load your data from external sources and it's optional, obviously. So, it's kind of like, I don't know, some kind of file system or database or stored on the cloud and you load it into memory. Why do you need to load it into memory? Simply because you want to make sure your application is as fast as possible. So, we're talking about, for example, here speed where it's sub milliseconds or fractions of seconds. So, this is very important. For example, in fraud detection scenario, if you, for example, you're using your cards and someone else is using your card somewhere else, you want to get alert in this specific moment. It doesn't make sense to get alert, I don't know, in the afternoon, for example, or next day. So, this type of machine critical solutions, what you want is to make sure that your data is stored in memory which allows you to access it in really good time. So, once you have this data, you can do some kind of transformation for your, for example, on data because remember, we're not talking about one single data source. For scene processing, we're talking about multiple sources. So, it could be, for example, I don't know, some transactions coming in Kafka topic and some IoT device for, weather forecast for examine coming from other topic, sorry, from other source. So, you want to make sure that also you can combine it. Obviously, because here we're in Java run, so you can use the Java, obviously, client for it. So, essentially what you need is kind of like a Java jar. You need to download it and plug it into your pump file. But, for example, if you're a data scientist and you're not sure, you know, about programming language, maybe you use a little bit of Python, but, you know, programming languages is not something you want to invest in. What you can do is do some, everything I mentioned today in SQL. So, which means you do everything for team, written scene processing in terms of alerts, for example, or defining trace, or even query your data using SQL. So, once you do this process, you actually can output it in some kind of, I don't know, same thing for your input. So, the Kafka topic, for example, you can use the WebSocket, or you can create your Java application to do some kind of visualization for predictions, for example. Now, the cool thing about Hazelkast is not only the platform and the easiness of use, but also how to scale. Remember what we're talking about here? We're not talking about a few thousand. We're talking maybe a few million, or even billions of transactions. So, when it comes to scaling, you want to recognize between two different topics. So, for some, scaling might refer to, I don't know, your data. So, you want to scale this data, for example, and for others who work, for example, in programming or development, they focus mainly on the compute. So, you want to make sure, basically, to combine between data and compute when you want to scale. And the cool thing about it is it's partition aware, which means if you have your data stored in multiple places around the world, for example, in different data centers, what you want is your compute or your process or your application is to be stored as close as possible to your data. So, this will give you some kind of speed and lower latency when it comes to transactions. Now, we talked about transactions, but how many we're talking about today? So, we've done this kind of like benchmark, obviously. It's bit outdated now. It's kind of worth trying to add one more zero to it. So, it's one billion transactions per second on 45 nodes. And the cool thing about it is not only the latency, which is like 30 milliseconds, but also the linear scale, which means you just need to add more nodes to your application. So, with that being said, so let's just move directly to the demo and I'll show you how you can use HazardCast within your application. So, for this demo, what I wanted is kind of, you know, you're writing your application, Java application, obviously, and you have some kind of logging mechanism within your application. And your boss comes next day and says, hey, your log messages or your solution, we need to upgrade it in a way where we'll provide some kind of alerts or predict what's going to happen next. So, your task, essentially, what you want is to kind of take exactly the application and make sure you have some kind of scanning mechanism and real-time screen processing into it. So, obviously, you have two options here. So, the first option is to download this jar, plug it into your application and run it. So, that's good. But the problem with this is usually logs stored in different places. So, what you want is you want every machine to send its logs to some, you know, center place. So, usually, it's a cloud. So, the idea of you sending logs to cloud is kind of providing some kind of one place for every single machine which sends logs. HazardCast runs the HazardCast Viridian, which is exactly what we're talking about on the cloud. And this is kind of like what you need to do. So, you write your log message in some way. So, obviously, you need to have context for your message. And the idea is to store all logs into memory. So, we're not going to store it into database or file system because that means you're adding input-output latency to your application. So, we need to minimize this. So, the idea is to use some kind of map structure. And this map has a key, which is like where is this data coming from, from ID address and port number, and the message which is the value. So, it's your choice now. Whether you use Varchar, for example, string. Obviously, it's faster. Or if you want to use some kind of JSON format, for example, if you're trying to do some kind of machine learning and do, I don't know, maybe some classification on your logs. So, once you have your tm value, what you can do is to proceed to save it into HazardCast. So, remember what we're talking about here? So, what you want is to get this map within HazardCast. So, first step is to send your logs into the cloud, obviously. So, because different logs coming from different machines. And second step is to store it into memory. So, this will allow you to access your logs in much faster way. And obviously, you need to do this mapping. So, what you see here is kind of like SQL. You can write it in SQL. And once you have it in SQL, you can proceed to do this instance. So, in order to run HazardCast, what you need is have some kind, I don't know, from HazardCast instance and block in your pipeline. So, pipeline first to the JIT, for example, or your process. And in here, what you say is basically, I'm defining this is the IP address I want to run it on. This is my part and this is my data. Now, I mentioned SQL. So, the platform itself has management center. So, this is really cool, which allows you to query your data. So, what you see here is I'm trying to query my data of my logs and really trying to understand what's going on. So, on the left, you see the key, which is like IP address input. And on the far right side, or bottom right side, you see values. So, essentially what we're doing with logs is we're giving a score for each log message. So, if you're trying to define it, for example, I know, for example, you have a specific category for logs, whether it's information or warning or error. So, that's good, but if you're trying to predict what's going to happen next, you probably need to have some kind of linear scare for it. So, for example, you run it from 100 and you give it value for your log message and this is the value you see there. So, we take this data and we ingest it into memory. So, we have the IP address port number and we have a score for each log message. And once we import it, we define a trend. So, in my case, I'm taking a window on this real-time stream processing and, in my case, it's two minutes, but it could be anything. And I'm defining this trend based on the score value for each log message. And based on this, I try to create predictive, or prediction map, so another map, in order to say if I want to send alert, which has value of one or zero, don't send alert. Obviously, you need to do some kind of programming in it. So, the idea is to group messages based on the score to define the trend and from there, you can use some kind of machine learning, so linear regression or classification for example to do some kind of prediction and also you need to output it, which means you send it back to the user. Now, this is good, so this is how it works. So, this is your actual code for doing the map prediction. So, we take the score for each log message, we do the linear regression based on the window I define, and I'm simply checking if the value is greater than 100, send an alert, otherwise, don't send alert. And in here, what you need to focus on is kind of like three ways to proceed from here. So, there are three ways to do stream processing, and the first one is to use SQL for example, so you take it from the logs map and you store it in our log map with some filtering. And second option is to use a process or pipeline, so you read it from the map and you do as well, you do some kind of alerts for example or train. So, the first two ways is called batch stream processing, so it's not read time. And the third option, so this is the main thing you want to do if you want to provide some kind of read time messaging, is to look for kind of the map journal, which is also similar to map, but it's like ring puffer sucker, which allows you to start processing your logs, either from start or from end depends on what you want. And the pipeline itself takes this memory map, logs and do some filtering. So, if you see some value in this specific moment, you can do this alerts for example, or you can send it to the message. So, I think kind of like I wanted to cover everything, but because it's too 20 minutes here and it's not enough time to talk about everything, so just to give you somebody what you need to do and open this stage for questions. So, first of all, you need to store your logs to the cloud. So, in my case, I'm using HazerCast, but obviously you can't use any other cloud provider, but you need to import HazerCast into that cloud provider. So, once you upload all logs, what you want is kind of like have some kind of cloud solution for it, so where you can use for example, I don't know, Varchar, if you want speed or JSON, for example, if you want to apply some kind of machine learning, and you need to use some kind of map structure. So, the idea is to store logs into memory in order to have some kind of random artist and rebalancing. And obviously you need to configure this map because you have a specific size that's on limits on it, so you need to have some kind of eviction policy on your data. And obviously you need to consider security. So, whatever you send to the cloud, obviously it can, you need to have some kind of security mechanism just to make sure that you don't send sensitive data. So, if you're interested in this topic about stream processing, we're running an unconference in March next month, and it's free to join, and there is like training workshop as well. So, all you need to do is just scan this code and register for the real-time stream processing unconference. It's community-based, so it's open source. You get the training, you get batch on top of it, as well as we have run table where you can see industrial experts as well as community users how they can contribute to open source of projects. And, you know, you can basically ask questions if you have. So, with that being said, thanks very much for listening and I'll open for questions. Thank you. Thank you. Yeah, so it is possible to do like multiple streams, joining multiple sources. So, this is possible to do. Obviously, you need just to find the configuration for this specific case. So, I mentioned sources here. So, the sources is not only a single source, it could be multiple sources, and that's where you do join multiple sources. So, it is possible to do it with data. Any other questions? Yeah? Yeah. Please take a seat. What is the difference? So, the question is what is the difference between HazardCast and Apache Flake? So, there was a slide, but I decided to remove it from here. So, essentially what you want is kind of like, for real time is to look for minimizing latency within your application. So, we've done benchmark between HazardCast and Flake. So, this is where things can make difference. So, if you're trying to basically write an application for real-time sequencing, we want to minimize latency. So, the lower is better, and this is where HazardCast has performed a link or Apache Spark. So, the latency is the key difference between these two. The results are online, but I decided not to include it just for this. So, basically it's the latency between different platforms. So, that's what you want to focus, whether it's HazardCast, Flake, or any other platform which offers real-time sequencing. Thank you. Thank you very much, Thomas. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " The next session is a very important one around streaming and Java.", "tokens": [50364, 440, 958, 5481, 307, 257, 588, 1021, 472, 926, 11791, 293, 10745, 13, 50864], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 1, "seek": 0, "start": 10.0, "end": 15.0, "text": " Of course, streaming is an increasingly, or has been for years, important popular topic.", "tokens": [50864, 2720, 1164, 11, 11791, 307, 364, 12980, 11, 420, 575, 668, 337, 924, 11, 1021, 3743, 4829, 13, 51114], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 2, "seek": 0, "start": 15.0, "end": 17.0, "text": " And Thibautz is going to tell us more about it.", "tokens": [51114, 400, 334, 897, 1375, 89, 307, 516, 281, 980, 505, 544, 466, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 3, "seek": 0, "start": 17.0, "end": 18.0, "text": " Yes.", "tokens": [51214, 1079, 13, 51264], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 4, "seek": 0, "start": 18.0, "end": 19.0, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 5, "seek": 0, "start": 19.0, "end": 20.0, "text": " You have to talk loudly.", "tokens": [51314, 509, 362, 281, 751, 22958, 13, 51364], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 6, "seek": 0, "start": 20.0, "end": 21.0, "text": " Yes.", "tokens": [51364, 1079, 13, 51414], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 7, "seek": 0, "start": 21.0, "end": 22.0, "text": " Yeah.", "tokens": [51414, 865, 13, 51464], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 8, "seek": 0, "start": 22.0, "end": 25.0, "text": " So, welcome everyone.", "tokens": [51464, 407, 11, 2928, 1518, 13, 51614], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 9, "seek": 0, "start": 25.0, "end": 29.0, "text": " So, this session is mainly about three-time stream processing.", "tokens": [51614, 407, 11, 341, 5481, 307, 8704, 466, 1045, 12, 3766, 4309, 9007, 13, 51814], "temperature": 0.0, "avg_logprob": -0.313614501953125, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.3253234922885895}, {"id": 10, "seek": 2900, "start": 29.0, "end": 33.0, "text": " So, what I'm planning to do today, because it's Sunday and early morning,", "tokens": [50364, 407, 11, 437, 286, 478, 5038, 281, 360, 965, 11, 570, 309, 311, 7776, 293, 2440, 2446, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 11, "seek": 2900, "start": 33.0, "end": 35.0, "text": " is to make it as easy as possible.", "tokens": [50564, 307, 281, 652, 309, 382, 1858, 382, 1944, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 12, "seek": 2900, "start": 35.0, "end": 38.0, "text": " And the fact is, I don't know your background,", "tokens": [50664, 400, 264, 1186, 307, 11, 286, 500, 380, 458, 428, 3678, 11, 50814], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 13, "seek": 2900, "start": 38.0, "end": 42.0, "text": " so I'm not sure how much you know about real-time stream processing.", "tokens": [50814, 370, 286, 478, 406, 988, 577, 709, 291, 458, 466, 957, 12, 3766, 4309, 9007, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 14, "seek": 2900, "start": 42.0, "end": 47.0, "text": " So, I will take it from scratch, basically, to get up to speed everyone.", "tokens": [51014, 407, 11, 286, 486, 747, 309, 490, 8459, 11, 1936, 11, 281, 483, 493, 281, 3073, 1518, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 15, "seek": 2900, "start": 47.0, "end": 54.0, "text": " And I will also show you, demo how you can basically use real-time stream processing in your work as well.", "tokens": [51264, 400, 286, 486, 611, 855, 291, 11, 10723, 577, 291, 393, 1936, 764, 957, 12, 3766, 4309, 9007, 294, 428, 589, 382, 731, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 16, "seek": 2900, "start": 54.0, "end": 58.0, "text": " So, before we start, anyone recognize these guys on the screen here?", "tokens": [51614, 407, 11, 949, 321, 722, 11, 2878, 5521, 613, 1074, 322, 264, 2568, 510, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10049938410520554, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.09997527301311493}, {"id": 17, "seek": 5800, "start": 59.0, "end": 61.0, "text": " On the left.", "tokens": [50414, 1282, 264, 1411, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 18, "seek": 5800, "start": 65.0, "end": 66.0, "text": " Details.", "tokens": [50714, 42811, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 19, "seek": 5800, "start": 66.0, "end": 67.0, "text": " Yes, that's correct.", "tokens": [50764, 1079, 11, 300, 311, 3006, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 20, "seek": 5800, "start": 67.0, "end": 69.0, "text": " So, these are the details.", "tokens": [50814, 407, 11, 613, 366, 264, 4365, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 21, "seek": 5800, "start": 69.0, "end": 72.0, "text": " On the right side is the Liverpool Football Cup.", "tokens": [50914, 1282, 264, 558, 1252, 307, 264, 32473, 31406, 13751, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 22, "seek": 5800, "start": 72.0, "end": 74.0, "text": " That's where I came from.", "tokens": [51064, 663, 311, 689, 286, 1361, 490, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 23, "seek": 5800, "start": 74.0, "end": 79.0, "text": " So, I wanted to highlight these two images here, because I wanted to say,", "tokens": [51164, 407, 11, 286, 1415, 281, 5078, 613, 732, 5267, 510, 11, 570, 286, 1415, 281, 584, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 24, "seek": 5800, "start": 79.0, "end": 82.0, "text": " you know, real-time stream processing is not about domain specific.", "tokens": [51414, 291, 458, 11, 957, 12, 3766, 4309, 9007, 307, 406, 466, 9274, 2685, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 25, "seek": 5800, "start": 82.0, "end": 84.0, "text": " So, it could be anywhere.", "tokens": [51564, 407, 11, 309, 727, 312, 4992, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15158792252236225, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.009725627489387989}, {"id": 26, "seek": 8400, "start": 84.0, "end": 88.0, "text": " So, it doesn't have to be, like, for example, in financial institutions,", "tokens": [50364, 407, 11, 309, 1177, 380, 362, 281, 312, 11, 411, 11, 337, 1365, 11, 294, 4669, 8142, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 27, "seek": 8400, "start": 88.0, "end": 90.0, "text": " or machine learning, or IT or IT.", "tokens": [50564, 420, 3479, 2539, 11, 420, 6783, 420, 6783, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 28, "seek": 8400, "start": 90.0, "end": 94.0, "text": " It could be, for example, in sports, or music, or any domain, basically.", "tokens": [50664, 467, 727, 312, 11, 337, 1365, 11, 294, 6573, 11, 420, 1318, 11, 420, 604, 9274, 11, 1936, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 29, "seek": 8400, "start": 94.0, "end": 98.0, "text": " The fact is, you're using real-time stream processing in every single day.", "tokens": [50864, 440, 1186, 307, 11, 291, 434, 1228, 957, 12, 3766, 4309, 9007, 294, 633, 2167, 786, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 30, "seek": 8400, "start": 98.0, "end": 101.0, "text": " So, just to give you an idea what real-time means,", "tokens": [51064, 407, 11, 445, 281, 976, 291, 364, 1558, 437, 957, 12, 3766, 1355, 11, 51214], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 31, "seek": 8400, "start": 101.0, "end": 104.0, "text": " and how you can actually approach it.", "tokens": [51214, 293, 577, 291, 393, 767, 3109, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 32, "seek": 8400, "start": 104.0, "end": 107.0, "text": " So, anyone can guess how long it takes for an IT blink?", "tokens": [51364, 407, 11, 2878, 393, 2041, 577, 938, 309, 2516, 337, 364, 6783, 24667, 30, 51514], "temperature": 0.0, "avg_logprob": -0.14597662220830504, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017249686643481255}, {"id": 33, "seek": 10700, "start": 108.0, "end": 110.0, "text": " The question is a second.", "tokens": [50414, 440, 1168, 307, 257, 1150, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 34, "seek": 10700, "start": 110.0, "end": 113.0, "text": " So, yeah, sub-milli-seconds are roughly one-third of a second.", "tokens": [50514, 407, 11, 1338, 11, 1422, 12, 18841, 72, 12, 8159, 10166, 366, 9810, 472, 12, 25095, 295, 257, 1150, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 35, "seek": 10700, "start": 113.0, "end": 114.0, "text": " So, that's pretty fast.", "tokens": [50664, 407, 11, 300, 311, 1238, 2370, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 36, "seek": 10700, "start": 114.0, "end": 117.0, "text": " So, the same thing applies if you want to clap hands,", "tokens": [50714, 407, 11, 264, 912, 551, 13165, 498, 291, 528, 281, 20760, 2377, 11, 50864], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 37, "seek": 10700, "start": 117.0, "end": 119.0, "text": " or if you want to take a photo as well.", "tokens": [50864, 420, 498, 291, 528, 281, 747, 257, 5052, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 38, "seek": 10700, "start": 119.0, "end": 121.0, "text": " So, we're not talking about minutes here.", "tokens": [50964, 407, 11, 321, 434, 406, 1417, 466, 2077, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 39, "seek": 10700, "start": 121.0, "end": 125.0, "text": " We're not talking about days or weeks, which what batch system is all about.", "tokens": [51064, 492, 434, 406, 1417, 466, 1708, 420, 3259, 11, 597, 437, 15245, 1185, 307, 439, 466, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 40, "seek": 10700, "start": 125.0, "end": 128.0, "text": " We were talking about, like, sub-milli-seconds,", "tokens": [51264, 492, 645, 1417, 466, 11, 411, 11, 1422, 12, 18841, 72, 12, 8159, 10166, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 41, "seek": 10700, "start": 128.0, "end": 130.0, "text": " how you can process it in real-time.", "tokens": [51414, 577, 291, 393, 1399, 309, 294, 957, 12, 3766, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 42, "seek": 10700, "start": 130.0, "end": 132.0, "text": " So, as you can see, it's everywhere.", "tokens": [51514, 407, 11, 382, 291, 393, 536, 11, 309, 311, 5315, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 43, "seek": 10700, "start": 132.0, "end": 135.0, "text": " So, it's not domain specific.", "tokens": [51614, 407, 11, 309, 311, 406, 9274, 2685, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15370902262235941, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.010677975602447987}, {"id": 44, "seek": 13500, "start": 135.0, "end": 139.0, "text": " And some of you who work already with real-time know that, basically,", "tokens": [50364, 400, 512, 295, 291, 567, 589, 1217, 365, 957, 12, 3766, 458, 300, 11, 1936, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 45, "seek": 13500, "start": 139.0, "end": 142.0, "text": " you have some kind of events coming into this moment,", "tokens": [50564, 291, 362, 512, 733, 295, 3931, 1348, 666, 341, 1623, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 46, "seek": 13500, "start": 142.0, "end": 145.0, "text": " and you try to make sense out of it.", "tokens": [50714, 293, 291, 853, 281, 652, 2020, 484, 295, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 47, "seek": 13500, "start": 145.0, "end": 148.0, "text": " So, looking at it from user perspective,", "tokens": [50864, 407, 11, 1237, 412, 309, 490, 4195, 4585, 11, 51014], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 48, "seek": 13500, "start": 148.0, "end": 152.0, "text": " what you want is to make sure that you have some kind of secret source,", "tokens": [51014, 437, 291, 528, 307, 281, 652, 988, 300, 291, 362, 512, 733, 295, 4054, 4009, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 49, "seek": 13500, "start": 152.0, "end": 155.0, "text": " or key element, when it comes to real-time stream processing.", "tokens": [51214, 420, 2141, 4478, 11, 562, 309, 1487, 281, 957, 12, 3766, 4309, 9007, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 50, "seek": 13500, "start": 155.0, "end": 157.0, "text": " So, I've seen it so many times.", "tokens": [51364, 407, 11, 286, 600, 1612, 309, 370, 867, 1413, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 51, "seek": 13500, "start": 157.0, "end": 160.0, "text": " People approach it from the wrong angle.", "tokens": [51464, 3432, 3109, 309, 490, 264, 2085, 5802, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 52, "seek": 13500, "start": 160.0, "end": 164.0, "text": " So, they try, basically, to read the data in real-time,", "tokens": [51614, 407, 11, 436, 853, 11, 1936, 11, 281, 1401, 264, 1412, 294, 957, 12, 3766, 11, 51814], "temperature": 0.0, "avg_logprob": -0.08081099436833308, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.005244710016995668}, {"id": 53, "seek": 16400, "start": 164.0, "end": 168.0, "text": " and they try, basically, to provide some kind of meaning of this data.", "tokens": [50364, 293, 436, 853, 11, 1936, 11, 281, 2893, 512, 733, 295, 3620, 295, 341, 1412, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 54, "seek": 16400, "start": 168.0, "end": 170.0, "text": " So, I'll give you a demo today for logs,", "tokens": [50564, 407, 11, 286, 603, 976, 291, 257, 10723, 965, 337, 20820, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 55, "seek": 16400, "start": 170.0, "end": 173.0, "text": " so this should be easy to follow.", "tokens": [50664, 370, 341, 820, 312, 1858, 281, 1524, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 56, "seek": 16400, "start": 173.0, "end": 177.0, "text": " But the secret source here is to kind of combine new data,", "tokens": [50814, 583, 264, 4054, 4009, 510, 307, 281, 733, 295, 10432, 777, 1412, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 57, "seek": 16400, "start": 177.0, "end": 179.0, "text": " real-time data with historical data.", "tokens": [51014, 957, 12, 3766, 1412, 365, 8584, 1412, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 58, "seek": 16400, "start": 179.0, "end": 183.0, "text": " So, what we mean by real-time data is this data is coming this moment,", "tokens": [51114, 407, 11, 437, 321, 914, 538, 957, 12, 3766, 1412, 307, 341, 1412, 307, 1348, 341, 1623, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 59, "seek": 16400, "start": 183.0, "end": 185.0, "text": " and you read it in this moment.", "tokens": [51314, 293, 291, 1401, 309, 294, 341, 1623, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 60, "seek": 16400, "start": 185.0, "end": 188.0, "text": " Obviously, you want to make sense out of it.", "tokens": [51414, 7580, 11, 291, 528, 281, 652, 2020, 484, 295, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 61, "seek": 16400, "start": 188.0, "end": 190.0, "text": " You want to understand what's going on here.", "tokens": [51564, 509, 528, 281, 1223, 437, 311, 516, 322, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09239179857315556, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0009680832154117525}, {"id": 62, "seek": 19000, "start": 190.0, "end": 193.0, "text": " And the historical data is normal data.", "tokens": [50364, 400, 264, 8584, 1412, 307, 2710, 1412, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 63, "seek": 19000, "start": 193.0, "end": 196.0, "text": " We know about, like, stored somewhere on physical drive,", "tokens": [50514, 492, 458, 466, 11, 411, 11, 12187, 4079, 322, 4001, 3332, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 64, "seek": 19000, "start": 196.0, "end": 198.0, "text": " for example, or database, whatever.", "tokens": [50664, 337, 1365, 11, 420, 8149, 11, 2035, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 65, "seek": 19000, "start": 198.0, "end": 200.0, "text": " So, you want to make sure, basically,", "tokens": [50764, 407, 11, 291, 528, 281, 652, 988, 11, 1936, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 66, "seek": 19000, "start": 200.0, "end": 203.0, "text": " to have these two types of data at the same speed.", "tokens": [50864, 281, 362, 613, 732, 3467, 295, 1412, 412, 264, 912, 3073, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 67, "seek": 19000, "start": 203.0, "end": 205.0, "text": " So, now we're talking about two types of data,", "tokens": [51014, 407, 11, 586, 321, 434, 1417, 466, 732, 3467, 295, 1412, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 68, "seek": 19000, "start": 205.0, "end": 208.0, "text": " but how many, you know, is too many, basically?", "tokens": [51114, 457, 577, 867, 11, 291, 458, 11, 307, 886, 867, 11, 1936, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 69, "seek": 19000, "start": 208.0, "end": 211.0, "text": " What size we're talking about here?", "tokens": [51264, 708, 2744, 321, 434, 1417, 466, 510, 30, 51414], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 70, "seek": 19000, "start": 211.0, "end": 213.0, "text": " So, for some might be, like, a few thousand,", "tokens": [51414, 407, 11, 337, 512, 1062, 312, 11, 411, 11, 257, 1326, 4714, 11, 51514], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 71, "seek": 19000, "start": 213.0, "end": 216.0, "text": " others might be millions, others might be billions.", "tokens": [51514, 2357, 1062, 312, 6803, 11, 2357, 1062, 312, 17375, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 72, "seek": 19000, "start": 216.0, "end": 219.0, "text": " So, essentially, what we're trying to do here", "tokens": [51664, 407, 11, 4476, 11, 437, 321, 434, 1382, 281, 360, 510, 51814], "temperature": 0.0, "avg_logprob": -0.09804265718933539, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.0012668498093262315}, {"id": 73, "seek": 21900, "start": 219.0, "end": 222.0, "text": " is not taking, like, a small data set and trying to process it,", "tokens": [50364, 307, 406, 1940, 11, 411, 11, 257, 1359, 1412, 992, 293, 1382, 281, 1399, 309, 11, 50514], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 74, "seek": 21900, "start": 222.0, "end": 224.0, "text": " because that's, you know, easy to do.", "tokens": [50514, 570, 300, 311, 11, 291, 458, 11, 1858, 281, 360, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 75, "seek": 21900, "start": 224.0, "end": 226.0, "text": " But we're talking about, like, over a billion,", "tokens": [50614, 583, 321, 434, 1417, 466, 11, 411, 11, 670, 257, 5218, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 76, "seek": 21900, "start": 226.0, "end": 230.0, "text": " or over, like, ten billion of seconds in transactions per second.", "tokens": [50714, 420, 670, 11, 411, 11, 2064, 5218, 295, 3949, 294, 16856, 680, 1150, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 77, "seek": 21900, "start": 230.0, "end": 234.0, "text": " So, the idea here is to take a huge amount of data", "tokens": [50914, 407, 11, 264, 1558, 510, 307, 281, 747, 257, 2603, 2372, 295, 1412, 51114], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 78, "seek": 21900, "start": 234.0, "end": 237.0, "text": " and, you know, trying to find some kind of trains", "tokens": [51114, 293, 11, 291, 458, 11, 1382, 281, 915, 512, 733, 295, 16329, 51264], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 79, "seek": 21900, "start": 237.0, "end": 239.0, "text": " and alerts from this data.", "tokens": [51264, 293, 28061, 490, 341, 1412, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 80, "seek": 21900, "start": 239.0, "end": 241.0, "text": " So, I'll give you an example here,", "tokens": [51364, 407, 11, 286, 603, 976, 291, 364, 1365, 510, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 81, "seek": 21900, "start": 241.0, "end": 244.0, "text": " so you can start now to work on what's going on here.", "tokens": [51464, 370, 291, 393, 722, 586, 281, 589, 322, 437, 311, 516, 322, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 82, "seek": 21900, "start": 244.0, "end": 247.0, "text": " So, imagine, basically, you write a Java program,", "tokens": [51614, 407, 11, 3811, 11, 1936, 11, 291, 2464, 257, 10745, 1461, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11702659768117986, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0017278045415878296}, {"id": 83, "seek": 24700, "start": 247.0, "end": 251.0, "text": " and you obviously have some kind of logging mechanism", "tokens": [50364, 293, 291, 2745, 362, 512, 733, 295, 27991, 7513, 50564], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 84, "seek": 24700, "start": 251.0, "end": 252.0, "text": " in your application.", "tokens": [50564, 294, 428, 3861, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 85, "seek": 24700, "start": 252.0, "end": 256.0, "text": " So, in order to understand what's going on with your log system,", "tokens": [50614, 407, 11, 294, 1668, 281, 1223, 437, 311, 516, 322, 365, 428, 3565, 1185, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 86, "seek": 24700, "start": 256.0, "end": 260.0, "text": " essentially, what you want is to have some kind of platform", "tokens": [50814, 4476, 11, 437, 291, 528, 307, 281, 362, 512, 733, 295, 3663, 51014], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 87, "seek": 24700, "start": 260.0, "end": 263.0, "text": " to allow you to actually, you know, analyze it,", "tokens": [51014, 281, 2089, 291, 281, 767, 11, 291, 458, 11, 12477, 309, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 88, "seek": 24700, "start": 263.0, "end": 265.0, "text": " but also, like, at the same time,", "tokens": [51164, 457, 611, 11, 411, 11, 412, 264, 912, 565, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 89, "seek": 24700, "start": 265.0, "end": 267.0, "text": " we're not talking about logs from yesterday.", "tokens": [51264, 321, 434, 406, 1417, 466, 20820, 490, 5186, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 90, "seek": 24700, "start": 267.0, "end": 269.0, "text": " So, for example, events happened yesterday,", "tokens": [51364, 407, 11, 337, 1365, 11, 3931, 2011, 5186, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 91, "seek": 24700, "start": 269.0, "end": 272.0, "text": " you want to make sure that, for example,", "tokens": [51464, 291, 528, 281, 652, 988, 300, 11, 337, 1365, 11, 51614], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 92, "seek": 24700, "start": 272.0, "end": 275.0, "text": " you actually do alerts or trains in the same moment.", "tokens": [51614, 291, 767, 360, 28061, 420, 16329, 294, 264, 912, 1623, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07517199787667127, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.001979220425710082}, {"id": 93, "seek": 27500, "start": 275.0, "end": 278.0, "text": " So, same thing for trains as well.", "tokens": [50364, 407, 11, 912, 551, 337, 16329, 382, 731, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 94, "seek": 27500, "start": 278.0, "end": 281.0, "text": " So, if you want to know if your application", "tokens": [50514, 407, 11, 498, 291, 528, 281, 458, 498, 428, 3861, 50664], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 95, "seek": 27500, "start": 281.0, "end": 283.0, "text": " is going to crash or not,", "tokens": [50664, 307, 516, 281, 8252, 420, 406, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 96, "seek": 27500, "start": 283.0, "end": 287.0, "text": " what you want is to kind of have a platform or solution", "tokens": [50764, 437, 291, 528, 307, 281, 733, 295, 362, 257, 3663, 420, 3827, 50964], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 97, "seek": 27500, "start": 287.0, "end": 291.0, "text": " where it is easy for you to actually look at the data", "tokens": [50964, 689, 309, 307, 1858, 337, 291, 281, 767, 574, 412, 264, 1412, 51164], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 98, "seek": 27500, "start": 291.0, "end": 294.0, "text": " in this moment and say, hey, something is going wrong here.", "tokens": [51164, 294, 341, 1623, 293, 584, 11, 4177, 11, 746, 307, 516, 2085, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 99, "seek": 27500, "start": 294.0, "end": 298.0, "text": " I need to basically define trains out of it", "tokens": [51314, 286, 643, 281, 1936, 6964, 16329, 484, 295, 309, 51514], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 100, "seek": 27500, "start": 298.0, "end": 300.0, "text": " and do some alerts.", "tokens": [51514, 293, 360, 512, 28061, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 101, "seek": 27500, "start": 300.0, "end": 303.0, "text": " Now, for manual work, this is kind of like painful", "tokens": [51614, 823, 11, 337, 9688, 589, 11, 341, 307, 733, 295, 411, 11697, 51764], "temperature": 0.0, "avg_logprob": -0.10181707035411489, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.011726992204785347}, {"id": 102, "seek": 30300, "start": 303.0, "end": 306.0, "text": " because you need to go through loops, for example,", "tokens": [50364, 570, 291, 643, 281, 352, 807, 16121, 11, 337, 1365, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 103, "seek": 30300, "start": 306.0, "end": 309.0, "text": " and you want to make sure that you know how to scale it", "tokens": [50514, 293, 291, 528, 281, 652, 988, 300, 291, 458, 577, 281, 4373, 309, 50664], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 104, "seek": 30300, "start": 309.0, "end": 311.0, "text": " and also kind of like, you know,", "tokens": [50664, 293, 611, 733, 295, 411, 11, 291, 458, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 105, "seek": 30300, "start": 311.0, "end": 314.0, "text": " knowing exactly where your data is stored", "tokens": [50764, 5276, 2293, 689, 428, 1412, 307, 12187, 50914], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 106, "seek": 30300, "start": 314.0, "end": 317.0, "text": " because your enemy, when it comes to real-time scene processing,", "tokens": [50914, 570, 428, 5945, 11, 562, 309, 1487, 281, 957, 12, 3766, 4145, 9007, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 107, "seek": 30300, "start": 317.0, "end": 318.0, "text": " is latency.", "tokens": [51064, 307, 27043, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 108, "seek": 30300, "start": 318.0, "end": 322.0, "text": " So, you want to make sure your application is as low", "tokens": [51114, 407, 11, 291, 528, 281, 652, 988, 428, 3861, 307, 382, 2295, 51314], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 109, "seek": 30300, "start": 322.0, "end": 326.0, "text": " as latency when it comes to delay, basically.", "tokens": [51314, 382, 27043, 562, 309, 1487, 281, 8577, 11, 1936, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 110, "seek": 30300, "start": 326.0, "end": 328.0, "text": " Obviously, the scaling is bottleneck.", "tokens": [51514, 7580, 11, 264, 21589, 307, 44641, 547, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 111, "seek": 30300, "start": 328.0, "end": 330.0, "text": " And now, if you look at platforms,", "tokens": [51614, 400, 586, 11, 498, 291, 574, 412, 9473, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1174559916480113, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.003429575590416789}, {"id": 112, "seek": 33000, "start": 330.0, "end": 333.0, "text": " now, you might have heard of some of these.", "tokens": [50364, 586, 11, 291, 1062, 362, 2198, 295, 512, 295, 613, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 113, "seek": 33000, "start": 333.0, "end": 336.0, "text": " So, the easiest way is to split these platforms", "tokens": [50514, 407, 11, 264, 12889, 636, 307, 281, 7472, 613, 9473, 50664], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 114, "seek": 33000, "start": 336.0, "end": 338.0, "text": " into various categories.", "tokens": [50664, 666, 3683, 10479, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 115, "seek": 33000, "start": 338.0, "end": 342.0, "text": " So, on this one, here, you can see,", "tokens": [50764, 407, 11, 322, 341, 472, 11, 510, 11, 291, 393, 536, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 116, "seek": 33000, "start": 342.0, "end": 344.0, "text": " you can have open source solutions", "tokens": [50964, 291, 393, 362, 1269, 4009, 6547, 51064], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 117, "seek": 33000, "start": 344.0, "end": 346.0, "text": " or you can have hybrid,", "tokens": [51064, 420, 291, 393, 362, 13051, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 118, "seek": 33000, "start": 346.0, "end": 350.0, "text": " which is mixed between open source and the managed service.", "tokens": [51164, 597, 307, 7467, 1296, 1269, 4009, 293, 264, 6453, 2643, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 119, "seek": 33000, "start": 350.0, "end": 354.0, "text": " And on the horizontal, as you can see,", "tokens": [51364, 400, 322, 264, 12750, 11, 382, 291, 393, 536, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 120, "seek": 33000, "start": 354.0, "end": 356.0, "text": " you need to capture your data.", "tokens": [51564, 291, 643, 281, 7983, 428, 1412, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 121, "seek": 33000, "start": 356.0, "end": 359.0, "text": " Obviously, in real-time, you need to do some kind of transport", "tokens": [51664, 7580, 11, 294, 957, 12, 3766, 11, 291, 643, 281, 360, 512, 733, 295, 5495, 51814], "temperature": 0.0, "avg_logprob": -0.11837880653247498, "compression_ratio": 1.748917748917749, "no_speech_prob": 0.0035624930169433355}, {"id": 122, "seek": 35900, "start": 359.0, "end": 361.0, "text": " as well as some kind of transformation", "tokens": [50364, 382, 731, 382, 512, 733, 295, 9887, 50464], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 123, "seek": 35900, "start": 361.0, "end": 363.0, "text": " and processing as well.", "tokens": [50464, 293, 9007, 382, 731, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 124, "seek": 35900, "start": 363.0, "end": 366.0, "text": " So, you can split it into 12 squares", "tokens": [50564, 407, 11, 291, 393, 7472, 309, 666, 2272, 19368, 50714], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 125, "seek": 35900, "start": 366.0, "end": 368.0, "text": " and it becomes like, you know,", "tokens": [50714, 293, 309, 3643, 411, 11, 291, 458, 11, 50814], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 126, "seek": 35900, "start": 368.0, "end": 371.0, "text": " easier to understand which tool you need to use.", "tokens": [50814, 3571, 281, 1223, 597, 2290, 291, 643, 281, 764, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 127, "seek": 35900, "start": 371.0, "end": 373.0, "text": " But obviously, this is still a bit complex", "tokens": [50964, 583, 2745, 11, 341, 307, 920, 257, 857, 3997, 51064], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 128, "seek": 35900, "start": 373.0, "end": 376.0, "text": " because the area for real-time scene processing", "tokens": [51064, 570, 264, 1859, 337, 957, 12, 3766, 4145, 9007, 51214], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 129, "seek": 35900, "start": 376.0, "end": 379.0, "text": " is mainly about two different subjects.", "tokens": [51214, 307, 8704, 466, 732, 819, 13066, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 130, "seek": 35900, "start": 379.0, "end": 382.0, "text": " So, it's not only, you know, capture or transport", "tokens": [51364, 407, 11, 309, 311, 406, 787, 11, 291, 458, 11, 7983, 420, 5495, 51514], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 131, "seek": 35900, "start": 382.0, "end": 385.0, "text": " because you have to do all of these at the same time.", "tokens": [51514, 570, 291, 362, 281, 360, 439, 295, 613, 412, 264, 912, 565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 132, "seek": 35900, "start": 385.0, "end": 387.0, "text": " It's kind of like, if you want,", "tokens": [51664, 467, 311, 733, 295, 411, 11, 498, 291, 528, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06710386276245117, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0008402297971770167}, {"id": 133, "seek": 38700, "start": 387.0, "end": 389.0, "text": " you need basically to decide if you're going to use", "tokens": [50364, 291, 643, 1936, 281, 4536, 498, 291, 434, 516, 281, 764, 50464], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 134, "seek": 38700, "start": 389.0, "end": 392.0, "text": " swim processing engines from one side", "tokens": [50464, 7110, 9007, 12982, 490, 472, 1252, 50614], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 135, "seek": 38700, "start": 392.0, "end": 394.0, "text": " or you want to have some kind of fast data storage", "tokens": [50614, 420, 291, 528, 281, 362, 512, 733, 295, 2370, 1412, 6725, 50714], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 136, "seek": 38700, "start": 394.0, "end": 396.0, "text": " from the other side.", "tokens": [50714, 490, 264, 661, 1252, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 137, "seek": 38700, "start": 396.0, "end": 398.0, "text": " So, swim processing engines are pretty good", "tokens": [50814, 407, 11, 7110, 9007, 12982, 366, 1238, 665, 50914], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 138, "seek": 38700, "start": 398.0, "end": 401.0, "text": " in handling data coming in real-time,", "tokens": [50914, 294, 13175, 1412, 1348, 294, 957, 12, 3766, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 139, "seek": 38700, "start": 401.0, "end": 405.0, "text": " which is like, for example, Kafka or TX equals and so on.", "tokens": [51064, 597, 307, 411, 11, 337, 1365, 11, 47064, 420, 314, 55, 6915, 293, 370, 322, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 140, "seek": 38700, "start": 405.0, "end": 408.0, "text": " Or from the far right side, you can see fast data storage,", "tokens": [51264, 1610, 490, 264, 1400, 558, 1252, 11, 291, 393, 536, 2370, 1412, 6725, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 141, "seek": 38700, "start": 408.0, "end": 411.0, "text": " which is kind of like essentially caching solutions", "tokens": [51414, 597, 307, 733, 295, 411, 4476, 269, 2834, 6547, 51564], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 142, "seek": 38700, "start": 411.0, "end": 413.0, "text": " to your application.", "tokens": [51564, 281, 428, 3861, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 143, "seek": 38700, "start": 413.0, "end": 415.0, "text": " So, for example, MongoDB, Redis and so on.", "tokens": [51664, 407, 11, 337, 1365, 11, 48380, 27735, 11, 4477, 271, 293, 370, 322, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12968195849702557, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0020601367577910423}, {"id": 144, "seek": 41500, "start": 415.0, "end": 417.0, "text": " So, if you want to apply this solution,", "tokens": [50364, 407, 11, 498, 291, 528, 281, 3079, 341, 3827, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 145, "seek": 41500, "start": 417.0, "end": 420.0, "text": " essentially you need one tool from the left", "tokens": [50464, 4476, 291, 643, 472, 2290, 490, 264, 1411, 50614], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 146, "seek": 41500, "start": 420.0, "end": 422.0, "text": " and one tool from the right,", "tokens": [50614, 293, 472, 2290, 490, 264, 558, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 147, "seek": 41500, "start": 422.0, "end": 424.0, "text": " which means that it adds more work on your side.", "tokens": [50714, 597, 1355, 300, 309, 10860, 544, 589, 322, 428, 1252, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 148, "seek": 41500, "start": 424.0, "end": 427.0, "text": " So, what you want is kind of looking at it in this way", "tokens": [50814, 407, 11, 437, 291, 528, 307, 733, 295, 1237, 412, 309, 294, 341, 636, 50964], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 149, "seek": 41500, "start": 427.0, "end": 430.0, "text": " and say, hey, I want one solution for you,", "tokens": [50964, 293, 584, 11, 4177, 11, 286, 528, 472, 3827, 337, 291, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 150, "seek": 41500, "start": 430.0, "end": 433.0, "text": " and that's where Hazercast comes into place.", "tokens": [51114, 293, 300, 311, 689, 15852, 260, 3734, 1487, 666, 1081, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 151, "seek": 41500, "start": 433.0, "end": 435.0, "text": " Obviously, I work for Hazercast and the Hazercast", "tokens": [51264, 7580, 11, 286, 589, 337, 15852, 260, 3734, 293, 264, 15852, 260, 3734, 51364], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 152, "seek": 41500, "start": 435.0, "end": 439.0, "text": " as itself is built on top of the Java virtual machine.", "tokens": [51364, 382, 2564, 307, 3094, 322, 1192, 295, 264, 10745, 6374, 3479, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 153, "seek": 41500, "start": 439.0, "end": 441.0, "text": " So, it's Java based and it's open source.", "tokens": [51564, 407, 11, 309, 311, 10745, 2361, 293, 309, 311, 1269, 4009, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 154, "seek": 41500, "start": 441.0, "end": 444.0, "text": " So, this is the platform here.", "tokens": [51664, 407, 11, 341, 307, 264, 3663, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11587754101820395, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0012890510261058807}, {"id": 155, "seek": 44400, "start": 445.0, "end": 449.0, "text": " It's kind of like a A to Z solution.", "tokens": [50414, 467, 311, 733, 295, 411, 257, 316, 281, 1176, 3827, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 156, "seek": 44400, "start": 449.0, "end": 452.0, "text": " So, what you want is to catch your data,", "tokens": [50614, 407, 11, 437, 291, 528, 307, 281, 3745, 428, 1412, 11, 50764], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 157, "seek": 44400, "start": 452.0, "end": 454.0, "text": " capture your data.", "tokens": [50764, 7983, 428, 1412, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 158, "seek": 44400, "start": 454.0, "end": 456.0, "text": " It could be coming from Apache, for example,", "tokens": [50864, 467, 727, 312, 1348, 490, 46597, 11, 337, 1365, 11, 50964], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 159, "seek": 44400, "start": 456.0, "end": 459.0, "text": " Apache Kafka and from IoT devices.", "tokens": [50964, 46597, 47064, 293, 490, 30112, 5759, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 160, "seek": 44400, "start": 459.0, "end": 461.0, "text": " It could be coming from some kind of custom connectors", "tokens": [51114, 467, 727, 312, 1348, 490, 512, 733, 295, 2375, 31865, 51214], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 161, "seek": 44400, "start": 461.0, "end": 463.0, "text": " because it's open source,", "tokens": [51214, 570, 309, 311, 1269, 4009, 11, 51314], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 162, "seek": 44400, "start": 463.0, "end": 465.0, "text": " which means feel free to contribute to this project", "tokens": [51314, 597, 1355, 841, 1737, 281, 10586, 281, 341, 1716, 51414], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 163, "seek": 44400, "start": 465.0, "end": 468.0, "text": " or if it comes from file watch up,", "tokens": [51414, 420, 498, 309, 1487, 490, 3991, 1159, 493, 11, 51564], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 164, "seek": 44400, "start": 468.0, "end": 470.0, "text": " for example, or from work suffix.", "tokens": [51564, 337, 1365, 11, 420, 490, 589, 3889, 970, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 165, "seek": 44400, "start": 470.0, "end": 472.0, "text": " So, once you have this data,", "tokens": [51664, 407, 11, 1564, 291, 362, 341, 1412, 11, 51764], "temperature": 0.0, "avg_logprob": -0.19258179098872816, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.0014316352317109704}, {"id": 166, "seek": 47200, "start": 472.0, "end": 475.0, "text": " free time data ingested into the platform,", "tokens": [50364, 1737, 565, 1412, 3957, 21885, 666, 264, 3663, 11, 50514], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 167, "seek": 47200, "start": 475.0, "end": 478.0, "text": " platform itself has two main components.", "tokens": [50514, 3663, 2564, 575, 732, 2135, 6677, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 168, "seek": 47200, "start": 478.0, "end": 480.0, "text": " So, the first one is the jet engine.", "tokens": [50664, 407, 11, 264, 700, 472, 307, 264, 14452, 2848, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 169, "seek": 47200, "start": 480.0, "end": 482.0, "text": " So, this is the engine for scene processing", "tokens": [50764, 407, 11, 341, 307, 264, 2848, 337, 4145, 9007, 50864], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 170, "seek": 47200, "start": 482.0, "end": 485.0, "text": " and the demo I will show you how to use it", "tokens": [50864, 293, 264, 10723, 286, 486, 855, 291, 577, 281, 764, 309, 51014], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 171, "seek": 47200, "start": 485.0, "end": 488.0, "text": " and also the fast data storage or fast data management.", "tokens": [51014, 293, 611, 264, 2370, 1412, 6725, 420, 2370, 1412, 4592, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 172, "seek": 47200, "start": 488.0, "end": 490.0, "text": " So, this is essentially a component", "tokens": [51164, 407, 11, 341, 307, 4476, 257, 6542, 51264], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 173, "seek": 47200, "start": 490.0, "end": 493.0, "text": " which allows you to load your data from external sources", "tokens": [51264, 597, 4045, 291, 281, 3677, 428, 1412, 490, 8320, 7139, 51414], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 174, "seek": 47200, "start": 493.0, "end": 496.0, "text": " and it's optional, obviously.", "tokens": [51414, 293, 309, 311, 17312, 11, 2745, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 175, "seek": 47200, "start": 496.0, "end": 498.0, "text": " So, it's kind of like, I don't know,", "tokens": [51564, 407, 11, 309, 311, 733, 295, 411, 11, 286, 500, 380, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 176, "seek": 47200, "start": 498.0, "end": 501.0, "text": " some kind of file system or database or stored on the cloud", "tokens": [51664, 512, 733, 295, 3991, 1185, 420, 8149, 420, 12187, 322, 264, 4588, 51814], "temperature": 0.0, "avg_logprob": -0.13273526705228367, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0009949041996151209}, {"id": 177, "seek": 50100, "start": 501.0, "end": 503.0, "text": " and you load it into memory.", "tokens": [50364, 293, 291, 3677, 309, 666, 4675, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 178, "seek": 50100, "start": 503.0, "end": 505.0, "text": " Why do you need to load it into memory?", "tokens": [50464, 1545, 360, 291, 643, 281, 3677, 309, 666, 4675, 30, 50564], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 179, "seek": 50100, "start": 505.0, "end": 507.0, "text": " Simply because you want to make sure", "tokens": [50564, 19596, 570, 291, 528, 281, 652, 988, 50664], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 180, "seek": 50100, "start": 507.0, "end": 509.0, "text": " your application is as fast as possible.", "tokens": [50664, 428, 3861, 307, 382, 2370, 382, 1944, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 181, "seek": 50100, "start": 509.0, "end": 511.0, "text": " So, we're talking about, for example,", "tokens": [50764, 407, 11, 321, 434, 1417, 466, 11, 337, 1365, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 182, "seek": 50100, "start": 511.0, "end": 514.0, "text": " here speed where it's sub milliseconds", "tokens": [50864, 510, 3073, 689, 309, 311, 1422, 34184, 51014], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 183, "seek": 50100, "start": 514.0, "end": 515.0, "text": " or fractions of seconds.", "tokens": [51014, 420, 36058, 295, 3949, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 184, "seek": 50100, "start": 515.0, "end": 517.0, "text": " So, this is very important.", "tokens": [51064, 407, 11, 341, 307, 588, 1021, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 185, "seek": 50100, "start": 517.0, "end": 520.0, "text": " For example, in fraud detection scenario,", "tokens": [51164, 1171, 1365, 11, 294, 14560, 17784, 9005, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 186, "seek": 50100, "start": 520.0, "end": 522.0, "text": " if you, for example, you're using your cards", "tokens": [51314, 498, 291, 11, 337, 1365, 11, 291, 434, 1228, 428, 5632, 51414], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 187, "seek": 50100, "start": 522.0, "end": 525.0, "text": " and someone else is using your card somewhere else,", "tokens": [51414, 293, 1580, 1646, 307, 1228, 428, 2920, 4079, 1646, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 188, "seek": 50100, "start": 525.0, "end": 528.0, "text": " you want to get alert in this specific moment.", "tokens": [51564, 291, 528, 281, 483, 9615, 294, 341, 2685, 1623, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 189, "seek": 50100, "start": 528.0, "end": 530.0, "text": " It doesn't make sense to get alert,", "tokens": [51714, 467, 1177, 380, 652, 2020, 281, 483, 9615, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09281399302238966, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.012286926619708538}, {"id": 190, "seek": 53000, "start": 530.0, "end": 533.0, "text": " I don't know, in the afternoon, for example, or next day.", "tokens": [50364, 286, 500, 380, 458, 11, 294, 264, 6499, 11, 337, 1365, 11, 420, 958, 786, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 191, "seek": 53000, "start": 533.0, "end": 536.0, "text": " So, this type of machine critical solutions,", "tokens": [50514, 407, 11, 341, 2010, 295, 3479, 4924, 6547, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 192, "seek": 53000, "start": 536.0, "end": 539.0, "text": " what you want is to make sure that your data is stored", "tokens": [50664, 437, 291, 528, 307, 281, 652, 988, 300, 428, 1412, 307, 12187, 50814], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 193, "seek": 53000, "start": 539.0, "end": 543.0, "text": " in memory which allows you to access it in really good time.", "tokens": [50814, 294, 4675, 597, 4045, 291, 281, 2105, 309, 294, 534, 665, 565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 194, "seek": 53000, "start": 543.0, "end": 545.0, "text": " So, once you have this data,", "tokens": [51014, 407, 11, 1564, 291, 362, 341, 1412, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 195, "seek": 53000, "start": 545.0, "end": 547.0, "text": " you can do some kind of transformation for your,", "tokens": [51114, 291, 393, 360, 512, 733, 295, 9887, 337, 428, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 196, "seek": 53000, "start": 547.0, "end": 549.0, "text": " for example, on data because remember,", "tokens": [51214, 337, 1365, 11, 322, 1412, 570, 1604, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 197, "seek": 53000, "start": 549.0, "end": 551.0, "text": " we're not talking about one single data source.", "tokens": [51314, 321, 434, 406, 1417, 466, 472, 2167, 1412, 4009, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 198, "seek": 53000, "start": 551.0, "end": 554.0, "text": " For scene processing, we're talking about multiple sources.", "tokens": [51414, 1171, 4145, 9007, 11, 321, 434, 1417, 466, 3866, 7139, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 199, "seek": 53000, "start": 554.0, "end": 556.0, "text": " So, it could be, for example, I don't know,", "tokens": [51564, 407, 11, 309, 727, 312, 11, 337, 1365, 11, 286, 500, 380, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 200, "seek": 53000, "start": 556.0, "end": 558.0, "text": " some transactions coming in Kafka topic", "tokens": [51664, 512, 16856, 1348, 294, 47064, 4829, 51764], "temperature": 0.0, "avg_logprob": -0.10026202641480358, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.001685730996541679}, {"id": 201, "seek": 55800, "start": 558.0, "end": 560.0, "text": " and some IoT device for,", "tokens": [50364, 293, 512, 30112, 4302, 337, 11, 50464], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 202, "seek": 55800, "start": 560.0, "end": 563.0, "text": " weather forecast for examine coming from other topic,", "tokens": [50464, 5503, 14330, 337, 17496, 1348, 490, 661, 4829, 11, 50614], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 203, "seek": 55800, "start": 563.0, "end": 565.0, "text": " sorry, from other source.", "tokens": [50614, 2597, 11, 490, 661, 4009, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 204, "seek": 55800, "start": 565.0, "end": 568.0, "text": " So, you want to make sure that also you can combine it.", "tokens": [50714, 407, 11, 291, 528, 281, 652, 988, 300, 611, 291, 393, 10432, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 205, "seek": 55800, "start": 568.0, "end": 571.0, "text": " Obviously, because here we're in Java run,", "tokens": [50864, 7580, 11, 570, 510, 321, 434, 294, 10745, 1190, 11, 51014], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 206, "seek": 55800, "start": 571.0, "end": 574.0, "text": " so you can use the Java, obviously, client for it.", "tokens": [51014, 370, 291, 393, 764, 264, 10745, 11, 2745, 11, 6423, 337, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 207, "seek": 55800, "start": 574.0, "end": 578.0, "text": " So, essentially what you need is kind of like a Java jar.", "tokens": [51164, 407, 11, 4476, 437, 291, 643, 307, 733, 295, 411, 257, 10745, 15181, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 208, "seek": 55800, "start": 578.0, "end": 581.0, "text": " You need to download it and plug it into your pump file.", "tokens": [51364, 509, 643, 281, 5484, 309, 293, 5452, 309, 666, 428, 5889, 3991, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 209, "seek": 55800, "start": 581.0, "end": 584.0, "text": " But, for example, if you're a data scientist", "tokens": [51514, 583, 11, 337, 1365, 11, 498, 291, 434, 257, 1412, 12662, 51664], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 210, "seek": 55800, "start": 584.0, "end": 586.0, "text": " and you're not sure, you know, about programming language,", "tokens": [51664, 293, 291, 434, 406, 988, 11, 291, 458, 11, 466, 9410, 2856, 11, 51764], "temperature": 0.0, "avg_logprob": -0.18937927712010974, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.0036569086369127035}, {"id": 211, "seek": 58600, "start": 586.0, "end": 588.0, "text": " maybe you use a little bit of Python,", "tokens": [50364, 1310, 291, 764, 257, 707, 857, 295, 15329, 11, 50464], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 212, "seek": 58600, "start": 588.0, "end": 590.0, "text": " but, you know, programming languages is not", "tokens": [50464, 457, 11, 291, 458, 11, 9410, 8650, 307, 406, 50564], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 213, "seek": 58600, "start": 590.0, "end": 592.0, "text": " something you want to invest in.", "tokens": [50564, 746, 291, 528, 281, 1963, 294, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 214, "seek": 58600, "start": 592.0, "end": 594.0, "text": " What you can do is do some,", "tokens": [50664, 708, 291, 393, 360, 307, 360, 512, 11, 50764], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 215, "seek": 58600, "start": 594.0, "end": 596.0, "text": " everything I mentioned today in SQL.", "tokens": [50764, 1203, 286, 2835, 965, 294, 19200, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 216, "seek": 58600, "start": 596.0, "end": 599.0, "text": " So, which means you do everything for team,", "tokens": [50864, 407, 11, 597, 1355, 291, 360, 1203, 337, 1469, 11, 51014], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 217, "seek": 58600, "start": 599.0, "end": 601.0, "text": " written scene processing in terms of alerts,", "tokens": [51014, 3720, 4145, 9007, 294, 2115, 295, 28061, 11, 51114], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 218, "seek": 58600, "start": 601.0, "end": 603.0, "text": " for example, or defining trace,", "tokens": [51114, 337, 1365, 11, 420, 17827, 13508, 11, 51214], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 219, "seek": 58600, "start": 603.0, "end": 606.0, "text": " or even query your data using SQL.", "tokens": [51214, 420, 754, 14581, 428, 1412, 1228, 19200, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 220, "seek": 58600, "start": 606.0, "end": 608.0, "text": " So, once you do this process,", "tokens": [51364, 407, 11, 1564, 291, 360, 341, 1399, 11, 51464], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 221, "seek": 58600, "start": 608.0, "end": 610.0, "text": " you actually can output it in some kind of,", "tokens": [51464, 291, 767, 393, 5598, 309, 294, 512, 733, 295, 11, 51564], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 222, "seek": 58600, "start": 610.0, "end": 613.0, "text": " I don't know, same thing for your input.", "tokens": [51564, 286, 500, 380, 458, 11, 912, 551, 337, 428, 4846, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 223, "seek": 58600, "start": 613.0, "end": 615.0, "text": " So, the Kafka topic, for example,", "tokens": [51714, 407, 11, 264, 47064, 4829, 11, 337, 1365, 11, 51814], "temperature": 0.0, "avg_logprob": -0.14162760672809407, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.006712359841912985}, {"id": 224, "seek": 61500, "start": 615.0, "end": 617.0, "text": " you can use the WebSocket, or you can create your", "tokens": [50364, 291, 393, 764, 264, 9573, 50, 31380, 11, 420, 291, 393, 1884, 428, 50464], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 225, "seek": 61500, "start": 617.0, "end": 620.0, "text": " Java application to do some kind of visualization", "tokens": [50464, 10745, 3861, 281, 360, 512, 733, 295, 25801, 50614], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 226, "seek": 61500, "start": 620.0, "end": 622.0, "text": " for predictions, for example.", "tokens": [50614, 337, 21264, 11, 337, 1365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 227, "seek": 61500, "start": 622.0, "end": 625.0, "text": " Now, the cool thing about Hazelkast is not only", "tokens": [50714, 823, 11, 264, 1627, 551, 466, 15852, 338, 74, 525, 307, 406, 787, 50864], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 228, "seek": 61500, "start": 625.0, "end": 628.0, "text": " the platform and the easiness of use,", "tokens": [50864, 264, 3663, 293, 264, 1195, 1324, 295, 764, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 229, "seek": 61500, "start": 628.0, "end": 629.0, "text": " but also how to scale.", "tokens": [51014, 457, 611, 577, 281, 4373, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 230, "seek": 61500, "start": 629.0, "end": 631.0, "text": " Remember what we're talking about here?", "tokens": [51064, 5459, 437, 321, 434, 1417, 466, 510, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 231, "seek": 61500, "start": 631.0, "end": 633.0, "text": " We're not talking about a few thousand.", "tokens": [51164, 492, 434, 406, 1417, 466, 257, 1326, 4714, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 232, "seek": 61500, "start": 633.0, "end": 635.0, "text": " We're talking maybe a few million,", "tokens": [51264, 492, 434, 1417, 1310, 257, 1326, 2459, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 233, "seek": 61500, "start": 635.0, "end": 637.0, "text": " or even billions of transactions.", "tokens": [51364, 420, 754, 17375, 295, 16856, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 234, "seek": 61500, "start": 637.0, "end": 639.0, "text": " So, when it comes to scaling,", "tokens": [51464, 407, 11, 562, 309, 1487, 281, 21589, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 235, "seek": 61500, "start": 639.0, "end": 642.0, "text": " you want to recognize between two different topics.", "tokens": [51564, 291, 528, 281, 5521, 1296, 732, 819, 8378, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1773087701132131, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.003672803984954953}, {"id": 236, "seek": 64200, "start": 642.0, "end": 645.0, "text": " So, for some, scaling might refer to, I don't know,", "tokens": [50364, 407, 11, 337, 512, 11, 21589, 1062, 2864, 281, 11, 286, 500, 380, 458, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 237, "seek": 64200, "start": 645.0, "end": 646.0, "text": " your data.", "tokens": [50514, 428, 1412, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 238, "seek": 64200, "start": 646.0, "end": 648.0, "text": " So, you want to scale this data, for example,", "tokens": [50564, 407, 11, 291, 528, 281, 4373, 341, 1412, 11, 337, 1365, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 239, "seek": 64200, "start": 648.0, "end": 650.0, "text": " and for others who work, for example,", "tokens": [50664, 293, 337, 2357, 567, 589, 11, 337, 1365, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 240, "seek": 64200, "start": 650.0, "end": 652.0, "text": " in programming or development,", "tokens": [50764, 294, 9410, 420, 3250, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 241, "seek": 64200, "start": 652.0, "end": 654.0, "text": " they focus mainly on the compute.", "tokens": [50864, 436, 1879, 8704, 322, 264, 14722, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 242, "seek": 64200, "start": 654.0, "end": 656.0, "text": " So, you want to make sure, basically,", "tokens": [50964, 407, 11, 291, 528, 281, 652, 988, 11, 1936, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 243, "seek": 64200, "start": 656.0, "end": 660.0, "text": " to combine between data and compute when you want to scale.", "tokens": [51064, 281, 10432, 1296, 1412, 293, 14722, 562, 291, 528, 281, 4373, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 244, "seek": 64200, "start": 660.0, "end": 663.0, "text": " And the cool thing about it is it's partition aware,", "tokens": [51264, 400, 264, 1627, 551, 466, 309, 307, 309, 311, 24808, 3650, 11, 51414], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 245, "seek": 64200, "start": 663.0, "end": 667.0, "text": " which means if you have your data stored in multiple places", "tokens": [51414, 597, 1355, 498, 291, 362, 428, 1412, 12187, 294, 3866, 3190, 51614], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 246, "seek": 64200, "start": 667.0, "end": 669.0, "text": " around the world, for example, in different data centers,", "tokens": [51614, 926, 264, 1002, 11, 337, 1365, 11, 294, 819, 1412, 10898, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09487415805007472, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.003551686881110072}, {"id": 247, "seek": 66900, "start": 669.0, "end": 672.0, "text": " what you want is your compute or your process", "tokens": [50364, 437, 291, 528, 307, 428, 14722, 420, 428, 1399, 50514], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 248, "seek": 66900, "start": 672.0, "end": 676.0, "text": " or your application is to be stored as close", "tokens": [50514, 420, 428, 3861, 307, 281, 312, 12187, 382, 1998, 50714], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 249, "seek": 66900, "start": 676.0, "end": 678.0, "text": " as possible to your data.", "tokens": [50714, 382, 1944, 281, 428, 1412, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 250, "seek": 66900, "start": 678.0, "end": 681.0, "text": " So, this will give you some kind of speed", "tokens": [50814, 407, 11, 341, 486, 976, 291, 512, 733, 295, 3073, 50964], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 251, "seek": 66900, "start": 681.0, "end": 685.0, "text": " and lower latency when it comes to transactions.", "tokens": [50964, 293, 3126, 27043, 562, 309, 1487, 281, 16856, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 252, "seek": 66900, "start": 685.0, "end": 687.0, "text": " Now, we talked about transactions,", "tokens": [51164, 823, 11, 321, 2825, 466, 16856, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 253, "seek": 66900, "start": 687.0, "end": 690.0, "text": " but how many we're talking about today?", "tokens": [51264, 457, 577, 867, 321, 434, 1417, 466, 965, 30, 51414], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 254, "seek": 66900, "start": 690.0, "end": 693.0, "text": " So, we've done this kind of like benchmark, obviously.", "tokens": [51414, 407, 11, 321, 600, 1096, 341, 733, 295, 411, 18927, 11, 2745, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 255, "seek": 66900, "start": 693.0, "end": 695.0, "text": " It's bit outdated now.", "tokens": [51564, 467, 311, 857, 36313, 586, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08942725707073601, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.006440606899559498}, {"id": 256, "seek": 69500, "start": 695.0, "end": 699.0, "text": " It's kind of worth trying to add one more zero to it.", "tokens": [50364, 467, 311, 733, 295, 3163, 1382, 281, 909, 472, 544, 4018, 281, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 257, "seek": 69500, "start": 699.0, "end": 702.0, "text": " So, it's one billion transactions per second on 45 nodes.", "tokens": [50564, 407, 11, 309, 311, 472, 5218, 16856, 680, 1150, 322, 6905, 13891, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 258, "seek": 69500, "start": 702.0, "end": 705.0, "text": " And the cool thing about it is not only the latency,", "tokens": [50714, 400, 264, 1627, 551, 466, 309, 307, 406, 787, 264, 27043, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 259, "seek": 69500, "start": 705.0, "end": 707.0, "text": " which is like 30 milliseconds,", "tokens": [50864, 597, 307, 411, 2217, 34184, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 260, "seek": 69500, "start": 707.0, "end": 709.0, "text": " but also the linear scale,", "tokens": [50964, 457, 611, 264, 8213, 4373, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 261, "seek": 69500, "start": 709.0, "end": 712.0, "text": " which means you just need to add more nodes to your application.", "tokens": [51064, 597, 1355, 291, 445, 643, 281, 909, 544, 13891, 281, 428, 3861, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 262, "seek": 69500, "start": 712.0, "end": 714.0, "text": " So, with that being said,", "tokens": [51214, 407, 11, 365, 300, 885, 848, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 263, "seek": 69500, "start": 714.0, "end": 717.0, "text": " so let's just move directly to the demo", "tokens": [51314, 370, 718, 311, 445, 1286, 3838, 281, 264, 10723, 51464], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 264, "seek": 69500, "start": 717.0, "end": 720.0, "text": " and I'll show you how you can use HazardCast", "tokens": [51464, 293, 286, 603, 855, 291, 577, 291, 393, 764, 15852, 515, 34, 525, 51614], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 265, "seek": 69500, "start": 720.0, "end": 722.0, "text": " within your application.", "tokens": [51614, 1951, 428, 3861, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11026714228782333, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.0036901445128023624}, {"id": 266, "seek": 72200, "start": 723.0, "end": 727.0, "text": " So, for this demo, what I wanted is kind of, you know,", "tokens": [50414, 407, 11, 337, 341, 10723, 11, 437, 286, 1415, 307, 733, 295, 11, 291, 458, 11, 50614], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 267, "seek": 72200, "start": 727.0, "end": 729.0, "text": " you're writing your application,", "tokens": [50614, 291, 434, 3579, 428, 3861, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 268, "seek": 72200, "start": 729.0, "end": 731.0, "text": " Java application, obviously,", "tokens": [50714, 10745, 3861, 11, 2745, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 269, "seek": 72200, "start": 731.0, "end": 733.0, "text": " and you have some kind of logging mechanism", "tokens": [50814, 293, 291, 362, 512, 733, 295, 27991, 7513, 50914], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 270, "seek": 72200, "start": 733.0, "end": 735.0, "text": " within your application.", "tokens": [50914, 1951, 428, 3861, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 271, "seek": 72200, "start": 735.0, "end": 737.0, "text": " And your boss comes next day and says,", "tokens": [51014, 400, 428, 5741, 1487, 958, 786, 293, 1619, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 272, "seek": 72200, "start": 737.0, "end": 741.0, "text": " hey, your log messages or your solution,", "tokens": [51114, 4177, 11, 428, 3565, 7897, 420, 428, 3827, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 273, "seek": 72200, "start": 741.0, "end": 743.0, "text": " we need to upgrade it in a way", "tokens": [51314, 321, 643, 281, 11484, 309, 294, 257, 636, 51414], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 274, "seek": 72200, "start": 743.0, "end": 745.0, "text": " where we'll provide some kind of alerts", "tokens": [51414, 689, 321, 603, 2893, 512, 733, 295, 28061, 51514], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 275, "seek": 72200, "start": 745.0, "end": 748.0, "text": " or predict what's going to happen next.", "tokens": [51514, 420, 6069, 437, 311, 516, 281, 1051, 958, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11204330737774189, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.001883082208223641}, {"id": 276, "seek": 74800, "start": 748.0, "end": 751.0, "text": " So, your task, essentially,", "tokens": [50364, 407, 11, 428, 5633, 11, 4476, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 277, "seek": 74800, "start": 751.0, "end": 756.0, "text": " what you want is to kind of take exactly the application", "tokens": [50514, 437, 291, 528, 307, 281, 733, 295, 747, 2293, 264, 3861, 50764], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 278, "seek": 74800, "start": 756.0, "end": 759.0, "text": " and make sure you have some kind of scanning mechanism", "tokens": [50764, 293, 652, 988, 291, 362, 512, 733, 295, 27019, 7513, 50914], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 279, "seek": 74800, "start": 759.0, "end": 762.0, "text": " and real-time screen processing into it.", "tokens": [50914, 293, 957, 12, 3766, 2568, 9007, 666, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 280, "seek": 74800, "start": 762.0, "end": 764.0, "text": " So, obviously, you have two options here.", "tokens": [51064, 407, 11, 2745, 11, 291, 362, 732, 3956, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 281, "seek": 74800, "start": 764.0, "end": 767.0, "text": " So, the first option is to download this jar,", "tokens": [51164, 407, 11, 264, 700, 3614, 307, 281, 5484, 341, 15181, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 282, "seek": 74800, "start": 767.0, "end": 769.0, "text": " plug it into your application and run it.", "tokens": [51314, 5452, 309, 666, 428, 3861, 293, 1190, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 283, "seek": 74800, "start": 769.0, "end": 771.0, "text": " So, that's good.", "tokens": [51414, 407, 11, 300, 311, 665, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 284, "seek": 74800, "start": 771.0, "end": 774.0, "text": " But the problem with this is usually logs stored", "tokens": [51514, 583, 264, 1154, 365, 341, 307, 2673, 20820, 12187, 51664], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 285, "seek": 74800, "start": 774.0, "end": 776.0, "text": " in different places.", "tokens": [51664, 294, 819, 3190, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10889835710878726, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.0020748486276715994}, {"id": 286, "seek": 77600, "start": 776.0, "end": 780.0, "text": " So, what you want is you want every machine", "tokens": [50364, 407, 11, 437, 291, 528, 307, 291, 528, 633, 3479, 50564], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 287, "seek": 77600, "start": 780.0, "end": 783.0, "text": " to send its logs to some, you know, center place.", "tokens": [50564, 281, 2845, 1080, 20820, 281, 512, 11, 291, 458, 11, 3056, 1081, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 288, "seek": 77600, "start": 783.0, "end": 785.0, "text": " So, usually, it's a cloud.", "tokens": [50714, 407, 11, 2673, 11, 309, 311, 257, 4588, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 289, "seek": 77600, "start": 785.0, "end": 788.0, "text": " So, the idea of you sending logs to cloud", "tokens": [50814, 407, 11, 264, 1558, 295, 291, 7750, 20820, 281, 4588, 50964], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 290, "seek": 77600, "start": 788.0, "end": 792.0, "text": " is kind of providing some kind of one place", "tokens": [50964, 307, 733, 295, 6530, 512, 733, 295, 472, 1081, 51164], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 291, "seek": 77600, "start": 792.0, "end": 795.0, "text": " for every single machine which sends logs.", "tokens": [51164, 337, 633, 2167, 3479, 597, 14790, 20820, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 292, "seek": 77600, "start": 795.0, "end": 798.0, "text": " HazardCast runs the HazardCast Viridian,", "tokens": [51314, 15852, 515, 34, 525, 6676, 264, 15852, 515, 34, 525, 7566, 34681, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 293, "seek": 77600, "start": 798.0, "end": 801.0, "text": " which is exactly what we're talking about on the cloud.", "tokens": [51464, 597, 307, 2293, 437, 321, 434, 1417, 466, 322, 264, 4588, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 294, "seek": 77600, "start": 801.0, "end": 804.0, "text": " And this is kind of like what you need to do.", "tokens": [51614, 400, 341, 307, 733, 295, 411, 437, 291, 643, 281, 360, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08360319301999848, "compression_ratio": 1.75, "no_speech_prob": 0.0011910925386473536}, {"id": 295, "seek": 80400, "start": 804.0, "end": 808.0, "text": " So, you write your log message in some way.", "tokens": [50364, 407, 11, 291, 2464, 428, 3565, 3636, 294, 512, 636, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 296, "seek": 80400, "start": 808.0, "end": 812.0, "text": " So, obviously, you need to have context for your message.", "tokens": [50564, 407, 11, 2745, 11, 291, 643, 281, 362, 4319, 337, 428, 3636, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 297, "seek": 80400, "start": 812.0, "end": 817.0, "text": " And the idea is to store all logs into memory.", "tokens": [50764, 400, 264, 1558, 307, 281, 3531, 439, 20820, 666, 4675, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 298, "seek": 80400, "start": 817.0, "end": 821.0, "text": " So, we're not going to store it into database or file system", "tokens": [51014, 407, 11, 321, 434, 406, 516, 281, 3531, 309, 666, 8149, 420, 3991, 1185, 51214], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 299, "seek": 80400, "start": 821.0, "end": 824.0, "text": " because that means you're adding input-output latency", "tokens": [51214, 570, 300, 1355, 291, 434, 5127, 4846, 12, 346, 2582, 27043, 51364], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 300, "seek": 80400, "start": 824.0, "end": 825.0, "text": " to your application.", "tokens": [51364, 281, 428, 3861, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 301, "seek": 80400, "start": 825.0, "end": 827.0, "text": " So, we need to minimize this.", "tokens": [51414, 407, 11, 321, 643, 281, 17522, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 302, "seek": 80400, "start": 827.0, "end": 830.0, "text": " So, the idea is to use some kind of map structure.", "tokens": [51514, 407, 11, 264, 1558, 307, 281, 764, 512, 733, 295, 4471, 3877, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 303, "seek": 80400, "start": 830.0, "end": 832.0, "text": " And this map has a key,", "tokens": [51664, 400, 341, 4471, 575, 257, 2141, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06621126617704119, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0021425208542495966}, {"id": 304, "seek": 83200, "start": 832.0, "end": 834.0, "text": " which is like where is this data coming from,", "tokens": [50364, 597, 307, 411, 689, 307, 341, 1412, 1348, 490, 11, 50464], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 305, "seek": 83200, "start": 834.0, "end": 836.0, "text": " from ID address and port number,", "tokens": [50464, 490, 7348, 2985, 293, 2436, 1230, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 306, "seek": 83200, "start": 836.0, "end": 838.0, "text": " and the message which is the value.", "tokens": [50564, 293, 264, 3636, 597, 307, 264, 2158, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 307, "seek": 83200, "start": 838.0, "end": 839.0, "text": " So, it's your choice now.", "tokens": [50664, 407, 11, 309, 311, 428, 3922, 586, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 308, "seek": 83200, "start": 839.0, "end": 842.0, "text": " Whether you use Varchar, for example, string.", "tokens": [50714, 8503, 291, 764, 691, 1178, 289, 11, 337, 1365, 11, 6798, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 309, "seek": 83200, "start": 842.0, "end": 843.0, "text": " Obviously, it's faster.", "tokens": [50864, 7580, 11, 309, 311, 4663, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 310, "seek": 83200, "start": 843.0, "end": 846.0, "text": " Or if you want to use some kind of JSON format, for example,", "tokens": [50914, 1610, 498, 291, 528, 281, 764, 512, 733, 295, 31828, 7877, 11, 337, 1365, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 311, "seek": 83200, "start": 846.0, "end": 849.0, "text": " if you're trying to do some kind of machine learning", "tokens": [51064, 498, 291, 434, 1382, 281, 360, 512, 733, 295, 3479, 2539, 51214], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 312, "seek": 83200, "start": 849.0, "end": 852.0, "text": " and do, I don't know, maybe some classification on your logs.", "tokens": [51214, 293, 360, 11, 286, 500, 380, 458, 11, 1310, 512, 21538, 322, 428, 20820, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 313, "seek": 83200, "start": 852.0, "end": 854.0, "text": " So, once you have your tm value,", "tokens": [51364, 407, 11, 1564, 291, 362, 428, 256, 76, 2158, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 314, "seek": 83200, "start": 854.0, "end": 857.0, "text": " what you can do is to proceed to save it into HazardCast.", "tokens": [51464, 437, 291, 393, 360, 307, 281, 8991, 281, 3155, 309, 666, 15852, 515, 34, 525, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 315, "seek": 83200, "start": 857.0, "end": 859.0, "text": " So, remember what we're talking about here?", "tokens": [51614, 407, 11, 1604, 437, 321, 434, 1417, 466, 510, 30, 51714], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 316, "seek": 83200, "start": 859.0, "end": 861.0, "text": " So, what you want is to get this map", "tokens": [51714, 407, 11, 437, 291, 528, 307, 281, 483, 341, 4471, 51814], "temperature": 0.0, "avg_logprob": -0.12252988011003976, "compression_ratio": 1.7116564417177915, "no_speech_prob": 0.0038234891835600138}, {"id": 317, "seek": 86100, "start": 861.0, "end": 863.0, "text": " within HazardCast.", "tokens": [50364, 1951, 15852, 515, 34, 525, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 318, "seek": 86100, "start": 863.0, "end": 867.0, "text": " So, first step is to send your logs into the cloud, obviously.", "tokens": [50464, 407, 11, 700, 1823, 307, 281, 2845, 428, 20820, 666, 264, 4588, 11, 2745, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 319, "seek": 86100, "start": 867.0, "end": 870.0, "text": " So, because different logs coming from different machines.", "tokens": [50664, 407, 11, 570, 819, 20820, 1348, 490, 819, 8379, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 320, "seek": 86100, "start": 870.0, "end": 873.0, "text": " And second step is to store it into memory.", "tokens": [50814, 400, 1150, 1823, 307, 281, 3531, 309, 666, 4675, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 321, "seek": 86100, "start": 873.0, "end": 877.0, "text": " So, this will allow you to access your logs in much faster way.", "tokens": [50964, 407, 11, 341, 486, 2089, 291, 281, 2105, 428, 20820, 294, 709, 4663, 636, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 322, "seek": 86100, "start": 877.0, "end": 879.0, "text": " And obviously, you need to do this mapping.", "tokens": [51164, 400, 2745, 11, 291, 643, 281, 360, 341, 18350, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 323, "seek": 86100, "start": 879.0, "end": 882.0, "text": " So, what you see here is kind of like SQL.", "tokens": [51264, 407, 11, 437, 291, 536, 510, 307, 733, 295, 411, 19200, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 324, "seek": 86100, "start": 882.0, "end": 884.0, "text": " You can write it in SQL.", "tokens": [51414, 509, 393, 2464, 309, 294, 19200, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 325, "seek": 86100, "start": 884.0, "end": 886.0, "text": " And once you have it in SQL,", "tokens": [51514, 400, 1564, 291, 362, 309, 294, 19200, 11, 51614], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 326, "seek": 86100, "start": 886.0, "end": 889.0, "text": " you can proceed to do this instance.", "tokens": [51614, 291, 393, 8991, 281, 360, 341, 5197, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06930935092088653, "compression_ratio": 1.782426778242678, "no_speech_prob": 0.007645148318260908}, {"id": 327, "seek": 88900, "start": 889.0, "end": 891.0, "text": " So, in order to run HazardCast,", "tokens": [50364, 407, 11, 294, 1668, 281, 1190, 15852, 515, 34, 525, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 328, "seek": 88900, "start": 891.0, "end": 894.0, "text": " what you need is have some kind, I don't know,", "tokens": [50464, 437, 291, 643, 307, 362, 512, 733, 11, 286, 500, 380, 458, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 329, "seek": 88900, "start": 894.0, "end": 898.0, "text": " from HazardCast instance and block in your pipeline.", "tokens": [50614, 490, 15852, 515, 34, 525, 5197, 293, 3461, 294, 428, 15517, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 330, "seek": 88900, "start": 898.0, "end": 902.0, "text": " So, pipeline first to the JIT, for example, or your process.", "tokens": [50814, 407, 11, 15517, 700, 281, 264, 508, 3927, 11, 337, 1365, 11, 420, 428, 1399, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 331, "seek": 88900, "start": 902.0, "end": 905.0, "text": " And in here, what you say is basically,", "tokens": [51014, 400, 294, 510, 11, 437, 291, 584, 307, 1936, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 332, "seek": 88900, "start": 905.0, "end": 908.0, "text": " I'm defining this is the IP address I want to run it on.", "tokens": [51164, 286, 478, 17827, 341, 307, 264, 8671, 2985, 286, 528, 281, 1190, 309, 322, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 333, "seek": 88900, "start": 908.0, "end": 911.0, "text": " This is my part and this is my data.", "tokens": [51314, 639, 307, 452, 644, 293, 341, 307, 452, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 334, "seek": 88900, "start": 911.0, "end": 913.0, "text": " Now, I mentioned SQL.", "tokens": [51464, 823, 11, 286, 2835, 19200, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 335, "seek": 88900, "start": 913.0, "end": 916.0, "text": " So, the platform itself has management center.", "tokens": [51564, 407, 11, 264, 3663, 2564, 575, 4592, 3056, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10037804431602602, "compression_ratio": 1.5674603174603174, "no_speech_prob": 0.002688063308596611}, {"id": 336, "seek": 91600, "start": 917.0, "end": 922.0, "text": " So, this is really cool, which allows you to query your data.", "tokens": [50414, 407, 11, 341, 307, 534, 1627, 11, 597, 4045, 291, 281, 14581, 428, 1412, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 337, "seek": 91600, "start": 922.0, "end": 926.0, "text": " So, what you see here is I'm trying to query my data", "tokens": [50664, 407, 11, 437, 291, 536, 510, 307, 286, 478, 1382, 281, 14581, 452, 1412, 50864], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 338, "seek": 91600, "start": 926.0, "end": 929.0, "text": " of my logs and really trying to understand what's going on.", "tokens": [50864, 295, 452, 20820, 293, 534, 1382, 281, 1223, 437, 311, 516, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 339, "seek": 91600, "start": 929.0, "end": 931.0, "text": " So, on the left, you see the key,", "tokens": [51014, 407, 11, 322, 264, 1411, 11, 291, 536, 264, 2141, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 340, "seek": 91600, "start": 931.0, "end": 933.0, "text": " which is like IP address input.", "tokens": [51114, 597, 307, 411, 8671, 2985, 4846, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 341, "seek": 91600, "start": 933.0, "end": 936.0, "text": " And on the far right side, or bottom right side,", "tokens": [51214, 400, 322, 264, 1400, 558, 1252, 11, 420, 2767, 558, 1252, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 342, "seek": 91600, "start": 936.0, "end": 938.0, "text": " you see values.", "tokens": [51364, 291, 536, 4190, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 343, "seek": 91600, "start": 938.0, "end": 942.0, "text": " So, essentially what we're doing with logs", "tokens": [51464, 407, 11, 4476, 437, 321, 434, 884, 365, 20820, 51664], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 344, "seek": 91600, "start": 942.0, "end": 945.0, "text": " is we're giving a score for each log message.", "tokens": [51664, 307, 321, 434, 2902, 257, 6175, 337, 1184, 3565, 3636, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11481391972508924, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.003791155992075801}, {"id": 345, "seek": 94500, "start": 945.0, "end": 947.0, "text": " So, if you're trying to define it, for example,", "tokens": [50364, 407, 11, 498, 291, 434, 1382, 281, 6964, 309, 11, 337, 1365, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 346, "seek": 94500, "start": 947.0, "end": 951.0, "text": " I know, for example, you have a specific category for logs,", "tokens": [50464, 286, 458, 11, 337, 1365, 11, 291, 362, 257, 2685, 7719, 337, 20820, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 347, "seek": 94500, "start": 951.0, "end": 954.0, "text": " whether it's information or warning or error.", "tokens": [50664, 1968, 309, 311, 1589, 420, 9164, 420, 6713, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 348, "seek": 94500, "start": 954.0, "end": 956.0, "text": " So, that's good, but if you're trying to predict", "tokens": [50814, 407, 11, 300, 311, 665, 11, 457, 498, 291, 434, 1382, 281, 6069, 50914], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 349, "seek": 94500, "start": 956.0, "end": 958.0, "text": " what's going to happen next,", "tokens": [50914, 437, 311, 516, 281, 1051, 958, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 350, "seek": 94500, "start": 958.0, "end": 961.0, "text": " you probably need to have some kind of linear scare for it.", "tokens": [51014, 291, 1391, 643, 281, 362, 512, 733, 295, 8213, 17185, 337, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 351, "seek": 94500, "start": 961.0, "end": 963.0, "text": " So, for example, you run it from 100", "tokens": [51164, 407, 11, 337, 1365, 11, 291, 1190, 309, 490, 2319, 51264], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 352, "seek": 94500, "start": 963.0, "end": 966.0, "text": " and you give it value for your log message", "tokens": [51264, 293, 291, 976, 309, 2158, 337, 428, 3565, 3636, 51414], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 353, "seek": 94500, "start": 966.0, "end": 969.0, "text": " and this is the value you see there.", "tokens": [51414, 293, 341, 307, 264, 2158, 291, 536, 456, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 354, "seek": 94500, "start": 969.0, "end": 972.0, "text": " So, we take this data and we ingest it into memory.", "tokens": [51564, 407, 11, 321, 747, 341, 1412, 293, 321, 3957, 377, 309, 666, 4675, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09455944720963787, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.0025658677332103252}, {"id": 355, "seek": 97200, "start": 972.0, "end": 974.0, "text": " So, we have the IP address port number", "tokens": [50364, 407, 11, 321, 362, 264, 8671, 2985, 2436, 1230, 50464], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 356, "seek": 97200, "start": 974.0, "end": 977.0, "text": " and we have a score for each log message.", "tokens": [50464, 293, 321, 362, 257, 6175, 337, 1184, 3565, 3636, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 357, "seek": 97200, "start": 977.0, "end": 980.0, "text": " And once we import it, we define a trend.", "tokens": [50614, 400, 1564, 321, 974, 309, 11, 321, 6964, 257, 6028, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 358, "seek": 97200, "start": 980.0, "end": 983.0, "text": " So, in my case, I'm taking a window", "tokens": [50764, 407, 11, 294, 452, 1389, 11, 286, 478, 1940, 257, 4910, 50914], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 359, "seek": 97200, "start": 983.0, "end": 985.0, "text": " on this real-time stream processing", "tokens": [50914, 322, 341, 957, 12, 3766, 4309, 9007, 51014], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 360, "seek": 97200, "start": 985.0, "end": 989.0, "text": " and, in my case, it's two minutes, but it could be anything.", "tokens": [51014, 293, 11, 294, 452, 1389, 11, 309, 311, 732, 2077, 11, 457, 309, 727, 312, 1340, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 361, "seek": 97200, "start": 989.0, "end": 992.0, "text": " And I'm defining this trend based on the score value", "tokens": [51214, 400, 286, 478, 17827, 341, 6028, 2361, 322, 264, 6175, 2158, 51364], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 362, "seek": 97200, "start": 992.0, "end": 994.0, "text": " for each log message.", "tokens": [51364, 337, 1184, 3565, 3636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 363, "seek": 97200, "start": 994.0, "end": 997.0, "text": " And based on this, I try to create predictive,", "tokens": [51464, 400, 2361, 322, 341, 11, 286, 853, 281, 1884, 35521, 11, 51614], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 364, "seek": 97200, "start": 997.0, "end": 1000.0, "text": " or prediction map, so another map,", "tokens": [51614, 420, 17630, 4471, 11, 370, 1071, 4471, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14183555665563363, "compression_ratio": 1.7606837606837606, "no_speech_prob": 0.018362408503890038}, {"id": 365, "seek": 100000, "start": 1000.0, "end": 1003.0, "text": " in order to say if I want to send alert,", "tokens": [50364, 294, 1668, 281, 584, 498, 286, 528, 281, 2845, 9615, 11, 50514], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 366, "seek": 100000, "start": 1003.0, "end": 1008.0, "text": " which has value of one or zero, don't send alert.", "tokens": [50514, 597, 575, 2158, 295, 472, 420, 4018, 11, 500, 380, 2845, 9615, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 367, "seek": 100000, "start": 1008.0, "end": 1012.0, "text": " Obviously, you need to do some kind of programming in it.", "tokens": [50764, 7580, 11, 291, 643, 281, 360, 512, 733, 295, 9410, 294, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 368, "seek": 100000, "start": 1012.0, "end": 1015.0, "text": " So, the idea is to group messages based on the score", "tokens": [50964, 407, 11, 264, 1558, 307, 281, 1594, 7897, 2361, 322, 264, 6175, 51114], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 369, "seek": 100000, "start": 1015.0, "end": 1017.0, "text": " to define the trend and from there,", "tokens": [51114, 281, 6964, 264, 6028, 293, 490, 456, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 370, "seek": 100000, "start": 1017.0, "end": 1019.0, "text": " you can use some kind of machine learning,", "tokens": [51214, 291, 393, 764, 512, 733, 295, 3479, 2539, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 371, "seek": 100000, "start": 1019.0, "end": 1022.0, "text": " so linear regression or classification for example", "tokens": [51314, 370, 8213, 24590, 420, 21538, 337, 1365, 51464], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 372, "seek": 100000, "start": 1022.0, "end": 1024.0, "text": " to do some kind of prediction", "tokens": [51464, 281, 360, 512, 733, 295, 17630, 51564], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 373, "seek": 100000, "start": 1024.0, "end": 1026.0, "text": " and also you need to output it,", "tokens": [51564, 293, 611, 291, 643, 281, 5598, 309, 11, 51664], "temperature": 0.0, "avg_logprob": -0.13142807684212088, "compression_ratio": 1.708695652173913, "no_speech_prob": 0.002384747611358762}, {"id": 374, "seek": 102600, "start": 1026.0, "end": 1030.0, "text": " which means you send it back to the user.", "tokens": [50364, 597, 1355, 291, 2845, 309, 646, 281, 264, 4195, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 375, "seek": 102600, "start": 1030.0, "end": 1032.0, "text": " Now, this is good, so this is how it works.", "tokens": [50564, 823, 11, 341, 307, 665, 11, 370, 341, 307, 577, 309, 1985, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 376, "seek": 102600, "start": 1032.0, "end": 1036.0, "text": " So, this is your actual code for doing the map prediction.", "tokens": [50664, 407, 11, 341, 307, 428, 3539, 3089, 337, 884, 264, 4471, 17630, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 377, "seek": 102600, "start": 1036.0, "end": 1039.0, "text": " So, we take the score for each log message,", "tokens": [50864, 407, 11, 321, 747, 264, 6175, 337, 1184, 3565, 3636, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 378, "seek": 102600, "start": 1039.0, "end": 1043.0, "text": " we do the linear regression based on the window I define,", "tokens": [51014, 321, 360, 264, 8213, 24590, 2361, 322, 264, 4910, 286, 6964, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 379, "seek": 102600, "start": 1043.0, "end": 1047.0, "text": " and I'm simply checking if the value is greater than 100,", "tokens": [51214, 293, 286, 478, 2935, 8568, 498, 264, 2158, 307, 5044, 813, 2319, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 380, "seek": 102600, "start": 1047.0, "end": 1051.0, "text": " send an alert, otherwise, don't send alert.", "tokens": [51414, 2845, 364, 9615, 11, 5911, 11, 500, 380, 2845, 9615, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 381, "seek": 102600, "start": 1051.0, "end": 1054.0, "text": " And in here, what you need to focus on", "tokens": [51614, 400, 294, 510, 11, 437, 291, 643, 281, 1879, 322, 51764], "temperature": 0.0, "avg_logprob": -0.10105072915017067, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001850753091275692}, {"id": 382, "seek": 105400, "start": 1054.0, "end": 1057.0, "text": " is kind of like three ways to proceed from here.", "tokens": [50364, 307, 733, 295, 411, 1045, 2098, 281, 8991, 490, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 383, "seek": 105400, "start": 1057.0, "end": 1061.0, "text": " So, there are three ways to do stream processing,", "tokens": [50514, 407, 11, 456, 366, 1045, 2098, 281, 360, 4309, 9007, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 384, "seek": 105400, "start": 1061.0, "end": 1064.0, "text": " and the first one is to use SQL for example,", "tokens": [50714, 293, 264, 700, 472, 307, 281, 764, 19200, 337, 1365, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 385, "seek": 105400, "start": 1064.0, "end": 1067.0, "text": " so you take it from the logs map", "tokens": [50864, 370, 291, 747, 309, 490, 264, 20820, 4471, 51014], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 386, "seek": 105400, "start": 1067.0, "end": 1071.0, "text": " and you store it in our log map with some filtering.", "tokens": [51014, 293, 291, 3531, 309, 294, 527, 3565, 4471, 365, 512, 30822, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 387, "seek": 105400, "start": 1071.0, "end": 1075.0, "text": " And second option is to use a process or pipeline,", "tokens": [51214, 400, 1150, 3614, 307, 281, 764, 257, 1399, 420, 15517, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 388, "seek": 105400, "start": 1075.0, "end": 1078.0, "text": " so you read it from the map and you do as well,", "tokens": [51414, 370, 291, 1401, 309, 490, 264, 4471, 293, 291, 360, 382, 731, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 389, "seek": 105400, "start": 1078.0, "end": 1081.0, "text": " you do some kind of alerts for example or train.", "tokens": [51564, 291, 360, 512, 733, 295, 28061, 337, 1365, 420, 3847, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10418980076627911, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.003432055003941059}, {"id": 390, "seek": 108100, "start": 1081.0, "end": 1085.0, "text": " So, the first two ways is called batch stream processing,", "tokens": [50364, 407, 11, 264, 700, 732, 2098, 307, 1219, 15245, 4309, 9007, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 391, "seek": 108100, "start": 1085.0, "end": 1087.0, "text": " so it's not read time.", "tokens": [50564, 370, 309, 311, 406, 1401, 565, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 392, "seek": 108100, "start": 1087.0, "end": 1090.0, "text": " And the third option, so this is the main thing you want to do", "tokens": [50664, 400, 264, 2636, 3614, 11, 370, 341, 307, 264, 2135, 551, 291, 528, 281, 360, 50814], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 393, "seek": 108100, "start": 1090.0, "end": 1094.0, "text": " if you want to provide some kind of read time messaging,", "tokens": [50814, 498, 291, 528, 281, 2893, 512, 733, 295, 1401, 565, 21812, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 394, "seek": 108100, "start": 1094.0, "end": 1097.0, "text": " is to look for kind of the map journal,", "tokens": [51014, 307, 281, 574, 337, 733, 295, 264, 4471, 6708, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 395, "seek": 108100, "start": 1097.0, "end": 1100.0, "text": " which is also similar to map,", "tokens": [51164, 597, 307, 611, 2531, 281, 4471, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 396, "seek": 108100, "start": 1100.0, "end": 1102.0, "text": " but it's like ring puffer sucker,", "tokens": [51314, 457, 309, 311, 411, 4875, 19613, 260, 43259, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 397, "seek": 108100, "start": 1102.0, "end": 1105.0, "text": " which allows you to start processing your logs,", "tokens": [51414, 597, 4045, 291, 281, 722, 9007, 428, 20820, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 398, "seek": 108100, "start": 1105.0, "end": 1108.0, "text": " either from start or from end depends on what you want.", "tokens": [51564, 2139, 490, 722, 420, 490, 917, 5946, 322, 437, 291, 528, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12870602903112902, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.00815725326538086}, {"id": 399, "seek": 110800, "start": 1108.0, "end": 1112.0, "text": " And the pipeline itself takes this memory map,", "tokens": [50364, 400, 264, 15517, 2564, 2516, 341, 4675, 4471, 11, 50564], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 400, "seek": 110800, "start": 1112.0, "end": 1115.0, "text": " logs and do some filtering.", "tokens": [50564, 20820, 293, 360, 512, 30822, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 401, "seek": 110800, "start": 1115.0, "end": 1118.0, "text": " So, if you see some value in this specific moment,", "tokens": [50714, 407, 11, 498, 291, 536, 512, 2158, 294, 341, 2685, 1623, 11, 50864], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 402, "seek": 110800, "start": 1118.0, "end": 1121.0, "text": " you can do this alerts for example,", "tokens": [50864, 291, 393, 360, 341, 28061, 337, 1365, 11, 51014], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 403, "seek": 110800, "start": 1121.0, "end": 1124.0, "text": " or you can send it to the message.", "tokens": [51014, 420, 291, 393, 2845, 309, 281, 264, 3636, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 404, "seek": 110800, "start": 1124.0, "end": 1128.0, "text": " So, I think kind of like I wanted to cover everything,", "tokens": [51164, 407, 11, 286, 519, 733, 295, 411, 286, 1415, 281, 2060, 1203, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 405, "seek": 110800, "start": 1128.0, "end": 1131.0, "text": " but because it's too 20 minutes here", "tokens": [51364, 457, 570, 309, 311, 886, 945, 2077, 510, 51514], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 406, "seek": 110800, "start": 1131.0, "end": 1134.0, "text": " and it's not enough time to talk about everything,", "tokens": [51514, 293, 309, 311, 406, 1547, 565, 281, 751, 466, 1203, 11, 51664], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 407, "seek": 110800, "start": 1134.0, "end": 1137.0, "text": " so just to give you somebody what you need to do", "tokens": [51664, 370, 445, 281, 976, 291, 2618, 437, 291, 643, 281, 360, 51814], "temperature": 0.0, "avg_logprob": -0.15335749688549577, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.003668744582682848}, {"id": 408, "seek": 113700, "start": 1137.0, "end": 1140.0, "text": " and open this stage for questions.", "tokens": [50364, 293, 1269, 341, 3233, 337, 1651, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 409, "seek": 113700, "start": 1140.0, "end": 1144.0, "text": " So, first of all, you need to store your logs to the cloud.", "tokens": [50514, 407, 11, 700, 295, 439, 11, 291, 643, 281, 3531, 428, 20820, 281, 264, 4588, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 410, "seek": 113700, "start": 1144.0, "end": 1146.0, "text": " So, in my case, I'm using HazerCast,", "tokens": [50714, 407, 11, 294, 452, 1389, 11, 286, 478, 1228, 15852, 260, 34, 525, 11, 50814], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 411, "seek": 113700, "start": 1146.0, "end": 1149.0, "text": " but obviously you can't use any other cloud provider,", "tokens": [50814, 457, 2745, 291, 393, 380, 764, 604, 661, 4588, 12398, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 412, "seek": 113700, "start": 1149.0, "end": 1154.0, "text": " but you need to import HazerCast into that cloud provider.", "tokens": [50964, 457, 291, 643, 281, 974, 15852, 260, 34, 525, 666, 300, 4588, 12398, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 413, "seek": 113700, "start": 1154.0, "end": 1157.0, "text": " So, once you upload all logs,", "tokens": [51214, 407, 11, 1564, 291, 6580, 439, 20820, 11, 51364], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 414, "seek": 113700, "start": 1157.0, "end": 1161.0, "text": " what you want is kind of like have some kind of cloud solution", "tokens": [51364, 437, 291, 528, 307, 733, 295, 411, 362, 512, 733, 295, 4588, 3827, 51564], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 415, "seek": 113700, "start": 1161.0, "end": 1165.0, "text": " for it, so where you can use for example, I don't know,", "tokens": [51564, 337, 309, 11, 370, 689, 291, 393, 764, 337, 1365, 11, 286, 500, 380, 458, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13644027709960938, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.006750946864485741}, {"id": 416, "seek": 116500, "start": 1166.0, "end": 1169.0, "text": " Varchar, if you want speed or JSON,", "tokens": [50414, 691, 1178, 289, 11, 498, 291, 528, 3073, 420, 31828, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 417, "seek": 116500, "start": 1169.0, "end": 1173.0, "text": " for example, if you want to apply some kind of machine learning,", "tokens": [50564, 337, 1365, 11, 498, 291, 528, 281, 3079, 512, 733, 295, 3479, 2539, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 418, "seek": 116500, "start": 1173.0, "end": 1176.0, "text": " and you need to use some kind of map structure.", "tokens": [50764, 293, 291, 643, 281, 764, 512, 733, 295, 4471, 3877, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 419, "seek": 116500, "start": 1176.0, "end": 1179.0, "text": " So, the idea is to store logs into memory", "tokens": [50914, 407, 11, 264, 1558, 307, 281, 3531, 20820, 666, 4675, 51064], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 420, "seek": 116500, "start": 1179.0, "end": 1183.0, "text": " in order to have some kind of random artist and rebalancing.", "tokens": [51064, 294, 1668, 281, 362, 512, 733, 295, 4974, 5748, 293, 319, 2645, 8779, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 421, "seek": 116500, "start": 1183.0, "end": 1186.0, "text": " And obviously you need to configure this map", "tokens": [51264, 400, 2745, 291, 643, 281, 22162, 341, 4471, 51414], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 422, "seek": 116500, "start": 1186.0, "end": 1189.0, "text": " because you have a specific size that's on limits on it,", "tokens": [51414, 570, 291, 362, 257, 2685, 2744, 300, 311, 322, 10406, 322, 309, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 423, "seek": 116500, "start": 1189.0, "end": 1194.0, "text": " so you need to have some kind of eviction policy on your data.", "tokens": [51564, 370, 291, 643, 281, 362, 512, 733, 295, 1073, 4105, 3897, 322, 428, 1412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1155132159852145, "compression_ratio": 1.7627118644067796, "no_speech_prob": 0.0012008192716166377}, {"id": 424, "seek": 119400, "start": 1194.0, "end": 1196.0, "text": " And obviously you need to consider security.", "tokens": [50364, 400, 2745, 291, 643, 281, 1949, 3825, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 425, "seek": 119400, "start": 1196.0, "end": 1199.0, "text": " So, whatever you send to the cloud, obviously it can,", "tokens": [50464, 407, 11, 2035, 291, 2845, 281, 264, 4588, 11, 2745, 309, 393, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 426, "seek": 119400, "start": 1199.0, "end": 1202.0, "text": " you need to have some kind of security mechanism", "tokens": [50614, 291, 643, 281, 362, 512, 733, 295, 3825, 7513, 50764], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 427, "seek": 119400, "start": 1202.0, "end": 1205.0, "text": " just to make sure that you don't send sensitive data.", "tokens": [50764, 445, 281, 652, 988, 300, 291, 500, 380, 2845, 9477, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 428, "seek": 119400, "start": 1205.0, "end": 1209.0, "text": " So, if you're interested in this topic about stream processing,", "tokens": [50914, 407, 11, 498, 291, 434, 3102, 294, 341, 4829, 466, 4309, 9007, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 429, "seek": 119400, "start": 1209.0, "end": 1213.0, "text": " we're running an unconference in March next month,", "tokens": [51114, 321, 434, 2614, 364, 35847, 5158, 294, 6129, 958, 1618, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 430, "seek": 119400, "start": 1213.0, "end": 1217.0, "text": " and it's free to join, and there is like training workshop as well.", "tokens": [51314, 293, 309, 311, 1737, 281, 3917, 11, 293, 456, 307, 411, 3097, 13541, 382, 731, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 431, "seek": 119400, "start": 1217.0, "end": 1219.0, "text": " So, all you need to do is just scan this code", "tokens": [51514, 407, 11, 439, 291, 643, 281, 360, 307, 445, 11049, 341, 3089, 51614], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 432, "seek": 119400, "start": 1219.0, "end": 1222.0, "text": " and register for the real-time stream processing unconference.", "tokens": [51614, 293, 7280, 337, 264, 957, 12, 3766, 4309, 9007, 35847, 5158, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1328342528570266, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.003816354088485241}, {"id": 433, "seek": 122200, "start": 1222.0, "end": 1224.0, "text": " It's community-based, so it's open source.", "tokens": [50364, 467, 311, 1768, 12, 6032, 11, 370, 309, 311, 1269, 4009, 13, 50464], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 434, "seek": 122200, "start": 1224.0, "end": 1227.0, "text": " You get the training, you get batch on top of it,", "tokens": [50464, 509, 483, 264, 3097, 11, 291, 483, 15245, 322, 1192, 295, 309, 11, 50614], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 435, "seek": 122200, "start": 1227.0, "end": 1230.0, "text": " as well as we have run table where you can see", "tokens": [50614, 382, 731, 382, 321, 362, 1190, 3199, 689, 291, 393, 536, 50764], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 436, "seek": 122200, "start": 1230.0, "end": 1233.0, "text": " industrial experts as well as community users", "tokens": [50764, 9987, 8572, 382, 731, 382, 1768, 5022, 50914], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 437, "seek": 122200, "start": 1233.0, "end": 1235.0, "text": " how they can contribute to open source of projects.", "tokens": [50914, 577, 436, 393, 10586, 281, 1269, 4009, 295, 4455, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 438, "seek": 122200, "start": 1235.0, "end": 1239.0, "text": " And, you know, you can basically ask questions if you have.", "tokens": [51014, 400, 11, 291, 458, 11, 291, 393, 1936, 1029, 1651, 498, 291, 362, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 439, "seek": 122200, "start": 1239.0, "end": 1240.0, "text": " So, with that being said,", "tokens": [51214, 407, 11, 365, 300, 885, 848, 11, 51264], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 440, "seek": 122200, "start": 1240.0, "end": 1243.0, "text": " thanks very much for listening and I'll open for questions.", "tokens": [51264, 3231, 588, 709, 337, 4764, 293, 286, 603, 1269, 337, 1651, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 441, "seek": 122200, "start": 1243.0, "end": 1244.0, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17243065090354429, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.029255034402012825}, {"id": 442, "seek": 124400, "start": 1244.0, "end": 1245.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.46149102846781415, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.16594178974628448}, {"id": 443, "seek": 127400, "start": 1274.0, "end": 1300.0, "text": " Yeah, so it is possible to do like multiple streams,", "tokens": [50364, 865, 11, 370, 309, 307, 1944, 281, 360, 411, 3866, 15842, 11, 51664], "temperature": 0.0, "avg_logprob": -0.4147770188071511, "compression_ratio": 1.1142857142857143, "no_speech_prob": 0.3354983925819397}, {"id": 444, "seek": 127400, "start": 1300.0, "end": 1302.0, "text": " joining multiple sources.", "tokens": [51664, 5549, 3866, 7139, 13, 51764], "temperature": 0.0, "avg_logprob": -0.4147770188071511, "compression_ratio": 1.1142857142857143, "no_speech_prob": 0.3354983925819397}, {"id": 445, "seek": 130200, "start": 1303.0, "end": 1306.0, "text": " So, this is possible to do.", "tokens": [50414, 407, 11, 341, 307, 1944, 281, 360, 13, 50564], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 446, "seek": 130200, "start": 1306.0, "end": 1309.0, "text": " Obviously, you need just to find the configuration", "tokens": [50564, 7580, 11, 291, 643, 445, 281, 915, 264, 11694, 50714], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 447, "seek": 130200, "start": 1309.0, "end": 1311.0, "text": " for this specific case.", "tokens": [50714, 337, 341, 2685, 1389, 13, 50814], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 448, "seek": 130200, "start": 1311.0, "end": 1312.0, "text": " So, I mentioned sources here.", "tokens": [50814, 407, 11, 286, 2835, 7139, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 449, "seek": 130200, "start": 1312.0, "end": 1315.0, "text": " So, the sources is not only a single source,", "tokens": [50864, 407, 11, 264, 7139, 307, 406, 787, 257, 2167, 4009, 11, 51014], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 450, "seek": 130200, "start": 1315.0, "end": 1316.0, "text": " it could be multiple sources,", "tokens": [51014, 309, 727, 312, 3866, 7139, 11, 51064], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 451, "seek": 130200, "start": 1316.0, "end": 1320.0, "text": " and that's where you do join multiple sources.", "tokens": [51064, 293, 300, 311, 689, 291, 360, 3917, 3866, 7139, 13, 51264], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 452, "seek": 130200, "start": 1320.0, "end": 1326.0, "text": " So, it is possible to do it with data.", "tokens": [51264, 407, 11, 309, 307, 1944, 281, 360, 309, 365, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 453, "seek": 130200, "start": 1326.0, "end": 1328.0, "text": " Any other questions?", "tokens": [51564, 2639, 661, 1651, 30, 51664], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 454, "seek": 130200, "start": 1328.0, "end": 1329.0, "text": " Yeah?", "tokens": [51664, 865, 30, 51714], "temperature": 0.0, "avg_logprob": -0.24121611745733965, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.38617610931396484}, {"id": 455, "seek": 132900, "start": 1330.0, "end": 1331.0, "text": " Yeah.", "tokens": [50414, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 456, "seek": 132900, "start": 1331.0, "end": 1333.0, "text": " Please take a seat.", "tokens": [50464, 2555, 747, 257, 6121, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 457, "seek": 132900, "start": 1333.0, "end": 1336.0, "text": " What is the difference?", "tokens": [50564, 708, 307, 264, 2649, 30, 50714], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 458, "seek": 132900, "start": 1336.0, "end": 1338.0, "text": " So, the question is what is the difference", "tokens": [50714, 407, 11, 264, 1168, 307, 437, 307, 264, 2649, 50814], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 459, "seek": 132900, "start": 1338.0, "end": 1340.0, "text": " between HazardCast and Apache Flake?", "tokens": [50814, 1296, 15852, 515, 34, 525, 293, 46597, 3235, 619, 30, 50914], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 460, "seek": 132900, "start": 1340.0, "end": 1341.0, "text": " So, there was a slide,", "tokens": [50914, 407, 11, 456, 390, 257, 4137, 11, 50964], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 461, "seek": 132900, "start": 1341.0, "end": 1343.0, "text": " but I decided to remove it from here.", "tokens": [50964, 457, 286, 3047, 281, 4159, 309, 490, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 462, "seek": 132900, "start": 1343.0, "end": 1346.0, "text": " So, essentially what you want is kind of like,", "tokens": [51064, 407, 11, 4476, 437, 291, 528, 307, 733, 295, 411, 11, 51214], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 463, "seek": 132900, "start": 1346.0, "end": 1350.0, "text": " for real time is to look for minimizing latency", "tokens": [51214, 337, 957, 565, 307, 281, 574, 337, 46608, 27043, 51414], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 464, "seek": 132900, "start": 1350.0, "end": 1351.0, "text": " within your application.", "tokens": [51414, 1951, 428, 3861, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 465, "seek": 132900, "start": 1351.0, "end": 1355.0, "text": " So, we've done benchmark between HazardCast and Flake.", "tokens": [51464, 407, 11, 321, 600, 1096, 18927, 1296, 15852, 515, 34, 525, 293, 3235, 619, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 466, "seek": 132900, "start": 1355.0, "end": 1357.0, "text": " So, this is where things can make difference.", "tokens": [51664, 407, 11, 341, 307, 689, 721, 393, 652, 2649, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21756977331442912, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.1298859566450119}, {"id": 467, "seek": 135700, "start": 1357.0, "end": 1360.0, "text": " So, if you're trying to basically write an application", "tokens": [50364, 407, 11, 498, 291, 434, 1382, 281, 1936, 2464, 364, 3861, 50514], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 468, "seek": 135700, "start": 1360.0, "end": 1361.0, "text": " for real-time sequencing,", "tokens": [50514, 337, 957, 12, 3766, 32693, 11, 50564], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 469, "seek": 135700, "start": 1361.0, "end": 1363.0, "text": " we want to minimize latency.", "tokens": [50564, 321, 528, 281, 17522, 27043, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 470, "seek": 135700, "start": 1363.0, "end": 1364.0, "text": " So, the lower is better,", "tokens": [50664, 407, 11, 264, 3126, 307, 1101, 11, 50714], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 471, "seek": 135700, "start": 1364.0, "end": 1369.0, "text": " and this is where HazardCast has performed a link", "tokens": [50714, 293, 341, 307, 689, 15852, 515, 34, 525, 575, 10332, 257, 2113, 50964], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 472, "seek": 135700, "start": 1369.0, "end": 1370.0, "text": " or Apache Spark.", "tokens": [50964, 420, 46597, 23424, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 473, "seek": 135700, "start": 1370.0, "end": 1373.0, "text": " So, the latency is the key difference between these two.", "tokens": [51014, 407, 11, 264, 27043, 307, 264, 2141, 2649, 1296, 613, 732, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 474, "seek": 135700, "start": 1373.0, "end": 1375.0, "text": " The results are online,", "tokens": [51164, 440, 3542, 366, 2950, 11, 51264], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 475, "seek": 135700, "start": 1375.0, "end": 1378.0, "text": " but I decided not to include it just for this.", "tokens": [51264, 457, 286, 3047, 406, 281, 4090, 309, 445, 337, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 476, "seek": 135700, "start": 1378.0, "end": 1382.0, "text": " So, basically it's the latency between different platforms.", "tokens": [51414, 407, 11, 1936, 309, 311, 264, 27043, 1296, 819, 9473, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 477, "seek": 135700, "start": 1382.0, "end": 1383.0, "text": " So, that's what you want to focus,", "tokens": [51614, 407, 11, 300, 311, 437, 291, 528, 281, 1879, 11, 51664], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 478, "seek": 135700, "start": 1383.0, "end": 1384.0, "text": " whether it's HazardCast, Flake,", "tokens": [51664, 1968, 309, 311, 15852, 515, 34, 525, 11, 3235, 619, 11, 51714], "temperature": 0.0, "avg_logprob": -0.17403450879183682, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.013320490717887878}, {"id": 479, "seek": 138400, "start": 1384.0, "end": 1387.0, "text": " or any other platform which offers real-time sequencing.", "tokens": [50364, 420, 604, 661, 3663, 597, 7736, 957, 12, 3766, 32693, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2540277423280658, "compression_ratio": 1.244186046511628, "no_speech_prob": 0.02527124062180519}, {"id": 480, "seek": 138400, "start": 1390.0, "end": 1391.0, "text": " Thank you.", "tokens": [50664, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2540277423280658, "compression_ratio": 1.244186046511628, "no_speech_prob": 0.02527124062180519}, {"id": 481, "seek": 138400, "start": 1394.0, "end": 1395.0, "text": " Thank you very much, Thomas.", "tokens": [50864, 1044, 291, 588, 709, 11, 8500, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2540277423280658, "compression_ratio": 1.244186046511628, "no_speech_prob": 0.02527124062180519}, {"id": 482, "seek": 138400, "start": 1395.0, "end": 1396.0, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2540277423280658, "compression_ratio": 1.244186046511628, "no_speech_prob": 0.02527124062180519}], "language": "en"}