{"text": " Just two classical optimizations that will help modern but mature virtual machine where we have that powers native images and why is it important? Well, and who I am? My name is Mito Chukko. I work at a company named Bellsoft which actively participates in OpenJDK community and we release our own JDK distribution which you probably met if you have ever built a Spring Boot container with default build pack. So it's in there. And now Spring Boot, since version 3, supports containers with native images. It can be built as a native image and if you do that, the compiler being used is the American native image kit which is a Bellsoft distribution of GrowLVM. So that's another project that we participate and GrowLVM itself can be seen as different things at least two major modes that we can absorb. It can run as a JIT where compiler is GrowLVM or we can build a native image with a static compilation and it will utilize a special virtual machine substrate VM and here it's different from the traditional Java, traditional way of how we run it. Well, another interesting and peculiar point here is that it is written in Java. So it is a complex project but the most of the code is Java and this is beautiful. So you have a virtual machine and a compiler for JVM languages and Java in particular written in Java. So if you look at Java itself, why is it so beautiful? Well, not so beautiful compared to Kotlin as we know, right? But still, both Java and Kotlin, they share those concepts. So from the very beginning, there is a way to write correct parallel programs. So then the right parallel programs, we need some means of synchronization or to orchestrate so our threads, if we share data, most typically we do that. And also it's a managed runtime where we don't have to worry that much about pre-memory because we have garbage collection and garbage is collected for us and our programs just, they can't have memory leak but you have to work hard to get one. And having that native image implementation makes our final binaries very, sometimes makes them very performant. Of course, we have an instant startup. It was mentioned today several times. But we can also have a very good peak performance. In certain cases, that's not a rule but it can happen, like it happens here on this plot. That's just a simple spring boot application and we just ping the same endpoint. And here the native image works better and also it warms up instantly and it has very good latency. So for this small amount of memory that it takes, so this is a small service, it takes small amount of memory, very small heap, and it also has low latency. And under the hood, it uses, well, serial GC and we'll talk about that later. Well, what about relationship between Graal VM and OpenJDK? Well, we're here in a Friends of OpenJDK room and Graal has been integrated as an additional experimental compiler in JDK9. But while it has been removed from recent JDKs, but what's the left over? It's an interface to plug it in. So now it's going to be a second attempt to do that. So here on slides it's mentioned that there is a discussion about project, new project they all had, but last week it was already called for votes in OpenJDK to start the project of bringing the most sweet parts of this technology into OpenJDK, back into OpenJDK. It's something that happens right now. So that default garbage collector that sometimes shows very good latency even compared to ParallelGC or G1 in hotspot, well, on small heaps. Well, it's a kind of garbage collector we can easily understand. And it's generational stop the world collection. So here only one survivor space, but actually it's 16 by default. But anyway, so we stop all our application threads and we collect garbage in a single thread, so this is a kind of a basic garbage collector, right, but from the other hand it's reliable and it's very effective, especially if you have only a single core available. So you see the problem. We have some CPU which may be enough to run many threads, but we run only one at least for garbage collection. Now garbage collection can take significant time during our application execution, well, that's obvious. Well, what would we do? Of course, we would like to do exactly the same thing, but in parallel, to decrease the time garbage collection takes to reduce the garbage collection pause, because it still stopped the world pause, but we reduce it because we process data with multiple threads. So that's the idea of parallel garbage collection. The idea is not new, but surprisingly, this modern runtime doesn't have it yet. Well, we decided to implement it and it's still being under review and some implementation details, well, they change, but the idea is very simple. You just say, pass the garbage collection selection during the creation of your native image. For instance, if you use some Maven or Gradle configuration for your Spring Boot container, you also can do that. And then you have some GRIPS in runtime, which you also can twist when you run your application. And well, you enable that implementation. I'll show some performance results later, but basically the implementation itself, well, it can be analyzed as a change in a big Java program, which Brawl VM is. And there are now two GC interfaces and implementations. And this functionality just re-use existing things in a very, I would say, smart way just to keep what is all about the parallelization as a code. So everything else is reused from serial GC. Basically there's a problem of how do we synchronize and share the work? Because parallel threads for garbage collection, they also have the same problem because they work on the same data, so they have contention or may have contention. So we need to share in some smart manner. Well, it's implemented with a work divided in its volume. So every thread operates its local memory, and it's a chunk of memory of one megabyte. So if we need an extra memory, like we scan objects and we fulfill some set of data that we operate on. And then we have an extra chunk. We can just put it aside so someone else can pick it. So that's the stack that contains the chunks of work. And then the work is finished, the thread just takes the next chunk of work. There may be a situation when several threads try to copy to promote the same object. And this is actually solved very simply. They just reserve some space for the object and then tries to install forward pointer using an atomic operation. And as this is an atomic operation, only one thread succeeds, so others just roll back and this is a lightweight operation. Again this is Java. This is not a strict AML, sorry, but still all existing places that manage memory were reused without changing the architecture of Growl itself. So there are already possibilities to add garbage collectors. So if you want to implement one, it's not that complex. The major problem is to be correct when you deal with memory. When you deal with concurrency, and then you inject your code into this virtual machine because it's all declarative magic that requires you to be careful. Well, some performance results. With relatively large heaps with serial GC, you can have pauses of several seconds, which is long, of course. And there's a big difference if you have a two or three or four second pause or if you decrease it by one second. So that's possible with this implementation already. So that's the order of this improvement. With another benchmark, hyperalogue, you see that latency here, latency of pauses can be decreased like two times. Those pauses are not that big, and we have frequent collections here, so x-axis is epoch, so each point is a garbage collection, and y-axis is time in, I believe, milliseconds. Well, that's paralogy. So we can obviously improve many applications and many installations where we have an option to use several CPUs. If we use one CPU, of course, we won't see much difference. There is some increase in memory used for service needs, but that's kind of moderate. So other parts of this complex system. I mentioned synchronization, and, well, synchronization is useful, but it has tradeoffs. Because if we implement the non-synchronization, we need to save our CPU resources to put aside threads that won't get the resource. We need to stop them, to queue them, to manage that queues, to wake them up, to involve operating system in that process. So that's not cheap, but there are situations that, that's another queue, right? And that even influences the design of standard library, because, like, we all know string buffer and string builder, right? One class appeared because, well, another one wasn't very pleasant in terms of performance. Yeah, we need it sometimes, but in many cases, we need a non-synchronized implementation, saying, like, hash table and hash map, whoever uses hash table, right? But it's very good synchronized. But not all classes that have any synchronization in them have their twins without synchronization. That makes no sense, right? So there's a well-known technology, how to deal with a case where accesses to our data structures, to our classes, are mostly sequential than at any point in time, only a single thread owns and operates with an object. And it's called bus-locking or thing-locking. Well, why is it simpler and more lightweight? Because we don't want to manage all the complex cases. We know that we are in a good situation. And if we're not, yes, we can fall back, and it's called inflate our monitor. Well, it existed in OpenJDK for ages, and it has been removed from OpenJDK. If it was deprecated, then no one noticed, I believe, because still, are there too many people using something newer than JDK 11? Well, some consequences were noticed probably too late. Well, what are the reasons, first of all? What are the reasons to remove a bus-locking from OpenJDK from hotspot JVM? Well, to ease the implementation of virtual threads, to deliver project loom, to decrease the amount of work there. So some consequences here, initials discovered. In certain cases, things like input streams can be slowed down, like here it's 8x or something. That's enormously slow. And for GraVM, there is a mode that you say during static compilation, OK, this native image doesn't try to work with many cores. It's a single-treaded program. So it's simple, and it works really better in these circumstances. So there is an optimization for that. But you have to know it in advance, then you compile your program. Well, and there is, of course, a runtime option that supports all kinds of situations, and it's complex. So the missing part is in the left lower corner. Well, to dynamically be able to process the situation of sequential access pattern. So we've lamented quite a classical approach to this problem. That helps to, that brings that thing locking to GraVM. The initial idea was operating with object header. So where it already contains a pointer to a FAT monitor object. But it can be treated as well as some words. We can atomically access and put some information there. Probably close to final implementation that we have right now still, or again, uses a pointer because it turned to be not so easy to keep correctness across the whole VM with some memory that you treat as a pointer or as a word depending on the situation. Well, anyway, inside that part of header or inside that special object, we can have 64 bits of information. And we can mark it as a thin log, this is a flag, then we can do it atomically. We can keep the ID of an owner thread, which we can obtain, then we work with threads. And account of recursive logs that we currently hold. That, by the way, means that after a certain amount of recursive logs, we have to inflate the monitor because we can store more information in that part of this work. Yeah. So again, it's a pure Java implementation where we work with some atomic magic and we update this information. What we've got, and the most recent numbers are even better. So we see that effect on exactly that example, the streams. We can speed them up. And even in a very kind of nano-benchmark kind of measurement, you also see the improvement. And even in multi-threaded case, there is now no difference with the original.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.32, "text": " Just two classical optimizations that will help modern but mature virtual machine where", "tokens": [1449, 732, 13735, 5028, 14455, 300, 486, 854, 4363, 457, 14442, 6374, 3479, 689], "temperature": 0.0, "avg_logprob": -0.3919317987230089, "compression_ratio": 1.4196891191709844, "no_speech_prob": 0.1962338536977768}, {"id": 1, "seek": 0, "start": 14.32, "end": 19.92, "text": " we have that powers native images and why is it important?", "tokens": [321, 362, 300, 8674, 8470, 5267, 293, 983, 307, 309, 1021, 30], "temperature": 0.0, "avg_logprob": -0.3919317987230089, "compression_ratio": 1.4196891191709844, "no_speech_prob": 0.1962338536977768}, {"id": 2, "seek": 0, "start": 19.92, "end": 22.92, "text": " Well, and who I am?", "tokens": [1042, 11, 293, 567, 286, 669, 30], "temperature": 0.0, "avg_logprob": -0.3919317987230089, "compression_ratio": 1.4196891191709844, "no_speech_prob": 0.1962338536977768}, {"id": 3, "seek": 0, "start": 22.92, "end": 23.92, "text": " My name is Mito Chukko.", "tokens": [1222, 1315, 307, 376, 3528, 761, 2034, 4093, 13], "temperature": 0.0, "avg_logprob": -0.3919317987230089, "compression_ratio": 1.4196891191709844, "no_speech_prob": 0.1962338536977768}, {"id": 4, "seek": 0, "start": 23.92, "end": 29.76, "text": " I work at a company named Bellsoft which actively participates in OpenJDK community", "tokens": [286, 589, 412, 257, 2237, 4926, 11485, 13908, 597, 13022, 3421, 1024, 294, 7238, 41, 35, 42, 1768], "temperature": 0.0, "avg_logprob": -0.3919317987230089, "compression_ratio": 1.4196891191709844, "no_speech_prob": 0.1962338536977768}, {"id": 5, "seek": 2976, "start": 29.76, "end": 37.32, "text": " and we release our own JDK distribution which you probably met if you have ever built a", "tokens": [293, 321, 4374, 527, 1065, 37082, 42, 7316, 597, 291, 1391, 1131, 498, 291, 362, 1562, 3094, 257], "temperature": 0.0, "avg_logprob": -0.22692835330963135, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00033660486224107444}, {"id": 6, "seek": 2976, "start": 37.32, "end": 40.32, "text": " Spring Boot container with default build pack.", "tokens": [14013, 37263, 10129, 365, 7576, 1322, 2844, 13], "temperature": 0.0, "avg_logprob": -0.22692835330963135, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00033660486224107444}, {"id": 7, "seek": 2976, "start": 40.32, "end": 42.56, "text": " So it's in there.", "tokens": [407, 309, 311, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.22692835330963135, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00033660486224107444}, {"id": 8, "seek": 2976, "start": 42.56, "end": 50.28, "text": " And now Spring Boot, since version 3, supports containers with native images.", "tokens": [400, 586, 14013, 37263, 11, 1670, 3037, 805, 11, 9346, 17089, 365, 8470, 5267, 13], "temperature": 0.0, "avg_logprob": -0.22692835330963135, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00033660486224107444}, {"id": 9, "seek": 2976, "start": 50.28, "end": 56.28, "text": " It can be built as a native image and if you do that, the compiler being used is the American", "tokens": [467, 393, 312, 3094, 382, 257, 8470, 3256, 293, 498, 291, 360, 300, 11, 264, 31958, 885, 1143, 307, 264, 2665], "temperature": 0.0, "avg_logprob": -0.22692835330963135, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00033660486224107444}, {"id": 10, "seek": 5628, "start": 56.28, "end": 62.4, "text": " native image kit which is a Bellsoft distribution of GrowLVM.", "tokens": [8470, 3256, 8260, 597, 307, 257, 11485, 13908, 7316, 295, 18476, 43, 53, 44, 13], "temperature": 0.0, "avg_logprob": -0.180687130622144, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.0002864220296032727}, {"id": 11, "seek": 5628, "start": 62.4, "end": 75.56, "text": " So that's another project that we participate and GrowLVM itself can be seen as different", "tokens": [407, 300, 311, 1071, 1716, 300, 321, 8197, 293, 18476, 43, 53, 44, 2564, 393, 312, 1612, 382, 819], "temperature": 0.0, "avg_logprob": -0.180687130622144, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.0002864220296032727}, {"id": 12, "seek": 5628, "start": 75.56, "end": 80.16, "text": " things at least two major modes that we can absorb.", "tokens": [721, 412, 1935, 732, 2563, 14068, 300, 321, 393, 15631, 13], "temperature": 0.0, "avg_logprob": -0.180687130622144, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.0002864220296032727}, {"id": 13, "seek": 8016, "start": 80.16, "end": 89.44, "text": " It can run as a JIT where compiler is GrowLVM or we can build a native image with a static", "tokens": [467, 393, 1190, 382, 257, 508, 3927, 689, 31958, 307, 18476, 43, 53, 44, 420, 321, 393, 1322, 257, 8470, 3256, 365, 257, 13437], "temperature": 0.0, "avg_logprob": -0.2315219503934266, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0002048000314971432}, {"id": 14, "seek": 8016, "start": 89.44, "end": 101.88, "text": " compilation and it will utilize a special virtual machine substrate VM and here it's", "tokens": [40261, 293, 309, 486, 16117, 257, 2121, 6374, 3479, 27585, 18038, 293, 510, 309, 311], "temperature": 0.0, "avg_logprob": -0.2315219503934266, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0002048000314971432}, {"id": 15, "seek": 8016, "start": 101.88, "end": 108.47999999999999, "text": " different from the traditional Java, traditional way of how we run it.", "tokens": [819, 490, 264, 5164, 10745, 11, 5164, 636, 295, 577, 321, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.2315219503934266, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0002048000314971432}, {"id": 16, "seek": 10848, "start": 108.48, "end": 115.48, "text": " Well, another interesting and peculiar point here is that it is written in Java.", "tokens": [1042, 11, 1071, 1880, 293, 27149, 935, 510, 307, 300, 309, 307, 3720, 294, 10745, 13], "temperature": 0.0, "avg_logprob": -0.11835870458118951, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.00016730056086089462}, {"id": 17, "seek": 10848, "start": 115.48, "end": 124.48, "text": " So it is a complex project but the most of the code is Java and this is beautiful.", "tokens": [407, 309, 307, 257, 3997, 1716, 457, 264, 881, 295, 264, 3089, 307, 10745, 293, 341, 307, 2238, 13], "temperature": 0.0, "avg_logprob": -0.11835870458118951, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.00016730056086089462}, {"id": 18, "seek": 10848, "start": 124.48, "end": 132.52, "text": " So you have a virtual machine and a compiler for JVM languages and Java in particular written", "tokens": [407, 291, 362, 257, 6374, 3479, 293, 257, 31958, 337, 508, 53, 44, 8650, 293, 10745, 294, 1729, 3720], "temperature": 0.0, "avg_logprob": -0.11835870458118951, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.00016730056086089462}, {"id": 19, "seek": 10848, "start": 132.52, "end": 134.24, "text": " in Java.", "tokens": [294, 10745, 13], "temperature": 0.0, "avg_logprob": -0.11835870458118951, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.00016730056086089462}, {"id": 20, "seek": 13424, "start": 134.24, "end": 141.08, "text": " So if you look at Java itself, why is it so beautiful?", "tokens": [407, 498, 291, 574, 412, 10745, 2564, 11, 983, 307, 309, 370, 2238, 30], "temperature": 0.0, "avg_logprob": -0.17939060725522846, "compression_ratio": 1.599078341013825, "no_speech_prob": 8.775083551881835e-05}, {"id": 21, "seek": 13424, "start": 141.08, "end": 144.68, "text": " Well, not so beautiful compared to Kotlin as we know, right?", "tokens": [1042, 11, 406, 370, 2238, 5347, 281, 30123, 5045, 382, 321, 458, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17939060725522846, "compression_ratio": 1.599078341013825, "no_speech_prob": 8.775083551881835e-05}, {"id": 22, "seek": 13424, "start": 144.68, "end": 150.48000000000002, "text": " But still, both Java and Kotlin, they share those concepts.", "tokens": [583, 920, 11, 1293, 10745, 293, 30123, 5045, 11, 436, 2073, 729, 10392, 13], "temperature": 0.0, "avg_logprob": -0.17939060725522846, "compression_ratio": 1.599078341013825, "no_speech_prob": 8.775083551881835e-05}, {"id": 23, "seek": 13424, "start": 150.48000000000002, "end": 156.84, "text": " So from the very beginning, there is a way to write correct parallel programs.", "tokens": [407, 490, 264, 588, 2863, 11, 456, 307, 257, 636, 281, 2464, 3006, 8952, 4268, 13], "temperature": 0.0, "avg_logprob": -0.17939060725522846, "compression_ratio": 1.599078341013825, "no_speech_prob": 8.775083551881835e-05}, {"id": 24, "seek": 13424, "start": 156.84, "end": 161.28, "text": " So then the right parallel programs, we need some means of synchronization or to orchestrate", "tokens": [407, 550, 264, 558, 8952, 4268, 11, 321, 643, 512, 1355, 295, 19331, 2144, 420, 281, 14161, 4404], "temperature": 0.0, "avg_logprob": -0.17939060725522846, "compression_ratio": 1.599078341013825, "no_speech_prob": 8.775083551881835e-05}, {"id": 25, "seek": 16128, "start": 161.28, "end": 168.84, "text": " so our threads, if we share data, most typically we do that.", "tokens": [370, 527, 19314, 11, 498, 321, 2073, 1412, 11, 881, 5850, 321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.21515936729235527, "compression_ratio": 1.6084656084656084, "no_speech_prob": 0.00011557197285583243}, {"id": 26, "seek": 16128, "start": 168.84, "end": 175.92000000000002, "text": " And also it's a managed runtime where we don't have to worry that much about pre-memory", "tokens": [400, 611, 309, 311, 257, 6453, 34474, 689, 321, 500, 380, 362, 281, 3292, 300, 709, 466, 659, 12, 17886, 827], "temperature": 0.0, "avg_logprob": -0.21515936729235527, "compression_ratio": 1.6084656084656084, "no_speech_prob": 0.00011557197285583243}, {"id": 27, "seek": 16128, "start": 175.92000000000002, "end": 184.52, "text": " because we have garbage collection and garbage is collected for us and our programs just,", "tokens": [570, 321, 362, 14150, 5765, 293, 14150, 307, 11087, 337, 505, 293, 527, 4268, 445, 11], "temperature": 0.0, "avg_logprob": -0.21515936729235527, "compression_ratio": 1.6084656084656084, "no_speech_prob": 0.00011557197285583243}, {"id": 28, "seek": 16128, "start": 184.52, "end": 190.48, "text": " they can't have memory leak but you have to work hard to get one.", "tokens": [436, 393, 380, 362, 4675, 17143, 457, 291, 362, 281, 589, 1152, 281, 483, 472, 13], "temperature": 0.0, "avg_logprob": -0.21515936729235527, "compression_ratio": 1.6084656084656084, "no_speech_prob": 0.00011557197285583243}, {"id": 29, "seek": 19048, "start": 190.48, "end": 201.56, "text": " And having that native image implementation makes our final binaries very, sometimes makes", "tokens": [400, 1419, 300, 8470, 3256, 11420, 1669, 527, 2572, 5171, 4889, 588, 11, 2171, 1669], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 30, "seek": 19048, "start": 201.56, "end": 203.67999999999998, "text": " them very performant.", "tokens": [552, 588, 2042, 394, 13], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 31, "seek": 19048, "start": 203.67999999999998, "end": 205.88, "text": " Of course, we have an instant startup.", "tokens": [2720, 1164, 11, 321, 362, 364, 9836, 18578, 13], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 32, "seek": 19048, "start": 205.88, "end": 209.28, "text": " It was mentioned today several times.", "tokens": [467, 390, 2835, 965, 2940, 1413, 13], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 33, "seek": 19048, "start": 209.28, "end": 211.56, "text": " But we can also have a very good peak performance.", "tokens": [583, 321, 393, 611, 362, 257, 588, 665, 10651, 3389, 13], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 34, "seek": 19048, "start": 211.56, "end": 220.12, "text": " In certain cases, that's not a rule but it can happen, like it happens here on this plot.", "tokens": [682, 1629, 3331, 11, 300, 311, 406, 257, 4978, 457, 309, 393, 1051, 11, 411, 309, 2314, 510, 322, 341, 7542, 13], "temperature": 0.0, "avg_logprob": -0.19732086917003952, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0007169221644289792}, {"id": 35, "seek": 22012, "start": 220.12, "end": 227.44, "text": " That's just a simple spring boot application and we just ping the same endpoint.", "tokens": [663, 311, 445, 257, 2199, 5587, 11450, 3861, 293, 321, 445, 26151, 264, 912, 35795, 13], "temperature": 0.0, "avg_logprob": -0.16446165197035845, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.0005673696869052947}, {"id": 36, "seek": 22012, "start": 227.44, "end": 234.4, "text": " And here the native image works better and also it warms up instantly and it has very", "tokens": [400, 510, 264, 8470, 3256, 1985, 1101, 293, 611, 309, 1516, 2592, 493, 13518, 293, 309, 575, 588], "temperature": 0.0, "avg_logprob": -0.16446165197035845, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.0005673696869052947}, {"id": 37, "seek": 22012, "start": 234.4, "end": 235.96, "text": " good latency.", "tokens": [665, 27043, 13], "temperature": 0.0, "avg_logprob": -0.16446165197035845, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.0005673696869052947}, {"id": 38, "seek": 22012, "start": 235.96, "end": 241.64000000000001, "text": " So for this small amount of memory that it takes, so this is a small service, it takes", "tokens": [407, 337, 341, 1359, 2372, 295, 4675, 300, 309, 2516, 11, 370, 341, 307, 257, 1359, 2643, 11, 309, 2516], "temperature": 0.0, "avg_logprob": -0.16446165197035845, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.0005673696869052947}, {"id": 39, "seek": 22012, "start": 241.64000000000001, "end": 246.88, "text": " small amount of memory, very small heap, and it also has low latency.", "tokens": [1359, 2372, 295, 4675, 11, 588, 1359, 33591, 11, 293, 309, 611, 575, 2295, 27043, 13], "temperature": 0.0, "avg_logprob": -0.16446165197035845, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.0005673696869052947}, {"id": 40, "seek": 24688, "start": 246.88, "end": 255.51999999999998, "text": " And under the hood, it uses, well, serial GC and we'll talk about that later.", "tokens": [400, 833, 264, 13376, 11, 309, 4960, 11, 731, 11, 17436, 29435, 293, 321, 603, 751, 466, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.2777067565917969, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0007126020500436425}, {"id": 41, "seek": 24688, "start": 255.51999999999998, "end": 261.04, "text": " Well, what about relationship between Graal VM and OpenJDK?", "tokens": [1042, 11, 437, 466, 2480, 1296, 8985, 304, 18038, 293, 7238, 41, 35, 42, 30], "temperature": 0.0, "avg_logprob": -0.2777067565917969, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0007126020500436425}, {"id": 42, "seek": 24688, "start": 261.04, "end": 271.15999999999997, "text": " Well, we're here in a Friends of OpenJDK room and Graal has been integrated as an additional", "tokens": [1042, 11, 321, 434, 510, 294, 257, 14042, 295, 7238, 41, 35, 42, 1808, 293, 8985, 304, 575, 668, 10919, 382, 364, 4497], "temperature": 0.0, "avg_logprob": -0.2777067565917969, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0007126020500436425}, {"id": 43, "seek": 24688, "start": 271.15999999999997, "end": 275.0, "text": " experimental compiler in JDK9.", "tokens": [17069, 31958, 294, 37082, 42, 24, 13], "temperature": 0.0, "avg_logprob": -0.2777067565917969, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.0007126020500436425}, {"id": 44, "seek": 27500, "start": 275.0, "end": 281.36, "text": " But while it has been removed from recent JDKs, but what's the left over?", "tokens": [583, 1339, 309, 575, 668, 7261, 490, 5162, 37082, 42, 82, 11, 457, 437, 311, 264, 1411, 670, 30], "temperature": 0.0, "avg_logprob": -0.15930115279331003, "compression_ratio": 1.5363636363636364, "no_speech_prob": 5.6663651776034385e-05}, {"id": 45, "seek": 27500, "start": 281.36, "end": 283.8, "text": " It's an interface to plug it in.", "tokens": [467, 311, 364, 9226, 281, 5452, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.15930115279331003, "compression_ratio": 1.5363636363636364, "no_speech_prob": 5.6663651776034385e-05}, {"id": 46, "seek": 27500, "start": 283.8, "end": 289.36, "text": " So now it's going to be a second attempt to do that.", "tokens": [407, 586, 309, 311, 516, 281, 312, 257, 1150, 5217, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15930115279331003, "compression_ratio": 1.5363636363636364, "no_speech_prob": 5.6663651776034385e-05}, {"id": 47, "seek": 27500, "start": 289.36, "end": 294.48, "text": " So here on slides it's mentioned that there is a discussion about project, new project", "tokens": [407, 510, 322, 9788, 309, 311, 2835, 300, 456, 307, 257, 5017, 466, 1716, 11, 777, 1716], "temperature": 0.0, "avg_logprob": -0.15930115279331003, "compression_ratio": 1.5363636363636364, "no_speech_prob": 5.6663651776034385e-05}, {"id": 48, "seek": 27500, "start": 294.48, "end": 302.56, "text": " they all had, but last week it was already called for votes in OpenJDK to start the project", "tokens": [436, 439, 632, 11, 457, 1036, 1243, 309, 390, 1217, 1219, 337, 12068, 294, 7238, 41, 35, 42, 281, 722, 264, 1716], "temperature": 0.0, "avg_logprob": -0.15930115279331003, "compression_ratio": 1.5363636363636364, "no_speech_prob": 5.6663651776034385e-05}, {"id": 49, "seek": 30256, "start": 302.56, "end": 315.08, "text": " of bringing the most sweet parts of this technology into OpenJDK, back into OpenJDK.", "tokens": [295, 5062, 264, 881, 3844, 3166, 295, 341, 2899, 666, 7238, 41, 35, 42, 11, 646, 666, 7238, 41, 35, 42, 13], "temperature": 0.0, "avg_logprob": -0.17009028343305196, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.9286728578154e-05}, {"id": 50, "seek": 30256, "start": 315.08, "end": 318.04, "text": " It's something that happens right now.", "tokens": [467, 311, 746, 300, 2314, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.17009028343305196, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.9286728578154e-05}, {"id": 51, "seek": 30256, "start": 318.04, "end": 324.2, "text": " So that default garbage collector that sometimes shows very good latency even compared to ParallelGC", "tokens": [407, 300, 7576, 14150, 23960, 300, 2171, 3110, 588, 665, 27043, 754, 5347, 281, 3457, 336, 338, 38, 34], "temperature": 0.0, "avg_logprob": -0.17009028343305196, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.9286728578154e-05}, {"id": 52, "seek": 30256, "start": 324.2, "end": 329.2, "text": " or G1 in hotspot, well, on small heaps.", "tokens": [420, 460, 16, 294, 36121, 17698, 11, 731, 11, 322, 1359, 415, 2382, 13], "temperature": 0.0, "avg_logprob": -0.17009028343305196, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.9286728578154e-05}, {"id": 53, "seek": 32920, "start": 329.2, "end": 335.24, "text": " Well, it's a kind of garbage collector we can easily understand.", "tokens": [1042, 11, 309, 311, 257, 733, 295, 14150, 23960, 321, 393, 3612, 1223, 13], "temperature": 0.0, "avg_logprob": -0.21950174478384166, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.48611385258846e-05}, {"id": 54, "seek": 32920, "start": 335.24, "end": 337.88, "text": " And it's generational stop the world collection.", "tokens": [400, 309, 311, 48320, 1590, 264, 1002, 5765, 13], "temperature": 0.0, "avg_logprob": -0.21950174478384166, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.48611385258846e-05}, {"id": 55, "seek": 32920, "start": 337.88, "end": 347.12, "text": " So here only one survivor space, but actually it's 16 by default.", "tokens": [407, 510, 787, 472, 25953, 1901, 11, 457, 767, 309, 311, 3165, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.21950174478384166, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.48611385258846e-05}, {"id": 56, "seek": 32920, "start": 347.12, "end": 355.76, "text": " But anyway, so we stop all our application threads and we collect garbage in a single", "tokens": [583, 4033, 11, 370, 321, 1590, 439, 527, 3861, 19314, 293, 321, 2500, 14150, 294, 257, 2167], "temperature": 0.0, "avg_logprob": -0.21950174478384166, "compression_ratio": 1.4722222222222223, "no_speech_prob": 6.48611385258846e-05}, {"id": 57, "seek": 35576, "start": 355.76, "end": 363.0, "text": " thread, so this is a kind of a basic garbage collector, right, but from the other hand", "tokens": [7207, 11, 370, 341, 307, 257, 733, 295, 257, 3875, 14150, 23960, 11, 558, 11, 457, 490, 264, 661, 1011], "temperature": 0.0, "avg_logprob": -0.16329349706202378, "compression_ratio": 1.5343137254901962, "no_speech_prob": 6.479138392023742e-05}, {"id": 58, "seek": 35576, "start": 363.0, "end": 370.28, "text": " it's reliable and it's very effective, especially if you have only a single core available.", "tokens": [309, 311, 12924, 293, 309, 311, 588, 4942, 11, 2318, 498, 291, 362, 787, 257, 2167, 4965, 2435, 13], "temperature": 0.0, "avg_logprob": -0.16329349706202378, "compression_ratio": 1.5343137254901962, "no_speech_prob": 6.479138392023742e-05}, {"id": 59, "seek": 35576, "start": 370.28, "end": 372.8, "text": " So you see the problem.", "tokens": [407, 291, 536, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16329349706202378, "compression_ratio": 1.5343137254901962, "no_speech_prob": 6.479138392023742e-05}, {"id": 60, "seek": 35576, "start": 372.8, "end": 380.28, "text": " We have some CPU which may be enough to run many threads, but we run only one at least", "tokens": [492, 362, 512, 13199, 597, 815, 312, 1547, 281, 1190, 867, 19314, 11, 457, 321, 1190, 787, 472, 412, 1935], "temperature": 0.0, "avg_logprob": -0.16329349706202378, "compression_ratio": 1.5343137254901962, "no_speech_prob": 6.479138392023742e-05}, {"id": 61, "seek": 35576, "start": 380.28, "end": 381.28, "text": " for garbage collection.", "tokens": [337, 14150, 5765, 13], "temperature": 0.0, "avg_logprob": -0.16329349706202378, "compression_ratio": 1.5343137254901962, "no_speech_prob": 6.479138392023742e-05}, {"id": 62, "seek": 38128, "start": 381.28, "end": 386.91999999999996, "text": " Now garbage collection can take significant time during our application execution, well,", "tokens": [823, 14150, 5765, 393, 747, 4776, 565, 1830, 527, 3861, 15058, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.2211171674056792, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.075894553854596e-05}, {"id": 63, "seek": 38128, "start": 386.91999999999996, "end": 388.91999999999996, "text": " that's obvious.", "tokens": [300, 311, 6322, 13], "temperature": 0.0, "avg_logprob": -0.2211171674056792, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.075894553854596e-05}, {"id": 64, "seek": 38128, "start": 388.91999999999996, "end": 391.64, "text": " Well, what would we do?", "tokens": [1042, 11, 437, 576, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.2211171674056792, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.075894553854596e-05}, {"id": 65, "seek": 38128, "start": 391.64, "end": 399.79999999999995, "text": " Of course, we would like to do exactly the same thing, but in parallel, to decrease the", "tokens": [2720, 1164, 11, 321, 576, 411, 281, 360, 2293, 264, 912, 551, 11, 457, 294, 8952, 11, 281, 11514, 264], "temperature": 0.0, "avg_logprob": -0.2211171674056792, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.075894553854596e-05}, {"id": 66, "seek": 38128, "start": 399.79999999999995, "end": 406.35999999999996, "text": " time garbage collection takes to reduce the garbage collection pause, because it still", "tokens": [565, 14150, 5765, 2516, 281, 5407, 264, 14150, 5765, 10465, 11, 570, 309, 920], "temperature": 0.0, "avg_logprob": -0.2211171674056792, "compression_ratio": 1.6467391304347827, "no_speech_prob": 2.075894553854596e-05}, {"id": 67, "seek": 40636, "start": 406.36, "end": 411.88, "text": " stopped the world pause, but we reduce it because we process data with multiple threads.", "tokens": [5936, 264, 1002, 10465, 11, 457, 321, 5407, 309, 570, 321, 1399, 1412, 365, 3866, 19314, 13], "temperature": 0.0, "avg_logprob": -0.1623290018601851, "compression_ratio": 1.6096491228070176, "no_speech_prob": 7.125056436052546e-05}, {"id": 68, "seek": 40636, "start": 411.88, "end": 414.28000000000003, "text": " So that's the idea of parallel garbage collection.", "tokens": [407, 300, 311, 264, 1558, 295, 8952, 14150, 5765, 13], "temperature": 0.0, "avg_logprob": -0.1623290018601851, "compression_ratio": 1.6096491228070176, "no_speech_prob": 7.125056436052546e-05}, {"id": 69, "seek": 40636, "start": 414.28000000000003, "end": 420.96000000000004, "text": " The idea is not new, but surprisingly, this modern runtime doesn't have it yet.", "tokens": [440, 1558, 307, 406, 777, 11, 457, 17600, 11, 341, 4363, 34474, 1177, 380, 362, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1623290018601851, "compression_ratio": 1.6096491228070176, "no_speech_prob": 7.125056436052546e-05}, {"id": 70, "seek": 40636, "start": 420.96000000000004, "end": 428.88, "text": " Well, we decided to implement it and it's still being under review and some implementation", "tokens": [1042, 11, 321, 3047, 281, 4445, 309, 293, 309, 311, 920, 885, 833, 3131, 293, 512, 11420], "temperature": 0.0, "avg_logprob": -0.1623290018601851, "compression_ratio": 1.6096491228070176, "no_speech_prob": 7.125056436052546e-05}, {"id": 71, "seek": 40636, "start": 428.88, "end": 435.56, "text": " details, well, they change, but the idea is very simple.", "tokens": [4365, 11, 731, 11, 436, 1319, 11, 457, 264, 1558, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1623290018601851, "compression_ratio": 1.6096491228070176, "no_speech_prob": 7.125056436052546e-05}, {"id": 72, "seek": 43556, "start": 435.56, "end": 442.68, "text": " You just say, pass the garbage collection selection during the creation of your native", "tokens": [509, 445, 584, 11, 1320, 264, 14150, 5765, 9450, 1830, 264, 8016, 295, 428, 8470], "temperature": 0.0, "avg_logprob": -0.20977135708457545, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00011213586549274623}, {"id": 73, "seek": 43556, "start": 442.68, "end": 443.68, "text": " image.", "tokens": [3256, 13], "temperature": 0.0, "avg_logprob": -0.20977135708457545, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00011213586549274623}, {"id": 74, "seek": 43556, "start": 443.68, "end": 450.0, "text": " For instance, if you use some Maven or Gradle configuration for your Spring Boot container,", "tokens": [1171, 5197, 11, 498, 291, 764, 512, 4042, 553, 420, 16710, 306, 11694, 337, 428, 14013, 37263, 10129, 11], "temperature": 0.0, "avg_logprob": -0.20977135708457545, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00011213586549274623}, {"id": 75, "seek": 43556, "start": 450.0, "end": 452.04, "text": " you also can do that.", "tokens": [291, 611, 393, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20977135708457545, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00011213586549274623}, {"id": 76, "seek": 43556, "start": 452.04, "end": 462.2, "text": " And then you have some GRIPS in runtime, which you also can twist when you run your application.", "tokens": [400, 550, 291, 362, 512, 460, 5577, 6273, 294, 34474, 11, 597, 291, 611, 393, 8203, 562, 291, 1190, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.20977135708457545, "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.00011213586549274623}, {"id": 77, "seek": 46220, "start": 462.2, "end": 466.68, "text": " And well, you enable that implementation.", "tokens": [400, 731, 11, 291, 9528, 300, 11420, 13], "temperature": 0.0, "avg_logprob": -0.19869565963745117, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.8577854613540694e-05}, {"id": 78, "seek": 46220, "start": 466.68, "end": 473.28, "text": " I'll show some performance results later, but basically the implementation itself, well,", "tokens": [286, 603, 855, 512, 3389, 3542, 1780, 11, 457, 1936, 264, 11420, 2564, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.19869565963745117, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.8577854613540694e-05}, {"id": 79, "seek": 46220, "start": 473.28, "end": 480.2, "text": " it can be analyzed as a change in a big Java program, which Brawl VM is.", "tokens": [309, 393, 312, 28181, 382, 257, 1319, 294, 257, 955, 10745, 1461, 11, 597, 4991, 39192, 18038, 307, 13], "temperature": 0.0, "avg_logprob": -0.19869565963745117, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.8577854613540694e-05}, {"id": 80, "seek": 46220, "start": 480.2, "end": 487.44, "text": " And there are now two GC interfaces and implementations.", "tokens": [400, 456, 366, 586, 732, 29435, 28416, 293, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.19869565963745117, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.8577854613540694e-05}, {"id": 81, "seek": 48744, "start": 487.44, "end": 498.48, "text": " And this functionality just re-use existing things in a very, I would say, smart way just", "tokens": [400, 341, 14980, 445, 319, 12, 438, 6741, 721, 294, 257, 588, 11, 286, 576, 584, 11, 4069, 636, 445], "temperature": 0.0, "avg_logprob": -0.1806763106701421, "compression_ratio": 1.3263888888888888, "no_speech_prob": 6.410266360035166e-05}, {"id": 82, "seek": 48744, "start": 498.48, "end": 505.96, "text": " to keep what is all about the parallelization as a code.", "tokens": [281, 1066, 437, 307, 439, 466, 264, 8952, 2144, 382, 257, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1806763106701421, "compression_ratio": 1.3263888888888888, "no_speech_prob": 6.410266360035166e-05}, {"id": 83, "seek": 48744, "start": 505.96, "end": 513.48, "text": " So everything else is reused from serial GC.", "tokens": [407, 1203, 1646, 307, 319, 4717, 490, 17436, 29435, 13], "temperature": 0.0, "avg_logprob": -0.1806763106701421, "compression_ratio": 1.3263888888888888, "no_speech_prob": 6.410266360035166e-05}, {"id": 84, "seek": 51348, "start": 513.48, "end": 519.84, "text": " Basically there's a problem of how do we synchronize and share the work?", "tokens": [8537, 456, 311, 257, 1154, 295, 577, 360, 321, 19331, 1125, 293, 2073, 264, 589, 30], "temperature": 0.0, "avg_logprob": -0.15953731536865234, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00011189541692147031}, {"id": 85, "seek": 51348, "start": 519.84, "end": 527.9200000000001, "text": " Because parallel threads for garbage collection, they also have the same problem because they", "tokens": [1436, 8952, 19314, 337, 14150, 5765, 11, 436, 611, 362, 264, 912, 1154, 570, 436], "temperature": 0.0, "avg_logprob": -0.15953731536865234, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00011189541692147031}, {"id": 86, "seek": 51348, "start": 527.9200000000001, "end": 534.76, "text": " work on the same data, so they have contention or may have contention.", "tokens": [589, 322, 264, 912, 1412, 11, 370, 436, 362, 660, 1251, 420, 815, 362, 660, 1251, 13], "temperature": 0.0, "avg_logprob": -0.15953731536865234, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00011189541692147031}, {"id": 87, "seek": 51348, "start": 534.76, "end": 539.12, "text": " So we need to share in some smart manner.", "tokens": [407, 321, 643, 281, 2073, 294, 512, 4069, 9060, 13], "temperature": 0.0, "avg_logprob": -0.15953731536865234, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00011189541692147031}, {"id": 88, "seek": 53912, "start": 539.12, "end": 546.36, "text": " Well, it's implemented with a work divided in its volume.", "tokens": [1042, 11, 309, 311, 12270, 365, 257, 589, 6666, 294, 1080, 5523, 13], "temperature": 0.0, "avg_logprob": -0.13974192235376928, "compression_ratio": 1.543956043956044, "no_speech_prob": 7.172565528890118e-05}, {"id": 89, "seek": 53912, "start": 546.36, "end": 554.8, "text": " So every thread operates its local memory, and it's a chunk of memory of one megabyte.", "tokens": [407, 633, 7207, 22577, 1080, 2654, 4675, 11, 293, 309, 311, 257, 16635, 295, 4675, 295, 472, 10816, 34529, 13], "temperature": 0.0, "avg_logprob": -0.13974192235376928, "compression_ratio": 1.543956043956044, "no_speech_prob": 7.172565528890118e-05}, {"id": 90, "seek": 53912, "start": 554.8, "end": 561.96, "text": " So if we need an extra memory, like we scan objects and we fulfill some set of data that", "tokens": [407, 498, 321, 643, 364, 2857, 4675, 11, 411, 321, 11049, 6565, 293, 321, 13875, 512, 992, 295, 1412, 300], "temperature": 0.0, "avg_logprob": -0.13974192235376928, "compression_ratio": 1.543956043956044, "no_speech_prob": 7.172565528890118e-05}, {"id": 91, "seek": 53912, "start": 561.96, "end": 563.8, "text": " we operate on.", "tokens": [321, 9651, 322, 13], "temperature": 0.0, "avg_logprob": -0.13974192235376928, "compression_ratio": 1.543956043956044, "no_speech_prob": 7.172565528890118e-05}, {"id": 92, "seek": 53912, "start": 563.8, "end": 565.28, "text": " And then we have an extra chunk.", "tokens": [400, 550, 321, 362, 364, 2857, 16635, 13], "temperature": 0.0, "avg_logprob": -0.13974192235376928, "compression_ratio": 1.543956043956044, "no_speech_prob": 7.172565528890118e-05}, {"id": 93, "seek": 56528, "start": 565.28, "end": 571.12, "text": " We can just put it aside so someone else can pick it.", "tokens": [492, 393, 445, 829, 309, 7359, 370, 1580, 1646, 393, 1888, 309, 13], "temperature": 0.0, "avg_logprob": -0.12234642194664996, "compression_ratio": 1.560693641618497, "no_speech_prob": 8.188905485440046e-05}, {"id": 94, "seek": 56528, "start": 571.12, "end": 576.48, "text": " So that's the stack that contains the chunks of work.", "tokens": [407, 300, 311, 264, 8630, 300, 8306, 264, 24004, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.12234642194664996, "compression_ratio": 1.560693641618497, "no_speech_prob": 8.188905485440046e-05}, {"id": 95, "seek": 56528, "start": 576.48, "end": 584.36, "text": " And then the work is finished, the thread just takes the next chunk of work.", "tokens": [400, 550, 264, 589, 307, 4335, 11, 264, 7207, 445, 2516, 264, 958, 16635, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.12234642194664996, "compression_ratio": 1.560693641618497, "no_speech_prob": 8.188905485440046e-05}, {"id": 96, "seek": 56528, "start": 584.36, "end": 594.28, "text": " There may be a situation when several threads try to copy to promote the same object.", "tokens": [821, 815, 312, 257, 2590, 562, 2940, 19314, 853, 281, 5055, 281, 9773, 264, 912, 2657, 13], "temperature": 0.0, "avg_logprob": -0.12234642194664996, "compression_ratio": 1.560693641618497, "no_speech_prob": 8.188905485440046e-05}, {"id": 97, "seek": 59428, "start": 594.28, "end": 596.4, "text": " And this is actually solved very simply.", "tokens": [400, 341, 307, 767, 13041, 588, 2935, 13], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 98, "seek": 59428, "start": 596.4, "end": 602.04, "text": " They just reserve some space for the object and then tries to install forward pointer", "tokens": [814, 445, 17824, 512, 1901, 337, 264, 2657, 293, 550, 9898, 281, 3625, 2128, 23918], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 99, "seek": 59428, "start": 602.04, "end": 605.48, "text": " using an atomic operation.", "tokens": [1228, 364, 22275, 6916, 13], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 100, "seek": 59428, "start": 605.48, "end": 611.16, "text": " And as this is an atomic operation, only one thread succeeds, so others just roll back", "tokens": [400, 382, 341, 307, 364, 22275, 6916, 11, 787, 472, 7207, 49263, 11, 370, 2357, 445, 3373, 646], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 101, "seek": 59428, "start": 611.16, "end": 616.28, "text": " and this is a lightweight operation.", "tokens": [293, 341, 307, 257, 22052, 6916, 13], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 102, "seek": 59428, "start": 616.28, "end": 618.28, "text": " Again this is Java.", "tokens": [3764, 341, 307, 10745, 13], "temperature": 0.0, "avg_logprob": -0.22463528315226236, "compression_ratio": 1.6408839779005524, "no_speech_prob": 5.280787809169851e-05}, {"id": 103, "seek": 61828, "start": 618.28, "end": 627.4399999999999, "text": " This is not a strict AML, sorry, but still all existing places that manage memory were", "tokens": [639, 307, 406, 257, 10910, 6475, 43, 11, 2597, 11, 457, 920, 439, 6741, 3190, 300, 3067, 4675, 645], "temperature": 0.0, "avg_logprob": -0.23042186302474782, "compression_ratio": 1.5069767441860464, "no_speech_prob": 4.391741458675824e-05}, {"id": 104, "seek": 61828, "start": 627.4399999999999, "end": 631.4, "text": " reused without changing the architecture of Growl itself.", "tokens": [319, 4717, 1553, 4473, 264, 9482, 295, 18476, 75, 2564, 13], "temperature": 0.0, "avg_logprob": -0.23042186302474782, "compression_ratio": 1.5069767441860464, "no_speech_prob": 4.391741458675824e-05}, {"id": 105, "seek": 61828, "start": 631.4, "end": 635.8399999999999, "text": " So there are already possibilities to add garbage collectors.", "tokens": [407, 456, 366, 1217, 12178, 281, 909, 14150, 35384, 13], "temperature": 0.0, "avg_logprob": -0.23042186302474782, "compression_ratio": 1.5069767441860464, "no_speech_prob": 4.391741458675824e-05}, {"id": 106, "seek": 61828, "start": 635.8399999999999, "end": 640.56, "text": " So if you want to implement one, it's not that complex.", "tokens": [407, 498, 291, 528, 281, 4445, 472, 11, 309, 311, 406, 300, 3997, 13], "temperature": 0.0, "avg_logprob": -0.23042186302474782, "compression_ratio": 1.5069767441860464, "no_speech_prob": 4.391741458675824e-05}, {"id": 107, "seek": 61828, "start": 640.56, "end": 647.4399999999999, "text": " The major problem is to be correct when you deal with memory.", "tokens": [440, 2563, 1154, 307, 281, 312, 3006, 562, 291, 2028, 365, 4675, 13], "temperature": 0.0, "avg_logprob": -0.23042186302474782, "compression_ratio": 1.5069767441860464, "no_speech_prob": 4.391741458675824e-05}, {"id": 108, "seek": 64744, "start": 647.44, "end": 655.4000000000001, "text": " When you deal with concurrency, and then you inject your code into this virtual machine", "tokens": [1133, 291, 2028, 365, 23702, 10457, 11, 293, 550, 291, 10711, 428, 3089, 666, 341, 6374, 3479], "temperature": 0.0, "avg_logprob": -0.16680087465228458, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.00027362603577785194}, {"id": 109, "seek": 64744, "start": 655.4000000000001, "end": 662.4000000000001, "text": " because it's all declarative magic that requires you to be careful.", "tokens": [570, 309, 311, 439, 16694, 1166, 5585, 300, 7029, 291, 281, 312, 5026, 13], "temperature": 0.0, "avg_logprob": -0.16680087465228458, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.00027362603577785194}, {"id": 110, "seek": 64744, "start": 662.4000000000001, "end": 666.44, "text": " Well, some performance results.", "tokens": [1042, 11, 512, 3389, 3542, 13], "temperature": 0.0, "avg_logprob": -0.16680087465228458, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.00027362603577785194}, {"id": 111, "seek": 64744, "start": 666.44, "end": 675.6, "text": " With relatively large heaps with serial GC, you can have pauses of several seconds, which", "tokens": [2022, 7226, 2416, 415, 2382, 365, 17436, 29435, 11, 291, 393, 362, 2502, 8355, 295, 2940, 3949, 11, 597], "temperature": 0.0, "avg_logprob": -0.16680087465228458, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.00027362603577785194}, {"id": 112, "seek": 67560, "start": 675.6, "end": 678.2, "text": " is long, of course.", "tokens": [307, 938, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 113, "seek": 67560, "start": 678.2, "end": 684.2, "text": " And there's a big difference if you have a two or three or four second pause or if you", "tokens": [400, 456, 311, 257, 955, 2649, 498, 291, 362, 257, 732, 420, 1045, 420, 1451, 1150, 10465, 420, 498, 291], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 114, "seek": 67560, "start": 684.2, "end": 686.2, "text": " decrease it by one second.", "tokens": [11514, 309, 538, 472, 1150, 13], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 115, "seek": 67560, "start": 686.2, "end": 692.0, "text": " So that's possible with this implementation already.", "tokens": [407, 300, 311, 1944, 365, 341, 11420, 1217, 13], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 116, "seek": 67560, "start": 692.0, "end": 696.32, "text": " So that's the order of this improvement.", "tokens": [407, 300, 311, 264, 1668, 295, 341, 10444, 13], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 117, "seek": 67560, "start": 696.32, "end": 703.28, "text": " With another benchmark, hyperalogue, you see that latency here, latency of pauses can be", "tokens": [2022, 1071, 18927, 11, 9848, 304, 7213, 11, 291, 536, 300, 27043, 510, 11, 27043, 295, 2502, 8355, 393, 312], "temperature": 0.0, "avg_logprob": -0.17642854508899508, "compression_ratio": 1.58, "no_speech_prob": 4.9016784032573923e-05}, {"id": 118, "seek": 70328, "start": 703.28, "end": 707.28, "text": " decreased like two times.", "tokens": [24436, 411, 732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.2669820207538027, "compression_ratio": 1.4331210191082802, "no_speech_prob": 0.00012225999671500176}, {"id": 119, "seek": 70328, "start": 707.28, "end": 715.04, "text": " Those pauses are not that big, and we have frequent collections here, so x-axis is epoch,", "tokens": [3950, 2502, 8355, 366, 406, 300, 955, 11, 293, 321, 362, 18004, 16641, 510, 11, 370, 2031, 12, 24633, 307, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.2669820207538027, "compression_ratio": 1.4331210191082802, "no_speech_prob": 0.00012225999671500176}, {"id": 120, "seek": 70328, "start": 715.04, "end": 726.04, "text": " so each point is a garbage collection, and y-axis is time in, I believe, milliseconds.", "tokens": [370, 1184, 935, 307, 257, 14150, 5765, 11, 293, 288, 12, 24633, 307, 565, 294, 11, 286, 1697, 11, 34184, 13], "temperature": 0.0, "avg_logprob": -0.2669820207538027, "compression_ratio": 1.4331210191082802, "no_speech_prob": 0.00012225999671500176}, {"id": 121, "seek": 70328, "start": 726.04, "end": 729.8, "text": " Well, that's paralogy.", "tokens": [1042, 11, 300, 311, 26009, 7794, 13], "temperature": 0.0, "avg_logprob": -0.2669820207538027, "compression_ratio": 1.4331210191082802, "no_speech_prob": 0.00012225999671500176}, {"id": 122, "seek": 72980, "start": 729.8, "end": 738.12, "text": " So we can obviously improve many applications and many installations where we have an option", "tokens": [407, 321, 393, 2745, 3470, 867, 5821, 293, 867, 41932, 689, 321, 362, 364, 3614], "temperature": 0.0, "avg_logprob": -0.1314226531982422, "compression_ratio": 1.4607843137254901, "no_speech_prob": 3.5723547625821084e-05}, {"id": 123, "seek": 72980, "start": 738.12, "end": 740.88, "text": " to use several CPUs.", "tokens": [281, 764, 2940, 13199, 82, 13], "temperature": 0.0, "avg_logprob": -0.1314226531982422, "compression_ratio": 1.4607843137254901, "no_speech_prob": 3.5723547625821084e-05}, {"id": 124, "seek": 72980, "start": 740.88, "end": 745.88, "text": " If we use one CPU, of course, we won't see much difference.", "tokens": [759, 321, 764, 472, 13199, 11, 295, 1164, 11, 321, 1582, 380, 536, 709, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1314226531982422, "compression_ratio": 1.4607843137254901, "no_speech_prob": 3.5723547625821084e-05}, {"id": 125, "seek": 72980, "start": 745.88, "end": 752.92, "text": " There is some increase in memory used for service needs, but that's kind of moderate.", "tokens": [821, 307, 512, 3488, 294, 4675, 1143, 337, 2643, 2203, 11, 457, 300, 311, 733, 295, 18174, 13], "temperature": 0.0, "avg_logprob": -0.1314226531982422, "compression_ratio": 1.4607843137254901, "no_speech_prob": 3.5723547625821084e-05}, {"id": 126, "seek": 72980, "start": 752.92, "end": 757.24, "text": " So other parts of this complex system.", "tokens": [407, 661, 3166, 295, 341, 3997, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1314226531982422, "compression_ratio": 1.4607843137254901, "no_speech_prob": 3.5723547625821084e-05}, {"id": 127, "seek": 75724, "start": 757.24, "end": 765.36, "text": " I mentioned synchronization, and, well, synchronization is useful, but it has tradeoffs.", "tokens": [286, 2835, 19331, 2144, 11, 293, 11, 731, 11, 19331, 2144, 307, 4420, 11, 457, 309, 575, 4923, 19231, 13], "temperature": 0.0, "avg_logprob": -0.1292419221666124, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0001629946636967361}, {"id": 128, "seek": 75724, "start": 765.36, "end": 771.2, "text": " Because if we implement the non-synchronization, we need to save our CPU resources to put aside", "tokens": [1436, 498, 321, 4445, 264, 2107, 12, 82, 36420, 2144, 11, 321, 643, 281, 3155, 527, 13199, 3593, 281, 829, 7359], "temperature": 0.0, "avg_logprob": -0.1292419221666124, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0001629946636967361}, {"id": 129, "seek": 75724, "start": 771.2, "end": 774.5600000000001, "text": " threads that won't get the resource.", "tokens": [19314, 300, 1582, 380, 483, 264, 7684, 13], "temperature": 0.0, "avg_logprob": -0.1292419221666124, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0001629946636967361}, {"id": 130, "seek": 75724, "start": 774.5600000000001, "end": 782.72, "text": " We need to stop them, to queue them, to manage that queues, to wake them up, to involve operating", "tokens": [492, 643, 281, 1590, 552, 11, 281, 18639, 552, 11, 281, 3067, 300, 631, 1247, 11, 281, 6634, 552, 493, 11, 281, 9494, 7447], "temperature": 0.0, "avg_logprob": -0.1292419221666124, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0001629946636967361}, {"id": 131, "seek": 75724, "start": 782.72, "end": 785.04, "text": " system in that process.", "tokens": [1185, 294, 300, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1292419221666124, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0001629946636967361}, {"id": 132, "seek": 78504, "start": 785.04, "end": 792.28, "text": " So that's not cheap, but there are situations that, that's another queue, right?", "tokens": [407, 300, 311, 406, 7084, 11, 457, 456, 366, 6851, 300, 11, 300, 311, 1071, 18639, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19217365317874485, "compression_ratio": 1.546875, "no_speech_prob": 4.8704307118896395e-05}, {"id": 133, "seek": 78504, "start": 792.28, "end": 799.68, "text": " And that even influences the design of standard library, because, like, we all know string", "tokens": [400, 300, 754, 21222, 264, 1715, 295, 3832, 6405, 11, 570, 11, 411, 11, 321, 439, 458, 6798], "temperature": 0.0, "avg_logprob": -0.19217365317874485, "compression_ratio": 1.546875, "no_speech_prob": 4.8704307118896395e-05}, {"id": 134, "seek": 78504, "start": 799.68, "end": 802.48, "text": " buffer and string builder, right?", "tokens": [21762, 293, 6798, 27377, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19217365317874485, "compression_ratio": 1.546875, "no_speech_prob": 4.8704307118896395e-05}, {"id": 135, "seek": 78504, "start": 802.48, "end": 808.88, "text": " One class appeared because, well, another one wasn't very pleasant in terms of performance.", "tokens": [1485, 1508, 8516, 570, 11, 731, 11, 1071, 472, 2067, 380, 588, 16232, 294, 2115, 295, 3389, 13], "temperature": 0.0, "avg_logprob": -0.19217365317874485, "compression_ratio": 1.546875, "no_speech_prob": 4.8704307118896395e-05}, {"id": 136, "seek": 80888, "start": 808.88, "end": 815.28, "text": " Yeah, we need it sometimes, but in many cases, we need a non-synchronized implementation,", "tokens": [865, 11, 321, 643, 309, 2171, 11, 457, 294, 867, 3331, 11, 321, 643, 257, 2107, 12, 82, 36420, 1602, 11420, 11], "temperature": 0.0, "avg_logprob": -0.17369420187813894, "compression_ratio": 1.671875, "no_speech_prob": 4.486018588067964e-05}, {"id": 137, "seek": 80888, "start": 815.28, "end": 820.12, "text": " saying, like, hash table and hash map, whoever uses hash table, right?", "tokens": [1566, 11, 411, 11, 22019, 3199, 293, 22019, 4471, 11, 11387, 4960, 22019, 3199, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17369420187813894, "compression_ratio": 1.671875, "no_speech_prob": 4.486018588067964e-05}, {"id": 138, "seek": 80888, "start": 820.12, "end": 823.32, "text": " But it's very good synchronized.", "tokens": [583, 309, 311, 588, 665, 19331, 1602, 13], "temperature": 0.0, "avg_logprob": -0.17369420187813894, "compression_ratio": 1.671875, "no_speech_prob": 4.486018588067964e-05}, {"id": 139, "seek": 80888, "start": 823.32, "end": 830.6, "text": " But not all classes that have any synchronization in them have their twins without synchronization.", "tokens": [583, 406, 439, 5359, 300, 362, 604, 19331, 2144, 294, 552, 362, 641, 22555, 1553, 19331, 2144, 13], "temperature": 0.0, "avg_logprob": -0.17369420187813894, "compression_ratio": 1.671875, "no_speech_prob": 4.486018588067964e-05}, {"id": 140, "seek": 80888, "start": 830.6, "end": 832.8, "text": " That makes no sense, right?", "tokens": [663, 1669, 572, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17369420187813894, "compression_ratio": 1.671875, "no_speech_prob": 4.486018588067964e-05}, {"id": 141, "seek": 83280, "start": 832.8, "end": 841.8399999999999, "text": " So there's a well-known technology, how to deal with a case where accesses to our data", "tokens": [407, 456, 311, 257, 731, 12, 6861, 2899, 11, 577, 281, 2028, 365, 257, 1389, 689, 2105, 279, 281, 527, 1412], "temperature": 0.0, "avg_logprob": -0.23450508571806408, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.357605964993127e-05}, {"id": 142, "seek": 83280, "start": 841.8399999999999, "end": 849.3199999999999, "text": " structures, to our classes, are mostly sequential than at any point in time, only a single thread", "tokens": [9227, 11, 281, 527, 5359, 11, 366, 5240, 42881, 813, 412, 604, 935, 294, 565, 11, 787, 257, 2167, 7207], "temperature": 0.0, "avg_logprob": -0.23450508571806408, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.357605964993127e-05}, {"id": 143, "seek": 83280, "start": 849.3199999999999, "end": 851.3199999999999, "text": " owns and operates with an object.", "tokens": [19143, 293, 22577, 365, 364, 2657, 13], "temperature": 0.0, "avg_logprob": -0.23450508571806408, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.357605964993127e-05}, {"id": 144, "seek": 83280, "start": 851.3199999999999, "end": 855.64, "text": " And it's called bus-locking or thing-locking.", "tokens": [400, 309, 311, 1219, 1255, 12, 4102, 278, 420, 551, 12, 4102, 278, 13], "temperature": 0.0, "avg_logprob": -0.23450508571806408, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.357605964993127e-05}, {"id": 145, "seek": 83280, "start": 855.64, "end": 862.76, "text": " Well, why is it simpler and more lightweight?", "tokens": [1042, 11, 983, 307, 309, 18587, 293, 544, 22052, 30], "temperature": 0.0, "avg_logprob": -0.23450508571806408, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.357605964993127e-05}, {"id": 146, "seek": 86276, "start": 862.76, "end": 868.08, "text": " Because we don't want to manage all the complex cases.", "tokens": [1436, 321, 500, 380, 528, 281, 3067, 439, 264, 3997, 3331, 13], "temperature": 0.0, "avg_logprob": -0.17934418963147447, "compression_ratio": 1.439306358381503, "no_speech_prob": 3.934703636332415e-05}, {"id": 147, "seek": 86276, "start": 868.08, "end": 871.52, "text": " We know that we are in a good situation.", "tokens": [492, 458, 300, 321, 366, 294, 257, 665, 2590, 13], "temperature": 0.0, "avg_logprob": -0.17934418963147447, "compression_ratio": 1.439306358381503, "no_speech_prob": 3.934703636332415e-05}, {"id": 148, "seek": 86276, "start": 871.52, "end": 878.0, "text": " And if we're not, yes, we can fall back, and it's called inflate our monitor.", "tokens": [400, 498, 321, 434, 406, 11, 2086, 11, 321, 393, 2100, 646, 11, 293, 309, 311, 1219, 9922, 473, 527, 6002, 13], "temperature": 0.0, "avg_logprob": -0.17934418963147447, "compression_ratio": 1.439306358381503, "no_speech_prob": 3.934703636332415e-05}, {"id": 149, "seek": 86276, "start": 878.0, "end": 885.96, "text": " Well, it existed in OpenJDK for ages, and it has been removed from OpenJDK.", "tokens": [1042, 11, 309, 13135, 294, 7238, 41, 35, 42, 337, 12357, 11, 293, 309, 575, 668, 7261, 490, 7238, 41, 35, 42, 13], "temperature": 0.0, "avg_logprob": -0.17934418963147447, "compression_ratio": 1.439306358381503, "no_speech_prob": 3.934703636332415e-05}, {"id": 150, "seek": 88596, "start": 885.96, "end": 894.2800000000001, "text": " If it was deprecated, then no one noticed, I believe, because still, are there too many", "tokens": [759, 309, 390, 1367, 13867, 770, 11, 550, 572, 472, 5694, 11, 286, 1697, 11, 570, 920, 11, 366, 456, 886, 867], "temperature": 0.0, "avg_logprob": -0.22938340285728717, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0001217729804920964}, {"id": 151, "seek": 88596, "start": 894.2800000000001, "end": 898.64, "text": " people using something newer than JDK 11?", "tokens": [561, 1228, 746, 17628, 813, 37082, 42, 2975, 30], "temperature": 0.0, "avg_logprob": -0.22938340285728717, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0001217729804920964}, {"id": 152, "seek": 88596, "start": 898.64, "end": 906.08, "text": " Well, some consequences were noticed probably too late.", "tokens": [1042, 11, 512, 10098, 645, 5694, 1391, 886, 3469, 13], "temperature": 0.0, "avg_logprob": -0.22938340285728717, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0001217729804920964}, {"id": 153, "seek": 88596, "start": 906.08, "end": 908.96, "text": " Well, what are the reasons, first of all?", "tokens": [1042, 11, 437, 366, 264, 4112, 11, 700, 295, 439, 30], "temperature": 0.0, "avg_logprob": -0.22938340285728717, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0001217729804920964}, {"id": 154, "seek": 88596, "start": 908.96, "end": 914.5600000000001, "text": " What are the reasons to remove a bus-locking from OpenJDK from hotspot JVM?", "tokens": [708, 366, 264, 4112, 281, 4159, 257, 1255, 12, 4102, 278, 490, 7238, 41, 35, 42, 490, 36121, 17698, 508, 53, 44, 30], "temperature": 0.0, "avg_logprob": -0.22938340285728717, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0001217729804920964}, {"id": 155, "seek": 91456, "start": 914.56, "end": 922.8, "text": " Well, to ease the implementation of virtual threads, to deliver project loom, to decrease", "tokens": [1042, 11, 281, 12708, 264, 11420, 295, 6374, 19314, 11, 281, 4239, 1716, 450, 298, 11, 281, 11514], "temperature": 0.0, "avg_logprob": -0.24037877596341647, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.16626756102778e-05}, {"id": 156, "seek": 91456, "start": 922.8, "end": 925.4799999999999, "text": " the amount of work there.", "tokens": [264, 2372, 295, 589, 456, 13], "temperature": 0.0, "avg_logprob": -0.24037877596341647, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.16626756102778e-05}, {"id": 157, "seek": 91456, "start": 925.4799999999999, "end": 932.4799999999999, "text": " So some consequences here, initials discovered.", "tokens": [407, 512, 10098, 510, 11, 5883, 82, 6941, 13], "temperature": 0.0, "avg_logprob": -0.24037877596341647, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.16626756102778e-05}, {"id": 158, "seek": 91456, "start": 932.4799999999999, "end": 942.4399999999999, "text": " In certain cases, things like input streams can be slowed down, like here it's 8x or something.", "tokens": [682, 1629, 3331, 11, 721, 411, 4846, 15842, 393, 312, 32057, 760, 11, 411, 510, 309, 311, 1649, 87, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.24037877596341647, "compression_ratio": 1.4715909090909092, "no_speech_prob": 3.16626756102778e-05}, {"id": 159, "seek": 94244, "start": 942.44, "end": 945.48, "text": " That's enormously slow.", "tokens": [663, 311, 39669, 2964, 13], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 160, "seek": 94244, "start": 945.48, "end": 954.9200000000001, "text": " And for GraVM, there is a mode that you say during static compilation, OK, this native", "tokens": [400, 337, 8985, 53, 44, 11, 456, 307, 257, 4391, 300, 291, 584, 1830, 13437, 40261, 11, 2264, 11, 341, 8470], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 161, "seek": 94244, "start": 954.9200000000001, "end": 960.2, "text": " image doesn't try to work with many cores.", "tokens": [3256, 1177, 380, 853, 281, 589, 365, 867, 24826, 13], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 162, "seek": 94244, "start": 960.2, "end": 961.72, "text": " It's a single-treaded program.", "tokens": [467, 311, 257, 2167, 12, 83, 2538, 292, 1461, 13], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 163, "seek": 94244, "start": 961.72, "end": 967.84, "text": " So it's simple, and it works really better in these circumstances.", "tokens": [407, 309, 311, 2199, 11, 293, 309, 1985, 534, 1101, 294, 613, 9121, 13], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 164, "seek": 94244, "start": 967.84, "end": 970.0, "text": " So there is an optimization for that.", "tokens": [407, 456, 307, 364, 19618, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18231369809406558, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.00011731892300304025}, {"id": 165, "seek": 97000, "start": 970.0, "end": 974.88, "text": " But you have to know it in advance, then you compile your program.", "tokens": [583, 291, 362, 281, 458, 309, 294, 7295, 11, 550, 291, 31413, 428, 1461, 13], "temperature": 0.0, "avg_logprob": -0.1657065602092, "compression_ratio": 1.5431472081218274, "no_speech_prob": 3.523615305311978e-05}, {"id": 166, "seek": 97000, "start": 974.88, "end": 979.52, "text": " Well, and there is, of course, a runtime option that supports all kinds of situations, and", "tokens": [1042, 11, 293, 456, 307, 11, 295, 1164, 11, 257, 34474, 3614, 300, 9346, 439, 3685, 295, 6851, 11, 293], "temperature": 0.0, "avg_logprob": -0.1657065602092, "compression_ratio": 1.5431472081218274, "no_speech_prob": 3.523615305311978e-05}, {"id": 167, "seek": 97000, "start": 979.52, "end": 981.04, "text": " it's complex.", "tokens": [309, 311, 3997, 13], "temperature": 0.0, "avg_logprob": -0.1657065602092, "compression_ratio": 1.5431472081218274, "no_speech_prob": 3.523615305311978e-05}, {"id": 168, "seek": 97000, "start": 981.04, "end": 984.68, "text": " So the missing part is in the left lower corner.", "tokens": [407, 264, 5361, 644, 307, 294, 264, 1411, 3126, 4538, 13], "temperature": 0.0, "avg_logprob": -0.1657065602092, "compression_ratio": 1.5431472081218274, "no_speech_prob": 3.523615305311978e-05}, {"id": 169, "seek": 97000, "start": 984.68, "end": 994.0, "text": " Well, to dynamically be able to process the situation of sequential access pattern.", "tokens": [1042, 11, 281, 43492, 312, 1075, 281, 1399, 264, 2590, 295, 42881, 2105, 5102, 13], "temperature": 0.0, "avg_logprob": -0.1657065602092, "compression_ratio": 1.5431472081218274, "no_speech_prob": 3.523615305311978e-05}, {"id": 170, "seek": 99400, "start": 994.0, "end": 1002.2, "text": " So we've lamented quite a classical approach to this problem.", "tokens": [407, 321, 600, 35888, 292, 1596, 257, 13735, 3109, 281, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.21520389210094104, "compression_ratio": 1.302325581395349, "no_speech_prob": 7.720300345681608e-05}, {"id": 171, "seek": 99400, "start": 1002.2, "end": 1010.68, "text": " That helps to, that brings that thing locking to GraVM.", "tokens": [663, 3665, 281, 11, 300, 5607, 300, 551, 23954, 281, 8985, 53, 44, 13], "temperature": 0.0, "avg_logprob": -0.21520389210094104, "compression_ratio": 1.302325581395349, "no_speech_prob": 7.720300345681608e-05}, {"id": 172, "seek": 99400, "start": 1010.68, "end": 1017.32, "text": " The initial idea was operating with object header.", "tokens": [440, 5883, 1558, 390, 7447, 365, 2657, 23117, 13], "temperature": 0.0, "avg_logprob": -0.21520389210094104, "compression_ratio": 1.302325581395349, "no_speech_prob": 7.720300345681608e-05}, {"id": 173, "seek": 101732, "start": 1017.32, "end": 1025.92, "text": " So where it already contains a pointer to a FAT monitor object.", "tokens": [407, 689, 309, 1217, 8306, 257, 23918, 281, 257, 479, 2218, 6002, 2657, 13], "temperature": 0.0, "avg_logprob": -0.21006086468696594, "compression_ratio": 1.446927374301676, "no_speech_prob": 4.67026729893405e-05}, {"id": 174, "seek": 101732, "start": 1025.92, "end": 1030.1200000000001, "text": " But it can be treated as well as some words.", "tokens": [583, 309, 393, 312, 8668, 382, 731, 382, 512, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21006086468696594, "compression_ratio": 1.446927374301676, "no_speech_prob": 4.67026729893405e-05}, {"id": 175, "seek": 101732, "start": 1030.1200000000001, "end": 1035.92, "text": " We can atomically access and put some information there.", "tokens": [492, 393, 12018, 984, 2105, 293, 829, 512, 1589, 456, 13], "temperature": 0.0, "avg_logprob": -0.21006086468696594, "compression_ratio": 1.446927374301676, "no_speech_prob": 4.67026729893405e-05}, {"id": 176, "seek": 101732, "start": 1035.92, "end": 1041.4, "text": " Probably close to final implementation that we have right now still, or again, uses a pointer", "tokens": [9210, 1998, 281, 2572, 11420, 300, 321, 362, 558, 586, 920, 11, 420, 797, 11, 4960, 257, 23918], "temperature": 0.0, "avg_logprob": -0.21006086468696594, "compression_ratio": 1.446927374301676, "no_speech_prob": 4.67026729893405e-05}, {"id": 177, "seek": 104140, "start": 1041.4, "end": 1049.96, "text": " because it turned to be not so easy to keep correctness across the whole VM with some", "tokens": [570, 309, 3574, 281, 312, 406, 370, 1858, 281, 1066, 3006, 1287, 2108, 264, 1379, 18038, 365, 512], "temperature": 0.0, "avg_logprob": -0.1713172001625175, "compression_ratio": 1.5027932960893855, "no_speech_prob": 7.128190190996975e-05}, {"id": 178, "seek": 104140, "start": 1049.96, "end": 1057.3200000000002, "text": " memory that you treat as a pointer or as a word depending on the situation.", "tokens": [4675, 300, 291, 2387, 382, 257, 23918, 420, 382, 257, 1349, 5413, 322, 264, 2590, 13], "temperature": 0.0, "avg_logprob": -0.1713172001625175, "compression_ratio": 1.5027932960893855, "no_speech_prob": 7.128190190996975e-05}, {"id": 179, "seek": 104140, "start": 1057.3200000000002, "end": 1066.16, "text": " Well, anyway, inside that part of header or inside that special object, we can have 64", "tokens": [1042, 11, 4033, 11, 1854, 300, 644, 295, 23117, 420, 1854, 300, 2121, 2657, 11, 321, 393, 362, 12145], "temperature": 0.0, "avg_logprob": -0.1713172001625175, "compression_ratio": 1.5027932960893855, "no_speech_prob": 7.128190190996975e-05}, {"id": 180, "seek": 104140, "start": 1066.16, "end": 1068.4, "text": " bits of information.", "tokens": [9239, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1713172001625175, "compression_ratio": 1.5027932960893855, "no_speech_prob": 7.128190190996975e-05}, {"id": 181, "seek": 106840, "start": 1068.4, "end": 1075.52, "text": " And we can mark it as a thin log, this is a flag, then we can do it atomically.", "tokens": [400, 321, 393, 1491, 309, 382, 257, 5862, 3565, 11, 341, 307, 257, 7166, 11, 550, 321, 393, 360, 309, 12018, 984, 13], "temperature": 0.0, "avg_logprob": -0.225961548941476, "compression_ratio": 1.4864864864864864, "no_speech_prob": 7.197784725576639e-05}, {"id": 182, "seek": 106840, "start": 1075.52, "end": 1085.96, "text": " We can keep the ID of an owner thread, which we can obtain, then we work with threads.", "tokens": [492, 393, 1066, 264, 7348, 295, 364, 7289, 7207, 11, 597, 321, 393, 12701, 11, 550, 321, 589, 365, 19314, 13], "temperature": 0.0, "avg_logprob": -0.225961548941476, "compression_ratio": 1.4864864864864864, "no_speech_prob": 7.197784725576639e-05}, {"id": 183, "seek": 106840, "start": 1085.96, "end": 1091.52, "text": " And account of recursive logs that we currently hold.", "tokens": [400, 2696, 295, 20560, 488, 20820, 300, 321, 4362, 1797, 13], "temperature": 0.0, "avg_logprob": -0.225961548941476, "compression_ratio": 1.4864864864864864, "no_speech_prob": 7.197784725576639e-05}, {"id": 184, "seek": 109152, "start": 1091.52, "end": 1099.92, "text": " That, by the way, means that after a certain amount of recursive logs, we have to inflate", "tokens": [663, 11, 538, 264, 636, 11, 1355, 300, 934, 257, 1629, 2372, 295, 20560, 488, 20820, 11, 321, 362, 281, 9922, 473], "temperature": 0.0, "avg_logprob": -0.249680270887401, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.729072629241273e-05}, {"id": 185, "seek": 109152, "start": 1099.92, "end": 1107.0, "text": " the monitor because we can store more information in that part of this work.", "tokens": [264, 6002, 570, 321, 393, 3531, 544, 1589, 294, 300, 644, 295, 341, 589, 13], "temperature": 0.0, "avg_logprob": -0.249680270887401, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.729072629241273e-05}, {"id": 186, "seek": 109152, "start": 1107.0, "end": 1108.48, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.249680270887401, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.729072629241273e-05}, {"id": 187, "seek": 109152, "start": 1108.48, "end": 1119.8, "text": " So again, it's a pure Java implementation where we work with some atomic magic and we", "tokens": [407, 797, 11, 309, 311, 257, 6075, 10745, 11420, 689, 321, 589, 365, 512, 22275, 5585, 293, 321], "temperature": 0.0, "avg_logprob": -0.249680270887401, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.729072629241273e-05}, {"id": 188, "seek": 109152, "start": 1119.8, "end": 1121.48, "text": " update this information.", "tokens": [5623, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.249680270887401, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.729072629241273e-05}, {"id": 189, "seek": 112148, "start": 1121.48, "end": 1126.1200000000001, "text": " What we've got, and the most recent numbers are even better.", "tokens": [708, 321, 600, 658, 11, 293, 264, 881, 5162, 3547, 366, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19404974437895275, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.00034310665796510875}, {"id": 190, "seek": 112148, "start": 1126.1200000000001, "end": 1131.44, "text": " So we see that effect on exactly that example, the streams.", "tokens": [407, 321, 536, 300, 1802, 322, 2293, 300, 1365, 11, 264, 15842, 13], "temperature": 0.0, "avg_logprob": -0.19404974437895275, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.00034310665796510875}, {"id": 191, "seek": 112148, "start": 1131.44, "end": 1133.6, "text": " We can speed them up.", "tokens": [492, 393, 3073, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.19404974437895275, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.00034310665796510875}, {"id": 192, "seek": 112148, "start": 1133.6, "end": 1142.44, "text": " And even in a very kind of nano-benchmark kind of measurement, you also see the improvement.", "tokens": [400, 754, 294, 257, 588, 733, 295, 30129, 12, 47244, 5638, 733, 295, 13160, 11, 291, 611, 536, 264, 10444, 13], "temperature": 0.0, "avg_logprob": -0.19404974437895275, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.00034310665796510875}, {"id": 193, "seek": 114244, "start": 1142.44, "end": 1153.6000000000001, "text": " And even in multi-threaded case, there is now no difference with the original.", "tokens": [50364, 400, 754, 294, 4825, 12, 392, 2538, 292, 1389, 11, 456, 307, 586, 572, 2649, 365, 264, 3380, 13, 50922], "temperature": 0.0, "avg_logprob": -0.2698072086681019, "compression_ratio": 1.04, "no_speech_prob": 0.0017954851500689983}], "language": "en"}