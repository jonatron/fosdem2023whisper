{"text": " So, hello and welcome to my talk about green software engineering and more specifically about building energy measurement tools and ecosystems around software. My name is Arne and I work for Green Coding Berlin, which is a company that specializes in making open source tools for energy aware software measurement. I would like to take you on a tour today of a concept for a possible future ecosystem we imagine where energy consumption of software is a first world metric and available for every developer and user. So let's have a look at a hypothetical scenario. The Windows 10 operating system typically comes with a minimum system requirements. So if you look on the vendors web page, you can see it has a processor that is needed, one gigahertz, one gigabyte of RAM, a particular amount of hard disk space, graphics ports, etc. However, what is never given is the power on, for instance, idle that this operating system uses on this reference hardware that it apparently already specifies. So this should be pretty doable, right? Also something like power the desktop activity. So how much power does it use just to go around in the operating system, opening the file explorer, using the taskbar and stuff like this. On the reference system, for instance, that with Microsoft specifies or on a reference system that we or the community specifies. And imagine then you can make could make informed choices. So by just saying, hey, I'm looking at Windows 10, and I see that it has 45 watts in idle. But apparently, my computer is mostly in idle. So it might be more interesting to use Ubuntu, for instance, which has just 20 watts in idle or desktop activity is even lower. So why not choose this operating system if energy is my main concern. And this is what I, what I cherish the most in the operating system, or which is an important metric for me. If you think this process even further, you can think about comparing energy of applications very specific, not only in the idle scenario or in one scenario, but in very specific usage scenarios that are ingrained to how people typically use such an application. What you see here is two radar charts on the left side is WhatsApp, and on the right side is Telegram. Please keep in mind that these are concept pictures. So this is not actually the energy that these application use for this use case. But let's say your use case is that you message a lot with an app, but you don't do that many video calls. So if you look then at WhatsApp, you see here that it has quite a high energy budget when it comes to messaging, whereas Telegram has quite a lower budget. Telegram is, however, very bad when it comes to video where WhatsApp could be, for instance, better. So let's say that you are mostly doing messaging with your application, and you would like to keep your battery life, or maybe use Telegram on the desktop, your desktop energy consumption low, then with such metrics, you could actually make an informed decision if WhatsApp or Telegram is the better app for you if energy is an important concern. And imagine as a developer, if you think even one step further, that you go to GitHub or to GitLab or wherever your software is hosted, and you look in the repository and you see right away with something like an open energy batch, how we call it internally, to see how much the software, you see it down here, how much the software is actually using for its intended use case that the developer of the software had in mind. So you can compare one software that maybe has very limited use case to another software or library, just by the energy budget, because you have the metrics so readily available. We actually try to build these tools, and I would like to take you in this very short time frame that we have been given by FOSDAM, so just about 20 minutes, I would like you to take a tour through our projects that we are doing, more as an appetizer, so you see what we are working on and what we think could be possible or a possible ecosystem in the future. You will be presented with a view that looks like such, so the green metrics tool, EcoCI, Open Energy Badge and Cloud Energy. So the green metrics tool is what I would like to talk about today, mostly, because I think it is the tool that outlines our concept of transparency in the software community the best, and then we'll talk about later about our approaches for CI pipelines or restricted environments like the cloud. So first of all, I think it makes sense, although I know people tend to hate diagrams or flowcharts to some degree, but I think it makes sense to quickly go over how the concept of the tool works from a high-level perspective. So in order to measure software, we follow the container-based approach. So we assume that your software is already in a containerized format or can be put in such a format. So for instance, even a Firefox browser, if you want to measure desktop applications, can be put in a container and be measured with our tool. Also machine learning applications, simple command line applications, but also web applications. Typically when you develop software, you already have infrastructure files like Docker files, Docker compose file, or even a Kubernetes file available, which our tool can consume in all fairness, Kubernetes is still a work in progress, but Docker files can consume. And then what the tool basically orchestrates the containers and attaches every reporter that you want in terms of measuring metrics. So here we are still very similar to typical data logging approaches like Datadoc does it for instance, or other big players. So the memory, the AC power, DC power, the network traffic, CPU percentage, CPU and RAM is all locked during the execution of what we call a standard usage scenario. So in the first couple of slides, I've shown you the concept of looking at software from how is it typically used. And people already have thought about this concept quite a lot when they make end-to-end tests with their software, because this is a typical flow that a user goes through in your application, or unit tests, which might be very reduced amounts of functionality that is tested in a block, or benchmarks that are already inside of the software repository, session replays, shell scripts, build files that basically measure where we could measure your build process. All of this is already available typically, and our tool can consume these files, will run these workflows and then tell you the energy budget over the time of this run in particular. This slide is more just, if you're not too familiar with Docker, the idea is just to have every service or every component of the application in a separate container, so that we can later on better granularize the metrics and better look at which component might be interesting to look at if you want to do energy optimizations in particular. When you use the tool, and I will just go quickly over that and then probably go with you through a live version of what we are hosting at the moment, you will get a lot of metrics. So you will obviously get something like the CPU utilization, or the average memory that was used, or maybe the network bandwidth that was used. But what is interesting for this dashboard, and basically it's USP, is that you get also the energy metrics from the CPU, from the memory, you get a calculation what the network has used in energy, and you get convoluted or basically aggregated values where it makes often sense to look at CPU and memory in conjunction, or it makes sense to look at all the metrics that you have available to get something like a total energy budget. Then you obviously can look also at the AC, so at the wall plugs, so not only what is your CPU and your RAM using, but what is the total machine using, or something that we have in our lab as a setup, you just look at the main board, so not on the outside of the PSU, so what is basically plugged in the desktop computer, but only the power that flows directly into the main board. And here you can see that our tool automatically calculates the CO2 budget based on the energy that it has used for this run. The tool also shows you which reporters have been used in an overview, and then it tells you a lot of charts, so this is a sample chart, and what the tool can basically give you is not only an overview capability, but also an introspection where you, for instance, are interested in the idle time of the application. So what is my application doing when no user is interacting with it? Is it actually using energy, and is this too much energy for my belief or for the belief of the community? So for instance, here we have an example of a setup, of a WordPress setup that we have done with an Apache, a Puppeteer container that runs Chrome, and also a MariaDB instance. And you can see here that here are a couple of requests that have been done to the WordPress instance, and then we are basically just idling, but still the web server is doing quite some work, and there have been no web sockets active, so why is there server and database activity here? Is this valid, is this maybe some caching, some housekeeping, or is this unintended behavior? We picture that our tool could highlight such energy hotspot or energy malfunctions, as we call them, to better understand how software uses energy. You can also look at energy anomalies, so we work sometimes with features like TurboBoost, which is typically not turned on in cloud environments, but very often for desktops, which brings a processor in kind of like an overdrive state so that it can react very quickly in a frequency above its normal frequency. However, what we have done here in this example, we have run a constant CPU utilization, but as you can see here, the CPU clocks at different frequency over the time, and sometimes it uses exponentially more energy for the same tasks. So it finishes quicker, but it uses more than only a linear amount more of energy to do the task. So this is a very interesting insight that our tool can, for instance, deliver when you try for energy optimizations of your software. So what is the whole idea that we have behind all this project? And let me move myself down here a little bit so you can see the full slide. We want to create an open source community or a green software community that focuses on the transparency of software so that you have basically an interface, which we call the usage scenario, where you can measure software against and then ask later on questions against a database or against an API, which has measured all these softwares, questions like how much does this software consume? Is there a more carbon friendly alternative, or is there a software that makes less energy requests, less network requests? The idea, if these softwares are available in your country, so Yucca to my knowledge is, for instance, from the US and code check is more like a German application, is we want to be the Yucca or the code check of software. So we want to deliver answers to developers where they can ask questions about the energy budgeting of a library, of a software, or of a functionality by providing a framework to make these measurements. So let me move up here again and then back to the slides. So let me show you our other tools that we believe are needed to build an ecosystem around green software because software is not only running in desktop environments or is not only on a single machine, it also runs a lot in the clouds, where these measurements that we have, and I would like to encourage you to read a bit on what sensors are available in our tool, but where these sensors are not available, which is for instance in the cloud. So let me bring up my browser again. So if you are on the homepage and you have seen the green metrics tool that I've just talked about, you'll also see that we have the cloud energy project and the EcoCI project. So EcoCI focuses on measuring the energy of software in a continuous integration pipeline that for instance runs in a virtual machine. Our focus is currently on GitHub actions. In order to estimate the energy in a virtual machine, because you cannot measure, you have no access to the wall plug in the data center, you have no access to sensors in the CPU or whatever, you have to estimate the machine based on measurements that you already have for the same hardware. If you click on cloud energy, you can see here that we have based our machine learning model on a research paper from InterACTC and the University of London, and they have basically taken the data from the spec power database, which is an open database for servers that have been measured just with a fixed workload to compare it against each other. And based on this data, we can create a machine learning model, which is also free and open source to use, that is just a Python tool, which you call with the information that you have. So let's say you have the information that your CPU is from Intel, that the frequency that you're running is 2.6 gigahertz, you have 7 gigabytes of RAM, and you know the CPU has 24 threads. But you don't know any more info. You don't know if it's a Skylake processor or a more modern internal processor. You have no more information because the hypervisor limits this to you. So if you give the model more information, it can give you more accurate estimates, but it can also work with the limited information in the cloud. And then it spits out to the standard out the current watchers that you have been using, and then you can reuse that in a tool that we build upon that. So now that you've understood that there is a machine learning model behind the idea, I would like to bring you to EcoCI. So EcoCI is a GitHub action that is based on the work from the Cloud Energy Project that can give you in a GitHub action the information of how much a CI pipeline has used in terms of energy. So if you go, for instance, to the GitHub repository, you can also go to the marketplace. So we go one step further. And here you can see you can directly use it. It is very easy to use. It just needs two calls to initialize a tool and then one more call whenever you want to get a measurement. And what it does for you, so let's quickly go to our repository where we actually use GitHub actions to measure every of our workflows in the tool. So we click on actions, let's say we go to manual test run virtual machine, we click on main. And you see here, I've run this run yesterday, it succeeded. So our log tells us, hey, all tests have to work fine. So to run a work point fine and also the API. And for this run in the Azure Cloud where GitHub actions runs as virtual machines, I have used 650 joules of energy. And you get a nice ASCII graph over time. We were a bit limited here in the graphs we can display in the GitHub actions overview. But you can see here at what point in time the energy, for instance, is the highest and then maybe look at the later tests if they, if you deem them to be more energy consuming than for instance at the start where it was using only a fixed amount of energy. So this gives a developer and also a user the information how much energy is not only the software using, but also the development of the software is it maybe using more than we want as developers or maybe even as a community. And these are all concept tools to just get a first start of what we, what we think could be possible, of what we think could be possible in a new future where software is basically measured and the data of the, of its usage is constantly published by developers also. The idea is then to have something like an open energy batch that is basically in every repository that tells you for this software and for this usage scenario that comes with it. So be it for instance running the tests or be it for instance building the containers or the intended use case of the software. So let's say the NumPy library of Python has an energy batch where it says, hey, for 1000 times 1000 metrics multiplication, this software uses this amount of energy on the reference system that we have specified. And when you use the same reference systems to compare software against each other, you come to a scenario that we have basically shown from the starters in the first slides where you can basically tell is the one software more energy hungry than the other one comparing the same use case. So let me quickly get back to my slide deck. So let's wrap up. Measuring software energy consumption we believe is still too hard. The goal should be easy as starting a Docker container and it should happen transparently. Therefore we have created the green metrics tool which can reuse Docker files and infrastructure files to make it very easy to orchestrate your architecture. And then in a flow that you already have, be it a puppeteer file or be it just a shell script, you can run that with our tool just as a parameter appended and it will tell you how much energy has been used over this particular scenario that you feed in. Measuring software is also very complex. So this is what we have integrated best practices or tool like pausing between measurements, letting systems idle before you actually use them, turning functionalities like SGX off, looking at if TurboBoost is on and very more features. Just inline measuring like Datadoc or other providers are doing it at the moment, we believe is not enough and is too arbitrary to talk about energy. Software must be measured against a standard usage case. So we provide standard usage cases for software as an interface, but we ask you the community also or we need to see over time what are the standard usage cases we can all agree on. A software must be comparable to another similar software in terms of energy. This is why we need these standard usage cases to make it comparable. This also means it must be measured on reference machines that everybody has access to that we want to provide for the community as a free service. Energy metrics must also be available in restricted environments like the cloud. So I've talked about estimation models that need to be open source and available and for everybody to implement. And energy must be transparent and a first order metric and order in developing and using software. People should know before they use the software how much energy it is consuming. And this is what we are trying to achieve with the tools we are developing. I hope it could pique your interest in our work and in the tools we are developing, some as concepts, some already production ready. And thank you for listening and now I hope it's time for questions.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.48, "text": " So, hello and welcome to my talk about green software engineering and more specifically", "tokens": [407, 11, 7751, 293, 2928, 281, 452, 751, 466, 3092, 4722, 7043, 293, 544, 4682], "temperature": 0.0, "avg_logprob": -0.14939168647483544, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.10776931047439575}, {"id": 1, "seek": 0, "start": 11.48, "end": 17.2, "text": " about building energy measurement tools and ecosystems around software.", "tokens": [466, 2390, 2281, 13160, 3873, 293, 32647, 926, 4722, 13], "temperature": 0.0, "avg_logprob": -0.14939168647483544, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.10776931047439575}, {"id": 2, "seek": 0, "start": 17.2, "end": 23.68, "text": " My name is Arne and I work for Green Coding Berlin, which is a company that specializes", "tokens": [1222, 1315, 307, 1587, 716, 293, 286, 589, 337, 6969, 383, 8616, 13848, 11, 597, 307, 257, 2237, 300, 2121, 5660], "temperature": 0.0, "avg_logprob": -0.14939168647483544, "compression_ratio": 1.4277456647398843, "no_speech_prob": 0.10776931047439575}, {"id": 3, "seek": 2368, "start": 23.68, "end": 31.04, "text": " in making open source tools for energy aware software measurement.", "tokens": [294, 1455, 1269, 4009, 3873, 337, 2281, 3650, 4722, 13160, 13], "temperature": 0.0, "avg_logprob": -0.1422497034072876, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.00012124329805374146}, {"id": 4, "seek": 2368, "start": 31.04, "end": 38.0, "text": " I would like to take you on a tour today of a concept for a possible future ecosystem", "tokens": [286, 576, 411, 281, 747, 291, 322, 257, 3512, 965, 295, 257, 3410, 337, 257, 1944, 2027, 11311], "temperature": 0.0, "avg_logprob": -0.1422497034072876, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.00012124329805374146}, {"id": 5, "seek": 2368, "start": 38.0, "end": 43.879999999999995, "text": " we imagine where energy consumption of software is a first world metric and available for", "tokens": [321, 3811, 689, 2281, 12126, 295, 4722, 307, 257, 700, 1002, 20678, 293, 2435, 337], "temperature": 0.0, "avg_logprob": -0.1422497034072876, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.00012124329805374146}, {"id": 6, "seek": 2368, "start": 43.879999999999995, "end": 48.04, "text": " every developer and user.", "tokens": [633, 10754, 293, 4195, 13], "temperature": 0.0, "avg_logprob": -0.1422497034072876, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.00012124329805374146}, {"id": 7, "seek": 2368, "start": 48.04, "end": 50.64, "text": " So let's have a look at a hypothetical scenario.", "tokens": [407, 718, 311, 362, 257, 574, 412, 257, 33053, 9005, 13], "temperature": 0.0, "avg_logprob": -0.1422497034072876, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.00012124329805374146}, {"id": 8, "seek": 5064, "start": 50.64, "end": 56.28, "text": " The Windows 10 operating system typically comes with a minimum system requirements.", "tokens": [440, 8591, 1266, 7447, 1185, 5850, 1487, 365, 257, 7285, 1185, 7728, 13], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 9, "seek": 5064, "start": 56.28, "end": 60.28, "text": " So if you look on the vendors web page, you can see it has a processor that is needed,", "tokens": [407, 498, 291, 574, 322, 264, 22056, 3670, 3028, 11, 291, 393, 536, 309, 575, 257, 15321, 300, 307, 2978, 11], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 10, "seek": 5064, "start": 60.28, "end": 65.44, "text": " one gigahertz, one gigabyte of RAM, a particular amount of hard disk space, graphics ports,", "tokens": [472, 8741, 64, 35655, 11, 472, 8741, 34529, 295, 14561, 11, 257, 1729, 2372, 295, 1152, 12355, 1901, 11, 11837, 18160, 11], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 11, "seek": 5064, "start": 65.44, "end": 66.44, "text": " etc.", "tokens": [5183, 13], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 12, "seek": 5064, "start": 66.44, "end": 72.52, "text": " However, what is never given is the power on, for instance, idle that this operating system", "tokens": [2908, 11, 437, 307, 1128, 2212, 307, 264, 1347, 322, 11, 337, 5197, 11, 30650, 300, 341, 7447, 1185], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 13, "seek": 5064, "start": 72.52, "end": 78.24000000000001, "text": " uses on this reference hardware that it apparently already specifies.", "tokens": [4960, 322, 341, 6408, 8837, 300, 309, 7970, 1217, 1608, 11221, 13], "temperature": 0.0, "avg_logprob": -0.2251010820703599, "compression_ratio": 1.612781954887218, "no_speech_prob": 0.00011230563541175798}, {"id": 14, "seek": 7824, "start": 78.24, "end": 80.64, "text": " So this should be pretty doable, right?", "tokens": [407, 341, 820, 312, 1238, 41183, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 15, "seek": 7824, "start": 80.64, "end": 83.64, "text": " Also something like power the desktop activity.", "tokens": [2743, 746, 411, 1347, 264, 14502, 5191, 13], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 16, "seek": 7824, "start": 83.64, "end": 88.36, "text": " So how much power does it use just to go around in the operating system, opening the", "tokens": [407, 577, 709, 1347, 775, 309, 764, 445, 281, 352, 926, 294, 264, 7447, 1185, 11, 5193, 264], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 17, "seek": 7824, "start": 88.36, "end": 93.16, "text": " file explorer, using the taskbar and stuff like this.", "tokens": [3991, 39680, 11, 1228, 264, 5633, 5356, 293, 1507, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 18, "seek": 7824, "start": 93.16, "end": 97.39999999999999, "text": " On the reference system, for instance, that with Microsoft specifies or on a reference", "tokens": [1282, 264, 6408, 1185, 11, 337, 5197, 11, 300, 365, 8116, 1608, 11221, 420, 322, 257, 6408], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 19, "seek": 7824, "start": 97.39999999999999, "end": 101.24, "text": " system that we or the community specifies.", "tokens": [1185, 300, 321, 420, 264, 1768, 1608, 11221, 13], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 20, "seek": 7824, "start": 101.24, "end": 103.88, "text": " And imagine then you can make could make informed choices.", "tokens": [400, 3811, 550, 291, 393, 652, 727, 652, 11740, 7994, 13], "temperature": 0.0, "avg_logprob": -0.15306812286376953, "compression_ratio": 1.653386454183267, "no_speech_prob": 4.8323781811632216e-05}, {"id": 21, "seek": 10388, "start": 103.88, "end": 111.47999999999999, "text": " So by just saying, hey, I'm looking at Windows 10, and I see that it has 45 watts in idle.", "tokens": [407, 538, 445, 1566, 11, 4177, 11, 286, 478, 1237, 412, 8591, 1266, 11, 293, 286, 536, 300, 309, 575, 6905, 31247, 294, 30650, 13], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 22, "seek": 10388, "start": 111.47999999999999, "end": 114.11999999999999, "text": " But apparently, my computer is mostly in idle.", "tokens": [583, 7970, 11, 452, 3820, 307, 5240, 294, 30650, 13], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 23, "seek": 10388, "start": 114.11999999999999, "end": 118.6, "text": " So it might be more interesting to use Ubuntu, for instance, which has just 20 watts in idle", "tokens": [407, 309, 1062, 312, 544, 1880, 281, 764, 30230, 45605, 11, 337, 5197, 11, 597, 575, 445, 945, 31247, 294, 30650], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 24, "seek": 10388, "start": 118.6, "end": 120.8, "text": " or desktop activity is even lower.", "tokens": [420, 14502, 5191, 307, 754, 3126, 13], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 25, "seek": 10388, "start": 120.8, "end": 125.36, "text": " So why not choose this operating system if energy is my main concern.", "tokens": [407, 983, 406, 2826, 341, 7447, 1185, 498, 2281, 307, 452, 2135, 3136, 13], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 26, "seek": 10388, "start": 125.36, "end": 130.16, "text": " And this is what I, what I cherish the most in the operating system, or which is an important", "tokens": [400, 341, 307, 437, 286, 11, 437, 286, 38277, 264, 881, 294, 264, 7447, 1185, 11, 420, 597, 307, 364, 1021], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 27, "seek": 10388, "start": 130.16, "end": 131.94, "text": " metric for me.", "tokens": [20678, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.14818477630615234, "compression_ratio": 1.6204379562043796, "no_speech_prob": 4.683644510805607e-05}, {"id": 28, "seek": 13194, "start": 131.94, "end": 138.28, "text": " If you think this process even further, you can think about comparing energy of applications", "tokens": [759, 291, 519, 341, 1399, 754, 3052, 11, 291, 393, 519, 466, 15763, 2281, 295, 5821], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 29, "seek": 13194, "start": 138.28, "end": 144.48, "text": " very specific, not only in the idle scenario or in one scenario, but in very specific usage", "tokens": [588, 2685, 11, 406, 787, 294, 264, 30650, 9005, 420, 294, 472, 9005, 11, 457, 294, 588, 2685, 14924], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 30, "seek": 13194, "start": 144.48, "end": 150.76, "text": " scenarios that are ingrained to how people typically use such an application.", "tokens": [15077, 300, 366, 3957, 31774, 281, 577, 561, 5850, 764, 1270, 364, 3861, 13], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 31, "seek": 13194, "start": 150.76, "end": 155.12, "text": " What you see here is two radar charts on the left side is WhatsApp, and on the right side", "tokens": [708, 291, 536, 510, 307, 732, 16544, 17767, 322, 264, 1411, 1252, 307, 30513, 11, 293, 322, 264, 558, 1252], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 32, "seek": 13194, "start": 155.12, "end": 156.12, "text": " is Telegram.", "tokens": [307, 14889, 1342, 13], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 33, "seek": 13194, "start": 156.12, "end": 158.2, "text": " Please keep in mind that these are concept pictures.", "tokens": [2555, 1066, 294, 1575, 300, 613, 366, 3410, 5242, 13], "temperature": 0.0, "avg_logprob": -0.08352011257840186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.5465526050538756e-05}, {"id": 34, "seek": 15820, "start": 158.2, "end": 164.79999999999998, "text": " So this is not actually the energy that these application use for this use case.", "tokens": [407, 341, 307, 406, 767, 264, 2281, 300, 613, 3861, 764, 337, 341, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 35, "seek": 15820, "start": 164.79999999999998, "end": 169.79999999999998, "text": " But let's say your use case is that you message a lot with an app, but you don't do that many", "tokens": [583, 718, 311, 584, 428, 764, 1389, 307, 300, 291, 3636, 257, 688, 365, 364, 724, 11, 457, 291, 500, 380, 360, 300, 867], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 36, "seek": 15820, "start": 169.79999999999998, "end": 171.35999999999999, "text": " video calls.", "tokens": [960, 5498, 13], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 37, "seek": 15820, "start": 171.35999999999999, "end": 175.28, "text": " So if you look then at WhatsApp, you see here that it has quite a high energy budget when", "tokens": [407, 498, 291, 574, 550, 412, 30513, 11, 291, 536, 510, 300, 309, 575, 1596, 257, 1090, 2281, 4706, 562], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 38, "seek": 15820, "start": 175.28, "end": 179.35999999999999, "text": " it comes to messaging, whereas Telegram has quite a lower budget.", "tokens": [309, 1487, 281, 21812, 11, 9735, 14889, 1342, 575, 1596, 257, 3126, 4706, 13], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 39, "seek": 15820, "start": 179.35999999999999, "end": 184.07999999999998, "text": " Telegram is, however, very bad when it comes to video where WhatsApp could be, for instance,", "tokens": [14889, 1342, 307, 11, 4461, 11, 588, 1578, 562, 309, 1487, 281, 960, 689, 30513, 727, 312, 11, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 40, "seek": 15820, "start": 184.07999999999998, "end": 185.07999999999998, "text": " better.", "tokens": [1101, 13], "temperature": 0.0, "avg_logprob": -0.14069460178243703, "compression_ratio": 1.776, "no_speech_prob": 2.885634603444487e-05}, {"id": 41, "seek": 18508, "start": 185.08, "end": 189.72, "text": " So let's say that you are mostly doing messaging with your application, and you would like", "tokens": [407, 718, 311, 584, 300, 291, 366, 5240, 884, 21812, 365, 428, 3861, 11, 293, 291, 576, 411], "temperature": 0.0, "avg_logprob": -0.12689607450277499, "compression_ratio": 1.6292134831460674, "no_speech_prob": 8.34918100736104e-05}, {"id": 42, "seek": 18508, "start": 189.72, "end": 194.52, "text": " to keep your battery life, or maybe use Telegram on the desktop, your desktop energy consumption", "tokens": [281, 1066, 428, 5809, 993, 11, 420, 1310, 764, 14889, 1342, 322, 264, 14502, 11, 428, 14502, 2281, 12126], "temperature": 0.0, "avg_logprob": -0.12689607450277499, "compression_ratio": 1.6292134831460674, "no_speech_prob": 8.34918100736104e-05}, {"id": 43, "seek": 18508, "start": 194.52, "end": 200.60000000000002, "text": " low, then with such metrics, you could actually make an informed decision if WhatsApp or Telegram", "tokens": [2295, 11, 550, 365, 1270, 16367, 11, 291, 727, 767, 652, 364, 11740, 3537, 498, 30513, 420, 14889, 1342], "temperature": 0.0, "avg_logprob": -0.12689607450277499, "compression_ratio": 1.6292134831460674, "no_speech_prob": 8.34918100736104e-05}, {"id": 44, "seek": 18508, "start": 200.60000000000002, "end": 206.16000000000003, "text": " is the better app for you if energy is an important concern.", "tokens": [307, 264, 1101, 724, 337, 291, 498, 2281, 307, 364, 1021, 3136, 13], "temperature": 0.0, "avg_logprob": -0.12689607450277499, "compression_ratio": 1.6292134831460674, "no_speech_prob": 8.34918100736104e-05}, {"id": 45, "seek": 18508, "start": 206.16000000000003, "end": 211.52, "text": " And imagine as a developer, if you think even one step further, that you go to GitHub or", "tokens": [400, 3811, 382, 257, 10754, 11, 498, 291, 519, 754, 472, 1823, 3052, 11, 300, 291, 352, 281, 23331, 420], "temperature": 0.0, "avg_logprob": -0.12689607450277499, "compression_ratio": 1.6292134831460674, "no_speech_prob": 8.34918100736104e-05}, {"id": 46, "seek": 21152, "start": 211.52, "end": 216.16, "text": " to GitLab or wherever your software is hosted, and you look in the repository and you see", "tokens": [281, 16939, 37880, 420, 8660, 428, 4722, 307, 19204, 11, 293, 291, 574, 294, 264, 25841, 293, 291, 536], "temperature": 0.0, "avg_logprob": -0.10619838870301539, "compression_ratio": 1.811965811965812, "no_speech_prob": 5.307279570843093e-05}, {"id": 47, "seek": 21152, "start": 216.16, "end": 222.72, "text": " right away with something like an open energy batch, how we call it internally, to see how", "tokens": [558, 1314, 365, 746, 411, 364, 1269, 2281, 15245, 11, 577, 321, 818, 309, 19501, 11, 281, 536, 577], "temperature": 0.0, "avg_logprob": -0.10619838870301539, "compression_ratio": 1.811965811965812, "no_speech_prob": 5.307279570843093e-05}, {"id": 48, "seek": 21152, "start": 222.72, "end": 228.36, "text": " much the software, you see it down here, how much the software is actually using for its", "tokens": [709, 264, 4722, 11, 291, 536, 309, 760, 510, 11, 577, 709, 264, 4722, 307, 767, 1228, 337, 1080], "temperature": 0.0, "avg_logprob": -0.10619838870301539, "compression_ratio": 1.811965811965812, "no_speech_prob": 5.307279570843093e-05}, {"id": 49, "seek": 21152, "start": 228.36, "end": 232.72, "text": " intended use case that the developer of the software had in mind.", "tokens": [10226, 764, 1389, 300, 264, 10754, 295, 264, 4722, 632, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.10619838870301539, "compression_ratio": 1.811965811965812, "no_speech_prob": 5.307279570843093e-05}, {"id": 50, "seek": 21152, "start": 232.72, "end": 237.88, "text": " So you can compare one software that maybe has very limited use case to another software", "tokens": [407, 291, 393, 6794, 472, 4722, 300, 1310, 575, 588, 5567, 764, 1389, 281, 1071, 4722], "temperature": 0.0, "avg_logprob": -0.10619838870301539, "compression_ratio": 1.811965811965812, "no_speech_prob": 5.307279570843093e-05}, {"id": 51, "seek": 23788, "start": 237.88, "end": 244.56, "text": " or library, just by the energy budget, because you have the metrics so readily available.", "tokens": [420, 6405, 11, 445, 538, 264, 2281, 4706, 11, 570, 291, 362, 264, 16367, 370, 26336, 2435, 13], "temperature": 0.0, "avg_logprob": -0.14579241984599345, "compression_ratio": 1.6492537313432836, "no_speech_prob": 5.47566705790814e-05}, {"id": 52, "seek": 23788, "start": 244.56, "end": 250.76, "text": " We actually try to build these tools, and I would like to take you in this very short", "tokens": [492, 767, 853, 281, 1322, 613, 3873, 11, 293, 286, 576, 411, 281, 747, 291, 294, 341, 588, 2099], "temperature": 0.0, "avg_logprob": -0.14579241984599345, "compression_ratio": 1.6492537313432836, "no_speech_prob": 5.47566705790814e-05}, {"id": 53, "seek": 23788, "start": 250.76, "end": 255.32, "text": " time frame that we have been given by FOSDAM, so just about 20 minutes, I would like you", "tokens": [565, 3920, 300, 321, 362, 668, 2212, 538, 479, 4367, 35, 2865, 11, 370, 445, 466, 945, 2077, 11, 286, 576, 411, 291], "temperature": 0.0, "avg_logprob": -0.14579241984599345, "compression_ratio": 1.6492537313432836, "no_speech_prob": 5.47566705790814e-05}, {"id": 54, "seek": 23788, "start": 255.32, "end": 261.44, "text": " to take a tour through our projects that we are doing, more as an appetizer, so you see", "tokens": [281, 747, 257, 3512, 807, 527, 4455, 300, 321, 366, 884, 11, 544, 382, 364, 16159, 6545, 11, 370, 291, 536], "temperature": 0.0, "avg_logprob": -0.14579241984599345, "compression_ratio": 1.6492537313432836, "no_speech_prob": 5.47566705790814e-05}, {"id": 55, "seek": 23788, "start": 261.44, "end": 266.56, "text": " what we are working on and what we think could be possible or a possible ecosystem in the", "tokens": [437, 321, 366, 1364, 322, 293, 437, 321, 519, 727, 312, 1944, 420, 257, 1944, 11311, 294, 264], "temperature": 0.0, "avg_logprob": -0.14579241984599345, "compression_ratio": 1.6492537313432836, "no_speech_prob": 5.47566705790814e-05}, {"id": 56, "seek": 26656, "start": 266.56, "end": 269.64, "text": " future.", "tokens": [2027, 13], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 57, "seek": 26656, "start": 269.64, "end": 276.36, "text": " You will be presented with a view that looks like such, so the green metrics tool, EcoCI,", "tokens": [509, 486, 312, 8212, 365, 257, 1910, 300, 1542, 411, 1270, 11, 370, 264, 3092, 16367, 2290, 11, 40263, 25240, 11], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 58, "seek": 26656, "start": 276.36, "end": 278.84000000000003, "text": " Open Energy Badge and Cloud Energy.", "tokens": [7238, 14939, 11523, 432, 293, 8061, 14939, 13], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 59, "seek": 26656, "start": 278.84000000000003, "end": 283.48, "text": " So the green metrics tool is what I would like to talk about today, mostly, because", "tokens": [407, 264, 3092, 16367, 2290, 307, 437, 286, 576, 411, 281, 751, 466, 965, 11, 5240, 11, 570], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 60, "seek": 26656, "start": 283.48, "end": 289.72, "text": " I think it is the tool that outlines our concept of transparency in the software community", "tokens": [286, 519, 309, 307, 264, 2290, 300, 40125, 527, 3410, 295, 17131, 294, 264, 4722, 1768], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 61, "seek": 26656, "start": 289.72, "end": 294.8, "text": " the best, and then we'll talk about later about our approaches for CI pipelines or restricted", "tokens": [264, 1151, 11, 293, 550, 321, 603, 751, 466, 1780, 466, 527, 11587, 337, 37777, 40168, 420, 20608], "temperature": 0.0, "avg_logprob": -0.1708464475022149, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0001272991648875177}, {"id": 62, "seek": 29480, "start": 294.8, "end": 298.04, "text": " environments like the cloud.", "tokens": [12388, 411, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 63, "seek": 29480, "start": 298.04, "end": 303.08, "text": " So first of all, I think it makes sense, although I know people tend to hate diagrams", "tokens": [407, 700, 295, 439, 11, 286, 519, 309, 1669, 2020, 11, 4878, 286, 458, 561, 3928, 281, 4700, 36709], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 64, "seek": 29480, "start": 303.08, "end": 309.96000000000004, "text": " or flowcharts to some degree, but I think it makes sense to quickly go over how the concept", "tokens": [420, 3095, 339, 11814, 281, 512, 4314, 11, 457, 286, 519, 309, 1669, 2020, 281, 2661, 352, 670, 577, 264, 3410], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 65, "seek": 29480, "start": 309.96000000000004, "end": 313.44, "text": " of the tool works from a high-level perspective.", "tokens": [295, 264, 2290, 1985, 490, 257, 1090, 12, 12418, 4585, 13], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 66, "seek": 29480, "start": 313.44, "end": 318.44, "text": " So in order to measure software, we follow the container-based approach.", "tokens": [407, 294, 1668, 281, 3481, 4722, 11, 321, 1524, 264, 10129, 12, 6032, 3109, 13], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 67, "seek": 29480, "start": 318.44, "end": 324.0, "text": " So we assume that your software is already in a containerized format or can be put in", "tokens": [407, 321, 6552, 300, 428, 4722, 307, 1217, 294, 257, 10129, 1602, 7877, 420, 393, 312, 829, 294], "temperature": 0.0, "avg_logprob": -0.09245591024750645, "compression_ratio": 1.656, "no_speech_prob": 5.8288285799790174e-05}, {"id": 68, "seek": 32400, "start": 324.0, "end": 325.36, "text": " such a format.", "tokens": [1270, 257, 7877, 13], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 69, "seek": 32400, "start": 325.36, "end": 329.84, "text": " So for instance, even a Firefox browser, if you want to measure desktop applications,", "tokens": [407, 337, 5197, 11, 754, 257, 46613, 11185, 11, 498, 291, 528, 281, 3481, 14502, 5821, 11], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 70, "seek": 32400, "start": 329.84, "end": 333.08, "text": " can be put in a container and be measured with our tool.", "tokens": [393, 312, 829, 294, 257, 10129, 293, 312, 12690, 365, 527, 2290, 13], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 71, "seek": 32400, "start": 333.08, "end": 339.36, "text": " Also machine learning applications, simple command line applications, but also web applications.", "tokens": [2743, 3479, 2539, 5821, 11, 2199, 5622, 1622, 5821, 11, 457, 611, 3670, 5821, 13], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 72, "seek": 32400, "start": 339.36, "end": 344.4, "text": " Typically when you develop software, you already have infrastructure files like Docker files,", "tokens": [23129, 562, 291, 1499, 4722, 11, 291, 1217, 362, 6896, 7098, 411, 33772, 7098, 11], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 73, "seek": 32400, "start": 344.4, "end": 350.6, "text": " Docker compose file, or even a Kubernetes file available, which our tool can consume", "tokens": [33772, 35925, 3991, 11, 420, 754, 257, 23145, 3991, 2435, 11, 597, 527, 2290, 393, 14732], "temperature": 0.0, "avg_logprob": -0.16746422585020673, "compression_ratio": 1.6718146718146718, "no_speech_prob": 9.169447730528191e-05}, {"id": 74, "seek": 35060, "start": 350.6, "end": 358.16, "text": " in all fairness, Kubernetes is still a work in progress, but Docker files can consume.", "tokens": [294, 439, 29765, 11, 23145, 307, 920, 257, 589, 294, 4205, 11, 457, 33772, 7098, 393, 14732, 13], "temperature": 0.0, "avg_logprob": -0.12552487095700035, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.198358146823011e-05}, {"id": 75, "seek": 35060, "start": 358.16, "end": 363.64000000000004, "text": " And then what the tool basically orchestrates the containers and attaches every reporter", "tokens": [400, 550, 437, 264, 2290, 1936, 14161, 12507, 264, 17089, 293, 49404, 633, 19152], "temperature": 0.0, "avg_logprob": -0.12552487095700035, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.198358146823011e-05}, {"id": 76, "seek": 35060, "start": 363.64000000000004, "end": 366.40000000000003, "text": " that you want in terms of measuring metrics.", "tokens": [300, 291, 528, 294, 2115, 295, 13389, 16367, 13], "temperature": 0.0, "avg_logprob": -0.12552487095700035, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.198358146823011e-05}, {"id": 77, "seek": 35060, "start": 366.40000000000003, "end": 372.0, "text": " So here we are still very similar to typical data logging approaches like Datadoc does", "tokens": [407, 510, 321, 366, 920, 588, 2531, 281, 7476, 1412, 27991, 11587, 411, 9315, 345, 905, 775], "temperature": 0.0, "avg_logprob": -0.12552487095700035, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.198358146823011e-05}, {"id": 78, "seek": 35060, "start": 372.0, "end": 374.48, "text": " it for instance, or other big players.", "tokens": [309, 337, 5197, 11, 420, 661, 955, 4150, 13], "temperature": 0.0, "avg_logprob": -0.12552487095700035, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.198358146823011e-05}, {"id": 79, "seek": 37448, "start": 374.48, "end": 380.6, "text": " So the memory, the AC power, DC power, the network traffic, CPU percentage, CPU and RAM", "tokens": [407, 264, 4675, 11, 264, 8157, 1347, 11, 9114, 1347, 11, 264, 3209, 6419, 11, 13199, 9668, 11, 13199, 293, 14561], "temperature": 0.0, "avg_logprob": -0.0960913430089536, "compression_ratio": 1.506122448979592, "no_speech_prob": 1.8057686247630045e-05}, {"id": 80, "seek": 37448, "start": 380.6, "end": 387.68, "text": " is all locked during the execution of what we call a standard usage scenario.", "tokens": [307, 439, 9376, 1830, 264, 15058, 295, 437, 321, 818, 257, 3832, 14924, 9005, 13], "temperature": 0.0, "avg_logprob": -0.0960913430089536, "compression_ratio": 1.506122448979592, "no_speech_prob": 1.8057686247630045e-05}, {"id": 81, "seek": 37448, "start": 387.68, "end": 392.12, "text": " So in the first couple of slides, I've shown you the concept of looking at software from", "tokens": [407, 294, 264, 700, 1916, 295, 9788, 11, 286, 600, 4898, 291, 264, 3410, 295, 1237, 412, 4722, 490], "temperature": 0.0, "avg_logprob": -0.0960913430089536, "compression_ratio": 1.506122448979592, "no_speech_prob": 1.8057686247630045e-05}, {"id": 82, "seek": 37448, "start": 392.12, "end": 394.84000000000003, "text": " how is it typically used.", "tokens": [577, 307, 309, 5850, 1143, 13], "temperature": 0.0, "avg_logprob": -0.0960913430089536, "compression_ratio": 1.506122448979592, "no_speech_prob": 1.8057686247630045e-05}, {"id": 83, "seek": 37448, "start": 394.84000000000003, "end": 399.44, "text": " And people already have thought about this concept quite a lot when they make end-to-end", "tokens": [400, 561, 1217, 362, 1194, 466, 341, 3410, 1596, 257, 688, 562, 436, 652, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.0960913430089536, "compression_ratio": 1.506122448979592, "no_speech_prob": 1.8057686247630045e-05}, {"id": 84, "seek": 39944, "start": 399.44, "end": 405.18, "text": " tests with their software, because this is a typical flow that a user goes through in", "tokens": [6921, 365, 641, 4722, 11, 570, 341, 307, 257, 7476, 3095, 300, 257, 4195, 1709, 807, 294], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 85, "seek": 39944, "start": 405.18, "end": 409.92, "text": " your application, or unit tests, which might be very reduced amounts of functionality that", "tokens": [428, 3861, 11, 420, 4985, 6921, 11, 597, 1062, 312, 588, 9212, 11663, 295, 14980, 300], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 86, "seek": 39944, "start": 409.92, "end": 416.15999999999997, "text": " is tested in a block, or benchmarks that are already inside of the software repository,", "tokens": [307, 8246, 294, 257, 3461, 11, 420, 43751, 300, 366, 1217, 1854, 295, 264, 4722, 25841, 11], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 87, "seek": 39944, "start": 416.15999999999997, "end": 421.28, "text": " session replays, shell scripts, build files that basically measure where we could measure", "tokens": [5481, 23836, 82, 11, 8720, 23294, 11, 1322, 7098, 300, 1936, 3481, 689, 321, 727, 3481], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 88, "seek": 39944, "start": 421.28, "end": 422.28, "text": " your build process.", "tokens": [428, 1322, 1399, 13], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 89, "seek": 39944, "start": 422.28, "end": 427.44, "text": " All of this is already available typically, and our tool can consume these files, will", "tokens": [1057, 295, 341, 307, 1217, 2435, 5850, 11, 293, 527, 2290, 393, 14732, 613, 7098, 11, 486], "temperature": 0.0, "avg_logprob": -0.18081757573798152, "compression_ratio": 1.7011070110701108, "no_speech_prob": 6.604877125937492e-05}, {"id": 90, "seek": 42744, "start": 427.44, "end": 433.96, "text": " run these workflows and then tell you the energy budget over the time of this run in", "tokens": [1190, 613, 43461, 293, 550, 980, 291, 264, 2281, 4706, 670, 264, 565, 295, 341, 1190, 294], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 91, "seek": 42744, "start": 433.96, "end": 434.96, "text": " particular.", "tokens": [1729, 13], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 92, "seek": 42744, "start": 434.96, "end": 439.2, "text": " This slide is more just, if you're not too familiar with Docker, the idea is just to", "tokens": [639, 4137, 307, 544, 445, 11, 498, 291, 434, 406, 886, 4963, 365, 33772, 11, 264, 1558, 307, 445, 281], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 93, "seek": 42744, "start": 439.2, "end": 444.44, "text": " have every service or every component of the application in a separate container, so that", "tokens": [362, 633, 2643, 420, 633, 6542, 295, 264, 3861, 294, 257, 4994, 10129, 11, 370, 300], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 94, "seek": 42744, "start": 444.44, "end": 451.0, "text": " we can later on better granularize the metrics and better look at which component might be", "tokens": [321, 393, 1780, 322, 1101, 39962, 1125, 264, 16367, 293, 1101, 574, 412, 597, 6542, 1062, 312], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 95, "seek": 42744, "start": 451.0, "end": 456.96, "text": " interesting to look at if you want to do energy optimizations in particular.", "tokens": [1880, 281, 574, 412, 498, 291, 528, 281, 360, 2281, 5028, 14455, 294, 1729, 13], "temperature": 0.0, "avg_logprob": -0.13805106134697942, "compression_ratio": 1.7351778656126482, "no_speech_prob": 8.219962910516188e-05}, {"id": 96, "seek": 45696, "start": 456.96, "end": 460.56, "text": " When you use the tool, and I will just go quickly over that and then probably go with", "tokens": [1133, 291, 764, 264, 2290, 11, 293, 286, 486, 445, 352, 2661, 670, 300, 293, 550, 1391, 352, 365], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 97, "seek": 45696, "start": 460.56, "end": 464.52, "text": " you through a live version of what we are hosting at the moment, you will get a lot", "tokens": [291, 807, 257, 1621, 3037, 295, 437, 321, 366, 16058, 412, 264, 1623, 11, 291, 486, 483, 257, 688], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 98, "seek": 45696, "start": 464.52, "end": 465.52, "text": " of metrics.", "tokens": [295, 16367, 13], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 99, "seek": 45696, "start": 465.52, "end": 470.56, "text": " So you will obviously get something like the CPU utilization, or the average memory that", "tokens": [407, 291, 486, 2745, 483, 746, 411, 264, 13199, 37074, 11, 420, 264, 4274, 4675, 300], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 100, "seek": 45696, "start": 470.56, "end": 473.79999999999995, "text": " was used, or maybe the network bandwidth that was used.", "tokens": [390, 1143, 11, 420, 1310, 264, 3209, 23647, 300, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 101, "seek": 45696, "start": 473.79999999999995, "end": 479.88, "text": " But what is interesting for this dashboard, and basically it's USP, is that you get also", "tokens": [583, 437, 307, 1880, 337, 341, 18342, 11, 293, 1936, 309, 311, 2546, 47, 11, 307, 300, 291, 483, 611], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 102, "seek": 45696, "start": 479.88, "end": 485.08, "text": " the energy metrics from the CPU, from the memory, you get a calculation what the network", "tokens": [264, 2281, 16367, 490, 264, 13199, 11, 490, 264, 4675, 11, 291, 483, 257, 17108, 437, 264, 3209], "temperature": 0.0, "avg_logprob": -0.12120640762453157, "compression_ratio": 1.7809187279151943, "no_speech_prob": 4.069303395226598e-05}, {"id": 103, "seek": 48508, "start": 485.08, "end": 493.03999999999996, "text": " has used in energy, and you get convoluted or basically aggregated values where it makes", "tokens": [575, 1143, 294, 2281, 11, 293, 291, 483, 3754, 2308, 292, 420, 1936, 16743, 770, 4190, 689, 309, 1669], "temperature": 0.0, "avg_logprob": -0.1194690431867327, "compression_ratio": 1.7510204081632652, "no_speech_prob": 5.307337414706126e-05}, {"id": 104, "seek": 48508, "start": 493.03999999999996, "end": 498.52, "text": " often sense to look at CPU and memory in conjunction, or it makes sense to look at all the metrics", "tokens": [2049, 2020, 281, 574, 412, 13199, 293, 4675, 294, 27482, 11, 420, 309, 1669, 2020, 281, 574, 412, 439, 264, 16367], "temperature": 0.0, "avg_logprob": -0.1194690431867327, "compression_ratio": 1.7510204081632652, "no_speech_prob": 5.307337414706126e-05}, {"id": 105, "seek": 48508, "start": 498.52, "end": 503.79999999999995, "text": " that you have available to get something like a total energy budget.", "tokens": [300, 291, 362, 2435, 281, 483, 746, 411, 257, 3217, 2281, 4706, 13], "temperature": 0.0, "avg_logprob": -0.1194690431867327, "compression_ratio": 1.7510204081632652, "no_speech_prob": 5.307337414706126e-05}, {"id": 106, "seek": 48508, "start": 503.79999999999995, "end": 508.15999999999997, "text": " Then you obviously can look also at the AC, so at the wall plugs, so not only what is", "tokens": [1396, 291, 2745, 393, 574, 611, 412, 264, 8157, 11, 370, 412, 264, 2929, 33899, 11, 370, 406, 787, 437, 307], "temperature": 0.0, "avg_logprob": -0.1194690431867327, "compression_ratio": 1.7510204081632652, "no_speech_prob": 5.307337414706126e-05}, {"id": 107, "seek": 48508, "start": 508.15999999999997, "end": 513.0, "text": " your CPU and your RAM using, but what is the total machine using, or something that we", "tokens": [428, 13199, 293, 428, 14561, 1228, 11, 457, 437, 307, 264, 3217, 3479, 1228, 11, 420, 746, 300, 321], "temperature": 0.0, "avg_logprob": -0.1194690431867327, "compression_ratio": 1.7510204081632652, "no_speech_prob": 5.307337414706126e-05}, {"id": 108, "seek": 51300, "start": 513.0, "end": 517.76, "text": " have in our lab as a setup, you just look at the main board, so not on the outside of", "tokens": [362, 294, 527, 2715, 382, 257, 8657, 11, 291, 445, 574, 412, 264, 2135, 3150, 11, 370, 406, 322, 264, 2380, 295], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 109, "seek": 51300, "start": 517.76, "end": 522.12, "text": " the PSU, so what is basically plugged in the desktop computer, but only the power that", "tokens": [264, 8168, 52, 11, 370, 437, 307, 1936, 25679, 294, 264, 14502, 3820, 11, 457, 787, 264, 1347, 300], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 110, "seek": 51300, "start": 522.12, "end": 525.16, "text": " flows directly into the main board.", "tokens": [12867, 3838, 666, 264, 2135, 3150, 13], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 111, "seek": 51300, "start": 525.16, "end": 530.3, "text": " And here you can see that our tool automatically calculates the CO2 budget based on the energy", "tokens": [400, 510, 291, 393, 536, 300, 527, 2290, 6772, 4322, 1024, 264, 3002, 17, 4706, 2361, 322, 264, 2281], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 112, "seek": 51300, "start": 530.3, "end": 534.52, "text": " that it has used for this run.", "tokens": [300, 309, 575, 1143, 337, 341, 1190, 13], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 113, "seek": 51300, "start": 534.52, "end": 539.68, "text": " The tool also shows you which reporters have been used in an overview, and then it tells", "tokens": [440, 2290, 611, 3110, 291, 597, 26249, 362, 668, 1143, 294, 364, 12492, 11, 293, 550, 309, 5112], "temperature": 0.0, "avg_logprob": -0.10842221696800161, "compression_ratio": 1.6206896551724137, "no_speech_prob": 5.562048318097368e-05}, {"id": 114, "seek": 53968, "start": 539.68, "end": 544.9599999999999, "text": " you a lot of charts, so this is a sample chart, and what the tool can basically give you is", "tokens": [291, 257, 688, 295, 17767, 11, 370, 341, 307, 257, 6889, 6927, 11, 293, 437, 264, 2290, 393, 1936, 976, 291, 307], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 115, "seek": 53968, "start": 544.9599999999999, "end": 550.0, "text": " not only an overview capability, but also an introspection where you, for instance,", "tokens": [406, 787, 364, 12492, 13759, 11, 457, 611, 364, 560, 2635, 19997, 689, 291, 11, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 116, "seek": 53968, "start": 550.0, "end": 552.92, "text": " are interested in the idle time of the application.", "tokens": [366, 3102, 294, 264, 30650, 565, 295, 264, 3861, 13], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 117, "seek": 53968, "start": 552.92, "end": 557.12, "text": " So what is my application doing when no user is interacting with it?", "tokens": [407, 437, 307, 452, 3861, 884, 562, 572, 4195, 307, 18017, 365, 309, 30], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 118, "seek": 53968, "start": 557.12, "end": 563.4599999999999, "text": " Is it actually using energy, and is this too much energy for my belief or for the belief", "tokens": [1119, 309, 767, 1228, 2281, 11, 293, 307, 341, 886, 709, 2281, 337, 452, 7107, 420, 337, 264, 7107], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 119, "seek": 53968, "start": 563.4599999999999, "end": 564.9599999999999, "text": " of the community?", "tokens": [295, 264, 1768, 30], "temperature": 0.0, "avg_logprob": -0.11155105817435991, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.00010389741510152817}, {"id": 120, "seek": 56496, "start": 564.96, "end": 569.48, "text": " So for instance, here we have an example of a setup, of a WordPress setup that we have", "tokens": [407, 337, 5197, 11, 510, 321, 362, 364, 1365, 295, 257, 8657, 11, 295, 257, 23239, 8657, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.16827647453915756, "compression_ratio": 1.7576923076923077, "no_speech_prob": 1.80573551915586e-05}, {"id": 121, "seek": 56496, "start": 569.48, "end": 578.44, "text": " done with an Apache, a Puppeteer container that runs Chrome, and also a MariaDB instance.", "tokens": [1096, 365, 364, 46597, 11, 257, 430, 10504, 3498, 260, 10129, 300, 6676, 15327, 11, 293, 611, 257, 12734, 27735, 5197, 13], "temperature": 0.0, "avg_logprob": -0.16827647453915756, "compression_ratio": 1.7576923076923077, "no_speech_prob": 1.80573551915586e-05}, {"id": 122, "seek": 56496, "start": 578.44, "end": 582.2800000000001, "text": " And you can see here that here are a couple of requests that have been done to the WordPress", "tokens": [400, 291, 393, 536, 510, 300, 510, 366, 257, 1916, 295, 12475, 300, 362, 668, 1096, 281, 264, 23239], "temperature": 0.0, "avg_logprob": -0.16827647453915756, "compression_ratio": 1.7576923076923077, "no_speech_prob": 1.80573551915586e-05}, {"id": 123, "seek": 56496, "start": 582.2800000000001, "end": 587.44, "text": " instance, and then we are basically just idling, but still the web server is doing quite some", "tokens": [5197, 11, 293, 550, 321, 366, 1936, 445, 4496, 1688, 11, 457, 920, 264, 3670, 7154, 307, 884, 1596, 512], "temperature": 0.0, "avg_logprob": -0.16827647453915756, "compression_ratio": 1.7576923076923077, "no_speech_prob": 1.80573551915586e-05}, {"id": 124, "seek": 56496, "start": 587.44, "end": 592.48, "text": " work, and there have been no web sockets active, so why is there server and database activity", "tokens": [589, 11, 293, 456, 362, 668, 572, 3670, 370, 11984, 4967, 11, 370, 983, 307, 456, 7154, 293, 8149, 5191], "temperature": 0.0, "avg_logprob": -0.16827647453915756, "compression_ratio": 1.7576923076923077, "no_speech_prob": 1.80573551915586e-05}, {"id": 125, "seek": 59248, "start": 592.48, "end": 599.12, "text": " here? Is this valid, is this maybe some caching, some housekeeping, or is this unintended behavior?", "tokens": [510, 30, 1119, 341, 7363, 11, 307, 341, 1310, 512, 269, 2834, 11, 512, 48033, 11, 420, 307, 341, 49902, 5223, 30], "temperature": 0.0, "avg_logprob": -0.09696374220006607, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00016602266987320036}, {"id": 126, "seek": 59248, "start": 599.12, "end": 604.9200000000001, "text": " We picture that our tool could highlight such energy hotspot or energy malfunctions, as", "tokens": [492, 3036, 300, 527, 2290, 727, 5078, 1270, 2281, 36121, 17698, 420, 2281, 41318, 409, 3916, 11, 382], "temperature": 0.0, "avg_logprob": -0.09696374220006607, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00016602266987320036}, {"id": 127, "seek": 59248, "start": 604.9200000000001, "end": 610.52, "text": " we call them, to better understand how software uses energy.", "tokens": [321, 818, 552, 11, 281, 1101, 1223, 577, 4722, 4960, 2281, 13], "temperature": 0.0, "avg_logprob": -0.09696374220006607, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00016602266987320036}, {"id": 128, "seek": 59248, "start": 610.52, "end": 616.44, "text": " You can also look at energy anomalies, so we work sometimes with features like TurboBoost,", "tokens": [509, 393, 611, 574, 412, 2281, 24769, 48872, 11, 370, 321, 589, 2171, 365, 4122, 411, 35848, 22493, 555, 11], "temperature": 0.0, "avg_logprob": -0.09696374220006607, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00016602266987320036}, {"id": 129, "seek": 59248, "start": 616.44, "end": 620.6, "text": " which is typically not turned on in cloud environments, but very often for desktops,", "tokens": [597, 307, 5850, 406, 3574, 322, 294, 4588, 12388, 11, 457, 588, 2049, 337, 730, 2320, 3370, 11], "temperature": 0.0, "avg_logprob": -0.09696374220006607, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.00016602266987320036}, {"id": 130, "seek": 62060, "start": 620.6, "end": 624.52, "text": " which brings a processor in kind of like an overdrive state so that it can react very", "tokens": [597, 5607, 257, 15321, 294, 733, 295, 411, 364, 19853, 8003, 1785, 370, 300, 309, 393, 4515, 588], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 131, "seek": 62060, "start": 624.52, "end": 628.4, "text": " quickly in a frequency above its normal frequency.", "tokens": [2661, 294, 257, 7893, 3673, 1080, 2710, 7893, 13], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 132, "seek": 62060, "start": 628.4, "end": 634.64, "text": " However, what we have done here in this example, we have run a constant CPU utilization, but", "tokens": [2908, 11, 437, 321, 362, 1096, 510, 294, 341, 1365, 11, 321, 362, 1190, 257, 5754, 13199, 37074, 11, 457], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 133, "seek": 62060, "start": 634.64, "end": 639.72, "text": " as you can see here, the CPU clocks at different frequency over the time, and sometimes it", "tokens": [382, 291, 393, 536, 510, 11, 264, 13199, 41528, 412, 819, 7893, 670, 264, 565, 11, 293, 2171, 309], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 134, "seek": 62060, "start": 639.72, "end": 643.6, "text": " uses exponentially more energy for the same tasks.", "tokens": [4960, 37330, 544, 2281, 337, 264, 912, 9608, 13], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 135, "seek": 62060, "start": 643.6, "end": 650.2, "text": " So it finishes quicker, but it uses more than only a linear amount more of energy to do", "tokens": [407, 309, 23615, 16255, 11, 457, 309, 4960, 544, 813, 787, 257, 8213, 2372, 544, 295, 2281, 281, 360], "temperature": 0.0, "avg_logprob": -0.12427977279380516, "compression_ratio": 1.6510791366906474, "no_speech_prob": 8.887444710126147e-05}, {"id": 136, "seek": 65020, "start": 650.2, "end": 651.32, "text": " the task.", "tokens": [264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 137, "seek": 65020, "start": 651.32, "end": 655.6, "text": " So this is a very interesting insight that our tool can, for instance, deliver when you", "tokens": [407, 341, 307, 257, 588, 1880, 11269, 300, 527, 2290, 393, 11, 337, 5197, 11, 4239, 562, 291], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 138, "seek": 65020, "start": 655.6, "end": 660.0200000000001, "text": " try for energy optimizations of your software.", "tokens": [853, 337, 2281, 5028, 14455, 295, 428, 4722, 13], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 139, "seek": 65020, "start": 660.0200000000001, "end": 665.0400000000001, "text": " So what is the whole idea that we have behind all this project? And let me move myself down", "tokens": [407, 437, 307, 264, 1379, 1558, 300, 321, 362, 2261, 439, 341, 1716, 30, 400, 718, 385, 1286, 2059, 760], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 140, "seek": 65020, "start": 665.0400000000001, "end": 669.12, "text": " here a little bit so you can see the full slide.", "tokens": [510, 257, 707, 857, 370, 291, 393, 536, 264, 1577, 4137, 13], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 141, "seek": 65020, "start": 669.12, "end": 677.0400000000001, "text": " We want to create an open source community or a green software community that focuses", "tokens": [492, 528, 281, 1884, 364, 1269, 4009, 1768, 420, 257, 3092, 4722, 1768, 300, 16109], "temperature": 0.0, "avg_logprob": -0.11032033228612208, "compression_ratio": 1.578723404255319, "no_speech_prob": 6.401581777026877e-05}, {"id": 142, "seek": 67704, "start": 677.04, "end": 682.4399999999999, "text": " on the transparency of software so that you have basically an interface, which we call", "tokens": [322, 264, 17131, 295, 4722, 370, 300, 291, 362, 1936, 364, 9226, 11, 597, 321, 818], "temperature": 0.0, "avg_logprob": -0.1384813965007823, "compression_ratio": 1.8185654008438819, "no_speech_prob": 6.401559221558273e-05}, {"id": 143, "seek": 67704, "start": 682.4399999999999, "end": 689.1999999999999, "text": " the usage scenario, where you can measure software against and then ask later on questions", "tokens": [264, 14924, 9005, 11, 689, 291, 393, 3481, 4722, 1970, 293, 550, 1029, 1780, 322, 1651], "temperature": 0.0, "avg_logprob": -0.1384813965007823, "compression_ratio": 1.8185654008438819, "no_speech_prob": 6.401559221558273e-05}, {"id": 144, "seek": 67704, "start": 689.1999999999999, "end": 693.8, "text": " against a database or against an API, which has measured all these softwares, questions", "tokens": [1970, 257, 8149, 420, 1970, 364, 9362, 11, 597, 575, 12690, 439, 613, 2787, 4151, 495, 11, 1651], "temperature": 0.0, "avg_logprob": -0.1384813965007823, "compression_ratio": 1.8185654008438819, "no_speech_prob": 6.401559221558273e-05}, {"id": 145, "seek": 67704, "start": 693.8, "end": 698.8, "text": " like how much does this software consume? Is there a more carbon friendly alternative,", "tokens": [411, 577, 709, 775, 341, 4722, 14732, 30, 1119, 456, 257, 544, 5954, 9208, 8535, 11], "temperature": 0.0, "avg_logprob": -0.1384813965007823, "compression_ratio": 1.8185654008438819, "no_speech_prob": 6.401559221558273e-05}, {"id": 146, "seek": 67704, "start": 698.8, "end": 704.48, "text": " or is there a software that makes less energy requests, less network requests?", "tokens": [420, 307, 456, 257, 4722, 300, 1669, 1570, 2281, 12475, 11, 1570, 3209, 12475, 30], "temperature": 0.0, "avg_logprob": -0.1384813965007823, "compression_ratio": 1.8185654008438819, "no_speech_prob": 6.401559221558273e-05}, {"id": 147, "seek": 70448, "start": 704.48, "end": 710.04, "text": " The idea, if these softwares are available in your country, so Yucca to my knowledge", "tokens": [440, 1558, 11, 498, 613, 2787, 4151, 495, 366, 2435, 294, 428, 1941, 11, 370, 398, 1311, 496, 281, 452, 3601], "temperature": 0.0, "avg_logprob": -0.17461438698343712, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.943788760807365e-05}, {"id": 148, "seek": 70448, "start": 710.04, "end": 715.72, "text": " is, for instance, from the US and code check is more like a German application, is we want", "tokens": [307, 11, 337, 5197, 11, 490, 264, 2546, 293, 3089, 1520, 307, 544, 411, 257, 6521, 3861, 11, 307, 321, 528], "temperature": 0.0, "avg_logprob": -0.17461438698343712, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.943788760807365e-05}, {"id": 149, "seek": 70448, "start": 715.72, "end": 718.96, "text": " to be the Yucca or the code check of software.", "tokens": [281, 312, 264, 398, 1311, 496, 420, 264, 3089, 1520, 295, 4722, 13], "temperature": 0.0, "avg_logprob": -0.17461438698343712, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.943788760807365e-05}, {"id": 150, "seek": 70448, "start": 718.96, "end": 725.84, "text": " So we want to deliver answers to developers where they can ask questions about the energy", "tokens": [407, 321, 528, 281, 4239, 6338, 281, 8849, 689, 436, 393, 1029, 1651, 466, 264, 2281], "temperature": 0.0, "avg_logprob": -0.17461438698343712, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.943788760807365e-05}, {"id": 151, "seek": 70448, "start": 725.84, "end": 731.64, "text": " budgeting of a library, of a software, or of a functionality by providing a framework to", "tokens": [47855, 295, 257, 6405, 11, 295, 257, 4722, 11, 420, 295, 257, 14980, 538, 6530, 257, 8388, 281], "temperature": 0.0, "avg_logprob": -0.17461438698343712, "compression_ratio": 1.6502057613168724, "no_speech_prob": 3.943788760807365e-05}, {"id": 152, "seek": 73164, "start": 731.64, "end": 735.6, "text": " make these measurements.", "tokens": [652, 613, 15383, 13], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 153, "seek": 73164, "start": 735.6, "end": 739.48, "text": " So let me move up here again and then back to the slides.", "tokens": [407, 718, 385, 1286, 493, 510, 797, 293, 550, 646, 281, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 154, "seek": 73164, "start": 739.48, "end": 743.56, "text": " So let me show you our other tools that we believe are needed to build an ecosystem around", "tokens": [407, 718, 385, 855, 291, 527, 661, 3873, 300, 321, 1697, 366, 2978, 281, 1322, 364, 11311, 926], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 155, "seek": 73164, "start": 743.56, "end": 749.4399999999999, "text": " green software because software is not only running in desktop environments or is not", "tokens": [3092, 4722, 570, 4722, 307, 406, 787, 2614, 294, 14502, 12388, 420, 307, 406], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 156, "seek": 73164, "start": 749.4399999999999, "end": 754.08, "text": " only on a single machine, it also runs a lot in the clouds, where these measurements that", "tokens": [787, 322, 257, 2167, 3479, 11, 309, 611, 6676, 257, 688, 294, 264, 12193, 11, 689, 613, 15383, 300], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 157, "seek": 73164, "start": 754.08, "end": 758.64, "text": " we have, and I would like to encourage you to read a bit on what sensors are available", "tokens": [321, 362, 11, 293, 286, 576, 411, 281, 5373, 291, 281, 1401, 257, 857, 322, 437, 14840, 366, 2435], "temperature": 0.0, "avg_logprob": -0.08572098788093119, "compression_ratio": 1.744, "no_speech_prob": 7.601817196700722e-05}, {"id": 158, "seek": 75864, "start": 758.64, "end": 765.4399999999999, "text": " in our tool, but where these sensors are not available, which is for instance in the cloud.", "tokens": [294, 527, 2290, 11, 457, 689, 613, 14840, 366, 406, 2435, 11, 597, 307, 337, 5197, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.12162978523655942, "compression_ratio": 1.6527196652719665, "no_speech_prob": 3.024025136255659e-05}, {"id": 159, "seek": 75864, "start": 765.4399999999999, "end": 771.12, "text": " So let me bring up my browser again.", "tokens": [407, 718, 385, 1565, 493, 452, 11185, 797, 13], "temperature": 0.0, "avg_logprob": -0.12162978523655942, "compression_ratio": 1.6527196652719665, "no_speech_prob": 3.024025136255659e-05}, {"id": 160, "seek": 75864, "start": 771.12, "end": 774.72, "text": " So if you are on the homepage and you have seen the green metrics tool that I've just", "tokens": [407, 498, 291, 366, 322, 264, 31301, 293, 291, 362, 1612, 264, 3092, 16367, 2290, 300, 286, 600, 445], "temperature": 0.0, "avg_logprob": -0.12162978523655942, "compression_ratio": 1.6527196652719665, "no_speech_prob": 3.024025136255659e-05}, {"id": 161, "seek": 75864, "start": 774.72, "end": 781.0, "text": " talked about, you'll also see that we have the cloud energy project and the EcoCI project.", "tokens": [2825, 466, 11, 291, 603, 611, 536, 300, 321, 362, 264, 4588, 2281, 1716, 293, 264, 40263, 25240, 1716, 13], "temperature": 0.0, "avg_logprob": -0.12162978523655942, "compression_ratio": 1.6527196652719665, "no_speech_prob": 3.024025136255659e-05}, {"id": 162, "seek": 75864, "start": 781.0, "end": 788.4399999999999, "text": " So EcoCI focuses on measuring the energy of software in a continuous integration pipeline", "tokens": [407, 40263, 25240, 16109, 322, 13389, 264, 2281, 295, 4722, 294, 257, 10957, 10980, 15517], "temperature": 0.0, "avg_logprob": -0.12162978523655942, "compression_ratio": 1.6527196652719665, "no_speech_prob": 3.024025136255659e-05}, {"id": 163, "seek": 78844, "start": 788.44, "end": 790.8000000000001, "text": " that for instance runs in a virtual machine.", "tokens": [300, 337, 5197, 6676, 294, 257, 6374, 3479, 13], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 164, "seek": 78844, "start": 790.8000000000001, "end": 793.32, "text": " Our focus is currently on GitHub actions.", "tokens": [2621, 1879, 307, 4362, 322, 23331, 5909, 13], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 165, "seek": 78844, "start": 793.32, "end": 798.36, "text": " In order to estimate the energy in a virtual machine, because you cannot measure, you have", "tokens": [682, 1668, 281, 12539, 264, 2281, 294, 257, 6374, 3479, 11, 570, 291, 2644, 3481, 11, 291, 362], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 166, "seek": 78844, "start": 798.36, "end": 802.9200000000001, "text": " no access to the wall plug in the data center, you have no access to sensors in the CPU or", "tokens": [572, 2105, 281, 264, 2929, 5452, 294, 264, 1412, 3056, 11, 291, 362, 572, 2105, 281, 14840, 294, 264, 13199, 420], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 167, "seek": 78844, "start": 802.9200000000001, "end": 807.44, "text": " whatever, you have to estimate the machine based on measurements that you already have", "tokens": [2035, 11, 291, 362, 281, 12539, 264, 3479, 2361, 322, 15383, 300, 291, 1217, 362], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 168, "seek": 78844, "start": 807.44, "end": 809.6800000000001, "text": " for the same hardware.", "tokens": [337, 264, 912, 8837, 13], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 169, "seek": 78844, "start": 809.6800000000001, "end": 814.5600000000001, "text": " If you click on cloud energy, you can see here that we have based our machine learning", "tokens": [759, 291, 2052, 322, 4588, 2281, 11, 291, 393, 536, 510, 300, 321, 362, 2361, 527, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.11788993315263228, "compression_ratio": 1.86, "no_speech_prob": 4.264465314918198e-05}, {"id": 170, "seek": 81456, "start": 814.56, "end": 820.38, "text": " model on a research paper from InterACTC and the University of London, and they have basically", "tokens": [2316, 322, 257, 2132, 3035, 490, 5751, 4378, 18238, 293, 264, 3535, 295, 7042, 11, 293, 436, 362, 1936], "temperature": 0.0, "avg_logprob": -0.10616925321979287, "compression_ratio": 1.5733333333333333, "no_speech_prob": 8.74940596986562e-05}, {"id": 171, "seek": 81456, "start": 820.38, "end": 827.88, "text": " taken the data from the spec power database, which is an open database for servers that", "tokens": [2726, 264, 1412, 490, 264, 1608, 1347, 8149, 11, 597, 307, 364, 1269, 8149, 337, 15909, 300], "temperature": 0.0, "avg_logprob": -0.10616925321979287, "compression_ratio": 1.5733333333333333, "no_speech_prob": 8.74940596986562e-05}, {"id": 172, "seek": 81456, "start": 827.88, "end": 833.76, "text": " have been measured just with a fixed workload to compare it against each other.", "tokens": [362, 668, 12690, 445, 365, 257, 6806, 20139, 281, 6794, 309, 1970, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.10616925321979287, "compression_ratio": 1.5733333333333333, "no_speech_prob": 8.74940596986562e-05}, {"id": 173, "seek": 81456, "start": 833.76, "end": 838.68, "text": " And based on this data, we can create a machine learning model, which is also free and open", "tokens": [400, 2361, 322, 341, 1412, 11, 321, 393, 1884, 257, 3479, 2539, 2316, 11, 597, 307, 611, 1737, 293, 1269], "temperature": 0.0, "avg_logprob": -0.10616925321979287, "compression_ratio": 1.5733333333333333, "no_speech_prob": 8.74940596986562e-05}, {"id": 174, "seek": 83868, "start": 838.68, "end": 845.4399999999999, "text": " source to use, that is just a Python tool, which you call with the information that you", "tokens": [4009, 281, 764, 11, 300, 307, 445, 257, 15329, 2290, 11, 597, 291, 818, 365, 264, 1589, 300, 291], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 175, "seek": 83868, "start": 845.4399999999999, "end": 846.4399999999999, "text": " have.", "tokens": [362, 13], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 176, "seek": 83868, "start": 846.4399999999999, "end": 850.04, "text": " So let's say you have the information that your CPU is from Intel, that the frequency", "tokens": [407, 718, 311, 584, 291, 362, 264, 1589, 300, 428, 13199, 307, 490, 19762, 11, 300, 264, 7893], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 177, "seek": 83868, "start": 850.04, "end": 855.0799999999999, "text": " that you're running is 2.6 gigahertz, you have 7 gigabytes of RAM, and you know the CPU", "tokens": [300, 291, 434, 2614, 307, 568, 13, 21, 8741, 64, 35655, 11, 291, 362, 1614, 42741, 295, 14561, 11, 293, 291, 458, 264, 13199], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 178, "seek": 83868, "start": 855.0799999999999, "end": 857.04, "text": " has 24 threads.", "tokens": [575, 4022, 19314, 13], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 179, "seek": 83868, "start": 857.04, "end": 858.56, "text": " But you don't know any more info.", "tokens": [583, 291, 500, 380, 458, 604, 544, 13614, 13], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 180, "seek": 83868, "start": 858.56, "end": 865.88, "text": " You don't know if it's a Skylake processor or a more modern internal processor.", "tokens": [509, 500, 380, 458, 498, 309, 311, 257, 9879, 75, 619, 15321, 420, 257, 544, 4363, 6920, 15321, 13], "temperature": 0.0, "avg_logprob": -0.13690897795531126, "compression_ratio": 1.6680672268907564, "no_speech_prob": 9.02731335372664e-05}, {"id": 181, "seek": 86588, "start": 865.88, "end": 870.08, "text": " You have no more information because the hypervisor limits this to you.", "tokens": [509, 362, 572, 544, 1589, 570, 264, 9848, 16457, 10406, 341, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 182, "seek": 86588, "start": 870.08, "end": 874.08, "text": " So if you give the model more information, it can give you more accurate estimates, but", "tokens": [407, 498, 291, 976, 264, 2316, 544, 1589, 11, 309, 393, 976, 291, 544, 8559, 20561, 11, 457], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 183, "seek": 86588, "start": 874.08, "end": 877.16, "text": " it can also work with the limited information in the cloud.", "tokens": [309, 393, 611, 589, 365, 264, 5567, 1589, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 184, "seek": 86588, "start": 877.16, "end": 882.48, "text": " And then it spits out to the standard out the current watchers that you have been using,", "tokens": [400, 550, 309, 637, 1208, 484, 281, 264, 3832, 484, 264, 2190, 1159, 433, 300, 291, 362, 668, 1228, 11], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 185, "seek": 86588, "start": 882.48, "end": 887.04, "text": " and then you can reuse that in a tool that we build upon that.", "tokens": [293, 550, 291, 393, 26225, 300, 294, 257, 2290, 300, 321, 1322, 3564, 300, 13], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 186, "seek": 86588, "start": 887.04, "end": 890.28, "text": " So now that you've understood that there is a machine learning model behind the idea,", "tokens": [407, 586, 300, 291, 600, 7320, 300, 456, 307, 257, 3479, 2539, 2316, 2261, 264, 1558, 11], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 187, "seek": 86588, "start": 890.28, "end": 892.96, "text": " I would like to bring you to EcoCI.", "tokens": [286, 576, 411, 281, 1565, 291, 281, 40263, 25240, 13], "temperature": 0.0, "avg_logprob": -0.11036859574865122, "compression_ratio": 1.7733812949640289, "no_speech_prob": 2.7107695132144727e-05}, {"id": 188, "seek": 89296, "start": 892.96, "end": 898.32, "text": " So EcoCI is a GitHub action that is based on the work from the Cloud Energy Project", "tokens": [407, 40263, 25240, 307, 257, 23331, 3069, 300, 307, 2361, 322, 264, 589, 490, 264, 8061, 14939, 9849], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 189, "seek": 89296, "start": 898.32, "end": 905.44, "text": " that can give you in a GitHub action the information of how much a CI pipeline has used in terms", "tokens": [300, 393, 976, 291, 294, 257, 23331, 3069, 264, 1589, 295, 577, 709, 257, 37777, 15517, 575, 1143, 294, 2115], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 190, "seek": 89296, "start": 905.44, "end": 907.1600000000001, "text": " of energy.", "tokens": [295, 2281, 13], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 191, "seek": 89296, "start": 907.1600000000001, "end": 915.12, "text": " So if you go, for instance, to the GitHub repository, you can also go to the marketplace.", "tokens": [407, 498, 291, 352, 11, 337, 5197, 11, 281, 264, 23331, 25841, 11, 291, 393, 611, 352, 281, 264, 19455, 13], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 192, "seek": 89296, "start": 915.12, "end": 917.0400000000001, "text": " So we go one step further.", "tokens": [407, 321, 352, 472, 1823, 3052, 13], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 193, "seek": 89296, "start": 917.0400000000001, "end": 920.5600000000001, "text": " And here you can see you can directly use it.", "tokens": [400, 510, 291, 393, 536, 291, 393, 3838, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 194, "seek": 89296, "start": 920.5600000000001, "end": 922.36, "text": " It is very easy to use.", "tokens": [467, 307, 588, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.11487639529033772, "compression_ratio": 1.6506550218340612, "no_speech_prob": 1.028922270052135e-05}, {"id": 195, "seek": 92236, "start": 922.36, "end": 927.52, "text": " It just needs two calls to initialize a tool and then one more call whenever you want to", "tokens": [467, 445, 2203, 732, 5498, 281, 5883, 1125, 257, 2290, 293, 550, 472, 544, 818, 5699, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 196, "seek": 92236, "start": 927.52, "end": 929.16, "text": " get a measurement.", "tokens": [483, 257, 13160, 13], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 197, "seek": 92236, "start": 929.16, "end": 936.48, "text": " And what it does for you, so let's quickly go to our repository where we actually use", "tokens": [400, 437, 309, 775, 337, 291, 11, 370, 718, 311, 2661, 352, 281, 527, 25841, 689, 321, 767, 764], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 198, "seek": 92236, "start": 936.48, "end": 941.72, "text": " GitHub actions to measure every of our workflows in the tool.", "tokens": [23331, 5909, 281, 3481, 633, 295, 527, 43461, 294, 264, 2290, 13], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 199, "seek": 92236, "start": 941.72, "end": 946.76, "text": " So we click on actions, let's say we go to manual test run virtual machine, we click", "tokens": [407, 321, 2052, 322, 5909, 11, 718, 311, 584, 321, 352, 281, 9688, 1500, 1190, 6374, 3479, 11, 321, 2052], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 200, "seek": 92236, "start": 946.76, "end": 948.36, "text": " on main.", "tokens": [322, 2135, 13], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 201, "seek": 92236, "start": 948.36, "end": 951.84, "text": " And you see here, I've run this run yesterday, it succeeded.", "tokens": [400, 291, 536, 510, 11, 286, 600, 1190, 341, 1190, 5186, 11, 309, 20263, 13], "temperature": 0.0, "avg_logprob": -0.13101326977765118, "compression_ratio": 1.6465863453815262, "no_speech_prob": 4.9857473641168326e-05}, {"id": 202, "seek": 95184, "start": 951.84, "end": 954.8000000000001, "text": " So our log tells us, hey, all tests have to work fine.", "tokens": [407, 527, 3565, 5112, 505, 11, 4177, 11, 439, 6921, 362, 281, 589, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 203, "seek": 95184, "start": 954.8000000000001, "end": 957.9200000000001, "text": " So to run a work point fine and also the API.", "tokens": [407, 281, 1190, 257, 589, 935, 2489, 293, 611, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 204, "seek": 95184, "start": 957.9200000000001, "end": 963.48, "text": " And for this run in the Azure Cloud where GitHub actions runs as virtual machines, I", "tokens": [400, 337, 341, 1190, 294, 264, 11969, 8061, 689, 23331, 5909, 6676, 382, 6374, 8379, 11, 286], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 205, "seek": 95184, "start": 963.48, "end": 966.5600000000001, "text": " have used 650 joules of energy.", "tokens": [362, 1143, 38566, 11110, 904, 295, 2281, 13], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 206, "seek": 95184, "start": 966.5600000000001, "end": 968.64, "text": " And you get a nice ASCII graph over time.", "tokens": [400, 291, 483, 257, 1481, 7469, 34, 9503, 4295, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 207, "seek": 95184, "start": 968.64, "end": 972.6800000000001, "text": " We were a bit limited here in the graphs we can display in the GitHub actions overview.", "tokens": [492, 645, 257, 857, 5567, 510, 294, 264, 24877, 321, 393, 4674, 294, 264, 23331, 5909, 12492, 13], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 208, "seek": 95184, "start": 972.6800000000001, "end": 976.6800000000001, "text": " But you can see here at what point in time the energy, for instance, is the highest and", "tokens": [583, 291, 393, 536, 510, 412, 437, 935, 294, 565, 264, 2281, 11, 337, 5197, 11, 307, 264, 6343, 293], "temperature": 0.0, "avg_logprob": -0.1720525935544806, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.9222365153837018e-05}, {"id": 209, "seek": 97668, "start": 976.68, "end": 982.8399999999999, "text": " then maybe look at the later tests if they, if you deem them to be more energy consuming", "tokens": [550, 1310, 574, 412, 264, 1780, 6921, 498, 436, 11, 498, 291, 368, 443, 552, 281, 312, 544, 2281, 19867], "temperature": 0.0, "avg_logprob": -0.14809064662202875, "compression_ratio": 1.752212389380531, "no_speech_prob": 2.7534100809134543e-05}, {"id": 210, "seek": 97668, "start": 982.8399999999999, "end": 989.1999999999999, "text": " than for instance at the start where it was using only a fixed amount of energy.", "tokens": [813, 337, 5197, 412, 264, 722, 689, 309, 390, 1228, 787, 257, 6806, 2372, 295, 2281, 13], "temperature": 0.0, "avg_logprob": -0.14809064662202875, "compression_ratio": 1.752212389380531, "no_speech_prob": 2.7534100809134543e-05}, {"id": 211, "seek": 97668, "start": 989.1999999999999, "end": 995.16, "text": " So this gives a developer and also a user the information how much energy is not only", "tokens": [407, 341, 2709, 257, 10754, 293, 611, 257, 4195, 264, 1589, 577, 709, 2281, 307, 406, 787], "temperature": 0.0, "avg_logprob": -0.14809064662202875, "compression_ratio": 1.752212389380531, "no_speech_prob": 2.7534100809134543e-05}, {"id": 212, "seek": 97668, "start": 995.16, "end": 1001.24, "text": " the software using, but also the development of the software is it maybe using more than", "tokens": [264, 4722, 1228, 11, 457, 611, 264, 3250, 295, 264, 4722, 307, 309, 1310, 1228, 544, 813], "temperature": 0.0, "avg_logprob": -0.14809064662202875, "compression_ratio": 1.752212389380531, "no_speech_prob": 2.7534100809134543e-05}, {"id": 213, "seek": 97668, "start": 1001.24, "end": 1005.4399999999999, "text": " we want as developers or maybe even as a community.", "tokens": [321, 528, 382, 8849, 420, 1310, 754, 382, 257, 1768, 13], "temperature": 0.0, "avg_logprob": -0.14809064662202875, "compression_ratio": 1.752212389380531, "no_speech_prob": 2.7534100809134543e-05}, {"id": 214, "seek": 100544, "start": 1005.44, "end": 1010.8000000000001, "text": " And these are all concept tools to just get a first start of what we, what we think could", "tokens": [400, 613, 366, 439, 3410, 3873, 281, 445, 483, 257, 700, 722, 295, 437, 321, 11, 437, 321, 519, 727], "temperature": 0.0, "avg_logprob": -0.12186383038032346, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.469087070901878e-05}, {"id": 215, "seek": 100544, "start": 1010.8000000000001, "end": 1018.08, "text": " be possible, of what we think could be possible in a new future where software is basically", "tokens": [312, 1944, 11, 295, 437, 321, 519, 727, 312, 1944, 294, 257, 777, 2027, 689, 4722, 307, 1936], "temperature": 0.0, "avg_logprob": -0.12186383038032346, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.469087070901878e-05}, {"id": 216, "seek": 100544, "start": 1018.08, "end": 1025.88, "text": " measured and the data of the, of its usage is constantly published by developers also.", "tokens": [12690, 293, 264, 1412, 295, 264, 11, 295, 1080, 14924, 307, 6460, 6572, 538, 8849, 611, 13], "temperature": 0.0, "avg_logprob": -0.12186383038032346, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.469087070901878e-05}, {"id": 217, "seek": 100544, "start": 1025.88, "end": 1029.88, "text": " The idea is then to have something like an open energy batch that is basically in every", "tokens": [440, 1558, 307, 550, 281, 362, 746, 411, 364, 1269, 2281, 15245, 300, 307, 1936, 294, 633], "temperature": 0.0, "avg_logprob": -0.12186383038032346, "compression_ratio": 1.679245283018868, "no_speech_prob": 4.469087070901878e-05}, {"id": 218, "seek": 102988, "start": 1029.88, "end": 1035.6000000000001, "text": " repository that tells you for this software and for this usage scenario that comes with", "tokens": [25841, 300, 5112, 291, 337, 341, 4722, 293, 337, 341, 14924, 9005, 300, 1487, 365], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 219, "seek": 102988, "start": 1035.6000000000001, "end": 1036.6000000000001, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 220, "seek": 102988, "start": 1036.6000000000001, "end": 1041.64, "text": " So be it for instance running the tests or be it for instance building the containers", "tokens": [407, 312, 309, 337, 5197, 2614, 264, 6921, 420, 312, 309, 337, 5197, 2390, 264, 17089], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 221, "seek": 102988, "start": 1041.64, "end": 1044.2, "text": " or the intended use case of the software.", "tokens": [420, 264, 10226, 764, 1389, 295, 264, 4722, 13], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 222, "seek": 102988, "start": 1044.2, "end": 1049.48, "text": " So let's say the NumPy library of Python has an energy batch where it says, hey, for 1000", "tokens": [407, 718, 311, 584, 264, 22592, 47, 88, 6405, 295, 15329, 575, 364, 2281, 15245, 689, 309, 1619, 11, 4177, 11, 337, 9714], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 223, "seek": 102988, "start": 1049.48, "end": 1055.8000000000002, "text": " times 1000 metrics multiplication, this software uses this amount of energy on the reference", "tokens": [1413, 9714, 16367, 27290, 11, 341, 4722, 4960, 341, 2372, 295, 2281, 322, 264, 6408], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 224, "seek": 102988, "start": 1055.8000000000002, "end": 1058.92, "text": " system that we have specified.", "tokens": [1185, 300, 321, 362, 22206, 13], "temperature": 0.0, "avg_logprob": -0.15003448374131145, "compression_ratio": 1.7389558232931728, "no_speech_prob": 6.013757229084149e-05}, {"id": 225, "seek": 105892, "start": 1058.92, "end": 1062.8400000000001, "text": " And when you use the same reference systems to compare software against each other, you", "tokens": [400, 562, 291, 764, 264, 912, 6408, 3652, 281, 6794, 4722, 1970, 1184, 661, 11, 291], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 226, "seek": 105892, "start": 1062.8400000000001, "end": 1067.64, "text": " come to a scenario that we have basically shown from the starters in the first slides", "tokens": [808, 281, 257, 9005, 300, 321, 362, 1936, 4898, 490, 264, 35131, 294, 264, 700, 9788], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 227, "seek": 105892, "start": 1067.64, "end": 1073.5600000000002, "text": " where you can basically tell is the one software more energy hungry than the other one comparing", "tokens": [689, 291, 393, 1936, 980, 307, 264, 472, 4722, 544, 2281, 8067, 813, 264, 661, 472, 15763], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 228, "seek": 105892, "start": 1073.5600000000002, "end": 1076.68, "text": " the same use case.", "tokens": [264, 912, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 229, "seek": 105892, "start": 1076.68, "end": 1080.68, "text": " So let me quickly get back to my slide deck.", "tokens": [407, 718, 385, 2661, 483, 646, 281, 452, 4137, 9341, 13], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 230, "seek": 105892, "start": 1080.68, "end": 1082.6000000000001, "text": " So let's wrap up.", "tokens": [407, 718, 311, 7019, 493, 13], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 231, "seek": 105892, "start": 1082.6000000000001, "end": 1085.8000000000002, "text": " Measuring software energy consumption we believe is still too hard.", "tokens": [1923, 296, 1345, 4722, 2281, 12126, 321, 1697, 307, 920, 886, 1152, 13], "temperature": 0.0, "avg_logprob": -0.11579561233520508, "compression_ratio": 1.7004048582995952, "no_speech_prob": 1.805785359465517e-05}, {"id": 232, "seek": 108580, "start": 1085.8, "end": 1090.9199999999998, "text": " The goal should be easy as starting a Docker container and it should happen transparently.", "tokens": [440, 3387, 820, 312, 1858, 382, 2891, 257, 33772, 10129, 293, 309, 820, 1051, 7132, 6420, 13], "temperature": 0.0, "avg_logprob": -0.1464500908899789, "compression_ratio": 1.696, "no_speech_prob": 1.147854436567286e-05}, {"id": 233, "seek": 108580, "start": 1090.9199999999998, "end": 1095.68, "text": " Therefore we have created the green metrics tool which can reuse Docker files and infrastructure", "tokens": [7504, 321, 362, 2942, 264, 3092, 16367, 2290, 597, 393, 26225, 33772, 7098, 293, 6896], "temperature": 0.0, "avg_logprob": -0.1464500908899789, "compression_ratio": 1.696, "no_speech_prob": 1.147854436567286e-05}, {"id": 234, "seek": 108580, "start": 1095.68, "end": 1099.36, "text": " files to make it very easy to orchestrate your architecture.", "tokens": [7098, 281, 652, 309, 588, 1858, 281, 14161, 4404, 428, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1464500908899789, "compression_ratio": 1.696, "no_speech_prob": 1.147854436567286e-05}, {"id": 235, "seek": 108580, "start": 1099.36, "end": 1104.3999999999999, "text": " And then in a flow that you already have, be it a puppeteer file or be it just a shell", "tokens": [400, 550, 294, 257, 3095, 300, 291, 1217, 362, 11, 312, 309, 257, 17014, 3498, 260, 3991, 420, 312, 309, 445, 257, 8720], "temperature": 0.0, "avg_logprob": -0.1464500908899789, "compression_ratio": 1.696, "no_speech_prob": 1.147854436567286e-05}, {"id": 236, "seek": 108580, "start": 1104.3999999999999, "end": 1110.8, "text": " script, you can run that with our tool just as a parameter appended and it will tell you", "tokens": [5755, 11, 291, 393, 1190, 300, 365, 527, 2290, 445, 382, 257, 13075, 724, 3502, 293, 309, 486, 980, 291], "temperature": 0.0, "avg_logprob": -0.1464500908899789, "compression_ratio": 1.696, "no_speech_prob": 1.147854436567286e-05}, {"id": 237, "seek": 111080, "start": 1110.8, "end": 1116.24, "text": " how much energy has been used over this particular scenario that you feed in.", "tokens": [577, 709, 2281, 575, 668, 1143, 670, 341, 1729, 9005, 300, 291, 3154, 294, 13], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 238, "seek": 111080, "start": 1116.24, "end": 1118.04, "text": " Measuring software is also very complex.", "tokens": [1923, 296, 1345, 4722, 307, 611, 588, 3997, 13], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 239, "seek": 111080, "start": 1118.04, "end": 1123.3999999999999, "text": " So this is what we have integrated best practices or tool like pausing between measurements,", "tokens": [407, 341, 307, 437, 321, 362, 10919, 1151, 7525, 420, 2290, 411, 2502, 7981, 1296, 15383, 11], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 240, "seek": 111080, "start": 1123.3999999999999, "end": 1129.48, "text": " letting systems idle before you actually use them, turning functionalities like SGX off,", "tokens": [8295, 3652, 30650, 949, 291, 767, 764, 552, 11, 6246, 11745, 1088, 411, 34520, 55, 766, 11], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 241, "seek": 111080, "start": 1129.48, "end": 1134.34, "text": " looking at if TurboBoost is on and very more features.", "tokens": [1237, 412, 498, 35848, 22493, 555, 307, 322, 293, 588, 544, 4122, 13], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 242, "seek": 111080, "start": 1134.34, "end": 1138.48, "text": " Just inline measuring like Datadoc or other providers are doing it at the moment, we believe", "tokens": [1449, 294, 1889, 13389, 411, 9315, 345, 905, 420, 661, 11330, 366, 884, 309, 412, 264, 1623, 11, 321, 1697], "temperature": 0.0, "avg_logprob": -0.14557355244954426, "compression_ratio": 1.5886524822695036, "no_speech_prob": 7.72162020439282e-05}, {"id": 243, "seek": 113848, "start": 1138.48, "end": 1142.48, "text": " is not enough and is too arbitrary to talk about energy.", "tokens": [307, 406, 1547, 293, 307, 886, 23211, 281, 751, 466, 2281, 13], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 244, "seek": 113848, "start": 1142.48, "end": 1145.76, "text": " Software must be measured against a standard usage case.", "tokens": [27428, 1633, 312, 12690, 1970, 257, 3832, 14924, 1389, 13], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 245, "seek": 113848, "start": 1145.76, "end": 1152.08, "text": " So we provide standard usage cases for software as an interface, but we ask you the community", "tokens": [407, 321, 2893, 3832, 14924, 3331, 337, 4722, 382, 364, 9226, 11, 457, 321, 1029, 291, 264, 1768], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 246, "seek": 113848, "start": 1152.08, "end": 1159.84, "text": " also or we need to see over time what are the standard usage cases we can all agree on.", "tokens": [611, 420, 321, 643, 281, 536, 670, 565, 437, 366, 264, 3832, 14924, 3331, 321, 393, 439, 3986, 322, 13], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 247, "seek": 113848, "start": 1159.84, "end": 1163.68, "text": " A software must be comparable to another similar software in terms of energy.", "tokens": [316, 4722, 1633, 312, 25323, 281, 1071, 2531, 4722, 294, 2115, 295, 2281, 13], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 248, "seek": 113848, "start": 1163.68, "end": 1168.46, "text": " This is why we need these standard usage cases to make it comparable.", "tokens": [639, 307, 983, 321, 643, 613, 3832, 14924, 3331, 281, 652, 309, 25323, 13], "temperature": 0.0, "avg_logprob": -0.13487945818433575, "compression_ratio": 1.8851063829787233, "no_speech_prob": 6.401682912837714e-05}, {"id": 249, "seek": 116846, "start": 1168.46, "end": 1174.04, "text": " This also means it must be measured on reference machines that everybody has access to that", "tokens": [639, 611, 1355, 309, 1633, 312, 12690, 322, 6408, 8379, 300, 2201, 575, 2105, 281, 300], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 250, "seek": 116846, "start": 1174.04, "end": 1178.64, "text": " we want to provide for the community as a free service.", "tokens": [321, 528, 281, 2893, 337, 264, 1768, 382, 257, 1737, 2643, 13], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 251, "seek": 116846, "start": 1178.64, "end": 1182.24, "text": " Energy metrics must also be available in restricted environments like the cloud.", "tokens": [14939, 16367, 1633, 611, 312, 2435, 294, 20608, 12388, 411, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 252, "seek": 116846, "start": 1182.24, "end": 1186.76, "text": " So I've talked about estimation models that need to be open source and available and for", "tokens": [407, 286, 600, 2825, 466, 35701, 5245, 300, 643, 281, 312, 1269, 4009, 293, 2435, 293, 337], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 253, "seek": 116846, "start": 1186.76, "end": 1188.68, "text": " everybody to implement.", "tokens": [2201, 281, 4445, 13], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 254, "seek": 116846, "start": 1188.68, "end": 1194.24, "text": " And energy must be transparent and a first order metric and order in developing and using", "tokens": [400, 2281, 1633, 312, 12737, 293, 257, 700, 1668, 20678, 293, 1668, 294, 6416, 293, 1228], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 255, "seek": 116846, "start": 1194.24, "end": 1195.24, "text": " software.", "tokens": [4722, 13], "temperature": 0.0, "avg_logprob": -0.13165494799613953, "compression_ratio": 1.6961538461538461, "no_speech_prob": 3.1691502954345196e-05}, {"id": 256, "seek": 119524, "start": 1195.24, "end": 1199.88, "text": " People should know before they use the software how much energy it is consuming.", "tokens": [3432, 820, 458, 949, 436, 764, 264, 4722, 577, 709, 2281, 309, 307, 19867, 13], "temperature": 0.0, "avg_logprob": -0.20075381079385446, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00011771605204558}, {"id": 257, "seek": 119524, "start": 1199.88, "end": 1203.8, "text": " And this is what we are trying to achieve with the tools we are developing.", "tokens": [400, 341, 307, 437, 321, 366, 1382, 281, 4584, 365, 264, 3873, 321, 366, 6416, 13], "temperature": 0.0, "avg_logprob": -0.20075381079385446, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00011771605204558}, {"id": 258, "seek": 119524, "start": 1203.8, "end": 1208.04, "text": " I hope it could pique your interest in our work and in the tools we are developing, some", "tokens": [286, 1454, 309, 727, 280, 1925, 428, 1179, 294, 527, 589, 293, 294, 264, 3873, 321, 366, 6416, 11, 512], "temperature": 0.0, "avg_logprob": -0.20075381079385446, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00011771605204558}, {"id": 259, "seek": 119524, "start": 1208.04, "end": 1211.44, "text": " as concepts, some already production ready.", "tokens": [382, 10392, 11, 512, 1217, 4265, 1919, 13], "temperature": 0.0, "avg_logprob": -0.20075381079385446, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00011771605204558}, {"id": 260, "seek": 121144, "start": 1211.44, "end": 1227.92, "text": " And thank you for listening and now I hope it's time for questions.", "tokens": [50364, 400, 1309, 291, 337, 4764, 293, 586, 286, 1454, 309, 311, 565, 337, 1651, 13, 51188], "temperature": 0.0, "avg_logprob": -0.2904093795352512, "compression_ratio": 0.9571428571428572, "no_speech_prob": 0.0004024910740554333}], "language": "en"}