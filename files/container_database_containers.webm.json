{"text": " Hi, everyone. How are you? How is hosting the weekend? Good? Yes. That's nice. I'm happy to be here. It's my first time in Europe and it's the first time that I will talk in English for a first event in person. This is pretty nice. My name is Edet Buja. I am a technology evangelist at Percona and this is a very basic and friendly introduction about databases and containers. About me, I am from Peru in South America. I am working as Six Months in Percona. It's an open source company. We create open source databases free. I am a Google woman tech maker. I was nominated as a docker captain last year and I am a database and container enthusiast. You can follow me on Twitter and LinkedIn. I used to post about containers, Kubernetes, open source. For the agenda today, we are going to see about containers. We will see docker architecture. We will see the workflow between the components of docker. We are going to have two examples of how we are running a single Percona server MySQL container and we are going to run multiple containers for Percona server MySQL. We will see the docker volume, how this is important in this work of databases on containers. We will see backups, restores of databases and best practices. Let's start it. What's a container? How many of you knows what's with docker? Yeah, a lot. Okay. That's nice. Docker or do you use other tools? Yeah, there are different kinds of tools for container application. But a container is like a single unit, lightweight unit of software that package everything that you need for your application. When we run application, when we build application, we know that we need a lot of packages. If you are running, for example, if you are building a Java application, you need libraries, dependencies, many things to run your application. So everything have to be containerized in a single unit of software and this is going to be isolated for other things like your infrastructure. And the good thing is that your container can run on different platforms in your laptop, in your server, in your cloud. With this, we end with a problem that we have when we say, hey, your program runs. Yes, this works just on my computer. But no, it has to run in different platforms. We don't need to have this problem to dependencies and other kind of things when we test our application in other platforms. There are different tools, as I say, for containerization. We have container interface, for example. We have container D and we have Docker that is the tool that we are going to focus now. All these tools are also in the cloud native computing foundation ecosystem. If you see the landscape, you will see a lot of tools there. There is a part for containerization and there are more than three. There are a lot of tools for them. The Docker architecture, it works like a client-server model. We have the Docker DMO, which is going to process all the commands. It's going to start to listen to the client always and the client is going to send a request to the DMO through the REST app. With this model, the Docker DMO can also manage network containers, images, and Docker volumes. If we go more in detail, we will see that we have the client, the DMO that is also called the engine of Docker, and we have another component that could be your Docker registry, the public, which is Docker Hub, where all the official images are published, and also we can have our own private registry in case we don't want to share it with the public. In this case, this is the flow of a component. For example, if we do a pull, we are going to try to bring the image from the Docker Hub into the Docker DMO cache. If the Docker DMO doesn't find the image in cache, it's going to bring it from the Docker Hub. But if this is in cache, it's going to take it just that and start to process. The same with Docker build. When we run Docker build from the client, the Docker DMO will try to take a Docker file. A Docker file is a recipe with a lot of instructions where we put all the commands to run our application and deploy it. So I'm going to, the Docker DMO is going to take the Docker file and build it, build the image, and if you want, we can also run it. We run, we will create a container. The container is our application that is already alive and is ready to make connections of petitions. One more thing here is that we can have everything in our host or we can have clients, remote clients that could make petitions to the Docker DMO. Container benefits. There are pros and cons, but now I'm going to focus on these benefits, the containers give us. So one of these is we can reduce costs with this because we can run several containers in a single infrastructure. That's infrastructure that we have because of the technology of containers is different than the virtualization. In virtualization, we use the hypervisor and when you create virtual machines, it consumes more resources from your, from your infrastructure, but when you use containers, it's very different. You are using that technology, a container would make it possible to run different, a lot of containers in a single machine. So for that reason, it's possible to reduce costs. Also, the containers are very friendly with continuous integration and continuous delivery process. If you have like a big application, a monolithic application, this, and you want to, you want to run container, you want to integrate it in the DevOps process. This is going to be hard. We have to work like microservices to make each service as a container and included in the continuous integration and continuous delivery process. It's easy. When we build, when we build our application over a container, it's easy to kill it. It's easy to create it again. It's easy to fail and the process is faster. Another benefit is the multicloud compatibility with the time several companies try to migrate to a hybrid cloud. They just don't, don't want to have everything on premise. They also want to scale. They want to grow. So for a reason, they opt for cloud and containers fit very good in this. You can install Docker. I know you did it. You can choose your distro. You are, you use Debian, the CentOS, everything. So you can go to the official Docker documentation and easily look all the steps. When you install this, it will install it, the Docker client, the Docker DMO and other tools that you will need to use Docker in your local matching. We already talk about containers, right? But this talk is about exploring database on containers. We are going to talk about my SQL, which is at this base relational database. We know that it's a database. And to run my SQL on containers, we need to understand how volumes works because the most important thing running databases on containers is the data. If we lost the data, we lost everything. For the next slides, we are going to focus in this part. We will use the image of Percona server for my SQL. This Percona server for my SQL is open source. It's like my SQL, but with more nice things. You can use it. It's open source. It's in Docker Hub. So we will use this image and we will create a Docker container. We will see how it works with all volumes. We will see the layers in Docker and then we will create a persistent volume and we will see how it changes in the layers of Docker. So just here to see that if you want to have an image, it's necessary to have a Docker file. You can use a Docker file before by yourself. That's good. A Docker file is a recipe where you will put everything for your application. So you need this to create an image. Then you need an image to create your Docker container. There are three essential steps here to remember how Docker works. We will run a single Percona server for my SQL container. We will use Docker run to create the image. No. We don't use Docker run to create the image. We use Docker run to create a container. So we use this to create a container. So we will do dash D to say run this container in the background. I don't want to use the terminal. And I will call it Percona server for my Percona server one. I will pass it like the environment variable, for the root. This is not a good practice here. This is just to show how we are going to create a container. And we will use this official Percona server for my SQL. With this I am creating a container, right? I'm creating a container with this one. Okay? So if we go to Docker image LS, this is going to pull the image of Percona server and then it will create the container. That command is going to do two things. It's going to bring the image from the official Dockerfab and it's going to create a container. So if we see Docker container PS, our container is up. Okay. After we have the database, we need to add data. We will add databases, we will add data, we will change registers, we will have transactions, many things that we can do like a regular database. Okay. If we run a single Percona server in my SQL container, we know how it works in layers. If we see this in green, there are layers from Percona, Percona server image. This is the image that we pull it, that we can change. This is just react only. We can change this, but in top of that, it's going to be created a layer, a new layer. This layer, this layer is react only. I can add data. This layer is the one that will contain all the things that I am doing in Docker on that image, on that container. I added a new database. Yes. I create a new registry. I delete it. I add the transactions. All this is going to save it here. But what happens if I don't have volume? My container is ephemeral, right? It could die. It could crash. My machine could crash. And all my data is going to be lost. I will, I will lose all the data. We will see how it works with multiple containers. To run multiple containers with the same image, if we see this is the same image, the same version of the image, we will just change the name of this container. Also, we can change another thing because this is a database, right? What thing we can change? They run in a port, right? In which port my SQL used to run? Yeah. So I need to change the port for the other container to avoid the conflict. Okay. How it works in layers. The same. We will use the same layer. We will use the same layer for Percona, Percona server, which can, we can modify. But in top of that, we are going to have two layers more. One of the first containers that I created and the second for the other that I can add. I can add data. I can change things. But once again, if I don't have volume, this is going to die. But this is how to work if we want to create an application when it doesn't matter if we save the state of this application. This is important. Persist data in databases is really important for this kind of application because sometimes we think that, like Kubernetes, since it was created for a state less application, but now we have options to use stateful applications on containers. And this is one of the reasons. Create volumes. So it's pretty easy to create volume. We can create a volume just with dash V or dash, dash volume. And we can say it, we can create a local volume with local run and detach. We will call it Percona server. The same process. And when we say dash V, we are saying, okay, this will be my volume in a host, in my local data directory. And this one is going to be inside my container. So this is like a mirror from this image. And how it works. In layers, we have the same, the layer that we can modify. And in top of that, we are going to create another layer. But in this case, we are adding, we are creating the mounted volume in BarLivMySQL. There are other directories that we can create the volume. I am just adding, as an example, this, because in MySQL, we have configuration files. We have logs. We have another things. But for that, we want to create these volumes for all of that things. I am just adding, as an example, BarLivMySQL, which is also a directory that is very important. And this local directory is the one that could be in my host. But it is not recommended, because if your host crashes, everything crashes too with your volumes. It is preferable to run it in a remote host. Okay. Two backups. Who here make backups? Okay. I use the very easy way to make backups. I use it for logical backups, my SQL dump used in the container. And for physical backups, we use in the company PerconextraVacup, which is, have more features to have that physical backup. And for restore, I will use also my SQL dump. And we don't use PerconextraVacup in this case, because it has a lot of pins. For backup, I will execute a backup in a container that is already running. PerconaserverVacup is already running. Let's see that we created. And we are executing Docker exit, it, to enter into the Percona in that container and type that common, my SQL dump, to create a backup of the database. So the backup is going to be in that file, dump SQL. And the same process with restore, we can take that backup. And this is a different container. I'm going to restore the dot SQL file in a different container. In this case, in PerconaserverRestore, using my SQL, use that command, my SQL. Okay. Best practices or some recommendation to use containers in database. Okay. And one of this is that we can keep constantly monitoring our database and the whole system, because we don't know when we are going to don't have enough resources for our containers. We should be aware of that or have notifications to say, hey, you don't have a note disk, you don't have a note memory, so provision or try to scale in your resources. So we should keep monitoring. Using some tools for that, for example, is PMM. We can use open source monitors to monitor our databases on containers. And we can store this data in persistent volume outside the container. It recommended no inside the container, because it's easy to create plans for recovery. We can restore the data easily also and fast. We should limit the resources of utilization of our containers. Our containers, we know that they are small, but also we should limit when they are a lot. And we should regularly have backups of the database and store these backups in a different location. And have a plan of migration and disaster recovery is really great. In that case, having a monitoring tool helps a lot. And what more? That's all. You can find me in LinkedIn and Twitter. Okay, we have time for questions. If you absolutely need to leave and you can't wait until the talk is over, please do so as quietly as possible so we can understand the questions. Thanks. Hi. Thank you so much for your talk. It was really interesting. I'm wondering what kind of limitations do you see when you're speaking about having a databases arriving in containers? There is storage limitations, CPU, or something else? Guys, can you please be a little quiet so we can understand the question? All right, I will try it with the microphone. Yeah, you. The people can you. Thank you. I was wondering maybe, first of all, really cool talk. Thank you so much. My question would be, could you maybe talk us through some kind of limitations that you can see when you're running databases from containers? You didn't understand it? Thank you so much for the talk. It was really cool. Maybe you can share with us some kind of limitations that you see when you're running to the solution of running databases inside containers, right? You cannot really run very big database. You probably will have a problem with that. What kind of limitations do you see? So, yeah, the question is about sorry, the question is about what limitations you can run into with database containers? Yeah, I don't want to say this, but it depends really of the business. Okay, if you want to invest a lot of money in infrastructure, but because at the end, your database, the volume that you have is not going to be part of your container, it's going to be outside. And this depends on you. You want to invest a lot of money to save that data. It's good. You want to replicate it? Please try and be quiet while we are asking questions. Are there any more questions? There is one more question from the back, so please be quiet. Thank you. Hello. I wanted to ask, did you notice any kind of performance issues? Did you benchmark things? Did you identify some kind of overheads going on when you containerize a database like MySQL or other kind of databases really? Sorry, I didn't get your question. All right, I'm just going to ask you. When you containerize a database, be it MySQL or Postgres or any kind of open source database that you may have tested on this kind of setup, did you notice any kind of overheads, compute, memory, or disk, essentially, where you can see that the database performance or operation is significantly affected by the fact of being containerized? I'm not sure about that, but if you use open source to monitor your containers on databases, you can have a visualization of these things if you don't have enough resources so it can show you alerts or things like that where you can figure out where exactly is your limitation. Okay, so for example, did you run Benchmark? Could you help me? Okay, could you help me to answer? Okay, my friend is going to help me to answer this. All right, thank you. Yeah, thank you to you. Hey, so usually the performance degradation is around two, three, four percent. The issue is more about how you configure the database, kind of storage, if it's local or network storage, but the virtualization is minimal. It's like running on a EC2 instance. Okay, so there is an impact, miserable, at least you say around four or five percent, but you say that's not going to be the, that there are configurations we can do to try to avoid that. Do you have any kind of paper or any kind of resources that we might use to avoid those kind of bottlenecks? If I got correctly, not much. The measure that we do in databases is measuring TPS. So you will notice on, if we're running Benchmarks, we've seen Bench, for example, three percent, like if you are running 1,000 credits per second, you will get 980, 990 credits per second when containerized. Okay, and do you have any kind of recommendations, kind of generic recommendations you can do so that when you run a database in a container, here is what you can do to try and negate some of the performance bottleneck that you guys have noticed? To be honest, on real-day activities, I would say 99 percent of the performance will come from how you configure my SQL, not the containerization is like just a small piece of the game. You can make more effect by modifying the database configuration. All right, thank you very much. Thanks to you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Hi, everyone. How are you? How is hosting the weekend? Good?", "tokens": [2421, 11, 1518, 13, 1012, 366, 291, 30, 1012, 307, 16058, 264, 6711, 30, 2205, 30], "temperature": 0.0, "avg_logprob": -0.2717765651337088, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.7155705690383911}, {"id": 1, "seek": 0, "start": 10.0, "end": 11.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2717765651337088, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.7155705690383911}, {"id": 2, "seek": 0, "start": 11.0, "end": 16.14, "text": " That's nice. I'm happy to be here. It's my first time in Europe and it's the first time", "tokens": [663, 311, 1481, 13, 286, 478, 2055, 281, 312, 510, 13, 467, 311, 452, 700, 565, 294, 3315, 293, 309, 311, 264, 700, 565], "temperature": 0.0, "avg_logprob": -0.2717765651337088, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.7155705690383911}, {"id": 3, "seek": 0, "start": 16.14, "end": 23.400000000000002, "text": " that I will talk in English for a first event in person. This is pretty nice. My name is", "tokens": [300, 286, 486, 751, 294, 3669, 337, 257, 700, 2280, 294, 954, 13, 639, 307, 1238, 1481, 13, 1222, 1315, 307], "temperature": 0.0, "avg_logprob": -0.2717765651337088, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.7155705690383911}, {"id": 4, "seek": 2340, "start": 23.4, "end": 31.119999999999997, "text": " Edet Buja. I am a technology evangelist at Percona and this is a very basic and friendly", "tokens": [3977, 302, 4078, 2938, 13, 286, 669, 257, 2899, 24546, 468, 412, 3026, 1671, 64, 293, 341, 307, 257, 588, 3875, 293, 9208], "temperature": 0.0, "avg_logprob": -0.20137676722566847, "compression_ratio": 1.4378378378378378, "no_speech_prob": 0.0024196775630116463}, {"id": 5, "seek": 2340, "start": 31.119999999999997, "end": 43.04, "text": " introduction about databases and containers. About me, I am from Peru in South America.", "tokens": [9339, 466, 22380, 293, 17089, 13, 7769, 385, 11, 286, 669, 490, 31571, 294, 4242, 3374, 13], "temperature": 0.0, "avg_logprob": -0.20137676722566847, "compression_ratio": 1.4378378378378378, "no_speech_prob": 0.0024196775630116463}, {"id": 6, "seek": 2340, "start": 43.04, "end": 49.8, "text": " I am working as Six Months in Percona. It's an open source company. We create open source", "tokens": [286, 669, 1364, 382, 11678, 24255, 82, 294, 3026, 1671, 64, 13, 467, 311, 364, 1269, 4009, 2237, 13, 492, 1884, 1269, 4009], "temperature": 0.0, "avg_logprob": -0.20137676722566847, "compression_ratio": 1.4378378378378378, "no_speech_prob": 0.0024196775630116463}, {"id": 7, "seek": 4980, "start": 49.8, "end": 59.8, "text": " databases free. I am a Google woman tech maker. I was nominated as a docker captain last year", "tokens": [22380, 1737, 13, 286, 669, 257, 3329, 3059, 7553, 17127, 13, 286, 390, 25159, 382, 257, 360, 9178, 14871, 1036, 1064], "temperature": 0.0, "avg_logprob": -0.2241034227259019, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0006421642028726637}, {"id": 8, "seek": 4980, "start": 59.8, "end": 66.28, "text": " and I am a database and container enthusiast. You can follow me on Twitter and LinkedIn.", "tokens": [293, 286, 669, 257, 8149, 293, 10129, 18076, 525, 13, 509, 393, 1524, 385, 322, 5794, 293, 20657, 13], "temperature": 0.0, "avg_logprob": -0.2241034227259019, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0006421642028726637}, {"id": 9, "seek": 4980, "start": 66.28, "end": 78.36, "text": " I used to post about containers, Kubernetes, open source. For the agenda today, we are going", "tokens": [286, 1143, 281, 2183, 466, 17089, 11, 23145, 11, 1269, 4009, 13, 1171, 264, 9829, 965, 11, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.2241034227259019, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0006421642028726637}, {"id": 10, "seek": 7836, "start": 78.36, "end": 88.12, "text": " to see about containers. We will see docker architecture. We will see the workflow between", "tokens": [281, 536, 466, 17089, 13, 492, 486, 536, 360, 9178, 9482, 13, 492, 486, 536, 264, 20993, 1296], "temperature": 0.0, "avg_logprob": -0.20045650188739483, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0002937891404144466}, {"id": 11, "seek": 7836, "start": 88.12, "end": 95.24, "text": " the components of docker. We are going to have two examples of how we are running a single", "tokens": [264, 6677, 295, 360, 9178, 13, 492, 366, 516, 281, 362, 732, 5110, 295, 577, 321, 366, 2614, 257, 2167], "temperature": 0.0, "avg_logprob": -0.20045650188739483, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0002937891404144466}, {"id": 12, "seek": 7836, "start": 95.24, "end": 101.16, "text": " Percona server MySQL container and we are going to run multiple containers for Percona", "tokens": [3026, 1671, 64, 7154, 1222, 39934, 10129, 293, 321, 366, 516, 281, 1190, 3866, 17089, 337, 3026, 1671, 64], "temperature": 0.0, "avg_logprob": -0.20045650188739483, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0002937891404144466}, {"id": 13, "seek": 10116, "start": 101.16, "end": 108.96, "text": " server MySQL. We will see the docker volume, how this is important in this work of databases", "tokens": [7154, 1222, 39934, 13, 492, 486, 536, 264, 360, 9178, 5523, 11, 577, 341, 307, 1021, 294, 341, 589, 295, 22380], "temperature": 0.0, "avg_logprob": -0.235689697265625, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0002477772068232298}, {"id": 14, "seek": 10116, "start": 108.96, "end": 118.47999999999999, "text": " on containers. We will see backups, restores of databases and best practices. Let's start", "tokens": [322, 17089, 13, 492, 486, 536, 50160, 11, 1472, 2706, 295, 22380, 293, 1151, 7525, 13, 961, 311, 722], "temperature": 0.0, "avg_logprob": -0.235689697265625, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0002477772068232298}, {"id": 15, "seek": 10116, "start": 118.47999999999999, "end": 130.51999999999998, "text": " it. What's a container? How many of you knows what's with docker? Yeah, a lot. Okay. That's", "tokens": [309, 13, 708, 311, 257, 10129, 30, 1012, 867, 295, 291, 3255, 437, 311, 365, 360, 9178, 30, 865, 11, 257, 688, 13, 1033, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.235689697265625, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0002477772068232298}, {"id": 16, "seek": 13052, "start": 130.52, "end": 136.96, "text": " nice. Docker or do you use other tools? Yeah, there are different kinds of tools for container", "tokens": [1481, 13, 1144, 9178, 420, 360, 291, 764, 661, 3873, 30, 865, 11, 456, 366, 819, 3685, 295, 3873, 337, 10129], "temperature": 0.0, "avg_logprob": -0.22823307730934836, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0007506444817408919}, {"id": 17, "seek": 13052, "start": 136.96, "end": 147.76000000000002, "text": " application. But a container is like a single unit, lightweight unit of software that package", "tokens": [3861, 13, 583, 257, 10129, 307, 411, 257, 2167, 4985, 11, 22052, 4985, 295, 4722, 300, 7372], "temperature": 0.0, "avg_logprob": -0.22823307730934836, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0007506444817408919}, {"id": 18, "seek": 13052, "start": 147.76000000000002, "end": 152.4, "text": " everything that you need for your application. When we run application, when we build application,", "tokens": [1203, 300, 291, 643, 337, 428, 3861, 13, 1133, 321, 1190, 3861, 11, 562, 321, 1322, 3861, 11], "temperature": 0.0, "avg_logprob": -0.22823307730934836, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0007506444817408919}, {"id": 19, "seek": 13052, "start": 152.4, "end": 157.12, "text": " we know that we need a lot of packages. If you are running, for example, if you are building", "tokens": [321, 458, 300, 321, 643, 257, 688, 295, 17401, 13, 759, 291, 366, 2614, 11, 337, 1365, 11, 498, 291, 366, 2390], "temperature": 0.0, "avg_logprob": -0.22823307730934836, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0007506444817408919}, {"id": 20, "seek": 15712, "start": 157.12, "end": 164.64000000000001, "text": " a Java application, you need libraries, dependencies, many things to run your application. So everything", "tokens": [257, 10745, 3861, 11, 291, 643, 15148, 11, 36606, 11, 867, 721, 281, 1190, 428, 3861, 13, 407, 1203], "temperature": 0.0, "avg_logprob": -0.17235975730709913, "compression_ratio": 1.6820276497695852, "no_speech_prob": 6.544867210322991e-05}, {"id": 21, "seek": 15712, "start": 164.64000000000001, "end": 170.48000000000002, "text": " have to be containerized in a single unit of software and this is going to be isolated", "tokens": [362, 281, 312, 10129, 1602, 294, 257, 2167, 4985, 295, 4722, 293, 341, 307, 516, 281, 312, 14621], "temperature": 0.0, "avg_logprob": -0.17235975730709913, "compression_ratio": 1.6820276497695852, "no_speech_prob": 6.544867210322991e-05}, {"id": 22, "seek": 15712, "start": 170.48000000000002, "end": 176.44, "text": " for other things like your infrastructure. And the good thing is that your container", "tokens": [337, 661, 721, 411, 428, 6896, 13, 400, 264, 665, 551, 307, 300, 428, 10129], "temperature": 0.0, "avg_logprob": -0.17235975730709913, "compression_ratio": 1.6820276497695852, "no_speech_prob": 6.544867210322991e-05}, {"id": 23, "seek": 15712, "start": 176.44, "end": 182.8, "text": " can run on different platforms in your laptop, in your server, in your cloud. With this,", "tokens": [393, 1190, 322, 819, 9473, 294, 428, 10732, 11, 294, 428, 7154, 11, 294, 428, 4588, 13, 2022, 341, 11], "temperature": 0.0, "avg_logprob": -0.17235975730709913, "compression_ratio": 1.6820276497695852, "no_speech_prob": 6.544867210322991e-05}, {"id": 24, "seek": 18280, "start": 182.8, "end": 188.92000000000002, "text": " we end with a problem that we have when we say, hey, your program runs. Yes, this works", "tokens": [321, 917, 365, 257, 1154, 300, 321, 362, 562, 321, 584, 11, 4177, 11, 428, 1461, 6676, 13, 1079, 11, 341, 1985], "temperature": 0.0, "avg_logprob": -0.19183210893110794, "compression_ratio": 1.6330275229357798, "no_speech_prob": 6.372125062625855e-05}, {"id": 25, "seek": 18280, "start": 188.92000000000002, "end": 197.04000000000002, "text": " just on my computer. But no, it has to run in different platforms. We don't need to have", "tokens": [445, 322, 452, 3820, 13, 583, 572, 11, 309, 575, 281, 1190, 294, 819, 9473, 13, 492, 500, 380, 643, 281, 362], "temperature": 0.0, "avg_logprob": -0.19183210893110794, "compression_ratio": 1.6330275229357798, "no_speech_prob": 6.372125062625855e-05}, {"id": 26, "seek": 18280, "start": 197.04000000000002, "end": 204.60000000000002, "text": " this problem to dependencies and other kind of things when we test our application in", "tokens": [341, 1154, 281, 36606, 293, 661, 733, 295, 721, 562, 321, 1500, 527, 3861, 294], "temperature": 0.0, "avg_logprob": -0.19183210893110794, "compression_ratio": 1.6330275229357798, "no_speech_prob": 6.372125062625855e-05}, {"id": 27, "seek": 18280, "start": 204.60000000000002, "end": 212.68, "text": " other platforms. There are different tools, as I say, for containerization. We have container", "tokens": [661, 9473, 13, 821, 366, 819, 3873, 11, 382, 286, 584, 11, 337, 10129, 2144, 13, 492, 362, 10129], "temperature": 0.0, "avg_logprob": -0.19183210893110794, "compression_ratio": 1.6330275229357798, "no_speech_prob": 6.372125062625855e-05}, {"id": 28, "seek": 21268, "start": 212.68, "end": 219.56, "text": " interface, for example. We have container D and we have Docker that is the tool that", "tokens": [9226, 11, 337, 1365, 13, 492, 362, 10129, 413, 293, 321, 362, 33772, 300, 307, 264, 2290, 300], "temperature": 0.0, "avg_logprob": -0.25555126483623797, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.00041378068272024393}, {"id": 29, "seek": 21268, "start": 219.56, "end": 228.8, "text": " we are going to focus now. All these tools are also in the cloud native computing foundation", "tokens": [321, 366, 516, 281, 1879, 586, 13, 1057, 613, 3873, 366, 611, 294, 264, 4588, 8470, 15866, 7030], "temperature": 0.0, "avg_logprob": -0.25555126483623797, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.00041378068272024393}, {"id": 30, "seek": 21268, "start": 228.8, "end": 234.52, "text": " ecosystem. If you see the landscape, you will see a lot of tools there. There is a part", "tokens": [11311, 13, 759, 291, 536, 264, 9661, 11, 291, 486, 536, 257, 688, 295, 3873, 456, 13, 821, 307, 257, 644], "temperature": 0.0, "avg_logprob": -0.25555126483623797, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.00041378068272024393}, {"id": 31, "seek": 23452, "start": 234.52, "end": 242.76000000000002, "text": " for containerization and there are more than three. There are a lot of tools for them.", "tokens": [337, 10129, 2144, 293, 456, 366, 544, 813, 1045, 13, 821, 366, 257, 688, 295, 3873, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.22928357805524555, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.00034616925404407084}, {"id": 32, "seek": 23452, "start": 242.76000000000002, "end": 251.84, "text": " The Docker architecture, it works like a client-server model. We have the Docker DMO, which is going", "tokens": [440, 33772, 9482, 11, 309, 1985, 411, 257, 6423, 12, 12484, 331, 2316, 13, 492, 362, 264, 33772, 15322, 46, 11, 597, 307, 516], "temperature": 0.0, "avg_logprob": -0.22928357805524555, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.00034616925404407084}, {"id": 33, "seek": 23452, "start": 251.84, "end": 260.04, "text": " to process all the commands. It's going to start to listen to the client always and the", "tokens": [281, 1399, 439, 264, 16901, 13, 467, 311, 516, 281, 722, 281, 2140, 281, 264, 6423, 1009, 293, 264], "temperature": 0.0, "avg_logprob": -0.22928357805524555, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.00034616925404407084}, {"id": 34, "seek": 26004, "start": 260.04, "end": 269.20000000000005, "text": " client is going to send a request to the DMO through the REST app. With this model, the", "tokens": [6423, 307, 516, 281, 2845, 257, 5308, 281, 264, 15322, 46, 807, 264, 497, 14497, 724, 13, 2022, 341, 2316, 11, 264], "temperature": 0.0, "avg_logprob": -0.21179623074001735, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0002529757039155811}, {"id": 35, "seek": 26004, "start": 269.20000000000005, "end": 276.6, "text": " Docker DMO can also manage network containers, images, and Docker volumes. If we go more", "tokens": [33772, 15322, 46, 393, 611, 3067, 3209, 17089, 11, 5267, 11, 293, 33772, 22219, 13, 759, 321, 352, 544], "temperature": 0.0, "avg_logprob": -0.21179623074001735, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0002529757039155811}, {"id": 36, "seek": 26004, "start": 276.6, "end": 283.04, "text": " in detail, we will see that we have the client, the DMO that is also called the engine of", "tokens": [294, 2607, 11, 321, 486, 536, 300, 321, 362, 264, 6423, 11, 264, 15322, 46, 300, 307, 611, 1219, 264, 2848, 295], "temperature": 0.0, "avg_logprob": -0.21179623074001735, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0002529757039155811}, {"id": 37, "seek": 26004, "start": 283.04, "end": 288.48, "text": " Docker, and we have another component that could be your Docker registry, the public,", "tokens": [33772, 11, 293, 321, 362, 1071, 6542, 300, 727, 312, 428, 33772, 36468, 11, 264, 1908, 11], "temperature": 0.0, "avg_logprob": -0.21179623074001735, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0002529757039155811}, {"id": 38, "seek": 28848, "start": 288.48, "end": 295.84000000000003, "text": " which is Docker Hub, where all the official images are published, and also we can have", "tokens": [597, 307, 33772, 18986, 11, 689, 439, 264, 4783, 5267, 366, 6572, 11, 293, 611, 321, 393, 362], "temperature": 0.0, "avg_logprob": -0.17174170998965993, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0006207947735674679}, {"id": 39, "seek": 28848, "start": 295.84000000000003, "end": 302.36, "text": " our own private registry in case we don't want to share it with the public. In this", "tokens": [527, 1065, 4551, 36468, 294, 1389, 321, 500, 380, 528, 281, 2073, 309, 365, 264, 1908, 13, 682, 341], "temperature": 0.0, "avg_logprob": -0.17174170998965993, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0006207947735674679}, {"id": 40, "seek": 28848, "start": 302.36, "end": 309.68, "text": " case, this is the flow of a component. For example, if we do a pull, we are going to", "tokens": [1389, 11, 341, 307, 264, 3095, 295, 257, 6542, 13, 1171, 1365, 11, 498, 321, 360, 257, 2235, 11, 321, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.17174170998965993, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0006207947735674679}, {"id": 41, "seek": 30968, "start": 309.68, "end": 318.84000000000003, "text": " try to bring the image from the Docker Hub into the Docker DMO cache. If the Docker DMO", "tokens": [853, 281, 1565, 264, 3256, 490, 264, 33772, 18986, 666, 264, 33772, 15322, 46, 19459, 13, 759, 264, 33772, 15322, 46], "temperature": 0.0, "avg_logprob": -0.1830487398757148, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.00027357693761587143}, {"id": 42, "seek": 30968, "start": 318.84000000000003, "end": 324.12, "text": " doesn't find the image in cache, it's going to bring it from the Docker Hub. But if this", "tokens": [1177, 380, 915, 264, 3256, 294, 19459, 11, 309, 311, 516, 281, 1565, 309, 490, 264, 33772, 18986, 13, 583, 498, 341], "temperature": 0.0, "avg_logprob": -0.1830487398757148, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.00027357693761587143}, {"id": 43, "seek": 30968, "start": 324.12, "end": 328.6, "text": " is in cache, it's going to take it just that and start to process. The same with Docker", "tokens": [307, 294, 19459, 11, 309, 311, 516, 281, 747, 309, 445, 300, 293, 722, 281, 1399, 13, 440, 912, 365, 33772], "temperature": 0.0, "avg_logprob": -0.1830487398757148, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.00027357693761587143}, {"id": 44, "seek": 30968, "start": 328.6, "end": 337.64, "text": " build. When we run Docker build from the client, the Docker DMO will try to take a Docker file.", "tokens": [1322, 13, 1133, 321, 1190, 33772, 1322, 490, 264, 6423, 11, 264, 33772, 15322, 46, 486, 853, 281, 747, 257, 33772, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1830487398757148, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.00027357693761587143}, {"id": 45, "seek": 33764, "start": 337.64, "end": 345.71999999999997, "text": " A Docker file is a recipe with a lot of instructions where we put all the commands to run our application", "tokens": [316, 33772, 3991, 307, 257, 6782, 365, 257, 688, 295, 9415, 689, 321, 829, 439, 264, 16901, 281, 1190, 527, 3861], "temperature": 0.0, "avg_logprob": -0.17407923246684828, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00024246901739388704}, {"id": 46, "seek": 33764, "start": 345.71999999999997, "end": 351.4, "text": " and deploy it. So I'm going to, the Docker DMO is going to take the Docker file and build", "tokens": [293, 7274, 309, 13, 407, 286, 478, 516, 281, 11, 264, 33772, 15322, 46, 307, 516, 281, 747, 264, 33772, 3991, 293, 1322], "temperature": 0.0, "avg_logprob": -0.17407923246684828, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00024246901739388704}, {"id": 47, "seek": 33764, "start": 351.4, "end": 359.0, "text": " it, build the image, and if you want, we can also run it. We run, we will create a container.", "tokens": [309, 11, 1322, 264, 3256, 11, 293, 498, 291, 528, 11, 321, 393, 611, 1190, 309, 13, 492, 1190, 11, 321, 486, 1884, 257, 10129, 13], "temperature": 0.0, "avg_logprob": -0.17407923246684828, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00024246901739388704}, {"id": 48, "seek": 33764, "start": 359.0, "end": 366.47999999999996, "text": " The container is our application that is already alive and is ready to make connections", "tokens": [440, 10129, 307, 527, 3861, 300, 307, 1217, 5465, 293, 307, 1919, 281, 652, 9271], "temperature": 0.0, "avg_logprob": -0.17407923246684828, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00024246901739388704}, {"id": 49, "seek": 36648, "start": 366.48, "end": 375.84000000000003, "text": " of petitions. One more thing here is that we can have everything in our host or we can", "tokens": [295, 3817, 2451, 13, 1485, 544, 551, 510, 307, 300, 321, 393, 362, 1203, 294, 527, 3975, 420, 321, 393], "temperature": 0.0, "avg_logprob": -0.2198050125785496, "compression_ratio": 1.44, "no_speech_prob": 0.0014990700874477625}, {"id": 50, "seek": 36648, "start": 375.84000000000003, "end": 391.6, "text": " have clients, remote clients that could make petitions to the Docker DMO. Container benefits.", "tokens": [362, 6982, 11, 8607, 6982, 300, 727, 652, 3817, 2451, 281, 264, 33772, 15322, 46, 13, 43732, 260, 5311, 13], "temperature": 0.0, "avg_logprob": -0.2198050125785496, "compression_ratio": 1.44, "no_speech_prob": 0.0014990700874477625}, {"id": 51, "seek": 39160, "start": 391.6, "end": 397.08000000000004, "text": " There are pros and cons, but now I'm going to focus on these benefits, the containers", "tokens": [821, 366, 6267, 293, 1014, 11, 457, 586, 286, 478, 516, 281, 1879, 322, 613, 5311, 11, 264, 17089], "temperature": 0.0, "avg_logprob": -0.25958233569041794, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0001577019429532811}, {"id": 52, "seek": 39160, "start": 397.08000000000004, "end": 404.92, "text": " give us. So one of these is we can reduce costs with this because we can run several containers", "tokens": [976, 505, 13, 407, 472, 295, 613, 307, 321, 393, 5407, 5497, 365, 341, 570, 321, 393, 1190, 2940, 17089], "temperature": 0.0, "avg_logprob": -0.25958233569041794, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0001577019429532811}, {"id": 53, "seek": 39160, "start": 404.92, "end": 411.24, "text": " in a single infrastructure. That's infrastructure that we have because of the technology of", "tokens": [294, 257, 2167, 6896, 13, 663, 311, 6896, 300, 321, 362, 570, 295, 264, 2899, 295], "temperature": 0.0, "avg_logprob": -0.25958233569041794, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0001577019429532811}, {"id": 54, "seek": 39160, "start": 411.24, "end": 418.56, "text": " containers is different than the virtualization. In virtualization, we use the hypervisor and", "tokens": [17089, 307, 819, 813, 264, 6374, 2144, 13, 682, 6374, 2144, 11, 321, 764, 264, 9848, 16457, 293], "temperature": 0.0, "avg_logprob": -0.25958233569041794, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0001577019429532811}, {"id": 55, "seek": 41856, "start": 418.56, "end": 426.44, "text": " when you create virtual machines, it consumes more resources from your, from your infrastructure,", "tokens": [562, 291, 1884, 6374, 8379, 11, 309, 48823, 544, 3593, 490, 428, 11, 490, 428, 6896, 11], "temperature": 0.0, "avg_logprob": -0.18934364778449736, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.00015826887101866305}, {"id": 56, "seek": 41856, "start": 426.44, "end": 431.16, "text": " but when you use containers, it's very different. You are using that technology, a container", "tokens": [457, 562, 291, 764, 17089, 11, 309, 311, 588, 819, 13, 509, 366, 1228, 300, 2899, 11, 257, 10129], "temperature": 0.0, "avg_logprob": -0.18934364778449736, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.00015826887101866305}, {"id": 57, "seek": 41856, "start": 431.16, "end": 436.68, "text": " would make it possible to run different, a lot of containers in a single machine. So", "tokens": [576, 652, 309, 1944, 281, 1190, 819, 11, 257, 688, 295, 17089, 294, 257, 2167, 3479, 13, 407], "temperature": 0.0, "avg_logprob": -0.18934364778449736, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.00015826887101866305}, {"id": 58, "seek": 41856, "start": 436.68, "end": 443.04, "text": " for that reason, it's possible to reduce costs. Also, the containers are very friendly with", "tokens": [337, 300, 1778, 11, 309, 311, 1944, 281, 5407, 5497, 13, 2743, 11, 264, 17089, 366, 588, 9208, 365], "temperature": 0.0, "avg_logprob": -0.18934364778449736, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.00015826887101866305}, {"id": 59, "seek": 44304, "start": 443.04, "end": 448.76000000000005, "text": " continuous integration and continuous delivery process. If you have like a big application,", "tokens": [10957, 10980, 293, 10957, 8982, 1399, 13, 759, 291, 362, 411, 257, 955, 3861, 11], "temperature": 0.0, "avg_logprob": -0.1871437793824731, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0002947926113847643}, {"id": 60, "seek": 44304, "start": 448.76000000000005, "end": 455.68, "text": " a monolithic application, this, and you want to, you want to run container, you want to", "tokens": [257, 1108, 42878, 3861, 11, 341, 11, 293, 291, 528, 281, 11, 291, 528, 281, 1190, 10129, 11, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.1871437793824731, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0002947926113847643}, {"id": 61, "seek": 44304, "start": 455.68, "end": 462.08000000000004, "text": " integrate it in the DevOps process. This is going to be hard. We have to work like microservices", "tokens": [13365, 309, 294, 264, 43051, 1399, 13, 639, 307, 516, 281, 312, 1152, 13, 492, 362, 281, 589, 411, 15547, 47480], "temperature": 0.0, "avg_logprob": -0.1871437793824731, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0002947926113847643}, {"id": 62, "seek": 44304, "start": 462.08000000000004, "end": 470.32000000000005, "text": " to make each service as a container and included in the continuous integration and continuous", "tokens": [281, 652, 1184, 2643, 382, 257, 10129, 293, 5556, 294, 264, 10957, 10980, 293, 10957], "temperature": 0.0, "avg_logprob": -0.1871437793824731, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0002947926113847643}, {"id": 63, "seek": 47032, "start": 470.32, "end": 476.44, "text": " delivery process. It's easy. When we build, when we build our application over a container,", "tokens": [8982, 1399, 13, 467, 311, 1858, 13, 1133, 321, 1322, 11, 562, 321, 1322, 527, 3861, 670, 257, 10129, 11], "temperature": 0.0, "avg_logprob": -0.1855095969306098, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.00020900050003547221}, {"id": 64, "seek": 47032, "start": 476.44, "end": 481.12, "text": " it's easy to kill it. It's easy to create it again. It's easy to fail and the process", "tokens": [309, 311, 1858, 281, 1961, 309, 13, 467, 311, 1858, 281, 1884, 309, 797, 13, 467, 311, 1858, 281, 3061, 293, 264, 1399], "temperature": 0.0, "avg_logprob": -0.1855095969306098, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.00020900050003547221}, {"id": 65, "seek": 47032, "start": 481.12, "end": 491.48, "text": " is faster. Another benefit is the multicloud compatibility with the time several companies", "tokens": [307, 4663, 13, 3996, 5121, 307, 264, 30608, 29221, 34237, 365, 264, 565, 2940, 3431], "temperature": 0.0, "avg_logprob": -0.1855095969306098, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.00020900050003547221}, {"id": 66, "seek": 47032, "start": 491.48, "end": 497.76, "text": " try to migrate to a hybrid cloud. They just don't, don't want to have everything on premise.", "tokens": [853, 281, 31821, 281, 257, 13051, 4588, 13, 814, 445, 500, 380, 11, 500, 380, 528, 281, 362, 1203, 322, 22045, 13], "temperature": 0.0, "avg_logprob": -0.1855095969306098, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.00020900050003547221}, {"id": 67, "seek": 49776, "start": 497.76, "end": 507.64, "text": " They also want to scale. They want to grow. So for a reason, they opt for cloud and containers", "tokens": [814, 611, 528, 281, 4373, 13, 814, 528, 281, 1852, 13, 407, 337, 257, 1778, 11, 436, 2427, 337, 4588, 293, 17089], "temperature": 0.0, "avg_logprob": -0.27175898366160206, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00019020546460524201}, {"id": 68, "seek": 49776, "start": 507.64, "end": 520.52, "text": " fit very good in this. You can install Docker. I know you did it. You can choose your distro.", "tokens": [3318, 588, 665, 294, 341, 13, 509, 393, 3625, 33772, 13, 286, 458, 291, 630, 309, 13, 509, 393, 2826, 428, 1483, 340, 13], "temperature": 0.0, "avg_logprob": -0.27175898366160206, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00019020546460524201}, {"id": 69, "seek": 49776, "start": 520.52, "end": 526.12, "text": " You are, you use Debian, the CentOS, everything. So you can go to the official Docker documentation", "tokens": [509, 366, 11, 291, 764, 1346, 20196, 11, 264, 3408, 4367, 11, 1203, 13, 407, 291, 393, 352, 281, 264, 4783, 33772, 14333], "temperature": 0.0, "avg_logprob": -0.27175898366160206, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00019020546460524201}, {"id": 70, "seek": 52612, "start": 526.12, "end": 532.68, "text": " and easily look all the steps. When you install this, it will install it, the Docker client,", "tokens": [293, 3612, 574, 439, 264, 4439, 13, 1133, 291, 3625, 341, 11, 309, 486, 3625, 309, 11, 264, 33772, 6423, 11], "temperature": 0.0, "avg_logprob": -0.2390332276793732, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0001312417589360848}, {"id": 71, "seek": 52612, "start": 532.68, "end": 541.36, "text": " the Docker DMO and other tools that you will need to use Docker in your local matching.", "tokens": [264, 33772, 15322, 46, 293, 661, 3873, 300, 291, 486, 643, 281, 764, 33772, 294, 428, 2654, 14324, 13], "temperature": 0.0, "avg_logprob": -0.2390332276793732, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0001312417589360848}, {"id": 72, "seek": 52612, "start": 541.36, "end": 548.72, "text": " We already talk about containers, right? But this talk is about exploring database on containers.", "tokens": [492, 1217, 751, 466, 17089, 11, 558, 30, 583, 341, 751, 307, 466, 12736, 8149, 322, 17089, 13], "temperature": 0.0, "avg_logprob": -0.2390332276793732, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0001312417589360848}, {"id": 73, "seek": 52612, "start": 548.72, "end": 554.4, "text": " We are going to talk about my SQL, which is at this base relational database. We know", "tokens": [492, 366, 516, 281, 751, 466, 452, 19200, 11, 597, 307, 412, 341, 3096, 38444, 8149, 13, 492, 458], "temperature": 0.0, "avg_logprob": -0.2390332276793732, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0001312417589360848}, {"id": 74, "seek": 55440, "start": 554.4, "end": 562.56, "text": " that it's a database. And to run my SQL on containers, we need to understand how volumes", "tokens": [300, 309, 311, 257, 8149, 13, 400, 281, 1190, 452, 19200, 322, 17089, 11, 321, 643, 281, 1223, 577, 22219], "temperature": 0.0, "avg_logprob": -0.2581945061683655, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0004150629392825067}, {"id": 75, "seek": 55440, "start": 562.56, "end": 568.68, "text": " works because the most important thing running databases on containers is the data. If we", "tokens": [1985, 570, 264, 881, 1021, 551, 2614, 22380, 322, 17089, 307, 264, 1412, 13, 759, 321], "temperature": 0.0, "avg_logprob": -0.2581945061683655, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0004150629392825067}, {"id": 76, "seek": 55440, "start": 568.68, "end": 578.1999999999999, "text": " lost the data, we lost everything. For the next slides, we are going to focus in this", "tokens": [2731, 264, 1412, 11, 321, 2731, 1203, 13, 1171, 264, 958, 9788, 11, 321, 366, 516, 281, 1879, 294, 341], "temperature": 0.0, "avg_logprob": -0.2581945061683655, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.0004150629392825067}, {"id": 77, "seek": 57820, "start": 578.2, "end": 587.76, "text": " part. We will use the image of Percona server for my SQL. This Percona server for my SQL", "tokens": [644, 13, 492, 486, 764, 264, 3256, 295, 3026, 1671, 64, 7154, 337, 452, 19200, 13, 639, 3026, 1671, 64, 7154, 337, 452, 19200], "temperature": 0.0, "avg_logprob": -0.2234183824979342, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.00012723507825285196}, {"id": 78, "seek": 57820, "start": 587.76, "end": 595.8000000000001, "text": " is open source. It's like my SQL, but with more nice things. You can use it. It's open", "tokens": [307, 1269, 4009, 13, 467, 311, 411, 452, 19200, 11, 457, 365, 544, 1481, 721, 13, 509, 393, 764, 309, 13, 467, 311, 1269], "temperature": 0.0, "avg_logprob": -0.2234183824979342, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.00012723507825285196}, {"id": 79, "seek": 57820, "start": 595.8000000000001, "end": 602.9200000000001, "text": " source. It's in Docker Hub. So we will use this image and we will create a Docker container.", "tokens": [4009, 13, 467, 311, 294, 33772, 18986, 13, 407, 321, 486, 764, 341, 3256, 293, 321, 486, 1884, 257, 33772, 10129, 13], "temperature": 0.0, "avg_logprob": -0.2234183824979342, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.00012723507825285196}, {"id": 80, "seek": 60292, "start": 602.92, "end": 608.24, "text": " We will see how it works with all volumes. We will see the layers in Docker and then", "tokens": [492, 486, 536, 577, 309, 1985, 365, 439, 22219, 13, 492, 486, 536, 264, 7914, 294, 33772, 293, 550], "temperature": 0.0, "avg_logprob": -0.17298159391983695, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0004231701313983649}, {"id": 81, "seek": 60292, "start": 608.24, "end": 616.12, "text": " we will create a persistent volume and we will see how it changes in the layers of Docker.", "tokens": [321, 486, 1884, 257, 24315, 5523, 293, 321, 486, 536, 577, 309, 2962, 294, 264, 7914, 295, 33772, 13], "temperature": 0.0, "avg_logprob": -0.17298159391983695, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0004231701313983649}, {"id": 82, "seek": 60292, "start": 616.12, "end": 625.7199999999999, "text": " So just here to see that if you want to have an image, it's necessary to have a Docker file.", "tokens": [407, 445, 510, 281, 536, 300, 498, 291, 528, 281, 362, 364, 3256, 11, 309, 311, 4818, 281, 362, 257, 33772, 3991, 13], "temperature": 0.0, "avg_logprob": -0.17298159391983695, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.0004231701313983649}, {"id": 83, "seek": 62572, "start": 625.72, "end": 633.36, "text": " You can use a Docker file before by yourself. That's good. A Docker file is a recipe where", "tokens": [509, 393, 764, 257, 33772, 3991, 949, 538, 1803, 13, 663, 311, 665, 13, 316, 33772, 3991, 307, 257, 6782, 689], "temperature": 0.0, "avg_logprob": -0.24334080035869893, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.00029356710729189217}, {"id": 84, "seek": 62572, "start": 633.36, "end": 639.9200000000001, "text": " you will put everything for your application. So you need this to create an image. Then", "tokens": [291, 486, 829, 1203, 337, 428, 3861, 13, 407, 291, 643, 341, 281, 1884, 364, 3256, 13, 1396], "temperature": 0.0, "avg_logprob": -0.24334080035869893, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.00029356710729189217}, {"id": 85, "seek": 62572, "start": 639.9200000000001, "end": 647.0400000000001, "text": " you need an image to create your Docker container. There are three essential steps here to remember", "tokens": [291, 643, 364, 3256, 281, 1884, 428, 33772, 10129, 13, 821, 366, 1045, 7115, 4439, 510, 281, 1604], "temperature": 0.0, "avg_logprob": -0.24334080035869893, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.00029356710729189217}, {"id": 86, "seek": 64704, "start": 647.04, "end": 660.8399999999999, "text": " how Docker works. We will run a single Percona server for my SQL container. We will use Docker", "tokens": [577, 33772, 1985, 13, 492, 486, 1190, 257, 2167, 3026, 1671, 64, 7154, 337, 452, 19200, 10129, 13, 492, 486, 764, 33772], "temperature": 0.0, "avg_logprob": -0.21151880264282227, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00032434400054626167}, {"id": 87, "seek": 64704, "start": 660.8399999999999, "end": 670.68, "text": " run to create the image. No. We don't use Docker run to create the image. We use Docker", "tokens": [1190, 281, 1884, 264, 3256, 13, 883, 13, 492, 500, 380, 764, 33772, 1190, 281, 1884, 264, 3256, 13, 492, 764, 33772], "temperature": 0.0, "avg_logprob": -0.21151880264282227, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.00032434400054626167}, {"id": 88, "seek": 67068, "start": 670.68, "end": 679.52, "text": " run to create a container. So we use this to create a container. So we will do dash", "tokens": [1190, 281, 1884, 257, 10129, 13, 407, 321, 764, 341, 281, 1884, 257, 10129, 13, 407, 321, 486, 360, 8240], "temperature": 0.0, "avg_logprob": -0.2791718260882652, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.00021936913253739476}, {"id": 89, "seek": 67068, "start": 679.52, "end": 688.9599999999999, "text": " D to say run this container in the background. I don't want to use the terminal. And I will", "tokens": [413, 281, 584, 1190, 341, 10129, 294, 264, 3678, 13, 286, 500, 380, 528, 281, 764, 264, 14709, 13, 400, 286, 486], "temperature": 0.0, "avg_logprob": -0.2791718260882652, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.00021936913253739476}, {"id": 90, "seek": 67068, "start": 688.9599999999999, "end": 695.88, "text": " call it Percona server for my Percona server one. I will pass it like the environment variable,", "tokens": [818, 309, 3026, 1671, 64, 7154, 337, 452, 3026, 1671, 64, 7154, 472, 13, 286, 486, 1320, 309, 411, 264, 2823, 7006, 11], "temperature": 0.0, "avg_logprob": -0.2791718260882652, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.00021936913253739476}, {"id": 91, "seek": 69588, "start": 695.88, "end": 701.88, "text": " for the root. This is not a good practice here. This is just to show how we are going", "tokens": [337, 264, 5593, 13, 639, 307, 406, 257, 665, 3124, 510, 13, 639, 307, 445, 281, 855, 577, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.1667413910230001, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00017951702466234565}, {"id": 92, "seek": 69588, "start": 701.88, "end": 706.88, "text": " to create a container. And we will use this official Percona server for my SQL. With this", "tokens": [281, 1884, 257, 10129, 13, 400, 321, 486, 764, 341, 4783, 3026, 1671, 64, 7154, 337, 452, 19200, 13, 2022, 341], "temperature": 0.0, "avg_logprob": -0.1667413910230001, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00017951702466234565}, {"id": 93, "seek": 69588, "start": 706.88, "end": 713.8, "text": " I am creating a container, right? I'm creating a container with this one. Okay? So if we", "tokens": [286, 669, 4084, 257, 10129, 11, 558, 30, 286, 478, 4084, 257, 10129, 365, 341, 472, 13, 1033, 30, 407, 498, 321], "temperature": 0.0, "avg_logprob": -0.1667413910230001, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00017951702466234565}, {"id": 94, "seek": 69588, "start": 713.8, "end": 723.0, "text": " go to Docker image LS, this is going to pull the image of Percona server and then it will", "tokens": [352, 281, 33772, 3256, 36657, 11, 341, 307, 516, 281, 2235, 264, 3256, 295, 3026, 1671, 64, 7154, 293, 550, 309, 486], "temperature": 0.0, "avg_logprob": -0.1667413910230001, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00017951702466234565}, {"id": 95, "seek": 72300, "start": 723.0, "end": 729.16, "text": " create the container. That command is going to do two things. It's going to bring the", "tokens": [1884, 264, 10129, 13, 663, 5622, 307, 516, 281, 360, 732, 721, 13, 467, 311, 516, 281, 1565, 264], "temperature": 0.0, "avg_logprob": -0.22272372968269116, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00024546476197429}, {"id": 96, "seek": 72300, "start": 729.16, "end": 735.6, "text": " image from the official Dockerfab and it's going to create a container. So if we see", "tokens": [3256, 490, 264, 4783, 33772, 69, 455, 293, 309, 311, 516, 281, 1884, 257, 10129, 13, 407, 498, 321, 536], "temperature": 0.0, "avg_logprob": -0.22272372968269116, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00024546476197429}, {"id": 97, "seek": 72300, "start": 735.6, "end": 747.4, "text": " Docker container PS, our container is up. Okay. After we have the database, we need", "tokens": [33772, 10129, 8168, 11, 527, 10129, 307, 493, 13, 1033, 13, 2381, 321, 362, 264, 8149, 11, 321, 643], "temperature": 0.0, "avg_logprob": -0.22272372968269116, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.00024546476197429}, {"id": 98, "seek": 74740, "start": 747.4, "end": 753.12, "text": " to add data. We will add databases, we will add data, we will change registers, we will", "tokens": [281, 909, 1412, 13, 492, 486, 909, 22380, 11, 321, 486, 909, 1412, 11, 321, 486, 1319, 38351, 11, 321, 486], "temperature": 0.0, "avg_logprob": -0.2041416576930455, "compression_ratio": 1.5914634146341464, "no_speech_prob": 0.00018414275837130845}, {"id": 99, "seek": 74740, "start": 753.12, "end": 763.0799999999999, "text": " have transactions, many things that we can do like a regular database. Okay. If we run", "tokens": [362, 16856, 11, 867, 721, 300, 321, 393, 360, 411, 257, 3890, 8149, 13, 1033, 13, 759, 321, 1190], "temperature": 0.0, "avg_logprob": -0.2041416576930455, "compression_ratio": 1.5914634146341464, "no_speech_prob": 0.00018414275837130845}, {"id": 100, "seek": 74740, "start": 763.0799999999999, "end": 770.52, "text": " a single Percona server in my SQL container, we know how it works in layers. If we see", "tokens": [257, 2167, 3026, 1671, 64, 7154, 294, 452, 19200, 10129, 11, 321, 458, 577, 309, 1985, 294, 7914, 13, 759, 321, 536], "temperature": 0.0, "avg_logprob": -0.2041416576930455, "compression_ratio": 1.5914634146341464, "no_speech_prob": 0.00018414275837130845}, {"id": 101, "seek": 77052, "start": 770.52, "end": 777.6, "text": " this in green, there are layers from Percona, Percona server image. This is the image that", "tokens": [341, 294, 3092, 11, 456, 366, 7914, 490, 3026, 1671, 64, 11, 3026, 1671, 64, 7154, 3256, 13, 639, 307, 264, 3256, 300], "temperature": 0.0, "avg_logprob": -0.18382266998291016, "compression_ratio": 1.8481675392670156, "no_speech_prob": 0.0009742708061821759}, {"id": 102, "seek": 77052, "start": 777.6, "end": 784.04, "text": " we pull it, that we can change. This is just react only. We can change this, but in top", "tokens": [321, 2235, 309, 11, 300, 321, 393, 1319, 13, 639, 307, 445, 4515, 787, 13, 492, 393, 1319, 341, 11, 457, 294, 1192], "temperature": 0.0, "avg_logprob": -0.18382266998291016, "compression_ratio": 1.8481675392670156, "no_speech_prob": 0.0009742708061821759}, {"id": 103, "seek": 77052, "start": 784.04, "end": 790.04, "text": " of that, it's going to be created a layer, a new layer. This layer, this layer is react", "tokens": [295, 300, 11, 309, 311, 516, 281, 312, 2942, 257, 4583, 11, 257, 777, 4583, 13, 639, 4583, 11, 341, 4583, 307, 4515], "temperature": 0.0, "avg_logprob": -0.18382266998291016, "compression_ratio": 1.8481675392670156, "no_speech_prob": 0.0009742708061821759}, {"id": 104, "seek": 77052, "start": 790.04, "end": 796.3199999999999, "text": " only. I can add data. This layer is the one that will contain all the things that I am", "tokens": [787, 13, 286, 393, 909, 1412, 13, 639, 4583, 307, 264, 472, 300, 486, 5304, 439, 264, 721, 300, 286, 669], "temperature": 0.0, "avg_logprob": -0.18382266998291016, "compression_ratio": 1.8481675392670156, "no_speech_prob": 0.0009742708061821759}, {"id": 105, "seek": 79632, "start": 796.32, "end": 803.72, "text": " doing in Docker on that image, on that container. I added a new database. Yes. I create a new", "tokens": [884, 294, 33772, 322, 300, 3256, 11, 322, 300, 10129, 13, 286, 3869, 257, 777, 8149, 13, 1079, 13, 286, 1884, 257, 777], "temperature": 0.0, "avg_logprob": -0.2261410903930664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0004043829976581037}, {"id": 106, "seek": 79632, "start": 803.72, "end": 809.12, "text": " registry. I delete it. I add the transactions. All this is going to save it here. But what", "tokens": [36468, 13, 286, 12097, 309, 13, 286, 909, 264, 16856, 13, 1057, 341, 307, 516, 281, 3155, 309, 510, 13, 583, 437], "temperature": 0.0, "avg_logprob": -0.2261410903930664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0004043829976581037}, {"id": 107, "seek": 79632, "start": 809.12, "end": 815.72, "text": " happens if I don't have volume? My container is ephemeral, right? It could die. It could", "tokens": [2314, 498, 286, 500, 380, 362, 5523, 30, 1222, 10129, 307, 308, 41245, 2790, 11, 558, 30, 467, 727, 978, 13, 467, 727], "temperature": 0.0, "avg_logprob": -0.2261410903930664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0004043829976581037}, {"id": 108, "seek": 79632, "start": 815.72, "end": 821.48, "text": " crash. My machine could crash. And all my data is going to be lost. I will, I will", "tokens": [8252, 13, 1222, 3479, 727, 8252, 13, 400, 439, 452, 1412, 307, 516, 281, 312, 2731, 13, 286, 486, 11, 286, 486], "temperature": 0.0, "avg_logprob": -0.2261410903930664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0004043829976581037}, {"id": 109, "seek": 82148, "start": 821.48, "end": 828.88, "text": " lose all the data. We will see how it works with multiple containers. To run multiple", "tokens": [3624, 439, 264, 1412, 13, 492, 486, 536, 577, 309, 1985, 365, 3866, 17089, 13, 1407, 1190, 3866], "temperature": 0.0, "avg_logprob": -0.15041041642092587, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.00017073581693693995}, {"id": 110, "seek": 82148, "start": 828.88, "end": 833.96, "text": " containers with the same image, if we see this is the same image, the same version of", "tokens": [17089, 365, 264, 912, 3256, 11, 498, 321, 536, 341, 307, 264, 912, 3256, 11, 264, 912, 3037, 295], "temperature": 0.0, "avg_logprob": -0.15041041642092587, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.00017073581693693995}, {"id": 111, "seek": 82148, "start": 833.96, "end": 838.6, "text": " the image, we will just change the name of this container. Also, we can change another", "tokens": [264, 3256, 11, 321, 486, 445, 1319, 264, 1315, 295, 341, 10129, 13, 2743, 11, 321, 393, 1319, 1071], "temperature": 0.0, "avg_logprob": -0.15041041642092587, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.00017073581693693995}, {"id": 112, "seek": 82148, "start": 838.6, "end": 849.32, "text": " thing because this is a database, right? What thing we can change? They run in a port, right?", "tokens": [551, 570, 341, 307, 257, 8149, 11, 558, 30, 708, 551, 321, 393, 1319, 30, 814, 1190, 294, 257, 2436, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15041041642092587, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.00017073581693693995}, {"id": 113, "seek": 84932, "start": 849.32, "end": 857.6, "text": " In which port my SQL used to run? Yeah. So I need to change the port for the other container", "tokens": [682, 597, 2436, 452, 19200, 1143, 281, 1190, 30, 865, 13, 407, 286, 643, 281, 1319, 264, 2436, 337, 264, 661, 10129], "temperature": 0.0, "avg_logprob": -0.18614324738707724, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004031423886772245}, {"id": 114, "seek": 84932, "start": 857.6, "end": 869.7600000000001, "text": " to avoid the conflict. Okay. How it works in layers. The same. We will use the same layer.", "tokens": [281, 5042, 264, 6596, 13, 1033, 13, 1012, 309, 1985, 294, 7914, 13, 440, 912, 13, 492, 486, 764, 264, 912, 4583, 13], "temperature": 0.0, "avg_logprob": -0.18614324738707724, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004031423886772245}, {"id": 115, "seek": 84932, "start": 869.7600000000001, "end": 875.2800000000001, "text": " We will use the same layer for Percona, Percona server, which can, we can modify. But in top", "tokens": [492, 486, 764, 264, 912, 4583, 337, 3026, 1671, 64, 11, 3026, 1671, 64, 7154, 11, 597, 393, 11, 321, 393, 16927, 13, 583, 294, 1192], "temperature": 0.0, "avg_logprob": -0.18614324738707724, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.0004031423886772245}, {"id": 116, "seek": 87528, "start": 875.28, "end": 880.16, "text": " of that, we are going to have two layers more. One of the first containers that I created", "tokens": [295, 300, 11, 321, 366, 516, 281, 362, 732, 7914, 544, 13, 1485, 295, 264, 700, 17089, 300, 286, 2942], "temperature": 0.0, "avg_logprob": -0.11479632728978208, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00042877194937318563}, {"id": 117, "seek": 87528, "start": 880.16, "end": 885.88, "text": " and the second for the other that I can add. I can add data. I can change things. But once", "tokens": [293, 264, 1150, 337, 264, 661, 300, 286, 393, 909, 13, 286, 393, 909, 1412, 13, 286, 393, 1319, 721, 13, 583, 1564], "temperature": 0.0, "avg_logprob": -0.11479632728978208, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00042877194937318563}, {"id": 118, "seek": 87528, "start": 885.88, "end": 891.04, "text": " again, if I don't have volume, this is going to die. But this is how to work if we want", "tokens": [797, 11, 498, 286, 500, 380, 362, 5523, 11, 341, 307, 516, 281, 978, 13, 583, 341, 307, 577, 281, 589, 498, 321, 528], "temperature": 0.0, "avg_logprob": -0.11479632728978208, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00042877194937318563}, {"id": 119, "seek": 87528, "start": 891.04, "end": 902.28, "text": " to create an application when it doesn't matter if we save the state of this application.", "tokens": [281, 1884, 364, 3861, 562, 309, 1177, 380, 1871, 498, 321, 3155, 264, 1785, 295, 341, 3861, 13], "temperature": 0.0, "avg_logprob": -0.11479632728978208, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00042877194937318563}, {"id": 120, "seek": 90228, "start": 902.28, "end": 913.76, "text": " This is important. Persist data in databases is really important for this kind of application", "tokens": [639, 307, 1021, 13, 14006, 468, 1412, 294, 22380, 307, 534, 1021, 337, 341, 733, 295, 3861], "temperature": 0.0, "avg_logprob": -0.34746407728928785, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0006240351358428597}, {"id": 121, "seek": 90228, "start": 913.76, "end": 920.8399999999999, "text": " because sometimes we think that, like Kubernetes, since it was created for a state less application,", "tokens": [570, 2171, 321, 519, 300, 11, 411, 23145, 11, 1670, 309, 390, 2942, 337, 257, 1785, 1570, 3861, 11], "temperature": 0.0, "avg_logprob": -0.34746407728928785, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0006240351358428597}, {"id": 122, "seek": 90228, "start": 920.8399999999999, "end": 926.48, "text": " but now we have options to use stateful applications on containers. And this is one of the reasons.", "tokens": [457, 586, 321, 362, 3956, 281, 764, 1785, 906, 5821, 322, 17089, 13, 400, 341, 307, 472, 295, 264, 4112, 13], "temperature": 0.0, "avg_logprob": -0.34746407728928785, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.0006240351358428597}, {"id": 123, "seek": 92648, "start": 926.48, "end": 933.24, "text": " Create volumes. So it's pretty easy to create volume. We can create a volume just with dash", "tokens": [20248, 22219, 13, 407, 309, 311, 1238, 1858, 281, 1884, 5523, 13, 492, 393, 1884, 257, 5523, 445, 365, 8240], "temperature": 0.0, "avg_logprob": -0.23744909542123066, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0008768087718635798}, {"id": 124, "seek": 92648, "start": 933.24, "end": 941.9200000000001, "text": " V or dash, dash volume. And we can say it, we can create a local volume with local run", "tokens": [691, 420, 8240, 11, 8240, 5523, 13, 400, 321, 393, 584, 309, 11, 321, 393, 1884, 257, 2654, 5523, 365, 2654, 1190], "temperature": 0.0, "avg_logprob": -0.23744909542123066, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0008768087718635798}, {"id": 125, "seek": 92648, "start": 941.9200000000001, "end": 949.24, "text": " and detach. We will call it Percona server. The same process. And when we say dash V,", "tokens": [293, 43245, 13, 492, 486, 818, 309, 3026, 1671, 64, 7154, 13, 440, 912, 1399, 13, 400, 562, 321, 584, 8240, 691, 11], "temperature": 0.0, "avg_logprob": -0.23744909542123066, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0008768087718635798}, {"id": 126, "seek": 92648, "start": 949.24, "end": 956.32, "text": " we are saying, okay, this will be my volume in a host, in my local data directory. And", "tokens": [321, 366, 1566, 11, 1392, 11, 341, 486, 312, 452, 5523, 294, 257, 3975, 11, 294, 452, 2654, 1412, 21120, 13, 400], "temperature": 0.0, "avg_logprob": -0.23744909542123066, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0008768087718635798}, {"id": 127, "seek": 95632, "start": 956.32, "end": 971.6400000000001, "text": " this one is going to be inside my container. So this is like a mirror from this image.", "tokens": [341, 472, 307, 516, 281, 312, 1854, 452, 10129, 13, 407, 341, 307, 411, 257, 8013, 490, 341, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14334090980323586, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00017468241276219487}, {"id": 128, "seek": 95632, "start": 971.6400000000001, "end": 977.88, "text": " And how it works. In layers, we have the same, the layer that we can modify. And in top of", "tokens": [400, 577, 309, 1985, 13, 682, 7914, 11, 321, 362, 264, 912, 11, 264, 4583, 300, 321, 393, 16927, 13, 400, 294, 1192, 295], "temperature": 0.0, "avg_logprob": -0.14334090980323586, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00017468241276219487}, {"id": 129, "seek": 95632, "start": 977.88, "end": 983.2800000000001, "text": " that, we are going to create another layer. But in this case, we are adding, we are creating", "tokens": [300, 11, 321, 366, 516, 281, 1884, 1071, 4583, 13, 583, 294, 341, 1389, 11, 321, 366, 5127, 11, 321, 366, 4084], "temperature": 0.0, "avg_logprob": -0.14334090980323586, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00017468241276219487}, {"id": 130, "seek": 98328, "start": 983.28, "end": 993.76, "text": " the mounted volume in BarLivMySQL. There are other directories that we can create the volume.", "tokens": [264, 19138, 5523, 294, 4156, 43, 592, 8506, 39934, 13, 821, 366, 661, 5391, 530, 300, 321, 393, 1884, 264, 5523, 13], "temperature": 0.0, "avg_logprob": -0.20372867584228516, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.00019273457292001694}, {"id": 131, "seek": 98328, "start": 993.76, "end": 1000.36, "text": " I am just adding, as an example, this, because in MySQL, we have configuration files. We", "tokens": [286, 669, 445, 5127, 11, 382, 364, 1365, 11, 341, 11, 570, 294, 1222, 39934, 11, 321, 362, 11694, 7098, 13, 492], "temperature": 0.0, "avg_logprob": -0.20372867584228516, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.00019273457292001694}, {"id": 132, "seek": 98328, "start": 1000.36, "end": 1006.4, "text": " have logs. We have another things. But for that, we want to create these volumes for", "tokens": [362, 20820, 13, 492, 362, 1071, 721, 13, 583, 337, 300, 11, 321, 528, 281, 1884, 613, 22219, 337], "temperature": 0.0, "avg_logprob": -0.20372867584228516, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.00019273457292001694}, {"id": 133, "seek": 98328, "start": 1006.4, "end": 1011.8, "text": " all of that things. I am just adding, as an example, BarLivMySQL, which is also a directory", "tokens": [439, 295, 300, 721, 13, 286, 669, 445, 5127, 11, 382, 364, 1365, 11, 4156, 43, 592, 8506, 39934, 11, 597, 307, 611, 257, 21120], "temperature": 0.0, "avg_logprob": -0.20372867584228516, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.00019273457292001694}, {"id": 134, "seek": 101180, "start": 1011.8, "end": 1017.8399999999999, "text": " that is very important. And this local directory is the one that could be in my host. But it", "tokens": [300, 307, 588, 1021, 13, 400, 341, 2654, 21120, 307, 264, 472, 300, 727, 312, 294, 452, 3975, 13, 583, 309], "temperature": 0.0, "avg_logprob": -0.27234938565422506, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.002164305653423071}, {"id": 135, "seek": 101180, "start": 1017.8399999999999, "end": 1022.7199999999999, "text": " is not recommended, because if your host crashes, everything crashes too with your volumes.", "tokens": [307, 406, 9628, 11, 570, 498, 428, 3975, 28642, 11, 1203, 28642, 886, 365, 428, 22219, 13], "temperature": 0.0, "avg_logprob": -0.27234938565422506, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.002164305653423071}, {"id": 136, "seek": 101180, "start": 1022.7199999999999, "end": 1041.68, "text": " It is preferable to run it in a remote host. Okay. Two backups. Who here make backups?", "tokens": [467, 307, 4382, 712, 281, 1190, 309, 294, 257, 8607, 3975, 13, 1033, 13, 4453, 50160, 13, 2102, 510, 652, 50160, 30], "temperature": 0.0, "avg_logprob": -0.27234938565422506, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.002164305653423071}, {"id": 137, "seek": 104168, "start": 1041.68, "end": 1049.76, "text": " Okay. I use the very easy way to make backups. I use it for logical backups, my SQL dump", "tokens": [1033, 13, 286, 764, 264, 588, 1858, 636, 281, 652, 50160, 13, 286, 764, 309, 337, 14978, 50160, 11, 452, 19200, 11430], "temperature": 0.0, "avg_logprob": -0.24791151104551373, "compression_ratio": 1.7438423645320198, "no_speech_prob": 0.0005625959602184594}, {"id": 138, "seek": 104168, "start": 1049.76, "end": 1056.52, "text": " used in the container. And for physical backups, we use in the company PerconextraVacup, which", "tokens": [1143, 294, 264, 10129, 13, 400, 337, 4001, 50160, 11, 321, 764, 294, 264, 2237, 3026, 1671, 3828, 424, 53, 326, 1010, 11, 597], "temperature": 0.0, "avg_logprob": -0.24791151104551373, "compression_ratio": 1.7438423645320198, "no_speech_prob": 0.0005625959602184594}, {"id": 139, "seek": 104168, "start": 1056.52, "end": 1064.64, "text": " is, have more features to have that physical backup. And for restore, I will use also my", "tokens": [307, 11, 362, 544, 4122, 281, 362, 300, 4001, 14807, 13, 400, 337, 15227, 11, 286, 486, 764, 611, 452], "temperature": 0.0, "avg_logprob": -0.24791151104551373, "compression_ratio": 1.7438423645320198, "no_speech_prob": 0.0005625959602184594}, {"id": 140, "seek": 106464, "start": 1064.64, "end": 1074.0400000000002, "text": " SQL dump. And we don't use PerconextraVacup in this case, because it has a lot of pins.", "tokens": [19200, 11430, 13, 400, 321, 500, 380, 764, 3026, 1671, 3828, 424, 53, 326, 1010, 294, 341, 1389, 11, 570, 309, 575, 257, 688, 295, 16392, 13], "temperature": 0.0, "avg_logprob": -0.24040212631225585, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00025255067157559097}, {"id": 141, "seek": 106464, "start": 1074.0400000000002, "end": 1083.4, "text": " For backup, I will execute a backup in a container that is already running. PerconaserverVacup", "tokens": [1171, 14807, 11, 286, 486, 14483, 257, 14807, 294, 257, 10129, 300, 307, 1217, 2614, 13, 3026, 1671, 296, 38241, 53, 326, 1010], "temperature": 0.0, "avg_logprob": -0.24040212631225585, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00025255067157559097}, {"id": 142, "seek": 106464, "start": 1083.4, "end": 1090.2800000000002, "text": " is already running. Let's see that we created. And we are executing Docker exit, it, to enter", "tokens": [307, 1217, 2614, 13, 961, 311, 536, 300, 321, 2942, 13, 400, 321, 366, 32368, 33772, 11043, 11, 309, 11, 281, 3242], "temperature": 0.0, "avg_logprob": -0.24040212631225585, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00025255067157559097}, {"id": 143, "seek": 109028, "start": 1090.28, "end": 1099.92, "text": " into the Percona in that container and type that common, my SQL dump, to create a backup", "tokens": [666, 264, 3026, 1671, 64, 294, 300, 10129, 293, 2010, 300, 2689, 11, 452, 19200, 11430, 11, 281, 1884, 257, 14807], "temperature": 0.0, "avg_logprob": -0.20103591596576528, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.0003506596840452403}, {"id": 144, "seek": 109028, "start": 1099.92, "end": 1107.52, "text": " of the database. So the backup is going to be in that file, dump SQL. And the same process", "tokens": [295, 264, 8149, 13, 407, 264, 14807, 307, 516, 281, 312, 294, 300, 3991, 11, 11430, 19200, 13, 400, 264, 912, 1399], "temperature": 0.0, "avg_logprob": -0.20103591596576528, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.0003506596840452403}, {"id": 145, "seek": 109028, "start": 1107.52, "end": 1115.24, "text": " with restore, we can take that backup. And this is a different container. I'm going to", "tokens": [365, 15227, 11, 321, 393, 747, 300, 14807, 13, 400, 341, 307, 257, 819, 10129, 13, 286, 478, 516, 281], "temperature": 0.0, "avg_logprob": -0.20103591596576528, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.0003506596840452403}, {"id": 146, "seek": 111524, "start": 1115.24, "end": 1122.48, "text": " restore the dot SQL file in a different container. In this case, in PerconaserverRestore, using", "tokens": [15227, 264, 5893, 19200, 3991, 294, 257, 819, 10129, 13, 682, 341, 1389, 11, 294, 3026, 1671, 296, 38241, 49, 377, 418, 11, 1228], "temperature": 0.0, "avg_logprob": -0.22946153368268693, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00022828718647360802}, {"id": 147, "seek": 111524, "start": 1122.48, "end": 1132.04, "text": " my SQL, use that command, my SQL. Okay. Best practices or some recommendation to use containers", "tokens": [452, 19200, 11, 764, 300, 5622, 11, 452, 19200, 13, 1033, 13, 9752, 7525, 420, 512, 11879, 281, 764, 17089], "temperature": 0.0, "avg_logprob": -0.22946153368268693, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00022828718647360802}, {"id": 148, "seek": 111524, "start": 1132.04, "end": 1141.1200000000001, "text": " in database. Okay. And one of this is that we can keep constantly monitoring our database", "tokens": [294, 8149, 13, 1033, 13, 400, 472, 295, 341, 307, 300, 321, 393, 1066, 6460, 11028, 527, 8149], "temperature": 0.0, "avg_logprob": -0.22946153368268693, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00022828718647360802}, {"id": 149, "seek": 114112, "start": 1141.12, "end": 1145.84, "text": " and the whole system, because we don't know when we are going to don't have enough resources", "tokens": [293, 264, 1379, 1185, 11, 570, 321, 500, 380, 458, 562, 321, 366, 516, 281, 500, 380, 362, 1547, 3593], "temperature": 0.0, "avg_logprob": -0.23716775008610316, "compression_ratio": 1.766798418972332, "no_speech_prob": 0.00035400650813244283}, {"id": 150, "seek": 114112, "start": 1145.84, "end": 1151.9599999999998, "text": " for our containers. We should be aware of that or have notifications to say, hey, you", "tokens": [337, 527, 17089, 13, 492, 820, 312, 3650, 295, 300, 420, 362, 13426, 281, 584, 11, 4177, 11, 291], "temperature": 0.0, "avg_logprob": -0.23716775008610316, "compression_ratio": 1.766798418972332, "no_speech_prob": 0.00035400650813244283}, {"id": 151, "seek": 114112, "start": 1151.9599999999998, "end": 1157.28, "text": " don't have a note disk, you don't have a note memory, so provision or try to scale in your", "tokens": [500, 380, 362, 257, 3637, 12355, 11, 291, 500, 380, 362, 257, 3637, 4675, 11, 370, 17225, 420, 853, 281, 4373, 294, 428], "temperature": 0.0, "avg_logprob": -0.23716775008610316, "compression_ratio": 1.766798418972332, "no_speech_prob": 0.00035400650813244283}, {"id": 152, "seek": 114112, "start": 1157.28, "end": 1162.8799999999999, "text": " resources. So we should keep monitoring. Using some tools for that, for example, is PMM.", "tokens": [3593, 13, 407, 321, 820, 1066, 11028, 13, 11142, 512, 3873, 337, 300, 11, 337, 1365, 11, 307, 12499, 44, 13], "temperature": 0.0, "avg_logprob": -0.23716775008610316, "compression_ratio": 1.766798418972332, "no_speech_prob": 0.00035400650813244283}, {"id": 153, "seek": 114112, "start": 1162.8799999999999, "end": 1170.9599999999998, "text": " We can use open source monitors to monitor our databases on containers. And we can store", "tokens": [492, 393, 764, 1269, 4009, 26518, 281, 6002, 527, 22380, 322, 17089, 13, 400, 321, 393, 3531], "temperature": 0.0, "avg_logprob": -0.23716775008610316, "compression_ratio": 1.766798418972332, "no_speech_prob": 0.00035400650813244283}, {"id": 154, "seek": 117096, "start": 1170.96, "end": 1176.0, "text": " this data in persistent volume outside the container. It recommended no inside the container,", "tokens": [341, 1412, 294, 24315, 5523, 2380, 264, 10129, 13, 467, 9628, 572, 1854, 264, 10129, 11], "temperature": 0.0, "avg_logprob": -0.2873598416646322, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.00018456151883583516}, {"id": 155, "seek": 117096, "start": 1176.0, "end": 1184.72, "text": " because it's easy to create plans for recovery. We can restore the data easily also and fast.", "tokens": [570, 309, 311, 1858, 281, 1884, 5482, 337, 8597, 13, 492, 393, 15227, 264, 1412, 3612, 611, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.2873598416646322, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.00018456151883583516}, {"id": 156, "seek": 117096, "start": 1184.72, "end": 1192.64, "text": " We should limit the resources of utilization of our containers. Our containers, we know", "tokens": [492, 820, 4948, 264, 3593, 295, 37074, 295, 527, 17089, 13, 2621, 17089, 11, 321, 458], "temperature": 0.0, "avg_logprob": -0.2873598416646322, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.00018456151883583516}, {"id": 157, "seek": 119264, "start": 1192.64, "end": 1201.2800000000002, "text": " that they are small, but also we should limit when they are a lot. And we should regularly", "tokens": [300, 436, 366, 1359, 11, 457, 611, 321, 820, 4948, 562, 436, 366, 257, 688, 13, 400, 321, 820, 11672], "temperature": 0.0, "avg_logprob": -0.1809164785569714, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.00023712724214419723}, {"id": 158, "seek": 119264, "start": 1201.2800000000002, "end": 1209.8000000000002, "text": " have backups of the database and store these backups in a different location. And have", "tokens": [362, 50160, 295, 264, 8149, 293, 3531, 613, 50160, 294, 257, 819, 4914, 13, 400, 362], "temperature": 0.0, "avg_logprob": -0.1809164785569714, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.00023712724214419723}, {"id": 159, "seek": 119264, "start": 1209.8000000000002, "end": 1216.4, "text": " a plan of migration and disaster recovery is really great. In that case, having a monitoring", "tokens": [257, 1393, 295, 17011, 293, 11293, 8597, 307, 534, 869, 13, 682, 300, 1389, 11, 1419, 257, 11028], "temperature": 0.0, "avg_logprob": -0.1809164785569714, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.00023712724214419723}, {"id": 160, "seek": 121640, "start": 1216.4, "end": 1228.16, "text": " tool helps a lot. And what more? That's all. You can find me in LinkedIn and Twitter.", "tokens": [2290, 3665, 257, 688, 13, 400, 437, 544, 30, 663, 311, 439, 13, 509, 393, 915, 385, 294, 20657, 293, 5794, 13], "temperature": 0.0, "avg_logprob": -0.24334738205890266, "compression_ratio": 1.267605633802817, "no_speech_prob": 0.0008023249101825058}, {"id": 161, "seek": 121640, "start": 1228.16, "end": 1243.52, "text": " Okay, we have time for questions. If you absolutely need to leave and you can't wait until the", "tokens": [1033, 11, 321, 362, 565, 337, 1651, 13, 759, 291, 3122, 643, 281, 1856, 293, 291, 393, 380, 1699, 1826, 264], "temperature": 0.0, "avg_logprob": -0.24334738205890266, "compression_ratio": 1.267605633802817, "no_speech_prob": 0.0008023249101825058}, {"id": 162, "seek": 124352, "start": 1243.52, "end": 1258.8, "text": " talk is over, please do so as quietly as possible so we can understand the questions. Thanks.", "tokens": [751, 307, 670, 11, 1767, 360, 370, 382, 19141, 382, 1944, 370, 321, 393, 1223, 264, 1651, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.2500664344200721, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.001145119545981288}, {"id": 163, "seek": 124352, "start": 1258.8, "end": 1262.96, "text": " Hi. Thank you so much for your talk. It was really interesting. I'm wondering what kind", "tokens": [2421, 13, 1044, 291, 370, 709, 337, 428, 751, 13, 467, 390, 534, 1880, 13, 286, 478, 6359, 437, 733], "temperature": 0.0, "avg_logprob": -0.2500664344200721, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.001145119545981288}, {"id": 164, "seek": 124352, "start": 1262.96, "end": 1268.32, "text": " of limitations do you see when you're speaking about having a databases arriving in containers?", "tokens": [295, 15705, 360, 291, 536, 562, 291, 434, 4124, 466, 1419, 257, 22380, 22436, 294, 17089, 30], "temperature": 0.0, "avg_logprob": -0.2500664344200721, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.001145119545981288}, {"id": 165, "seek": 126832, "start": 1268.32, "end": 1276.08, "text": " There is storage limitations, CPU, or something else? Guys, can you please be a little quiet", "tokens": [821, 307, 6725, 15705, 11, 13199, 11, 420, 746, 1646, 30, 7855, 11, 393, 291, 1767, 312, 257, 707, 5677], "temperature": 0.0, "avg_logprob": -0.2927264957637577, "compression_ratio": 1.5550660792951543, "no_speech_prob": 0.003024398349225521}, {"id": 166, "seek": 126832, "start": 1276.08, "end": 1281.84, "text": " so we can understand the question? All right, I will try it with the microphone.", "tokens": [370, 321, 393, 1223, 264, 1168, 30, 1057, 558, 11, 286, 486, 853, 309, 365, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.2927264957637577, "compression_ratio": 1.5550660792951543, "no_speech_prob": 0.003024398349225521}, {"id": 167, "seek": 126832, "start": 1281.84, "end": 1287.12, "text": " Yeah, you. The people can you. Thank you. I was wondering maybe, first of all, really", "tokens": [865, 11, 291, 13, 440, 561, 393, 291, 13, 1044, 291, 13, 286, 390, 6359, 1310, 11, 700, 295, 439, 11, 534], "temperature": 0.0, "avg_logprob": -0.2927264957637577, "compression_ratio": 1.5550660792951543, "no_speech_prob": 0.003024398349225521}, {"id": 168, "seek": 126832, "start": 1287.12, "end": 1291.76, "text": " cool talk. Thank you so much. My question would be, could you maybe talk us through some kind", "tokens": [1627, 751, 13, 1044, 291, 370, 709, 13, 1222, 1168, 576, 312, 11, 727, 291, 1310, 751, 505, 807, 512, 733], "temperature": 0.0, "avg_logprob": -0.2927264957637577, "compression_ratio": 1.5550660792951543, "no_speech_prob": 0.003024398349225521}, {"id": 169, "seek": 129176, "start": 1291.76, "end": 1299.44, "text": " of limitations that you can see when you're running databases from containers? You didn't", "tokens": [295, 15705, 300, 291, 393, 536, 562, 291, 434, 2614, 22380, 490, 17089, 30, 509, 994, 380], "temperature": 0.0, "avg_logprob": -0.2408168287162321, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.003468624083325267}, {"id": 170, "seek": 129176, "start": 1299.44, "end": 1306.8799999999999, "text": " understand it? Thank you so much for the talk. It was really cool. Maybe you can share with", "tokens": [1223, 309, 30, 1044, 291, 370, 709, 337, 264, 751, 13, 467, 390, 534, 1627, 13, 2704, 291, 393, 2073, 365], "temperature": 0.0, "avg_logprob": -0.2408168287162321, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.003468624083325267}, {"id": 171, "seek": 129176, "start": 1306.8799999999999, "end": 1312.48, "text": " us some kind of limitations that you see when you're running to the solution of running databases", "tokens": [505, 512, 733, 295, 15705, 300, 291, 536, 562, 291, 434, 2614, 281, 264, 3827, 295, 2614, 22380], "temperature": 0.0, "avg_logprob": -0.2408168287162321, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.003468624083325267}, {"id": 172, "seek": 129176, "start": 1312.48, "end": 1317.12, "text": " inside containers, right? You cannot really run very big database. You probably will have", "tokens": [1854, 17089, 11, 558, 30, 509, 2644, 534, 1190, 588, 955, 8149, 13, 509, 1391, 486, 362], "temperature": 0.0, "avg_logprob": -0.2408168287162321, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.003468624083325267}, {"id": 173, "seek": 131712, "start": 1317.12, "end": 1323.1999999999998, "text": " a problem with that. What kind of limitations do you see?", "tokens": [257, 1154, 365, 300, 13, 708, 733, 295, 15705, 360, 291, 536, 30], "temperature": 0.0, "avg_logprob": -0.265045233333812, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.001488961512222886}, {"id": 174, "seek": 131712, "start": 1323.1999999999998, "end": 1330.7199999999998, "text": " So, yeah, the question is about sorry, the question is about what limitations you can", "tokens": [407, 11, 1338, 11, 264, 1168, 307, 466, 2597, 11, 264, 1168, 307, 466, 437, 15705, 291, 393], "temperature": 0.0, "avg_logprob": -0.265045233333812, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.001488961512222886}, {"id": 175, "seek": 131712, "start": 1330.7199999999998, "end": 1337.84, "text": " run into with database containers? Yeah, I don't want to say this, but it depends really of the", "tokens": [1190, 666, 365, 8149, 17089, 30, 865, 11, 286, 500, 380, 528, 281, 584, 341, 11, 457, 309, 5946, 534, 295, 264], "temperature": 0.0, "avg_logprob": -0.265045233333812, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.001488961512222886}, {"id": 176, "seek": 131712, "start": 1338.56, "end": 1344.0, "text": " business. Okay, if you want to invest a lot of money in infrastructure, but because at the end,", "tokens": [1606, 13, 1033, 11, 498, 291, 528, 281, 1963, 257, 688, 295, 1460, 294, 6896, 11, 457, 570, 412, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.265045233333812, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.001488961512222886}, {"id": 177, "seek": 134400, "start": 1344.0, "end": 1350.32, "text": " your database, the volume that you have is not going to be part of your container,", "tokens": [428, 8149, 11, 264, 5523, 300, 291, 362, 307, 406, 516, 281, 312, 644, 295, 428, 10129, 11], "temperature": 0.0, "avg_logprob": -0.15745549962140512, "compression_ratio": 1.514792899408284, "no_speech_prob": 0.0003374164807610214}, {"id": 178, "seek": 134400, "start": 1350.32, "end": 1355.2, "text": " it's going to be outside. And this depends on you. You want to invest a lot of money", "tokens": [309, 311, 516, 281, 312, 2380, 13, 400, 341, 5946, 322, 291, 13, 509, 528, 281, 1963, 257, 688, 295, 1460], "temperature": 0.0, "avg_logprob": -0.15745549962140512, "compression_ratio": 1.514792899408284, "no_speech_prob": 0.0003374164807610214}, {"id": 179, "seek": 134400, "start": 1355.2, "end": 1362.24, "text": " to save that data. It's good. You want to replicate it? Please try and be quiet while we", "tokens": [281, 3155, 300, 1412, 13, 467, 311, 665, 13, 509, 528, 281, 25356, 309, 30, 2555, 853, 293, 312, 5677, 1339, 321], "temperature": 0.0, "avg_logprob": -0.15745549962140512, "compression_ratio": 1.514792899408284, "no_speech_prob": 0.0003374164807610214}, {"id": 180, "seek": 136224, "start": 1362.24, "end": 1376.32, "text": " are asking questions. Are there any more questions?", "tokens": [366, 3365, 1651, 13, 2014, 456, 604, 544, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2618440219334194, "compression_ratio": 1.0851063829787233, "no_speech_prob": 0.00027736465563066304}, {"id": 181, "seek": 137632, "start": 1376.32, "end": 1393.6799999999998, "text": " There is one more question from the back, so please be quiet.", "tokens": [821, 307, 472, 544, 1168, 490, 264, 646, 11, 370, 1767, 312, 5677, 13], "temperature": 0.0, "avg_logprob": -0.1875588221427722, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0010550465667620301}, {"id": 182, "seek": 137632, "start": 1393.6799999999998, "end": 1402.8, "text": " Thank you. Hello. I wanted to ask, did you notice any kind of performance issues?", "tokens": [1044, 291, 13, 2425, 13, 286, 1415, 281, 1029, 11, 630, 291, 3449, 604, 733, 295, 3389, 2663, 30], "temperature": 0.0, "avg_logprob": -0.1875588221427722, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0010550465667620301}, {"id": 183, "seek": 140280, "start": 1402.8, "end": 1409.84, "text": " Did you benchmark things? Did you identify some kind of overheads going on when you", "tokens": [2589, 291, 18927, 721, 30, 2589, 291, 5876, 512, 733, 295, 19922, 82, 516, 322, 562, 291], "temperature": 0.0, "avg_logprob": -0.11000720659891765, "compression_ratio": 1.077922077922078, "no_speech_prob": 0.001228425418958068}, {"id": 184, "seek": 140984, "start": 1409.84, "end": 1437.1999999999998, "text": " containerize a database like MySQL or other kind of databases really? Sorry, I didn't get your", "tokens": [10129, 1125, 257, 8149, 411, 1222, 39934, 420, 661, 733, 295, 22380, 534, 30, 4919, 11, 286, 994, 380, 483, 428], "temperature": 0.0, "avg_logprob": -0.28939237594604494, "compression_ratio": 1.0804597701149425, "no_speech_prob": 0.0004062712541781366}, {"id": 185, "seek": 143720, "start": 1437.2, "end": 1443.3600000000001, "text": " question. All right, I'm just going to ask you. When you containerize a database,", "tokens": [1168, 13, 1057, 558, 11, 286, 478, 445, 516, 281, 1029, 291, 13, 1133, 291, 10129, 1125, 257, 8149, 11], "temperature": 0.0, "avg_logprob": -0.14108293511894313, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.00017646315973252058}, {"id": 186, "seek": 143720, "start": 1443.3600000000001, "end": 1449.76, "text": " be it MySQL or Postgres or any kind of open source database that you may have tested on this kind", "tokens": [312, 309, 1222, 39934, 420, 10223, 45189, 420, 604, 733, 295, 1269, 4009, 8149, 300, 291, 815, 362, 8246, 322, 341, 733], "temperature": 0.0, "avg_logprob": -0.14108293511894313, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.00017646315973252058}, {"id": 187, "seek": 143720, "start": 1449.76, "end": 1458.56, "text": " of setup, did you notice any kind of overheads, compute, memory, or disk, essentially, where", "tokens": [295, 8657, 11, 630, 291, 3449, 604, 733, 295, 19922, 82, 11, 14722, 11, 4675, 11, 420, 12355, 11, 4476, 11, 689], "temperature": 0.0, "avg_logprob": -0.14108293511894313, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.00017646315973252058}, {"id": 188, "seek": 143720, "start": 1458.56, "end": 1464.96, "text": " you can see that the database performance or operation is significantly affected by the fact", "tokens": [291, 393, 536, 300, 264, 8149, 3389, 420, 6916, 307, 10591, 8028, 538, 264, 1186], "temperature": 0.0, "avg_logprob": -0.14108293511894313, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.00017646315973252058}, {"id": 189, "seek": 146496, "start": 1464.96, "end": 1473.92, "text": " of being containerized? I'm not sure about that, but if you use open source to monitor", "tokens": [295, 885, 10129, 1602, 30, 286, 478, 406, 988, 466, 300, 11, 457, 498, 291, 764, 1269, 4009, 281, 6002], "temperature": 0.0, "avg_logprob": -0.1540515273809433, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0007285104366019368}, {"id": 190, "seek": 146496, "start": 1473.92, "end": 1480.8, "text": " your containers on databases, you can have a visualization of these things if you don't have", "tokens": [428, 17089, 322, 22380, 11, 291, 393, 362, 257, 25801, 295, 613, 721, 498, 291, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.1540515273809433, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0007285104366019368}, {"id": 191, "seek": 146496, "start": 1480.8, "end": 1487.1200000000001, "text": " enough resources so it can show you alerts or things like that where you can figure out where", "tokens": [1547, 3593, 370, 309, 393, 855, 291, 28061, 420, 721, 411, 300, 689, 291, 393, 2573, 484, 689], "temperature": 0.0, "avg_logprob": -0.1540515273809433, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0007285104366019368}, {"id": 192, "seek": 148712, "start": 1487.12, "end": 1495.52, "text": " exactly is your limitation. Okay, so for example, did you run Benchmark?", "tokens": [2293, 307, 428, 27432, 13, 1033, 11, 370, 337, 1365, 11, 630, 291, 1190, 3964, 339, 5638, 30], "temperature": 0.0, "avg_logprob": -0.25660448604159886, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.000955055991653353}, {"id": 193, "seek": 148712, "start": 1502.3999999999999, "end": 1506.9599999999998, "text": " Could you help me? Okay, could you help me to answer? Okay, my friend is going to help me to", "tokens": [7497, 291, 854, 385, 30, 1033, 11, 727, 291, 854, 385, 281, 1867, 30, 1033, 11, 452, 1277, 307, 516, 281, 854, 385, 281], "temperature": 0.0, "avg_logprob": -0.25660448604159886, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.000955055991653353}, {"id": 194, "seek": 148712, "start": 1506.9599999999998, "end": 1514.8799999999999, "text": " answer this. All right, thank you. Yeah, thank you to you. Hey, so usually the performance", "tokens": [1867, 341, 13, 1057, 558, 11, 1309, 291, 13, 865, 11, 1309, 291, 281, 291, 13, 1911, 11, 370, 2673, 264, 3389], "temperature": 0.0, "avg_logprob": -0.25660448604159886, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.000955055991653353}, {"id": 195, "seek": 151488, "start": 1514.88, "end": 1522.24, "text": " degradation is around two, three, four percent. The issue is more about how you configure the", "tokens": [40519, 307, 926, 732, 11, 1045, 11, 1451, 3043, 13, 440, 2734, 307, 544, 466, 577, 291, 22162, 264], "temperature": 0.0, "avg_logprob": -0.12456726020490619, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.0007783104083500803}, {"id": 196, "seek": 151488, "start": 1522.24, "end": 1529.2800000000002, "text": " database, kind of storage, if it's local or network storage, but the virtualization is", "tokens": [8149, 11, 733, 295, 6725, 11, 498, 309, 311, 2654, 420, 3209, 6725, 11, 457, 264, 6374, 2144, 307], "temperature": 0.0, "avg_logprob": -0.12456726020490619, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.0007783104083500803}, {"id": 197, "seek": 151488, "start": 1529.2800000000002, "end": 1538.0800000000002, "text": " minimal. It's like running on a EC2 instance. Okay, so there is an impact, miserable, at least you", "tokens": [13206, 13, 467, 311, 411, 2614, 322, 257, 19081, 17, 5197, 13, 1033, 11, 370, 456, 307, 364, 2712, 11, 22321, 11, 412, 1935, 291], "temperature": 0.0, "avg_logprob": -0.12456726020490619, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.0007783104083500803}, {"id": 198, "seek": 153808, "start": 1538.08, "end": 1545.28, "text": " say around four or five percent, but you say that's not going to be the, that there are configurations", "tokens": [584, 926, 1451, 420, 1732, 3043, 11, 457, 291, 584, 300, 311, 406, 516, 281, 312, 264, 11, 300, 456, 366, 31493], "temperature": 0.0, "avg_logprob": -0.08714903175056755, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0006503628101199865}, {"id": 199, "seek": 153808, "start": 1545.28, "end": 1549.84, "text": " we can do to try to avoid that. Do you have any kind of paper or any kind of resources that we might", "tokens": [321, 393, 360, 281, 853, 281, 5042, 300, 13, 1144, 291, 362, 604, 733, 295, 3035, 420, 604, 733, 295, 3593, 300, 321, 1062], "temperature": 0.0, "avg_logprob": -0.08714903175056755, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0006503628101199865}, {"id": 200, "seek": 153808, "start": 1549.84, "end": 1562.08, "text": " use to avoid those kind of bottlenecks? If I got correctly, not much. The measure that we do in", "tokens": [764, 281, 5042, 729, 733, 295, 44641, 2761, 30, 759, 286, 658, 8944, 11, 406, 709, 13, 440, 3481, 300, 321, 360, 294], "temperature": 0.0, "avg_logprob": -0.08714903175056755, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0006503628101199865}, {"id": 201, "seek": 156208, "start": 1562.08, "end": 1569.6799999999998, "text": " databases is measuring TPS. So you will notice on, if we're running Benchmarks, we've seen Bench,", "tokens": [22380, 307, 13389, 314, 6273, 13, 407, 291, 486, 3449, 322, 11, 498, 321, 434, 2614, 3964, 339, 37307, 11, 321, 600, 1612, 3964, 339, 11], "temperature": 0.0, "avg_logprob": -0.18863767232650366, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.00033100962173193693}, {"id": 202, "seek": 156208, "start": 1569.6799999999998, "end": 1577.12, "text": " for example, three percent, like if you are running 1,000 credits per second, you will get", "tokens": [337, 1365, 11, 1045, 3043, 11, 411, 498, 291, 366, 2614, 502, 11, 1360, 16816, 680, 1150, 11, 291, 486, 483], "temperature": 0.0, "avg_logprob": -0.18863767232650366, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.00033100962173193693}, {"id": 203, "seek": 156208, "start": 1577.76, "end": 1587.28, "text": " 980, 990 credits per second when containerized. Okay, and do you have any kind of recommendations,", "tokens": [1722, 4702, 11, 1722, 7771, 16816, 680, 1150, 562, 10129, 1602, 13, 1033, 11, 293, 360, 291, 362, 604, 733, 295, 10434, 11], "temperature": 0.0, "avg_logprob": -0.18863767232650366, "compression_ratio": 1.4870466321243523, "no_speech_prob": 0.00033100962173193693}, {"id": 204, "seek": 158728, "start": 1587.28, "end": 1592.0, "text": " kind of generic recommendations you can do so that when you run a database in a container,", "tokens": [733, 295, 19577, 10434, 291, 393, 360, 370, 300, 562, 291, 1190, 257, 8149, 294, 257, 10129, 11], "temperature": 0.0, "avg_logprob": -0.11465984150983285, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.00042676887824200094}, {"id": 205, "seek": 158728, "start": 1592.0, "end": 1596.48, "text": " here is what you can do to try and negate some of the performance bottleneck that you guys have", "tokens": [510, 307, 437, 291, 393, 360, 281, 853, 293, 2485, 473, 512, 295, 264, 3389, 44641, 547, 300, 291, 1074, 362], "temperature": 0.0, "avg_logprob": -0.11465984150983285, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.00042676887824200094}, {"id": 206, "seek": 158728, "start": 1596.48, "end": 1608.24, "text": " noticed? To be honest, on real-day activities, I would say 99 percent of the performance will come", "tokens": [5694, 30, 1407, 312, 3245, 11, 322, 957, 12, 810, 5354, 11, 286, 576, 584, 11803, 3043, 295, 264, 3389, 486, 808], "temperature": 0.0, "avg_logprob": -0.11465984150983285, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.00042676887824200094}, {"id": 207, "seek": 160824, "start": 1608.24, "end": 1617.52, "text": " from how you configure my SQL, not the containerization is like just a small piece of the game.", "tokens": [490, 577, 291, 22162, 452, 19200, 11, 406, 264, 10129, 2144, 307, 411, 445, 257, 1359, 2522, 295, 264, 1216, 13], "temperature": 0.0, "avg_logprob": -0.21160077149013304, "compression_ratio": 1.368421052631579, "no_speech_prob": 0.0006718302611261606}, {"id": 208, "seek": 160824, "start": 1619.76, "end": 1623.68, "text": " You can make more effect by modifying the database configuration.", "tokens": [509, 393, 652, 544, 1802, 538, 42626, 264, 8149, 11694, 13], "temperature": 0.0, "avg_logprob": -0.21160077149013304, "compression_ratio": 1.368421052631579, "no_speech_prob": 0.0006718302611261606}, {"id": 209, "seek": 160824, "start": 1624.48, "end": 1625.76, "text": " All right, thank you very much.", "tokens": [1057, 558, 11, 1309, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.21160077149013304, "compression_ratio": 1.368421052631579, "no_speech_prob": 0.0006718302611261606}, {"id": 210, "seek": 162576, "start": 1625.76, "end": 1639.04, "text": " Thanks to you.", "tokens": [50364, 2561, 281, 291, 13, 51028], "temperature": 0.0, "avg_logprob": -0.4886927604675293, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.0026097893714904785}], "language": "en"}