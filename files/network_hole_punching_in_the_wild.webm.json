{"text": " Hello, everyone. Thanks for joining today. Welcome to our talk on hole punching in the wild. Sometimes I would say we're going to talk about the biggest hack of the internet, which I would refer to as hole punching. We want to talk a bit about learnings from doing hole punching on larger networks. Some might remember me from last year in FOSDEM where I introduced our way of doing hole punching, and today we're coming here with a bunch of data. So, who are we? Dennis, do you want to introduce yourself? Yeah, okay. My name is Dennis. I'm working at ProCollab as a research engineer at a team called ProBlab, and I'm mainly focusing on network measurements and protocol optimizations that come out of these measurements, and yeah, I was working with Max on this hole punching campaign. Very cool. And Max, again, software engineer. Yeah, you can find us anywhere there online if you want. Yeah, happy to communicate online further after the talk, and we're also around at the venue. Wonderful. Okay, what are we doing today? I want to do a very quick intro to LiPi2P, a peer-to-peer networking library, but then dive right into the problem of why firewalls and NATs are rather hard for peer-to-peer networking. The solution, which in some cases is hole punching, then how LiPi2P does all that, and then we have been running a large measurement campaign on the internet in the wild, collecting data, how well hole punching works out there, and we're going to present those findings, and then kind of have takeaways of what we learned from there and where we're going from there. All right, LiPi2P, just a quick introduction. It's a peer-to-peer networking library. It's an open source project. There is one specification, and then there are many implementations of that specification, among other things, other languages in Go, JS, Rust, NIM, C++, Java, many, many out there. Cool. It provides, I would say, two levels. On the low level, it provides all kinds of different connectivity options. It takes care of the encryption and authentication here, being mutual authentication, and then things like hole punching, for example. Once you have these low level features of being able to connect to anyone out there in an encrypted and authenticated way, you can then build higher level protocols on top of that, which LiPi2P also provides like a DHT distributed hash table or gossiping protocols and things like that. My big statement always about LiPi2P is it's all you need to build your peer-to-peer application. All right, so to zoom out a little bit, that's LiPi2P. All the things that we're talking about today are implemented in LiPi2P, but that doesn't mean you can't implement it in any other networking library if you want to. Our great motivation for LiPi2P and in general for peer-to-peer networking is that we have full connectivity among all the nodes within the network to the best of their capabilities, obviously. In this talk, we're going to focus on the problem of NATs and firewalls for peer-to-peer networking. Now, before all of you yell, like, I'm not saying let's get rid of firewalls. At least let's not do that. They have a very important purpose, but in some cases we want to get around them. Okay, cool. Yeah, I'm here in the network dev room. I'm not going to explain what NATs and firewalls are, but we will go a little bit into what that means for whole-punching. In general, full-punching NATs and firewalls are big ones that we can have to get around. Okay, what is the problem in some fancy pictures? A wants to send a packet to B, whether that's a TCP syn or anything, right? And A and B are both behind their home routers. Just imagine two laptops in two different houses and they want to communicate directly with each other. So A sends a packet to B. It crosses A's router. A's router sets a five tuple in its routing table for that packet and the packet makes it to B. And obviously a very good thing is that B drops that packet because it's a packet that it has no clue where it's coming from, probably some wider internet and it might be an attack, so it's dropping it. It doesn't have any five tuple in its routing table, right? Okay, so that is the problem and we somehow want to make A and B communicate with each other. So the solution here, in some cases, it's whole-punching. Again, we want A and B to connect to each other. Instead of only having A send a packet to B, we have both of them send a packet at the same time. I'm talking in a little bit about what at the same time means, but that's just for now. Say we have some magic synchronization mechanism. So A sends a packet to B. B sends a packet to A. The packet from A punches a hole in its routing table, so adding a five tuple for it. The packet from B punches a hole in its routing table on its side. The packets cross somewhere in the internet. Obviously they don't, but it's a nice metaphor. And at some point packet B arrives at router A. Router A checks its routing table. A little bit simplified here. It lets packet B pass same on router B, and this way we actually exchange packets. Cool. So now the big problem is how does A and B know when to send those packets, right? It has to happen at the same time, at least for TCP. We might go a little bit into what that means for UDP, but at least for TCP, this needs to happen at the same time for TCP is simultaneous open to happen in the end. So how do we do that? This is lippie-to-pee specific. It doesn't need to be lippie-to-pee. You can use any signaling protocol on top. Let's say A and B want to connect, and they need to hole punch at the same time, right? They need to send those two packets from both sides at the same time, so one can go through the hole of the other to the other side. What do we do? We need some kind of coordination mechanism, so some kind of public server out there that is not behind a firewall and that. B connects to the relay. A learns B's address through the relay. A connects through the relay, so now the two A and B have a communication channel over the relay. B sends a message to A. You can just think of it as like a time synchronization protocol. And at the same time, while sending that message, it measures the time it takes for A to send a message back. So at this time, we know the round trip time. And then once we know the round trip time, B sends another message to A and waits exactly half the round trip time. And once A receives that sun down there, you can do the math. If now both of them start, so A when it receives the packet and B after half the round trip time, they actually do the hole punch. They exchange the packets. They cross somewhere in the internet. Both of them punch the hole into their routers and ta-da. We succeeded. We have a hole punch. We have a connection established. Cool. Okay. A little bit in terms of timeline on all of this. Hole punching is nothing new. It's definitely nothing that Lippity-P invented, not at all. The most obvious mention I know is an RFC 5128. But again, it predates that for sure. But I think it's a nice introduction to hole punching in general, in case you like reading ITF documents. Since then, we have been implementing it around 2021-22, basing on a lot of past knowledge around that. I've been presenting this work at FOSDEM 2022 last year remotely. And since then, we have rolled it out on a larger network, which is the IPFS network, in a two-phase way where all public nodes act as relay nodes, very limited relays. And then in a second phase, all the clients gained the hole punching capabilities. And now on this large peer-to-peer network, we actually have on non-hand the public nodes relaying for the signaling, and then the clients actually being able to do the hole punching work. Yeah. And so we have this deployed now in this large network, but it's very hard to know whether how it's working, especially across the internet, across all the networks, across all the different endpoints, across all the routing hardware, and so on. So that's why we launched the hole punching month, which is kind of like a measurement campaign, which Dennis now is going to introduce. Sorry. Can you hear me? Yes. All right. Thanks, Max. Yeah, as Max said, the LPDP folks conceived this new DCUTR protocol, and at some point, and then deployed it to the network. And now we want to know how well does it actually work. And for this, we launched, as Max said, a measurement campaign during December. I will get to this in a second. But how actually do we measure these hole punching success rates? And the challenge here is that we actually don't know the clients that are DCUTR capable. So where are the clients that we want to hole punch? Because they are behind nets. We cannot enumerate them. They don't register themselves in a central registry or so. So we conceived this three component architecture. And the crucial thing here probably is this honeypot component, which is just a DHT server node that interacts with, as Max said, the IPFS network. And it's a very stable node. And this means that it gets added to routing tables of different peers in the network. And this increases chances if peers behind nets interact with this IPFS network, come across this honeypot. So peers behind nets is in this diagram, the top right corner, some DCUTR capable peer. This one by chance by interacting with the network comes across the honeypot. And the honeypot then keeps track of those peers and writes it into a database. And then this database is interfaced by a server component that serves those identified and detected peers to a fleet of clients. And the hole punch measurement campaign consisted of a deployment of those clients to a wide variety of different laptops or users that agreed to run these kinds of clients. And this client then queries the server for a peer to hole punch. As Max said, it connects to the other peer through a relay node and then exchanges those couple of packages, tries to establish a direct connection. And then at the end, it reports back if it worked, if it didn't work, what went wrong, and so on. And so we can probe the whole network or like many, many clients and many network configurations. So we did this measurement campaign. We made some fuss about it during November internally, pro-collapse, and also reached out to the community. And starting from the beginning of December, we said, okay, please download these clients, run it on your machines, and let's try to gather as much data as possible during that time. And as you can see here, so we collected around 6.25 million hole punch results. So this is quite a lot of data from 154 clients that participated. And we punched around 47,000 unique peers in this network. And on the right hand side, you can see the deployment of our clients, of our controlled clients. So the color here is the number of contributed results. So the US was dominant here, but we have many other nodes deployed in Europe, but also Australia, New Zealand, and also South America, and also one client from the continent of Africa. And this actually, and these clients interacted with these other peers that are basically all around the world. So we could measure hole punch success rates all across the globe. And I think we have a very comprehensive data set here. And so these, so we now gathered the data. And at the beginning of December, sorry, of January, I started, so I said, okay, the hole punching month is over, and I started to analyze the data a little bit. And what we can see here on the X axis is the, so each bar is a unique client. And on the Y axis, we can see these different outcomes. So each hole punch result, as I said, can have, so the clients report back these results and each result can have a different outcome. These outcomes are at the top. So it can be successful. So we actually were able to establish a direct connection through hole punching, then connection reversed. This means, I'm trying to hole punch as I'm connecting to the other peer through the relay. And the first thing before we do the hole punching dance is for the peer to directly connect to us. Because if we are directly reachable, because we have a port mapping in place in the router, we don't actually need to do the hole punching exchange. This is the connection reversed. And as we can see here, it's a little hard to see. But some clients actually have a lot of these results. So this means they have a unique router configuration in place. Then failed is the obvious thing. So we tried, we exchanged these messages, but in the end, weren't able to establish a connection. No stream is some internal error that's unique to our setup. So probably nothing to worry about here. And no connection means we try to connect to the other peer through a relay, but the other peer was already gone. It's a permissionless peer-to-peer network. So it could be from the time that the honeypot detected the peer to the client trying to establish a connection to the peer that the client has already churned and left the network. But actually looking at these clients is distorted view on the data because we allowed everyone who participated in the campaign to move, to freely move around. So I was running this client in my laptop and I was moving from a coffee shop, a Wi-Fi network to a home network to a university network and so on. And hole punching is actually dependent on those network configurations instead of just me running the client. So the challenge here with the data analysis was, so I'm also not done with that yet and happy to open for the suggestions to detect these individual networks that the clients operated in. With each hole punch results, the client reported their listening IP addresses and so on. And I grouped them together to actually find out, to identify unique networks that those clients operated in. And at the end, I arrived at 342 unique client networks. And then the graph looks like this, probably not much different than before. But also there are some interesting unique network outcomes here that I will also get to in a bit. The most interesting graph is probably this one. So what's the success rate of this protocol? And on the x-axis, we have the success rate been by, yeah, just 5% binnings. And on the y-axis, the number of networks that had the success rate by probing the whole other network. And the majority of networks actually had a success rate of 70%. So I think this is already, actually, I think it's amazing because from not being able to connect at all to having a 70% chance to establish a direct connection without an intermediary, it's actually pretty great. But then also there are some networks that have very low success rate. And these are the ones that are probably the most interesting ones. Then also, oops, the IP and transport dependence is also quite interesting to like as an angle to look at the data. Here we can see that the top row, we used IPv4 and TCP to hole punch. So when these clients exchange these connect messages, they actually exchange the publicly listen, the publicly reachable IP addresses of those two peers that want to hole punch. And in our measurement campaign, we restricted this to actually only IPv4 and TCP and with some other hole punches only to IPv6 and quick, which is on the bottom right. And so we can take a look which combination is more successful than the other. And here we can see that IPv4 in TCP and quick is actually, if you average the numbers has a similar success rate. But on IPv6, we have actually, it's basically not working at all. And these unexpected things are actually the interesting ones for us. Either it's a measurement error, or there's some inherent property to the networking setup that prevents IPv6 from being hole punchable, basically. If we actually allow both transports, so in the first, in the previous graph, we showed we're only using TCP and quick. But if we allow both transports to simultaneously try to hole punch, we can see that we, with 81%, we end up with a quick connection. And this is just because quick connection establishment is way faster than TCP connection. So this is like an expected result here, just to verify some of the data here. And now two takeaways for us, for ProCo improvements. So if we took a private VPN, so if clients are running in VPNs, we can see that the success rate actually drops significantly from around 70% to less than 40%. And my hypothesis here is that the router, the router time that Max showed previously is measured between A and B. But what we actually need is the router time between the router A and router B. And if your router basically is the exit node, or your gateway that you're connected to from your VPN, this can differ by dozens of milliseconds, actually. And so the router time doesn't add up, and the hole synchronization is a little off. So this is potentially a protocol improvement here. And then, also interesting, so Max said they are exchanging these messages during the hole punch. But actually, we try this three times. So if it doesn't work the first time, we try it again. And if it doesn't work the second time, we try it yet again. But when we look at the data, if we end up with a successful hole punch connection, it was actually successful with the first attempt in 97% or 98% of the cases. So this is also something for the next steps for us. We should consider changing our strategy on the second and third try to increase the odds. So if we stick with the three retries, we shouldn't do the same thing over again, because as we saw from the data, it doesn't make a difference. So we should change our strategy here. And so one thing would be to reverse the client server roles in this quick hole punching exchange. This would be something, and also the other protocol improvement for us, as I said, would be to change the measurement of the round trip time. And for the future, the data analysis, right now, what I showed here is basically aggregates across all the data. And the interesting part is basically, so why is a specific client or a specific network, why has it less or a worse success rate than others? So these are like these individual things to look into to increase, maybe there's a common pattern that we can address with the protocol to increase the success rate. And yeah, then identify those causes. And also, at the end of all of this, we want to craft a follow up publication to something that maxed and some fellow friends, I would say, have it published just last year. And yeah, we want to make the data set public and so on and so forth for others to benefit from the data and can do their own analysis. Yeah, and with that, get involved, talk to us here at the venue about all of this. LipidFee is a great project. Have a look at all these links. Get in touch and contribute to join our community calls. And yeah, I think that's it. Thank you very much. At least what you implemented there, is it exactly ICE turnstaff or how different it is from this? So we differ in some cases, it's definitely very much motivated by ICE in turn. So a couple of things, we don't do turn itself, we have our own relay protocol, because nodes in the network act for the public as relay nodes. And the problem is you don't want to relay any traffic for anyone, but you want to make this really restricted in terms of how much traffic, how long. If you run a public node, you don't want to be the next relay node for everyone out there. And then, what we built here is very much TCP specific, but it also works well with UDP. We need the synchronization. And as far as I know, at least the WebRTC stack is very focused on UDP, where timing doesn't matter as much. So you saw the timing protocol, right? And that is very TCP specific, where we want a TCP simultaneous connect, which allows two sends to actually result in a single TCP connection. This is for your analysis. I guess a lot of this depends on the default configurations of the firewall. Did you kind of find out what are the Brian's type of firewalls or configurations that stops whole punching in your research? So, yeah. So, not in its entirety, but what we did is, so people that signed up for the measurement campaign gave us information about the networks. And so, if we find something fishy in the data, we could also reach out to them and ask what's the firewall setup in your specific network. We also gather data about port mappings that are in place. So, what LiPTP host tries to do is establish a port mapping inside your router. And this is also reported back. And what we also did is try to query the login page from these routers and get some information about what kind of firewall router actually was preventing you from connecting to someone else. So, these are the data points that we have to get some conclusions around this. But more than this, we don't have. But I think this is already pretty conclusive to a wide variety of analysis. What I was just wondering about is, do you have any data? How many clients actually were behind the net? So, all these clients that the Honeypot detected were only, so were clients that are behind the net. So, these are all LiPTP hosts. And with the default configuration of LiPTP hosts, if they only announce relay addresses, this means that they must be not publicly reachable, which is for us equivalent with being behind the net. So, yeah, it should be. There's probably some error there. So, then all of the IPv6 kind of hosts you were trying to connect to also were behind the net. Kind of IPv6. Yes, yes. And this is the interesting thing. So, I cannot explain this yet. Maybe it's a measurement like a measurement error from us. Maybe it's some, as I said, inherent property to something. Maybe it's a protocol error. I don't know. And this is the interesting stuff in these kinds of things. Thanks. I'm very curious. Yeah. I was wondering, does it also work with multiple nets? Can you open through two nets? So, if another friend of mine who I convinced to run these clients actually was running behind two nets and it was working. But I'm not sure how many people actually ran behind two nets. But in theory, yeah, maybe Max, you can explain this. Yes. So, right now, we don't have really a lot of data about two nets. And also, we don't have the data, which I think was called needle. I don't quite know where you're within the same network. But you don't know that you're next to each other. And you actually want a hole punch through your own net, even though you can't connect to each other. So, there's some challenges. Do we still have time for another question? So, you said that for UDP it should work. Similarly, did you do any experiments with that? Because in the past, we had a custom UDP hole punching thing and the routers were pretty branded. They forgot the mapping within 20 seconds or something. Yeah. So, we run this measurement campaign on TCP and QIC. And QIC in the end is just UDP. And what we do is something similar to STUN in the ICE suit, where we continuously try to keep our mapping up. And then on nets that do endpoint independent mappings, that actually helps. So, as long as we keep that up for, like, I don't know, every 10 seconds or so, then our mapping survives, even on UDP. Okay, cool. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.72, "text": " Hello, everyone. Thanks for joining today. Welcome to our talk on hole punching in the", "tokens": [2425, 11, 1518, 13, 2561, 337, 5549, 965, 13, 4027, 281, 527, 751, 322, 5458, 34866, 294, 264], "temperature": 0.0, "avg_logprob": -0.20977015862098106, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.40172991156578064}, {"id": 1, "seek": 0, "start": 11.72, "end": 18.76, "text": " wild. Sometimes I would say we're going to talk about the biggest hack of the internet,", "tokens": [4868, 13, 4803, 286, 576, 584, 321, 434, 516, 281, 751, 466, 264, 3880, 10339, 295, 264, 4705, 11], "temperature": 0.0, "avg_logprob": -0.20977015862098106, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.40172991156578064}, {"id": 2, "seek": 0, "start": 18.76, "end": 25.86, "text": " which I would refer to as hole punching. We want to talk a bit about learnings from doing", "tokens": [597, 286, 576, 2864, 281, 382, 5458, 34866, 13, 492, 528, 281, 751, 257, 857, 466, 2539, 82, 490, 884], "temperature": 0.0, "avg_logprob": -0.20977015862098106, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.40172991156578064}, {"id": 3, "seek": 2586, "start": 25.86, "end": 31.98, "text": " hole punching on larger networks. Some might remember me from last year in FOSDEM where", "tokens": [5458, 34866, 322, 4833, 9590, 13, 2188, 1062, 1604, 385, 490, 1036, 1064, 294, 479, 4367, 35, 6683, 689], "temperature": 0.0, "avg_logprob": -0.24066242324971707, "compression_ratio": 1.5708955223880596, "no_speech_prob": 8.01308051450178e-05}, {"id": 4, "seek": 2586, "start": 31.98, "end": 37.56, "text": " I introduced our way of doing hole punching, and today we're coming here with a bunch of", "tokens": [286, 7268, 527, 636, 295, 884, 5458, 34866, 11, 293, 965, 321, 434, 1348, 510, 365, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.24066242324971707, "compression_ratio": 1.5708955223880596, "no_speech_prob": 8.01308051450178e-05}, {"id": 5, "seek": 2586, "start": 37.56, "end": 44.92, "text": " data. So, who are we? Dennis, do you want to introduce yourself?", "tokens": [1412, 13, 407, 11, 567, 366, 321, 30, 23376, 11, 360, 291, 528, 281, 5366, 1803, 30], "temperature": 0.0, "avg_logprob": -0.24066242324971707, "compression_ratio": 1.5708955223880596, "no_speech_prob": 8.01308051450178e-05}, {"id": 6, "seek": 2586, "start": 44.92, "end": 49.760000000000005, "text": " Yeah, okay. My name is Dennis. I'm working at ProCollab as a research engineer at a team", "tokens": [865, 11, 1392, 13, 1222, 1315, 307, 23376, 13, 286, 478, 1364, 412, 1705, 35294, 455, 382, 257, 2132, 11403, 412, 257, 1469], "temperature": 0.0, "avg_logprob": -0.24066242324971707, "compression_ratio": 1.5708955223880596, "no_speech_prob": 8.01308051450178e-05}, {"id": 7, "seek": 2586, "start": 49.760000000000005, "end": 55.64, "text": " called ProBlab, and I'm mainly focusing on network measurements and protocol optimizations", "tokens": [1219, 1705, 33, 44990, 11, 293, 286, 478, 8704, 8416, 322, 3209, 15383, 293, 10336, 5028, 14455], "temperature": 0.0, "avg_logprob": -0.24066242324971707, "compression_ratio": 1.5708955223880596, "no_speech_prob": 8.01308051450178e-05}, {"id": 8, "seek": 5564, "start": 55.64, "end": 60.76, "text": " that come out of these measurements, and yeah, I was working with Max on this hole punching", "tokens": [300, 808, 484, 295, 613, 15383, 11, 293, 1338, 11, 286, 390, 1364, 365, 7402, 322, 341, 5458, 34866], "temperature": 0.0, "avg_logprob": -0.18773196725284352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 4.659231854020618e-05}, {"id": 9, "seek": 5564, "start": 60.76, "end": 70.04, "text": " campaign. Very cool. And Max, again, software engineer. Yeah, you can find us anywhere there", "tokens": [5129, 13, 4372, 1627, 13, 400, 7402, 11, 797, 11, 4722, 11403, 13, 865, 11, 291, 393, 915, 505, 4992, 456], "temperature": 0.0, "avg_logprob": -0.18773196725284352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 4.659231854020618e-05}, {"id": 10, "seek": 5564, "start": 70.04, "end": 76.24000000000001, "text": " online if you want. Yeah, happy to communicate online further after the talk, and we're also", "tokens": [2950, 498, 291, 528, 13, 865, 11, 2055, 281, 7890, 2950, 3052, 934, 264, 751, 11, 293, 321, 434, 611], "temperature": 0.0, "avg_logprob": -0.18773196725284352, "compression_ratio": 1.4656084656084656, "no_speech_prob": 4.659231854020618e-05}, {"id": 11, "seek": 7624, "start": 76.24, "end": 86.46, "text": " around at the venue. Wonderful. Okay, what are we doing today? I want to do a very quick", "tokens": [926, 412, 264, 21645, 13, 22768, 13, 1033, 11, 437, 366, 321, 884, 965, 30, 286, 528, 281, 360, 257, 588, 1702], "temperature": 0.0, "avg_logprob": -0.16229234422956193, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.00010092199227074161}, {"id": 12, "seek": 7624, "start": 86.46, "end": 93.08, "text": " intro to LiPi2P, a peer-to-peer networking library, but then dive right into the problem", "tokens": [12897, 281, 8349, 47, 72, 17, 47, 11, 257, 15108, 12, 1353, 12, 494, 260, 17985, 6405, 11, 457, 550, 9192, 558, 666, 264, 1154], "temperature": 0.0, "avg_logprob": -0.16229234422956193, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.00010092199227074161}, {"id": 13, "seek": 7624, "start": 93.08, "end": 100.75999999999999, "text": " of why firewalls and NATs are rather hard for peer-to-peer networking. The solution,", "tokens": [295, 983, 36109, 82, 293, 14500, 82, 366, 2831, 1152, 337, 15108, 12, 1353, 12, 494, 260, 17985, 13, 440, 3827, 11], "temperature": 0.0, "avg_logprob": -0.16229234422956193, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.00010092199227074161}, {"id": 14, "seek": 10076, "start": 100.76, "end": 108.0, "text": " which in some cases is hole punching, then how LiPi2P does all that, and then we have", "tokens": [597, 294, 512, 3331, 307, 5458, 34866, 11, 550, 577, 8349, 47, 72, 17, 47, 775, 439, 300, 11, 293, 550, 321, 362], "temperature": 0.0, "avg_logprob": -0.17852239830549374, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000195456319488585}, {"id": 15, "seek": 10076, "start": 108.0, "end": 113.68, "text": " been running a large measurement campaign on the internet in the wild, collecting data,", "tokens": [668, 2614, 257, 2416, 13160, 5129, 322, 264, 4705, 294, 264, 4868, 11, 12510, 1412, 11], "temperature": 0.0, "avg_logprob": -0.17852239830549374, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000195456319488585}, {"id": 16, "seek": 10076, "start": 113.68, "end": 119.4, "text": " how well hole punching works out there, and we're going to present those findings, and", "tokens": [577, 731, 5458, 34866, 1985, 484, 456, 11, 293, 321, 434, 516, 281, 1974, 729, 16483, 11, 293], "temperature": 0.0, "avg_logprob": -0.17852239830549374, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000195456319488585}, {"id": 17, "seek": 10076, "start": 119.4, "end": 126.68, "text": " then kind of have takeaways of what we learned from there and where we're going from there.", "tokens": [550, 733, 295, 362, 45584, 295, 437, 321, 3264, 490, 456, 293, 689, 321, 434, 516, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.17852239830549374, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.000195456319488585}, {"id": 18, "seek": 12668, "start": 126.68, "end": 133.64000000000001, "text": " All right, LiPi2P, just a quick introduction. It's a peer-to-peer networking library. It's", "tokens": [1057, 558, 11, 8349, 47, 72, 17, 47, 11, 445, 257, 1702, 9339, 13, 467, 311, 257, 15108, 12, 1353, 12, 494, 260, 17985, 6405, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.17731501505925104, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.0001766535424394533}, {"id": 19, "seek": 12668, "start": 133.64000000000001, "end": 140.20000000000002, "text": " an open source project. There is one specification, and then there are many implementations of", "tokens": [364, 1269, 4009, 1716, 13, 821, 307, 472, 31256, 11, 293, 550, 456, 366, 867, 4445, 763, 295], "temperature": 0.0, "avg_logprob": -0.17731501505925104, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.0001766535424394533}, {"id": 20, "seek": 12668, "start": 140.20000000000002, "end": 146.84, "text": " that specification, among other things, other languages in Go, JS, Rust, NIM, C++, Java,", "tokens": [300, 31256, 11, 3654, 661, 721, 11, 661, 8650, 294, 1037, 11, 33063, 11, 34952, 11, 426, 6324, 11, 383, 25472, 11, 10745, 11], "temperature": 0.0, "avg_logprob": -0.17731501505925104, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.0001766535424394533}, {"id": 21, "seek": 12668, "start": 146.84, "end": 154.88, "text": " many, many out there. Cool. It provides, I would say, two levels. On the low level, it", "tokens": [867, 11, 867, 484, 456, 13, 8561, 13, 467, 6417, 11, 286, 576, 584, 11, 732, 4358, 13, 1282, 264, 2295, 1496, 11, 309], "temperature": 0.0, "avg_logprob": -0.17731501505925104, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.0001766535424394533}, {"id": 22, "seek": 15488, "start": 154.88, "end": 160.24, "text": " provides all kinds of different connectivity options. It takes care of the encryption", "tokens": [6417, 439, 3685, 295, 819, 21095, 3956, 13, 467, 2516, 1127, 295, 264, 29575], "temperature": 0.0, "avg_logprob": -0.1569712571423463, "compression_ratio": 1.6840148698884758, "no_speech_prob": 8.927925227908418e-05}, {"id": 23, "seek": 15488, "start": 160.24, "end": 168.35999999999999, "text": " and authentication here, being mutual authentication, and then things like hole punching, for example.", "tokens": [293, 26643, 510, 11, 885, 16917, 26643, 11, 293, 550, 721, 411, 5458, 34866, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1569712571423463, "compression_ratio": 1.6840148698884758, "no_speech_prob": 8.927925227908418e-05}, {"id": 24, "seek": 15488, "start": 168.35999999999999, "end": 172.56, "text": " Once you have these low level features of being able to connect to anyone out there", "tokens": [3443, 291, 362, 613, 2295, 1496, 4122, 295, 885, 1075, 281, 1745, 281, 2878, 484, 456], "temperature": 0.0, "avg_logprob": -0.1569712571423463, "compression_ratio": 1.6840148698884758, "no_speech_prob": 8.927925227908418e-05}, {"id": 25, "seek": 15488, "start": 172.56, "end": 177.2, "text": " in an encrypted and authenticated way, you can then build higher level protocols on top", "tokens": [294, 364, 36663, 293, 9214, 3587, 636, 11, 291, 393, 550, 1322, 2946, 1496, 20618, 322, 1192], "temperature": 0.0, "avg_logprob": -0.1569712571423463, "compression_ratio": 1.6840148698884758, "no_speech_prob": 8.927925227908418e-05}, {"id": 26, "seek": 15488, "start": 177.2, "end": 183.24, "text": " of that, which LiPi2P also provides like a DHT distributed hash table or gossiping protocols", "tokens": [295, 300, 11, 597, 8349, 47, 72, 17, 47, 611, 6417, 411, 257, 28606, 51, 12631, 22019, 3199, 420, 31788, 278, 20618], "temperature": 0.0, "avg_logprob": -0.1569712571423463, "compression_ratio": 1.6840148698884758, "no_speech_prob": 8.927925227908418e-05}, {"id": 27, "seek": 18324, "start": 183.24, "end": 189.56, "text": " and things like that. My big statement always about LiPi2P is it's all you need to build", "tokens": [293, 721, 411, 300, 13, 1222, 955, 5629, 1009, 466, 8349, 47, 72, 17, 47, 307, 309, 311, 439, 291, 643, 281, 1322], "temperature": 0.0, "avg_logprob": -0.09214423684512868, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.618897269712761e-05}, {"id": 28, "seek": 18324, "start": 189.56, "end": 199.24, "text": " your peer-to-peer application. All right, so to zoom out a little bit, that's LiPi2P.", "tokens": [428, 15108, 12, 1353, 12, 494, 260, 3861, 13, 1057, 558, 11, 370, 281, 8863, 484, 257, 707, 857, 11, 300, 311, 8349, 47, 72, 17, 47, 13], "temperature": 0.0, "avg_logprob": -0.09214423684512868, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.618897269712761e-05}, {"id": 29, "seek": 18324, "start": 199.24, "end": 203.4, "text": " All the things that we're talking about today are implemented in LiPi2P, but that doesn't", "tokens": [1057, 264, 721, 300, 321, 434, 1417, 466, 965, 366, 12270, 294, 8349, 47, 72, 17, 47, 11, 457, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.09214423684512868, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.618897269712761e-05}, {"id": 30, "seek": 18324, "start": 203.4, "end": 210.4, "text": " mean you can't implement it in any other networking library if you want to. Our great motivation", "tokens": [914, 291, 393, 380, 4445, 309, 294, 604, 661, 17985, 6405, 498, 291, 528, 281, 13, 2621, 869, 12335], "temperature": 0.0, "avg_logprob": -0.09214423684512868, "compression_ratio": 1.6044444444444443, "no_speech_prob": 2.618897269712761e-05}, {"id": 31, "seek": 21040, "start": 210.4, "end": 216.12, "text": " for LiPi2P and in general for peer-to-peer networking is that we have full connectivity", "tokens": [337, 8349, 47, 72, 17, 47, 293, 294, 2674, 337, 15108, 12, 1353, 12, 494, 260, 17985, 307, 300, 321, 362, 1577, 21095], "temperature": 0.0, "avg_logprob": -0.118772740266761, "compression_ratio": 1.5669642857142858, "no_speech_prob": 6.763560668332502e-05}, {"id": 32, "seek": 21040, "start": 216.12, "end": 223.84, "text": " among all the nodes within the network to the best of their capabilities, obviously.", "tokens": [3654, 439, 264, 13891, 1951, 264, 3209, 281, 264, 1151, 295, 641, 10862, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.118772740266761, "compression_ratio": 1.5669642857142858, "no_speech_prob": 6.763560668332502e-05}, {"id": 33, "seek": 21040, "start": 223.84, "end": 229.0, "text": " In this talk, we're going to focus on the problem of NATs and firewalls for peer-to-peer", "tokens": [682, 341, 751, 11, 321, 434, 516, 281, 1879, 322, 264, 1154, 295, 14500, 82, 293, 36109, 82, 337, 15108, 12, 1353, 12, 494, 260], "temperature": 0.0, "avg_logprob": -0.118772740266761, "compression_ratio": 1.5669642857142858, "no_speech_prob": 6.763560668332502e-05}, {"id": 34, "seek": 21040, "start": 229.0, "end": 235.6, "text": " networking. Now, before all of you yell, like, I'm not saying let's get rid of firewalls.", "tokens": [17985, 13, 823, 11, 949, 439, 295, 291, 20525, 11, 411, 11, 286, 478, 406, 1566, 718, 311, 483, 3973, 295, 36109, 82, 13], "temperature": 0.0, "avg_logprob": -0.118772740266761, "compression_ratio": 1.5669642857142858, "no_speech_prob": 6.763560668332502e-05}, {"id": 35, "seek": 23560, "start": 235.6, "end": 240.84, "text": " At least let's not do that. They have a very important purpose, but in some cases we want", "tokens": [1711, 1935, 718, 311, 406, 360, 300, 13, 814, 362, 257, 588, 1021, 4334, 11, 457, 294, 512, 3331, 321, 528], "temperature": 0.0, "avg_logprob": -0.15187338236215953, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00018058775458484888}, {"id": 36, "seek": 23560, "start": 240.84, "end": 249.32, "text": " to get around them. Okay, cool. Yeah, I'm here in the network dev room. I'm not going", "tokens": [281, 483, 926, 552, 13, 1033, 11, 1627, 13, 865, 11, 286, 478, 510, 294, 264, 3209, 1905, 1808, 13, 286, 478, 406, 516], "temperature": 0.0, "avg_logprob": -0.15187338236215953, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00018058775458484888}, {"id": 37, "seek": 23560, "start": 249.32, "end": 258.96, "text": " to explain what NATs and firewalls are, but we will go a little bit into what that means", "tokens": [281, 2903, 437, 14500, 82, 293, 36109, 82, 366, 11, 457, 321, 486, 352, 257, 707, 857, 666, 437, 300, 1355], "temperature": 0.0, "avg_logprob": -0.15187338236215953, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00018058775458484888}, {"id": 38, "seek": 25896, "start": 258.96, "end": 265.32, "text": " for whole-punching. In general, full-punching NATs and firewalls are big ones that we can", "tokens": [337, 1379, 12, 79, 46079, 13, 682, 2674, 11, 1577, 12, 79, 46079, 14500, 82, 293, 36109, 82, 366, 955, 2306, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.18254676818847657, "compression_ratio": 1.398936170212766, "no_speech_prob": 4.652600546251051e-05}, {"id": 39, "seek": 25896, "start": 265.32, "end": 276.32, "text": " have to get around. Okay, what is the problem in some fancy pictures? A wants to send a", "tokens": [362, 281, 483, 926, 13, 1033, 11, 437, 307, 264, 1154, 294, 512, 10247, 5242, 30, 316, 2738, 281, 2845, 257], "temperature": 0.0, "avg_logprob": -0.18254676818847657, "compression_ratio": 1.398936170212766, "no_speech_prob": 4.652600546251051e-05}, {"id": 40, "seek": 25896, "start": 276.32, "end": 283.59999999999997, "text": " packet to B, whether that's a TCP syn or anything, right? And A and B are both behind", "tokens": [20300, 281, 363, 11, 1968, 300, 311, 257, 48965, 5451, 420, 1340, 11, 558, 30, 400, 316, 293, 363, 366, 1293, 2261], "temperature": 0.0, "avg_logprob": -0.18254676818847657, "compression_ratio": 1.398936170212766, "no_speech_prob": 4.652600546251051e-05}, {"id": 41, "seek": 28360, "start": 283.6, "end": 289.20000000000005, "text": " their home routers. Just imagine two laptops in two different houses and they want to communicate", "tokens": [641, 1280, 4020, 433, 13, 1449, 3811, 732, 27642, 294, 732, 819, 8078, 293, 436, 528, 281, 7890], "temperature": 0.0, "avg_logprob": -0.1400928497314453, "compression_ratio": 1.6894977168949772, "no_speech_prob": 7.80222617322579e-05}, {"id": 42, "seek": 28360, "start": 289.20000000000005, "end": 298.44, "text": " directly with each other. So A sends a packet to B. It crosses A's router. A's router sets", "tokens": [3838, 365, 1184, 661, 13, 407, 316, 14790, 257, 20300, 281, 363, 13, 467, 28467, 316, 311, 22492, 13, 316, 311, 22492, 6352], "temperature": 0.0, "avg_logprob": -0.1400928497314453, "compression_ratio": 1.6894977168949772, "no_speech_prob": 7.80222617322579e-05}, {"id": 43, "seek": 28360, "start": 298.44, "end": 305.88, "text": " a five tuple in its routing table for that packet and the packet makes it to B. And obviously", "tokens": [257, 1732, 2604, 781, 294, 1080, 32722, 3199, 337, 300, 20300, 293, 264, 20300, 1669, 309, 281, 363, 13, 400, 2745], "temperature": 0.0, "avg_logprob": -0.1400928497314453, "compression_ratio": 1.6894977168949772, "no_speech_prob": 7.80222617322579e-05}, {"id": 44, "seek": 28360, "start": 305.88, "end": 311.32000000000005, "text": " a very good thing is that B drops that packet because it's a packet that it has no clue", "tokens": [257, 588, 665, 551, 307, 300, 363, 11438, 300, 20300, 570, 309, 311, 257, 20300, 300, 309, 575, 572, 13602], "temperature": 0.0, "avg_logprob": -0.1400928497314453, "compression_ratio": 1.6894977168949772, "no_speech_prob": 7.80222617322579e-05}, {"id": 45, "seek": 31132, "start": 311.32, "end": 316.48, "text": " where it's coming from, probably some wider internet and it might be an attack, so it's", "tokens": [689, 309, 311, 1348, 490, 11, 1391, 512, 11842, 4705, 293, 309, 1062, 312, 364, 2690, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.13944751024246216, "compression_ratio": 1.5526315789473684, "no_speech_prob": 6.189999839989468e-05}, {"id": 46, "seek": 31132, "start": 316.48, "end": 324.12, "text": " dropping it. It doesn't have any five tuple in its routing table, right? Okay, so that", "tokens": [13601, 309, 13, 467, 1177, 380, 362, 604, 1732, 2604, 781, 294, 1080, 32722, 3199, 11, 558, 30, 1033, 11, 370, 300], "temperature": 0.0, "avg_logprob": -0.13944751024246216, "compression_ratio": 1.5526315789473684, "no_speech_prob": 6.189999839989468e-05}, {"id": 47, "seek": 31132, "start": 324.12, "end": 331.12, "text": " is the problem and we somehow want to make A and B communicate with each other. So the", "tokens": [307, 264, 1154, 293, 321, 6063, 528, 281, 652, 316, 293, 363, 7890, 365, 1184, 661, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.13944751024246216, "compression_ratio": 1.5526315789473684, "no_speech_prob": 6.189999839989468e-05}, {"id": 48, "seek": 31132, "start": 331.12, "end": 338.32, "text": " solution here, in some cases, it's whole-punching. Again, we want A and B to connect to each", "tokens": [3827, 510, 11, 294, 512, 3331, 11, 309, 311, 1379, 12, 79, 46079, 13, 3764, 11, 321, 528, 316, 293, 363, 281, 1745, 281, 1184], "temperature": 0.0, "avg_logprob": -0.13944751024246216, "compression_ratio": 1.5526315789473684, "no_speech_prob": 6.189999839989468e-05}, {"id": 49, "seek": 33832, "start": 338.32, "end": 345.84, "text": " other. Instead of only having A send a packet to B, we have both of them send a packet at", "tokens": [661, 13, 7156, 295, 787, 1419, 316, 2845, 257, 20300, 281, 363, 11, 321, 362, 1293, 295, 552, 2845, 257, 20300, 412], "temperature": 0.0, "avg_logprob": -0.09674887754479233, "compression_ratio": 1.7285714285714286, "no_speech_prob": 7.751132943667471e-05}, {"id": 50, "seek": 33832, "start": 345.84, "end": 351.12, "text": " the same time. I'm talking in a little bit about what at the same time means, but that's", "tokens": [264, 912, 565, 13, 286, 478, 1417, 294, 257, 707, 857, 466, 437, 412, 264, 912, 565, 1355, 11, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.09674887754479233, "compression_ratio": 1.7285714285714286, "no_speech_prob": 7.751132943667471e-05}, {"id": 51, "seek": 33832, "start": 351.12, "end": 357.96, "text": " just for now. Say we have some magic synchronization mechanism. So A sends a packet to B. B sends", "tokens": [445, 337, 586, 13, 6463, 321, 362, 512, 5585, 19331, 2144, 7513, 13, 407, 316, 14790, 257, 20300, 281, 363, 13, 363, 14790], "temperature": 0.0, "avg_logprob": -0.09674887754479233, "compression_ratio": 1.7285714285714286, "no_speech_prob": 7.751132943667471e-05}, {"id": 52, "seek": 33832, "start": 357.96, "end": 365.48, "text": " a packet to A. The packet from A punches a hole in its routing table, so adding a five", "tokens": [257, 20300, 281, 316, 13, 440, 20300, 490, 316, 34103, 257, 5458, 294, 1080, 32722, 3199, 11, 370, 5127, 257, 1732], "temperature": 0.0, "avg_logprob": -0.09674887754479233, "compression_ratio": 1.7285714285714286, "no_speech_prob": 7.751132943667471e-05}, {"id": 53, "seek": 36548, "start": 365.48, "end": 374.44, "text": " tuple for it. The packet from B punches a hole in its routing table on its side. The", "tokens": [2604, 781, 337, 309, 13, 440, 20300, 490, 363, 34103, 257, 5458, 294, 1080, 32722, 3199, 322, 1080, 1252, 13, 440], "temperature": 0.0, "avg_logprob": -0.16631010304326596, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0001354053383693099}, {"id": 54, "seek": 36548, "start": 374.44, "end": 382.36, "text": " packets cross somewhere in the internet. Obviously they don't, but it's a nice metaphor. And", "tokens": [30364, 3278, 4079, 294, 264, 4705, 13, 7580, 436, 500, 380, 11, 457, 309, 311, 257, 1481, 19157, 13, 400], "temperature": 0.0, "avg_logprob": -0.16631010304326596, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0001354053383693099}, {"id": 55, "seek": 36548, "start": 382.36, "end": 387.88, "text": " at some point packet B arrives at router A. Router A checks its routing table. A little", "tokens": [412, 512, 935, 20300, 363, 20116, 412, 22492, 316, 13, 497, 23985, 316, 13834, 1080, 32722, 3199, 13, 316, 707], "temperature": 0.0, "avg_logprob": -0.16631010304326596, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0001354053383693099}, {"id": 56, "seek": 38788, "start": 387.88, "end": 396.71999999999997, "text": " bit simplified here. It lets packet B pass same on router B, and this way we actually", "tokens": [857, 26335, 510, 13, 467, 6653, 20300, 363, 1320, 912, 322, 22492, 363, 11, 293, 341, 636, 321, 767], "temperature": 0.0, "avg_logprob": -0.1333307749788526, "compression_ratio": 1.4378378378378378, "no_speech_prob": 7.176677172537893e-05}, {"id": 57, "seek": 38788, "start": 396.71999999999997, "end": 409.24, "text": " exchange packets. Cool. So now the big problem is how does A and B know when to send those", "tokens": [7742, 30364, 13, 8561, 13, 407, 586, 264, 955, 1154, 307, 577, 775, 316, 293, 363, 458, 562, 281, 2845, 729], "temperature": 0.0, "avg_logprob": -0.1333307749788526, "compression_ratio": 1.4378378378378378, "no_speech_prob": 7.176677172537893e-05}, {"id": 58, "seek": 38788, "start": 409.24, "end": 414.48, "text": " packets, right? It has to happen at the same time, at least for TCP. We might go a little", "tokens": [30364, 11, 558, 30, 467, 575, 281, 1051, 412, 264, 912, 565, 11, 412, 1935, 337, 48965, 13, 492, 1062, 352, 257, 707], "temperature": 0.0, "avg_logprob": -0.1333307749788526, "compression_ratio": 1.4378378378378378, "no_speech_prob": 7.176677172537893e-05}, {"id": 59, "seek": 41448, "start": 414.48, "end": 419.28000000000003, "text": " bit into what that means for UDP, but at least for TCP, this needs to happen at the same", "tokens": [857, 666, 437, 300, 1355, 337, 624, 11373, 11, 457, 412, 1935, 337, 48965, 11, 341, 2203, 281, 1051, 412, 264, 912], "temperature": 0.0, "avg_logprob": -0.1549155731201172, "compression_ratio": 1.7233201581027668, "no_speech_prob": 6.98965959600173e-05}, {"id": 60, "seek": 41448, "start": 419.28000000000003, "end": 426.72, "text": " time for TCP is simultaneous open to happen in the end. So how do we do that? This is", "tokens": [565, 337, 48965, 307, 46218, 1269, 281, 1051, 294, 264, 917, 13, 407, 577, 360, 321, 360, 300, 30, 639, 307], "temperature": 0.0, "avg_logprob": -0.1549155731201172, "compression_ratio": 1.7233201581027668, "no_speech_prob": 6.98965959600173e-05}, {"id": 61, "seek": 41448, "start": 426.72, "end": 431.16, "text": " lippie-to-pee specific. It doesn't need to be lippie-to-pee. You can use any signaling", "tokens": [375, 427, 414, 12, 1353, 12, 39544, 2685, 13, 467, 1177, 380, 643, 281, 312, 375, 427, 414, 12, 1353, 12, 39544, 13, 509, 393, 764, 604, 38639], "temperature": 0.0, "avg_logprob": -0.1549155731201172, "compression_ratio": 1.7233201581027668, "no_speech_prob": 6.98965959600173e-05}, {"id": 62, "seek": 41448, "start": 431.16, "end": 437.8, "text": " protocol on top. Let's say A and B want to connect, and they need to hole punch at the", "tokens": [10336, 322, 1192, 13, 961, 311, 584, 316, 293, 363, 528, 281, 1745, 11, 293, 436, 643, 281, 5458, 8135, 412, 264], "temperature": 0.0, "avg_logprob": -0.1549155731201172, "compression_ratio": 1.7233201581027668, "no_speech_prob": 6.98965959600173e-05}, {"id": 63, "seek": 41448, "start": 437.8, "end": 442.28000000000003, "text": " same time, right? They need to send those two packets from both sides at the same time,", "tokens": [912, 565, 11, 558, 30, 814, 643, 281, 2845, 729, 732, 30364, 490, 1293, 4881, 412, 264, 912, 565, 11], "temperature": 0.0, "avg_logprob": -0.1549155731201172, "compression_ratio": 1.7233201581027668, "no_speech_prob": 6.98965959600173e-05}, {"id": 64, "seek": 44228, "start": 442.28, "end": 449.55999999999995, "text": " so one can go through the hole of the other to the other side. What do we do? We need", "tokens": [370, 472, 393, 352, 807, 264, 5458, 295, 264, 661, 281, 264, 661, 1252, 13, 708, 360, 321, 360, 30, 492, 643], "temperature": 0.0, "avg_logprob": -0.11391161776137078, "compression_ratio": 1.736318407960199, "no_speech_prob": 7.008879765635356e-05}, {"id": 65, "seek": 44228, "start": 449.55999999999995, "end": 453.84, "text": " some kind of coordination mechanism, so some kind of public server out there that is not", "tokens": [512, 733, 295, 21252, 7513, 11, 370, 512, 733, 295, 1908, 7154, 484, 456, 300, 307, 406], "temperature": 0.0, "avg_logprob": -0.11391161776137078, "compression_ratio": 1.736318407960199, "no_speech_prob": 7.008879765635356e-05}, {"id": 66, "seek": 44228, "start": 453.84, "end": 463.47999999999996, "text": " behind a firewall and that. B connects to the relay. A learns B's address through the", "tokens": [2261, 257, 36109, 293, 300, 13, 363, 16967, 281, 264, 24214, 13, 316, 27152, 363, 311, 2985, 807, 264], "temperature": 0.0, "avg_logprob": -0.11391161776137078, "compression_ratio": 1.736318407960199, "no_speech_prob": 7.008879765635356e-05}, {"id": 67, "seek": 44228, "start": 463.47999999999996, "end": 469.35999999999996, "text": " relay. A connects through the relay, so now the two A and B have a communication channel", "tokens": [24214, 13, 316, 16967, 807, 264, 24214, 11, 370, 586, 264, 732, 316, 293, 363, 362, 257, 6101, 2269], "temperature": 0.0, "avg_logprob": -0.11391161776137078, "compression_ratio": 1.736318407960199, "no_speech_prob": 7.008879765635356e-05}, {"id": 68, "seek": 46936, "start": 469.36, "end": 479.24, "text": " over the relay. B sends a message to A. You can just think of it as like a time synchronization", "tokens": [670, 264, 24214, 13, 363, 14790, 257, 3636, 281, 316, 13, 509, 393, 445, 519, 295, 309, 382, 411, 257, 565, 19331, 2144], "temperature": 0.0, "avg_logprob": -0.12893165954171795, "compression_ratio": 1.5625, "no_speech_prob": 0.00012818806862924248}, {"id": 69, "seek": 46936, "start": 479.24, "end": 489.48, "text": " protocol. And at the same time, while sending that message, it measures the time it takes", "tokens": [10336, 13, 400, 412, 264, 912, 565, 11, 1339, 7750, 300, 3636, 11, 309, 8000, 264, 565, 309, 2516], "temperature": 0.0, "avg_logprob": -0.12893165954171795, "compression_ratio": 1.5625, "no_speech_prob": 0.00012818806862924248}, {"id": 70, "seek": 46936, "start": 489.48, "end": 495.72, "text": " for A to send a message back. So at this time, we know the round trip time. And then once", "tokens": [337, 316, 281, 2845, 257, 3636, 646, 13, 407, 412, 341, 565, 11, 321, 458, 264, 3098, 4931, 565, 13, 400, 550, 1564], "temperature": 0.0, "avg_logprob": -0.12893165954171795, "compression_ratio": 1.5625, "no_speech_prob": 0.00012818806862924248}, {"id": 71, "seek": 49572, "start": 495.72, "end": 504.12, "text": " we know the round trip time, B sends another message to A and waits exactly half the round", "tokens": [321, 458, 264, 3098, 4931, 565, 11, 363, 14790, 1071, 3636, 281, 316, 293, 40597, 2293, 1922, 264, 3098], "temperature": 0.0, "avg_logprob": -0.11774251080941463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00011609849752858281}, {"id": 72, "seek": 49572, "start": 504.12, "end": 512.36, "text": " trip time. And once A receives that sun down there, you can do the math. If now both of", "tokens": [4931, 565, 13, 400, 1564, 316, 20717, 300, 3295, 760, 456, 11, 291, 393, 360, 264, 5221, 13, 759, 586, 1293, 295], "temperature": 0.0, "avg_logprob": -0.11774251080941463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00011609849752858281}, {"id": 73, "seek": 49572, "start": 512.36, "end": 518.36, "text": " them start, so A when it receives the packet and B after half the round trip time, they", "tokens": [552, 722, 11, 370, 316, 562, 309, 20717, 264, 20300, 293, 363, 934, 1922, 264, 3098, 4931, 565, 11, 436], "temperature": 0.0, "avg_logprob": -0.11774251080941463, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00011609849752858281}, {"id": 74, "seek": 51836, "start": 518.36, "end": 526.0, "text": " actually do the hole punch. They exchange the packets. They cross somewhere in the internet.", "tokens": [767, 360, 264, 5458, 8135, 13, 814, 7742, 264, 30364, 13, 814, 3278, 4079, 294, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.24328184127807617, "compression_ratio": 1.6217391304347826, "no_speech_prob": 8.798785711405799e-05}, {"id": 75, "seek": 51836, "start": 526.0, "end": 533.04, "text": " Both of them punch the hole into their routers and ta-da. We succeeded. We have a hole punch.", "tokens": [6767, 295, 552, 8135, 264, 5458, 666, 641, 4020, 433, 293, 1846, 12, 2675, 13, 492, 20263, 13, 492, 362, 257, 5458, 8135, 13], "temperature": 0.0, "avg_logprob": -0.24328184127807617, "compression_ratio": 1.6217391304347826, "no_speech_prob": 8.798785711405799e-05}, {"id": 76, "seek": 51836, "start": 533.04, "end": 541.96, "text": " We have a connection established. Cool. Okay. A little bit in terms of timeline on all of", "tokens": [492, 362, 257, 4984, 7545, 13, 8561, 13, 1033, 13, 316, 707, 857, 294, 2115, 295, 12933, 322, 439, 295], "temperature": 0.0, "avg_logprob": -0.24328184127807617, "compression_ratio": 1.6217391304347826, "no_speech_prob": 8.798785711405799e-05}, {"id": 77, "seek": 51836, "start": 541.96, "end": 548.32, "text": " this. Hole punching is nothing new. It's definitely nothing that Lippity-P invented, not at all.", "tokens": [341, 13, 47635, 34866, 307, 1825, 777, 13, 467, 311, 2138, 1825, 300, 441, 2488, 507, 12, 47, 14479, 11, 406, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.24328184127807617, "compression_ratio": 1.6217391304347826, "no_speech_prob": 8.798785711405799e-05}, {"id": 78, "seek": 54832, "start": 548.32, "end": 560.2800000000001, "text": " The most obvious mention I know is an RFC 5128. But again, it predates that for sure. But I", "tokens": [440, 881, 6322, 2152, 286, 458, 307, 364, 497, 18671, 1025, 4762, 23, 13, 583, 797, 11, 309, 3852, 1024, 300, 337, 988, 13, 583, 286], "temperature": 0.0, "avg_logprob": -0.1474343224575645, "compression_ratio": 1.3689320388349515, "no_speech_prob": 0.00017573409422766417}, {"id": 79, "seek": 54832, "start": 560.2800000000001, "end": 567.24, "text": " think it's a nice introduction to hole punching in general, in case you like reading ITF documents.", "tokens": [519, 309, 311, 257, 1481, 9339, 281, 5458, 34866, 294, 2674, 11, 294, 1389, 291, 411, 3760, 6783, 37, 8512, 13], "temperature": 0.0, "avg_logprob": -0.1474343224575645, "compression_ratio": 1.3689320388349515, "no_speech_prob": 0.00017573409422766417}, {"id": 80, "seek": 54832, "start": 567.24, "end": 573.7600000000001, "text": " Since then, we have been implementing it around 2021-22, basing on a lot of past knowledge", "tokens": [4162, 550, 11, 321, 362, 668, 18114, 309, 926, 7201, 12, 7490, 11, 987, 278, 322, 257, 688, 295, 1791, 3601], "temperature": 0.0, "avg_logprob": -0.1474343224575645, "compression_ratio": 1.3689320388349515, "no_speech_prob": 0.00017573409422766417}, {"id": 81, "seek": 57376, "start": 573.76, "end": 583.64, "text": " around that. I've been presenting this work at FOSDEM 2022 last year remotely. And since", "tokens": [926, 300, 13, 286, 600, 668, 15578, 341, 589, 412, 479, 4367, 35, 6683, 20229, 1036, 1064, 20824, 13, 400, 1670], "temperature": 0.0, "avg_logprob": -0.1543097300072239, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.00020928226877003908}, {"id": 82, "seek": 57376, "start": 583.64, "end": 590.96, "text": " then, we have rolled it out on a larger network, which is the IPFS network, in a two-phase way", "tokens": [550, 11, 321, 362, 14306, 309, 484, 322, 257, 4833, 3209, 11, 597, 307, 264, 8671, 29318, 3209, 11, 294, 257, 732, 12, 43331, 636], "temperature": 0.0, "avg_logprob": -0.1543097300072239, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.00020928226877003908}, {"id": 83, "seek": 57376, "start": 590.96, "end": 597.52, "text": " where all public nodes act as relay nodes, very limited relays. And then in a second", "tokens": [689, 439, 1908, 13891, 605, 382, 24214, 13891, 11, 588, 5567, 1039, 3772, 13, 400, 550, 294, 257, 1150], "temperature": 0.0, "avg_logprob": -0.1543097300072239, "compression_ratio": 1.3958333333333333, "no_speech_prob": 0.00020928226877003908}, {"id": 84, "seek": 59752, "start": 597.52, "end": 603.76, "text": " phase, all the clients gained the hole punching capabilities. And now on this large peer-to-peer", "tokens": [5574, 11, 439, 264, 6982, 12634, 264, 5458, 34866, 10862, 13, 400, 586, 322, 341, 2416, 15108, 12, 1353, 12, 494, 260], "temperature": 0.0, "avg_logprob": -0.15364318981505276, "compression_ratio": 1.924, "no_speech_prob": 0.00016909967234823853}, {"id": 85, "seek": 59752, "start": 603.76, "end": 610.64, "text": " network, we actually have on non-hand the public nodes relaying for the signaling, and then the", "tokens": [3209, 11, 321, 767, 362, 322, 2107, 12, 5543, 264, 1908, 13891, 24214, 278, 337, 264, 38639, 11, 293, 550, 264], "temperature": 0.0, "avg_logprob": -0.15364318981505276, "compression_ratio": 1.924, "no_speech_prob": 0.00016909967234823853}, {"id": 86, "seek": 59752, "start": 610.64, "end": 616.6, "text": " clients actually being able to do the hole punching work. Yeah. And so we have this deployed now in", "tokens": [6982, 767, 885, 1075, 281, 360, 264, 5458, 34866, 589, 13, 865, 13, 400, 370, 321, 362, 341, 17826, 586, 294], "temperature": 0.0, "avg_logprob": -0.15364318981505276, "compression_ratio": 1.924, "no_speech_prob": 0.00016909967234823853}, {"id": 87, "seek": 59752, "start": 616.6, "end": 621.56, "text": " this large network, but it's very hard to know whether how it's working, especially across the", "tokens": [341, 2416, 3209, 11, 457, 309, 311, 588, 1152, 281, 458, 1968, 577, 309, 311, 1364, 11, 2318, 2108, 264], "temperature": 0.0, "avg_logprob": -0.15364318981505276, "compression_ratio": 1.924, "no_speech_prob": 0.00016909967234823853}, {"id": 88, "seek": 59752, "start": 621.56, "end": 627.0, "text": " internet, across all the networks, across all the different endpoints, across all the routing", "tokens": [4705, 11, 2108, 439, 264, 9590, 11, 2108, 439, 264, 819, 917, 20552, 11, 2108, 439, 264, 32722], "temperature": 0.0, "avg_logprob": -0.15364318981505276, "compression_ratio": 1.924, "no_speech_prob": 0.00016909967234823853}, {"id": 89, "seek": 62700, "start": 627.0, "end": 632.28, "text": " hardware, and so on. So that's why we launched the hole punching month, which is kind of like a", "tokens": [8837, 11, 293, 370, 322, 13, 407, 300, 311, 983, 321, 8730, 264, 5458, 34866, 1618, 11, 597, 307, 733, 295, 411, 257], "temperature": 0.0, "avg_logprob": -0.16270861579376517, "compression_ratio": 1.5098039215686274, "no_speech_prob": 6.0818470956292003e-05}, {"id": 90, "seek": 62700, "start": 632.28, "end": 640.48, "text": " measurement campaign, which Dennis now is going to introduce. Sorry. Can you hear me? Yes. All", "tokens": [13160, 5129, 11, 597, 23376, 586, 307, 516, 281, 5366, 13, 4919, 13, 1664, 291, 1568, 385, 30, 1079, 13, 1057], "temperature": 0.0, "avg_logprob": -0.16270861579376517, "compression_ratio": 1.5098039215686274, "no_speech_prob": 6.0818470956292003e-05}, {"id": 91, "seek": 62700, "start": 640.48, "end": 648.24, "text": " right. Thanks, Max. Yeah, as Max said, the LPDP folks conceived this new DCUTR protocol, and at", "tokens": [558, 13, 2561, 11, 7402, 13, 865, 11, 382, 7402, 848, 11, 264, 441, 17349, 47, 4024, 34898, 341, 777, 9114, 8709, 49, 10336, 11, 293, 412], "temperature": 0.0, "avg_logprob": -0.16270861579376517, "compression_ratio": 1.5098039215686274, "no_speech_prob": 6.0818470956292003e-05}, {"id": 92, "seek": 62700, "start": 648.24, "end": 652.44, "text": " some point, and then deployed it to the network. And now we want to know how well does it actually", "tokens": [512, 935, 11, 293, 550, 17826, 309, 281, 264, 3209, 13, 400, 586, 321, 528, 281, 458, 577, 731, 775, 309, 767], "temperature": 0.0, "avg_logprob": -0.16270861579376517, "compression_ratio": 1.5098039215686274, "no_speech_prob": 6.0818470956292003e-05}, {"id": 93, "seek": 65244, "start": 652.44, "end": 657.9200000000001, "text": " work. And for this, we launched, as Max said, a measurement campaign during December. I will get", "tokens": [589, 13, 400, 337, 341, 11, 321, 8730, 11, 382, 7402, 848, 11, 257, 13160, 5129, 1830, 7687, 13, 286, 486, 483], "temperature": 0.0, "avg_logprob": -0.105014701684316, "compression_ratio": 1.6192468619246863, "no_speech_prob": 2.143448182323482e-05}, {"id": 94, "seek": 65244, "start": 657.9200000000001, "end": 664.08, "text": " to this in a second. But how actually do we measure these hole punching success rates? And the", "tokens": [281, 341, 294, 257, 1150, 13, 583, 577, 767, 360, 321, 3481, 613, 5458, 34866, 2245, 6846, 30, 400, 264], "temperature": 0.0, "avg_logprob": -0.105014701684316, "compression_ratio": 1.6192468619246863, "no_speech_prob": 2.143448182323482e-05}, {"id": 95, "seek": 65244, "start": 664.08, "end": 672.6800000000001, "text": " challenge here is that we actually don't know the clients that are DCUTR capable. So where are the", "tokens": [3430, 510, 307, 300, 321, 767, 500, 380, 458, 264, 6982, 300, 366, 9114, 8709, 49, 8189, 13, 407, 689, 366, 264], "temperature": 0.0, "avg_logprob": -0.105014701684316, "compression_ratio": 1.6192468619246863, "no_speech_prob": 2.143448182323482e-05}, {"id": 96, "seek": 65244, "start": 672.6800000000001, "end": 676.8800000000001, "text": " clients that we want to hole punch? Because they are behind nets. We cannot enumerate them. They", "tokens": [6982, 300, 321, 528, 281, 5458, 8135, 30, 1436, 436, 366, 2261, 36170, 13, 492, 2644, 465, 15583, 473, 552, 13, 814], "temperature": 0.0, "avg_logprob": -0.105014701684316, "compression_ratio": 1.6192468619246863, "no_speech_prob": 2.143448182323482e-05}, {"id": 97, "seek": 67688, "start": 676.88, "end": 684.52, "text": " don't register themselves in a central registry or so. So we conceived this three component", "tokens": [500, 380, 7280, 2969, 294, 257, 5777, 36468, 420, 370, 13, 407, 321, 34898, 341, 1045, 6542], "temperature": 0.0, "avg_logprob": -0.1267258644104004, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.4061763067729771e-05}, {"id": 98, "seek": 67688, "start": 684.52, "end": 689.64, "text": " architecture. And the crucial thing here probably is this honeypot component, which is just a DHT", "tokens": [9482, 13, 400, 264, 11462, 551, 510, 1391, 307, 341, 8330, 17698, 6542, 11, 597, 307, 445, 257, 28606, 51], "temperature": 0.0, "avg_logprob": -0.1267258644104004, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.4061763067729771e-05}, {"id": 99, "seek": 67688, "start": 689.64, "end": 696.56, "text": " server node that interacts with, as Max said, the IPFS network. And it's a very stable node. And this", "tokens": [7154, 9984, 300, 43582, 365, 11, 382, 7402, 848, 11, 264, 8671, 29318, 3209, 13, 400, 309, 311, 257, 588, 8351, 9984, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.1267258644104004, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.4061763067729771e-05}, {"id": 100, "seek": 67688, "start": 696.56, "end": 701.96, "text": " means that it gets added to routing tables of different peers in the network. And this increases", "tokens": [1355, 300, 309, 2170, 3869, 281, 32722, 8020, 295, 819, 16739, 294, 264, 3209, 13, 400, 341, 8637], "temperature": 0.0, "avg_logprob": -0.1267258644104004, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.4061763067729771e-05}, {"id": 101, "seek": 70196, "start": 701.96, "end": 709.64, "text": " chances if peers behind nets interact with this IPFS network, come across this honeypot. So peers", "tokens": [10486, 498, 16739, 2261, 36170, 4648, 365, 341, 8671, 29318, 3209, 11, 808, 2108, 341, 8330, 17698, 13, 407, 16739], "temperature": 0.0, "avg_logprob": -0.12654561262864333, "compression_ratio": 1.7813953488372094, "no_speech_prob": 2.247059455839917e-05}, {"id": 102, "seek": 70196, "start": 709.64, "end": 715.32, "text": " behind nets is in this diagram, the top right corner, some DCUTR capable peer. This one by", "tokens": [2261, 36170, 307, 294, 341, 10686, 11, 264, 1192, 558, 4538, 11, 512, 9114, 8709, 49, 8189, 15108, 13, 639, 472, 538], "temperature": 0.0, "avg_logprob": -0.12654561262864333, "compression_ratio": 1.7813953488372094, "no_speech_prob": 2.247059455839917e-05}, {"id": 103, "seek": 70196, "start": 715.32, "end": 720.76, "text": " chance by interacting with the network comes across the honeypot. And the honeypot then keeps", "tokens": [2931, 538, 18017, 365, 264, 3209, 1487, 2108, 264, 8330, 17698, 13, 400, 264, 8330, 17698, 550, 5965], "temperature": 0.0, "avg_logprob": -0.12654561262864333, "compression_ratio": 1.7813953488372094, "no_speech_prob": 2.247059455839917e-05}, {"id": 104, "seek": 70196, "start": 720.76, "end": 726.4000000000001, "text": " track of those peers and writes it into a database. And then this database is interfaced by a server", "tokens": [2837, 295, 729, 16739, 293, 13657, 309, 666, 257, 8149, 13, 400, 550, 341, 8149, 307, 14510, 3839, 538, 257, 7154], "temperature": 0.0, "avg_logprob": -0.12654561262864333, "compression_ratio": 1.7813953488372094, "no_speech_prob": 2.247059455839917e-05}, {"id": 105, "seek": 72640, "start": 726.4, "end": 734.56, "text": " component that serves those identified and detected peers to a fleet of clients. And the hole punch", "tokens": [6542, 300, 13451, 729, 9234, 293, 21896, 16739, 281, 257, 19396, 295, 6982, 13, 400, 264, 5458, 8135], "temperature": 0.0, "avg_logprob": -0.11285922106574564, "compression_ratio": 1.7312775330396475, "no_speech_prob": 9.659702300268691e-06}, {"id": 106, "seek": 72640, "start": 734.56, "end": 741.72, "text": " measurement campaign consisted of a deployment of those clients to a wide variety of different", "tokens": [13160, 5129, 38227, 295, 257, 19317, 295, 729, 6982, 281, 257, 4874, 5673, 295, 819], "temperature": 0.0, "avg_logprob": -0.11285922106574564, "compression_ratio": 1.7312775330396475, "no_speech_prob": 9.659702300268691e-06}, {"id": 107, "seek": 72640, "start": 741.72, "end": 750.4399999999999, "text": " laptops or users that agreed to run these kinds of clients. And this client then queries the server", "tokens": [27642, 420, 5022, 300, 9166, 281, 1190, 613, 3685, 295, 6982, 13, 400, 341, 6423, 550, 24109, 264, 7154], "temperature": 0.0, "avg_logprob": -0.11285922106574564, "compression_ratio": 1.7312775330396475, "no_speech_prob": 9.659702300268691e-06}, {"id": 108, "seek": 72640, "start": 750.4399999999999, "end": 756.24, "text": " for a peer to hole punch. As Max said, it connects to the other peer through a relay node and then", "tokens": [337, 257, 15108, 281, 5458, 8135, 13, 1018, 7402, 848, 11, 309, 16967, 281, 264, 661, 15108, 807, 257, 24214, 9984, 293, 550], "temperature": 0.0, "avg_logprob": -0.11285922106574564, "compression_ratio": 1.7312775330396475, "no_speech_prob": 9.659702300268691e-06}, {"id": 109, "seek": 75624, "start": 756.24, "end": 761.76, "text": " exchanges those couple of packages, tries to establish a direct connection. And then at the", "tokens": [27374, 729, 1916, 295, 17401, 11, 9898, 281, 8327, 257, 2047, 4984, 13, 400, 550, 412, 264], "temperature": 0.0, "avg_logprob": -0.14716907214092953, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.045347537205089e-05}, {"id": 110, "seek": 75624, "start": 761.76, "end": 767.6800000000001, "text": " end, it reports back if it worked, if it didn't work, what went wrong, and so on. And so we can probe", "tokens": [917, 11, 309, 7122, 646, 498, 309, 2732, 11, 498, 309, 994, 380, 589, 11, 437, 1437, 2085, 11, 293, 370, 322, 13, 400, 370, 321, 393, 22715], "temperature": 0.0, "avg_logprob": -0.14716907214092953, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.045347537205089e-05}, {"id": 111, "seek": 75624, "start": 767.6800000000001, "end": 776.2, "text": " the whole network or like many, many clients and many network configurations. So we did this", "tokens": [264, 1379, 3209, 420, 411, 867, 11, 867, 6982, 293, 867, 3209, 31493, 13, 407, 321, 630, 341], "temperature": 0.0, "avg_logprob": -0.14716907214092953, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.045347537205089e-05}, {"id": 112, "seek": 75624, "start": 776.2, "end": 784.52, "text": " measurement campaign. We made some fuss about it during November internally, pro-collapse, and also", "tokens": [13160, 5129, 13, 492, 1027, 512, 34792, 466, 309, 1830, 7674, 19501, 11, 447, 12, 33891, 11145, 11, 293, 611], "temperature": 0.0, "avg_logprob": -0.14716907214092953, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.045347537205089e-05}, {"id": 113, "seek": 78452, "start": 784.52, "end": 789.6, "text": " reached out to the community. And starting from the beginning of December, we said, okay, please", "tokens": [6488, 484, 281, 264, 1768, 13, 400, 2891, 490, 264, 2863, 295, 7687, 11, 321, 848, 11, 1392, 11, 1767], "temperature": 0.0, "avg_logprob": -0.1296324630578359, "compression_ratio": 1.5436507936507937, "no_speech_prob": 3.166657916153781e-05}, {"id": 114, "seek": 78452, "start": 789.6, "end": 799.64, "text": " download these clients, run it on your machines, and let's try to gather as much data as possible", "tokens": [5484, 613, 6982, 11, 1190, 309, 322, 428, 8379, 11, 293, 718, 311, 853, 281, 5448, 382, 709, 1412, 382, 1944], "temperature": 0.0, "avg_logprob": -0.1296324630578359, "compression_ratio": 1.5436507936507937, "no_speech_prob": 3.166657916153781e-05}, {"id": 115, "seek": 78452, "start": 799.64, "end": 805.1999999999999, "text": " during that time. And as you can see here, so we collected around 6.25 million hole punch results.", "tokens": [1830, 300, 565, 13, 400, 382, 291, 393, 536, 510, 11, 370, 321, 11087, 926, 1386, 13, 6074, 2459, 5458, 8135, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1296324630578359, "compression_ratio": 1.5436507936507937, "no_speech_prob": 3.166657916153781e-05}, {"id": 116, "seek": 78452, "start": 805.1999999999999, "end": 814.12, "text": " So this is quite a lot of data from 154 clients that participated. And we punched around 47,000", "tokens": [407, 341, 307, 1596, 257, 688, 295, 1412, 490, 2119, 19, 6982, 300, 17978, 13, 400, 321, 37842, 926, 16953, 11, 1360], "temperature": 0.0, "avg_logprob": -0.1296324630578359, "compression_ratio": 1.5436507936507937, "no_speech_prob": 3.166657916153781e-05}, {"id": 117, "seek": 81412, "start": 814.12, "end": 819.64, "text": " unique peers in this network. And on the right hand side, you can see the deployment of our", "tokens": [3845, 16739, 294, 341, 3209, 13, 400, 322, 264, 558, 1011, 1252, 11, 291, 393, 536, 264, 19317, 295, 527], "temperature": 0.0, "avg_logprob": -0.11434881874684537, "compression_ratio": 1.668141592920354, "no_speech_prob": 5.133540616952814e-05}, {"id": 118, "seek": 81412, "start": 819.64, "end": 826.12, "text": " clients, of our controlled clients. So the color here is the number of contributed results. So the", "tokens": [6982, 11, 295, 527, 10164, 6982, 13, 407, 264, 2017, 510, 307, 264, 1230, 295, 18434, 3542, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.11434881874684537, "compression_ratio": 1.668141592920354, "no_speech_prob": 5.133540616952814e-05}, {"id": 119, "seek": 81412, "start": 826.12, "end": 832.28, "text": " US was dominant here, but we have many other nodes deployed in Europe, but also Australia, New", "tokens": [2546, 390, 15657, 510, 11, 457, 321, 362, 867, 661, 13891, 17826, 294, 3315, 11, 457, 611, 7060, 11, 1873], "temperature": 0.0, "avg_logprob": -0.11434881874684537, "compression_ratio": 1.668141592920354, "no_speech_prob": 5.133540616952814e-05}, {"id": 120, "seek": 81412, "start": 832.28, "end": 838.88, "text": " Zealand, and also South America, and also one client from the continent of Africa. And this", "tokens": [13883, 11, 293, 611, 4242, 3374, 11, 293, 611, 472, 6423, 490, 264, 18932, 295, 7349, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.11434881874684537, "compression_ratio": 1.668141592920354, "no_speech_prob": 5.133540616952814e-05}, {"id": 121, "seek": 83888, "start": 838.88, "end": 845.08, "text": " actually, and these clients interacted with these other peers that are basically all around the", "tokens": [767, 11, 293, 613, 6982, 49621, 365, 613, 661, 16739, 300, 366, 1936, 439, 926, 264], "temperature": 0.0, "avg_logprob": -0.14169708887736002, "compression_ratio": 1.5706521739130435, "no_speech_prob": 6.737466264894465e-06}, {"id": 122, "seek": 83888, "start": 845.08, "end": 852.68, "text": " world. So we could measure hole punch success rates all across the globe. And I think we have a", "tokens": [1002, 13, 407, 321, 727, 3481, 5458, 8135, 2245, 6846, 439, 2108, 264, 15371, 13, 400, 286, 519, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.14169708887736002, "compression_ratio": 1.5706521739130435, "no_speech_prob": 6.737466264894465e-06}, {"id": 123, "seek": 83888, "start": 852.68, "end": 862.04, "text": " very comprehensive data set here. And so these, so we now gathered the data. And at the beginning", "tokens": [588, 13914, 1412, 992, 510, 13, 400, 370, 613, 11, 370, 321, 586, 13032, 264, 1412, 13, 400, 412, 264, 2863], "temperature": 0.0, "avg_logprob": -0.14169708887736002, "compression_ratio": 1.5706521739130435, "no_speech_prob": 6.737466264894465e-06}, {"id": 124, "seek": 86204, "start": 862.04, "end": 868.24, "text": " of December, sorry, of January, I started, so I said, okay, the hole punching month is over, and I", "tokens": [295, 7687, 11, 2597, 11, 295, 7061, 11, 286, 1409, 11, 370, 286, 848, 11, 1392, 11, 264, 5458, 34866, 1618, 307, 670, 11, 293, 286], "temperature": 0.0, "avg_logprob": -0.1565231686546689, "compression_ratio": 1.7377777777777779, "no_speech_prob": 9.074766239791643e-06}, {"id": 125, "seek": 86204, "start": 868.24, "end": 874.68, "text": " started to analyze the data a little bit. And what we can see here on the X axis is the, so each", "tokens": [1409, 281, 12477, 264, 1412, 257, 707, 857, 13, 400, 437, 321, 393, 536, 510, 322, 264, 1783, 10298, 307, 264, 11, 370, 1184], "temperature": 0.0, "avg_logprob": -0.1565231686546689, "compression_ratio": 1.7377777777777779, "no_speech_prob": 9.074766239791643e-06}, {"id": 126, "seek": 86204, "start": 874.68, "end": 881.48, "text": " bar is a unique client. And on the Y axis, we can see these different outcomes. So each hole punch", "tokens": [2159, 307, 257, 3845, 6423, 13, 400, 322, 264, 398, 10298, 11, 321, 393, 536, 613, 819, 10070, 13, 407, 1184, 5458, 8135], "temperature": 0.0, "avg_logprob": -0.1565231686546689, "compression_ratio": 1.7377777777777779, "no_speech_prob": 9.074766239791643e-06}, {"id": 127, "seek": 86204, "start": 881.48, "end": 886.4, "text": " result, as I said, can have, so the clients report back these results and each result can have a", "tokens": [1874, 11, 382, 286, 848, 11, 393, 362, 11, 370, 264, 6982, 2275, 646, 613, 3542, 293, 1184, 1874, 393, 362, 257], "temperature": 0.0, "avg_logprob": -0.1565231686546689, "compression_ratio": 1.7377777777777779, "no_speech_prob": 9.074766239791643e-06}, {"id": 128, "seek": 88640, "start": 886.4, "end": 892.64, "text": " different outcome. These outcomes are at the top. So it can be successful. So we actually were able", "tokens": [819, 9700, 13, 1981, 10070, 366, 412, 264, 1192, 13, 407, 309, 393, 312, 4406, 13, 407, 321, 767, 645, 1075], "temperature": 0.0, "avg_logprob": -0.11700851875439025, "compression_ratio": 1.828996282527881, "no_speech_prob": 2.9290124075487256e-05}, {"id": 129, "seek": 88640, "start": 892.64, "end": 897.56, "text": " to establish a direct connection through hole punching, then connection reversed. This means,", "tokens": [281, 8327, 257, 2047, 4984, 807, 5458, 34866, 11, 550, 4984, 30563, 13, 639, 1355, 11], "temperature": 0.0, "avg_logprob": -0.11700851875439025, "compression_ratio": 1.828996282527881, "no_speech_prob": 2.9290124075487256e-05}, {"id": 130, "seek": 88640, "start": 897.56, "end": 903.9599999999999, "text": " I'm trying to hole punch as I'm connecting to the other peer through the relay. And the first thing", "tokens": [286, 478, 1382, 281, 5458, 8135, 382, 286, 478, 11015, 281, 264, 661, 15108, 807, 264, 24214, 13, 400, 264, 700, 551], "temperature": 0.0, "avg_logprob": -0.11700851875439025, "compression_ratio": 1.828996282527881, "no_speech_prob": 2.9290124075487256e-05}, {"id": 131, "seek": 88640, "start": 903.9599999999999, "end": 909.56, "text": " before we do the hole punching dance is for the peer to directly connect to us. Because if we are", "tokens": [949, 321, 360, 264, 5458, 34866, 4489, 307, 337, 264, 15108, 281, 3838, 1745, 281, 505, 13, 1436, 498, 321, 366], "temperature": 0.0, "avg_logprob": -0.11700851875439025, "compression_ratio": 1.828996282527881, "no_speech_prob": 2.9290124075487256e-05}, {"id": 132, "seek": 88640, "start": 909.56, "end": 914.28, "text": " directly reachable, because we have a port mapping in place in the router, we don't actually need to", "tokens": [3838, 2524, 712, 11, 570, 321, 362, 257, 2436, 18350, 294, 1081, 294, 264, 22492, 11, 321, 500, 380, 767, 643, 281], "temperature": 0.0, "avg_logprob": -0.11700851875439025, "compression_ratio": 1.828996282527881, "no_speech_prob": 2.9290124075487256e-05}, {"id": 133, "seek": 91428, "start": 914.28, "end": 919.4, "text": " do the hole punching exchange. This is the connection reversed. And as we can see here, it's", "tokens": [360, 264, 5458, 34866, 7742, 13, 639, 307, 264, 4984, 30563, 13, 400, 382, 321, 393, 536, 510, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.12388660061743952, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.319671264354838e-05}, {"id": 134, "seek": 91428, "start": 919.4, "end": 925.76, "text": " a little hard to see. But some clients actually have a lot of these results. So this means they", "tokens": [257, 707, 1152, 281, 536, 13, 583, 512, 6982, 767, 362, 257, 688, 295, 613, 3542, 13, 407, 341, 1355, 436], "temperature": 0.0, "avg_logprob": -0.12388660061743952, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.319671264354838e-05}, {"id": 135, "seek": 91428, "start": 925.76, "end": 932.72, "text": " have a unique router configuration in place. Then failed is the obvious thing. So we tried, we", "tokens": [362, 257, 3845, 22492, 11694, 294, 1081, 13, 1396, 7612, 307, 264, 6322, 551, 13, 407, 321, 3031, 11, 321], "temperature": 0.0, "avg_logprob": -0.12388660061743952, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.319671264354838e-05}, {"id": 136, "seek": 91428, "start": 932.72, "end": 938.4, "text": " exchanged these messages, but in the end, weren't able to establish a connection. No stream is some", "tokens": [38378, 613, 7897, 11, 457, 294, 264, 917, 11, 4999, 380, 1075, 281, 8327, 257, 4984, 13, 883, 4309, 307, 512], "temperature": 0.0, "avg_logprob": -0.12388660061743952, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.319671264354838e-05}, {"id": 137, "seek": 93840, "start": 938.4, "end": 946.72, "text": " internal error that's unique to our setup. So probably nothing to worry about here. And no", "tokens": [6920, 6713, 300, 311, 3845, 281, 527, 8657, 13, 407, 1391, 1825, 281, 3292, 466, 510, 13, 400, 572], "temperature": 0.0, "avg_logprob": -0.13432922533580235, "compression_ratio": 1.8022813688212929, "no_speech_prob": 2.3903394321678206e-05}, {"id": 138, "seek": 93840, "start": 946.72, "end": 951.28, "text": " connection means we try to connect to the other peer through a relay, but the other peer was", "tokens": [4984, 1355, 321, 853, 281, 1745, 281, 264, 661, 15108, 807, 257, 24214, 11, 457, 264, 661, 15108, 390], "temperature": 0.0, "avg_logprob": -0.13432922533580235, "compression_ratio": 1.8022813688212929, "no_speech_prob": 2.3903394321678206e-05}, {"id": 139, "seek": 93840, "start": 951.28, "end": 955.4399999999999, "text": " already gone. It's a permissionless peer-to-peer network. So it could be from the time that the", "tokens": [1217, 2780, 13, 467, 311, 257, 11226, 1832, 15108, 12, 1353, 12, 494, 260, 3209, 13, 407, 309, 727, 312, 490, 264, 565, 300, 264], "temperature": 0.0, "avg_logprob": -0.13432922533580235, "compression_ratio": 1.8022813688212929, "no_speech_prob": 2.3903394321678206e-05}, {"id": 140, "seek": 93840, "start": 955.4399999999999, "end": 961.24, "text": " honeypot detected the peer to the client trying to establish a connection to the peer that the", "tokens": [8330, 17698, 21896, 264, 15108, 281, 264, 6423, 1382, 281, 8327, 257, 4984, 281, 264, 15108, 300, 264], "temperature": 0.0, "avg_logprob": -0.13432922533580235, "compression_ratio": 1.8022813688212929, "no_speech_prob": 2.3903394321678206e-05}, {"id": 141, "seek": 93840, "start": 961.24, "end": 967.92, "text": " client has already churned and left the network. But actually looking at these clients is distorted", "tokens": [6423, 575, 1217, 417, 925, 292, 293, 1411, 264, 3209, 13, 583, 767, 1237, 412, 613, 6982, 307, 33431], "temperature": 0.0, "avg_logprob": -0.13432922533580235, "compression_ratio": 1.8022813688212929, "no_speech_prob": 2.3903394321678206e-05}, {"id": 142, "seek": 96792, "start": 967.92, "end": 974.04, "text": " view on the data because we allowed everyone who participated in the campaign to move, to freely", "tokens": [1910, 322, 264, 1412, 570, 321, 4350, 1518, 567, 17978, 294, 264, 5129, 281, 1286, 11, 281, 16433], "temperature": 0.0, "avg_logprob": -0.1466947503992029, "compression_ratio": 1.704626334519573, "no_speech_prob": 2.2806538254371844e-05}, {"id": 143, "seek": 96792, "start": 974.04, "end": 978.9599999999999, "text": " move around. So I was running this client in my laptop and I was moving from a coffee shop,", "tokens": [1286, 926, 13, 407, 286, 390, 2614, 341, 6423, 294, 452, 10732, 293, 286, 390, 2684, 490, 257, 4982, 3945, 11], "temperature": 0.0, "avg_logprob": -0.1466947503992029, "compression_ratio": 1.704626334519573, "no_speech_prob": 2.2806538254371844e-05}, {"id": 144, "seek": 96792, "start": 978.9599999999999, "end": 984.8, "text": " a Wi-Fi network to a home network to a university network and so on. And hole punching is actually", "tokens": [257, 14035, 12, 13229, 3209, 281, 257, 1280, 3209, 281, 257, 5454, 3209, 293, 370, 322, 13, 400, 5458, 34866, 307, 767], "temperature": 0.0, "avg_logprob": -0.1466947503992029, "compression_ratio": 1.704626334519573, "no_speech_prob": 2.2806538254371844e-05}, {"id": 145, "seek": 96792, "start": 984.8, "end": 992.1999999999999, "text": " dependent on those network configurations instead of just me running the client. So the challenge", "tokens": [12334, 322, 729, 3209, 31493, 2602, 295, 445, 385, 2614, 264, 6423, 13, 407, 264, 3430], "temperature": 0.0, "avg_logprob": -0.1466947503992029, "compression_ratio": 1.704626334519573, "no_speech_prob": 2.2806538254371844e-05}, {"id": 146, "seek": 96792, "start": 992.1999999999999, "end": 996.88, "text": " here with the data analysis was, so I'm also not done with that yet and happy to open for the", "tokens": [510, 365, 264, 1412, 5215, 390, 11, 370, 286, 478, 611, 406, 1096, 365, 300, 1939, 293, 2055, 281, 1269, 337, 264], "temperature": 0.0, "avg_logprob": -0.1466947503992029, "compression_ratio": 1.704626334519573, "no_speech_prob": 2.2806538254371844e-05}, {"id": 147, "seek": 99688, "start": 996.88, "end": 1001.84, "text": " suggestions to detect these individual networks that the clients operated in. With each hole", "tokens": [13396, 281, 5531, 613, 2609, 9590, 300, 264, 6982, 20826, 294, 13, 2022, 1184, 5458], "temperature": 0.0, "avg_logprob": -0.1569263504212161, "compression_ratio": 1.7098214285714286, "no_speech_prob": 2.3141516066971235e-05}, {"id": 148, "seek": 99688, "start": 1001.84, "end": 1008.12, "text": " punch results, the client reported their listening IP addresses and so on. And I grouped them", "tokens": [8135, 3542, 11, 264, 6423, 7055, 641, 4764, 8671, 16862, 293, 370, 322, 13, 400, 286, 41877, 552], "temperature": 0.0, "avg_logprob": -0.1569263504212161, "compression_ratio": 1.7098214285714286, "no_speech_prob": 2.3141516066971235e-05}, {"id": 149, "seek": 99688, "start": 1008.12, "end": 1015.52, "text": " together to actually find out, to identify unique networks that those clients operated in. And at", "tokens": [1214, 281, 767, 915, 484, 11, 281, 5876, 3845, 9590, 300, 729, 6982, 20826, 294, 13, 400, 412], "temperature": 0.0, "avg_logprob": -0.1569263504212161, "compression_ratio": 1.7098214285714286, "no_speech_prob": 2.3141516066971235e-05}, {"id": 150, "seek": 99688, "start": 1015.52, "end": 1021.36, "text": " the end, I arrived at 342 unique client networks. And then the graph looks like this, probably not", "tokens": [264, 917, 11, 286, 6678, 412, 12790, 17, 3845, 6423, 9590, 13, 400, 550, 264, 4295, 1542, 411, 341, 11, 1391, 406], "temperature": 0.0, "avg_logprob": -0.1569263504212161, "compression_ratio": 1.7098214285714286, "no_speech_prob": 2.3141516066971235e-05}, {"id": 151, "seek": 102136, "start": 1021.36, "end": 1029.24, "text": " much different than before. But also there are some interesting unique network outcomes here", "tokens": [709, 819, 813, 949, 13, 583, 611, 456, 366, 512, 1880, 3845, 3209, 10070, 510], "temperature": 0.0, "avg_logprob": -0.17281538248062134, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.2016268556180876e-05}, {"id": 152, "seek": 102136, "start": 1029.24, "end": 1036.08, "text": " that I will also get to in a bit. The most interesting graph is probably this one. So what's", "tokens": [300, 286, 486, 611, 483, 281, 294, 257, 857, 13, 440, 881, 1880, 4295, 307, 1391, 341, 472, 13, 407, 437, 311], "temperature": 0.0, "avg_logprob": -0.17281538248062134, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.2016268556180876e-05}, {"id": 153, "seek": 102136, "start": 1036.08, "end": 1043.6, "text": " the success rate of this protocol? And on the x-axis, we have the success rate been by, yeah,", "tokens": [264, 2245, 3314, 295, 341, 10336, 30, 400, 322, 264, 2031, 12, 24633, 11, 321, 362, 264, 2245, 3314, 668, 538, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.17281538248062134, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.2016268556180876e-05}, {"id": 154, "seek": 102136, "start": 1043.6, "end": 1051.0, "text": " just 5% binnings. And on the y-axis, the number of networks that had the success rate by probing", "tokens": [445, 1025, 4, 5171, 24451, 13, 400, 322, 264, 288, 12, 24633, 11, 264, 1230, 295, 9590, 300, 632, 264, 2245, 3314, 538, 1239, 278], "temperature": 0.0, "avg_logprob": -0.17281538248062134, "compression_ratio": 1.663716814159292, "no_speech_prob": 1.2016268556180876e-05}, {"id": 155, "seek": 105100, "start": 1051.0, "end": 1056.52, "text": " the whole other network. And the majority of networks actually had a success rate of 70%. So", "tokens": [264, 1379, 661, 3209, 13, 400, 264, 6286, 295, 9590, 767, 632, 257, 2245, 3314, 295, 5285, 6856, 407], "temperature": 0.0, "avg_logprob": -0.12213507152739025, "compression_ratio": 1.6, "no_speech_prob": 1.3207724805397447e-05}, {"id": 156, "seek": 105100, "start": 1056.52, "end": 1063.24, "text": " I think this is already, actually, I think it's amazing because from not being able to connect", "tokens": [286, 519, 341, 307, 1217, 11, 767, 11, 286, 519, 309, 311, 2243, 570, 490, 406, 885, 1075, 281, 1745], "temperature": 0.0, "avg_logprob": -0.12213507152739025, "compression_ratio": 1.6, "no_speech_prob": 1.3207724805397447e-05}, {"id": 157, "seek": 105100, "start": 1063.24, "end": 1068.12, "text": " at all to having a 70% chance to establish a direct connection without an intermediary,", "tokens": [412, 439, 281, 1419, 257, 5285, 4, 2931, 281, 8327, 257, 2047, 4984, 1553, 364, 15184, 822, 11], "temperature": 0.0, "avg_logprob": -0.12213507152739025, "compression_ratio": 1.6, "no_speech_prob": 1.3207724805397447e-05}, {"id": 158, "seek": 105100, "start": 1068.12, "end": 1073.64, "text": " it's actually pretty great. But then also there are some networks that have very low", "tokens": [309, 311, 767, 1238, 869, 13, 583, 550, 611, 456, 366, 512, 9590, 300, 362, 588, 2295], "temperature": 0.0, "avg_logprob": -0.12213507152739025, "compression_ratio": 1.6, "no_speech_prob": 1.3207724805397447e-05}, {"id": 159, "seek": 107364, "start": 1073.64, "end": 1081.2, "text": " success rate. And these are the ones that are probably the most interesting ones. Then also, oops,", "tokens": [2245, 3314, 13, 400, 613, 366, 264, 2306, 300, 366, 1391, 264, 881, 1880, 2306, 13, 1396, 611, 11, 34166, 11], "temperature": 0.0, "avg_logprob": -0.16764286586216517, "compression_ratio": 1.6695278969957081, "no_speech_prob": 6.043174835212994e-06}, {"id": 160, "seek": 107364, "start": 1081.2, "end": 1088.68, "text": " the IP and transport dependence is also quite interesting to like as an angle to look at the", "tokens": [264, 8671, 293, 5495, 31704, 307, 611, 1596, 1880, 281, 411, 382, 364, 5802, 281, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.16764286586216517, "compression_ratio": 1.6695278969957081, "no_speech_prob": 6.043174835212994e-06}, {"id": 161, "seek": 107364, "start": 1088.68, "end": 1097.0, "text": " data. Here we can see that the top row, we used IPv4 and TCP to hole punch. So when these clients", "tokens": [1412, 13, 1692, 321, 393, 536, 300, 264, 1192, 5386, 11, 321, 1143, 8671, 85, 19, 293, 48965, 281, 5458, 8135, 13, 407, 562, 613, 6982], "temperature": 0.0, "avg_logprob": -0.16764286586216517, "compression_ratio": 1.6695278969957081, "no_speech_prob": 6.043174835212994e-06}, {"id": 162, "seek": 107364, "start": 1097.0, "end": 1103.2, "text": " exchange these connect messages, they actually exchange the publicly listen, the publicly reachable", "tokens": [7742, 613, 1745, 7897, 11, 436, 767, 7742, 264, 14843, 2140, 11, 264, 14843, 2524, 712], "temperature": 0.0, "avg_logprob": -0.16764286586216517, "compression_ratio": 1.6695278969957081, "no_speech_prob": 6.043174835212994e-06}, {"id": 163, "seek": 110320, "start": 1103.2, "end": 1108.3600000000001, "text": " IP addresses of those two peers that want to hole punch. And in our measurement campaign,", "tokens": [8671, 16862, 295, 729, 732, 16739, 300, 528, 281, 5458, 8135, 13, 400, 294, 527, 13160, 5129, 11], "temperature": 0.0, "avg_logprob": -0.11741576296217898, "compression_ratio": 1.6130434782608696, "no_speech_prob": 3.1636325729778036e-05}, {"id": 164, "seek": 110320, "start": 1108.3600000000001, "end": 1114.92, "text": " we restricted this to actually only IPv4 and TCP and with some other hole punches only to IPv6", "tokens": [321, 20608, 341, 281, 767, 787, 8671, 85, 19, 293, 48965, 293, 365, 512, 661, 5458, 34103, 787, 281, 8671, 85, 21], "temperature": 0.0, "avg_logprob": -0.11741576296217898, "compression_ratio": 1.6130434782608696, "no_speech_prob": 3.1636325729778036e-05}, {"id": 165, "seek": 110320, "start": 1114.92, "end": 1120.92, "text": " and quick, which is on the bottom right. And so we can take a look which combination is more", "tokens": [293, 1702, 11, 597, 307, 322, 264, 2767, 558, 13, 400, 370, 321, 393, 747, 257, 574, 597, 6562, 307, 544], "temperature": 0.0, "avg_logprob": -0.11741576296217898, "compression_ratio": 1.6130434782608696, "no_speech_prob": 3.1636325729778036e-05}, {"id": 166, "seek": 110320, "start": 1120.92, "end": 1127.92, "text": " successful than the other. And here we can see that IPv4 in TCP and quick is actually, if you", "tokens": [4406, 813, 264, 661, 13, 400, 510, 321, 393, 536, 300, 8671, 85, 19, 294, 48965, 293, 1702, 307, 767, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.11741576296217898, "compression_ratio": 1.6130434782608696, "no_speech_prob": 3.1636325729778036e-05}, {"id": 167, "seek": 112792, "start": 1127.92, "end": 1133.8000000000002, "text": " average the numbers has a similar success rate. But on IPv6, we have actually, it's basically not", "tokens": [4274, 264, 3547, 575, 257, 2531, 2245, 3314, 13, 583, 322, 8671, 85, 21, 11, 321, 362, 767, 11, 309, 311, 1936, 406], "temperature": 0.0, "avg_logprob": -0.1579363105002414, "compression_ratio": 1.609442060085837, "no_speech_prob": 1.980768320208881e-05}, {"id": 168, "seek": 112792, "start": 1133.8000000000002, "end": 1139.96, "text": " working at all. And these unexpected things are actually the interesting ones for us. Either it's", "tokens": [1364, 412, 439, 13, 400, 613, 13106, 721, 366, 767, 264, 1880, 2306, 337, 505, 13, 13746, 309, 311], "temperature": 0.0, "avg_logprob": -0.1579363105002414, "compression_ratio": 1.609442060085837, "no_speech_prob": 1.980768320208881e-05}, {"id": 169, "seek": 112792, "start": 1139.96, "end": 1145.1200000000001, "text": " a measurement error, or there's some inherent property to the networking setup that prevents", "tokens": [257, 13160, 6713, 11, 420, 456, 311, 512, 26387, 4707, 281, 264, 17985, 8657, 300, 22367], "temperature": 0.0, "avg_logprob": -0.1579363105002414, "compression_ratio": 1.609442060085837, "no_speech_prob": 1.980768320208881e-05}, {"id": 170, "seek": 112792, "start": 1145.1200000000001, "end": 1157.8000000000002, "text": " IPv6 from being hole punchable, basically. If we actually allow both transports, so in", "tokens": [8671, 85, 21, 490, 885, 5458, 8135, 712, 11, 1936, 13, 759, 321, 767, 2089, 1293, 5495, 82, 11, 370, 294], "temperature": 0.0, "avg_logprob": -0.1579363105002414, "compression_ratio": 1.609442060085837, "no_speech_prob": 1.980768320208881e-05}, {"id": 171, "seek": 115780, "start": 1157.8, "end": 1162.68, "text": " the first, in the previous graph, we showed we're only using TCP and quick. But if we allow both", "tokens": [264, 700, 11, 294, 264, 3894, 4295, 11, 321, 4712, 321, 434, 787, 1228, 48965, 293, 1702, 13, 583, 498, 321, 2089, 1293], "temperature": 0.0, "avg_logprob": -0.14952022096385126, "compression_ratio": 1.5726141078838174, "no_speech_prob": 7.760398148093373e-06}, {"id": 172, "seek": 115780, "start": 1162.68, "end": 1168.36, "text": " transports to simultaneously try to hole punch, we can see that we, with 81%, we end up with a", "tokens": [5495, 82, 281, 16561, 853, 281, 5458, 8135, 11, 321, 393, 536, 300, 321, 11, 365, 30827, 8923, 321, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.14952022096385126, "compression_ratio": 1.5726141078838174, "no_speech_prob": 7.760398148093373e-06}, {"id": 173, "seek": 115780, "start": 1168.36, "end": 1173.84, "text": " quick connection. And this is just because quick connection establishment is way faster than TCP", "tokens": [1702, 4984, 13, 400, 341, 307, 445, 570, 1702, 4984, 20971, 307, 636, 4663, 813, 48965], "temperature": 0.0, "avg_logprob": -0.14952022096385126, "compression_ratio": 1.5726141078838174, "no_speech_prob": 7.760398148093373e-06}, {"id": 174, "seek": 115780, "start": 1173.84, "end": 1178.84, "text": " connection. So this is like an expected result here, just to verify some of the data here.", "tokens": [4984, 13, 407, 341, 307, 411, 364, 5176, 1874, 510, 11, 445, 281, 16888, 512, 295, 264, 1412, 510, 13], "temperature": 0.0, "avg_logprob": -0.14952022096385126, "compression_ratio": 1.5726141078838174, "no_speech_prob": 7.760398148093373e-06}, {"id": 175, "seek": 117884, "start": 1178.84, "end": 1187.9199999999998, "text": " And now two takeaways for us, for ProCo improvements. So if we took a private VPN, so if", "tokens": [400, 586, 732, 45584, 337, 505, 11, 337, 1705, 21141, 13797, 13, 407, 498, 321, 1890, 257, 4551, 24512, 11, 370, 498], "temperature": 0.0, "avg_logprob": -0.19152965545654296, "compression_ratio": 1.5606694560669456, "no_speech_prob": 1.341138886346016e-05}, {"id": 176, "seek": 117884, "start": 1187.9199999999998, "end": 1192.6799999999998, "text": " clients are running in VPNs, we can see that the success rate actually drops significantly from", "tokens": [6982, 366, 2614, 294, 24512, 82, 11, 321, 393, 536, 300, 264, 2245, 3314, 767, 11438, 10591, 490], "temperature": 0.0, "avg_logprob": -0.19152965545654296, "compression_ratio": 1.5606694560669456, "no_speech_prob": 1.341138886346016e-05}, {"id": 177, "seek": 117884, "start": 1192.6799999999998, "end": 1199.04, "text": " around 70% to less than 40%. And my hypothesis here is that the router, the router time that", "tokens": [926, 5285, 4, 281, 1570, 813, 3356, 6856, 400, 452, 17291, 510, 307, 300, 264, 22492, 11, 264, 22492, 565, 300], "temperature": 0.0, "avg_logprob": -0.19152965545654296, "compression_ratio": 1.5606694560669456, "no_speech_prob": 1.341138886346016e-05}, {"id": 178, "seek": 117884, "start": 1199.04, "end": 1204.3999999999999, "text": " Max showed previously is measured between A and B. But what we actually need is the router time", "tokens": [7402, 4712, 8046, 307, 12690, 1296, 316, 293, 363, 13, 583, 437, 321, 767, 643, 307, 264, 22492, 565], "temperature": 0.0, "avg_logprob": -0.19152965545654296, "compression_ratio": 1.5606694560669456, "no_speech_prob": 1.341138886346016e-05}, {"id": 179, "seek": 120440, "start": 1204.4, "end": 1211.48, "text": " between the router A and router B. And if your router basically is the exit node, or your gateway", "tokens": [1296, 264, 22492, 316, 293, 22492, 363, 13, 400, 498, 428, 22492, 1936, 307, 264, 11043, 9984, 11, 420, 428, 28532], "temperature": 0.0, "avg_logprob": -0.1428901884290907, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.494074801186798e-06}, {"id": 180, "seek": 120440, "start": 1211.48, "end": 1217.72, "text": " that you're connected to from your VPN, this can differ by dozens of milliseconds, actually.", "tokens": [300, 291, 434, 4582, 281, 490, 428, 24512, 11, 341, 393, 743, 538, 18431, 295, 34184, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.1428901884290907, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.494074801186798e-06}, {"id": 181, "seek": 120440, "start": 1217.72, "end": 1222.72, "text": " And so the router time doesn't add up, and the hole synchronization is a little off. So this is", "tokens": [400, 370, 264, 22492, 565, 1177, 380, 909, 493, 11, 293, 264, 5458, 19331, 2144, 307, 257, 707, 766, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.1428901884290907, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.494074801186798e-06}, {"id": 182, "seek": 120440, "start": 1222.72, "end": 1229.6000000000001, "text": " potentially a protocol improvement here. And then, also interesting, so Max said they are", "tokens": [7263, 257, 10336, 10444, 510, 13, 400, 550, 11, 611, 1880, 11, 370, 7402, 848, 436, 366], "temperature": 0.0, "avg_logprob": -0.1428901884290907, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.494074801186798e-06}, {"id": 183, "seek": 122960, "start": 1229.6, "end": 1236.1599999999999, "text": " exchanging these messages during the hole punch. But actually, we try this three times. So if it", "tokens": [6210, 9741, 613, 7897, 1830, 264, 5458, 8135, 13, 583, 767, 11, 321, 853, 341, 1045, 1413, 13, 407, 498, 309], "temperature": 0.0, "avg_logprob": -0.10453271865844727, "compression_ratio": 1.7887323943661972, "no_speech_prob": 1.2602020433405414e-05}, {"id": 184, "seek": 122960, "start": 1236.1599999999999, "end": 1239.9599999999998, "text": " doesn't work the first time, we try it again. And if it doesn't work the second time, we try it", "tokens": [1177, 380, 589, 264, 700, 565, 11, 321, 853, 309, 797, 13, 400, 498, 309, 1177, 380, 589, 264, 1150, 565, 11, 321, 853, 309], "temperature": 0.0, "avg_logprob": -0.10453271865844727, "compression_ratio": 1.7887323943661972, "no_speech_prob": 1.2602020433405414e-05}, {"id": 185, "seek": 122960, "start": 1239.9599999999998, "end": 1246.8799999999999, "text": " yet again. But when we look at the data, if we end up with a successful hole punch connection,", "tokens": [1939, 797, 13, 583, 562, 321, 574, 412, 264, 1412, 11, 498, 321, 917, 493, 365, 257, 4406, 5458, 8135, 4984, 11], "temperature": 0.0, "avg_logprob": -0.10453271865844727, "compression_ratio": 1.7887323943661972, "no_speech_prob": 1.2602020433405414e-05}, {"id": 186, "seek": 122960, "start": 1246.8799999999999, "end": 1255.08, "text": " it was actually successful with the first attempt in 97% or 98% of the cases. So this is also", "tokens": [309, 390, 767, 4406, 365, 264, 700, 5217, 294, 23399, 4, 420, 20860, 4, 295, 264, 3331, 13, 407, 341, 307, 611], "temperature": 0.0, "avg_logprob": -0.10453271865844727, "compression_ratio": 1.7887323943661972, "no_speech_prob": 1.2602020433405414e-05}, {"id": 187, "seek": 125508, "start": 1255.08, "end": 1262.84, "text": " something for the next steps for us. We should consider changing our strategy on the second and", "tokens": [746, 337, 264, 958, 4439, 337, 505, 13, 492, 820, 1949, 4473, 527, 5206, 322, 264, 1150, 293], "temperature": 0.0, "avg_logprob": -0.08153409689245089, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.1120151611976326e-05}, {"id": 188, "seek": 125508, "start": 1262.84, "end": 1269.0, "text": " third try to increase the odds. So if we stick with the three retries, we shouldn't do the same", "tokens": [2636, 853, 281, 3488, 264, 17439, 13, 407, 498, 321, 2897, 365, 264, 1045, 1533, 2244, 11, 321, 4659, 380, 360, 264, 912], "temperature": 0.0, "avg_logprob": -0.08153409689245089, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.1120151611976326e-05}, {"id": 189, "seek": 125508, "start": 1269.0, "end": 1274.9199999999998, "text": " thing over again, because as we saw from the data, it doesn't make a difference. So we should", "tokens": [551, 670, 797, 11, 570, 382, 321, 1866, 490, 264, 1412, 11, 309, 1177, 380, 652, 257, 2649, 13, 407, 321, 820], "temperature": 0.0, "avg_logprob": -0.08153409689245089, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.1120151611976326e-05}, {"id": 190, "seek": 127492, "start": 1274.92, "end": 1286.28, "text": " change our strategy here. And so one thing would be to reverse the client server roles in this", "tokens": [1319, 527, 5206, 510, 13, 400, 370, 472, 551, 576, 312, 281, 9943, 264, 6423, 7154, 9604, 294, 341], "temperature": 0.0, "avg_logprob": -0.1292701134314904, "compression_ratio": 1.5942857142857143, "no_speech_prob": 6.745760856574634e-06}, {"id": 191, "seek": 127492, "start": 1286.28, "end": 1294.16, "text": " quick hole punching exchange. This would be something, and also the other protocol improvement", "tokens": [1702, 5458, 34866, 7742, 13, 639, 576, 312, 746, 11, 293, 611, 264, 661, 10336, 10444], "temperature": 0.0, "avg_logprob": -0.1292701134314904, "compression_ratio": 1.5942857142857143, "no_speech_prob": 6.745760856574634e-06}, {"id": 192, "seek": 127492, "start": 1294.16, "end": 1300.88, "text": " for us, as I said, would be to change the measurement of the round trip time. And for the", "tokens": [337, 505, 11, 382, 286, 848, 11, 576, 312, 281, 1319, 264, 13160, 295, 264, 3098, 4931, 565, 13, 400, 337, 264], "temperature": 0.0, "avg_logprob": -0.1292701134314904, "compression_ratio": 1.5942857142857143, "no_speech_prob": 6.745760856574634e-06}, {"id": 193, "seek": 130088, "start": 1300.88, "end": 1306.0400000000002, "text": " future, the data analysis, right now, what I showed here is basically aggregates across all", "tokens": [2027, 11, 264, 1412, 5215, 11, 558, 586, 11, 437, 286, 4712, 510, 307, 1936, 16743, 1024, 2108, 439], "temperature": 0.0, "avg_logprob": -0.1733687391905027, "compression_ratio": 1.7320754716981133, "no_speech_prob": 8.660308594699018e-06}, {"id": 194, "seek": 130088, "start": 1306.0400000000002, "end": 1312.1200000000001, "text": " the data. And the interesting part is basically, so why is a specific client or a specific network,", "tokens": [264, 1412, 13, 400, 264, 1880, 644, 307, 1936, 11, 370, 983, 307, 257, 2685, 6423, 420, 257, 2685, 3209, 11], "temperature": 0.0, "avg_logprob": -0.1733687391905027, "compression_ratio": 1.7320754716981133, "no_speech_prob": 8.660308594699018e-06}, {"id": 195, "seek": 130088, "start": 1312.1200000000001, "end": 1318.1200000000001, "text": " why has it less or a worse success rate than others? So these are like these individual", "tokens": [983, 575, 309, 1570, 420, 257, 5324, 2245, 3314, 813, 2357, 30, 407, 613, 366, 411, 613, 2609], "temperature": 0.0, "avg_logprob": -0.1733687391905027, "compression_ratio": 1.7320754716981133, "no_speech_prob": 8.660308594699018e-06}, {"id": 196, "seek": 130088, "start": 1318.1200000000001, "end": 1322.8400000000001, "text": " things to look into to increase, maybe there's a common pattern that we can address with the", "tokens": [721, 281, 574, 666, 281, 3488, 11, 1310, 456, 311, 257, 2689, 5102, 300, 321, 393, 2985, 365, 264], "temperature": 0.0, "avg_logprob": -0.1733687391905027, "compression_ratio": 1.7320754716981133, "no_speech_prob": 8.660308594699018e-06}, {"id": 197, "seek": 130088, "start": 1322.8400000000001, "end": 1328.5200000000002, "text": " protocol to increase the success rate. And yeah, then identify those causes. And also,", "tokens": [10336, 281, 3488, 264, 2245, 3314, 13, 400, 1338, 11, 550, 5876, 729, 7700, 13, 400, 611, 11], "temperature": 0.0, "avg_logprob": -0.1733687391905027, "compression_ratio": 1.7320754716981133, "no_speech_prob": 8.660308594699018e-06}, {"id": 198, "seek": 132852, "start": 1328.52, "end": 1333.04, "text": " at the end of all of this, we want to craft a follow up publication to something that maxed", "tokens": [412, 264, 917, 295, 439, 295, 341, 11, 321, 528, 281, 8448, 257, 1524, 493, 19953, 281, 746, 300, 11469, 292], "temperature": 0.0, "avg_logprob": -0.15952285130818686, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.752226464508567e-05}, {"id": 199, "seek": 132852, "start": 1333.04, "end": 1341.12, "text": " and some fellow friends, I would say, have it published just last year. And yeah, we want to", "tokens": [293, 512, 7177, 1855, 11, 286, 576, 584, 11, 362, 309, 6572, 445, 1036, 1064, 13, 400, 1338, 11, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.15952285130818686, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.752226464508567e-05}, {"id": 200, "seek": 132852, "start": 1341.12, "end": 1348.28, "text": " make the data set public and so on and so forth for others to benefit from the data and can do", "tokens": [652, 264, 1412, 992, 1908, 293, 370, 322, 293, 370, 5220, 337, 2357, 281, 5121, 490, 264, 1412, 293, 393, 360], "temperature": 0.0, "avg_logprob": -0.15952285130818686, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.752226464508567e-05}, {"id": 201, "seek": 132852, "start": 1348.28, "end": 1354.16, "text": " their own analysis. Yeah, and with that, get involved, talk to us here at the venue about all", "tokens": [641, 1065, 5215, 13, 865, 11, 293, 365, 300, 11, 483, 3288, 11, 751, 281, 505, 510, 412, 264, 21645, 466, 439], "temperature": 0.0, "avg_logprob": -0.15952285130818686, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.752226464508567e-05}, {"id": 202, "seek": 135416, "start": 1354.16, "end": 1360.0800000000002, "text": " of this. LipidFee is a great project. Have a look at all these links. Get in touch and", "tokens": [295, 341, 13, 27475, 327, 37, 1653, 307, 257, 869, 1716, 13, 3560, 257, 574, 412, 439, 613, 6123, 13, 3240, 294, 2557, 293], "temperature": 0.0, "avg_logprob": -0.274993615991929, "compression_ratio": 1.2661870503597121, "no_speech_prob": 0.00020204376778565347}, {"id": 203, "seek": 135416, "start": 1360.0800000000002, "end": 1365.3200000000002, "text": " contribute to join our community calls. And yeah, I think that's it. Thank you very much.", "tokens": [10586, 281, 3917, 527, 1768, 5498, 13, 400, 1338, 11, 286, 519, 300, 311, 309, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.274993615991929, "compression_ratio": 1.2661870503597121, "no_speech_prob": 0.00020204376778565347}, {"id": 204, "seek": 136532, "start": 1365.32, "end": 1385.24, "text": " At least what you implemented there, is it exactly ICE turnstaff or how different it is from this?", "tokens": [1711, 1935, 437, 291, 12270, 456, 11, 307, 309, 2293, 43337, 1261, 372, 2518, 420, 577, 819, 309, 307, 490, 341, 30], "temperature": 0.0, "avg_logprob": -0.2777771268572126, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.00034750052145682275}, {"id": 205, "seek": 136532, "start": 1385.24, "end": 1395.2, "text": " So we differ in some cases, it's definitely very much motivated by ICE in turn. So a couple", "tokens": [407, 321, 743, 294, 512, 3331, 11, 309, 311, 2138, 588, 709, 14515, 538, 43337, 294, 1261, 13, 407, 257, 1916], "temperature": 0.0, "avg_logprob": -0.2777771268572126, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.00034750052145682275}, {"id": 206, "seek": 139520, "start": 1395.2, "end": 1401.96, "text": " of things, we don't do turn itself, we have our own relay protocol, because nodes in the network", "tokens": [295, 721, 11, 321, 500, 380, 360, 1261, 2564, 11, 321, 362, 527, 1065, 24214, 10336, 11, 570, 13891, 294, 264, 3209], "temperature": 0.0, "avg_logprob": -0.12968072112725706, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0004085893160663545}, {"id": 207, "seek": 139520, "start": 1401.96, "end": 1409.2, "text": " act for the public as relay nodes. And the problem is you don't want to relay any traffic for", "tokens": [605, 337, 264, 1908, 382, 24214, 13891, 13, 400, 264, 1154, 307, 291, 500, 380, 528, 281, 24214, 604, 6419, 337], "temperature": 0.0, "avg_logprob": -0.12968072112725706, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0004085893160663545}, {"id": 208, "seek": 139520, "start": 1409.2, "end": 1414.24, "text": " anyone, but you want to make this really restricted in terms of how much traffic, how long. If you", "tokens": [2878, 11, 457, 291, 528, 281, 652, 341, 534, 20608, 294, 2115, 295, 577, 709, 6419, 11, 577, 938, 13, 759, 291], "temperature": 0.0, "avg_logprob": -0.12968072112725706, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0004085893160663545}, {"id": 209, "seek": 139520, "start": 1414.24, "end": 1420.96, "text": " run a public node, you don't want to be the next relay node for everyone out there. And then,", "tokens": [1190, 257, 1908, 9984, 11, 291, 500, 380, 528, 281, 312, 264, 958, 24214, 9984, 337, 1518, 484, 456, 13, 400, 550, 11], "temperature": 0.0, "avg_logprob": -0.12968072112725706, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0004085893160663545}, {"id": 210, "seek": 142096, "start": 1420.96, "end": 1428.68, "text": " what we built here is very much TCP specific, but it also works well with UDP. We need the", "tokens": [437, 321, 3094, 510, 307, 588, 709, 48965, 2685, 11, 457, 309, 611, 1985, 731, 365, 624, 11373, 13, 492, 643, 264], "temperature": 0.0, "avg_logprob": -0.1512657936583174, "compression_ratio": 1.5381355932203389, "no_speech_prob": 8.59668871271424e-05}, {"id": 211, "seek": 142096, "start": 1428.68, "end": 1433.8, "text": " synchronization. And as far as I know, at least the WebRTC stack is very focused on UDP, where", "tokens": [19331, 2144, 13, 400, 382, 1400, 382, 286, 458, 11, 412, 1935, 264, 9573, 49, 18238, 8630, 307, 588, 5178, 322, 624, 11373, 11, 689], "temperature": 0.0, "avg_logprob": -0.1512657936583174, "compression_ratio": 1.5381355932203389, "no_speech_prob": 8.59668871271424e-05}, {"id": 212, "seek": 142096, "start": 1433.8, "end": 1440.6000000000001, "text": " timing doesn't matter as much. So you saw the timing protocol, right? And that is very TCP", "tokens": [10822, 1177, 380, 1871, 382, 709, 13, 407, 291, 1866, 264, 10822, 10336, 11, 558, 30, 400, 300, 307, 588, 48965], "temperature": 0.0, "avg_logprob": -0.1512657936583174, "compression_ratio": 1.5381355932203389, "no_speech_prob": 8.59668871271424e-05}, {"id": 213, "seek": 142096, "start": 1440.6000000000001, "end": 1446.6000000000001, "text": " specific, where we want a TCP simultaneous connect, which allows two sends to actually", "tokens": [2685, 11, 689, 321, 528, 257, 48965, 46218, 1745, 11, 597, 4045, 732, 14790, 281, 767], "temperature": 0.0, "avg_logprob": -0.1512657936583174, "compression_ratio": 1.5381355932203389, "no_speech_prob": 8.59668871271424e-05}, {"id": 214, "seek": 144660, "start": 1446.6, "end": 1460.28, "text": " result in a single TCP connection. This is for your analysis. I guess a lot of this depends on", "tokens": [1874, 294, 257, 2167, 48965, 4984, 13, 639, 307, 337, 428, 5215, 13, 286, 2041, 257, 688, 295, 341, 5946, 322], "temperature": 0.0, "avg_logprob": -0.1672667005787725, "compression_ratio": 1.35, "no_speech_prob": 0.00031547763501293957}, {"id": 215, "seek": 144660, "start": 1460.28, "end": 1467.9199999999998, "text": " the default configurations of the firewall. Did you kind of find out what are the Brian's type", "tokens": [264, 7576, 31493, 295, 264, 36109, 13, 2589, 291, 733, 295, 915, 484, 437, 366, 264, 10765, 311, 2010], "temperature": 0.0, "avg_logprob": -0.1672667005787725, "compression_ratio": 1.35, "no_speech_prob": 0.00031547763501293957}, {"id": 216, "seek": 146792, "start": 1467.92, "end": 1476.88, "text": " of firewalls or configurations that stops whole punching in your research? So, yeah. So, not in", "tokens": [295, 36109, 82, 420, 31493, 300, 10094, 1379, 34866, 294, 428, 2132, 30, 407, 11, 1338, 13, 407, 11, 406, 294], "temperature": 0.0, "avg_logprob": -0.1930590099758572, "compression_ratio": 1.6016949152542372, "no_speech_prob": 6.0750080592697486e-05}, {"id": 217, "seek": 146792, "start": 1476.88, "end": 1481.48, "text": " its entirety, but what we did is, so people that signed up for the measurement campaign gave us", "tokens": [1080, 31557, 11, 457, 437, 321, 630, 307, 11, 370, 561, 300, 8175, 493, 337, 264, 13160, 5129, 2729, 505], "temperature": 0.0, "avg_logprob": -0.1930590099758572, "compression_ratio": 1.6016949152542372, "no_speech_prob": 6.0750080592697486e-05}, {"id": 218, "seek": 146792, "start": 1481.48, "end": 1487.76, "text": " information about the networks. And so, if we find something fishy in the data, we could also", "tokens": [1589, 466, 264, 9590, 13, 400, 370, 11, 498, 321, 915, 746, 41991, 294, 264, 1412, 11, 321, 727, 611], "temperature": 0.0, "avg_logprob": -0.1930590099758572, "compression_ratio": 1.6016949152542372, "no_speech_prob": 6.0750080592697486e-05}, {"id": 219, "seek": 146792, "start": 1487.76, "end": 1493.8400000000001, "text": " reach out to them and ask what's the firewall setup in your specific network. We also gather", "tokens": [2524, 484, 281, 552, 293, 1029, 437, 311, 264, 36109, 8657, 294, 428, 2685, 3209, 13, 492, 611, 5448], "temperature": 0.0, "avg_logprob": -0.1930590099758572, "compression_ratio": 1.6016949152542372, "no_speech_prob": 6.0750080592697486e-05}, {"id": 220, "seek": 149384, "start": 1493.84, "end": 1500.0, "text": " data about port mappings that are in place. So, what LiPTP host tries to do is establish a port", "tokens": [1412, 466, 2436, 463, 28968, 300, 366, 294, 1081, 13, 407, 11, 437, 8349, 47, 16804, 3975, 9898, 281, 360, 307, 8327, 257, 2436], "temperature": 0.0, "avg_logprob": -0.2001539036847543, "compression_ratio": 1.5414364640883977, "no_speech_prob": 3.068170553888194e-05}, {"id": 221, "seek": 149384, "start": 1500.0, "end": 1508.08, "text": " mapping inside your router. And this is also reported back. And what we also did is try to", "tokens": [18350, 1854, 428, 22492, 13, 400, 341, 307, 611, 7055, 646, 13, 400, 437, 321, 611, 630, 307, 853, 281], "temperature": 0.0, "avg_logprob": -0.2001539036847543, "compression_ratio": 1.5414364640883977, "no_speech_prob": 3.068170553888194e-05}, {"id": 222, "seek": 149384, "start": 1508.08, "end": 1518.6399999999999, "text": " query the login page from these routers and get some information about what kind of firewall", "tokens": [14581, 264, 24276, 3028, 490, 613, 4020, 433, 293, 483, 512, 1589, 466, 437, 733, 295, 36109], "temperature": 0.0, "avg_logprob": -0.2001539036847543, "compression_ratio": 1.5414364640883977, "no_speech_prob": 3.068170553888194e-05}, {"id": 223, "seek": 151864, "start": 1518.64, "end": 1527.2, "text": " router actually was preventing you from connecting to someone else. So, these are the data points", "tokens": [22492, 767, 390, 19965, 291, 490, 11015, 281, 1580, 1646, 13, 407, 11, 613, 366, 264, 1412, 2793], "temperature": 0.0, "avg_logprob": -0.1484742520460442, "compression_ratio": 1.532258064516129, "no_speech_prob": 4.7221350541803986e-05}, {"id": 224, "seek": 151864, "start": 1527.2, "end": 1534.88, "text": " that we have to get some conclusions around this. But more than this, we don't have. But I think", "tokens": [300, 321, 362, 281, 483, 512, 22865, 926, 341, 13, 583, 544, 813, 341, 11, 321, 500, 380, 362, 13, 583, 286, 519], "temperature": 0.0, "avg_logprob": -0.1484742520460442, "compression_ratio": 1.532258064516129, "no_speech_prob": 4.7221350541803986e-05}, {"id": 225, "seek": 151864, "start": 1534.88, "end": 1544.16, "text": " this is already pretty conclusive to a wide variety of analysis. What I was just wondering", "tokens": [341, 307, 1217, 1238, 1588, 7233, 281, 257, 4874, 5673, 295, 5215, 13, 708, 286, 390, 445, 6359], "temperature": 0.0, "avg_logprob": -0.1484742520460442, "compression_ratio": 1.532258064516129, "no_speech_prob": 4.7221350541803986e-05}, {"id": 226, "seek": 154416, "start": 1544.16, "end": 1550.16, "text": " about is, do you have any data? How many clients actually were behind the net? So, all these", "tokens": [466, 307, 11, 360, 291, 362, 604, 1412, 30, 1012, 867, 6982, 767, 645, 2261, 264, 2533, 30, 407, 11, 439, 613], "temperature": 0.0, "avg_logprob": -0.15642640923941006, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.00010465009836480021}, {"id": 227, "seek": 154416, "start": 1550.16, "end": 1557.2, "text": " clients that the Honeypot detected were only, so were clients that are behind the net. So,", "tokens": [6982, 300, 264, 16187, 17698, 21896, 645, 787, 11, 370, 645, 6982, 300, 366, 2261, 264, 2533, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.15642640923941006, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.00010465009836480021}, {"id": 228, "seek": 154416, "start": 1557.2, "end": 1561.6000000000001, "text": " these are all LiPTP hosts. And with the default configuration of LiPTP hosts, if they only", "tokens": [613, 366, 439, 8349, 47, 16804, 21573, 13, 400, 365, 264, 7576, 11694, 295, 8349, 47, 16804, 21573, 11, 498, 436, 787], "temperature": 0.0, "avg_logprob": -0.15642640923941006, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.00010465009836480021}, {"id": 229, "seek": 154416, "start": 1561.6000000000001, "end": 1569.28, "text": " announce relay addresses, this means that they must be not publicly reachable, which is for us", "tokens": [7478, 24214, 16862, 11, 341, 1355, 300, 436, 1633, 312, 406, 14843, 2524, 712, 11, 597, 307, 337, 505], "temperature": 0.0, "avg_logprob": -0.15642640923941006, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.00010465009836480021}, {"id": 230, "seek": 156928, "start": 1569.28, "end": 1575.2, "text": " equivalent with being behind the net. So, yeah, it should be. There's probably some error there.", "tokens": [10344, 365, 885, 2261, 264, 2533, 13, 407, 11, 1338, 11, 309, 820, 312, 13, 821, 311, 1391, 512, 6713, 456, 13], "temperature": 0.0, "avg_logprob": -0.17422880337932917, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.00010043221845990047}, {"id": 231, "seek": 156928, "start": 1576.3999999999999, "end": 1581.52, "text": " So, then all of the IPv6 kind of hosts you were trying to connect to also were behind the net.", "tokens": [407, 11, 550, 439, 295, 264, 8671, 85, 21, 733, 295, 21573, 291, 645, 1382, 281, 1745, 281, 611, 645, 2261, 264, 2533, 13], "temperature": 0.0, "avg_logprob": -0.17422880337932917, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.00010043221845990047}, {"id": 232, "seek": 156928, "start": 1581.52, "end": 1586.8799999999999, "text": " Kind of IPv6. Yes, yes. And this is the interesting thing. So, I cannot explain this yet. Maybe it's", "tokens": [9242, 295, 8671, 85, 21, 13, 1079, 11, 2086, 13, 400, 341, 307, 264, 1880, 551, 13, 407, 11, 286, 2644, 2903, 341, 1939, 13, 2704, 309, 311], "temperature": 0.0, "avg_logprob": -0.17422880337932917, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.00010043221845990047}, {"id": 233, "seek": 156928, "start": 1586.8799999999999, "end": 1591.52, "text": " a measurement like a measurement error from us. Maybe it's some, as I said, inherent property", "tokens": [257, 13160, 411, 257, 13160, 6713, 490, 505, 13, 2704, 309, 311, 512, 11, 382, 286, 848, 11, 26387, 4707], "temperature": 0.0, "avg_logprob": -0.17422880337932917, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.00010043221845990047}, {"id": 234, "seek": 156928, "start": 1591.52, "end": 1597.6, "text": " to something. Maybe it's a protocol error. I don't know. And this is the interesting stuff", "tokens": [281, 746, 13, 2704, 309, 311, 257, 10336, 6713, 13, 286, 500, 380, 458, 13, 400, 341, 307, 264, 1880, 1507], "temperature": 0.0, "avg_logprob": -0.17422880337932917, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.00010043221845990047}, {"id": 235, "seek": 159760, "start": 1597.6, "end": 1600.7199999999998, "text": " in these kinds of things. Thanks. I'm very curious. Yeah.", "tokens": [294, 613, 3685, 295, 721, 13, 2561, 13, 286, 478, 588, 6369, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1450598398844401, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.000278486026218161}, {"id": 236, "seek": 159760, "start": 1605.76, "end": 1611.84, "text": " I was wondering, does it also work with multiple nets? Can you open through two nets?", "tokens": [286, 390, 6359, 11, 775, 309, 611, 589, 365, 3866, 36170, 30, 1664, 291, 1269, 807, 732, 36170, 30], "temperature": 0.0, "avg_logprob": -0.1450598398844401, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.000278486026218161}, {"id": 237, "seek": 159760, "start": 1619.9199999999998, "end": 1624.9599999999998, "text": " So, if another friend of mine who I convinced to run these clients actually was running behind", "tokens": [407, 11, 498, 1071, 1277, 295, 3892, 567, 286, 12561, 281, 1190, 613, 6982, 767, 390, 2614, 2261], "temperature": 0.0, "avg_logprob": -0.1450598398844401, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.000278486026218161}, {"id": 238, "seek": 162496, "start": 1624.96, "end": 1632.8, "text": " two nets and it was working. But I'm not sure how many people actually ran behind two nets.", "tokens": [732, 36170, 293, 309, 390, 1364, 13, 583, 286, 478, 406, 988, 577, 867, 561, 767, 5872, 2261, 732, 36170, 13], "temperature": 0.0, "avg_logprob": -0.14922196095384013, "compression_ratio": 1.7216117216117217, "no_speech_prob": 9.282108658226207e-05}, {"id": 239, "seek": 162496, "start": 1632.8, "end": 1637.76, "text": " But in theory, yeah, maybe Max, you can explain this. Yes. So, right now, we don't have really a", "tokens": [583, 294, 5261, 11, 1338, 11, 1310, 7402, 11, 291, 393, 2903, 341, 13, 1079, 13, 407, 11, 558, 586, 11, 321, 500, 380, 362, 534, 257], "temperature": 0.0, "avg_logprob": -0.14922196095384013, "compression_ratio": 1.7216117216117217, "no_speech_prob": 9.282108658226207e-05}, {"id": 240, "seek": 162496, "start": 1637.76, "end": 1643.1200000000001, "text": " lot of data about two nets. And also, we don't have the data, which I think was called needle.", "tokens": [688, 295, 1412, 466, 732, 36170, 13, 400, 611, 11, 321, 500, 380, 362, 264, 1412, 11, 597, 286, 519, 390, 1219, 11037, 13], "temperature": 0.0, "avg_logprob": -0.14922196095384013, "compression_ratio": 1.7216117216117217, "no_speech_prob": 9.282108658226207e-05}, {"id": 241, "seek": 162496, "start": 1643.92, "end": 1648.96, "text": " I don't quite know where you're within the same network. But you don't know that you're next to", "tokens": [286, 500, 380, 1596, 458, 689, 291, 434, 1951, 264, 912, 3209, 13, 583, 291, 500, 380, 458, 300, 291, 434, 958, 281], "temperature": 0.0, "avg_logprob": -0.14922196095384013, "compression_ratio": 1.7216117216117217, "no_speech_prob": 9.282108658226207e-05}, {"id": 242, "seek": 162496, "start": 1648.96, "end": 1653.2, "text": " each other. And you actually want a hole punch through your own net, even though you can't", "tokens": [1184, 661, 13, 400, 291, 767, 528, 257, 5458, 8135, 807, 428, 1065, 2533, 11, 754, 1673, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.14922196095384013, "compression_ratio": 1.7216117216117217, "no_speech_prob": 9.282108658226207e-05}, {"id": 243, "seek": 165320, "start": 1653.2, "end": 1659.52, "text": " connect to each other. So, there's some challenges. Do we still have time for another question?", "tokens": [1745, 281, 1184, 661, 13, 407, 11, 456, 311, 512, 4759, 13, 1144, 321, 920, 362, 565, 337, 1071, 1168, 30], "temperature": 0.0, "avg_logprob": -0.15555207158478213, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00015856926620472223}, {"id": 244, "seek": 165320, "start": 1672.24, "end": 1676.88, "text": " So, you said that for UDP it should work. Similarly, did you do any experiments with that?", "tokens": [407, 11, 291, 848, 300, 337, 624, 11373, 309, 820, 589, 13, 13157, 11, 630, 291, 360, 604, 12050, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.15555207158478213, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00015856926620472223}, {"id": 245, "seek": 165320, "start": 1676.88, "end": 1681.3600000000001, "text": " Because in the past, we had a custom UDP hole punching thing and the routers were pretty", "tokens": [1436, 294, 264, 1791, 11, 321, 632, 257, 2375, 624, 11373, 5458, 34866, 551, 293, 264, 4020, 433, 645, 1238], "temperature": 0.0, "avg_logprob": -0.15555207158478213, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00015856926620472223}, {"id": 246, "seek": 168136, "start": 1681.36, "end": 1685.28, "text": " branded. They forgot the mapping within 20 seconds or something.", "tokens": [38510, 13, 814, 5298, 264, 18350, 1951, 945, 3949, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.17421178817749022, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.00017445511184632778}, {"id": 247, "seek": 168136, "start": 1687.52, "end": 1692.9599999999998, "text": " Yeah. So, we run this measurement campaign on TCP and QIC. And QIC in the end is just UDP.", "tokens": [865, 13, 407, 11, 321, 1190, 341, 13160, 5129, 322, 48965, 293, 1249, 2532, 13, 400, 1249, 2532, 294, 264, 917, 307, 445, 624, 11373, 13], "temperature": 0.0, "avg_logprob": -0.17421178817749022, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.00017445511184632778}, {"id": 248, "seek": 168136, "start": 1692.9599999999998, "end": 1699.4399999999998, "text": " And what we do is something similar to STUN in the ICE suit, where we continuously try to keep", "tokens": [400, 437, 321, 360, 307, 746, 2531, 281, 4904, 3979, 294, 264, 43337, 5722, 11, 689, 321, 15684, 853, 281, 1066], "temperature": 0.0, "avg_logprob": -0.17421178817749022, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.00017445511184632778}, {"id": 249, "seek": 168136, "start": 1699.4399999999998, "end": 1707.36, "text": " our mapping up. And then on nets that do endpoint independent mappings, that actually helps. So,", "tokens": [527, 18350, 493, 13, 400, 550, 322, 36170, 300, 360, 35795, 6695, 463, 28968, 11, 300, 767, 3665, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.17421178817749022, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.00017445511184632778}, {"id": 250, "seek": 170736, "start": 1707.36, "end": 1712.24, "text": " as long as we keep that up for, like, I don't know, every 10 seconds or so, then our mapping", "tokens": [382, 938, 382, 321, 1066, 300, 493, 337, 11, 411, 11, 286, 500, 380, 458, 11, 633, 1266, 3949, 420, 370, 11, 550, 527, 18350], "temperature": 0.0, "avg_logprob": -0.13212432861328124, "compression_ratio": 1.1951219512195121, "no_speech_prob": 0.00011510150216054171}, {"id": 251, "seek": 171224, "start": 1712.24, "end": 1740.96, "text": " survives, even on UDP. Okay, cool. Thank you very much.", "tokens": [50364, 46231, 11, 754, 322, 624, 11373, 13, 1033, 11, 1627, 13, 1044, 291, 588, 709, 13, 51800], "temperature": 0.0, "avg_logprob": -0.21661028109098734, "compression_ratio": 0.873015873015873, "no_speech_prob": 7.095396722434089e-05}], "language": "en"}