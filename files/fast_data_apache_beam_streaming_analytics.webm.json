{"text": " Okay, you're good to go. Thank you. Okay. Thanks. I think the technical issues are now solved. Thanks again, everyone, for being here. So I'm going to talk today about Apache VIN. Apache VIN is a framework for data processing that runs on top of several platforms, and it's especially meant for doing streaming analytics. So Javier already introduced me. My name is Israel. So I work as a cloud data engineer in Google Cloud, helping customers doing data engineering on top of Google Cloud. A lot of the work that I do is actually helping customers with Apache VIN, and particularly Dataflow, which is our runner for Apache VIN. So Apache VIN, what is it? Apache VIN is a framework for data processing. It allows to run data pipelines, like Flink, like Spark, like so many other big data systems. It has two main features. The first one is that it's a unified computing model for batch and streaming. Any pipeline that you have written in Apache VIN for a batch use case, you may easily use it in streaming as well. So the same code, you reuse all that code, and you have to add some small additions that we are going to talk about in a bit. And the other main feature is that it runs everywhere, and you can run, you can write your pipeline in many different languages, for some definition of everywhere. So you can write your pipeline in Java, in Python, in Go. You may also run your pipeline in any of the programming languages of the Java Victor machine, for instance. So I have here highlighted Scala, because that's a framework called SIO, the Don't Buy Spotify, on top of Apache VIN for, let's say, Scala native development of pipeline. So you don't have to use Java looking code in Scala. So it's a functional code. There are lots of people using, for instance, Kotlin also, on top of the Java Victor machine with the Java VIN SDK. So that's about the programming language that you may use. And you may run a VIN pipeline on top of runners. So there's a direct runner for running and local testing pipelines. It's not meant for, let's say, to be used, let's say, with real world use cases. But then you can run your pipeline on top of Dataflow, on top of Flink, on top of Hazelcast, Spark, many different runners. So basically, when you write the pipeline in Apache VIN, you are not tied to the platform where you're running. So you may move it to different platforms, which are minor comments. So Apache VIN is a theoretical model for computing. Not all the runners implement the model, let's say, to the same degree of extent. So right now, as of now, I would say Dataflow and Flink are probably the ones that are fully covered, or the runners may have some gaps. See the example. Hadoop. You may run Apache VIN pipeline on top of Hadoop also, but you cannot run streaming on top of Hadoop because it doesn't support streaming. So it also depends on the capability of the runner. So what you're able to do with VIN, it depends on the capabilities of the runner. So there's no magic. So if VIN is batch and streaming, but if your platform doesn't support streaming, for instance, so you cannot do streaming. So let's talk about streaming. What's the problem with streaming? It's extremely interesting. So in streaming, you are getting data, a lot of data, continuously. There's no beginning, and there's no end. So you cannot know in advance where are the boundaries of your data. It's coming continuously from many different places. Think, I don't know. Like you are designing a mobile application for a game or whatever, and people are using it and sending events to your systems every once in a while. So because the application is deployed in the wild world, data will come. Who knows how? So it will come out of order. So some users will be in the underground and without the phone coverage. They still try attempting to send in events, and then they will send the events late. Like, for instance, here, let's see if I can put the pointer. So this is data that is supposed to be produced around 8 in the morning. Maybe there are network latencies and so on, but more or less you get it around 8 in the morning in your system. So this is the time where you are seeing the data in your system. But for whatever reason, you may also get very late data. And depending on what you want to do, you may want to process this data as it was produced, as you are receiving it. So this is actually the problem with micro-batching. So I remember when I started hearing about streaming in the first days, many years ago, a lot of people said, oh, Spark, I don't like it because it does micro-batching. It doesn't do real streaming. I had no clue what they meant. It's like, well, you have to group things to process it somehow. So if the data is infinite, you will need to process it. The problem with micro-batching, which is not happening in Spark anymore, so this was really ancient times, the problem with micro-batching is that you are doing the batches as you see the data. And then you may have the data that belongs together in buckets that are separate. Like, for instance, this was data that was produced at 8 in the morning. If you are doing buckets of one hour, so well, you may capture here this message, but then if you have late data, you will capture it in a bucket that doesn't belong, that element doesn't belong with the rest of the elements there if you want to process them together. So you need to solve this problem of lack of order in streaming. And this is what you can easily solve with Apache Bin. Let's talk about the watermark. There are many ways of doing stream processing. One of the most popular is using a concept of watermark. There is no one dimension of time. There are two dimensions of times. At least that is the event time, the moment in which the data was produced, and that is the processing time, the moment in which you see the data. They will never be the same. They can be really close sometimes, but you cannot grant it how close or how far you are going to be from that moment. So we put time in two dimensions. So the ideal is, for instance, like this straight line in blue, for sure, this zone is impossible. You cannot see data before it's produced, or not yet at least according to the laws of physics. But then most likely what will happen is that you will have some delay. Sometimes it will be closer to the ideal, sometimes it will be farther from the ideal. And you need to take this into account. So let's see an example that might be a little bit more telling. So Star Wars. So have you ever watched a Star Wars movie? So Star Wars were released out of order, out of order. So the first movie was Episode 4. This is purely streaming. This is what happens in streaming. You are expecting events at the beginning of the session, in the middle of the session, the end of the session, and then you get end of the session, middle of the session, beginning of the session. And you need to reorder things. Depending on what you want to do, you may need to reorder. If you don't care, look, I don't want to, I don't care. I just want to count how many movies per year were released. Well, you don't need the event time. But if you want to reconstruct the story who did what before or after, what happened, so you need to actually be able to reconstruct that time. So the time where the movies were released, its processing time, event time is the time in which actually the events are happening. And this is the kind of problems that we can solve using Apache VIN or Flink or many other streaming systems. Let's see how we can deal with this. The classical way is using windowing. Windowing is grouping things based on temporal properties. When we do a data pipeline, we need to solve one question, which is what we are going to compute. But if you want to do this in streaming and group things based on temporal properties, you need to answer three additional questions. Where, when, and how. Let's see some details. What? This is easy. We are going to be aggregating. So this is Java code, and it's in Java here as an example. So it's Apache VIN API. I haven't entered into details. There's a link at the end with more details. So don't mind the details right now. So we are aggregating things together. So this is what we are happening. We are not doing any kind of temporal based logic yet. We are just aggregating stuff together. So we are summing up all the numbers here. So this is the operation that we are doing. Same as in batch. This is in batch. So imagine that we are getting this batch. The problem is that when we are working in streaming, we don't see the full data at once. And we need to produce output at some point so we cannot wait forever. So we need to decide how to group things together. So for instance here, we are going to group things in windows of two minutes. But the windows of two minutes are not in processing time. They are in event time. For instance, so here, this message here, so this message here, we see it around 12, and we put it in the window of 12. But this message over here, so this was received between 12.08 and 12.09. And we are able still to attribute it to assign it to the window between 12 and 12.02 in event time. Because, well, so we can wait for late data and put it in the right window, despite the message being quite late compared to the processing time. And same with the rest of windows. Now the question is, okay, good. So you are waiting until 12.08. So what if your message shows up at 8 p.m.? What do you do? Like eight hours after. So we need to do another decision, okay? So we have already made the decision on how we are going to group things together. Here is with easy windows. There are more windows in Apache bin, not entering into details right now. But now we need to decide how long do we wait, okay? So we are going to wait until the watermark. Okay, the watermark is this relationship between processing time and event time that in the case of bin and depending on the runner. It's calculated and estimated on the fly as data goes through our pipeline. And it's this curve is estimated. And when you trespass the watermark, you have a certain degree of warranty that your data is complete, okay? A certain degree of warranty, okay? It cannot be granted because, well, so the future cannot be known, okay? So we cannot travel in time, okay? So here, for instance, so we are processing data in the watermark and the nine, this number here that we were processing before, now it's left out of the window. So what does it mean if we are processing data? We were summing up numbers, that number, that nine, we are not counting it. As soon as we see it in our pipeline, it will be dropped, like lost, okay? So the pipeline will ignore it, okay? And it may make sense, okay? So you cannot wait forever. At some point, you will have to stop and move on, okay? But maybe you want to take it into account, okay? So maybe you, I don't know, like this is a billing, invoicing thing and every penny counts, okay? So then you need to process it. Well, you have to take yet another decision. How we are gonna wait for late data and how we are gonna actually update the data, okay? Here, I'm summing numbers. It's easy, commutative, associative, really no big deal, okay? So I can do it like, say, I can do it, I can do it like a monoid in big data processing, so I can just take the aggregation, the previous aggregation and keep aggregating. I don't need to keep all the numbers that I have seen so far, so it is easy. In other cases, for any non-associative, non-commutative operation, so you may need to have actually full data to produce an update, okay? And if you are working in streaming, maybe you don't want to accumulate all the data, okay? Because that will increase the amount of resources that you will need for your pipeline. It will have impact in performance, latency and so on. So here, we are accumulating because the operation allows it and we are actually waiting for late data, okay? So now, we are waiting for late data, but we don't want to wait forever. We want to have some numbers, okay? So we are actually producing several outputs per window, okay? So like for instance here, continuing with the first, so when the watermark is trespassed, we produce an output, okay? And then when we see the new number, so we produce the output. We produce it really late, okay? But well, so we cannot make magic, okay? So this is when we see the data, so we cannot process it earlier than this, okay? We may actually decide to produce data, some output, even before the watermark, because the watermark can be really slow. It depends on the pace of the updates of the data. If for whatever reason, users are sending your data with a lot of lateness, the watermark can progress really slowly, okay? And so the watermark, how you produce output is always a trade-off in the streaming between completeness and latency. You need to make a decision, okay? So here, we put an early trigger. So we're producing output soon, low latency, but it's incomplete, because well, so later on we're gonna keep seeing numbers until the watermark. Good. So basically, this is streaming in Apache Bin in 10 minutes. This is a lot of information, explained very quickly. If you want to get deeper, if you want to get deeper, there's this example here, okay? So in Java and in Python, so it's available in the two languages, and you can see everything that we have seen in the previous slides with all details, okay? And you may run this locally if you want, so you don't have to have like an environment, so like a cloud environment, a cluster, or a stream process or anything like that, so it may run locally with some synthetic data, made-up data, okay? Now, this is the classic way of doing streaming in Apache Bin. This has been around for years already, okay? So this is the same model that is implemented in Spark, it's the same model that is implemented in Flink, so they are all kind of similar. There are other things that you can also do in Apache Bin in streaming, like anything that you can do in Apache Bin, you can also do it in streaming, and I'm gonna highlight here a couple of those, okay? I'm leaving out a lot of stuff, because, well, so time is limited, and leave it out for instance SQL, so that was a great talk by Timo focusing on SQL, so you can also do SQL in Apache Bin if you want in streaming, okay? So similar examples to what Timo did, and you can actually run that on Flink if you want, okay? So it may make sense if you, well, I don't know, at some point you want to move away from Flink to Dataflow, you want to move away from Dataflow to Spark, so in order to have this portability. One thing that you can do in streaming is stateful functions, and stateful functions are very interesting for windowing between quotes that doesn't depend on time. Very typically, I work with customers, like all these windowing trigger things, it's super interesting, but look, whenever I see a message of this type, I want to have all the messages that I have seen so far in a group and do these calculations, and I don't care about time, okay? I don't care about grouping things in time. I want to group things by some logic, okay? I'm gonna give you a predicate, you pass a message, if the message fulfills a condition, I want to close the previous window and start a new one. How can you do that in Apache bin? You can do that with stateful functions, okay? Stateful functions, so here we have some input, here we have a map, it's called a part doing Apache bin, and we do some transformation, and we want to accumulate a state here, okay? So depending on what we see at some point, we do something else, and this is mutable state, okay? In a system like Dataflow, like Flink, like all the systems where Apache bin runs, having state, mutable state in a streaming that is computed in a consistent way is extremely difficult, okay? One way to shoot yourself in your feet with systems like this in streaming is trying to keep accumulating state using some kind of external system, okay? Because runners will have... sometimes will have issues that will be errors, that will be retries, infrastructure will die, you will have auto-scaling. There are all kinds of situations that the runner may want to retract the computation and recompute again, okay? And then in these kinds of situations, having any kind of external system for mutable state, it's complex, okay? It's doable, okay? You may have, and you will have with Apache bin in any kind of the runners that you can run, you will have this end-to-end exactly once processing, but this end-to-end exactly once processing doesn't mean that your code is going to be executed exactly once. It may be executed more than once, okay? This is what makes maintaining external state to a pipeline complex. But if the state is internal to the pipeline, then, well, so the system itself can, let's say, take care of the problems of reprocessing and maintain a mutable state in a consistent way. So this is where it's a stateful function in Apache bin, and you can use it for use cases like this, okay? For instance, say that I want to produce windows between quotes based on some kind of property. So I keep seeing messages, okay, that I keep processing, okay? And then I keep accumulating the messages in some state, okay? I maintain a buffer, like I keep every single message that I see and I count, okay? Because, well, the buffer cannot, so the buffer must have some boundaries, okay? So because this is local state that is maintaining the machine in the worker, in the executor where you are running, and the executor will have limited resources. It might be very large resources, but limited anyways, okay? So you keep accumulating, and then you keep processing here, for instance. So typically, you can use this, for instance, batching to call in an external service, but you can also do here, whenever I see a specific type of message, I emit some output, okay? I emit some output, and then I have applied a window. All the messages that I have in the buffer, I tag a new session ID, a new window ID, and then I emit them. I hold them for a while until I see the right message that I need, and then I emit them. There are two problems here. We want to, so customers always think that streaming is complex, and they want to get away of all the temporal-based calculations, okay? It's so complex, so messy. Look, my algorithm is really much simpler, but it is not. So you are in the streaming, so you cannot ignore time, okay? You have situations where you will see the messages out of order, and you will see, you will have situations where you will not see the messages for a while, okay? And then you need to decide what to do in these two cases, even if you don't want to, okay? What happens when I see out of order? You may say, I don't care, unlikely, but well, in some situations, it might be true, okay? Or you may have to wait, like, some timer in order to give room for late data to arrive into your code and actually produce the actual output, okay? So this would be an event-time timer, okay? Look, in event-time, you are going to see the messages in order, okay? So wait 30 seconds, two minutes, and so on. And then when you have seen all the messages, it's the moment in which you apply the session. That's called an event-time timer. And you may have also problems of staleness, okay? I'm waiting for the end of my session, but I've not seen messages. I haven't seen messages in the last five minutes. In processing time, okay? The problem with event-time is that it depends on the progress of the watermark. But if you stop seeing messages, the watermark will stop advancing. The watermark is always estimated in being runners or normally estimated as the time stamp of the oldest message waiting to be processed, okay? So literally you may stop your pipeline waiting forever for some data that maybe it will never arrive, okay? So processing time-time will stop this problem, okay? After 10 minutes, like, measure with a clock. If nothing comes, I don't care about the watermark. I don't care. Keep going, okay? So data has been lost for whatever reason, and we cannot wait forever. So this is a stateful function, and it's also very useful in streaming because it allows you to apply logic that goes beyond the temporal properties that we have seen in the previous slides. And here you have some examples and links. The slides are already available through the first-time website, so I encourage you to have a look at these examples. What else can I do in streaming? Machine learning inference, okay? So there are many ways to do machine learning inference in streaming at a scale, okay? Many of those quite expensive. So you can deploy endpoints in cloud platforms, with GPUs, with a lot of stuff, okay? And normally, so, well, so those are... those solve a lot of functionality for you, but they are expensive. So what if you want to apply machine learning inference in a pipeline, in Apache Bin? Well, you could do that, okay? You could be thinking, well, I can do that. So I can, I don't know, like, import TensorFlow, load the model, apply it, so you could do a lot of stuff, okay, yourself. But this is already solved for you in Apache Bin, okay? So you can run machine learning inference with the so-called run inference... run inference transform, okay? So we see it here. So right now, it has, let's say, out-of-the-box support for PyTorch, TensorFlow, as I can learn, with more coming. When you're running a distributed system and you want to apply a model, each one of the workers in the distributed system will have its own memory. So Apache Bin runs on top of share-nothing architecture, okay, like fling, dataflow, spark. Workers are independent of each other. They don't share any common state. The state that we have seen before is actually maintained per key and per window if we apply the per window. It's totally local for the worker, and two workers cannot share a state. But the model, we don't want to instantiate a model if we have 100 workers 100 times because the model is going to be the same for every worker, right? So the model hasn't changed. The model is actually read-only. Run inference solves these problems by having some state that is shared across all the workers and it's transparent for you. So this is something that you can always implement in a distributed system, but it's complex. This is the problem that is solved with run inference. It's only one copy of the model per, let's say, machine where you're running, okay? So in memory, okay? Because, well, you need always to make an instance in memory to be able to apply it. But if you have, I don't know, like 100 threads, 100 sub-workers, 100 CPUs inside the same machine, you will not have 100 copies of the model. Regardless of, let's say, what's the computation model of how the runner is implemented on top of the machines. That will be only one copy in the memory of the machine. So this is the problem that is solved with run inference, okay? So if you want to apply a streaming inference, it's a very convenient way of doing this with very little code. And depending on the runner, this is a possibility in data flow, this is also a possibility if you are running on top of Kubernetes in a runner that supports Kubernetes, like, for instance, Flink. You can do also hinting of the resources that your transformation is gonna need, okay? Hinting of transformations could be, look, this transformation is gonna need this amount of memory, minimal, okay? But hinting could also be, look, this is a step that is running ML inference. So use a GPU for this step, okay? And then the runner will take care of making sure that the step that is running a virtual machine, let's say, matches the infrastructure hints that you provide through the code, and you will have, let's say, different types of nodes for different types of transformations. One of the problems of shared nothing architecture is that all the workers are alike. All the workers are the same. With this, you can have different types of workers for different kinds of transformations, which, let's say, in terms of cost, it's better. I don't have to say optimal, so maybe that's a better alternative. But basically, you use GPUs in the workers, where you need to use GPUs, you don't use them in the workers where you don't need to use them. And you don't have to worry about assigning work to different workers, that's actually done by the runner, automatically, with these hints, okay? If you want to know more, here you have some links. So I have only five minutes left, so I'm leaving the best for the end of the presentation. Great, look, Israel, you showed San Java at the beginning, now you tell me ML inference is so cool, but it's Python, right? So it's PyTorch, TensorFlow, Scikit-learn, it's all Python. One of the things that you can do in ApacheVinus is in cross-language transforms. Anything that you have available in any of the SDKs, in any language, you may use it in any other language, as long as the runner supports this, okay? So it's not supported by all the runners, but it's supported by the main runners, okay? So basically, run inference may be used in Java. If you want to use any transformation from any language, you have to add some boilerplate code, not so much, but a little bit. This is already done, let's say, for the main transforms that are most popular, all that they say, that make more sense to be used in different languages, like a map, well, using a map from Java in Python doesn't really make sense, okay? Using run inference makes. Using connectors, input-output connectors from another SDK in Python, for instance, makes sense, because the amount of input-output connectors that you have per SDK is not the same. So you may write, I don't know, like, to databases in Java and to message queues in Python, but maybe you don't have the same functionalities in all the SDKs, you can use any connector from any SDK, and this makes it quite flexible, okay? So these are the so-called multi-language pipelines, and basically, it means that you can run any transformation in any SDK, and this is implemented because the runner environment is containerized, okay? So there's a container per language, and there's some magic that makes, let's say, the container communicate between themselves, okay? And the serialization and the serialization between programming languages. So this is part of the boiler press that you need to take care of. If you use things like Apache Vida schemas that I haven't talked about in this talk, so it will be transparent for you, anything that you have in one schema in one language, you will be able to serialize it, serialize it to any other language. So if you follow, let's say, if you follow the Apache VIN custom, it's quite straightforward to use these kind of things. Well, thanks everyone so far for your attention. So here, and almost there are some links that I recommend you to have a look if you want to learn more about Apache VIN. I have covered a lot of stuff in very short time, okay? So there's a lot of things behind everything that I have explained here. If you want to know more about all the window in streaming, triggers, watermarks and so on, I strongly recommend you this book. It was released sometime ago. You may think that it's outdated, it's not outdated, so let's say this is the same model that is applied in many different streaming systems, and this is not a book about Apache VIN, it's a book about streaming systems with lots of examples coming from Apache VIN, but also examples coming from Flink, Kafka, PAPSA, and many other systems. Actually, it's very interesting. It's my favorite book, one of my favorite books, and the other one being the book actually from Martin Kledman about data intensive applications. And if you want to know more about VIN, so I recommend you the VIN College. There are lots of videos with lots of details about the things that I have explained here in YouTube. Some of them are actually linked in the slides. For sure, the main site of Apache VIN guide and all the documentation that is there. And if you want to learn more about Apache VIN, there is also the videos of the Apache VIN Summit, the previous editions. And if you want to participate, if you are here today, so you may be interested in streaming, so the call for papers is open until March 20th, I think. VIN Summit will be in June in New York, and I encourage you to submit talks. Well, so this is all. So thanks all for your attention. It's time for questions now. Thank you. So what's the advantage of using VIN if you are already using VIN? So it's portability, mainly. So if tomorrow you want to move away from Flink forward for whatever reason, so you should be able to move to other runners that have the same level of functionality, like, for instance, Dataflow. I don't know. So we have one of the main committers here of Apache Flink say that he gets hit by a bus. We don't want that to happen, but that may happen. Everything may happen. The world is really very uncertain. So basically you have portability. Yes. Thank you very much. Unfortunately, we don't have time for more questions right now, but I'm sure we'll be happy to answer any questions. Yes, anytime. Yes, thanks. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Okay, you're good to go.", "tokens": [50364, 1033, 11, 291, 434, 665, 281, 352, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 1, "seek": 0, "start": 7.0, "end": 9.0, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 2, "seek": 0, "start": 9.0, "end": 10.0, "text": " Okay.", "tokens": [50814, 1033, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 3, "seek": 0, "start": 10.0, "end": 11.0, "text": " Thanks.", "tokens": [50864, 2561, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 4, "seek": 0, "start": 11.0, "end": 21.0, "text": " I think the technical issues are now solved.", "tokens": [50914, 286, 519, 264, 6191, 2663, 366, 586, 13041, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 5, "seek": 0, "start": 21.0, "end": 25.0, "text": " Thanks again, everyone, for being here.", "tokens": [51414, 2561, 797, 11, 1518, 11, 337, 885, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 6, "seek": 0, "start": 25.0, "end": 27.0, "text": " So I'm going to talk today about Apache VIN.", "tokens": [51614, 407, 286, 478, 516, 281, 751, 965, 466, 46597, 691, 1464, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 7, "seek": 0, "start": 27.0, "end": 29.0, "text": " Apache VIN is a framework for data processing that runs", "tokens": [51714, 46597, 691, 1464, 307, 257, 8388, 337, 1412, 9007, 300, 6676, 51814], "temperature": 0.0, "avg_logprob": -0.3514148866808092, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.2893722951412201}, {"id": 8, "seek": 2900, "start": 29.0, "end": 34.0, "text": " on top of several platforms, and it's especially meant", "tokens": [50364, 322, 1192, 295, 2940, 9473, 11, 293, 309, 311, 2318, 4140, 50614], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 9, "seek": 2900, "start": 34.0, "end": 36.0, "text": " for doing streaming analytics.", "tokens": [50614, 337, 884, 11791, 15370, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 10, "seek": 2900, "start": 36.0, "end": 38.0, "text": " So Javier already introduced me.", "tokens": [50714, 407, 508, 25384, 1217, 7268, 385, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 11, "seek": 2900, "start": 38.0, "end": 39.0, "text": " My name is Israel.", "tokens": [50814, 1222, 1315, 307, 5674, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 12, "seek": 2900, "start": 39.0, "end": 43.0, "text": " So I work as a cloud data engineer in Google Cloud,", "tokens": [50864, 407, 286, 589, 382, 257, 4588, 1412, 11403, 294, 3329, 8061, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 13, "seek": 2900, "start": 43.0, "end": 46.0, "text": " helping customers doing data engineering on top of Google", "tokens": [51064, 4315, 4581, 884, 1412, 7043, 322, 1192, 295, 3329, 51214], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 14, "seek": 2900, "start": 46.0, "end": 47.0, "text": " Cloud.", "tokens": [51214, 8061, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 15, "seek": 2900, "start": 47.0, "end": 49.0, "text": " A lot of the work that I do is actually helping customers", "tokens": [51264, 316, 688, 295, 264, 589, 300, 286, 360, 307, 767, 4315, 4581, 51364], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 16, "seek": 2900, "start": 49.0, "end": 52.0, "text": " with Apache VIN, and particularly Dataflow,", "tokens": [51364, 365, 46597, 691, 1464, 11, 293, 4098, 9315, 2792, 14107, 11, 51514], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 17, "seek": 2900, "start": 52.0, "end": 55.0, "text": " which is our runner for Apache VIN.", "tokens": [51514, 597, 307, 527, 24376, 337, 46597, 691, 1464, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 18, "seek": 2900, "start": 55.0, "end": 58.0, "text": " So Apache VIN, what is it?", "tokens": [51664, 407, 46597, 691, 1464, 11, 437, 307, 309, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10820591860804064, "compression_ratio": 1.676, "no_speech_prob": 0.02779042162001133}, {"id": 19, "seek": 5800, "start": 59.0, "end": 61.0, "text": " Apache VIN is a framework for data processing.", "tokens": [50414, 46597, 691, 1464, 307, 257, 8388, 337, 1412, 9007, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 20, "seek": 5800, "start": 61.0, "end": 65.0, "text": " It allows to run data pipelines, like Flink, like Spark,", "tokens": [50514, 467, 4045, 281, 1190, 1412, 40168, 11, 411, 3235, 475, 11, 411, 23424, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 21, "seek": 5800, "start": 65.0, "end": 69.0, "text": " like so many other big data systems.", "tokens": [50714, 411, 370, 867, 661, 955, 1412, 3652, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 22, "seek": 5800, "start": 69.0, "end": 71.0, "text": " It has two main features.", "tokens": [50914, 467, 575, 732, 2135, 4122, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 23, "seek": 5800, "start": 71.0, "end": 75.0, "text": " The first one is that it's a unified computing model", "tokens": [51014, 440, 700, 472, 307, 300, 309, 311, 257, 26787, 15866, 2316, 51214], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 24, "seek": 5800, "start": 75.0, "end": 77.0, "text": " for batch and streaming.", "tokens": [51214, 337, 15245, 293, 11791, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 25, "seek": 5800, "start": 77.0, "end": 81.0, "text": " Any pipeline that you have written in Apache VIN for a batch", "tokens": [51314, 2639, 15517, 300, 291, 362, 3720, 294, 46597, 691, 1464, 337, 257, 15245, 51514], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 26, "seek": 5800, "start": 81.0, "end": 85.0, "text": " use case, you may easily use it in streaming as well.", "tokens": [51514, 764, 1389, 11, 291, 815, 3612, 764, 309, 294, 11791, 382, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08461587283076072, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.002717413241043687}, {"id": 27, "seek": 8500, "start": 85.0, "end": 88.0, "text": " So the same code, you reuse all that code,", "tokens": [50364, 407, 264, 912, 3089, 11, 291, 26225, 439, 300, 3089, 11, 50514], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 28, "seek": 8500, "start": 88.0, "end": 91.0, "text": " and you have to add some small additions", "tokens": [50514, 293, 291, 362, 281, 909, 512, 1359, 35113, 50664], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 29, "seek": 8500, "start": 91.0, "end": 94.0, "text": " that we are going to talk about in a bit.", "tokens": [50664, 300, 321, 366, 516, 281, 751, 466, 294, 257, 857, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 30, "seek": 8500, "start": 94.0, "end": 99.0, "text": " And the other main feature is that it runs everywhere,", "tokens": [50814, 400, 264, 661, 2135, 4111, 307, 300, 309, 6676, 5315, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 31, "seek": 8500, "start": 99.0, "end": 102.0, "text": " and you can run, you can write your pipeline in many different", "tokens": [51064, 293, 291, 393, 1190, 11, 291, 393, 2464, 428, 15517, 294, 867, 819, 51214], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 32, "seek": 8500, "start": 102.0, "end": 104.0, "text": " languages, for some definition of everywhere.", "tokens": [51214, 8650, 11, 337, 512, 7123, 295, 5315, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 33, "seek": 8500, "start": 104.0, "end": 111.0, "text": " So you can write your pipeline in Java, in Python, in Go.", "tokens": [51314, 407, 291, 393, 2464, 428, 15517, 294, 10745, 11, 294, 15329, 11, 294, 1037, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08684888450048303, "compression_ratio": 1.7525252525252526, "no_speech_prob": 0.00493679428473115}, {"id": 34, "seek": 11100, "start": 111.0, "end": 115.0, "text": " You may also run your pipeline in any of the programming", "tokens": [50364, 509, 815, 611, 1190, 428, 15517, 294, 604, 295, 264, 9410, 50564], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 35, "seek": 11100, "start": 115.0, "end": 118.0, "text": " languages of the Java Victor machine, for instance.", "tokens": [50564, 8650, 295, 264, 10745, 15777, 3479, 11, 337, 5197, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 36, "seek": 11100, "start": 118.0, "end": 122.0, "text": " So I have here highlighted Scala, because that's a framework", "tokens": [50714, 407, 286, 362, 510, 17173, 2747, 5159, 11, 570, 300, 311, 257, 8388, 50914], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 37, "seek": 11100, "start": 122.0, "end": 125.0, "text": " called SIO, the Don't Buy Spotify, on top of Apache VIN", "tokens": [50914, 1219, 318, 15167, 11, 264, 1468, 380, 19146, 29036, 11, 322, 1192, 295, 46597, 691, 1464, 51064], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 38, "seek": 11100, "start": 125.0, "end": 130.0, "text": " for, let's say, Scala native development of pipeline.", "tokens": [51064, 337, 11, 718, 311, 584, 11, 2747, 5159, 8470, 3250, 295, 15517, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 39, "seek": 11100, "start": 130.0, "end": 137.0, "text": " So you don't have to use Java looking code in Scala.", "tokens": [51314, 407, 291, 500, 380, 362, 281, 764, 10745, 1237, 3089, 294, 2747, 5159, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18735325991452395, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.02325381524860859}, {"id": 40, "seek": 13700, "start": 137.0, "end": 142.0, "text": " So it's a functional code.", "tokens": [50364, 407, 309, 311, 257, 11745, 3089, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 41, "seek": 13700, "start": 142.0, "end": 145.0, "text": " There are lots of people using, for instance, Kotlin also,", "tokens": [50614, 821, 366, 3195, 295, 561, 1228, 11, 337, 5197, 11, 30123, 5045, 611, 11, 50764], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 42, "seek": 13700, "start": 145.0, "end": 151.0, "text": " on top of the Java Victor machine with the Java VIN SDK.", "tokens": [50764, 322, 1192, 295, 264, 10745, 15777, 3479, 365, 264, 10745, 691, 1464, 37135, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 43, "seek": 13700, "start": 151.0, "end": 154.0, "text": " So that's about the programming language that you may use.", "tokens": [51064, 407, 300, 311, 466, 264, 9410, 2856, 300, 291, 815, 764, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 44, "seek": 13700, "start": 154.0, "end": 159.0, "text": " And you may run a VIN pipeline on top of runners.", "tokens": [51214, 400, 291, 815, 1190, 257, 691, 1464, 15517, 322, 1192, 295, 33892, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 45, "seek": 13700, "start": 159.0, "end": 163.0, "text": " So there's a direct runner for running and local testing", "tokens": [51464, 407, 456, 311, 257, 2047, 24376, 337, 2614, 293, 2654, 4997, 51664], "temperature": 0.0, "avg_logprob": -0.13960437774658202, "compression_ratio": 1.54, "no_speech_prob": 0.010578871704638004}, {"id": 46, "seek": 16300, "start": 163.0, "end": 164.0, "text": " pipelines.", "tokens": [50364, 40168, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 47, "seek": 16300, "start": 164.0, "end": 168.0, "text": " It's not meant for, let's say, to be used, let's say,", "tokens": [50414, 467, 311, 406, 4140, 337, 11, 718, 311, 584, 11, 281, 312, 1143, 11, 718, 311, 584, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 48, "seek": 16300, "start": 168.0, "end": 170.0, "text": " with real world use cases.", "tokens": [50614, 365, 957, 1002, 764, 3331, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 49, "seek": 16300, "start": 170.0, "end": 172.0, "text": " But then you can run your pipeline on top of Dataflow,", "tokens": [50714, 583, 550, 291, 393, 1190, 428, 15517, 322, 1192, 295, 9315, 2792, 14107, 11, 50814], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 50, "seek": 16300, "start": 172.0, "end": 176.0, "text": " on top of Flink, on top of Hazelcast, Spark,", "tokens": [50814, 322, 1192, 295, 3235, 475, 11, 322, 1192, 295, 15852, 338, 3734, 11, 23424, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 51, "seek": 16300, "start": 176.0, "end": 177.0, "text": " many different runners.", "tokens": [51014, 867, 819, 33892, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 52, "seek": 16300, "start": 177.0, "end": 179.0, "text": " So basically, when you write the pipeline in Apache VIN,", "tokens": [51064, 407, 1936, 11, 562, 291, 2464, 264, 15517, 294, 46597, 691, 1464, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 53, "seek": 16300, "start": 179.0, "end": 182.0, "text": " you are not tied to the platform where you're running.", "tokens": [51164, 291, 366, 406, 9601, 281, 264, 3663, 689, 291, 434, 2614, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 54, "seek": 16300, "start": 182.0, "end": 184.0, "text": " So you may move it to different platforms,", "tokens": [51314, 407, 291, 815, 1286, 309, 281, 819, 9473, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 55, "seek": 16300, "start": 184.0, "end": 186.0, "text": " which are minor comments.", "tokens": [51414, 597, 366, 6696, 3053, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 56, "seek": 16300, "start": 186.0, "end": 189.0, "text": " So Apache VIN is a theoretical model for computing.", "tokens": [51514, 407, 46597, 691, 1464, 307, 257, 20864, 2316, 337, 15866, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12504579070815466, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.021792231127619743}, {"id": 57, "seek": 18900, "start": 189.0, "end": 192.0, "text": " Not all the runners implement the model,", "tokens": [50364, 1726, 439, 264, 33892, 4445, 264, 2316, 11, 50514], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 58, "seek": 18900, "start": 192.0, "end": 195.0, "text": " let's say, to the same degree of extent.", "tokens": [50514, 718, 311, 584, 11, 281, 264, 912, 4314, 295, 8396, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 59, "seek": 18900, "start": 195.0, "end": 199.0, "text": " So right now, as of now, I would say Dataflow and Flink", "tokens": [50664, 407, 558, 586, 11, 382, 295, 586, 11, 286, 576, 584, 9315, 2792, 14107, 293, 3235, 475, 50864], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 60, "seek": 18900, "start": 199.0, "end": 204.0, "text": " are probably the ones that are fully covered,", "tokens": [50864, 366, 1391, 264, 2306, 300, 366, 4498, 5343, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 61, "seek": 18900, "start": 204.0, "end": 207.0, "text": " or the runners may have some gaps.", "tokens": [51114, 420, 264, 33892, 815, 362, 512, 15031, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 62, "seek": 18900, "start": 207.0, "end": 208.0, "text": " See the example.", "tokens": [51264, 3008, 264, 1365, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 63, "seek": 18900, "start": 208.0, "end": 209.0, "text": " Hadoop.", "tokens": [51314, 389, 1573, 404, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 64, "seek": 18900, "start": 209.0, "end": 211.0, "text": " You may run Apache VIN pipeline on top of Hadoop also,", "tokens": [51364, 509, 815, 1190, 46597, 691, 1464, 15517, 322, 1192, 295, 389, 1573, 404, 611, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 65, "seek": 18900, "start": 211.0, "end": 213.0, "text": " but you cannot run streaming on top of Hadoop", "tokens": [51464, 457, 291, 2644, 1190, 11791, 322, 1192, 295, 389, 1573, 404, 51564], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 66, "seek": 18900, "start": 213.0, "end": 215.0, "text": " because it doesn't support streaming.", "tokens": [51564, 570, 309, 1177, 380, 1406, 11791, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 67, "seek": 18900, "start": 215.0, "end": 217.0, "text": " So it also depends on the capability of the runner.", "tokens": [51664, 407, 309, 611, 5946, 322, 264, 13759, 295, 264, 24376, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11130596131317375, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.012074577622115612}, {"id": 68, "seek": 21700, "start": 217.0, "end": 219.0, "text": " So what you're able to do with VIN,", "tokens": [50364, 407, 437, 291, 434, 1075, 281, 360, 365, 691, 1464, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 69, "seek": 21700, "start": 219.0, "end": 221.0, "text": " it depends on the capabilities of the runner.", "tokens": [50464, 309, 5946, 322, 264, 10862, 295, 264, 24376, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 70, "seek": 21700, "start": 221.0, "end": 222.0, "text": " So there's no magic.", "tokens": [50564, 407, 456, 311, 572, 5585, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 71, "seek": 21700, "start": 222.0, "end": 223.0, "text": " So if VIN is batch and streaming,", "tokens": [50614, 407, 498, 691, 1464, 307, 15245, 293, 11791, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 72, "seek": 21700, "start": 223.0, "end": 225.0, "text": " but if your platform doesn't support streaming, for instance,", "tokens": [50664, 457, 498, 428, 3663, 1177, 380, 1406, 11791, 11, 337, 5197, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 73, "seek": 21700, "start": 225.0, "end": 227.0, "text": " so you cannot do streaming.", "tokens": [50764, 370, 291, 2644, 360, 11791, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 74, "seek": 21700, "start": 227.0, "end": 229.0, "text": " So let's talk about streaming.", "tokens": [50864, 407, 718, 311, 751, 466, 11791, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 75, "seek": 21700, "start": 229.0, "end": 231.0, "text": " What's the problem with streaming?", "tokens": [50964, 708, 311, 264, 1154, 365, 11791, 30, 51064], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 76, "seek": 21700, "start": 231.0, "end": 232.0, "text": " It's extremely interesting.", "tokens": [51064, 467, 311, 4664, 1880, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 77, "seek": 21700, "start": 232.0, "end": 235.0, "text": " So in streaming, you are getting data, a lot of data, continuously.", "tokens": [51114, 407, 294, 11791, 11, 291, 366, 1242, 1412, 11, 257, 688, 295, 1412, 11, 15684, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 78, "seek": 21700, "start": 235.0, "end": 238.0, "text": " There's no beginning, and there's no end.", "tokens": [51264, 821, 311, 572, 2863, 11, 293, 456, 311, 572, 917, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 79, "seek": 21700, "start": 238.0, "end": 240.0, "text": " So you cannot know in advance where", "tokens": [51414, 407, 291, 2644, 458, 294, 7295, 689, 51514], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 80, "seek": 21700, "start": 240.0, "end": 243.0, "text": " are the boundaries of your data.", "tokens": [51514, 366, 264, 13180, 295, 428, 1412, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 81, "seek": 21700, "start": 243.0, "end": 246.0, "text": " It's coming continuously from many different places.", "tokens": [51664, 467, 311, 1348, 15684, 490, 867, 819, 3190, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09407350891514828, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.003173337085172534}, {"id": 82, "seek": 24600, "start": 246.0, "end": 247.0, "text": " Think, I don't know.", "tokens": [50364, 6557, 11, 286, 500, 380, 458, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 83, "seek": 24600, "start": 247.0, "end": 251.0, "text": " Like you are designing a mobile application for a game", "tokens": [50414, 1743, 291, 366, 14685, 257, 6013, 3861, 337, 257, 1216, 50614], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 84, "seek": 24600, "start": 251.0, "end": 254.0, "text": " or whatever, and people are using it and sending events", "tokens": [50614, 420, 2035, 11, 293, 561, 366, 1228, 309, 293, 7750, 3931, 50764], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 85, "seek": 24600, "start": 254.0, "end": 258.0, "text": " to your systems every once in a while.", "tokens": [50764, 281, 428, 3652, 633, 1564, 294, 257, 1339, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 86, "seek": 24600, "start": 258.0, "end": 264.0, "text": " So because the application is deployed in the wild world,", "tokens": [50964, 407, 570, 264, 3861, 307, 17826, 294, 264, 4868, 1002, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 87, "seek": 24600, "start": 264.0, "end": 266.0, "text": " data will come.", "tokens": [51264, 1412, 486, 808, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 88, "seek": 24600, "start": 266.0, "end": 268.0, "text": " Who knows how?", "tokens": [51364, 2102, 3255, 577, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 89, "seek": 24600, "start": 268.0, "end": 270.0, "text": " So it will come out of order.", "tokens": [51464, 407, 309, 486, 808, 484, 295, 1668, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 90, "seek": 24600, "start": 270.0, "end": 273.0, "text": " So some users will be in the underground", "tokens": [51564, 407, 512, 5022, 486, 312, 294, 264, 14977, 51714], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 91, "seek": 24600, "start": 273.0, "end": 275.0, "text": " and without the phone coverage.", "tokens": [51714, 293, 1553, 264, 2593, 9645, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1278951358795166, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.009212895296514034}, {"id": 92, "seek": 27500, "start": 275.0, "end": 277.0, "text": " They still try attempting to send in events,", "tokens": [50364, 814, 920, 853, 22001, 281, 2845, 294, 3931, 11, 50464], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 93, "seek": 27500, "start": 277.0, "end": 280.0, "text": " and then they will send the events late.", "tokens": [50464, 293, 550, 436, 486, 2845, 264, 3931, 3469, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 94, "seek": 27500, "start": 280.0, "end": 283.0, "text": " Like, for instance, here, let's see if I can put the pointer.", "tokens": [50614, 1743, 11, 337, 5197, 11, 510, 11, 718, 311, 536, 498, 286, 393, 829, 264, 23918, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 95, "seek": 27500, "start": 283.0, "end": 292.0, "text": " So this is data that is supposed to be produced", "tokens": [50764, 407, 341, 307, 1412, 300, 307, 3442, 281, 312, 7126, 51214], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 96, "seek": 27500, "start": 292.0, "end": 294.0, "text": " around 8 in the morning.", "tokens": [51214, 926, 1649, 294, 264, 2446, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 97, "seek": 27500, "start": 294.0, "end": 296.0, "text": " Maybe there are network latencies and so on,", "tokens": [51314, 2704, 456, 366, 3209, 4465, 6464, 293, 370, 322, 11, 51414], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 98, "seek": 27500, "start": 296.0, "end": 299.0, "text": " but more or less you get it around 8 in the morning", "tokens": [51414, 457, 544, 420, 1570, 291, 483, 309, 926, 1649, 294, 264, 2446, 51564], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 99, "seek": 27500, "start": 299.0, "end": 300.0, "text": " in your system.", "tokens": [51564, 294, 428, 1185, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 100, "seek": 27500, "start": 300.0, "end": 303.0, "text": " So this is the time where you are seeing the data in your system.", "tokens": [51614, 407, 341, 307, 264, 565, 689, 291, 366, 2577, 264, 1412, 294, 428, 1185, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14923652580806188, "compression_ratio": 1.74235807860262, "no_speech_prob": 0.014803116209805012}, {"id": 101, "seek": 30300, "start": 303.0, "end": 308.0, "text": " But for whatever reason, you may also get very late data.", "tokens": [50364, 583, 337, 2035, 1778, 11, 291, 815, 611, 483, 588, 3469, 1412, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 102, "seek": 30300, "start": 308.0, "end": 310.0, "text": " And depending on what you want to do,", "tokens": [50614, 400, 5413, 322, 437, 291, 528, 281, 360, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 103, "seek": 30300, "start": 310.0, "end": 314.0, "text": " you may want to process this data as it was produced,", "tokens": [50714, 291, 815, 528, 281, 1399, 341, 1412, 382, 309, 390, 7126, 11, 50914], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 104, "seek": 30300, "start": 314.0, "end": 316.0, "text": " as you are receiving it.", "tokens": [50914, 382, 291, 366, 10040, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 105, "seek": 30300, "start": 316.0, "end": 318.0, "text": " So this is actually the problem with micro-batching.", "tokens": [51014, 407, 341, 307, 767, 264, 1154, 365, 4532, 12, 65, 29569, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 106, "seek": 30300, "start": 318.0, "end": 321.0, "text": " So I remember when I started hearing about streaming", "tokens": [51114, 407, 286, 1604, 562, 286, 1409, 4763, 466, 11791, 51264], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 107, "seek": 30300, "start": 321.0, "end": 324.0, "text": " in the first days, many years ago,", "tokens": [51264, 294, 264, 700, 1708, 11, 867, 924, 2057, 11, 51414], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 108, "seek": 30300, "start": 324.0, "end": 327.0, "text": " a lot of people said, oh, Spark, I don't like it", "tokens": [51414, 257, 688, 295, 561, 848, 11, 1954, 11, 23424, 11, 286, 500, 380, 411, 309, 51564], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 109, "seek": 30300, "start": 327.0, "end": 329.0, "text": " because it does micro-batching.", "tokens": [51564, 570, 309, 775, 4532, 12, 65, 29569, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 110, "seek": 30300, "start": 329.0, "end": 331.0, "text": " It doesn't do real streaming.", "tokens": [51664, 467, 1177, 380, 360, 957, 11791, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0793548412010318, "compression_ratio": 1.6771653543307086, "no_speech_prob": 0.021013787016272545}, {"id": 111, "seek": 33100, "start": 332.0, "end": 333.0, "text": " I had no clue what they meant.", "tokens": [50414, 286, 632, 572, 13602, 437, 436, 4140, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 112, "seek": 33100, "start": 333.0, "end": 336.0, "text": " It's like, well, you have to group things to process it somehow.", "tokens": [50464, 467, 311, 411, 11, 731, 11, 291, 362, 281, 1594, 721, 281, 1399, 309, 6063, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 113, "seek": 33100, "start": 336.0, "end": 339.0, "text": " So if the data is infinite, you will need to process it.", "tokens": [50614, 407, 498, 264, 1412, 307, 13785, 11, 291, 486, 643, 281, 1399, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 114, "seek": 33100, "start": 339.0, "end": 341.0, "text": " The problem with micro-batching, which is not happening", "tokens": [50764, 440, 1154, 365, 4532, 12, 65, 29569, 11, 597, 307, 406, 2737, 50864], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 115, "seek": 33100, "start": 341.0, "end": 344.0, "text": " in Spark anymore, so this was really ancient times,", "tokens": [50864, 294, 23424, 3602, 11, 370, 341, 390, 534, 7832, 1413, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 116, "seek": 33100, "start": 344.0, "end": 346.0, "text": " the problem with micro-batching is that you are doing the batches", "tokens": [51014, 264, 1154, 365, 4532, 12, 65, 29569, 307, 300, 291, 366, 884, 264, 15245, 279, 51114], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 117, "seek": 33100, "start": 346.0, "end": 348.0, "text": " as you see the data.", "tokens": [51114, 382, 291, 536, 264, 1412, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 118, "seek": 33100, "start": 348.0, "end": 352.0, "text": " And then you may have the data that belongs together", "tokens": [51214, 400, 550, 291, 815, 362, 264, 1412, 300, 12953, 1214, 51414], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 119, "seek": 33100, "start": 352.0, "end": 354.0, "text": " in buckets that are separate.", "tokens": [51414, 294, 32191, 300, 366, 4994, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 120, "seek": 33100, "start": 354.0, "end": 356.0, "text": " Like, for instance, this was data that was produced", "tokens": [51514, 1743, 11, 337, 5197, 11, 341, 390, 1412, 300, 390, 7126, 51614], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 121, "seek": 33100, "start": 356.0, "end": 357.0, "text": " at 8 in the morning.", "tokens": [51614, 412, 1649, 294, 264, 2446, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 122, "seek": 33100, "start": 357.0, "end": 359.0, "text": " If you are doing buckets of one hour,", "tokens": [51664, 759, 291, 366, 884, 32191, 295, 472, 1773, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07412888844807942, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.005659229587763548}, {"id": 123, "seek": 35900, "start": 359.0, "end": 362.0, "text": " so well, you may capture here this message,", "tokens": [50364, 370, 731, 11, 291, 815, 7983, 510, 341, 3636, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 124, "seek": 35900, "start": 362.0, "end": 364.0, "text": " but then if you have late data, you will capture it", "tokens": [50514, 457, 550, 498, 291, 362, 3469, 1412, 11, 291, 486, 7983, 309, 50614], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 125, "seek": 35900, "start": 364.0, "end": 367.0, "text": " in a bucket that doesn't belong,", "tokens": [50614, 294, 257, 13058, 300, 1177, 380, 5784, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 126, "seek": 35900, "start": 367.0, "end": 370.0, "text": " that element doesn't belong with the rest of the elements there", "tokens": [50764, 300, 4478, 1177, 380, 5784, 365, 264, 1472, 295, 264, 4959, 456, 50914], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 127, "seek": 35900, "start": 370.0, "end": 372.0, "text": " if you want to process them together.", "tokens": [50914, 498, 291, 528, 281, 1399, 552, 1214, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 128, "seek": 35900, "start": 372.0, "end": 377.0, "text": " So you need to solve this problem of lack of order in streaming.", "tokens": [51014, 407, 291, 643, 281, 5039, 341, 1154, 295, 5011, 295, 1668, 294, 11791, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 129, "seek": 35900, "start": 377.0, "end": 381.0, "text": " And this is what you can easily solve with Apache Bin.", "tokens": [51264, 400, 341, 307, 437, 291, 393, 3612, 5039, 365, 46597, 18983, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 130, "seek": 35900, "start": 381.0, "end": 383.0, "text": " Let's talk about the watermark.", "tokens": [51464, 961, 311, 751, 466, 264, 1281, 5638, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 131, "seek": 35900, "start": 383.0, "end": 386.0, "text": " There are many ways of doing stream processing.", "tokens": [51564, 821, 366, 867, 2098, 295, 884, 4309, 9007, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10609851263265695, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.0023072869516909122}, {"id": 132, "seek": 38600, "start": 386.0, "end": 392.0, "text": " One of the most popular is using a concept of watermark.", "tokens": [50364, 1485, 295, 264, 881, 3743, 307, 1228, 257, 3410, 295, 1281, 5638, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 133, "seek": 38600, "start": 392.0, "end": 394.0, "text": " There is no one dimension of time.", "tokens": [50664, 821, 307, 572, 472, 10139, 295, 565, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 134, "seek": 38600, "start": 394.0, "end": 396.0, "text": " There are two dimensions of times.", "tokens": [50764, 821, 366, 732, 12819, 295, 1413, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 135, "seek": 38600, "start": 396.0, "end": 398.0, "text": " At least that is the event time,", "tokens": [50864, 1711, 1935, 300, 307, 264, 2280, 565, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 136, "seek": 38600, "start": 398.0, "end": 401.0, "text": " the moment in which the data was produced,", "tokens": [50964, 264, 1623, 294, 597, 264, 1412, 390, 7126, 11, 51114], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 137, "seek": 38600, "start": 401.0, "end": 403.0, "text": " and that is the processing time,", "tokens": [51114, 293, 300, 307, 264, 9007, 565, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 138, "seek": 38600, "start": 403.0, "end": 405.0, "text": " the moment in which you see the data.", "tokens": [51214, 264, 1623, 294, 597, 291, 536, 264, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 139, "seek": 38600, "start": 405.0, "end": 407.0, "text": " They will never be the same.", "tokens": [51314, 814, 486, 1128, 312, 264, 912, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 140, "seek": 38600, "start": 407.0, "end": 409.0, "text": " They can be really close sometimes,", "tokens": [51414, 814, 393, 312, 534, 1998, 2171, 11, 51514], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 141, "seek": 38600, "start": 409.0, "end": 412.0, "text": " but you cannot grant it how close or how far", "tokens": [51514, 457, 291, 2644, 6386, 309, 577, 1998, 420, 577, 1400, 51664], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 142, "seek": 38600, "start": 412.0, "end": 414.0, "text": " you are going to be from that moment.", "tokens": [51664, 291, 366, 516, 281, 312, 490, 300, 1623, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07836495415639069, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.01631039008498192}, {"id": 143, "seek": 41400, "start": 415.0, "end": 418.0, "text": " So we put time in two dimensions.", "tokens": [50414, 407, 321, 829, 565, 294, 732, 12819, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 144, "seek": 41400, "start": 418.0, "end": 420.0, "text": " So the ideal is, for instance,", "tokens": [50564, 407, 264, 7157, 307, 11, 337, 5197, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 145, "seek": 41400, "start": 420.0, "end": 423.0, "text": " like this straight line in blue, for sure,", "tokens": [50664, 411, 341, 2997, 1622, 294, 3344, 11, 337, 988, 11, 50814], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 146, "seek": 41400, "start": 423.0, "end": 425.0, "text": " this zone is impossible.", "tokens": [50814, 341, 6668, 307, 6243, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 147, "seek": 41400, "start": 425.0, "end": 427.0, "text": " You cannot see data before it's produced,", "tokens": [50914, 509, 2644, 536, 1412, 949, 309, 311, 7126, 11, 51014], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 148, "seek": 41400, "start": 427.0, "end": 431.0, "text": " or not yet at least according to the laws of physics.", "tokens": [51014, 420, 406, 1939, 412, 1935, 4650, 281, 264, 6064, 295, 10649, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 149, "seek": 41400, "start": 431.0, "end": 434.0, "text": " But then most likely what will happen", "tokens": [51214, 583, 550, 881, 3700, 437, 486, 1051, 51364], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 150, "seek": 41400, "start": 434.0, "end": 436.0, "text": " is that you will have some delay.", "tokens": [51364, 307, 300, 291, 486, 362, 512, 8577, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 151, "seek": 41400, "start": 436.0, "end": 438.0, "text": " Sometimes it will be closer to the ideal,", "tokens": [51464, 4803, 309, 486, 312, 4966, 281, 264, 7157, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 152, "seek": 41400, "start": 438.0, "end": 441.0, "text": " sometimes it will be farther from the ideal.", "tokens": [51564, 2171, 309, 486, 312, 20344, 490, 264, 7157, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13018879043721707, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0034940557088702917}, {"id": 153, "seek": 44100, "start": 442.0, "end": 446.0, "text": " And you need to take this into account.", "tokens": [50414, 400, 291, 643, 281, 747, 341, 666, 2696, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 154, "seek": 44100, "start": 446.0, "end": 448.0, "text": " So let's see an example", "tokens": [50614, 407, 718, 311, 536, 364, 1365, 50714], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 155, "seek": 44100, "start": 448.0, "end": 450.0, "text": " that might be a little bit more telling.", "tokens": [50714, 300, 1062, 312, 257, 707, 857, 544, 3585, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 156, "seek": 44100, "start": 450.0, "end": 451.0, "text": " So Star Wars.", "tokens": [50814, 407, 5705, 9818, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 157, "seek": 44100, "start": 451.0, "end": 454.0, "text": " So have you ever watched a Star Wars movie?", "tokens": [50864, 407, 362, 291, 1562, 6337, 257, 5705, 9818, 3169, 30, 51014], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 158, "seek": 44100, "start": 454.0, "end": 458.0, "text": " So Star Wars were released out of order, out of order.", "tokens": [51014, 407, 5705, 9818, 645, 4736, 484, 295, 1668, 11, 484, 295, 1668, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 159, "seek": 44100, "start": 458.0, "end": 461.0, "text": " So the first movie was Episode 4.", "tokens": [51214, 407, 264, 700, 3169, 390, 19882, 1017, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 160, "seek": 44100, "start": 461.0, "end": 462.0, "text": " This is purely streaming.", "tokens": [51364, 639, 307, 17491, 11791, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 161, "seek": 44100, "start": 462.0, "end": 464.0, "text": " This is what happens in streaming.", "tokens": [51414, 639, 307, 437, 2314, 294, 11791, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 162, "seek": 44100, "start": 464.0, "end": 466.0, "text": " You are expecting events at the beginning of the session,", "tokens": [51514, 509, 366, 9650, 3931, 412, 264, 2863, 295, 264, 5481, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 163, "seek": 44100, "start": 466.0, "end": 469.0, "text": " in the middle of the session, the end of the session,", "tokens": [51614, 294, 264, 2808, 295, 264, 5481, 11, 264, 917, 295, 264, 5481, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12372038544726972, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.005824429914355278}, {"id": 164, "seek": 46900, "start": 469.0, "end": 471.0, "text": " and then you get end of the session,", "tokens": [50364, 293, 550, 291, 483, 917, 295, 264, 5481, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 165, "seek": 46900, "start": 471.0, "end": 473.0, "text": " middle of the session, beginning of the session.", "tokens": [50464, 2808, 295, 264, 5481, 11, 2863, 295, 264, 5481, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 166, "seek": 46900, "start": 473.0, "end": 476.0, "text": " And you need to reorder things.", "tokens": [50564, 400, 291, 643, 281, 319, 4687, 721, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 167, "seek": 46900, "start": 476.0, "end": 478.0, "text": " Depending on what you want to do,", "tokens": [50714, 22539, 322, 437, 291, 528, 281, 360, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 168, "seek": 46900, "start": 478.0, "end": 480.0, "text": " you may need to reorder.", "tokens": [50814, 291, 815, 643, 281, 319, 4687, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 169, "seek": 46900, "start": 480.0, "end": 482.0, "text": " If you don't care, look, I don't want to,", "tokens": [50914, 759, 291, 500, 380, 1127, 11, 574, 11, 286, 500, 380, 528, 281, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 170, "seek": 46900, "start": 482.0, "end": 483.0, "text": " I don't care.", "tokens": [51014, 286, 500, 380, 1127, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 171, "seek": 46900, "start": 483.0, "end": 485.0, "text": " I just want to count how many movies per year were released.", "tokens": [51064, 286, 445, 528, 281, 1207, 577, 867, 6233, 680, 1064, 645, 4736, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 172, "seek": 46900, "start": 485.0, "end": 487.0, "text": " Well, you don't need the event time.", "tokens": [51164, 1042, 11, 291, 500, 380, 643, 264, 2280, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 173, "seek": 46900, "start": 487.0, "end": 490.0, "text": " But if you want to reconstruct the story", "tokens": [51264, 583, 498, 291, 528, 281, 31499, 264, 1657, 51414], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 174, "seek": 46900, "start": 490.0, "end": 494.0, "text": " who did what before or after,", "tokens": [51414, 567, 630, 437, 949, 420, 934, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 175, "seek": 46900, "start": 494.0, "end": 497.0, "text": " what happened, so you need to actually", "tokens": [51614, 437, 2011, 11, 370, 291, 643, 281, 767, 51764], "temperature": 0.0, "avg_logprob": -0.11071246010916573, "compression_ratio": 1.8565400843881856, "no_speech_prob": 0.012371456250548363}, {"id": 176, "seek": 49700, "start": 497.0, "end": 499.0, "text": " be able to reconstruct that time.", "tokens": [50364, 312, 1075, 281, 31499, 300, 565, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 177, "seek": 49700, "start": 499.0, "end": 502.0, "text": " So the time where the movies were released,", "tokens": [50464, 407, 264, 565, 689, 264, 6233, 645, 4736, 11, 50614], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 178, "seek": 49700, "start": 502.0, "end": 504.0, "text": " its processing time, event time is", "tokens": [50614, 1080, 9007, 565, 11, 2280, 565, 307, 50714], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 179, "seek": 49700, "start": 504.0, "end": 507.0, "text": " the time in which actually the events are happening.", "tokens": [50714, 264, 565, 294, 597, 767, 264, 3931, 366, 2737, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 180, "seek": 49700, "start": 507.0, "end": 509.0, "text": " And this is the kind of problems that we can solve", "tokens": [50864, 400, 341, 307, 264, 733, 295, 2740, 300, 321, 393, 5039, 50964], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 181, "seek": 49700, "start": 509.0, "end": 512.0, "text": " using Apache VIN or Flink or many other streaming systems.", "tokens": [50964, 1228, 46597, 691, 1464, 420, 3235, 475, 420, 867, 661, 11791, 3652, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 182, "seek": 49700, "start": 512.0, "end": 515.0, "text": " Let's see how we can deal with this.", "tokens": [51114, 961, 311, 536, 577, 321, 393, 2028, 365, 341, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 183, "seek": 49700, "start": 515.0, "end": 518.0, "text": " The classical way is using windowing.", "tokens": [51264, 440, 13735, 636, 307, 1228, 4910, 278, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 184, "seek": 49700, "start": 518.0, "end": 522.0, "text": " Windowing is grouping things based on temporal properties.", "tokens": [51414, 44933, 278, 307, 40149, 721, 2361, 322, 30881, 7221, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 185, "seek": 49700, "start": 522.0, "end": 524.0, "text": " When we do a data pipeline,", "tokens": [51614, 1133, 321, 360, 257, 1412, 15517, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11463555118493866, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0037528749089688063}, {"id": 186, "seek": 52400, "start": 524.0, "end": 526.0, "text": " we need to solve one question,", "tokens": [50364, 321, 643, 281, 5039, 472, 1168, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 187, "seek": 52400, "start": 526.0, "end": 530.0, "text": " which is what we are going to compute.", "tokens": [50464, 597, 307, 437, 321, 366, 516, 281, 14722, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 188, "seek": 52400, "start": 530.0, "end": 532.0, "text": " But if you want to do this in streaming", "tokens": [50664, 583, 498, 291, 528, 281, 360, 341, 294, 11791, 50764], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 189, "seek": 52400, "start": 532.0, "end": 534.0, "text": " and group things based on temporal properties,", "tokens": [50764, 293, 1594, 721, 2361, 322, 30881, 7221, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 190, "seek": 52400, "start": 534.0, "end": 537.0, "text": " you need to answer three additional questions.", "tokens": [50864, 291, 643, 281, 1867, 1045, 4497, 1651, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 191, "seek": 52400, "start": 537.0, "end": 539.0, "text": " Where, when, and how.", "tokens": [51014, 2305, 11, 562, 11, 293, 577, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 192, "seek": 52400, "start": 539.0, "end": 541.0, "text": " Let's see some details.", "tokens": [51114, 961, 311, 536, 512, 4365, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 193, "seek": 52400, "start": 541.0, "end": 542.0, "text": " What? This is easy.", "tokens": [51214, 708, 30, 639, 307, 1858, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 194, "seek": 52400, "start": 542.0, "end": 543.0, "text": " We are going to be aggregating.", "tokens": [51264, 492, 366, 516, 281, 312, 16743, 990, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 195, "seek": 52400, "start": 543.0, "end": 545.0, "text": " So this is Java code,", "tokens": [51314, 407, 341, 307, 10745, 3089, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 196, "seek": 52400, "start": 545.0, "end": 547.0, "text": " and it's in Java here as an example.", "tokens": [51414, 293, 309, 311, 294, 10745, 510, 382, 364, 1365, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 197, "seek": 52400, "start": 547.0, "end": 549.0, "text": " So it's Apache VIN API.", "tokens": [51514, 407, 309, 311, 46597, 691, 1464, 9362, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 198, "seek": 52400, "start": 549.0, "end": 551.0, "text": " I haven't entered into details.", "tokens": [51614, 286, 2378, 380, 9065, 666, 4365, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 199, "seek": 52400, "start": 551.0, "end": 553.0, "text": " There's a link at the end with more details.", "tokens": [51714, 821, 311, 257, 2113, 412, 264, 917, 365, 544, 4365, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11527083260672433, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.008591823279857635}, {"id": 200, "seek": 55300, "start": 553.0, "end": 555.0, "text": " So don't mind the details right now.", "tokens": [50364, 407, 500, 380, 1575, 264, 4365, 558, 586, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 201, "seek": 55300, "start": 555.0, "end": 557.0, "text": " So we are aggregating things together.", "tokens": [50464, 407, 321, 366, 16743, 990, 721, 1214, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 202, "seek": 55300, "start": 557.0, "end": 559.0, "text": " So this is what we are happening.", "tokens": [50564, 407, 341, 307, 437, 321, 366, 2737, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 203, "seek": 55300, "start": 559.0, "end": 561.0, "text": " We are not doing any kind of temporal based logic yet.", "tokens": [50664, 492, 366, 406, 884, 604, 733, 295, 30881, 2361, 9952, 1939, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 204, "seek": 55300, "start": 561.0, "end": 563.0, "text": " We are just aggregating stuff together.", "tokens": [50764, 492, 366, 445, 16743, 990, 1507, 1214, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 205, "seek": 55300, "start": 563.0, "end": 565.0, "text": " So we are summing up all the numbers here.", "tokens": [50864, 407, 321, 366, 2408, 2810, 493, 439, 264, 3547, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 206, "seek": 55300, "start": 565.0, "end": 567.0, "text": " So this is the operation that we are doing.", "tokens": [50964, 407, 341, 307, 264, 6916, 300, 321, 366, 884, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 207, "seek": 55300, "start": 567.0, "end": 568.0, "text": " Same as in batch.", "tokens": [51064, 10635, 382, 294, 15245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 208, "seek": 55300, "start": 568.0, "end": 569.0, "text": " This is in batch.", "tokens": [51114, 639, 307, 294, 15245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 209, "seek": 55300, "start": 569.0, "end": 571.0, "text": " So imagine that we are getting this batch.", "tokens": [51164, 407, 3811, 300, 321, 366, 1242, 341, 15245, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 210, "seek": 55300, "start": 571.0, "end": 574.0, "text": " The problem is that when we are working in streaming,", "tokens": [51264, 440, 1154, 307, 300, 562, 321, 366, 1364, 294, 11791, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 211, "seek": 55300, "start": 574.0, "end": 576.0, "text": " we don't see the full data at once.", "tokens": [51414, 321, 500, 380, 536, 264, 1577, 1412, 412, 1564, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 212, "seek": 55300, "start": 576.0, "end": 579.0, "text": " And we need to produce output at some point", "tokens": [51514, 400, 321, 643, 281, 5258, 5598, 412, 512, 935, 51664], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 213, "seek": 55300, "start": 579.0, "end": 582.0, "text": " so we cannot wait forever.", "tokens": [51664, 370, 321, 2644, 1699, 5680, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07272303499133381, "compression_ratio": 1.8763250883392226, "no_speech_prob": 0.004458163399249315}, {"id": 214, "seek": 58200, "start": 582.0, "end": 584.0, "text": " So we need to decide how to group things together.", "tokens": [50364, 407, 321, 643, 281, 4536, 577, 281, 1594, 721, 1214, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 215, "seek": 58200, "start": 584.0, "end": 585.0, "text": " So for instance here,", "tokens": [50464, 407, 337, 5197, 510, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 216, "seek": 58200, "start": 585.0, "end": 590.0, "text": " we are going to group things in windows of two minutes.", "tokens": [50514, 321, 366, 516, 281, 1594, 721, 294, 9309, 295, 732, 2077, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 217, "seek": 58200, "start": 590.0, "end": 594.0, "text": " But the windows of two minutes are not in processing time.", "tokens": [50764, 583, 264, 9309, 295, 732, 2077, 366, 406, 294, 9007, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 218, "seek": 58200, "start": 594.0, "end": 596.0, "text": " They are in event time.", "tokens": [50964, 814, 366, 294, 2280, 565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 219, "seek": 58200, "start": 596.0, "end": 600.0, "text": " For instance, so here, this message here,", "tokens": [51064, 1171, 5197, 11, 370, 510, 11, 341, 3636, 510, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 220, "seek": 58200, "start": 600.0, "end": 602.0, "text": " so this message here,", "tokens": [51264, 370, 341, 3636, 510, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 221, "seek": 58200, "start": 602.0, "end": 604.0, "text": " we see it around 12,", "tokens": [51364, 321, 536, 309, 926, 2272, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 222, "seek": 58200, "start": 604.0, "end": 607.0, "text": " and we put it in the window of 12.", "tokens": [51464, 293, 321, 829, 309, 294, 264, 4910, 295, 2272, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 223, "seek": 58200, "start": 607.0, "end": 609.0, "text": " But this message over here,", "tokens": [51614, 583, 341, 3636, 670, 510, 11, 51714], "temperature": 0.0, "avg_logprob": -0.10308848789760044, "compression_ratio": 1.8795811518324608, "no_speech_prob": 0.0023445531260222197}, {"id": 224, "seek": 60900, "start": 609.0, "end": 614.0, "text": " so this was received between 12.08 and 12.09.", "tokens": [50364, 370, 341, 390, 4613, 1296, 2272, 13, 16133, 293, 2272, 13, 13811, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 225, "seek": 60900, "start": 614.0, "end": 616.0, "text": " And we are able still to attribute it", "tokens": [50614, 400, 321, 366, 1075, 920, 281, 19667, 309, 50714], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 226, "seek": 60900, "start": 616.0, "end": 619.0, "text": " to assign it to the window between 12 and 12.02", "tokens": [50714, 281, 6269, 309, 281, 264, 4910, 1296, 2272, 293, 2272, 13, 12756, 50864], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 227, "seek": 60900, "start": 619.0, "end": 621.0, "text": " in event time.", "tokens": [50864, 294, 2280, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 228, "seek": 60900, "start": 621.0, "end": 623.0, "text": " Because, well, so we can wait for late data", "tokens": [50964, 1436, 11, 731, 11, 370, 321, 393, 1699, 337, 3469, 1412, 51064], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 229, "seek": 60900, "start": 623.0, "end": 625.0, "text": " and put it in the right window,", "tokens": [51064, 293, 829, 309, 294, 264, 558, 4910, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 230, "seek": 60900, "start": 625.0, "end": 627.0, "text": " despite the message being quite late", "tokens": [51164, 7228, 264, 3636, 885, 1596, 3469, 51264], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 231, "seek": 60900, "start": 627.0, "end": 629.0, "text": " compared to the processing time.", "tokens": [51264, 5347, 281, 264, 9007, 565, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 232, "seek": 60900, "start": 629.0, "end": 631.0, "text": " And same with the rest of windows.", "tokens": [51364, 400, 912, 365, 264, 1472, 295, 9309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 233, "seek": 60900, "start": 631.0, "end": 633.0, "text": " Now the question is, okay, good.", "tokens": [51464, 823, 264, 1168, 307, 11, 1392, 11, 665, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 234, "seek": 60900, "start": 633.0, "end": 636.0, "text": " So you are waiting until 12.08.", "tokens": [51564, 407, 291, 366, 3806, 1826, 2272, 13, 16133, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14252921282234837, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.0029062244575470686}, {"id": 235, "seek": 63600, "start": 636.0, "end": 640.0, "text": " So what if your message shows up at 8 p.m.?", "tokens": [50364, 407, 437, 498, 428, 3636, 3110, 493, 412, 1649, 280, 13, 76, 41401, 50564], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 236, "seek": 63600, "start": 640.0, "end": 641.0, "text": " What do you do?", "tokens": [50564, 708, 360, 291, 360, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 237, "seek": 63600, "start": 641.0, "end": 643.0, "text": " Like eight hours after.", "tokens": [50614, 1743, 3180, 2496, 934, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 238, "seek": 63600, "start": 643.0, "end": 645.0, "text": " So we need to do another decision, okay?", "tokens": [50714, 407, 321, 643, 281, 360, 1071, 3537, 11, 1392, 30, 50814], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 239, "seek": 63600, "start": 645.0, "end": 647.0, "text": " So we have already made the decision", "tokens": [50814, 407, 321, 362, 1217, 1027, 264, 3537, 50914], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 240, "seek": 63600, "start": 647.0, "end": 649.0, "text": " on how we are going to group things together.", "tokens": [50914, 322, 577, 321, 366, 516, 281, 1594, 721, 1214, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 241, "seek": 63600, "start": 649.0, "end": 650.0, "text": " Here is with easy windows.", "tokens": [51014, 1692, 307, 365, 1858, 9309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 242, "seek": 63600, "start": 650.0, "end": 652.0, "text": " There are more windows in Apache bin,", "tokens": [51064, 821, 366, 544, 9309, 294, 46597, 5171, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 243, "seek": 63600, "start": 652.0, "end": 653.0, "text": " not entering into details right now.", "tokens": [51164, 406, 11104, 666, 4365, 558, 586, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 244, "seek": 63600, "start": 653.0, "end": 659.0, "text": " But now we need to decide how long do we wait, okay?", "tokens": [51214, 583, 586, 321, 643, 281, 4536, 577, 938, 360, 321, 1699, 11, 1392, 30, 51514], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 245, "seek": 63600, "start": 659.0, "end": 662.0, "text": " So we are going to wait until the watermark.", "tokens": [51514, 407, 321, 366, 516, 281, 1699, 1826, 264, 1281, 5638, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14352105458577474, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.0021539584267884493}, {"id": 246, "seek": 66200, "start": 662.0, "end": 665.0, "text": " Okay, the watermark is this relationship", "tokens": [50364, 1033, 11, 264, 1281, 5638, 307, 341, 2480, 50514], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 247, "seek": 66200, "start": 665.0, "end": 667.0, "text": " between processing time and event time", "tokens": [50514, 1296, 9007, 565, 293, 2280, 565, 50614], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 248, "seek": 66200, "start": 667.0, "end": 670.0, "text": " that in the case of bin and depending on the runner.", "tokens": [50614, 300, 294, 264, 1389, 295, 5171, 293, 5413, 322, 264, 24376, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 249, "seek": 66200, "start": 670.0, "end": 672.0, "text": " It's calculated and estimated on the fly", "tokens": [50764, 467, 311, 15598, 293, 14109, 322, 264, 3603, 50864], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 250, "seek": 66200, "start": 672.0, "end": 676.0, "text": " as data goes through our pipeline.", "tokens": [50864, 382, 1412, 1709, 807, 527, 15517, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 251, "seek": 66200, "start": 676.0, "end": 679.0, "text": " And it's this curve is estimated.", "tokens": [51064, 400, 309, 311, 341, 7605, 307, 14109, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 252, "seek": 66200, "start": 679.0, "end": 682.0, "text": " And when you trespass the watermark,", "tokens": [51214, 400, 562, 291, 46347, 640, 264, 1281, 5638, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 253, "seek": 66200, "start": 682.0, "end": 685.0, "text": " you have a certain degree of warranty", "tokens": [51364, 291, 362, 257, 1629, 4314, 295, 26852, 51514], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 254, "seek": 66200, "start": 685.0, "end": 687.0, "text": " that your data is complete, okay?", "tokens": [51514, 300, 428, 1412, 307, 3566, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 255, "seek": 66200, "start": 687.0, "end": 688.0, "text": " A certain degree of warranty, okay?", "tokens": [51614, 316, 1629, 4314, 295, 26852, 11, 1392, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 256, "seek": 66200, "start": 688.0, "end": 689.0, "text": " It cannot be granted because, well,", "tokens": [51664, 467, 2644, 312, 12344, 570, 11, 731, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 257, "seek": 66200, "start": 689.0, "end": 691.0, "text": " so the future cannot be known, okay?", "tokens": [51714, 370, 264, 2027, 2644, 312, 2570, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.1162802634700652, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.040307287126779556}, {"id": 258, "seek": 69100, "start": 691.0, "end": 693.0, "text": " So we cannot travel in time, okay?", "tokens": [50364, 407, 321, 2644, 3147, 294, 565, 11, 1392, 30, 50464], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 259, "seek": 69100, "start": 693.0, "end": 696.0, "text": " So here, for instance, so we are processing data", "tokens": [50464, 407, 510, 11, 337, 5197, 11, 370, 321, 366, 9007, 1412, 50614], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 260, "seek": 69100, "start": 696.0, "end": 699.0, "text": " in the watermark and the nine, this number here", "tokens": [50614, 294, 264, 1281, 5638, 293, 264, 4949, 11, 341, 1230, 510, 50764], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 261, "seek": 69100, "start": 699.0, "end": 702.0, "text": " that we were processing before,", "tokens": [50764, 300, 321, 645, 9007, 949, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 262, "seek": 69100, "start": 702.0, "end": 704.0, "text": " now it's left out of the window.", "tokens": [50914, 586, 309, 311, 1411, 484, 295, 264, 4910, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 263, "seek": 69100, "start": 704.0, "end": 707.0, "text": " So what does it mean if we are processing data?", "tokens": [51014, 407, 437, 775, 309, 914, 498, 321, 366, 9007, 1412, 30, 51164], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 264, "seek": 69100, "start": 707.0, "end": 709.0, "text": " We were summing up numbers, that number,", "tokens": [51164, 492, 645, 2408, 2810, 493, 3547, 11, 300, 1230, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 265, "seek": 69100, "start": 709.0, "end": 713.0, "text": " that nine, we are not counting it.", "tokens": [51264, 300, 4949, 11, 321, 366, 406, 13251, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 266, "seek": 69100, "start": 713.0, "end": 716.0, "text": " As soon as we see it in our pipeline,", "tokens": [51464, 1018, 2321, 382, 321, 536, 309, 294, 527, 15517, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 267, "seek": 69100, "start": 716.0, "end": 719.0, "text": " it will be dropped, like lost, okay?", "tokens": [51614, 309, 486, 312, 8119, 11, 411, 2731, 11, 1392, 30, 51764], "temperature": 0.0, "avg_logprob": -0.09774475582575394, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.0037655984051525593}, {"id": 268, "seek": 71900, "start": 719.0, "end": 722.0, "text": " So the pipeline will ignore it, okay?", "tokens": [50364, 407, 264, 15517, 486, 11200, 309, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 269, "seek": 71900, "start": 722.0, "end": 725.0, "text": " And it may make sense, okay?", "tokens": [50514, 400, 309, 815, 652, 2020, 11, 1392, 30, 50664], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 270, "seek": 71900, "start": 725.0, "end": 727.0, "text": " So you cannot wait forever.", "tokens": [50664, 407, 291, 2644, 1699, 5680, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 271, "seek": 71900, "start": 727.0, "end": 731.0, "text": " At some point, you will have to stop and move on, okay?", "tokens": [50764, 1711, 512, 935, 11, 291, 486, 362, 281, 1590, 293, 1286, 322, 11, 1392, 30, 50964], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 272, "seek": 71900, "start": 731.0, "end": 733.0, "text": " But maybe you want to take it into account, okay?", "tokens": [50964, 583, 1310, 291, 528, 281, 747, 309, 666, 2696, 11, 1392, 30, 51064], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 273, "seek": 71900, "start": 733.0, "end": 736.0, "text": " So maybe you, I don't know, like this is a billing,", "tokens": [51064, 407, 1310, 291, 11, 286, 500, 380, 458, 11, 411, 341, 307, 257, 35618, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 274, "seek": 71900, "start": 736.0, "end": 739.0, "text": " invoicing thing and every penny counts, okay?", "tokens": [51214, 1048, 78, 5776, 551, 293, 633, 24178, 14893, 11, 1392, 30, 51364], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 275, "seek": 71900, "start": 739.0, "end": 741.0, "text": " So then you need to process it.", "tokens": [51364, 407, 550, 291, 643, 281, 1399, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 276, "seek": 71900, "start": 741.0, "end": 744.0, "text": " Well, you have to take yet another decision.", "tokens": [51464, 1042, 11, 291, 362, 281, 747, 1939, 1071, 3537, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 277, "seek": 71900, "start": 744.0, "end": 746.0, "text": " How we are gonna wait for late data", "tokens": [51614, 1012, 321, 366, 799, 1699, 337, 3469, 1412, 51714], "temperature": 0.0, "avg_logprob": -0.11311592594269783, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.005096003878861666}, {"id": 278, "seek": 74600, "start": 746.0, "end": 750.0, "text": " and how we are gonna actually update the data, okay?", "tokens": [50364, 293, 577, 321, 366, 799, 767, 5623, 264, 1412, 11, 1392, 30, 50564], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 279, "seek": 74600, "start": 750.0, "end": 752.0, "text": " Here, I'm summing numbers.", "tokens": [50564, 1692, 11, 286, 478, 2408, 2810, 3547, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 280, "seek": 74600, "start": 752.0, "end": 754.0, "text": " It's easy, commutative, associative,", "tokens": [50664, 467, 311, 1858, 11, 800, 325, 1166, 11, 4180, 1166, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 281, "seek": 74600, "start": 754.0, "end": 755.0, "text": " really no big deal, okay?", "tokens": [50764, 534, 572, 955, 2028, 11, 1392, 30, 50814], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 282, "seek": 74600, "start": 755.0, "end": 759.0, "text": " So I can do it like, say, I can do it,", "tokens": [50814, 407, 286, 393, 360, 309, 411, 11, 584, 11, 286, 393, 360, 309, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 283, "seek": 74600, "start": 759.0, "end": 762.0, "text": " I can do it like a monoid in big data processing,", "tokens": [51014, 286, 393, 360, 309, 411, 257, 1108, 17079, 294, 955, 1412, 9007, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 284, "seek": 74600, "start": 762.0, "end": 764.0, "text": " so I can just take the aggregation,", "tokens": [51164, 370, 286, 393, 445, 747, 264, 16743, 399, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 285, "seek": 74600, "start": 764.0, "end": 766.0, "text": " the previous aggregation and keep aggregating.", "tokens": [51264, 264, 3894, 16743, 399, 293, 1066, 16743, 990, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 286, "seek": 74600, "start": 766.0, "end": 768.0, "text": " I don't need to keep all the numbers", "tokens": [51364, 286, 500, 380, 643, 281, 1066, 439, 264, 3547, 51464], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 287, "seek": 74600, "start": 768.0, "end": 770.0, "text": " that I have seen so far, so it is easy.", "tokens": [51464, 300, 286, 362, 1612, 370, 1400, 11, 370, 309, 307, 1858, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 288, "seek": 74600, "start": 770.0, "end": 773.0, "text": " In other cases, for any non-associative,", "tokens": [51564, 682, 661, 3331, 11, 337, 604, 2107, 12, 49146, 1166, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 289, "seek": 74600, "start": 773.0, "end": 775.0, "text": " non-commutative operation, so you may need", "tokens": [51714, 2107, 12, 13278, 325, 1166, 6916, 11, 370, 291, 815, 643, 51814], "temperature": 0.0, "avg_logprob": -0.11151789364061858, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.005535943433642387}, {"id": 290, "seek": 77500, "start": 775.0, "end": 779.0, "text": " to have actually full data to produce an update, okay?", "tokens": [50364, 281, 362, 767, 1577, 1412, 281, 5258, 364, 5623, 11, 1392, 30, 50564], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 291, "seek": 77500, "start": 779.0, "end": 781.0, "text": " And if you are working in streaming,", "tokens": [50564, 400, 498, 291, 366, 1364, 294, 11791, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 292, "seek": 77500, "start": 781.0, "end": 784.0, "text": " maybe you don't want to accumulate all the data, okay?", "tokens": [50664, 1310, 291, 500, 380, 528, 281, 33384, 439, 264, 1412, 11, 1392, 30, 50814], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 293, "seek": 77500, "start": 784.0, "end": 786.0, "text": " Because that will increase the amount of resources", "tokens": [50814, 1436, 300, 486, 3488, 264, 2372, 295, 3593, 50914], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 294, "seek": 77500, "start": 786.0, "end": 787.0, "text": " that you will need for your pipeline.", "tokens": [50914, 300, 291, 486, 643, 337, 428, 15517, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 295, "seek": 77500, "start": 787.0, "end": 790.0, "text": " It will have impact in performance, latency and so on.", "tokens": [50964, 467, 486, 362, 2712, 294, 3389, 11, 27043, 293, 370, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 296, "seek": 77500, "start": 790.0, "end": 793.0, "text": " So here, we are accumulating because the operation allows it", "tokens": [51114, 407, 510, 11, 321, 366, 12989, 12162, 570, 264, 6916, 4045, 309, 51264], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 297, "seek": 77500, "start": 793.0, "end": 796.0, "text": " and we are actually waiting for late data, okay?", "tokens": [51264, 293, 321, 366, 767, 3806, 337, 3469, 1412, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 298, "seek": 77500, "start": 796.0, "end": 799.0, "text": " So now, we are waiting for late data,", "tokens": [51414, 407, 586, 11, 321, 366, 3806, 337, 3469, 1412, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 299, "seek": 77500, "start": 799.0, "end": 800.0, "text": " but we don't want to wait forever.", "tokens": [51564, 457, 321, 500, 380, 528, 281, 1699, 5680, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 300, "seek": 77500, "start": 800.0, "end": 802.0, "text": " We want to have some numbers, okay?", "tokens": [51614, 492, 528, 281, 362, 512, 3547, 11, 1392, 30, 51714], "temperature": 0.0, "avg_logprob": -0.07115648073308609, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.0030485671013593674}, {"id": 301, "seek": 80200, "start": 802.0, "end": 806.0, "text": " So we are actually producing several outputs per window, okay?", "tokens": [50364, 407, 321, 366, 767, 10501, 2940, 23930, 680, 4910, 11, 1392, 30, 50564], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 302, "seek": 80200, "start": 806.0, "end": 808.0, "text": " So like for instance here, continuing with the first,", "tokens": [50564, 407, 411, 337, 5197, 510, 11, 9289, 365, 264, 700, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 303, "seek": 80200, "start": 808.0, "end": 810.0, "text": " so when the watermark is trespassed,", "tokens": [50664, 370, 562, 264, 1281, 5638, 307, 46347, 46447, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 304, "seek": 80200, "start": 810.0, "end": 812.0, "text": " we produce an output, okay?", "tokens": [50764, 321, 5258, 364, 5598, 11, 1392, 30, 50864], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 305, "seek": 80200, "start": 812.0, "end": 814.0, "text": " And then when we see the new number,", "tokens": [50864, 400, 550, 562, 321, 536, 264, 777, 1230, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 306, "seek": 80200, "start": 814.0, "end": 815.0, "text": " so we produce the output.", "tokens": [50964, 370, 321, 5258, 264, 5598, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 307, "seek": 80200, "start": 815.0, "end": 817.0, "text": " We produce it really late, okay?", "tokens": [51014, 492, 5258, 309, 534, 3469, 11, 1392, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 308, "seek": 80200, "start": 817.0, "end": 820.0, "text": " But well, so we cannot make magic, okay?", "tokens": [51114, 583, 731, 11, 370, 321, 2644, 652, 5585, 11, 1392, 30, 51264], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 309, "seek": 80200, "start": 820.0, "end": 822.0, "text": " So this is when we see the data,", "tokens": [51264, 407, 341, 307, 562, 321, 536, 264, 1412, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 310, "seek": 80200, "start": 822.0, "end": 825.0, "text": " so we cannot process it earlier than this, okay?", "tokens": [51364, 370, 321, 2644, 1399, 309, 3071, 813, 341, 11, 1392, 30, 51514], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 311, "seek": 80200, "start": 825.0, "end": 828.0, "text": " We may actually decide to produce data, some output,", "tokens": [51514, 492, 815, 767, 4536, 281, 5258, 1412, 11, 512, 5598, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 312, "seek": 80200, "start": 828.0, "end": 830.0, "text": " even before the watermark,", "tokens": [51664, 754, 949, 264, 1281, 5638, 11, 51764], "temperature": 0.0, "avg_logprob": -0.10261483052197624, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.010461517609655857}, {"id": 313, "seek": 83000, "start": 830.0, "end": 833.0, "text": " because the watermark can be really slow.", "tokens": [50364, 570, 264, 1281, 5638, 393, 312, 534, 2964, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 314, "seek": 83000, "start": 833.0, "end": 835.0, "text": " It depends on the pace of the updates of the data.", "tokens": [50514, 467, 5946, 322, 264, 11638, 295, 264, 9205, 295, 264, 1412, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 315, "seek": 83000, "start": 835.0, "end": 836.0, "text": " If for whatever reason,", "tokens": [50614, 759, 337, 2035, 1778, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 316, "seek": 83000, "start": 836.0, "end": 840.0, "text": " users are sending your data with a lot of lateness,", "tokens": [50664, 5022, 366, 7750, 428, 1412, 365, 257, 688, 295, 4465, 15264, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 317, "seek": 83000, "start": 840.0, "end": 843.0, "text": " the watermark can progress really slowly, okay?", "tokens": [50864, 264, 1281, 5638, 393, 4205, 534, 5692, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 318, "seek": 83000, "start": 843.0, "end": 846.0, "text": " And so the watermark, how you produce output is always", "tokens": [51014, 400, 370, 264, 1281, 5638, 11, 577, 291, 5258, 5598, 307, 1009, 51164], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 319, "seek": 83000, "start": 846.0, "end": 848.0, "text": " a trade-off in the streaming between completeness", "tokens": [51164, 257, 4923, 12, 4506, 294, 264, 11791, 1296, 1557, 15264, 51264], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 320, "seek": 83000, "start": 848.0, "end": 849.0, "text": " and latency.", "tokens": [51264, 293, 27043, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 321, "seek": 83000, "start": 849.0, "end": 851.0, "text": " You need to make a decision, okay?", "tokens": [51314, 509, 643, 281, 652, 257, 3537, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 322, "seek": 83000, "start": 851.0, "end": 855.0, "text": " So here, we put an early trigger.", "tokens": [51414, 407, 510, 11, 321, 829, 364, 2440, 7875, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09232338761861346, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.003490026108920574}, {"id": 323, "seek": 85500, "start": 855.0, "end": 859.0, "text": " So we're producing output soon, low latency,", "tokens": [50364, 407, 321, 434, 10501, 5598, 2321, 11, 2295, 27043, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 324, "seek": 85500, "start": 859.0, "end": 861.0, "text": " but it's incomplete, because well,", "tokens": [50564, 457, 309, 311, 31709, 11, 570, 731, 11, 50664], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 325, "seek": 85500, "start": 861.0, "end": 863.0, "text": " so later on we're gonna keep seeing numbers", "tokens": [50664, 370, 1780, 322, 321, 434, 799, 1066, 2577, 3547, 50764], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 326, "seek": 85500, "start": 863.0, "end": 865.0, "text": " until the watermark.", "tokens": [50764, 1826, 264, 1281, 5638, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 327, "seek": 85500, "start": 865.0, "end": 866.0, "text": " Good.", "tokens": [50864, 2205, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 328, "seek": 85500, "start": 866.0, "end": 870.0, "text": " So basically, this is streaming in Apache Bin in 10 minutes.", "tokens": [50914, 407, 1936, 11, 341, 307, 11791, 294, 46597, 18983, 294, 1266, 2077, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 329, "seek": 85500, "start": 870.0, "end": 874.0, "text": " This is a lot of information, explained very quickly.", "tokens": [51114, 639, 307, 257, 688, 295, 1589, 11, 8825, 588, 2661, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 330, "seek": 85500, "start": 874.0, "end": 876.0, "text": " If you want to get deeper,", "tokens": [51314, 759, 291, 528, 281, 483, 7731, 11, 51414], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 331, "seek": 85500, "start": 876.0, "end": 878.0, "text": " if you want to get deeper,", "tokens": [51414, 498, 291, 528, 281, 483, 7731, 11, 51514], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 332, "seek": 85500, "start": 878.0, "end": 880.0, "text": " there's this example here, okay?", "tokens": [51514, 456, 311, 341, 1365, 510, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 333, "seek": 85500, "start": 880.0, "end": 882.0, "text": " So in Java and in Python,", "tokens": [51614, 407, 294, 10745, 293, 294, 15329, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 334, "seek": 85500, "start": 882.0, "end": 884.0, "text": " so it's available in the two languages,", "tokens": [51714, 370, 309, 311, 2435, 294, 264, 732, 8650, 11, 51814], "temperature": 0.0, "avg_logprob": -0.14737840558661788, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.018034152686595917}, {"id": 335, "seek": 88400, "start": 884.0, "end": 887.0, "text": " and you can see everything that we have seen", "tokens": [50364, 293, 291, 393, 536, 1203, 300, 321, 362, 1612, 50514], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 336, "seek": 88400, "start": 887.0, "end": 890.0, "text": " in the previous slides with all details, okay?", "tokens": [50514, 294, 264, 3894, 9788, 365, 439, 4365, 11, 1392, 30, 50664], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 337, "seek": 88400, "start": 890.0, "end": 892.0, "text": " And you may run this locally if you want,", "tokens": [50664, 400, 291, 815, 1190, 341, 16143, 498, 291, 528, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 338, "seek": 88400, "start": 892.0, "end": 896.0, "text": " so you don't have to have like an environment,", "tokens": [50764, 370, 291, 500, 380, 362, 281, 362, 411, 364, 2823, 11, 50964], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 339, "seek": 88400, "start": 896.0, "end": 898.0, "text": " so like a cloud environment, a cluster,", "tokens": [50964, 370, 411, 257, 4588, 2823, 11, 257, 13630, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 340, "seek": 88400, "start": 898.0, "end": 900.0, "text": " or a stream process or anything like that,", "tokens": [51064, 420, 257, 4309, 1399, 420, 1340, 411, 300, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 341, "seek": 88400, "start": 900.0, "end": 906.0, "text": " so it may run locally with some synthetic data,", "tokens": [51164, 370, 309, 815, 1190, 16143, 365, 512, 23420, 1412, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 342, "seek": 88400, "start": 906.0, "end": 909.0, "text": " made-up data, okay?", "tokens": [51464, 1027, 12, 1010, 1412, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 343, "seek": 88400, "start": 909.0, "end": 912.0, "text": " Now, this is the classic way of doing streaming in Apache Bin.", "tokens": [51614, 823, 11, 341, 307, 264, 7230, 636, 295, 884, 11791, 294, 46597, 18983, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12675154974701208, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.00110274413600564}, {"id": 344, "seek": 91200, "start": 912.0, "end": 915.0, "text": " This has been around for years already, okay?", "tokens": [50364, 639, 575, 668, 926, 337, 924, 1217, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 345, "seek": 91200, "start": 915.0, "end": 918.0, "text": " So this is the same model that is implemented in Spark,", "tokens": [50514, 407, 341, 307, 264, 912, 2316, 300, 307, 12270, 294, 23424, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 346, "seek": 91200, "start": 918.0, "end": 920.0, "text": " it's the same model that is implemented in Flink,", "tokens": [50664, 309, 311, 264, 912, 2316, 300, 307, 12270, 294, 3235, 475, 11, 50764], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 347, "seek": 91200, "start": 920.0, "end": 923.0, "text": " so they are all kind of similar.", "tokens": [50764, 370, 436, 366, 439, 733, 295, 2531, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 348, "seek": 91200, "start": 923.0, "end": 927.0, "text": " There are other things that you can also do in Apache Bin", "tokens": [50914, 821, 366, 661, 721, 300, 291, 393, 611, 360, 294, 46597, 18983, 51114], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 349, "seek": 91200, "start": 927.0, "end": 931.0, "text": " in streaming, like anything that you can do in Apache Bin,", "tokens": [51114, 294, 11791, 11, 411, 1340, 300, 291, 393, 360, 294, 46597, 18983, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 350, "seek": 91200, "start": 931.0, "end": 933.0, "text": " you can also do it in streaming,", "tokens": [51314, 291, 393, 611, 360, 309, 294, 11791, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 351, "seek": 91200, "start": 933.0, "end": 936.0, "text": " and I'm gonna highlight here a couple of those, okay?", "tokens": [51414, 293, 286, 478, 799, 5078, 510, 257, 1916, 295, 729, 11, 1392, 30, 51564], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 352, "seek": 91200, "start": 936.0, "end": 938.0, "text": " I'm leaving out a lot of stuff,", "tokens": [51564, 286, 478, 5012, 484, 257, 688, 295, 1507, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07482854729024772, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.022896450012922287}, {"id": 353, "seek": 93800, "start": 938.0, "end": 941.0, "text": " because, well, so time is limited,", "tokens": [50364, 570, 11, 731, 11, 370, 565, 307, 5567, 11, 50514], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 354, "seek": 93800, "start": 941.0, "end": 944.0, "text": " and leave it out for instance SQL,", "tokens": [50514, 293, 1856, 309, 484, 337, 5197, 19200, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 355, "seek": 93800, "start": 944.0, "end": 947.0, "text": " so that was a great talk by Timo focusing on SQL,", "tokens": [50664, 370, 300, 390, 257, 869, 751, 538, 314, 6934, 8416, 322, 19200, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 356, "seek": 93800, "start": 947.0, "end": 949.0, "text": " so you can also do SQL in Apache Bin", "tokens": [50814, 370, 291, 393, 611, 360, 19200, 294, 46597, 18983, 50914], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 357, "seek": 93800, "start": 949.0, "end": 951.0, "text": " if you want in streaming, okay?", "tokens": [50914, 498, 291, 528, 294, 11791, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 358, "seek": 93800, "start": 951.0, "end": 954.0, "text": " So similar examples to what Timo did,", "tokens": [51014, 407, 2531, 5110, 281, 437, 314, 6934, 630, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 359, "seek": 93800, "start": 954.0, "end": 956.0, "text": " and you can actually run that on Flink if you want, okay?", "tokens": [51164, 293, 291, 393, 767, 1190, 300, 322, 3235, 475, 498, 291, 528, 11, 1392, 30, 51264], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 360, "seek": 93800, "start": 956.0, "end": 959.0, "text": " So it may make sense if you, well, I don't know,", "tokens": [51264, 407, 309, 815, 652, 2020, 498, 291, 11, 731, 11, 286, 500, 380, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 361, "seek": 93800, "start": 959.0, "end": 962.0, "text": " at some point you want to move away from Flink to Dataflow,", "tokens": [51414, 412, 512, 935, 291, 528, 281, 1286, 1314, 490, 3235, 475, 281, 9315, 2792, 14107, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 362, "seek": 93800, "start": 962.0, "end": 964.0, "text": " you want to move away from Dataflow to Spark,", "tokens": [51564, 291, 528, 281, 1286, 1314, 490, 9315, 2792, 14107, 281, 23424, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 363, "seek": 93800, "start": 964.0, "end": 967.0, "text": " so in order to have this portability.", "tokens": [51664, 370, 294, 1668, 281, 362, 341, 2436, 2310, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11274048422469574, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.025219019502401352}, {"id": 364, "seek": 96700, "start": 967.0, "end": 970.0, "text": " One thing that you can do in streaming is stateful functions,", "tokens": [50364, 1485, 551, 300, 291, 393, 360, 294, 11791, 307, 1785, 906, 6828, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 365, "seek": 96700, "start": 970.0, "end": 973.0, "text": " and stateful functions are very interesting", "tokens": [50514, 293, 1785, 906, 6828, 366, 588, 1880, 50664], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 366, "seek": 96700, "start": 973.0, "end": 977.0, "text": " for windowing between quotes that doesn't depend on time.", "tokens": [50664, 337, 4910, 278, 1296, 19963, 300, 1177, 380, 5672, 322, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 367, "seek": 96700, "start": 977.0, "end": 979.0, "text": " Very typically, I work with customers,", "tokens": [50864, 4372, 5850, 11, 286, 589, 365, 4581, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 368, "seek": 96700, "start": 979.0, "end": 981.0, "text": " like all these windowing trigger things,", "tokens": [50964, 411, 439, 613, 4910, 278, 7875, 721, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 369, "seek": 96700, "start": 981.0, "end": 983.0, "text": " it's super interesting, but look,", "tokens": [51064, 309, 311, 1687, 1880, 11, 457, 574, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 370, "seek": 96700, "start": 983.0, "end": 986.0, "text": " whenever I see a message of this type,", "tokens": [51164, 5699, 286, 536, 257, 3636, 295, 341, 2010, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 371, "seek": 96700, "start": 986.0, "end": 990.0, "text": " I want to have all the messages that I have seen so far", "tokens": [51314, 286, 528, 281, 362, 439, 264, 7897, 300, 286, 362, 1612, 370, 1400, 51514], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 372, "seek": 96700, "start": 990.0, "end": 992.0, "text": " in a group and do these calculations,", "tokens": [51514, 294, 257, 1594, 293, 360, 613, 20448, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 373, "seek": 96700, "start": 992.0, "end": 994.0, "text": " and I don't care about time, okay?", "tokens": [51614, 293, 286, 500, 380, 1127, 466, 565, 11, 1392, 30, 51714], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 374, "seek": 96700, "start": 994.0, "end": 996.0, "text": " I don't care about grouping things in time.", "tokens": [51714, 286, 500, 380, 1127, 466, 40149, 721, 294, 565, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09148841271033654, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.00814855471253395}, {"id": 375, "seek": 99600, "start": 996.0, "end": 999.0, "text": " I want to group things by some logic, okay?", "tokens": [50364, 286, 528, 281, 1594, 721, 538, 512, 9952, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 376, "seek": 99600, "start": 999.0, "end": 1002.0, "text": " I'm gonna give you a predicate, you pass a message,", "tokens": [50514, 286, 478, 799, 976, 291, 257, 3852, 8700, 11, 291, 1320, 257, 3636, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 377, "seek": 99600, "start": 1002.0, "end": 1004.0, "text": " if the message fulfills a condition,", "tokens": [50664, 498, 264, 3636, 8081, 2565, 257, 4188, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 378, "seek": 99600, "start": 1004.0, "end": 1007.0, "text": " I want to close the previous window and start a new one.", "tokens": [50764, 286, 528, 281, 1998, 264, 3894, 4910, 293, 722, 257, 777, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 379, "seek": 99600, "start": 1007.0, "end": 1009.0, "text": " How can you do that in Apache bin?", "tokens": [50914, 1012, 393, 291, 360, 300, 294, 46597, 5171, 30, 51014], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 380, "seek": 99600, "start": 1009.0, "end": 1012.0, "text": " You can do that with stateful functions, okay?", "tokens": [51014, 509, 393, 360, 300, 365, 1785, 906, 6828, 11, 1392, 30, 51164], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 381, "seek": 99600, "start": 1012.0, "end": 1015.0, "text": " Stateful functions, so here we have some input,", "tokens": [51164, 4533, 906, 6828, 11, 370, 510, 321, 362, 512, 4846, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 382, "seek": 99600, "start": 1015.0, "end": 1018.0, "text": " here we have a map, it's called a part doing Apache bin,", "tokens": [51314, 510, 321, 362, 257, 4471, 11, 309, 311, 1219, 257, 644, 884, 46597, 5171, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 383, "seek": 99600, "start": 1018.0, "end": 1020.0, "text": " and we do some transformation,", "tokens": [51464, 293, 321, 360, 512, 9887, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 384, "seek": 99600, "start": 1020.0, "end": 1022.0, "text": " and we want to accumulate a state here, okay?", "tokens": [51564, 293, 321, 528, 281, 33384, 257, 1785, 510, 11, 1392, 30, 51664], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 385, "seek": 99600, "start": 1022.0, "end": 1025.0, "text": " So depending on what we see at some point,", "tokens": [51664, 407, 5413, 322, 437, 321, 536, 412, 512, 935, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11163319407643138, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.004358573351055384}, {"id": 386, "seek": 102500, "start": 1025.0, "end": 1029.0, "text": " we do something else, and this is mutable state, okay?", "tokens": [50364, 321, 360, 746, 1646, 11, 293, 341, 307, 5839, 712, 1785, 11, 1392, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 387, "seek": 102500, "start": 1029.0, "end": 1032.0, "text": " In a system like Dataflow, like Flink,", "tokens": [50564, 682, 257, 1185, 411, 9315, 2792, 14107, 11, 411, 3235, 475, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 388, "seek": 102500, "start": 1032.0, "end": 1036.0, "text": " like all the systems where Apache bin runs,", "tokens": [50714, 411, 439, 264, 3652, 689, 46597, 5171, 6676, 11, 50914], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 389, "seek": 102500, "start": 1036.0, "end": 1041.0, "text": " having state, mutable state in a streaming", "tokens": [50914, 1419, 1785, 11, 5839, 712, 1785, 294, 257, 11791, 51164], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 390, "seek": 102500, "start": 1041.0, "end": 1044.0, "text": " that is computed in a consistent way", "tokens": [51164, 300, 307, 40610, 294, 257, 8398, 636, 51314], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 391, "seek": 102500, "start": 1044.0, "end": 1046.0, "text": " is extremely difficult, okay?", "tokens": [51314, 307, 4664, 2252, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 392, "seek": 102500, "start": 1046.0, "end": 1050.0, "text": " One way to shoot yourself in your feet", "tokens": [51414, 1485, 636, 281, 3076, 1803, 294, 428, 3521, 51614], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 393, "seek": 102500, "start": 1050.0, "end": 1053.0, "text": " with systems like this in streaming", "tokens": [51614, 365, 3652, 411, 341, 294, 11791, 51764], "temperature": 0.0, "avg_logprob": -0.08217512623647626, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.006532949395477772}, {"id": 394, "seek": 105300, "start": 1053.0, "end": 1055.0, "text": " is trying to keep accumulating state", "tokens": [50364, 307, 1382, 281, 1066, 12989, 12162, 1785, 50464], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 395, "seek": 105300, "start": 1055.0, "end": 1058.0, "text": " using some kind of external system, okay?", "tokens": [50464, 1228, 512, 733, 295, 8320, 1185, 11, 1392, 30, 50614], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 396, "seek": 105300, "start": 1058.0, "end": 1062.0, "text": " Because runners will have...", "tokens": [50614, 1436, 33892, 486, 362, 485, 50814], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 397, "seek": 105300, "start": 1062.0, "end": 1064.0, "text": " sometimes will have issues that will be errors,", "tokens": [50814, 2171, 486, 362, 2663, 300, 486, 312, 13603, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 398, "seek": 105300, "start": 1064.0, "end": 1068.0, "text": " that will be retries, infrastructure will die,", "tokens": [50914, 300, 486, 312, 1533, 2244, 11, 6896, 486, 978, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 399, "seek": 105300, "start": 1068.0, "end": 1070.0, "text": " you will have auto-scaling.", "tokens": [51114, 291, 486, 362, 8399, 12, 4417, 4270, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 400, "seek": 105300, "start": 1070.0, "end": 1072.0, "text": " There are all kinds of situations", "tokens": [51214, 821, 366, 439, 3685, 295, 6851, 51314], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 401, "seek": 105300, "start": 1072.0, "end": 1076.0, "text": " that the runner may want to retract the computation", "tokens": [51314, 300, 264, 24376, 815, 528, 281, 41107, 264, 24903, 51514], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 402, "seek": 105300, "start": 1076.0, "end": 1078.0, "text": " and recompute again, okay?", "tokens": [51514, 293, 48000, 1169, 797, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 403, "seek": 105300, "start": 1078.0, "end": 1080.0, "text": " And then in these kinds of situations,", "tokens": [51614, 400, 550, 294, 613, 3685, 295, 6851, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 404, "seek": 105300, "start": 1080.0, "end": 1082.0, "text": " having any kind of external system for mutable state,", "tokens": [51714, 1419, 604, 733, 295, 8320, 1185, 337, 5839, 712, 1785, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11595212885763793, "compression_ratio": 1.839662447257384, "no_speech_prob": 0.007048478815704584}, {"id": 405, "seek": 108200, "start": 1082.0, "end": 1083.0, "text": " it's complex, okay?", "tokens": [50364, 309, 311, 3997, 11, 1392, 30, 50414], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 406, "seek": 108200, "start": 1083.0, "end": 1085.0, "text": " It's doable, okay?", "tokens": [50414, 467, 311, 41183, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 407, "seek": 108200, "start": 1085.0, "end": 1087.0, "text": " You may have, and you will have with Apache bin", "tokens": [50514, 509, 815, 362, 11, 293, 291, 486, 362, 365, 46597, 5171, 50614], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 408, "seek": 108200, "start": 1087.0, "end": 1089.0, "text": " in any kind of the runners that you can run,", "tokens": [50614, 294, 604, 733, 295, 264, 33892, 300, 291, 393, 1190, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 409, "seek": 108200, "start": 1089.0, "end": 1092.0, "text": " you will have this end-to-end exactly once processing,", "tokens": [50714, 291, 486, 362, 341, 917, 12, 1353, 12, 521, 2293, 1564, 9007, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 410, "seek": 108200, "start": 1092.0, "end": 1094.0, "text": " but this end-to-end exactly once processing", "tokens": [50864, 457, 341, 917, 12, 1353, 12, 521, 2293, 1564, 9007, 50964], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 411, "seek": 108200, "start": 1094.0, "end": 1096.0, "text": " doesn't mean that your code is going to be executed", "tokens": [50964, 1177, 380, 914, 300, 428, 3089, 307, 516, 281, 312, 17577, 51064], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 412, "seek": 108200, "start": 1096.0, "end": 1098.0, "text": " exactly once.", "tokens": [51064, 2293, 1564, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 413, "seek": 108200, "start": 1098.0, "end": 1100.0, "text": " It may be executed more than once, okay?", "tokens": [51164, 467, 815, 312, 17577, 544, 813, 1564, 11, 1392, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 414, "seek": 108200, "start": 1100.0, "end": 1102.0, "text": " This is what makes maintaining", "tokens": [51264, 639, 307, 437, 1669, 14916, 51364], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 415, "seek": 108200, "start": 1102.0, "end": 1104.0, "text": " external state to a pipeline complex.", "tokens": [51364, 8320, 1785, 281, 257, 15517, 3997, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 416, "seek": 108200, "start": 1104.0, "end": 1107.0, "text": " But if the state is internal to the pipeline,", "tokens": [51464, 583, 498, 264, 1785, 307, 6920, 281, 264, 15517, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 417, "seek": 108200, "start": 1107.0, "end": 1111.0, "text": " then, well, so the system itself can, let's say,", "tokens": [51614, 550, 11, 731, 11, 370, 264, 1185, 2564, 393, 11, 718, 311, 584, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09573353734509699, "compression_ratio": 1.8834586466165413, "no_speech_prob": 0.019866585731506348}, {"id": 418, "seek": 111100, "start": 1111.0, "end": 1113.0, "text": " take care of the problems of reprocessing", "tokens": [50364, 747, 1127, 295, 264, 2740, 295, 35257, 780, 278, 50464], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 419, "seek": 111100, "start": 1113.0, "end": 1116.0, "text": " and maintain a mutable state in a consistent way.", "tokens": [50464, 293, 6909, 257, 5839, 712, 1785, 294, 257, 8398, 636, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 420, "seek": 111100, "start": 1116.0, "end": 1120.0, "text": " So this is where it's a stateful function in Apache bin,", "tokens": [50614, 407, 341, 307, 689, 309, 311, 257, 1785, 906, 2445, 294, 46597, 5171, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 421, "seek": 111100, "start": 1120.0, "end": 1124.0, "text": " and you can use it for use cases like this, okay?", "tokens": [50814, 293, 291, 393, 764, 309, 337, 764, 3331, 411, 341, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 422, "seek": 111100, "start": 1124.0, "end": 1129.0, "text": " For instance, say that I want to produce windows", "tokens": [51014, 1171, 5197, 11, 584, 300, 286, 528, 281, 5258, 9309, 51264], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 423, "seek": 111100, "start": 1129.0, "end": 1132.0, "text": " between quotes based on some kind of property.", "tokens": [51264, 1296, 19963, 2361, 322, 512, 733, 295, 4707, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 424, "seek": 111100, "start": 1132.0, "end": 1136.0, "text": " So I keep seeing messages, okay, that I keep processing, okay?", "tokens": [51414, 407, 286, 1066, 2577, 7897, 11, 1392, 11, 300, 286, 1066, 9007, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 425, "seek": 111100, "start": 1136.0, "end": 1140.0, "text": " And then I keep accumulating the messages in some state, okay?", "tokens": [51614, 400, 550, 286, 1066, 12989, 12162, 264, 7897, 294, 512, 1785, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.09222852656271606, "compression_ratio": 1.68, "no_speech_prob": 0.004322024527937174}, {"id": 426, "seek": 114000, "start": 1140.0, "end": 1143.0, "text": " I maintain a buffer, like I keep every single message", "tokens": [50364, 286, 6909, 257, 21762, 11, 411, 286, 1066, 633, 2167, 3636, 50514], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 427, "seek": 114000, "start": 1143.0, "end": 1145.0, "text": " that I see and I count, okay?", "tokens": [50514, 300, 286, 536, 293, 286, 1207, 11, 1392, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 428, "seek": 114000, "start": 1145.0, "end": 1147.0, "text": " Because, well, the buffer cannot,", "tokens": [50614, 1436, 11, 731, 11, 264, 21762, 2644, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 429, "seek": 114000, "start": 1147.0, "end": 1149.0, "text": " so the buffer must have some boundaries, okay?", "tokens": [50714, 370, 264, 21762, 1633, 362, 512, 13180, 11, 1392, 30, 50814], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 430, "seek": 114000, "start": 1149.0, "end": 1152.0, "text": " So because this is local state that is maintaining the machine", "tokens": [50814, 407, 570, 341, 307, 2654, 1785, 300, 307, 14916, 264, 3479, 50964], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 431, "seek": 114000, "start": 1152.0, "end": 1155.0, "text": " in the worker, in the executor where you are running,", "tokens": [50964, 294, 264, 11346, 11, 294, 264, 7568, 284, 689, 291, 366, 2614, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 432, "seek": 114000, "start": 1155.0, "end": 1157.0, "text": " and the executor will have limited resources.", "tokens": [51114, 293, 264, 7568, 284, 486, 362, 5567, 3593, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 433, "seek": 114000, "start": 1157.0, "end": 1161.0, "text": " It might be very large resources, but limited anyways, okay?", "tokens": [51214, 467, 1062, 312, 588, 2416, 3593, 11, 457, 5567, 13448, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 434, "seek": 114000, "start": 1161.0, "end": 1163.0, "text": " So you keep accumulating,", "tokens": [51414, 407, 291, 1066, 12989, 12162, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 435, "seek": 114000, "start": 1163.0, "end": 1166.0, "text": " and then you keep processing here, for instance.", "tokens": [51514, 293, 550, 291, 1066, 9007, 510, 11, 337, 5197, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 436, "seek": 114000, "start": 1166.0, "end": 1168.0, "text": " So typically, you can use this, for instance,", "tokens": [51664, 407, 5850, 11, 291, 393, 764, 341, 11, 337, 5197, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1096703917891891, "compression_ratio": 1.8576642335766422, "no_speech_prob": 0.004453735426068306}, {"id": 437, "seek": 116800, "start": 1168.0, "end": 1170.0, "text": " batching to call in an external service,", "tokens": [50364, 15245, 278, 281, 818, 294, 364, 8320, 2643, 11, 50464], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 438, "seek": 116800, "start": 1170.0, "end": 1172.0, "text": " but you can also do here,", "tokens": [50464, 457, 291, 393, 611, 360, 510, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 439, "seek": 116800, "start": 1172.0, "end": 1174.0, "text": " whenever I see a specific type of message,", "tokens": [50564, 5699, 286, 536, 257, 2685, 2010, 295, 3636, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 440, "seek": 116800, "start": 1174.0, "end": 1176.0, "text": " I emit some output, okay?", "tokens": [50664, 286, 32084, 512, 5598, 11, 1392, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 441, "seek": 116800, "start": 1176.0, "end": 1179.0, "text": " I emit some output, and then I have applied a window.", "tokens": [50764, 286, 32084, 512, 5598, 11, 293, 550, 286, 362, 6456, 257, 4910, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 442, "seek": 116800, "start": 1179.0, "end": 1181.0, "text": " All the messages that I have in the buffer,", "tokens": [50914, 1057, 264, 7897, 300, 286, 362, 294, 264, 21762, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 443, "seek": 116800, "start": 1181.0, "end": 1184.0, "text": " I tag a new session ID, a new window ID,", "tokens": [51014, 286, 6162, 257, 777, 5481, 7348, 11, 257, 777, 4910, 7348, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 444, "seek": 116800, "start": 1184.0, "end": 1186.0, "text": " and then I emit them.", "tokens": [51164, 293, 550, 286, 32084, 552, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 445, "seek": 116800, "start": 1186.0, "end": 1188.0, "text": " I hold them for a while until I see the right message", "tokens": [51264, 286, 1797, 552, 337, 257, 1339, 1826, 286, 536, 264, 558, 3636, 51364], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 446, "seek": 116800, "start": 1188.0, "end": 1190.0, "text": " that I need, and then I emit them.", "tokens": [51364, 300, 286, 643, 11, 293, 550, 286, 32084, 552, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 447, "seek": 116800, "start": 1190.0, "end": 1193.0, "text": " There are two problems here.", "tokens": [51464, 821, 366, 732, 2740, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 448, "seek": 116800, "start": 1193.0, "end": 1197.0, "text": " We want to, so customers always think that streaming is complex,", "tokens": [51614, 492, 528, 281, 11, 370, 4581, 1009, 519, 300, 11791, 307, 3997, 11, 51814], "temperature": 0.0, "avg_logprob": -0.07624153409685408, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.012658342719078064}, {"id": 449, "seek": 119700, "start": 1197.0, "end": 1202.0, "text": " and they want to get away of all the temporal-based", "tokens": [50364, 293, 436, 528, 281, 483, 1314, 295, 439, 264, 30881, 12, 6032, 50614], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 450, "seek": 119700, "start": 1202.0, "end": 1204.0, "text": " calculations, okay?", "tokens": [50614, 20448, 11, 1392, 30, 50714], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 451, "seek": 119700, "start": 1204.0, "end": 1206.0, "text": " It's so complex, so messy.", "tokens": [50714, 467, 311, 370, 3997, 11, 370, 16191, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 452, "seek": 119700, "start": 1206.0, "end": 1209.0, "text": " Look, my algorithm is really much simpler, but it is not.", "tokens": [50814, 2053, 11, 452, 9284, 307, 534, 709, 18587, 11, 457, 309, 307, 406, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 453, "seek": 119700, "start": 1209.0, "end": 1213.0, "text": " So you are in the streaming, so you cannot ignore time, okay?", "tokens": [50964, 407, 291, 366, 294, 264, 11791, 11, 370, 291, 2644, 11200, 565, 11, 1392, 30, 51164], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 454, "seek": 119700, "start": 1213.0, "end": 1215.0, "text": " You have situations where you will see the messages", "tokens": [51164, 509, 362, 6851, 689, 291, 486, 536, 264, 7897, 51264], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 455, "seek": 119700, "start": 1215.0, "end": 1217.0, "text": " out of order, and you will see,", "tokens": [51264, 484, 295, 1668, 11, 293, 291, 486, 536, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 456, "seek": 119700, "start": 1217.0, "end": 1219.0, "text": " you will have situations where you will not see", "tokens": [51364, 291, 486, 362, 6851, 689, 291, 486, 406, 536, 51464], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 457, "seek": 119700, "start": 1219.0, "end": 1221.0, "text": " the messages for a while, okay?", "tokens": [51464, 264, 7897, 337, 257, 1339, 11, 1392, 30, 51564], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 458, "seek": 119700, "start": 1221.0, "end": 1223.0, "text": " And then you need to decide what to do in these two cases,", "tokens": [51564, 400, 550, 291, 643, 281, 4536, 437, 281, 360, 294, 613, 732, 3331, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 459, "seek": 119700, "start": 1223.0, "end": 1225.0, "text": " even if you don't want to, okay?", "tokens": [51664, 754, 498, 291, 500, 380, 528, 281, 11, 1392, 30, 51764], "temperature": 0.0, "avg_logprob": -0.11397243247312658, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.003270806511864066}, {"id": 460, "seek": 122500, "start": 1225.0, "end": 1227.0, "text": " What happens when I see out of order?", "tokens": [50364, 708, 2314, 562, 286, 536, 484, 295, 1668, 30, 50464], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 461, "seek": 122500, "start": 1227.0, "end": 1229.0, "text": " You may say, I don't care, unlikely,", "tokens": [50464, 509, 815, 584, 11, 286, 500, 380, 1127, 11, 17518, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 462, "seek": 122500, "start": 1229.0, "end": 1232.0, "text": " but well, in some situations, it might be true, okay?", "tokens": [50564, 457, 731, 11, 294, 512, 6851, 11, 309, 1062, 312, 2074, 11, 1392, 30, 50714], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 463, "seek": 122500, "start": 1232.0, "end": 1238.0, "text": " Or you may have to wait, like, some timer in order to give room", "tokens": [50714, 1610, 291, 815, 362, 281, 1699, 11, 411, 11, 512, 19247, 294, 1668, 281, 976, 1808, 51014], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 464, "seek": 122500, "start": 1238.0, "end": 1241.0, "text": " for late data to arrive into your code", "tokens": [51014, 337, 3469, 1412, 281, 8881, 666, 428, 3089, 51164], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 465, "seek": 122500, "start": 1241.0, "end": 1244.0, "text": " and actually produce the actual output, okay?", "tokens": [51164, 293, 767, 5258, 264, 3539, 5598, 11, 1392, 30, 51314], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 466, "seek": 122500, "start": 1244.0, "end": 1249.0, "text": " So this would be an event-time timer, okay?", "tokens": [51314, 407, 341, 576, 312, 364, 2280, 12, 3766, 19247, 11, 1392, 30, 51564], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 467, "seek": 122500, "start": 1249.0, "end": 1253.0, "text": " Look, in event-time, you are going to see the messages", "tokens": [51564, 2053, 11, 294, 2280, 12, 3766, 11, 291, 366, 516, 281, 536, 264, 7897, 51764], "temperature": 0.0, "avg_logprob": -0.11181658667487067, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.006078952923417091}, {"id": 468, "seek": 125300, "start": 1253.0, "end": 1254.0, "text": " in order, okay?", "tokens": [50364, 294, 1668, 11, 1392, 30, 50414], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 469, "seek": 125300, "start": 1254.0, "end": 1257.0, "text": " So wait 30 seconds, two minutes, and so on.", "tokens": [50414, 407, 1699, 2217, 3949, 11, 732, 2077, 11, 293, 370, 322, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 470, "seek": 125300, "start": 1257.0, "end": 1259.0, "text": " And then when you have seen all the messages,", "tokens": [50564, 400, 550, 562, 291, 362, 1612, 439, 264, 7897, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 471, "seek": 125300, "start": 1259.0, "end": 1261.0, "text": " it's the moment in which you apply the session.", "tokens": [50664, 309, 311, 264, 1623, 294, 597, 291, 3079, 264, 5481, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 472, "seek": 125300, "start": 1261.0, "end": 1263.0, "text": " That's called an event-time timer.", "tokens": [50764, 663, 311, 1219, 364, 2280, 12, 3766, 19247, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 473, "seek": 125300, "start": 1263.0, "end": 1266.0, "text": " And you may have also problems of staleness, okay?", "tokens": [50864, 400, 291, 815, 362, 611, 2740, 295, 49875, 15264, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 474, "seek": 125300, "start": 1266.0, "end": 1268.0, "text": " I'm waiting for the end of my session,", "tokens": [51014, 286, 478, 3806, 337, 264, 917, 295, 452, 5481, 11, 51114], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 475, "seek": 125300, "start": 1268.0, "end": 1269.0, "text": " but I've not seen messages.", "tokens": [51114, 457, 286, 600, 406, 1612, 7897, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 476, "seek": 125300, "start": 1269.0, "end": 1272.0, "text": " I haven't seen messages in the last five minutes.", "tokens": [51164, 286, 2378, 380, 1612, 7897, 294, 264, 1036, 1732, 2077, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 477, "seek": 125300, "start": 1272.0, "end": 1274.0, "text": " In processing time, okay?", "tokens": [51314, 682, 9007, 565, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 478, "seek": 125300, "start": 1274.0, "end": 1276.0, "text": " The problem with event-time is that it depends", "tokens": [51414, 440, 1154, 365, 2280, 12, 3766, 307, 300, 309, 5946, 51514], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 479, "seek": 125300, "start": 1276.0, "end": 1277.0, "text": " on the progress of the watermark.", "tokens": [51514, 322, 264, 4205, 295, 264, 1281, 5638, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 480, "seek": 125300, "start": 1277.0, "end": 1279.0, "text": " But if you stop seeing messages,", "tokens": [51564, 583, 498, 291, 1590, 2577, 7897, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 481, "seek": 125300, "start": 1279.0, "end": 1281.0, "text": " the watermark will stop advancing.", "tokens": [51664, 264, 1281, 5638, 486, 1590, 27267, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07843611932569934, "compression_ratio": 1.8184931506849316, "no_speech_prob": 0.010943969711661339}, {"id": 482, "seek": 128100, "start": 1281.0, "end": 1286.0, "text": " The watermark is always estimated in being runners", "tokens": [50364, 440, 1281, 5638, 307, 1009, 14109, 294, 885, 33892, 50614], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 483, "seek": 128100, "start": 1286.0, "end": 1289.0, "text": " or normally estimated as the time stamp", "tokens": [50614, 420, 5646, 14109, 382, 264, 565, 9921, 50764], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 484, "seek": 128100, "start": 1289.0, "end": 1292.0, "text": " of the oldest message waiting to be processed, okay?", "tokens": [50764, 295, 264, 14026, 3636, 3806, 281, 312, 18846, 11, 1392, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 485, "seek": 128100, "start": 1292.0, "end": 1295.0, "text": " So literally you may stop your pipeline waiting forever", "tokens": [50914, 407, 3736, 291, 815, 1590, 428, 15517, 3806, 5680, 51064], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 486, "seek": 128100, "start": 1295.0, "end": 1298.0, "text": " for some data that maybe it will never arrive, okay?", "tokens": [51064, 337, 512, 1412, 300, 1310, 309, 486, 1128, 8881, 11, 1392, 30, 51214], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 487, "seek": 128100, "start": 1298.0, "end": 1301.0, "text": " So processing time-time will stop this problem, okay?", "tokens": [51214, 407, 9007, 565, 12, 3766, 486, 1590, 341, 1154, 11, 1392, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 488, "seek": 128100, "start": 1301.0, "end": 1304.0, "text": " After 10 minutes, like, measure with a clock.", "tokens": [51364, 2381, 1266, 2077, 11, 411, 11, 3481, 365, 257, 7830, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 489, "seek": 128100, "start": 1304.0, "end": 1306.0, "text": " If nothing comes, I don't care about the watermark.", "tokens": [51514, 759, 1825, 1487, 11, 286, 500, 380, 1127, 466, 264, 1281, 5638, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 490, "seek": 128100, "start": 1306.0, "end": 1307.0, "text": " I don't care.", "tokens": [51614, 286, 500, 380, 1127, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 491, "seek": 128100, "start": 1307.0, "end": 1309.0, "text": " Keep going, okay?", "tokens": [51664, 5527, 516, 11, 1392, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1044743718772099, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.006042795721441507}, {"id": 492, "seek": 130900, "start": 1309.0, "end": 1311.0, "text": " So data has been lost for whatever reason,", "tokens": [50364, 407, 1412, 575, 668, 2731, 337, 2035, 1778, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 493, "seek": 130900, "start": 1311.0, "end": 1313.0, "text": " and we cannot wait forever.", "tokens": [50464, 293, 321, 2644, 1699, 5680, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 494, "seek": 130900, "start": 1313.0, "end": 1315.0, "text": " So this is a stateful function,", "tokens": [50564, 407, 341, 307, 257, 1785, 906, 2445, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 495, "seek": 130900, "start": 1315.0, "end": 1317.0, "text": " and it's also very useful in streaming", "tokens": [50664, 293, 309, 311, 611, 588, 4420, 294, 11791, 50764], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 496, "seek": 130900, "start": 1317.0, "end": 1320.0, "text": " because it allows you to apply logic that goes beyond", "tokens": [50764, 570, 309, 4045, 291, 281, 3079, 9952, 300, 1709, 4399, 50914], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 497, "seek": 130900, "start": 1320.0, "end": 1322.0, "text": " the temporal properties that we have seen", "tokens": [50914, 264, 30881, 7221, 300, 321, 362, 1612, 51014], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 498, "seek": 130900, "start": 1322.0, "end": 1324.0, "text": " in the previous slides.", "tokens": [51014, 294, 264, 3894, 9788, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 499, "seek": 130900, "start": 1324.0, "end": 1327.0, "text": " And here you have some examples and links.", "tokens": [51114, 400, 510, 291, 362, 512, 5110, 293, 6123, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 500, "seek": 130900, "start": 1327.0, "end": 1330.0, "text": " The slides are already available through the first-time website,", "tokens": [51264, 440, 9788, 366, 1217, 2435, 807, 264, 700, 12, 3766, 3144, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 501, "seek": 130900, "start": 1330.0, "end": 1334.0, "text": " so I encourage you to have a look at these examples.", "tokens": [51414, 370, 286, 5373, 291, 281, 362, 257, 574, 412, 613, 5110, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 502, "seek": 130900, "start": 1334.0, "end": 1336.0, "text": " What else can I do in streaming?", "tokens": [51614, 708, 1646, 393, 286, 360, 294, 11791, 30, 51714], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 503, "seek": 130900, "start": 1336.0, "end": 1338.0, "text": " Machine learning inference, okay?", "tokens": [51714, 22155, 2539, 38253, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10061250414167132, "compression_ratio": 1.6576271186440679, "no_speech_prob": 0.004048279952257872}, {"id": 504, "seek": 133800, "start": 1338.0, "end": 1342.0, "text": " So there are many ways to do machine learning inference", "tokens": [50364, 407, 456, 366, 867, 2098, 281, 360, 3479, 2539, 38253, 50564], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 505, "seek": 133800, "start": 1342.0, "end": 1345.0, "text": " in streaming at a scale, okay?", "tokens": [50564, 294, 11791, 412, 257, 4373, 11, 1392, 30, 50714], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 506, "seek": 133800, "start": 1345.0, "end": 1347.0, "text": " Many of those quite expensive.", "tokens": [50714, 5126, 295, 729, 1596, 5124, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 507, "seek": 133800, "start": 1347.0, "end": 1350.0, "text": " So you can deploy endpoints in cloud platforms,", "tokens": [50814, 407, 291, 393, 7274, 917, 20552, 294, 4588, 9473, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 508, "seek": 133800, "start": 1350.0, "end": 1352.0, "text": " with GPUs, with a lot of stuff, okay?", "tokens": [50964, 365, 18407, 82, 11, 365, 257, 688, 295, 1507, 11, 1392, 30, 51064], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 509, "seek": 133800, "start": 1352.0, "end": 1356.0, "text": " And normally, so, well, so those are...", "tokens": [51064, 400, 5646, 11, 370, 11, 731, 11, 370, 729, 366, 485, 51264], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 510, "seek": 133800, "start": 1356.0, "end": 1358.0, "text": " those solve a lot of functionality for you,", "tokens": [51264, 729, 5039, 257, 688, 295, 14980, 337, 291, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 511, "seek": 133800, "start": 1358.0, "end": 1360.0, "text": " but they are expensive.", "tokens": [51364, 457, 436, 366, 5124, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 512, "seek": 133800, "start": 1360.0, "end": 1363.0, "text": " So what if you want to apply machine learning inference", "tokens": [51464, 407, 437, 498, 291, 528, 281, 3079, 3479, 2539, 38253, 51614], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 513, "seek": 133800, "start": 1363.0, "end": 1365.0, "text": " in a pipeline, in Apache Bin?", "tokens": [51614, 294, 257, 15517, 11, 294, 46597, 18983, 30, 51714], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 514, "seek": 133800, "start": 1365.0, "end": 1367.0, "text": " Well, you could do that, okay?", "tokens": [51714, 1042, 11, 291, 727, 360, 300, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.11745103460843445, "compression_ratio": 1.712, "no_speech_prob": 0.004174212925136089}, {"id": 515, "seek": 136700, "start": 1367.0, "end": 1369.0, "text": " You could be thinking, well, I can do that.", "tokens": [50364, 509, 727, 312, 1953, 11, 731, 11, 286, 393, 360, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 516, "seek": 136700, "start": 1369.0, "end": 1371.0, "text": " So I can, I don't know, like, import TensorFlow,", "tokens": [50464, 407, 286, 393, 11, 286, 500, 380, 458, 11, 411, 11, 974, 37624, 11, 50564], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 517, "seek": 136700, "start": 1371.0, "end": 1373.0, "text": " load the model, apply it,", "tokens": [50564, 3677, 264, 2316, 11, 3079, 309, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 518, "seek": 136700, "start": 1373.0, "end": 1375.0, "text": " so you could do a lot of stuff, okay, yourself.", "tokens": [50664, 370, 291, 727, 360, 257, 688, 295, 1507, 11, 1392, 11, 1803, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 519, "seek": 136700, "start": 1375.0, "end": 1378.0, "text": " But this is already solved for you in Apache Bin, okay?", "tokens": [50764, 583, 341, 307, 1217, 13041, 337, 291, 294, 46597, 18983, 11, 1392, 30, 50914], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 520, "seek": 136700, "start": 1378.0, "end": 1380.0, "text": " So you can run machine learning inference", "tokens": [50914, 407, 291, 393, 1190, 3479, 2539, 38253, 51014], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 521, "seek": 136700, "start": 1380.0, "end": 1384.0, "text": " with the so-called run inference...", "tokens": [51014, 365, 264, 370, 12, 11880, 1190, 38253, 485, 51214], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 522, "seek": 136700, "start": 1384.0, "end": 1386.0, "text": " run inference transform, okay?", "tokens": [51214, 1190, 38253, 4088, 11, 1392, 30, 51314], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 523, "seek": 136700, "start": 1386.0, "end": 1388.0, "text": " So we see it here.", "tokens": [51314, 407, 321, 536, 309, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 524, "seek": 136700, "start": 1388.0, "end": 1390.0, "text": " So right now, it has, let's say,", "tokens": [51414, 407, 558, 586, 11, 309, 575, 11, 718, 311, 584, 11, 51514], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 525, "seek": 136700, "start": 1390.0, "end": 1392.0, "text": " out-of-the-box support for PyTorch, TensorFlow,", "tokens": [51514, 484, 12, 2670, 12, 3322, 12, 4995, 1406, 337, 9953, 51, 284, 339, 11, 37624, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 526, "seek": 136700, "start": 1392.0, "end": 1395.0, "text": " as I can learn, with more coming.", "tokens": [51614, 382, 286, 393, 1466, 11, 365, 544, 1348, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15066320831711227, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.011240050196647644}, {"id": 527, "seek": 139500, "start": 1395.0, "end": 1397.0, "text": " When you're running a distributed system", "tokens": [50364, 1133, 291, 434, 2614, 257, 12631, 1185, 50464], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 528, "seek": 139500, "start": 1397.0, "end": 1399.0, "text": " and you want to apply a model,", "tokens": [50464, 293, 291, 528, 281, 3079, 257, 2316, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 529, "seek": 139500, "start": 1399.0, "end": 1401.0, "text": " each one of the workers in the distributed system", "tokens": [50564, 1184, 472, 295, 264, 5600, 294, 264, 12631, 1185, 50664], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 530, "seek": 139500, "start": 1401.0, "end": 1403.0, "text": " will have its own memory.", "tokens": [50664, 486, 362, 1080, 1065, 4675, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 531, "seek": 139500, "start": 1403.0, "end": 1407.0, "text": " So Apache Bin runs on top of share-nothing architecture,", "tokens": [50764, 407, 46597, 18983, 6676, 322, 1192, 295, 2073, 12, 49518, 9482, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 532, "seek": 139500, "start": 1407.0, "end": 1409.0, "text": " okay, like fling, dataflow, spark.", "tokens": [50964, 1392, 11, 411, 932, 278, 11, 1412, 10565, 11, 9908, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 533, "seek": 139500, "start": 1409.0, "end": 1411.0, "text": " Workers are independent of each other.", "tokens": [51064, 42375, 366, 6695, 295, 1184, 661, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 534, "seek": 139500, "start": 1411.0, "end": 1413.0, "text": " They don't share any common state.", "tokens": [51164, 814, 500, 380, 2073, 604, 2689, 1785, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 535, "seek": 139500, "start": 1413.0, "end": 1415.0, "text": " The state that we have seen before", "tokens": [51264, 440, 1785, 300, 321, 362, 1612, 949, 51364], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 536, "seek": 139500, "start": 1415.0, "end": 1419.0, "text": " is actually maintained per key and per window", "tokens": [51364, 307, 767, 17578, 680, 2141, 293, 680, 4910, 51564], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 537, "seek": 139500, "start": 1419.0, "end": 1421.0, "text": " if we apply the per window.", "tokens": [51564, 498, 321, 3079, 264, 680, 4910, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 538, "seek": 139500, "start": 1421.0, "end": 1422.0, "text": " It's totally local for the worker,", "tokens": [51664, 467, 311, 3879, 2654, 337, 264, 11346, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 539, "seek": 139500, "start": 1422.0, "end": 1424.0, "text": " and two workers cannot share a state.", "tokens": [51714, 293, 732, 5600, 2644, 2073, 257, 1785, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11496649215470499, "compression_ratio": 1.71875, "no_speech_prob": 0.007309922482818365}, {"id": 540, "seek": 142400, "start": 1424.0, "end": 1427.0, "text": " But the model, we don't want to instantiate a model", "tokens": [50364, 583, 264, 2316, 11, 321, 500, 380, 528, 281, 9836, 13024, 257, 2316, 50514], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 541, "seek": 142400, "start": 1427.0, "end": 1430.0, "text": " if we have 100 workers 100 times", "tokens": [50514, 498, 321, 362, 2319, 5600, 2319, 1413, 50664], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 542, "seek": 142400, "start": 1430.0, "end": 1432.0, "text": " because the model is going to be the same", "tokens": [50664, 570, 264, 2316, 307, 516, 281, 312, 264, 912, 50764], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 543, "seek": 142400, "start": 1432.0, "end": 1434.0, "text": " for every worker, right?", "tokens": [50764, 337, 633, 11346, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 544, "seek": 142400, "start": 1434.0, "end": 1435.0, "text": " So the model hasn't changed.", "tokens": [50864, 407, 264, 2316, 6132, 380, 3105, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 545, "seek": 142400, "start": 1435.0, "end": 1437.0, "text": " The model is actually read-only.", "tokens": [50914, 440, 2316, 307, 767, 1401, 12, 25202, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 546, "seek": 142400, "start": 1437.0, "end": 1439.0, "text": " Run inference solves these problems", "tokens": [51014, 8950, 38253, 39890, 613, 2740, 51114], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 547, "seek": 142400, "start": 1439.0, "end": 1441.0, "text": " by having some state that is shared", "tokens": [51114, 538, 1419, 512, 1785, 300, 307, 5507, 51214], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 548, "seek": 142400, "start": 1441.0, "end": 1443.0, "text": " across all the workers and it's transparent for you.", "tokens": [51214, 2108, 439, 264, 5600, 293, 309, 311, 12737, 337, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 549, "seek": 142400, "start": 1443.0, "end": 1445.0, "text": " So this is something that you can always implement", "tokens": [51314, 407, 341, 307, 746, 300, 291, 393, 1009, 4445, 51414], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 550, "seek": 142400, "start": 1445.0, "end": 1447.0, "text": " in a distributed system, but it's complex.", "tokens": [51414, 294, 257, 12631, 1185, 11, 457, 309, 311, 3997, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 551, "seek": 142400, "start": 1447.0, "end": 1449.0, "text": " This is the problem that is solved with run inference.", "tokens": [51514, 639, 307, 264, 1154, 300, 307, 13041, 365, 1190, 38253, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 552, "seek": 142400, "start": 1449.0, "end": 1452.0, "text": " It's only one copy of the model", "tokens": [51614, 467, 311, 787, 472, 5055, 295, 264, 2316, 51764], "temperature": 0.0, "avg_logprob": -0.08755001232778425, "compression_ratio": 1.7653061224489797, "no_speech_prob": 0.0017108351457864046}, {"id": 553, "seek": 145200, "start": 1453.0, "end": 1455.0, "text": " per, let's say, machine where you're running, okay?", "tokens": [50414, 680, 11, 718, 311, 584, 11, 3479, 689, 291, 434, 2614, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 554, "seek": 145200, "start": 1455.0, "end": 1457.0, "text": " So in memory, okay?", "tokens": [50514, 407, 294, 4675, 11, 1392, 30, 50614], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 555, "seek": 145200, "start": 1457.0, "end": 1459.0, "text": " Because, well, you need always to make an instance", "tokens": [50614, 1436, 11, 731, 11, 291, 643, 1009, 281, 652, 364, 5197, 50714], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 556, "seek": 145200, "start": 1459.0, "end": 1461.0, "text": " in memory to be able to apply it.", "tokens": [50714, 294, 4675, 281, 312, 1075, 281, 3079, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 557, "seek": 145200, "start": 1461.0, "end": 1463.0, "text": " But if you have, I don't know, like 100 threads,", "tokens": [50814, 583, 498, 291, 362, 11, 286, 500, 380, 458, 11, 411, 2319, 19314, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 558, "seek": 145200, "start": 1463.0, "end": 1468.0, "text": " 100 sub-workers, 100 CPUs inside the same machine,", "tokens": [50914, 2319, 1422, 12, 37101, 11, 2319, 13199, 82, 1854, 264, 912, 3479, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 559, "seek": 145200, "start": 1468.0, "end": 1472.0, "text": " you will not have 100 copies of the model.", "tokens": [51164, 291, 486, 406, 362, 2319, 14341, 295, 264, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 560, "seek": 145200, "start": 1472.0, "end": 1474.0, "text": " Regardless of, let's say, what's the computation model", "tokens": [51364, 25148, 295, 11, 718, 311, 584, 11, 437, 311, 264, 24903, 2316, 51464], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 561, "seek": 145200, "start": 1474.0, "end": 1476.0, "text": " of how the runner is implemented on top of the machines.", "tokens": [51464, 295, 577, 264, 24376, 307, 12270, 322, 1192, 295, 264, 8379, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 562, "seek": 145200, "start": 1476.0, "end": 1478.0, "text": " That will be only one copy in the memory of the machine.", "tokens": [51564, 663, 486, 312, 787, 472, 5055, 294, 264, 4675, 295, 264, 3479, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 563, "seek": 145200, "start": 1478.0, "end": 1481.0, "text": " So this is the problem that is solved with run inference, okay?", "tokens": [51664, 407, 341, 307, 264, 1154, 300, 307, 13041, 365, 1190, 38253, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.11662380318892629, "compression_ratio": 1.72168284789644, "no_speech_prob": 0.01496987883001566}, {"id": 564, "seek": 148100, "start": 1482.0, "end": 1486.0, "text": " So if you want to apply a streaming inference,", "tokens": [50414, 407, 498, 291, 528, 281, 3079, 257, 11791, 38253, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 565, "seek": 148100, "start": 1486.0, "end": 1489.0, "text": " it's a very convenient way of doing this", "tokens": [50614, 309, 311, 257, 588, 10851, 636, 295, 884, 341, 50764], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 566, "seek": 148100, "start": 1489.0, "end": 1491.0, "text": " with very little code.", "tokens": [50764, 365, 588, 707, 3089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 567, "seek": 148100, "start": 1491.0, "end": 1494.0, "text": " And depending on the runner, this is a possibility", "tokens": [50864, 400, 5413, 322, 264, 24376, 11, 341, 307, 257, 7959, 51014], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 568, "seek": 148100, "start": 1494.0, "end": 1496.0, "text": " in data flow, this is also a possibility", "tokens": [51014, 294, 1412, 3095, 11, 341, 307, 611, 257, 7959, 51114], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 569, "seek": 148100, "start": 1496.0, "end": 1498.0, "text": " if you are running on top of Kubernetes", "tokens": [51114, 498, 291, 366, 2614, 322, 1192, 295, 23145, 51214], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 570, "seek": 148100, "start": 1498.0, "end": 1499.0, "text": " in a runner that supports Kubernetes,", "tokens": [51214, 294, 257, 24376, 300, 9346, 23145, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 571, "seek": 148100, "start": 1499.0, "end": 1500.0, "text": " like, for instance, Flink.", "tokens": [51264, 411, 11, 337, 5197, 11, 3235, 475, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 572, "seek": 148100, "start": 1500.0, "end": 1504.0, "text": " You can do also hinting of the resources", "tokens": [51314, 509, 393, 360, 611, 12075, 278, 295, 264, 3593, 51514], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 573, "seek": 148100, "start": 1504.0, "end": 1506.0, "text": " that your transformation is gonna need, okay?", "tokens": [51514, 300, 428, 9887, 307, 799, 643, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 574, "seek": 148100, "start": 1506.0, "end": 1508.0, "text": " Hinting of transformations could be,", "tokens": [51614, 389, 686, 278, 295, 34852, 727, 312, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 575, "seek": 148100, "start": 1508.0, "end": 1509.0, "text": " look, this transformation is gonna need", "tokens": [51714, 574, 11, 341, 9887, 307, 799, 643, 51764], "temperature": 0.0, "avg_logprob": -0.12037397766113281, "compression_ratio": 1.83984375, "no_speech_prob": 0.005921603180468082}, {"id": 576, "seek": 150900, "start": 1510.0, "end": 1512.0, "text": " this amount of memory, minimal, okay?", "tokens": [50414, 341, 2372, 295, 4675, 11, 13206, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 577, "seek": 150900, "start": 1512.0, "end": 1515.0, "text": " But hinting could also be, look,", "tokens": [50514, 583, 12075, 278, 727, 611, 312, 11, 574, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 578, "seek": 150900, "start": 1515.0, "end": 1517.0, "text": " this is a step that is running ML inference.", "tokens": [50664, 341, 307, 257, 1823, 300, 307, 2614, 21601, 38253, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 579, "seek": 150900, "start": 1517.0, "end": 1520.0, "text": " So use a GPU for this step, okay?", "tokens": [50764, 407, 764, 257, 18407, 337, 341, 1823, 11, 1392, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 580, "seek": 150900, "start": 1520.0, "end": 1523.0, "text": " And then the runner will take care of making sure", "tokens": [50914, 400, 550, 264, 24376, 486, 747, 1127, 295, 1455, 988, 51064], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 581, "seek": 150900, "start": 1523.0, "end": 1526.0, "text": " that the step that is running a virtual machine,", "tokens": [51064, 300, 264, 1823, 300, 307, 2614, 257, 6374, 3479, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 582, "seek": 150900, "start": 1526.0, "end": 1528.0, "text": " let's say, matches the infrastructure hints", "tokens": [51214, 718, 311, 584, 11, 10676, 264, 6896, 27271, 51314], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 583, "seek": 150900, "start": 1528.0, "end": 1530.0, "text": " that you provide through the code,", "tokens": [51314, 300, 291, 2893, 807, 264, 3089, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 584, "seek": 150900, "start": 1530.0, "end": 1532.0, "text": " and you will have, let's say, different types of nodes", "tokens": [51414, 293, 291, 486, 362, 11, 718, 311, 584, 11, 819, 3467, 295, 13891, 51514], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 585, "seek": 150900, "start": 1532.0, "end": 1534.0, "text": " for different types of transformations.", "tokens": [51514, 337, 819, 3467, 295, 34852, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 586, "seek": 150900, "start": 1534.0, "end": 1536.0, "text": " One of the problems of shared nothing architecture", "tokens": [51614, 1485, 295, 264, 2740, 295, 5507, 1825, 9482, 51714], "temperature": 0.0, "avg_logprob": -0.1044363514069588, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.007788763381540775}, {"id": 587, "seek": 153600, "start": 1536.0, "end": 1538.0, "text": " is that all the workers are alike.", "tokens": [50364, 307, 300, 439, 264, 5600, 366, 20025, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 588, "seek": 153600, "start": 1538.0, "end": 1540.0, "text": " All the workers are the same.", "tokens": [50464, 1057, 264, 5600, 366, 264, 912, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 589, "seek": 153600, "start": 1540.0, "end": 1542.0, "text": " With this, you can have different types of workers", "tokens": [50564, 2022, 341, 11, 291, 393, 362, 819, 3467, 295, 5600, 50664], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 590, "seek": 153600, "start": 1542.0, "end": 1544.0, "text": " for different kinds of transformations,", "tokens": [50664, 337, 819, 3685, 295, 34852, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 591, "seek": 153600, "start": 1544.0, "end": 1549.0, "text": " which, let's say, in terms of cost, it's better.", "tokens": [50764, 597, 11, 718, 311, 584, 11, 294, 2115, 295, 2063, 11, 309, 311, 1101, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 592, "seek": 153600, "start": 1549.0, "end": 1551.0, "text": " I don't have to say optimal,", "tokens": [51014, 286, 500, 380, 362, 281, 584, 16252, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 593, "seek": 153600, "start": 1551.0, "end": 1553.0, "text": " so maybe that's a better alternative.", "tokens": [51114, 370, 1310, 300, 311, 257, 1101, 8535, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 594, "seek": 153600, "start": 1553.0, "end": 1555.0, "text": " But basically, you use GPUs in the workers,", "tokens": [51214, 583, 1936, 11, 291, 764, 18407, 82, 294, 264, 5600, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 595, "seek": 153600, "start": 1555.0, "end": 1557.0, "text": " where you need to use GPUs, you don't use them", "tokens": [51314, 689, 291, 643, 281, 764, 18407, 82, 11, 291, 500, 380, 764, 552, 51414], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 596, "seek": 153600, "start": 1557.0, "end": 1559.0, "text": " in the workers where you don't need to use them.", "tokens": [51414, 294, 264, 5600, 689, 291, 500, 380, 643, 281, 764, 552, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 597, "seek": 153600, "start": 1559.0, "end": 1562.0, "text": " And you don't have to worry about assigning work", "tokens": [51514, 400, 291, 500, 380, 362, 281, 3292, 466, 49602, 589, 51664], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 598, "seek": 153600, "start": 1562.0, "end": 1564.0, "text": " to different workers, that's actually done", "tokens": [51664, 281, 819, 5600, 11, 300, 311, 767, 1096, 51764], "temperature": 0.0, "avg_logprob": -0.10045220818318112, "compression_ratio": 1.949612403100775, "no_speech_prob": 0.011913343332707882}, {"id": 599, "seek": 156400, "start": 1564.0, "end": 1567.0, "text": " by the runner, automatically, with these hints, okay?", "tokens": [50364, 538, 264, 24376, 11, 6772, 11, 365, 613, 27271, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 600, "seek": 156400, "start": 1567.0, "end": 1571.0, "text": " If you want to know more, here you have some links.", "tokens": [50514, 759, 291, 528, 281, 458, 544, 11, 510, 291, 362, 512, 6123, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 601, "seek": 156400, "start": 1571.0, "end": 1573.0, "text": " So I have only five minutes left,", "tokens": [50714, 407, 286, 362, 787, 1732, 2077, 1411, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 602, "seek": 156400, "start": 1573.0, "end": 1577.0, "text": " so I'm leaving the best for the end of the presentation.", "tokens": [50814, 370, 286, 478, 5012, 264, 1151, 337, 264, 917, 295, 264, 5860, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 603, "seek": 156400, "start": 1577.0, "end": 1581.0, "text": " Great, look, Israel, you showed San Java at the beginning,", "tokens": [51014, 3769, 11, 574, 11, 5674, 11, 291, 4712, 5271, 10745, 412, 264, 2863, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 604, "seek": 156400, "start": 1581.0, "end": 1583.0, "text": " now you tell me ML inference is so cool,", "tokens": [51214, 586, 291, 980, 385, 21601, 38253, 307, 370, 1627, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 605, "seek": 156400, "start": 1583.0, "end": 1585.0, "text": " but it's Python, right?", "tokens": [51314, 457, 309, 311, 15329, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 606, "seek": 156400, "start": 1585.0, "end": 1587.0, "text": " So it's PyTorch, TensorFlow,", "tokens": [51414, 407, 309, 311, 9953, 51, 284, 339, 11, 37624, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 607, "seek": 156400, "start": 1587.0, "end": 1590.0, "text": " Scikit-learn, it's all Python.", "tokens": [51514, 16942, 22681, 12, 306, 1083, 11, 309, 311, 439, 15329, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 608, "seek": 156400, "start": 1590.0, "end": 1592.0, "text": " One of the things that you can do in ApacheVinus", "tokens": [51664, 1485, 295, 264, 721, 300, 291, 393, 360, 294, 46597, 53, 259, 301, 51764], "temperature": 0.0, "avg_logprob": -0.1669828658713434, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0014936672523617744}, {"id": 609, "seek": 159200, "start": 1593.0, "end": 1595.0, "text": " is in cross-language transforms.", "tokens": [50414, 307, 294, 3278, 12, 25241, 20473, 35592, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 610, "seek": 159200, "start": 1595.0, "end": 1599.0, "text": " Anything that you have available in any of the SDKs,", "tokens": [50514, 11998, 300, 291, 362, 2435, 294, 604, 295, 264, 37135, 82, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 611, "seek": 159200, "start": 1599.0, "end": 1602.0, "text": " in any language, you may use it in any other language,", "tokens": [50714, 294, 604, 2856, 11, 291, 815, 764, 309, 294, 604, 661, 2856, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 612, "seek": 159200, "start": 1602.0, "end": 1605.0, "text": " as long as the runner supports this, okay?", "tokens": [50864, 382, 938, 382, 264, 24376, 9346, 341, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 613, "seek": 159200, "start": 1605.0, "end": 1607.0, "text": " So it's not supported by all the runners,", "tokens": [51014, 407, 309, 311, 406, 8104, 538, 439, 264, 33892, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 614, "seek": 159200, "start": 1607.0, "end": 1609.0, "text": " but it's supported by the main runners, okay?", "tokens": [51114, 457, 309, 311, 8104, 538, 264, 2135, 33892, 11, 1392, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 615, "seek": 159200, "start": 1609.0, "end": 1614.0, "text": " So basically, run inference may be used in Java.", "tokens": [51214, 407, 1936, 11, 1190, 38253, 815, 312, 1143, 294, 10745, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 616, "seek": 159200, "start": 1614.0, "end": 1618.0, "text": " If you want to use any transformation from any language,", "tokens": [51464, 759, 291, 528, 281, 764, 604, 9887, 490, 604, 2856, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 617, "seek": 159200, "start": 1618.0, "end": 1620.0, "text": " you have to add some boilerplate code,", "tokens": [51664, 291, 362, 281, 909, 512, 39228, 37008, 3089, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09032761117686396, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.010286842472851276}, {"id": 618, "seek": 162000, "start": 1620.0, "end": 1622.0, "text": " not so much, but a little bit.", "tokens": [50364, 406, 370, 709, 11, 457, 257, 707, 857, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 619, "seek": 162000, "start": 1622.0, "end": 1624.0, "text": " This is already done, let's say, for the main transforms", "tokens": [50464, 639, 307, 1217, 1096, 11, 718, 311, 584, 11, 337, 264, 2135, 35592, 50564], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 620, "seek": 162000, "start": 1624.0, "end": 1626.0, "text": " that are most popular, all that they say,", "tokens": [50564, 300, 366, 881, 3743, 11, 439, 300, 436, 584, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 621, "seek": 162000, "start": 1626.0, "end": 1630.0, "text": " that make more sense to be used in different languages,", "tokens": [50664, 300, 652, 544, 2020, 281, 312, 1143, 294, 819, 8650, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 622, "seek": 162000, "start": 1630.0, "end": 1633.0, "text": " like a map, well, using a map from Java in Python", "tokens": [50864, 411, 257, 4471, 11, 731, 11, 1228, 257, 4471, 490, 10745, 294, 15329, 51014], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 623, "seek": 162000, "start": 1633.0, "end": 1636.0, "text": " doesn't really make sense, okay?", "tokens": [51014, 1177, 380, 534, 652, 2020, 11, 1392, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 624, "seek": 162000, "start": 1636.0, "end": 1638.0, "text": " Using run inference makes.", "tokens": [51164, 11142, 1190, 38253, 1669, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 625, "seek": 162000, "start": 1638.0, "end": 1642.0, "text": " Using connectors, input-output connectors from another SDK", "tokens": [51264, 11142, 31865, 11, 4846, 12, 346, 2582, 31865, 490, 1071, 37135, 51464], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 626, "seek": 162000, "start": 1642.0, "end": 1644.0, "text": " in Python, for instance, makes sense,", "tokens": [51464, 294, 15329, 11, 337, 5197, 11, 1669, 2020, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 627, "seek": 162000, "start": 1644.0, "end": 1648.0, "text": " because the amount of input-output connectors", "tokens": [51564, 570, 264, 2372, 295, 4846, 12, 346, 2582, 31865, 51764], "temperature": 0.0, "avg_logprob": -0.1324346701304118, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.017843542620539665}, {"id": 628, "seek": 164800, "start": 1648.0, "end": 1651.0, "text": " that you have per SDK is not the same.", "tokens": [50364, 300, 291, 362, 680, 37135, 307, 406, 264, 912, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 629, "seek": 164800, "start": 1651.0, "end": 1653.0, "text": " So you may write, I don't know, like,", "tokens": [50514, 407, 291, 815, 2464, 11, 286, 500, 380, 458, 11, 411, 11, 50614], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 630, "seek": 164800, "start": 1653.0, "end": 1659.0, "text": " to databases in Java and to message queues in Python,", "tokens": [50614, 281, 22380, 294, 10745, 293, 281, 3636, 631, 1247, 294, 15329, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 631, "seek": 164800, "start": 1659.0, "end": 1661.0, "text": " but maybe you don't have the same functionalities", "tokens": [50914, 457, 1310, 291, 500, 380, 362, 264, 912, 11745, 1088, 51014], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 632, "seek": 164800, "start": 1661.0, "end": 1664.0, "text": " in all the SDKs, you can use any connector from any SDK,", "tokens": [51014, 294, 439, 264, 37135, 82, 11, 291, 393, 764, 604, 19127, 490, 604, 37135, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 633, "seek": 164800, "start": 1664.0, "end": 1667.0, "text": " and this makes it quite flexible, okay?", "tokens": [51164, 293, 341, 1669, 309, 1596, 11358, 11, 1392, 30, 51314], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 634, "seek": 164800, "start": 1667.0, "end": 1670.0, "text": " So these are the so-called multi-language pipelines,", "tokens": [51314, 407, 613, 366, 264, 370, 12, 11880, 4825, 12, 25241, 20473, 40168, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 635, "seek": 164800, "start": 1670.0, "end": 1673.0, "text": " and basically, it means that you can run any transformation", "tokens": [51464, 293, 1936, 11, 309, 1355, 300, 291, 393, 1190, 604, 9887, 51614], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 636, "seek": 164800, "start": 1673.0, "end": 1677.0, "text": " in any SDK, and this is implemented because", "tokens": [51614, 294, 604, 37135, 11, 293, 341, 307, 12270, 570, 51814], "temperature": 0.0, "avg_logprob": -0.11692430165188372, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.006451611407101154}, {"id": 637, "seek": 167700, "start": 1677.0, "end": 1680.0, "text": " the runner environment is containerized, okay?", "tokens": [50364, 264, 24376, 2823, 307, 10129, 1602, 11, 1392, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 638, "seek": 167700, "start": 1680.0, "end": 1682.0, "text": " So there's a container per language,", "tokens": [50514, 407, 456, 311, 257, 10129, 680, 2856, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 639, "seek": 167700, "start": 1682.0, "end": 1684.0, "text": " and there's some magic that makes, let's say,", "tokens": [50614, 293, 456, 311, 512, 5585, 300, 1669, 11, 718, 311, 584, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 640, "seek": 167700, "start": 1684.0, "end": 1686.0, "text": " the container communicate between themselves, okay?", "tokens": [50714, 264, 10129, 7890, 1296, 2969, 11, 1392, 30, 50814], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 641, "seek": 167700, "start": 1686.0, "end": 1689.0, "text": " And the serialization and the serialization", "tokens": [50814, 400, 264, 17436, 2144, 293, 264, 17436, 2144, 50964], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 642, "seek": 167700, "start": 1689.0, "end": 1691.0, "text": " between programming languages.", "tokens": [50964, 1296, 9410, 8650, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 643, "seek": 167700, "start": 1691.0, "end": 1694.0, "text": " So this is part of the boiler press that you need to take care of.", "tokens": [51064, 407, 341, 307, 644, 295, 264, 39228, 1886, 300, 291, 643, 281, 747, 1127, 295, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 644, "seek": 167700, "start": 1694.0, "end": 1697.0, "text": " If you use things like Apache Vida schemas", "tokens": [51214, 759, 291, 764, 721, 411, 46597, 691, 2887, 22627, 296, 51364], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 645, "seek": 167700, "start": 1697.0, "end": 1699.0, "text": " that I haven't talked about in this talk,", "tokens": [51364, 300, 286, 2378, 380, 2825, 466, 294, 341, 751, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 646, "seek": 167700, "start": 1699.0, "end": 1701.0, "text": " so it will be transparent for you,", "tokens": [51464, 370, 309, 486, 312, 12737, 337, 291, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 647, "seek": 167700, "start": 1701.0, "end": 1703.0, "text": " anything that you have in one schema in one language,", "tokens": [51564, 1340, 300, 291, 362, 294, 472, 34078, 294, 472, 2856, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 648, "seek": 167700, "start": 1703.0, "end": 1705.0, "text": " you will be able to serialize it,", "tokens": [51664, 291, 486, 312, 1075, 281, 17436, 1125, 309, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1285883951530182, "compression_ratio": 1.8501742160278745, "no_speech_prob": 0.007364329416304827}, {"id": 649, "seek": 170500, "start": 1705.0, "end": 1707.0, "text": " serialize it to any other language.", "tokens": [50364, 17436, 1125, 309, 281, 604, 661, 2856, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 650, "seek": 170500, "start": 1707.0, "end": 1708.0, "text": " So if you follow, let's say,", "tokens": [50464, 407, 498, 291, 1524, 11, 718, 311, 584, 11, 50514], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 651, "seek": 170500, "start": 1708.0, "end": 1710.0, "text": " if you follow the Apache VIN custom,", "tokens": [50514, 498, 291, 1524, 264, 46597, 691, 1464, 2375, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 652, "seek": 170500, "start": 1710.0, "end": 1714.0, "text": " it's quite straightforward to use these kind of things.", "tokens": [50614, 309, 311, 1596, 15325, 281, 764, 613, 733, 295, 721, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 653, "seek": 170500, "start": 1714.0, "end": 1718.0, "text": " Well, thanks everyone so far for your attention.", "tokens": [50814, 1042, 11, 3231, 1518, 370, 1400, 337, 428, 3202, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 654, "seek": 170500, "start": 1718.0, "end": 1721.0, "text": " So here, and almost there are some links", "tokens": [51014, 407, 510, 11, 293, 1920, 456, 366, 512, 6123, 51164], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 655, "seek": 170500, "start": 1721.0, "end": 1723.0, "text": " that I recommend you to have a look", "tokens": [51164, 300, 286, 2748, 291, 281, 362, 257, 574, 51264], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 656, "seek": 170500, "start": 1723.0, "end": 1726.0, "text": " if you want to learn more about Apache VIN.", "tokens": [51264, 498, 291, 528, 281, 1466, 544, 466, 46597, 691, 1464, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 657, "seek": 170500, "start": 1726.0, "end": 1730.0, "text": " I have covered a lot of stuff in very short time, okay?", "tokens": [51414, 286, 362, 5343, 257, 688, 295, 1507, 294, 588, 2099, 565, 11, 1392, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 658, "seek": 170500, "start": 1730.0, "end": 1732.0, "text": " So there's a lot of things behind everything", "tokens": [51614, 407, 456, 311, 257, 688, 295, 721, 2261, 1203, 51714], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 659, "seek": 170500, "start": 1732.0, "end": 1734.0, "text": " that I have explained here.", "tokens": [51714, 300, 286, 362, 8825, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12185396254062653, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.01386550534516573}, {"id": 660, "seek": 173400, "start": 1734.0, "end": 1737.0, "text": " If you want to know more about all the window in streaming,", "tokens": [50364, 759, 291, 528, 281, 458, 544, 466, 439, 264, 4910, 294, 11791, 11, 50514], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 661, "seek": 173400, "start": 1737.0, "end": 1741.0, "text": " triggers, watermarks and so on,", "tokens": [50514, 22827, 11, 1281, 37307, 293, 370, 322, 11, 50714], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 662, "seek": 173400, "start": 1741.0, "end": 1743.0, "text": " I strongly recommend you this book.", "tokens": [50714, 286, 10613, 2748, 291, 341, 1446, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 663, "seek": 173400, "start": 1743.0, "end": 1745.0, "text": " It was released sometime ago.", "tokens": [50814, 467, 390, 4736, 15053, 2057, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 664, "seek": 173400, "start": 1745.0, "end": 1747.0, "text": " You may think that it's outdated, it's not outdated,", "tokens": [50914, 509, 815, 519, 300, 309, 311, 36313, 11, 309, 311, 406, 36313, 11, 51014], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 665, "seek": 173400, "start": 1747.0, "end": 1749.0, "text": " so let's say this is the same model that is applied", "tokens": [51014, 370, 718, 311, 584, 341, 307, 264, 912, 2316, 300, 307, 6456, 51114], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 666, "seek": 173400, "start": 1749.0, "end": 1751.0, "text": " in many different streaming systems,", "tokens": [51114, 294, 867, 819, 11791, 3652, 11, 51214], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 667, "seek": 173400, "start": 1751.0, "end": 1754.0, "text": " and this is not a book about Apache VIN,", "tokens": [51214, 293, 341, 307, 406, 257, 1446, 466, 46597, 691, 1464, 11, 51364], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 668, "seek": 173400, "start": 1754.0, "end": 1755.0, "text": " it's a book about streaming systems", "tokens": [51364, 309, 311, 257, 1446, 466, 11791, 3652, 51414], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 669, "seek": 173400, "start": 1755.0, "end": 1758.0, "text": " with lots of examples coming from Apache VIN,", "tokens": [51414, 365, 3195, 295, 5110, 1348, 490, 46597, 691, 1464, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 670, "seek": 173400, "start": 1758.0, "end": 1761.0, "text": " but also examples coming from Flink, Kafka,", "tokens": [51564, 457, 611, 5110, 1348, 490, 3235, 475, 11, 47064, 11, 51714], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 671, "seek": 173400, "start": 1761.0, "end": 1763.0, "text": " PAPSA, and many other systems.", "tokens": [51714, 430, 4715, 8886, 11, 293, 867, 661, 3652, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13914279661316803, "compression_ratio": 1.775, "no_speech_prob": 0.059226054698228836}, {"id": 672, "seek": 176300, "start": 1763.0, "end": 1764.0, "text": " Actually, it's very interesting.", "tokens": [50364, 5135, 11, 309, 311, 588, 1880, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 673, "seek": 176300, "start": 1764.0, "end": 1768.0, "text": " It's my favorite book, one of my favorite books,", "tokens": [50414, 467, 311, 452, 2954, 1446, 11, 472, 295, 452, 2954, 3642, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 674, "seek": 176300, "start": 1768.0, "end": 1770.0, "text": " and the other one being the book actually", "tokens": [50614, 293, 264, 661, 472, 885, 264, 1446, 767, 50714], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 675, "seek": 176300, "start": 1770.0, "end": 1774.0, "text": " from Martin Kledman about data intensive applications.", "tokens": [50714, 490, 9184, 591, 1493, 1601, 466, 1412, 18957, 5821, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 676, "seek": 176300, "start": 1774.0, "end": 1777.0, "text": " And if you want to know more about VIN,", "tokens": [50914, 400, 498, 291, 528, 281, 458, 544, 466, 691, 1464, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 677, "seek": 176300, "start": 1777.0, "end": 1779.0, "text": " so I recommend you the VIN College.", "tokens": [51064, 370, 286, 2748, 291, 264, 691, 1464, 6745, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 678, "seek": 176300, "start": 1779.0, "end": 1782.0, "text": " There are lots of videos with lots of details", "tokens": [51164, 821, 366, 3195, 295, 2145, 365, 3195, 295, 4365, 51314], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 679, "seek": 176300, "start": 1782.0, "end": 1786.0, "text": " about the things that I have explained here in YouTube.", "tokens": [51314, 466, 264, 721, 300, 286, 362, 8825, 510, 294, 3088, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 680, "seek": 176300, "start": 1786.0, "end": 1789.0, "text": " Some of them are actually linked in the slides.", "tokens": [51514, 2188, 295, 552, 366, 767, 9408, 294, 264, 9788, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 681, "seek": 176300, "start": 1789.0, "end": 1791.0, "text": " For sure, the main site of Apache VIN", "tokens": [51664, 1171, 988, 11, 264, 2135, 3621, 295, 46597, 691, 1464, 51764], "temperature": 0.0, "avg_logprob": -0.1402848027333492, "compression_ratio": 1.6309963099630995, "no_speech_prob": 0.025083554908633232}, {"id": 682, "seek": 179100, "start": 1791.0, "end": 1793.0, "text": " guide and all the documentation that is there.", "tokens": [50364, 5934, 293, 439, 264, 14333, 300, 307, 456, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 683, "seek": 179100, "start": 1793.0, "end": 1797.0, "text": " And if you want to learn more about Apache VIN,", "tokens": [50464, 400, 498, 291, 528, 281, 1466, 544, 466, 46597, 691, 1464, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 684, "seek": 179100, "start": 1797.0, "end": 1799.0, "text": " there is also the videos of the Apache VIN Summit,", "tokens": [50664, 456, 307, 611, 264, 2145, 295, 264, 46597, 691, 1464, 28726, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 685, "seek": 179100, "start": 1799.0, "end": 1801.0, "text": " the previous editions.", "tokens": [50764, 264, 3894, 44840, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 686, "seek": 179100, "start": 1801.0, "end": 1803.0, "text": " And if you want to participate, if you are here today,", "tokens": [50864, 400, 498, 291, 528, 281, 8197, 11, 498, 291, 366, 510, 965, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 687, "seek": 179100, "start": 1803.0, "end": 1805.0, "text": " so you may be interested in streaming,", "tokens": [50964, 370, 291, 815, 312, 3102, 294, 11791, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 688, "seek": 179100, "start": 1805.0, "end": 1808.0, "text": " so the call for papers is open until March 20th, I think.", "tokens": [51064, 370, 264, 818, 337, 10577, 307, 1269, 1826, 6129, 945, 392, 11, 286, 519, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 689, "seek": 179100, "start": 1808.0, "end": 1812.0, "text": " VIN Summit will be in June in New York,", "tokens": [51214, 691, 1464, 28726, 486, 312, 294, 6928, 294, 1873, 3609, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 690, "seek": 179100, "start": 1812.0, "end": 1815.0, "text": " and I encourage you to submit talks.", "tokens": [51414, 293, 286, 5373, 291, 281, 10315, 6686, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 691, "seek": 179100, "start": 1815.0, "end": 1816.0, "text": " Well, so this is all.", "tokens": [51564, 1042, 11, 370, 341, 307, 439, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 692, "seek": 179100, "start": 1816.0, "end": 1817.0, "text": " So thanks all for your attention.", "tokens": [51614, 407, 3231, 439, 337, 428, 3202, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 693, "seek": 179100, "start": 1817.0, "end": 1819.0, "text": " It's time for questions now.", "tokens": [51664, 467, 311, 565, 337, 1651, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11329337332746107, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0414838083088398}, {"id": 694, "seek": 181900, "start": 1819.0, "end": 1821.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50464], "temperature": 0.4, "avg_logprob": -0.3430700642721994, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.09036976099014282}, {"id": 695, "seek": 181900, "start": 1843.0, "end": 1846.0, "text": " So what's the advantage of using VIN", "tokens": [51564, 407, 437, 311, 264, 5002, 295, 1228, 691, 1464, 51714], "temperature": 0.4, "avg_logprob": -0.3430700642721994, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.09036976099014282}, {"id": 696, "seek": 181900, "start": 1846.0, "end": 1848.0, "text": " if you are already using VIN?", "tokens": [51714, 498, 291, 366, 1217, 1228, 691, 1464, 30, 51814], "temperature": 0.4, "avg_logprob": -0.3430700642721994, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.09036976099014282}, {"id": 697, "seek": 184800, "start": 1848.0, "end": 1851.0, "text": " So it's portability, mainly.", "tokens": [50364, 407, 309, 311, 2436, 2310, 11, 8704, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 698, "seek": 184800, "start": 1851.0, "end": 1855.0, "text": " So if tomorrow you want to move away from Flink forward", "tokens": [50514, 407, 498, 4153, 291, 528, 281, 1286, 1314, 490, 3235, 475, 2128, 50714], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 699, "seek": 184800, "start": 1855.0, "end": 1858.0, "text": " for whatever reason, so you should be able to move", "tokens": [50714, 337, 2035, 1778, 11, 370, 291, 820, 312, 1075, 281, 1286, 50864], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 700, "seek": 184800, "start": 1858.0, "end": 1861.0, "text": " to other runners that have the same level of functionality,", "tokens": [50864, 281, 661, 33892, 300, 362, 264, 912, 1496, 295, 14980, 11, 51014], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 701, "seek": 184800, "start": 1861.0, "end": 1862.0, "text": " like, for instance, Dataflow.", "tokens": [51014, 411, 11, 337, 5197, 11, 9315, 2792, 14107, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 702, "seek": 184800, "start": 1862.0, "end": 1863.0, "text": " I don't know.", "tokens": [51064, 286, 500, 380, 458, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 703, "seek": 184800, "start": 1863.0, "end": 1866.0, "text": " So we have one of the main committers here of Apache Flink", "tokens": [51114, 407, 321, 362, 472, 295, 264, 2135, 5599, 1559, 510, 295, 46597, 3235, 475, 51264], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 704, "seek": 184800, "start": 1866.0, "end": 1869.0, "text": " say that he gets hit by a bus.", "tokens": [51264, 584, 300, 415, 2170, 2045, 538, 257, 1255, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 705, "seek": 184800, "start": 1869.0, "end": 1872.0, "text": " We don't want that to happen, but that may happen.", "tokens": [51414, 492, 500, 380, 528, 300, 281, 1051, 11, 457, 300, 815, 1051, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 706, "seek": 184800, "start": 1872.0, "end": 1874.0, "text": " Everything may happen.", "tokens": [51564, 5471, 815, 1051, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16088117179224046, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.24323050677776337}, {"id": 707, "seek": 187400, "start": 1875.0, "end": 1879.0, "text": " The world is really very uncertain.", "tokens": [50414, 440, 1002, 307, 534, 588, 11308, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 708, "seek": 187400, "start": 1879.0, "end": 1881.0, "text": " So basically you have portability.", "tokens": [50614, 407, 1936, 291, 362, 2436, 2310, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 709, "seek": 187400, "start": 1881.0, "end": 1882.0, "text": " Yes.", "tokens": [50714, 1079, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 710, "seek": 187400, "start": 1882.0, "end": 1883.0, "text": " Thank you very much.", "tokens": [50764, 1044, 291, 588, 709, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 711, "seek": 187400, "start": 1883.0, "end": 1886.0, "text": " Unfortunately, we don't have time for more questions right now,", "tokens": [50814, 8590, 11, 321, 500, 380, 362, 565, 337, 544, 1651, 558, 586, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 712, "seek": 187400, "start": 1886.0, "end": 1888.0, "text": " but I'm sure we'll be happy to answer any questions.", "tokens": [50964, 457, 286, 478, 988, 321, 603, 312, 2055, 281, 1867, 604, 1651, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 713, "seek": 187400, "start": 1888.0, "end": 1889.0, "text": " Yes, anytime.", "tokens": [51064, 1079, 11, 13038, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 714, "seek": 187400, "start": 1889.0, "end": 1890.0, "text": " Yes, thanks.", "tokens": [51114, 1079, 11, 3231, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}, {"id": 715, "seek": 187400, "start": 1890.0, "end": 1891.0, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1836587360927037, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.050904348492622375}], "language": "en"}