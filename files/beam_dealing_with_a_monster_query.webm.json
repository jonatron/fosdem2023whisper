{"text": " And this is the last talk of the day room, and Mackenzie Morgan is going to talk to us about dealing with a monster query. Give it up. So, hello. I'm Mackenzie. I actually work at Nextro now, and I did not put that on the thing, because it's kind of weird, since this is about something that happened at a previous job. Let's see. Let's go over here in the spacebar. There we go. So, back in 2020, I learned Elixir because the company that I was working at, which is Axios, it's a news company, they launched our new mobile app, and then it promptly crashed every morning at 6 a.m., and I am not a morning person, so I did not want those pings. And so, we needed to do something about this, and so there's a quick rewrite into Elixir. I was not involved in the rewritings. I didn't know Elixir yet. They grabbed a couple of contractors and said, hey, learn Elixir, because we're going to be handing this off to you. Okay. And everything worked out, worked really great, except that there was this one query. So, we had this one query that had a whole lot of OR clauses in it, because, well, and this was responsible for the majority of our database load. And we also had the biggest day in U.S. politics coming up, the U.S. presidential election. If you have dealt with news organizations, you know that politics, big political events mean a ton of traffic, right? And so, that is a huge day working in a news org, and this was the second newspaper I'd worked for, so I knew how this went. Usually, for advice for optimizing stuff is to move as much computation as possible out of the code and into the database, right? But this is the story of how refactoring the opposite direction was what actually saved us. So, it's pretty standard in ACMS to have a structure that looks kind of like this, right, where you've got, okay, you've got a post, and it can be in a category, and it can be tagged, and it can be this, and it can be that, and you're trying to find posts in any of these different ways. So, we had four different taxonomies that we were using to decide what we were going to show you in the mobile app. You could subscribe to a channel, either tag or whatever. And so, we had all these four ORs where you go get the post through taxonomy one, one's the aggregator, two, three, four, all those things. And that's where our four big OR queries came in. Which looked like this. And that's the simplified version. That doesn't have the sorting, that doesn't have the time limits. That's the simplified version. But that's what that looks like, and it's ridiculous. And so, AWS stats told us that this was going absolutely bonkers. I did a Postgres explain, analyze on the query that the Ecto generated. And Postgres said it was over 3,600, was like cost for the analyze, and that it would take eight milliseconds to execute. But that's like, just computing what it needed to run was the huge problem for it. So, I'm going to go through how we factor this to be super fast. So, okay, so we had four taxonomies, so four smaller queries. So, really, they each look like that. That's very simple. And those each take one-eighth of a millisecond. So, this is a good start. It's still kind of ugly if you write that four times, though. So, but what if we take advantage of Adams and the PIN operator in Elixir, because Elixir's got some pretty cool syntax features. And we make it into a query, sort of make a function that we can call four times, passing those in. And that's a bit better, but we're still calling it four times separately. And so, if we go a little bit further and we take advantage of the concurrency that we all know the beam has, we can pass in the list of what the taxonomies are that we're going through and use the task async stream. And now, we can make all four queries running at the same exact, like running simultaneously, just by passing in that list, which makes it really easy to, you know, instead of copying and pasting more and more code, just adds the list when we add another taxonomy. And guess what? By the time I left the company, yes, there were five. So, what did changing over from that big nasty block to this get us? The database CPU utilization went down from 50% to 40%, so that's a 20% drop because math. I know it looks like 10, but, you know, you do 40s, 80% of 50, yeah. The Postgres, remember I said the Postgres analyzes over 3,600? It was 16 after that. That was a much happier database server. And the execution time, like I said, went from eight milliseconds to one eighth of a millisecond each, so a total of a half a millisecond if you were to string them along continuously. So, yeah, so much faster and our database overhead down by 20%. Great. We also had a seven times increase in the number of requests per second that we could handle according to our benchmarking scripts. So, we got to have a stress free election night. I did not have to be trying to restarting servers at two o'clock in the morning as we waited and waited for results. So, that's it. That's all I'm going to show you about. And that's how to find me.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.96, "text": " And this is the last talk of the day room, and Mackenzie Morgan is going to talk to us", "tokens": [50364, 400, 341, 307, 264, 1036, 751, 295, 264, 786, 1808, 11, 293, 24295, 32203, 16724, 307, 516, 281, 751, 281, 505, 50962], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 1, "seek": 0, "start": 11.96, "end": 14.0, "text": " about dealing with a monster query.", "tokens": [50962, 466, 6260, 365, 257, 10090, 14581, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 2, "seek": 0, "start": 14.0, "end": 15.0, "text": " Give it up.", "tokens": [51064, 5303, 309, 493, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 3, "seek": 0, "start": 15.0, "end": 16.0, "text": " So, hello.", "tokens": [51114, 407, 11, 7751, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 4, "seek": 0, "start": 16.0, "end": 17.0, "text": " I'm Mackenzie.", "tokens": [51164, 286, 478, 24295, 32203, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 5, "seek": 0, "start": 17.0, "end": 22.0, "text": " I actually work at Nextro now, and I did not put that on the thing, because it's kind of", "tokens": [51214, 286, 767, 589, 412, 3087, 340, 586, 11, 293, 286, 630, 406, 829, 300, 322, 264, 551, 11, 570, 309, 311, 733, 295, 51464], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 6, "seek": 0, "start": 22.0, "end": 29.0, "text": " weird, since this is about something that happened at a previous job.", "tokens": [51464, 3657, 11, 1670, 341, 307, 466, 746, 300, 2011, 412, 257, 3894, 1691, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3242630248374127, "compression_ratio": 1.4976525821596245, "no_speech_prob": 0.12306482344865799}, {"id": 7, "seek": 2900, "start": 30.0, "end": 31.0, "text": " Let's see.", "tokens": [50414, 961, 311, 536, 13, 50464], "temperature": 0.0, "avg_logprob": -0.19858301008069837, "compression_ratio": 1.3885714285714286, "no_speech_prob": 0.003221105318516493}, {"id": 8, "seek": 2900, "start": 31.0, "end": 33.0, "text": " Let's go over here in the spacebar.", "tokens": [50464, 961, 311, 352, 670, 510, 294, 264, 1901, 5356, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19858301008069837, "compression_ratio": 1.3885714285714286, "no_speech_prob": 0.003221105318516493}, {"id": 9, "seek": 2900, "start": 33.0, "end": 35.0, "text": " There we go.", "tokens": [50564, 821, 321, 352, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19858301008069837, "compression_ratio": 1.3885714285714286, "no_speech_prob": 0.003221105318516493}, {"id": 10, "seek": 2900, "start": 35.0, "end": 42.0, "text": " So, back in 2020, I learned Elixir because the company that I was working at, which is Axios,", "tokens": [50664, 407, 11, 646, 294, 4808, 11, 286, 3264, 2699, 970, 347, 570, 264, 2237, 300, 286, 390, 1364, 412, 11, 597, 307, 20118, 2717, 11, 51014], "temperature": 0.0, "avg_logprob": -0.19858301008069837, "compression_ratio": 1.3885714285714286, "no_speech_prob": 0.003221105318516493}, {"id": 11, "seek": 2900, "start": 42.0, "end": 51.0, "text": " it's a news company, they launched our new mobile app, and then it promptly crashed every", "tokens": [51014, 309, 311, 257, 2583, 2237, 11, 436, 8730, 527, 777, 6013, 724, 11, 293, 550, 309, 48594, 24190, 633, 51464], "temperature": 0.0, "avg_logprob": -0.19858301008069837, "compression_ratio": 1.3885714285714286, "no_speech_prob": 0.003221105318516493}, {"id": 12, "seek": 5100, "start": 51.0, "end": 62.0, "text": " morning at 6 a.m., and I am not a morning person, so I did not want those pings.", "tokens": [50364, 2446, 412, 1386, 257, 13, 76, 7933, 293, 286, 669, 406, 257, 2446, 954, 11, 370, 286, 630, 406, 528, 729, 280, 1109, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 13, "seek": 5100, "start": 62.0, "end": 66.0, "text": " And so, we needed to do something about this, and so there's a quick rewrite into Elixir.", "tokens": [50914, 400, 370, 11, 321, 2978, 281, 360, 746, 466, 341, 11, 293, 370, 456, 311, 257, 1702, 28132, 666, 2699, 970, 347, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 14, "seek": 5100, "start": 66.0, "end": 68.0, "text": " I was not involved in the rewritings.", "tokens": [51114, 286, 390, 406, 3288, 294, 264, 319, 86, 3210, 1109, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 15, "seek": 5100, "start": 68.0, "end": 70.0, "text": " I didn't know Elixir yet.", "tokens": [51214, 286, 994, 380, 458, 2699, 970, 347, 1939, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 16, "seek": 5100, "start": 70.0, "end": 74.0, "text": " They grabbed a couple of contractors and said, hey, learn Elixir, because we're going to", "tokens": [51314, 814, 18607, 257, 1916, 295, 28377, 293, 848, 11, 4177, 11, 1466, 2699, 970, 347, 11, 570, 321, 434, 516, 281, 51514], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 17, "seek": 5100, "start": 74.0, "end": 75.0, "text": " be handing this off to you.", "tokens": [51514, 312, 34774, 341, 766, 281, 291, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12250496050633422, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.018536614254117012}, {"id": 18, "seek": 7500, "start": 75.0, "end": 76.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.21038022994995118, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.006485702935606241}, {"id": 19, "seek": 7500, "start": 82.0, "end": 87.0, "text": " And everything worked out, worked really great, except that there was this one query.", "tokens": [50714, 400, 1203, 2732, 484, 11, 2732, 534, 869, 11, 3993, 300, 456, 390, 341, 472, 14581, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21038022994995118, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.006485702935606241}, {"id": 20, "seek": 7500, "start": 95.0, "end": 104.0, "text": " So, we had this one query that had a whole lot of OR clauses in it, because, well, and", "tokens": [51364, 407, 11, 321, 632, 341, 472, 14581, 300, 632, 257, 1379, 688, 295, 19654, 49072, 294, 309, 11, 570, 11, 731, 11, 293, 51814], "temperature": 0.0, "avg_logprob": -0.21038022994995118, "compression_ratio": 1.4015748031496063, "no_speech_prob": 0.006485702935606241}, {"id": 21, "seek": 10400, "start": 104.0, "end": 107.0, "text": " this was responsible for the majority of our database load.", "tokens": [50364, 341, 390, 6250, 337, 264, 6286, 295, 527, 8149, 3677, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 22, "seek": 10400, "start": 107.0, "end": 116.0, "text": " And we also had the biggest day in U.S. politics coming up, the U.S. presidential election.", "tokens": [50514, 400, 321, 611, 632, 264, 3880, 786, 294, 624, 13, 50, 13, 7341, 1348, 493, 11, 264, 624, 13, 50, 13, 16902, 6618, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 23, "seek": 10400, "start": 116.0, "end": 122.0, "text": " If you have dealt with news organizations, you know that politics, big political events", "tokens": [50964, 759, 291, 362, 15991, 365, 2583, 6150, 11, 291, 458, 300, 7341, 11, 955, 3905, 3931, 51264], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 24, "seek": 10400, "start": 122.0, "end": 125.0, "text": " mean a ton of traffic, right?", "tokens": [51264, 914, 257, 2952, 295, 6419, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 25, "seek": 10400, "start": 125.0, "end": 129.0, "text": " And so, that is a huge day working in a news org, and this was the second newspaper I'd", "tokens": [51414, 400, 370, 11, 300, 307, 257, 2603, 786, 1364, 294, 257, 2583, 14045, 11, 293, 341, 390, 264, 1150, 13669, 286, 1116, 51614], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 26, "seek": 10400, "start": 129.0, "end": 133.0, "text": " worked for, so I knew how this went.", "tokens": [51614, 2732, 337, 11, 370, 286, 2586, 577, 341, 1437, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12846384502592542, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.006895948201417923}, {"id": 27, "seek": 13300, "start": 133.0, "end": 139.0, "text": " Usually, for advice for optimizing stuff is to move as much computation as possible out", "tokens": [50364, 11419, 11, 337, 5192, 337, 40425, 1507, 307, 281, 1286, 382, 709, 24903, 382, 1944, 484, 50664], "temperature": 0.0, "avg_logprob": -0.12120437622070312, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.00035694247344508767}, {"id": 28, "seek": 13300, "start": 139.0, "end": 142.0, "text": " of the code and into the database, right?", "tokens": [50664, 295, 264, 3089, 293, 666, 264, 8149, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.12120437622070312, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.00035694247344508767}, {"id": 29, "seek": 13300, "start": 142.0, "end": 147.0, "text": " But this is the story of how refactoring the opposite direction was what actually saved us.", "tokens": [50814, 583, 341, 307, 264, 1657, 295, 577, 1895, 578, 3662, 264, 6182, 3513, 390, 437, 767, 6624, 505, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12120437622070312, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.00035694247344508767}, {"id": 30, "seek": 13300, "start": 155.0, "end": 160.0, "text": " So, it's pretty standard in ACMS to have a structure that looks kind of like this, right,", "tokens": [51464, 407, 11, 309, 311, 1238, 3832, 294, 8157, 10288, 281, 362, 257, 3877, 300, 1542, 733, 295, 411, 341, 11, 558, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12120437622070312, "compression_ratio": 1.480952380952381, "no_speech_prob": 0.00035694247344508767}, {"id": 31, "seek": 16000, "start": 160.0, "end": 165.0, "text": " where you've got, okay, you've got a post, and it can be in a category, and it can be", "tokens": [50364, 689, 291, 600, 658, 11, 1392, 11, 291, 600, 658, 257, 2183, 11, 293, 309, 393, 312, 294, 257, 7719, 11, 293, 309, 393, 312, 50614], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 32, "seek": 16000, "start": 165.0, "end": 169.0, "text": " tagged, and it can be this, and it can be that, and you're trying to find posts in", "tokens": [50614, 40239, 11, 293, 309, 393, 312, 341, 11, 293, 309, 393, 312, 300, 11, 293, 291, 434, 1382, 281, 915, 12300, 294, 50814], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 33, "seek": 16000, "start": 169.0, "end": 172.0, "text": " any of these different ways.", "tokens": [50814, 604, 295, 613, 819, 2098, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 34, "seek": 16000, "start": 172.0, "end": 176.0, "text": " So, we had four different taxonomies that we were using to decide what we were going to", "tokens": [50964, 407, 11, 321, 632, 1451, 819, 3366, 12481, 530, 300, 321, 645, 1228, 281, 4536, 437, 321, 645, 516, 281, 51164], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 35, "seek": 16000, "start": 176.0, "end": 178.0, "text": " show you in the mobile app.", "tokens": [51164, 855, 291, 294, 264, 6013, 724, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 36, "seek": 16000, "start": 178.0, "end": 181.0, "text": " You could subscribe to a channel, either tag or whatever.", "tokens": [51264, 509, 727, 3022, 281, 257, 2269, 11, 2139, 6162, 420, 2035, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 37, "seek": 16000, "start": 181.0, "end": 186.0, "text": " And so, we had all these four ORs where you go get the post through taxonomy one,", "tokens": [51414, 400, 370, 11, 321, 632, 439, 613, 1451, 19654, 82, 689, 291, 352, 483, 264, 2183, 807, 3366, 23423, 472, 11, 51664], "temperature": 0.0, "avg_logprob": -0.14451485271601713, "compression_ratio": 1.8641975308641976, "no_speech_prob": 0.007936294190585613}, {"id": 38, "seek": 18600, "start": 186.0, "end": 189.0, "text": " one's the aggregator, two, three, four, all those things.", "tokens": [50364, 472, 311, 264, 16743, 1639, 11, 732, 11, 1045, 11, 1451, 11, 439, 729, 721, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 39, "seek": 18600, "start": 189.0, "end": 194.0, "text": " And that's where our four big OR queries came in.", "tokens": [50514, 400, 300, 311, 689, 527, 1451, 955, 19654, 24109, 1361, 294, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 40, "seek": 18600, "start": 194.0, "end": 198.0, "text": " Which looked like this.", "tokens": [50764, 3013, 2956, 411, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 41, "seek": 18600, "start": 198.0, "end": 200.0, "text": " And that's the simplified version.", "tokens": [50964, 400, 300, 311, 264, 26335, 3037, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 42, "seek": 18600, "start": 200.0, "end": 203.0, "text": " That doesn't have the sorting, that doesn't have the time limits.", "tokens": [51064, 663, 1177, 380, 362, 264, 32411, 11, 300, 1177, 380, 362, 264, 565, 10406, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 43, "seek": 18600, "start": 203.0, "end": 205.0, "text": " That's the simplified version.", "tokens": [51214, 663, 311, 264, 26335, 3037, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 44, "seek": 18600, "start": 205.0, "end": 207.0, "text": " But that's what that looks like, and it's ridiculous.", "tokens": [51314, 583, 300, 311, 437, 300, 1542, 411, 11, 293, 309, 311, 11083, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 45, "seek": 18600, "start": 207.0, "end": 213.0, "text": " And so, AWS stats told us that this was going absolutely bonkers.", "tokens": [51414, 400, 370, 11, 17650, 18152, 1907, 505, 300, 341, 390, 516, 3122, 4428, 24259, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10252357197699145, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0004582497349474579}, {"id": 46, "seek": 21300, "start": 214.0, "end": 222.0, "text": " I did a Postgres explain, analyze on the query that the Ecto generated.", "tokens": [50414, 286, 630, 257, 10223, 45189, 2903, 11, 12477, 322, 264, 14581, 300, 264, 462, 349, 78, 10833, 13, 50814], "temperature": 0.0, "avg_logprob": -0.22886192321777343, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0003681878442876041}, {"id": 47, "seek": 21300, "start": 222.0, "end": 230.0, "text": " And Postgres said it was over 3,600, was like cost for the analyze, and that it would take", "tokens": [50814, 400, 10223, 45189, 848, 309, 390, 670, 805, 11, 15707, 11, 390, 411, 2063, 337, 264, 12477, 11, 293, 300, 309, 576, 747, 51214], "temperature": 0.0, "avg_logprob": -0.22886192321777343, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0003681878442876041}, {"id": 48, "seek": 21300, "start": 230.0, "end": 233.0, "text": " eight milliseconds to execute.", "tokens": [51214, 3180, 34184, 281, 14483, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22886192321777343, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0003681878442876041}, {"id": 49, "seek": 21300, "start": 233.0, "end": 239.0, "text": " But that's like, just computing what it needed to run was the huge problem for it.", "tokens": [51364, 583, 300, 311, 411, 11, 445, 15866, 437, 309, 2978, 281, 1190, 390, 264, 2603, 1154, 337, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22886192321777343, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.0003681878442876041}, {"id": 50, "seek": 23900, "start": 239.0, "end": 243.0, "text": " So, I'm going to go through how we factor this to be super fast.", "tokens": [50364, 407, 11, 286, 478, 516, 281, 352, 807, 577, 321, 5952, 341, 281, 312, 1687, 2370, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 51, "seek": 23900, "start": 252.0, "end": 256.0, "text": " So, okay, so we had four taxonomies, so four smaller queries.", "tokens": [51014, 407, 11, 1392, 11, 370, 321, 632, 1451, 3366, 12481, 530, 11, 370, 1451, 4356, 24109, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 52, "seek": 23900, "start": 256.0, "end": 259.0, "text": " So, really, they each look like that.", "tokens": [51214, 407, 11, 534, 11, 436, 1184, 574, 411, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 53, "seek": 23900, "start": 259.0, "end": 261.0, "text": " That's very simple.", "tokens": [51364, 663, 311, 588, 2199, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 54, "seek": 23900, "start": 261.0, "end": 265.0, "text": " And those each take one-eighth of a millisecond.", "tokens": [51464, 400, 729, 1184, 747, 472, 12, 36309, 71, 295, 257, 27940, 18882, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 55, "seek": 23900, "start": 265.0, "end": 268.0, "text": " So, this is a good start.", "tokens": [51664, 407, 11, 341, 307, 257, 665, 722, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2303293546040853, "compression_ratio": 1.4388888888888889, "no_speech_prob": 0.00011958858522120863}, {"id": 56, "seek": 26800, "start": 268.0, "end": 271.0, "text": " It's still kind of ugly if you write that four times, though.", "tokens": [50364, 467, 311, 920, 733, 295, 12246, 498, 291, 2464, 300, 1451, 1413, 11, 1673, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 57, "seek": 26800, "start": 274.0, "end": 280.0, "text": " So, but what if we take advantage of Adams and the PIN operator in Elixir,", "tokens": [50664, 407, 11, 457, 437, 498, 321, 747, 5002, 295, 25214, 293, 264, 430, 1464, 12973, 294, 2699, 970, 347, 11, 50964], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 58, "seek": 26800, "start": 280.0, "end": 284.0, "text": " because Elixir's got some pretty cool syntax features.", "tokens": [50964, 570, 2699, 970, 347, 311, 658, 512, 1238, 1627, 28431, 4122, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 59, "seek": 26800, "start": 287.0, "end": 292.0, "text": " And we make it into a query, sort of make a function that we can call four times,", "tokens": [51314, 400, 321, 652, 309, 666, 257, 14581, 11, 1333, 295, 652, 257, 2445, 300, 321, 393, 818, 1451, 1413, 11, 51564], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 60, "seek": 26800, "start": 292.0, "end": 293.0, "text": " passing those in.", "tokens": [51564, 8437, 729, 294, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 61, "seek": 26800, "start": 293.0, "end": 297.0, "text": " And that's a bit better, but we're still calling it four times separately.", "tokens": [51614, 400, 300, 311, 257, 857, 1101, 11, 457, 321, 434, 920, 5141, 309, 1451, 1413, 14759, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15215103299010033, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0006360203842632473}, {"id": 62, "seek": 29700, "start": 297.0, "end": 301.0, "text": " And so, if we go a little bit further and we take advantage of the concurrency", "tokens": [50364, 400, 370, 11, 498, 321, 352, 257, 707, 857, 3052, 293, 321, 747, 5002, 295, 264, 23702, 10457, 50564], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 63, "seek": 29700, "start": 301.0, "end": 306.0, "text": " that we all know the beam has, we can pass in the list of what the taxonomies are", "tokens": [50564, 300, 321, 439, 458, 264, 14269, 575, 11, 321, 393, 1320, 294, 264, 1329, 295, 437, 264, 3366, 12481, 530, 366, 50814], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 64, "seek": 29700, "start": 306.0, "end": 309.0, "text": " that we're going through and use the task async stream.", "tokens": [50814, 300, 321, 434, 516, 807, 293, 764, 264, 5633, 382, 34015, 4309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 65, "seek": 29700, "start": 311.0, "end": 315.0, "text": " And now, we can make all four queries running at the same exact,", "tokens": [51064, 400, 586, 11, 321, 393, 652, 439, 1451, 24109, 2614, 412, 264, 912, 1900, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 66, "seek": 29700, "start": 315.0, "end": 321.0, "text": " like running simultaneously, just by passing in that list, which makes it really easy", "tokens": [51264, 411, 2614, 16561, 11, 445, 538, 8437, 294, 300, 1329, 11, 597, 1669, 309, 534, 1858, 51564], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 67, "seek": 29700, "start": 321.0, "end": 323.0, "text": " to, you know, instead of copying and pasting more and more code,", "tokens": [51564, 281, 11, 291, 458, 11, 2602, 295, 27976, 293, 1791, 278, 544, 293, 544, 3089, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 68, "seek": 29700, "start": 323.0, "end": 326.0, "text": " just adds the list when we add another taxonomy.", "tokens": [51664, 445, 10860, 264, 1329, 562, 321, 909, 1071, 3366, 23423, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12822750636509486, "compression_ratio": 1.7683823529411764, "no_speech_prob": 4.1332485125167295e-05}, {"id": 69, "seek": 32600, "start": 326.0, "end": 327.0, "text": " And guess what?", "tokens": [50364, 400, 2041, 437, 30, 50414], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 70, "seek": 32600, "start": 327.0, "end": 330.0, "text": " By the time I left the company, yes, there were five.", "tokens": [50414, 3146, 264, 565, 286, 1411, 264, 2237, 11, 2086, 11, 456, 645, 1732, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 71, "seek": 32600, "start": 336.0, "end": 342.0, "text": " So, what did changing over from that big nasty block to this get us?", "tokens": [50864, 407, 11, 437, 630, 4473, 670, 490, 300, 955, 17923, 3461, 281, 341, 483, 505, 30, 51164], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 72, "seek": 32600, "start": 342.0, "end": 346.0, "text": " The database CPU utilization went down from 50% to 40%,", "tokens": [51164, 440, 8149, 13199, 37074, 1437, 760, 490, 2625, 4, 281, 3356, 8923, 51364], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 73, "seek": 32600, "start": 346.0, "end": 349.0, "text": " so that's a 20% drop because math.", "tokens": [51364, 370, 300, 311, 257, 945, 4, 3270, 570, 5221, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 74, "seek": 32600, "start": 349.0, "end": 355.0, "text": " I know it looks like 10, but, you know, you do 40s, 80% of 50, yeah.", "tokens": [51514, 286, 458, 309, 1542, 411, 1266, 11, 457, 11, 291, 458, 11, 291, 360, 3356, 82, 11, 4688, 4, 295, 2625, 11, 1338, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14272479600803825, "compression_ratio": 1.3796296296296295, "no_speech_prob": 9.31189424591139e-05}, {"id": 75, "seek": 35600, "start": 357.0, "end": 362.0, "text": " The Postgres, remember I said the Postgres analyzes over 3,600?", "tokens": [50414, 440, 10223, 45189, 11, 1604, 286, 848, 264, 10223, 45189, 6459, 12214, 670, 805, 11, 15707, 30, 50664], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 76, "seek": 35600, "start": 362.0, "end": 363.0, "text": " It was 16 after that.", "tokens": [50664, 467, 390, 3165, 934, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 77, "seek": 35600, "start": 363.0, "end": 366.0, "text": " That was a much happier database server.", "tokens": [50714, 663, 390, 257, 709, 20423, 8149, 7154, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 78, "seek": 35600, "start": 366.0, "end": 371.0, "text": " And the execution time, like I said, went from eight milliseconds to one eighth of a millisecond each,", "tokens": [50864, 400, 264, 15058, 565, 11, 411, 286, 848, 11, 1437, 490, 3180, 34184, 281, 472, 19495, 295, 257, 27940, 18882, 1184, 11, 51114], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 79, "seek": 35600, "start": 371.0, "end": 376.0, "text": " so a total of a half a millisecond if you were to string them along continuously.", "tokens": [51114, 370, 257, 3217, 295, 257, 1922, 257, 27940, 18882, 498, 291, 645, 281, 6798, 552, 2051, 15684, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 80, "seek": 35600, "start": 376.0, "end": 382.0, "text": " So, yeah, so much faster and our database overhead down by 20%.", "tokens": [51364, 407, 11, 1338, 11, 370, 709, 4663, 293, 527, 8149, 19922, 760, 538, 945, 6856, 51664], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 81, "seek": 35600, "start": 382.0, "end": 383.0, "text": " Great.", "tokens": [51664, 3769, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16122225418831537, "compression_ratio": 1.5850622406639003, "no_speech_prob": 7.253659714479e-05}, {"id": 82, "seek": 38600, "start": 386.0, "end": 393.0, "text": " We also had a seven times increase in the number of requests per second that we could handle according to our benchmarking scripts.", "tokens": [50364, 492, 611, 632, 257, 3407, 1413, 3488, 294, 264, 1230, 295, 12475, 680, 1150, 300, 321, 727, 4813, 4650, 281, 527, 18927, 278, 23294, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11730040444268121, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.00020339844923000783}, {"id": 83, "seek": 38600, "start": 402.0, "end": 405.0, "text": " So, we got to have a stress free election night.", "tokens": [51164, 407, 11, 321, 658, 281, 362, 257, 4244, 1737, 6618, 1818, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11730040444268121, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.00020339844923000783}, {"id": 84, "seek": 38600, "start": 405.0, "end": 412.0, "text": " I did not have to be trying to restarting servers at two o'clock in the morning as we waited and waited for results.", "tokens": [51314, 286, 630, 406, 362, 281, 312, 1382, 281, 21022, 278, 15909, 412, 732, 277, 6, 9023, 294, 264, 2446, 382, 321, 15240, 293, 15240, 337, 3542, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11730040444268121, "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.00020339844923000783}, {"id": 85, "seek": 41200, "start": 413.0, "end": 415.0, "text": " So, that's it.", "tokens": [50414, 407, 11, 300, 311, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2423858493566513, "compression_ratio": 1.1095890410958904, "no_speech_prob": 0.01204749010503292}, {"id": 86, "seek": 41200, "start": 415.0, "end": 417.0, "text": " That's all I'm going to show you about.", "tokens": [50514, 663, 311, 439, 286, 478, 516, 281, 855, 291, 466, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2423858493566513, "compression_ratio": 1.1095890410958904, "no_speech_prob": 0.01204749010503292}, {"id": 87, "seek": 41200, "start": 417.0, "end": 420.0, "text": " And that's how to find me.", "tokens": [50614, 400, 300, 311, 577, 281, 915, 385, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2423858493566513, "compression_ratio": 1.1095890410958904, "no_speech_prob": 0.01204749010503292}], "language": "en"}