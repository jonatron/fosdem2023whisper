{"text": " Okay, now we have Nelson Vides with the actor model as a load testing framework. Give it up. Thank you very much. Thank you for coming. Let's get started. As you heard, I'm Nelson Vides. We only have so many minutes, so I'm not going to go deep into an introduction of who I am. Just ask me on the corridors. I love talking. I'm Senior Erlang Consultant for Erlang Solutions and Core Mongolian Developer, messaging back-end, different questions. Again, ask me on the corridors. I would love to talk about it. Let's start with an analogy, an intro, a catchy intro. Now let's see how the internet works. While this loads, and I hope it loads, otherwise I have it downloaded, I had a fantastic teacher in high school, a fantastic physics teacher, kudos to him, whatever he is, hello. When we were studying aerodynamics and the Newton laws, we studied this bridge that is not loading. I think I will just save time and reproduce it here. Don't ask me how to make it bigger. Back in the 40s, they built a bridge in the Tacoma Narrows in Washington State, crossing from Tacoma to the peninsula, the other side of the Narrows, and the bridge had that problem. It had a very spectacular build. Through the build, they already realized that this is happening, that the bridge is not really stable, and very shortly after the grand opening, they had to evacuate. They left one car with the only casualty. Unfortunately, a dog was left inside of that car, the only casualty of this accident. This spectacular happening, if you check it on Wikipedia, it will be written that something like this left a mark in the history of engineering, engineers went all mad and crazy, what happened here, what mistake have we made? Eventually, the bridge fell in 1940, so then there was World War II, they didn't have a chance to build it. In the 50s, they built a new one. The old one, these pieces that fell are now a fantastic house for fishes in the bottom of the river. Let's go back to the presentation. Yeah, this never loaded, good that I don't load it. Why am I talking about this bridge? Back in the days, bridges were like this. In the Roman times, it was a solid piece of stone, you could just hammer it in all directions, it was just solid. What is the load that this bridge was having? A few Roman centurions walking, a hundred of them at a time. How heavy that is? Some armory, what armors they had anyway? It was not like big modern missiles and things that weighed tons. But one day, we went from bridges like that to bridges like this, that are very lightweight. Even if they are much bigger and they spawn way longer distances, they are way lighter than the previous one and they are not as solid. So there are forces that didn't used to matter in the previous bridge, that now make a really big difference. For example, wind. The previous bridge put it through a hurricane, probably like what kind of hurricane you need to do something. But that bridge, not this one in the picture, this is a model, but the Tacoma bridge fell under a wind of 40 miles per hour. It's not that I like miles, sorry, I'm supporter of the international system, but the Wikipedia article was written by an American, so it's in miles. How many kilometers per hour that is, I don't know how to convert it. But it's not a lot, it's not a hurricane. So my analogy, in the previous bridge, there was just a few people with a small load and forces that were there, didn't play any difference whatsoever. But in the new bridge, there is hundreds of cars with lots of loads, probably transports of goods and much bigger weapons than in the past, and forces that were always there really make a huge difference. Let's have an analogy that matters to us here, we are not bridge engineers. Not long ago we had these huge computers, but you can probably just punch them and nothing would ever happen. If I punch this one, the presentation is over. That were used by just a few people with a few use cases. And then we went to this magic infrastructure of God knows what is going on, of lots of things put somewhere, used by millions of people, God knows what use case people are finding out. You know, you probably, you design your service with one or two use cases in mind, and then people surprise you. So, the questions again. What are all the interactions? There was one or two use cases, but one or two people now is the limit. What is the traffic capacity? In the Roman bridge, there was Centurion, an army, a small army, a division with a few weapons. Now, just imagine a modern bridge. What about the amplifying factors? The problem with the wind asked me in the Q&A or in the questions like the details of why this bridge fell. I love that story. There was a little bit of wind that amplified the movement to more than the bridge would support. This can happen also to us. Imagine a client sends a packet that is compressed. We decompress it and, you know, he sends half a kilobyte, but we decompress it and it's five gigas. And, you know, you run out of memory. What about amplifying factors? And what about all forces that didn't make any difference? For example, punching a computer. That now they really do. All right. Let's get with a little bit of terminology. I'm coming back to the title of my presentation. What is a framework? Here you have a bunch of copy-pasted definitions from different dictionaries. And Wikipedia is the first, which is not the best dictionary, but we all love it. Basically, probably you have an idea like Phoenix is a web framework, for example. It's a set of tools that gives you a way to build a system to solve a problem. In turn, what is a model? You can have a model of a bridge, but you cannot have a framework of a bridge. You have a framework to build a bridge and a model that represents the bridge. Again, some copy-pasted definitions from diverse dictionaries for you to enjoy. And ask me later. This model, in particular, is the inverted model of the catenarius of the Sagrada Familia. Again, ask me. I love this topic, but we are here to talk about Erlang. This is how Gaud\u00ed designed the Sagrada Familia. That is just about to finish any day now. Let's some data. We'll finish it. So we have a framework, a set of tools to solve a problem, and a model, a representation, a theoretical representation of your problem set. Testing and load. Testing, like kids go to school and they get a test, just to prove that they know what they're supposed to know. It's a process of making sure that things are doing what they're supposed to do, that they know their knowledge, that the software does what it's supposed to do, et cetera. And load. This is what Newton would probably love to call work. Again, thank you, physics teacher. Probably what Newton would love to call work is a mass of quantity of something that has to be worked on. Like, moved, or supported, or resisted against gravity, or wind, or transported in these virtual bridges of cables that we have under the ocean, et cetera. So load testing is testing that the software, a service, can handle the load that we are giving it. And how it behaves under different such quantities. So we have this roughly scheme of, like, three points of performance testing, of load testing, that you have to test. Performance is basically how fast your algorithm is, like, executed once. It takes 10 seconds, or 10 nanoseconds. It's the theoretical performance, but what happens when you make a lot of requests at the point where you expect your service to still be able, but not more than that. It depends on the hardware you deploy, your architecture. You expect that this should behave like this, and then you test it. And then you put more load and see how it dies. We have this luxury in IT that we can destroy our software, because we can just replicate it, build infinite copies. You know, the bridge guy would be very yellow. He cannot build two bridges to break one. He has no second chance. There is one bridge. Don't break it. It's very expensive. Make sure it works. How do you test what happens when it dies? So a load testing framework is going to be, of course, a set of tools that gives you a way to test these different kinds of loads. And for these kinds of loads, you need some units of measurement. What is a load? In the case of the bridge, Newton would love to call that the forces. And you need the interactions. How are these possible loads applied? You know, in the case of the bridge, we would usually think of gravity. There is just one interaction. It goes down, but wind and turbulence and your users can be very crazy. Forces can be applied in any way. So we need to think about the unit of measurement and the interactions. So as I said, there is the forces. Newton would love this. And the equivalent. You have a service, some backend that has users. And as I said before, you would never imagine the ways they find to use your service. You're usually designed with three or four things in mind, but you know. So I would say that the equivalent of the forces that can be applied in different directions are, like, self-independent programs. Imagine that each one of those users is a program that decides how to apply his force, decides how to interact. Like, each one of those many arrows that you can draw in this bridge, and this is infinite if you get involved with differential equations and, you know, complicated mathematics, everything moves like crazy. All those moving arrows can be represented with an independent program on its own. And those programs interact with each other. This is the model of the actor that I can imagine that most of you, more or less, would be familiar with, like, what we do in Erlang and Elixir. For those of you that are not, the idea, basically, by the way, before I go to the next slide, this is Karl Hewitt, the guy that named the actor model that put it into paper. He died a month ago, or almost two, maybe, somewhere in mid-December. So, a bit of a tribute to him. Thank you for the theory. For those of you that may not be familiar with the concept of the actor, basically, it's the universal primitive. In a language like Ruby, for example, everything is an object. You can do whatever, dot something, and maybe it will crash because it's not valid. The compiler may tell you, but you can. That's how you design your program. In a language like Lisp, everything is a function. Absolutely everything. You can do whatever parentheses. And maybe it's not valid. Maybe it will crash. Maybe the compiler will tell you before compiling. In a language like Erlang, everything is an actor. You can do whatever exclamation marks send a message. And it's almost never valid. It's only by a process identifier, or if it has a name, a proper name. So, this is the model of your program. This is how you structure the program. How are we going to load test a service? Light thickens, and the crawl makes wing to the rocky wood. This has lots of background. It's a very personal thing. First of all, of course, I love Shakespeare, but that's not the point. I work, as I said, at the beginning in MongoSIM service. That is an XMPP implementation. And in XMPP, I don't know why, but I'm very happy about it. All the examples in the RFC are given with Shakespeare quotes. So, when it comes to messages, you know, there is Alice writing to, not Alice. Juliet writing to Romeo from the balcony, and then all the examples are like this. So, we made a piece of service based on a quote from Shakespeare, the name. That is called a murder of crows. I also love Hitchcock. If you haven't watched it, please watch this movie. So, there is this library that we created in my team to test MongoSIM on the load. That is called a murder of crows, because crows are dangerous and are there to kill you and eat your corpse. So, this is what we try to do, to just kill MongoSIM, see dying, and then try to make it stronger next time. And with this project, we reflect about the interactions, the traffic capacity, amplifying factor, all new forces. So, in the case of a messaging system, there is this vulnerability that happens to everyone back in the day. You know, there is compression. Somebody sends you a small packet, you decompress it, and boom, your run out of memory. These kind of things, you have to look for these amplifying forces, the traffic capacity, how much traffic each client can send, how many clients can you have, all new forces. Something that may not be a surprise for old schoolers, Erlang developers. This new world of cloud, that is someone else's computer, really. If all your microservices connection are a lot less stable, distribution is not as cool and easy as it was when Ericsson made it and hardware was indestructible, you know, the punching theory. Nothing happens. Now it dies. So, all new forces that now make a difference in the new way of building a system. In the case of MongoSIM, we have these usual use cases, session establishment. So, you know, somebody logs in, authentication, password, password less, make up your mind. Send messages. Obviously, it's all about sending messages. Fetch in your archive. You reconnect after a while, you are on holidays, and then you fetch all the messages you lost. This is stored somewhere. It has to be stored as you send it. What is the impact that it has on sending, on receiving? Joining and leaving group chats. This is something, and in all classic XMPP, it's a problem to scale, but all classic with the time happened. We had solutions for that. So, I had these problems. We need to test them. And we think how to test them. So, you start your scenario, and at testing time, you need a init, a startup. Like, start the metrics, start the functionality that is going to coordinate all your actors when they have some interaction between them. For example, in a group chat, you are going to create so many actors that then they will join the same group chat and talk to each other. Or in a multi-user game, you are going to have millions of users, but they will cluster in groups. So, you need to coordinate them. So, you will start logic to capture users and to coordinate them and join the same group, et cetera, et cetera. So, you start all the actors. After all your init, then you spawn all the process, you know, and each one executes the program they are supposed to, that they have been coded to do. And then you run it. Locally or distributed. At some point, the load that you can generate doesn't fit in a single computer, so it has to be distributed, so you need your service to handle the distribution for you. The purpose of the load testing is checking how your software is going to survive or die and not implementing the load testing idea. We want a load testing library that will just give me all the users, give me a way to coordinate them when I have to, to throttle them when I have to, and the rate that I have to, to handle whatever place I need to start this load testing. And I don't want to think about all of that. I just want to describe the scenario that I'm going to use to kill my service. So, we build a library that does all that other stuff. Very important thing is the throttle idea. In the case of the chat service, imagine that a million users connect exactly at the same time and looking at the same time. It's probably not a real use case. You can test for that, but that is the stress part when you want to kill the service. That later, you would usually see what happens when you connect 100 per second, and then you increment 200 per second, 500 per second, 1,000 per second, and you want to have a functionality that will throttle and progressively increment the rate. And then seeing your metrics, both load testing library will output to Grafana, your service that you're testing will output to Grafana, and then see the correlations. You want actors to wait for the permission. Am I allowed to do this already? And the cases, the session establishment, but also joining a group chat, how many messages are you going to send. There is this, you know, you have an arrow that is going to be applied in one place. How big do you want the arrow to be? You want that arrow to grow incrementally. And you may want to ask another actor to wait for the approval. You can tell the throttle logic to tell that actor to wait for something. And then that actor, which is not yourself, will wait for the action. For example, in the case of joining a group chat, first you have to create it. So there is a first user that says to everyone, wait, don't join because I need to create the group chat first. Voila is created, come here, et cetera. And another very piece of important functionality is the coordination idea. So as actors are appearing in your load test, one thing that you will want to do, as I said before, is to coordinate sets of them. For example, who is going to write to whom? So you want an actor to know about another one, so it can send him a message. You want a functionality that will pick up actors as they are starting in a configurable way, either all of them that are started or sets of pairs or a list of them. And once the configurable amount of actors has started, then make them do something. There is a callback that will get the list of actors that they identify as and will coordinate how they interact with each other. And, yeah, the actor, as they join the coordinator, they will be given the function that they have to do. So to us, this is what my load testing framework is supposed to help me do. We use it for XMPP. So then we have scenarios and functionality written that knows how to do the authentication for the protocol, that knows the functionality of Mongoose IM. But we don't believe that the load testing library is the one that decides your scenario. I have seen different load testing frameworks that give you functionality to run HTTP requests. So what if you are not testing something HTTP related? We believe that the best way to write what you want to test is to write the code that you know how to write anyway. So the idea is that you write Erlang, Elixir is on the way. This library is not integrated with Elixir, but we will pull requests accepted. The library, as I say, is called AMOC, an acronym for an order of crowds because you want to see your service dying. There is the repo, you can look it up. We have this other repo that we call AMOC Arsenal where we have all the scenarios for XMPP where you can take inspiration on how they work. And I'm about to finish here. I propose to myself that I would make this presentation without showing a single line of code. So I actually cutted the screenshot before the code starts. Let's see how it works. In previous presentation I have shown a lot and it's a bit more complicated to explain. So the library has documentation. Another thing that I have pending is to use the new XDOP documentation. It doesn't have it yet, but it has a beautiful markdown that you can read in GitHub pages. And the scenarios library for inspiration. That is all I will have for you. This is my handle. That is to repos links for MongoSIM and for AMOC. And this is a picture that I have everywhere if you see some Nelson videos and you don't know if it's me. It's going to be that one if it has that picture. That's all from me. Thank you very much. Thank you, Nelson. So is there any questions? Yeah. I know that there's also a library called Zang. It's a low testing library written in early. So how is this one different? In that one you write the scenario in XML. And it has a, how do you call it? Like a domain specific language, XML base to describe what you want to do. And the library has to offer you the protocol. So that library actually has HTTP and XMPP helper functionality. But if you want a different protocol, the library doesn't give it. So we thought that we just want to write the airline code. It's way more pleasant to write and also less limited. Okay, thank you. Other questions? So by using the murder of Kraus, did you already find any like bugs in Mongoose I am that you've been able to fix based on the... Every single time. Fair. Useful bottlenecks sometimes are database interactions. And all fours that didn't used to matter in the computer you could punch. But now, so as you write messages, you need to make sure that they are recoverable. But the amount of messages that you can send might not be as scalable as the amount of inserts a database can have. So this is something that we test a lot. And another functionality that we do is the time to delivery. So the sender puts a timestamp and the receiver just measures the difference. And that's something that we also test continuously when we change something to see that we didn't introduce a computation that would make the time to delivery longer. So those are the two most common tests that we test almost all the time and then there are each case that we don't test as regularly. But we have all the scenarios for them. Any other question? I wanted to mention another library I saw that is called MZBench, I think. I don't know that one. Yeah, I think it was... I know it because it was used by VernMQ to do its load testing, I think. And I think it's in Erlang, too, and you write scenarios. But is Emoq able to... If I have an actor that has to perform some action and then pass the state to another actor, is that possible? Or is that... You have to write your own code, basically, to do that. I have. The coordinator would help. Okay. So in the coordinator you can say to pick up pairs of actors and then they say... Okay, you had the first one. Okay. Yeah. Any other question? We have something similar for changing the owner of a room and then actors have to pass the state, the knowledge to another one. Okay. Okay, thank you again. We'll see if there are any other questions.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.0, "text": " Okay, now we have Nelson Vides with the actor model as a load testing framework.", "tokens": [1033, 11, 586, 321, 362, 23857, 691, 1875, 365, 264, 8747, 2316, 382, 257, 3677, 4997, 8388, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 1, "seek": 0, "start": 11.0, "end": 16.8, "text": " Give it up.", "tokens": [5303, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 2, "seek": 0, "start": 16.8, "end": 17.8, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 3, "seek": 0, "start": 17.8, "end": 18.8, "text": " Thank you for coming.", "tokens": [1044, 291, 337, 1348, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 4, "seek": 0, "start": 18.8, "end": 19.8, "text": " Let's get started.", "tokens": [961, 311, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 5, "seek": 0, "start": 19.8, "end": 21.080000000000002, "text": " As you heard, I'm Nelson Vides.", "tokens": [1018, 291, 2198, 11, 286, 478, 23857, 691, 1875, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 6, "seek": 0, "start": 21.080000000000002, "end": 25.0, "text": " We only have so many minutes, so I'm not going to go deep into an introduction of who I am.", "tokens": [492, 787, 362, 370, 867, 2077, 11, 370, 286, 478, 406, 516, 281, 352, 2452, 666, 364, 9339, 295, 567, 286, 669, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 7, "seek": 0, "start": 25.0, "end": 26.0, "text": " Just ask me on the corridors.", "tokens": [1449, 1029, 385, 322, 264, 46920, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 8, "seek": 0, "start": 26.0, "end": 27.0, "text": " I love talking.", "tokens": [286, 959, 1417, 13], "temperature": 0.0, "avg_logprob": -0.1980865969516263, "compression_ratio": 1.4862385321100917, "no_speech_prob": 0.2181972861289978}, {"id": 9, "seek": 2700, "start": 27.0, "end": 35.0, "text": " I'm Senior Erlang Consultant for Erlang Solutions and Core Mongolian Developer, messaging back-end,", "tokens": [286, 478, 18370, 3300, 25241, 40057, 394, 337, 3300, 25241, 36295, 293, 14798, 43573, 952, 44915, 11, 21812, 646, 12, 521, 11], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 10, "seek": 2700, "start": 35.0, "end": 36.0, "text": " different questions.", "tokens": [819, 1651, 13], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 11, "seek": 2700, "start": 36.0, "end": 37.0, "text": " Again, ask me on the corridors.", "tokens": [3764, 11, 1029, 385, 322, 264, 46920, 13], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 12, "seek": 2700, "start": 37.0, "end": 39.0, "text": " I would love to talk about it.", "tokens": [286, 576, 959, 281, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 13, "seek": 2700, "start": 39.0, "end": 46.0, "text": " Let's start with an analogy, an intro, a catchy intro.", "tokens": [961, 311, 722, 365, 364, 21663, 11, 364, 12897, 11, 257, 47168, 12897, 13], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 14, "seek": 2700, "start": 46.0, "end": 51.0, "text": " Now let's see how the internet works.", "tokens": [823, 718, 311, 536, 577, 264, 4705, 1985, 13], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 15, "seek": 2700, "start": 51.0, "end": 56.0, "text": " While this loads, and I hope it loads, otherwise I have it downloaded,", "tokens": [3987, 341, 12668, 11, 293, 286, 1454, 309, 12668, 11, 5911, 286, 362, 309, 21748, 11], "temperature": 0.0, "avg_logprob": -0.23647095759709677, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.000568078423384577}, {"id": 16, "seek": 5600, "start": 56.0, "end": 60.0, "text": " I had a fantastic teacher in high school, a fantastic physics teacher,", "tokens": [286, 632, 257, 5456, 5027, 294, 1090, 1395, 11, 257, 5456, 10649, 5027, 11], "temperature": 0.0, "avg_logprob": -0.16131654219193892, "compression_ratio": 1.4275862068965517, "no_speech_prob": 0.0008170462679117918}, {"id": 17, "seek": 5600, "start": 60.0, "end": 63.0, "text": " kudos to him, whatever he is, hello.", "tokens": [350, 35063, 281, 796, 11, 2035, 415, 307, 11, 7751, 13], "temperature": 0.0, "avg_logprob": -0.16131654219193892, "compression_ratio": 1.4275862068965517, "no_speech_prob": 0.0008170462679117918}, {"id": 18, "seek": 5600, "start": 63.0, "end": 68.0, "text": " When we were studying aerodynamics and the Newton laws,", "tokens": [1133, 321, 645, 7601, 11207, 35483, 293, 264, 19541, 6064, 11], "temperature": 0.0, "avg_logprob": -0.16131654219193892, "compression_ratio": 1.4275862068965517, "no_speech_prob": 0.0008170462679117918}, {"id": 19, "seek": 5600, "start": 68.0, "end": 71.0, "text": " we studied this bridge that is not loading.", "tokens": [321, 9454, 341, 7283, 300, 307, 406, 15114, 13], "temperature": 0.0, "avg_logprob": -0.16131654219193892, "compression_ratio": 1.4275862068965517, "no_speech_prob": 0.0008170462679117918}, {"id": 20, "seek": 7100, "start": 71.0, "end": 88.0, "text": " I think I will just save time and reproduce it here.", "tokens": [286, 519, 286, 486, 445, 3155, 565, 293, 29501, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.14263229370117186, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.000421309465309605}, {"id": 21, "seek": 7100, "start": 88.0, "end": 92.0, "text": " Don't ask me how to make it bigger.", "tokens": [1468, 380, 1029, 385, 577, 281, 652, 309, 3801, 13], "temperature": 0.0, "avg_logprob": -0.14263229370117186, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.000421309465309605}, {"id": 22, "seek": 7100, "start": 92.0, "end": 98.0, "text": " Back in the 40s, they built a bridge in the Tacoma Narrows in Washington State,", "tokens": [5833, 294, 264, 3356, 82, 11, 436, 3094, 257, 7283, 294, 264, 38848, 6440, 45658, 1509, 294, 6149, 4533, 11], "temperature": 0.0, "avg_logprob": -0.14263229370117186, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.000421309465309605}, {"id": 23, "seek": 9800, "start": 98.0, "end": 102.0, "text": " crossing from Tacoma to the peninsula, the other side of the Narrows,", "tokens": [14712, 490, 38848, 6440, 281, 264, 45065, 11, 264, 661, 1252, 295, 264, 45658, 1509, 11], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 24, "seek": 9800, "start": 102.0, "end": 104.0, "text": " and the bridge had that problem.", "tokens": [293, 264, 7283, 632, 300, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 25, "seek": 9800, "start": 104.0, "end": 106.0, "text": " It had a very spectacular build.", "tokens": [467, 632, 257, 588, 18149, 1322, 13], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 26, "seek": 9800, "start": 106.0, "end": 108.0, "text": " Through the build, they already realized that this is happening,", "tokens": [8927, 264, 1322, 11, 436, 1217, 5334, 300, 341, 307, 2737, 11], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 27, "seek": 9800, "start": 108.0, "end": 114.0, "text": " that the bridge is not really stable, and very shortly after the grand opening,", "tokens": [300, 264, 7283, 307, 406, 534, 8351, 11, 293, 588, 13392, 934, 264, 2697, 5193, 11], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 28, "seek": 9800, "start": 114.0, "end": 117.0, "text": " they had to evacuate.", "tokens": [436, 632, 281, 48570, 13], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 29, "seek": 9800, "start": 117.0, "end": 120.0, "text": " They left one car with the only casualty.", "tokens": [814, 1411, 472, 1032, 365, 264, 787, 13052, 874, 13], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 30, "seek": 9800, "start": 120.0, "end": 126.0, "text": " Unfortunately, a dog was left inside of that car, the only casualty of this accident.", "tokens": [8590, 11, 257, 3000, 390, 1411, 1854, 295, 300, 1032, 11, 264, 787, 13052, 874, 295, 341, 6398, 13], "temperature": 0.0, "avg_logprob": -0.10018222982233221, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.00041232057265006006}, {"id": 31, "seek": 12600, "start": 126.0, "end": 131.0, "text": " This spectacular happening, if you check it on Wikipedia,", "tokens": [639, 18149, 2737, 11, 498, 291, 1520, 309, 322, 28999, 11], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 32, "seek": 12600, "start": 131.0, "end": 136.0, "text": " it will be written that something like this left a mark in the history of engineering,", "tokens": [309, 486, 312, 3720, 300, 746, 411, 341, 1411, 257, 1491, 294, 264, 2503, 295, 7043, 11], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 33, "seek": 12600, "start": 136.0, "end": 142.0, "text": " engineers went all mad and crazy, what happened here, what mistake have we made?", "tokens": [11955, 1437, 439, 5244, 293, 3219, 11, 437, 2011, 510, 11, 437, 6146, 362, 321, 1027, 30], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 34, "seek": 12600, "start": 142.0, "end": 147.0, "text": " Eventually, the bridge fell in 1940, so then there was World War II,", "tokens": [17586, 11, 264, 7283, 5696, 294, 24158, 11, 370, 550, 456, 390, 3937, 3630, 6351, 11], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 35, "seek": 12600, "start": 147.0, "end": 149.0, "text": " they didn't have a chance to build it.", "tokens": [436, 994, 380, 362, 257, 2931, 281, 1322, 309, 13], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 36, "seek": 12600, "start": 149.0, "end": 151.0, "text": " In the 50s, they built a new one.", "tokens": [682, 264, 2625, 82, 11, 436, 3094, 257, 777, 472, 13], "temperature": 0.0, "avg_logprob": -0.13074921568234762, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.0004310858785174787}, {"id": 37, "seek": 15100, "start": 151.0, "end": 158.0, "text": " The old one, these pieces that fell are now a fantastic house for fishes in the bottom of the river.", "tokens": [440, 1331, 472, 11, 613, 3755, 300, 5696, 366, 586, 257, 5456, 1782, 337, 41734, 294, 264, 2767, 295, 264, 6810, 13], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 38, "seek": 15100, "start": 158.0, "end": 162.0, "text": " Let's go back to the presentation.", "tokens": [961, 311, 352, 646, 281, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 39, "seek": 15100, "start": 162.0, "end": 170.0, "text": " Yeah, this never loaded, good that I don't load it.", "tokens": [865, 11, 341, 1128, 13210, 11, 665, 300, 286, 500, 380, 3677, 309, 13], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 40, "seek": 15100, "start": 170.0, "end": 172.0, "text": " Why am I talking about this bridge?", "tokens": [1545, 669, 286, 1417, 466, 341, 7283, 30], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 41, "seek": 15100, "start": 172.0, "end": 174.0, "text": " Back in the days, bridges were like this.", "tokens": [5833, 294, 264, 1708, 11, 21114, 645, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 42, "seek": 15100, "start": 174.0, "end": 178.0, "text": " In the Roman times, it was a solid piece of stone,", "tokens": [682, 264, 8566, 1413, 11, 309, 390, 257, 5100, 2522, 295, 7581, 11], "temperature": 0.0, "avg_logprob": -0.1578552374679051, "compression_ratio": 1.5192307692307692, "no_speech_prob": 0.0002251662517664954}, {"id": 43, "seek": 17800, "start": 178.0, "end": 181.0, "text": " you could just hammer it in all directions, it was just solid.", "tokens": [291, 727, 445, 13017, 309, 294, 439, 11095, 11, 309, 390, 445, 5100, 13], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 44, "seek": 17800, "start": 181.0, "end": 183.0, "text": " What is the load that this bridge was having?", "tokens": [708, 307, 264, 3677, 300, 341, 7283, 390, 1419, 30], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 45, "seek": 17800, "start": 183.0, "end": 187.0, "text": " A few Roman centurions walking, a hundred of them at a time.", "tokens": [316, 1326, 8566, 1489, 374, 626, 4494, 11, 257, 3262, 295, 552, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 46, "seek": 17800, "start": 187.0, "end": 188.0, "text": " How heavy that is?", "tokens": [1012, 4676, 300, 307, 30], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 47, "seek": 17800, "start": 188.0, "end": 191.0, "text": " Some armory, what armors they had anyway?", "tokens": [2188, 3726, 827, 11, 437, 3726, 830, 436, 632, 4033, 30], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 48, "seek": 17800, "start": 191.0, "end": 196.0, "text": " It was not like big modern missiles and things that weighed tons.", "tokens": [467, 390, 406, 411, 955, 4363, 23133, 293, 721, 300, 32844, 9131, 13], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 49, "seek": 17800, "start": 196.0, "end": 200.0, "text": " But one day, we went from bridges like that to bridges like this,", "tokens": [583, 472, 786, 11, 321, 1437, 490, 21114, 411, 300, 281, 21114, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 50, "seek": 17800, "start": 200.0, "end": 202.0, "text": " that are very lightweight.", "tokens": [300, 366, 588, 22052, 13], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 51, "seek": 17800, "start": 202.0, "end": 206.0, "text": " Even if they are much bigger and they spawn way longer distances,", "tokens": [2754, 498, 436, 366, 709, 3801, 293, 436, 17088, 636, 2854, 22182, 11], "temperature": 0.0, "avg_logprob": -0.1461115352443007, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00014459903468377888}, {"id": 52, "seek": 20600, "start": 206.0, "end": 210.0, "text": " they are way lighter than the previous one and they are not as solid.", "tokens": [436, 366, 636, 11546, 813, 264, 3894, 472, 293, 436, 366, 406, 382, 5100, 13], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 53, "seek": 20600, "start": 210.0, "end": 215.0, "text": " So there are forces that didn't used to matter in the previous bridge,", "tokens": [407, 456, 366, 5874, 300, 994, 380, 1143, 281, 1871, 294, 264, 3894, 7283, 11], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 54, "seek": 20600, "start": 215.0, "end": 218.0, "text": " that now make a really big difference.", "tokens": [300, 586, 652, 257, 534, 955, 2649, 13], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 55, "seek": 20600, "start": 218.0, "end": 219.0, "text": " For example, wind.", "tokens": [1171, 1365, 11, 2468, 13], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 56, "seek": 20600, "start": 219.0, "end": 223.0, "text": " The previous bridge put it through a hurricane,", "tokens": [440, 3894, 7283, 829, 309, 807, 257, 27136, 11], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 57, "seek": 20600, "start": 223.0, "end": 226.0, "text": " probably like what kind of hurricane you need to do something.", "tokens": [1391, 411, 437, 733, 295, 27136, 291, 643, 281, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 58, "seek": 20600, "start": 226.0, "end": 230.0, "text": " But that bridge, not this one in the picture, this is a model,", "tokens": [583, 300, 7283, 11, 406, 341, 472, 294, 264, 3036, 11, 341, 307, 257, 2316, 11], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 59, "seek": 20600, "start": 230.0, "end": 234.0, "text": " but the Tacoma bridge fell under a wind of 40 miles per hour.", "tokens": [457, 264, 38848, 6440, 7283, 5696, 833, 257, 2468, 295, 3356, 6193, 680, 1773, 13], "temperature": 0.0, "avg_logprob": -0.12741261878899768, "compression_ratio": 1.6953125, "no_speech_prob": 5.156936822459102e-05}, {"id": 60, "seek": 23400, "start": 234.0, "end": 238.0, "text": " It's not that I like miles, sorry, I'm supporter of the international system,", "tokens": [467, 311, 406, 300, 286, 411, 6193, 11, 2597, 11, 286, 478, 28600, 295, 264, 5058, 1185, 11], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 61, "seek": 23400, "start": 238.0, "end": 241.0, "text": " but the Wikipedia article was written by an American, so it's in miles.", "tokens": [457, 264, 28999, 7222, 390, 3720, 538, 364, 2665, 11, 370, 309, 311, 294, 6193, 13], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 62, "seek": 23400, "start": 241.0, "end": 244.0, "text": " How many kilometers per hour that is, I don't know how to convert it.", "tokens": [1012, 867, 13904, 680, 1773, 300, 307, 11, 286, 500, 380, 458, 577, 281, 7620, 309, 13], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 63, "seek": 23400, "start": 244.0, "end": 247.0, "text": " But it's not a lot, it's not a hurricane.", "tokens": [583, 309, 311, 406, 257, 688, 11, 309, 311, 406, 257, 27136, 13], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 64, "seek": 23400, "start": 247.0, "end": 254.0, "text": " So my analogy, in the previous bridge,", "tokens": [407, 452, 21663, 11, 294, 264, 3894, 7283, 11], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 65, "seek": 23400, "start": 254.0, "end": 259.0, "text": " there was just a few people with a small load and forces that were there,", "tokens": [456, 390, 445, 257, 1326, 561, 365, 257, 1359, 3677, 293, 5874, 300, 645, 456, 11], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 66, "seek": 23400, "start": 259.0, "end": 261.0, "text": " didn't play any difference whatsoever.", "tokens": [994, 380, 862, 604, 2649, 17076, 13], "temperature": 0.0, "avg_logprob": -0.11916571004050118, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00011249546514591202}, {"id": 67, "seek": 26100, "start": 261.0, "end": 265.0, "text": " But in the new bridge, there is hundreds of cars with lots of loads,", "tokens": [583, 294, 264, 777, 7283, 11, 456, 307, 6779, 295, 5163, 365, 3195, 295, 12668, 11], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 68, "seek": 26100, "start": 265.0, "end": 270.0, "text": " probably transports of goods and much bigger weapons than in the past,", "tokens": [1391, 5495, 82, 295, 10179, 293, 709, 3801, 7278, 813, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 69, "seek": 26100, "start": 270.0, "end": 275.0, "text": " and forces that were always there really make a huge difference.", "tokens": [293, 5874, 300, 645, 1009, 456, 534, 652, 257, 2603, 2649, 13], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 70, "seek": 26100, "start": 275.0, "end": 281.0, "text": " Let's have an analogy that matters to us here, we are not bridge engineers.", "tokens": [961, 311, 362, 364, 21663, 300, 7001, 281, 505, 510, 11, 321, 366, 406, 7283, 11955, 13], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 71, "seek": 26100, "start": 281.0, "end": 284.0, "text": " Not long ago we had these huge computers,", "tokens": [1726, 938, 2057, 321, 632, 613, 2603, 10807, 11], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 72, "seek": 26100, "start": 284.0, "end": 287.0, "text": " but you can probably just punch them and nothing would ever happen.", "tokens": [457, 291, 393, 1391, 445, 8135, 552, 293, 1825, 576, 1562, 1051, 13], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 73, "seek": 26100, "start": 287.0, "end": 290.0, "text": " If I punch this one, the presentation is over.", "tokens": [759, 286, 8135, 341, 472, 11, 264, 5860, 307, 670, 13], "temperature": 0.0, "avg_logprob": -0.11023303314491555, "compression_ratio": 1.6367041198501873, "no_speech_prob": 7.322752935579047e-05}, {"id": 74, "seek": 29000, "start": 290.0, "end": 295.0, "text": " That were used by just a few people with a few use cases.", "tokens": [663, 645, 1143, 538, 445, 257, 1326, 561, 365, 257, 1326, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 75, "seek": 29000, "start": 295.0, "end": 300.0, "text": " And then we went to this magic infrastructure of God knows what is going on,", "tokens": [400, 550, 321, 1437, 281, 341, 5585, 6896, 295, 1265, 3255, 437, 307, 516, 322, 11], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 76, "seek": 29000, "start": 300.0, "end": 306.0, "text": " of lots of things put somewhere, used by millions of people,", "tokens": [295, 3195, 295, 721, 829, 4079, 11, 1143, 538, 6803, 295, 561, 11], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 77, "seek": 29000, "start": 306.0, "end": 309.0, "text": " God knows what use case people are finding out.", "tokens": [1265, 3255, 437, 764, 1389, 561, 366, 5006, 484, 13], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 78, "seek": 29000, "start": 309.0, "end": 313.0, "text": " You know, you probably, you design your service with one or two use cases in mind,", "tokens": [509, 458, 11, 291, 1391, 11, 291, 1715, 428, 2643, 365, 472, 420, 732, 764, 3331, 294, 1575, 11], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 79, "seek": 29000, "start": 313.0, "end": 315.0, "text": " and then people surprise you.", "tokens": [293, 550, 561, 6365, 291, 13], "temperature": 0.0, "avg_logprob": -0.12330293655395508, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012130929826525971}, {"id": 80, "seek": 31500, "start": 315.0, "end": 322.0, "text": " So, the questions again.", "tokens": [407, 11, 264, 1651, 797, 13], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 81, "seek": 31500, "start": 322.0, "end": 324.0, "text": " What are all the interactions?", "tokens": [708, 366, 439, 264, 13280, 30], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 82, "seek": 31500, "start": 324.0, "end": 328.0, "text": " There was one or two use cases, but one or two people now is the limit.", "tokens": [821, 390, 472, 420, 732, 764, 3331, 11, 457, 472, 420, 732, 561, 586, 307, 264, 4948, 13], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 83, "seek": 31500, "start": 328.0, "end": 332.0, "text": " What is the traffic capacity?", "tokens": [708, 307, 264, 6419, 6042, 30], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 84, "seek": 31500, "start": 332.0, "end": 336.0, "text": " In the Roman bridge, there was Centurion, an army, a small army,", "tokens": [682, 264, 8566, 7283, 11, 456, 390, 3408, 374, 313, 11, 364, 7267, 11, 257, 1359, 7267, 11], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 85, "seek": 31500, "start": 336.0, "end": 338.0, "text": " a division with a few weapons.", "tokens": [257, 10044, 365, 257, 1326, 7278, 13], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 86, "seek": 31500, "start": 338.0, "end": 340.0, "text": " Now, just imagine a modern bridge.", "tokens": [823, 11, 445, 3811, 257, 4363, 7283, 13], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 87, "seek": 31500, "start": 340.0, "end": 342.0, "text": " What about the amplifying factors?", "tokens": [708, 466, 264, 9731, 5489, 6771, 30], "temperature": 0.0, "avg_logprob": -0.15422303625877867, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.0001932063023559749}, {"id": 88, "seek": 34200, "start": 342.0, "end": 346.0, "text": " The problem with the wind asked me in the Q&A or in the questions like the details", "tokens": [440, 1154, 365, 264, 2468, 2351, 385, 294, 264, 1249, 5, 32, 420, 294, 264, 1651, 411, 264, 4365], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 89, "seek": 34200, "start": 346.0, "end": 348.0, "text": " of why this bridge fell.", "tokens": [295, 983, 341, 7283, 5696, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 90, "seek": 34200, "start": 348.0, "end": 350.0, "text": " I love that story.", "tokens": [286, 959, 300, 1657, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 91, "seek": 34200, "start": 350.0, "end": 355.0, "text": " There was a little bit of wind that amplified the movement to more than the bridge would support.", "tokens": [821, 390, 257, 707, 857, 295, 2468, 300, 49237, 264, 3963, 281, 544, 813, 264, 7283, 576, 1406, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 92, "seek": 34200, "start": 355.0, "end": 357.0, "text": " This can happen also to us.", "tokens": [639, 393, 1051, 611, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 93, "seek": 34200, "start": 357.0, "end": 359.0, "text": " Imagine a client sends a packet that is compressed.", "tokens": [11739, 257, 6423, 14790, 257, 20300, 300, 307, 30353, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 94, "seek": 34200, "start": 359.0, "end": 362.0, "text": " We decompress it and, you know, he sends half a kilobyte,", "tokens": [492, 22867, 735, 309, 293, 11, 291, 458, 11, 415, 14790, 1922, 257, 5128, 13944, 975, 11], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 95, "seek": 34200, "start": 362.0, "end": 365.0, "text": " but we decompress it and it's five gigas.", "tokens": [457, 321, 22867, 735, 309, 293, 309, 311, 1732, 8741, 296, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 96, "seek": 34200, "start": 365.0, "end": 367.0, "text": " And, you know, you run out of memory.", "tokens": [400, 11, 291, 458, 11, 291, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 97, "seek": 34200, "start": 367.0, "end": 369.0, "text": " What about amplifying factors?", "tokens": [708, 466, 9731, 5489, 6771, 30], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 98, "seek": 34200, "start": 369.0, "end": 371.0, "text": " And what about all forces that didn't make any difference?", "tokens": [400, 437, 466, 439, 5874, 300, 994, 380, 652, 604, 2649, 30], "temperature": 0.0, "avg_logprob": -0.12422359311902845, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.00019879723549820483}, {"id": 99, "seek": 37100, "start": 371.0, "end": 373.0, "text": " For example, punching a computer.", "tokens": [1171, 1365, 11, 34866, 257, 3820, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 100, "seek": 37100, "start": 373.0, "end": 377.0, "text": " That now they really do.", "tokens": [663, 586, 436, 534, 360, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 101, "seek": 37100, "start": 377.0, "end": 378.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 102, "seek": 37100, "start": 378.0, "end": 381.0, "text": " Let's get with a little bit of terminology.", "tokens": [961, 311, 483, 365, 257, 707, 857, 295, 27575, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 103, "seek": 37100, "start": 381.0, "end": 385.0, "text": " I'm coming back to the title of my presentation.", "tokens": [286, 478, 1348, 646, 281, 264, 4876, 295, 452, 5860, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 104, "seek": 37100, "start": 385.0, "end": 387.0, "text": " What is a framework?", "tokens": [708, 307, 257, 8388, 30], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 105, "seek": 37100, "start": 387.0, "end": 391.0, "text": " Here you have a bunch of copy-pasted definitions from different dictionaries.", "tokens": [1692, 291, 362, 257, 3840, 295, 5055, 12, 79, 34440, 21988, 490, 819, 22352, 4889, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 106, "seek": 37100, "start": 391.0, "end": 396.0, "text": " And Wikipedia is the first, which is not the best dictionary, but we all love it.", "tokens": [400, 28999, 307, 264, 700, 11, 597, 307, 406, 264, 1151, 25890, 11, 457, 321, 439, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.10228609787790399, "compression_ratio": 1.4721030042918455, "no_speech_prob": 5.799428981845267e-05}, {"id": 107, "seek": 39600, "start": 396.0, "end": 402.0, "text": " Basically, probably you have an idea like Phoenix is a web framework, for example.", "tokens": [8537, 11, 1391, 291, 362, 364, 1558, 411, 18383, 307, 257, 3670, 8388, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.08056825140248174, "compression_ratio": 1.7164179104477613, "no_speech_prob": 7.658607501070946e-05}, {"id": 108, "seek": 39600, "start": 402.0, "end": 408.0, "text": " It's a set of tools that gives you a way to build a system to solve a problem.", "tokens": [467, 311, 257, 992, 295, 3873, 300, 2709, 291, 257, 636, 281, 1322, 257, 1185, 281, 5039, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08056825140248174, "compression_ratio": 1.7164179104477613, "no_speech_prob": 7.658607501070946e-05}, {"id": 109, "seek": 39600, "start": 408.0, "end": 414.0, "text": " In turn, what is a model?", "tokens": [682, 1261, 11, 437, 307, 257, 2316, 30], "temperature": 0.0, "avg_logprob": -0.08056825140248174, "compression_ratio": 1.7164179104477613, "no_speech_prob": 7.658607501070946e-05}, {"id": 110, "seek": 39600, "start": 414.0, "end": 420.0, "text": " You can have a model of a bridge, but you cannot have a framework of a bridge.", "tokens": [509, 393, 362, 257, 2316, 295, 257, 7283, 11, 457, 291, 2644, 362, 257, 8388, 295, 257, 7283, 13], "temperature": 0.0, "avg_logprob": -0.08056825140248174, "compression_ratio": 1.7164179104477613, "no_speech_prob": 7.658607501070946e-05}, {"id": 111, "seek": 39600, "start": 420.0, "end": 425.0, "text": " You have a framework to build a bridge and a model that represents the bridge.", "tokens": [509, 362, 257, 8388, 281, 1322, 257, 7283, 293, 257, 2316, 300, 8855, 264, 7283, 13], "temperature": 0.0, "avg_logprob": -0.08056825140248174, "compression_ratio": 1.7164179104477613, "no_speech_prob": 7.658607501070946e-05}, {"id": 112, "seek": 42500, "start": 425.0, "end": 430.0, "text": " Again, some copy-pasted definitions from diverse dictionaries for you to enjoy.", "tokens": [3764, 11, 512, 5055, 12, 79, 34440, 21988, 490, 9521, 22352, 4889, 337, 291, 281, 2103, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 113, "seek": 42500, "start": 430.0, "end": 431.0, "text": " And ask me later.", "tokens": [400, 1029, 385, 1780, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 114, "seek": 42500, "start": 431.0, "end": 437.0, "text": " This model, in particular, is the inverted model of the catenarius of the Sagrada Familia.", "tokens": [639, 2316, 11, 294, 1729, 11, 307, 264, 38969, 2316, 295, 264, 3857, 268, 27440, 295, 264, 34551, 19120, 15672, 654, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 115, "seek": 42500, "start": 437.0, "end": 438.0, "text": " Again, ask me.", "tokens": [3764, 11, 1029, 385, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 116, "seek": 42500, "start": 438.0, "end": 441.0, "text": " I love this topic, but we are here to talk about Erlang.", "tokens": [286, 959, 341, 4829, 11, 457, 321, 366, 510, 281, 751, 466, 3300, 25241, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 117, "seek": 42500, "start": 441.0, "end": 445.0, "text": " This is how Gaud\u00ed designed the Sagrada Familia.", "tokens": [639, 307, 577, 460, 3751, 870, 4761, 264, 34551, 19120, 15672, 654, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 118, "seek": 42500, "start": 445.0, "end": 446.0, "text": " That is just about to finish any day now.", "tokens": [663, 307, 445, 466, 281, 2413, 604, 786, 586, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 119, "seek": 42500, "start": 446.0, "end": 447.0, "text": " Let's some data.", "tokens": [961, 311, 512, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 120, "seek": 42500, "start": 447.0, "end": 450.0, "text": " We'll finish it.", "tokens": [492, 603, 2413, 309, 13], "temperature": 0.0, "avg_logprob": -0.15728914635813135, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00019324904133100063}, {"id": 121, "seek": 45000, "start": 450.0, "end": 456.0, "text": " So we have a framework, a set of tools to solve a problem, and a model, a representation,", "tokens": [407, 321, 362, 257, 8388, 11, 257, 992, 295, 3873, 281, 5039, 257, 1154, 11, 293, 257, 2316, 11, 257, 10290, 11], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 122, "seek": 45000, "start": 456.0, "end": 461.0, "text": " a theoretical representation of your problem set.", "tokens": [257, 20864, 10290, 295, 428, 1154, 992, 13], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 123, "seek": 45000, "start": 461.0, "end": 463.0, "text": " Testing and load.", "tokens": [45517, 293, 3677, 13], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 124, "seek": 45000, "start": 463.0, "end": 471.0, "text": " Testing, like kids go to school and they get a test, just to prove that they know what they're supposed to know.", "tokens": [45517, 11, 411, 2301, 352, 281, 1395, 293, 436, 483, 257, 1500, 11, 445, 281, 7081, 300, 436, 458, 437, 436, 434, 3442, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 125, "seek": 45000, "start": 471.0, "end": 477.0, "text": " It's a process of making sure that things are doing what they're supposed to do, that they know their knowledge,", "tokens": [467, 311, 257, 1399, 295, 1455, 988, 300, 721, 366, 884, 437, 436, 434, 3442, 281, 360, 11, 300, 436, 458, 641, 3601, 11], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 126, "seek": 45000, "start": 477.0, "end": 479.0, "text": " that the software does what it's supposed to do, et cetera.", "tokens": [300, 264, 4722, 775, 437, 309, 311, 3442, 281, 360, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.11069001470293317, "compression_ratio": 1.9429824561403508, "no_speech_prob": 0.0003442541928961873}, {"id": 127, "seek": 47900, "start": 479.0, "end": 480.0, "text": " And load.", "tokens": [400, 3677, 13], "temperature": 0.0, "avg_logprob": -0.11284232669406467, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0001620874390937388}, {"id": 128, "seek": 47900, "start": 480.0, "end": 482.0, "text": " This is what Newton would probably love to call work.", "tokens": [639, 307, 437, 19541, 576, 1391, 959, 281, 818, 589, 13], "temperature": 0.0, "avg_logprob": -0.11284232669406467, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0001620874390937388}, {"id": 129, "seek": 47900, "start": 482.0, "end": 485.0, "text": " Again, thank you, physics teacher.", "tokens": [3764, 11, 1309, 291, 11, 10649, 5027, 13], "temperature": 0.0, "avg_logprob": -0.11284232669406467, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0001620874390937388}, {"id": 130, "seek": 47900, "start": 485.0, "end": 490.0, "text": " Probably what Newton would love to call work is a mass of quantity of something that has to be worked on.", "tokens": [9210, 437, 19541, 576, 959, 281, 818, 589, 307, 257, 2758, 295, 11275, 295, 746, 300, 575, 281, 312, 2732, 322, 13], "temperature": 0.0, "avg_logprob": -0.11284232669406467, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0001620874390937388}, {"id": 131, "seek": 47900, "start": 490.0, "end": 502.0, "text": " Like, moved, or supported, or resisted against gravity, or wind, or transported in these virtual bridges of cables that we have under the ocean, et cetera.", "tokens": [1743, 11, 4259, 11, 420, 8104, 11, 420, 4597, 292, 1970, 12110, 11, 420, 2468, 11, 420, 29373, 294, 613, 6374, 21114, 295, 17555, 300, 321, 362, 833, 264, 7810, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.11284232669406467, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0001620874390937388}, {"id": 132, "seek": 50200, "start": 502.0, "end": 511.0, "text": " So load testing is testing that the software, a service, can handle the load that we are giving it.", "tokens": [407, 3677, 4997, 307, 4997, 300, 264, 4722, 11, 257, 2643, 11, 393, 4813, 264, 3677, 300, 321, 366, 2902, 309, 13], "temperature": 0.0, "avg_logprob": -0.1194907249288356, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.390614099567756e-05}, {"id": 133, "seek": 50200, "start": 511.0, "end": 515.0, "text": " And how it behaves under different such quantities.", "tokens": [400, 577, 309, 36896, 833, 819, 1270, 22927, 13], "temperature": 0.0, "avg_logprob": -0.1194907249288356, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.390614099567756e-05}, {"id": 134, "seek": 50200, "start": 515.0, "end": 523.0, "text": " So we have this roughly scheme of, like, three points of performance testing, of load testing, that you have to test.", "tokens": [407, 321, 362, 341, 9810, 12232, 295, 11, 411, 11, 1045, 2793, 295, 3389, 4997, 11, 295, 3677, 4997, 11, 300, 291, 362, 281, 1500, 13], "temperature": 0.0, "avg_logprob": -0.1194907249288356, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.390614099567756e-05}, {"id": 135, "seek": 50200, "start": 523.0, "end": 526.0, "text": " Performance is basically how fast your algorithm is, like, executed once.", "tokens": [25047, 307, 1936, 577, 2370, 428, 9284, 307, 11, 411, 11, 17577, 1564, 13], "temperature": 0.0, "avg_logprob": -0.1194907249288356, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.390614099567756e-05}, {"id": 136, "seek": 50200, "start": 526.0, "end": 528.0, "text": " It takes 10 seconds, or 10 nanoseconds.", "tokens": [467, 2516, 1266, 3949, 11, 420, 1266, 14067, 541, 28750, 13], "temperature": 0.0, "avg_logprob": -0.1194907249288356, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.390614099567756e-05}, {"id": 137, "seek": 52800, "start": 528.0, "end": 541.0, "text": " It's the theoretical performance, but what happens when you make a lot of requests at the point where you expect your service to still be able, but not more than that.", "tokens": [467, 311, 264, 20864, 3389, 11, 457, 437, 2314, 562, 291, 652, 257, 688, 295, 12475, 412, 264, 935, 689, 291, 2066, 428, 2643, 281, 920, 312, 1075, 11, 457, 406, 544, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.11588304588593633, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.2890144211705774e-05}, {"id": 138, "seek": 52800, "start": 541.0, "end": 543.0, "text": " It depends on the hardware you deploy, your architecture.", "tokens": [467, 5946, 322, 264, 8837, 291, 7274, 11, 428, 9482, 13], "temperature": 0.0, "avg_logprob": -0.11588304588593633, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.2890144211705774e-05}, {"id": 139, "seek": 52800, "start": 543.0, "end": 547.0, "text": " You expect that this should behave like this, and then you test it.", "tokens": [509, 2066, 300, 341, 820, 15158, 411, 341, 11, 293, 550, 291, 1500, 309, 13], "temperature": 0.0, "avg_logprob": -0.11588304588593633, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.2890144211705774e-05}, {"id": 140, "seek": 52800, "start": 547.0, "end": 551.0, "text": " And then you put more load and see how it dies.", "tokens": [400, 550, 291, 829, 544, 3677, 293, 536, 577, 309, 2714, 13], "temperature": 0.0, "avg_logprob": -0.11588304588593633, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.2890144211705774e-05}, {"id": 141, "seek": 55100, "start": 551.0, "end": 561.0, "text": " We have this luxury in IT that we can destroy our software, because we can just replicate it, build infinite copies.", "tokens": [492, 362, 341, 15558, 294, 6783, 300, 321, 393, 5293, 527, 4722, 11, 570, 321, 393, 445, 25356, 309, 11, 1322, 13785, 14341, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 142, "seek": 55100, "start": 561.0, "end": 563.0, "text": " You know, the bridge guy would be very yellow.", "tokens": [509, 458, 11, 264, 7283, 2146, 576, 312, 588, 5566, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 143, "seek": 55100, "start": 563.0, "end": 565.0, "text": " He cannot build two bridges to break one.", "tokens": [634, 2644, 1322, 732, 21114, 281, 1821, 472, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 144, "seek": 55100, "start": 565.0, "end": 567.0, "text": " He has no second chance.", "tokens": [634, 575, 572, 1150, 2931, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 145, "seek": 55100, "start": 567.0, "end": 568.0, "text": " There is one bridge.", "tokens": [821, 307, 472, 7283, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 146, "seek": 55100, "start": 568.0, "end": 569.0, "text": " Don't break it.", "tokens": [1468, 380, 1821, 309, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 147, "seek": 55100, "start": 569.0, "end": 570.0, "text": " It's very expensive.", "tokens": [467, 311, 588, 5124, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 148, "seek": 55100, "start": 570.0, "end": 571.0, "text": " Make sure it works.", "tokens": [4387, 988, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 149, "seek": 55100, "start": 571.0, "end": 577.0, "text": " How do you test what happens when it dies?", "tokens": [1012, 360, 291, 1500, 437, 2314, 562, 309, 2714, 30], "temperature": 0.0, "avg_logprob": -0.0876882553100586, "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.0001488482957938686}, {"id": 150, "seek": 57700, "start": 577.0, "end": 586.0, "text": " So a load testing framework is going to be, of course, a set of tools that gives you a way to test these different kinds of loads.", "tokens": [407, 257, 3677, 4997, 8388, 307, 516, 281, 312, 11, 295, 1164, 11, 257, 992, 295, 3873, 300, 2709, 291, 257, 636, 281, 1500, 613, 819, 3685, 295, 12668, 13], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 151, "seek": 57700, "start": 586.0, "end": 589.0, "text": " And for these kinds of loads, you need some units of measurement.", "tokens": [400, 337, 613, 3685, 295, 12668, 11, 291, 643, 512, 6815, 295, 13160, 13], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 152, "seek": 57700, "start": 589.0, "end": 591.0, "text": " What is a load?", "tokens": [708, 307, 257, 3677, 30], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 153, "seek": 57700, "start": 591.0, "end": 595.0, "text": " In the case of the bridge, Newton would love to call that the forces.", "tokens": [682, 264, 1389, 295, 264, 7283, 11, 19541, 576, 959, 281, 818, 300, 264, 5874, 13], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 154, "seek": 57700, "start": 595.0, "end": 598.0, "text": " And you need the interactions.", "tokens": [400, 291, 643, 264, 13280, 13], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 155, "seek": 57700, "start": 598.0, "end": 601.0, "text": " How are these possible loads applied?", "tokens": [1012, 366, 613, 1944, 12668, 6456, 30], "temperature": 0.0, "avg_logprob": -0.08692940421726393, "compression_ratio": 1.625, "no_speech_prob": 8.256860019173473e-05}, {"id": 156, "seek": 60100, "start": 601.0, "end": 608.0, "text": " You know, in the case of the bridge, we would usually think of gravity.", "tokens": [509, 458, 11, 294, 264, 1389, 295, 264, 7283, 11, 321, 576, 2673, 519, 295, 12110, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 157, "seek": 60100, "start": 608.0, "end": 609.0, "text": " There is just one interaction.", "tokens": [821, 307, 445, 472, 9285, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 158, "seek": 60100, "start": 609.0, "end": 612.0, "text": " It goes down, but wind and turbulence and your users can be very crazy.", "tokens": [467, 1709, 760, 11, 457, 2468, 293, 48612, 293, 428, 5022, 393, 312, 588, 3219, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 159, "seek": 60100, "start": 612.0, "end": 614.0, "text": " Forces can be applied in any way.", "tokens": [27445, 393, 312, 6456, 294, 604, 636, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 160, "seek": 60100, "start": 614.0, "end": 620.0, "text": " So we need to think about the unit of measurement and the interactions.", "tokens": [407, 321, 643, 281, 519, 466, 264, 4985, 295, 13160, 293, 264, 13280, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 161, "seek": 60100, "start": 620.0, "end": 622.0, "text": " So as I said, there is the forces.", "tokens": [407, 382, 286, 848, 11, 456, 307, 264, 5874, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 162, "seek": 60100, "start": 622.0, "end": 624.0, "text": " Newton would love this.", "tokens": [19541, 576, 959, 341, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 163, "seek": 60100, "start": 624.0, "end": 627.0, "text": " And the equivalent.", "tokens": [400, 264, 10344, 13], "temperature": 0.0, "avg_logprob": -0.15082951954432897, "compression_ratio": 1.5608695652173914, "no_speech_prob": 7.87904136814177e-05}, {"id": 164, "seek": 62700, "start": 627.0, "end": 631.0, "text": " You have a service, some backend that has users.", "tokens": [509, 362, 257, 2643, 11, 512, 38087, 300, 575, 5022, 13], "temperature": 0.0, "avg_logprob": -0.11585365642200816, "compression_ratio": 1.5097087378640777, "no_speech_prob": 4.0467926737619564e-05}, {"id": 165, "seek": 62700, "start": 631.0, "end": 638.0, "text": " And as I said before, you would never imagine the ways they find to use your service.", "tokens": [400, 382, 286, 848, 949, 11, 291, 576, 1128, 3811, 264, 2098, 436, 915, 281, 764, 428, 2643, 13], "temperature": 0.0, "avg_logprob": -0.11585365642200816, "compression_ratio": 1.5097087378640777, "no_speech_prob": 4.0467926737619564e-05}, {"id": 166, "seek": 62700, "start": 638.0, "end": 643.0, "text": " You're usually designed with three or four things in mind, but you know.", "tokens": [509, 434, 2673, 4761, 365, 1045, 420, 1451, 721, 294, 1575, 11, 457, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.11585365642200816, "compression_ratio": 1.5097087378640777, "no_speech_prob": 4.0467926737619564e-05}, {"id": 167, "seek": 62700, "start": 643.0, "end": 655.0, "text": " So I would say that the equivalent of the forces that can be applied in different directions are, like,", "tokens": [407, 286, 576, 584, 300, 264, 10344, 295, 264, 5874, 300, 393, 312, 6456, 294, 819, 11095, 366, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.11585365642200816, "compression_ratio": 1.5097087378640777, "no_speech_prob": 4.0467926737619564e-05}, {"id": 168, "seek": 65500, "start": 655.0, "end": 657.0, "text": " self-independent programs.", "tokens": [2698, 12, 471, 4217, 317, 4268, 13], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 169, "seek": 65500, "start": 657.0, "end": 663.0, "text": " Imagine that each one of those users is a program that decides how to apply his force,", "tokens": [11739, 300, 1184, 472, 295, 729, 5022, 307, 257, 1461, 300, 14898, 577, 281, 3079, 702, 3464, 11], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 170, "seek": 65500, "start": 663.0, "end": 665.0, "text": " decides how to interact.", "tokens": [14898, 577, 281, 4648, 13], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 171, "seek": 65500, "start": 665.0, "end": 669.0, "text": " Like, each one of those many arrows that you can draw in this bridge,", "tokens": [1743, 11, 1184, 472, 295, 729, 867, 19669, 300, 291, 393, 2642, 294, 341, 7283, 11], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 172, "seek": 65500, "start": 669.0, "end": 672.0, "text": " and this is infinite if you get involved with differential equations and, you know,", "tokens": [293, 341, 307, 13785, 498, 291, 483, 3288, 365, 15756, 11787, 293, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 173, "seek": 65500, "start": 672.0, "end": 675.0, "text": " complicated mathematics, everything moves like crazy.", "tokens": [6179, 18666, 11, 1203, 6067, 411, 3219, 13], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 174, "seek": 65500, "start": 675.0, "end": 683.0, "text": " All those moving arrows can be represented with an independent program on its own.", "tokens": [1057, 729, 2684, 19669, 393, 312, 10379, 365, 364, 6695, 1461, 322, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.11299354251068418, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00011584709864109755}, {"id": 175, "seek": 68300, "start": 683.0, "end": 686.0, "text": " And those programs interact with each other.", "tokens": [400, 729, 4268, 4648, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.12574404188730184, "compression_ratio": 1.6177777777777778, "no_speech_prob": 7.464081136276945e-05}, {"id": 176, "seek": 68300, "start": 686.0, "end": 690.0, "text": " This is the model of the actor that I can imagine that most of you, more or less,", "tokens": [639, 307, 264, 2316, 295, 264, 8747, 300, 286, 393, 3811, 300, 881, 295, 291, 11, 544, 420, 1570, 11], "temperature": 0.0, "avg_logprob": -0.12574404188730184, "compression_ratio": 1.6177777777777778, "no_speech_prob": 7.464081136276945e-05}, {"id": 177, "seek": 68300, "start": 690.0, "end": 693.0, "text": " would be familiar with, like, what we do in Erlang and Elixir.", "tokens": [576, 312, 4963, 365, 11, 411, 11, 437, 321, 360, 294, 3300, 25241, 293, 2699, 970, 347, 13], "temperature": 0.0, "avg_logprob": -0.12574404188730184, "compression_ratio": 1.6177777777777778, "no_speech_prob": 7.464081136276945e-05}, {"id": 178, "seek": 68300, "start": 693.0, "end": 702.0, "text": " For those of you that are not, the idea, basically, by the way, before I go to the next slide,", "tokens": [1171, 729, 295, 291, 300, 366, 406, 11, 264, 1558, 11, 1936, 11, 538, 264, 636, 11, 949, 286, 352, 281, 264, 958, 4137, 11], "temperature": 0.0, "avg_logprob": -0.12574404188730184, "compression_ratio": 1.6177777777777778, "no_speech_prob": 7.464081136276945e-05}, {"id": 179, "seek": 68300, "start": 702.0, "end": 710.0, "text": " this is Karl Hewitt, the guy that named the actor model that put it into paper.", "tokens": [341, 307, 20405, 634, 86, 593, 11, 264, 2146, 300, 4926, 264, 8747, 2316, 300, 829, 309, 666, 3035, 13], "temperature": 0.0, "avg_logprob": -0.12574404188730184, "compression_ratio": 1.6177777777777778, "no_speech_prob": 7.464081136276945e-05}, {"id": 180, "seek": 71000, "start": 710.0, "end": 715.0, "text": " He died a month ago, or almost two, maybe, somewhere in mid-December.", "tokens": [634, 4539, 257, 1618, 2057, 11, 420, 1920, 732, 11, 1310, 11, 4079, 294, 2062, 12, 11089, 7432, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 181, "seek": 71000, "start": 715.0, "end": 717.0, "text": " So, a bit of a tribute to him.", "tokens": [407, 11, 257, 857, 295, 257, 24722, 281, 796, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 182, "seek": 71000, "start": 717.0, "end": 719.0, "text": " Thank you for the theory.", "tokens": [1044, 291, 337, 264, 5261, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 183, "seek": 71000, "start": 719.0, "end": 723.0, "text": " For those of you that may not be familiar with the concept of the actor,", "tokens": [1171, 729, 295, 291, 300, 815, 406, 312, 4963, 365, 264, 3410, 295, 264, 8747, 11], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 184, "seek": 71000, "start": 723.0, "end": 726.0, "text": " basically, it's the universal primitive.", "tokens": [1936, 11, 309, 311, 264, 11455, 28540, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 185, "seek": 71000, "start": 726.0, "end": 730.0, "text": " In a language like Ruby, for example, everything is an object.", "tokens": [682, 257, 2856, 411, 19907, 11, 337, 1365, 11, 1203, 307, 364, 2657, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 186, "seek": 71000, "start": 730.0, "end": 734.0, "text": " You can do whatever, dot something, and maybe it will crash because it's not valid.", "tokens": [509, 393, 360, 2035, 11, 5893, 746, 11, 293, 1310, 309, 486, 8252, 570, 309, 311, 406, 7363, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 187, "seek": 71000, "start": 734.0, "end": 736.0, "text": " The compiler may tell you, but you can.", "tokens": [440, 31958, 815, 980, 291, 11, 457, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 188, "seek": 71000, "start": 736.0, "end": 738.0, "text": " That's how you design your program.", "tokens": [663, 311, 577, 291, 1715, 428, 1461, 13], "temperature": 0.0, "avg_logprob": -0.07211774679330679, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00017025452689267695}, {"id": 189, "seek": 73800, "start": 738.0, "end": 740.0, "text": " In a language like Lisp, everything is a function.", "tokens": [682, 257, 2856, 411, 441, 7631, 11, 1203, 307, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 190, "seek": 73800, "start": 740.0, "end": 741.0, "text": " Absolutely everything.", "tokens": [7021, 1203, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 191, "seek": 73800, "start": 741.0, "end": 743.0, "text": " You can do whatever parentheses.", "tokens": [509, 393, 360, 2035, 34153, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 192, "seek": 73800, "start": 743.0, "end": 744.0, "text": " And maybe it's not valid.", "tokens": [400, 1310, 309, 311, 406, 7363, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 193, "seek": 73800, "start": 744.0, "end": 745.0, "text": " Maybe it will crash.", "tokens": [2704, 309, 486, 8252, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 194, "seek": 73800, "start": 745.0, "end": 748.0, "text": " Maybe the compiler will tell you before compiling.", "tokens": [2704, 264, 31958, 486, 980, 291, 949, 715, 4883, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 195, "seek": 73800, "start": 748.0, "end": 750.0, "text": " In a language like Erlang, everything is an actor.", "tokens": [682, 257, 2856, 411, 3300, 25241, 11, 1203, 307, 364, 8747, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 196, "seek": 73800, "start": 750.0, "end": 753.0, "text": " You can do whatever exclamation marks send a message.", "tokens": [509, 393, 360, 2035, 1624, 43233, 10640, 2845, 257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 197, "seek": 73800, "start": 753.0, "end": 755.0, "text": " And it's almost never valid.", "tokens": [400, 309, 311, 1920, 1128, 7363, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 198, "seek": 73800, "start": 755.0, "end": 760.0, "text": " It's only by a process identifier, or if it has a name, a proper name.", "tokens": [467, 311, 787, 538, 257, 1399, 45690, 11, 420, 498, 309, 575, 257, 1315, 11, 257, 2296, 1315, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 199, "seek": 73800, "start": 760.0, "end": 764.0, "text": " So, this is the model of your program.", "tokens": [407, 11, 341, 307, 264, 2316, 295, 428, 1461, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 200, "seek": 73800, "start": 764.0, "end": 767.0, "text": " This is how you structure the program.", "tokens": [639, 307, 577, 291, 3877, 264, 1461, 13], "temperature": 0.0, "avg_logprob": -0.08603835105895996, "compression_ratio": 1.783882783882784, "no_speech_prob": 3.362496863701381e-05}, {"id": 201, "seek": 76700, "start": 767.0, "end": 777.0, "text": " How are we going to load test a service?", "tokens": [1012, 366, 321, 516, 281, 3677, 1500, 257, 2643, 30], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 202, "seek": 76700, "start": 777.0, "end": 784.0, "text": " Light thickens, and the crawl makes wing to the rocky wood.", "tokens": [8279, 5060, 694, 11, 293, 264, 24767, 1669, 11162, 281, 264, 33301, 4576, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 203, "seek": 76700, "start": 784.0, "end": 786.0, "text": " This has lots of background.", "tokens": [639, 575, 3195, 295, 3678, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 204, "seek": 76700, "start": 786.0, "end": 787.0, "text": " It's a very personal thing.", "tokens": [467, 311, 257, 588, 2973, 551, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 205, "seek": 76700, "start": 787.0, "end": 790.0, "text": " First of all, of course, I love Shakespeare, but that's not the point.", "tokens": [2386, 295, 439, 11, 295, 1164, 11, 286, 959, 22825, 11, 457, 300, 311, 406, 264, 935, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 206, "seek": 76700, "start": 790.0, "end": 793.0, "text": " I work, as I said, at the beginning in MongoSIM service.", "tokens": [286, 589, 11, 382, 286, 848, 11, 412, 264, 2863, 294, 48380, 50, 6324, 2643, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 207, "seek": 76700, "start": 793.0, "end": 795.0, "text": " That is an XMPP implementation.", "tokens": [663, 307, 364, 1783, 12224, 47, 11420, 13], "temperature": 0.0, "avg_logprob": -0.16889917474043997, "compression_ratio": 1.440909090909091, "no_speech_prob": 0.00015021911531221122}, {"id": 208, "seek": 79500, "start": 795.0, "end": 799.0, "text": " And in XMPP, I don't know why, but I'm very happy about it.", "tokens": [400, 294, 1783, 12224, 47, 11, 286, 500, 380, 458, 983, 11, 457, 286, 478, 588, 2055, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1271195125579834, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00037971753044985235}, {"id": 209, "seek": 79500, "start": 799.0, "end": 806.0, "text": " All the examples in the RFC are given with Shakespeare quotes.", "tokens": [1057, 264, 5110, 294, 264, 497, 18671, 366, 2212, 365, 22825, 19963, 13], "temperature": 0.0, "avg_logprob": -0.1271195125579834, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00037971753044985235}, {"id": 210, "seek": 79500, "start": 806.0, "end": 811.0, "text": " So, when it comes to messages, you know, there is Alice writing to, not Alice.", "tokens": [407, 11, 562, 309, 1487, 281, 7897, 11, 291, 458, 11, 456, 307, 16004, 3579, 281, 11, 406, 16004, 13], "temperature": 0.0, "avg_logprob": -0.1271195125579834, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00037971753044985235}, {"id": 211, "seek": 79500, "start": 811.0, "end": 816.0, "text": " Juliet writing to Romeo from the balcony, and then all the examples are like this.", "tokens": [33532, 3579, 281, 33563, 490, 264, 29468, 11, 293, 550, 439, 264, 5110, 366, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1271195125579834, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00037971753044985235}, {"id": 212, "seek": 79500, "start": 816.0, "end": 824.0, "text": " So, we made a piece of service based on a quote from Shakespeare, the name.", "tokens": [407, 11, 321, 1027, 257, 2522, 295, 2643, 2361, 322, 257, 6513, 490, 22825, 11, 264, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1271195125579834, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00037971753044985235}, {"id": 213, "seek": 82400, "start": 824.0, "end": 828.0, "text": " That is called a murder of crows.", "tokens": [663, 307, 1219, 257, 6568, 295, 941, 1509, 13], "temperature": 0.0, "avg_logprob": -0.12486484438874, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.751083805691451e-05}, {"id": 214, "seek": 82400, "start": 828.0, "end": 831.0, "text": " I also love Hitchcock.", "tokens": [286, 611, 959, 389, 1549, 29779, 13], "temperature": 0.0, "avg_logprob": -0.12486484438874, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.751083805691451e-05}, {"id": 215, "seek": 82400, "start": 831.0, "end": 835.0, "text": " If you haven't watched it, please watch this movie.", "tokens": [759, 291, 2378, 380, 6337, 309, 11, 1767, 1159, 341, 3169, 13], "temperature": 0.0, "avg_logprob": -0.12486484438874, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.751083805691451e-05}, {"id": 216, "seek": 82400, "start": 835.0, "end": 843.0, "text": " So, there is this library that we created in my team to test MongoSIM on the load.", "tokens": [407, 11, 456, 307, 341, 6405, 300, 321, 2942, 294, 452, 1469, 281, 1500, 48380, 50, 6324, 322, 264, 3677, 13], "temperature": 0.0, "avg_logprob": -0.12486484438874, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.751083805691451e-05}, {"id": 217, "seek": 82400, "start": 843.0, "end": 849.0, "text": " That is called a murder of crows, because crows are dangerous and are there to kill you and eat your corpse.", "tokens": [663, 307, 1219, 257, 6568, 295, 941, 1509, 11, 570, 941, 1509, 366, 5795, 293, 366, 456, 281, 1961, 291, 293, 1862, 428, 30324, 13], "temperature": 0.0, "avg_logprob": -0.12486484438874, "compression_ratio": 1.6129032258064515, "no_speech_prob": 8.751083805691451e-05}, {"id": 218, "seek": 84900, "start": 849.0, "end": 857.0, "text": " So, this is what we try to do, to just kill MongoSIM, see dying, and then try to make it stronger next time.", "tokens": [407, 11, 341, 307, 437, 321, 853, 281, 360, 11, 281, 445, 1961, 48380, 50, 6324, 11, 536, 8639, 11, 293, 550, 853, 281, 652, 309, 7249, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 219, "seek": 84900, "start": 857.0, "end": 863.0, "text": " And with this project, we reflect about the interactions, the traffic capacity,", "tokens": [400, 365, 341, 1716, 11, 321, 5031, 466, 264, 13280, 11, 264, 6419, 6042, 11], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 220, "seek": 84900, "start": 863.0, "end": 866.0, "text": " amplifying factor, all new forces.", "tokens": [9731, 5489, 5952, 11, 439, 777, 5874, 13], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 221, "seek": 84900, "start": 866.0, "end": 871.0, "text": " So, in the case of a messaging system, there is this vulnerability that happens to everyone back in the day.", "tokens": [407, 11, 294, 264, 1389, 295, 257, 21812, 1185, 11, 456, 307, 341, 24210, 300, 2314, 281, 1518, 646, 294, 264, 786, 13], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 222, "seek": 84900, "start": 871.0, "end": 872.0, "text": " You know, there is compression.", "tokens": [509, 458, 11, 456, 307, 19355, 13], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 223, "seek": 84900, "start": 872.0, "end": 877.0, "text": " Somebody sends you a small packet, you decompress it, and boom, your run out of memory.", "tokens": [13463, 14790, 291, 257, 1359, 20300, 11, 291, 22867, 735, 309, 11, 293, 9351, 11, 428, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.10618874178094379, "compression_ratio": 1.6200716845878136, "no_speech_prob": 8.20673449197784e-05}, {"id": 224, "seek": 87700, "start": 877.0, "end": 882.0, "text": " These kind of things, you have to look for these amplifying forces, the traffic capacity,", "tokens": [1981, 733, 295, 721, 11, 291, 362, 281, 574, 337, 613, 9731, 5489, 5874, 11, 264, 6419, 6042, 11], "temperature": 0.0, "avg_logprob": -0.16632973393307457, "compression_ratio": 1.58, "no_speech_prob": 0.0001928304263856262}, {"id": 225, "seek": 87700, "start": 882.0, "end": 892.0, "text": " how much traffic each client can send, how many clients can you have, all new forces.", "tokens": [577, 709, 6419, 1184, 6423, 393, 2845, 11, 577, 867, 6982, 393, 291, 362, 11, 439, 777, 5874, 13], "temperature": 0.0, "avg_logprob": -0.16632973393307457, "compression_ratio": 1.58, "no_speech_prob": 0.0001928304263856262}, {"id": 226, "seek": 87700, "start": 892.0, "end": 900.0, "text": " Something that may not be a surprise for old schoolers, Erlang developers.", "tokens": [6595, 300, 815, 406, 312, 257, 6365, 337, 1331, 1395, 433, 11, 3300, 25241, 8849, 13], "temperature": 0.0, "avg_logprob": -0.16632973393307457, "compression_ratio": 1.58, "no_speech_prob": 0.0001928304263856262}, {"id": 227, "seek": 87700, "start": 900.0, "end": 906.0, "text": " This new world of cloud, that is someone else's computer, really.", "tokens": [639, 777, 1002, 295, 4588, 11, 300, 307, 1580, 1646, 311, 3820, 11, 534, 13], "temperature": 0.0, "avg_logprob": -0.16632973393307457, "compression_ratio": 1.58, "no_speech_prob": 0.0001928304263856262}, {"id": 228, "seek": 90600, "start": 906.0, "end": 911.0, "text": " If all your microservices connection are a lot less stable, distribution is not as cool and easy", "tokens": [759, 439, 428, 15547, 47480, 4984, 366, 257, 688, 1570, 8351, 11, 7316, 307, 406, 382, 1627, 293, 1858], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 229, "seek": 90600, "start": 911.0, "end": 915.0, "text": " as it was when Ericsson made it and hardware was indestructible, you know, the punching theory.", "tokens": [382, 309, 390, 562, 3300, 1167, 3015, 1027, 309, 293, 8837, 390, 1016, 43056, 964, 11, 291, 458, 11, 264, 34866, 5261, 13], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 230, "seek": 90600, "start": 915.0, "end": 916.0, "text": " Nothing happens.", "tokens": [6693, 2314, 13], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 231, "seek": 90600, "start": 916.0, "end": 918.0, "text": " Now it dies.", "tokens": [823, 309, 2714, 13], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 232, "seek": 90600, "start": 918.0, "end": 925.0, "text": " So, all new forces that now make a difference in the new way of building a system.", "tokens": [407, 11, 439, 777, 5874, 300, 586, 652, 257, 2649, 294, 264, 777, 636, 295, 2390, 257, 1185, 13], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 233, "seek": 90600, "start": 925.0, "end": 931.0, "text": " In the case of MongoSIM, we have these usual use cases, session establishment.", "tokens": [682, 264, 1389, 295, 48380, 50, 6324, 11, 321, 362, 613, 7713, 764, 3331, 11, 5481, 20971, 13], "temperature": 0.0, "avg_logprob": -0.11617814064025879, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.1880672647967003e-05}, {"id": 234, "seek": 93100, "start": 931.0, "end": 936.0, "text": " So, you know, somebody logs in, authentication, password, password less, make up your mind.", "tokens": [407, 11, 291, 458, 11, 2618, 20820, 294, 11, 26643, 11, 11524, 11, 11524, 1570, 11, 652, 493, 428, 1575, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 235, "seek": 93100, "start": 936.0, "end": 937.0, "text": " Send messages.", "tokens": [17908, 7897, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 236, "seek": 93100, "start": 937.0, "end": 939.0, "text": " Obviously, it's all about sending messages.", "tokens": [7580, 11, 309, 311, 439, 466, 7750, 7897, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 237, "seek": 93100, "start": 939.0, "end": 941.0, "text": " Fetch in your archive.", "tokens": [479, 7858, 294, 428, 23507, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 238, "seek": 93100, "start": 941.0, "end": 945.0, "text": " You reconnect after a while, you are on holidays, and then you fetch all the messages you lost.", "tokens": [509, 30095, 934, 257, 1339, 11, 291, 366, 322, 15734, 11, 293, 550, 291, 23673, 439, 264, 7897, 291, 2731, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 239, "seek": 93100, "start": 945.0, "end": 946.0, "text": " This is stored somewhere.", "tokens": [639, 307, 12187, 4079, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 240, "seek": 93100, "start": 946.0, "end": 948.0, "text": " It has to be stored as you send it.", "tokens": [467, 575, 281, 312, 12187, 382, 291, 2845, 309, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 241, "seek": 93100, "start": 948.0, "end": 952.0, "text": " What is the impact that it has on sending, on receiving?", "tokens": [708, 307, 264, 2712, 300, 309, 575, 322, 7750, 11, 322, 10040, 30], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 242, "seek": 93100, "start": 952.0, "end": 955.0, "text": " Joining and leaving group chats.", "tokens": [40229, 293, 5012, 1594, 38057, 13], "temperature": 0.0, "avg_logprob": -0.13694481264080918, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.0001042396470438689}, {"id": 243, "seek": 95500, "start": 955.0, "end": 963.0, "text": " This is something, and in all classic XMPP, it's a problem to scale, but all classic", "tokens": [639, 307, 746, 11, 293, 294, 439, 7230, 1783, 12224, 47, 11, 309, 311, 257, 1154, 281, 4373, 11, 457, 439, 7230], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 244, "seek": 95500, "start": 963.0, "end": 964.0, "text": " with the time happened.", "tokens": [365, 264, 565, 2011, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 245, "seek": 95500, "start": 964.0, "end": 966.0, "text": " We had solutions for that.", "tokens": [492, 632, 6547, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 246, "seek": 95500, "start": 966.0, "end": 967.0, "text": " So, I had these problems.", "tokens": [407, 11, 286, 632, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 247, "seek": 95500, "start": 967.0, "end": 968.0, "text": " We need to test them.", "tokens": [492, 643, 281, 1500, 552, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 248, "seek": 95500, "start": 968.0, "end": 972.0, "text": " And we think how to test them.", "tokens": [400, 321, 519, 577, 281, 1500, 552, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 249, "seek": 95500, "start": 972.0, "end": 979.0, "text": " So, you start your scenario, and at testing time, you need a init, a startup.", "tokens": [407, 11, 291, 722, 428, 9005, 11, 293, 412, 4997, 565, 11, 291, 643, 257, 3157, 11, 257, 18578, 13], "temperature": 0.0, "avg_logprob": -0.1675582143995497, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00011969799379585311}, {"id": 250, "seek": 97900, "start": 979.0, "end": 985.0, "text": " Like, start the metrics, start the functionality that is going to coordinate all your actors", "tokens": [1743, 11, 722, 264, 16367, 11, 722, 264, 14980, 300, 307, 516, 281, 15670, 439, 428, 10037], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 251, "seek": 97900, "start": 985.0, "end": 987.0, "text": " when they have some interaction between them.", "tokens": [562, 436, 362, 512, 9285, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 252, "seek": 97900, "start": 987.0, "end": 991.0, "text": " For example, in a group chat, you are going to create so many actors that then they will", "tokens": [1171, 1365, 11, 294, 257, 1594, 5081, 11, 291, 366, 516, 281, 1884, 370, 867, 10037, 300, 550, 436, 486], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 253, "seek": 97900, "start": 991.0, "end": 993.0, "text": " join the same group chat and talk to each other.", "tokens": [3917, 264, 912, 1594, 5081, 293, 751, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 254, "seek": 97900, "start": 993.0, "end": 998.0, "text": " Or in a multi-user game, you are going to have millions of users, but they will cluster", "tokens": [1610, 294, 257, 4825, 12, 18088, 1216, 11, 291, 366, 516, 281, 362, 6803, 295, 5022, 11, 457, 436, 486, 13630], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 255, "seek": 97900, "start": 998.0, "end": 999.0, "text": " in groups.", "tokens": [294, 3935, 13], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 256, "seek": 97900, "start": 999.0, "end": 1000.0, "text": " So, you need to coordinate them.", "tokens": [407, 11, 291, 643, 281, 15670, 552, 13], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 257, "seek": 97900, "start": 1000.0, "end": 1006.0, "text": " So, you will start logic to capture users and to coordinate them and join the same group,", "tokens": [407, 11, 291, 486, 722, 9952, 281, 7983, 5022, 293, 281, 15670, 552, 293, 3917, 264, 912, 1594, 11], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 258, "seek": 97900, "start": 1006.0, "end": 1008.0, "text": " et cetera, et cetera.", "tokens": [1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.06888070500882945, "compression_ratio": 2.0, "no_speech_prob": 7.724176975898445e-05}, {"id": 259, "seek": 100800, "start": 1008.0, "end": 1010.0, "text": " So, you start all the actors.", "tokens": [407, 11, 291, 722, 439, 264, 10037, 13], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 260, "seek": 100800, "start": 1010.0, "end": 1018.0, "text": " After all your init, then you spawn all the process, you know, and each one executes the", "tokens": [2381, 439, 428, 3157, 11, 550, 291, 17088, 439, 264, 1399, 11, 291, 458, 11, 293, 1184, 472, 4454, 1819, 264], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 261, "seek": 100800, "start": 1018.0, "end": 1023.0, "text": " program they are supposed to, that they have been coded to do.", "tokens": [1461, 436, 366, 3442, 281, 11, 300, 436, 362, 668, 34874, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 262, "seek": 100800, "start": 1023.0, "end": 1025.0, "text": " And then you run it.", "tokens": [400, 550, 291, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 263, "seek": 100800, "start": 1025.0, "end": 1026.0, "text": " Locally or distributed.", "tokens": [12859, 379, 420, 12631, 13], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 264, "seek": 100800, "start": 1026.0, "end": 1031.0, "text": " At some point, the load that you can generate doesn't fit in a single computer, so it has", "tokens": [1711, 512, 935, 11, 264, 3677, 300, 291, 393, 8460, 1177, 380, 3318, 294, 257, 2167, 3820, 11, 370, 309, 575], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 265, "seek": 100800, "start": 1031.0, "end": 1035.0, "text": " to be distributed, so you need your service to handle the distribution for you.", "tokens": [281, 312, 12631, 11, 370, 291, 643, 428, 2643, 281, 4813, 264, 7316, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15108901763630805, "compression_ratio": 1.706896551724138, "no_speech_prob": 7.744088361505419e-05}, {"id": 266, "seek": 103500, "start": 1035.0, "end": 1043.0, "text": " The purpose of the load testing is checking how your software is going to survive or die", "tokens": [440, 4334, 295, 264, 3677, 4997, 307, 8568, 577, 428, 4722, 307, 516, 281, 7867, 420, 978], "temperature": 0.0, "avg_logprob": -0.07722818398777442, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.00019434049318078905}, {"id": 267, "seek": 103500, "start": 1043.0, "end": 1048.0, "text": " and not implementing the load testing idea.", "tokens": [293, 406, 18114, 264, 3677, 4997, 1558, 13], "temperature": 0.0, "avg_logprob": -0.07722818398777442, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.00019434049318078905}, {"id": 268, "seek": 103500, "start": 1048.0, "end": 1055.0, "text": " We want a load testing library that will just give me all the users, give me a way to coordinate", "tokens": [492, 528, 257, 3677, 4997, 6405, 300, 486, 445, 976, 385, 439, 264, 5022, 11, 976, 385, 257, 636, 281, 15670], "temperature": 0.0, "avg_logprob": -0.07722818398777442, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.00019434049318078905}, {"id": 269, "seek": 103500, "start": 1055.0, "end": 1060.0, "text": " them when I have to, to throttle them when I have to, and the rate that I have to, to", "tokens": [552, 562, 286, 362, 281, 11, 281, 24235, 552, 562, 286, 362, 281, 11, 293, 264, 3314, 300, 286, 362, 281, 11, 281], "temperature": 0.0, "avg_logprob": -0.07722818398777442, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.00019434049318078905}, {"id": 270, "seek": 106000, "start": 1060.0, "end": 1066.0, "text": " handle whatever place I need to start this load testing.", "tokens": [4813, 2035, 1081, 286, 643, 281, 722, 341, 3677, 4997, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 271, "seek": 106000, "start": 1066.0, "end": 1068.0, "text": " And I don't want to think about all of that.", "tokens": [400, 286, 500, 380, 528, 281, 519, 466, 439, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 272, "seek": 106000, "start": 1068.0, "end": 1073.0, "text": " I just want to describe the scenario that I'm going to use to kill my service.", "tokens": [286, 445, 528, 281, 6786, 264, 9005, 300, 286, 478, 516, 281, 764, 281, 1961, 452, 2643, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 273, "seek": 106000, "start": 1073.0, "end": 1079.0, "text": " So, we build a library that does all that other stuff.", "tokens": [407, 11, 321, 1322, 257, 6405, 300, 775, 439, 300, 661, 1507, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 274, "seek": 106000, "start": 1079.0, "end": 1082.0, "text": " Very important thing is the throttle idea.", "tokens": [4372, 1021, 551, 307, 264, 24235, 1558, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 275, "seek": 106000, "start": 1082.0, "end": 1087.0, "text": " In the case of the chat service, imagine that a million users connect exactly at the same", "tokens": [682, 264, 1389, 295, 264, 5081, 2643, 11, 3811, 300, 257, 2459, 5022, 1745, 2293, 412, 264, 912], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 276, "seek": 106000, "start": 1087.0, "end": 1089.0, "text": " time and looking at the same time.", "tokens": [565, 293, 1237, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.0970007456265963, "compression_ratio": 1.631578947368421, "no_speech_prob": 3.4290827898075804e-05}, {"id": 277, "seek": 108900, "start": 1089.0, "end": 1091.0, "text": " It's probably not a real use case.", "tokens": [467, 311, 1391, 406, 257, 957, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.11436695056957202, "compression_ratio": 1.7311320754716981, "no_speech_prob": 5.705700459657237e-05}, {"id": 278, "seek": 108900, "start": 1091.0, "end": 1096.0, "text": " You can test for that, but that is the stress part when you want to kill the service.", "tokens": [509, 393, 1500, 337, 300, 11, 457, 300, 307, 264, 4244, 644, 562, 291, 528, 281, 1961, 264, 2643, 13], "temperature": 0.0, "avg_logprob": -0.11436695056957202, "compression_ratio": 1.7311320754716981, "no_speech_prob": 5.705700459657237e-05}, {"id": 279, "seek": 108900, "start": 1096.0, "end": 1102.0, "text": " That later, you would usually see what happens when you connect 100 per second, and then", "tokens": [663, 1780, 11, 291, 576, 2673, 536, 437, 2314, 562, 291, 1745, 2319, 680, 1150, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.11436695056957202, "compression_ratio": 1.7311320754716981, "no_speech_prob": 5.705700459657237e-05}, {"id": 280, "seek": 108900, "start": 1102.0, "end": 1108.0, "text": " you increment 200 per second, 500 per second, 1,000 per second, and you want to have a", "tokens": [291, 26200, 2331, 680, 1150, 11, 5923, 680, 1150, 11, 502, 11, 1360, 680, 1150, 11, 293, 291, 528, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.11436695056957202, "compression_ratio": 1.7311320754716981, "no_speech_prob": 5.705700459657237e-05}, {"id": 281, "seek": 108900, "start": 1108.0, "end": 1113.0, "text": " functionality that will throttle and progressively increment the rate.", "tokens": [14980, 300, 486, 24235, 293, 46667, 26200, 264, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11436695056957202, "compression_ratio": 1.7311320754716981, "no_speech_prob": 5.705700459657237e-05}, {"id": 282, "seek": 111300, "start": 1113.0, "end": 1119.0, "text": " And then seeing your metrics, both load testing library will output to Grafana, your service", "tokens": [400, 550, 2577, 428, 16367, 11, 1293, 3677, 4997, 6405, 486, 5598, 281, 8985, 69, 2095, 11, 428, 2643], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 283, "seek": 111300, "start": 1119.0, "end": 1124.0, "text": " that you're testing will output to Grafana, and then see the correlations.", "tokens": [300, 291, 434, 4997, 486, 5598, 281, 8985, 69, 2095, 11, 293, 550, 536, 264, 13983, 763, 13], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 284, "seek": 111300, "start": 1124.0, "end": 1129.0, "text": " You want actors to wait for the permission.", "tokens": [509, 528, 10037, 281, 1699, 337, 264, 11226, 13], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 285, "seek": 111300, "start": 1129.0, "end": 1131.0, "text": " Am I allowed to do this already?", "tokens": [2012, 286, 4350, 281, 360, 341, 1217, 30], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 286, "seek": 111300, "start": 1131.0, "end": 1136.0, "text": " And the cases, the session establishment, but also joining a group chat, how many messages", "tokens": [400, 264, 3331, 11, 264, 5481, 20971, 11, 457, 611, 5549, 257, 1594, 5081, 11, 577, 867, 7897], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 287, "seek": 111300, "start": 1136.0, "end": 1138.0, "text": " are you going to send.", "tokens": [366, 291, 516, 281, 2845, 13], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 288, "seek": 111300, "start": 1138.0, "end": 1142.0, "text": " There is this, you know, you have an arrow that is going to be applied in one place.", "tokens": [821, 307, 341, 11, 291, 458, 11, 291, 362, 364, 11610, 300, 307, 516, 281, 312, 6456, 294, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10356687462848166, "compression_ratio": 1.6908396946564885, "no_speech_prob": 0.00013832698459737003}, {"id": 289, "seek": 114200, "start": 1142.0, "end": 1144.0, "text": " How big do you want the arrow to be?", "tokens": [1012, 955, 360, 291, 528, 264, 11610, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 290, "seek": 114200, "start": 1144.0, "end": 1149.0, "text": " You want that arrow to grow incrementally.", "tokens": [509, 528, 300, 11610, 281, 1852, 26200, 379, 13], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 291, "seek": 114200, "start": 1149.0, "end": 1153.0, "text": " And you may want to ask another actor to wait for the approval.", "tokens": [400, 291, 815, 528, 281, 1029, 1071, 8747, 281, 1699, 337, 264, 13317, 13], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 292, "seek": 114200, "start": 1153.0, "end": 1160.0, "text": " You can tell the throttle logic to tell that actor to wait for something.", "tokens": [509, 393, 980, 264, 24235, 9952, 281, 980, 300, 8747, 281, 1699, 337, 746, 13], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 293, "seek": 114200, "start": 1160.0, "end": 1165.0, "text": " And then that actor, which is not yourself, will wait for the action.", "tokens": [400, 550, 300, 8747, 11, 597, 307, 406, 1803, 11, 486, 1699, 337, 264, 3069, 13], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 294, "seek": 114200, "start": 1165.0, "end": 1168.0, "text": " For example, in the case of joining a group chat, first you have to create it.", "tokens": [1171, 1365, 11, 294, 264, 1389, 295, 5549, 257, 1594, 5081, 11, 700, 291, 362, 281, 1884, 309, 13], "temperature": 0.0, "avg_logprob": -0.06212840129419701, "compression_ratio": 1.7102803738317758, "no_speech_prob": 2.6294490453437902e-05}, {"id": 295, "seek": 116800, "start": 1168.0, "end": 1173.0, "text": " So there is a first user that says to everyone, wait, don't join because I need to create the group chat first.", "tokens": [407, 456, 307, 257, 700, 4195, 300, 1619, 281, 1518, 11, 1699, 11, 500, 380, 3917, 570, 286, 643, 281, 1884, 264, 1594, 5081, 700, 13], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 296, "seek": 116800, "start": 1173.0, "end": 1178.0, "text": " Voila is created, come here, et cetera.", "tokens": [7518, 7371, 307, 2942, 11, 808, 510, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 297, "seek": 116800, "start": 1178.0, "end": 1183.0, "text": " And another very piece of important functionality is the coordination idea.", "tokens": [400, 1071, 588, 2522, 295, 1021, 14980, 307, 264, 21252, 1558, 13], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 298, "seek": 116800, "start": 1183.0, "end": 1192.0, "text": " So as actors are appearing in your load test, one thing that you will want to do, as I said before,", "tokens": [407, 382, 10037, 366, 19870, 294, 428, 3677, 1500, 11, 472, 551, 300, 291, 486, 528, 281, 360, 11, 382, 286, 848, 949, 11], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 299, "seek": 116800, "start": 1192.0, "end": 1194.0, "text": " is to coordinate sets of them.", "tokens": [307, 281, 15670, 6352, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 300, "seek": 116800, "start": 1194.0, "end": 1197.0, "text": " For example, who is going to write to whom?", "tokens": [1171, 1365, 11, 567, 307, 516, 281, 2464, 281, 7101, 30], "temperature": 0.0, "avg_logprob": -0.0989960443405878, "compression_ratio": 1.5952380952380953, "no_speech_prob": 4.6251388994278386e-05}, {"id": 301, "seek": 119700, "start": 1197.0, "end": 1203.0, "text": " So you want an actor to know about another one, so it can send him a message.", "tokens": [407, 291, 528, 364, 8747, 281, 458, 466, 1071, 472, 11, 370, 309, 393, 2845, 796, 257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.08195296147974526, "compression_ratio": 1.6649746192893402, "no_speech_prob": 7.403880590572953e-05}, {"id": 302, "seek": 119700, "start": 1203.0, "end": 1211.0, "text": " You want a functionality that will pick up actors as they are starting in a configurable way,", "tokens": [509, 528, 257, 14980, 300, 486, 1888, 493, 10037, 382, 436, 366, 2891, 294, 257, 22192, 712, 636, 11], "temperature": 0.0, "avg_logprob": -0.08195296147974526, "compression_ratio": 1.6649746192893402, "no_speech_prob": 7.403880590572953e-05}, {"id": 303, "seek": 119700, "start": 1211.0, "end": 1218.0, "text": " either all of them that are started or sets of pairs or a list of them.", "tokens": [2139, 439, 295, 552, 300, 366, 1409, 420, 6352, 295, 15494, 420, 257, 1329, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.08195296147974526, "compression_ratio": 1.6649746192893402, "no_speech_prob": 7.403880590572953e-05}, {"id": 304, "seek": 119700, "start": 1218.0, "end": 1225.0, "text": " And once the configurable amount of actors has started, then make them do something.", "tokens": [400, 1564, 264, 22192, 712, 2372, 295, 10037, 575, 1409, 11, 550, 652, 552, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.08195296147974526, "compression_ratio": 1.6649746192893402, "no_speech_prob": 7.403880590572953e-05}, {"id": 305, "seek": 122500, "start": 1225.0, "end": 1233.0, "text": " There is a callback that will get the list of actors that they identify as and will coordinate how they interact with each other.", "tokens": [821, 307, 257, 818, 3207, 300, 486, 483, 264, 1329, 295, 10037, 300, 436, 5876, 382, 293, 486, 15670, 577, 436, 4648, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11264470335725066, "compression_ratio": 1.61139896373057, "no_speech_prob": 6.220900831976905e-05}, {"id": 306, "seek": 122500, "start": 1233.0, "end": 1243.0, "text": " And, yeah, the actor, as they join the coordinator, they will be given the function that they have to do.", "tokens": [400, 11, 1338, 11, 264, 8747, 11, 382, 436, 3917, 264, 27394, 11, 436, 486, 312, 2212, 264, 2445, 300, 436, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.11264470335725066, "compression_ratio": 1.61139896373057, "no_speech_prob": 6.220900831976905e-05}, {"id": 307, "seek": 122500, "start": 1243.0, "end": 1253.0, "text": " So to us, this is what my load testing framework is supposed to help me do.", "tokens": [407, 281, 505, 11, 341, 307, 437, 452, 3677, 4997, 8388, 307, 3442, 281, 854, 385, 360, 13], "temperature": 0.0, "avg_logprob": -0.11264470335725066, "compression_ratio": 1.61139896373057, "no_speech_prob": 6.220900831976905e-05}, {"id": 308, "seek": 125300, "start": 1253.0, "end": 1255.0, "text": " We use it for XMPP.", "tokens": [492, 764, 309, 337, 1783, 12224, 47, 13], "temperature": 0.0, "avg_logprob": -0.13131385896264053, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00018737014033831656}, {"id": 309, "seek": 125300, "start": 1255.0, "end": 1262.0, "text": " So then we have scenarios and functionality written that knows how to do the authentication for the protocol,", "tokens": [407, 550, 321, 362, 15077, 293, 14980, 3720, 300, 3255, 577, 281, 360, 264, 26643, 337, 264, 10336, 11], "temperature": 0.0, "avg_logprob": -0.13131385896264053, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00018737014033831656}, {"id": 310, "seek": 125300, "start": 1262.0, "end": 1264.0, "text": " that knows the functionality of Mongoose IM.", "tokens": [300, 3255, 264, 14980, 295, 48380, 541, 21463, 13], "temperature": 0.0, "avg_logprob": -0.13131385896264053, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00018737014033831656}, {"id": 311, "seek": 125300, "start": 1264.0, "end": 1273.0, "text": " But we don't believe that the load testing library is the one that decides your scenario.", "tokens": [583, 321, 500, 380, 1697, 300, 264, 3677, 4997, 6405, 307, 264, 472, 300, 14898, 428, 9005, 13], "temperature": 0.0, "avg_logprob": -0.13131385896264053, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00018737014033831656}, {"id": 312, "seek": 125300, "start": 1273.0, "end": 1280.0, "text": " I have seen different load testing frameworks that give you functionality to run HTTP requests.", "tokens": [286, 362, 1612, 819, 3677, 4997, 29834, 300, 976, 291, 14980, 281, 1190, 33283, 12475, 13], "temperature": 0.0, "avg_logprob": -0.13131385896264053, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00018737014033831656}, {"id": 313, "seek": 128000, "start": 1280.0, "end": 1284.0, "text": " So what if you are not testing something HTTP related?", "tokens": [407, 437, 498, 291, 366, 406, 4997, 746, 33283, 4077, 30], "temperature": 0.0, "avg_logprob": -0.10770739578619236, "compression_ratio": 1.597938144329897, "no_speech_prob": 9.33104456635192e-05}, {"id": 314, "seek": 128000, "start": 1284.0, "end": 1290.0, "text": " We believe that the best way to write what you want to test is to write the code that you know how to write anyway.", "tokens": [492, 1697, 300, 264, 1151, 636, 281, 2464, 437, 291, 528, 281, 1500, 307, 281, 2464, 264, 3089, 300, 291, 458, 577, 281, 2464, 4033, 13], "temperature": 0.0, "avg_logprob": -0.10770739578619236, "compression_ratio": 1.597938144329897, "no_speech_prob": 9.33104456635192e-05}, {"id": 315, "seek": 128000, "start": 1290.0, "end": 1295.0, "text": " So the idea is that you write Erlang, Elixir is on the way.", "tokens": [407, 264, 1558, 307, 300, 291, 2464, 3300, 25241, 11, 2699, 970, 347, 307, 322, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.10770739578619236, "compression_ratio": 1.597938144329897, "no_speech_prob": 9.33104456635192e-05}, {"id": 316, "seek": 128000, "start": 1295.0, "end": 1301.0, "text": " This library is not integrated with Elixir, but we will pull requests accepted.", "tokens": [639, 6405, 307, 406, 10919, 365, 2699, 970, 347, 11, 457, 321, 486, 2235, 12475, 9035, 13], "temperature": 0.0, "avg_logprob": -0.10770739578619236, "compression_ratio": 1.597938144329897, "no_speech_prob": 9.33104456635192e-05}, {"id": 317, "seek": 130100, "start": 1301.0, "end": 1311.0, "text": " The library, as I say, is called AMOC, an acronym for an order of crowds because you want to see your service dying.", "tokens": [440, 6405, 11, 382, 286, 584, 11, 307, 1219, 6475, 30087, 11, 364, 39195, 337, 364, 1668, 295, 26070, 570, 291, 528, 281, 536, 428, 2643, 8639, 13], "temperature": 0.0, "avg_logprob": -0.19613048357841295, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.00015845056623220444}, {"id": 318, "seek": 130100, "start": 1311.0, "end": 1313.0, "text": " There is the repo, you can look it up.", "tokens": [821, 307, 264, 49040, 11, 291, 393, 574, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.19613048357841295, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.00015845056623220444}, {"id": 319, "seek": 130100, "start": 1313.0, "end": 1323.0, "text": " We have this other repo that we call AMOC Arsenal where we have all the scenarios for XMPP where you can take inspiration on how they work.", "tokens": [492, 362, 341, 661, 49040, 300, 321, 818, 6475, 30087, 49156, 689, 321, 362, 439, 264, 15077, 337, 1783, 12224, 47, 689, 291, 393, 747, 10249, 322, 577, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.19613048357841295, "compression_ratio": 1.5206185567010309, "no_speech_prob": 0.00015845056623220444}, {"id": 320, "seek": 132300, "start": 1323.0, "end": 1332.0, "text": " And I'm about to finish here. I propose to myself that I would make this presentation without showing a single line of code.", "tokens": [400, 286, 478, 466, 281, 2413, 510, 13, 286, 17421, 281, 2059, 300, 286, 576, 652, 341, 5860, 1553, 4099, 257, 2167, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10500491808538567, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.640619489597157e-05}, {"id": 321, "seek": 132300, "start": 1332.0, "end": 1337.0, "text": " So I actually cutted the screenshot before the code starts.", "tokens": [407, 286, 767, 1723, 14727, 264, 27712, 949, 264, 3089, 3719, 13], "temperature": 0.0, "avg_logprob": -0.10500491808538567, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.640619489597157e-05}, {"id": 322, "seek": 132300, "start": 1337.0, "end": 1339.0, "text": " Let's see how it works.", "tokens": [961, 311, 536, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.10500491808538567, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.640619489597157e-05}, {"id": 323, "seek": 132300, "start": 1339.0, "end": 1344.0, "text": " In previous presentation I have shown a lot and it's a bit more complicated to explain.", "tokens": [682, 3894, 5860, 286, 362, 4898, 257, 688, 293, 309, 311, 257, 857, 544, 6179, 281, 2903, 13], "temperature": 0.0, "avg_logprob": -0.10500491808538567, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.640619489597157e-05}, {"id": 324, "seek": 134400, "start": 1344.0, "end": 1353.0, "text": " So the library has documentation. Another thing that I have pending is to use the new XDOP documentation.", "tokens": [407, 264, 6405, 575, 14333, 13, 3996, 551, 300, 286, 362, 32110, 307, 281, 764, 264, 777, 1783, 26649, 47, 14333, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 325, "seek": 134400, "start": 1353.0, "end": 1358.0, "text": " It doesn't have it yet, but it has a beautiful markdown that you can read in GitHub pages.", "tokens": [467, 1177, 380, 362, 309, 1939, 11, 457, 309, 575, 257, 2238, 1491, 5093, 300, 291, 393, 1401, 294, 23331, 7183, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 326, "seek": 134400, "start": 1358.0, "end": 1363.0, "text": " And the scenarios library for inspiration.", "tokens": [400, 264, 15077, 6405, 337, 10249, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 327, "seek": 134400, "start": 1363.0, "end": 1365.0, "text": " That is all I will have for you.", "tokens": [663, 307, 439, 286, 486, 362, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 328, "seek": 134400, "start": 1365.0, "end": 1366.0, "text": " This is my handle.", "tokens": [639, 307, 452, 4813, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 329, "seek": 134400, "start": 1366.0, "end": 1371.0, "text": " That is to repos links for MongoSIM and for AMOC.", "tokens": [663, 307, 281, 1085, 329, 6123, 337, 48380, 50, 6324, 293, 337, 6475, 30087, 13], "temperature": 0.0, "avg_logprob": -0.2234037277546335, "compression_ratio": 1.55, "no_speech_prob": 0.00017980992561206222}, {"id": 330, "seek": 137100, "start": 1371.0, "end": 1376.0, "text": " And this is a picture that I have everywhere if you see some Nelson videos and you don't know if it's me.", "tokens": [400, 341, 307, 257, 3036, 300, 286, 362, 5315, 498, 291, 536, 512, 23857, 2145, 293, 291, 500, 380, 458, 498, 309, 311, 385, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 331, "seek": 137100, "start": 1376.0, "end": 1379.0, "text": " It's going to be that one if it has that picture.", "tokens": [467, 311, 516, 281, 312, 300, 472, 498, 309, 575, 300, 3036, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 332, "seek": 137100, "start": 1379.0, "end": 1381.0, "text": " That's all from me. Thank you very much.", "tokens": [663, 311, 439, 490, 385, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 333, "seek": 137100, "start": 1381.0, "end": 1388.0, "text": " Thank you, Nelson.", "tokens": [1044, 291, 11, 23857, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 334, "seek": 137100, "start": 1388.0, "end": 1391.0, "text": " So is there any questions?", "tokens": [407, 307, 456, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 335, "seek": 137100, "start": 1391.0, "end": 1394.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 336, "seek": 137100, "start": 1394.0, "end": 1396.0, "text": " I know that there's also a library called Zang.", "tokens": [286, 458, 300, 456, 311, 611, 257, 6405, 1219, 1176, 656, 13], "temperature": 0.0, "avg_logprob": -0.19673192765977648, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.00041450693970546126}, {"id": 337, "seek": 139600, "start": 1396.0, "end": 1401.0, "text": " It's a low testing library written in early. So how is this one different?", "tokens": [467, 311, 257, 2295, 4997, 6405, 3720, 294, 2440, 13, 407, 577, 307, 341, 472, 819, 30], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 338, "seek": 139600, "start": 1401.0, "end": 1404.0, "text": " In that one you write the scenario in XML.", "tokens": [682, 300, 472, 291, 2464, 264, 9005, 294, 43484, 13], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 339, "seek": 139600, "start": 1404.0, "end": 1408.0, "text": " And it has a, how do you call it?", "tokens": [400, 309, 575, 257, 11, 577, 360, 291, 818, 309, 30], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 340, "seek": 139600, "start": 1408.0, "end": 1413.0, "text": " Like a domain specific language, XML base to describe what you want to do.", "tokens": [1743, 257, 9274, 2685, 2856, 11, 43484, 3096, 281, 6786, 437, 291, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 341, "seek": 139600, "start": 1413.0, "end": 1417.0, "text": " And the library has to offer you the protocol.", "tokens": [400, 264, 6405, 575, 281, 2626, 291, 264, 10336, 13], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 342, "seek": 139600, "start": 1417.0, "end": 1422.0, "text": " So that library actually has HTTP and XMPP helper functionality.", "tokens": [407, 300, 6405, 767, 575, 33283, 293, 1783, 12224, 47, 36133, 14980, 13], "temperature": 0.0, "avg_logprob": -0.16922210861038375, "compression_ratio": 1.5576036866359446, "no_speech_prob": 0.000849681266117841}, {"id": 343, "seek": 142200, "start": 1422.0, "end": 1426.0, "text": " But if you want a different protocol, the library doesn't give it.", "tokens": [583, 498, 291, 528, 257, 819, 10336, 11, 264, 6405, 1177, 380, 976, 309, 13], "temperature": 0.0, "avg_logprob": -0.14476688702901205, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.0001057228582794778}, {"id": 344, "seek": 142200, "start": 1426.0, "end": 1429.0, "text": " So we thought that we just want to write the airline code.", "tokens": [407, 321, 1194, 300, 321, 445, 528, 281, 2464, 264, 29528, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14476688702901205, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.0001057228582794778}, {"id": 345, "seek": 142200, "start": 1429.0, "end": 1434.0, "text": " It's way more pleasant to write and also less limited.", "tokens": [467, 311, 636, 544, 16232, 281, 2464, 293, 611, 1570, 5567, 13], "temperature": 0.0, "avg_logprob": -0.14476688702901205, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.0001057228582794778}, {"id": 346, "seek": 142200, "start": 1434.0, "end": 1438.0, "text": " Okay, thank you.", "tokens": [1033, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.14476688702901205, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.0001057228582794778}, {"id": 347, "seek": 142200, "start": 1438.0, "end": 1442.0, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.14476688702901205, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.0001057228582794778}, {"id": 348, "seek": 144200, "start": 1442.0, "end": 1454.0, "text": " So by using the murder of Kraus, did you already find any like bugs in Mongoose I am that you've been able to fix based on the...", "tokens": [407, 538, 1228, 264, 6568, 295, 26988, 301, 11, 630, 291, 1217, 915, 604, 411, 15120, 294, 48380, 541, 286, 669, 300, 291, 600, 668, 1075, 281, 3191, 2361, 322, 264, 485], "temperature": 0.0, "avg_logprob": -0.28227487363313375, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.000673231203109026}, {"id": 349, "seek": 144200, "start": 1454.0, "end": 1457.0, "text": " Every single time.", "tokens": [2048, 2167, 565, 13], "temperature": 0.0, "avg_logprob": -0.28227487363313375, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.000673231203109026}, {"id": 350, "seek": 144200, "start": 1457.0, "end": 1460.0, "text": " Fair.", "tokens": [12157, 13], "temperature": 0.0, "avg_logprob": -0.28227487363313375, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.000673231203109026}, {"id": 351, "seek": 144200, "start": 1460.0, "end": 1465.0, "text": " Useful bottlenecks sometimes are database interactions.", "tokens": [8278, 906, 44641, 2761, 2171, 366, 8149, 13280, 13], "temperature": 0.0, "avg_logprob": -0.28227487363313375, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.000673231203109026}, {"id": 352, "seek": 144200, "start": 1465.0, "end": 1468.0, "text": " And all fours that didn't used to matter in the computer you could punch.", "tokens": [400, 439, 1451, 82, 300, 994, 380, 1143, 281, 1871, 294, 264, 3820, 291, 727, 8135, 13], "temperature": 0.0, "avg_logprob": -0.28227487363313375, "compression_ratio": 1.4271356783919598, "no_speech_prob": 0.000673231203109026}, {"id": 353, "seek": 146800, "start": 1468.0, "end": 1474.0, "text": " But now, so as you write messages, you need to make sure that they are recoverable.", "tokens": [583, 586, 11, 370, 382, 291, 2464, 7897, 11, 291, 643, 281, 652, 988, 300, 436, 366, 8114, 712, 13], "temperature": 0.0, "avg_logprob": -0.1023903206775063, "compression_ratio": 1.6063829787234043, "no_speech_prob": 9.314360067946836e-05}, {"id": 354, "seek": 146800, "start": 1474.0, "end": 1485.0, "text": " But the amount of messages that you can send might not be as scalable as the amount of inserts a database can have.", "tokens": [583, 264, 2372, 295, 7897, 300, 291, 393, 2845, 1062, 406, 312, 382, 38481, 382, 264, 2372, 295, 49163, 257, 8149, 393, 362, 13], "temperature": 0.0, "avg_logprob": -0.1023903206775063, "compression_ratio": 1.6063829787234043, "no_speech_prob": 9.314360067946836e-05}, {"id": 355, "seek": 146800, "start": 1485.0, "end": 1488.0, "text": " So this is something that we test a lot.", "tokens": [407, 341, 307, 746, 300, 321, 1500, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1023903206775063, "compression_ratio": 1.6063829787234043, "no_speech_prob": 9.314360067946836e-05}, {"id": 356, "seek": 146800, "start": 1488.0, "end": 1494.0, "text": " And another functionality that we do is the time to delivery.", "tokens": [400, 1071, 14980, 300, 321, 360, 307, 264, 565, 281, 8982, 13], "temperature": 0.0, "avg_logprob": -0.1023903206775063, "compression_ratio": 1.6063829787234043, "no_speech_prob": 9.314360067946836e-05}, {"id": 357, "seek": 149400, "start": 1494.0, "end": 1498.0, "text": " So the sender puts a timestamp and the receiver just measures the difference.", "tokens": [407, 264, 2845, 260, 8137, 257, 49108, 1215, 293, 264, 20086, 445, 8000, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.08801719929912302, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.00013844094064552337}, {"id": 358, "seek": 149400, "start": 1498.0, "end": 1508.0, "text": " And that's something that we also test continuously when we change something to see that we didn't introduce a computation that would make the time to delivery longer.", "tokens": [400, 300, 311, 746, 300, 321, 611, 1500, 15684, 562, 321, 1319, 746, 281, 536, 300, 321, 994, 380, 5366, 257, 24903, 300, 576, 652, 264, 565, 281, 8982, 2854, 13], "temperature": 0.0, "avg_logprob": -0.08801719929912302, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.00013844094064552337}, {"id": 359, "seek": 149400, "start": 1508.0, "end": 1514.0, "text": " So those are the two most common tests that we test almost all the time and then there are each case that we don't test as regularly.", "tokens": [407, 729, 366, 264, 732, 881, 2689, 6921, 300, 321, 1500, 1920, 439, 264, 565, 293, 550, 456, 366, 1184, 1389, 300, 321, 500, 380, 1500, 382, 11672, 13], "temperature": 0.0, "avg_logprob": -0.08801719929912302, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.00013844094064552337}, {"id": 360, "seek": 149400, "start": 1514.0, "end": 1520.0, "text": " But we have all the scenarios for them.", "tokens": [583, 321, 362, 439, 264, 15077, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.08801719929912302, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.00013844094064552337}, {"id": 361, "seek": 149400, "start": 1520.0, "end": 1523.0, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.08801719929912302, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.00013844094064552337}, {"id": 362, "seek": 152300, "start": 1523.0, "end": 1529.0, "text": " I wanted to mention another library I saw that is called MZBench, I think.", "tokens": [286, 1415, 281, 2152, 1071, 6405, 286, 1866, 300, 307, 1219, 376, 57, 21736, 339, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.19691064086141466, "compression_ratio": 1.5, "no_speech_prob": 0.002915195655077696}, {"id": 363, "seek": 152300, "start": 1529.0, "end": 1531.0, "text": " I don't know that one.", "tokens": [286, 500, 380, 458, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.19691064086141466, "compression_ratio": 1.5, "no_speech_prob": 0.002915195655077696}, {"id": 364, "seek": 152300, "start": 1531.0, "end": 1538.0, "text": " Yeah, I think it was... I know it because it was used by VernMQ to do its load testing, I think.", "tokens": [865, 11, 286, 519, 309, 390, 485, 286, 458, 309, 570, 309, 390, 1143, 538, 33220, 44, 48, 281, 360, 1080, 3677, 4997, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.19691064086141466, "compression_ratio": 1.5, "no_speech_prob": 0.002915195655077696}, {"id": 365, "seek": 152300, "start": 1538.0, "end": 1542.0, "text": " And I think it's in Erlang, too, and you write scenarios.", "tokens": [400, 286, 519, 309, 311, 294, 3300, 25241, 11, 886, 11, 293, 291, 2464, 15077, 13], "temperature": 0.0, "avg_logprob": -0.19691064086141466, "compression_ratio": 1.5, "no_speech_prob": 0.002915195655077696}, {"id": 366, "seek": 154200, "start": 1542.0, "end": 1555.0, "text": " But is Emoq able to... If I have an actor that has to perform some action and then pass the state to another actor, is that possible?", "tokens": [583, 307, 462, 3280, 80, 1075, 281, 485, 759, 286, 362, 364, 8747, 300, 575, 281, 2042, 512, 3069, 293, 550, 1320, 264, 1785, 281, 1071, 8747, 11, 307, 300, 1944, 30], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 367, "seek": 154200, "start": 1555.0, "end": 1559.0, "text": " Or is that... You have to write your own code, basically, to do that.", "tokens": [1610, 307, 300, 485, 509, 362, 281, 2464, 428, 1065, 3089, 11, 1936, 11, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 368, "seek": 154200, "start": 1559.0, "end": 1561.0, "text": " I have. The coordinator would help.", "tokens": [286, 362, 13, 440, 27394, 576, 854, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 369, "seek": 154200, "start": 1561.0, "end": 1562.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 370, "seek": 154200, "start": 1562.0, "end": 1566.0, "text": " So in the coordinator you can say to pick up pairs of actors and then they say...", "tokens": [407, 294, 264, 27394, 291, 393, 584, 281, 1888, 493, 15494, 295, 10037, 293, 550, 436, 584, 485], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 371, "seek": 154200, "start": 1566.0, "end": 1567.0, "text": " Okay, you had the first one.", "tokens": [1033, 11, 291, 632, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 372, "seek": 154200, "start": 1567.0, "end": 1568.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 373, "seek": 154200, "start": 1568.0, "end": 1569.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 374, "seek": 154200, "start": 1569.0, "end": 1571.0, "text": " Any other question?", "tokens": [2639, 661, 1168, 30], "temperature": 0.0, "avg_logprob": -0.20108512409946375, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.001225096988491714}, {"id": 375, "seek": 157100, "start": 1571.0, "end": 1578.0, "text": " We have something similar for changing the owner of a room and then actors have to pass the state, the knowledge to another one.", "tokens": [492, 362, 746, 2531, 337, 4473, 264, 7289, 295, 257, 1808, 293, 550, 10037, 362, 281, 1320, 264, 1785, 11, 264, 3601, 281, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.20689870834350585, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0016124772373586893}, {"id": 376, "seek": 157100, "start": 1578.0, "end": 1582.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20689870834350585, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0016124772373586893}, {"id": 377, "seek": 157100, "start": 1582.0, "end": 1584.0, "text": " Okay, thank you again.", "tokens": [1033, 11, 1309, 291, 797, 13], "temperature": 0.0, "avg_logprob": -0.20689870834350585, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0016124772373586893}, {"id": 378, "seek": 158400, "start": 1584.0, "end": 1602.0, "text": " We'll see if there are any other questions.", "tokens": [50364, 492, 603, 536, 498, 456, 366, 604, 661, 1651, 13, 51264], "temperature": 0.0, "avg_logprob": -0.5651030540466309, "compression_ratio": 0.9347826086956522, "no_speech_prob": 0.0005628644721582532}], "language": "en"}