{"text": " Okay, so we're switching topics a bit away from MPI to something at least a little bit different, link time call graph analysis. All right, thank you. So, yeah, we're going to talk about user-guided program instrumentation approach and especially a link time call graph analysis extension to that project, which kind of makes it more usable. So, to give some background on this work, I'm involved in a project called XRFoam that deals with the Open Foam Computational Fluid Dynamics Toolbox. And that's a very complex code, quite large. And the goal here is to improve the performance of Open Foam for HPC systems, especially for the exascale error. And one of the things we do is, yeah, develop empirical performance models. And for that, we have sort of developed like workflow, how we do that. We start with several measurements where we get an initial overview and identify hotspots. Then, based on these hotspots, do an analysis of the critical kernels. And finally, we can do the empirical modeling in order to find scalability bugs and predict performance when scaling out. And so, especially for the focus measurements and the modeling, you need quite accurate and reliable methods to be sure that the data you collect is right and you have the right level of detail. And what we use for that is code instrumentation. And just to give the background, I want to give an example. For example, in GCC and Clang, you have the F instrument functions flag, which does a very basic instrumentation. But if you activate that flag, you insert these, or the compiler inserts these function enter and exit probes, which are then a runtime interface with a profiling tool which records runtime. Or, yeah, more involved metrics, maybe performance counters, stuff like that. And the big problem with this instrumentation approach, especially compared to other mechanisms like sampling, is that it can increase the run times by orders of magnitude if you're not careful, if you just instrument every function. So, you have to have some kind of selection mechanism in place to prevent that. And sort of the method that's most commonly used is to use profile-based filtering, either manual, where you look at the profile that is generated by the instrumented measurement, and then determine, like, which functions, maybe, for example, very small or called very often and don't do much work, so that can be filtered out. And there are also tools like Scope, which can help in generating these filters. However, this is only, like, on a purely profile base and there's no, yeah, no calling contacts involved in the decision. So, there are some other call graph-based approaches where you have a static call graph that is then used to make the selection. I want to highlight, too, there's Pira, which does automatic iterative refinement. So, we start with a static selection based on the statically collected call graph, and then iteratively compile a program with instrumentation, execute it, measure it, and then look at the profile in order to determine what to filter out. And then the tool we're working on is the copy tool, which is short for compile-assisted performance instrumentation, and that one is more focused on allowing the user to specify what he wants to measure, so he can say, okay, I'm interested in MPI communications, so that is what I want to measure. So, here's a very high-level overview of how copy works. So, you have your source code, and that is put into the static analysis in order to generate the call graph. And then at the same time, the user specifies the measurement objective in front of a selection spec. We have a simple DSL for that. And this together is then fed into the copy tool in order to generate the instrumentation configuration, which is hopefully low overhead. And, yeah, so let's consider an example for that. So, we might have the following scenario. So, I want to record all call paths that contain MPI communication. Additionally, I want to measure functions that contain loops with at least 10 floating point operations, and I don't care about system headers or inline functions. And this is how this looks as a copy spec. I won't get into the details of the syntax, but you can combine different selection modules where you start with the set of all functions and then sort of here select inline functions or system header functions. And then you can combine them to, in the end, produce one set that you want to instrument. And for this particular example, this reduces the number of instrument functions by 74 percent for our form test case. So, the call graph is sort of the base data structure that we use for all the analysis and for the selection. And this is currently generated based on the source level by a tool called MetaCG. And this can be a bit cumbersome for very complex applications, such as Open Foom, because you, yeah, you need sort of a separate source code analysis step, and that can be difficult to set up, especially if there's like shared library dependencies stuff like that. So, it can be tricky. And so, we looked into how we can maybe generate the call graph at different stages. And so, this is what Tim is going to talk about, how we can generate the call graph at different program stages. And then we introduced the caged compiler plugin, where we, yeah, evaluate how this can be done at link time. So, well, thank you. So, we were interested in the whole program call graphs. This is because CAPI, the tool that does our instrumentation, needs to be aware of every function inside our call graph, which means that we are, that it's necessary to have a whole program view. And the tools that were used like MetaCG were able to create a call graph, and they have the distinct advantage of being able to annotate metadata to this call graph. This is where the name MetaCG came from. And this means that we not only can answer structural questions like we want to instrument every function that eventually call paths down to some MPI call, but we can also answer instrument based on metadata. For example, the loop depth specification or floating point operations. And yes, there were multiple possible ways to generate a call graph. One of them is source code. One of them is the intermediate representation that is basically part of every compiler, especially the LLVM compile pipeline, and machine code at the very last. So, the basic idea is, well, we have the source code. Let's do call graph generation on the source code, which is relatively easy. MetaCG is doing it on a source code basis. But this means as you feed every single source code file into MetaCG, this means that your view of the program is limited to one translation unit at a time. So, you then need to merge those part call graphs of every translation unit together to generate this overview of the whole program that you need. The information you then gather from your profiling maps very cleanly back to your source code, because once you find a function, well, it's named the same way in your source code. But on the other hand, what you write in your source code is not necessarily what's actually executed on the machine, right? Because there might be code transformation, constant propagation, dead code elimination, inlining. And this is actually something we want to be aware of if we are doing instrumentation, not that we want to instrument a function that doesn't exist anymore. Also, this merging of the translation units means that the user is involved. The user currently has to specify when he uses MetaCG which source code translation units belong to one target. And then manually has to tell MetaCG these functions are all to be merged. Please generate the call graph for that. And the user might not perfectly emulate the linker behavior. So, there are different resolution types that a linker might choose if there are samely named structs or classes. And, depending on how you are implementing your merging process, you might have slight differences between your call graph that you generated and that's what the linker will eventually do. So, the other extreme would be, well, let's do it on the compiled machine code then. Reverse engineering tools like radar or Ghidra are able to generate call graphs and binary data just fine. And those have the very distinct advantages that this is actually what is run on the machine. There are no code transformation left. You have the advantage of being able to see machine code optimization passes if they are influencing the generated call graph. But, on the other hand, a lot of information, the metadata that we also would like to be able to instrument based upon are lost as soon as we go down to machine code. Inlining already happened, so there is no function annotated with please inline anymore. Also, pointer type information, as we heard in the talk earlier, gets lost as soon as we go down to machine type. And constness is also something that is more to be inferred than actually stated once we go down to machine code. And so, we decided the best of both worlds is probably the LLVMIR because it's a heavily annotated representation. It is close enough to what will run on the machine that we have the ability to observe the code transformation. We are able to give more specific estimates on what the actual cost of a function might be because we have more clear way of tracking, for example, instruction counts, floating ops, and integer ops. On the other hand, it's also close enough to what the user actually wrote because we're not down on the machine code yet. And we can figure out the inlining stuff, the constness, the virtual functions. We can get type information in the IR. And if we do it at link time, we are not even limited to the translation unit by translation unit scope that source code based approaches are. So, if you have your pretty default compile pipeline, you have your source code, which builds a translation unit, gets fed into the compiler, which outputs intermediate representation, and then the optimizer runs there and multiple source code translation unit optimized IR modules are fed into the linker. And we can do our call graph analysis inside the linker, solve our translation unit problem, and are able to have all our information ready. So, to do this, we developed the cage plugin. Cage stands for call graph embedding LLVM plugin. And it basically generates a call graph using some of LLVM tools, does some annotation, virtual function call analysis, and this can run as part of the optimizer in the pipeline, but it can also run as part of the LLVM linker. Also as a plugin, for which we use a slightly modified version of the LLVM linker, but the basic logic of running plugins in the LLVM linker was there. So, then we do a V table analysis, our metadata annotation, because it's all available to us. And then we embed the result into the binary, which enables dynamic augmentation. And I will come to that one later. So, at link time, we're doing static analysis basically. And as I already mentioned, we split our information in basically two types. One of them is structural information like call hierarchies, call paths, call depth, the number of children, how deep we are in our path. And you also have virtual function calls, which are mostly structural, because once you have virtual and polymorphic calls, you have like a set of functions that are probably being called by that pointer. And so, we can narrow down the possibilities of which functions are called, but we cannot actually statically figure out this function is getting called. So, it's slightly metadata based, but it's also mostly structural. On the metadata information side, we have instruction composition, so we can determine what is the relation between arithmetic operations and memory operations, for example, or we can generate local and global loop depth estimates. And then we have inlining information, which is metadata, because inlining is not like a must do for a compiler, just because you have specified inlining for a function doesn't mean that the compiler will actually inline the function. So, it's partly structural information and partly metadata, so you see there's no clear like line between those, they blur at some points, but we can represent all those in the metadata annotated call graph. And if you remember, we were able to do dynamic augmentation. Well, what is dynamic augmentation? If you remember, each object contains the call graph that we generated, which means that the call graphs can be aggregated at runtime if a shared library is loaded, because even if you are at link time, even if you can see all the statically linked objects, all the translation units that belong to your target, your binary might load a shared library, which then, well, you're unaware of. So, the idea is, as soon as the main executable is loaded, it passes its embedded call graph on startup to a runtime collecting library. And then, the main executable can load whatever shared object it wants. And if this shared object also contains a call graph, then this runtime collector gets passed this call graph on the first load of the shared object and can aggregate it like merging. So, we're basically back to the translation unit by translation unit based approach, but now we're doing shared library on binary and executable based merging. And then, we attach all this data together to one really big whole program call graph, now including shared objects. And then, we can export this, for example, and pass it back to Karpi for some further refinement of the instrumentation. Go ahead. All right. Thanks. So, to put it all together, for Karpi, we have the call graph analysis approach, Tim just explained. So, for each object file, we have the call graph analysis and then the embedding. And then, on the runtime side, we can sort of merge the main executable call graph with the shared libraries as they are loaded. And we defer the instrumentation using a dynamic instrumentation approach in order to sort of apply that selection dynamically. And, yeah, that's how it works. So, to summarize, we are developing the Karpi tool for instrumentation selection based on call graph analysis. And we have explored this cage plugin for generating this call graph information at link time, which allows whole program visibility and this dynamic documentation. And together with Karpi, we can, yeah, use the embedded call graph to run the selection at runtime and thereby improving Karpi to make the compilation process and the analysis process more streamlined. And this is sort of an active development. So, at this point, we don't have a very detailed evaluation about performance and stuff like that. So, there's some concerns, for example, when you go to very big programs and do LTO, there might be performance problems. So, there might be more work to make it viable in that regard. But, yeah, it works well in a prototype fashion. Yeah. Thank you. Any questions? Perhaps I was a bit distracted. So, I have two questions. If you can comment on Lambda functions, OMP sections, or OMP sections of the code, not specific constructs, and instruction cache. If you can comment how those are handled, those three aspects, let's say. So when it comes to OpenMP, we don't at the moment have any specific OpenMP profiling interface that we target. So, this might be something that is probably useful in the future. But for now, we just do the basic functions mutation and select on that. Yeah. Then the other point was, sorry, could you repeat? So, do you want to comment on that? So, lambdas and caches, so caching, no, there is no logic to handle caching in any way. And regarding lambdas and OpenMP, it's, if you're talking about profiling, you've got your answer right there. But if you're talking about generating call graphs in which lambdas and OpenMP runtime calls are available, then the call graph will actually figure out that there are OpenMP runtime calls, and will correctly, if I remember correctly, will figure out that this function calls back to the OpenMP runtime, because once you are in IR, the runtimes actually carved out, all the pragmas were removed from the actual source code. So, we are aware of OpenMP, but we are not using that information currently for any profiling. But you could do metadata-based copy selection with it. So, every call path that eventually leads to an OpenMP runtime call would be a valid instrumentation using copy. Just that the question of the instruction cache is whether, this is my ignorance, but if you are reintroducing a lot of new instructions here in the code, or in the code that is being read, I was thinking whether too much data ends up, maybe I didn't understand well. So, this is more of a performance-related question, right? Okay, so, yes, of course, you introduce a new instruction whenever a shared library is loaded, because we then add instructions that pass the call graph back to our runtime collecting facility, and we also introduce instructions because we are using a profiling approach, which are function calls. So, yes, we are impeding the instruction fetching, instruction caching flow, which is also why profiling has rather high overhead compared to sampling approaches, for example. But as Sebastian told, we have not really extensively profiled our application, quite ironic, so we are not aware how much the benefit or the impact actually would be. So in your slide, you say you have a fork of LLD. The obvious question is, what's stopping you from upstreaming this? So, yes, we have a fork of LLD that just basically exposes the flag load new pass manager plugin, and you pass the plugin, and it does the rest. What's currently holding us back from upstreaming is it's not very well developed, it was coupled together in half a week or something, and there already is an open merge request on fabricator that implements this exact functionality for, if I remember correctly, LLDM9, which was abandoned for a year until it was totally abandoned and closed, and so we didn't actually figure out how to make this more interesting. Don't take that as a signal. People just move jobs or whatever, so try it again and bash people, and I can help you with that as well, find the right people to get this, because it seems like a simple and obvious thing to have. It isn't actually that hard. Apparently, there wasn't much interest in the community back in 2020? Yes, that's too long ago. We will polish it a little much, and then hopefully get this one upstream. Thanks. Any other questions? No. Okay. Thank you very much. Thank you very much, Sebastian.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Okay, so we're switching topics a bit away from MPI to something at least a little bit", "tokens": [1033, 11, 370, 321, 434, 16493, 8378, 257, 857, 1314, 490, 14146, 40, 281, 746, 412, 1935, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.30973399312872635, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.11353844404220581}, {"id": 1, "seek": 0, "start": 12.26, "end": 15.46, "text": " different, link time call graph analysis.", "tokens": [819, 11, 2113, 565, 818, 4295, 5215, 13], "temperature": 0.0, "avg_logprob": -0.30973399312872635, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.11353844404220581}, {"id": 2, "seek": 0, "start": 15.46, "end": 22.46, "text": " All right, thank you. So, yeah, we're going to talk about user-guided program instrumentation", "tokens": [1057, 558, 11, 1309, 291, 13, 407, 11, 1338, 11, 321, 434, 516, 281, 751, 466, 4195, 12, 2794, 2112, 1461, 7198, 399], "temperature": 0.0, "avg_logprob": -0.30973399312872635, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.11353844404220581}, {"id": 3, "seek": 0, "start": 23.38, "end": 29.88, "text": " approach and especially a link time call graph analysis extension to that project, which", "tokens": [3109, 293, 2318, 257, 2113, 565, 818, 4295, 5215, 10320, 281, 300, 1716, 11, 597], "temperature": 0.0, "avg_logprob": -0.30973399312872635, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.11353844404220581}, {"id": 4, "seek": 2988, "start": 29.88, "end": 36.879999999999995, "text": " kind of makes it more usable. So, to give some background on this work, I'm involved in", "tokens": [733, 295, 1669, 309, 544, 29975, 13, 407, 11, 281, 976, 512, 3678, 322, 341, 589, 11, 286, 478, 3288, 294], "temperature": 0.0, "avg_logprob": -0.23127640399736227, "compression_ratio": 1.477366255144033, "no_speech_prob": 9.73831774899736e-05}, {"id": 5, "seek": 2988, "start": 36.96, "end": 42.92, "text": " a project called XRFoam that deals with the Open Foam Computational Fluid Dynamics Toolbox.", "tokens": [257, 1716, 1219, 1783, 49, 37, 78, 335, 300, 11215, 365, 264, 7238, 8564, 335, 37804, 1478, 33612, 327, 22947, 1167, 15934, 4995, 13], "temperature": 0.0, "avg_logprob": -0.23127640399736227, "compression_ratio": 1.477366255144033, "no_speech_prob": 9.73831774899736e-05}, {"id": 6, "seek": 2988, "start": 42.92, "end": 49.0, "text": " And that's a very complex code, quite large. And the goal here is to improve the performance", "tokens": [400, 300, 311, 257, 588, 3997, 3089, 11, 1596, 2416, 13, 400, 264, 3387, 510, 307, 281, 3470, 264, 3389], "temperature": 0.0, "avg_logprob": -0.23127640399736227, "compression_ratio": 1.477366255144033, "no_speech_prob": 9.73831774899736e-05}, {"id": 7, "seek": 2988, "start": 49.0, "end": 55.68, "text": " of Open Foam for HPC systems, especially for the exascale error. And one of the things", "tokens": [295, 7238, 8564, 335, 337, 12557, 34, 3652, 11, 2318, 337, 264, 454, 4806, 1220, 6713, 13, 400, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.23127640399736227, "compression_ratio": 1.477366255144033, "no_speech_prob": 9.73831774899736e-05}, {"id": 8, "seek": 5568, "start": 55.68, "end": 62.6, "text": " we do is, yeah, develop empirical performance models. And for that, we have sort of developed", "tokens": [321, 360, 307, 11, 1338, 11, 1499, 31886, 3389, 5245, 13, 400, 337, 300, 11, 321, 362, 1333, 295, 4743], "temperature": 0.0, "avg_logprob": -0.15297368939003247, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0001260224380530417}, {"id": 9, "seek": 5568, "start": 62.6, "end": 68.24, "text": " like workflow, how we do that. We start with several measurements where we get an initial", "tokens": [411, 20993, 11, 577, 321, 360, 300, 13, 492, 722, 365, 2940, 15383, 689, 321, 483, 364, 5883], "temperature": 0.0, "avg_logprob": -0.15297368939003247, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0001260224380530417}, {"id": 10, "seek": 5568, "start": 68.24, "end": 74.76, "text": " overview and identify hotspots. Then, based on these hotspots, do an analysis of the critical", "tokens": [12492, 293, 5876, 36121, 79, 1971, 13, 1396, 11, 2361, 322, 613, 36121, 79, 1971, 11, 360, 364, 5215, 295, 264, 4924], "temperature": 0.0, "avg_logprob": -0.15297368939003247, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0001260224380530417}, {"id": 11, "seek": 5568, "start": 74.76, "end": 81.76, "text": " kernels. And finally, we can do the empirical modeling in order to find scalability bugs", "tokens": [23434, 1625, 13, 400, 2721, 11, 321, 393, 360, 264, 31886, 15983, 294, 1668, 281, 915, 15664, 2310, 15120], "temperature": 0.0, "avg_logprob": -0.15297368939003247, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0001260224380530417}, {"id": 12, "seek": 8176, "start": 81.76, "end": 88.76, "text": " and predict performance when scaling out. And so, especially for the focus measurements", "tokens": [293, 6069, 3389, 562, 21589, 484, 13, 400, 370, 11, 2318, 337, 264, 1879, 15383], "temperature": 0.0, "avg_logprob": -0.15614826679229737, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.000117107956612017}, {"id": 13, "seek": 8176, "start": 90.24000000000001, "end": 95.28, "text": " and the modeling, you need quite accurate and reliable methods to be sure that the data", "tokens": [293, 264, 15983, 11, 291, 643, 1596, 8559, 293, 12924, 7150, 281, 312, 988, 300, 264, 1412], "temperature": 0.0, "avg_logprob": -0.15614826679229737, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.000117107956612017}, {"id": 14, "seek": 8176, "start": 95.28, "end": 100.88000000000001, "text": " you collect is right and you have the right level of detail. And what we use for that", "tokens": [291, 2500, 307, 558, 293, 291, 362, 264, 558, 1496, 295, 2607, 13, 400, 437, 321, 764, 337, 300], "temperature": 0.0, "avg_logprob": -0.15614826679229737, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.000117107956612017}, {"id": 15, "seek": 8176, "start": 100.88000000000001, "end": 107.88000000000001, "text": " is code instrumentation. And just to give the background, I want to give an example.", "tokens": [307, 3089, 7198, 399, 13, 400, 445, 281, 976, 264, 3678, 11, 286, 528, 281, 976, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15614826679229737, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.000117107956612017}, {"id": 16, "seek": 10788, "start": 107.88, "end": 114.11999999999999, "text": " For example, in GCC and Clang, you have the F instrument functions flag, which does a", "tokens": [1171, 1365, 11, 294, 460, 11717, 293, 2033, 656, 11, 291, 362, 264, 479, 7198, 6828, 7166, 11, 597, 775, 257], "temperature": 0.0, "avg_logprob": -0.27935386839367093, "compression_ratio": 1.6359447004608294, "no_speech_prob": 2.793062958517112e-05}, {"id": 17, "seek": 10788, "start": 114.11999999999999, "end": 121.11999999999999, "text": " very basic instrumentation. But if you activate that flag, you insert these, or the compiler", "tokens": [588, 3875, 7198, 399, 13, 583, 498, 291, 13615, 300, 7166, 11, 291, 8969, 613, 11, 420, 264, 31958], "temperature": 0.0, "avg_logprob": -0.27935386839367093, "compression_ratio": 1.6359447004608294, "no_speech_prob": 2.793062958517112e-05}, {"id": 18, "seek": 10788, "start": 123.0, "end": 130.0, "text": " inserts these function enter and exit probes, which are then a runtime interface with a", "tokens": [49163, 613, 2445, 3242, 293, 11043, 1239, 279, 11, 597, 366, 550, 257, 34474, 9226, 365, 257], "temperature": 0.0, "avg_logprob": -0.27935386839367093, "compression_ratio": 1.6359447004608294, "no_speech_prob": 2.793062958517112e-05}, {"id": 19, "seek": 10788, "start": 130.16, "end": 136.88, "text": " profiling tool which records runtime. Or, yeah, more involved metrics, maybe performance", "tokens": [1740, 4883, 2290, 597, 7724, 34474, 13, 1610, 11, 1338, 11, 544, 3288, 16367, 11, 1310, 3389], "temperature": 0.0, "avg_logprob": -0.27935386839367093, "compression_ratio": 1.6359447004608294, "no_speech_prob": 2.793062958517112e-05}, {"id": 20, "seek": 13688, "start": 136.88, "end": 143.12, "text": " counters, stuff like that. And the big problem with this instrumentation approach, especially", "tokens": [39338, 11, 1507, 411, 300, 13, 400, 264, 955, 1154, 365, 341, 7198, 399, 3109, 11, 2318], "temperature": 0.0, "avg_logprob": -0.17635406682520738, "compression_ratio": 1.6061946902654867, "no_speech_prob": 4.736371920444071e-05}, {"id": 21, "seek": 13688, "start": 143.12, "end": 150.12, "text": " compared to other mechanisms like sampling, is that it can increase the run times by orders", "tokens": [5347, 281, 661, 15902, 411, 21179, 11, 307, 300, 309, 393, 3488, 264, 1190, 1413, 538, 9470], "temperature": 0.0, "avg_logprob": -0.17635406682520738, "compression_ratio": 1.6061946902654867, "no_speech_prob": 4.736371920444071e-05}, {"id": 22, "seek": 13688, "start": 150.12, "end": 155.28, "text": " of magnitude if you're not careful, if you just instrument every function. So, you have", "tokens": [295, 15668, 498, 291, 434, 406, 5026, 11, 498, 291, 445, 7198, 633, 2445, 13, 407, 11, 291, 362], "temperature": 0.0, "avg_logprob": -0.17635406682520738, "compression_ratio": 1.6061946902654867, "no_speech_prob": 4.736371920444071e-05}, {"id": 23, "seek": 13688, "start": 155.28, "end": 162.28, "text": " to have some kind of selection mechanism in place to prevent that. And sort of the method", "tokens": [281, 362, 512, 733, 295, 9450, 7513, 294, 1081, 281, 4871, 300, 13, 400, 1333, 295, 264, 3170], "temperature": 0.0, "avg_logprob": -0.17635406682520738, "compression_ratio": 1.6061946902654867, "no_speech_prob": 4.736371920444071e-05}, {"id": 24, "seek": 16228, "start": 162.28, "end": 169.28, "text": " that's most commonly used is to use profile-based filtering, either manual, where you look at", "tokens": [300, 311, 881, 12719, 1143, 307, 281, 764, 7964, 12, 6032, 30822, 11, 2139, 9688, 11, 689, 291, 574, 412], "temperature": 0.0, "avg_logprob": -0.2460248711403836, "compression_ratio": 1.6299559471365639, "no_speech_prob": 5.284076542011462e-05}, {"id": 25, "seek": 16228, "start": 170.0, "end": 176.2, "text": " the profile that is generated by the instrumented measurement, and then determine, like, which", "tokens": [264, 7964, 300, 307, 10833, 538, 264, 7198, 292, 13160, 11, 293, 550, 6997, 11, 411, 11, 597], "temperature": 0.0, "avg_logprob": -0.2460248711403836, "compression_ratio": 1.6299559471365639, "no_speech_prob": 5.284076542011462e-05}, {"id": 26, "seek": 16228, "start": 176.2, "end": 182.28, "text": " functions, maybe, for example, very small or called very often and don't do much work,", "tokens": [6828, 11, 1310, 11, 337, 1365, 11, 588, 1359, 420, 1219, 588, 2049, 293, 500, 380, 360, 709, 589, 11], "temperature": 0.0, "avg_logprob": -0.2460248711403836, "compression_ratio": 1.6299559471365639, "no_speech_prob": 5.284076542011462e-05}, {"id": 27, "seek": 16228, "start": 182.28, "end": 188.96, "text": " so that can be filtered out. And there are also tools like Scope, which can help in generating", "tokens": [370, 300, 393, 312, 37111, 484, 13, 400, 456, 366, 611, 3873, 411, 2747, 1114, 11, 597, 393, 854, 294, 17746], "temperature": 0.0, "avg_logprob": -0.2460248711403836, "compression_ratio": 1.6299559471365639, "no_speech_prob": 5.284076542011462e-05}, {"id": 28, "seek": 18896, "start": 188.96, "end": 195.96, "text": " these filters. However, this is only, like, on a purely profile base and there's no, yeah,", "tokens": [613, 15995, 13, 2908, 11, 341, 307, 787, 11, 411, 11, 322, 257, 17491, 7964, 3096, 293, 456, 311, 572, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.23309129657167377, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.830790466163307e-05}, {"id": 29, "seek": 18896, "start": 203.24, "end": 208.44, "text": " no calling contacts involved in the decision. So, there are some other call graph-based", "tokens": [572, 5141, 15836, 3288, 294, 264, 3537, 13, 407, 11, 456, 366, 512, 661, 818, 4295, 12, 6032], "temperature": 0.0, "avg_logprob": -0.23309129657167377, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.830790466163307e-05}, {"id": 30, "seek": 18896, "start": 208.44, "end": 214.44, "text": " approaches where you have a static call graph that is then used to make the selection.", "tokens": [11587, 689, 291, 362, 257, 13437, 818, 4295, 300, 307, 550, 1143, 281, 652, 264, 9450, 13], "temperature": 0.0, "avg_logprob": -0.23309129657167377, "compression_ratio": 1.5142857142857142, "no_speech_prob": 8.830790466163307e-05}, {"id": 31, "seek": 21444, "start": 214.44, "end": 221.44, "text": " I want to highlight, too, there's Pira, which does automatic iterative refinement. So, we", "tokens": [286, 528, 281, 5078, 11, 886, 11, 456, 311, 430, 4271, 11, 597, 775, 12509, 17138, 1166, 1895, 30229, 13, 407, 11, 321], "temperature": 0.0, "avg_logprob": -0.24344118251356966, "compression_ratio": 1.6045454545454545, "no_speech_prob": 5.804976899526082e-05}, {"id": 32, "seek": 21444, "start": 222.64, "end": 229.64, "text": " start with a static selection based on the statically collected call graph, and then", "tokens": [722, 365, 257, 13437, 9450, 2361, 322, 264, 2219, 984, 11087, 818, 4295, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.24344118251356966, "compression_ratio": 1.6045454545454545, "no_speech_prob": 5.804976899526082e-05}, {"id": 33, "seek": 21444, "start": 230.04, "end": 236.44, "text": " iteratively compile a program with instrumentation, execute it, measure it, and then look at", "tokens": [17138, 19020, 31413, 257, 1461, 365, 7198, 399, 11, 14483, 309, 11, 3481, 309, 11, 293, 550, 574, 412], "temperature": 0.0, "avg_logprob": -0.24344118251356966, "compression_ratio": 1.6045454545454545, "no_speech_prob": 5.804976899526082e-05}, {"id": 34, "seek": 21444, "start": 236.44, "end": 243.2, "text": " the profile in order to determine what to filter out. And then the tool we're working", "tokens": [264, 7964, 294, 1668, 281, 6997, 437, 281, 6608, 484, 13, 400, 550, 264, 2290, 321, 434, 1364], "temperature": 0.0, "avg_logprob": -0.24344118251356966, "compression_ratio": 1.6045454545454545, "no_speech_prob": 5.804976899526082e-05}, {"id": 35, "seek": 24320, "start": 243.2, "end": 249.56, "text": " on is the copy tool, which is short for compile-assisted performance instrumentation, and that one", "tokens": [322, 307, 264, 5055, 2290, 11, 597, 307, 2099, 337, 31413, 12, 640, 33250, 3389, 7198, 399, 11, 293, 300, 472], "temperature": 0.0, "avg_logprob": -0.16626225748369772, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.971413778141141e-05}, {"id": 36, "seek": 24320, "start": 249.56, "end": 256.56, "text": " is more focused on allowing the user to specify what he wants to measure, so he can say, okay,", "tokens": [307, 544, 5178, 322, 8293, 264, 4195, 281, 16500, 437, 415, 2738, 281, 3481, 11, 370, 415, 393, 584, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.16626225748369772, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.971413778141141e-05}, {"id": 37, "seek": 24320, "start": 256.96, "end": 262.08, "text": " I'm interested in MPI communications, so that is what I want to measure.", "tokens": [286, 478, 3102, 294, 14146, 40, 15163, 11, 370, 300, 307, 437, 286, 528, 281, 3481, 13], "temperature": 0.0, "avg_logprob": -0.16626225748369772, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.971413778141141e-05}, {"id": 38, "seek": 24320, "start": 262.08, "end": 269.08, "text": " So, here's a very high-level overview of how copy works. So, you have your source code,", "tokens": [407, 11, 510, 311, 257, 588, 1090, 12, 12418, 12492, 295, 577, 5055, 1985, 13, 407, 11, 291, 362, 428, 4009, 3089, 11], "temperature": 0.0, "avg_logprob": -0.16626225748369772, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.971413778141141e-05}, {"id": 39, "seek": 26908, "start": 269.08, "end": 276.08, "text": " and that is put into the static analysis in order to generate the call graph. And then", "tokens": [293, 300, 307, 829, 666, 264, 13437, 5215, 294, 1668, 281, 8460, 264, 818, 4295, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.17698163162043065, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.1419249605969526e-05}, {"id": 40, "seek": 26908, "start": 276.84, "end": 283.34, "text": " at the same time, the user specifies the measurement objective in front of a selection spec. We", "tokens": [412, 264, 912, 565, 11, 264, 4195, 1608, 11221, 264, 13160, 10024, 294, 1868, 295, 257, 9450, 1608, 13, 492], "temperature": 0.0, "avg_logprob": -0.17698163162043065, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.1419249605969526e-05}, {"id": 41, "seek": 26908, "start": 283.34, "end": 290.34, "text": " have a simple DSL for that. And this together is then fed into the copy tool in order to", "tokens": [362, 257, 2199, 15816, 43, 337, 300, 13, 400, 341, 1214, 307, 550, 4636, 666, 264, 5055, 2290, 294, 1668, 281], "temperature": 0.0, "avg_logprob": -0.17698163162043065, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.1419249605969526e-05}, {"id": 42, "seek": 26908, "start": 290.56, "end": 297.56, "text": " generate the instrumentation configuration, which is hopefully low overhead.", "tokens": [8460, 264, 7198, 399, 11694, 11, 597, 307, 4696, 2295, 19922, 13], "temperature": 0.0, "avg_logprob": -0.17698163162043065, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.1419249605969526e-05}, {"id": 43, "seek": 29756, "start": 297.56, "end": 304.56, "text": " And, yeah, so let's consider an example for that. So, we might have the following scenario.", "tokens": [400, 11, 1338, 11, 370, 718, 311, 1949, 364, 1365, 337, 300, 13, 407, 11, 321, 1062, 362, 264, 3480, 9005, 13], "temperature": 0.0, "avg_logprob": -0.1717054344886957, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.527594526531175e-05}, {"id": 44, "seek": 29756, "start": 304.6, "end": 310.56, "text": " So, I want to record all call paths that contain MPI communication. Additionally, I want to", "tokens": [407, 11, 286, 528, 281, 2136, 439, 818, 14518, 300, 5304, 14146, 40, 6101, 13, 19927, 11, 286, 528, 281], "temperature": 0.0, "avg_logprob": -0.1717054344886957, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.527594526531175e-05}, {"id": 45, "seek": 29756, "start": 310.56, "end": 315.4, "text": " measure functions that contain loops with at least 10 floating point operations, and", "tokens": [3481, 6828, 300, 5304, 16121, 365, 412, 1935, 1266, 12607, 935, 7705, 11, 293], "temperature": 0.0, "avg_logprob": -0.1717054344886957, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.527594526531175e-05}, {"id": 46, "seek": 29756, "start": 315.4, "end": 321.92, "text": " I don't care about system headers or inline functions. And this is how this looks as a", "tokens": [286, 500, 380, 1127, 466, 1185, 45101, 420, 294, 1889, 6828, 13, 400, 341, 307, 577, 341, 1542, 382, 257], "temperature": 0.0, "avg_logprob": -0.1717054344886957, "compression_ratio": 1.5707964601769913, "no_speech_prob": 3.527594526531175e-05}, {"id": 47, "seek": 32192, "start": 321.92, "end": 328.92, "text": " copy spec. I won't get into the details of the syntax, but you can combine different", "tokens": [5055, 1608, 13, 286, 1582, 380, 483, 666, 264, 4365, 295, 264, 28431, 11, 457, 291, 393, 10432, 819], "temperature": 0.0, "avg_logprob": -0.2294360130063949, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.969722431269474e-05}, {"id": 48, "seek": 32192, "start": 331.04, "end": 338.04, "text": " selection modules where you start with the set of all functions and then sort of here", "tokens": [9450, 16679, 689, 291, 722, 365, 264, 992, 295, 439, 6828, 293, 550, 1333, 295, 510], "temperature": 0.0, "avg_logprob": -0.2294360130063949, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.969722431269474e-05}, {"id": 49, "seek": 32192, "start": 338.8, "end": 345.0, "text": " select inline functions or system header functions. And then you can combine them to, in the", "tokens": [3048, 294, 1889, 6828, 420, 1185, 23117, 6828, 13, 400, 550, 291, 393, 10432, 552, 281, 11, 294, 264], "temperature": 0.0, "avg_logprob": -0.2294360130063949, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.969722431269474e-05}, {"id": 50, "seek": 34500, "start": 345.0, "end": 352.0, "text": " end, produce one set that you want to instrument. And for this particular example, this reduces", "tokens": [917, 11, 5258, 472, 992, 300, 291, 528, 281, 7198, 13, 400, 337, 341, 1729, 1365, 11, 341, 18081], "temperature": 0.0, "avg_logprob": -0.17337266206741334, "compression_ratio": 1.6093023255813954, "no_speech_prob": 2.084365041810088e-05}, {"id": 51, "seek": 34500, "start": 353.88, "end": 360.88, "text": " the number of instrument functions by 74 percent for our form test case.", "tokens": [264, 1230, 295, 7198, 6828, 538, 28868, 3043, 337, 527, 1254, 1500, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17337266206741334, "compression_ratio": 1.6093023255813954, "no_speech_prob": 2.084365041810088e-05}, {"id": 52, "seek": 34500, "start": 361.72, "end": 367.28, "text": " So, the call graph is sort of the base data structure that we use for all the analysis", "tokens": [407, 11, 264, 818, 4295, 307, 1333, 295, 264, 3096, 1412, 3877, 300, 321, 764, 337, 439, 264, 5215], "temperature": 0.0, "avg_logprob": -0.17337266206741334, "compression_ratio": 1.6093023255813954, "no_speech_prob": 2.084365041810088e-05}, {"id": 53, "seek": 34500, "start": 367.28, "end": 374.2, "text": " and for the selection. And this is currently generated based on the source level by a tool", "tokens": [293, 337, 264, 9450, 13, 400, 341, 307, 4362, 10833, 2361, 322, 264, 4009, 1496, 538, 257, 2290], "temperature": 0.0, "avg_logprob": -0.17337266206741334, "compression_ratio": 1.6093023255813954, "no_speech_prob": 2.084365041810088e-05}, {"id": 54, "seek": 37420, "start": 374.2, "end": 380.44, "text": " called MetaCG. And this can be a bit cumbersome for very complex applications, such as Open", "tokens": [1219, 6377, 64, 34, 38, 13, 400, 341, 393, 312, 257, 857, 12713, 1616, 423, 337, 588, 3997, 5821, 11, 1270, 382, 7238], "temperature": 0.0, "avg_logprob": -0.21462559962010647, "compression_ratio": 1.5064377682403434, "no_speech_prob": 9.502591274213046e-05}, {"id": 55, "seek": 37420, "start": 380.44, "end": 386.92, "text": " Foom, because you, yeah, you need sort of a separate source code analysis step, and", "tokens": [8564, 298, 11, 570, 291, 11, 1338, 11, 291, 643, 1333, 295, 257, 4994, 4009, 3089, 5215, 1823, 11, 293], "temperature": 0.0, "avg_logprob": -0.21462559962010647, "compression_ratio": 1.5064377682403434, "no_speech_prob": 9.502591274213046e-05}, {"id": 56, "seek": 37420, "start": 386.92, "end": 391.4, "text": " that can be difficult to set up, especially if there's like shared library dependencies", "tokens": [300, 393, 312, 2252, 281, 992, 493, 11, 2318, 498, 456, 311, 411, 5507, 6405, 36606], "temperature": 0.0, "avg_logprob": -0.21462559962010647, "compression_ratio": 1.5064377682403434, "no_speech_prob": 9.502591274213046e-05}, {"id": 57, "seek": 37420, "start": 391.4, "end": 397.76, "text": " stuff like that. So, it can be tricky. And so, we looked into how we can maybe generate", "tokens": [1507, 411, 300, 13, 407, 11, 309, 393, 312, 12414, 13, 400, 370, 11, 321, 2956, 666, 577, 321, 393, 1310, 8460], "temperature": 0.0, "avg_logprob": -0.21462559962010647, "compression_ratio": 1.5064377682403434, "no_speech_prob": 9.502591274213046e-05}, {"id": 58, "seek": 39776, "start": 397.76, "end": 404.4, "text": " the call graph at different stages. And so, this is what Tim is going to talk about, how", "tokens": [264, 818, 4295, 412, 819, 10232, 13, 400, 370, 11, 341, 307, 437, 7172, 307, 516, 281, 751, 466, 11, 577], "temperature": 0.0, "avg_logprob": -0.22454405564528246, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00012295189662836492}, {"id": 59, "seek": 39776, "start": 404.4, "end": 410.56, "text": " we can generate the call graph at different program stages. And then we introduced the", "tokens": [321, 393, 8460, 264, 818, 4295, 412, 819, 1461, 10232, 13, 400, 550, 321, 7268, 264], "temperature": 0.0, "avg_logprob": -0.22454405564528246, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00012295189662836492}, {"id": 60, "seek": 39776, "start": 410.56, "end": 417.56, "text": " caged compiler plugin, where we, yeah, evaluate how this can be done at link time.", "tokens": [269, 2980, 31958, 23407, 11, 689, 321, 11, 1338, 11, 13059, 577, 341, 393, 312, 1096, 412, 2113, 565, 13], "temperature": 0.0, "avg_logprob": -0.22454405564528246, "compression_ratio": 1.5828220858895705, "no_speech_prob": 0.00012295189662836492}, {"id": 61, "seek": 41756, "start": 417.56, "end": 424.56, "text": " So, well, thank you. So, we were interested in the whole program call graphs. This is", "tokens": [407, 11, 731, 11, 1309, 291, 13, 407, 11, 321, 645, 3102, 294, 264, 1379, 1461, 818, 24877, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.1329196294148763, "compression_ratio": 1.6451612903225807, "no_speech_prob": 4.8080000851769e-05}, {"id": 62, "seek": 41756, "start": 430.68, "end": 435.72, "text": " because CAPI, the tool that does our instrumentation, needs to be aware of every function inside", "tokens": [570, 33636, 40, 11, 264, 2290, 300, 775, 527, 7198, 399, 11, 2203, 281, 312, 3650, 295, 633, 2445, 1854], "temperature": 0.0, "avg_logprob": -0.1329196294148763, "compression_ratio": 1.6451612903225807, "no_speech_prob": 4.8080000851769e-05}, {"id": 63, "seek": 41756, "start": 435.72, "end": 441.08, "text": " our call graph, which means that we are, that it's necessary to have a whole program view.", "tokens": [527, 818, 4295, 11, 597, 1355, 300, 321, 366, 11, 300, 309, 311, 4818, 281, 362, 257, 1379, 1461, 1910, 13], "temperature": 0.0, "avg_logprob": -0.1329196294148763, "compression_ratio": 1.6451612903225807, "no_speech_prob": 4.8080000851769e-05}, {"id": 64, "seek": 41756, "start": 441.08, "end": 447.48, "text": " And the tools that were used like MetaCG were able to create a call graph, and they", "tokens": [400, 264, 3873, 300, 645, 1143, 411, 6377, 64, 34, 38, 645, 1075, 281, 1884, 257, 818, 4295, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.1329196294148763, "compression_ratio": 1.6451612903225807, "no_speech_prob": 4.8080000851769e-05}, {"id": 65, "seek": 44748, "start": 447.48, "end": 451.84000000000003, "text": " have the distinct advantage of being able to annotate metadata to this call graph. This", "tokens": [362, 264, 10644, 5002, 295, 885, 1075, 281, 25339, 473, 26603, 281, 341, 818, 4295, 13, 639], "temperature": 0.0, "avg_logprob": -0.13255759923145025, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.9599515477893874e-05}, {"id": 66, "seek": 44748, "start": 451.84000000000003, "end": 458.0, "text": " is where the name MetaCG came from. And this means that we not only can answer structural", "tokens": [307, 689, 264, 1315, 6377, 64, 34, 38, 1361, 490, 13, 400, 341, 1355, 300, 321, 406, 787, 393, 1867, 15067], "temperature": 0.0, "avg_logprob": -0.13255759923145025, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.9599515477893874e-05}, {"id": 67, "seek": 44748, "start": 458.0, "end": 463.12, "text": " questions like we want to instrument every function that eventually call paths down", "tokens": [1651, 411, 321, 528, 281, 7198, 633, 2445, 300, 4728, 818, 14518, 760], "temperature": 0.0, "avg_logprob": -0.13255759923145025, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.9599515477893874e-05}, {"id": 68, "seek": 44748, "start": 463.12, "end": 469.96000000000004, "text": " to some MPI call, but we can also answer instrument based on metadata. For example, the loop depth", "tokens": [281, 512, 14146, 40, 818, 11, 457, 321, 393, 611, 1867, 7198, 2361, 322, 26603, 13, 1171, 1365, 11, 264, 6367, 7161], "temperature": 0.0, "avg_logprob": -0.13255759923145025, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.9599515477893874e-05}, {"id": 69, "seek": 44748, "start": 469.96000000000004, "end": 476.6, "text": " specification or floating point operations. And yes, there were multiple possible ways", "tokens": [31256, 420, 12607, 935, 7705, 13, 400, 2086, 11, 456, 645, 3866, 1944, 2098], "temperature": 0.0, "avg_logprob": -0.13255759923145025, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.9599515477893874e-05}, {"id": 70, "seek": 47660, "start": 476.6, "end": 481.32000000000005, "text": " to generate a call graph. One of them is source code. One of them is the intermediate representation", "tokens": [281, 8460, 257, 818, 4295, 13, 1485, 295, 552, 307, 4009, 3089, 13, 1485, 295, 552, 307, 264, 19376, 10290], "temperature": 0.0, "avg_logprob": -0.1031964761870248, "compression_ratio": 1.805668016194332, "no_speech_prob": 7.678137626498938e-05}, {"id": 71, "seek": 47660, "start": 481.32000000000005, "end": 488.04, "text": " that is basically part of every compiler, especially the LLVM compile pipeline, and", "tokens": [300, 307, 1936, 644, 295, 633, 31958, 11, 2318, 264, 441, 43, 53, 44, 31413, 15517, 11, 293], "temperature": 0.0, "avg_logprob": -0.1031964761870248, "compression_ratio": 1.805668016194332, "no_speech_prob": 7.678137626498938e-05}, {"id": 72, "seek": 47660, "start": 488.04, "end": 494.76000000000005, "text": " machine code at the very last. So, the basic idea is, well, we have the source code. Let's", "tokens": [3479, 3089, 412, 264, 588, 1036, 13, 407, 11, 264, 3875, 1558, 307, 11, 731, 11, 321, 362, 264, 4009, 3089, 13, 961, 311], "temperature": 0.0, "avg_logprob": -0.1031964761870248, "compression_ratio": 1.805668016194332, "no_speech_prob": 7.678137626498938e-05}, {"id": 73, "seek": 47660, "start": 494.76000000000005, "end": 500.20000000000005, "text": " do call graph generation on the source code, which is relatively easy. MetaCG is doing", "tokens": [360, 818, 4295, 5125, 322, 264, 4009, 3089, 11, 597, 307, 7226, 1858, 13, 6377, 64, 34, 38, 307, 884], "temperature": 0.0, "avg_logprob": -0.1031964761870248, "compression_ratio": 1.805668016194332, "no_speech_prob": 7.678137626498938e-05}, {"id": 74, "seek": 47660, "start": 500.20000000000005, "end": 505.16, "text": " it on a source code basis. But this means as you feed every single source code file", "tokens": [309, 322, 257, 4009, 3089, 5143, 13, 583, 341, 1355, 382, 291, 3154, 633, 2167, 4009, 3089, 3991], "temperature": 0.0, "avg_logprob": -0.1031964761870248, "compression_ratio": 1.805668016194332, "no_speech_prob": 7.678137626498938e-05}, {"id": 75, "seek": 50516, "start": 505.16, "end": 510.0, "text": " into MetaCG, this means that your view of the program is limited to one translation unit", "tokens": [666, 6377, 64, 34, 38, 11, 341, 1355, 300, 428, 1910, 295, 264, 1461, 307, 5567, 281, 472, 12853, 4985], "temperature": 0.0, "avg_logprob": -0.09422003548100309, "compression_ratio": 1.745019920318725, "no_speech_prob": 4.240041016601026e-05}, {"id": 76, "seek": 50516, "start": 510.0, "end": 516.88, "text": " at a time. So, you then need to merge those part call graphs of every translation unit", "tokens": [412, 257, 565, 13, 407, 11, 291, 550, 643, 281, 22183, 729, 644, 818, 24877, 295, 633, 12853, 4985], "temperature": 0.0, "avg_logprob": -0.09422003548100309, "compression_ratio": 1.745019920318725, "no_speech_prob": 4.240041016601026e-05}, {"id": 77, "seek": 50516, "start": 516.88, "end": 522.96, "text": " together to generate this overview of the whole program that you need. The information", "tokens": [1214, 281, 8460, 341, 12492, 295, 264, 1379, 1461, 300, 291, 643, 13, 440, 1589], "temperature": 0.0, "avg_logprob": -0.09422003548100309, "compression_ratio": 1.745019920318725, "no_speech_prob": 4.240041016601026e-05}, {"id": 78, "seek": 50516, "start": 522.96, "end": 526.6800000000001, "text": " you then gather from your profiling maps very cleanly back to your source code, because", "tokens": [291, 550, 5448, 490, 428, 1740, 4883, 11317, 588, 2541, 356, 646, 281, 428, 4009, 3089, 11, 570], "temperature": 0.0, "avg_logprob": -0.09422003548100309, "compression_ratio": 1.745019920318725, "no_speech_prob": 4.240041016601026e-05}, {"id": 79, "seek": 50516, "start": 526.6800000000001, "end": 532.48, "text": " once you find a function, well, it's named the same way in your source code. But on the", "tokens": [1564, 291, 915, 257, 2445, 11, 731, 11, 309, 311, 4926, 264, 912, 636, 294, 428, 4009, 3089, 13, 583, 322, 264], "temperature": 0.0, "avg_logprob": -0.09422003548100309, "compression_ratio": 1.745019920318725, "no_speech_prob": 4.240041016601026e-05}, {"id": 80, "seek": 53248, "start": 532.48, "end": 537.16, "text": " other hand, what you write in your source code is not necessarily what's actually executed", "tokens": [661, 1011, 11, 437, 291, 2464, 294, 428, 4009, 3089, 307, 406, 4725, 437, 311, 767, 17577], "temperature": 0.0, "avg_logprob": -0.11711408615112305, "compression_ratio": 1.7509727626459144, "no_speech_prob": 1.801137659640517e-05}, {"id": 81, "seek": 53248, "start": 537.16, "end": 541.36, "text": " on the machine, right? Because there might be code transformation, constant propagation,", "tokens": [322, 264, 3479, 11, 558, 30, 1436, 456, 1062, 312, 3089, 9887, 11, 5754, 38377, 11], "temperature": 0.0, "avg_logprob": -0.11711408615112305, "compression_ratio": 1.7509727626459144, "no_speech_prob": 1.801137659640517e-05}, {"id": 82, "seek": 53248, "start": 541.36, "end": 545.8000000000001, "text": " dead code elimination, inlining. And this is actually something we want to be aware", "tokens": [3116, 3089, 29224, 11, 294, 31079, 13, 400, 341, 307, 767, 746, 321, 528, 281, 312, 3650], "temperature": 0.0, "avg_logprob": -0.11711408615112305, "compression_ratio": 1.7509727626459144, "no_speech_prob": 1.801137659640517e-05}, {"id": 83, "seek": 53248, "start": 545.8000000000001, "end": 549.4, "text": " of if we are doing instrumentation, not that we want to instrument a function that doesn't", "tokens": [295, 498, 321, 366, 884, 7198, 399, 11, 406, 300, 321, 528, 281, 7198, 257, 2445, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.11711408615112305, "compression_ratio": 1.7509727626459144, "no_speech_prob": 1.801137659640517e-05}, {"id": 84, "seek": 53248, "start": 549.4, "end": 556.5600000000001, "text": " exist anymore. Also, this merging of the translation units means that the user is involved. The", "tokens": [2514, 3602, 13, 2743, 11, 341, 44559, 295, 264, 12853, 6815, 1355, 300, 264, 4195, 307, 3288, 13, 440], "temperature": 0.0, "avg_logprob": -0.11711408615112305, "compression_ratio": 1.7509727626459144, "no_speech_prob": 1.801137659640517e-05}, {"id": 85, "seek": 55656, "start": 556.56, "end": 562.4, "text": " user currently has to specify when he uses MetaCG which source code translation units", "tokens": [4195, 4362, 575, 281, 16500, 562, 415, 4960, 6377, 64, 34, 38, 597, 4009, 3089, 12853, 6815], "temperature": 0.0, "avg_logprob": -0.11891247852739081, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.153871021931991e-05}, {"id": 86, "seek": 55656, "start": 562.4, "end": 568.52, "text": " belong to one target. And then manually has to tell MetaCG these functions are all to", "tokens": [5784, 281, 472, 3779, 13, 400, 550, 16945, 575, 281, 980, 6377, 64, 34, 38, 613, 6828, 366, 439, 281], "temperature": 0.0, "avg_logprob": -0.11891247852739081, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.153871021931991e-05}, {"id": 87, "seek": 55656, "start": 568.52, "end": 574.04, "text": " be merged. Please generate the call graph for that. And the user might not perfectly emulate", "tokens": [312, 36427, 13, 2555, 8460, 264, 818, 4295, 337, 300, 13, 400, 264, 4195, 1062, 406, 6239, 45497], "temperature": 0.0, "avg_logprob": -0.11891247852739081, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.153871021931991e-05}, {"id": 88, "seek": 55656, "start": 574.04, "end": 580.2399999999999, "text": " the linker behavior. So, there are different resolution types that a linker might choose", "tokens": [264, 2113, 260, 5223, 13, 407, 11, 456, 366, 819, 8669, 3467, 300, 257, 2113, 260, 1062, 2826], "temperature": 0.0, "avg_logprob": -0.11891247852739081, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.153871021931991e-05}, {"id": 89, "seek": 58024, "start": 580.24, "end": 587.08, "text": " if there are samely named structs or classes. And, depending on how you are implementing", "tokens": [498, 456, 366, 912, 356, 4926, 6594, 82, 420, 5359, 13, 400, 11, 5413, 322, 577, 291, 366, 18114], "temperature": 0.0, "avg_logprob": -0.1781190483315477, "compression_ratio": 1.620817843866171, "no_speech_prob": 2.384130311838817e-05}, {"id": 90, "seek": 58024, "start": 587.08, "end": 591.6800000000001, "text": " your merging process, you might have slight differences between your call graph that you", "tokens": [428, 44559, 1399, 11, 291, 1062, 362, 4036, 7300, 1296, 428, 818, 4295, 300, 291], "temperature": 0.0, "avg_logprob": -0.1781190483315477, "compression_ratio": 1.620817843866171, "no_speech_prob": 2.384130311838817e-05}, {"id": 91, "seek": 58024, "start": 591.6800000000001, "end": 597.2, "text": " generated and that's what the linker will eventually do. So, the other extreme would", "tokens": [10833, 293, 300, 311, 437, 264, 2113, 260, 486, 4728, 360, 13, 407, 11, 264, 661, 8084, 576], "temperature": 0.0, "avg_logprob": -0.1781190483315477, "compression_ratio": 1.620817843866171, "no_speech_prob": 2.384130311838817e-05}, {"id": 92, "seek": 58024, "start": 597.2, "end": 601.32, "text": " be, well, let's do it on the compiled machine code then. Reverse engineering tools like", "tokens": [312, 11, 731, 11, 718, 311, 360, 309, 322, 264, 36548, 3479, 3089, 550, 13, 26314, 405, 7043, 3873, 411], "temperature": 0.0, "avg_logprob": -0.1781190483315477, "compression_ratio": 1.620817843866171, "no_speech_prob": 2.384130311838817e-05}, {"id": 93, "seek": 58024, "start": 601.32, "end": 606.36, "text": " radar or Ghidra are able to generate call graphs and binary data just fine. And those", "tokens": [16544, 420, 20321, 327, 424, 366, 1075, 281, 8460, 818, 24877, 293, 17434, 1412, 445, 2489, 13, 400, 729], "temperature": 0.0, "avg_logprob": -0.1781190483315477, "compression_ratio": 1.620817843866171, "no_speech_prob": 2.384130311838817e-05}, {"id": 94, "seek": 60636, "start": 606.36, "end": 610.24, "text": " have the very distinct advantages that this is actually what is run on the machine. There", "tokens": [362, 264, 588, 10644, 14906, 300, 341, 307, 767, 437, 307, 1190, 322, 264, 3479, 13, 821], "temperature": 0.0, "avg_logprob": -0.11879907608032227, "compression_ratio": 1.7104247104247103, "no_speech_prob": 8.55404359754175e-05}, {"id": 95, "seek": 60636, "start": 610.24, "end": 615.2, "text": " are no code transformation left. You have the advantage of being able to see machine", "tokens": [366, 572, 3089, 9887, 1411, 13, 509, 362, 264, 5002, 295, 885, 1075, 281, 536, 3479], "temperature": 0.0, "avg_logprob": -0.11879907608032227, "compression_ratio": 1.7104247104247103, "no_speech_prob": 8.55404359754175e-05}, {"id": 96, "seek": 60636, "start": 615.2, "end": 623.0, "text": " code optimization passes if they are influencing the generated call graph. But, on the other", "tokens": [3089, 19618, 11335, 498, 436, 366, 40396, 264, 10833, 818, 4295, 13, 583, 11, 322, 264, 661], "temperature": 0.0, "avg_logprob": -0.11879907608032227, "compression_ratio": 1.7104247104247103, "no_speech_prob": 8.55404359754175e-05}, {"id": 97, "seek": 60636, "start": 623.0, "end": 627.72, "text": " hand, a lot of information, the metadata that we also would like to be able to instrument", "tokens": [1011, 11, 257, 688, 295, 1589, 11, 264, 26603, 300, 321, 611, 576, 411, 281, 312, 1075, 281, 7198], "temperature": 0.0, "avg_logprob": -0.11879907608032227, "compression_ratio": 1.7104247104247103, "no_speech_prob": 8.55404359754175e-05}, {"id": 98, "seek": 60636, "start": 627.72, "end": 632.48, "text": " based upon are lost as soon as we go down to machine code. Inlining already happened,", "tokens": [2361, 3564, 366, 2731, 382, 2321, 382, 321, 352, 760, 281, 3479, 3089, 13, 682, 31079, 1217, 2011, 11], "temperature": 0.0, "avg_logprob": -0.11879907608032227, "compression_ratio": 1.7104247104247103, "no_speech_prob": 8.55404359754175e-05}, {"id": 99, "seek": 63248, "start": 632.48, "end": 638.36, "text": " so there is no function annotated with please inline anymore. Also, pointer type information,", "tokens": [370, 456, 307, 572, 2445, 25339, 770, 365, 1767, 294, 1889, 3602, 13, 2743, 11, 23918, 2010, 1589, 11], "temperature": 0.0, "avg_logprob": -0.11226545061383929, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.137719462160021e-05}, {"id": 100, "seek": 63248, "start": 638.36, "end": 642.24, "text": " as we heard in the talk earlier, gets lost as soon as we go down to machine type. And", "tokens": [382, 321, 2198, 294, 264, 751, 3071, 11, 2170, 2731, 382, 2321, 382, 321, 352, 760, 281, 3479, 2010, 13, 400], "temperature": 0.0, "avg_logprob": -0.11226545061383929, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.137719462160021e-05}, {"id": 101, "seek": 63248, "start": 642.24, "end": 647.36, "text": " constness is also something that is more to be inferred than actually stated once we", "tokens": [1817, 1287, 307, 611, 746, 300, 307, 544, 281, 312, 13596, 986, 813, 767, 11323, 1564, 321], "temperature": 0.0, "avg_logprob": -0.11226545061383929, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.137719462160021e-05}, {"id": 102, "seek": 63248, "start": 647.36, "end": 654.0, "text": " go down to machine code. And so, we decided the best of both worlds is probably the LLVMIR", "tokens": [352, 760, 281, 3479, 3089, 13, 400, 370, 11, 321, 3047, 264, 1151, 295, 1293, 13401, 307, 1391, 264, 441, 43, 53, 44, 7740], "temperature": 0.0, "avg_logprob": -0.11226545061383929, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.137719462160021e-05}, {"id": 103, "seek": 63248, "start": 654.0, "end": 660.24, "text": " because it's a heavily annotated representation. It is close enough to what will run on the", "tokens": [570, 309, 311, 257, 10950, 25339, 770, 10290, 13, 467, 307, 1998, 1547, 281, 437, 486, 1190, 322, 264], "temperature": 0.0, "avg_logprob": -0.11226545061383929, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.137719462160021e-05}, {"id": 104, "seek": 66024, "start": 660.24, "end": 667.52, "text": " machine that we have the ability to observe the code transformation. We are able to give", "tokens": [3479, 300, 321, 362, 264, 3485, 281, 11441, 264, 3089, 9887, 13, 492, 366, 1075, 281, 976], "temperature": 0.0, "avg_logprob": -0.13674661545526415, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.00010033006401499733}, {"id": 105, "seek": 66024, "start": 667.52, "end": 672.72, "text": " more specific estimates on what the actual cost of a function might be because we have", "tokens": [544, 2685, 20561, 322, 437, 264, 3539, 2063, 295, 257, 2445, 1062, 312, 570, 321, 362], "temperature": 0.0, "avg_logprob": -0.13674661545526415, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.00010033006401499733}, {"id": 106, "seek": 66024, "start": 672.72, "end": 679.24, "text": " more clear way of tracking, for example, instruction counts, floating ops, and integer ops. On", "tokens": [544, 1850, 636, 295, 11603, 11, 337, 1365, 11, 10951, 14893, 11, 12607, 44663, 11, 293, 24922, 44663, 13, 1282], "temperature": 0.0, "avg_logprob": -0.13674661545526415, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.00010033006401499733}, {"id": 107, "seek": 66024, "start": 679.24, "end": 683.8, "text": " the other hand, it's also close enough to what the user actually wrote because we're", "tokens": [264, 661, 1011, 11, 309, 311, 611, 1998, 1547, 281, 437, 264, 4195, 767, 4114, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.13674661545526415, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.00010033006401499733}, {"id": 108, "seek": 66024, "start": 683.8, "end": 688.36, "text": " not down on the machine code yet. And we can figure out the inlining stuff, the constness,", "tokens": [406, 760, 322, 264, 3479, 3089, 1939, 13, 400, 321, 393, 2573, 484, 264, 294, 31079, 1507, 11, 264, 1817, 1287, 11], "temperature": 0.0, "avg_logprob": -0.13674661545526415, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.00010033006401499733}, {"id": 109, "seek": 68836, "start": 688.36, "end": 695.24, "text": " the virtual functions. We can get type information in the IR. And if we do it at link time, we", "tokens": [264, 6374, 6828, 13, 492, 393, 483, 2010, 1589, 294, 264, 16486, 13, 400, 498, 321, 360, 309, 412, 2113, 565, 11, 321], "temperature": 0.0, "avg_logprob": -0.11784747152617484, "compression_ratio": 1.7707509881422925, "no_speech_prob": 3.201582876499742e-05}, {"id": 110, "seek": 68836, "start": 695.24, "end": 700.72, "text": " are not even limited to the translation unit by translation unit scope that source code", "tokens": [366, 406, 754, 5567, 281, 264, 12853, 4985, 538, 12853, 4985, 11923, 300, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.11784747152617484, "compression_ratio": 1.7707509881422925, "no_speech_prob": 3.201582876499742e-05}, {"id": 111, "seek": 68836, "start": 700.72, "end": 705.5600000000001, "text": " based approaches are. So, if you have your pretty default compile pipeline, you have", "tokens": [2361, 11587, 366, 13, 407, 11, 498, 291, 362, 428, 1238, 7576, 31413, 15517, 11, 291, 362], "temperature": 0.0, "avg_logprob": -0.11784747152617484, "compression_ratio": 1.7707509881422925, "no_speech_prob": 3.201582876499742e-05}, {"id": 112, "seek": 68836, "start": 705.5600000000001, "end": 709.4, "text": " your source code, which builds a translation unit, gets fed into the compiler, which outputs", "tokens": [428, 4009, 3089, 11, 597, 15182, 257, 12853, 4985, 11, 2170, 4636, 666, 264, 31958, 11, 597, 23930], "temperature": 0.0, "avg_logprob": -0.11784747152617484, "compression_ratio": 1.7707509881422925, "no_speech_prob": 3.201582876499742e-05}, {"id": 113, "seek": 68836, "start": 709.4, "end": 714.64, "text": " intermediate representation, and then the optimizer runs there and multiple source code", "tokens": [19376, 10290, 11, 293, 550, 264, 5028, 6545, 6676, 456, 293, 3866, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.11784747152617484, "compression_ratio": 1.7707509881422925, "no_speech_prob": 3.201582876499742e-05}, {"id": 114, "seek": 71464, "start": 714.64, "end": 720.72, "text": " translation unit optimized IR modules are fed into the linker. And we can do our call", "tokens": [12853, 4985, 26941, 16486, 16679, 366, 4636, 666, 264, 2113, 260, 13, 400, 321, 393, 360, 527, 818], "temperature": 0.0, "avg_logprob": -0.1622156302134196, "compression_ratio": 1.6350710900473933, "no_speech_prob": 3.065997225348838e-05}, {"id": 115, "seek": 71464, "start": 720.72, "end": 725.96, "text": " graph analysis inside the linker, solve our translation unit problem, and are able to", "tokens": [4295, 5215, 1854, 264, 2113, 260, 11, 5039, 527, 12853, 4985, 1154, 11, 293, 366, 1075, 281], "temperature": 0.0, "avg_logprob": -0.1622156302134196, "compression_ratio": 1.6350710900473933, "no_speech_prob": 3.065997225348838e-05}, {"id": 116, "seek": 71464, "start": 725.96, "end": 732.16, "text": " have all our information ready. So, to do this, we developed the cage plugin. Cage stands", "tokens": [362, 439, 527, 1589, 1919, 13, 407, 11, 281, 360, 341, 11, 321, 4743, 264, 17302, 23407, 13, 48677, 7382], "temperature": 0.0, "avg_logprob": -0.1622156302134196, "compression_ratio": 1.6350710900473933, "no_speech_prob": 3.065997225348838e-05}, {"id": 117, "seek": 71464, "start": 732.16, "end": 739.72, "text": " for call graph embedding LLVM plugin. And it basically generates a call graph using", "tokens": [337, 818, 4295, 12240, 3584, 441, 43, 53, 44, 23407, 13, 400, 309, 1936, 23815, 257, 818, 4295, 1228], "temperature": 0.0, "avg_logprob": -0.1622156302134196, "compression_ratio": 1.6350710900473933, "no_speech_prob": 3.065997225348838e-05}, {"id": 118, "seek": 73972, "start": 739.72, "end": 746.12, "text": " some of LLVM tools, does some annotation, virtual function call analysis, and this can", "tokens": [512, 295, 441, 43, 53, 44, 3873, 11, 775, 512, 48654, 11, 6374, 2445, 818, 5215, 11, 293, 341, 393], "temperature": 0.0, "avg_logprob": -0.1297360475246723, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.9452490960247815e-05}, {"id": 119, "seek": 73972, "start": 746.12, "end": 752.24, "text": " run as part of the optimizer in the pipeline, but it can also run as part of the LLVM linker.", "tokens": [1190, 382, 644, 295, 264, 5028, 6545, 294, 264, 15517, 11, 457, 309, 393, 611, 1190, 382, 644, 295, 264, 441, 43, 53, 44, 2113, 260, 13], "temperature": 0.0, "avg_logprob": -0.1297360475246723, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.9452490960247815e-05}, {"id": 120, "seek": 73972, "start": 752.24, "end": 758.84, "text": " Also as a plugin, for which we use a slightly modified version of the LLVM linker, but", "tokens": [2743, 382, 257, 23407, 11, 337, 597, 321, 764, 257, 4748, 15873, 3037, 295, 264, 441, 43, 53, 44, 2113, 260, 11, 457], "temperature": 0.0, "avg_logprob": -0.1297360475246723, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.9452490960247815e-05}, {"id": 121, "seek": 73972, "start": 758.84, "end": 765.4, "text": " the basic logic of running plugins in the LLVM linker was there. So, then we do a V", "tokens": [264, 3875, 9952, 295, 2614, 33759, 294, 264, 441, 43, 53, 44, 2113, 260, 390, 456, 13, 407, 11, 550, 321, 360, 257, 691], "temperature": 0.0, "avg_logprob": -0.1297360475246723, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.9452490960247815e-05}, {"id": 122, "seek": 76540, "start": 765.4, "end": 769.9599999999999, "text": " table analysis, our metadata annotation, because it's all available to us. And then", "tokens": [3199, 5215, 11, 527, 26603, 48654, 11, 570, 309, 311, 439, 2435, 281, 505, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.13388824462890625, "compression_ratio": 1.6654135338345866, "no_speech_prob": 4.390618414618075e-05}, {"id": 123, "seek": 76540, "start": 769.9599999999999, "end": 775.1999999999999, "text": " we embed the result into the binary, which enables dynamic augmentation. And I will come", "tokens": [321, 12240, 264, 1874, 666, 264, 17434, 11, 597, 17077, 8546, 14501, 19631, 13, 400, 286, 486, 808], "temperature": 0.0, "avg_logprob": -0.13388824462890625, "compression_ratio": 1.6654135338345866, "no_speech_prob": 4.390618414618075e-05}, {"id": 124, "seek": 76540, "start": 775.1999999999999, "end": 780.9599999999999, "text": " to that one later. So, at link time, we're doing static analysis basically. And as I", "tokens": [281, 300, 472, 1780, 13, 407, 11, 412, 2113, 565, 11, 321, 434, 884, 13437, 5215, 1936, 13, 400, 382, 286], "temperature": 0.0, "avg_logprob": -0.13388824462890625, "compression_ratio": 1.6654135338345866, "no_speech_prob": 4.390618414618075e-05}, {"id": 125, "seek": 76540, "start": 780.9599999999999, "end": 785.9599999999999, "text": " already mentioned, we split our information in basically two types. One of them is structural", "tokens": [1217, 2835, 11, 321, 7472, 527, 1589, 294, 1936, 732, 3467, 13, 1485, 295, 552, 307, 15067], "temperature": 0.0, "avg_logprob": -0.13388824462890625, "compression_ratio": 1.6654135338345866, "no_speech_prob": 4.390618414618075e-05}, {"id": 126, "seek": 76540, "start": 785.9599999999999, "end": 791.8, "text": " information like call hierarchies, call paths, call depth, the number of children, how deep", "tokens": [1589, 411, 818, 35250, 530, 11, 818, 14518, 11, 818, 7161, 11, 264, 1230, 295, 2227, 11, 577, 2452], "temperature": 0.0, "avg_logprob": -0.13388824462890625, "compression_ratio": 1.6654135338345866, "no_speech_prob": 4.390618414618075e-05}, {"id": 127, "seek": 79180, "start": 791.8, "end": 798.4799999999999, "text": " we are in our path. And you also have virtual function calls, which are mostly structural,", "tokens": [321, 366, 294, 527, 3100, 13, 400, 291, 611, 362, 6374, 2445, 5498, 11, 597, 366, 5240, 15067, 11], "temperature": 0.0, "avg_logprob": -0.13002944909609282, "compression_ratio": 1.8137651821862348, "no_speech_prob": 3.877196286339313e-05}, {"id": 128, "seek": 79180, "start": 798.4799999999999, "end": 802.68, "text": " because once you have virtual and polymorphic calls, you have like a set of functions that", "tokens": [570, 1564, 291, 362, 6374, 293, 6754, 76, 18191, 299, 5498, 11, 291, 362, 411, 257, 992, 295, 6828, 300], "temperature": 0.0, "avg_logprob": -0.13002944909609282, "compression_ratio": 1.8137651821862348, "no_speech_prob": 3.877196286339313e-05}, {"id": 129, "seek": 79180, "start": 802.68, "end": 808.52, "text": " are probably being called by that pointer. And so, we can narrow down the possibilities", "tokens": [366, 1391, 885, 1219, 538, 300, 23918, 13, 400, 370, 11, 321, 393, 9432, 760, 264, 12178], "temperature": 0.0, "avg_logprob": -0.13002944909609282, "compression_ratio": 1.8137651821862348, "no_speech_prob": 3.877196286339313e-05}, {"id": 130, "seek": 79180, "start": 808.52, "end": 813.24, "text": " of which functions are called, but we cannot actually statically figure out this function", "tokens": [295, 597, 6828, 366, 1219, 11, 457, 321, 2644, 767, 2219, 984, 2573, 484, 341, 2445], "temperature": 0.0, "avg_logprob": -0.13002944909609282, "compression_ratio": 1.8137651821862348, "no_speech_prob": 3.877196286339313e-05}, {"id": 131, "seek": 79180, "start": 813.24, "end": 818.5999999999999, "text": " is getting called. So, it's slightly metadata based, but it's also mostly structural. On", "tokens": [307, 1242, 1219, 13, 407, 11, 309, 311, 4748, 26603, 2361, 11, 457, 309, 311, 611, 5240, 15067, 13, 1282], "temperature": 0.0, "avg_logprob": -0.13002944909609282, "compression_ratio": 1.8137651821862348, "no_speech_prob": 3.877196286339313e-05}, {"id": 132, "seek": 81860, "start": 818.6, "end": 822.96, "text": " the metadata information side, we have instruction composition, so we can determine what is the", "tokens": [264, 26603, 1589, 1252, 11, 321, 362, 10951, 12686, 11, 370, 321, 393, 6997, 437, 307, 264], "temperature": 0.0, "avg_logprob": -0.1374038557211558, "compression_ratio": 1.8178137651821862, "no_speech_prob": 1.7734795619617216e-05}, {"id": 133, "seek": 81860, "start": 822.96, "end": 827.72, "text": " relation between arithmetic operations and memory operations, for example, or we can", "tokens": [9721, 1296, 42973, 7705, 293, 4675, 7705, 11, 337, 1365, 11, 420, 321, 393], "temperature": 0.0, "avg_logprob": -0.1374038557211558, "compression_ratio": 1.8178137651821862, "no_speech_prob": 1.7734795619617216e-05}, {"id": 134, "seek": 81860, "start": 827.72, "end": 834.88, "text": " generate local and global loop depth estimates. And then we have inlining information, which", "tokens": [8460, 2654, 293, 4338, 6367, 7161, 20561, 13, 400, 550, 321, 362, 294, 31079, 1589, 11, 597], "temperature": 0.0, "avg_logprob": -0.1374038557211558, "compression_ratio": 1.8178137651821862, "no_speech_prob": 1.7734795619617216e-05}, {"id": 135, "seek": 81860, "start": 834.88, "end": 840.96, "text": " is metadata, because inlining is not like a must do for a compiler, just because you", "tokens": [307, 26603, 11, 570, 294, 31079, 307, 406, 411, 257, 1633, 360, 337, 257, 31958, 11, 445, 570, 291], "temperature": 0.0, "avg_logprob": -0.1374038557211558, "compression_ratio": 1.8178137651821862, "no_speech_prob": 1.7734795619617216e-05}, {"id": 136, "seek": 81860, "start": 840.96, "end": 845.2, "text": " have specified inlining for a function doesn't mean that the compiler will actually inline", "tokens": [362, 22206, 294, 31079, 337, 257, 2445, 1177, 380, 914, 300, 264, 31958, 486, 767, 294, 1889], "temperature": 0.0, "avg_logprob": -0.1374038557211558, "compression_ratio": 1.8178137651821862, "no_speech_prob": 1.7734795619617216e-05}, {"id": 137, "seek": 84520, "start": 845.2, "end": 851.1600000000001, "text": " the function. So, it's partly structural information and partly metadata, so you see", "tokens": [264, 2445, 13, 407, 11, 309, 311, 17031, 15067, 1589, 293, 17031, 26603, 11, 370, 291, 536], "temperature": 0.0, "avg_logprob": -0.11876991183258766, "compression_ratio": 1.6484018264840183, "no_speech_prob": 6.66261839796789e-05}, {"id": 138, "seek": 84520, "start": 851.1600000000001, "end": 856.96, "text": " there's no clear like line between those, they blur at some points, but we can represent", "tokens": [456, 311, 572, 1850, 411, 1622, 1296, 729, 11, 436, 14257, 412, 512, 2793, 11, 457, 321, 393, 2906], "temperature": 0.0, "avg_logprob": -0.11876991183258766, "compression_ratio": 1.6484018264840183, "no_speech_prob": 6.66261839796789e-05}, {"id": 139, "seek": 84520, "start": 856.96, "end": 863.5200000000001, "text": " all those in the metadata annotated call graph. And if you remember, we were able to do dynamic", "tokens": [439, 729, 294, 264, 26603, 25339, 770, 818, 4295, 13, 400, 498, 291, 1604, 11, 321, 645, 1075, 281, 360, 8546], "temperature": 0.0, "avg_logprob": -0.11876991183258766, "compression_ratio": 1.6484018264840183, "no_speech_prob": 6.66261839796789e-05}, {"id": 140, "seek": 84520, "start": 863.5200000000001, "end": 869.0, "text": " augmentation. Well, what is dynamic augmentation? If you remember, each object contains the", "tokens": [14501, 19631, 13, 1042, 11, 437, 307, 8546, 14501, 19631, 30, 759, 291, 1604, 11, 1184, 2657, 8306, 264], "temperature": 0.0, "avg_logprob": -0.11876991183258766, "compression_ratio": 1.6484018264840183, "no_speech_prob": 6.66261839796789e-05}, {"id": 141, "seek": 86900, "start": 869.0, "end": 875.72, "text": " call graph that we generated, which means that the call graphs can be aggregated at runtime", "tokens": [818, 4295, 300, 321, 10833, 11, 597, 1355, 300, 264, 818, 24877, 393, 312, 16743, 770, 412, 34474], "temperature": 0.0, "avg_logprob": -0.10162632682106712, "compression_ratio": 1.8112449799196788, "no_speech_prob": 4.8912432248471305e-05}, {"id": 142, "seek": 86900, "start": 875.72, "end": 880.36, "text": " if a shared library is loaded, because even if you are at link time, even if you can see", "tokens": [498, 257, 5507, 6405, 307, 13210, 11, 570, 754, 498, 291, 366, 412, 2113, 565, 11, 754, 498, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.10162632682106712, "compression_ratio": 1.8112449799196788, "no_speech_prob": 4.8912432248471305e-05}, {"id": 143, "seek": 86900, "start": 880.36, "end": 885.16, "text": " all the statically linked objects, all the translation units that belong to your target,", "tokens": [439, 264, 2219, 984, 9408, 6565, 11, 439, 264, 12853, 6815, 300, 5784, 281, 428, 3779, 11], "temperature": 0.0, "avg_logprob": -0.10162632682106712, "compression_ratio": 1.8112449799196788, "no_speech_prob": 4.8912432248471305e-05}, {"id": 144, "seek": 86900, "start": 885.16, "end": 891.8, "text": " your binary might load a shared library, which then, well, you're unaware of. So, the idea", "tokens": [428, 17434, 1062, 3677, 257, 5507, 6405, 11, 597, 550, 11, 731, 11, 291, 434, 32065, 295, 13, 407, 11, 264, 1558], "temperature": 0.0, "avg_logprob": -0.10162632682106712, "compression_ratio": 1.8112449799196788, "no_speech_prob": 4.8912432248471305e-05}, {"id": 145, "seek": 86900, "start": 891.8, "end": 897.84, "text": " is, as soon as the main executable is loaded, it passes its embedded call graph on startup", "tokens": [307, 11, 382, 2321, 382, 264, 2135, 7568, 712, 307, 13210, 11, 309, 11335, 1080, 16741, 818, 4295, 322, 18578], "temperature": 0.0, "avg_logprob": -0.10162632682106712, "compression_ratio": 1.8112449799196788, "no_speech_prob": 4.8912432248471305e-05}, {"id": 146, "seek": 89784, "start": 897.84, "end": 903.12, "text": " to a runtime collecting library. And then, the main executable can load whatever shared", "tokens": [281, 257, 34474, 12510, 6405, 13, 400, 550, 11, 264, 2135, 7568, 712, 393, 3677, 2035, 5507], "temperature": 0.0, "avg_logprob": -0.12433228348240707, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.4937729904195294e-05}, {"id": 147, "seek": 89784, "start": 903.12, "end": 910.32, "text": " object it wants. And if this shared object also contains a call graph, then this runtime", "tokens": [2657, 309, 2738, 13, 400, 498, 341, 5507, 2657, 611, 8306, 257, 818, 4295, 11, 550, 341, 34474], "temperature": 0.0, "avg_logprob": -0.12433228348240707, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.4937729904195294e-05}, {"id": 148, "seek": 89784, "start": 910.32, "end": 915.48, "text": " collector gets passed this call graph on the first load of the shared object and can aggregate", "tokens": [23960, 2170, 4678, 341, 818, 4295, 322, 264, 700, 3677, 295, 264, 5507, 2657, 293, 393, 26118], "temperature": 0.0, "avg_logprob": -0.12433228348240707, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.4937729904195294e-05}, {"id": 149, "seek": 89784, "start": 915.48, "end": 920.2, "text": " it like merging. So, we're basically back to the translation unit by translation unit", "tokens": [309, 411, 44559, 13, 407, 11, 321, 434, 1936, 646, 281, 264, 12853, 4985, 538, 12853, 4985], "temperature": 0.0, "avg_logprob": -0.12433228348240707, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.4937729904195294e-05}, {"id": 150, "seek": 89784, "start": 920.2, "end": 927.08, "text": " based approach, but now we're doing shared library on binary and executable based merging.", "tokens": [2361, 3109, 11, 457, 586, 321, 434, 884, 5507, 6405, 322, 17434, 293, 7568, 712, 2361, 44559, 13], "temperature": 0.0, "avg_logprob": -0.12433228348240707, "compression_ratio": 1.8666666666666667, "no_speech_prob": 2.4937729904195294e-05}, {"id": 151, "seek": 92708, "start": 927.08, "end": 931.88, "text": " And then, we attach all this data together to one really big whole program call graph,", "tokens": [400, 550, 11, 321, 5085, 439, 341, 1412, 1214, 281, 472, 534, 955, 1379, 1461, 818, 4295, 11], "temperature": 0.0, "avg_logprob": -0.1972062998804553, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.00017561264394316822}, {"id": 152, "seek": 92708, "start": 931.88, "end": 936.44, "text": " now including shared objects. And then, we can export this, for example, and pass it", "tokens": [586, 3009, 5507, 6565, 13, 400, 550, 11, 321, 393, 10725, 341, 11, 337, 1365, 11, 293, 1320, 309], "temperature": 0.0, "avg_logprob": -0.1972062998804553, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.00017561264394316822}, {"id": 153, "seek": 92708, "start": 936.44, "end": 939.96, "text": " back to Karpi for some further refinement of the instrumentation.", "tokens": [646, 281, 591, 6529, 72, 337, 512, 3052, 1895, 30229, 295, 264, 7198, 399, 13], "temperature": 0.0, "avg_logprob": -0.1972062998804553, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.00017561264394316822}, {"id": 154, "seek": 92708, "start": 939.96, "end": 954.88, "text": " Go ahead. All right. Thanks. So, to put it all together, for Karpi, we have the call", "tokens": [1037, 2286, 13, 1057, 558, 13, 2561, 13, 407, 11, 281, 829, 309, 439, 1214, 11, 337, 591, 6529, 72, 11, 321, 362, 264, 818], "temperature": 0.0, "avg_logprob": -0.1972062998804553, "compression_ratio": 1.5480769230769231, "no_speech_prob": 0.00017561264394316822}, {"id": 155, "seek": 95488, "start": 954.88, "end": 960.04, "text": " graph analysis approach, Tim just explained. So, for each object file, we have the call", "tokens": [4295, 5215, 3109, 11, 7172, 445, 8825, 13, 407, 11, 337, 1184, 2657, 3991, 11, 321, 362, 264, 818], "temperature": 0.0, "avg_logprob": -0.14187055752601152, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.00013303651940077543}, {"id": 156, "seek": 95488, "start": 960.04, "end": 966.8, "text": " graph analysis and then the embedding. And then, on the runtime side, we can sort of", "tokens": [4295, 5215, 293, 550, 264, 12240, 3584, 13, 400, 550, 11, 322, 264, 34474, 1252, 11, 321, 393, 1333, 295], "temperature": 0.0, "avg_logprob": -0.14187055752601152, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.00013303651940077543}, {"id": 157, "seek": 95488, "start": 966.8, "end": 973.04, "text": " merge the main executable call graph with the shared libraries as they are loaded. And", "tokens": [22183, 264, 2135, 7568, 712, 818, 4295, 365, 264, 5507, 15148, 382, 436, 366, 13210, 13, 400], "temperature": 0.0, "avg_logprob": -0.14187055752601152, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.00013303651940077543}, {"id": 158, "seek": 95488, "start": 973.04, "end": 979.32, "text": " we defer the instrumentation using a dynamic instrumentation approach in order to sort", "tokens": [321, 25704, 264, 7198, 399, 1228, 257, 8546, 7198, 399, 3109, 294, 1668, 281, 1333], "temperature": 0.0, "avg_logprob": -0.14187055752601152, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.00013303651940077543}, {"id": 159, "seek": 97932, "start": 979.32, "end": 990.12, "text": " of apply that selection dynamically. And, yeah, that's how it works. So, to summarize,", "tokens": [295, 3079, 300, 9450, 43492, 13, 400, 11, 1338, 11, 300, 311, 577, 309, 1985, 13, 407, 11, 281, 20858, 11], "temperature": 0.0, "avg_logprob": -0.17950011789798737, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.00011459628149168566}, {"id": 160, "seek": 97932, "start": 990.12, "end": 997.5200000000001, "text": " we are developing the Karpi tool for instrumentation selection based on call graph analysis. And", "tokens": [321, 366, 6416, 264, 591, 6529, 72, 2290, 337, 7198, 399, 9450, 2361, 322, 818, 4295, 5215, 13, 400], "temperature": 0.0, "avg_logprob": -0.17950011789798737, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.00011459628149168566}, {"id": 161, "seek": 97932, "start": 997.5200000000001, "end": 1004.5600000000001, "text": " we have explored this cage plugin for generating this call graph information at link time,", "tokens": [321, 362, 24016, 341, 17302, 23407, 337, 17746, 341, 818, 4295, 1589, 412, 2113, 565, 11], "temperature": 0.0, "avg_logprob": -0.17950011789798737, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.00011459628149168566}, {"id": 162, "seek": 100456, "start": 1004.56, "end": 1010.4399999999999, "text": " which allows whole program visibility and this dynamic documentation. And together", "tokens": [597, 4045, 1379, 1461, 19883, 293, 341, 8546, 14333, 13, 400, 1214], "temperature": 0.0, "avg_logprob": -0.1356546792639307, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00010598220251267776}, {"id": 163, "seek": 100456, "start": 1010.4399999999999, "end": 1017.1199999999999, "text": " with Karpi, we can, yeah, use the embedded call graph to run the selection at runtime", "tokens": [365, 591, 6529, 72, 11, 321, 393, 11, 1338, 11, 764, 264, 16741, 818, 4295, 281, 1190, 264, 9450, 412, 34474], "temperature": 0.0, "avg_logprob": -0.1356546792639307, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00010598220251267776}, {"id": 164, "seek": 100456, "start": 1017.1199999999999, "end": 1027.76, "text": " and thereby improving Karpi to make the compilation process and the analysis process more streamlined.", "tokens": [293, 28281, 11470, 591, 6529, 72, 281, 652, 264, 40261, 1399, 293, 264, 5215, 1399, 544, 48155, 13], "temperature": 0.0, "avg_logprob": -0.1356546792639307, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00010598220251267776}, {"id": 165, "seek": 100456, "start": 1027.76, "end": 1033.6399999999999, "text": " And this is sort of an active development. So, at this point, we don't have a very detailed", "tokens": [400, 341, 307, 1333, 295, 364, 4967, 3250, 13, 407, 11, 412, 341, 935, 11, 321, 500, 380, 362, 257, 588, 9942], "temperature": 0.0, "avg_logprob": -0.1356546792639307, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00010598220251267776}, {"id": 166, "seek": 103364, "start": 1033.64, "end": 1040.0, "text": " evaluation about performance and stuff like that. So, there's some concerns, for example,", "tokens": [13344, 466, 3389, 293, 1507, 411, 300, 13, 407, 11, 456, 311, 512, 7389, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.15203767582990121, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.00030096579575911164}, {"id": 167, "seek": 103364, "start": 1040.0, "end": 1047.24, "text": " when you go to very big programs and do LTO, there might be performance problems. So, there", "tokens": [562, 291, 352, 281, 588, 955, 4268, 293, 360, 441, 15427, 11, 456, 1062, 312, 3389, 2740, 13, 407, 11, 456], "temperature": 0.0, "avg_logprob": -0.15203767582990121, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.00030096579575911164}, {"id": 168, "seek": 103364, "start": 1047.24, "end": 1057.72, "text": " might be more work to make it viable in that regard. But, yeah, it works well in a prototype", "tokens": [1062, 312, 544, 589, 281, 652, 309, 22024, 294, 300, 3843, 13, 583, 11, 1338, 11, 309, 1985, 731, 294, 257, 19475], "temperature": 0.0, "avg_logprob": -0.15203767582990121, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.00030096579575911164}, {"id": 169, "seek": 105772, "start": 1057.72, "end": 1064.72, "text": " fashion. Yeah. Thank you.", "tokens": [6700, 13, 865, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3508083192925704, "compression_ratio": 1.1441441441441442, "no_speech_prob": 0.001909976708702743}, {"id": 170, "seek": 105772, "start": 1064.72, "end": 1073.24, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.3508083192925704, "compression_ratio": 1.1441441441441442, "no_speech_prob": 0.001909976708702743}, {"id": 171, "seek": 105772, "start": 1073.24, "end": 1087.32, "text": " Perhaps I was a bit distracted. So, I have two questions. If you can comment on Lambda", "tokens": [10517, 286, 390, 257, 857, 21658, 13, 407, 11, 286, 362, 732, 1651, 13, 759, 291, 393, 2871, 322, 45691], "temperature": 0.0, "avg_logprob": -0.3508083192925704, "compression_ratio": 1.1441441441441442, "no_speech_prob": 0.001909976708702743}, {"id": 172, "seek": 108732, "start": 1087.32, "end": 1099.1599999999999, "text": " functions, OMP sections, or OMP sections of the code, not specific constructs, and instruction", "tokens": [6828, 11, 422, 12224, 10863, 11, 420, 422, 12224, 10863, 295, 264, 3089, 11, 406, 2685, 7690, 82, 11, 293, 10951], "temperature": 0.0, "avg_logprob": -0.20455100570899853, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.003560928860679269}, {"id": 173, "seek": 108732, "start": 1099.1599999999999, "end": 1107.36, "text": " cache. If you can comment how those are handled, those three aspects, let's say.", "tokens": [19459, 13, 759, 291, 393, 2871, 577, 729, 366, 18033, 11, 729, 1045, 7270, 11, 718, 311, 584, 13], "temperature": 0.0, "avg_logprob": -0.20455100570899853, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.003560928860679269}, {"id": 174, "seek": 108732, "start": 1107.36, "end": 1114.28, "text": " So when it comes to OpenMP, we don't at the moment have any specific OpenMP profiling", "tokens": [407, 562, 309, 1487, 281, 7238, 12224, 11, 321, 500, 380, 412, 264, 1623, 362, 604, 2685, 7238, 12224, 1740, 4883], "temperature": 0.0, "avg_logprob": -0.20455100570899853, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.003560928860679269}, {"id": 175, "seek": 111428, "start": 1114.28, "end": 1121.44, "text": " interface that we target. So, this might be something that is probably useful in the future.", "tokens": [9226, 300, 321, 3779, 13, 407, 11, 341, 1062, 312, 746, 300, 307, 1391, 4420, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.2514570484990659, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.0002590170770417899}, {"id": 176, "seek": 111428, "start": 1121.44, "end": 1129.36, "text": " But for now, we just do the basic functions mutation and select on that. Yeah. Then the", "tokens": [583, 337, 586, 11, 321, 445, 360, 264, 3875, 6828, 27960, 293, 3048, 322, 300, 13, 865, 13, 1396, 264], "temperature": 0.0, "avg_logprob": -0.2514570484990659, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.0002590170770417899}, {"id": 177, "seek": 111428, "start": 1129.36, "end": 1135.72, "text": " other point was, sorry, could you repeat?", "tokens": [661, 935, 390, 11, 2597, 11, 727, 291, 7149, 30], "temperature": 0.0, "avg_logprob": -0.2514570484990659, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.0002590170770417899}, {"id": 178, "seek": 111428, "start": 1135.72, "end": 1139.48, "text": " So, do you want to comment on that?", "tokens": [407, 11, 360, 291, 528, 281, 2871, 322, 300, 30], "temperature": 0.0, "avg_logprob": -0.2514570484990659, "compression_ratio": 1.4576271186440677, "no_speech_prob": 0.0002590170770417899}, {"id": 179, "seek": 113948, "start": 1139.48, "end": 1145.84, "text": " So, lambdas and caches, so caching, no, there is no logic to handle caching in any way.", "tokens": [407, 11, 10097, 27476, 293, 269, 13272, 11, 370, 269, 2834, 11, 572, 11, 456, 307, 572, 9952, 281, 4813, 269, 2834, 294, 604, 636, 13], "temperature": 0.0, "avg_logprob": -0.14768615845711, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0001625655422685668}, {"id": 180, "seek": 113948, "start": 1145.84, "end": 1151.2, "text": " And regarding lambdas and OpenMP, it's, if you're talking about profiling, you've got", "tokens": [400, 8595, 10097, 27476, 293, 7238, 12224, 11, 309, 311, 11, 498, 291, 434, 1417, 466, 1740, 4883, 11, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.14768615845711, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0001625655422685668}, {"id": 181, "seek": 113948, "start": 1151.2, "end": 1156.1200000000001, "text": " your answer right there. But if you're talking about generating call graphs in which lambdas", "tokens": [428, 1867, 558, 456, 13, 583, 498, 291, 434, 1417, 466, 17746, 818, 24877, 294, 597, 10097, 27476], "temperature": 0.0, "avg_logprob": -0.14768615845711, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0001625655422685668}, {"id": 182, "seek": 113948, "start": 1156.1200000000001, "end": 1163.68, "text": " and OpenMP runtime calls are available, then the call graph will actually figure out that", "tokens": [293, 7238, 12224, 34474, 5498, 366, 2435, 11, 550, 264, 818, 4295, 486, 767, 2573, 484, 300], "temperature": 0.0, "avg_logprob": -0.14768615845711, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.0001625655422685668}, {"id": 183, "seek": 116368, "start": 1163.68, "end": 1172.8, "text": " there are OpenMP runtime calls, and will correctly, if I remember correctly, will figure out that", "tokens": [456, 366, 7238, 12224, 34474, 5498, 11, 293, 486, 8944, 11, 498, 286, 1604, 8944, 11, 486, 2573, 484, 300], "temperature": 0.0, "avg_logprob": -0.13764939989362443, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.00010693757940316573}, {"id": 184, "seek": 116368, "start": 1172.8, "end": 1178.4, "text": " this function calls back to the OpenMP runtime, because once you are in IR, the runtimes actually", "tokens": [341, 2445, 5498, 646, 281, 264, 7238, 12224, 34474, 11, 570, 1564, 291, 366, 294, 16486, 11, 264, 49435, 1532, 767], "temperature": 0.0, "avg_logprob": -0.13764939989362443, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.00010693757940316573}, {"id": 185, "seek": 116368, "start": 1178.4, "end": 1183.8, "text": " carved out, all the pragmas were removed from the actual source code. So, we are aware", "tokens": [28613, 484, 11, 439, 264, 33394, 3799, 645, 7261, 490, 264, 3539, 4009, 3089, 13, 407, 11, 321, 366, 3650], "temperature": 0.0, "avg_logprob": -0.13764939989362443, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.00010693757940316573}, {"id": 186, "seek": 116368, "start": 1183.8, "end": 1188.68, "text": " of OpenMP, but we are not using that information currently for any profiling. But you could", "tokens": [295, 7238, 12224, 11, 457, 321, 366, 406, 1228, 300, 1589, 4362, 337, 604, 1740, 4883, 13, 583, 291, 727], "temperature": 0.0, "avg_logprob": -0.13764939989362443, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.00010693757940316573}, {"id": 187, "seek": 118868, "start": 1188.68, "end": 1195.3200000000002, "text": " do metadata-based copy selection with it. So, every call path that eventually leads", "tokens": [360, 26603, 12, 6032, 5055, 9450, 365, 309, 13, 407, 11, 633, 818, 3100, 300, 4728, 6689], "temperature": 0.0, "avg_logprob": -0.18552525450543658, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.0012739954981952906}, {"id": 188, "seek": 118868, "start": 1195.3200000000002, "end": 1203.0800000000002, "text": " to an OpenMP runtime call would be a valid instrumentation using copy.", "tokens": [281, 364, 7238, 12224, 34474, 818, 576, 312, 257, 7363, 7198, 399, 1228, 5055, 13], "temperature": 0.0, "avg_logprob": -0.18552525450543658, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.0012739954981952906}, {"id": 189, "seek": 118868, "start": 1203.0800000000002, "end": 1208.3600000000001, "text": " Just that the question of the instruction cache is whether, this is my ignorance, but", "tokens": [1449, 300, 264, 1168, 295, 264, 10951, 19459, 307, 1968, 11, 341, 307, 452, 25390, 11, 457], "temperature": 0.0, "avg_logprob": -0.18552525450543658, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.0012739954981952906}, {"id": 190, "seek": 118868, "start": 1208.3600000000001, "end": 1214.64, "text": " if you are reintroducing a lot of new instructions here in the code, or in the code that is being", "tokens": [498, 291, 366, 319, 38132, 2175, 257, 688, 295, 777, 9415, 510, 294, 264, 3089, 11, 420, 294, 264, 3089, 300, 307, 885], "temperature": 0.0, "avg_logprob": -0.18552525450543658, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.0012739954981952906}, {"id": 191, "seek": 121464, "start": 1214.64, "end": 1222.2800000000002, "text": " read, I was thinking whether too much data ends up, maybe I didn't understand well.", "tokens": [1401, 11, 286, 390, 1953, 1968, 886, 709, 1412, 5314, 493, 11, 1310, 286, 994, 380, 1223, 731, 13], "temperature": 0.0, "avg_logprob": -0.2037643623352051, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.00037435663398355246}, {"id": 192, "seek": 121464, "start": 1222.2800000000002, "end": 1227.5200000000002, "text": " So, this is more of a performance-related question, right? Okay, so, yes, of course,", "tokens": [407, 11, 341, 307, 544, 295, 257, 3389, 12, 12004, 1168, 11, 558, 30, 1033, 11, 370, 11, 2086, 11, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.2037643623352051, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.00037435663398355246}, {"id": 193, "seek": 121464, "start": 1227.5200000000002, "end": 1231.8400000000001, "text": " you introduce a new instruction whenever a shared library is loaded, because we then", "tokens": [291, 5366, 257, 777, 10951, 5699, 257, 5507, 6405, 307, 13210, 11, 570, 321, 550], "temperature": 0.0, "avg_logprob": -0.2037643623352051, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.00037435663398355246}, {"id": 194, "seek": 121464, "start": 1231.8400000000001, "end": 1237.8400000000001, "text": " add instructions that pass the call graph back to our runtime collecting facility, and", "tokens": [909, 9415, 300, 1320, 264, 818, 4295, 646, 281, 527, 34474, 12510, 8973, 11, 293], "temperature": 0.0, "avg_logprob": -0.2037643623352051, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.00037435663398355246}, {"id": 195, "seek": 121464, "start": 1237.8400000000001, "end": 1242.92, "text": " we also introduce instructions because we are using a profiling approach, which are function", "tokens": [321, 611, 5366, 9415, 570, 321, 366, 1228, 257, 1740, 4883, 3109, 11, 597, 366, 2445], "temperature": 0.0, "avg_logprob": -0.2037643623352051, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.00037435663398355246}, {"id": 196, "seek": 124292, "start": 1242.92, "end": 1248.2, "text": " calls. So, yes, we are impeding the instruction fetching, instruction caching flow, which", "tokens": [5498, 13, 407, 11, 2086, 11, 321, 366, 704, 9794, 264, 10951, 23673, 278, 11, 10951, 269, 2834, 3095, 11, 597], "temperature": 0.0, "avg_logprob": -0.15534970068162487, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.00021767006546724588}, {"id": 197, "seek": 124292, "start": 1248.2, "end": 1254.0, "text": " is also why profiling has rather high overhead compared to sampling approaches, for example.", "tokens": [307, 611, 983, 1740, 4883, 575, 2831, 1090, 19922, 5347, 281, 21179, 11587, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15534970068162487, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.00021767006546724588}, {"id": 198, "seek": 124292, "start": 1254.0, "end": 1260.8400000000001, "text": " But as Sebastian told, we have not really extensively profiled our application, quite", "tokens": [583, 382, 31102, 1907, 11, 321, 362, 406, 534, 32636, 1740, 7292, 527, 3861, 11, 1596], "temperature": 0.0, "avg_logprob": -0.15534970068162487, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.00021767006546724588}, {"id": 199, "seek": 126084, "start": 1260.84, "end": 1273.9599999999998, "text": " ironic, so we are not aware how much the benefit or the impact actually would be.", "tokens": [33719, 11, 370, 321, 366, 406, 3650, 577, 709, 264, 5121, 420, 264, 2712, 767, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.11096664788066476, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0003153582802042365}, {"id": 200, "seek": 126084, "start": 1273.9599999999998, "end": 1280.28, "text": " So in your slide, you say you have a fork of LLD. The obvious question is, what's stopping", "tokens": [407, 294, 428, 4137, 11, 291, 584, 291, 362, 257, 17716, 295, 441, 23704, 13, 440, 6322, 1168, 307, 11, 437, 311, 12767], "temperature": 0.0, "avg_logprob": -0.11096664788066476, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0003153582802042365}, {"id": 201, "seek": 126084, "start": 1280.28, "end": 1286.28, "text": " you from upstreaming this? So, yes, we have a fork of LLD that just basically", "tokens": [291, 490, 33915, 278, 341, 30, 407, 11, 2086, 11, 321, 362, 257, 17716, 295, 441, 23704, 300, 445, 1936], "temperature": 0.0, "avg_logprob": -0.11096664788066476, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0003153582802042365}, {"id": 202, "seek": 128628, "start": 1286.28, "end": 1294.08, "text": " exposes the flag load new pass manager plugin, and you pass the plugin, and it does the rest.", "tokens": [1278, 4201, 264, 7166, 3677, 777, 1320, 6598, 23407, 11, 293, 291, 1320, 264, 23407, 11, 293, 309, 775, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.15865855746799046, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.00020273978589102626}, {"id": 203, "seek": 128628, "start": 1294.08, "end": 1299.28, "text": " What's currently holding us back from upstreaming is it's not very well developed, it was coupled", "tokens": [708, 311, 4362, 5061, 505, 646, 490, 33915, 278, 307, 309, 311, 406, 588, 731, 4743, 11, 309, 390, 29482], "temperature": 0.0, "avg_logprob": -0.15865855746799046, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.00020273978589102626}, {"id": 204, "seek": 128628, "start": 1299.28, "end": 1304.72, "text": " together in half a week or something, and there already is an open merge request on", "tokens": [1214, 294, 1922, 257, 1243, 420, 746, 11, 293, 456, 1217, 307, 364, 1269, 22183, 5308, 322], "temperature": 0.0, "avg_logprob": -0.15865855746799046, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.00020273978589102626}, {"id": 205, "seek": 128628, "start": 1304.72, "end": 1312.32, "text": " fabricator that implements this exact functionality for, if I remember correctly, LLDM9, which", "tokens": [7253, 1639, 300, 704, 17988, 341, 1900, 14980, 337, 11, 498, 286, 1604, 8944, 11, 441, 23704, 44, 24, 11, 597], "temperature": 0.0, "avg_logprob": -0.15865855746799046, "compression_ratio": 1.5352697095435686, "no_speech_prob": 0.00020273978589102626}, {"id": 206, "seek": 131232, "start": 1312.32, "end": 1323.0, "text": " was abandoned for a year until it was totally abandoned and closed, and so we didn't actually", "tokens": [390, 13732, 337, 257, 1064, 1826, 309, 390, 3879, 13732, 293, 5395, 11, 293, 370, 321, 994, 380, 767], "temperature": 0.0, "avg_logprob": -0.1646446485197946, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0006529558449983597}, {"id": 207, "seek": 131232, "start": 1323.0, "end": 1329.28, "text": " figure out how to make this more interesting. Don't take that as a signal. People just move", "tokens": [2573, 484, 577, 281, 652, 341, 544, 1880, 13, 1468, 380, 747, 300, 382, 257, 6358, 13, 3432, 445, 1286], "temperature": 0.0, "avg_logprob": -0.1646446485197946, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0006529558449983597}, {"id": 208, "seek": 131232, "start": 1329.28, "end": 1334.4399999999998, "text": " jobs or whatever, so try it again and bash people, and I can help you with that as well,", "tokens": [4782, 420, 2035, 11, 370, 853, 309, 797, 293, 46183, 561, 11, 293, 286, 393, 854, 291, 365, 300, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.1646446485197946, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0006529558449983597}, {"id": 209, "seek": 131232, "start": 1334.4399999999998, "end": 1338.28, "text": " find the right people to get this, because it seems like a simple and obvious thing to", "tokens": [915, 264, 558, 561, 281, 483, 341, 11, 570, 309, 2544, 411, 257, 2199, 293, 6322, 551, 281], "temperature": 0.0, "avg_logprob": -0.1646446485197946, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0006529558449983597}, {"id": 210, "seek": 133828, "start": 1338.28, "end": 1344.6399999999999, "text": " have. It isn't actually that hard. Apparently, there wasn't much interest in the community", "tokens": [362, 13, 467, 1943, 380, 767, 300, 1152, 13, 16755, 11, 456, 2067, 380, 709, 1179, 294, 264, 1768], "temperature": 0.0, "avg_logprob": -0.2916918992996216, "compression_ratio": 1.2816901408450705, "no_speech_prob": 0.0018729810835793614}, {"id": 211, "seek": 133828, "start": 1344.6399999999999, "end": 1357.0, "text": " back in 2020? Yes, that's too long ago. We will polish it a little much, and then hopefully", "tokens": [646, 294, 4808, 30, 1079, 11, 300, 311, 886, 938, 2057, 13, 492, 486, 20452, 309, 257, 707, 709, 11, 293, 550, 4696], "temperature": 0.0, "avg_logprob": -0.2916918992996216, "compression_ratio": 1.2816901408450705, "no_speech_prob": 0.0018729810835793614}, {"id": 212, "seek": 135700, "start": 1357.0, "end": 1368.76, "text": " get this one upstream. Thanks. Any other questions? No. Okay. Thank you very much.", "tokens": [483, 341, 472, 33915, 13, 2561, 13, 2639, 661, 1651, 30, 883, 13, 1033, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.42807503541310626, "compression_ratio": 1.025, "no_speech_prob": 0.0013207944575697184}, {"id": 213, "seek": 136876, "start": 1368.76, "end": 1387.72, "text": " Thank you very much, Sebastian.", "tokens": [50364, 1044, 291, 588, 709, 11, 31102, 13, 51312], "temperature": 0.0, "avg_logprob": -0.8969367980957031, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.00022450349933933467}], "language": "en"}