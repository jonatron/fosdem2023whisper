{"text": " Thank you. Thank you for having us. We are really happy to be there with the Kotlin community and the open source community today. It's really, really great. Welcome to the intermission between the great talks today. I'm Fran\u00e7ois Nolen. I'm with Julien Bur\u00e9. We both work at SNCF Connect and Tech, SNCF being the French railway company. At first, we wanted to introduce TOC, TOC, the open conversation kit. So TOC, it's an open platform. Maybe we can score a bit. In case you don't understand me, everything is written there. It's an open platform written in Kotlin to build chatbots, voicebots, callbots, conversational agents, and natural language processing applications. We started building TOC in 2016 when we started building production-ready business to customer chatbots for SNCF, which means for millions of French people, or millions of people in France, I should say. It builds on open source libraries, such as, at this time it was Apache, OpenNLP, and Stanford CoreNLP. To build chatbots and be able to build these conversational services without having only data scientists and experts and developers, we wanted not only to have something wrapping and encapsulating the NLP libraries with user interfaces, but also a conversational framework, Kotlin DSL, to be able to build the answers, a bunch of connectors to be able to integrate with our websites, our mobile applications, the messaging platforms such as Messenger, WhatsApp, Twitter, Slack, and smart speakers such as Google Home, Alexa, and more. So the solution we've built from open source foundations, we've shared it with the community on GitHub the year after, and now it's used by several companies and universities in the field of energy, banking, transport, and even healthcare. But I say the initial idea was to introduce TOC to you, and as you know, recently everything has changed in the field of conversational AI. That's why we won't do the initial presentation, and if you're disappointed, please go to the TOC AI website, and this link, this presentation is the demo we wanted to make initially, and you'll see the multi-channel capabilities and the analytics and everything that is in TOC. Now today, we'll try to do something a little more interesting and fun, focusing on code, because most time when people come and see me to know about TOC, they want to know how to build chatbots without having to code, with no code, and I assume you're more interested in Kotlin. So let's stop talking about TOC. Julien, could you run a new stack and create a chatbot running Kotlin from scratch, please? We'll use OpenNLP or Stanford. It's possible to integrate with other NLP models using their API integrating them through their APIs. Initially, it was OpenNLP and CoreNLP from Stanford because Java implementations are available, so it was really convenient to embed that in Kotlin. After creating an application, we will add a connector. The simplest connector is a web connector. It just exposes an API and we'll use it to have a web page and talk to the chatbot. And that's already done. You have a chatbot. We can talk to him. Okay, that's interesting. So we have a chatbot up on your name, but like Jon Snow, he knows nothing. So can you do something about it, please? I'd like to be able to say hello to the newborn. So we're in the language understanding section of TOC Studio, the graphical user interface, and you've just created an intent, so you create intents and entities inside the sentences sometimes. We'll see that a bit later. So we are training a new model to understand a greetings intent, and I'd like to have an answer. So as we said, we'll use only Kotlin to build the answers. Obviously, you can go inside the user interface, TOC Studio, and build answers, configure answers graphically. But it's really fun to code everything. Oh, really nice. And okay, we've got a chatbot. And yes, I like it. I had this question. I wanted to ask the chatbot. Okay, so you did something. You actually did something, and the answer is always different, but I'm quite disappointed by the answer. So we could do something else. I'm sure chatGPT could answer. Okay. So you decided to code nothing, in fact, and delegate everything to chatGPT. So you're using a Kotlin client, which is available in GitHub, just to perform the request. And so as I ask you to have answers to anything, actually, you just code something to delegate everything to chatGPT. That's it. Exactly. Okay, great. Exactly what I was expecting. So it's a GPT 3.5 model, this one. I hope you can read. It's not too small. Okay. Nothing really interesting in terms of Kotlin code there. So it's just a client to the chatGPT API, and you wrap it into a talk story so that the chatbot, when it triggers the right intent, it runs the client, calls chatGPT, and will have a chatbot answering anything in minutes. So let's try it. Okay. You've just tried it programmatically before trying with the chatbot interface. And we've got an answer from chatGPT. That's Matt. Instead of defining a new story for chatGPT and having our model to be trained to detect some intent, then trigger this story, you are using the new story, the chatGPT delegation, as a fallback story, which means every time the chatbot doesn't detect any intent, we'll have the chatGPT answer. Next time. I can't wait. Okay. That's the answer for chatGPT. Thank you. Congrats. No, we have a chatbot capable of answering about anything because the chatGPT does. I have to, I have to, sorry, you have to remember that tomorrow we go back to Paris. So could you please find something to get back? Yes, the train schedules from Brussels to Paris. I'm sure chatGPT will have the answer. Okay. Links to websites. Actually, there is our website. Okay. We've got another answer out there. Okay. So chatGPT definitely has the answer. But the problem is every time it's a different answer. We've got a real timetable there. But are we sure that's for tomorrow morning? Maybe. Actually, we did a test yesterday. Two days ago, we had no answers, no departure dates. Yesterday, we started having departure dates. But we also had answers for June, for September. And it wasn't always for tomorrow morning. So as we can see, every time we ask chatGPT, we apparently get a different answer, which could be a problem because we have to go back to Paris tomorrow. And we would like to have some predictability. You can find the real departure date maybe on our website. Obviously, that's really impressive and interesting to have a chatbot capable of answering about any question. But sometimes, in particular, when you're a big company and you provide a conversational service to answer your customers, sometimes for some use cases, you would like to control the answer to be able to guarantee it's always the same answer. It's the answer from your database or your API. So differently, we would like sometimes to have some predictability. It depends on maybe. But there, for the train schedule, it would be useful. So what did you do? You just created a new intent, train travel. You added the notion of entities, which is you train the model to tell them this sentence is train travel search. And the terms Brussels, Paris, and tomorrow morning have to be detected as entities, the origin, the destination, and the departure date. So you've just trained a new intent. And now we should have a custom story, not chat GPT this time, using these entities and these variables to perform a real search. Using, for instance, the SNCF OpenData API. And get a precise data always the same answer to the customer. So using the talk DSL, it looks like this. We add a new story. I didn't precise. When you run the program, there's a small WebSocket client which connects to the chatbot you've just started at the beginning. And it adds stories to the chatbot. So there you are defining a new story. And what does it do? It takes from the original sentence the origin entity, the destination entity, the departure date entity, and hopefully it's not text, it's already a date time. So it's been recognized, detected by the model we train. And it was openNLP, if I remember. And then you just have to call your favorite API, get the data, do things, implement business rules or anything, and put the result in traditional conversational widgets like buttons, cards. So we'll see what you choose to answer. A carousel, yes, for the departure dates. It would be great. So it's a generic DSL and model of widgets. But you also have specific widgets. When you integrate to specific channels like WhatsApp or Messenger, you might want to use a specific widget for your answers on these channels. So you are building a carousel with cards, a card for each proposal, and you take the... Yes, you've used the entities to perform the request to the open data client. And from the return proposals, you just have to build cards. I can't wait to see the results. There seems to be a nice image. We've got a natural language detection issue, because we had a chat GPT answer there. As you may know, it takes time to train a model. Obviously, it's not... With two, three, four sentences, you get a performance model. Okay, here is our custom story with the real proposals to get back to Paris tomorrow. That's great. And what about going by airplane? So in minutes, we've created a chatbot who is mixing chat GPT answers. And still some training to do. And several custom stories when we want to control the results. Okay, I'd like, as a company in railway, I'd like to have a custom answer for these questions and point out it's not so good for the planet to take airplane when I can take train. So you can configure something really quickly. Okay, that should do it. Maybe you've got a nice graph to show that. So that's the part for people who don't like to code. So for all the static contents and it's possible to build static stories and decision trees without having to code. Okay, and maybe we... Last question to ask why it's not so good. Maybe to go back to chat GPT answers. If we have time, not so much. So that's absolutely not the demonstration we proposed to do at the beginning. And we haven't seen much of the talk features or even the Kotlin DSL. But it's something a bit different, a bit new. And obviously, as you know, the chat GPT progress and everything they do at OpenAPI, it's really impressive. And every one of you knows about it. And for the moment, for companies like us, it's still... It may be difficult to integrate question answering like chat GPT because depending on the use case, we might want to control, supervise having a supervised model and control the results. Nevertheless, it can be interesting to integrate with chat GPT and other similar models to be able to answer many, many things with, in fact, really few code and training and efforts. And that's what we wanted to show you and to demonstrate today. You can, in minutes, create a chatbot in Kotlin, running Kotlin for other developers. It's also possible to write stories in JavaScript or Python. But let's stick with the best language in the world. So in minutes, you can have a chatbot Kotlin. You can integrate with very powerful solutions like chat GPT for question answering and choose to program your own custom stories when it's required to control the results and have a kind of guarantee of predictability. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.56, "text": " Thank you. Thank you for having us. We are really happy to be there with the Kotlin community", "tokens": [50364, 1044, 291, 13, 1044, 291, 337, 1419, 505, 13, 492, 366, 534, 2055, 281, 312, 456, 365, 264, 30123, 5045, 1768, 51142], "temperature": 0.0, "avg_logprob": -0.2798910555632218, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.19192859530448914}, {"id": 1, "seek": 0, "start": 15.56, "end": 21.48, "text": " and the open source community today. It's really, really great. Welcome to the intermission", "tokens": [51142, 293, 264, 1269, 4009, 1768, 965, 13, 467, 311, 534, 11, 534, 869, 13, 4027, 281, 264, 728, 29797, 51438], "temperature": 0.0, "avg_logprob": -0.2798910555632218, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.19192859530448914}, {"id": 2, "seek": 2148, "start": 21.48, "end": 29.12, "text": " between the great talks today. I'm Fran\u00e7ois Nolen. I'm with Julien Bur\u00e9. We both work", "tokens": [50364, 1296, 264, 869, 6686, 965, 13, 286, 478, 1526, 12368, 7376, 426, 11940, 13, 286, 478, 365, 7174, 1053, 7031, 526, 13, 492, 1293, 589, 50746], "temperature": 0.0, "avg_logprob": -0.2659640734708762, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.07607606798410416}, {"id": 3, "seek": 2148, "start": 29.12, "end": 37.120000000000005, "text": " at SNCF Connect and Tech, SNCF being the French railway company. At first, we wanted", "tokens": [50746, 412, 13955, 34, 37, 11653, 293, 13795, 11, 13955, 34, 37, 885, 264, 5522, 25812, 2237, 13, 1711, 700, 11, 321, 1415, 51146], "temperature": 0.0, "avg_logprob": -0.2659640734708762, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.07607606798410416}, {"id": 4, "seek": 2148, "start": 37.120000000000005, "end": 44.760000000000005, "text": " to introduce TOC, TOC, the open conversation kit. So TOC, it's an open platform. Maybe", "tokens": [51146, 281, 5366, 8232, 34, 11, 8232, 34, 11, 264, 1269, 3761, 8260, 13, 407, 8232, 34, 11, 309, 311, 364, 1269, 3663, 13, 2704, 51528], "temperature": 0.0, "avg_logprob": -0.2659640734708762, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.07607606798410416}, {"id": 5, "seek": 4476, "start": 44.76, "end": 55.28, "text": " we can score a bit. In case you don't understand me, everything is written there. It's an", "tokens": [50364, 321, 393, 6175, 257, 857, 13, 682, 1389, 291, 500, 380, 1223, 385, 11, 1203, 307, 3720, 456, 13, 467, 311, 364, 50890], "temperature": 0.0, "avg_logprob": -0.15394551413399832, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.13172733783721924}, {"id": 6, "seek": 4476, "start": 55.28, "end": 63.2, "text": " open platform written in Kotlin to build chatbots, voicebots, callbots, conversational agents,", "tokens": [50890, 1269, 3663, 3720, 294, 30123, 5045, 281, 1322, 5081, 65, 1971, 11, 3177, 65, 1971, 11, 818, 65, 1971, 11, 2615, 1478, 12554, 11, 51286], "temperature": 0.0, "avg_logprob": -0.15394551413399832, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.13172733783721924}, {"id": 7, "seek": 4476, "start": 63.2, "end": 72.0, "text": " and natural language processing applications. We started building TOC in 2016 when we started", "tokens": [51286, 293, 3303, 2856, 9007, 5821, 13, 492, 1409, 2390, 8232, 34, 294, 6549, 562, 321, 1409, 51726], "temperature": 0.0, "avg_logprob": -0.15394551413399832, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.13172733783721924}, {"id": 8, "seek": 7200, "start": 72.44, "end": 77.84, "text": " building production-ready business to customer chatbots for SNCF, which means for millions", "tokens": [50386, 2390, 4265, 12, 1201, 1606, 281, 5474, 5081, 65, 1971, 337, 13955, 34, 37, 11, 597, 1355, 337, 6803, 50656], "temperature": 0.0, "avg_logprob": -0.24158222610886032, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.02658979967236519}, {"id": 9, "seek": 7200, "start": 77.84, "end": 90.44, "text": " of French people, or millions of people in France, I should say. It builds on open source", "tokens": [50656, 295, 5522, 561, 11, 420, 6803, 295, 561, 294, 6190, 11, 286, 820, 584, 13, 467, 15182, 322, 1269, 4009, 51286], "temperature": 0.0, "avg_logprob": -0.24158222610886032, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.02658979967236519}, {"id": 10, "seek": 7200, "start": 90.44, "end": 101.44, "text": " libraries, such as, at this time it was Apache, OpenNLP, and Stanford CoreNLP. To build chatbots", "tokens": [51286, 15148, 11, 1270, 382, 11, 412, 341, 565, 309, 390, 46597, 11, 7238, 45, 45196, 11, 293, 20374, 14798, 45, 45196, 13, 1407, 1322, 5081, 65, 1971, 51836], "temperature": 0.0, "avg_logprob": -0.24158222610886032, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.02658979967236519}, {"id": 11, "seek": 10144, "start": 101.48, "end": 108.16, "text": " and be able to build these conversational services without having only data scientists", "tokens": [50366, 293, 312, 1075, 281, 1322, 613, 2615, 1478, 3328, 1553, 1419, 787, 1412, 7708, 50700], "temperature": 0.0, "avg_logprob": -0.20185090755594187, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.010532578453421593}, {"id": 12, "seek": 10144, "start": 108.16, "end": 115.16, "text": " and experts and developers, we wanted not only to have something wrapping and encapsulating", "tokens": [50700, 293, 8572, 293, 8849, 11, 321, 1415, 406, 787, 281, 362, 746, 21993, 293, 38745, 12162, 51050], "temperature": 0.0, "avg_logprob": -0.20185090755594187, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.010532578453421593}, {"id": 13, "seek": 10144, "start": 115.16, "end": 123.08, "text": " the NLP libraries with user interfaces, but also a conversational framework, Kotlin DSL,", "tokens": [51050, 264, 426, 45196, 15148, 365, 4195, 28416, 11, 457, 611, 257, 2615, 1478, 8388, 11, 30123, 5045, 15816, 43, 11, 51446], "temperature": 0.0, "avg_logprob": -0.20185090755594187, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.010532578453421593}, {"id": 14, "seek": 12308, "start": 123.08, "end": 131.76, "text": " to be able to build the answers, a bunch of connectors to be able to integrate with our", "tokens": [50364, 281, 312, 1075, 281, 1322, 264, 6338, 11, 257, 3840, 295, 31865, 281, 312, 1075, 281, 13365, 365, 527, 50798], "temperature": 0.0, "avg_logprob": -0.2567507523756761, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.13296297192573547}, {"id": 15, "seek": 12308, "start": 131.76, "end": 139.12, "text": " websites, our mobile applications, the messaging platforms such as Messenger, WhatsApp, Twitter,", "tokens": [50798, 12891, 11, 527, 6013, 5821, 11, 264, 21812, 9473, 1270, 382, 34226, 11, 30513, 11, 5794, 11, 51166], "temperature": 0.0, "avg_logprob": -0.2567507523756761, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.13296297192573547}, {"id": 16, "seek": 12308, "start": 139.12, "end": 147.4, "text": " Slack, and smart speakers such as Google Home, Alexa, and more. So the solution we've built", "tokens": [51166, 37211, 11, 293, 4069, 9518, 1270, 382, 3329, 8719, 11, 22595, 11, 293, 544, 13, 407, 264, 3827, 321, 600, 3094, 51580], "temperature": 0.0, "avg_logprob": -0.2567507523756761, "compression_ratio": 1.4526315789473685, "no_speech_prob": 0.13296297192573547}, {"id": 17, "seek": 14740, "start": 147.4, "end": 154.4, "text": " from open source foundations, we've shared it with the community on GitHub the year after,", "tokens": [50364, 490, 1269, 4009, 22467, 11, 321, 600, 5507, 309, 365, 264, 1768, 322, 23331, 264, 1064, 934, 11, 50714], "temperature": 0.0, "avg_logprob": -0.25538697838783264, "compression_ratio": 1.4202127659574468, "no_speech_prob": 0.039942748844623566}, {"id": 18, "seek": 14740, "start": 156.24, "end": 162.84, "text": " and now it's used by several companies and universities in the field of energy, banking,", "tokens": [50806, 293, 586, 309, 311, 1143, 538, 2940, 3431, 293, 11779, 294, 264, 2519, 295, 2281, 11, 18261, 11, 51136], "temperature": 0.0, "avg_logprob": -0.25538697838783264, "compression_ratio": 1.4202127659574468, "no_speech_prob": 0.039942748844623566}, {"id": 19, "seek": 14740, "start": 162.84, "end": 169.84, "text": " transport, and even healthcare. But I say the initial idea was to introduce TOC to you,", "tokens": [51136, 5495, 11, 293, 754, 8884, 13, 583, 286, 584, 264, 5883, 1558, 390, 281, 5366, 8232, 34, 281, 291, 11, 51486], "temperature": 0.0, "avg_logprob": -0.25538697838783264, "compression_ratio": 1.4202127659574468, "no_speech_prob": 0.039942748844623566}, {"id": 20, "seek": 16984, "start": 170.84, "end": 179.84, "text": " and as you know, recently everything has changed in the field of conversational AI. That's why we", "tokens": [50414, 293, 382, 291, 458, 11, 3938, 1203, 575, 3105, 294, 264, 2519, 295, 2615, 1478, 7318, 13, 663, 311, 983, 321, 50864], "temperature": 0.0, "avg_logprob": -0.21711333254550366, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.04823648929595947}, {"id": 21, "seek": 16984, "start": 179.84, "end": 185.84, "text": " won't do the initial presentation, and if you're disappointed, please go to the TOC AI website,", "tokens": [50864, 1582, 380, 360, 264, 5883, 5860, 11, 293, 498, 291, 434, 13856, 11, 1767, 352, 281, 264, 8232, 34, 7318, 3144, 11, 51164], "temperature": 0.0, "avg_logprob": -0.21711333254550366, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.04823648929595947}, {"id": 22, "seek": 16984, "start": 185.84, "end": 192.84, "text": " and this link, this presentation is the demo we wanted to make initially, and you'll see", "tokens": [51164, 293, 341, 2113, 11, 341, 5860, 307, 264, 10723, 321, 1415, 281, 652, 9105, 11, 293, 291, 603, 536, 51514], "temperature": 0.0, "avg_logprob": -0.21711333254550366, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.04823648929595947}, {"id": 23, "seek": 16984, "start": 192.84, "end": 198.84, "text": " the multi-channel capabilities and the analytics and everything that is in TOC. Now today, we'll", "tokens": [51514, 264, 4825, 12, 339, 11444, 10862, 293, 264, 15370, 293, 1203, 300, 307, 294, 8232, 34, 13, 823, 965, 11, 321, 603, 51814], "temperature": 0.0, "avg_logprob": -0.21711333254550366, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.04823648929595947}, {"id": 24, "seek": 19884, "start": 198.84, "end": 205.84, "text": " try to do something a little more interesting and fun, focusing on code, because most time when", "tokens": [50364, 853, 281, 360, 746, 257, 707, 544, 1880, 293, 1019, 11, 8416, 322, 3089, 11, 570, 881, 565, 562, 50714], "temperature": 0.0, "avg_logprob": -0.10774787484783016, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.010759484954178333}, {"id": 25, "seek": 19884, "start": 205.84, "end": 210.84, "text": " people come and see me to know about TOC, they want to know how to build chatbots without having", "tokens": [50714, 561, 808, 293, 536, 385, 281, 458, 466, 8232, 34, 11, 436, 528, 281, 458, 577, 281, 1322, 5081, 65, 1971, 1553, 1419, 50964], "temperature": 0.0, "avg_logprob": -0.10774787484783016, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.010759484954178333}, {"id": 26, "seek": 19884, "start": 210.84, "end": 219.84, "text": " to code, with no code, and I assume you're more interested in Kotlin. So let's stop talking", "tokens": [50964, 281, 3089, 11, 365, 572, 3089, 11, 293, 286, 6552, 291, 434, 544, 3102, 294, 30123, 5045, 13, 407, 718, 311, 1590, 1417, 51414], "temperature": 0.0, "avg_logprob": -0.10774787484783016, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.010759484954178333}, {"id": 27, "seek": 21984, "start": 220.84, "end": 230.84, "text": " about TOC. Julien, could you run a new stack and create a chatbot running Kotlin from scratch, please?", "tokens": [50414, 466, 8232, 34, 13, 7174, 1053, 11, 727, 291, 1190, 257, 777, 8630, 293, 1884, 257, 5081, 18870, 2614, 30123, 5045, 490, 8459, 11, 1767, 30, 50914], "temperature": 0.0, "avg_logprob": -0.21252418447423865, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.03289087116718292}, {"id": 28, "seek": 21984, "start": 238.84, "end": 247.84, "text": " We'll use OpenNLP or Stanford. It's possible to integrate with other NLP models using their API", "tokens": [51314, 492, 603, 764, 7238, 45, 45196, 420, 20374, 13, 467, 311, 1944, 281, 13365, 365, 661, 426, 45196, 5245, 1228, 641, 9362, 51764], "temperature": 0.0, "avg_logprob": -0.21252418447423865, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.03289087116718292}, {"id": 29, "seek": 24784, "start": 247.84, "end": 254.84, "text": " integrating them through their APIs. Initially, it was OpenNLP and CoreNLP from Stanford because", "tokens": [50364, 26889, 552, 807, 641, 21445, 13, 29446, 11, 309, 390, 7238, 45, 45196, 293, 14798, 45, 45196, 490, 20374, 570, 50714], "temperature": 0.0, "avg_logprob": -0.14182959029923625, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.010657173581421375}, {"id": 30, "seek": 24784, "start": 254.84, "end": 264.84000000000003, "text": " Java implementations are available, so it was really convenient to embed that in Kotlin. After", "tokens": [50714, 10745, 4445, 763, 366, 2435, 11, 370, 309, 390, 534, 10851, 281, 12240, 300, 294, 30123, 5045, 13, 2381, 51214], "temperature": 0.0, "avg_logprob": -0.14182959029923625, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.010657173581421375}, {"id": 31, "seek": 24784, "start": 264.84000000000003, "end": 270.84000000000003, "text": " creating an application, we will add a connector. The simplest connector is a web connector. It just", "tokens": [51214, 4084, 364, 3861, 11, 321, 486, 909, 257, 19127, 13, 440, 22811, 19127, 307, 257, 3670, 19127, 13, 467, 445, 51514], "temperature": 0.0, "avg_logprob": -0.14182959029923625, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.010657173581421375}, {"id": 32, "seek": 27084, "start": 271.84, "end": 278.84, "text": " exposes an API and we'll use it to have a web page and talk to the chatbot. And that's already", "tokens": [50414, 1278, 4201, 364, 9362, 293, 321, 603, 764, 309, 281, 362, 257, 3670, 3028, 293, 751, 281, 264, 5081, 18870, 13, 400, 300, 311, 1217, 50764], "temperature": 0.0, "avg_logprob": -0.15502103169759116, "compression_ratio": 1.4274809160305344, "no_speech_prob": 0.03213910385966301}, {"id": 33, "seek": 27084, "start": 278.84, "end": 286.84, "text": " done. You have a chatbot. We can talk to him. Okay, that's interesting. So we have a chatbot", "tokens": [50764, 1096, 13, 509, 362, 257, 5081, 18870, 13, 492, 393, 751, 281, 796, 13, 1033, 11, 300, 311, 1880, 13, 407, 321, 362, 257, 5081, 18870, 51164], "temperature": 0.0, "avg_logprob": -0.15502103169759116, "compression_ratio": 1.4274809160305344, "no_speech_prob": 0.03213910385966301}, {"id": 34, "seek": 28684, "start": 287.84, "end": 295.84, "text": " up on your name, but like Jon Snow, he knows nothing. So can you do something about it, please? I'd", "tokens": [50414, 493, 322, 428, 1315, 11, 457, 411, 7745, 14827, 11, 415, 3255, 1825, 13, 407, 393, 291, 360, 746, 466, 309, 11, 1767, 30, 286, 1116, 50814], "temperature": 0.0, "avg_logprob": -0.24581815050793931, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.020125772804021835}, {"id": 35, "seek": 28684, "start": 295.84, "end": 305.84, "text": " like to be able to say hello to the newborn. So we're in the language understanding section of TOC", "tokens": [50814, 411, 281, 312, 1075, 281, 584, 7751, 281, 264, 32928, 13, 407, 321, 434, 294, 264, 2856, 3701, 3541, 295, 8232, 34, 51314], "temperature": 0.0, "avg_logprob": -0.24581815050793931, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.020125772804021835}, {"id": 36, "seek": 28684, "start": 305.84, "end": 313.84, "text": " Studio, the graphical user interface, and you've just created an intent, so you create intents and", "tokens": [51314, 13500, 11, 264, 35942, 4195, 9226, 11, 293, 291, 600, 445, 2942, 364, 8446, 11, 370, 291, 1884, 560, 791, 293, 51714], "temperature": 0.0, "avg_logprob": -0.24581815050793931, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.020125772804021835}, {"id": 37, "seek": 31384, "start": 314.84, "end": 322.84, "text": " entities inside the sentences sometimes. We'll see that a bit later. So we are training a new", "tokens": [50414, 16667, 1854, 264, 16579, 2171, 13, 492, 603, 536, 300, 257, 857, 1780, 13, 407, 321, 366, 3097, 257, 777, 50814], "temperature": 0.0, "avg_logprob": -0.1083746592203776, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00425300607457757}, {"id": 38, "seek": 31384, "start": 322.84, "end": 332.84, "text": " model to understand a greetings intent, and I'd like to have an answer. So as we said, we'll use", "tokens": [50814, 2316, 281, 1223, 257, 33667, 8446, 11, 293, 286, 1116, 411, 281, 362, 364, 1867, 13, 407, 382, 321, 848, 11, 321, 603, 764, 51314], "temperature": 0.0, "avg_logprob": -0.1083746592203776, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00425300607457757}, {"id": 39, "seek": 31384, "start": 332.84, "end": 339.84, "text": " only Kotlin to build the answers. Obviously, you can go inside the user interface, TOC Studio, and", "tokens": [51314, 787, 30123, 5045, 281, 1322, 264, 6338, 13, 7580, 11, 291, 393, 352, 1854, 264, 4195, 9226, 11, 8232, 34, 13500, 11, 293, 51664], "temperature": 0.0, "avg_logprob": -0.1083746592203776, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00425300607457757}, {"id": 40, "seek": 33984, "start": 339.84, "end": 355.84, "text": " build answers, configure answers graphically. But it's really fun to code everything. Oh, really nice.", "tokens": [50364, 1322, 6338, 11, 22162, 6338, 4295, 984, 13, 583, 309, 311, 534, 1019, 281, 3089, 1203, 13, 876, 11, 534, 1481, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20260407374455378, "compression_ratio": 1.146067415730337, "no_speech_prob": 0.009365469217300415}, {"id": 41, "seek": 35584, "start": 355.84, "end": 366.84, "text": " And okay, we've got a chatbot. And yes, I like it. I had this question. I wanted to ask the chatbot.", "tokens": [50364, 400, 1392, 11, 321, 600, 658, 257, 5081, 18870, 13, 400, 2086, 11, 286, 411, 309, 13, 286, 632, 341, 1168, 13, 286, 1415, 281, 1029, 264, 5081, 18870, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19885225953726932, "compression_ratio": 1.408450704225352, "no_speech_prob": 0.08169795572757721}, {"id": 42, "seek": 35584, "start": 366.84, "end": 374.84, "text": " Okay, so you did something. You actually did something, and the answer is always different, but I'm", "tokens": [50914, 1033, 11, 370, 291, 630, 746, 13, 509, 767, 630, 746, 11, 293, 264, 1867, 307, 1009, 819, 11, 457, 286, 478, 51314], "temperature": 0.0, "avg_logprob": -0.19885225953726932, "compression_ratio": 1.408450704225352, "no_speech_prob": 0.08169795572757721}, {"id": 43, "seek": 37484, "start": 374.84, "end": 383.84, "text": " quite disappointed by the answer. So we could do something else. I'm sure chatGPT could answer.", "tokens": [50364, 1596, 13856, 538, 264, 1867, 13, 407, 321, 727, 360, 746, 1646, 13, 286, 478, 988, 5081, 38, 47, 51, 727, 1867, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21099272480717413, "compression_ratio": 1.1046511627906976, "no_speech_prob": 0.058883871883153915}, {"id": 44, "seek": 38384, "start": 383.84, "end": 402.84, "text": " Okay. So you decided to code nothing, in fact, and delegate everything to chatGPT. So you're using", "tokens": [50364, 1033, 13, 407, 291, 3047, 281, 3089, 1825, 11, 294, 1186, 11, 293, 40999, 1203, 281, 5081, 38, 47, 51, 13, 407, 291, 434, 1228, 51314], "temperature": 0.0, "avg_logprob": -0.21641760739413174, "compression_ratio": 1.304635761589404, "no_speech_prob": 0.018636053428053856}, {"id": 45, "seek": 38384, "start": 402.84, "end": 412.84, "text": " a Kotlin client, which is available in GitHub, just to perform the request. And so as I ask you to", "tokens": [51314, 257, 30123, 5045, 6423, 11, 597, 307, 2435, 294, 23331, 11, 445, 281, 2042, 264, 5308, 13, 400, 370, 382, 286, 1029, 291, 281, 51814], "temperature": 0.0, "avg_logprob": -0.21641760739413174, "compression_ratio": 1.304635761589404, "no_speech_prob": 0.018636053428053856}, {"id": 46, "seek": 41284, "start": 412.84, "end": 422.84, "text": " have answers to anything, actually, you just code something to delegate everything to chatGPT. That's it.", "tokens": [50364, 362, 6338, 281, 1340, 11, 767, 11, 291, 445, 3089, 746, 281, 40999, 1203, 281, 5081, 38, 47, 51, 13, 663, 311, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11858208690370832, "compression_ratio": 1.3150684931506849, "no_speech_prob": 0.03435007110238075}, {"id": 47, "seek": 41284, "start": 422.84, "end": 441.84, "text": " Exactly. Okay, great. Exactly what I was expecting. So it's a GPT 3.5 model, this one.", "tokens": [50864, 7587, 13, 1033, 11, 869, 13, 7587, 437, 286, 390, 9650, 13, 407, 309, 311, 257, 26039, 51, 805, 13, 20, 2316, 11, 341, 472, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11858208690370832, "compression_ratio": 1.3150684931506849, "no_speech_prob": 0.03435007110238075}, {"id": 48, "seek": 44184, "start": 441.84, "end": 456.84, "text": " I hope you can read. It's not too small. Okay. Nothing really interesting in terms of Kotlin code there.", "tokens": [50364, 286, 1454, 291, 393, 1401, 13, 467, 311, 406, 886, 1359, 13, 1033, 13, 6693, 534, 1880, 294, 2115, 295, 30123, 5045, 3089, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.142311901881777, "compression_ratio": 1.1428571428571428, "no_speech_prob": 0.032398927956819534}, {"id": 49, "seek": 45684, "start": 456.84, "end": 468.84, "text": " So it's just a client to the chatGPT API, and you wrap it into a talk story so that the chatbot, when it triggers the", "tokens": [50364, 407, 309, 311, 445, 257, 6423, 281, 264, 5081, 38, 47, 51, 9362, 11, 293, 291, 7019, 309, 666, 257, 751, 1657, 370, 300, 264, 5081, 18870, 11, 562, 309, 22827, 264, 50964], "temperature": 0.0, "avg_logprob": -0.22051665090745495, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.37717658281326294}, {"id": 50, "seek": 45684, "start": 468.84, "end": 483.84, "text": " right intent, it runs the client, calls chatGPT, and will have a chatbot answering anything in minutes.", "tokens": [50964, 558, 8446, 11, 309, 6676, 264, 6423, 11, 5498, 5081, 38, 47, 51, 11, 293, 486, 362, 257, 5081, 18870, 13430, 1340, 294, 2077, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22051665090745495, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.37717658281326294}, {"id": 51, "seek": 48684, "start": 486.84, "end": 494.84, "text": " So let's try it.", "tokens": [50414, 407, 718, 311, 853, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.41087905565897626, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.30626317858695984}, {"id": 52, "seek": 51684, "start": 516.84, "end": 540.84, "text": " Okay. You've just tried it programmatically before trying with the chatbot interface. And we've got an answer from chatGPT.", "tokens": [50364, 1033, 13, 509, 600, 445, 3031, 309, 37648, 5030, 949, 1382, 365, 264, 5081, 18870, 9226, 13, 400, 321, 600, 658, 364, 1867, 490, 5081, 38, 47, 51, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19723945675474225, "compression_ratio": 1.1181818181818182, "no_speech_prob": 0.022225139662623405}, {"id": 53, "seek": 54084, "start": 540.84, "end": 551.84, "text": " That's Matt. Instead of defining a new story for chatGPT and having our model to be trained to detect some intent, then", "tokens": [50364, 663, 311, 7397, 13, 7156, 295, 17827, 257, 777, 1657, 337, 5081, 38, 47, 51, 293, 1419, 527, 2316, 281, 312, 8895, 281, 5531, 512, 8446, 11, 550, 50914], "temperature": 0.0, "avg_logprob": -0.15259255935896687, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.023550553247332573}, {"id": 54, "seek": 54084, "start": 551.84, "end": 563.84, "text": " trigger this story, you are using the new story, the chatGPT delegation, as a fallback story, which means every time the chatbot doesn't", "tokens": [50914, 7875, 341, 1657, 11, 291, 366, 1228, 264, 777, 1657, 11, 264, 5081, 38, 47, 51, 36602, 11, 382, 257, 2100, 3207, 1657, 11, 597, 1355, 633, 565, 264, 5081, 18870, 1177, 380, 51514], "temperature": 0.0, "avg_logprob": -0.15259255935896687, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.023550553247332573}, {"id": 55, "seek": 56384, "start": 563.84, "end": 568.84, "text": " detect any intent, we'll have the chatGPT answer.", "tokens": [50364, 5531, 604, 8446, 11, 321, 603, 362, 264, 5081, 38, 47, 51, 1867, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1887661616007487, "compression_ratio": 1.2110091743119267, "no_speech_prob": 0.05658213794231415}, {"id": 56, "seek": 56384, "start": 577.84, "end": 587.84, "text": " Next time. I can't wait. Okay. That's the answer for chatGPT. Thank you. Congrats.", "tokens": [51064, 3087, 565, 13, 286, 393, 380, 1699, 13, 1033, 13, 663, 311, 264, 1867, 337, 5081, 38, 47, 51, 13, 1044, 291, 13, 40219, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1887661616007487, "compression_ratio": 1.2110091743119267, "no_speech_prob": 0.05658213794231415}, {"id": 57, "seek": 58784, "start": 587.84, "end": 600.84, "text": " No, we have a chatbot capable of answering about anything because the chatGPT does. I have to, I have to, sorry, you have to", "tokens": [50364, 883, 11, 321, 362, 257, 5081, 18870, 8189, 295, 13430, 466, 1340, 570, 264, 5081, 38, 47, 51, 775, 13, 286, 362, 281, 11, 286, 362, 281, 11, 2597, 11, 291, 362, 281, 51014], "temperature": 0.0, "avg_logprob": -0.23036669194698334, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.033355552703142166}, {"id": 58, "seek": 58784, "start": 600.84, "end": 609.84, "text": " remember that tomorrow we go back to Paris. So could you please find something to get back? Yes, the train schedules from", "tokens": [51014, 1604, 300, 4153, 321, 352, 646, 281, 8380, 13, 407, 727, 291, 1767, 915, 746, 281, 483, 646, 30, 1079, 11, 264, 3847, 28078, 490, 51464], "temperature": 0.0, "avg_logprob": -0.23036669194698334, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.033355552703142166}, {"id": 59, "seek": 60984, "start": 609.84, "end": 627.84, "text": " Brussels to Paris. I'm sure chatGPT will have the answer. Okay. Links to websites. Actually, there is our website.", "tokens": [50364, 38717, 281, 8380, 13, 286, 478, 988, 5081, 38, 47, 51, 486, 362, 264, 1867, 13, 1033, 13, 37156, 281, 12891, 13, 5135, 11, 456, 307, 527, 3144, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1444694490143747, "compression_ratio": 1.0961538461538463, "no_speech_prob": 0.0359770804643631}, {"id": 60, "seek": 62784, "start": 627.84, "end": 630.84, "text": " Okay. We've got another answer out there.", "tokens": [50364, 1033, 13, 492, 600, 658, 1071, 1867, 484, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.17131513357162476, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.013670703396201134}, {"id": 61, "seek": 62784, "start": 638.84, "end": 648.84, "text": " Okay. So chatGPT definitely has the answer. But the problem is every time it's a different answer.", "tokens": [50914, 1033, 13, 407, 5081, 38, 47, 51, 2138, 575, 264, 1867, 13, 583, 264, 1154, 307, 633, 565, 309, 311, 257, 819, 1867, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17131513357162476, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.013670703396201134}, {"id": 62, "seek": 64884, "start": 649.84, "end": 662.84, "text": " We've got a real timetable there. But are we sure that's for tomorrow morning? Maybe. Actually, we did a test yesterday. Two days", "tokens": [50414, 492, 600, 658, 257, 957, 524, 302, 712, 456, 13, 583, 366, 321, 988, 300, 311, 337, 4153, 2446, 30, 2704, 13, 5135, 11, 321, 630, 257, 1500, 5186, 13, 4453, 1708, 51064], "temperature": 0.0, "avg_logprob": -0.16104151710631356, "compression_ratio": 1.5153374233128833, "no_speech_prob": 0.06354998797178268}, {"id": 63, "seek": 64884, "start": 662.84, "end": 671.84, "text": " ago, we had no answers, no departure dates. Yesterday, we started having departure dates. But we also had answers for", "tokens": [51064, 2057, 11, 321, 632, 572, 6338, 11, 572, 25866, 11691, 13, 19765, 11, 321, 1409, 1419, 25866, 11691, 13, 583, 321, 611, 632, 6338, 337, 51514], "temperature": 0.0, "avg_logprob": -0.16104151710631356, "compression_ratio": 1.5153374233128833, "no_speech_prob": 0.06354998797178268}, {"id": 64, "seek": 67184, "start": 671.84, "end": 680.84, "text": " June, for September. And it wasn't always for tomorrow morning. So as we can see, every time we ask chatGPT, we", "tokens": [50364, 6928, 11, 337, 7216, 13, 400, 309, 2067, 380, 1009, 337, 4153, 2446, 13, 407, 382, 321, 393, 536, 11, 633, 565, 321, 1029, 5081, 38, 47, 51, 11, 321, 50814], "temperature": 0.0, "avg_logprob": -0.13715585585563414, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.056586965918540955}, {"id": 65, "seek": 67184, "start": 680.84, "end": 690.84, "text": " apparently get a different answer, which could be a problem because we have to go back to Paris tomorrow. And we would like to", "tokens": [50814, 7970, 483, 257, 819, 1867, 11, 597, 727, 312, 257, 1154, 570, 321, 362, 281, 352, 646, 281, 8380, 4153, 13, 400, 321, 576, 411, 281, 51314], "temperature": 0.0, "avg_logprob": -0.13715585585563414, "compression_ratio": 1.391812865497076, "no_speech_prob": 0.056586965918540955}, {"id": 66, "seek": 69084, "start": 690.84, "end": 705.84, "text": " have some predictability. You can find the real departure date maybe on our website. Obviously, that's really", "tokens": [50364, 362, 512, 6069, 2310, 13, 509, 393, 915, 264, 957, 25866, 4002, 1310, 322, 527, 3144, 13, 7580, 11, 300, 311, 534, 51114], "temperature": 0.0, "avg_logprob": -0.11257724559053461, "compression_ratio": 1.3653846153846154, "no_speech_prob": 0.08802464604377747}, {"id": 67, "seek": 69084, "start": 705.84, "end": 713.84, "text": " impressive and interesting to have a chatbot capable of answering about any question. But sometimes, in", "tokens": [51114, 8992, 293, 1880, 281, 362, 257, 5081, 18870, 8189, 295, 13430, 466, 604, 1168, 13, 583, 2171, 11, 294, 51514], "temperature": 0.0, "avg_logprob": -0.11257724559053461, "compression_ratio": 1.3653846153846154, "no_speech_prob": 0.08802464604377747}, {"id": 68, "seek": 71384, "start": 714.84, "end": 722.84, "text": " particular, when you're a big company and you provide a conversational service to answer your customers, sometimes for some", "tokens": [50414, 1729, 11, 562, 291, 434, 257, 955, 2237, 293, 291, 2893, 257, 2615, 1478, 2643, 281, 1867, 428, 4581, 11, 2171, 337, 512, 50814], "temperature": 0.0, "avg_logprob": -0.13720874786376952, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.15633338689804077}, {"id": 69, "seek": 71384, "start": 722.84, "end": 730.84, "text": " use cases, you would like to control the answer to be able to guarantee it's always the same answer. It's the answer", "tokens": [50814, 764, 3331, 11, 291, 576, 411, 281, 1969, 264, 1867, 281, 312, 1075, 281, 10815, 309, 311, 1009, 264, 912, 1867, 13, 467, 311, 264, 1867, 51214], "temperature": 0.0, "avg_logprob": -0.13720874786376952, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.15633338689804077}, {"id": 70, "seek": 71384, "start": 730.84, "end": 742.84, "text": " from your database or your API. So differently, we would like sometimes to have some predictability. It depends on", "tokens": [51214, 490, 428, 8149, 420, 428, 9362, 13, 407, 7614, 11, 321, 576, 411, 2171, 281, 362, 512, 6069, 2310, 13, 467, 5946, 322, 51814], "temperature": 0.0, "avg_logprob": -0.13720874786376952, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.15633338689804077}, {"id": 71, "seek": 74284, "start": 742.84, "end": 752.84, "text": " maybe. But there, for the train schedule, it would be useful. So what did you do? You just created a new intent,", "tokens": [50364, 1310, 13, 583, 456, 11, 337, 264, 3847, 7567, 11, 309, 576, 312, 4420, 13, 407, 437, 630, 291, 360, 30, 509, 445, 2942, 257, 777, 8446, 11, 50864], "temperature": 0.0, "avg_logprob": -0.21379382150215015, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0315035916864872}, {"id": 72, "seek": 74284, "start": 752.84, "end": 763.84, "text": " train travel. You added the notion of entities, which is you train the model to tell them this sentence is train", "tokens": [50864, 3847, 3147, 13, 509, 3869, 264, 10710, 295, 16667, 11, 597, 307, 291, 3847, 264, 2316, 281, 980, 552, 341, 8174, 307, 3847, 51414], "temperature": 0.0, "avg_logprob": -0.21379382150215015, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0315035916864872}, {"id": 73, "seek": 76384, "start": 763.84, "end": 774.84, "text": " travel search. And the terms Brussels, Paris, and tomorrow morning have to be detected as entities, the origin, the", "tokens": [50364, 3147, 3164, 13, 400, 264, 2115, 38717, 11, 8380, 11, 293, 4153, 2446, 362, 281, 312, 21896, 382, 16667, 11, 264, 4957, 11, 264, 50914], "temperature": 0.0, "avg_logprob": -0.15131319219415837, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.08126824349164963}, {"id": 74, "seek": 76384, "start": 774.84, "end": 784.84, "text": " destination, and the departure date. So you've just trained a new intent. And now we should have a custom story,", "tokens": [50914, 12236, 11, 293, 264, 25866, 4002, 13, 407, 291, 600, 445, 8895, 257, 777, 8446, 13, 400, 586, 321, 820, 362, 257, 2375, 1657, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15131319219415837, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.08126824349164963}, {"id": 75, "seek": 78484, "start": 784.84, "end": 796.84, "text": " not chat GPT this time, using these entities and these variables to perform a real search. Using, for instance, the", "tokens": [50364, 406, 5081, 26039, 51, 341, 565, 11, 1228, 613, 16667, 293, 613, 9102, 281, 2042, 257, 957, 3164, 13, 11142, 11, 337, 5197, 11, 264, 50964], "temperature": 0.0, "avg_logprob": -0.221488890100698, "compression_ratio": 1.3795180722891567, "no_speech_prob": 0.06532078236341476}, {"id": 76, "seek": 78484, "start": 796.84, "end": 812.84, "text": " SNCF OpenData API. And get a precise data always the same answer to the customer. So using the talk DSL, it looks", "tokens": [50964, 13955, 34, 37, 7238, 35, 3274, 9362, 13, 400, 483, 257, 13600, 1412, 1009, 264, 912, 1867, 281, 264, 5474, 13, 407, 1228, 264, 751, 15816, 43, 11, 309, 1542, 51764], "temperature": 0.0, "avg_logprob": -0.221488890100698, "compression_ratio": 1.3795180722891567, "no_speech_prob": 0.06532078236341476}, {"id": 77, "seek": 81284, "start": 812.84, "end": 823.84, "text": " like this. We add a new story. I didn't precise. When you run the program, there's a small WebSocket client which connects to the", "tokens": [50364, 411, 341, 13, 492, 909, 257, 777, 1657, 13, 286, 994, 380, 13600, 13, 1133, 291, 1190, 264, 1461, 11, 456, 311, 257, 1359, 9573, 50, 31380, 6423, 597, 16967, 281, 264, 50914], "temperature": 0.0, "avg_logprob": -0.2020629644393921, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.02225610986351967}, {"id": 78, "seek": 81284, "start": 823.84, "end": 834.84, "text": " chatbot you've just started at the beginning. And it adds stories to the chatbot. So there you are defining a new", "tokens": [50914, 5081, 18870, 291, 600, 445, 1409, 412, 264, 2863, 13, 400, 309, 10860, 3676, 281, 264, 5081, 18870, 13, 407, 456, 291, 366, 17827, 257, 777, 51464], "temperature": 0.0, "avg_logprob": -0.2020629644393921, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.02225610986351967}, {"id": 79, "seek": 83484, "start": 834.84, "end": 850.84, "text": " story. And what does it do? It takes from the original sentence the origin entity, the destination entity, the departure date", "tokens": [50364, 1657, 13, 400, 437, 775, 309, 360, 30, 467, 2516, 490, 264, 3380, 8174, 264, 4957, 13977, 11, 264, 12236, 13977, 11, 264, 25866, 4002, 51164], "temperature": 0.0, "avg_logprob": -0.15723678043910436, "compression_ratio": 1.5515151515151515, "no_speech_prob": 0.10836993902921677}, {"id": 80, "seek": 83484, "start": 850.84, "end": 863.84, "text": " entity, and hopefully it's not text, it's already a date time. So it's been recognized, detected by the model we train. And it was", "tokens": [51164, 13977, 11, 293, 4696, 309, 311, 406, 2487, 11, 309, 311, 1217, 257, 4002, 565, 13, 407, 309, 311, 668, 9823, 11, 21896, 538, 264, 2316, 321, 3847, 13, 400, 309, 390, 51814], "temperature": 0.0, "avg_logprob": -0.15723678043910436, "compression_ratio": 1.5515151515151515, "no_speech_prob": 0.10836993902921677}, {"id": 81, "seek": 86384, "start": 863.84, "end": 873.84, "text": " openNLP, if I remember. And then you just have to call your favorite API, get the data, do things, implement business rules or", "tokens": [50364, 1269, 45, 45196, 11, 498, 286, 1604, 13, 400, 550, 291, 445, 362, 281, 818, 428, 2954, 9362, 11, 483, 264, 1412, 11, 360, 721, 11, 4445, 1606, 4474, 420, 50864], "temperature": 0.0, "avg_logprob": -0.1486150218594459, "compression_ratio": 1.3956043956043955, "no_speech_prob": 0.1149015948176384}, {"id": 82, "seek": 86384, "start": 873.84, "end": 888.84, "text": " anything, and put the result in traditional conversational widgets like buttons, cards. So we'll see what you choose to answer.", "tokens": [50864, 1340, 11, 293, 829, 264, 1874, 294, 5164, 2615, 1478, 43355, 411, 9905, 11, 5632, 13, 407, 321, 603, 536, 437, 291, 2826, 281, 1867, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1486150218594459, "compression_ratio": 1.3956043956043955, "no_speech_prob": 0.1149015948176384}, {"id": 83, "seek": 88884, "start": 888.84, "end": 907.84, "text": " A carousel, yes, for the departure dates. It would be great. So it's a generic DSL and model of widgets. But you also have", "tokens": [50364, 316, 1032, 563, 338, 11, 2086, 11, 337, 264, 25866, 11691, 13, 467, 576, 312, 869, 13, 407, 309, 311, 257, 19577, 15816, 43, 293, 2316, 295, 43355, 13, 583, 291, 611, 362, 51314], "temperature": 0.0, "avg_logprob": -0.1881736437479655, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.12048051506280899}, {"id": 84, "seek": 88884, "start": 907.84, "end": 917.84, "text": " specific widgets. When you integrate to specific channels like WhatsApp or Messenger, you might want to use a specific", "tokens": [51314, 2685, 43355, 13, 1133, 291, 13365, 281, 2685, 9235, 411, 30513, 420, 34226, 11, 291, 1062, 528, 281, 764, 257, 2685, 51814], "temperature": 0.0, "avg_logprob": -0.1881736437479655, "compression_ratio": 1.409356725146199, "no_speech_prob": 0.12048051506280899}, {"id": 85, "seek": 91784, "start": 917.84, "end": 932.84, "text": " widget for your answers on these channels. So you are building a carousel with cards, a card for each proposal, and you take the...", "tokens": [50364, 34047, 337, 428, 6338, 322, 613, 9235, 13, 407, 291, 366, 2390, 257, 1032, 563, 338, 365, 5632, 11, 257, 2920, 337, 1184, 11494, 11, 293, 291, 747, 264, 485, 51114], "temperature": 0.0, "avg_logprob": -0.22550418972969055, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04486100375652313}, {"id": 86, "seek": 91784, "start": 932.84, "end": 943.84, "text": " Yes, you've used the entities to perform the request to the open data client. And from the return proposals, you just have to build", "tokens": [51114, 1079, 11, 291, 600, 1143, 264, 16667, 281, 2042, 264, 5308, 281, 264, 1269, 1412, 6423, 13, 400, 490, 264, 2736, 20198, 11, 291, 445, 362, 281, 1322, 51664], "temperature": 0.0, "avg_logprob": -0.22550418972969055, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.04486100375652313}, {"id": 87, "seek": 94384, "start": 944.84, "end": 952.84, "text": " cards. I can't wait to see the results. There seems to be a nice image.", "tokens": [50414, 5632, 13, 286, 393, 380, 1699, 281, 536, 264, 3542, 13, 821, 2544, 281, 312, 257, 1481, 3256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.32112165119337, "compression_ratio": 1.0, "no_speech_prob": 0.07753942906856537}, {"id": 88, "seek": 95284, "start": 952.84, "end": 973.84, "text": " We've got a natural language detection issue, because we had a chat GPT answer there.", "tokens": [50364, 492, 600, 658, 257, 3303, 2856, 17784, 2734, 11, 570, 321, 632, 257, 5081, 26039, 51, 1867, 456, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3746291036191194, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.12184227257966995}, {"id": 89, "seek": 97384, "start": 973.84, "end": 988.84, "text": " As you may know, it takes time to train a model. Obviously, it's not... With two, three, four sentences, you get a", "tokens": [50364, 1018, 291, 815, 458, 11, 309, 2516, 565, 281, 3847, 257, 2316, 13, 7580, 11, 309, 311, 406, 485, 2022, 732, 11, 1045, 11, 1451, 16579, 11, 291, 483, 257, 51114], "temperature": 0.0, "avg_logprob": -0.14968298463260427, "compression_ratio": 1.14, "no_speech_prob": 0.06344465166330338}, {"id": 90, "seek": 98884, "start": 989.84, "end": 1007.84, "text": " performance model. Okay, here is our custom story with the real proposals to get back to Paris tomorrow. That's great. And what about going by airplane?", "tokens": [50414, 3389, 2316, 13, 1033, 11, 510, 307, 527, 2375, 1657, 365, 264, 957, 20198, 281, 483, 646, 281, 8380, 4153, 13, 663, 311, 869, 13, 400, 437, 466, 516, 538, 17130, 30, 51314], "temperature": 0.0, "avg_logprob": -0.20633606115976968, "compression_ratio": 1.216, "no_speech_prob": 0.06303280591964722}, {"id": 91, "seek": 100784, "start": 1007.84, "end": 1031.8400000000001, "text": " So in minutes, we've created a chatbot who is mixing chat GPT answers. And still some training to do. And several custom stories when we want to", "tokens": [50364, 407, 294, 2077, 11, 321, 600, 2942, 257, 5081, 18870, 567, 307, 11983, 5081, 26039, 51, 6338, 13, 400, 920, 512, 3097, 281, 360, 13, 400, 2940, 2375, 3676, 562, 321, 528, 281, 51564], "temperature": 0.0, "avg_logprob": -0.2907101399189717, "compression_ratio": 1.2413793103448276, "no_speech_prob": 0.024473153054714203}, {"id": 92, "seek": 103184, "start": 1031.84, "end": 1054.84, "text": " control the results. Okay, I'd like, as a company in railway, I'd like to have a custom answer for these questions and point out it's not so good for the", "tokens": [50364, 1969, 264, 3542, 13, 1033, 11, 286, 1116, 411, 11, 382, 257, 2237, 294, 25812, 11, 286, 1116, 411, 281, 362, 257, 2375, 1867, 337, 613, 1651, 293, 935, 484, 309, 311, 406, 370, 665, 337, 264, 51514], "temperature": 0.0, "avg_logprob": -0.17872157910975014, "compression_ratio": 1.3304347826086957, "no_speech_prob": 0.04277139902114868}, {"id": 93, "seek": 105484, "start": 1054.84, "end": 1080.84, "text": " planet to take airplane when I can take train. So you can configure something really quickly. Okay, that should do it. Maybe you've got a", "tokens": [50364, 5054, 281, 747, 17130, 562, 286, 393, 747, 3847, 13, 407, 291, 393, 22162, 746, 534, 2661, 13, 1033, 11, 300, 820, 360, 309, 13, 2704, 291, 600, 658, 257, 51664], "temperature": 0.0, "avg_logprob": -0.18895600823795095, "compression_ratio": 1.2017543859649122, "no_speech_prob": 0.28284695744514465}, {"id": 94, "seek": 108084, "start": 1080.84, "end": 1094.84, "text": " nice graph to show that. So that's the part for people who don't like to code. So for all the static contents and it's possible to build", "tokens": [50364, 1481, 4295, 281, 855, 300, 13, 407, 300, 311, 264, 644, 337, 561, 567, 500, 380, 411, 281, 3089, 13, 407, 337, 439, 264, 13437, 15768, 293, 309, 311, 1944, 281, 1322, 51064], "temperature": 0.0, "avg_logprob": -0.13720185226864284, "compression_ratio": 1.3203883495145632, "no_speech_prob": 0.07392171025276184}, {"id": 95, "seek": 109484, "start": 1095.84, "end": 1119.84, "text": " static stories and decision trees without having to code. Okay, and maybe we... Last question to ask why it's not so good. Maybe to go back to chat GPT answers. If we have time, not so much. So that's absolutely not the", "tokens": [50414, 13437, 3676, 293, 3537, 5852, 1553, 1419, 281, 3089, 13, 1033, 11, 293, 1310, 321, 485, 5264, 1168, 281, 1029, 983, 309, 311, 406, 370, 665, 13, 2704, 281, 352, 646, 281, 5081, 26039, 51, 6338, 13, 759, 321, 362, 565, 11, 406, 370, 709, 13, 407, 300, 311, 3122, 406, 264, 51614], "temperature": 0.0, "avg_logprob": -0.1983354091644287, "compression_ratio": 1.3860759493670887, "no_speech_prob": 0.07229230552911758}, {"id": 96, "seek": 111984, "start": 1120.84, "end": 1141.84, "text": " demonstration we proposed to do at the beginning. And we haven't seen much of the talk features or even the Kotlin DSL. But it's something a bit different, a bit new. And obviously, as you know, the chat GPT progress and", "tokens": [50414, 16520, 321, 10348, 281, 360, 412, 264, 2863, 13, 400, 321, 2378, 380, 1612, 709, 295, 264, 751, 4122, 420, 754, 264, 30123, 5045, 15816, 43, 13, 583, 309, 311, 746, 257, 857, 819, 11, 257, 857, 777, 13, 400, 2745, 11, 382, 291, 458, 11, 264, 5081, 26039, 51, 4205, 293, 51464], "temperature": 0.0, "avg_logprob": -0.12521969420569284, "compression_ratio": 1.3496932515337423, "no_speech_prob": 0.13433755934238434}, {"id": 97, "seek": 114184, "start": 1142.84, "end": 1157.84, "text": " everything they do at OpenAPI, it's really impressive. And every one of you knows about it. And for the moment, for companies like us, it's still... It may be difficult to integrate question answering like", "tokens": [50414, 1203, 436, 360, 412, 7238, 4715, 40, 11, 309, 311, 534, 8992, 13, 400, 633, 472, 295, 291, 3255, 466, 309, 13, 400, 337, 264, 1623, 11, 337, 3431, 411, 505, 11, 309, 311, 920, 485, 467, 815, 312, 2252, 281, 13365, 1168, 13430, 411, 51164], "temperature": 0.0, "avg_logprob": -0.2134220162216498, "compression_ratio": 1.3576158940397351, "no_speech_prob": 0.21841156482696533}, {"id": 98, "seek": 115784, "start": 1158.84, "end": 1183.84, "text": " chat GPT because depending on the use case, we might want to control, supervise having a supervised model and control the results. Nevertheless, it can be interesting to integrate with chat GPT and other similar models to be able to answer many, many things with, in fact, really few code and training and", "tokens": [50414, 5081, 26039, 51, 570, 5413, 322, 264, 764, 1389, 11, 321, 1062, 528, 281, 1969, 11, 37971, 908, 1419, 257, 46533, 2316, 293, 1969, 264, 3542, 13, 26554, 11, 309, 393, 312, 1880, 281, 13365, 365, 5081, 26039, 51, 293, 661, 2531, 5245, 281, 312, 1075, 281, 1867, 867, 11, 867, 721, 365, 11, 294, 1186, 11, 534, 1326, 3089, 293, 3097, 293, 51664], "temperature": 0.0, "avg_logprob": -0.23042789857778975, "compression_ratio": 1.5803108808290156, "no_speech_prob": 0.15598952770233154}, {"id": 99, "seek": 118384, "start": 1183.84, "end": 1206.84, "text": " efforts. And that's what we wanted to show you and to demonstrate today. You can, in minutes, create a chatbot in Kotlin, running Kotlin for other developers. It's also possible to write stories in JavaScript or Python. But let's stick with the best language in", "tokens": [50364, 6484, 13, 400, 300, 311, 437, 321, 1415, 281, 855, 291, 293, 281, 11698, 965, 13, 509, 393, 11, 294, 2077, 11, 1884, 257, 5081, 18870, 294, 30123, 5045, 11, 2614, 30123, 5045, 337, 661, 8849, 13, 467, 311, 611, 1944, 281, 2464, 3676, 294, 15778, 420, 15329, 13, 583, 718, 311, 2897, 365, 264, 1151, 2856, 294, 51514], "temperature": 0.0, "avg_logprob": -0.14202887012112525, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.21645349264144897}, {"id": 100, "seek": 120684, "start": 1206.84, "end": 1229.84, "text": " the world. So in minutes, you can have a chatbot Kotlin. You can integrate with very powerful solutions like chat GPT for question answering and choose to program your own custom stories when it's required to control the results and have a kind of guarantee of predictability. Thank you.", "tokens": [50364, 264, 1002, 13, 407, 294, 2077, 11, 291, 393, 362, 257, 5081, 18870, 30123, 5045, 13, 509, 393, 13365, 365, 588, 4005, 6547, 411, 5081, 26039, 51, 337, 1168, 13430, 293, 2826, 281, 1461, 428, 1065, 2375, 3676, 562, 309, 311, 4739, 281, 1969, 264, 3542, 293, 362, 257, 733, 295, 10815, 295, 6069, 2310, 13, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1308018930496708, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.05164865031838417}, {"id": 101, "seek": 123684, "start": 1236.84, "end": 1237.84, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.4395948648452759, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9694907665252686}], "language": "en"}