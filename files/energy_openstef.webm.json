{"text": " Okay, awesome to see there's so many people here, really cool that there's a big interest in the energy topic. My name is Frederik Stool and I'm from Alliander which is a grid operator and I'll be talking about the open staff today. So first of all I put here in the graph this is a load profile, so the energy load somewhere in the grid and well you can see how it fluctuates over time, sometimes it's positive, sometimes it's negative, this means whether there's neto production or neto consumption. Now the question is, or you could ask, if we are at the red line right now, what will be the load in the future? And that's what we want to predict and if you're interested in that then you can use open staff because open staff means short-term energy forecasting. Okay, let's zoom out a bit first and before I go into a bit more detail about what open staff does, first I want to talk about, give a short introduction about why this is relevant. I don't have hours so I have to keep it short but there's a lot of to talk about here. But I want to start out with this picture and it's actually quite cool because I think the last presentation talked about flexible energy that consumers can use and this is one of the many things that are changing in the energy sector. So consumers have flexible products that also start producing, consumers also have solar panels, your local farmer might have a wind turbine somewhere, you have big wind parks on the sea, so there's all kinds of developments going on right now that make it harder for grid operators to forecast what's going to happen tomorrow or even the day after tomorrow. And I'll put this picture because all the things that I mentioned you can see right there and probably in the future it's only going to get harder and harder. For now I want to focus on the renewable energy part because it's also quite impactful. As you can see on this graph here this is for the Netherlands, the percentage of renewable energy production and you can see that in just a couple of years like five years it has more than doubled the electricity percentage that has been produced by renewable sources and renewable sources don't produce at a constant load of course, they change all the time depending on weather and this means it's harder to forecast and to put that into perspective I have another slide here and this is a typical consumption profile. If you have your local neighborhood then this is what the energy load will often look like. So you have the five peaks which means it's a peak for every day and in the weekend it's a bit lower, you can see the dips, these dips in the middle of the day that's because there's a couple of solar panels on some roofs you know, it still looks easy to predict. If you go to other places where there's way more renewable energy you can see these energy profiles change dramatically. So here you can see really a profile for a big solar park and you can see these huge negative peaks which on some days are there and some days they're not probably that's a cloudy day so there's no or less energy generated. And this is an energy profile for a big wind farm which you can see is well, seems hard to predict because there's seems no real, yeah so a negative energy means that the consumer or the customer I mean is giving back to the grid so then it's negative for us. If it's positive it means sorry, yeah exactly, so if it means a big negative peak it means the customer is producing a lot of energy. So it's just a convention you could also switch the sign but you have to choose one convention. Yeah power, yeah. Yeah so it's not just one side it's more like a general profile for like a substation for the grid operator but connected to it is a lot of solar. Yeah exactly so on all of these there's load and production but yeah I just wanted to share this feeling. So this can be difficult and this also leads to problems and this is two maps of the Netherlands and the colored areas are the areas where Alliander is currently active and on the left is energy consumption and on the right is energy production and this map shows if you're a new customer and you want to be connected to the grid if it's red it's probably difficult because there's no more room. According to the Dutch law the energy grid is full over there and you can see that this is for huge areas in the Netherlands and also large areas on the consumption side. And of course Alliander is doing everything they can to solve this by building new cables and new substations but this takes time, a lot of time and we don't have the time as you can see in the graph before. So I don't have that one but I assume it's very similar because we're not the only one who are having these issues. So how can we solve this? Well one important thing is that we need grid inside and therefore this also includes forecasts. So transmission forecasts and these are important for all three parts in the electrical grid so all three parties. So for customers, for DSOs such as Alliander and for TSOs such as Tenet which control the high voltage grid. Using these forecasts operators can try to maintain grid safety and grid balance and can give customers as much electricity as they want and as they need because the need is high. With these forecasts we can also enable smart solutions and I put here two brochure pictures of those solutions, one of them is a pilot FlexPower which was in Amsterdam which was about charging electrical vehicles and charging them faster if it's possible and not charging them as fast if it's not possible. We at Alliander supplied forecasts for this project and another platform is the GOPEX platform which is like a trading platform for electricity where customers can trade with operators to either consume or to produce energy flexibly and this is also being used right now at Alliander and we also provide a forecast for that. So it's no longer working so let's use it. So now let's talk about Opelstaff again because that's why I'm here and I'm going to give a short introduction to Opelstaff and then I'm just going to give a short demo about Opelstaff how you can make a forecast and also want to talk a bit about using Opelstaff in an operational setting. So first of all, the primary thing Opelstaff can help you with is that it's just a complete machine learning pipeline. So I'm just going to give a short list of what it can do. It handles input validation such as checking whether your data is complete. It has feature engineering so it automatically calculates for you lag features or other features that are based on input features. So for example, if you input it with wind speed, it can calculate wind turbine power output for you or the same for direct normal irradiance. Next it is some kind of intelligent train validation split of the time series. It has support for multiple type of regressors. So right now we have, for example, HGBoost which is at Allende the most commonly used but we also had a collaboration with Sonyo which added ProLove to Opelstaff and we also have support for probabilistic forecasts. So that means not just one line but quantiles. And unless it has integrated the model and artifact storage using MLflow. So what does this all mean then? Let's go to an actual demo. So I'm going to put this up here. That's a low resolution. Let's zoom out. It's a bit too much. Okay, so I'm just going to walk you through an example or how you could make a forecast. So first we need to make some kind of config object that's just what you have to feed Opelstaff. Let's close this. So let's run this line. Next I put some example input in this project. So we can load it and we can visualize it. So as you can see here, well, this is upon a stator frame and here we have the load and we have a lot of predictors. Well, some of these, well, the names should make sense. So for example, the amount of variation predicted by the KMI or the temperature, well, all these predictors are already in this example data. So if we have this, I can also plot it for you so you can see, well, this is another power profile. Okay, so now imagine you have this and you want to know, well, what's next? Then first we need to train a model. So Opelstaff has a train model pipeline which basically does all those things I just mentioned. So we can just call the pipeline and let's hope the live demo does not fill me. It will take about 15 seconds I think to train a model and store it. So you can see some info about what it's doing, well, and it's stored it. So let's have a look and we'll flow comes with an interface so we can directly see that we train a model. So right here, this was the run. Now let's hope this works. I see that my internet is no longer working so apparently this, then this figure this will work. All right, I'm not showing it. So, well, this is the MLflow interface and you can see that we just train a model. You can also click on the model or on the train run, this is just MLflow and you can see a bit more, well, information about what happened during the training. The next, of course, we want to make a prediction. So again, OpenStep has a pipeline for that so we can just say, okay, I want a prediction. So it's loading the model and using data to create a prediction and then we can visualize that as well. And then we have a graph right there. So this is the forecast that it made for the next, well, this was in some example data in 2021 but about 48 hours of forecast so that's OpenStep in practice. Let's go back to the presentation. Do this slide. Yeah, so, well, this flow has been a minimal flow but of course in reality, at least for if you're a grid operator, you want to do this in an operational setting. So this means that you want to do daily forecasts for a lot of different locations with all kinds of configurations. And OpenStep also comes with a so-called reference implementation about how you could do this. So this is a picture of what you would have to do so we have OpenStep right here which is basically, I just showed you the training and forecasting pipeline. Then we have another package which is called OpenStep DBC, database connector which can connect to a database. And we use MySQL and Influx DB to store all the data required to run it operationally. And we also have a Gafana dashboard built upon this database stack so we can also see what's going on. And again, as I already have shown, you can use MLflow to keep track of all the models and all the runs that are being done to see what's going on. So I want to show this dashboard as well. So this dashboard is just example data so it's not our real dashboard. But here you can for example see some load that was there on the system and you can also see that for example this is not just one area but it has a sum of two systems which is quite common in an electrical grid that you have a lot of measurement points that you have to add together with different signs. And you can see for example here's then a live forecast of this location as well. You can also see plots of the feature importance that obtained during training of the model. You can see on which data the model has been trained. Over here these plots are really small but here you can see them. So it's a dashboard where you can see everything that Oberstaff does for every location that you are could be interested in. You talk about forecast, are you within the forecast also taking other forecasts like the weather forecast into account or are you forecasting that yourself? No so we use all kinds of data and the weather forecast is, oh the question is whether we are using other forecasted data or whether we forecast, do those forecasts as well? Like whether we forecast the weather ourselves and the answer is that it depends a bit. So in general we use the weather forecasts for multiple sources and also for example price like the head pricing. So we use those data but sometimes we also feed the prediction itself or we feed one prediction into another prediction. So I mean you can play around with that but you have to feed Oberstaff with all the predictors that you wanted to know. Okay let's move on, I have one last slide and that's basically key information because that was my presentation so here I put all the info you might be interested in on this slide and if there are any questions then feel free to ask. So the information is really useful but what is the purpose for the net grid operator, what use does a grid operator make of this information, is it for congestion management, is it for some kind of load shedding, what is the role of this exercise? So I think the question is why would the grid operator be interested in forecast I guess is what you're asking. So there are many reasons but I think you already mentioned congestion management is indeed an important reason but also well grid insight. So the more congestion management is going to be used for the grid the more important it is also to maintain grid safety and grid safety is not just one operator, we are all connected to multiple grid operators so everyone has to communicate what they are going to do and what they expect the energy flow to be the next day so every operator can decide to do what's necessary to maintain grid safety. So that's what I mentioned before the transmission forecast, every operator has to communicate to everyone who is connected to what they expect the load to be on the next day. I see that my time is up so I'm afraid I have to answer the questions in the chat. Yeah or in the hallway, I think for time management we have to learn from these talks and see if we can manage to keep a couple of more minutes for questions, sorry. Thank you very much for listening.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.68, "text": " Okay, awesome to see there's so many people here, really cool that there's a big interest", "tokens": [1033, 11, 3476, 281, 536, 456, 311, 370, 867, 561, 510, 11, 534, 1627, 300, 456, 311, 257, 955, 1179], "temperature": 0.0, "avg_logprob": -0.33731956481933595, "compression_ratio": 1.3975903614457832, "no_speech_prob": 0.294596403837204}, {"id": 1, "seek": 0, "start": 12.68, "end": 15.8, "text": " in the energy topic.", "tokens": [294, 264, 2281, 4829, 13], "temperature": 0.0, "avg_logprob": -0.33731956481933595, "compression_ratio": 1.3975903614457832, "no_speech_prob": 0.294596403837204}, {"id": 2, "seek": 0, "start": 15.8, "end": 24.2, "text": " My name is Frederik Stool and I'm from Alliander which is a grid operator and I'll be talking", "tokens": [1222, 1315, 307, 27535, 1035, 745, 1092, 293, 286, 478, 490, 1057, 72, 4483, 597, 307, 257, 10748, 12973, 293, 286, 603, 312, 1417], "temperature": 0.0, "avg_logprob": -0.33731956481933595, "compression_ratio": 1.3975903614457832, "no_speech_prob": 0.294596403837204}, {"id": 3, "seek": 0, "start": 24.2, "end": 27.2, "text": " about the open staff today.", "tokens": [466, 264, 1269, 3525, 965, 13], "temperature": 0.0, "avg_logprob": -0.33731956481933595, "compression_ratio": 1.3975903614457832, "no_speech_prob": 0.294596403837204}, {"id": 4, "seek": 2720, "start": 27.2, "end": 37.44, "text": " So first of all I put here in the graph this is a load profile, so the energy load somewhere", "tokens": [407, 700, 295, 439, 286, 829, 510, 294, 264, 4295, 341, 307, 257, 3677, 7964, 11, 370, 264, 2281, 3677, 4079], "temperature": 0.0, "avg_logprob": -0.21934988961290958, "compression_ratio": 1.6242424242424243, "no_speech_prob": 0.00038335900171659887}, {"id": 5, "seek": 2720, "start": 37.44, "end": 45.04, "text": " in the grid and well you can see how it fluctuates over time, sometimes it's positive, sometimes", "tokens": [294, 264, 10748, 293, 731, 291, 393, 536, 577, 309, 23448, 27710, 670, 565, 11, 2171, 309, 311, 3353, 11, 2171], "temperature": 0.0, "avg_logprob": -0.21934988961290958, "compression_ratio": 1.6242424242424243, "no_speech_prob": 0.00038335900171659887}, {"id": 6, "seek": 2720, "start": 45.04, "end": 52.04, "text": " it's negative, this means whether there's neto production or neto consumption.", "tokens": [309, 311, 3671, 11, 341, 1355, 1968, 456, 311, 2533, 78, 4265, 420, 2533, 78, 12126, 13], "temperature": 0.0, "avg_logprob": -0.21934988961290958, "compression_ratio": 1.6242424242424243, "no_speech_prob": 0.00038335900171659887}, {"id": 7, "seek": 5204, "start": 52.04, "end": 58.92, "text": " Now the question is, or you could ask, if we are at the red line right now, what will", "tokens": [823, 264, 1168, 307, 11, 420, 291, 727, 1029, 11, 498, 321, 366, 412, 264, 2182, 1622, 558, 586, 11, 437, 486], "temperature": 0.0, "avg_logprob": -0.1796257700238909, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.597008981159888e-05}, {"id": 8, "seek": 5204, "start": 58.92, "end": 63.92, "text": " be the load in the future?", "tokens": [312, 264, 3677, 294, 264, 2027, 30], "temperature": 0.0, "avg_logprob": -0.1796257700238909, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.597008981159888e-05}, {"id": 9, "seek": 5204, "start": 63.92, "end": 69.36, "text": " And that's what we want to predict and if you're interested in that then you can use", "tokens": [400, 300, 311, 437, 321, 528, 281, 6069, 293, 498, 291, 434, 3102, 294, 300, 550, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.1796257700238909, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.597008981159888e-05}, {"id": 10, "seek": 5204, "start": 69.36, "end": 74.72, "text": " open staff because open staff means short-term energy forecasting.", "tokens": [1269, 3525, 570, 1269, 3525, 1355, 2099, 12, 7039, 2281, 44331, 13], "temperature": 0.0, "avg_logprob": -0.1796257700238909, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.597008981159888e-05}, {"id": 11, "seek": 7472, "start": 74.72, "end": 82.0, "text": " Okay, let's zoom out a bit first and before I go into a bit more detail about what open", "tokens": [1033, 11, 718, 311, 8863, 484, 257, 857, 700, 293, 949, 286, 352, 666, 257, 857, 544, 2607, 466, 437, 1269], "temperature": 0.0, "avg_logprob": -0.1637561735899552, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.00025398554862476885}, {"id": 12, "seek": 7472, "start": 82.0, "end": 89.92, "text": " staff does, first I want to talk about, give a short introduction about why this is relevant.", "tokens": [3525, 775, 11, 700, 286, 528, 281, 751, 466, 11, 976, 257, 2099, 9339, 466, 983, 341, 307, 7340, 13], "temperature": 0.0, "avg_logprob": -0.1637561735899552, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.00025398554862476885}, {"id": 13, "seek": 7472, "start": 89.92, "end": 97.68, "text": " I don't have hours so I have to keep it short but there's a lot of to talk about here.", "tokens": [286, 500, 380, 362, 2496, 370, 286, 362, 281, 1066, 309, 2099, 457, 456, 311, 257, 688, 295, 281, 751, 466, 510, 13], "temperature": 0.0, "avg_logprob": -0.1637561735899552, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.00025398554862476885}, {"id": 14, "seek": 7472, "start": 97.68, "end": 102.68, "text": " But I want to start out with this picture and it's actually quite cool because I think", "tokens": [583, 286, 528, 281, 722, 484, 365, 341, 3036, 293, 309, 311, 767, 1596, 1627, 570, 286, 519], "temperature": 0.0, "avg_logprob": -0.1637561735899552, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.00025398554862476885}, {"id": 15, "seek": 10268, "start": 102.68, "end": 111.76, "text": " the last presentation talked about flexible energy that consumers can use and this is", "tokens": [264, 1036, 5860, 2825, 466, 11358, 2281, 300, 11883, 393, 764, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.12794902745415182, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001448299444746226}, {"id": 16, "seek": 10268, "start": 111.76, "end": 116.4, "text": " one of the many things that are changing in the energy sector.", "tokens": [472, 295, 264, 867, 721, 300, 366, 4473, 294, 264, 2281, 6977, 13], "temperature": 0.0, "avg_logprob": -0.12794902745415182, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001448299444746226}, {"id": 17, "seek": 10268, "start": 116.4, "end": 123.12, "text": " So consumers have flexible products that also start producing, consumers also have solar", "tokens": [407, 11883, 362, 11358, 3383, 300, 611, 722, 10501, 11, 11883, 611, 362, 7936], "temperature": 0.0, "avg_logprob": -0.12794902745415182, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001448299444746226}, {"id": 18, "seek": 10268, "start": 123.12, "end": 129.04000000000002, "text": " panels, your local farmer might have a wind turbine somewhere, you have big wind parks", "tokens": [13419, 11, 428, 2654, 17891, 1062, 362, 257, 2468, 27536, 4079, 11, 291, 362, 955, 2468, 16213], "temperature": 0.0, "avg_logprob": -0.12794902745415182, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001448299444746226}, {"id": 19, "seek": 12904, "start": 129.04, "end": 135.68, "text": " on the sea, so there's all kinds of developments going on right now that make it harder for", "tokens": [322, 264, 4158, 11, 370, 456, 311, 439, 3685, 295, 20862, 516, 322, 558, 586, 300, 652, 309, 6081, 337], "temperature": 0.0, "avg_logprob": -0.14164455475345736, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00020194475655443966}, {"id": 20, "seek": 12904, "start": 135.68, "end": 144.88, "text": " grid operators to forecast what's going to happen tomorrow or even the day after tomorrow.", "tokens": [10748, 19077, 281, 14330, 437, 311, 516, 281, 1051, 4153, 420, 754, 264, 786, 934, 4153, 13], "temperature": 0.0, "avg_logprob": -0.14164455475345736, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00020194475655443966}, {"id": 21, "seek": 12904, "start": 144.88, "end": 152.04, "text": " And I'll put this picture because all the things that I mentioned you can see right", "tokens": [400, 286, 603, 829, 341, 3036, 570, 439, 264, 721, 300, 286, 2835, 291, 393, 536, 558], "temperature": 0.0, "avg_logprob": -0.14164455475345736, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.00020194475655443966}, {"id": 22, "seek": 15204, "start": 152.04, "end": 159.48, "text": " there and probably in the future it's only going to get harder and harder.", "tokens": [456, 293, 1391, 294, 264, 2027, 309, 311, 787, 516, 281, 483, 6081, 293, 6081, 13], "temperature": 0.0, "avg_logprob": -0.12029938758174076, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.00034229501034133136}, {"id": 23, "seek": 15204, "start": 159.48, "end": 167.48, "text": " For now I want to focus on the renewable energy part because it's also quite impactful.", "tokens": [1171, 586, 286, 528, 281, 1879, 322, 264, 20938, 2281, 644, 570, 309, 311, 611, 1596, 30842, 13], "temperature": 0.0, "avg_logprob": -0.12029938758174076, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.00034229501034133136}, {"id": 24, "seek": 15204, "start": 167.48, "end": 173.44, "text": " As you can see on this graph here this is for the Netherlands, the percentage of renewable", "tokens": [1018, 291, 393, 536, 322, 341, 4295, 510, 341, 307, 337, 264, 20873, 11, 264, 9668, 295, 20938], "temperature": 0.0, "avg_logprob": -0.12029938758174076, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.00034229501034133136}, {"id": 25, "seek": 15204, "start": 173.44, "end": 178.48, "text": " energy production and you can see that in just a couple of years like five years it", "tokens": [2281, 4265, 293, 291, 393, 536, 300, 294, 445, 257, 1916, 295, 924, 411, 1732, 924, 309], "temperature": 0.0, "avg_logprob": -0.12029938758174076, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.00034229501034133136}, {"id": 26, "seek": 17848, "start": 178.48, "end": 185.2, "text": " has more than doubled the electricity percentage that has been produced by renewable sources", "tokens": [575, 544, 813, 24405, 264, 10356, 9668, 300, 575, 668, 7126, 538, 20938, 7139], "temperature": 0.0, "avg_logprob": -0.1248977714114719, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00013629325258079916}, {"id": 27, "seek": 17848, "start": 185.2, "end": 190.04, "text": " and renewable sources don't produce at a constant load of course, they change all the", "tokens": [293, 20938, 7139, 500, 380, 5258, 412, 257, 5754, 3677, 295, 1164, 11, 436, 1319, 439, 264], "temperature": 0.0, "avg_logprob": -0.1248977714114719, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00013629325258079916}, {"id": 28, "seek": 17848, "start": 190.04, "end": 199.28, "text": " time depending on weather and this means it's harder to forecast and to put that into perspective", "tokens": [565, 5413, 322, 5503, 293, 341, 1355, 309, 311, 6081, 281, 14330, 293, 281, 829, 300, 666, 4585], "temperature": 0.0, "avg_logprob": -0.1248977714114719, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00013629325258079916}, {"id": 29, "seek": 17848, "start": 199.28, "end": 204.07999999999998, "text": " I have another slide here and this is a typical consumption profile.", "tokens": [286, 362, 1071, 4137, 510, 293, 341, 307, 257, 7476, 12126, 7964, 13], "temperature": 0.0, "avg_logprob": -0.1248977714114719, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00013629325258079916}, {"id": 30, "seek": 20408, "start": 204.08, "end": 209.88000000000002, "text": " If you have your local neighborhood then this is what the energy load will often look like.", "tokens": [759, 291, 362, 428, 2654, 7630, 550, 341, 307, 437, 264, 2281, 3677, 486, 2049, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.20037420060899522, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00022867147345095873}, {"id": 31, "seek": 20408, "start": 209.88000000000002, "end": 215.08, "text": " So you have the five peaks which means it's a peak for every day and in the weekend it's", "tokens": [407, 291, 362, 264, 1732, 26897, 597, 1355, 309, 311, 257, 10651, 337, 633, 786, 293, 294, 264, 6711, 309, 311], "temperature": 0.0, "avg_logprob": -0.20037420060899522, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00022867147345095873}, {"id": 32, "seek": 20408, "start": 215.08, "end": 221.88000000000002, "text": " a bit lower, you can see the dips, these dips in the middle of the day that's because there's", "tokens": [257, 857, 3126, 11, 291, 393, 536, 264, 47814, 11, 613, 47814, 294, 264, 2808, 295, 264, 786, 300, 311, 570, 456, 311], "temperature": 0.0, "avg_logprob": -0.20037420060899522, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00022867147345095873}, {"id": 33, "seek": 20408, "start": 221.88000000000002, "end": 229.0, "text": " a couple of solar panels on some roofs you know, it still looks easy to predict.", "tokens": [257, 1916, 295, 7936, 13419, 322, 512, 48555, 291, 458, 11, 309, 920, 1542, 1858, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.20037420060899522, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.00022867147345095873}, {"id": 34, "seek": 22900, "start": 229.0, "end": 237.52, "text": " If you go to other places where there's way more renewable energy you can see these energy", "tokens": [759, 291, 352, 281, 661, 3190, 689, 456, 311, 636, 544, 20938, 2281, 291, 393, 536, 613, 2281], "temperature": 0.0, "avg_logprob": -0.14150719526337413, "compression_ratio": 1.782383419689119, "no_speech_prob": 0.00010783314792206511}, {"id": 35, "seek": 22900, "start": 237.52, "end": 239.8, "text": " profiles change dramatically.", "tokens": [23693, 1319, 17548, 13], "temperature": 0.0, "avg_logprob": -0.14150719526337413, "compression_ratio": 1.782383419689119, "no_speech_prob": 0.00010783314792206511}, {"id": 36, "seek": 22900, "start": 239.8, "end": 248.04, "text": " So here you can see really a profile for a big solar park and you can see these huge", "tokens": [407, 510, 291, 393, 536, 534, 257, 7964, 337, 257, 955, 7936, 3884, 293, 291, 393, 536, 613, 2603], "temperature": 0.0, "avg_logprob": -0.14150719526337413, "compression_ratio": 1.782383419689119, "no_speech_prob": 0.00010783314792206511}, {"id": 37, "seek": 22900, "start": 248.04, "end": 252.72, "text": " negative peaks which on some days are there and some days they're not probably that's", "tokens": [3671, 26897, 597, 322, 512, 1708, 366, 456, 293, 512, 1708, 436, 434, 406, 1391, 300, 311], "temperature": 0.0, "avg_logprob": -0.14150719526337413, "compression_ratio": 1.782383419689119, "no_speech_prob": 0.00010783314792206511}, {"id": 38, "seek": 22900, "start": 252.72, "end": 257.8, "text": " a cloudy day so there's no or less energy generated.", "tokens": [257, 33060, 786, 370, 456, 311, 572, 420, 1570, 2281, 10833, 13], "temperature": 0.0, "avg_logprob": -0.14150719526337413, "compression_ratio": 1.782383419689119, "no_speech_prob": 0.00010783314792206511}, {"id": 39, "seek": 25780, "start": 257.8, "end": 265.68, "text": " And this is an energy profile for a big wind farm which you can see is well, seems hard", "tokens": [400, 341, 307, 364, 2281, 7964, 337, 257, 955, 2468, 5421, 597, 291, 393, 536, 307, 731, 11, 2544, 1152], "temperature": 0.0, "avg_logprob": -0.2277211702786959, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.00021288696734700352}, {"id": 40, "seek": 25780, "start": 265.68, "end": 278.16, "text": " to predict because there's seems no real, yeah so a negative energy means that the consumer", "tokens": [281, 6069, 570, 456, 311, 2544, 572, 957, 11, 1338, 370, 257, 3671, 2281, 1355, 300, 264, 9711], "temperature": 0.0, "avg_logprob": -0.2277211702786959, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.00021288696734700352}, {"id": 41, "seek": 25780, "start": 278.16, "end": 286.32, "text": " or the customer I mean is giving back to the grid so then it's negative for us.", "tokens": [420, 264, 5474, 286, 914, 307, 2902, 646, 281, 264, 10748, 370, 550, 309, 311, 3671, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.2277211702786959, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.00021288696734700352}, {"id": 42, "seek": 28632, "start": 286.32, "end": 295.84, "text": " If it's positive it means sorry, yeah exactly, so if it means a big negative peak it means", "tokens": [759, 309, 311, 3353, 309, 1355, 2597, 11, 1338, 2293, 11, 370, 498, 309, 1355, 257, 955, 3671, 10651, 309, 1355], "temperature": 0.0, "avg_logprob": -0.30777341928055035, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.00015193862782325596}, {"id": 43, "seek": 28632, "start": 295.84, "end": 299.0, "text": " the customer is producing a lot of energy.", "tokens": [264, 5474, 307, 10501, 257, 688, 295, 2281, 13], "temperature": 0.0, "avg_logprob": -0.30777341928055035, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.00015193862782325596}, {"id": 44, "seek": 28632, "start": 299.0, "end": 303.12, "text": " So it's just a convention you could also switch the sign but you have to choose one", "tokens": [407, 309, 311, 445, 257, 10286, 291, 727, 611, 3679, 264, 1465, 457, 291, 362, 281, 2826, 472], "temperature": 0.0, "avg_logprob": -0.30777341928055035, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.00015193862782325596}, {"id": 45, "seek": 28632, "start": 303.12, "end": 304.12, "text": " convention.", "tokens": [10286, 13], "temperature": 0.0, "avg_logprob": -0.30777341928055035, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.00015193862782325596}, {"id": 46, "seek": 28632, "start": 304.12, "end": 305.12, "text": " Yeah power, yeah.", "tokens": [865, 1347, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.30777341928055035, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.00015193862782325596}, {"id": 47, "seek": 30512, "start": 305.12, "end": 325.24, "text": " Yeah so it's not just one side it's more like a general profile for like a substation for", "tokens": [865, 370, 309, 311, 406, 445, 472, 1252, 309, 311, 544, 411, 257, 2674, 7964, 337, 411, 257, 4594, 399, 337], "temperature": 0.0, "avg_logprob": -0.30767624378204345, "compression_ratio": 1.3394495412844036, "no_speech_prob": 0.000718608673196286}, {"id": 48, "seek": 30512, "start": 325.24, "end": 329.2, "text": " the grid operator but connected to it is a lot of solar.", "tokens": [264, 10748, 12973, 457, 4582, 281, 309, 307, 257, 688, 295, 7936, 13], "temperature": 0.0, "avg_logprob": -0.30767624378204345, "compression_ratio": 1.3394495412844036, "no_speech_prob": 0.000718608673196286}, {"id": 49, "seek": 32920, "start": 329.2, "end": 342.08, "text": " Yeah exactly so on all of these there's load and production but yeah I just wanted to share", "tokens": [865, 2293, 370, 322, 439, 295, 613, 456, 311, 3677, 293, 4265, 457, 1338, 286, 445, 1415, 281, 2073], "temperature": 0.0, "avg_logprob": -0.2519624476530114, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0007792215328663588}, {"id": 50, "seek": 32920, "start": 342.08, "end": 345.32, "text": " this feeling.", "tokens": [341, 2633, 13], "temperature": 0.0, "avg_logprob": -0.2519624476530114, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0007792215328663588}, {"id": 51, "seek": 32920, "start": 345.32, "end": 356.8, "text": " So this can be difficult and this also leads to problems and this is two maps of the Netherlands", "tokens": [407, 341, 393, 312, 2252, 293, 341, 611, 6689, 281, 2740, 293, 341, 307, 732, 11317, 295, 264, 20873], "temperature": 0.0, "avg_logprob": -0.2519624476530114, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.0007792215328663588}, {"id": 52, "seek": 35680, "start": 356.8, "end": 363.0, "text": " and the colored areas are the areas where Alliander is currently active and on the left", "tokens": [293, 264, 14332, 3179, 366, 264, 3179, 689, 1057, 72, 4483, 307, 4362, 4967, 293, 322, 264, 1411], "temperature": 0.0, "avg_logprob": -0.17850289137467093, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.00022263418941292912}, {"id": 53, "seek": 35680, "start": 363.0, "end": 368.28000000000003, "text": " is energy consumption and on the right is energy production and this map shows if you're", "tokens": [307, 2281, 12126, 293, 322, 264, 558, 307, 2281, 4265, 293, 341, 4471, 3110, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.17850289137467093, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.00022263418941292912}, {"id": 54, "seek": 35680, "start": 368.28000000000003, "end": 375.24, "text": " a new customer and you want to be connected to the grid if it's red it's probably difficult", "tokens": [257, 777, 5474, 293, 291, 528, 281, 312, 4582, 281, 264, 10748, 498, 309, 311, 2182, 309, 311, 1391, 2252], "temperature": 0.0, "avg_logprob": -0.17850289137467093, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.00022263418941292912}, {"id": 55, "seek": 35680, "start": 375.24, "end": 377.24, "text": " because there's no more room.", "tokens": [570, 456, 311, 572, 544, 1808, 13], "temperature": 0.0, "avg_logprob": -0.17850289137467093, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.00022263418941292912}, {"id": 56, "seek": 35680, "start": 377.24, "end": 386.08000000000004, "text": " According to the Dutch law the energy grid is full over there and you can see that this", "tokens": [7328, 281, 264, 15719, 2101, 264, 2281, 10748, 307, 1577, 670, 456, 293, 291, 393, 536, 300, 341], "temperature": 0.0, "avg_logprob": -0.17850289137467093, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.00022263418941292912}, {"id": 57, "seek": 38608, "start": 386.08, "end": 393.12, "text": " is for huge areas in the Netherlands and also large areas on the consumption side.", "tokens": [307, 337, 2603, 3179, 294, 264, 20873, 293, 611, 2416, 3179, 322, 264, 12126, 1252, 13], "temperature": 0.0, "avg_logprob": -0.11352626591512602, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0001832963462220505}, {"id": 58, "seek": 38608, "start": 393.12, "end": 398.28, "text": " And of course Alliander is doing everything they can to solve this by building new cables", "tokens": [400, 295, 1164, 1057, 72, 4483, 307, 884, 1203, 436, 393, 281, 5039, 341, 538, 2390, 777, 17555], "temperature": 0.0, "avg_logprob": -0.11352626591512602, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0001832963462220505}, {"id": 59, "seek": 38608, "start": 398.28, "end": 403.84, "text": " and new substations but this takes time, a lot of time and we don't have the time as", "tokens": [293, 777, 4594, 763, 457, 341, 2516, 565, 11, 257, 688, 295, 565, 293, 321, 500, 380, 362, 264, 565, 382], "temperature": 0.0, "avg_logprob": -0.11352626591512602, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0001832963462220505}, {"id": 60, "seek": 38608, "start": 403.84, "end": 407.12, "text": " you can see in the graph before.", "tokens": [291, 393, 536, 294, 264, 4295, 949, 13], "temperature": 0.0, "avg_logprob": -0.11352626591512602, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0001832963462220505}, {"id": 61, "seek": 40712, "start": 407.12, "end": 420.12, "text": " So I don't have that one but I assume it's very similar because we're not the only one", "tokens": [407, 286, 500, 380, 362, 300, 472, 457, 286, 6552, 309, 311, 588, 2531, 570, 321, 434, 406, 264, 787, 472], "temperature": 0.0, "avg_logprob": -0.14318167577024365, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0002445979043841362}, {"id": 62, "seek": 40712, "start": 420.12, "end": 424.72, "text": " who are having these issues.", "tokens": [567, 366, 1419, 613, 2663, 13], "temperature": 0.0, "avg_logprob": -0.14318167577024365, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0002445979043841362}, {"id": 63, "seek": 40712, "start": 424.72, "end": 426.04, "text": " So how can we solve this?", "tokens": [407, 577, 393, 321, 5039, 341, 30], "temperature": 0.0, "avg_logprob": -0.14318167577024365, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0002445979043841362}, {"id": 64, "seek": 40712, "start": 426.04, "end": 436.76, "text": " Well one important thing is that we need grid inside and therefore this also includes forecasts.", "tokens": [1042, 472, 1021, 551, 307, 300, 321, 643, 10748, 1854, 293, 4412, 341, 611, 5974, 49421, 13], "temperature": 0.0, "avg_logprob": -0.14318167577024365, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.0002445979043841362}, {"id": 65, "seek": 43676, "start": 436.76, "end": 445.28, "text": " So transmission forecasts and these are important for all three parts in the electrical grid", "tokens": [407, 11574, 49421, 293, 613, 366, 1021, 337, 439, 1045, 3166, 294, 264, 12147, 10748], "temperature": 0.0, "avg_logprob": -0.1662854161755792, "compression_ratio": 1.5273972602739727, "no_speech_prob": 8.771482680458575e-05}, {"id": 66, "seek": 43676, "start": 445.28, "end": 447.52, "text": " so all three parties.", "tokens": [370, 439, 1045, 8265, 13], "temperature": 0.0, "avg_logprob": -0.1662854161755792, "compression_ratio": 1.5273972602739727, "no_speech_prob": 8.771482680458575e-05}, {"id": 67, "seek": 43676, "start": 447.52, "end": 458.68, "text": " So for customers, for DSOs such as Alliander and for TSOs such as Tenet which control the", "tokens": [407, 337, 4581, 11, 337, 15816, 31376, 1270, 382, 1057, 72, 4483, 293, 337, 314, 17188, 82, 1270, 382, 9380, 302, 597, 1969, 264], "temperature": 0.0, "avg_logprob": -0.1662854161755792, "compression_ratio": 1.5273972602739727, "no_speech_prob": 8.771482680458575e-05}, {"id": 68, "seek": 43676, "start": 458.68, "end": 463.96, "text": " high voltage grid.", "tokens": [1090, 8352, 10748, 13], "temperature": 0.0, "avg_logprob": -0.1662854161755792, "compression_ratio": 1.5273972602739727, "no_speech_prob": 8.771482680458575e-05}, {"id": 69, "seek": 46396, "start": 463.96, "end": 471.03999999999996, "text": " Using these forecasts operators can try to maintain grid safety and grid balance and", "tokens": [11142, 613, 49421, 19077, 393, 853, 281, 6909, 10748, 4514, 293, 10748, 4772, 293], "temperature": 0.0, "avg_logprob": -0.10803954601287842, "compression_ratio": 1.5813953488372092, "no_speech_prob": 9.75274306256324e-05}, {"id": 70, "seek": 46396, "start": 471.03999999999996, "end": 479.03999999999996, "text": " can give customers as much electricity as they want and as they need because the need", "tokens": [393, 976, 4581, 382, 709, 10356, 382, 436, 528, 293, 382, 436, 643, 570, 264, 643], "temperature": 0.0, "avg_logprob": -0.10803954601287842, "compression_ratio": 1.5813953488372092, "no_speech_prob": 9.75274306256324e-05}, {"id": 71, "seek": 46396, "start": 479.03999999999996, "end": 483.2, "text": " is high.", "tokens": [307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.10803954601287842, "compression_ratio": 1.5813953488372092, "no_speech_prob": 9.75274306256324e-05}, {"id": 72, "seek": 46396, "start": 483.2, "end": 492.28, "text": " With these forecasts we can also enable smart solutions and I put here two brochure pictures", "tokens": [2022, 613, 49421, 321, 393, 611, 9528, 4069, 6547, 293, 286, 829, 510, 732, 48147, 540, 5242], "temperature": 0.0, "avg_logprob": -0.10803954601287842, "compression_ratio": 1.5813953488372092, "no_speech_prob": 9.75274306256324e-05}, {"id": 73, "seek": 49228, "start": 492.28, "end": 502.4, "text": " of those solutions, one of them is a pilot FlexPower which was in Amsterdam which was", "tokens": [295, 729, 6547, 11, 472, 295, 552, 307, 257, 9691, 29208, 46057, 597, 390, 294, 28291, 597, 390], "temperature": 0.0, "avg_logprob": -0.1924139976501465, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.00038622168358415365}, {"id": 74, "seek": 49228, "start": 502.4, "end": 510.08, "text": " about charging electrical vehicles and charging them faster if it's possible and not charging", "tokens": [466, 11379, 12147, 8948, 293, 11379, 552, 4663, 498, 309, 311, 1944, 293, 406, 11379], "temperature": 0.0, "avg_logprob": -0.1924139976501465, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.00038622168358415365}, {"id": 75, "seek": 49228, "start": 510.08, "end": 516.24, "text": " them as fast if it's not possible.", "tokens": [552, 382, 2370, 498, 309, 311, 406, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1924139976501465, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.00038622168358415365}, {"id": 76, "seek": 51624, "start": 516.24, "end": 523.08, "text": " We at Alliander supplied forecasts for this project and another platform is the GOPEX", "tokens": [492, 412, 1057, 72, 4483, 27625, 49421, 337, 341, 1716, 293, 1071, 3663, 307, 264, 10365, 5208, 55], "temperature": 0.0, "avg_logprob": -0.15519343336967573, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00013758095155935735}, {"id": 77, "seek": 51624, "start": 523.08, "end": 528.96, "text": " platform which is like a trading platform for electricity where customers can trade", "tokens": [3663, 597, 307, 411, 257, 9529, 3663, 337, 10356, 689, 4581, 393, 4923], "temperature": 0.0, "avg_logprob": -0.15519343336967573, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00013758095155935735}, {"id": 78, "seek": 51624, "start": 528.96, "end": 538.4, "text": " with operators to either consume or to produce energy flexibly and this is also being used", "tokens": [365, 19077, 281, 2139, 14732, 420, 281, 5258, 2281, 5896, 3545, 293, 341, 307, 611, 885, 1143], "temperature": 0.0, "avg_logprob": -0.15519343336967573, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00013758095155935735}, {"id": 79, "seek": 51624, "start": 538.4, "end": 542.4, "text": " right now at Alliander and we also provide a forecast for that.", "tokens": [558, 586, 412, 1057, 72, 4483, 293, 321, 611, 2893, 257, 14330, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.15519343336967573, "compression_ratio": 1.603960396039604, "no_speech_prob": 0.00013758095155935735}, {"id": 80, "seek": 54240, "start": 542.4, "end": 557.4399999999999, "text": " So it's no longer working so let's use it.", "tokens": [407, 309, 311, 572, 2854, 1364, 370, 718, 311, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.20200389623641968, "compression_ratio": 1.55, "no_speech_prob": 0.0002520964771974832}, {"id": 81, "seek": 54240, "start": 557.4399999999999, "end": 562.8, "text": " So now let's talk about Opelstaff again because that's why I'm here and I'm going to give", "tokens": [407, 586, 718, 311, 751, 466, 12011, 338, 372, 2518, 797, 570, 300, 311, 983, 286, 478, 510, 293, 286, 478, 516, 281, 976], "temperature": 0.0, "avg_logprob": -0.20200389623641968, "compression_ratio": 1.55, "no_speech_prob": 0.0002520964771974832}, {"id": 82, "seek": 54240, "start": 562.8, "end": 568.4399999999999, "text": " a short introduction to Opelstaff and then I'm just going to give a short demo about", "tokens": [257, 2099, 9339, 281, 12011, 338, 372, 2518, 293, 550, 286, 478, 445, 516, 281, 976, 257, 2099, 10723, 466], "temperature": 0.0, "avg_logprob": -0.20200389623641968, "compression_ratio": 1.55, "no_speech_prob": 0.0002520964771974832}, {"id": 83, "seek": 56844, "start": 568.44, "end": 574.0, "text": " Opelstaff how you can make a forecast and also want to talk a bit about using Opelstaff", "tokens": [12011, 338, 372, 2518, 577, 291, 393, 652, 257, 14330, 293, 611, 528, 281, 751, 257, 857, 466, 1228, 12011, 338, 372, 2518], "temperature": 0.0, "avg_logprob": -0.1343440205217844, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.0001217345634358935}, {"id": 84, "seek": 56844, "start": 574.0, "end": 578.2, "text": " in an operational setting.", "tokens": [294, 364, 16607, 3287, 13], "temperature": 0.0, "avg_logprob": -0.1343440205217844, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.0001217345634358935}, {"id": 85, "seek": 56844, "start": 578.2, "end": 584.44, "text": " So first of all, the primary thing Opelstaff can help you with is that it's just a complete", "tokens": [407, 700, 295, 439, 11, 264, 6194, 551, 12011, 338, 372, 2518, 393, 854, 291, 365, 307, 300, 309, 311, 445, 257, 3566], "temperature": 0.0, "avg_logprob": -0.1343440205217844, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.0001217345634358935}, {"id": 86, "seek": 56844, "start": 584.44, "end": 586.6800000000001, "text": " machine learning pipeline.", "tokens": [3479, 2539, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1343440205217844, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.0001217345634358935}, {"id": 87, "seek": 56844, "start": 586.6800000000001, "end": 593.2, "text": " So I'm just going to give a short list of what it can do.", "tokens": [407, 286, 478, 445, 516, 281, 976, 257, 2099, 1329, 295, 437, 309, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.1343440205217844, "compression_ratio": 1.547872340425532, "no_speech_prob": 0.0001217345634358935}, {"id": 88, "seek": 59320, "start": 593.2, "end": 600.76, "text": " It handles input validation such as checking whether your data is complete.", "tokens": [467, 18722, 4846, 24071, 1270, 382, 8568, 1968, 428, 1412, 307, 3566, 13], "temperature": 0.0, "avg_logprob": -0.15927468240261078, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002660956233739853}, {"id": 89, "seek": 59320, "start": 600.76, "end": 610.72, "text": " It has feature engineering so it automatically calculates for you lag features or other features", "tokens": [467, 575, 4111, 7043, 370, 309, 6772, 4322, 1024, 337, 291, 8953, 4122, 420, 661, 4122], "temperature": 0.0, "avg_logprob": -0.15927468240261078, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002660956233739853}, {"id": 90, "seek": 59320, "start": 610.72, "end": 613.08, "text": " that are based on input features.", "tokens": [300, 366, 2361, 322, 4846, 4122, 13], "temperature": 0.0, "avg_logprob": -0.15927468240261078, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002660956233739853}, {"id": 91, "seek": 59320, "start": 613.08, "end": 620.12, "text": " So for example, if you input it with wind speed, it can calculate wind turbine power", "tokens": [407, 337, 1365, 11, 498, 291, 4846, 309, 365, 2468, 3073, 11, 309, 393, 8873, 2468, 27536, 1347], "temperature": 0.0, "avg_logprob": -0.15927468240261078, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002660956233739853}, {"id": 92, "seek": 62012, "start": 620.12, "end": 630.0, "text": " output for you or the same for direct normal irradiance.", "tokens": [5598, 337, 291, 420, 264, 912, 337, 2047, 2710, 29413, 5688, 719, 13], "temperature": 0.0, "avg_logprob": -0.1664272775041296, "compression_ratio": 1.3656716417910448, "no_speech_prob": 5.045792931923643e-05}, {"id": 93, "seek": 62012, "start": 630.0, "end": 639.72, "text": " Next it is some kind of intelligent train validation split of the time series.", "tokens": [3087, 309, 307, 512, 733, 295, 13232, 3847, 24071, 7472, 295, 264, 565, 2638, 13], "temperature": 0.0, "avg_logprob": -0.1664272775041296, "compression_ratio": 1.3656716417910448, "no_speech_prob": 5.045792931923643e-05}, {"id": 94, "seek": 62012, "start": 639.72, "end": 643.5600000000001, "text": " It has support for multiple type of regressors.", "tokens": [467, 575, 1406, 337, 3866, 2010, 295, 1121, 735, 830, 13], "temperature": 0.0, "avg_logprob": -0.1664272775041296, "compression_ratio": 1.3656716417910448, "no_speech_prob": 5.045792931923643e-05}, {"id": 95, "seek": 64356, "start": 643.56, "end": 650.2399999999999, "text": " So right now we have, for example, HGBoost which is at Allende the most commonly used", "tokens": [407, 558, 586, 321, 362, 11, 337, 1365, 11, 389, 38, 22493, 555, 597, 307, 412, 1057, 5445, 264, 881, 12719, 1143], "temperature": 0.0, "avg_logprob": -0.33771189776333893, "compression_ratio": 1.4801762114537445, "no_speech_prob": 5.286457235342823e-05}, {"id": 96, "seek": 64356, "start": 650.2399999999999, "end": 656.8399999999999, "text": " but we also had a collaboration with Sonyo which added ProLove to Opelstaff and we also", "tokens": [457, 321, 611, 632, 257, 9363, 365, 13575, 78, 597, 3869, 1705, 43, 1682, 281, 12011, 338, 372, 2518, 293, 321, 611], "temperature": 0.0, "avg_logprob": -0.33771189776333893, "compression_ratio": 1.4801762114537445, "no_speech_prob": 5.286457235342823e-05}, {"id": 97, "seek": 64356, "start": 656.8399999999999, "end": 658.92, "text": " have support for probabilistic forecasts.", "tokens": [362, 1406, 337, 31959, 3142, 49421, 13], "temperature": 0.0, "avg_logprob": -0.33771189776333893, "compression_ratio": 1.4801762114537445, "no_speech_prob": 5.286457235342823e-05}, {"id": 98, "seek": 64356, "start": 658.92, "end": 665.56, "text": " So that means not just one line but quantiles.", "tokens": [407, 300, 1355, 406, 445, 472, 1622, 457, 4426, 4680, 13], "temperature": 0.0, "avg_logprob": -0.33771189776333893, "compression_ratio": 1.4801762114537445, "no_speech_prob": 5.286457235342823e-05}, {"id": 99, "seek": 64356, "start": 665.56, "end": 672.88, "text": " And unless it has integrated the model and artifact storage using MLflow.", "tokens": [400, 5969, 309, 575, 10919, 264, 2316, 293, 34806, 6725, 1228, 21601, 10565, 13], "temperature": 0.0, "avg_logprob": -0.33771189776333893, "compression_ratio": 1.4801762114537445, "no_speech_prob": 5.286457235342823e-05}, {"id": 100, "seek": 67288, "start": 672.88, "end": 674.92, "text": " So what does this all mean then?", "tokens": [407, 437, 775, 341, 439, 914, 550, 30], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 101, "seek": 67288, "start": 674.92, "end": 679.52, "text": " Let's go to an actual demo.", "tokens": [961, 311, 352, 281, 364, 3539, 10723, 13], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 102, "seek": 67288, "start": 679.52, "end": 684.84, "text": " So I'm going to put this up here.", "tokens": [407, 286, 478, 516, 281, 829, 341, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 103, "seek": 67288, "start": 684.84, "end": 691.24, "text": " That's a low resolution.", "tokens": [663, 311, 257, 2295, 8669, 13], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 104, "seek": 67288, "start": 691.24, "end": 692.24, "text": " Let's zoom out.", "tokens": [961, 311, 8863, 484, 13], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 105, "seek": 67288, "start": 692.24, "end": 696.96, "text": " It's a bit too much.", "tokens": [467, 311, 257, 857, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.3783056653779128, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.000151371699757874}, {"id": 106, "seek": 69696, "start": 696.96, "end": 704.52, "text": " Okay, so I'm just going to walk you through an example or how you could make a forecast.", "tokens": [1033, 11, 370, 286, 478, 445, 516, 281, 1792, 291, 807, 364, 1365, 420, 577, 291, 727, 652, 257, 14330, 13], "temperature": 0.0, "avg_logprob": -0.16686717166176326, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.00010194243805017322}, {"id": 107, "seek": 69696, "start": 704.52, "end": 711.08, "text": " So first we need to make some kind of config object that's just what you have to feed Opelstaff.", "tokens": [407, 700, 321, 643, 281, 652, 512, 733, 295, 6662, 2657, 300, 311, 445, 437, 291, 362, 281, 3154, 12011, 338, 372, 2518, 13], "temperature": 0.0, "avg_logprob": -0.16686717166176326, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.00010194243805017322}, {"id": 108, "seek": 69696, "start": 711.08, "end": 713.8000000000001, "text": " Let's close this.", "tokens": [961, 311, 1998, 341, 13], "temperature": 0.0, "avg_logprob": -0.16686717166176326, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.00010194243805017322}, {"id": 109, "seek": 69696, "start": 713.8000000000001, "end": 717.1600000000001, "text": " So let's run this line.", "tokens": [407, 718, 311, 1190, 341, 1622, 13], "temperature": 0.0, "avg_logprob": -0.16686717166176326, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.00010194243805017322}, {"id": 110, "seek": 69696, "start": 717.1600000000001, "end": 722.6800000000001, "text": " Next I put some example input in this project.", "tokens": [3087, 286, 829, 512, 1365, 4846, 294, 341, 1716, 13], "temperature": 0.0, "avg_logprob": -0.16686717166176326, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.00010194243805017322}, {"id": 111, "seek": 72268, "start": 722.68, "end": 731.4799999999999, "text": " So we can load it and we can visualize it.", "tokens": [407, 321, 393, 3677, 309, 293, 321, 393, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.2066424832199559, "compression_ratio": 1.524822695035461, "no_speech_prob": 3.438088606344536e-05}, {"id": 112, "seek": 72268, "start": 731.4799999999999, "end": 738.76, "text": " So as you can see here, well, this is upon a stator frame and here we have the load and", "tokens": [407, 382, 291, 393, 536, 510, 11, 731, 11, 341, 307, 3564, 257, 342, 1639, 3920, 293, 510, 321, 362, 264, 3677, 293], "temperature": 0.0, "avg_logprob": -0.2066424832199559, "compression_ratio": 1.524822695035461, "no_speech_prob": 3.438088606344536e-05}, {"id": 113, "seek": 72268, "start": 738.76, "end": 740.9599999999999, "text": " we have a lot of predictors.", "tokens": [321, 362, 257, 688, 295, 6069, 830, 13], "temperature": 0.0, "avg_logprob": -0.2066424832199559, "compression_ratio": 1.524822695035461, "no_speech_prob": 3.438088606344536e-05}, {"id": 114, "seek": 72268, "start": 740.9599999999999, "end": 746.28, "text": " Well, some of these, well, the names should make sense.", "tokens": [1042, 11, 512, 295, 613, 11, 731, 11, 264, 5288, 820, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2066424832199559, "compression_ratio": 1.524822695035461, "no_speech_prob": 3.438088606344536e-05}, {"id": 115, "seek": 74628, "start": 746.28, "end": 754.12, "text": " So for example, the amount of variation predicted by the KMI or the temperature, well, all", "tokens": [407, 337, 1365, 11, 264, 2372, 295, 12990, 19147, 538, 264, 591, 13808, 420, 264, 4292, 11, 731, 11, 439], "temperature": 0.0, "avg_logprob": -0.28381824493408203, "compression_ratio": 1.535031847133758, "no_speech_prob": 1.250937748409342e-05}, {"id": 116, "seek": 74628, "start": 754.12, "end": 758.92, "text": " these predictors are already in this example data.", "tokens": [613, 6069, 830, 366, 1217, 294, 341, 1365, 1412, 13], "temperature": 0.0, "avg_logprob": -0.28381824493408203, "compression_ratio": 1.535031847133758, "no_speech_prob": 1.250937748409342e-05}, {"id": 117, "seek": 74628, "start": 758.92, "end": 770.28, "text": " So if we have this, I can also plot it for you so you can see, well, this is another", "tokens": [407, 498, 321, 362, 341, 11, 286, 393, 611, 7542, 309, 337, 291, 370, 291, 393, 536, 11, 731, 11, 341, 307, 1071], "temperature": 0.0, "avg_logprob": -0.28381824493408203, "compression_ratio": 1.535031847133758, "no_speech_prob": 1.250937748409342e-05}, {"id": 118, "seek": 74628, "start": 770.28, "end": 772.48, "text": " power profile.", "tokens": [1347, 7964, 13], "temperature": 0.0, "avg_logprob": -0.28381824493408203, "compression_ratio": 1.535031847133758, "no_speech_prob": 1.250937748409342e-05}, {"id": 119, "seek": 77248, "start": 772.48, "end": 779.08, "text": " Okay, so now imagine you have this and you want to know, well, what's next?", "tokens": [1033, 11, 370, 586, 3811, 291, 362, 341, 293, 291, 528, 281, 458, 11, 731, 11, 437, 311, 958, 30], "temperature": 0.0, "avg_logprob": -0.21028572473770532, "compression_ratio": 1.5401069518716577, "no_speech_prob": 6.287552241701633e-05}, {"id": 120, "seek": 77248, "start": 779.08, "end": 782.5600000000001, "text": " Then first we need to train a model.", "tokens": [1396, 700, 321, 643, 281, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21028572473770532, "compression_ratio": 1.5401069518716577, "no_speech_prob": 6.287552241701633e-05}, {"id": 121, "seek": 77248, "start": 782.5600000000001, "end": 788.72, "text": " So Opelstaff has a train model pipeline which basically does all those things I just mentioned.", "tokens": [407, 12011, 338, 372, 2518, 575, 257, 3847, 2316, 15517, 597, 1936, 775, 439, 729, 721, 286, 445, 2835, 13], "temperature": 0.0, "avg_logprob": -0.21028572473770532, "compression_ratio": 1.5401069518716577, "no_speech_prob": 6.287552241701633e-05}, {"id": 122, "seek": 77248, "start": 788.72, "end": 796.32, "text": " So we can just call the pipeline and let's hope the live demo does not fill me.", "tokens": [407, 321, 393, 445, 818, 264, 15517, 293, 718, 311, 1454, 264, 1621, 10723, 775, 406, 2836, 385, 13], "temperature": 0.0, "avg_logprob": -0.21028572473770532, "compression_ratio": 1.5401069518716577, "no_speech_prob": 6.287552241701633e-05}, {"id": 123, "seek": 79632, "start": 796.32, "end": 802.7600000000001, "text": " It will take about 15 seconds I think to train a model and store it.", "tokens": [467, 486, 747, 466, 2119, 3949, 286, 519, 281, 3847, 257, 2316, 293, 3531, 309, 13], "temperature": 0.0, "avg_logprob": -0.21477442979812622, "compression_ratio": 1.464968152866242, "no_speech_prob": 1.2771015462931246e-05}, {"id": 124, "seek": 79632, "start": 802.7600000000001, "end": 811.8000000000001, "text": " So you can see some info about what it's doing, well, and it's stored it.", "tokens": [407, 291, 393, 536, 512, 13614, 466, 437, 309, 311, 884, 11, 731, 11, 293, 309, 311, 12187, 309, 13], "temperature": 0.0, "avg_logprob": -0.21477442979812622, "compression_ratio": 1.464968152866242, "no_speech_prob": 1.2771015462931246e-05}, {"id": 125, "seek": 79632, "start": 811.8000000000001, "end": 824.84, "text": " So let's have a look and we'll flow comes with an interface so we can directly see that", "tokens": [407, 718, 311, 362, 257, 574, 293, 321, 603, 3095, 1487, 365, 364, 9226, 370, 321, 393, 3838, 536, 300], "temperature": 0.0, "avg_logprob": -0.21477442979812622, "compression_ratio": 1.464968152866242, "no_speech_prob": 1.2771015462931246e-05}, {"id": 126, "seek": 82484, "start": 824.84, "end": 828.88, "text": " we train a model.", "tokens": [321, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 127, "seek": 82484, "start": 828.88, "end": 833.36, "text": " So right here, this was the run.", "tokens": [407, 558, 510, 11, 341, 390, 264, 1190, 13], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 128, "seek": 82484, "start": 833.36, "end": 836.0, "text": " Now let's hope this works.", "tokens": [823, 718, 311, 1454, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 129, "seek": 82484, "start": 836.0, "end": 841.8000000000001, "text": " I see that my internet is no longer working so apparently this, then this figure this", "tokens": [286, 536, 300, 452, 4705, 307, 572, 2854, 1364, 370, 7970, 341, 11, 550, 341, 2573, 341], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 130, "seek": 82484, "start": 841.8000000000001, "end": 842.8000000000001, "text": " will work.", "tokens": [486, 589, 13], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 131, "seek": 82484, "start": 842.8000000000001, "end": 850.88, "text": " All right, I'm not showing it.", "tokens": [1057, 558, 11, 286, 478, 406, 4099, 309, 13], "temperature": 0.0, "avg_logprob": -0.2908198833465576, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.00016913133731577545}, {"id": 132, "seek": 85088, "start": 850.88, "end": 860.08, "text": " So, well, this is the MLflow interface and you can see that we just train a model.", "tokens": [407, 11, 731, 11, 341, 307, 264, 21601, 10565, 9226, 293, 291, 393, 536, 300, 321, 445, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20592668056488037, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.00017095831572078168}, {"id": 133, "seek": 85088, "start": 860.08, "end": 865.4399999999999, "text": " You can also click on the model or on the train run, this is just MLflow and you can", "tokens": [509, 393, 611, 2052, 322, 264, 2316, 420, 322, 264, 3847, 1190, 11, 341, 307, 445, 21601, 10565, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.20592668056488037, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.00017095831572078168}, {"id": 134, "seek": 85088, "start": 865.4399999999999, "end": 873.28, "text": " see a bit more, well, information about what happened during the training.", "tokens": [536, 257, 857, 544, 11, 731, 11, 1589, 466, 437, 2011, 1830, 264, 3097, 13], "temperature": 0.0, "avg_logprob": -0.20592668056488037, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.00017095831572078168}, {"id": 135, "seek": 85088, "start": 873.28, "end": 878.28, "text": " The next, of course, we want to make a prediction.", "tokens": [440, 958, 11, 295, 1164, 11, 321, 528, 281, 652, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.20592668056488037, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.00017095831572078168}, {"id": 136, "seek": 87828, "start": 878.28, "end": 886.12, "text": " So again, OpenStep has a pipeline for that so we can just say, okay, I want a prediction.", "tokens": [407, 797, 11, 7238, 23624, 575, 257, 15517, 337, 300, 370, 321, 393, 445, 584, 11, 1392, 11, 286, 528, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.20548631978589435, "compression_ratio": 1.5820895522388059, "no_speech_prob": 3.3166081266244873e-05}, {"id": 137, "seek": 87828, "start": 886.12, "end": 893.12, "text": " So it's loading the model and using data to create a prediction and then we can visualize", "tokens": [407, 309, 311, 15114, 264, 2316, 293, 1228, 1412, 281, 1884, 257, 17630, 293, 550, 321, 393, 23273], "temperature": 0.0, "avg_logprob": -0.20548631978589435, "compression_ratio": 1.5820895522388059, "no_speech_prob": 3.3166081266244873e-05}, {"id": 138, "seek": 87828, "start": 893.12, "end": 896.4, "text": " that as well.", "tokens": [300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20548631978589435, "compression_ratio": 1.5820895522388059, "no_speech_prob": 3.3166081266244873e-05}, {"id": 139, "seek": 87828, "start": 896.4, "end": 898.56, "text": " And then we have a graph right there.", "tokens": [400, 550, 321, 362, 257, 4295, 558, 456, 13], "temperature": 0.0, "avg_logprob": -0.20548631978589435, "compression_ratio": 1.5820895522388059, "no_speech_prob": 3.3166081266244873e-05}, {"id": 140, "seek": 87828, "start": 898.56, "end": 906.3199999999999, "text": " So this is the forecast that it made for the next, well, this was in some example data", "tokens": [407, 341, 307, 264, 14330, 300, 309, 1027, 337, 264, 958, 11, 731, 11, 341, 390, 294, 512, 1365, 1412], "temperature": 0.0, "avg_logprob": -0.20548631978589435, "compression_ratio": 1.5820895522388059, "no_speech_prob": 3.3166081266244873e-05}, {"id": 141, "seek": 90632, "start": 906.32, "end": 914.6, "text": " in 2021 but about 48 hours of forecast so that's OpenStep in practice.", "tokens": [294, 7201, 457, 466, 11174, 2496, 295, 14330, 370, 300, 311, 7238, 23624, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.3179599444071452, "compression_ratio": 1.3312101910828025, "no_speech_prob": 5.332536602509208e-05}, {"id": 142, "seek": 90632, "start": 914.6, "end": 920.8000000000001, "text": " Let's go back to the presentation.", "tokens": [961, 311, 352, 646, 281, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.3179599444071452, "compression_ratio": 1.3312101910828025, "no_speech_prob": 5.332536602509208e-05}, {"id": 143, "seek": 90632, "start": 920.8000000000001, "end": 923.0, "text": " Do this slide.", "tokens": [1144, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.3179599444071452, "compression_ratio": 1.3312101910828025, "no_speech_prob": 5.332536602509208e-05}, {"id": 144, "seek": 90632, "start": 923.0, "end": 932.6800000000001, "text": " Yeah, so, well, this flow has been a minimal flow but of course in reality, at least for", "tokens": [865, 11, 370, 11, 731, 11, 341, 3095, 575, 668, 257, 13206, 3095, 457, 295, 1164, 294, 4103, 11, 412, 1935, 337], "temperature": 0.0, "avg_logprob": -0.3179599444071452, "compression_ratio": 1.3312101910828025, "no_speech_prob": 5.332536602509208e-05}, {"id": 145, "seek": 93268, "start": 932.68, "end": 937.12, "text": " if you're a grid operator, you want to do this in an operational setting.", "tokens": [498, 291, 434, 257, 10748, 12973, 11, 291, 528, 281, 360, 341, 294, 364, 16607, 3287, 13], "temperature": 0.0, "avg_logprob": -0.13069984647962782, "compression_ratio": 1.7064220183486238, "no_speech_prob": 9.085171768674627e-05}, {"id": 146, "seek": 93268, "start": 937.12, "end": 943.12, "text": " So this means that you want to do daily forecasts for a lot of different locations with all", "tokens": [407, 341, 1355, 300, 291, 528, 281, 360, 5212, 49421, 337, 257, 688, 295, 819, 9253, 365, 439], "temperature": 0.0, "avg_logprob": -0.13069984647962782, "compression_ratio": 1.7064220183486238, "no_speech_prob": 9.085171768674627e-05}, {"id": 147, "seek": 93268, "start": 943.12, "end": 945.0799999999999, "text": " kinds of configurations.", "tokens": [3685, 295, 31493, 13], "temperature": 0.0, "avg_logprob": -0.13069984647962782, "compression_ratio": 1.7064220183486238, "no_speech_prob": 9.085171768674627e-05}, {"id": 148, "seek": 93268, "start": 945.0799999999999, "end": 953.52, "text": " And OpenStep also comes with a so-called reference implementation about how you could do this.", "tokens": [400, 7238, 23624, 611, 1487, 365, 257, 370, 12, 11880, 6408, 11420, 466, 577, 291, 727, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.13069984647962782, "compression_ratio": 1.7064220183486238, "no_speech_prob": 9.085171768674627e-05}, {"id": 149, "seek": 93268, "start": 953.52, "end": 960.4, "text": " So this is a picture of what you would have to do so we have OpenStep right here which", "tokens": [407, 341, 307, 257, 3036, 295, 437, 291, 576, 362, 281, 360, 370, 321, 362, 7238, 23624, 558, 510, 597], "temperature": 0.0, "avg_logprob": -0.13069984647962782, "compression_ratio": 1.7064220183486238, "no_speech_prob": 9.085171768674627e-05}, {"id": 150, "seek": 96040, "start": 960.4, "end": 964.56, "text": " is basically, I just showed you the training and forecasting pipeline.", "tokens": [307, 1936, 11, 286, 445, 4712, 291, 264, 3097, 293, 44331, 15517, 13], "temperature": 0.0, "avg_logprob": -0.2025266476531527, "compression_ratio": 1.4331550802139037, "no_speech_prob": 7.440895569743589e-05}, {"id": 151, "seek": 96040, "start": 964.56, "end": 971.92, "text": " Then we have another package which is called OpenStep DBC, database connector which can", "tokens": [1396, 321, 362, 1071, 7372, 597, 307, 1219, 7238, 23624, 413, 7869, 11, 8149, 19127, 597, 393], "temperature": 0.0, "avg_logprob": -0.2025266476531527, "compression_ratio": 1.4331550802139037, "no_speech_prob": 7.440895569743589e-05}, {"id": 152, "seek": 96040, "start": 971.92, "end": 975.68, "text": " connect to a database.", "tokens": [1745, 281, 257, 8149, 13], "temperature": 0.0, "avg_logprob": -0.2025266476531527, "compression_ratio": 1.4331550802139037, "no_speech_prob": 7.440895569743589e-05}, {"id": 153, "seek": 96040, "start": 975.68, "end": 986.48, "text": " And we use MySQL and Influx DB to store all the data required to run it operationally.", "tokens": [400, 321, 764, 1222, 39934, 293, 682, 3423, 2449, 26754, 281, 3531, 439, 264, 1412, 4739, 281, 1190, 309, 6916, 379, 13], "temperature": 0.0, "avg_logprob": -0.2025266476531527, "compression_ratio": 1.4331550802139037, "no_speech_prob": 7.440895569743589e-05}, {"id": 154, "seek": 98648, "start": 986.48, "end": 992.72, "text": " And we also have a Gafana dashboard built upon this database stack so we can also see", "tokens": [400, 321, 611, 362, 257, 460, 2792, 2095, 18342, 3094, 3564, 341, 8149, 8630, 370, 321, 393, 611, 536], "temperature": 0.0, "avg_logprob": -0.17954299508071528, "compression_ratio": 1.5614973262032086, "no_speech_prob": 8.884297130862251e-05}, {"id": 155, "seek": 98648, "start": 992.72, "end": 995.0, "text": " what's going on.", "tokens": [437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.17954299508071528, "compression_ratio": 1.5614973262032086, "no_speech_prob": 8.884297130862251e-05}, {"id": 156, "seek": 98648, "start": 995.0, "end": 999.76, "text": " And again, as I already have shown, you can use MLflow to keep track of all the models", "tokens": [400, 797, 11, 382, 286, 1217, 362, 4898, 11, 291, 393, 764, 21601, 10565, 281, 1066, 2837, 295, 439, 264, 5245], "temperature": 0.0, "avg_logprob": -0.17954299508071528, "compression_ratio": 1.5614973262032086, "no_speech_prob": 8.884297130862251e-05}, {"id": 157, "seek": 98648, "start": 999.76, "end": 1004.12, "text": " and all the runs that are being done to see what's going on.", "tokens": [293, 439, 264, 6676, 300, 366, 885, 1096, 281, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.17954299508071528, "compression_ratio": 1.5614973262032086, "no_speech_prob": 8.884297130862251e-05}, {"id": 158, "seek": 98648, "start": 1004.12, "end": 1013.76, "text": " So I want to show this dashboard as well.", "tokens": [407, 286, 528, 281, 855, 341, 18342, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17954299508071528, "compression_ratio": 1.5614973262032086, "no_speech_prob": 8.884297130862251e-05}, {"id": 159, "seek": 101376, "start": 1013.76, "end": 1027.84, "text": " So this dashboard is just example data so it's not our real dashboard.", "tokens": [407, 341, 18342, 307, 445, 1365, 1412, 370, 309, 311, 406, 527, 957, 18342, 13], "temperature": 0.0, "avg_logprob": -0.27107155323028564, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.00027567645884118974}, {"id": 160, "seek": 101376, "start": 1027.84, "end": 1037.12, "text": " But here you can for example see some load that was there on the system and you can also", "tokens": [583, 510, 291, 393, 337, 1365, 536, 512, 3677, 300, 390, 456, 322, 264, 1185, 293, 291, 393, 611], "temperature": 0.0, "avg_logprob": -0.27107155323028564, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.00027567645884118974}, {"id": 161, "seek": 103712, "start": 1037.12, "end": 1044.9599999999998, "text": " see that for example this is not just one area but it has a sum of two systems which", "tokens": [536, 300, 337, 1365, 341, 307, 406, 445, 472, 1859, 457, 309, 575, 257, 2408, 295, 732, 3652, 597], "temperature": 0.0, "avg_logprob": -0.13919903172387016, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.631938155507669e-05}, {"id": 162, "seek": 103712, "start": 1044.9599999999998, "end": 1050.9199999999998, "text": " is quite common in an electrical grid that you have a lot of measurement points that", "tokens": [307, 1596, 2689, 294, 364, 12147, 10748, 300, 291, 362, 257, 688, 295, 13160, 2793, 300], "temperature": 0.0, "avg_logprob": -0.13919903172387016, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.631938155507669e-05}, {"id": 163, "seek": 103712, "start": 1050.9199999999998, "end": 1055.3999999999999, "text": " you have to add together with different signs.", "tokens": [291, 362, 281, 909, 1214, 365, 819, 7880, 13], "temperature": 0.0, "avg_logprob": -0.13919903172387016, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.631938155507669e-05}, {"id": 164, "seek": 103712, "start": 1055.3999999999999, "end": 1066.36, "text": " And you can see for example here's then a live forecast of this location as well.", "tokens": [400, 291, 393, 536, 337, 1365, 510, 311, 550, 257, 1621, 14330, 295, 341, 4914, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13919903172387016, "compression_ratio": 1.6021505376344085, "no_speech_prob": 9.631938155507669e-05}, {"id": 165, "seek": 106636, "start": 1066.36, "end": 1075.8, "text": " You can also see plots of the feature importance that obtained during training of the model.", "tokens": [509, 393, 611, 536, 28609, 295, 264, 4111, 7379, 300, 14879, 1830, 3097, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22293467580536266, "compression_ratio": 1.6766169154228856, "no_speech_prob": 6.515014683827758e-05}, {"id": 166, "seek": 106636, "start": 1075.8, "end": 1080.08, "text": " You can see on which data the model has been trained.", "tokens": [509, 393, 536, 322, 597, 1412, 264, 2316, 575, 668, 8895, 13], "temperature": 0.0, "avg_logprob": -0.22293467580536266, "compression_ratio": 1.6766169154228856, "no_speech_prob": 6.515014683827758e-05}, {"id": 167, "seek": 106636, "start": 1080.08, "end": 1086.04, "text": " Over here these plots are really small but here you can see them.", "tokens": [4886, 510, 613, 28609, 366, 534, 1359, 457, 510, 291, 393, 536, 552, 13], "temperature": 0.0, "avg_logprob": -0.22293467580536266, "compression_ratio": 1.6766169154228856, "no_speech_prob": 6.515014683827758e-05}, {"id": 168, "seek": 106636, "start": 1086.04, "end": 1092.24, "text": " So it's a dashboard where you can see everything that Oberstaff does for every location that", "tokens": [407, 309, 311, 257, 18342, 689, 291, 393, 536, 1203, 300, 27664, 372, 2518, 775, 337, 633, 4914, 300], "temperature": 0.0, "avg_logprob": -0.22293467580536266, "compression_ratio": 1.6766169154228856, "no_speech_prob": 6.515014683827758e-05}, {"id": 169, "seek": 106636, "start": 1092.24, "end": 1094.24, "text": " you are could be interested in.", "tokens": [291, 366, 727, 312, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.22293467580536266, "compression_ratio": 1.6766169154228856, "no_speech_prob": 6.515014683827758e-05}, {"id": 170, "seek": 109424, "start": 1094.24, "end": 1099.1200000000001, "text": " You talk about forecast, are you within the forecast also taking other forecasts like", "tokens": [509, 751, 466, 14330, 11, 366, 291, 1951, 264, 14330, 611, 1940, 661, 49421, 411], "temperature": 0.0, "avg_logprob": -0.30708748585469015, "compression_ratio": 1.9470588235294117, "no_speech_prob": 0.0002286428352817893}, {"id": 171, "seek": 109424, "start": 1099.1200000000001, "end": 1103.04, "text": " the weather forecast into account or are you forecasting that yourself?", "tokens": [264, 5503, 14330, 666, 2696, 420, 366, 291, 44331, 300, 1803, 30], "temperature": 0.0, "avg_logprob": -0.30708748585469015, "compression_ratio": 1.9470588235294117, "no_speech_prob": 0.0002286428352817893}, {"id": 172, "seek": 109424, "start": 1103.04, "end": 1110.76, "text": " No so we use all kinds of data and the weather forecast is, oh the question is whether we", "tokens": [883, 370, 321, 764, 439, 3685, 295, 1412, 293, 264, 5503, 14330, 307, 11, 1954, 264, 1168, 307, 1968, 321], "temperature": 0.0, "avg_logprob": -0.30708748585469015, "compression_ratio": 1.9470588235294117, "no_speech_prob": 0.0002286428352817893}, {"id": 173, "seek": 109424, "start": 1110.76, "end": 1118.36, "text": " are using other forecasted data or whether we forecast, do those forecasts as well?", "tokens": [366, 1228, 661, 14330, 292, 1412, 420, 1968, 321, 14330, 11, 360, 729, 49421, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.30708748585469015, "compression_ratio": 1.9470588235294117, "no_speech_prob": 0.0002286428352817893}, {"id": 174, "seek": 111836, "start": 1118.36, "end": 1124.28, "text": " Like whether we forecast the weather ourselves and the answer is that it depends a bit.", "tokens": [1743, 1968, 321, 14330, 264, 5503, 4175, 293, 264, 1867, 307, 300, 309, 5946, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.2039878405057467, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00012006259930785745}, {"id": 175, "seek": 111836, "start": 1124.28, "end": 1132.0, "text": " So in general we use the weather forecasts for multiple sources and also for example", "tokens": [407, 294, 2674, 321, 764, 264, 5503, 49421, 337, 3866, 7139, 293, 611, 337, 1365], "temperature": 0.0, "avg_logprob": -0.2039878405057467, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00012006259930785745}, {"id": 176, "seek": 111836, "start": 1132.0, "end": 1138.28, "text": " price like the head pricing.", "tokens": [3218, 411, 264, 1378, 17621, 13], "temperature": 0.0, "avg_logprob": -0.2039878405057467, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00012006259930785745}, {"id": 177, "seek": 111836, "start": 1138.28, "end": 1147.3999999999999, "text": " So we use those data but sometimes we also feed the prediction itself or we feed one", "tokens": [407, 321, 764, 729, 1412, 457, 2171, 321, 611, 3154, 264, 17630, 2564, 420, 321, 3154, 472], "temperature": 0.0, "avg_logprob": -0.2039878405057467, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.00012006259930785745}, {"id": 178, "seek": 114740, "start": 1147.4, "end": 1152.48, "text": " prediction into another prediction.", "tokens": [17630, 666, 1071, 17630, 13], "temperature": 0.0, "avg_logprob": -0.37566132381044587, "compression_ratio": 1.4437086092715232, "no_speech_prob": 0.0001660192501731217}, {"id": 179, "seek": 114740, "start": 1152.48, "end": 1159.96, "text": " So I mean you can play around with that but you have to feed Oberstaff with all the predictors", "tokens": [407, 286, 914, 291, 393, 862, 926, 365, 300, 457, 291, 362, 281, 3154, 27664, 372, 2518, 365, 439, 264, 6069, 830], "temperature": 0.0, "avg_logprob": -0.37566132381044587, "compression_ratio": 1.4437086092715232, "no_speech_prob": 0.0001660192501731217}, {"id": 180, "seek": 114740, "start": 1159.96, "end": 1161.96, "text": " that you wanted to know.", "tokens": [300, 291, 1415, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.37566132381044587, "compression_ratio": 1.4437086092715232, "no_speech_prob": 0.0001660192501731217}, {"id": 181, "seek": 114740, "start": 1161.96, "end": 1176.68, "text": " Okay let's move on, I have one last slide and that's basically", "tokens": [1033, 718, 311, 1286, 322, 11, 286, 362, 472, 1036, 4137, 293, 300, 311, 1936], "temperature": 0.0, "avg_logprob": -0.37566132381044587, "compression_ratio": 1.4437086092715232, "no_speech_prob": 0.0001660192501731217}, {"id": 182, "seek": 117668, "start": 1176.68, "end": 1189.76, "text": " key information because that was my presentation so here I put all the info you might be interested", "tokens": [2141, 1589, 570, 300, 390, 452, 5860, 370, 510, 286, 829, 439, 264, 13614, 291, 1062, 312, 3102], "temperature": 0.0, "avg_logprob": -0.3762823581695557, "compression_ratio": 1.3821138211382114, "no_speech_prob": 0.0002513749641366303}, {"id": 183, "seek": 117668, "start": 1189.76, "end": 1203.88, "text": " in on this slide and if there are any questions then feel free to ask.", "tokens": [294, 322, 341, 4137, 293, 498, 456, 366, 604, 1651, 550, 841, 1737, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.3762823581695557, "compression_ratio": 1.3821138211382114, "no_speech_prob": 0.0002513749641366303}, {"id": 184, "seek": 120388, "start": 1203.88, "end": 1214.5600000000002, "text": " So the information is really useful but what is the purpose for the net grid operator,", "tokens": [407, 264, 1589, 307, 534, 4420, 457, 437, 307, 264, 4334, 337, 264, 2533, 10748, 12973, 11], "temperature": 0.0, "avg_logprob": -0.2909824565305548, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001406642491929233}, {"id": 185, "seek": 120388, "start": 1214.5600000000002, "end": 1221.88, "text": " what use does a grid operator make of this information, is it for congestion management,", "tokens": [437, 764, 775, 257, 10748, 12973, 652, 295, 341, 1589, 11, 307, 309, 337, 40816, 4592, 11], "temperature": 0.0, "avg_logprob": -0.2909824565305548, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001406642491929233}, {"id": 186, "seek": 120388, "start": 1221.88, "end": 1228.24, "text": " is it for some kind of load shedding, what is the role of this exercise?", "tokens": [307, 309, 337, 512, 733, 295, 3677, 49934, 11, 437, 307, 264, 3090, 295, 341, 5380, 30], "temperature": 0.0, "avg_logprob": -0.2909824565305548, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.001406642491929233}, {"id": 187, "seek": 122824, "start": 1228.24, "end": 1239.92, "text": " So I think the question is why would the grid operator be interested in forecast I guess", "tokens": [407, 286, 519, 264, 1168, 307, 983, 576, 264, 10748, 12973, 312, 3102, 294, 14330, 286, 2041], "temperature": 0.0, "avg_logprob": -0.26390050586901215, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.0005720247281715274}, {"id": 188, "seek": 122824, "start": 1239.92, "end": 1241.4, "text": " is what you're asking.", "tokens": [307, 437, 291, 434, 3365, 13], "temperature": 0.0, "avg_logprob": -0.26390050586901215, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.0005720247281715274}, {"id": 189, "seek": 122824, "start": 1241.4, "end": 1247.6, "text": " So there are many reasons but I think you already mentioned congestion management is", "tokens": [407, 456, 366, 867, 4112, 457, 286, 519, 291, 1217, 2835, 40816, 4592, 307], "temperature": 0.0, "avg_logprob": -0.26390050586901215, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.0005720247281715274}, {"id": 190, "seek": 122824, "start": 1247.6, "end": 1253.56, "text": " indeed an important reason but also well grid insight.", "tokens": [6451, 364, 1021, 1778, 457, 611, 731, 10748, 11269, 13], "temperature": 0.0, "avg_logprob": -0.26390050586901215, "compression_ratio": 1.5304878048780488, "no_speech_prob": 0.0005720247281715274}, {"id": 191, "seek": 125356, "start": 1253.56, "end": 1263.72, "text": " So the more congestion management is going to be used for the grid the more important", "tokens": [407, 264, 544, 40816, 4592, 307, 516, 281, 312, 1143, 337, 264, 10748, 264, 544, 1021], "temperature": 0.0, "avg_logprob": -0.18264798460335568, "compression_ratio": 1.65, "no_speech_prob": 0.0003561053017619997}, {"id": 192, "seek": 125356, "start": 1263.72, "end": 1270.84, "text": " it is also to maintain grid safety and grid safety is not just one operator, we are all", "tokens": [309, 307, 611, 281, 6909, 10748, 4514, 293, 10748, 4514, 307, 406, 445, 472, 12973, 11, 321, 366, 439], "temperature": 0.0, "avg_logprob": -0.18264798460335568, "compression_ratio": 1.65, "no_speech_prob": 0.0003561053017619997}, {"id": 193, "seek": 125356, "start": 1270.84, "end": 1277.6799999999998, "text": " connected to multiple grid operators so everyone has to communicate what they are going to", "tokens": [4582, 281, 3866, 10748, 19077, 370, 1518, 575, 281, 7890, 437, 436, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.18264798460335568, "compression_ratio": 1.65, "no_speech_prob": 0.0003561053017619997}, {"id": 194, "seek": 127768, "start": 1277.68, "end": 1287.5600000000002, "text": " do and what they expect the energy flow to be the next day so every operator can decide", "tokens": [360, 293, 437, 436, 2066, 264, 2281, 3095, 281, 312, 264, 958, 786, 370, 633, 12973, 393, 4536], "temperature": 0.0, "avg_logprob": -0.14093931516011557, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00021699568605981767}, {"id": 195, "seek": 127768, "start": 1287.5600000000002, "end": 1289.8400000000001, "text": " to do what's necessary to maintain grid safety.", "tokens": [281, 360, 437, 311, 4818, 281, 6909, 10748, 4514, 13], "temperature": 0.0, "avg_logprob": -0.14093931516011557, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00021699568605981767}, {"id": 196, "seek": 127768, "start": 1289.8400000000001, "end": 1297.16, "text": " So that's what I mentioned before the transmission forecast, every operator has to communicate", "tokens": [407, 300, 311, 437, 286, 2835, 949, 264, 11574, 14330, 11, 633, 12973, 575, 281, 7890], "temperature": 0.0, "avg_logprob": -0.14093931516011557, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00021699568605981767}, {"id": 197, "seek": 127768, "start": 1297.16, "end": 1305.04, "text": " to everyone who is connected to what they expect the load to be on the next day.", "tokens": [281, 1518, 567, 307, 4582, 281, 437, 436, 2066, 264, 3677, 281, 312, 322, 264, 958, 786, 13], "temperature": 0.0, "avg_logprob": -0.14093931516011557, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00021699568605981767}, {"id": 198, "seek": 130504, "start": 1305.04, "end": 1309.24, "text": " I see that my time is up so I'm afraid I have to answer the questions in the chat.", "tokens": [286, 536, 300, 452, 565, 307, 493, 370, 286, 478, 4638, 286, 362, 281, 1867, 264, 1651, 294, 264, 5081, 13], "temperature": 0.0, "avg_logprob": -0.3085982962830426, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0020984599832445383}, {"id": 199, "seek": 130504, "start": 1309.24, "end": 1317.36, "text": " Yeah or in the hallway, I think for time management we have to learn from these talks and see", "tokens": [865, 420, 294, 264, 23903, 11, 286, 519, 337, 565, 4592, 321, 362, 281, 1466, 490, 613, 6686, 293, 536], "temperature": 0.0, "avg_logprob": -0.3085982962830426, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0020984599832445383}, {"id": 200, "seek": 130504, "start": 1317.36, "end": 1321.32, "text": " if we can manage to keep a couple of more minutes for questions, sorry.", "tokens": [498, 321, 393, 3067, 281, 1066, 257, 1916, 295, 544, 2077, 337, 1651, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.3085982962830426, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0020984599832445383}, {"id": 201, "seek": 132132, "start": 1321.32, "end": 1336.3999999999999, "text": " Thank you very much for listening.", "tokens": [50364, 1044, 291, 588, 709, 337, 4764, 13, 51118], "temperature": 0.0, "avg_logprob": -0.44841980934143066, "compression_ratio": 0.8095238095238095, "no_speech_prob": 9.557206794852391e-05}], "language": "en"}