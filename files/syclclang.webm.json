{"text": " Yeah, yeah, exactly. Okay, good afternoon. Yeah, so I'm going to be talking about compiler intrinsics in sickle in DPC plus plus specifically. This is Intel's open source sickle implementation. This is what I work on. Yeah, so hopefully I'll be able to say something without saying too much in 10 minutes. Yeah, so code play. I work for code play. We had the first sickle implementation, compute CBP. We now work. We were acquired by Intel. So now we work on the Intel sickle implementation DPC plus plus. That's what I work on. We have lots of partners, you know, hardware companies, that kind of thing, whoever needs an open CL implementation, sickle implementation, and so on, come to us. Yeah, so sickle is a single source heterogeneous programming API. So you can write single source code that can run on NVIDIA, Intel, AMD GPUs, close to the mic. Okay, voice up. Yeah, so it's great for someone who's developing scientific applications to be able to write single source code that runs on whatever GPU the implementation enables, such as CUDA, level zero for Intel, AMD GPUs, and so on. Yeah, this is a really good thing. So I work specifically on the NVIDIA and the HIP, the AMD backends for DPC plus plus. Okay, so yeah, I just want to talk a little bit about compiler intrinsics and how kind of, you know, math function calls work in sickle and DPC plus plus at the moment, and how we can hopefully improve them so that we're contributing upstream. So what happens to sickle cause? So essentially, you get your sickle cause in your source code. This is redirected to spear V open CL cause F, you compile the spear V, you make a spear V module, this is a symbol within the spear V module, and then that is the implementation is provided by a CL level zero Vulkan driver. Okay, as I said, I don't work on the spear V backend at all. I work the PTX, the CUDA or the AMD GPU backends. So what do we do with these symbols so that we get to the native implementations? We're not trying to reinvent the wheel. We're not trying to do anything that the people who are making the GPUs aren't doing already. We're just trying to redirect to that. So how do we go from this to that, and then compile to our PTX module, our AMD GPU module, HSA module, and so on. So, yeah, how do we go from spear V OCL to NV cause F? So use the shim library, easy peasy, that's fine. Okay, you just redirect it, you compile it to bitcode, you link it, a compilation time, and you get to this, this native bitcode implementation. This is great. Okay, so we use libclc for this. So libclc is written in open CL. Okay, open CL does lots of stuff that SQL doesn't expose as easily like address spaces, that kind of thing. So we write an open CL. This is great. This makes our lives really, really easy. We can do it. Say, before we get into this, just why do we want to use a BC library in the first place? Why don't we use a.so? Why don't we just resolve to some symbol that then a runtime is called and we don't care about it? So on a GPU, the overhead of a function call is really high. Okay, it's because we lose information about, say, address spaces, that kind of thing. The GPU memory hierarchy is a bit more complex than, say, for CPU. So we really, really need to worry about this. We want to inline everything so we don't lose any information about our memory hierarchies. We also allow compile time branching of code based on the architecture, based on the back end, that kind of thing. We don't want to have these checks at runtime. We want high performance. That's the name of the game for what we're doing. This gives us greater optimization opportunities as well. You can do lots of dead code elimination, lots of fun stuff in the middle end because you're doing all these checks at the IR level. Okay, so this is just kind of what it looks like. So we just have Spirvio CR-Casef. We return NV-Casef. Great. Amazing. That's so easy. And then this is the implementation which is provided by NVIDIA. This is in-bit code. We link this, and then this is just in-lined into our IR. This is great. Okay. Yes, so we're linking this echo code with LipsyLC. Then we link that with the vendor-provided BC library. So we're linking, linking. We get to the implementation. It's all in-lined. It's all great. We love it. So this works well, but so this is a bit of code from LipsyLC. Because we're dealing in OpenCL-C, we could choose something else. We could write a native IR. We find that OpenCL is actually easier to use than an easier to maintain than writing a native IR. So we end up with some funny kind of problems with mangling and all this kind of thing. This isn't nice. Sometimes we need manual mangling. It's got to do with namespaces when they're interpreted by the OpenCL mangler, unfortunately. Yes, we need to sometimes as well. Sometimes OpenCL isn't as good as we want it to be. So we need to actually write a native IR as well. So it's a mix of LVM IR, LipsyLC. It's a bit messy. It's not great. Yes, so also we're exposing some compiler internals here. This is the NVVM reflect pass, which essentially takes your function call for NVVM reflect, replaces it with a numeric value. This is totally done at the IR level, so you can branch at the IR level based on this is a high architecture, a newer architecture. Do this new implementation, do this new built-in. There's an old architecture, as well for things like rounding modes. This pass is used. We're exposing this in source code through hacks. This isn't really, you know, it's not, it's not kosher. But it works. Who cares? Okay, but consider the new proposal to add FP accuracy attributes to math built-ins. This is where we have, say, FP built-in cars, and we specify the accuracy in ULP that we want it to be computed to. Okay, this is totally lost on us. Okay, so this is what it would look like. Yeah, you have this attribute here. You've, FP max error. This is really, really needed in SQL because SQL is targeting lots and lots of different platforms. All these platforms have different numerical accuracy guarantees. We really, really need this, but we don't use built-ins at all. We're sorry, we don't use LVM intrinsics at all. So this is, we need to get to a point where we can start using this compiler infrastructure. We're not using it as much as you want to. So we could do this using a libclc compiler kind of hack workaround. We do another, you know, pass, you just say compiler precision value. If it's that, do some precise square root. If it's not, do some approximate thing. Yeah, we could do that. Okay, the problem with libclc and this stuff, it's not upstreamable. Okay, it's, it's a collection of hacks. It's not totally hacked, but like it's a little bit messy. It's not written in the same API. It's lib, it's OpenCL and it's, it's LVM IR. It's messy. We can upstream this. We can all benefit from this. Okay, so the pro about doing some, another, adding another hack to the, the kind of passes, another hack to the bunch is that it's easy to do. Okay, we can do this and we can keep going with our libclc implementation. It's pretty straightforward. We've been doing this the whole time. Yeah, fine. We don't need to worry about the broader LVM concerns. However, we miss out on LVM community collaboration, which is why we're, we're here. And then how many of these workarounds do we actually need in order to keep up with the latest trends and then libclc as bad as it could be now, like it just degenerates into an absolute mess and we don't want that. Okay, so we think the answer for this is to try and redirect, try and, try and actually have it calling the compiler intrinsic. Okay, we want to use compiler intrinsic and then have some generic behavior of these intrinsics for offload targets. Okay, and this would be used by say OpenMP by, by, you know, CUDA Clang and so on, all these different targets, but we don't have this transformation. We, we're not comfortable with this connection. Okay, from an intrinsic to a vendor provided BC built in. Okay, why is that? Essentially, this needs to happen as early as possible in the, at the IR level. So we're adding an external dependency in our LLVM kind of, you know, pipeline. We need to link this BC library early on in our, in our, yeah, pipeline. We don't do this. We're not comfortable with doing this. We need to figure out a way that people will be happy with us doing this. Okay, obviously we're used to things resolving to external symbols, but then that's a runtime thing. It's not, it's not a compile time thing. Okay, this needs to be inline. We need to do lots and lots of stuff with this at the IR level. Okay, so there will still be cases where we need libclc potentially. It's not going to, you know, just disappear from our SQL implementation, hopefully, but we need to start pushing towards better kind of resolution, better use of these intrinsics in LLVM for offload in general. Okay, so why? Why? Share infrastructure, keep an eye, keep on the cutting edge of new developments, less compiler hacks, and we make SQL compilation eventually work upstream. It doesn't at the moment, but eventually we want it to, of course. We're trying to upstream as much as possible, but libclc is not upstreamable, and that's a problem. Okay, so the first step, try and have this discussion about making the intrinsics work for offload. Okay, so time, okay, time's up. So we need to have this link step at the IR level early on in the IR kind of pipeline. This is problematic for some people, but we need to talk about this. So please join in the discussion here. This is NVPTX co-gen for LLVM sign-in friends, if you have any opinions on this. Sorry, I kind of ran over a little bit, but yeah, any questions? Yeah, I was wondering, would it make sense to try to get rid of the mess by going to an MLIR type of approach, or like, what are the benefits or downsides to MLIR? So I'm not an expert. So the question was, are there benefits? Can we avoid this by going to MLIR? I think it's more fundamental than MLIR. I'm not an expert on MLIR, but I think we need basic resolution of intrinsics. Presumably with MLIR, you'll have, you know, other MLIR intrinsics that will need the same kind of treatment. We'll have the same questions there. So this is the first case study. This is the most simple case. We're not trying to implement the new FU built-ins with the accuracy thing. We're just trying to decide how do we make this dependency on this external BCLib work, and do it in a very, very confined sort of way. Yeah, thank you. Yeah. Two questions. First one is a tutorial to generate NVPTX from MLIR. There is a work section about linking with the Bitcoin library from NVIDIA. So what's the difference with this? And the second question is, you mentioned NVM, which is the closed source ptx generator from NVIDIA, and there is also the LLVM NVPTX backend. Are we reaching speed parity with the closed source one? It depends on the application. We find that with, so the second question first, is there still a big performance gap between the native, say, NVCC compiler and LLVM client? So in terms of DPC++, which is a fork of LLVM, we're attaining, say, roughly comparable performance, whether you're using SQL or you're using CUDA with NVCC, and then any improvements that we make to the kind of compiler or whatever, they're shared by client CUDA as well. So the first question again was, how is this different from? So essentially, when you're linking Bitcode or whatever, you're not using any LLVM intrinsics. You're just redirecting things yourself. You're not using intrinsics. So you need to do everything explicitly. You need to either have a specific kind of driver path that will do this for you or you need to specifically say, I want to link this in at this time or whatever. And so it's more manual. It's not happening automatically. It's not happening really within the compiler. It's happening at link time, LLVM link time. All right. Thank you, Hugh. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.44, "text": " Yeah, yeah, exactly. Okay, good afternoon. Yeah, so I'm going to be talking about compiler", "tokens": [865, 11, 1338, 11, 2293, 13, 1033, 11, 665, 6499, 13, 865, 11, 370, 286, 478, 516, 281, 312, 1417, 466, 31958], "temperature": 0.0, "avg_logprob": -0.2459247907002767, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.33523744344711304}, {"id": 1, "seek": 0, "start": 16.44, "end": 24.0, "text": " intrinsics in sickle in DPC plus plus specifically. This is Intel's open source sickle implementation.", "tokens": [28621, 1167, 294, 4998, 306, 294, 413, 12986, 1804, 1804, 4682, 13, 639, 307, 19762, 311, 1269, 4009, 4998, 306, 11420, 13], "temperature": 0.0, "avg_logprob": -0.2459247907002767, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.33523744344711304}, {"id": 2, "seek": 0, "start": 24.0, "end": 29.36, "text": " This is what I work on. Yeah, so hopefully I'll be able to say something without saying", "tokens": [639, 307, 437, 286, 589, 322, 13, 865, 11, 370, 4696, 286, 603, 312, 1075, 281, 584, 746, 1553, 1566], "temperature": 0.0, "avg_logprob": -0.2459247907002767, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.33523744344711304}, {"id": 3, "seek": 2936, "start": 29.36, "end": 35.88, "text": " too much in 10 minutes. Yeah, so code play. I work for code play. We had the first sickle", "tokens": [886, 709, 294, 1266, 2077, 13, 865, 11, 370, 3089, 862, 13, 286, 589, 337, 3089, 862, 13, 492, 632, 264, 700, 4998, 306], "temperature": 0.0, "avg_logprob": -0.20000660636208273, "compression_ratio": 1.711297071129707, "no_speech_prob": 9.973353007808328e-05}, {"id": 4, "seek": 2936, "start": 35.88, "end": 42.64, "text": " implementation, compute CBP. We now work. We were acquired by Intel. So now we work", "tokens": [11420, 11, 14722, 18745, 47, 13, 492, 586, 589, 13, 492, 645, 17554, 538, 19762, 13, 407, 586, 321, 589], "temperature": 0.0, "avg_logprob": -0.20000660636208273, "compression_ratio": 1.711297071129707, "no_speech_prob": 9.973353007808328e-05}, {"id": 5, "seek": 2936, "start": 42.64, "end": 47.64, "text": " on the Intel sickle implementation DPC plus plus. That's what I work on. We have lots", "tokens": [322, 264, 19762, 4998, 306, 11420, 413, 12986, 1804, 1804, 13, 663, 311, 437, 286, 589, 322, 13, 492, 362, 3195], "temperature": 0.0, "avg_logprob": -0.20000660636208273, "compression_ratio": 1.711297071129707, "no_speech_prob": 9.973353007808328e-05}, {"id": 6, "seek": 2936, "start": 47.64, "end": 51.56, "text": " of partners, you know, hardware companies, that kind of thing, whoever needs an open", "tokens": [295, 4462, 11, 291, 458, 11, 8837, 3431, 11, 300, 733, 295, 551, 11, 11387, 2203, 364, 1269], "temperature": 0.0, "avg_logprob": -0.20000660636208273, "compression_ratio": 1.711297071129707, "no_speech_prob": 9.973353007808328e-05}, {"id": 7, "seek": 2936, "start": 51.56, "end": 55.16, "text": " CL implementation, sickle implementation, and so on, come to us.", "tokens": [12855, 11420, 11, 4998, 306, 11420, 11, 293, 370, 322, 11, 808, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.20000660636208273, "compression_ratio": 1.711297071129707, "no_speech_prob": 9.973353007808328e-05}, {"id": 8, "seek": 5516, "start": 55.16, "end": 61.76, "text": " Yeah, so sickle is a single source heterogeneous programming API. So you can write single source", "tokens": [865, 11, 370, 4998, 306, 307, 257, 2167, 4009, 20789, 31112, 9410, 9362, 13, 407, 291, 393, 2464, 2167, 4009], "temperature": 0.0, "avg_logprob": -0.2100245723985646, "compression_ratio": 1.458762886597938, "no_speech_prob": 9.274899639422074e-05}, {"id": 9, "seek": 5516, "start": 61.76, "end": 71.64, "text": " code that can run on NVIDIA, Intel, AMD GPUs, close to the mic. Okay, voice up. Yeah, so", "tokens": [3089, 300, 393, 1190, 322, 426, 3958, 6914, 11, 19762, 11, 34808, 18407, 82, 11, 1998, 281, 264, 3123, 13, 1033, 11, 3177, 493, 13, 865, 11, 370], "temperature": 0.0, "avg_logprob": -0.2100245723985646, "compression_ratio": 1.458762886597938, "no_speech_prob": 9.274899639422074e-05}, {"id": 10, "seek": 5516, "start": 71.64, "end": 76.6, "text": " it's great for someone who's developing scientific applications to be able to write single source", "tokens": [309, 311, 869, 337, 1580, 567, 311, 6416, 8134, 5821, 281, 312, 1075, 281, 2464, 2167, 4009], "temperature": 0.0, "avg_logprob": -0.2100245723985646, "compression_ratio": 1.458762886597938, "no_speech_prob": 9.274899639422074e-05}, {"id": 11, "seek": 7660, "start": 76.6, "end": 85.75999999999999, "text": " code that runs on whatever GPU the implementation enables, such as CUDA, level zero for Intel,", "tokens": [3089, 300, 6676, 322, 2035, 18407, 264, 11420, 17077, 11, 1270, 382, 29777, 7509, 11, 1496, 4018, 337, 19762, 11], "temperature": 0.0, "avg_logprob": -0.17572250751533894, "compression_ratio": 1.4453441295546559, "no_speech_prob": 5.9858928580069914e-05}, {"id": 12, "seek": 7660, "start": 85.75999999999999, "end": 91.28, "text": " AMD GPUs, and so on. Yeah, this is a really good thing. So I work specifically on the", "tokens": [34808, 18407, 82, 11, 293, 370, 322, 13, 865, 11, 341, 307, 257, 534, 665, 551, 13, 407, 286, 589, 4682, 322, 264], "temperature": 0.0, "avg_logprob": -0.17572250751533894, "compression_ratio": 1.4453441295546559, "no_speech_prob": 5.9858928580069914e-05}, {"id": 13, "seek": 7660, "start": 91.28, "end": 98.52, "text": " NVIDIA and the HIP, the AMD backends for DPC plus plus. Okay, so yeah, I just want to talk", "tokens": [426, 3958, 6914, 293, 264, 389, 9139, 11, 264, 34808, 646, 2581, 337, 413, 12986, 1804, 1804, 13, 1033, 11, 370, 1338, 11, 286, 445, 528, 281, 751], "temperature": 0.0, "avg_logprob": -0.17572250751533894, "compression_ratio": 1.4453441295546559, "no_speech_prob": 5.9858928580069914e-05}, {"id": 14, "seek": 7660, "start": 98.52, "end": 102.63999999999999, "text": " a little bit about compiler intrinsics and how kind of, you know, math function calls", "tokens": [257, 707, 857, 466, 31958, 28621, 1167, 293, 577, 733, 295, 11, 291, 458, 11, 5221, 2445, 5498], "temperature": 0.0, "avg_logprob": -0.17572250751533894, "compression_ratio": 1.4453441295546559, "no_speech_prob": 5.9858928580069914e-05}, {"id": 15, "seek": 10264, "start": 102.64, "end": 107.68, "text": " work in sickle and DPC plus plus at the moment, and how we can hopefully improve them so that", "tokens": [589, 294, 4998, 306, 293, 413, 12986, 1804, 1804, 412, 264, 1623, 11, 293, 577, 321, 393, 4696, 3470, 552, 370, 300], "temperature": 0.0, "avg_logprob": -0.21064695818670864, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9201694158255123e-05}, {"id": 16, "seek": 10264, "start": 107.68, "end": 113.12, "text": " we're contributing upstream. So what happens to sickle cause? So essentially, you get your", "tokens": [321, 434, 19270, 33915, 13, 407, 437, 2314, 281, 4998, 306, 3082, 30, 407, 4476, 11, 291, 483, 428], "temperature": 0.0, "avg_logprob": -0.21064695818670864, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9201694158255123e-05}, {"id": 17, "seek": 10264, "start": 113.12, "end": 118.8, "text": " sickle cause in your source code. This is redirected to spear V open CL cause F, you", "tokens": [4998, 306, 3082, 294, 428, 4009, 3089, 13, 639, 307, 29066, 292, 281, 26993, 691, 1269, 12855, 3082, 479, 11, 291], "temperature": 0.0, "avg_logprob": -0.21064695818670864, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9201694158255123e-05}, {"id": 18, "seek": 10264, "start": 118.8, "end": 123.2, "text": " compile the spear V, you make a spear V module, this is a symbol within the spear V module,", "tokens": [31413, 264, 26993, 691, 11, 291, 652, 257, 26993, 691, 10088, 11, 341, 307, 257, 5986, 1951, 264, 26993, 691, 10088, 11], "temperature": 0.0, "avg_logprob": -0.21064695818670864, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9201694158255123e-05}, {"id": 19, "seek": 10264, "start": 123.2, "end": 129.68, "text": " and then that is the implementation is provided by a CL level zero Vulkan driver. Okay, as", "tokens": [293, 550, 300, 307, 264, 11420, 307, 5649, 538, 257, 12855, 1496, 4018, 41434, 5225, 6787, 13, 1033, 11, 382], "temperature": 0.0, "avg_logprob": -0.21064695818670864, "compression_ratio": 1.674074074074074, "no_speech_prob": 1.9201694158255123e-05}, {"id": 20, "seek": 12968, "start": 129.68, "end": 136.72, "text": " I said, I don't work on the spear V backend at all. I work the PTX, the CUDA or the AMD", "tokens": [286, 848, 11, 286, 500, 380, 589, 322, 264, 26993, 691, 38087, 412, 439, 13, 286, 589, 264, 35460, 55, 11, 264, 29777, 7509, 420, 264, 34808], "temperature": 0.0, "avg_logprob": -0.15598201751708984, "compression_ratio": 1.7083333333333333, "no_speech_prob": 5.502570729731815e-06}, {"id": 21, "seek": 12968, "start": 136.72, "end": 141.44, "text": " GPU backends. So what do we do with these symbols so that we get to the native implementations?", "tokens": [18407, 646, 2581, 13, 407, 437, 360, 321, 360, 365, 613, 16944, 370, 300, 321, 483, 281, 264, 8470, 4445, 763, 30], "temperature": 0.0, "avg_logprob": -0.15598201751708984, "compression_ratio": 1.7083333333333333, "no_speech_prob": 5.502570729731815e-06}, {"id": 22, "seek": 12968, "start": 141.44, "end": 145.36, "text": " We're not trying to reinvent the wheel. We're not trying to do anything that the people", "tokens": [492, 434, 406, 1382, 281, 33477, 264, 5589, 13, 492, 434, 406, 1382, 281, 360, 1340, 300, 264, 561], "temperature": 0.0, "avg_logprob": -0.15598201751708984, "compression_ratio": 1.7083333333333333, "no_speech_prob": 5.502570729731815e-06}, {"id": 23, "seek": 12968, "start": 145.36, "end": 150.56, "text": " who are making the GPUs aren't doing already. We're just trying to redirect to that. So", "tokens": [567, 366, 1455, 264, 18407, 82, 3212, 380, 884, 1217, 13, 492, 434, 445, 1382, 281, 29066, 281, 300, 13, 407], "temperature": 0.0, "avg_logprob": -0.15598201751708984, "compression_ratio": 1.7083333333333333, "no_speech_prob": 5.502570729731815e-06}, {"id": 24, "seek": 12968, "start": 150.56, "end": 156.84, "text": " how do we go from this to that, and then compile to our PTX module, our AMD GPU module, HSA", "tokens": [577, 360, 321, 352, 490, 341, 281, 300, 11, 293, 550, 31413, 281, 527, 35460, 55, 10088, 11, 527, 34808, 18407, 10088, 11, 389, 8886], "temperature": 0.0, "avg_logprob": -0.15598201751708984, "compression_ratio": 1.7083333333333333, "no_speech_prob": 5.502570729731815e-06}, {"id": 25, "seek": 15684, "start": 156.84, "end": 168.32, "text": " module, and so on. So, yeah, how do we go from spear V OCL to NV cause F? So use the", "tokens": [10088, 11, 293, 370, 322, 13, 407, 11, 1338, 11, 577, 360, 321, 352, 490, 26993, 691, 422, 22458, 281, 46512, 3082, 479, 30, 407, 764, 264], "temperature": 0.0, "avg_logprob": -0.2488745440233935, "compression_ratio": 1.6, "no_speech_prob": 3.587567698559724e-05}, {"id": 26, "seek": 15684, "start": 168.32, "end": 172.88, "text": " shim library, easy peasy, that's fine. Okay, you just redirect it, you compile it to bitcode,", "tokens": [402, 332, 6405, 11, 1858, 520, 5871, 11, 300, 311, 2489, 13, 1033, 11, 291, 445, 29066, 309, 11, 291, 31413, 309, 281, 857, 22332, 11], "temperature": 0.0, "avg_logprob": -0.2488745440233935, "compression_ratio": 1.6, "no_speech_prob": 3.587567698559724e-05}, {"id": 27, "seek": 15684, "start": 172.88, "end": 177.44, "text": " you link it, a compilation time, and you get to this, this native bitcode implementation.", "tokens": [291, 2113, 309, 11, 257, 40261, 565, 11, 293, 291, 483, 281, 341, 11, 341, 8470, 857, 22332, 11420, 13], "temperature": 0.0, "avg_logprob": -0.2488745440233935, "compression_ratio": 1.6, "no_speech_prob": 3.587567698559724e-05}, {"id": 28, "seek": 15684, "start": 177.44, "end": 183.32, "text": " This is great. Okay, so we use libclc for this. So libclc is written in open CL. Okay, open", "tokens": [639, 307, 869, 13, 1033, 11, 370, 321, 764, 22854, 3474, 66, 337, 341, 13, 407, 22854, 3474, 66, 307, 3720, 294, 1269, 12855, 13, 1033, 11, 1269], "temperature": 0.0, "avg_logprob": -0.2488745440233935, "compression_ratio": 1.6, "no_speech_prob": 3.587567698559724e-05}, {"id": 29, "seek": 18332, "start": 183.32, "end": 188.51999999999998, "text": " CL does lots of stuff that SQL doesn't expose as easily like address spaces, that kind of", "tokens": [12855, 775, 3195, 295, 1507, 300, 19200, 1177, 380, 19219, 382, 3612, 411, 2985, 7673, 11, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.15275955200195312, "compression_ratio": 1.5992779783393503, "no_speech_prob": 4.240596172166988e-05}, {"id": 30, "seek": 18332, "start": 188.51999999999998, "end": 193.56, "text": " thing. So we write an open CL. This is great. This makes our lives really, really easy.", "tokens": [551, 13, 407, 321, 2464, 364, 1269, 12855, 13, 639, 307, 869, 13, 639, 1669, 527, 2909, 534, 11, 534, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15275955200195312, "compression_ratio": 1.5992779783393503, "no_speech_prob": 4.240596172166988e-05}, {"id": 31, "seek": 18332, "start": 193.56, "end": 200.6, "text": " We can do it. Say, before we get into this, just why do we want to use a BC library in", "tokens": [492, 393, 360, 309, 13, 6463, 11, 949, 321, 483, 666, 341, 11, 445, 983, 360, 321, 528, 281, 764, 257, 14359, 6405, 294], "temperature": 0.0, "avg_logprob": -0.15275955200195312, "compression_ratio": 1.5992779783393503, "no_speech_prob": 4.240596172166988e-05}, {"id": 32, "seek": 18332, "start": 200.6, "end": 204.51999999999998, "text": " the first place? Why don't we use a.so? Why don't we just resolve to some symbol that", "tokens": [264, 700, 1081, 30, 1545, 500, 380, 321, 764, 257, 2411, 539, 30, 1545, 500, 380, 321, 445, 14151, 281, 512, 5986, 300], "temperature": 0.0, "avg_logprob": -0.15275955200195312, "compression_ratio": 1.5992779783393503, "no_speech_prob": 4.240596172166988e-05}, {"id": 33, "seek": 18332, "start": 204.51999999999998, "end": 210.64, "text": " then a runtime is called and we don't care about it? So on a GPU, the overhead of a function", "tokens": [550, 257, 34474, 307, 1219, 293, 321, 500, 380, 1127, 466, 309, 30, 407, 322, 257, 18407, 11, 264, 19922, 295, 257, 2445], "temperature": 0.0, "avg_logprob": -0.15275955200195312, "compression_ratio": 1.5992779783393503, "no_speech_prob": 4.240596172166988e-05}, {"id": 34, "seek": 21064, "start": 210.64, "end": 215.0, "text": " call is really high. Okay, it's because we lose information about, say, address spaces,", "tokens": [818, 307, 534, 1090, 13, 1033, 11, 309, 311, 570, 321, 3624, 1589, 466, 11, 584, 11, 2985, 7673, 11], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 35, "seek": 21064, "start": 215.0, "end": 220.55999999999997, "text": " that kind of thing. The GPU memory hierarchy is a bit more complex than, say, for CPU.", "tokens": [300, 733, 295, 551, 13, 440, 18407, 4675, 22333, 307, 257, 857, 544, 3997, 813, 11, 584, 11, 337, 13199, 13], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 36, "seek": 21064, "start": 220.55999999999997, "end": 224.11999999999998, "text": " So we really, really need to worry about this. We want to inline everything so we don't lose", "tokens": [407, 321, 534, 11, 534, 643, 281, 3292, 466, 341, 13, 492, 528, 281, 294, 1889, 1203, 370, 321, 500, 380, 3624], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 37, "seek": 21064, "start": 224.11999999999998, "end": 230.72, "text": " any information about our memory hierarchies. We also allow compile time branching of code", "tokens": [604, 1589, 466, 527, 4675, 35250, 530, 13, 492, 611, 2089, 31413, 565, 9819, 278, 295, 3089], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 38, "seek": 21064, "start": 230.72, "end": 233.79999999999998, "text": " based on the architecture, based on the back end, that kind of thing. We don't want to", "tokens": [2361, 322, 264, 9482, 11, 2361, 322, 264, 646, 917, 11, 300, 733, 295, 551, 13, 492, 500, 380, 528, 281], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 39, "seek": 21064, "start": 233.79999999999998, "end": 237.0, "text": " have these checks at runtime. We want high performance. That's the name of the game for", "tokens": [362, 613, 13834, 412, 34474, 13, 492, 528, 1090, 3389, 13, 663, 311, 264, 1315, 295, 264, 1216, 337], "temperature": 0.0, "avg_logprob": -0.10029102439311013, "compression_ratio": 1.8253424657534247, "no_speech_prob": 5.218969818088226e-05}, {"id": 40, "seek": 23700, "start": 237.0, "end": 243.0, "text": " what we're doing. This gives us greater optimization opportunities as well. You can do lots of", "tokens": [437, 321, 434, 884, 13, 639, 2709, 505, 5044, 19618, 4786, 382, 731, 13, 509, 393, 360, 3195, 295], "temperature": 0.0, "avg_logprob": -0.19172746658325196, "compression_ratio": 1.5041666666666667, "no_speech_prob": 5.6411357945762575e-05}, {"id": 41, "seek": 23700, "start": 243.0, "end": 248.36, "text": " dead code elimination, lots of fun stuff in the middle end because you're doing all these", "tokens": [3116, 3089, 29224, 11, 3195, 295, 1019, 1507, 294, 264, 2808, 917, 570, 291, 434, 884, 439, 613], "temperature": 0.0, "avg_logprob": -0.19172746658325196, "compression_ratio": 1.5041666666666667, "no_speech_prob": 5.6411357945762575e-05}, {"id": 42, "seek": 23700, "start": 248.36, "end": 255.2, "text": " checks at the IR level. Okay, so this is just kind of what it looks like. So we just have", "tokens": [13834, 412, 264, 16486, 1496, 13, 1033, 11, 370, 341, 307, 445, 733, 295, 437, 309, 1542, 411, 13, 407, 321, 445, 362], "temperature": 0.0, "avg_logprob": -0.19172746658325196, "compression_ratio": 1.5041666666666667, "no_speech_prob": 5.6411357945762575e-05}, {"id": 43, "seek": 23700, "start": 255.2, "end": 261.52, "text": " Spirvio CR-Casef. We return NV-Casef. Great. Amazing. That's so easy. And then this is", "tokens": [1738, 347, 28226, 14123, 12, 34, 651, 69, 13, 492, 2736, 46512, 12, 34, 651, 69, 13, 3769, 13, 14165, 13, 663, 311, 370, 1858, 13, 400, 550, 341, 307], "temperature": 0.0, "avg_logprob": -0.19172746658325196, "compression_ratio": 1.5041666666666667, "no_speech_prob": 5.6411357945762575e-05}, {"id": 44, "seek": 26152, "start": 261.52, "end": 267.64, "text": " the implementation which is provided by NVIDIA. This is in-bit code. We link this, and then", "tokens": [264, 11420, 597, 307, 5649, 538, 426, 3958, 6914, 13, 639, 307, 294, 12, 5260, 3089, 13, 492, 2113, 341, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.2130490428996536, "compression_ratio": 1.6604651162790698, "no_speech_prob": 3.262979589635506e-05}, {"id": 45, "seek": 26152, "start": 267.64, "end": 276.56, "text": " this is just in-lined into our IR. This is great. Okay. Yes, so we're linking this echo", "tokens": [341, 307, 445, 294, 12, 13564, 666, 527, 16486, 13, 639, 307, 869, 13, 1033, 13, 1079, 11, 370, 321, 434, 25775, 341, 14300], "temperature": 0.0, "avg_logprob": -0.2130490428996536, "compression_ratio": 1.6604651162790698, "no_speech_prob": 3.262979589635506e-05}, {"id": 46, "seek": 26152, "start": 276.56, "end": 281.64, "text": " code with LipsyLC. Then we link that with the vendor-provided BC library. So we're linking,", "tokens": [3089, 365, 441, 2600, 88, 14766, 13, 1396, 321, 2113, 300, 365, 264, 24321, 12, 49911, 2112, 14359, 6405, 13, 407, 321, 434, 25775, 11], "temperature": 0.0, "avg_logprob": -0.2130490428996536, "compression_ratio": 1.6604651162790698, "no_speech_prob": 3.262979589635506e-05}, {"id": 47, "seek": 26152, "start": 281.64, "end": 288.47999999999996, "text": " linking. We get to the implementation. It's all in-lined. It's all great. We love it.", "tokens": [25775, 13, 492, 483, 281, 264, 11420, 13, 467, 311, 439, 294, 12, 13564, 13, 467, 311, 439, 869, 13, 492, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.2130490428996536, "compression_ratio": 1.6604651162790698, "no_speech_prob": 3.262979589635506e-05}, {"id": 48, "seek": 28848, "start": 288.48, "end": 295.0, "text": " So this works well, but so this is a bit of code from LipsyLC. Because we're dealing", "tokens": [407, 341, 1985, 731, 11, 457, 370, 341, 307, 257, 857, 295, 3089, 490, 441, 2600, 88, 14766, 13, 1436, 321, 434, 6260], "temperature": 0.0, "avg_logprob": -0.12316652139027913, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.2195721865282394e-05}, {"id": 49, "seek": 28848, "start": 295.0, "end": 299.16, "text": " in OpenCL-C, we could choose something else. We could write a native IR. We find that OpenCL", "tokens": [294, 7238, 22458, 12, 34, 11, 321, 727, 2826, 746, 1646, 13, 492, 727, 2464, 257, 8470, 16486, 13, 492, 915, 300, 7238, 22458], "temperature": 0.0, "avg_logprob": -0.12316652139027913, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.2195721865282394e-05}, {"id": 50, "seek": 28848, "start": 299.16, "end": 304.52000000000004, "text": " is actually easier to use than an easier to maintain than writing a native IR. So we end", "tokens": [307, 767, 3571, 281, 764, 813, 364, 3571, 281, 6909, 813, 3579, 257, 8470, 16486, 13, 407, 321, 917], "temperature": 0.0, "avg_logprob": -0.12316652139027913, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.2195721865282394e-05}, {"id": 51, "seek": 28848, "start": 304.52000000000004, "end": 309.88, "text": " up with some funny kind of problems with mangling and all this kind of thing. This isn't nice.", "tokens": [493, 365, 512, 4074, 733, 295, 2740, 365, 32432, 1688, 293, 439, 341, 733, 295, 551, 13, 639, 1943, 380, 1481, 13], "temperature": 0.0, "avg_logprob": -0.12316652139027913, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.2195721865282394e-05}, {"id": 52, "seek": 28848, "start": 309.88, "end": 315.44, "text": " Sometimes we need manual mangling. It's got to do with namespaces when they're interpreted", "tokens": [4803, 321, 643, 9688, 32432, 1688, 13, 467, 311, 658, 281, 360, 365, 5288, 79, 2116, 562, 436, 434, 26749], "temperature": 0.0, "avg_logprob": -0.12316652139027913, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.2195721865282394e-05}, {"id": 53, "seek": 31544, "start": 315.44, "end": 323.64, "text": " by the OpenCL mangler, unfortunately. Yes, we need to sometimes as well. Sometimes OpenCL", "tokens": [538, 264, 7238, 22458, 32432, 1918, 11, 7015, 13, 1079, 11, 321, 643, 281, 2171, 382, 731, 13, 4803, 7238, 22458], "temperature": 0.0, "avg_logprob": -0.1456556592668806, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.107971340592485e-05}, {"id": 54, "seek": 31544, "start": 323.64, "end": 327.32, "text": " isn't as good as we want it to be. So we need to actually write a native IR as well. So", "tokens": [1943, 380, 382, 665, 382, 321, 528, 309, 281, 312, 13, 407, 321, 643, 281, 767, 2464, 257, 8470, 16486, 382, 731, 13, 407], "temperature": 0.0, "avg_logprob": -0.1456556592668806, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.107971340592485e-05}, {"id": 55, "seek": 31544, "start": 327.32, "end": 337.12, "text": " it's a mix of LVM IR, LipsyLC. It's a bit messy. It's not great. Yes, so also we're", "tokens": [309, 311, 257, 2890, 295, 441, 53, 44, 16486, 11, 441, 2600, 88, 14766, 13, 467, 311, 257, 857, 16191, 13, 467, 311, 406, 869, 13, 1079, 11, 370, 611, 321, 434], "temperature": 0.0, "avg_logprob": -0.1456556592668806, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.107971340592485e-05}, {"id": 56, "seek": 31544, "start": 337.12, "end": 342.56, "text": " exposing some compiler internals here. This is the NVVM reflect pass, which essentially", "tokens": [33178, 512, 31958, 2154, 1124, 510, 13, 639, 307, 264, 46512, 53, 44, 5031, 1320, 11, 597, 4476], "temperature": 0.0, "avg_logprob": -0.1456556592668806, "compression_ratio": 1.5650224215246638, "no_speech_prob": 2.107971340592485e-05}, {"id": 57, "seek": 34256, "start": 342.56, "end": 346.8, "text": " takes your function call for NVVM reflect, replaces it with a numeric value. This is", "tokens": [2516, 428, 2445, 818, 337, 46512, 53, 44, 5031, 11, 46734, 309, 365, 257, 7866, 299, 2158, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.16551304878072537, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.8618189642438665e-05}, {"id": 58, "seek": 34256, "start": 346.8, "end": 353.92, "text": " totally done at the IR level, so you can branch at the IR level based on this is a high architecture,", "tokens": [3879, 1096, 412, 264, 16486, 1496, 11, 370, 291, 393, 9819, 412, 264, 16486, 1496, 2361, 322, 341, 307, 257, 1090, 9482, 11], "temperature": 0.0, "avg_logprob": -0.16551304878072537, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.8618189642438665e-05}, {"id": 59, "seek": 34256, "start": 353.92, "end": 360.0, "text": " a newer architecture. Do this new implementation, do this new built-in. There's an old architecture,", "tokens": [257, 17628, 9482, 13, 1144, 341, 777, 11420, 11, 360, 341, 777, 3094, 12, 259, 13, 821, 311, 364, 1331, 9482, 11], "temperature": 0.0, "avg_logprob": -0.16551304878072537, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.8618189642438665e-05}, {"id": 60, "seek": 34256, "start": 360.0, "end": 364.64, "text": " as well for things like rounding modes. This pass is used. We're exposing this in source", "tokens": [382, 731, 337, 721, 411, 48237, 14068, 13, 639, 1320, 307, 1143, 13, 492, 434, 33178, 341, 294, 4009], "temperature": 0.0, "avg_logprob": -0.16551304878072537, "compression_ratio": 1.6419213973799127, "no_speech_prob": 1.8618189642438665e-05}, {"id": 61, "seek": 36464, "start": 364.64, "end": 373.91999999999996, "text": " code through hacks. This isn't really, you know, it's not, it's not kosher. But it works.", "tokens": [3089, 807, 33617, 13, 639, 1943, 380, 534, 11, 291, 458, 11, 309, 311, 406, 11, 309, 311, 406, 19532, 511, 13, 583, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.20312863304501488, "compression_ratio": 1.605263157894737, "no_speech_prob": 3.264896076871082e-05}, {"id": 62, "seek": 36464, "start": 373.91999999999996, "end": 380.71999999999997, "text": " Who cares? Okay, but consider the new proposal to add FP accuracy attributes to math built-ins.", "tokens": [2102, 12310, 30, 1033, 11, 457, 1949, 264, 777, 11494, 281, 909, 36655, 14170, 17212, 281, 5221, 3094, 12, 1292, 13], "temperature": 0.0, "avg_logprob": -0.20312863304501488, "compression_ratio": 1.605263157894737, "no_speech_prob": 3.264896076871082e-05}, {"id": 63, "seek": 36464, "start": 380.71999999999997, "end": 386.68, "text": " This is where we have, say, FP built-in cars, and we specify the accuracy in ULP that we", "tokens": [639, 307, 689, 321, 362, 11, 584, 11, 36655, 3094, 12, 259, 5163, 11, 293, 321, 16500, 264, 14170, 294, 624, 45196, 300, 321], "temperature": 0.0, "avg_logprob": -0.20312863304501488, "compression_ratio": 1.605263157894737, "no_speech_prob": 3.264896076871082e-05}, {"id": 64, "seek": 36464, "start": 386.68, "end": 392.88, "text": " want it to be computed to. Okay, this is totally lost on us. Okay, so this is what it would", "tokens": [528, 309, 281, 312, 40610, 281, 13, 1033, 11, 341, 307, 3879, 2731, 322, 505, 13, 1033, 11, 370, 341, 307, 437, 309, 576], "temperature": 0.0, "avg_logprob": -0.20312863304501488, "compression_ratio": 1.605263157894737, "no_speech_prob": 3.264896076871082e-05}, {"id": 65, "seek": 39288, "start": 392.88, "end": 398.15999999999997, "text": " look like. Yeah, you have this attribute here. You've, FP max error. This is really, really", "tokens": [574, 411, 13, 865, 11, 291, 362, 341, 19667, 510, 13, 509, 600, 11, 36655, 11469, 6713, 13, 639, 307, 534, 11, 534], "temperature": 0.0, "avg_logprob": -0.10492157936096191, "compression_ratio": 1.7028985507246377, "no_speech_prob": 4.4590931793209165e-05}, {"id": 66, "seek": 39288, "start": 398.15999999999997, "end": 404.0, "text": " needed in SQL because SQL is targeting lots and lots of different platforms. All these platforms", "tokens": [2978, 294, 19200, 570, 19200, 307, 17918, 3195, 293, 3195, 295, 819, 9473, 13, 1057, 613, 9473], "temperature": 0.0, "avg_logprob": -0.10492157936096191, "compression_ratio": 1.7028985507246377, "no_speech_prob": 4.4590931793209165e-05}, {"id": 67, "seek": 39288, "start": 404.0, "end": 408.32, "text": " have different numerical accuracy guarantees. We really, really need this, but we don't use", "tokens": [362, 819, 29054, 14170, 32567, 13, 492, 534, 11, 534, 643, 341, 11, 457, 321, 500, 380, 764], "temperature": 0.0, "avg_logprob": -0.10492157936096191, "compression_ratio": 1.7028985507246377, "no_speech_prob": 4.4590931793209165e-05}, {"id": 68, "seek": 39288, "start": 408.32, "end": 415.04, "text": " built-ins at all. We're sorry, we don't use LVM intrinsics at all. So this is, we need to get to", "tokens": [3094, 12, 1292, 412, 439, 13, 492, 434, 2597, 11, 321, 500, 380, 764, 441, 53, 44, 28621, 1167, 412, 439, 13, 407, 341, 307, 11, 321, 643, 281, 483, 281], "temperature": 0.0, "avg_logprob": -0.10492157936096191, "compression_ratio": 1.7028985507246377, "no_speech_prob": 4.4590931793209165e-05}, {"id": 69, "seek": 39288, "start": 415.04, "end": 418.8, "text": " a point where we can start using this compiler infrastructure. We're not using it as much as", "tokens": [257, 935, 689, 321, 393, 722, 1228, 341, 31958, 6896, 13, 492, 434, 406, 1228, 309, 382, 709, 382], "temperature": 0.0, "avg_logprob": -0.10492157936096191, "compression_ratio": 1.7028985507246377, "no_speech_prob": 4.4590931793209165e-05}, {"id": 70, "seek": 41880, "start": 418.8, "end": 427.92, "text": " you want to. So we could do this using a libclc compiler kind of hack workaround. We do another,", "tokens": [291, 528, 281, 13, 407, 321, 727, 360, 341, 1228, 257, 22854, 3474, 66, 31958, 733, 295, 10339, 589, 25762, 13, 492, 360, 1071, 11], "temperature": 0.0, "avg_logprob": -0.1534079105482189, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.6181344108190387e-05}, {"id": 71, "seek": 41880, "start": 427.92, "end": 432.0, "text": " you know, pass, you just say compiler precision value. If it's that, do some precise square root.", "tokens": [291, 458, 11, 1320, 11, 291, 445, 584, 31958, 18356, 2158, 13, 759, 309, 311, 300, 11, 360, 512, 13600, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.1534079105482189, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.6181344108190387e-05}, {"id": 72, "seek": 41880, "start": 432.64, "end": 437.52, "text": " If it's not, do some approximate thing. Yeah, we could do that. Okay, the problem with libclc", "tokens": [759, 309, 311, 406, 11, 360, 512, 30874, 551, 13, 865, 11, 321, 727, 360, 300, 13, 1033, 11, 264, 1154, 365, 22854, 3474, 66], "temperature": 0.0, "avg_logprob": -0.1534079105482189, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.6181344108190387e-05}, {"id": 73, "seek": 41880, "start": 437.52, "end": 443.68, "text": " and this stuff, it's not upstreamable. Okay, it's, it's a collection of hacks. It's not totally", "tokens": [293, 341, 1507, 11, 309, 311, 406, 33915, 712, 13, 1033, 11, 309, 311, 11, 309, 311, 257, 5765, 295, 33617, 13, 467, 311, 406, 3879], "temperature": 0.0, "avg_logprob": -0.1534079105482189, "compression_ratio": 1.6916299559471366, "no_speech_prob": 1.6181344108190387e-05}, {"id": 74, "seek": 44368, "start": 443.68, "end": 449.04, "text": " hacked, but like it's a little bit messy. It's not written in the same API. It's lib, it's OpenCL", "tokens": [36218, 11, 457, 411, 309, 311, 257, 707, 857, 16191, 13, 467, 311, 406, 3720, 294, 264, 912, 9362, 13, 467, 311, 22854, 11, 309, 311, 7238, 22458], "temperature": 0.0, "avg_logprob": -0.10550843266879811, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.622080319270026e-05}, {"id": 75, "seek": 44368, "start": 449.04, "end": 453.76, "text": " and it's, it's LVM IR. It's messy. We can upstream this. We can all benefit from this.", "tokens": [293, 309, 311, 11, 309, 311, 441, 53, 44, 16486, 13, 467, 311, 16191, 13, 492, 393, 33915, 341, 13, 492, 393, 439, 5121, 490, 341, 13], "temperature": 0.0, "avg_logprob": -0.10550843266879811, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.622080319270026e-05}, {"id": 76, "seek": 44368, "start": 457.12, "end": 463.2, "text": " Okay, so the pro about doing some, another, adding another hack to the, the kind of passes,", "tokens": [1033, 11, 370, 264, 447, 466, 884, 512, 11, 1071, 11, 5127, 1071, 10339, 281, 264, 11, 264, 733, 295, 11335, 11], "temperature": 0.0, "avg_logprob": -0.10550843266879811, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.622080319270026e-05}, {"id": 77, "seek": 44368, "start": 463.2, "end": 467.44, "text": " another hack to the bunch is that it's easy to do. Okay, we can do this and we can keep going with our", "tokens": [1071, 10339, 281, 264, 3840, 307, 300, 309, 311, 1858, 281, 360, 13, 1033, 11, 321, 393, 360, 341, 293, 321, 393, 1066, 516, 365, 527], "temperature": 0.0, "avg_logprob": -0.10550843266879811, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.622080319270026e-05}, {"id": 78, "seek": 44368, "start": 468.08, "end": 472.08, "text": " libclc implementation. It's pretty straightforward. We've been doing this the whole time. Yeah,", "tokens": [22854, 3474, 66, 11420, 13, 467, 311, 1238, 15325, 13, 492, 600, 668, 884, 341, 264, 1379, 565, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.10550843266879811, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.622080319270026e-05}, {"id": 79, "seek": 47208, "start": 472.08, "end": 478.24, "text": " fine. We don't need to worry about the broader LVM concerns. However, we miss out on LVM community", "tokens": [2489, 13, 492, 500, 380, 643, 281, 3292, 466, 264, 13227, 441, 53, 44, 7389, 13, 2908, 11, 321, 1713, 484, 322, 441, 53, 44, 1768], "temperature": 0.0, "avg_logprob": -0.08421588334880892, "compression_ratio": 1.6568265682656826, "no_speech_prob": 3.696870771818794e-05}, {"id": 80, "seek": 47208, "start": 478.24, "end": 483.03999999999996, "text": " collaboration, which is why we're, we're here. And then how many of these workarounds do we", "tokens": [9363, 11, 597, 307, 983, 321, 434, 11, 321, 434, 510, 13, 400, 550, 577, 867, 295, 613, 589, 289, 4432, 360, 321], "temperature": 0.0, "avg_logprob": -0.08421588334880892, "compression_ratio": 1.6568265682656826, "no_speech_prob": 3.696870771818794e-05}, {"id": 81, "seek": 47208, "start": 483.03999999999996, "end": 488.71999999999997, "text": " actually need in order to keep up with the latest trends and then libclc as bad as it could be now,", "tokens": [767, 643, 294, 1668, 281, 1066, 493, 365, 264, 6792, 13892, 293, 550, 22854, 3474, 66, 382, 1578, 382, 309, 727, 312, 586, 11], "temperature": 0.0, "avg_logprob": -0.08421588334880892, "compression_ratio": 1.6568265682656826, "no_speech_prob": 3.696870771818794e-05}, {"id": 82, "seek": 47208, "start": 488.71999999999997, "end": 491.91999999999996, "text": " like it just degenerates into an absolute mess and we don't want that.", "tokens": [411, 309, 445, 40520, 1024, 666, 364, 8236, 2082, 293, 321, 500, 380, 528, 300, 13], "temperature": 0.0, "avg_logprob": -0.08421588334880892, "compression_ratio": 1.6568265682656826, "no_speech_prob": 3.696870771818794e-05}, {"id": 83, "seek": 47208, "start": 494.32, "end": 500.24, "text": " Okay, so we think the answer for this is to try and redirect, try and, try and actually", "tokens": [1033, 11, 370, 321, 519, 264, 1867, 337, 341, 307, 281, 853, 293, 29066, 11, 853, 293, 11, 853, 293, 767], "temperature": 0.0, "avg_logprob": -0.08421588334880892, "compression_ratio": 1.6568265682656826, "no_speech_prob": 3.696870771818794e-05}, {"id": 84, "seek": 50024, "start": 500.24, "end": 504.48, "text": " have it calling the compiler intrinsic. Okay, we want to use compiler intrinsic and then have", "tokens": [362, 309, 5141, 264, 31958, 35698, 13, 1033, 11, 321, 528, 281, 764, 31958, 35698, 293, 550, 362], "temperature": 0.0, "avg_logprob": -0.12153265656543379, "compression_ratio": 1.701067615658363, "no_speech_prob": 3.9359838410746306e-05}, {"id": 85, "seek": 50024, "start": 504.48, "end": 510.08, "text": " some generic behavior of these intrinsics for offload targets. Okay, and this would be used by", "tokens": [512, 19577, 5223, 295, 613, 28621, 1167, 337, 766, 2907, 12911, 13, 1033, 11, 293, 341, 576, 312, 1143, 538], "temperature": 0.0, "avg_logprob": -0.12153265656543379, "compression_ratio": 1.701067615658363, "no_speech_prob": 3.9359838410746306e-05}, {"id": 86, "seek": 50024, "start": 510.08, "end": 515.52, "text": " say OpenMP by, by, you know, CUDA Clang and so on, all these different targets, but we don't have", "tokens": [584, 7238, 12224, 538, 11, 538, 11, 291, 458, 11, 29777, 7509, 2033, 656, 293, 370, 322, 11, 439, 613, 819, 12911, 11, 457, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.12153265656543379, "compression_ratio": 1.701067615658363, "no_speech_prob": 3.9359838410746306e-05}, {"id": 87, "seek": 50024, "start": 515.52, "end": 521.36, "text": " this transformation. We, we're not comfortable with this connection. Okay, from an intrinsic to a", "tokens": [341, 9887, 13, 492, 11, 321, 434, 406, 4619, 365, 341, 4984, 13, 1033, 11, 490, 364, 35698, 281, 257], "temperature": 0.0, "avg_logprob": -0.12153265656543379, "compression_ratio": 1.701067615658363, "no_speech_prob": 3.9359838410746306e-05}, {"id": 88, "seek": 50024, "start": 521.36, "end": 527.28, "text": " vendor provided BC built in. Okay, why is that? Essentially, this needs to happen as early as", "tokens": [24321, 5649, 14359, 3094, 294, 13, 1033, 11, 983, 307, 300, 30, 23596, 11, 341, 2203, 281, 1051, 382, 2440, 382], "temperature": 0.0, "avg_logprob": -0.12153265656543379, "compression_ratio": 1.701067615658363, "no_speech_prob": 3.9359838410746306e-05}, {"id": 89, "seek": 52728, "start": 527.28, "end": 534.64, "text": " possible in the, at the IR level. So we're adding an external dependency in our LLVM kind of,", "tokens": [1944, 294, 264, 11, 412, 264, 16486, 1496, 13, 407, 321, 434, 5127, 364, 8320, 33621, 294, 527, 441, 43, 53, 44, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.07868849048177705, "compression_ratio": 1.740072202166065, "no_speech_prob": 8.395702025154606e-06}, {"id": 90, "seek": 52728, "start": 534.64, "end": 541.12, "text": " you know, pipeline. We need to link this BC library early on in our, in our, yeah, pipeline.", "tokens": [291, 458, 11, 15517, 13, 492, 643, 281, 2113, 341, 14359, 6405, 2440, 322, 294, 527, 11, 294, 527, 11, 1338, 11, 15517, 13], "temperature": 0.0, "avg_logprob": -0.07868849048177705, "compression_ratio": 1.740072202166065, "no_speech_prob": 8.395702025154606e-06}, {"id": 91, "seek": 52728, "start": 541.8399999999999, "end": 545.4399999999999, "text": " We don't do this. We're not comfortable with doing this. We need to figure out a way that people", "tokens": [492, 500, 380, 360, 341, 13, 492, 434, 406, 4619, 365, 884, 341, 13, 492, 643, 281, 2573, 484, 257, 636, 300, 561], "temperature": 0.0, "avg_logprob": -0.07868849048177705, "compression_ratio": 1.740072202166065, "no_speech_prob": 8.395702025154606e-06}, {"id": 92, "seek": 52728, "start": 545.4399999999999, "end": 551.52, "text": " will be happy with us doing this. Okay, obviously we're used to things resolving to external symbols,", "tokens": [486, 312, 2055, 365, 505, 884, 341, 13, 1033, 11, 2745, 321, 434, 1143, 281, 721, 49940, 281, 8320, 16944, 11], "temperature": 0.0, "avg_logprob": -0.07868849048177705, "compression_ratio": 1.740072202166065, "no_speech_prob": 8.395702025154606e-06}, {"id": 93, "seek": 52728, "start": 551.52, "end": 556.48, "text": " but then that's a runtime thing. It's not, it's not a compile time thing. Okay, this needs to be", "tokens": [457, 550, 300, 311, 257, 34474, 551, 13, 467, 311, 406, 11, 309, 311, 406, 257, 31413, 565, 551, 13, 1033, 11, 341, 2203, 281, 312], "temperature": 0.0, "avg_logprob": -0.07868849048177705, "compression_ratio": 1.740072202166065, "no_speech_prob": 8.395702025154606e-06}, {"id": 94, "seek": 55648, "start": 556.48, "end": 564.8000000000001, "text": " inline. We need to do lots and lots of stuff with this at the IR level. Okay, so there will still be", "tokens": [294, 1889, 13, 492, 643, 281, 360, 3195, 293, 3195, 295, 1507, 365, 341, 412, 264, 16486, 1496, 13, 1033, 11, 370, 456, 486, 920, 312], "temperature": 0.0, "avg_logprob": -0.11165093884025652, "compression_ratio": 1.540084388185654, "no_speech_prob": 2.143017809430603e-05}, {"id": 95, "seek": 55648, "start": 564.8000000000001, "end": 568.64, "text": " cases where we need libclc potentially. It's not going to, you know, just disappear from our", "tokens": [3331, 689, 321, 643, 22854, 3474, 66, 7263, 13, 467, 311, 406, 516, 281, 11, 291, 458, 11, 445, 11596, 490, 527], "temperature": 0.0, "avg_logprob": -0.11165093884025652, "compression_ratio": 1.540084388185654, "no_speech_prob": 2.143017809430603e-05}, {"id": 96, "seek": 55648, "start": 568.64, "end": 577.36, "text": " SQL implementation, hopefully, but we need to start pushing towards better kind of resolution,", "tokens": [19200, 11420, 11, 4696, 11, 457, 321, 643, 281, 722, 7380, 3030, 1101, 733, 295, 8669, 11], "temperature": 0.0, "avg_logprob": -0.11165093884025652, "compression_ratio": 1.540084388185654, "no_speech_prob": 2.143017809430603e-05}, {"id": 97, "seek": 55648, "start": 577.36, "end": 583.04, "text": " better use of these intrinsics in LLVM for offload in general. Okay, so why?", "tokens": [1101, 764, 295, 613, 28621, 1167, 294, 441, 43, 53, 44, 337, 766, 2907, 294, 2674, 13, 1033, 11, 370, 983, 30], "temperature": 0.0, "avg_logprob": -0.11165093884025652, "compression_ratio": 1.540084388185654, "no_speech_prob": 2.143017809430603e-05}, {"id": 98, "seek": 58304, "start": 583.04, "end": 588.9599999999999, "text": " Why? Share infrastructure, keep an eye, keep on the cutting edge of new developments,", "tokens": [1545, 30, 14945, 6896, 11, 1066, 364, 3313, 11, 1066, 322, 264, 6492, 4691, 295, 777, 20862, 11], "temperature": 0.0, "avg_logprob": -0.12424528251573877, "compression_ratio": 1.5923076923076922, "no_speech_prob": 2.9219627322163433e-05}, {"id": 99, "seek": 58304, "start": 588.9599999999999, "end": 594.16, "text": " less compiler hacks, and we make SQL compilation eventually work upstream. It doesn't at the", "tokens": [1570, 31958, 33617, 11, 293, 321, 652, 19200, 40261, 4728, 589, 33915, 13, 467, 1177, 380, 412, 264], "temperature": 0.0, "avg_logprob": -0.12424528251573877, "compression_ratio": 1.5923076923076922, "no_speech_prob": 2.9219627322163433e-05}, {"id": 100, "seek": 58304, "start": 594.16, "end": 598.24, "text": " moment, but eventually we want it to, of course. We're trying to upstream as much as possible,", "tokens": [1623, 11, 457, 4728, 321, 528, 309, 281, 11, 295, 1164, 13, 492, 434, 1382, 281, 33915, 382, 709, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.12424528251573877, "compression_ratio": 1.5923076923076922, "no_speech_prob": 2.9219627322163433e-05}, {"id": 101, "seek": 58304, "start": 598.24, "end": 600.4, "text": " but libclc is not upstreamable, and that's a problem.", "tokens": [457, 22854, 3474, 66, 307, 406, 33915, 712, 11, 293, 300, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12424528251573877, "compression_ratio": 1.5923076923076922, "no_speech_prob": 2.9219627322163433e-05}, {"id": 102, "seek": 58304, "start": 602.88, "end": 607.92, "text": " Okay, so the first step, try and have this discussion about making the intrinsics work", "tokens": [1033, 11, 370, 264, 700, 1823, 11, 853, 293, 362, 341, 5017, 466, 1455, 264, 28621, 1167, 589], "temperature": 0.0, "avg_logprob": -0.12424528251573877, "compression_ratio": 1.5923076923076922, "no_speech_prob": 2.9219627322163433e-05}, {"id": 103, "seek": 60792, "start": 607.92, "end": 614.8, "text": " for offload. Okay, so time, okay, time's up. So we need to have this link step at the IR level", "tokens": [337, 766, 2907, 13, 1033, 11, 370, 565, 11, 1392, 11, 565, 311, 493, 13, 407, 321, 643, 281, 362, 341, 2113, 1823, 412, 264, 16486, 1496], "temperature": 0.0, "avg_logprob": -0.17535689024798637, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.81985264539253e-05}, {"id": 104, "seek": 60792, "start": 614.8, "end": 619.4399999999999, "text": " early on in the IR kind of pipeline. This is problematic for some people, but we need to talk", "tokens": [2440, 322, 294, 264, 16486, 733, 295, 15517, 13, 639, 307, 19011, 337, 512, 561, 11, 457, 321, 643, 281, 751], "temperature": 0.0, "avg_logprob": -0.17535689024798637, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.81985264539253e-05}, {"id": 105, "seek": 60792, "start": 619.4399999999999, "end": 626.7199999999999, "text": " about this. So please join in the discussion here. This is NVPTX co-gen for LLVM sign-in friends,", "tokens": [466, 341, 13, 407, 1767, 3917, 294, 264, 5017, 510, 13, 639, 307, 46512, 47, 51, 55, 598, 12, 1766, 337, 441, 43, 53, 44, 1465, 12, 259, 1855, 11], "temperature": 0.0, "avg_logprob": -0.17535689024798637, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.81985264539253e-05}, {"id": 106, "seek": 60792, "start": 626.7199999999999, "end": 631.4399999999999, "text": " if you have any opinions on this. Sorry, I kind of ran over a little bit, but yeah, any questions?", "tokens": [498, 291, 362, 604, 11819, 322, 341, 13, 4919, 11, 286, 733, 295, 5872, 670, 257, 707, 857, 11, 457, 1338, 11, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.17535689024798637, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.81985264539253e-05}, {"id": 107, "seek": 63144, "start": 631.44, "end": 641.6800000000001, "text": " Yeah, I was wondering, would it make sense to try to get rid of the mess by going to an MLIR", "tokens": [865, 11, 286, 390, 6359, 11, 576, 309, 652, 2020, 281, 853, 281, 483, 3973, 295, 264, 2082, 538, 516, 281, 364, 21601, 7740], "temperature": 0.0, "avg_logprob": -0.14608243892067357, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011054738570237532}, {"id": 108, "seek": 63144, "start": 641.6800000000001, "end": 646.96, "text": " type of approach, or like, what are the benefits or downsides to MLIR?", "tokens": [2010, 295, 3109, 11, 420, 411, 11, 437, 366, 264, 5311, 420, 21554, 1875, 281, 21601, 7740, 30], "temperature": 0.0, "avg_logprob": -0.14608243892067357, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011054738570237532}, {"id": 109, "seek": 63144, "start": 647.6800000000001, "end": 654.6400000000001, "text": " So I'm not an expert. So the question was, are there benefits? Can we avoid this by going to MLIR?", "tokens": [407, 286, 478, 406, 364, 5844, 13, 407, 264, 1168, 390, 11, 366, 456, 5311, 30, 1664, 321, 5042, 341, 538, 516, 281, 21601, 7740, 30], "temperature": 0.0, "avg_logprob": -0.14608243892067357, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.00011054738570237532}, {"id": 110, "seek": 65464, "start": 654.64, "end": 663.36, "text": " I think it's more fundamental than MLIR. I'm not an expert on MLIR, but I think we need basic", "tokens": [286, 519, 309, 311, 544, 8088, 813, 21601, 7740, 13, 286, 478, 406, 364, 5844, 322, 21601, 7740, 11, 457, 286, 519, 321, 643, 3875], "temperature": 0.0, "avg_logprob": -0.10415218322257686, "compression_ratio": 1.7121771217712176, "no_speech_prob": 4.678314508055337e-05}, {"id": 111, "seek": 65464, "start": 663.36, "end": 669.6, "text": " resolution of intrinsics. Presumably with MLIR, you'll have, you know, other MLIR intrinsics that", "tokens": [8669, 295, 28621, 1167, 13, 2718, 449, 1188, 365, 21601, 7740, 11, 291, 603, 362, 11, 291, 458, 11, 661, 21601, 7740, 28621, 1167, 300], "temperature": 0.0, "avg_logprob": -0.10415218322257686, "compression_ratio": 1.7121771217712176, "no_speech_prob": 4.678314508055337e-05}, {"id": 112, "seek": 65464, "start": 669.6, "end": 673.52, "text": " will need the same kind of treatment. We'll have the same questions there. So this is the first", "tokens": [486, 643, 264, 912, 733, 295, 5032, 13, 492, 603, 362, 264, 912, 1651, 456, 13, 407, 341, 307, 264, 700], "temperature": 0.0, "avg_logprob": -0.10415218322257686, "compression_ratio": 1.7121771217712176, "no_speech_prob": 4.678314508055337e-05}, {"id": 113, "seek": 65464, "start": 674.64, "end": 677.6, "text": " case study. This is the most simple case. We're not trying to implement the new", "tokens": [1389, 2979, 13, 639, 307, 264, 881, 2199, 1389, 13, 492, 434, 406, 1382, 281, 4445, 264, 777], "temperature": 0.0, "avg_logprob": -0.10415218322257686, "compression_ratio": 1.7121771217712176, "no_speech_prob": 4.678314508055337e-05}, {"id": 114, "seek": 65464, "start": 678.3199999999999, "end": 682.72, "text": " FU built-ins with the accuracy thing. We're just trying to decide how do we make this dependency", "tokens": [479, 52, 3094, 12, 1292, 365, 264, 14170, 551, 13, 492, 434, 445, 1382, 281, 4536, 577, 360, 321, 652, 341, 33621], "temperature": 0.0, "avg_logprob": -0.10415218322257686, "compression_ratio": 1.7121771217712176, "no_speech_prob": 4.678314508055337e-05}, {"id": 115, "seek": 68272, "start": 682.72, "end": 689.36, "text": " on this external BCLib work, and do it in a very, very confined sort of way. Yeah, thank you.", "tokens": [322, 341, 8320, 14359, 43, 897, 589, 11, 293, 360, 309, 294, 257, 588, 11, 588, 31745, 1333, 295, 636, 13, 865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.26283059517542523, "compression_ratio": 1.4526748971193415, "no_speech_prob": 0.00019071092538069934}, {"id": 116, "seek": 68272, "start": 690.4, "end": 698.48, "text": " Yeah. Two questions. First one is a tutorial to generate NVPTX from MLIR. There is a work section", "tokens": [865, 13, 4453, 1651, 13, 2386, 472, 307, 257, 7073, 281, 8460, 46512, 47, 51, 55, 490, 21601, 7740, 13, 821, 307, 257, 589, 3541], "temperature": 0.0, "avg_logprob": -0.26283059517542523, "compression_ratio": 1.4526748971193415, "no_speech_prob": 0.00019071092538069934}, {"id": 117, "seek": 68272, "start": 698.48, "end": 702.96, "text": " about linking with the Bitcoin library from NVIDIA. So what's the difference with this?", "tokens": [466, 25775, 365, 264, 11414, 6405, 490, 426, 3958, 6914, 13, 407, 437, 311, 264, 2649, 365, 341, 30], "temperature": 0.0, "avg_logprob": -0.26283059517542523, "compression_ratio": 1.4526748971193415, "no_speech_prob": 0.00019071092538069934}, {"id": 118, "seek": 68272, "start": 703.52, "end": 708.96, "text": " And the second question is, you mentioned NVM, which is the closed source", "tokens": [400, 264, 1150, 1168, 307, 11, 291, 2835, 46512, 44, 11, 597, 307, 264, 5395, 4009], "temperature": 0.0, "avg_logprob": -0.26283059517542523, "compression_ratio": 1.4526748971193415, "no_speech_prob": 0.00019071092538069934}, {"id": 119, "seek": 70896, "start": 708.96, "end": 714.1600000000001, "text": " ptx generator from NVIDIA, and there is also the LLVM NVPTX backend.", "tokens": [280, 83, 87, 19265, 490, 426, 3958, 6914, 11, 293, 456, 307, 611, 264, 441, 43, 53, 44, 46512, 47, 51, 55, 38087, 13], "temperature": 0.0, "avg_logprob": -0.20331684378690498, "compression_ratio": 1.4265402843601895, "no_speech_prob": 0.00010195732465945184}, {"id": 120, "seek": 70896, "start": 716.0, "end": 720.4000000000001, "text": " Are we reaching speed parity with the closed source one?", "tokens": [2014, 321, 9906, 3073, 44747, 365, 264, 5395, 4009, 472, 30], "temperature": 0.0, "avg_logprob": -0.20331684378690498, "compression_ratio": 1.4265402843601895, "no_speech_prob": 0.00010195732465945184}, {"id": 121, "seek": 70896, "start": 721.76, "end": 725.2800000000001, "text": " It depends on the application. We find that with, so the second question first,", "tokens": [467, 5946, 322, 264, 3861, 13, 492, 915, 300, 365, 11, 370, 264, 1150, 1168, 700, 11], "temperature": 0.0, "avg_logprob": -0.20331684378690498, "compression_ratio": 1.4265402843601895, "no_speech_prob": 0.00010195732465945184}, {"id": 122, "seek": 70896, "start": 726.4000000000001, "end": 734.32, "text": " is there still a big performance gap between the native, say, NVCC compiler and LLVM client? So", "tokens": [307, 456, 920, 257, 955, 3389, 7417, 1296, 264, 8470, 11, 584, 11, 46512, 11717, 31958, 293, 441, 43, 53, 44, 6423, 30, 407], "temperature": 0.0, "avg_logprob": -0.20331684378690498, "compression_ratio": 1.4265402843601895, "no_speech_prob": 0.00010195732465945184}, {"id": 123, "seek": 73432, "start": 734.32, "end": 742.96, "text": " in terms of DPC++, which is a fork of LLVM, we're attaining, say, roughly comparable performance,", "tokens": [294, 2115, 295, 413, 12986, 25472, 11, 597, 307, 257, 17716, 295, 441, 43, 53, 44, 11, 321, 434, 951, 3686, 11, 584, 11, 9810, 25323, 3389, 11], "temperature": 0.0, "avg_logprob": -0.1393810083836685, "compression_ratio": 1.434782608695652, "no_speech_prob": 8.164011524058878e-05}, {"id": 124, "seek": 73432, "start": 743.6, "end": 749.9200000000001, "text": " whether you're using SQL or you're using CUDA with NVCC, and then any improvements that we make to", "tokens": [1968, 291, 434, 1228, 19200, 420, 291, 434, 1228, 29777, 7509, 365, 46512, 11717, 11, 293, 550, 604, 13797, 300, 321, 652, 281], "temperature": 0.0, "avg_logprob": -0.1393810083836685, "compression_ratio": 1.434782608695652, "no_speech_prob": 8.164011524058878e-05}, {"id": 125, "seek": 73432, "start": 749.9200000000001, "end": 756.5600000000001, "text": " the kind of compiler or whatever, they're shared by client CUDA as well. So the first question again", "tokens": [264, 733, 295, 31958, 420, 2035, 11, 436, 434, 5507, 538, 6423, 29777, 7509, 382, 731, 13, 407, 264, 700, 1168, 797], "temperature": 0.0, "avg_logprob": -0.1393810083836685, "compression_ratio": 1.434782608695652, "no_speech_prob": 8.164011524058878e-05}, {"id": 126, "seek": 75656, "start": 756.56, "end": 767.5999999999999, "text": " was, how is this different from? So essentially, when you're linking Bitcode or whatever,", "tokens": [390, 11, 577, 307, 341, 819, 490, 30, 407, 4476, 11, 562, 291, 434, 25775, 9101, 22332, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.12245266778128487, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.705604310904164e-05}, {"id": 127, "seek": 75656, "start": 768.64, "end": 776.4799999999999, "text": " you're not using any LLVM intrinsics. You're just redirecting things yourself. You're not", "tokens": [291, 434, 406, 1228, 604, 441, 43, 53, 44, 28621, 1167, 13, 509, 434, 445, 29066, 278, 721, 1803, 13, 509, 434, 406], "temperature": 0.0, "avg_logprob": -0.12245266778128487, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.705604310904164e-05}, {"id": 128, "seek": 75656, "start": 776.4799999999999, "end": 782.8, "text": " using intrinsics. So you need to do everything explicitly. You need to either have a specific", "tokens": [1228, 28621, 1167, 13, 407, 291, 643, 281, 360, 1203, 20803, 13, 509, 643, 281, 2139, 362, 257, 2685], "temperature": 0.0, "avg_logprob": -0.12245266778128487, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.705604310904164e-05}, {"id": 129, "seek": 78280, "start": 782.8, "end": 788.7199999999999, "text": " kind of driver path that will do this for you or you need to specifically say, I want to link this", "tokens": [733, 295, 6787, 3100, 300, 486, 360, 341, 337, 291, 420, 291, 643, 281, 4682, 584, 11, 286, 528, 281, 2113, 341], "temperature": 0.0, "avg_logprob": -0.15394839118508732, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.932222898583859e-05}, {"id": 130, "seek": 78280, "start": 788.7199999999999, "end": 794.0, "text": " in at this time or whatever. And so it's more manual. It's not happening automatically. It's", "tokens": [294, 412, 341, 565, 420, 2035, 13, 400, 370, 309, 311, 544, 9688, 13, 467, 311, 406, 2737, 6772, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.15394839118508732, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.932222898583859e-05}, {"id": 131, "seek": 78280, "start": 794.0, "end": 798.56, "text": " not happening really within the compiler. It's happening at link time, LLVM link time.", "tokens": [406, 2737, 534, 1951, 264, 31958, 13, 467, 311, 2737, 412, 2113, 565, 11, 441, 43, 53, 44, 2113, 565, 13], "temperature": 0.0, "avg_logprob": -0.15394839118508732, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.932222898583859e-05}, {"id": 132, "seek": 79856, "start": 798.56, "end": 811.3599999999999, "text": " All right. Thank you, Hugh. Thank you.", "tokens": [50364, 1057, 558, 13, 1044, 291, 11, 25893, 13, 1044, 291, 13, 51004], "temperature": 0.0, "avg_logprob": -0.5106580257415771, "compression_ratio": 1.027027027027027, "no_speech_prob": 0.00020697253057733178}], "language": "en"}