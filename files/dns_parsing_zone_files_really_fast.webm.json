{"text": " Hi everyone, I'm Jeroen, well I'm going to tell you something about parsing some files really fast and I worked for NL netlabs, oh yeah, so some numbers, there's some caveats here, so the fifth, I did not do measurements because I made finished the slides like today, so I did the measurements on the train so, but I think the 50 megabytes is actually slower, I'm pretty sure the 700 megabytes is correct and we will go beyond and I'm going to tell you how, so yeah, so basically the motivation is that currently the parser NSD isn't very fast and we have an operator where someone only takes the better part of an hour and at that point it stops being practical, so yeah, that's the motivation and I actually like to take you on a journey so that I went through, I will also show you the new algorithms but I also want to tell you why parsing some files currently in NSD at least is really slow and to do that we have to tell you a bit on parsing, so I've included an example with a whole world sea program and the NSD parser is based on lexing jak and that's really useful if you want to parse things like a computer language where each token has a meaning of in and of itself, for some files however that is definitely not a case, so if you look at the, also in this case I provide an example but everyone in the room will probably make out that on the last line I try to define an A record with a corresponding IP address and then what the zone parser actually does is it takes the A, it makes that the owner and then throws a syntax error because well an IP address is not really a valid record type obviously, so yeah lexing jak is really not a good choice but then there's also the fact that I think zone files are only more or less standardized, they're not really standardized and putting it modally and when you combine the two that just leads to a lot of trouble and well the first thing I did was analyze why the current parser is slow and the current parser is slow well, it's actually inherent to the tool because they're just not a good fit because what the lexer does is it gives you each token and then passes on to the parser but it does so by matching a whole bunch of inputs and then taking the longest one, the longest match and then executing corresponding action in which in the case of the NSD zone file parser means that you can, that it copies the token then tries to unescape it and for names it tries to, it needs the dots right because they have meaning in domain names and what it does there is that it actually splits the input and passes each label separately, that is of course copied and the parser then concatenates that all back together and that is not really a fast process. So my first thought was well what if I just change the process a bit and scrap lexen yak and cut all the memory allocations that gave us better numbers but they're not the numbers that I wanted to see right because under the 8 megabytes is, I mean you can express it in gigabytes a second but then it becomes even less impressive right than under megabytes a second so. So yeah I started looking into that and I will show you the algorithms that I used and I came up with in a minute but to make you understand why these algorithms work it's important that each and every one of you knows that your CPU is a pipeline CPU and all modern CPUs are pipeline CPUs and what that means is that each executing each instruction is not a single step it's actually a multi-stage process. So there's a fetch and decode step and there's a lot more to it in practice but this is a mechanism that was designed so that you to optimize performance in the CPU and the premise on getting fast codes is that you keep the pipeline nice and full. That does not always happen especially or one case where that doesn't happen is when you get a pipeline install and that happens essentially when there's when the next instruction that you want to execute depends on the result of the instruction that you're currently executing and in that case the CPU has installed a pipeline it has to wait until the result is printed back it can then go on to decode and then only then execute your instruction and you'll take a hit of a couple cycles. Then there's of course the well-known pipeline flashes and those happen essentially if there's a conditional jump for instance an F statement and the CPU goes on to load the instructions that come after that and if it turns out that it actually needs to execute other instructions then it needs to flush the pipeline and only then can it go on to execute your code. And there's bronze prediction that is used to improve the flow and modern CPUs actually do a pretty good job of that but well it's prediction so it's not it's not always right. And it turns out so if we look at that information then we can analyze why a parser is the process of parsing is just inherently slow because if we go over it byte by byte like the NSDE shown parser for example then before you can analyze the next byte you have to wait until you know you have to resolve the new state of your current byte. And also it turns out that as far as the CPU is concerned the zone files are just random right that anything can happen at any time so it's hard to predict branches in that case. Right so the base of the new instructions that I'm using at base is a thing called a CND or single instruction multiple data and my interest in all of this was really sparked by the CND JSON project and it caught my attention because they expressed their throughput in gigabytes a second. Now in the next slide I'm going to tell you something about the algorithms but I'm not going to go into them in great depth because there's not a whole lot of time so if you want to know more on that then I would advise you to watch the talk or just read the paper. CND what that is is actually an instruction set and what it does it adds factor in registers and instructions to operate on those registers and what that allows us to do is to classify blocks instead of just bytes and there's some trick re-involved and there's a super simple example on the slide but basically we can classify 16, 32 or 64 bytes in one go depending on your CPU and then we repeat that multiple times for each input that we actually want to know about and the idea is that we can cut branches and dependencies. So what's good to know about CND is that it's all vertical non-horizontal so it's really it's really an instruction that is executed for each of the inputs so you can actually do logic in CND and the way to work around that is to convert the inputs to a mask. So we would get a 64 bit mask for each of the inputs that we checked and with those bit masks in hand then the first thing that we are going to do is to classify all the escape bits because there's some files allow for escaping and this is actually an algorithm that CND JSON guys came up with and basically what we do is that for each uneven number of backslashes we take the next character and so that bit then represents the character that is actually escaped and we need that information so that we can identify the quoted sections or in the case of some zone files also the comment sections and this was actually kind of a hard problem because they don't have this problem in JSON documents but in zone files comments can cancel out quoted sections and quoted sections can contain semicolons and then new lines they limit comments but we only want the new lines that actually delimits the comment because what we really want to do is that we want to find out which of the characters that we identify as structural characters are contained in quoted sequences or in comments and there's a simple example in the bottom there so oh yeah I did a number of experiments but in the end it turned out that if there's a semicolon in the input we just branch so we have a slow path assuming that there's not too many comments in zone files which for generated zone files I guess it's okay and once we have that information all the bits that remain automatically belong to the non quoted strings and then and this is oversimplifying it but if we shift right and do an XOR then that would get us all the transitions and with that information we can then go on to create indexes of those because your CPU does not only provide SIMD instructions it also provides bit manipulation instructions really fast bit manipulation instructions so the first thing that it does is it takes the population count to find out how many transitions are actually in your input block and then we use the trailing zero count to find out the relative position of the bit and if we combine it with the index then that should give us the pointer to the exact input byte and there's some more trickery involved here because of course for zone files if there's an error we want to report that error and to do that we need a line count and quoted sections of course may contain just new lines but we don't want to worry about those in the parser because that would mean that each parse function would possibly need to update a line count and that would just not be very convenient so what we do there is we take an unlikely branch if there's new line in the quoted section which really doesn't happen it's an edge case in the case of zone files and we take the slope path to count all the new lines in the input or at least the one in the quoted sections and then once we generate a token for the actual, the limiting new line we add the number of new lines that we found in quoted sections yeah and that gives us basically that gives us a fast scanner in my initial measurements and I think it's a little bit fast now that would get me a scanning of two gigabytes a second for zone files at least with an older.com zone etc etc so there's caveats there too but it turns out that the rest of the DNS data because we of course we have to parse it we only now tokenize it we also have to parse it the rest of the DNS data allows for optimizations using cindy as well and of course we want to start with the data that occurs the most and that is of course the main names and with the cindy instruction we actually just repeat the scanning process we quickly identify all the dots we turn that into a bit mask and then use the bit manipulation instructions to go over the domain name because most of the time if we just fill in the length on the dot then that would give us a proper Y format and of course there's a slow path for edge cases as well there and next of course is the record type and normally I guess you would hash I'd initially just use binary search which is faster than just linear search of course but that took away quite some performance so we want a perfect actually we want a hash but then a hash table is pretty big and so I figured I want a perfect hash and it turns out we can do we can do that so if you take the first character of the records because there's not that many record types right and there's certainly if you take the first character there's never more than that many record never more than like eight or nine record types that start with the first letter so if you then take the last character and at length then it turns out that doesn't give me any collisions so we can also the hash of collisions occur but I mean for all the record types and what for 40 years it doesn't give me collisions so I guess we're good on that from with a number no for each record type someone asked if this only works for record types and then the number and answer is no it works for all record types because they're all closely they're all really close together right so they're and they're alphanumeric most of the time sometimes there's numbers so we just I think our uppercase or downcase it and then multiply together a good distribution and then just add length and that gives me that gives that gives me a unique key without collisions and from there I can just do a use in the instruction to do compare equal so I can do the exact right string compare and that gives me a really nice nice speed up yeah and it and the people who worked on some DJs and actually do a lot of did a lot of projects like using Cindy for for decoding base 64 so the plan is to incorporate all those things as well and then there's one tricky part your CPU actually supports multiple instructions that at least if you have modern CPU if you have like a pending for then you only get SSC 42 but we want our software to be able to run and all those devices without recompiling so we actually compile it four times in the case of xx86 then use the CPU ID instruction to pick the right one and then well it's still in progress projects I have hoped to be a little bit further along but unfortunately not it will be a standalone library because it might actually be useful to other people and that will make it easy to integrate into other projects it was initially just intended for NSD yeah the numbers are so far pretty good at least quite a bit better than what we have now I think it's possible to go to one gigabyte a second yeah so if you want to check it out there's a link in there and finally I want to there's slide with acknowledgement because these people help me a lot I just send them on unsolicited email at first and then I happen to get answers back and they help me and they even took a look at my presentation help me there as well so thanks to Jeff Daniel and all the sim DJs and people and with that I actually finished in time it's time for questions yes oh no but that's the slow path and exactly there the hash doesn't work but that's the slow path so we do a slow path sorry I should repeat the question what the person in the audience was actually referring to what happens if you we use a generic type notation where we start the type by type followed by a number and obviously it doesn't work there but it does the slow path so we have a slow path there is the person so complete that you can parse an output and you get the same output as a parsed in really good because I would so if the parser is good enough that to give you the exact same output as you put in no it does not do that no well you you I mean do you mean by access to white space exact or just yeah no it doesn't do that no and then but it's also not it does also strip comments yes yes yes yes yes yes yes how do you handle escape decimals because in the example you gave you strip and you have looked where the backslashes are yeah and then take the next character but if you have like backslash 003 then you need those four characters as a single unit to encode in the final I'm not actually I just do the I just really good yeah you're gonna have to I hope there's no more questions because you're gonna have to keep doing that but what I so what happens if I guess for certain type of input I record like a backslash 003 which encodes the byte with a value 3 single byte yeah how do you do that with your algorithm that just takes the next character when you have backslashes but that's just to that's just to so the question is what do I do with escape characters or with escape sequences and so what I explained on the what I do what happens with backslashes is just to tokenize so I don't strip any data I just mark out the starts in the ends of each string field and then the parsing comes after that so there's no data actually stripped yeah so the question is so the question is what's the output format and the output format in this case is just DNS wire format so the the idea here is that for each record it will invoke a callback and it will just give you wire format with pointers to where all the fields are in the like an internal description of the field so that you know the length and you know the type of the field yeah there is definitely value to large effectors because it takes less instructions so if you can do something so I did not look into using the GPU but yeah that might benefit so if you know I have not so the question is why do why does parsing zone files have to be fast well because they're they get reloaded quite a lot of times each time there's an update to a zone you need to reload you need to reload the zone so we want to yeah the so that happens multiple times like an hour it differs per zone right but in our case if it takes more than an hour and the operator actually or it takes the better part of an hour and the operator wants to lead more wants to reload more often then that becomes a problem right so we just need to be faster but then there's all the end so there's other benefits as well so NSD for instance we support zone verification where just before the zone goes live you can have a focus program to verify that your DNS stack data is correct and there you can use an AXFR or you can let the NSD feed you the zone data in which case you just get text representation and if the zone is big enough then you want that to be fast because it's in the critical path right yeah well actually if the question was if splitting the files and multi-threading is something that we consider well splitting the files no but split on them yeah well new lines yeah that can be tricky because zone files can contain parentheses which would then mean that the record continues on the next line but a colleague actually did do a parallel zone loading implementation and I guess we can even do that with with this implementation right because there the it was actually quite a bit faster but the scanning process still takes a long time because you go over it by by bite but now that we have a fast scanner there's no reason why we cannot also include like parallel parsing yeah that could work yeah so the question is if we did it", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Hi everyone, I'm Jeroen, well I'm going to tell you something about parsing some files", "tokens": [2421, 1518, 11, 286, 478, 508, 2032, 268, 11, 731, 286, 478, 516, 281, 980, 291, 746, 466, 21156, 278, 512, 7098], "temperature": 0.0, "avg_logprob": -0.38851660651129644, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.31892073154449463}, {"id": 1, "seek": 0, "start": 10.0, "end": 22.12, "text": " really fast and I worked for NL netlabs, oh yeah, so some numbers, there's some caveats", "tokens": [534, 2370, 293, 286, 2732, 337, 426, 43, 2533, 75, 17243, 11, 1954, 1338, 11, 370, 512, 3547, 11, 456, 311, 512, 11730, 1720], "temperature": 0.0, "avg_logprob": -0.38851660651129644, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.31892073154449463}, {"id": 2, "seek": 0, "start": 22.12, "end": 27.48, "text": " here, so the fifth, I did not do measurements because I made finished the slides like today,", "tokens": [510, 11, 370, 264, 9266, 11, 286, 630, 406, 360, 15383, 570, 286, 1027, 4335, 264, 9788, 411, 965, 11], "temperature": 0.0, "avg_logprob": -0.38851660651129644, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.31892073154449463}, {"id": 3, "seek": 2748, "start": 27.48, "end": 33.08, "text": " so I did the measurements on the train so, but I think the 50 megabytes is actually slower,", "tokens": [370, 286, 630, 264, 15383, 322, 264, 3847, 370, 11, 457, 286, 519, 264, 2625, 10816, 24538, 307, 767, 14009, 11], "temperature": 0.0, "avg_logprob": -0.18701239691840277, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0002347788686165586}, {"id": 4, "seek": 2748, "start": 33.08, "end": 39.16, "text": " I'm pretty sure the 700 megabytes is correct and we will go beyond and I'm going to tell", "tokens": [286, 478, 1238, 988, 264, 15204, 10816, 24538, 307, 3006, 293, 321, 486, 352, 4399, 293, 286, 478, 516, 281, 980], "temperature": 0.0, "avg_logprob": -0.18701239691840277, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0002347788686165586}, {"id": 5, "seek": 2748, "start": 39.16, "end": 46.2, "text": " you how, so yeah, so basically the motivation is that currently the parser NSD isn't very", "tokens": [291, 577, 11, 370, 1338, 11, 370, 1936, 264, 12335, 307, 300, 4362, 264, 21156, 260, 15943, 35, 1943, 380, 588], "temperature": 0.0, "avg_logprob": -0.18701239691840277, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0002347788686165586}, {"id": 6, "seek": 2748, "start": 46.2, "end": 51.68, "text": " fast and we have an operator where someone only takes the better part of an hour and", "tokens": [2370, 293, 321, 362, 364, 12973, 689, 1580, 787, 2516, 264, 1101, 644, 295, 364, 1773, 293], "temperature": 0.0, "avg_logprob": -0.18701239691840277, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.0002347788686165586}, {"id": 7, "seek": 5168, "start": 51.68, "end": 62.0, "text": " at that point it stops being practical, so yeah, that's the motivation and I actually", "tokens": [412, 300, 935, 309, 10094, 885, 8496, 11, 370, 1338, 11, 300, 311, 264, 12335, 293, 286, 767], "temperature": 0.0, "avg_logprob": -0.16916920588566706, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0001903063675854355}, {"id": 8, "seek": 5168, "start": 62.0, "end": 66.4, "text": " like to take you on a journey so that I went through, I will also show you the new algorithms", "tokens": [411, 281, 747, 291, 322, 257, 4671, 370, 300, 286, 1437, 807, 11, 286, 486, 611, 855, 291, 264, 777, 14642], "temperature": 0.0, "avg_logprob": -0.16916920588566706, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0001903063675854355}, {"id": 9, "seek": 5168, "start": 66.4, "end": 72.36, "text": " but I also want to tell you why parsing some files currently in NSD at least is really", "tokens": [457, 286, 611, 528, 281, 980, 291, 983, 21156, 278, 512, 7098, 4362, 294, 15943, 35, 412, 1935, 307, 534], "temperature": 0.0, "avg_logprob": -0.16916920588566706, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0001903063675854355}, {"id": 10, "seek": 5168, "start": 72.36, "end": 79.44, "text": " slow and to do that we have to tell you a bit on parsing, so I've included an example", "tokens": [2964, 293, 281, 360, 300, 321, 362, 281, 980, 291, 257, 857, 322, 21156, 278, 11, 370, 286, 600, 5556, 364, 1365], "temperature": 0.0, "avg_logprob": -0.16916920588566706, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0001903063675854355}, {"id": 11, "seek": 7944, "start": 79.44, "end": 85.08, "text": " with a whole world sea program and the NSD parser is based on lexing jak and that's really", "tokens": [365, 257, 1379, 1002, 4158, 1461, 293, 264, 15943, 35, 21156, 260, 307, 2361, 322, 476, 87, 278, 4207, 293, 300, 311, 534], "temperature": 0.0, "avg_logprob": -0.1885295867919922, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.00040750353946350515}, {"id": 12, "seek": 7944, "start": 85.08, "end": 90.92, "text": " useful if you want to parse things like a computer language where each token has a meaning", "tokens": [4420, 498, 291, 528, 281, 48377, 721, 411, 257, 3820, 2856, 689, 1184, 14862, 575, 257, 3620], "temperature": 0.0, "avg_logprob": -0.1885295867919922, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.00040750353946350515}, {"id": 13, "seek": 7944, "start": 90.92, "end": 99.16, "text": " of in and of itself, for some files however that is definitely not a case, so if you look", "tokens": [295, 294, 293, 295, 2564, 11, 337, 512, 7098, 4461, 300, 307, 2138, 406, 257, 1389, 11, 370, 498, 291, 574], "temperature": 0.0, "avg_logprob": -0.1885295867919922, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.00040750353946350515}, {"id": 14, "seek": 7944, "start": 99.16, "end": 105.52, "text": " at the, also in this case I provide an example but everyone in the room will probably make", "tokens": [412, 264, 11, 611, 294, 341, 1389, 286, 2893, 364, 1365, 457, 1518, 294, 264, 1808, 486, 1391, 652], "temperature": 0.0, "avg_logprob": -0.1885295867919922, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.00040750353946350515}, {"id": 15, "seek": 10552, "start": 105.52, "end": 110.52, "text": " out that on the last line I try to define an A record with a corresponding IP address", "tokens": [484, 300, 322, 264, 1036, 1622, 286, 853, 281, 6964, 364, 316, 2136, 365, 257, 11760, 8671, 2985], "temperature": 0.0, "avg_logprob": -0.15445663762647052, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0005332329892553389}, {"id": 16, "seek": 10552, "start": 110.52, "end": 115.8, "text": " and then what the zone parser actually does is it takes the A, it makes that the owner", "tokens": [293, 550, 437, 264, 6668, 21156, 260, 767, 775, 307, 309, 2516, 264, 316, 11, 309, 1669, 300, 264, 7289], "temperature": 0.0, "avg_logprob": -0.15445663762647052, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0005332329892553389}, {"id": 17, "seek": 10552, "start": 115.8, "end": 120.47999999999999, "text": " and then throws a syntax error because well an IP address is not really a valid record", "tokens": [293, 550, 19251, 257, 28431, 6713, 570, 731, 364, 8671, 2985, 307, 406, 534, 257, 7363, 2136], "temperature": 0.0, "avg_logprob": -0.15445663762647052, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0005332329892553389}, {"id": 18, "seek": 10552, "start": 120.47999999999999, "end": 130.8, "text": " type obviously, so yeah lexing jak is really not a good choice but then there's also the", "tokens": [2010, 2745, 11, 370, 1338, 476, 87, 278, 4207, 307, 534, 406, 257, 665, 3922, 457, 550, 456, 311, 611, 264], "temperature": 0.0, "avg_logprob": -0.15445663762647052, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0005332329892553389}, {"id": 19, "seek": 13080, "start": 130.8, "end": 139.48000000000002, "text": " fact that I think zone files are only more or less standardized, they're not really standardized", "tokens": [1186, 300, 286, 519, 6668, 7098, 366, 787, 544, 420, 1570, 31677, 11, 436, 434, 406, 534, 31677], "temperature": 0.0, "avg_logprob": -0.21058857440948486, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.0005453506601043046}, {"id": 20, "seek": 13080, "start": 139.48000000000002, "end": 149.56, "text": " and putting it modally and when you combine the two that just leads to a lot of trouble", "tokens": [293, 3372, 309, 1072, 379, 293, 562, 291, 10432, 264, 732, 300, 445, 6689, 281, 257, 688, 295, 5253], "temperature": 0.0, "avg_logprob": -0.21058857440948486, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.0005453506601043046}, {"id": 21, "seek": 13080, "start": 149.56, "end": 155.12, "text": " and well the first thing I did was analyze why the current parser is slow and the current", "tokens": [293, 731, 264, 700, 551, 286, 630, 390, 12477, 983, 264, 2190, 21156, 260, 307, 2964, 293, 264, 2190], "temperature": 0.0, "avg_logprob": -0.21058857440948486, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.0005453506601043046}, {"id": 22, "seek": 15512, "start": 155.12, "end": 161.0, "text": " parser is slow well, it's actually inherent to the tool because they're just not a good", "tokens": [21156, 260, 307, 2964, 731, 11, 309, 311, 767, 26387, 281, 264, 2290, 570, 436, 434, 445, 406, 257, 665], "temperature": 0.0, "avg_logprob": -0.1799966421994296, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0004785049532074481}, {"id": 23, "seek": 15512, "start": 161.0, "end": 168.4, "text": " fit because what the lexer does is it gives you each token and then passes on to the parser", "tokens": [3318, 570, 437, 264, 476, 87, 260, 775, 307, 309, 2709, 291, 1184, 14862, 293, 550, 11335, 322, 281, 264, 21156, 260], "temperature": 0.0, "avg_logprob": -0.1799966421994296, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0004785049532074481}, {"id": 24, "seek": 15512, "start": 168.4, "end": 174.88, "text": " but it does so by matching a whole bunch of inputs and then taking the longest one, the", "tokens": [457, 309, 775, 370, 538, 14324, 257, 1379, 3840, 295, 15743, 293, 550, 1940, 264, 15438, 472, 11, 264], "temperature": 0.0, "avg_logprob": -0.1799966421994296, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0004785049532074481}, {"id": 25, "seek": 15512, "start": 174.88, "end": 181.16, "text": " longest match and then executing corresponding action in which in the case of the NSD zone", "tokens": [15438, 2995, 293, 550, 32368, 11760, 3069, 294, 597, 294, 264, 1389, 295, 264, 15943, 35, 6668], "temperature": 0.0, "avg_logprob": -0.1799966421994296, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0004785049532074481}, {"id": 26, "seek": 18116, "start": 181.16, "end": 186.6, "text": " file parser means that you can, that it copies the token then tries to unescape it and for", "tokens": [3991, 21156, 260, 1355, 300, 291, 393, 11, 300, 309, 14341, 264, 14862, 550, 9898, 281, 517, 279, 4747, 309, 293, 337], "temperature": 0.0, "avg_logprob": -0.1445617073460629, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.00040444923797622323}, {"id": 27, "seek": 18116, "start": 186.6, "end": 193.12, "text": " names it tries to, it needs the dots right because they have meaning in domain names", "tokens": [5288, 309, 9898, 281, 11, 309, 2203, 264, 15026, 558, 570, 436, 362, 3620, 294, 9274, 5288], "temperature": 0.0, "avg_logprob": -0.1445617073460629, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.00040444923797622323}, {"id": 28, "seek": 18116, "start": 193.12, "end": 198.68, "text": " and what it does there is that it actually splits the input and passes each label separately,", "tokens": [293, 437, 309, 775, 456, 307, 300, 309, 767, 37741, 264, 4846, 293, 11335, 1184, 7645, 14759, 11], "temperature": 0.0, "avg_logprob": -0.1445617073460629, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.00040444923797622323}, {"id": 29, "seek": 18116, "start": 198.68, "end": 204.16, "text": " that is of course copied and the parser then concatenates that all back together and that", "tokens": [300, 307, 295, 1164, 25365, 293, 264, 21156, 260, 550, 1588, 7186, 1024, 300, 439, 646, 1214, 293, 300], "temperature": 0.0, "avg_logprob": -0.1445617073460629, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.00040444923797622323}, {"id": 30, "seek": 18116, "start": 204.16, "end": 208.56, "text": " is not really a fast process.", "tokens": [307, 406, 534, 257, 2370, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1445617073460629, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.00040444923797622323}, {"id": 31, "seek": 20856, "start": 208.56, "end": 214.36, "text": " So my first thought was well what if I just change the process a bit and scrap lexen", "tokens": [407, 452, 700, 1194, 390, 731, 437, 498, 286, 445, 1319, 264, 1399, 257, 857, 293, 23138, 476, 87, 268], "temperature": 0.0, "avg_logprob": -0.23806688645306756, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00032682076562196016}, {"id": 32, "seek": 20856, "start": 214.36, "end": 223.04, "text": " yak and cut all the memory allocations that gave us better numbers but they're not the", "tokens": [18603, 293, 1723, 439, 264, 4675, 12660, 763, 300, 2729, 505, 1101, 3547, 457, 436, 434, 406, 264], "temperature": 0.0, "avg_logprob": -0.23806688645306756, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00032682076562196016}, {"id": 33, "seek": 20856, "start": 223.04, "end": 228.0, "text": " numbers that I wanted to see right because under the 8 megabytes is, I mean you can express", "tokens": [3547, 300, 286, 1415, 281, 536, 558, 570, 833, 264, 1649, 10816, 24538, 307, 11, 286, 914, 291, 393, 5109], "temperature": 0.0, "avg_logprob": -0.23806688645306756, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00032682076562196016}, {"id": 34, "seek": 20856, "start": 228.0, "end": 233.84, "text": " it in gigabytes a second but then it becomes even less impressive right than under megabytes", "tokens": [309, 294, 42741, 257, 1150, 457, 550, 309, 3643, 754, 1570, 8992, 558, 813, 833, 10816, 24538], "temperature": 0.0, "avg_logprob": -0.23806688645306756, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.00032682076562196016}, {"id": 35, "seek": 23384, "start": 233.84, "end": 239.56, "text": " a second so.", "tokens": [257, 1150, 370, 13], "temperature": 0.0, "avg_logprob": -0.1440066047336744, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00019236131629440933}, {"id": 36, "seek": 23384, "start": 239.56, "end": 246.52, "text": " So yeah I started looking into that and I will show you the algorithms that I used and", "tokens": [407, 1338, 286, 1409, 1237, 666, 300, 293, 286, 486, 855, 291, 264, 14642, 300, 286, 1143, 293], "temperature": 0.0, "avg_logprob": -0.1440066047336744, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00019236131629440933}, {"id": 37, "seek": 23384, "start": 246.52, "end": 252.28, "text": " I came up with in a minute but to make you understand why these algorithms work it's", "tokens": [286, 1361, 493, 365, 294, 257, 3456, 457, 281, 652, 291, 1223, 983, 613, 14642, 589, 309, 311], "temperature": 0.0, "avg_logprob": -0.1440066047336744, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00019236131629440933}, {"id": 38, "seek": 23384, "start": 252.28, "end": 257.2, "text": " important that each and every one of you knows that your CPU is a pipeline CPU and all modern", "tokens": [1021, 300, 1184, 293, 633, 472, 295, 291, 3255, 300, 428, 13199, 307, 257, 15517, 13199, 293, 439, 4363], "temperature": 0.0, "avg_logprob": -0.1440066047336744, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00019236131629440933}, {"id": 39, "seek": 25720, "start": 257.2, "end": 264.2, "text": " CPUs are pipeline CPUs and what that means is that each executing each instruction is", "tokens": [13199, 82, 366, 15517, 13199, 82, 293, 437, 300, 1355, 307, 300, 1184, 32368, 1184, 10951, 307], "temperature": 0.0, "avg_logprob": -0.15695221816437155, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.00036032131174579263}, {"id": 40, "seek": 25720, "start": 264.2, "end": 268.64, "text": " not a single step it's actually a multi-stage process.", "tokens": [406, 257, 2167, 1823, 309, 311, 767, 257, 4825, 12, 17882, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15695221816437155, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.00036032131174579263}, {"id": 41, "seek": 25720, "start": 268.64, "end": 275.28, "text": " So there's a fetch and decode step and there's a lot more to it in practice but this is a", "tokens": [407, 456, 311, 257, 23673, 293, 979, 1429, 1823, 293, 456, 311, 257, 688, 544, 281, 309, 294, 3124, 457, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.15695221816437155, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.00036032131174579263}, {"id": 42, "seek": 25720, "start": 275.28, "end": 283.96, "text": " mechanism that was designed so that you to optimize performance in the CPU and the premise", "tokens": [7513, 300, 390, 4761, 370, 300, 291, 281, 19719, 3389, 294, 264, 13199, 293, 264, 22045], "temperature": 0.0, "avg_logprob": -0.15695221816437155, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.00036032131174579263}, {"id": 43, "seek": 28396, "start": 283.96, "end": 290.56, "text": " on getting fast codes is that you keep the pipeline nice and full.", "tokens": [322, 1242, 2370, 14211, 307, 300, 291, 1066, 264, 15517, 1481, 293, 1577, 13], "temperature": 0.0, "avg_logprob": -0.1495616076743766, "compression_ratio": 1.8105263157894738, "no_speech_prob": 0.00013104402751196176}, {"id": 44, "seek": 28396, "start": 290.56, "end": 294.76, "text": " That does not always happen especially or one case where that doesn't happen is when", "tokens": [663, 775, 406, 1009, 1051, 2318, 420, 472, 1389, 689, 300, 1177, 380, 1051, 307, 562], "temperature": 0.0, "avg_logprob": -0.1495616076743766, "compression_ratio": 1.8105263157894738, "no_speech_prob": 0.00013104402751196176}, {"id": 45, "seek": 28396, "start": 294.76, "end": 301.08, "text": " you get a pipeline install and that happens essentially when there's when the next instruction", "tokens": [291, 483, 257, 15517, 3625, 293, 300, 2314, 4476, 562, 456, 311, 562, 264, 958, 10951], "temperature": 0.0, "avg_logprob": -0.1495616076743766, "compression_ratio": 1.8105263157894738, "no_speech_prob": 0.00013104402751196176}, {"id": 46, "seek": 28396, "start": 301.08, "end": 308.03999999999996, "text": " that you want to execute depends on the result of the instruction that you're currently executing", "tokens": [300, 291, 528, 281, 14483, 5946, 322, 264, 1874, 295, 264, 10951, 300, 291, 434, 4362, 32368], "temperature": 0.0, "avg_logprob": -0.1495616076743766, "compression_ratio": 1.8105263157894738, "no_speech_prob": 0.00013104402751196176}, {"id": 47, "seek": 30804, "start": 308.04, "end": 314.16, "text": " and in that case the CPU has installed a pipeline it has to wait until the result is printed", "tokens": [293, 294, 300, 1389, 264, 13199, 575, 8899, 257, 15517, 309, 575, 281, 1699, 1826, 264, 1874, 307, 13567], "temperature": 0.0, "avg_logprob": -0.15920975401594833, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.0001041427458403632}, {"id": 48, "seek": 30804, "start": 314.16, "end": 321.68, "text": " back it can then go on to decode and then only then execute your instruction and you'll", "tokens": [646, 309, 393, 550, 352, 322, 281, 979, 1429, 293, 550, 787, 550, 14483, 428, 10951, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.15920975401594833, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.0001041427458403632}, {"id": 49, "seek": 30804, "start": 321.68, "end": 325.84000000000003, "text": " take a hit of a couple cycles.", "tokens": [747, 257, 2045, 295, 257, 1916, 17796, 13], "temperature": 0.0, "avg_logprob": -0.15920975401594833, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.0001041427458403632}, {"id": 50, "seek": 30804, "start": 325.84000000000003, "end": 332.52000000000004, "text": " Then there's of course the well-known pipeline flashes and those happen essentially if there's", "tokens": [1396, 456, 311, 295, 1164, 264, 731, 12, 6861, 15517, 39665, 293, 729, 1051, 4476, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.15920975401594833, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.0001041427458403632}, {"id": 51, "seek": 33252, "start": 332.52, "end": 339.32, "text": " a conditional jump for instance an F statement and the CPU goes on to load the instructions", "tokens": [257, 27708, 3012, 337, 5197, 364, 479, 5629, 293, 264, 13199, 1709, 322, 281, 3677, 264, 9415], "temperature": 0.0, "avg_logprob": -0.1406455159187317, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00021473753440659493}, {"id": 52, "seek": 33252, "start": 339.32, "end": 344.0, "text": " that come after that and if it turns out that it actually needs to execute other instructions", "tokens": [300, 808, 934, 300, 293, 498, 309, 4523, 484, 300, 309, 767, 2203, 281, 14483, 661, 9415], "temperature": 0.0, "avg_logprob": -0.1406455159187317, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00021473753440659493}, {"id": 53, "seek": 33252, "start": 344.0, "end": 350.44, "text": " then it needs to flush the pipeline and only then can it go on to execute your code.", "tokens": [550, 309, 2203, 281, 19568, 264, 15517, 293, 787, 550, 393, 309, 352, 322, 281, 14483, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1406455159187317, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00021473753440659493}, {"id": 54, "seek": 33252, "start": 350.44, "end": 356.64, "text": " And there's bronze prediction that is used to improve the flow and modern CPUs actually", "tokens": [400, 456, 311, 25454, 17630, 300, 307, 1143, 281, 3470, 264, 3095, 293, 4363, 13199, 82, 767], "temperature": 0.0, "avg_logprob": -0.1406455159187317, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00021473753440659493}, {"id": 55, "seek": 35664, "start": 356.64, "end": 363.88, "text": " do a pretty good job of that but well it's prediction so it's not it's not always right.", "tokens": [360, 257, 1238, 665, 1691, 295, 300, 457, 731, 309, 311, 17630, 370, 309, 311, 406, 309, 311, 406, 1009, 558, 13], "temperature": 0.0, "avg_logprob": -0.16571017970209537, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.00017111949273385108}, {"id": 56, "seek": 35664, "start": 363.88, "end": 371.96, "text": " And it turns out so if we look at that information then we can analyze why a parser is the process", "tokens": [400, 309, 4523, 484, 370, 498, 321, 574, 412, 300, 1589, 550, 321, 393, 12477, 983, 257, 21156, 260, 307, 264, 1399], "temperature": 0.0, "avg_logprob": -0.16571017970209537, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.00017111949273385108}, {"id": 57, "seek": 35664, "start": 371.96, "end": 376.76, "text": " of parsing is just inherently slow because if we go over it byte by byte like the NSDE", "tokens": [295, 21156, 278, 307, 445, 27993, 2964, 570, 498, 321, 352, 670, 309, 40846, 538, 40846, 411, 264, 15943, 22296], "temperature": 0.0, "avg_logprob": -0.16571017970209537, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.00017111949273385108}, {"id": 58, "seek": 35664, "start": 376.76, "end": 382.71999999999997, "text": " shown parser for example then before you can analyze the next byte you have to wait until", "tokens": [4898, 21156, 260, 337, 1365, 550, 949, 291, 393, 12477, 264, 958, 40846, 291, 362, 281, 1699, 1826], "temperature": 0.0, "avg_logprob": -0.16571017970209537, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.00017111949273385108}, {"id": 59, "seek": 38272, "start": 382.72, "end": 388.08000000000004, "text": " you know you have to resolve the new state of your current byte.", "tokens": [291, 458, 291, 362, 281, 14151, 264, 777, 1785, 295, 428, 2190, 40846, 13], "temperature": 0.0, "avg_logprob": -0.1497727632522583, "compression_ratio": 1.5748792270531402, "no_speech_prob": 0.00032087729778140783}, {"id": 60, "seek": 38272, "start": 388.08000000000004, "end": 395.40000000000003, "text": " And also it turns out that as far as the CPU is concerned the zone files are just random", "tokens": [400, 611, 309, 4523, 484, 300, 382, 1400, 382, 264, 13199, 307, 5922, 264, 6668, 7098, 366, 445, 4974], "temperature": 0.0, "avg_logprob": -0.1497727632522583, "compression_ratio": 1.5748792270531402, "no_speech_prob": 0.00032087729778140783}, {"id": 61, "seek": 38272, "start": 395.40000000000003, "end": 403.56, "text": " right that anything can happen at any time so it's hard to predict branches in that case.", "tokens": [558, 300, 1340, 393, 1051, 412, 604, 565, 370, 309, 311, 1152, 281, 6069, 14770, 294, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1497727632522583, "compression_ratio": 1.5748792270531402, "no_speech_prob": 0.00032087729778140783}, {"id": 62, "seek": 38272, "start": 403.56, "end": 411.04, "text": " Right so the base of the new instructions that I'm using at base is a thing called", "tokens": [1779, 370, 264, 3096, 295, 264, 777, 9415, 300, 286, 478, 1228, 412, 3096, 307, 257, 551, 1219], "temperature": 0.0, "avg_logprob": -0.1497727632522583, "compression_ratio": 1.5748792270531402, "no_speech_prob": 0.00032087729778140783}, {"id": 63, "seek": 41104, "start": 411.04, "end": 416.52000000000004, "text": " a CND or single instruction multiple data and my interest in all of this was really", "tokens": [257, 383, 13360, 420, 2167, 10951, 3866, 1412, 293, 452, 1179, 294, 439, 295, 341, 390, 534], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 64, "seek": 41104, "start": 416.52000000000004, "end": 422.16, "text": " sparked by the CND JSON project and it caught my attention because they expressed their", "tokens": [39653, 538, 264, 383, 13360, 31828, 1716, 293, 309, 5415, 452, 3202, 570, 436, 12675, 641], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 65, "seek": 41104, "start": 422.16, "end": 424.56, "text": " throughput in gigabytes a second.", "tokens": [44629, 294, 42741, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 66, "seek": 41104, "start": 424.56, "end": 428.84000000000003, "text": " Now in the next slide I'm going to tell you something about the algorithms but I'm not", "tokens": [823, 294, 264, 958, 4137, 286, 478, 516, 281, 980, 291, 746, 466, 264, 14642, 457, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 67, "seek": 41104, "start": 428.84000000000003, "end": 433.8, "text": " going to go into them in great depth because there's not a whole lot of time so if you", "tokens": [516, 281, 352, 666, 552, 294, 869, 7161, 570, 456, 311, 406, 257, 1379, 688, 295, 565, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 68, "seek": 41104, "start": 433.8, "end": 440.72, "text": " want to know more on that then I would advise you to watch the talk or just read the paper.", "tokens": [528, 281, 458, 544, 322, 300, 550, 286, 576, 18312, 291, 281, 1159, 264, 751, 420, 445, 1401, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.15861848814297566, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0007753766840323806}, {"id": 69, "seek": 44072, "start": 440.72, "end": 447.84000000000003, "text": " CND what that is is actually an instruction set and what it does it adds factor in registers", "tokens": [383, 13360, 437, 300, 307, 307, 767, 364, 10951, 992, 293, 437, 309, 775, 309, 10860, 5952, 294, 38351], "temperature": 0.0, "avg_logprob": -0.18796352973351113, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0010718146804720163}, {"id": 70, "seek": 44072, "start": 447.84000000000003, "end": 454.40000000000003, "text": " and instructions to operate on those registers and what that allows us to do is to classify", "tokens": [293, 9415, 281, 9651, 322, 729, 38351, 293, 437, 300, 4045, 505, 281, 360, 307, 281, 33872], "temperature": 0.0, "avg_logprob": -0.18796352973351113, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0010718146804720163}, {"id": 71, "seek": 44072, "start": 454.40000000000003, "end": 462.68, "text": " blocks instead of just bytes and there's some trick re-involved and there's a super simple", "tokens": [8474, 2602, 295, 445, 36088, 293, 456, 311, 512, 4282, 319, 12, 259, 9646, 937, 293, 456, 311, 257, 1687, 2199], "temperature": 0.0, "avg_logprob": -0.18796352973351113, "compression_ratio": 1.6467065868263473, "no_speech_prob": 0.0010718146804720163}, {"id": 72, "seek": 46268, "start": 462.68, "end": 471.8, "text": " example on the slide but basically we can classify 16, 32 or 64 bytes in one go depending", "tokens": [1365, 322, 264, 4137, 457, 1936, 321, 393, 33872, 3165, 11, 8858, 420, 12145, 36088, 294, 472, 352, 5413], "temperature": 0.0, "avg_logprob": -0.12785085581116756, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0002983876911457628}, {"id": 73, "seek": 46268, "start": 471.8, "end": 477.28000000000003, "text": " on your CPU and then we repeat that multiple times for each input that we actually want", "tokens": [322, 428, 13199, 293, 550, 321, 7149, 300, 3866, 1413, 337, 1184, 4846, 300, 321, 767, 528], "temperature": 0.0, "avg_logprob": -0.12785085581116756, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0002983876911457628}, {"id": 74, "seek": 46268, "start": 477.28000000000003, "end": 486.72, "text": " to know about and the idea is that we can cut branches and dependencies.", "tokens": [281, 458, 466, 293, 264, 1558, 307, 300, 321, 393, 1723, 14770, 293, 36606, 13], "temperature": 0.0, "avg_logprob": -0.12785085581116756, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0002983876911457628}, {"id": 75, "seek": 48672, "start": 486.72, "end": 494.0, "text": " So what's good to know about CND is that it's all vertical non-horizontal so it's really", "tokens": [407, 437, 311, 665, 281, 458, 466, 383, 13360, 307, 300, 309, 311, 439, 9429, 2107, 12, 2335, 590, 896, 304, 370, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.1500122177768761, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.0001636776578379795}, {"id": 76, "seek": 48672, "start": 494.0, "end": 500.56, "text": " it's really an instruction that is executed for each of the inputs so you can actually", "tokens": [309, 311, 534, 364, 10951, 300, 307, 17577, 337, 1184, 295, 264, 15743, 370, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.1500122177768761, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.0001636776578379795}, {"id": 77, "seek": 48672, "start": 500.56, "end": 508.84000000000003, "text": " do logic in CND and the way to work around that is to convert the inputs to a mask.", "tokens": [360, 9952, 294, 383, 13360, 293, 264, 636, 281, 589, 926, 300, 307, 281, 7620, 264, 15743, 281, 257, 6094, 13], "temperature": 0.0, "avg_logprob": -0.1500122177768761, "compression_ratio": 1.5696969696969696, "no_speech_prob": 0.0001636776578379795}, {"id": 78, "seek": 50884, "start": 508.84, "end": 518.04, "text": " So we would get a 64 bit mask for each of the inputs that we checked and with those", "tokens": [407, 321, 576, 483, 257, 12145, 857, 6094, 337, 1184, 295, 264, 15743, 300, 321, 10033, 293, 365, 729], "temperature": 0.0, "avg_logprob": -0.16739383375788308, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.00013140353257767856}, {"id": 79, "seek": 50884, "start": 518.04, "end": 522.1999999999999, "text": " bit masks in hand then the first thing that we are going to do is to classify all the", "tokens": [857, 11830, 294, 1011, 550, 264, 700, 551, 300, 321, 366, 516, 281, 360, 307, 281, 33872, 439, 264], "temperature": 0.0, "avg_logprob": -0.16739383375788308, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.00013140353257767856}, {"id": 80, "seek": 50884, "start": 522.1999999999999, "end": 529.8, "text": " escape bits because there's some files allow for escaping and this is actually an algorithm", "tokens": [7615, 9239, 570, 456, 311, 512, 7098, 2089, 337, 32554, 293, 341, 307, 767, 364, 9284], "temperature": 0.0, "avg_logprob": -0.16739383375788308, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.00013140353257767856}, {"id": 81, "seek": 50884, "start": 529.8, "end": 537.0799999999999, "text": " that CND JSON guys came up with and basically what we do is that for each uneven number", "tokens": [300, 383, 13360, 31828, 1074, 1361, 493, 365, 293, 1936, 437, 321, 360, 307, 300, 337, 1184, 34022, 1230], "temperature": 0.0, "avg_logprob": -0.16739383375788308, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.00013140353257767856}, {"id": 82, "seek": 53708, "start": 537.08, "end": 544.6800000000001, "text": " of backslashes we take the next character and so that bit then represents the character", "tokens": [295, 646, 10418, 12808, 321, 747, 264, 958, 2517, 293, 370, 300, 857, 550, 8855, 264, 2517], "temperature": 0.0, "avg_logprob": -0.1409786816301017, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.00038471462903544307}, {"id": 83, "seek": 53708, "start": 544.6800000000001, "end": 554.0, "text": " that is actually escaped and we need that information so that we can identify the quoted", "tokens": [300, 307, 767, 20397, 293, 321, 643, 300, 1589, 370, 300, 321, 393, 5876, 264, 30047], "temperature": 0.0, "avg_logprob": -0.1409786816301017, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.00038471462903544307}, {"id": 84, "seek": 53708, "start": 554.0, "end": 564.32, "text": " sections or in the case of some zone files also the comment sections and this was actually", "tokens": [10863, 420, 294, 264, 1389, 295, 512, 6668, 7098, 611, 264, 2871, 10863, 293, 341, 390, 767], "temperature": 0.0, "avg_logprob": -0.1409786816301017, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.00038471462903544307}, {"id": 85, "seek": 56432, "start": 564.32, "end": 569.0, "text": " kind of a hard problem because they don't have this problem in JSON documents but in", "tokens": [733, 295, 257, 1152, 1154, 570, 436, 500, 380, 362, 341, 1154, 294, 31828, 8512, 457, 294], "temperature": 0.0, "avg_logprob": -0.15797444331793137, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0002676307922229171}, {"id": 86, "seek": 56432, "start": 569.0, "end": 575.5200000000001, "text": " zone files comments can cancel out quoted sections and quoted sections can contain semicolons", "tokens": [6668, 7098, 3053, 393, 10373, 484, 30047, 10863, 293, 30047, 10863, 393, 5304, 27515, 401, 892], "temperature": 0.0, "avg_logprob": -0.15797444331793137, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0002676307922229171}, {"id": 87, "seek": 56432, "start": 575.5200000000001, "end": 583.1600000000001, "text": " and then new lines they limit comments but we only want the new lines that actually delimits", "tokens": [293, 550, 777, 3876, 436, 4948, 3053, 457, 321, 787, 528, 264, 777, 3876, 300, 767, 1103, 332, 1208], "temperature": 0.0, "avg_logprob": -0.15797444331793137, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0002676307922229171}, {"id": 88, "seek": 56432, "start": 583.1600000000001, "end": 591.84, "text": " the comment because what we really want to do is that we want to find out which of the", "tokens": [264, 2871, 570, 437, 321, 534, 528, 281, 360, 307, 300, 321, 528, 281, 915, 484, 597, 295, 264], "temperature": 0.0, "avg_logprob": -0.15797444331793137, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0002676307922229171}, {"id": 89, "seek": 59184, "start": 591.84, "end": 596.76, "text": " characters that we identify as structural characters are contained in quoted sequences", "tokens": [4342, 300, 321, 5876, 382, 15067, 4342, 366, 16212, 294, 30047, 22978], "temperature": 0.0, "avg_logprob": -0.18099338213602703, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.00012376871018204838}, {"id": 90, "seek": 59184, "start": 596.76, "end": 609.1600000000001, "text": " or in comments and there's a simple example in the bottom there so oh yeah I did a number", "tokens": [420, 294, 3053, 293, 456, 311, 257, 2199, 1365, 294, 264, 2767, 456, 370, 1954, 1338, 286, 630, 257, 1230], "temperature": 0.0, "avg_logprob": -0.18099338213602703, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.00012376871018204838}, {"id": 91, "seek": 59184, "start": 609.1600000000001, "end": 616.6800000000001, "text": " of experiments but in the end it turned out that if there's a semicolon in the input we", "tokens": [295, 12050, 457, 294, 264, 917, 309, 3574, 484, 300, 498, 456, 311, 257, 27515, 38780, 294, 264, 4846, 321], "temperature": 0.0, "avg_logprob": -0.18099338213602703, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.00012376871018204838}, {"id": 92, "seek": 61668, "start": 616.68, "end": 622.92, "text": " just branch so we have a slow path assuming that there's not too many comments in zone", "tokens": [445, 9819, 370, 321, 362, 257, 2964, 3100, 11926, 300, 456, 311, 406, 886, 867, 3053, 294, 6668], "temperature": 0.0, "avg_logprob": -0.2036465917314802, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00028459119494073093}, {"id": 93, "seek": 61668, "start": 622.92, "end": 630.04, "text": " files which for generated zone files I guess it's okay and once we have that information", "tokens": [7098, 597, 337, 10833, 6668, 7098, 286, 2041, 309, 311, 1392, 293, 1564, 321, 362, 300, 1589], "temperature": 0.0, "avg_logprob": -0.2036465917314802, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00028459119494073093}, {"id": 94, "seek": 61668, "start": 630.04, "end": 638.92, "text": " all the bits that remain automatically belong to the non quoted strings and then and this", "tokens": [439, 264, 9239, 300, 6222, 6772, 5784, 281, 264, 2107, 30047, 13985, 293, 550, 293, 341], "temperature": 0.0, "avg_logprob": -0.2036465917314802, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00028459119494073093}, {"id": 95, "seek": 61668, "start": 638.92, "end": 643.64, "text": " is oversimplifying it but if we shift right and do an XOR then that would get us all the", "tokens": [307, 15488, 332, 564, 5489, 309, 457, 498, 321, 5513, 558, 293, 360, 364, 1783, 2483, 550, 300, 576, 483, 505, 439, 264], "temperature": 0.0, "avg_logprob": -0.2036465917314802, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00028459119494073093}, {"id": 96, "seek": 64364, "start": 643.64, "end": 656.56, "text": " transitions and with that information we can then go on to create indexes of those because", "tokens": [23767, 293, 365, 300, 1589, 321, 393, 550, 352, 322, 281, 1884, 8186, 279, 295, 729, 570], "temperature": 0.0, "avg_logprob": -0.14438140175559305, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.00024031104112509638}, {"id": 97, "seek": 64364, "start": 656.56, "end": 663.16, "text": " your CPU does not only provide SIMD instructions it also provides bit manipulation instructions", "tokens": [428, 13199, 775, 406, 787, 2893, 24738, 35, 9415, 309, 611, 6417, 857, 26475, 9415], "temperature": 0.0, "avg_logprob": -0.14438140175559305, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.00024031104112509638}, {"id": 98, "seek": 64364, "start": 663.16, "end": 668.8, "text": " really fast bit manipulation instructions so the first thing that it does is it takes", "tokens": [534, 2370, 857, 26475, 9415, 370, 264, 700, 551, 300, 309, 775, 307, 309, 2516], "temperature": 0.0, "avg_logprob": -0.14438140175559305, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.00024031104112509638}, {"id": 99, "seek": 66880, "start": 668.8, "end": 674.92, "text": " the population count to find out how many transitions are actually in your input block", "tokens": [264, 4415, 1207, 281, 915, 484, 577, 867, 23767, 366, 767, 294, 428, 4846, 3461], "temperature": 0.0, "avg_logprob": -0.12667582947530864, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.00014835649926681072}, {"id": 100, "seek": 66880, "start": 674.92, "end": 680.3199999999999, "text": " and then we use the trailing zero count to find out the relative position of the bit", "tokens": [293, 550, 321, 764, 264, 944, 4883, 4018, 1207, 281, 915, 484, 264, 4972, 2535, 295, 264, 857], "temperature": 0.0, "avg_logprob": -0.12667582947530864, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.00014835649926681072}, {"id": 101, "seek": 66880, "start": 680.3199999999999, "end": 686.28, "text": " and if we combine it with the index then that should give us the pointer to the exact input", "tokens": [293, 498, 321, 10432, 309, 365, 264, 8186, 550, 300, 820, 976, 505, 264, 23918, 281, 264, 1900, 4846], "temperature": 0.0, "avg_logprob": -0.12667582947530864, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.00014835649926681072}, {"id": 102, "seek": 66880, "start": 686.28, "end": 698.4799999999999, "text": " byte and there's some more trickery involved here because of course for zone files if there's", "tokens": [40846, 293, 456, 311, 512, 544, 4282, 2109, 3288, 510, 570, 295, 1164, 337, 6668, 7098, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.12667582947530864, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.00014835649926681072}, {"id": 103, "seek": 69848, "start": 698.48, "end": 703.28, "text": " an error we want to report that error and to do that we need a line count and quoted", "tokens": [364, 6713, 321, 528, 281, 2275, 300, 6713, 293, 281, 360, 300, 321, 643, 257, 1622, 1207, 293, 30047], "temperature": 0.0, "avg_logprob": -0.13493490793618812, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.0002227432414656505}, {"id": 104, "seek": 69848, "start": 703.28, "end": 708.48, "text": " sections of course may contain just new lines but we don't want to worry about those in", "tokens": [10863, 295, 1164, 815, 5304, 445, 777, 3876, 457, 321, 500, 380, 528, 281, 3292, 466, 729, 294], "temperature": 0.0, "avg_logprob": -0.13493490793618812, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.0002227432414656505}, {"id": 105, "seek": 69848, "start": 708.48, "end": 716.6800000000001, "text": " the parser because that would mean that each parse function would possibly need to update", "tokens": [264, 21156, 260, 570, 300, 576, 914, 300, 1184, 48377, 2445, 576, 6264, 643, 281, 5623], "temperature": 0.0, "avg_logprob": -0.13493490793618812, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.0002227432414656505}, {"id": 106, "seek": 69848, "start": 716.6800000000001, "end": 722.88, "text": " a line count and that would just not be very convenient so what we do there is we take an", "tokens": [257, 1622, 1207, 293, 300, 576, 445, 406, 312, 588, 10851, 370, 437, 321, 360, 456, 307, 321, 747, 364], "temperature": 0.0, "avg_logprob": -0.13493490793618812, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.0002227432414656505}, {"id": 107, "seek": 72288, "start": 722.88, "end": 728.52, "text": " unlikely branch if there's new line in the quoted section which really doesn't happen", "tokens": [17518, 9819, 498, 456, 311, 777, 1622, 294, 264, 30047, 3541, 597, 534, 1177, 380, 1051], "temperature": 0.0, "avg_logprob": -0.17278664611106695, "compression_ratio": 1.7889447236180904, "no_speech_prob": 6.561749614775181e-05}, {"id": 108, "seek": 72288, "start": 728.52, "end": 735.4, "text": " it's an edge case in the case of zone files and we take the slope path to count all the", "tokens": [309, 311, 364, 4691, 1389, 294, 264, 1389, 295, 6668, 7098, 293, 321, 747, 264, 13525, 3100, 281, 1207, 439, 264], "temperature": 0.0, "avg_logprob": -0.17278664611106695, "compression_ratio": 1.7889447236180904, "no_speech_prob": 6.561749614775181e-05}, {"id": 109, "seek": 72288, "start": 735.4, "end": 742.24, "text": " new lines in the input or at least the one in the quoted sections and then once we generate", "tokens": [777, 3876, 294, 264, 4846, 420, 412, 1935, 264, 472, 294, 264, 30047, 10863, 293, 550, 1564, 321, 8460], "temperature": 0.0, "avg_logprob": -0.17278664611106695, "compression_ratio": 1.7889447236180904, "no_speech_prob": 6.561749614775181e-05}, {"id": 110, "seek": 72288, "start": 742.24, "end": 747.88, "text": " a token for the actual, the limiting new line we add the number of new lines that we found", "tokens": [257, 14862, 337, 264, 3539, 11, 264, 22083, 777, 1622, 321, 909, 264, 1230, 295, 777, 3876, 300, 321, 1352], "temperature": 0.0, "avg_logprob": -0.17278664611106695, "compression_ratio": 1.7889447236180904, "no_speech_prob": 6.561749614775181e-05}, {"id": 111, "seek": 74788, "start": 747.88, "end": 757.28, "text": " in quoted sections yeah and that gives us basically that gives us a fast scanner in", "tokens": [294, 30047, 10863, 1338, 293, 300, 2709, 505, 1936, 300, 2709, 505, 257, 2370, 30211, 294], "temperature": 0.0, "avg_logprob": -0.21171439675723805, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0002042036794591695}, {"id": 112, "seek": 74788, "start": 757.28, "end": 762.24, "text": " my initial measurements and I think it's a little bit fast now that would get me a scanning", "tokens": [452, 5883, 15383, 293, 286, 519, 309, 311, 257, 707, 857, 2370, 586, 300, 576, 483, 385, 257, 27019], "temperature": 0.0, "avg_logprob": -0.21171439675723805, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0002042036794591695}, {"id": 113, "seek": 74788, "start": 762.24, "end": 768.04, "text": " of two gigabytes a second for zone files at least with an older.com zone etc etc so there's", "tokens": [295, 732, 42741, 257, 1150, 337, 6668, 7098, 412, 1935, 365, 364, 4906, 13, 1112, 6668, 5183, 5183, 370, 456, 311], "temperature": 0.0, "avg_logprob": -0.21171439675723805, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0002042036794591695}, {"id": 114, "seek": 74788, "start": 768.04, "end": 774.8, "text": " caveats there too but it turns out that the rest of the DNS data because we of course", "tokens": [11730, 1720, 456, 886, 457, 309, 4523, 484, 300, 264, 1472, 295, 264, 35153, 1412, 570, 321, 295, 1164], "temperature": 0.0, "avg_logprob": -0.21171439675723805, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0002042036794591695}, {"id": 115, "seek": 77480, "start": 774.8, "end": 779.12, "text": " we have to parse it we only now tokenize it we also have to parse it the rest of the DNS", "tokens": [321, 362, 281, 48377, 309, 321, 787, 586, 14862, 1125, 309, 321, 611, 362, 281, 48377, 309, 264, 1472, 295, 264, 35153], "temperature": 0.0, "avg_logprob": -0.1841812910035599, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0002204725460615009}, {"id": 116, "seek": 77480, "start": 779.12, "end": 788.52, "text": " data allows for optimizations using cindy as well and of course we want to start with", "tokens": [1412, 4045, 337, 5028, 14455, 1228, 269, 471, 88, 382, 731, 293, 295, 1164, 321, 528, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.1841812910035599, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0002204725460615009}, {"id": 117, "seek": 77480, "start": 788.52, "end": 795.0799999999999, "text": " the data that occurs the most and that is of course the main names and with the cindy", "tokens": [264, 1412, 300, 11843, 264, 881, 293, 300, 307, 295, 1164, 264, 2135, 5288, 293, 365, 264, 269, 471, 88], "temperature": 0.0, "avg_logprob": -0.1841812910035599, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0002204725460615009}, {"id": 118, "seek": 77480, "start": 795.0799999999999, "end": 802.16, "text": " instruction we actually just repeat the scanning process we quickly identify all the dots we", "tokens": [10951, 321, 767, 445, 7149, 264, 27019, 1399, 321, 2661, 5876, 439, 264, 15026, 321], "temperature": 0.0, "avg_logprob": -0.1841812910035599, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0002204725460615009}, {"id": 119, "seek": 80216, "start": 802.16, "end": 806.04, "text": " turn that into a bit mask and then use the bit manipulation instructions to go over the", "tokens": [1261, 300, 666, 257, 857, 6094, 293, 550, 764, 264, 857, 26475, 9415, 281, 352, 670, 264], "temperature": 0.0, "avg_logprob": -0.14728183746337892, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.00019192424952052534}, {"id": 120, "seek": 80216, "start": 806.04, "end": 810.56, "text": " domain name because most of the time if we just fill in the length on the dot then that", "tokens": [9274, 1315, 570, 881, 295, 264, 565, 498, 321, 445, 2836, 294, 264, 4641, 322, 264, 5893, 550, 300], "temperature": 0.0, "avg_logprob": -0.14728183746337892, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.00019192424952052534}, {"id": 121, "seek": 80216, "start": 810.56, "end": 815.7199999999999, "text": " would give us a proper Y format and of course there's a slow path for edge cases as well", "tokens": [576, 976, 505, 257, 2296, 398, 7877, 293, 295, 1164, 456, 311, 257, 2964, 3100, 337, 4691, 3331, 382, 731], "temperature": 0.0, "avg_logprob": -0.14728183746337892, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.00019192424952052534}, {"id": 122, "seek": 80216, "start": 815.7199999999999, "end": 827.4, "text": " there and next of course is the record type and normally I guess you would hash I'd initially", "tokens": [456, 293, 958, 295, 1164, 307, 264, 2136, 2010, 293, 5646, 286, 2041, 291, 576, 22019, 286, 1116, 9105], "temperature": 0.0, "avg_logprob": -0.14728183746337892, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.00019192424952052534}, {"id": 123, "seek": 82740, "start": 827.4, "end": 835.72, "text": " just use binary search which is faster than just linear search of course but that took", "tokens": [445, 764, 17434, 3164, 597, 307, 4663, 813, 445, 8213, 3164, 295, 1164, 457, 300, 1890], "temperature": 0.0, "avg_logprob": -0.1542677425202869, "compression_ratio": 1.7365853658536585, "no_speech_prob": 0.00023852621961850673}, {"id": 124, "seek": 82740, "start": 835.72, "end": 843.0799999999999, "text": " away quite some performance so we want a perfect actually we want a hash but then a hash table", "tokens": [1314, 1596, 512, 3389, 370, 321, 528, 257, 2176, 767, 321, 528, 257, 22019, 457, 550, 257, 22019, 3199], "temperature": 0.0, "avg_logprob": -0.1542677425202869, "compression_ratio": 1.7365853658536585, "no_speech_prob": 0.00023852621961850673}, {"id": 125, "seek": 82740, "start": 843.0799999999999, "end": 850.12, "text": " is pretty big and so I figured I want a perfect hash and it turns out we can do we can do", "tokens": [307, 1238, 955, 293, 370, 286, 8932, 286, 528, 257, 2176, 22019, 293, 309, 4523, 484, 321, 393, 360, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.1542677425202869, "compression_ratio": 1.7365853658536585, "no_speech_prob": 0.00023852621961850673}, {"id": 126, "seek": 82740, "start": 850.12, "end": 854.48, "text": " that so if you take the first character of the records because there's not that many", "tokens": [300, 370, 498, 291, 747, 264, 700, 2517, 295, 264, 7724, 570, 456, 311, 406, 300, 867], "temperature": 0.0, "avg_logprob": -0.1542677425202869, "compression_ratio": 1.7365853658536585, "no_speech_prob": 0.00023852621961850673}, {"id": 127, "seek": 85448, "start": 854.48, "end": 859.6800000000001, "text": " record types right and there's certainly if you take the first character there's never", "tokens": [2136, 3467, 558, 293, 456, 311, 3297, 498, 291, 747, 264, 700, 2517, 456, 311, 1128], "temperature": 0.0, "avg_logprob": -0.13104965747931066, "compression_ratio": 1.8967391304347827, "no_speech_prob": 0.00026644906029105186}, {"id": 128, "seek": 85448, "start": 859.6800000000001, "end": 866.24, "text": " more than that many record never more than like eight or nine record types that start", "tokens": [544, 813, 300, 867, 2136, 1128, 544, 813, 411, 3180, 420, 4949, 2136, 3467, 300, 722], "temperature": 0.0, "avg_logprob": -0.13104965747931066, "compression_ratio": 1.8967391304347827, "no_speech_prob": 0.00026644906029105186}, {"id": 129, "seek": 85448, "start": 866.24, "end": 870.88, "text": " with the first letter so if you then take the last character and at length then it turns", "tokens": [365, 264, 700, 5063, 370, 498, 291, 550, 747, 264, 1036, 2517, 293, 412, 4641, 550, 309, 4523], "temperature": 0.0, "avg_logprob": -0.13104965747931066, "compression_ratio": 1.8967391304347827, "no_speech_prob": 0.00026644906029105186}, {"id": 130, "seek": 85448, "start": 870.88, "end": 878.96, "text": " out that doesn't give me any collisions so we can also the hash of collisions occur but", "tokens": [484, 300, 1177, 380, 976, 385, 604, 46537, 370, 321, 393, 611, 264, 22019, 295, 46537, 5160, 457], "temperature": 0.0, "avg_logprob": -0.13104965747931066, "compression_ratio": 1.8967391304347827, "no_speech_prob": 0.00026644906029105186}, {"id": 131, "seek": 87896, "start": 878.96, "end": 884.32, "text": " I mean for all the record types and what for 40 years it doesn't give me collisions so", "tokens": [286, 914, 337, 439, 264, 2136, 3467, 293, 437, 337, 3356, 924, 309, 1177, 380, 976, 385, 46537, 370], "temperature": 0.0, "avg_logprob": -0.3445478497129498, "compression_ratio": 1.6646341463414633, "no_speech_prob": 0.0008438812219537795}, {"id": 132, "seek": 87896, "start": 884.32, "end": 900.4000000000001, "text": " I guess we're good on that from with a number no for each record type someone asked if this", "tokens": [286, 2041, 321, 434, 665, 322, 300, 490, 365, 257, 1230, 572, 337, 1184, 2136, 2010, 1580, 2351, 498, 341], "temperature": 0.0, "avg_logprob": -0.3445478497129498, "compression_ratio": 1.6646341463414633, "no_speech_prob": 0.0008438812219537795}, {"id": 133, "seek": 87896, "start": 900.4000000000001, "end": 907.64, "text": " only works for record types and then the number and answer is no it works for all record types", "tokens": [787, 1985, 337, 2136, 3467, 293, 550, 264, 1230, 293, 1867, 307, 572, 309, 1985, 337, 439, 2136, 3467], "temperature": 0.0, "avg_logprob": -0.3445478497129498, "compression_ratio": 1.6646341463414633, "no_speech_prob": 0.0008438812219537795}, {"id": 134, "seek": 90764, "start": 907.64, "end": 911.84, "text": " because they're all closely they're all really close together right so they're and they're", "tokens": [570, 436, 434, 439, 8185, 436, 434, 439, 534, 1998, 1214, 558, 370, 436, 434, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.18202319991922825, "compression_ratio": 1.9083333333333334, "no_speech_prob": 0.0006502605974674225}, {"id": 135, "seek": 90764, "start": 911.84, "end": 918.84, "text": " alphanumeric most of the time sometimes there's numbers so we just I think our uppercase or", "tokens": [419, 950, 282, 15583, 299, 881, 295, 264, 565, 2171, 456, 311, 3547, 370, 321, 445, 286, 519, 527, 11775, 2869, 651, 420], "temperature": 0.0, "avg_logprob": -0.18202319991922825, "compression_ratio": 1.9083333333333334, "no_speech_prob": 0.0006502605974674225}, {"id": 136, "seek": 90764, "start": 918.84, "end": 924.76, "text": " downcase it and then multiply together a good distribution and then just add length and that", "tokens": [760, 9765, 309, 293, 550, 12972, 1214, 257, 665, 7316, 293, 550, 445, 909, 4641, 293, 300], "temperature": 0.0, "avg_logprob": -0.18202319991922825, "compression_ratio": 1.9083333333333334, "no_speech_prob": 0.0006502605974674225}, {"id": 137, "seek": 90764, "start": 924.76, "end": 929.88, "text": " gives me that gives that gives me a unique key without collisions and from there I can", "tokens": [2709, 385, 300, 2709, 300, 2709, 385, 257, 3845, 2141, 1553, 46537, 293, 490, 456, 286, 393], "temperature": 0.0, "avg_logprob": -0.18202319991922825, "compression_ratio": 1.9083333333333334, "no_speech_prob": 0.0006502605974674225}, {"id": 138, "seek": 90764, "start": 929.88, "end": 937.16, "text": " just do a use in the instruction to do compare equal so I can do the exact right string compare", "tokens": [445, 360, 257, 764, 294, 264, 10951, 281, 360, 6794, 2681, 370, 286, 393, 360, 264, 1900, 558, 6798, 6794], "temperature": 0.0, "avg_logprob": -0.18202319991922825, "compression_ratio": 1.9083333333333334, "no_speech_prob": 0.0006502605974674225}, {"id": 139, "seek": 93716, "start": 937.16, "end": 946.04, "text": " and that gives me a really nice nice speed up yeah and it and the people who worked on", "tokens": [293, 300, 2709, 385, 257, 534, 1481, 1481, 3073, 493, 1338, 293, 309, 293, 264, 561, 567, 2732, 322], "temperature": 0.0, "avg_logprob": -0.19551447666052615, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.000372416980098933}, {"id": 140, "seek": 93716, "start": 946.04, "end": 954.48, "text": " some DJs and actually do a lot of did a lot of projects like using Cindy for for decoding", "tokens": [512, 13078, 82, 293, 767, 360, 257, 688, 295, 630, 257, 688, 295, 4455, 411, 1228, 32185, 337, 337, 979, 8616], "temperature": 0.0, "avg_logprob": -0.19551447666052615, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.000372416980098933}, {"id": 141, "seek": 93716, "start": 954.48, "end": 964.4, "text": " base 64 so the plan is to incorporate all those things as well and then there's one", "tokens": [3096, 12145, 370, 264, 1393, 307, 281, 16091, 439, 729, 721, 382, 731, 293, 550, 456, 311, 472], "temperature": 0.0, "avg_logprob": -0.19551447666052615, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.000372416980098933}, {"id": 142, "seek": 96440, "start": 964.4, "end": 971.1999999999999, "text": " tricky part your CPU actually supports multiple instructions that at least if you have modern", "tokens": [12414, 644, 428, 13199, 767, 9346, 3866, 9415, 300, 412, 1935, 498, 291, 362, 4363], "temperature": 0.0, "avg_logprob": -0.24798984214907788, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00047633572830818594}, {"id": 143, "seek": 96440, "start": 971.1999999999999, "end": 976.88, "text": " CPU if you have like a pending for then you only get SSC 42 but we want our software to", "tokens": [13199, 498, 291, 362, 411, 257, 32110, 337, 550, 291, 787, 483, 12238, 34, 14034, 457, 321, 528, 527, 4722, 281], "temperature": 0.0, "avg_logprob": -0.24798984214907788, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00047633572830818594}, {"id": 144, "seek": 96440, "start": 976.88, "end": 983.1999999999999, "text": " be able to run and all those devices without recompiling so we actually compile it four", "tokens": [312, 1075, 281, 1190, 293, 439, 729, 5759, 1553, 48000, 4883, 370, 321, 767, 31413, 309, 1451], "temperature": 0.0, "avg_logprob": -0.24798984214907788, "compression_ratio": 1.4861878453038675, "no_speech_prob": 0.00047633572830818594}, {"id": 145, "seek": 98320, "start": 983.2, "end": 997.5200000000001, "text": " times in the case of xx86 then use the CPU ID instruction to pick the right one and then", "tokens": [1413, 294, 264, 1389, 295, 2031, 87, 22193, 550, 764, 264, 13199, 7348, 10951, 281, 1888, 264, 558, 472, 293, 550], "temperature": 0.0, "avg_logprob": -0.14331551848864946, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.00015317289216909558}, {"id": 146, "seek": 98320, "start": 997.5200000000001, "end": 1003.32, "text": " well it's still in progress projects I have hoped to be a little bit further along but", "tokens": [731, 309, 311, 920, 294, 4205, 4455, 286, 362, 19737, 281, 312, 257, 707, 857, 3052, 2051, 457], "temperature": 0.0, "avg_logprob": -0.14331551848864946, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.00015317289216909558}, {"id": 147, "seek": 98320, "start": 1003.32, "end": 1007.76, "text": " unfortunately not it will be a standalone library because it might actually be useful", "tokens": [7015, 406, 309, 486, 312, 257, 37454, 6405, 570, 309, 1062, 767, 312, 4420], "temperature": 0.0, "avg_logprob": -0.14331551848864946, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.00015317289216909558}, {"id": 148, "seek": 100776, "start": 1007.76, "end": 1014.08, "text": " to other people and that will make it easy to integrate into other projects it was initially", "tokens": [281, 661, 561, 293, 300, 486, 652, 309, 1858, 281, 13365, 666, 661, 4455, 309, 390, 9105], "temperature": 0.0, "avg_logprob": -0.12032296191686871, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0005886931321583688}, {"id": 149, "seek": 100776, "start": 1014.08, "end": 1020.56, "text": " just intended for NSD yeah the numbers are so far pretty good at least quite a bit better", "tokens": [445, 10226, 337, 15943, 35, 1338, 264, 3547, 366, 370, 1400, 1238, 665, 412, 1935, 1596, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.12032296191686871, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0005886931321583688}, {"id": 150, "seek": 100776, "start": 1020.56, "end": 1030.08, "text": " than what we have now I think it's possible to go to one gigabyte a second yeah so if", "tokens": [813, 437, 321, 362, 586, 286, 519, 309, 311, 1944, 281, 352, 281, 472, 8741, 34529, 257, 1150, 1338, 370, 498], "temperature": 0.0, "avg_logprob": -0.12032296191686871, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0005886931321583688}, {"id": 151, "seek": 100776, "start": 1030.08, "end": 1035.6, "text": " you want to check it out there's a link in there and finally I want to there's slide", "tokens": [291, 528, 281, 1520, 309, 484, 456, 311, 257, 2113, 294, 456, 293, 2721, 286, 528, 281, 456, 311, 4137], "temperature": 0.0, "avg_logprob": -0.12032296191686871, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0005886931321583688}, {"id": 152, "seek": 103560, "start": 1035.6, "end": 1039.76, "text": " with acknowledgement because these people help me a lot I just send them on unsolicited", "tokens": [365, 47227, 570, 613, 561, 854, 385, 257, 688, 286, 445, 2845, 552, 322, 2693, 7940, 1226], "temperature": 0.0, "avg_logprob": -0.2090619859241304, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.000966979656368494}, {"id": 153, "seek": 103560, "start": 1039.76, "end": 1044.6799999999998, "text": " email at first and then I happen to get answers back and they help me and they even took a", "tokens": [3796, 412, 700, 293, 550, 286, 1051, 281, 483, 6338, 646, 293, 436, 854, 385, 293, 436, 754, 1890, 257], "temperature": 0.0, "avg_logprob": -0.2090619859241304, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.000966979656368494}, {"id": 154, "seek": 103560, "start": 1044.6799999999998, "end": 1050.8, "text": " look at my presentation help me there as well so thanks to Jeff Daniel and all the sim", "tokens": [574, 412, 452, 5860, 854, 385, 456, 382, 731, 370, 3231, 281, 7506, 8033, 293, 439, 264, 1034], "temperature": 0.0, "avg_logprob": -0.2090619859241304, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.000966979656368494}, {"id": 155, "seek": 105080, "start": 1050.8, "end": 1074.28, "text": " DJs and people and with that I actually finished in time it's time for questions yes oh no but", "tokens": [13078, 82, 293, 561, 293, 365, 300, 286, 767, 4335, 294, 565, 309, 311, 565, 337, 1651, 2086, 1954, 572, 457], "temperature": 0.0, "avg_logprob": -0.43287895202636717, "compression_ratio": 1.146341463414634, "no_speech_prob": 0.0005230005481280386}, {"id": 156, "seek": 107428, "start": 1074.28, "end": 1080.76, "text": " that's the slow path and exactly there the hash doesn't work but that's the slow path", "tokens": [300, 311, 264, 2964, 3100, 293, 2293, 456, 264, 22019, 1177, 380, 589, 457, 300, 311, 264, 2964, 3100], "temperature": 0.0, "avg_logprob": -0.19333623095256527, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.00030307177803479135}, {"id": 157, "seek": 107428, "start": 1080.76, "end": 1086.84, "text": " so we do a slow path sorry I should repeat the question what the person in the audience", "tokens": [370, 321, 360, 257, 2964, 3100, 2597, 286, 820, 7149, 264, 1168, 437, 264, 954, 294, 264, 4034], "temperature": 0.0, "avg_logprob": -0.19333623095256527, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.00030307177803479135}, {"id": 158, "seek": 107428, "start": 1086.84, "end": 1092.8, "text": " was actually referring to what happens if you we use a generic type notation where we start", "tokens": [390, 767, 13761, 281, 437, 2314, 498, 291, 321, 764, 257, 19577, 2010, 24657, 689, 321, 722], "temperature": 0.0, "avg_logprob": -0.19333623095256527, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.00030307177803479135}, {"id": 159, "seek": 107428, "start": 1092.8, "end": 1098.32, "text": " the type by type followed by a number and obviously it doesn't work there but it does", "tokens": [264, 2010, 538, 2010, 6263, 538, 257, 1230, 293, 2745, 309, 1177, 380, 589, 456, 457, 309, 775], "temperature": 0.0, "avg_logprob": -0.19333623095256527, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.00030307177803479135}, {"id": 160, "seek": 109832, "start": 1098.32, "end": 1107.84, "text": " the slow path so we have a slow path there is the person so complete that you can parse", "tokens": [264, 2964, 3100, 370, 321, 362, 257, 2964, 3100, 456, 307, 264, 954, 370, 3566, 300, 291, 393, 48377], "temperature": 0.0, "avg_logprob": -0.30882206247813665, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.0012684834655374289}, {"id": 161, "seek": 109832, "start": 1107.84, "end": 1117.4399999999998, "text": " an output and you get the same output as a parsed in really good because I would so if", "tokens": [364, 5598, 293, 291, 483, 264, 912, 5598, 382, 257, 21156, 292, 294, 534, 665, 570, 286, 576, 370, 498], "temperature": 0.0, "avg_logprob": -0.30882206247813665, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.0012684834655374289}, {"id": 162, "seek": 109832, "start": 1117.4399999999998, "end": 1125.04, "text": " the parser is good enough that to give you the exact same output as you put in no it", "tokens": [264, 21156, 260, 307, 665, 1547, 300, 281, 976, 291, 264, 1900, 912, 5598, 382, 291, 829, 294, 572, 309], "temperature": 0.0, "avg_logprob": -0.30882206247813665, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.0012684834655374289}, {"id": 163, "seek": 112504, "start": 1125.04, "end": 1136.0, "text": " does not do that no well you you I mean do you mean by access to white space exact or", "tokens": [775, 406, 360, 300, 572, 731, 291, 291, 286, 914, 360, 291, 914, 538, 2105, 281, 2418, 1901, 1900, 420], "temperature": 0.0, "avg_logprob": -0.2760638176126683, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0021743006072938442}, {"id": 164, "seek": 112504, "start": 1136.0, "end": 1146.32, "text": " just yeah no it doesn't do that no and then but it's also not it does also strip comments", "tokens": [445, 1338, 572, 309, 1177, 380, 360, 300, 572, 293, 550, 457, 309, 311, 611, 406, 309, 775, 611, 12828, 3053], "temperature": 0.0, "avg_logprob": -0.2760638176126683, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0021743006072938442}, {"id": 165, "seek": 114632, "start": 1146.32, "end": 1156.8, "text": " yes yes yes yes yes yes yes how do you handle escape decimals because in the example you", "tokens": [2086, 2086, 2086, 2086, 2086, 2086, 2086, 577, 360, 291, 4813, 7615, 979, 332, 1124, 570, 294, 264, 1365, 291], "temperature": 0.0, "avg_logprob": -0.22823421771709734, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00098588562104851}, {"id": 166, "seek": 114632, "start": 1156.8, "end": 1160.8, "text": " gave you strip and you have looked where the backslashes are yeah and then take the next", "tokens": [2729, 291, 12828, 293, 291, 362, 2956, 689, 264, 646, 10418, 12808, 366, 1338, 293, 550, 747, 264, 958], "temperature": 0.0, "avg_logprob": -0.22823421771709734, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00098588562104851}, {"id": 167, "seek": 114632, "start": 1160.8, "end": 1165.3999999999999, "text": " character but if you have like backslash 003 then you need those four characters as a single", "tokens": [2517, 457, 498, 291, 362, 411, 646, 10418, 1299, 7143, 18, 550, 291, 643, 729, 1451, 4342, 382, 257, 2167], "temperature": 0.0, "avg_logprob": -0.22823421771709734, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00098588562104851}, {"id": 168, "seek": 114632, "start": 1165.3999999999999, "end": 1174.9199999999998, "text": " unit to encode in the final I'm not actually I just do the I just really good yeah you're", "tokens": [4985, 281, 2058, 1429, 294, 264, 2572, 286, 478, 406, 767, 286, 445, 360, 264, 286, 445, 534, 665, 1338, 291, 434], "temperature": 0.0, "avg_logprob": -0.22823421771709734, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00098588562104851}, {"id": 169, "seek": 117492, "start": 1174.92, "end": 1178.44, "text": " gonna have to I hope there's no more questions because you're gonna have to keep doing that", "tokens": [799, 362, 281, 286, 1454, 456, 311, 572, 544, 1651, 570, 291, 434, 799, 362, 281, 1066, 884, 300], "temperature": 0.0, "avg_logprob": -0.22453645978655135, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0014193678507581353}, {"id": 170, "seek": 117492, "start": 1178.44, "end": 1194.68, "text": " but what I so what happens if I guess for certain type of input I record like a backslash", "tokens": [457, 437, 286, 370, 437, 2314, 498, 286, 2041, 337, 1629, 2010, 295, 4846, 286, 2136, 411, 257, 646, 10418, 1299], "temperature": 0.0, "avg_logprob": -0.22453645978655135, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0014193678507581353}, {"id": 171, "seek": 117492, "start": 1194.68, "end": 1200.8400000000001, "text": " 003 which encodes the byte with a value 3 single byte yeah how do you do that with your algorithm", "tokens": [7143, 18, 597, 2058, 4789, 264, 40846, 365, 257, 2158, 805, 2167, 40846, 1338, 577, 360, 291, 360, 300, 365, 428, 9284], "temperature": 0.0, "avg_logprob": -0.22453645978655135, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.0014193678507581353}, {"id": 172, "seek": 120084, "start": 1200.84, "end": 1205.9199999999998, "text": " that just takes the next character when you have backslashes but that's just to that's", "tokens": [300, 445, 2516, 264, 958, 2517, 562, 291, 362, 646, 10418, 12808, 457, 300, 311, 445, 281, 300, 311], "temperature": 0.0, "avg_logprob": -0.1453308675480985, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.00022482036729343235}, {"id": 173, "seek": 120084, "start": 1205.9199999999998, "end": 1212.6799999999998, "text": " just to so the question is what do I do with escape characters or with escape sequences", "tokens": [445, 281, 370, 264, 1168, 307, 437, 360, 286, 360, 365, 7615, 4342, 420, 365, 7615, 22978], "temperature": 0.0, "avg_logprob": -0.1453308675480985, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.00022482036729343235}, {"id": 174, "seek": 120084, "start": 1212.6799999999998, "end": 1218.24, "text": " and so what I explained on the what I do what happens with backslashes is just to tokenize", "tokens": [293, 370, 437, 286, 8825, 322, 264, 437, 286, 360, 437, 2314, 365, 646, 10418, 12808, 307, 445, 281, 14862, 1125], "temperature": 0.0, "avg_logprob": -0.1453308675480985, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.00022482036729343235}, {"id": 175, "seek": 120084, "start": 1218.24, "end": 1223.56, "text": " so I don't strip any data I just mark out the starts in the ends of each string field", "tokens": [370, 286, 500, 380, 12828, 604, 1412, 286, 445, 1491, 484, 264, 3719, 294, 264, 5314, 295, 1184, 6798, 2519], "temperature": 0.0, "avg_logprob": -0.1453308675480985, "compression_ratio": 1.809278350515464, "no_speech_prob": 0.00022482036729343235}, {"id": 176, "seek": 122356, "start": 1223.56, "end": 1245.3999999999999, "text": " and then the parsing comes after that so there's no data actually stripped yeah so the question", "tokens": [293, 550, 264, 21156, 278, 1487, 934, 300, 370, 456, 311, 572, 1412, 767, 33221, 1338, 370, 264, 1168], "temperature": 0.0, "avg_logprob": -0.1526054252277721, "compression_ratio": 1.5948275862068966, "no_speech_prob": 6.777324597351253e-05}, {"id": 177, "seek": 122356, "start": 1245.3999999999999, "end": 1252.24, "text": " is so the question is what's the output format and the output format in this case is just", "tokens": [307, 370, 264, 1168, 307, 437, 311, 264, 5598, 7877, 293, 264, 5598, 7877, 294, 341, 1389, 307, 445], "temperature": 0.0, "avg_logprob": -0.1526054252277721, "compression_ratio": 1.5948275862068966, "no_speech_prob": 6.777324597351253e-05}, {"id": 178, "seek": 125224, "start": 1252.24, "end": 1259.48, "text": " DNS wire format so the the idea here is that for each record it will invoke a callback", "tokens": [35153, 6234, 7877, 370, 264, 264, 1558, 510, 307, 300, 337, 1184, 2136, 309, 486, 41117, 257, 818, 3207], "temperature": 0.0, "avg_logprob": -0.16611945244573778, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0007173758349381387}, {"id": 179, "seek": 125224, "start": 1259.48, "end": 1264.4, "text": " and it will just give you wire format with pointers to where all the fields are in the", "tokens": [293, 309, 486, 445, 976, 291, 6234, 7877, 365, 44548, 281, 689, 439, 264, 7909, 366, 294, 264], "temperature": 0.0, "avg_logprob": -0.16611945244573778, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0007173758349381387}, {"id": 180, "seek": 125224, "start": 1264.4, "end": 1269.04, "text": " like an internal description of the field so that you know the length and you know the", "tokens": [411, 364, 6920, 3855, 295, 264, 2519, 370, 300, 291, 458, 264, 4641, 293, 291, 458, 264], "temperature": 0.0, "avg_logprob": -0.16611945244573778, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0007173758349381387}, {"id": 181, "seek": 126904, "start": 1269.04, "end": 1288.2, "text": " type of the field yeah there is definitely value to large effectors because it takes", "tokens": [2010, 295, 264, 2519, 1338, 456, 307, 2138, 2158, 281, 2416, 1802, 830, 570, 309, 2516], "temperature": 0.0, "avg_logprob": -0.16412176736971226, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.00038909207796677947}, {"id": 182, "seek": 126904, "start": 1288.2, "end": 1296.2, "text": " less instructions so if you can do something so I did not look into using the GPU but yeah", "tokens": [1570, 9415, 370, 498, 291, 393, 360, 746, 370, 286, 630, 406, 574, 666, 1228, 264, 18407, 457, 1338], "temperature": 0.0, "avg_logprob": -0.16412176736971226, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.00038909207796677947}, {"id": 183, "seek": 129620, "start": 1296.2, "end": 1323.28, "text": " that might benefit so if you know I have not so the question is why do why does parsing", "tokens": [300, 1062, 5121, 370, 498, 291, 458, 286, 362, 406, 370, 264, 1168, 307, 983, 360, 983, 775, 21156, 278], "temperature": 0.0, "avg_logprob": -0.23690797885258993, "compression_ratio": 1.144736842105263, "no_speech_prob": 0.001352792140096426}, {"id": 184, "seek": 132328, "start": 1323.28, "end": 1330.16, "text": " zone files have to be fast well because they're they get reloaded quite a lot of times each", "tokens": [6668, 7098, 362, 281, 312, 2370, 731, 570, 436, 434, 436, 483, 25628, 292, 1596, 257, 688, 295, 1413, 1184], "temperature": 0.0, "avg_logprob": -0.18050991938664362, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00041505080298520625}, {"id": 185, "seek": 132328, "start": 1330.16, "end": 1338.84, "text": " time there's an update to a zone you need to reload you need to reload the zone so we", "tokens": [565, 456, 311, 364, 5623, 281, 257, 6668, 291, 643, 281, 25628, 291, 643, 281, 25628, 264, 6668, 370, 321], "temperature": 0.0, "avg_logprob": -0.18050991938664362, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00041505080298520625}, {"id": 186, "seek": 132328, "start": 1338.84, "end": 1346.44, "text": " want to yeah the so that happens multiple times like an hour it differs per zone right", "tokens": [528, 281, 1338, 264, 370, 300, 2314, 3866, 1413, 411, 364, 1773, 309, 37761, 680, 6668, 558], "temperature": 0.0, "avg_logprob": -0.18050991938664362, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.00041505080298520625}, {"id": 187, "seek": 134644, "start": 1346.44, "end": 1352.96, "text": " but in our case if it takes more than an hour and the operator actually or it takes the", "tokens": [457, 294, 527, 1389, 498, 309, 2516, 544, 813, 364, 1773, 293, 264, 12973, 767, 420, 309, 2516, 264], "temperature": 0.0, "avg_logprob": -0.15952792799616433, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.00029459907091222703}, {"id": 188, "seek": 134644, "start": 1352.96, "end": 1359.28, "text": " better part of an hour and the operator wants to lead more wants to reload more often then", "tokens": [1101, 644, 295, 364, 1773, 293, 264, 12973, 2738, 281, 1477, 544, 2738, 281, 25628, 544, 2049, 550], "temperature": 0.0, "avg_logprob": -0.15952792799616433, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.00029459907091222703}, {"id": 189, "seek": 134644, "start": 1359.28, "end": 1366.0, "text": " that becomes a problem right so we just need to be faster but then there's all the end", "tokens": [300, 3643, 257, 1154, 558, 370, 321, 445, 643, 281, 312, 4663, 457, 550, 456, 311, 439, 264, 917], "temperature": 0.0, "avg_logprob": -0.15952792799616433, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.00029459907091222703}, {"id": 190, "seek": 134644, "start": 1366.0, "end": 1371.52, "text": " so there's other benefits as well so NSD for instance we support zone verification where", "tokens": [370, 456, 311, 661, 5311, 382, 731, 370, 15943, 35, 337, 5197, 321, 1406, 6668, 30206, 689], "temperature": 0.0, "avg_logprob": -0.15952792799616433, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.00029459907091222703}, {"id": 191, "seek": 137152, "start": 1371.52, "end": 1379.2, "text": " just before the zone goes live you can have a focus program to verify that your DNS stack", "tokens": [445, 949, 264, 6668, 1709, 1621, 291, 393, 362, 257, 1879, 1461, 281, 16888, 300, 428, 35153, 8630], "temperature": 0.0, "avg_logprob": -0.21504504540387323, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00040451946551911533}, {"id": 192, "seek": 137152, "start": 1379.2, "end": 1389.0, "text": " data is correct and there you can use an AXFR or you can let the NSD feed you the zone data", "tokens": [1412, 307, 3006, 293, 456, 291, 393, 764, 364, 316, 55, 34658, 420, 291, 393, 718, 264, 15943, 35, 3154, 291, 264, 6668, 1412], "temperature": 0.0, "avg_logprob": -0.21504504540387323, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00040451946551911533}, {"id": 193, "seek": 137152, "start": 1389.0, "end": 1395.08, "text": " in which case you just get text representation and if the zone is big enough then you want", "tokens": [294, 597, 1389, 291, 445, 483, 2487, 10290, 293, 498, 264, 6668, 307, 955, 1547, 550, 291, 528], "temperature": 0.0, "avg_logprob": -0.21504504540387323, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00040451946551911533}, {"id": 194, "seek": 139508, "start": 1395.08, "end": 1408.84, "text": " that to be fast because it's in the critical path right yeah well actually if the question", "tokens": [300, 281, 312, 2370, 570, 309, 311, 294, 264, 4924, 3100, 558, 1338, 731, 767, 498, 264, 1168], "temperature": 0.0, "avg_logprob": -0.24773596127827963, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006959482561796904}, {"id": 195, "seek": 139508, "start": 1408.84, "end": 1412.96, "text": " was if splitting the files and multi-threading is something that we consider well splitting", "tokens": [390, 498, 30348, 264, 7098, 293, 4825, 12, 392, 35908, 307, 746, 300, 321, 1949, 731, 30348], "temperature": 0.0, "avg_logprob": -0.24773596127827963, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006959482561796904}, {"id": 196, "seek": 139508, "start": 1412.96, "end": 1424.84, "text": " the files no but split on them yeah well new lines yeah that can be tricky because", "tokens": [264, 7098, 572, 457, 7472, 322, 552, 1338, 731, 777, 3876, 1338, 300, 393, 312, 12414, 570], "temperature": 0.0, "avg_logprob": -0.24773596127827963, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006959482561796904}, {"id": 197, "seek": 142484, "start": 1424.84, "end": 1430.6, "text": " zone files can contain parentheses which would then mean that the record continues on the", "tokens": [6668, 7098, 393, 5304, 34153, 597, 576, 550, 914, 300, 264, 2136, 6515, 322, 264], "temperature": 0.0, "avg_logprob": -0.12389714188045925, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0002817630011122674}, {"id": 198, "seek": 142484, "start": 1430.6, "end": 1439.8799999999999, "text": " next line but a colleague actually did do a parallel zone loading implementation and", "tokens": [958, 1622, 457, 257, 13532, 767, 630, 360, 257, 8952, 6668, 15114, 11420, 293], "temperature": 0.0, "avg_logprob": -0.12389714188045925, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0002817630011122674}, {"id": 199, "seek": 142484, "start": 1439.8799999999999, "end": 1445.1999999999998, "text": " I guess we can even do that with with this implementation right because there the it", "tokens": [286, 2041, 321, 393, 754, 360, 300, 365, 365, 341, 11420, 558, 570, 456, 264, 309], "temperature": 0.0, "avg_logprob": -0.12389714188045925, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0002817630011122674}, {"id": 200, "seek": 142484, "start": 1445.1999999999998, "end": 1452.04, "text": " was actually quite a bit faster but the scanning process still takes a long time because you", "tokens": [390, 767, 1596, 257, 857, 4663, 457, 264, 27019, 1399, 920, 2516, 257, 938, 565, 570, 291], "temperature": 0.0, "avg_logprob": -0.12389714188045925, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0002817630011122674}, {"id": 201, "seek": 145204, "start": 1452.04, "end": 1456.76, "text": " go over it by by bite but now that we have a fast scanner there's no reason why we cannot", "tokens": [352, 670, 309, 538, 538, 7988, 457, 586, 300, 321, 362, 257, 2370, 30211, 456, 311, 572, 1778, 983, 321, 2644], "temperature": 0.0, "avg_logprob": -0.23277543385823568, "compression_ratio": 1.432, "no_speech_prob": 0.000337018194841221}, {"id": 202, "seek": 145204, "start": 1456.76, "end": 1475.08, "text": " also include like parallel parsing yeah that could work yeah so the question is if we did", "tokens": [611, 4090, 411, 8952, 21156, 278, 1338, 300, 727, 589, 1338, 370, 264, 1168, 307, 498, 321, 630], "temperature": 0.0, "avg_logprob": -0.23277543385823568, "compression_ratio": 1.432, "no_speech_prob": 0.000337018194841221}, {"id": 203, "seek": 147508, "start": 1475.08, "end": 1493.9199999999998, "text": " it", "tokens": [309], "temperature": 1.0, "avg_logprob": -1.7114660263061523, "compression_ratio": 0.2, "no_speech_prob": 0.0007215734804049134}], "language": "en"}