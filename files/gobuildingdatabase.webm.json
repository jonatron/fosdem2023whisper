{"text": " It's four o'clock, so let's look at our preview, sorry, now next talk. I have been doing some mattings in Go, but building a database, I honestly have strong respect for. So next up is Etienne, who is going to tell us everything about Crazy Kitchen is in Go. Thank you, thank you, yeah, welcome to our mad journey of building a database in Go, and yeah, it's pretty mad to build a database at all, it may be even worse or even a matter to build a database in Go when most are built in Go. Let me start over in case you didn't hear it, so hi, my name is Etienne, welcome to our mad journey of building a vector database in Go. So building a database at all could already be pretty mad, doing it in Go when most are built in C or C++ could be even matter or even more exciting, and we definitely encountered a couple of unique problems that led us to create creative solutions, and there's lots of shout outs in there and also a couple of wish lists, so we just released Go 1.20, and of course the occasional madness. So let's get one question out of the way right away, why does the world even need yet another database? There's so many out there already, but probably you've seen this thing called chat GPT, because that was pretty much everywhere and it's kind of hard to hide from it, and chat GPT is a large language model and it's really good at putting text together that sounds really sophisticated and sounds nice and sometimes is completely wrong, and so in this case we're asking you, is it mad to write a database and go, I might disagree with that, but either way, basically we're now in a situation where on the one hand we have these machine learning models that can do all the cool stuff and do this sort of interactively and on the fly, and on the other side we have traditional databases, and those traditional databases, they have the fact, because that's kind of what databases are for, right? So wouldn't it be cool if we could somehow combine those two? So for example on the query side, if I ask Wikipedia, why can airplanes fly? Then the kind of passage that I want that has the answer in it is titled the physics of flight, but that is difficult for a traditional search engine, because if you look at keyword overlap there's almost none in there, but a vector search engine can use machine learning models basically that can tell you these two things are the same, and searching through that at scale is a big problem. Then there's that sort of chat GPT side where you don't just want to search through it, but maybe you also want to say like take those results, summarize them, and also translate them to German. So basically not just return exactly what's in the database, but do something with it and basically generate more data from it. And that is exactly where VV8 comes in, so VV8 is a vector search engine which basically helps us solve this kind of searching by meaning instead of keywords without sort of losing what we've done in 20 plus years of search engine research. And now most recently you can also interact with these models such as chat GPT, GPT3, and of course also the open source versions of it. So VV8 is written in go. Is that a good idea? Is that a bad idea? Or have we just gone plain mad? So we're not alone, that's good. So you probably recognize these things, they're all bigger brands at the moment than VV8, so VV8 is growing fast. And some of those vendors have really great blog posts where you see some of the like optimization topics and some of the crazy stuff that they have to do. So if you've contributed to one of those, some of the things I'm going to say might sound familiar, if not then buckle up, it's going to get mad. So first stop on our mad journey memory allocation, then that also brings us to our friend the garbage collector. So for any high performance go application, sooner or later you're going to talk about memory allocations and definitely consider a database a high performance application or at least consider VV8 a high performance application. And if you think of what databases do, like in essence basically you have something on disk and you want to serve it to the user, that's like one of the most important user journeys in a database. And here this is represented by just a number, so it went for UN32, so that's just four bytes on disk and basically you can see sort of these four bytes. If you parse them into Go they would have the value of 16 in that UN32 and this is essentially something very much simplified that a database needs to do and it needs to do it over and over again. So the standard library gives us the encoding slash binary package and there we have this binary dot read method which I think looks really cool. To me it looks like idiomatic Go because it has the io dot reader interface like everyone's favorite interface and you can put all of that stuff in there and if you run this code and there's no error then basically you get exactly what you want. You could turn those sort of four bytes that were somewhere on disk, turn them into our in-memory representation of that UN32. So is this a good idea to do that exactly like well if you do it once or maybe twice could be a good idea. If you do it a billion times this is what happens. So for those of you who are new to CPU profiles in Go this is madness. This is pretty bad. So first of all you see it in the center parsing those 1 billion numbers took 26 seconds and 26 seconds is not the kind of time that we ever have in the database but worse than that if you look at that profile we have stuff like runtime, malloc, gc, runtime, mem, move, runtime, m, advice. So all these things they're related to memory allocations or to garbage collection. What they're not related to is parsing data which is what we wanted to do. So how much time of that 20 seconds did we spend what we wanted to do? Don't know. It doesn't even show up in the profile. So to understand why that is the case we need to quickly talk about the stack and the heap. So you can think of the stack as basically your function stack so you call one function that calls another function and then at some point basically you go back through the stack and this is very short lift and this is cheap and fast to allocate and why is it cheap? Because you know exactly the runtime of your variables or the life cycle of your variables so you don't even need to involve the garbage collector. So no garbage collector cheap and fast. Then on the other side you have the heap and the heap is basically this sort of long lift kind of memory and that's expensive and slow to allocate and why because and also to deallocate and why because it involves the garbage collector. So if the stack is so much cheaper then we can just always allocate on the stack right. So warning this is not real go please do not do this. This is sort of a fictional example of allocating a buffer of size 8 and then we're going to say like yeah please put this on the stack and that is not how it works and for most of you you probably say like this is pretty good that it's not that it works that way because why would you want to deal with that. But for me just trying to build a database and go sometimes like this something like this may be good or maybe not. So how does it work? Go does something that's called escape analysis. So if you compile your code with gcflags-m then go annotates your code basically and tells you sort of what's happening there. So here you can see in the second line that this num variable that we used was moved to the heap and then in the next point you see the bytes.reader which represents our io.reader escaped to the heap. So two times we see that something happened to the or went to the heap. We don't exactly know what happened yet but at least there's proof that we have this kind of allocation problem. So what can we do? Well we can simplify a bit. Turns out that the binary or encoding binary package also has another method that looks like this which is just called view in 32 on the little endian package and it kind of does the same thing. You just put in the buffer on the one side so no reader this time you just put in the raw buffer basically with the position offset and on the other side you get the number out. And the crazy thing is this one line needs no memory allocations. So if we do that again our one billion numbers that took 26 seconds before now take 600 milliseconds and now we're starting to get into a range where like this is acceptable for a data basis. And more importantly what we see on that profile, the profile is so much simpler right now. There's basically just this one function there and that is what we wanted to do. So admittedly we're not doing much other than parsing the data at the moment but at least we got sort of rid of all the noise and you can see the speed up. Okay so quickly to recap. If we say a database is nothing but reading data and sort of parsing it to serve it to the user then we do that over and over again then we need to take care of memory allocations. And the fix in this case was super simple. We changed two lines of code and reduced it from 26 seconds to 600 milliseconds. But why we had to do that wasn't very intuitive like that it wasn't very obvious. In fact I haven't even told you yet why this binary dot little nvn dot read why that escaped to the heap. And in this case it's because we passed in a pointer and we passed in an interface and that's kind of a hint basically that something might escape to the heap. So what I would wish is yes this is not a topic that you need every day you write go but maybe if you do need this would be cool if there was better education. Okay so second step delay the coding so this is kind of the idea that we wouldn't want to do the same work twice. And we're sticking with our example of serving data from disk but now while the number example was a bit too simple so let's make it slightly more complex. We have this nested array here basically a sort of slice off slice view in 64 and that's representative now for a more complex object on your database. Of course in reality you'd have like string props and other kind of things but just sort of to show that there's more going on than a single number. And let's say we have 80 million of them so 10 million of the outer slice and then eight elements in each inner slice and our task is just to sum those up. So these are 80 million numbers and we want to know what is the sum of them. So that is actually kind of a realistic database task for an OLAP kind of database. But we need to somehow represent that data on disk and we're looking at two ways to do this. The first one is JSON representation and then the second one would be the sort of binary encoding and then there'll be more. So JSON is basically just here for completeness aid. We can basically rule it out immediately so when you're building a database you're probably not using JSON to store stuff on disk unless it's sort of a JSON database. Why because it's space inefficient so if you want to represent those numbers on disk like JSON basically uses strings for it and then you have all these control characters like your curly braces and your quotes and your columns and everything that takes up space. So in our fictional example that would take up 1.6 gigabyte and you'll see soon that we can do that more efficient. But also it's slow and part of why it's slow is again because we have these memory allocations but also the whole parsing just takes time. So in our example this took 14 seconds to sum up those 80 million numbers and as I said before you just don't have double digit seconds in a database. So we can do something that's a bit smarter which is called length encoding. So we're encoding this basically as binary and we're spending one in this case one byte so that's basically a U and 8 and we're using that as a length indicator. So basically that tells us that when we're reading this from disk that just tells us what's coming up. So in this case it says we have eight elements coming up and then we know that our elements in this example is U and 32 so that's four bytes each. So basically the next 32 bytes that we're reading are going to be our eight inner arrays and then we just continue. Then we basically read the next length indicator and this way we can encode the stuff sort of in one contiguous thing. Then of course we have to decode it somehow and we can do that because we've learned from our previous example right so we're not going to use binary.lnlndian.read but we're doing this in an allocation free way, you can see that in the length line basically and yeah our goal is to take that data and put it into our nested sort of go slice of slice of slice of U in 64 and the code here basically you see we're reading the length and then we're increasing our offset so we know where to read from and then we're basically repeating this for the inner slice which is just hinted at here by the decode inner function. So what happens when we do this? First of all the good news, 660 megabytes that's way less than our 1.6 gigabyte before so basically just by using a more space efficient way to represent data we've yeah done exactly that we've reduced our size also it's much much faster so we were at 14 seconds before and now it's down to 260 milliseconds but this is our mad journey of building a database so we're not done here yet because there's some hidden madness and the hidden madness is that we actually spend 250 milliseconds decoding while we spend 10 milliseconds summing up those 80 million numbers so again we're kind of in that situation where we're doing something that we never really set out to do like we wanted to do something else but we're spending our time on yeah doing something that we didn't want to do so where does that come from and the first problem is basically that what we did what we set out to do was fought from the get go because we said we want to decode so we're basically thinking in the same way that we're thinking as we were with Jason we said that we want to decode this entire thing into this go data structure but that means that you see we need to allocate this massive slice again and that also means that we need to in each inner slice we also need to allocate again so we're basically allocating and allocating over and over again where our task is not to allocate our task was to sum up numbers so we can actually just simplify this a bit and we can basically just not decode it like while we're looping over that data anyway instead of storing it in an array we can just do with it what we plan to do and in this case this would be summing up the data so basically getting rid of that decoding step helps us to make this way faster so now we're at 46 milliseconds of course our footprint of the data on disk hasn't changed because it's the same data that we're reading we're just reading it in a slightly more efficient way but yeah we don't have to allocate slices and also because we don't have these like nested slices we don't have like slices that basically have pointers to other slices so we have better memory locality and now we're at 46 milliseconds and that is that is cool so 46 milliseconds is basically the time frame that can be acceptable for a database okay so quickly in recap we immediately ruled out JSON because it just wasn't space efficient and we knew that we needed something more space efficient and also way faster binary encoding already made it much faster which is great but if we decode it upfront then yeah we still lost a lot of time and it can be worth it in these kind of high-performance situations if you either sort of delay the decoding as late as possible until you really need it or just don't do it at all or do it in sort of small parts where we need it no wish list here but an honor we mentioned so go 1.20 they've actually removed it from the from the release notes because it's so experimental but go 1.20 has support for memory arenas the idea for memory arenas is basically that you can bypass the garbage collector and sort of manually free that data so if you have something that you know has the same sort of life cycle then you can say okay put it in the arena and basically in the end free the entire arena which would sort of bypass the garbage collector so that could also be a solution in this case if that ever makes it like right now it's super experimental and they basically tell you we might just remove it so don't use it third stop is something that when I first heard it almost sounded like too good to be true so something called SIMD we'll get to what that is in a second but first question to the audience who here remembers this thing raise your hands okay cool so you're just as old as I am so this is the Intel Pentium 2 processor and this came out in late 90s I think 1997 and was sold for a couple of couple of years and back then I did not build databases definitely not in go because that also didn't exist yet but what I would do was sort of try to play 3d video games and I would urge my parents to get one of those new computers with an Intel Pentium 2 processor and one of the arguments that I could have used in that discussion was hey it comes with MMX technology and of course I had no idea what that is and it probably took me 10 or so more more years to find out what MMX is but it's the first in a long list of SIMD instructions I haven't explained what SIMD is yet but I will in a second some of those especially the one in the in the top line they're not really used anymore these days but the the bottom line like AVX2 and AVX512 you may have heard them in in fact for for many open source project they sometimes just sort of slap that label in the read me like yeah yeah has AVX2 optimizations and that kind of signals you yeah we care about speed because it's like low level optimized and VVA does the exact same thing by the way so to understand how we could make use of that I quickly need to talk about vector embeddings because I said before that VVA doesn't doesn't search through data by keywords but rather through its meaning and it uses vector embeddings as a tool for that so this is basically just a long list of numbers in this case floats and then a machine learning model comes in and basically it says do something with my input and then you get this vector out and if you do this on all the objects then you can compare your vectors so you basically can do a vector similarity comparison and that tells you if something is close to to one another or not so for example the query and the the object that we had before so without any simd we can use something called the dot product the dot product is a simple calculation where basically you use you multiply each element of the first vector with the same corresponding element of the second vector and then you just sum up all of those elements and we can think of this like multiplication and summing as two instructions so if we look out first shout out here to the compiler explorer which is a super cool tool to see like what your go code compiles to we can see that this indeed turns into two instructions so this is a bit of a lie because there's more stuff going on because it's in a loop etc but let's just pretend that indeed we have these two instructions to multiply it and to add it so how could we possibly optimize this even further if we're already at such a low level well we can because this is our mad journey so all we have to do is introduce some madness and what we're doing now is a practice that's called unrolling so the idea here is that instead of looping over one element at a time we're now looping over eight elements at a time but we've got we've gained nothing like this is we're still doing the same kind of work like we're doing 16 instructions now in a single loop and we're just doing fewer iterations so by this point nothing gained but why would we do that well here comes the part where I thought it was too good to be true what if we could do those 16 operations for the cost of just two instructions sounds crazy right well no because simd I'm finally revealing what the acronym stands for it stands for single instruction multiple data and that is exactly what we're doing here so we want to do the same thing over and over again which is multiplication and then additions and this is exactly what these simd instructions provide so in this case we can multiply eight floats with other eight floats and then we can add them up so all this perfect here maybe not because there's a catch of course it's our mad journey how do you tell go to use these avx two instructions you don't you write assembly code because go has no way to do that directly the good part is that assembly code integrates really nicely into go and in the in the standard library it's used over and over again so it's kind of a standard practice and there is tooling here so shout out to avo really cool too that helps you basically you're you're still writing assembly with with avo but you're writing it in go and then it generates the assembly so you still need to know what you're doing but it's like it it protects you a bit so it definitely helped us a lot so simd recap using avx instructions or other simd instructions you can basically trick your cpu into doing more work for free but you need to sort of also trick go to use assembly and with this tooling such as avo it can be better but it would be even nicer if the language had some sort of support for it and you made my saying now okay this is this mad guy on stage that wants to build a database but no one else does needs that but we have this issue here that was open recently and unfortunately also closed recently because no consensus could be reached but it comes up back and back basically that go users are saying like hey we want something in the language such as intrinsic so intrinsics are basically the idea of having high level language instructions to do these these sort of avx or simd instructions and c or c++ has that for example one way to do that and maybe you're wondering like okay if you have such a performance hot path like why don't you just write that in c and you see go or write it in rust or something like that sounds good in theory but the problem is that the call overhead to call c or c++ is so high that you actually have to outsource quite a bit of your code for that to to pay off again so if you do that you basically end up writing more and more and more in that language and then you're not writing go anymore so fortunately that's not or it can be in some ways but it's not always a great idea so demo time um this was going to be a live demo and maybe it still is because i prepared this running nicely in a docker container and then my docker network just broke everything and it didn't work but i just rebuilt it without docker and i think it might work if not i have screenshots basically that um that do a backup so example query here i'm a big wine nerd so what i did is i put wine reviews into vv8 and i want to search them now and one way to do it to show you basically that the keyword um that you don't need a keyword match but can search by meaning is for example if i go for an affordable italian wine let's see if the internet connection works it does so what we got back um is basically this this wine review that i wrote about a barolo that i recently drank and you can see it doesn't say italy anywhere it doesn't say affordable what it says like without breaking the bank so this is a vector search that basically happened in the in the background we can take this one step further by using the generative side so this is basically the the chat gpt part um we can now ask our database based on the review which is what i wrote when is this wine going to be ready to drink so let's see you saw before that was the fail query when the internet didn't work now now it's actually working so that's nice um and here in this case you can see that so this is using open ai but you can plug in other tools can plug in open source versions of it um this is using open ai because that's nice to be hosted at a at a service i don't have to run the machine learning model on my laptop then you can see it tells you the wine is not ready to drink yet we will need at least five more years which is sort of a good summary of this and then you can see another wine is ready to drink right now it's in the perfect drinking window so for the final demo let's combine those two let's do a semantic search to identify something and then do an ai generation basically so in this case we're saying find me an aged classic riesling best best wine in the world riesling um and based on the review would you consider this wine to be a fruit bomb so let's have sort of an opinion from the machine learning model in it and um here we got one of my favorite wines and the the model says no i would not consider this a fruit bomb while it does have some fruity notes it is balanced by the mineralogy and acidity which keeps it from being overly sweet or fruity which is um if you read the text like this is nowhere in there so this is kind of cool that the that the model was was able to do this okay so let's go back now it's the the demo time by the way have a github repo with like this example so you can run it yourself and um and yeah try it out yourself so this was our mad journey and are we mad at go are we mad to do this well i would pretty much say no because yes there were a couple of parts where we have to give a get really creative and had to do some some yeah rather unique stuff but that was also basically like the highlight reel of building a database and all the other parts like i didn't even show the parts that went great like concurrency handling and the powerful standard library and of course all of you basically representing the gopher community which is super helpful and yeah this was my way to basically give back to all of you so if you ever want to build a database or run into other kind of high performance problems then maybe some of those", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 18.400000000000002, "text": " It's four o'clock, so let's look at our preview, sorry, now next talk.", "tokens": [467, 311, 1451, 277, 6, 9023, 11, 370, 718, 311, 574, 412, 527, 14281, 11, 2597, 11, 586, 958, 751, 13], "temperature": 0.0, "avg_logprob": -0.35330691704383266, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.4248667359352112}, {"id": 1, "seek": 0, "start": 18.400000000000002, "end": 24.32, "text": " I have been doing some mattings in Go, but building a database, I honestly have strong", "tokens": [286, 362, 668, 884, 512, 3803, 783, 82, 294, 1037, 11, 457, 2390, 257, 8149, 11, 286, 6095, 362, 2068], "temperature": 0.0, "avg_logprob": -0.35330691704383266, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.4248667359352112}, {"id": 2, "seek": 0, "start": 24.32, "end": 25.64, "text": " respect for.", "tokens": [3104, 337, 13], "temperature": 0.0, "avg_logprob": -0.35330691704383266, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.4248667359352112}, {"id": 3, "seek": 2564, "start": 25.64, "end": 32.64, "text": " So next up is Etienne, who is going to tell us everything about Crazy Kitchen is in Go.", "tokens": [407, 958, 493, 307, 3790, 21262, 11, 567, 307, 516, 281, 980, 505, 1203, 466, 22509, 23135, 307, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.36756511118220186, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.0007119751535356045}, {"id": 4, "seek": 2564, "start": 32.64, "end": 39.88, "text": " Thank you, thank you, yeah, welcome to our mad journey of building a database in Go,", "tokens": [1044, 291, 11, 1309, 291, 11, 1338, 11, 2928, 281, 527, 5244, 4671, 295, 2390, 257, 8149, 294, 1037, 11], "temperature": 0.0, "avg_logprob": -0.36756511118220186, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.0007119751535356045}, {"id": 5, "seek": 2564, "start": 39.88, "end": 45.6, "text": " and yeah, it's pretty mad to build a database at all, it may be even worse or even a matter", "tokens": [293, 1338, 11, 309, 311, 1238, 5244, 281, 1322, 257, 8149, 412, 439, 11, 309, 815, 312, 754, 5324, 420, 754, 257, 1871], "temperature": 0.0, "avg_logprob": -0.36756511118220186, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.0007119751535356045}, {"id": 6, "seek": 2564, "start": 45.6, "end": 53.84, "text": " to build a database in Go when most are built in Go.", "tokens": [281, 1322, 257, 8149, 294, 1037, 562, 881, 366, 3094, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.36756511118220186, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.0007119751535356045}, {"id": 7, "seek": 5384, "start": 53.84, "end": 60.720000000000006, "text": " Let me start over in case you didn't hear it, so hi, my name is Etienne, welcome to", "tokens": [961, 385, 722, 670, 294, 1389, 291, 994, 380, 1568, 309, 11, 370, 4879, 11, 452, 1315, 307, 3790, 21262, 11, 2928, 281], "temperature": 0.0, "avg_logprob": -0.20152356364939472, "compression_ratio": 1.625, "no_speech_prob": 0.00015341794642154127}, {"id": 8, "seek": 5384, "start": 60.720000000000006, "end": 63.92, "text": " our mad journey of building a vector database in Go.", "tokens": [527, 5244, 4671, 295, 2390, 257, 8062, 8149, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.20152356364939472, "compression_ratio": 1.625, "no_speech_prob": 0.00015341794642154127}, {"id": 9, "seek": 5384, "start": 63.92, "end": 68.12, "text": " So building a database at all could already be pretty mad, doing it in Go when most are", "tokens": [407, 2390, 257, 8149, 412, 439, 727, 1217, 312, 1238, 5244, 11, 884, 309, 294, 1037, 562, 881, 366], "temperature": 0.0, "avg_logprob": -0.20152356364939472, "compression_ratio": 1.625, "no_speech_prob": 0.00015341794642154127}, {"id": 10, "seek": 5384, "start": 68.12, "end": 74.72, "text": " built in C or C++ could be even matter or even more exciting, and we definitely encountered", "tokens": [3094, 294, 383, 420, 383, 25472, 727, 312, 754, 1871, 420, 754, 544, 4670, 11, 293, 321, 2138, 20381], "temperature": 0.0, "avg_logprob": -0.20152356364939472, "compression_ratio": 1.625, "no_speech_prob": 0.00015341794642154127}, {"id": 11, "seek": 5384, "start": 74.72, "end": 79.48, "text": " a couple of unique problems that led us to create creative solutions, and there's lots", "tokens": [257, 1916, 295, 3845, 2740, 300, 4684, 505, 281, 1884, 5880, 6547, 11, 293, 456, 311, 3195], "temperature": 0.0, "avg_logprob": -0.20152356364939472, "compression_ratio": 1.625, "no_speech_prob": 0.00015341794642154127}, {"id": 12, "seek": 7948, "start": 79.48, "end": 84.44, "text": " of shout outs in there and also a couple of wish lists, so we just released Go 1.20,", "tokens": [295, 8043, 14758, 294, 456, 293, 611, 257, 1916, 295, 3172, 14511, 11, 370, 321, 445, 4736, 1037, 502, 13, 2009, 11], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 13, "seek": 7948, "start": 84.44, "end": 86.80000000000001, "text": " and of course the occasional madness.", "tokens": [293, 295, 1164, 264, 31644, 28736, 13], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 14, "seek": 7948, "start": 86.80000000000001, "end": 91.92, "text": " So let's get one question out of the way right away, why does the world even need yet another", "tokens": [407, 718, 311, 483, 472, 1168, 484, 295, 264, 636, 558, 1314, 11, 983, 775, 264, 1002, 754, 643, 1939, 1071], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 15, "seek": 7948, "start": 91.92, "end": 92.92, "text": " database?", "tokens": [8149, 30], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 16, "seek": 7948, "start": 92.92, "end": 98.32000000000001, "text": " There's so many out there already, but probably you've seen this thing called chat GPT, because", "tokens": [821, 311, 370, 867, 484, 456, 1217, 11, 457, 1391, 291, 600, 1612, 341, 551, 1219, 5081, 26039, 51, 11, 570], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 17, "seek": 7948, "start": 98.32000000000001, "end": 103.84, "text": " that was pretty much everywhere and it's kind of hard to hide from it, and chat GPT is a", "tokens": [300, 390, 1238, 709, 5315, 293, 309, 311, 733, 295, 1152, 281, 6479, 490, 309, 11, 293, 5081, 26039, 51, 307, 257], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 18, "seek": 7948, "start": 103.84, "end": 108.80000000000001, "text": " large language model and it's really good at putting text together that sounds really", "tokens": [2416, 2856, 2316, 293, 309, 311, 534, 665, 412, 3372, 2487, 1214, 300, 3263, 534], "temperature": 0.0, "avg_logprob": -0.18693316928924075, "compression_ratio": 1.6566666666666667, "no_speech_prob": 3.477028803899884e-05}, {"id": 19, "seek": 10880, "start": 108.8, "end": 116.28, "text": " sophisticated and sounds nice and sometimes is completely wrong, and so in this case we're", "tokens": [16950, 293, 3263, 1481, 293, 2171, 307, 2584, 2085, 11, 293, 370, 294, 341, 1389, 321, 434], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 20, "seek": 10880, "start": 116.28, "end": 120.88, "text": " asking you, is it mad to write a database and go, I might disagree with that, but either", "tokens": [3365, 291, 11, 307, 309, 5244, 281, 2464, 257, 8149, 293, 352, 11, 286, 1062, 14091, 365, 300, 11, 457, 2139], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 21, "seek": 10880, "start": 120.88, "end": 124.72, "text": " way, basically we're now in a situation where on the one hand we have these machine learning", "tokens": [636, 11, 1936, 321, 434, 586, 294, 257, 2590, 689, 322, 264, 472, 1011, 321, 362, 613, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 22, "seek": 10880, "start": 124.72, "end": 129.0, "text": " models that can do all the cool stuff and do this sort of interactively and on the fly,", "tokens": [5245, 300, 393, 360, 439, 264, 1627, 1507, 293, 360, 341, 1333, 295, 4648, 3413, 293, 322, 264, 3603, 11], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 23, "seek": 10880, "start": 129.0, "end": 132.56, "text": " and on the other side we have traditional databases, and those traditional databases,", "tokens": [293, 322, 264, 661, 1252, 321, 362, 5164, 22380, 11, 293, 729, 5164, 22380, 11], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 24, "seek": 10880, "start": 132.56, "end": 135.96, "text": " they have the fact, because that's kind of what databases are for, right?", "tokens": [436, 362, 264, 1186, 11, 570, 300, 311, 733, 295, 437, 22380, 366, 337, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1631189361820376, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.9199796952307224e-05}, {"id": 25, "seek": 13596, "start": 135.96, "end": 139.64000000000001, "text": " So wouldn't it be cool if we could somehow combine those two?", "tokens": [407, 2759, 380, 309, 312, 1627, 498, 321, 727, 6063, 10432, 729, 732, 30], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 26, "seek": 13596, "start": 139.64000000000001, "end": 145.04000000000002, "text": " So for example on the query side, if I ask Wikipedia, why can airplanes fly?", "tokens": [407, 337, 1365, 322, 264, 14581, 1252, 11, 498, 286, 1029, 28999, 11, 983, 393, 32947, 3603, 30], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 27, "seek": 13596, "start": 145.04000000000002, "end": 149.0, "text": " Then the kind of passage that I want that has the answer in it is titled the physics", "tokens": [1396, 264, 733, 295, 11497, 300, 286, 528, 300, 575, 264, 1867, 294, 309, 307, 19841, 264, 10649], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 28, "seek": 13596, "start": 149.0, "end": 153.12, "text": " of flight, but that is difficult for a traditional search engine, because if you look at keyword", "tokens": [295, 7018, 11, 457, 300, 307, 2252, 337, 257, 5164, 3164, 2848, 11, 570, 498, 291, 574, 412, 20428], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 29, "seek": 13596, "start": 153.12, "end": 159.0, "text": " overlap there's almost none in there, but a vector search engine can use machine learning", "tokens": [19959, 456, 311, 1920, 6022, 294, 456, 11, 457, 257, 8062, 3164, 2848, 393, 764, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 30, "seek": 13596, "start": 159.0, "end": 162.8, "text": " models basically that can tell you these two things are the same, and searching through", "tokens": [5245, 1936, 300, 393, 980, 291, 613, 732, 721, 366, 264, 912, 11, 293, 10808, 807], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 31, "seek": 13596, "start": 162.8, "end": 165.68, "text": " that at scale is a big problem.", "tokens": [300, 412, 4373, 307, 257, 955, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10977427164713542, "compression_ratio": 1.6772151898734178, "no_speech_prob": 4.493946562433848e-06}, {"id": 32, "seek": 16568, "start": 165.68, "end": 171.0, "text": " Then there's that sort of chat GPT side where you don't just want to search through it,", "tokens": [1396, 456, 311, 300, 1333, 295, 5081, 26039, 51, 1252, 689, 291, 500, 380, 445, 528, 281, 3164, 807, 309, 11], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 33, "seek": 16568, "start": 171.0, "end": 175.44, "text": " but maybe you also want to say like take those results, summarize them, and also translate", "tokens": [457, 1310, 291, 611, 528, 281, 584, 411, 747, 729, 3542, 11, 20858, 552, 11, 293, 611, 13799], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 34, "seek": 16568, "start": 175.44, "end": 176.44, "text": " them to German.", "tokens": [552, 281, 6521, 13], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 35, "seek": 16568, "start": 176.44, "end": 180.56, "text": " So basically not just return exactly what's in the database, but do something with it", "tokens": [407, 1936, 406, 445, 2736, 2293, 437, 311, 294, 264, 8149, 11, 457, 360, 746, 365, 309], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 36, "seek": 16568, "start": 180.56, "end": 183.56, "text": " and basically generate more data from it.", "tokens": [293, 1936, 8460, 544, 1412, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 37, "seek": 16568, "start": 183.56, "end": 187.48000000000002, "text": " And that is exactly where VV8 comes in, so VV8 is a vector search engine which basically", "tokens": [400, 300, 307, 2293, 689, 691, 53, 23, 1487, 294, 11, 370, 691, 53, 23, 307, 257, 8062, 3164, 2848, 597, 1936], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 38, "seek": 16568, "start": 187.48000000000002, "end": 193.32, "text": " helps us solve this kind of searching by meaning instead of keywords without sort of losing", "tokens": [3665, 505, 5039, 341, 733, 295, 10808, 538, 3620, 2602, 295, 21009, 1553, 1333, 295, 7027], "temperature": 0.0, "avg_logprob": -0.16086356366266968, "compression_ratio": 1.7344827586206897, "no_speech_prob": 9.970765859179664e-06}, {"id": 39, "seek": 19332, "start": 193.32, "end": 197.48, "text": " what we've done in 20 plus years of search engine research.", "tokens": [437, 321, 600, 1096, 294, 945, 1804, 924, 295, 3164, 2848, 2132, 13], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 40, "seek": 19332, "start": 197.48, "end": 202.51999999999998, "text": " And now most recently you can also interact with these models such as chat GPT, GPT3,", "tokens": [400, 586, 881, 3938, 291, 393, 611, 4648, 365, 613, 5245, 1270, 382, 5081, 26039, 51, 11, 26039, 51, 18, 11], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 41, "seek": 19332, "start": 202.51999999999998, "end": 206.2, "text": " and of course also the open source versions of it.", "tokens": [293, 295, 1164, 611, 264, 1269, 4009, 9606, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 42, "seek": 19332, "start": 206.2, "end": 208.76, "text": " So VV8 is written in go.", "tokens": [407, 691, 53, 23, 307, 3720, 294, 352, 13], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 43, "seek": 19332, "start": 208.76, "end": 209.79999999999998, "text": " Is that a good idea?", "tokens": [1119, 300, 257, 665, 1558, 30], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 44, "seek": 19332, "start": 209.79999999999998, "end": 210.79999999999998, "text": " Is that a bad idea?", "tokens": [1119, 300, 257, 1578, 1558, 30], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 45, "seek": 19332, "start": 210.79999999999998, "end": 214.07999999999998, "text": " Or have we just gone plain mad?", "tokens": [1610, 362, 321, 445, 2780, 11121, 5244, 30], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 46, "seek": 19332, "start": 214.07999999999998, "end": 215.92, "text": " So we're not alone, that's good.", "tokens": [407, 321, 434, 406, 3312, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 47, "seek": 19332, "start": 215.92, "end": 221.56, "text": " So you probably recognize these things, they're all bigger brands at the moment than VV8,", "tokens": [407, 291, 1391, 5521, 613, 721, 11, 436, 434, 439, 3801, 11324, 412, 264, 1623, 813, 691, 53, 23, 11], "temperature": 0.0, "avg_logprob": -0.15868093121436336, "compression_ratio": 1.550185873605948, "no_speech_prob": 2.2462876586359926e-05}, {"id": 48, "seek": 22156, "start": 221.56, "end": 223.72, "text": " so VV8 is growing fast.", "tokens": [370, 691, 53, 23, 307, 4194, 2370, 13], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 49, "seek": 22156, "start": 223.72, "end": 228.28, "text": " And some of those vendors have really great blog posts where you see some of the like", "tokens": [400, 512, 295, 729, 22056, 362, 534, 869, 6968, 12300, 689, 291, 536, 512, 295, 264, 411], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 50, "seek": 22156, "start": 228.28, "end": 231.56, "text": " optimization topics and some of the crazy stuff that they have to do.", "tokens": [19618, 8378, 293, 512, 295, 264, 3219, 1507, 300, 436, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 51, "seek": 22156, "start": 231.56, "end": 235.24, "text": " So if you've contributed to one of those, some of the things I'm going to say might", "tokens": [407, 498, 291, 600, 18434, 281, 472, 295, 729, 11, 512, 295, 264, 721, 286, 478, 516, 281, 584, 1062], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 52, "seek": 22156, "start": 235.24, "end": 240.92000000000002, "text": " sound familiar, if not then buckle up, it's going to get mad.", "tokens": [1626, 4963, 11, 498, 406, 550, 37686, 493, 11, 309, 311, 516, 281, 483, 5244, 13], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 53, "seek": 22156, "start": 240.92000000000002, "end": 245.76, "text": " So first stop on our mad journey memory allocation, then that also brings us to our friend the", "tokens": [407, 700, 1590, 322, 527, 5244, 4671, 4675, 27599, 11, 550, 300, 611, 5607, 505, 281, 527, 1277, 264], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 54, "seek": 22156, "start": 245.76, "end": 247.08, "text": " garbage collector.", "tokens": [14150, 23960, 13], "temperature": 0.0, "avg_logprob": -0.15825875881498894, "compression_ratio": 1.6755725190839694, "no_speech_prob": 2.7956088160863146e-05}, {"id": 55, "seek": 24708, "start": 247.08, "end": 253.04000000000002, "text": " So for any high performance go application, sooner or later you're going to talk about", "tokens": [407, 337, 604, 1090, 3389, 352, 3861, 11, 15324, 420, 1780, 291, 434, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 56, "seek": 24708, "start": 253.04000000000002, "end": 256.56, "text": " memory allocations and definitely consider a database a high performance application", "tokens": [4675, 12660, 763, 293, 2138, 1949, 257, 8149, 257, 1090, 3389, 3861], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 57, "seek": 24708, "start": 256.56, "end": 259.8, "text": " or at least consider VV8 a high performance application.", "tokens": [420, 412, 1935, 1949, 691, 53, 23, 257, 1090, 3389, 3861, 13], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 58, "seek": 24708, "start": 259.8, "end": 264.40000000000003, "text": " And if you think of what databases do, like in essence basically you have something on", "tokens": [400, 498, 291, 519, 295, 437, 22380, 360, 11, 411, 294, 12801, 1936, 291, 362, 746, 322], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 59, "seek": 24708, "start": 264.40000000000003, "end": 267.84000000000003, "text": " disk and you want to serve it to the user, that's like one of the most important user", "tokens": [12355, 293, 291, 528, 281, 4596, 309, 281, 264, 4195, 11, 300, 311, 411, 472, 295, 264, 881, 1021, 4195], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 60, "seek": 24708, "start": 267.84000000000003, "end": 270.08000000000004, "text": " journeys in a database.", "tokens": [36736, 294, 257, 8149, 13], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 61, "seek": 24708, "start": 270.08000000000004, "end": 274.96000000000004, "text": " And here this is represented by just a number, so it went for UN32, so that's just four bytes", "tokens": [400, 510, 341, 307, 10379, 538, 445, 257, 1230, 11, 370, 309, 1437, 337, 8229, 11440, 11, 370, 300, 311, 445, 1451, 36088], "temperature": 0.0, "avg_logprob": -0.16022339805227812, "compression_ratio": 1.8020833333333333, "no_speech_prob": 4.091394202987431e-06}, {"id": 62, "seek": 27496, "start": 274.96, "end": 279.03999999999996, "text": " on disk and basically you can see sort of these four bytes.", "tokens": [322, 12355, 293, 1936, 291, 393, 536, 1333, 295, 613, 1451, 36088, 13], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 63, "seek": 27496, "start": 279.03999999999996, "end": 284.64, "text": " If you parse them into Go they would have the value of 16 in that UN32 and this is essentially", "tokens": [759, 291, 48377, 552, 666, 1037, 436, 576, 362, 264, 2158, 295, 3165, 294, 300, 8229, 11440, 293, 341, 307, 4476], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 64, "seek": 27496, "start": 284.64, "end": 289.12, "text": " something very much simplified that a database needs to do and it needs to do it over and", "tokens": [746, 588, 709, 26335, 300, 257, 8149, 2203, 281, 360, 293, 309, 2203, 281, 360, 309, 670, 293], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 65, "seek": 27496, "start": 289.12, "end": 290.96, "text": " over again.", "tokens": [670, 797, 13], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 66, "seek": 27496, "start": 290.96, "end": 296.08, "text": " So the standard library gives us the encoding slash binary package and there we have this", "tokens": [407, 264, 3832, 6405, 2709, 505, 264, 43430, 17330, 17434, 7372, 293, 456, 321, 362, 341], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 67, "seek": 27496, "start": 296.08, "end": 299.84, "text": " binary dot read method which I think looks really cool.", "tokens": [17434, 5893, 1401, 3170, 597, 286, 519, 1542, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 68, "seek": 27496, "start": 299.84, "end": 304.52, "text": " To me it looks like idiomatic Go because it has the io dot reader interface like everyone's", "tokens": [1407, 385, 309, 1542, 411, 18014, 13143, 1037, 570, 309, 575, 264, 19785, 5893, 15149, 9226, 411, 1518, 311], "temperature": 0.0, "avg_logprob": -0.16442796103974694, "compression_ratio": 1.703448275862069, "no_speech_prob": 1.012889788398752e-05}, {"id": 69, "seek": 30452, "start": 304.52, "end": 309.35999999999996, "text": " favorite interface and you can put all of that stuff in there and if you run this code", "tokens": [2954, 9226, 293, 291, 393, 829, 439, 295, 300, 1507, 294, 456, 293, 498, 291, 1190, 341, 3089], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 70, "seek": 30452, "start": 309.35999999999996, "end": 312.03999999999996, "text": " and there's no error then basically you get exactly what you want.", "tokens": [293, 456, 311, 572, 6713, 550, 1936, 291, 483, 2293, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 71, "seek": 30452, "start": 312.03999999999996, "end": 317.08, "text": " You could turn those sort of four bytes that were somewhere on disk, turn them into our", "tokens": [509, 727, 1261, 729, 1333, 295, 1451, 36088, 300, 645, 4079, 322, 12355, 11, 1261, 552, 666, 527], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 72, "seek": 30452, "start": 317.08, "end": 320.91999999999996, "text": " in-memory representation of that UN32.", "tokens": [294, 12, 17886, 827, 10290, 295, 300, 8229, 11440, 13], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 73, "seek": 30452, "start": 320.91999999999996, "end": 327.47999999999996, "text": " So is this a good idea to do that exactly like well if you do it once or maybe twice", "tokens": [407, 307, 341, 257, 665, 1558, 281, 360, 300, 2293, 411, 731, 498, 291, 360, 309, 1564, 420, 1310, 6091], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 74, "seek": 30452, "start": 327.47999999999996, "end": 329.15999999999997, "text": " could be a good idea.", "tokens": [727, 312, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 75, "seek": 30452, "start": 329.15999999999997, "end": 332.59999999999997, "text": " If you do it a billion times this is what happens.", "tokens": [759, 291, 360, 309, 257, 5218, 1413, 341, 307, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.13455875296341746, "compression_ratio": 1.7109375, "no_speech_prob": 2.795862383209169e-05}, {"id": 76, "seek": 33260, "start": 332.6, "end": 338.08000000000004, "text": " So for those of you who are new to CPU profiles in Go this is madness.", "tokens": [407, 337, 729, 295, 291, 567, 366, 777, 281, 13199, 23693, 294, 1037, 341, 307, 28736, 13], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 77, "seek": 33260, "start": 338.08000000000004, "end": 339.08000000000004, "text": " This is pretty bad.", "tokens": [639, 307, 1238, 1578, 13], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 78, "seek": 33260, "start": 339.08000000000004, "end": 344.44, "text": " So first of all you see it in the center parsing those 1 billion numbers took 26 seconds and", "tokens": [407, 700, 295, 439, 291, 536, 309, 294, 264, 3056, 21156, 278, 729, 502, 5218, 3547, 1890, 7551, 3949, 293], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 79, "seek": 33260, "start": 344.44, "end": 350.16, "text": " 26 seconds is not the kind of time that we ever have in the database but worse than that", "tokens": [7551, 3949, 307, 406, 264, 733, 295, 565, 300, 321, 1562, 362, 294, 264, 8149, 457, 5324, 813, 300], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 80, "seek": 33260, "start": 350.16, "end": 356.08000000000004, "text": " if you look at that profile we have stuff like runtime, malloc, gc, runtime, mem, move,", "tokens": [498, 291, 574, 412, 300, 7964, 321, 362, 1507, 411, 34474, 11, 275, 336, 905, 11, 290, 66, 11, 34474, 11, 1334, 11, 1286, 11], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 81, "seek": 33260, "start": 356.08000000000004, "end": 357.48, "text": " runtime, m, advice.", "tokens": [34474, 11, 275, 11, 5192, 13], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 82, "seek": 33260, "start": 357.48, "end": 361.84000000000003, "text": " So all these things they're related to memory allocations or to garbage collection.", "tokens": [407, 439, 613, 721, 436, 434, 4077, 281, 4675, 12660, 763, 420, 281, 14150, 5765, 13], "temperature": 0.0, "avg_logprob": -0.1817552505000945, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.0778816431411542e-05}, {"id": 83, "seek": 36184, "start": 361.84, "end": 365.79999999999995, "text": " What they're not related to is parsing data which is what we wanted to do.", "tokens": [708, 436, 434, 406, 4077, 281, 307, 21156, 278, 1412, 597, 307, 437, 321, 1415, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 84, "seek": 36184, "start": 365.79999999999995, "end": 370.0, "text": " So how much time of that 20 seconds did we spend what we wanted to do?", "tokens": [407, 577, 709, 565, 295, 300, 945, 3949, 630, 321, 3496, 437, 321, 1415, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 85, "seek": 36184, "start": 370.0, "end": 371.0, "text": " Don't know.", "tokens": [1468, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 86, "seek": 36184, "start": 371.0, "end": 373.11999999999995, "text": " It doesn't even show up in the profile.", "tokens": [467, 1177, 380, 754, 855, 493, 294, 264, 7964, 13], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 87, "seek": 36184, "start": 373.11999999999995, "end": 379.59999999999997, "text": " So to understand why that is the case we need to quickly talk about the stack and the heap.", "tokens": [407, 281, 1223, 983, 300, 307, 264, 1389, 321, 643, 281, 2661, 751, 466, 264, 8630, 293, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 88, "seek": 36184, "start": 379.59999999999997, "end": 384.15999999999997, "text": " So you can think of the stack as basically your function stack so you call one function", "tokens": [407, 291, 393, 519, 295, 264, 8630, 382, 1936, 428, 2445, 8630, 370, 291, 818, 472, 2445], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 89, "seek": 36184, "start": 384.15999999999997, "end": 387.96, "text": " that calls another function and then at some point basically you go back through the stack", "tokens": [300, 5498, 1071, 2445, 293, 550, 412, 512, 935, 1936, 291, 352, 646, 807, 264, 8630], "temperature": 0.0, "avg_logprob": -0.1393923193721448, "compression_ratio": 1.813953488372093, "no_speech_prob": 3.6110156997892773e-06}, {"id": 90, "seek": 38796, "start": 387.96, "end": 392.52, "text": " and this is very short lift and this is cheap and fast to allocate and why is it cheap?", "tokens": [293, 341, 307, 588, 2099, 5533, 293, 341, 307, 7084, 293, 2370, 281, 35713, 293, 983, 307, 309, 7084, 30], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 91, "seek": 38796, "start": 392.52, "end": 397.28, "text": " Because you know exactly the runtime of your variables or the life cycle of your variables", "tokens": [1436, 291, 458, 2293, 264, 34474, 295, 428, 9102, 420, 264, 993, 6586, 295, 428, 9102], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 92, "seek": 38796, "start": 397.28, "end": 399.12, "text": " so you don't even need to involve the garbage collector.", "tokens": [370, 291, 500, 380, 754, 643, 281, 9494, 264, 14150, 23960, 13], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 93, "seek": 38796, "start": 399.12, "end": 401.47999999999996, "text": " So no garbage collector cheap and fast.", "tokens": [407, 572, 14150, 23960, 7084, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 94, "seek": 38796, "start": 401.47999999999996, "end": 405.91999999999996, "text": " Then on the other side you have the heap and the heap is basically this sort of long lift", "tokens": [1396, 322, 264, 661, 1252, 291, 362, 264, 33591, 293, 264, 33591, 307, 1936, 341, 1333, 295, 938, 5533], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 95, "seek": 38796, "start": 405.91999999999996, "end": 411.12, "text": " kind of memory and that's expensive and slow to allocate and why because and also to deallocate", "tokens": [733, 295, 4675, 293, 300, 311, 5124, 293, 2964, 281, 35713, 293, 983, 570, 293, 611, 281, 368, 336, 42869], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 96, "seek": 38796, "start": 411.12, "end": 414.08, "text": " and why because it involves the garbage collector.", "tokens": [293, 983, 570, 309, 11626, 264, 14150, 23960, 13], "temperature": 0.0, "avg_logprob": -0.13121229807535809, "compression_ratio": 2.064516129032258, "no_speech_prob": 6.539580226672115e-06}, {"id": 97, "seek": 41408, "start": 414.08, "end": 418.03999999999996, "text": " So if the stack is so much cheaper then we can just always allocate on the stack right.", "tokens": [407, 498, 264, 8630, 307, 370, 709, 12284, 550, 321, 393, 445, 1009, 35713, 322, 264, 8630, 558, 13], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 98, "seek": 41408, "start": 418.03999999999996, "end": 422.12, "text": " So warning this is not real go please do not do this.", "tokens": [407, 9164, 341, 307, 406, 957, 352, 1767, 360, 406, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 99, "seek": 41408, "start": 422.12, "end": 425.84, "text": " This is sort of a fictional example of allocating a buffer of size 8 and then we're going to", "tokens": [639, 307, 1333, 295, 257, 28911, 1365, 295, 12660, 990, 257, 21762, 295, 2744, 1649, 293, 550, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 100, "seek": 41408, "start": 425.84, "end": 430.03999999999996, "text": " say like yeah please put this on the stack and that is not how it works and for most", "tokens": [584, 411, 1338, 1767, 829, 341, 322, 264, 8630, 293, 300, 307, 406, 577, 309, 1985, 293, 337, 881], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 101, "seek": 41408, "start": 430.03999999999996, "end": 433.24, "text": " of you you probably say like this is pretty good that it's not that it works that way", "tokens": [295, 291, 291, 1391, 584, 411, 341, 307, 1238, 665, 300, 309, 311, 406, 300, 309, 1985, 300, 636], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 102, "seek": 41408, "start": 433.24, "end": 435.03999999999996, "text": " because why would you want to deal with that.", "tokens": [570, 983, 576, 291, 528, 281, 2028, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 103, "seek": 41408, "start": 435.03999999999996, "end": 439.15999999999997, "text": " But for me just trying to build a database and go sometimes like this something like", "tokens": [583, 337, 385, 445, 1382, 281, 1322, 257, 8149, 293, 352, 2171, 411, 341, 746, 411], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 104, "seek": 41408, "start": 439.15999999999997, "end": 441.44, "text": " this may be good or maybe not.", "tokens": [341, 815, 312, 665, 420, 1310, 406, 13], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 105, "seek": 41408, "start": 441.44, "end": 442.88, "text": " So how does it work?", "tokens": [407, 577, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.17231129021044597, "compression_ratio": 1.9029126213592233, "no_speech_prob": 1.9031224383070366e-06}, {"id": 106, "seek": 44288, "start": 442.88, "end": 445.64, "text": " Go does something that's called escape analysis.", "tokens": [1037, 775, 746, 300, 311, 1219, 7615, 5215, 13], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 107, "seek": 44288, "start": 445.64, "end": 452.32, "text": " So if you compile your code with gcflags-m then go annotates your code basically and", "tokens": [407, 498, 291, 31413, 428, 3089, 365, 290, 66, 3423, 12109, 12, 76, 550, 352, 25339, 1024, 428, 3089, 1936, 293], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 108, "seek": 44288, "start": 452.32, "end": 454.28, "text": " tells you sort of what's happening there.", "tokens": [5112, 291, 1333, 295, 437, 311, 2737, 456, 13], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 109, "seek": 44288, "start": 454.28, "end": 460.64, "text": " So here you can see in the second line that this num variable that we used was moved to", "tokens": [407, 510, 291, 393, 536, 294, 264, 1150, 1622, 300, 341, 1031, 7006, 300, 321, 1143, 390, 4259, 281], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 110, "seek": 44288, "start": 460.64, "end": 465.71999999999997, "text": " the heap and then in the next point you see the bytes.reader which represents our io.reader", "tokens": [264, 33591, 293, 550, 294, 264, 958, 935, 291, 536, 264, 36088, 13, 2538, 260, 597, 8855, 527, 19785, 13, 2538, 260], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 111, "seek": 44288, "start": 465.71999999999997, "end": 466.92, "text": " escaped to the heap.", "tokens": [20397, 281, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 112, "seek": 44288, "start": 466.92, "end": 469.92, "text": " So two times we see that something happened to the or went to the heap.", "tokens": [407, 732, 1413, 321, 536, 300, 746, 2011, 281, 264, 420, 1437, 281, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.1502317607912243, "compression_ratio": 1.8285714285714285, "no_speech_prob": 8.662216714583337e-06}, {"id": 113, "seek": 46992, "start": 469.92, "end": 474.40000000000003, "text": " We don't exactly know what happened yet but at least there's proof that we have this kind", "tokens": [492, 500, 380, 2293, 458, 437, 2011, 1939, 457, 412, 1935, 456, 311, 8177, 300, 321, 362, 341, 733], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 114, "seek": 46992, "start": 474.40000000000003, "end": 477.0, "text": " of allocation problem.", "tokens": [295, 27599, 1154, 13], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 115, "seek": 46992, "start": 477.0, "end": 478.40000000000003, "text": " So what can we do?", "tokens": [407, 437, 393, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 116, "seek": 46992, "start": 478.40000000000003, "end": 479.92, "text": " Well we can simplify a bit.", "tokens": [1042, 321, 393, 20460, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 117, "seek": 46992, "start": 479.92, "end": 485.24, "text": " Turns out that the binary or encoding binary package also has another method that looks", "tokens": [29524, 484, 300, 264, 17434, 420, 43430, 17434, 7372, 611, 575, 1071, 3170, 300, 1542], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 118, "seek": 46992, "start": 485.24, "end": 490.92, "text": " like this which is just called view in 32 on the little endian package and it kind of", "tokens": [411, 341, 597, 307, 445, 1219, 1910, 294, 8858, 322, 264, 707, 917, 952, 7372, 293, 309, 733, 295], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 119, "seek": 46992, "start": 490.92, "end": 491.92, "text": " does the same thing.", "tokens": [775, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 120, "seek": 46992, "start": 491.92, "end": 495.32, "text": " You just put in the buffer on the one side so no reader this time you just put in the", "tokens": [509, 445, 829, 294, 264, 21762, 322, 264, 472, 1252, 370, 572, 15149, 341, 565, 291, 445, 829, 294, 264], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 121, "seek": 46992, "start": 495.32, "end": 499.8, "text": " raw buffer basically with the position offset and on the other side you get the number out.", "tokens": [8936, 21762, 1936, 365, 264, 2535, 18687, 293, 322, 264, 661, 1252, 291, 483, 264, 1230, 484, 13], "temperature": 0.0, "avg_logprob": -0.15952893307334498, "compression_ratio": 1.75, "no_speech_prob": 3.137894509563921e-06}, {"id": 122, "seek": 49980, "start": 499.8, "end": 504.0, "text": " And the crazy thing is this one line needs no memory allocations.", "tokens": [400, 264, 3219, 551, 307, 341, 472, 1622, 2203, 572, 4675, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 123, "seek": 49980, "start": 504.0, "end": 510.2, "text": " So if we do that again our one billion numbers that took 26 seconds before now take 600 milliseconds", "tokens": [407, 498, 321, 360, 300, 797, 527, 472, 5218, 3547, 300, 1890, 7551, 3949, 949, 586, 747, 11849, 34184], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 124, "seek": 49980, "start": 510.2, "end": 515.28, "text": " and now we're starting to get into a range where like this is acceptable for a data basis.", "tokens": [293, 586, 321, 434, 2891, 281, 483, 666, 257, 3613, 689, 411, 341, 307, 15513, 337, 257, 1412, 5143, 13], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 125, "seek": 49980, "start": 515.28, "end": 519.88, "text": " And more importantly what we see on that profile, the profile is so much simpler right now.", "tokens": [400, 544, 8906, 437, 321, 536, 322, 300, 7964, 11, 264, 7964, 307, 370, 709, 18587, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 126, "seek": 49980, "start": 519.88, "end": 525.6, "text": " There's basically just this one function there and that is what we wanted to do.", "tokens": [821, 311, 1936, 445, 341, 472, 2445, 456, 293, 300, 307, 437, 321, 1415, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 127, "seek": 49980, "start": 525.6, "end": 529.16, "text": " So admittedly we're not doing much other than parsing the data at the moment but at least", "tokens": [407, 14920, 356, 321, 434, 406, 884, 709, 661, 813, 21156, 278, 264, 1412, 412, 264, 1623, 457, 412, 1935], "temperature": 0.0, "avg_logprob": -0.12674289796410537, "compression_ratio": 1.6883116883116882, "no_speech_prob": 9.363365279568825e-06}, {"id": 128, "seek": 52916, "start": 529.16, "end": 534.12, "text": " we got sort of rid of all the noise and you can see the speed up.", "tokens": [321, 658, 1333, 295, 3973, 295, 439, 264, 5658, 293, 291, 393, 536, 264, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 129, "seek": 52916, "start": 534.12, "end": 536.56, "text": " Okay so quickly to recap.", "tokens": [1033, 370, 2661, 281, 20928, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 130, "seek": 52916, "start": 536.56, "end": 540.0, "text": " If we say a database is nothing but reading data and sort of parsing it to serve it to", "tokens": [759, 321, 584, 257, 8149, 307, 1825, 457, 3760, 1412, 293, 1333, 295, 21156, 278, 309, 281, 4596, 309, 281], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 131, "seek": 52916, "start": 540.0, "end": 546.16, "text": " the user then we do that over and over again then we need to take care of memory allocations.", "tokens": [264, 4195, 550, 321, 360, 300, 670, 293, 670, 797, 550, 321, 643, 281, 747, 1127, 295, 4675, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 132, "seek": 52916, "start": 546.16, "end": 548.6, "text": " And the fix in this case was super simple.", "tokens": [400, 264, 3191, 294, 341, 1389, 390, 1687, 2199, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 133, "seek": 52916, "start": 548.6, "end": 553.62, "text": " We changed two lines of code and reduced it from 26 seconds to 600 milliseconds.", "tokens": [492, 3105, 732, 3876, 295, 3089, 293, 9212, 309, 490, 7551, 3949, 281, 11849, 34184, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 134, "seek": 52916, "start": 553.62, "end": 557.28, "text": " But why we had to do that wasn't very intuitive like that it wasn't very obvious.", "tokens": [583, 983, 321, 632, 281, 360, 300, 2067, 380, 588, 21769, 411, 300, 309, 2067, 380, 588, 6322, 13], "temperature": 0.0, "avg_logprob": -0.16093008422851562, "compression_ratio": 1.707142857142857, "no_speech_prob": 5.172888904780848e-06}, {"id": 135, "seek": 55728, "start": 557.28, "end": 562.0, "text": " In fact I haven't even told you yet why this binary dot little nvn dot read why that escaped", "tokens": [682, 1186, 286, 2378, 380, 754, 1907, 291, 1939, 983, 341, 17434, 5893, 707, 297, 85, 77, 5893, 1401, 983, 300, 20397], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 136, "seek": 55728, "start": 562.0, "end": 563.0, "text": " to the heap.", "tokens": [281, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 137, "seek": 55728, "start": 563.0, "end": 566.6, "text": " And in this case it's because we passed in a pointer and we passed in an interface and", "tokens": [400, 294, 341, 1389, 309, 311, 570, 321, 4678, 294, 257, 23918, 293, 321, 4678, 294, 364, 9226, 293], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 138, "seek": 55728, "start": 566.6, "end": 570.6, "text": " that's kind of a hint basically that something might escape to the heap.", "tokens": [300, 311, 733, 295, 257, 12075, 1936, 300, 746, 1062, 7615, 281, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 139, "seek": 55728, "start": 570.6, "end": 576.16, "text": " So what I would wish is yes this is not a topic that you need every day you write go", "tokens": [407, 437, 286, 576, 3172, 307, 2086, 341, 307, 406, 257, 4829, 300, 291, 643, 633, 786, 291, 2464, 352], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 140, "seek": 55728, "start": 576.16, "end": 582.16, "text": " but maybe if you do need this would be cool if there was better education.", "tokens": [457, 1310, 498, 291, 360, 643, 341, 576, 312, 1627, 498, 456, 390, 1101, 3309, 13], "temperature": 0.0, "avg_logprob": -0.16115332516756925, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.2481567637223634e-06}, {"id": 141, "seek": 58216, "start": 582.16, "end": 588.12, "text": " Okay so second step delay the coding so this is kind of the idea that we wouldn't want", "tokens": [1033, 370, 1150, 1823, 8577, 264, 17720, 370, 341, 307, 733, 295, 264, 1558, 300, 321, 2759, 380, 528], "temperature": 0.0, "avg_logprob": -0.11460077497694228, "compression_ratio": 1.5701754385964912, "no_speech_prob": 7.887450919952244e-06}, {"id": 142, "seek": 58216, "start": 588.12, "end": 590.8399999999999, "text": " to do the same work twice.", "tokens": [281, 360, 264, 912, 589, 6091, 13], "temperature": 0.0, "avg_logprob": -0.11460077497694228, "compression_ratio": 1.5701754385964912, "no_speech_prob": 7.887450919952244e-06}, {"id": 143, "seek": 58216, "start": 590.8399999999999, "end": 596.68, "text": " And we're sticking with our example of serving data from disk but now while the number example", "tokens": [400, 321, 434, 13465, 365, 527, 1365, 295, 8148, 1412, 490, 12355, 457, 586, 1339, 264, 1230, 1365], "temperature": 0.0, "avg_logprob": -0.11460077497694228, "compression_ratio": 1.5701754385964912, "no_speech_prob": 7.887450919952244e-06}, {"id": 144, "seek": 58216, "start": 596.68, "end": 600.52, "text": " was a bit too simple so let's make it slightly more complex.", "tokens": [390, 257, 857, 886, 2199, 370, 718, 311, 652, 309, 4748, 544, 3997, 13], "temperature": 0.0, "avg_logprob": -0.11460077497694228, "compression_ratio": 1.5701754385964912, "no_speech_prob": 7.887450919952244e-06}, {"id": 145, "seek": 58216, "start": 600.52, "end": 608.12, "text": " We have this nested array here basically a sort of slice off slice view in 64 and that's", "tokens": [492, 362, 341, 15646, 292, 10225, 510, 1936, 257, 1333, 295, 13153, 766, 13153, 1910, 294, 12145, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.11460077497694228, "compression_ratio": 1.5701754385964912, "no_speech_prob": 7.887450919952244e-06}, {"id": 146, "seek": 60812, "start": 608.12, "end": 612.04, "text": " representative now for a more complex object on your database.", "tokens": [12424, 586, 337, 257, 544, 3997, 2657, 322, 428, 8149, 13], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 147, "seek": 60812, "start": 612.04, "end": 614.92, "text": " Of course in reality you'd have like string props and other kind of things but just sort", "tokens": [2720, 1164, 294, 4103, 291, 1116, 362, 411, 6798, 26173, 293, 661, 733, 295, 721, 457, 445, 1333], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 148, "seek": 60812, "start": 614.92, "end": 618.5600000000001, "text": " of to show that there's more going on than a single number.", "tokens": [295, 281, 855, 300, 456, 311, 544, 516, 322, 813, 257, 2167, 1230, 13], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 149, "seek": 60812, "start": 618.5600000000001, "end": 622.6, "text": " And let's say we have 80 million of them so 10 million of the outer slice and then eight", "tokens": [400, 718, 311, 584, 321, 362, 4688, 2459, 295, 552, 370, 1266, 2459, 295, 264, 10847, 13153, 293, 550, 3180], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 150, "seek": 60812, "start": 622.6, "end": 625.92, "text": " elements in each inner slice and our task is just to sum those up.", "tokens": [4959, 294, 1184, 7284, 13153, 293, 527, 5633, 307, 445, 281, 2408, 729, 493, 13], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 151, "seek": 60812, "start": 625.92, "end": 628.8, "text": " So these are 80 million numbers and we want to know what is the sum of them.", "tokens": [407, 613, 366, 4688, 2459, 3547, 293, 321, 528, 281, 458, 437, 307, 264, 2408, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 152, "seek": 60812, "start": 628.8, "end": 636.4, "text": " So that is actually kind of a realistic database task for an OLAP kind of database.", "tokens": [407, 300, 307, 767, 733, 295, 257, 12465, 8149, 5633, 337, 364, 39191, 4715, 733, 295, 8149, 13], "temperature": 0.0, "avg_logprob": -0.13975855020376352, "compression_ratio": 1.8020477815699658, "no_speech_prob": 4.563927177514415e-06}, {"id": 153, "seek": 63640, "start": 636.4, "end": 641.0799999999999, "text": " But we need to somehow represent that data on disk and we're looking at two ways to do", "tokens": [583, 321, 643, 281, 6063, 2906, 300, 1412, 322, 12355, 293, 321, 434, 1237, 412, 732, 2098, 281, 360], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 154, "seek": 63640, "start": 641.0799999999999, "end": 642.0799999999999, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 155, "seek": 63640, "start": 642.0799999999999, "end": 645.4399999999999, "text": " The first one is JSON representation and then the second one would be the sort of binary", "tokens": [440, 700, 472, 307, 31828, 10290, 293, 550, 264, 1150, 472, 576, 312, 264, 1333, 295, 17434], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 156, "seek": 63640, "start": 645.4399999999999, "end": 648.1999999999999, "text": " encoding and then there'll be more.", "tokens": [43430, 293, 550, 456, 603, 312, 544, 13], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 157, "seek": 63640, "start": 648.1999999999999, "end": 651.16, "text": " So JSON is basically just here for completeness aid.", "tokens": [407, 31828, 307, 1936, 445, 510, 337, 1557, 15264, 9418, 13], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 158, "seek": 63640, "start": 651.16, "end": 654.56, "text": " We can basically rule it out immediately so when you're building a database you're probably", "tokens": [492, 393, 1936, 4978, 309, 484, 4258, 370, 562, 291, 434, 2390, 257, 8149, 291, 434, 1391], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 159, "seek": 63640, "start": 654.56, "end": 660.4399999999999, "text": " not using JSON to store stuff on disk unless it's sort of a JSON database.", "tokens": [406, 1228, 31828, 281, 3531, 1507, 322, 12355, 5969, 309, 311, 1333, 295, 257, 31828, 8149, 13], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 160, "seek": 63640, "start": 660.4399999999999, "end": 665.12, "text": " Why because it's space inefficient so if you want to represent those numbers on disk like", "tokens": [1545, 570, 309, 311, 1901, 43495, 370, 498, 291, 528, 281, 2906, 729, 3547, 322, 12355, 411], "temperature": 0.0, "avg_logprob": -0.15811570485432944, "compression_ratio": 1.7804054054054055, "no_speech_prob": 4.858482952840859e-06}, {"id": 161, "seek": 66512, "start": 665.12, "end": 669.96, "text": " JSON basically uses strings for it and then you have all these control characters like", "tokens": [31828, 1936, 4960, 13985, 337, 309, 293, 550, 291, 362, 439, 613, 1969, 4342, 411], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 162, "seek": 66512, "start": 669.96, "end": 674.08, "text": " your curly braces and your quotes and your columns and everything that takes up space.", "tokens": [428, 32066, 41537, 293, 428, 19963, 293, 428, 13766, 293, 1203, 300, 2516, 493, 1901, 13], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 163, "seek": 66512, "start": 674.08, "end": 678.68, "text": " So in our fictional example that would take up 1.6 gigabyte and you'll see soon that we", "tokens": [407, 294, 527, 28911, 1365, 300, 576, 747, 493, 502, 13, 21, 8741, 34529, 293, 291, 603, 536, 2321, 300, 321], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 164, "seek": 66512, "start": 678.68, "end": 680.16, "text": " can do that more efficient.", "tokens": [393, 360, 300, 544, 7148, 13], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 165, "seek": 66512, "start": 680.16, "end": 685.12, "text": " But also it's slow and part of why it's slow is again because we have these memory allocations", "tokens": [583, 611, 309, 311, 2964, 293, 644, 295, 983, 309, 311, 2964, 307, 797, 570, 321, 362, 613, 4675, 12660, 763], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 166, "seek": 66512, "start": 685.12, "end": 687.64, "text": " but also the whole parsing just takes time.", "tokens": [457, 611, 264, 1379, 21156, 278, 445, 2516, 565, 13], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 167, "seek": 66512, "start": 687.64, "end": 694.24, "text": " So in our example this took 14 seconds to sum up those 80 million numbers and as I said", "tokens": [407, 294, 527, 1365, 341, 1890, 3499, 3949, 281, 2408, 493, 729, 4688, 2459, 3547, 293, 382, 286, 848], "temperature": 0.0, "avg_logprob": -0.13357636236375378, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.4501195437333081e-05}, {"id": 168, "seek": 69424, "start": 694.24, "end": 699.84, "text": " before you just don't have double digit seconds in a database.", "tokens": [949, 291, 445, 500, 380, 362, 3834, 14293, 3949, 294, 257, 8149, 13], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 169, "seek": 69424, "start": 699.84, "end": 704.8, "text": " So we can do something that's a bit smarter which is called length encoding.", "tokens": [407, 321, 393, 360, 746, 300, 311, 257, 857, 20294, 597, 307, 1219, 4641, 43430, 13], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 170, "seek": 69424, "start": 704.8, "end": 710.76, "text": " So we're encoding this basically as binary and we're spending one in this case one byte", "tokens": [407, 321, 434, 43430, 341, 1936, 382, 17434, 293, 321, 434, 6434, 472, 294, 341, 1389, 472, 40846], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 171, "seek": 69424, "start": 710.76, "end": 714.4, "text": " so that's basically a U and 8 and we're using that as a length indicator.", "tokens": [370, 300, 311, 1936, 257, 624, 293, 1649, 293, 321, 434, 1228, 300, 382, 257, 4641, 16961, 13], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 172, "seek": 69424, "start": 714.4, "end": 717.44, "text": " So basically that tells us that when we're reading this from disk that just tells us", "tokens": [407, 1936, 300, 5112, 505, 300, 562, 321, 434, 3760, 341, 490, 12355, 300, 445, 5112, 505], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 173, "seek": 69424, "start": 717.44, "end": 718.44, "text": " what's coming up.", "tokens": [437, 311, 1348, 493, 13], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 174, "seek": 69424, "start": 718.44, "end": 722.08, "text": " So in this case it says we have eight elements coming up and then we know that our elements", "tokens": [407, 294, 341, 1389, 309, 1619, 321, 362, 3180, 4959, 1348, 493, 293, 550, 321, 458, 300, 527, 4959], "temperature": 0.0, "avg_logprob": -0.1299916486271092, "compression_ratio": 1.9604743083003953, "no_speech_prob": 5.01455542689655e-06}, {"id": 175, "seek": 72208, "start": 722.08, "end": 725.5200000000001, "text": " in this example is U and 32 so that's four bytes each.", "tokens": [294, 341, 1365, 307, 624, 293, 8858, 370, 300, 311, 1451, 36088, 1184, 13], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 176, "seek": 72208, "start": 725.5200000000001, "end": 729.84, "text": " So basically the next 32 bytes that we're reading are going to be our eight inner arrays", "tokens": [407, 1936, 264, 958, 8858, 36088, 300, 321, 434, 3760, 366, 516, 281, 312, 527, 3180, 7284, 41011], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 177, "seek": 72208, "start": 729.84, "end": 730.84, "text": " and then we just continue.", "tokens": [293, 550, 321, 445, 2354, 13], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 178, "seek": 72208, "start": 730.84, "end": 735.2, "text": " Then we basically read the next length indicator and this way we can encode the stuff sort", "tokens": [1396, 321, 1936, 1401, 264, 958, 4641, 16961, 293, 341, 636, 321, 393, 2058, 1429, 264, 1507, 1333], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 179, "seek": 72208, "start": 735.2, "end": 739.0400000000001, "text": " of in one contiguous thing.", "tokens": [295, 294, 472, 660, 30525, 551, 13], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 180, "seek": 72208, "start": 739.0400000000001, "end": 743.44, "text": " Then of course we have to decode it somehow and we can do that because we've learned from", "tokens": [1396, 295, 1164, 321, 362, 281, 979, 1429, 309, 6063, 293, 321, 393, 360, 300, 570, 321, 600, 3264, 490], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 181, "seek": 72208, "start": 743.44, "end": 747.96, "text": " our previous example right so we're not going to use binary.lnlndian.read but we're doing", "tokens": [527, 3894, 1365, 558, 370, 321, 434, 406, 516, 281, 764, 17434, 13, 75, 77, 75, 273, 952, 13, 2538, 457, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.15204020244319263, "compression_ratio": 1.7900763358778626, "no_speech_prob": 1.1840414117614273e-05}, {"id": 182, "seek": 74796, "start": 747.96, "end": 753.76, "text": " this in an allocation free way, you can see that in the length line basically and yeah", "tokens": [341, 294, 364, 27599, 1737, 636, 11, 291, 393, 536, 300, 294, 264, 4641, 1622, 1936, 293, 1338], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 183, "seek": 74796, "start": 753.76, "end": 758.76, "text": " our goal is to take that data and put it into our nested sort of go slice of slice of slice", "tokens": [527, 3387, 307, 281, 747, 300, 1412, 293, 829, 309, 666, 527, 15646, 292, 1333, 295, 352, 13153, 295, 13153, 295, 13153], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 184, "seek": 74796, "start": 758.76, "end": 764.88, "text": " of U in 64 and the code here basically you see we're reading the length and then we're", "tokens": [295, 624, 294, 12145, 293, 264, 3089, 510, 1936, 291, 536, 321, 434, 3760, 264, 4641, 293, 550, 321, 434], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 185, "seek": 74796, "start": 764.88, "end": 768.1600000000001, "text": " increasing our offset so we know where to read from and then we're basically repeating", "tokens": [5662, 527, 18687, 370, 321, 458, 689, 281, 1401, 490, 293, 550, 321, 434, 1936, 18617], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 186, "seek": 74796, "start": 768.1600000000001, "end": 774.76, "text": " this for the inner slice which is just hinted at here by the decode inner function.", "tokens": [341, 337, 264, 7284, 13153, 597, 307, 445, 12075, 292, 412, 510, 538, 264, 979, 1429, 7284, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 187, "seek": 74796, "start": 774.76, "end": 777.0, "text": " So what happens when we do this?", "tokens": [407, 437, 2314, 562, 321, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.1471821059528579, "compression_ratio": 1.83203125, "no_speech_prob": 8.01137684902642e-06}, {"id": 188, "seek": 77700, "start": 777.0, "end": 781.96, "text": " First of all the good news, 660 megabytes that's way less than our 1.6 gigabyte before", "tokens": [2386, 295, 439, 264, 665, 2583, 11, 1386, 4550, 10816, 24538, 300, 311, 636, 1570, 813, 527, 502, 13, 21, 8741, 34529, 949], "temperature": 0.0, "avg_logprob": -0.1353010551951756, "compression_ratio": 1.6754716981132076, "no_speech_prob": 1.221569527842803e-05}, {"id": 189, "seek": 77700, "start": 781.96, "end": 789.08, "text": " so basically just by using a more space efficient way to represent data we've yeah done exactly", "tokens": [370, 1936, 445, 538, 1228, 257, 544, 1901, 7148, 636, 281, 2906, 1412, 321, 600, 1338, 1096, 2293], "temperature": 0.0, "avg_logprob": -0.1353010551951756, "compression_ratio": 1.6754716981132076, "no_speech_prob": 1.221569527842803e-05}, {"id": 190, "seek": 77700, "start": 789.08, "end": 794.48, "text": " that we've reduced our size also it's much much faster so we were at 14 seconds before", "tokens": [300, 321, 600, 9212, 527, 2744, 611, 309, 311, 709, 709, 4663, 370, 321, 645, 412, 3499, 3949, 949], "temperature": 0.0, "avg_logprob": -0.1353010551951756, "compression_ratio": 1.6754716981132076, "no_speech_prob": 1.221569527842803e-05}, {"id": 191, "seek": 77700, "start": 794.48, "end": 801.56, "text": " and now it's down to 260 milliseconds but this is our mad journey of building a database", "tokens": [293, 586, 309, 311, 760, 281, 44624, 34184, 457, 341, 307, 527, 5244, 4671, 295, 2390, 257, 8149], "temperature": 0.0, "avg_logprob": -0.1353010551951756, "compression_ratio": 1.6754716981132076, "no_speech_prob": 1.221569527842803e-05}, {"id": 192, "seek": 77700, "start": 801.56, "end": 806.48, "text": " so we're not done here yet because there's some hidden madness and the hidden madness", "tokens": [370, 321, 434, 406, 1096, 510, 1939, 570, 456, 311, 512, 7633, 28736, 293, 264, 7633, 28736], "temperature": 0.0, "avg_logprob": -0.1353010551951756, "compression_ratio": 1.6754716981132076, "no_speech_prob": 1.221569527842803e-05}, {"id": 193, "seek": 80648, "start": 806.48, "end": 812.48, "text": " is that we actually spend 250 milliseconds decoding while we spend 10 milliseconds summing", "tokens": [307, 300, 321, 767, 3496, 11650, 34184, 979, 8616, 1339, 321, 3496, 1266, 34184, 2408, 2810], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 194, "seek": 80648, "start": 812.48, "end": 816.16, "text": " up those 80 million numbers so again we're kind of in that situation where we're doing", "tokens": [493, 729, 4688, 2459, 3547, 370, 797, 321, 434, 733, 295, 294, 300, 2590, 689, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 195, "seek": 80648, "start": 816.16, "end": 819.4, "text": " something that we never really set out to do like we wanted to do something else but", "tokens": [746, 300, 321, 1128, 534, 992, 484, 281, 360, 411, 321, 1415, 281, 360, 746, 1646, 457], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 196, "seek": 80648, "start": 819.4, "end": 825.04, "text": " we're spending our time on yeah doing something that we didn't want to do so where does that", "tokens": [321, 434, 6434, 527, 565, 322, 1338, 884, 746, 300, 321, 994, 380, 528, 281, 360, 370, 689, 775, 300], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 197, "seek": 80648, "start": 825.04, "end": 830.72, "text": " come from and the first problem is basically that what we did what we set out to do was", "tokens": [808, 490, 293, 264, 700, 1154, 307, 1936, 300, 437, 321, 630, 437, 321, 992, 484, 281, 360, 390], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 198, "seek": 80648, "start": 830.72, "end": 834.88, "text": " fought from the get go because we said we want to decode so we're basically thinking", "tokens": [11391, 490, 264, 483, 352, 570, 321, 848, 321, 528, 281, 979, 1429, 370, 321, 434, 1936, 1953], "temperature": 0.0, "avg_logprob": -0.11160084458648181, "compression_ratio": 2.015267175572519, "no_speech_prob": 2.4655288143549114e-05}, {"id": 199, "seek": 83488, "start": 834.88, "end": 839.32, "text": " in the same way that we're thinking as we were with Jason we said that we want to decode", "tokens": [294, 264, 912, 636, 300, 321, 434, 1953, 382, 321, 645, 365, 11181, 321, 848, 300, 321, 528, 281, 979, 1429], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 200, "seek": 83488, "start": 839.32, "end": 844.16, "text": " this entire thing into this go data structure but that means that you see we need to allocate", "tokens": [341, 2302, 551, 666, 341, 352, 1412, 3877, 457, 300, 1355, 300, 291, 536, 321, 643, 281, 35713], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 201, "seek": 83488, "start": 844.16, "end": 848.08, "text": " this massive slice again and that also means that we need to in each inner slice we also", "tokens": [341, 5994, 13153, 797, 293, 300, 611, 1355, 300, 321, 643, 281, 294, 1184, 7284, 13153, 321, 611], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 202, "seek": 83488, "start": 848.08, "end": 851.88, "text": " need to allocate again so we're basically allocating and allocating over and over again", "tokens": [643, 281, 35713, 797, 370, 321, 434, 1936, 12660, 990, 293, 12660, 990, 670, 293, 670, 797], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 203, "seek": 83488, "start": 851.88, "end": 857.4, "text": " where our task is not to allocate our task was to sum up numbers so we can actually just", "tokens": [689, 527, 5633, 307, 406, 281, 35713, 527, 5633, 390, 281, 2408, 493, 3547, 370, 321, 393, 767, 445], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 204, "seek": 83488, "start": 857.4, "end": 862.4, "text": " simplify this a bit and we can basically just not decode it like while we're looping over", "tokens": [20460, 341, 257, 857, 293, 321, 393, 1936, 445, 406, 979, 1429, 309, 411, 1339, 321, 434, 6367, 278, 670], "temperature": 0.0, "avg_logprob": -0.10071819786011703, "compression_ratio": 2.053435114503817, "no_speech_prob": 1.1120209819637239e-05}, {"id": 205, "seek": 86240, "start": 862.4, "end": 866.4399999999999, "text": " that data anyway instead of storing it in an array we can just do with it what we plan", "tokens": [300, 1412, 4033, 2602, 295, 26085, 309, 294, 364, 10225, 321, 393, 445, 360, 365, 309, 437, 321, 1393], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 206, "seek": 86240, "start": 866.4399999999999, "end": 871.64, "text": " to do and in this case this would be summing up the data so basically getting rid of that", "tokens": [281, 360, 293, 294, 341, 1389, 341, 576, 312, 2408, 2810, 493, 264, 1412, 370, 1936, 1242, 3973, 295, 300], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 207, "seek": 86240, "start": 871.64, "end": 879.12, "text": " decoding step helps us to make this way faster so now we're at 46 milliseconds of course our", "tokens": [979, 8616, 1823, 3665, 505, 281, 652, 341, 636, 4663, 370, 586, 321, 434, 412, 17835, 34184, 295, 1164, 527], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 208, "seek": 86240, "start": 879.12, "end": 882.92, "text": " footprint of the data on disk hasn't changed because it's the same data that we're reading", "tokens": [24222, 295, 264, 1412, 322, 12355, 6132, 380, 3105, 570, 309, 311, 264, 912, 1412, 300, 321, 434, 3760], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 209, "seek": 86240, "start": 882.92, "end": 887.84, "text": " we're just reading it in a slightly more efficient way but yeah we don't have to allocate slices", "tokens": [321, 434, 445, 3760, 309, 294, 257, 4748, 544, 7148, 636, 457, 1338, 321, 500, 380, 362, 281, 35713, 19793], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 210, "seek": 86240, "start": 887.84, "end": 890.84, "text": " and also because we don't have these like nested slices we don't have like slices that", "tokens": [293, 611, 570, 321, 500, 380, 362, 613, 411, 15646, 292, 19793, 321, 500, 380, 362, 411, 19793, 300], "temperature": 0.0, "avg_logprob": -0.09735944616885586, "compression_ratio": 1.8823529411764706, "no_speech_prob": 5.253765266388655e-06}, {"id": 211, "seek": 89084, "start": 890.84, "end": 895.24, "text": " basically have pointers to other slices so we have better memory locality and now we're", "tokens": [1936, 362, 44548, 281, 661, 19793, 370, 321, 362, 1101, 4675, 1628, 1860, 293, 586, 321, 434], "temperature": 0.0, "avg_logprob": -0.12477095230765965, "compression_ratio": 1.7109375, "no_speech_prob": 7.180391548899934e-06}, {"id": 212, "seek": 89084, "start": 895.24, "end": 901.1600000000001, "text": " at 46 milliseconds and that is that is cool so 46 milliseconds is basically the time frame", "tokens": [412, 17835, 34184, 293, 300, 307, 300, 307, 1627, 370, 17835, 34184, 307, 1936, 264, 565, 3920], "temperature": 0.0, "avg_logprob": -0.12477095230765965, "compression_ratio": 1.7109375, "no_speech_prob": 7.180391548899934e-06}, {"id": 213, "seek": 89084, "start": 901.1600000000001, "end": 908.0400000000001, "text": " that can be acceptable for a database okay so quickly in recap we immediately ruled out", "tokens": [300, 393, 312, 15513, 337, 257, 8149, 1392, 370, 2661, 294, 20928, 321, 4258, 20077, 484], "temperature": 0.0, "avg_logprob": -0.12477095230765965, "compression_ratio": 1.7109375, "no_speech_prob": 7.180391548899934e-06}, {"id": 214, "seek": 89084, "start": 908.0400000000001, "end": 911.48, "text": " JSON because it just wasn't space efficient and we knew that we needed something more", "tokens": [31828, 570, 309, 445, 2067, 380, 1901, 7148, 293, 321, 2586, 300, 321, 2978, 746, 544], "temperature": 0.0, "avg_logprob": -0.12477095230765965, "compression_ratio": 1.7109375, "no_speech_prob": 7.180391548899934e-06}, {"id": 215, "seek": 89084, "start": 911.48, "end": 916.72, "text": " space efficient and also way faster binary encoding already made it much faster which", "tokens": [1901, 7148, 293, 611, 636, 4663, 17434, 43430, 1217, 1027, 309, 709, 4663, 597], "temperature": 0.0, "avg_logprob": -0.12477095230765965, "compression_ratio": 1.7109375, "no_speech_prob": 7.180391548899934e-06}, {"id": 216, "seek": 91672, "start": 916.72, "end": 922.32, "text": " is great but if we decode it upfront then yeah we still lost a lot of time and it can", "tokens": [307, 869, 457, 498, 321, 979, 1429, 309, 30264, 550, 1338, 321, 920, 2731, 257, 688, 295, 565, 293, 309, 393], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 217, "seek": 91672, "start": 922.32, "end": 926.4, "text": " be worth it in these kind of high-performance situations if you either sort of delay the", "tokens": [312, 3163, 309, 294, 613, 733, 295, 1090, 12, 50242, 6851, 498, 291, 2139, 1333, 295, 8577, 264], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 218, "seek": 91672, "start": 926.4, "end": 930.88, "text": " decoding as late as possible until you really need it or just don't do it at all or do it", "tokens": [979, 8616, 382, 3469, 382, 1944, 1826, 291, 534, 643, 309, 420, 445, 500, 380, 360, 309, 412, 439, 420, 360, 309], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 219, "seek": 91672, "start": 930.88, "end": 935.0400000000001, "text": " in sort of small parts where we need it no wish list here but an honor we mentioned", "tokens": [294, 1333, 295, 1359, 3166, 689, 321, 643, 309, 572, 3172, 1329, 510, 457, 364, 5968, 321, 2835], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 220, "seek": 91672, "start": 935.0400000000001, "end": 940.52, "text": " so go 1.20 they've actually removed it from the from the release notes because it's so", "tokens": [370, 352, 502, 13, 2009, 436, 600, 767, 7261, 309, 490, 264, 490, 264, 4374, 5570, 570, 309, 311, 370], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 221, "seek": 91672, "start": 940.52, "end": 945.84, "text": " experimental but go 1.20 has support for memory arenas the idea for memory arenas is basically", "tokens": [17069, 457, 352, 502, 13, 2009, 575, 1406, 337, 4675, 3212, 296, 264, 1558, 337, 4675, 3212, 296, 307, 1936], "temperature": 0.0, "avg_logprob": -0.12920719519593662, "compression_ratio": 1.778523489932886, "no_speech_prob": 8.138421435432974e-06}, {"id": 222, "seek": 94584, "start": 945.84, "end": 950.48, "text": " that you can bypass the garbage collector and sort of manually free that data so if", "tokens": [300, 291, 393, 24996, 264, 14150, 23960, 293, 1333, 295, 16945, 1737, 300, 1412, 370, 498], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 223, "seek": 94584, "start": 950.48, "end": 954.4, "text": " you have something that you know has the same sort of life cycle then you can say okay put", "tokens": [291, 362, 746, 300, 291, 458, 575, 264, 912, 1333, 295, 993, 6586, 550, 291, 393, 584, 1392, 829], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 224, "seek": 94584, "start": 954.4, "end": 959.2800000000001, "text": " it in the arena and basically in the end free the entire arena which would sort of bypass", "tokens": [309, 294, 264, 18451, 293, 1936, 294, 264, 917, 1737, 264, 2302, 18451, 597, 576, 1333, 295, 24996], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 225, "seek": 94584, "start": 959.2800000000001, "end": 963.84, "text": " the garbage collector so that could also be a solution in this case if that ever makes", "tokens": [264, 14150, 23960, 370, 300, 727, 611, 312, 257, 3827, 294, 341, 1389, 498, 300, 1562, 1669], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 226, "seek": 94584, "start": 963.84, "end": 967.36, "text": " it like right now it's super experimental and they basically tell you we might just", "tokens": [309, 411, 558, 586, 309, 311, 1687, 17069, 293, 436, 1936, 980, 291, 321, 1062, 445], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 227, "seek": 94584, "start": 967.36, "end": 974.32, "text": " remove it so don't use it third stop is something that when I first heard it almost sounded", "tokens": [4159, 309, 370, 500, 380, 764, 309, 2636, 1590, 307, 746, 300, 562, 286, 700, 2198, 309, 1920, 17714], "temperature": 0.0, "avg_logprob": -0.08479085489481437, "compression_ratio": 1.9025270758122743, "no_speech_prob": 4.783995791513007e-06}, {"id": 228, "seek": 97432, "start": 974.32, "end": 979.12, "text": " like too good to be true so something called SIMD we'll get to what that is in a second", "tokens": [411, 886, 665, 281, 312, 2074, 370, 746, 1219, 24738, 35, 321, 603, 483, 281, 437, 300, 307, 294, 257, 1150], "temperature": 0.0, "avg_logprob": -0.12618775720949527, "compression_ratio": 1.5985401459854014, "no_speech_prob": 1.4280076356953941e-05}, {"id": 229, "seek": 97432, "start": 979.12, "end": 984.5600000000001, "text": " but first question to the audience who here remembers this thing raise your hands okay", "tokens": [457, 700, 1168, 281, 264, 4034, 567, 510, 26228, 341, 551, 5300, 428, 2377, 1392], "temperature": 0.0, "avg_logprob": -0.12618775720949527, "compression_ratio": 1.5985401459854014, "no_speech_prob": 1.4280076356953941e-05}, {"id": 230, "seek": 97432, "start": 984.5600000000001, "end": 990.96, "text": " cool so you're just as old as I am so this is the Intel Pentium 2 processor and this", "tokens": [1627, 370, 291, 434, 445, 382, 1331, 382, 286, 669, 370, 341, 307, 264, 19762, 20165, 2197, 568, 15321, 293, 341], "temperature": 0.0, "avg_logprob": -0.12618775720949527, "compression_ratio": 1.5985401459854014, "no_speech_prob": 1.4280076356953941e-05}, {"id": 231, "seek": 97432, "start": 990.96, "end": 997.36, "text": " came out in late 90s I think 1997 and was sold for a couple of couple of years and back then", "tokens": [1361, 484, 294, 3469, 4289, 82, 286, 519, 22383, 293, 390, 3718, 337, 257, 1916, 295, 1916, 295, 924, 293, 646, 550], "temperature": 0.0, "avg_logprob": -0.12618775720949527, "compression_ratio": 1.5985401459854014, "no_speech_prob": 1.4280076356953941e-05}, {"id": 232, "seek": 97432, "start": 997.36, "end": 1001.84, "text": " I did not build databases definitely not in go because that also didn't exist yet but", "tokens": [286, 630, 406, 1322, 22380, 2138, 406, 294, 352, 570, 300, 611, 994, 380, 2514, 1939, 457], "temperature": 0.0, "avg_logprob": -0.12618775720949527, "compression_ratio": 1.5985401459854014, "no_speech_prob": 1.4280076356953941e-05}, {"id": 233, "seek": 100184, "start": 1001.84, "end": 1006.32, "text": " what I would do was sort of try to play 3d video games and I would urge my parents to", "tokens": [437, 286, 576, 360, 390, 1333, 295, 853, 281, 862, 805, 67, 960, 2813, 293, 286, 576, 19029, 452, 3152, 281], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 234, "seek": 100184, "start": 1006.32, "end": 1010.32, "text": " get one of those new computers with an Intel Pentium 2 processor and one of the arguments", "tokens": [483, 472, 295, 729, 777, 10807, 365, 364, 19762, 20165, 2197, 568, 15321, 293, 472, 295, 264, 12869], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 235, "seek": 100184, "start": 1010.32, "end": 1015.36, "text": " that I could have used in that discussion was hey it comes with MMX technology and of", "tokens": [300, 286, 727, 362, 1143, 294, 300, 5017, 390, 4177, 309, 1487, 365, 34191, 55, 2899, 293, 295], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 236, "seek": 100184, "start": 1015.36, "end": 1020.32, "text": " course I had no idea what that is and it probably took me 10 or so more more years to find out", "tokens": [1164, 286, 632, 572, 1558, 437, 300, 307, 293, 309, 1391, 1890, 385, 1266, 420, 370, 544, 544, 924, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 237, "seek": 100184, "start": 1020.32, "end": 1024.88, "text": " what MMX is but it's the first in a long list of SIMD instructions I haven't explained what", "tokens": [437, 34191, 55, 307, 457, 309, 311, 264, 700, 294, 257, 938, 1329, 295, 24738, 35, 9415, 286, 2378, 380, 8825, 437], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 238, "seek": 100184, "start": 1024.88, "end": 1029.92, "text": " SIMD is yet but I will in a second some of those especially the one in the in the top", "tokens": [24738, 35, 307, 1939, 457, 286, 486, 294, 257, 1150, 512, 295, 729, 2318, 264, 472, 294, 264, 294, 264, 1192], "temperature": 0.0, "avg_logprob": -0.06779483486624326, "compression_ratio": 1.7450980392156863, "no_speech_prob": 1.6681358829373494e-05}, {"id": 239, "seek": 102992, "start": 1029.92, "end": 1035.44, "text": " line they're not really used anymore these days but the the bottom line like AVX2 and AVX512", "tokens": [1622, 436, 434, 406, 534, 1143, 3602, 613, 1708, 457, 264, 264, 2767, 1622, 411, 30198, 55, 17, 293, 30198, 55, 20, 4762], "temperature": 0.0, "avg_logprob": -0.10404377970202215, "compression_ratio": 1.6505190311418685, "no_speech_prob": 2.3548100216430612e-05}, {"id": 240, "seek": 102992, "start": 1035.44, "end": 1040.0800000000002, "text": " you may have heard them in in fact for for many open source project they sometimes just sort of", "tokens": [291, 815, 362, 2198, 552, 294, 294, 1186, 337, 337, 867, 1269, 4009, 1716, 436, 2171, 445, 1333, 295], "temperature": 0.0, "avg_logprob": -0.10404377970202215, "compression_ratio": 1.6505190311418685, "no_speech_prob": 2.3548100216430612e-05}, {"id": 241, "seek": 102992, "start": 1040.0800000000002, "end": 1045.8400000000001, "text": " slap that label in the read me like yeah yeah has AVX2 optimizations and that kind of signals you", "tokens": [21075, 300, 7645, 294, 264, 1401, 385, 411, 1338, 1338, 575, 30198, 55, 17, 5028, 14455, 293, 300, 733, 295, 12354, 291], "temperature": 0.0, "avg_logprob": -0.10404377970202215, "compression_ratio": 1.6505190311418685, "no_speech_prob": 2.3548100216430612e-05}, {"id": 242, "seek": 102992, "start": 1045.8400000000001, "end": 1050.0, "text": " yeah we care about speed because it's like low level optimized and VVA does the exact same thing", "tokens": [1338, 321, 1127, 466, 3073, 570, 309, 311, 411, 2295, 1496, 26941, 293, 691, 20914, 775, 264, 1900, 912, 551], "temperature": 0.0, "avg_logprob": -0.10404377970202215, "compression_ratio": 1.6505190311418685, "no_speech_prob": 2.3548100216430612e-05}, {"id": 243, "seek": 102992, "start": 1050.0, "end": 1056.48, "text": " by the way so to understand how we could make use of that I quickly need to talk about vector", "tokens": [538, 264, 636, 370, 281, 1223, 577, 321, 727, 652, 764, 295, 300, 286, 2661, 643, 281, 751, 466, 8062], "temperature": 0.0, "avg_logprob": -0.10404377970202215, "compression_ratio": 1.6505190311418685, "no_speech_prob": 2.3548100216430612e-05}, {"id": 244, "seek": 105648, "start": 1056.48, "end": 1062.16, "text": " embeddings because I said before that VVA doesn't doesn't search through data by keywords but rather", "tokens": [12240, 29432, 570, 286, 848, 949, 300, 691, 20914, 1177, 380, 1177, 380, 3164, 807, 1412, 538, 21009, 457, 2831], "temperature": 0.0, "avg_logprob": -0.05032964988991066, "compression_ratio": 1.8631178707224334, "no_speech_prob": 7.5252382885082625e-06}, {"id": 245, "seek": 105648, "start": 1062.16, "end": 1068.0, "text": " through its meaning and it uses vector embeddings as a tool for that so this is basically just a", "tokens": [807, 1080, 3620, 293, 309, 4960, 8062, 12240, 29432, 382, 257, 2290, 337, 300, 370, 341, 307, 1936, 445, 257], "temperature": 0.0, "avg_logprob": -0.05032964988991066, "compression_ratio": 1.8631178707224334, "no_speech_prob": 7.5252382885082625e-06}, {"id": 246, "seek": 105648, "start": 1068.0, "end": 1072.72, "text": " long list of numbers in this case floats and then a machine learning model comes in and basically it", "tokens": [938, 1329, 295, 3547, 294, 341, 1389, 37878, 293, 550, 257, 3479, 2539, 2316, 1487, 294, 293, 1936, 309], "temperature": 0.0, "avg_logprob": -0.05032964988991066, "compression_ratio": 1.8631178707224334, "no_speech_prob": 7.5252382885082625e-06}, {"id": 247, "seek": 105648, "start": 1072.72, "end": 1077.2, "text": " says do something with my input and then you get this vector out and if you do this on all the", "tokens": [1619, 360, 746, 365, 452, 4846, 293, 550, 291, 483, 341, 8062, 484, 293, 498, 291, 360, 341, 322, 439, 264], "temperature": 0.0, "avg_logprob": -0.05032964988991066, "compression_ratio": 1.8631178707224334, "no_speech_prob": 7.5252382885082625e-06}, {"id": 248, "seek": 105648, "start": 1077.2, "end": 1081.92, "text": " objects then you can compare your vectors so you basically can do a vector similarity comparison", "tokens": [6565, 550, 291, 393, 6794, 428, 18875, 370, 291, 1936, 393, 360, 257, 8062, 32194, 9660], "temperature": 0.0, "avg_logprob": -0.05032964988991066, "compression_ratio": 1.8631178707224334, "no_speech_prob": 7.5252382885082625e-06}, {"id": 249, "seek": 108192, "start": 1081.92, "end": 1086.48, "text": " and that tells you if something is close to to one another or not so for example the query and the", "tokens": [293, 300, 5112, 291, 498, 746, 307, 1998, 281, 281, 472, 1071, 420, 406, 370, 337, 1365, 264, 14581, 293, 264], "temperature": 0.0, "avg_logprob": -0.06216028103461632, "compression_ratio": 1.9015748031496063, "no_speech_prob": 4.709809672931442e-06}, {"id": 250, "seek": 108192, "start": 1086.48, "end": 1093.8400000000001, "text": " the object that we had before so without any simd we can use something called the dot product the", "tokens": [264, 2657, 300, 321, 632, 949, 370, 1553, 604, 1034, 67, 321, 393, 764, 746, 1219, 264, 5893, 1674, 264], "temperature": 0.0, "avg_logprob": -0.06216028103461632, "compression_ratio": 1.9015748031496063, "no_speech_prob": 4.709809672931442e-06}, {"id": 251, "seek": 108192, "start": 1093.8400000000001, "end": 1099.68, "text": " dot product is a simple calculation where basically you use you multiply each element of the first", "tokens": [5893, 1674, 307, 257, 2199, 17108, 689, 1936, 291, 764, 291, 12972, 1184, 4478, 295, 264, 700], "temperature": 0.0, "avg_logprob": -0.06216028103461632, "compression_ratio": 1.9015748031496063, "no_speech_prob": 4.709809672931442e-06}, {"id": 252, "seek": 108192, "start": 1099.68, "end": 1104.3200000000002, "text": " vector with the same corresponding element of the second vector and then you just sum up all of", "tokens": [8062, 365, 264, 912, 11760, 4478, 295, 264, 1150, 8062, 293, 550, 291, 445, 2408, 493, 439, 295], "temperature": 0.0, "avg_logprob": -0.06216028103461632, "compression_ratio": 1.9015748031496063, "no_speech_prob": 4.709809672931442e-06}, {"id": 253, "seek": 108192, "start": 1104.3200000000002, "end": 1110.24, "text": " those elements and we can think of this like multiplication and summing as two instructions", "tokens": [729, 4959, 293, 321, 393, 519, 295, 341, 411, 27290, 293, 2408, 2810, 382, 732, 9415], "temperature": 0.0, "avg_logprob": -0.06216028103461632, "compression_ratio": 1.9015748031496063, "no_speech_prob": 4.709809672931442e-06}, {"id": 254, "seek": 111024, "start": 1110.24, "end": 1114.96, "text": " so if we look out first shout out here to the compiler explorer which is a super cool tool to", "tokens": [370, 498, 321, 574, 484, 700, 8043, 484, 510, 281, 264, 31958, 39680, 597, 307, 257, 1687, 1627, 2290, 281], "temperature": 0.0, "avg_logprob": -0.050904805200141776, "compression_ratio": 1.788104089219331, "no_speech_prob": 5.421599780675024e-06}, {"id": 255, "seek": 111024, "start": 1114.96, "end": 1120.4, "text": " see like what your go code compiles to we can see that this indeed turns into two instructions so", "tokens": [536, 411, 437, 428, 352, 3089, 715, 4680, 281, 321, 393, 536, 300, 341, 6451, 4523, 666, 732, 9415, 370], "temperature": 0.0, "avg_logprob": -0.050904805200141776, "compression_ratio": 1.788104089219331, "no_speech_prob": 5.421599780675024e-06}, {"id": 256, "seek": 111024, "start": 1120.4, "end": 1124.48, "text": " this is a bit of a lie because there's more stuff going on because it's in a loop etc but let's just", "tokens": [341, 307, 257, 857, 295, 257, 4544, 570, 456, 311, 544, 1507, 516, 322, 570, 309, 311, 294, 257, 6367, 5183, 457, 718, 311, 445], "temperature": 0.0, "avg_logprob": -0.050904805200141776, "compression_ratio": 1.788104089219331, "no_speech_prob": 5.421599780675024e-06}, {"id": 257, "seek": 111024, "start": 1124.48, "end": 1132.0, "text": " pretend that indeed we have these two instructions to multiply it and to add it so how could we", "tokens": [11865, 300, 6451, 321, 362, 613, 732, 9415, 281, 12972, 309, 293, 281, 909, 309, 370, 577, 727, 321], "temperature": 0.0, "avg_logprob": -0.050904805200141776, "compression_ratio": 1.788104089219331, "no_speech_prob": 5.421599780675024e-06}, {"id": 258, "seek": 111024, "start": 1132.0, "end": 1137.1200000000001, "text": " possibly optimize this even further if we're already at such a low level well we can because", "tokens": [6264, 19719, 341, 754, 3052, 498, 321, 434, 1217, 412, 1270, 257, 2295, 1496, 731, 321, 393, 570], "temperature": 0.0, "avg_logprob": -0.050904805200141776, "compression_ratio": 1.788104089219331, "no_speech_prob": 5.421599780675024e-06}, {"id": 259, "seek": 113712, "start": 1137.12, "end": 1143.12, "text": " this is our mad journey so all we have to do is introduce some madness and what we're doing now", "tokens": [341, 307, 527, 5244, 4671, 370, 439, 321, 362, 281, 360, 307, 5366, 512, 28736, 293, 437, 321, 434, 884, 586], "temperature": 0.0, "avg_logprob": -0.047379430416411, "compression_ratio": 1.8795180722891567, "no_speech_prob": 9.2224345280556e-06}, {"id": 260, "seek": 113712, "start": 1143.76, "end": 1150.08, "text": " is a practice that's called unrolling so the idea here is that instead of looping over one", "tokens": [307, 257, 3124, 300, 311, 1219, 517, 18688, 370, 264, 1558, 510, 307, 300, 2602, 295, 6367, 278, 670, 472], "temperature": 0.0, "avg_logprob": -0.047379430416411, "compression_ratio": 1.8795180722891567, "no_speech_prob": 9.2224345280556e-06}, {"id": 261, "seek": 113712, "start": 1150.08, "end": 1154.9599999999998, "text": " element at a time we're now looping over eight elements at a time but we've got we've gained", "tokens": [4478, 412, 257, 565, 321, 434, 586, 6367, 278, 670, 3180, 4959, 412, 257, 565, 457, 321, 600, 658, 321, 600, 12634], "temperature": 0.0, "avg_logprob": -0.047379430416411, "compression_ratio": 1.8795180722891567, "no_speech_prob": 9.2224345280556e-06}, {"id": 262, "seek": 113712, "start": 1154.9599999999998, "end": 1159.04, "text": " nothing like this is we're still doing the same kind of work like we're doing 16 instructions now", "tokens": [1825, 411, 341, 307, 321, 434, 920, 884, 264, 912, 733, 295, 589, 411, 321, 434, 884, 3165, 9415, 586], "temperature": 0.0, "avg_logprob": -0.047379430416411, "compression_ratio": 1.8795180722891567, "no_speech_prob": 9.2224345280556e-06}, {"id": 263, "seek": 113712, "start": 1159.04, "end": 1163.6, "text": " in a single loop and we're just doing fewer iterations so by this point nothing gained but", "tokens": [294, 257, 2167, 6367, 293, 321, 434, 445, 884, 13366, 36540, 370, 538, 341, 935, 1825, 12634, 457], "temperature": 0.0, "avg_logprob": -0.047379430416411, "compression_ratio": 1.8795180722891567, "no_speech_prob": 9.2224345280556e-06}, {"id": 264, "seek": 116360, "start": 1163.6, "end": 1169.6, "text": " why would we do that well here comes the part where I thought it was too good to be true what if", "tokens": [983, 576, 321, 360, 300, 731, 510, 1487, 264, 644, 689, 286, 1194, 309, 390, 886, 665, 281, 312, 2074, 437, 498], "temperature": 0.0, "avg_logprob": -0.08518248406526084, "compression_ratio": 1.866412213740458, "no_speech_prob": 9.364543984702323e-06}, {"id": 265, "seek": 116360, "start": 1169.6, "end": 1176.9599999999998, "text": " we could do those 16 operations for the cost of just two instructions sounds crazy right well no", "tokens": [321, 727, 360, 729, 3165, 7705, 337, 264, 2063, 295, 445, 732, 9415, 3263, 3219, 558, 731, 572], "temperature": 0.0, "avg_logprob": -0.08518248406526084, "compression_ratio": 1.866412213740458, "no_speech_prob": 9.364543984702323e-06}, {"id": 266, "seek": 116360, "start": 1176.9599999999998, "end": 1182.1599999999999, "text": " because simd I'm finally revealing what the acronym stands for it stands for single instruction", "tokens": [570, 1034, 67, 286, 478, 2721, 23983, 437, 264, 39195, 7382, 337, 309, 7382, 337, 2167, 10951], "temperature": 0.0, "avg_logprob": -0.08518248406526084, "compression_ratio": 1.866412213740458, "no_speech_prob": 9.364543984702323e-06}, {"id": 267, "seek": 116360, "start": 1182.1599999999999, "end": 1187.52, "text": " multiple data and that is exactly what we're doing here so we want to do the same thing over and over", "tokens": [3866, 1412, 293, 300, 307, 2293, 437, 321, 434, 884, 510, 370, 321, 528, 281, 360, 264, 912, 551, 670, 293, 670], "temperature": 0.0, "avg_logprob": -0.08518248406526084, "compression_ratio": 1.866412213740458, "no_speech_prob": 9.364543984702323e-06}, {"id": 268, "seek": 116360, "start": 1187.52, "end": 1193.4399999999998, "text": " again which is multiplication and then additions and this is exactly what these simd instructions", "tokens": [797, 597, 307, 27290, 293, 550, 35113, 293, 341, 307, 2293, 437, 613, 1034, 67, 9415], "temperature": 0.0, "avg_logprob": -0.08518248406526084, "compression_ratio": 1.866412213740458, "no_speech_prob": 9.364543984702323e-06}, {"id": 269, "seek": 119344, "start": 1193.44, "end": 1198.0800000000002, "text": " provide so in this case we can multiply eight floats with other eight floats and then we can", "tokens": [2893, 370, 294, 341, 1389, 321, 393, 12972, 3180, 37878, 365, 661, 3180, 37878, 293, 550, 321, 393], "temperature": 0.0, "avg_logprob": -0.08100029555234042, "compression_ratio": 1.737327188940092, "no_speech_prob": 1.0288588782714214e-05}, {"id": 270, "seek": 119344, "start": 1198.0800000000002, "end": 1206.48, "text": " add them up so all this perfect here maybe not because there's a catch of course it's our mad", "tokens": [909, 552, 493, 370, 439, 341, 2176, 510, 1310, 406, 570, 456, 311, 257, 3745, 295, 1164, 309, 311, 527, 5244], "temperature": 0.0, "avg_logprob": -0.08100029555234042, "compression_ratio": 1.737327188940092, "no_speech_prob": 1.0288588782714214e-05}, {"id": 271, "seek": 119344, "start": 1206.48, "end": 1214.3200000000002, "text": " journey how do you tell go to use these avx two instructions you don't you write assembly code", "tokens": [4671, 577, 360, 291, 980, 352, 281, 764, 613, 1305, 87, 732, 9415, 291, 500, 380, 291, 2464, 12103, 3089], "temperature": 0.0, "avg_logprob": -0.08100029555234042, "compression_ratio": 1.737327188940092, "no_speech_prob": 1.0288588782714214e-05}, {"id": 272, "seek": 119344, "start": 1214.3200000000002, "end": 1219.28, "text": " because go has no way to do that directly the good part is that assembly code integrates really", "tokens": [570, 352, 575, 572, 636, 281, 360, 300, 3838, 264, 665, 644, 307, 300, 12103, 3089, 3572, 1024, 534], "temperature": 0.0, "avg_logprob": -0.08100029555234042, "compression_ratio": 1.737327188940092, "no_speech_prob": 1.0288588782714214e-05}, {"id": 273, "seek": 121928, "start": 1219.28, "end": 1224.3999999999999, "text": " nicely into go and in the in the standard library it's used over and over again so it's kind of a", "tokens": [9594, 666, 352, 293, 294, 264, 294, 264, 3832, 6405, 309, 311, 1143, 670, 293, 670, 797, 370, 309, 311, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.07444308737049932, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.4209158406592906e-06}, {"id": 274, "seek": 121928, "start": 1224.3999999999999, "end": 1229.92, "text": " standard practice and there is tooling here so shout out to avo really cool too that helps you", "tokens": [3832, 3124, 293, 456, 307, 46593, 510, 370, 8043, 484, 281, 1305, 78, 534, 1627, 886, 300, 3665, 291], "temperature": 0.0, "avg_logprob": -0.07444308737049932, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.4209158406592906e-06}, {"id": 275, "seek": 121928, "start": 1230.8, "end": 1235.36, "text": " basically you're you're still writing assembly with with avo but you're writing it in go and then", "tokens": [1936, 291, 434, 291, 434, 920, 3579, 12103, 365, 365, 1305, 78, 457, 291, 434, 3579, 309, 294, 352, 293, 550], "temperature": 0.0, "avg_logprob": -0.07444308737049932, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.4209158406592906e-06}, {"id": 276, "seek": 121928, "start": 1235.36, "end": 1239.12, "text": " it generates the assembly so you still need to know what you're doing but it's like it it", "tokens": [309, 23815, 264, 12103, 370, 291, 920, 643, 281, 458, 437, 291, 434, 884, 457, 309, 311, 411, 309, 309], "temperature": 0.0, "avg_logprob": -0.07444308737049932, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.4209158406592906e-06}, {"id": 277, "seek": 121928, "start": 1239.12, "end": 1247.52, "text": " protects you a bit so it definitely helped us a lot so simd recap using avx instructions or", "tokens": [22583, 291, 257, 857, 370, 309, 2138, 4254, 505, 257, 688, 370, 1034, 67, 20928, 1228, 1305, 87, 9415, 420], "temperature": 0.0, "avg_logprob": -0.07444308737049932, "compression_ratio": 1.8804780876494025, "no_speech_prob": 5.4209158406592906e-06}, {"id": 278, "seek": 124752, "start": 1247.52, "end": 1253.2, "text": " other simd instructions you can basically trick your cpu into doing more work for free but you", "tokens": [661, 1034, 67, 9415, 291, 393, 1936, 4282, 428, 269, 34859, 666, 884, 544, 589, 337, 1737, 457, 291], "temperature": 0.0, "avg_logprob": -0.08487490061167124, "compression_ratio": 1.8150943396226416, "no_speech_prob": 1.7225678675458767e-05}, {"id": 279, "seek": 124752, "start": 1253.2, "end": 1259.2, "text": " need to sort of also trick go to use assembly and with this tooling such as avo it can be better", "tokens": [643, 281, 1333, 295, 611, 4282, 352, 281, 764, 12103, 293, 365, 341, 46593, 1270, 382, 1305, 78, 309, 393, 312, 1101], "temperature": 0.0, "avg_logprob": -0.08487490061167124, "compression_ratio": 1.8150943396226416, "no_speech_prob": 1.7225678675458767e-05}, {"id": 280, "seek": 124752, "start": 1259.2, "end": 1263.84, "text": " but it would be even nicer if the language had some sort of support for it and you made my saying", "tokens": [457, 309, 576, 312, 754, 22842, 498, 264, 2856, 632, 512, 1333, 295, 1406, 337, 309, 293, 291, 1027, 452, 1566], "temperature": 0.0, "avg_logprob": -0.08487490061167124, "compression_ratio": 1.8150943396226416, "no_speech_prob": 1.7225678675458767e-05}, {"id": 281, "seek": 124752, "start": 1263.84, "end": 1267.68, "text": " now okay this is this mad guy on stage that wants to build a database but no one else does", "tokens": [586, 1392, 341, 307, 341, 5244, 2146, 322, 3233, 300, 2738, 281, 1322, 257, 8149, 457, 572, 472, 1646, 775], "temperature": 0.0, "avg_logprob": -0.08487490061167124, "compression_ratio": 1.8150943396226416, "no_speech_prob": 1.7225678675458767e-05}, {"id": 282, "seek": 124752, "start": 1267.68, "end": 1272.96, "text": " needs that but we have this issue here that was open recently and unfortunately also closed recently", "tokens": [2203, 300, 457, 321, 362, 341, 2734, 510, 300, 390, 1269, 3938, 293, 7015, 611, 5395, 3938], "temperature": 0.0, "avg_logprob": -0.08487490061167124, "compression_ratio": 1.8150943396226416, "no_speech_prob": 1.7225678675458767e-05}, {"id": 283, "seek": 127296, "start": 1272.96, "end": 1278.32, "text": " because no consensus could be reached but it comes up back and back basically that go users are", "tokens": [570, 572, 19115, 727, 312, 6488, 457, 309, 1487, 493, 646, 293, 646, 1936, 300, 352, 5022, 366], "temperature": 0.0, "avg_logprob": -0.06442759522294576, "compression_ratio": 1.825278810408922, "no_speech_prob": 7.295219347724924e-06}, {"id": 284, "seek": 127296, "start": 1278.32, "end": 1283.04, "text": " saying like hey we want something in the language such as intrinsic so intrinsics are basically the", "tokens": [1566, 411, 4177, 321, 528, 746, 294, 264, 2856, 1270, 382, 35698, 370, 28621, 1167, 366, 1936, 264], "temperature": 0.0, "avg_logprob": -0.06442759522294576, "compression_ratio": 1.825278810408922, "no_speech_prob": 7.295219347724924e-06}, {"id": 285, "seek": 127296, "start": 1283.04, "end": 1289.1200000000001, "text": " idea of having high level language instructions to do these these sort of avx or simd instructions", "tokens": [1558, 295, 1419, 1090, 1496, 2856, 9415, 281, 360, 613, 613, 1333, 295, 1305, 87, 420, 1034, 67, 9415], "temperature": 0.0, "avg_logprob": -0.06442759522294576, "compression_ratio": 1.825278810408922, "no_speech_prob": 7.295219347724924e-06}, {"id": 286, "seek": 127296, "start": 1289.1200000000001, "end": 1295.3600000000001, "text": " and c or c++ has that for example one way to do that and maybe you're wondering like okay if you", "tokens": [293, 269, 420, 269, 25472, 575, 300, 337, 1365, 472, 636, 281, 360, 300, 293, 1310, 291, 434, 6359, 411, 1392, 498, 291], "temperature": 0.0, "avg_logprob": -0.06442759522294576, "compression_ratio": 1.825278810408922, "no_speech_prob": 7.295219347724924e-06}, {"id": 287, "seek": 127296, "start": 1295.3600000000001, "end": 1300.64, "text": " have such a performance hot path like why don't you just write that in c and you see go or write it", "tokens": [362, 1270, 257, 3389, 2368, 3100, 411, 983, 500, 380, 291, 445, 2464, 300, 294, 269, 293, 291, 536, 352, 420, 2464, 309], "temperature": 0.0, "avg_logprob": -0.06442759522294576, "compression_ratio": 1.825278810408922, "no_speech_prob": 7.295219347724924e-06}, {"id": 288, "seek": 130064, "start": 1300.64, "end": 1306.16, "text": " in rust or something like that sounds good in theory but the problem is that the call overhead", "tokens": [294, 15259, 420, 746, 411, 300, 3263, 665, 294, 5261, 457, 264, 1154, 307, 300, 264, 818, 19922], "temperature": 0.0, "avg_logprob": -0.0571198988769014, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1910282182725496e-06}, {"id": 289, "seek": 130064, "start": 1306.16, "end": 1312.5600000000002, "text": " to call c or c++ is so high that you actually have to outsource quite a bit of your code for that to", "tokens": [281, 818, 269, 420, 269, 25472, 307, 370, 1090, 300, 291, 767, 362, 281, 14758, 2948, 1596, 257, 857, 295, 428, 3089, 337, 300, 281], "temperature": 0.0, "avg_logprob": -0.0571198988769014, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1910282182725496e-06}, {"id": 290, "seek": 130064, "start": 1312.5600000000002, "end": 1317.6000000000001, "text": " to pay off again so if you do that you basically end up writing more and more and more in that", "tokens": [281, 1689, 766, 797, 370, 498, 291, 360, 300, 291, 1936, 917, 493, 3579, 544, 293, 544, 293, 544, 294, 300], "temperature": 0.0, "avg_logprob": -0.0571198988769014, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1910282182725496e-06}, {"id": 291, "seek": 130064, "start": 1317.6000000000001, "end": 1322.16, "text": " language and then you're not writing go anymore so fortunately that's not or it can be in some", "tokens": [2856, 293, 550, 291, 434, 406, 3579, 352, 3602, 370, 25511, 300, 311, 406, 420, 309, 393, 312, 294, 512], "temperature": 0.0, "avg_logprob": -0.0571198988769014, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1910282182725496e-06}, {"id": 292, "seek": 130064, "start": 1322.16, "end": 1329.1200000000001, "text": " ways but it's not always a great idea so demo time um this was going to be a live demo and", "tokens": [2098, 457, 309, 311, 406, 1009, 257, 869, 1558, 370, 10723, 565, 1105, 341, 390, 516, 281, 312, 257, 1621, 10723, 293], "temperature": 0.0, "avg_logprob": -0.0571198988769014, "compression_ratio": 1.8098859315589353, "no_speech_prob": 1.1910282182725496e-06}, {"id": 293, "seek": 132912, "start": 1329.12, "end": 1334.32, "text": " maybe it still is because i prepared this running nicely in a docker container and then my docker", "tokens": [1310, 309, 920, 307, 570, 741, 4927, 341, 2614, 9594, 294, 257, 360, 9178, 10129, 293, 550, 452, 360, 9178], "temperature": 0.0, "avg_logprob": -0.04241052320448019, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2027682714688126e-05}, {"id": 294, "seek": 132912, "start": 1334.32, "end": 1339.28, "text": " network just broke everything and it didn't work but i just rebuilt it without docker and i think", "tokens": [3209, 445, 6902, 1203, 293, 309, 994, 380, 589, 457, 741, 445, 38532, 309, 1553, 360, 9178, 293, 741, 519], "temperature": 0.0, "avg_logprob": -0.04241052320448019, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2027682714688126e-05}, {"id": 295, "seek": 132912, "start": 1339.28, "end": 1344.8799999999999, "text": " it might work if not i have screenshots basically that um that do a backup so example query here", "tokens": [309, 1062, 589, 498, 406, 741, 362, 40661, 1936, 300, 1105, 300, 360, 257, 14807, 370, 1365, 14581, 510], "temperature": 0.0, "avg_logprob": -0.04241052320448019, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2027682714688126e-05}, {"id": 296, "seek": 132912, "start": 1344.8799999999999, "end": 1351.1999999999998, "text": " i'm a big wine nerd so what i did is i put wine reviews into vv8 and i want to search them now", "tokens": [741, 478, 257, 955, 7209, 23229, 370, 437, 741, 630, 307, 741, 829, 7209, 10229, 666, 371, 85, 23, 293, 741, 528, 281, 3164, 552, 586], "temperature": 0.0, "avg_logprob": -0.04241052320448019, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2027682714688126e-05}, {"id": 297, "seek": 132912, "start": 1351.1999999999998, "end": 1356.2399999999998, "text": " and one way to do it to show you basically that the keyword um that you don't need a keyword", "tokens": [293, 472, 636, 281, 360, 309, 281, 855, 291, 1936, 300, 264, 20428, 1105, 300, 291, 500, 380, 643, 257, 20428], "temperature": 0.0, "avg_logprob": -0.04241052320448019, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.2027682714688126e-05}, {"id": 298, "seek": 135624, "start": 1356.24, "end": 1363.36, "text": " match but can search by meaning is for example if i go for an affordable italian wine let's see if", "tokens": [2995, 457, 393, 3164, 538, 3620, 307, 337, 1365, 498, 741, 352, 337, 364, 12028, 22366, 952, 7209, 718, 311, 536, 498], "temperature": 0.0, "avg_logprob": -0.06418965127733019, "compression_ratio": 1.7511520737327189, "no_speech_prob": 1.5203737348201685e-05}, {"id": 299, "seek": 135624, "start": 1363.36, "end": 1371.1200000000001, "text": " the internet connection works it does so what we got back um is basically this this wine review", "tokens": [264, 4705, 4984, 1985, 309, 775, 370, 437, 321, 658, 646, 1105, 307, 1936, 341, 341, 7209, 3131], "temperature": 0.0, "avg_logprob": -0.06418965127733019, "compression_ratio": 1.7511520737327189, "no_speech_prob": 1.5203737348201685e-05}, {"id": 300, "seek": 135624, "start": 1371.1200000000001, "end": 1376.96, "text": " that i wrote about a barolo that i recently drank and you can see it doesn't say italy", "tokens": [300, 741, 4114, 466, 257, 2159, 7902, 300, 741, 3938, 21011, 293, 291, 393, 536, 309, 1177, 380, 584, 309, 5222], "temperature": 0.0, "avg_logprob": -0.06418965127733019, "compression_ratio": 1.7511520737327189, "no_speech_prob": 1.5203737348201685e-05}, {"id": 301, "seek": 135624, "start": 1376.96, "end": 1382.0, "text": " anywhere it doesn't say affordable what it says like without breaking the bank so this is a vector", "tokens": [4992, 309, 1177, 380, 584, 12028, 437, 309, 1619, 411, 1553, 7697, 264, 3765, 370, 341, 307, 257, 8062], "temperature": 0.0, "avg_logprob": -0.06418965127733019, "compression_ratio": 1.7511520737327189, "no_speech_prob": 1.5203737348201685e-05}, {"id": 302, "seek": 138200, "start": 1382.0, "end": 1387.76, "text": " search that basically happened in the in the background we can take this one step further by", "tokens": [3164, 300, 1936, 2011, 294, 264, 294, 264, 3678, 321, 393, 747, 341, 472, 1823, 3052, 538], "temperature": 0.0, "avg_logprob": -0.09220097804891653, "compression_ratio": 1.8288973384030418, "no_speech_prob": 2.143849451385904e-05}, {"id": 303, "seek": 138200, "start": 1387.76, "end": 1392.96, "text": " using the generative side so this is basically the the chat gpt part um we can now ask our", "tokens": [1228, 264, 1337, 1166, 1252, 370, 341, 307, 1936, 264, 264, 5081, 290, 662, 644, 1105, 321, 393, 586, 1029, 527], "temperature": 0.0, "avg_logprob": -0.09220097804891653, "compression_ratio": 1.8288973384030418, "no_speech_prob": 2.143849451385904e-05}, {"id": 304, "seek": 138200, "start": 1392.96, "end": 1398.4, "text": " database based on the review which is what i wrote when is this wine going to be ready to drink so", "tokens": [8149, 2361, 322, 264, 3131, 597, 307, 437, 741, 4114, 562, 307, 341, 7209, 516, 281, 312, 1919, 281, 2822, 370], "temperature": 0.0, "avg_logprob": -0.09220097804891653, "compression_ratio": 1.8288973384030418, "no_speech_prob": 2.143849451385904e-05}, {"id": 305, "seek": 138200, "start": 1398.4, "end": 1402.0, "text": " let's see you saw before that was the fail query when the internet didn't work now now it's actually", "tokens": [718, 311, 536, 291, 1866, 949, 300, 390, 264, 3061, 14581, 562, 264, 4705, 994, 380, 589, 586, 586, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.09220097804891653, "compression_ratio": 1.8288973384030418, "no_speech_prob": 2.143849451385904e-05}, {"id": 306, "seek": 138200, "start": 1402.0, "end": 1407.28, "text": " working so that's nice um and here in this case you can see that so this is using open ai but you", "tokens": [1364, 370, 300, 311, 1481, 1105, 293, 510, 294, 341, 1389, 291, 393, 536, 300, 370, 341, 307, 1228, 1269, 9783, 457, 291], "temperature": 0.0, "avg_logprob": -0.09220097804891653, "compression_ratio": 1.8288973384030418, "no_speech_prob": 2.143849451385904e-05}, {"id": 307, "seek": 140728, "start": 1407.28, "end": 1412.3999999999999, "text": " can plug in other tools can plug in open source versions of it um this is using open ai because", "tokens": [393, 5452, 294, 661, 3873, 393, 5452, 294, 1269, 4009, 9606, 295, 309, 1105, 341, 307, 1228, 1269, 9783, 570], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 308, "seek": 140728, "start": 1412.3999999999999, "end": 1416.08, "text": " that's nice to be hosted at a at a service i don't have to run the machine learning model on my", "tokens": [300, 311, 1481, 281, 312, 19204, 412, 257, 412, 257, 2643, 741, 500, 380, 362, 281, 1190, 264, 3479, 2539, 2316, 322, 452], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 309, "seek": 140728, "start": 1416.08, "end": 1420.3999999999999, "text": " laptop then you can see it tells you the wine is not ready to drink yet we will need at least five", "tokens": [10732, 550, 291, 393, 536, 309, 5112, 291, 264, 7209, 307, 406, 1919, 281, 2822, 1939, 321, 486, 643, 412, 1935, 1732], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 310, "seek": 140728, "start": 1420.3999999999999, "end": 1424.32, "text": " more years which is sort of a good summary of this and then you can see another wine is ready", "tokens": [544, 924, 597, 307, 1333, 295, 257, 665, 12691, 295, 341, 293, 550, 291, 393, 536, 1071, 7209, 307, 1919], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 311, "seek": 140728, "start": 1424.32, "end": 1429.52, "text": " to drink right now it's in the perfect drinking window so for the final demo let's combine those", "tokens": [281, 2822, 558, 586, 309, 311, 294, 264, 2176, 7583, 4910, 370, 337, 264, 2572, 10723, 718, 311, 10432, 729], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 312, "seek": 140728, "start": 1429.52, "end": 1436.16, "text": " two let's do a semantic search to identify something and then do an ai generation basically", "tokens": [732, 718, 311, 360, 257, 47982, 3164, 281, 5876, 746, 293, 550, 360, 364, 9783, 5125, 1936], "temperature": 0.0, "avg_logprob": -0.05454776216955746, "compression_ratio": 1.8664495114006514, "no_speech_prob": 1.259800683328649e-05}, {"id": 313, "seek": 143616, "start": 1436.16, "end": 1441.28, "text": " so in this case we're saying find me an aged classic riesling best best wine in the world", "tokens": [370, 294, 341, 1389, 321, 434, 1566, 915, 385, 364, 21213, 7230, 23932, 1688, 1151, 1151, 7209, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.07162681378816303, "compression_ratio": 1.8515625, "no_speech_prob": 7.410279522446217e-06}, {"id": 314, "seek": 143616, "start": 1441.28, "end": 1447.2, "text": " riesling um and based on the review would you consider this wine to be a fruit bomb so let's", "tokens": [23932, 1688, 1105, 293, 2361, 322, 264, 3131, 576, 291, 1949, 341, 7209, 281, 312, 257, 6773, 7851, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.07162681378816303, "compression_ratio": 1.8515625, "no_speech_prob": 7.410279522446217e-06}, {"id": 315, "seek": 143616, "start": 1447.2, "end": 1453.0400000000002, "text": " have sort of an opinion from the machine learning model in it and um here we got one of my favorite", "tokens": [362, 1333, 295, 364, 4800, 490, 264, 3479, 2539, 2316, 294, 309, 293, 1105, 510, 321, 658, 472, 295, 452, 2954], "temperature": 0.0, "avg_logprob": -0.07162681378816303, "compression_ratio": 1.8515625, "no_speech_prob": 7.410279522446217e-06}, {"id": 316, "seek": 143616, "start": 1453.0400000000002, "end": 1458.4, "text": " wines and the the model says no i would not consider this a fruit bomb while it does have", "tokens": [35970, 293, 264, 264, 2316, 1619, 572, 741, 576, 406, 1949, 341, 257, 6773, 7851, 1339, 309, 775, 362], "temperature": 0.0, "avg_logprob": -0.07162681378816303, "compression_ratio": 1.8515625, "no_speech_prob": 7.410279522446217e-06}, {"id": 317, "seek": 143616, "start": 1458.4, "end": 1462.72, "text": " some fruity notes it is balanced by the mineralogy and acidity which keeps it from being overly sweet", "tokens": [512, 431, 21757, 5570, 309, 307, 13902, 538, 264, 21630, 7794, 293, 8258, 507, 597, 5965, 309, 490, 885, 24324, 3844], "temperature": 0.0, "avg_logprob": -0.07162681378816303, "compression_ratio": 1.8515625, "no_speech_prob": 7.410279522446217e-06}, {"id": 318, "seek": 146272, "start": 1462.72, "end": 1467.28, "text": " or fruity which is um if you read the text like this is nowhere in there so this is kind of cool", "tokens": [420, 431, 21757, 597, 307, 1105, 498, 291, 1401, 264, 2487, 411, 341, 307, 11159, 294, 456, 370, 341, 307, 733, 295, 1627], "temperature": 0.0, "avg_logprob": -0.07004656330231697, "compression_ratio": 1.85546875, "no_speech_prob": 2.0456878701224923e-05}, {"id": 319, "seek": 146272, "start": 1467.28, "end": 1473.92, "text": " that the that the model was was able to do this okay so let's go back now it's the the demo time", "tokens": [300, 264, 300, 264, 2316, 390, 390, 1075, 281, 360, 341, 1392, 370, 718, 311, 352, 646, 586, 309, 311, 264, 264, 10723, 565], "temperature": 0.0, "avg_logprob": -0.07004656330231697, "compression_ratio": 1.85546875, "no_speech_prob": 2.0456878701224923e-05}, {"id": 320, "seek": 146272, "start": 1473.92, "end": 1478.72, "text": " by the way have a github repo with like this example so you can run it yourself and um and", "tokens": [538, 264, 636, 362, 257, 290, 355, 836, 49040, 365, 411, 341, 1365, 370, 291, 393, 1190, 309, 1803, 293, 1105, 293], "temperature": 0.0, "avg_logprob": -0.07004656330231697, "compression_ratio": 1.85546875, "no_speech_prob": 2.0456878701224923e-05}, {"id": 321, "seek": 146272, "start": 1479.3600000000001, "end": 1487.1200000000001, "text": " yeah try it out yourself so this was our mad journey and are we mad at go are we mad to do this", "tokens": [1338, 853, 309, 484, 1803, 370, 341, 390, 527, 5244, 4671, 293, 366, 321, 5244, 412, 352, 366, 321, 5244, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.07004656330231697, "compression_ratio": 1.85546875, "no_speech_prob": 2.0456878701224923e-05}, {"id": 322, "seek": 146272, "start": 1487.1200000000001, "end": 1491.68, "text": " well i would pretty much say no because yes there were a couple of parts where we have to give", "tokens": [731, 741, 576, 1238, 709, 584, 572, 570, 2086, 456, 645, 257, 1916, 295, 3166, 689, 321, 362, 281, 976], "temperature": 0.0, "avg_logprob": -0.07004656330231697, "compression_ratio": 1.85546875, "no_speech_prob": 2.0456878701224923e-05}, {"id": 323, "seek": 149168, "start": 1491.68, "end": 1498.96, "text": " a get really creative and had to do some some yeah rather unique stuff but that was also basically", "tokens": [257, 483, 534, 5880, 293, 632, 281, 360, 512, 512, 1338, 2831, 3845, 1507, 457, 300, 390, 611, 1936], "temperature": 0.0, "avg_logprob": -0.09050143767740125, "compression_ratio": 1.844106463878327, "no_speech_prob": 1.4965783520892728e-05}, {"id": 324, "seek": 149168, "start": 1498.96, "end": 1502.48, "text": " like the highlight reel of building a database and all the other parts like i didn't even show the", "tokens": [411, 264, 5078, 34973, 295, 2390, 257, 8149, 293, 439, 264, 661, 3166, 411, 741, 994, 380, 754, 855, 264], "temperature": 0.0, "avg_logprob": -0.09050143767740125, "compression_ratio": 1.844106463878327, "no_speech_prob": 1.4965783520892728e-05}, {"id": 325, "seek": 149168, "start": 1502.48, "end": 1508.0, "text": " parts that went great like concurrency handling and the powerful standard library and of course", "tokens": [3166, 300, 1437, 869, 411, 23702, 10457, 13175, 293, 264, 4005, 3832, 6405, 293, 295, 1164], "temperature": 0.0, "avg_logprob": -0.09050143767740125, "compression_ratio": 1.844106463878327, "no_speech_prob": 1.4965783520892728e-05}, {"id": 326, "seek": 149168, "start": 1508.0, "end": 1513.1200000000001, "text": " all of you basically representing the gopher community which is super helpful and yeah this", "tokens": [439, 295, 291, 1936, 13460, 264, 352, 79, 511, 1768, 597, 307, 1687, 4961, 293, 1338, 341], "temperature": 0.0, "avg_logprob": -0.09050143767740125, "compression_ratio": 1.844106463878327, "no_speech_prob": 1.4965783520892728e-05}, {"id": 327, "seek": 149168, "start": 1513.1200000000001, "end": 1518.88, "text": " was my way to basically give back to all of you so if you ever want to build a database or run into", "tokens": [390, 452, 636, 281, 1936, 976, 646, 281, 439, 295, 291, 370, 498, 291, 1562, 528, 281, 1322, 257, 8149, 420, 1190, 666], "temperature": 0.0, "avg_logprob": -0.09050143767740125, "compression_ratio": 1.844106463878327, "no_speech_prob": 1.4965783520892728e-05}, {"id": 328, "seek": 151888, "start": 1518.88, "end": 1523.68, "text": " other kind of high performance problems then maybe some of those", "tokens": [50364, 661, 733, 295, 1090, 3389, 2740, 550, 1310, 512, 295, 729, 50604], "temperature": 0.0, "avg_logprob": -0.29213098117283415, "compression_ratio": 1.032258064516129, "no_speech_prob": 4.6681314415764064e-05}], "language": "en"}