{"text": " Okay, welcome back. So while you all have been walking in, I've been quickly reading this book, Efficient Go, it reads very quickly, and now Bartek has made sure that my code is ten times quicker, so tell us everything about it. Thank you. Thank you very much, everybody. So welcome. I hope your travels went well. Mine were, like, canceled, flight canceled, change of route, so I had lots of adventures, but generally I'm super happy I made it, and we are at the FOSDEM. So in this talk, I would like to invite you to learn more about efficiency of our Go programs, and there are already two talks that I have been on who mentioned, you know, optimizations in its name, and, like, generally how to make software more efficient. I wonder where this, I don't know, it's not hype, but it's already three talks about one topic, why it's so popular, is it because everybody's saving me money, that might be a reason, but I'm super happy we are really uncovering this for Go, because Go alone might be fast, but that doesn't mean that we cannot, you know, doesn't need to care about, you know, making it better, and use these resources when we execute it, right? So let's learn about that, and turns out that, you know, you can save literally millions of dollars if you, you know, optimize some code, sometimes in production, long term, so it really matters, right? But before we start, short introduction, my name is Bartolome Vodka, I'm an engineer at Google, normally I work at Google Cloud, Google managed Prometheus service, but generally I'm open source, I love Go, I love distributed systems, observability topics, I maintain TANOS, which is like open source scalable Prometheus system, I maintain Prometheus as well, and generally, yeah, lots of things in open source, I mentor a lot, and I suggest you to check, you know, also try to mentor others, it's super important to bring new generation of people up to the speed in the open source, and yeah, I'm active in the CNCF. And recently, as you see, I published a book, and I think, you know, it's kind of unique, everybody's doing TikToks now, and, you know, YouTube, and I was like, yeah, let's be old school, because, you know, you need to be unique sometimes in the world, and I really enjoyed that, I learned a lot during that, and I would love you to learn as well, so I'm kind of summarizing of some concepts from my book here in the stock, so let's go. And I would like to start with this story, and, you know, apparently some of the talks, one of the best talks, have to start with the story, but this is something that kind of maybe triggered me to write the book, right, so imagine that, I mean, yeah, that was kind of five years ago, we just started the project called Thanos Open Source, really it doesn't matter what it does right now, but, you know, what happens is that it has microservices, it has, you know, I think, six different microservices written in Golang, you put in communities or any other cloud, and it's just a distributed database, and one part of this database is compactor, it's like a component, again, doesn't matter much what it does, what it matters is that it touches object storage, and it processes, you know, sometimes gigabytes or terabytes daily of metrics, right, of some data, so what happened is that at the very beginning of implementation, as you can imagine, you know, we implemented, yeah, MVP, it kind of functionally worked, but of course, you know, the implementation was kind of naive, definitely not optimized, we didn't even run any benchmark, right, other than just running on production and just, yeah, kind of works, so, and you're laughing, but this is usually, you know, what development in a higher velocity looks like, and it was working very well, until, of course, more people put load into this, and, you know, we have some issues like Ooms, you know, one user pointed us to some graphs of, you know, incredibly high spike of memory usage on the heap, on the Golan heap, right, and you can see it's a drop, which means, you know, there was a restart or someone killed this, and, yeah, and the numbers are not small, like 15 gates, I mean, for large data set, maybe it's fine, but it was kind of problematic, right, so it was really interesting to see what different feedback and what different suggestions community were giving us, and I mean, community, everybody, like users, other developers, maybe product managers, we don't know sometimes who they role are, but, you know, probably depending on their background, the answers, the proposals were totally different, right, so I would like you to kind of, you know, check, and like, check if you had the same situations in your experience, because, you know, this is kind of like very ongoing problem, and I would like to, yeah, showcase this, so, you know, first suggestion was that, can you give me a configuration that doesn't womb, and it's like, what, do you expect me, like, very new project to have, like, flags, not a womb, or like, useless memory, this is not as simple as that, yet many, many users are asking us this question, or person's, or person's project, probably you heard this question, okay, what configuration I should use, so it uses less memory, right, or like, it just, it's more optimized, how can I optimize using configuration, it's just, you know, it's not as simple as that, I guess, you know, maybe in Java, in JVM, you have lots of performance flags, you sometimes tune things, and it's better, but, you know, it's not so simple, it's a goal, like, kind of low level, you, I mean, yeah, it's, you need to do more than that, right, another, you know, interesting approach, but very, very good in some way, is it just, okay, I will just put this process into bigger machine, and it's that, and that's totally valid, you know, solution, maybe short term, maybe sometimes it's enough, but, you know, in our case, it was not sustainable, because of course, you couldn't grow vertically more and more, and also, even if you would maybe find the big enough machine that was working for your data set, then, you know, obviously, you were overpaying a lot, if the code is naive and maybe wasting a lot of memory, right, then finally, you know, the most fun approach, okay, let's split this one microservice into, you know, like a schedule there, and then, you know, warcares, and then we'll just replicate in my super nice computer, you know, communities cluster, and, you know, it will just horizontally scale, so I can use many, many hundreds of small machines, so it will work, yes, but, you know, you are putting on small, kind of, microservice so much complexity that it will be, like, more expensive, generally, right, so the network costs, like, distributed systems, you know, injects, you know, things that you have to replicate data, finally, so you overpay more and more and more, and you are, kind of, distributing this non-optimized code to different places, that's not always the solution. Sometimes the code cannot be optimized more, and we can, you know, we should probably horizontally scale, but not in the very beginning of the project, right, yet, that was the first suggestion from the community, right, of course, you can just switch from Thanos to something else, right, that's also solution, and then, if you have this approach, and probably you would just jump through project, this is not super efficient, but maybe, you know, some parts of the project are better or some worse, that's an option, some suggestion, of course, paying for vendor, right, like, they will solve the problems for me, for real money, so, but yeah, that's not always a good solution, like, that's just giving up, and also, you know, migration of data, huge cost of learning new tools, and so on, and, you know, all of this work we're in the code, we have this, and it's like, you know, it's bumping, and super easy ways that you could be avoided, right, and, yeah, so, you know, of course, that was Maloch, so in C++, I mean, in Bugo, we don't have Maloch and so on, but, you know, memory overhead, memory leaks like that, like, are very common in Golan, like, just imagine how many gorotins sometimes you put, you created, you forgot to close some kind of abstraction, and the gorotin is leaking, and so you are leaking memory like this Maloch, right, so, and, you know, what actually, you know, was the solution, was some contributor finally came up, investigated, what about this efficiency problem on the code level, algorithm and code level, right, and we wrote, or like, we wrote small part of the, of the compactor to stream data, right, so instead of building maybe the kind of resulted object that the compactor is doing in memory, it was as soon as possible streaming that to file system, easy, generally easy, easy, easy change, yet there was lots of discussions, lots of stress, lots of weird ideas, and I would just find it like, over time, amusing that this, this story was repeating in many, many cases, right, so, and you know, that's not only, you know, of course, experience, so many, so many kind of nice examples where only small character change, two character change there, and, you know, so much kind of like improvement over like large system, so sometimes, sometimes there are like, very easy ways that we can just pick it up and just do it, right, but we need to know how, right, so kind of two learnings from the story, one is that software efficiency on code level and algorithms, so changing code, you know, matters, and learning how to do it can be, can be useful, and second learning is that there is common pitfall, I think, generally in the, in this years, because in the past we have premature optimizations, everybody was playing with the code and trying to over-optimize things, I think now we are lazy and we are more like into DevOps, into changing, you know, configuration, into horizontal scaling because we have this power, we have cloud, and this is usually, you know, more chosen solution than actually checking the code, right, and I call it closed box thinking, and I think this is a threat a little bit in our ecosystem, so we should acknowledge that there are different levels, we can sometimes scale, we can sometimes put more bigger machine, we can sometimes throw right to rust, if that makes sense, but you know, that's not the first solution that should come to your mind, right? Okay, before we go forward, I will, I have five books to share, and I will start the link to quiz at the end, and it will be super simple, but pay attention, right, because maybe there will be some questions around, and you can answer, send me an email, and I will just, you know, just choose five people, lucky people, to have my book, so, yeah, pay attention, all right, five steps, five steps, yeah, for efficiency, efficiency progress. One thing I want to mention, I don't know if you have been in the previous talk, or like before previous, he kind of explained a lot of optimization ideas, like I think, and I might say before, like he mentioned, string optimizations with internings, has just mentioned, I think, something around, you know, allocations, and many kind of like, I think, padding, strike padding, and generally, you know, all those kind of ideas, this is fine, but it's optimizing stuff, it's not like looking through dictionary of things I did in the past, it's kind of more fuzzy, more involved, so what I would like you to focus, it's not all particular way of how we optimize an example I would show, because it's super simple and trivial, but how we get there, right, how we found what to optimize, how we found if we should even optimize, okay, so focus on that. So first step, first suggestion I would have, and this is from Book, I kind of found, yeah, I don't know, like I defined this name TFBO, which is essentially a flow for development, efficiency aware development that worked for me, and generally I see other professionals doing that a lot as well, so test, fix, benchmark, optimize, so essentially what it is, it's like a TDD with something else, and TDD you are probably familiar with, test-driven development, you test first, as you can see, and only then you kind of like implement or fix it until the test is passing, right, I would like to kind of do the same for optimizations as well, so we have benchmark-driven optimizations, because as you can see, we benchmark first, then we optimize, and then we profile, right, and I will tell you later why, but all of this is a closed loop, right, so after optimizations we have to test as well, okay, so it feels complex, but we'll make one loop, actually maybe two, during the stock on a simple code, so let's do it. So let's introduce a simple function, super simple, super stupid, we are creating millions of elements, I mean, a slice with millions of elements, and each of those elements are just a string, a constant string for them, super simple, it's the first, you know, kind of first iteration of this program we want to write, so what we do regarding TFBO, okay, so we test, right, I mean, now we have a code, for example, and we want to maybe improve it, we test, test-driven development, so let's assume I already had the test, right, but the test could look like this, and then, you know, I'm ensuring, okay, it's passing, so nothing functionally I have to fix, so what next? So next is this measurement, it's a benchmark, and again, has this already mentioned how to make benchmarks, but I have some additions, extensions to that that you might find helpful, something I want to mention is that, you know, we were talking about micro benchmarks, because the same level of testing behavior, like for example, like for this small function, like we have this create, you know, unit test is totally enough, right, this is on micro level, we are making just unit test, it's fine, but sometimes if you have a bigger system, you need to do something on macro level, like integration test, end-to-end test, whatever bigger, right, and the same happens in a benchmark, right, this is micro benchmark, this is kind of unit benchmark, there are also micro benchmarks I covered in my book, and then you need to have more sophisticated kind of setup with low testing, with maybe some automation, with some observability, like, you know, Prometheus, maybe, which measures over time some resources, but here we can, we have a simple unit create function, we can just make it simple with micro benchmarks, and, you know, it has already mentioned, but, you know, there is a special signature in a test file you have to put, and then there are optional helpers, for example, that I like actually to put almost everywhere, report allocs, which is by default making sure that this function will measure allocations as well, and the reset timer, which is super cool because it resets the measurement, so anything before you allocate, you spend time on, it will be discarded from benchmark result, so benchmark will only focus on what will happen within this loop iteration, right? And then this for loop, you cannot change it, don't try to change it, always copy, this is a boilerplate that has to be there, right? Because it allows Go to make repeatable, check the repeatability of your test by running it, you know, hundreds of times. Okay, so how we execute it, already, again, has this mentioned, but this is, you know, how I do it to, like, focus to one test, but this is not enough, in my opinion, right? By default, it runs only one test, one second. I recommend to actually make sure you explicitly state some parameters, right? And I have one liner, one liner in bash, for example, that I often use, so what it is essentially, I'm kind of creating some variables so I can reference this result later on in a short-term future, V1, for example, so this will create a V1.txt file in my locale, it will run this benchmark, it will actually run it, you know, sometime, I specify, again, which is super amazing because it was like, okay, so I have this V1 file, what I was doing with it, and then you check in your bash history, okay, oh, that was one second, and then that was something else, right, so it's kind of useful. And then this is crucial, this is something I don't know why I didn't learn in the beginning, maybe you learned the count, dash count, right? So what it is, is that it runs the same test couple of times, six times, actually, and so one second, six times, and this is super important because then you can use further tools you will see to check, you know, how reliable are your results, it will essentially calculate the variance between the, you know, the timings, for example, so if the variance is too big, then your environment is not stable, right? And then I pin to one CPU, this is super important to, generally pinning, not to one, right? Just pick something that works for you, for concurrency, pick something that runs on production maybe, or similar, but always between tests, don't change that, right? So that's super important, and also I recommend to change less than numbers of CPU because your operating system has to run on something, right? So those things matter, also don't run this on laptop without power connected because you will be CPU trolled off. There are lots of kind of small things that you think, oh, it doesn't matter, no, it matters because then you cannot rely on your results, right? So try to make this serious a little bit and at least, you know, don't put, don't benchmark on your lap, you know, in the bed, you know, because they will be overheating. So yeah, small things, but it matters, right? I was doing that all the time, by the way, yeah. So results, you know, result looks like this. You can see many of them. But this is not how I use it or how we supposed to use it, apparently. There is amazing tool called BenchStat, and it just brings in more human-readable way, and you can see it also aggregates and have some averages over those runs and tells you within this percentage. For example, the time, latency, there is a variance of 1%, which is tolerable, for example, right? And you can kind of like customize what exactly, how it calculates this variance and so on. So we can trust it, like it's within 1% of, I guess, free, you could trust it, depend on what you do, but generally it's not too bad. Allocations, fortunately, are super stable, right? So yeah, so we benchmark, we measure it, okay, we know our function has these numbers, like, I mean, what's next, right? Everybody was like, yeah, let's make it faster, let's make it faster, but wait, wait, wait, why, why should we make it faster, maybe, okay, maybe that's a lot, 100 megabytes of every, you know, create invocation, but maybe that's fine, right? So this is where I think we are missing a lot of experience, usually. I mean, you have to set some expectations, right, like, to what point you are optimizing, and usually we don't have any expectations, like, okay, yeah, I mean, even from product management here we have maybe functional requirements, but never really concrete performance requirements. So we don't know what to do, and honestly, if you don't, you just ignore those requirements, okay, I don't have, I just want to make it faster, then this premature optimization is always, right, because it's always premature, because it's a random, a random goal you don't really understand, right, so maybe, maybe just make it fast, right, that's also like very fuzzy, obviously, and that's not very helpful. So what is helpful? What I will, and I know it's super hard, I know it's kind of uncomfortable, but I suggest doing some kind of efficiency requirements, spec, super simple, as simple as possible, I call it rare, so there are efficiency requirements, and what it means is essentially try to find out some kind of function, right, some kind of, you know, complexity, but not as if it's very complex, it's just more concrete estimation of the complexity based on inputs, right, and for simple functions, like for example, our function, we can estimate, you know, what in our minds we think should happen, roughly, right, so, you know, for runtime, we know we, one million time we do something, we don't know how many now seconds, let's pick 30, this is actually pretty big for one iteration of just append, but just really pick some number, sometimes it's good, it's just, you know, you can iterate over this number, but if you don't know where you go, then, you know, how you can make any decisions, decisions. In allocations, it's a little bit bitter, a little bit easier, because we expect a slice of six, of one million elements of strings, and as we learn from MachiTalk, every string has these two parts, one part has 16 bits, which has length and capacity, or maybe capacity not, but then, yeah, length capacity and pointer, and then there's other parts, which lies in the heap, but for this, you know, 16 bytes, we can assume that we'll be 16, right, so it's every element is 16 bytes, so now we just multiply, that's our function, that's what we all expect, right, and with this, we can, you know, kind of expect that every invocation of create should, you know, kind of allocate 15 megabytes, but what we see, we allocate 80 megabytes, right, so already we see that, oh, there might be like easy ways to do, or something I don't understand about this program, and this is what leads us to better, to spotting maybe easy wins, and spotting, you know, if we need to do anything, right, in terms of time, latency, it's already kind of like, more than we kind of expected, right, but this is more of a guessing, like I just guessed this 30 seconds, right, okay, so what we do, now we know we are, you know, not fast enough, not allocating, we are over allocating, right, so then we profile, then we check, okay, we have a problem, now let's find what's going on, and this is where, on micro level, we can, you know, use profiling very easily by just adding those two flags, it will gather memory profiles and CPU profiles in the file, like v1.mempprof, on macro level, you can, there are other ways of gathering profiles, but you can use the same format, the same tools, there are even continuous profiling tools in open source, like parkadev, I really recommend them, and it's super easy then to gather those profiles over time, so this, what we want to really learn is that what causes this problem, and this is like a CPU profile, and we could spot, and the wider means it spends more CPU cycles, the depth doesn't matter, this is just how many functions we have, right, so we can see that create, of course, is one of the biggest contributors, but the growth slice, right, like why we spend so many cycles growing slice, ideally, I know how many elements I have, kind of why it doesn't grow me once, right, and then we can check, and by the way, you can use this go tool, pprof.gttp, locally, I kind of use it a lot on this file to kind of expose this kind of interactive UI, you can do the same for memory, but honestly, this is not useful because Append is a standard library function, and they are not very well exposed, right, so they're hidden, so this is not very helpful, actually CPU profile was more helpful, because it pointed us to the growth slice, and if we just Google for that, you will notice this comes from Append, and then you can go to documentation of Append and learn what it actually does, and as you probably are familiar, because this is like, should be a trivial case, Append resizes the slice, or assizes the underlying array, whenever it's full, right, and resizing, it's not super simple, it has to kind of create a new, bigger array, and copy things over, and garbage collection will kill the old one, but not fast enough because of the garbage collection, so we kind of aggregate that as another allocation, right, so this is what happens, and kind of the fix is to just preallocate right, so to tell, you know, when you create the slice, okay, how much capacity you want to prepare for that, and thanks for that, so what we do now, okay, we did optimize in our TFBO, now we test, before we're even measuring, because if you are not testing if this, you know, this code is correct, then, you know, you might be, you know, yeah, we would be happy that things are faster, but functionally broken, so always test, don't be, you know, lazy, run those unit tests, easy, and then, you know, once they are passing, you can comfortably measure, again, I just changed V2, just to specify another variable, right, on our file system, and then I can do a bunch that V1.txt and then V2.txt, actually, I can put like 100 of those variables, it will compare all of them, but here we compare two, and not only we have absolute values of those measurements, but also a diff, right, so you can see we improved a lot, and if we check absolute value in regards to our efficiency requirements, you see that we met our threshold roughly, but like we estimated it, so it's totally good, you know, 15 megabytes, we have 15 megabytes, and then it's faster than our goal, so now we are good to go and release it, right, so that's kind of the whole loop, and you kind of do it until you're happy with your results, so yeah, this is it, and learnings, again, five learnings, follow TFBO, test, fix, benchmark, optimize, use benchmarks, they are built into GoLang, they are super amazing, GoTest slash bench, set the clear goals, goals are super important here, right, and then profile, and you can, I mean, GoLang uses Pprof, which you can Google as well, it's like amazing kind of protocol, kind of set of tools, integrated with other, you know, clouds and so on, and use it, you know, every day whenever I have to optimize something, and then finally, the key is to try to understand what happens, what I expected, and you know, what's wrong, reading documentation, reading code, this is what you have to do sometimes, and a general tip, whenever you want to optimize something super, super carefully in some, you know, bottleneck part of your code, I mean, avoid standard library functions, because they are really built into generic functionality, it will test, I mean, it will do a lot of things with, you know, different edge cases that you might not have, so a lot of times, I just implemented my own parsing integer function, it was much faster, so this is a general tip that always works, but again, do it only when you need it, because you might have a box in this code, right? So that's it, thank you, you have a link here, bwplotka.dev. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.48, "text": " Okay, welcome back.", "tokens": [1033, 11, 2928, 646, 13], "temperature": 0.0, "avg_logprob": -0.2932625377879423, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.4747261703014374}, {"id": 1, "seek": 0, "start": 9.48, "end": 13.36, "text": " So while you all have been walking in, I've been quickly reading this book, Efficient", "tokens": [407, 1339, 291, 439, 362, 668, 4494, 294, 11, 286, 600, 668, 2661, 3760, 341, 1446, 11, 462, 7816], "temperature": 0.0, "avg_logprob": -0.2932625377879423, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.4747261703014374}, {"id": 2, "seek": 0, "start": 13.36, "end": 18.68, "text": " Go, it reads very quickly, and now Bartek has made sure that my code is ten times quicker,", "tokens": [1037, 11, 309, 15700, 588, 2661, 11, 293, 586, 22338, 916, 575, 1027, 988, 300, 452, 3089, 307, 2064, 1413, 16255, 11], "temperature": 0.0, "avg_logprob": -0.2932625377879423, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.4747261703014374}, {"id": 3, "seek": 0, "start": 18.68, "end": 19.88, "text": " so tell us everything about it.", "tokens": [370, 980, 505, 1203, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.2932625377879423, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.4747261703014374}, {"id": 4, "seek": 0, "start": 19.88, "end": 20.88, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2932625377879423, "compression_ratio": 1.3976608187134503, "no_speech_prob": 0.4747261703014374}, {"id": 5, "seek": 2088, "start": 20.88, "end": 28.56, "text": " Thank you very much, everybody.", "tokens": [1044, 291, 588, 709, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 6, "seek": 2088, "start": 28.56, "end": 29.56, "text": " So welcome.", "tokens": [407, 2928, 13], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 7, "seek": 2088, "start": 29.56, "end": 31.52, "text": " I hope your travels went well.", "tokens": [286, 1454, 428, 19863, 1437, 731, 13], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 8, "seek": 2088, "start": 31.52, "end": 37.519999999999996, "text": " Mine were, like, canceled, flight canceled, change of route, so I had lots of adventures,", "tokens": [11620, 645, 11, 411, 11, 24839, 11, 7018, 24839, 11, 1319, 295, 7955, 11, 370, 286, 632, 3195, 295, 20905, 11], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 9, "seek": 2088, "start": 37.519999999999996, "end": 41.599999999999994, "text": " but generally I'm super happy I made it, and we are at the FOSDEM.", "tokens": [457, 5101, 286, 478, 1687, 2055, 286, 1027, 309, 11, 293, 321, 366, 412, 264, 479, 4367, 35, 6683, 13], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 10, "seek": 2088, "start": 41.599999999999994, "end": 47.96, "text": " So in this talk, I would like to invite you to learn more about efficiency of our Go programs,", "tokens": [407, 294, 341, 751, 11, 286, 576, 411, 281, 7980, 291, 281, 1466, 544, 466, 10493, 295, 527, 1037, 4268, 11], "temperature": 0.0, "avg_logprob": -0.2987267278855847, "compression_ratio": 1.4618834080717489, "no_speech_prob": 0.000497556640766561}, {"id": 11, "seek": 4796, "start": 47.96, "end": 53.92, "text": " and there are already two talks that I have been on who mentioned, you know, optimizations", "tokens": [293, 456, 366, 1217, 732, 6686, 300, 286, 362, 668, 322, 567, 2835, 11, 291, 458, 11, 5028, 14455], "temperature": 0.0, "avg_logprob": -0.2004224072705518, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.00012629322009161115}, {"id": 12, "seek": 4796, "start": 53.92, "end": 58.2, "text": " in its name, and, like, generally how to make software more efficient.", "tokens": [294, 1080, 1315, 11, 293, 11, 411, 11, 5101, 577, 281, 652, 4722, 544, 7148, 13], "temperature": 0.0, "avg_logprob": -0.2004224072705518, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.00012629322009161115}, {"id": 13, "seek": 4796, "start": 58.2, "end": 62.480000000000004, "text": " I wonder where this, I don't know, it's not hype, but it's already three talks about", "tokens": [286, 2441, 689, 341, 11, 286, 500, 380, 458, 11, 309, 311, 406, 24144, 11, 457, 309, 311, 1217, 1045, 6686, 466], "temperature": 0.0, "avg_logprob": -0.2004224072705518, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.00012629322009161115}, {"id": 14, "seek": 4796, "start": 62.480000000000004, "end": 68.36, "text": " one topic, why it's so popular, is it because everybody's saving me money, that might be", "tokens": [472, 4829, 11, 983, 309, 311, 370, 3743, 11, 307, 309, 570, 2201, 311, 6816, 385, 1460, 11, 300, 1062, 312], "temperature": 0.0, "avg_logprob": -0.2004224072705518, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.00012629322009161115}, {"id": 15, "seek": 4796, "start": 68.36, "end": 75.4, "text": " a reason, but I'm super happy we are really uncovering this for Go, because Go alone might", "tokens": [257, 1778, 11, 457, 286, 478, 1687, 2055, 321, 366, 534, 21694, 278, 341, 337, 1037, 11, 570, 1037, 3312, 1062], "temperature": 0.0, "avg_logprob": -0.2004224072705518, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.00012629322009161115}, {"id": 16, "seek": 7540, "start": 75.4, "end": 80.12, "text": " be fast, but that doesn't mean that we cannot, you know, doesn't need to care about, you", "tokens": [312, 2370, 11, 457, 300, 1177, 380, 914, 300, 321, 2644, 11, 291, 458, 11, 1177, 380, 643, 281, 1127, 466, 11, 291], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 17, "seek": 7540, "start": 80.12, "end": 86.64, "text": " know, making it better, and use these resources when we execute it, right?", "tokens": [458, 11, 1455, 309, 1101, 11, 293, 764, 613, 3593, 562, 321, 14483, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 18, "seek": 7540, "start": 86.64, "end": 91.24000000000001, "text": " So let's learn about that, and turns out that, you know, you can save literally millions", "tokens": [407, 718, 311, 1466, 466, 300, 11, 293, 4523, 484, 300, 11, 291, 458, 11, 291, 393, 3155, 3736, 6803], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 19, "seek": 7540, "start": 91.24000000000001, "end": 95.60000000000001, "text": " of dollars if you, you know, optimize some code, sometimes in production, long term,", "tokens": [295, 3808, 498, 291, 11, 291, 458, 11, 19719, 512, 3089, 11, 2171, 294, 4265, 11, 938, 1433, 11], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 20, "seek": 7540, "start": 95.60000000000001, "end": 97.88000000000001, "text": " so it really matters, right?", "tokens": [370, 309, 534, 7001, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 21, "seek": 7540, "start": 97.88000000000001, "end": 101.88000000000001, "text": " But before we start, short introduction, my name is Bartolome Vodka, I'm an engineer", "tokens": [583, 949, 321, 722, 11, 2099, 9339, 11, 452, 1315, 307, 22338, 401, 423, 691, 30707, 11, 286, 478, 364, 11403], "temperature": 0.0, "avg_logprob": -0.1618869560809175, "compression_ratio": 1.7413127413127414, "no_speech_prob": 6.783824937883765e-05}, {"id": 22, "seek": 10188, "start": 101.88, "end": 110.11999999999999, "text": " at Google, normally I work at Google Cloud, Google managed Prometheus service, but generally", "tokens": [412, 3329, 11, 5646, 286, 589, 412, 3329, 8061, 11, 3329, 6453, 2114, 649, 42209, 2643, 11, 457, 5101], "temperature": 0.0, "avg_logprob": -0.1887176831563314, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0001398560416419059}, {"id": 23, "seek": 10188, "start": 110.11999999999999, "end": 118.0, "text": " I'm open source, I love Go, I love distributed systems, observability topics, I maintain TANOS,", "tokens": [286, 478, 1269, 4009, 11, 286, 959, 1037, 11, 286, 959, 12631, 3652, 11, 9951, 2310, 8378, 11, 286, 6909, 314, 1770, 4367, 11], "temperature": 0.0, "avg_logprob": -0.1887176831563314, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0001398560416419059}, {"id": 24, "seek": 10188, "start": 118.0, "end": 124.28, "text": " which is like open source scalable Prometheus system, I maintain Prometheus as well, and", "tokens": [597, 307, 411, 1269, 4009, 38481, 2114, 649, 42209, 1185, 11, 286, 6909, 2114, 649, 42209, 382, 731, 11, 293], "temperature": 0.0, "avg_logprob": -0.1887176831563314, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0001398560416419059}, {"id": 25, "seek": 10188, "start": 124.28, "end": 128.96, "text": " generally, yeah, lots of things in open source, I mentor a lot, and I suggest you to check,", "tokens": [5101, 11, 1338, 11, 3195, 295, 721, 294, 1269, 4009, 11, 286, 14478, 257, 688, 11, 293, 286, 3402, 291, 281, 1520, 11], "temperature": 0.0, "avg_logprob": -0.1887176831563314, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0001398560416419059}, {"id": 26, "seek": 12896, "start": 128.96, "end": 134.56, "text": " you know, also try to mentor others, it's super important to bring new generation of", "tokens": [291, 458, 11, 611, 853, 281, 14478, 2357, 11, 309, 311, 1687, 1021, 281, 1565, 777, 5125, 295], "temperature": 0.0, "avg_logprob": -0.1492498728854597, "compression_ratio": 1.6550387596899225, "no_speech_prob": 9.598223550710827e-05}, {"id": 27, "seek": 12896, "start": 134.56, "end": 140.08, "text": " people up to the speed in the open source, and yeah, I'm active in the CNCF.", "tokens": [561, 493, 281, 264, 3073, 294, 264, 1269, 4009, 11, 293, 1338, 11, 286, 478, 4967, 294, 264, 48714, 37, 13], "temperature": 0.0, "avg_logprob": -0.1492498728854597, "compression_ratio": 1.6550387596899225, "no_speech_prob": 9.598223550710827e-05}, {"id": 28, "seek": 12896, "start": 140.08, "end": 147.48000000000002, "text": " And recently, as you see, I published a book, and I think, you know, it's kind of unique,", "tokens": [400, 3938, 11, 382, 291, 536, 11, 286, 6572, 257, 1446, 11, 293, 286, 519, 11, 291, 458, 11, 309, 311, 733, 295, 3845, 11], "temperature": 0.0, "avg_logprob": -0.1492498728854597, "compression_ratio": 1.6550387596899225, "no_speech_prob": 9.598223550710827e-05}, {"id": 29, "seek": 12896, "start": 147.48000000000002, "end": 151.24, "text": " everybody's doing TikToks now, and, you know, YouTube, and I was like, yeah, let's be old", "tokens": [2201, 311, 884, 20211, 82, 586, 11, 293, 11, 291, 458, 11, 3088, 11, 293, 286, 390, 411, 11, 1338, 11, 718, 311, 312, 1331], "temperature": 0.0, "avg_logprob": -0.1492498728854597, "compression_ratio": 1.6550387596899225, "no_speech_prob": 9.598223550710827e-05}, {"id": 30, "seek": 12896, "start": 151.24, "end": 155.28, "text": " school, because, you know, you need to be unique sometimes in the world, and I really", "tokens": [1395, 11, 570, 11, 291, 458, 11, 291, 643, 281, 312, 3845, 2171, 294, 264, 1002, 11, 293, 286, 534], "temperature": 0.0, "avg_logprob": -0.1492498728854597, "compression_ratio": 1.6550387596899225, "no_speech_prob": 9.598223550710827e-05}, {"id": 31, "seek": 15528, "start": 155.28, "end": 160.0, "text": " enjoyed that, I learned a lot during that, and I would love you to learn as well, so", "tokens": [4626, 300, 11, 286, 3264, 257, 688, 1830, 300, 11, 293, 286, 576, 959, 291, 281, 1466, 382, 731, 11, 370], "temperature": 0.0, "avg_logprob": -0.1415644823494604, "compression_ratio": 1.779591836734694, "no_speech_prob": 1.4679896594316233e-05}, {"id": 32, "seek": 15528, "start": 160.0, "end": 166.48, "text": " I'm kind of summarizing of some concepts from my book here in the stock, so let's go.", "tokens": [286, 478, 733, 295, 14611, 3319, 295, 512, 10392, 490, 452, 1446, 510, 294, 264, 4127, 11, 370, 718, 311, 352, 13], "temperature": 0.0, "avg_logprob": -0.1415644823494604, "compression_ratio": 1.779591836734694, "no_speech_prob": 1.4679896594316233e-05}, {"id": 33, "seek": 15528, "start": 166.48, "end": 172.72, "text": " And I would like to start with this story, and, you know, apparently some of the talks,", "tokens": [400, 286, 576, 411, 281, 722, 365, 341, 1657, 11, 293, 11, 291, 458, 11, 7970, 512, 295, 264, 6686, 11], "temperature": 0.0, "avg_logprob": -0.1415644823494604, "compression_ratio": 1.779591836734694, "no_speech_prob": 1.4679896594316233e-05}, {"id": 34, "seek": 15528, "start": 172.72, "end": 176.2, "text": " one of the best talks, have to start with the story, but this is something that kind", "tokens": [472, 295, 264, 1151, 6686, 11, 362, 281, 722, 365, 264, 1657, 11, 457, 341, 307, 746, 300, 733], "temperature": 0.0, "avg_logprob": -0.1415644823494604, "compression_ratio": 1.779591836734694, "no_speech_prob": 1.4679896594316233e-05}, {"id": 35, "seek": 15528, "start": 176.2, "end": 182.8, "text": " of maybe triggered me to write the book, right, so imagine that, I mean, yeah, that was kind", "tokens": [295, 1310, 21710, 385, 281, 2464, 264, 1446, 11, 558, 11, 370, 3811, 300, 11, 286, 914, 11, 1338, 11, 300, 390, 733], "temperature": 0.0, "avg_logprob": -0.1415644823494604, "compression_ratio": 1.779591836734694, "no_speech_prob": 1.4679896594316233e-05}, {"id": 36, "seek": 18280, "start": 182.8, "end": 187.88000000000002, "text": " of five years ago, we just started the project called Thanos Open Source, really it doesn't", "tokens": [295, 1732, 924, 2057, 11, 321, 445, 1409, 264, 1716, 1219, 35993, 7238, 29629, 11, 534, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.17013380445283036, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.692364276095759e-05}, {"id": 37, "seek": 18280, "start": 187.88000000000002, "end": 193.36, "text": " matter what it does right now, but, you know, what happens is that it has microservices,", "tokens": [1871, 437, 309, 775, 558, 586, 11, 457, 11, 291, 458, 11, 437, 2314, 307, 300, 309, 575, 15547, 47480, 11], "temperature": 0.0, "avg_logprob": -0.17013380445283036, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.692364276095759e-05}, {"id": 38, "seek": 18280, "start": 193.36, "end": 197.68, "text": " it has, you know, I think, six different microservices written in Golang, you put in communities", "tokens": [309, 575, 11, 291, 458, 11, 286, 519, 11, 2309, 819, 15547, 47480, 3720, 294, 36319, 656, 11, 291, 829, 294, 4456], "temperature": 0.0, "avg_logprob": -0.17013380445283036, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.692364276095759e-05}, {"id": 39, "seek": 18280, "start": 197.68, "end": 202.44, "text": " or any other cloud, and it's just a distributed database, and one part of this database is", "tokens": [420, 604, 661, 4588, 11, 293, 309, 311, 445, 257, 12631, 8149, 11, 293, 472, 644, 295, 341, 8149, 307], "temperature": 0.0, "avg_logprob": -0.17013380445283036, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.692364276095759e-05}, {"id": 40, "seek": 18280, "start": 202.44, "end": 207.12, "text": " compactor, it's like a component, again, doesn't matter much what it does, what it matters", "tokens": [14679, 284, 11, 309, 311, 411, 257, 6542, 11, 797, 11, 1177, 380, 1871, 709, 437, 309, 775, 11, 437, 309, 7001], "temperature": 0.0, "avg_logprob": -0.17013380445283036, "compression_ratio": 1.7790697674418605, "no_speech_prob": 1.692364276095759e-05}, {"id": 41, "seek": 20712, "start": 207.12, "end": 212.8, "text": " is that it touches object storage, and it processes, you know, sometimes gigabytes or terabytes", "tokens": [307, 300, 309, 17431, 2657, 6725, 11, 293, 309, 7555, 11, 291, 458, 11, 2171, 42741, 420, 1796, 24538], "temperature": 0.0, "avg_logprob": -0.15013666765405498, "compression_ratio": 1.7442748091603053, "no_speech_prob": 2.7847949240822345e-05}, {"id": 42, "seek": 20712, "start": 212.8, "end": 219.44, "text": " daily of metrics, right, of some data, so what happened is that at the very beginning", "tokens": [5212, 295, 16367, 11, 558, 11, 295, 512, 1412, 11, 370, 437, 2011, 307, 300, 412, 264, 588, 2863], "temperature": 0.0, "avg_logprob": -0.15013666765405498, "compression_ratio": 1.7442748091603053, "no_speech_prob": 2.7847949240822345e-05}, {"id": 43, "seek": 20712, "start": 219.44, "end": 225.88, "text": " of implementation, as you can imagine, you know, we implemented, yeah, MVP, it kind of", "tokens": [295, 11420, 11, 382, 291, 393, 3811, 11, 291, 458, 11, 321, 12270, 11, 1338, 11, 37151, 11, 309, 733, 295], "temperature": 0.0, "avg_logprob": -0.15013666765405498, "compression_ratio": 1.7442748091603053, "no_speech_prob": 2.7847949240822345e-05}, {"id": 44, "seek": 20712, "start": 225.88, "end": 230.4, "text": " functionally worked, but of course, you know, the implementation was kind of naive, definitely", "tokens": [2445, 379, 2732, 11, 457, 295, 1164, 11, 291, 458, 11, 264, 11420, 390, 733, 295, 29052, 11, 2138], "temperature": 0.0, "avg_logprob": -0.15013666765405498, "compression_ratio": 1.7442748091603053, "no_speech_prob": 2.7847949240822345e-05}, {"id": 45, "seek": 20712, "start": 230.4, "end": 235.16, "text": " not optimized, we didn't even run any benchmark, right, other than just running on production", "tokens": [406, 26941, 11, 321, 994, 380, 754, 1190, 604, 18927, 11, 558, 11, 661, 813, 445, 2614, 322, 4265], "temperature": 0.0, "avg_logprob": -0.15013666765405498, "compression_ratio": 1.7442748091603053, "no_speech_prob": 2.7847949240822345e-05}, {"id": 46, "seek": 23516, "start": 235.16, "end": 241.32, "text": " and just, yeah, kind of works, so, and you're laughing, but this is usually, you know, what", "tokens": [293, 445, 11, 1338, 11, 733, 295, 1985, 11, 370, 11, 293, 291, 434, 5059, 11, 457, 341, 307, 2673, 11, 291, 458, 11, 437], "temperature": 0.0, "avg_logprob": -0.1764838744182976, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.0272141480527353e-05}, {"id": 47, "seek": 23516, "start": 241.32, "end": 247.16, "text": " development in a higher velocity looks like, and it was working very well, until, of course,", "tokens": [3250, 294, 257, 2946, 9269, 1542, 411, 11, 293, 309, 390, 1364, 588, 731, 11, 1826, 11, 295, 1164, 11], "temperature": 0.0, "avg_logprob": -0.1764838744182976, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.0272141480527353e-05}, {"id": 48, "seek": 23516, "start": 247.16, "end": 252.56, "text": " more people put load into this, and, you know, we have some issues like Ooms, you know, one", "tokens": [544, 561, 829, 3677, 666, 341, 11, 293, 11, 291, 458, 11, 321, 362, 512, 2663, 411, 422, 4785, 11, 291, 458, 11, 472], "temperature": 0.0, "avg_logprob": -0.1764838744182976, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.0272141480527353e-05}, {"id": 49, "seek": 23516, "start": 252.56, "end": 261.24, "text": " user pointed us to some graphs of, you know, incredibly high spike of memory usage on the", "tokens": [4195, 10932, 505, 281, 512, 24877, 295, 11, 291, 458, 11, 6252, 1090, 21053, 295, 4675, 14924, 322, 264], "temperature": 0.0, "avg_logprob": -0.1764838744182976, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.0272141480527353e-05}, {"id": 50, "seek": 26124, "start": 261.24, "end": 265.08, "text": " heap, on the Golan heap, right, and you can see it's a drop, which means, you know, there", "tokens": [33591, 11, 322, 264, 460, 23754, 33591, 11, 558, 11, 293, 291, 393, 536, 309, 311, 257, 3270, 11, 597, 1355, 11, 291, 458, 11, 456], "temperature": 0.0, "avg_logprob": -0.20733387702334244, "compression_ratio": 1.7171314741035857, "no_speech_prob": 7.99749614088796e-06}, {"id": 51, "seek": 26124, "start": 265.08, "end": 270.24, "text": " was a restart or someone killed this, and, yeah, and the numbers are not small, like", "tokens": [390, 257, 21022, 420, 1580, 4652, 341, 11, 293, 11, 1338, 11, 293, 264, 3547, 366, 406, 1359, 11, 411], "temperature": 0.0, "avg_logprob": -0.20733387702334244, "compression_ratio": 1.7171314741035857, "no_speech_prob": 7.99749614088796e-06}, {"id": 52, "seek": 26124, "start": 270.24, "end": 277.52, "text": " 15 gates, I mean, for large data set, maybe it's fine, but it was kind of problematic,", "tokens": [2119, 19792, 11, 286, 914, 11, 337, 2416, 1412, 992, 11, 1310, 309, 311, 2489, 11, 457, 309, 390, 733, 295, 19011, 11], "temperature": 0.0, "avg_logprob": -0.20733387702334244, "compression_ratio": 1.7171314741035857, "no_speech_prob": 7.99749614088796e-06}, {"id": 53, "seek": 26124, "start": 277.52, "end": 283.0, "text": " right, so it was really interesting to see what different feedback and what different", "tokens": [558, 11, 370, 309, 390, 534, 1880, 281, 536, 437, 819, 5824, 293, 437, 819], "temperature": 0.0, "avg_logprob": -0.20733387702334244, "compression_ratio": 1.7171314741035857, "no_speech_prob": 7.99749614088796e-06}, {"id": 54, "seek": 26124, "start": 283.0, "end": 287.52, "text": " suggestions community were giving us, and I mean, community, everybody, like users,", "tokens": [13396, 1768, 645, 2902, 505, 11, 293, 286, 914, 11, 1768, 11, 2201, 11, 411, 5022, 11], "temperature": 0.0, "avg_logprob": -0.20733387702334244, "compression_ratio": 1.7171314741035857, "no_speech_prob": 7.99749614088796e-06}, {"id": 55, "seek": 28752, "start": 287.52, "end": 291.76, "text": " other developers, maybe product managers, we don't know sometimes who they role are,", "tokens": [661, 8849, 11, 1310, 1674, 14084, 11, 321, 500, 380, 458, 2171, 567, 436, 3090, 366, 11], "temperature": 0.0, "avg_logprob": -0.15617893813946926, "compression_ratio": 1.7591836734693878, "no_speech_prob": 4.796065331902355e-05}, {"id": 56, "seek": 28752, "start": 291.76, "end": 297.35999999999996, "text": " but, you know, probably depending on their background, the answers, the proposals were", "tokens": [457, 11, 291, 458, 11, 1391, 5413, 322, 641, 3678, 11, 264, 6338, 11, 264, 20198, 645], "temperature": 0.0, "avg_logprob": -0.15617893813946926, "compression_ratio": 1.7591836734693878, "no_speech_prob": 4.796065331902355e-05}, {"id": 57, "seek": 28752, "start": 297.35999999999996, "end": 303.88, "text": " totally different, right, so I would like you to kind of, you know, check, and like,", "tokens": [3879, 819, 11, 558, 11, 370, 286, 576, 411, 291, 281, 733, 295, 11, 291, 458, 11, 1520, 11, 293, 411, 11], "temperature": 0.0, "avg_logprob": -0.15617893813946926, "compression_ratio": 1.7591836734693878, "no_speech_prob": 4.796065331902355e-05}, {"id": 58, "seek": 28752, "start": 303.88, "end": 311.15999999999997, "text": " check if you had the same situations in your experience, because, you know, this is kind", "tokens": [1520, 498, 291, 632, 264, 912, 6851, 294, 428, 1752, 11, 570, 11, 291, 458, 11, 341, 307, 733], "temperature": 0.0, "avg_logprob": -0.15617893813946926, "compression_ratio": 1.7591836734693878, "no_speech_prob": 4.796065331902355e-05}, {"id": 59, "seek": 28752, "start": 311.15999999999997, "end": 315.96, "text": " of like very ongoing problem, and I would like to, yeah, showcase this, so, you know,", "tokens": [295, 411, 588, 10452, 1154, 11, 293, 286, 576, 411, 281, 11, 1338, 11, 20388, 341, 11, 370, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15617893813946926, "compression_ratio": 1.7591836734693878, "no_speech_prob": 4.796065331902355e-05}, {"id": 60, "seek": 31596, "start": 315.96, "end": 321.0, "text": " first suggestion was that, can you give me a configuration that doesn't womb, and it's", "tokens": [700, 16541, 390, 300, 11, 393, 291, 976, 385, 257, 11694, 300, 1177, 380, 34310, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 61, "seek": 31596, "start": 321.0, "end": 326.64, "text": " like, what, do you expect me, like, very new project to have, like, flags, not a womb,", "tokens": [411, 11, 437, 11, 360, 291, 2066, 385, 11, 411, 11, 588, 777, 1716, 281, 362, 11, 411, 11, 23265, 11, 406, 257, 34310, 11], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 62, "seek": 31596, "start": 326.64, "end": 331.59999999999997, "text": " or like, useless memory, this is not as simple as that, yet many, many users are asking us", "tokens": [420, 411, 11, 14115, 4675, 11, 341, 307, 406, 382, 2199, 382, 300, 11, 1939, 867, 11, 867, 5022, 366, 3365, 505], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 63, "seek": 31596, "start": 331.59999999999997, "end": 335.84, "text": " this question, or person's, or person's project, probably you heard this question, okay, what", "tokens": [341, 1168, 11, 420, 954, 311, 11, 420, 954, 311, 1716, 11, 1391, 291, 2198, 341, 1168, 11, 1392, 11, 437], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 64, "seek": 31596, "start": 335.84, "end": 340.59999999999997, "text": " configuration I should use, so it uses less memory, right, or like, it just, it's more", "tokens": [11694, 286, 820, 764, 11, 370, 309, 4960, 1570, 4675, 11, 558, 11, 420, 411, 11, 309, 445, 11, 309, 311, 544], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 65, "seek": 31596, "start": 340.59999999999997, "end": 344.52, "text": " optimized, how can I optimize using configuration, it's just, you know, it's not as simple as", "tokens": [26941, 11, 577, 393, 286, 19719, 1228, 11694, 11, 309, 311, 445, 11, 291, 458, 11, 309, 311, 406, 382, 2199, 382], "temperature": 0.0, "avg_logprob": -0.2492956490352236, "compression_ratio": 1.988929889298893, "no_speech_prob": 2.121047509717755e-05}, {"id": 66, "seek": 34452, "start": 344.52, "end": 349.08, "text": " that, I guess, you know, maybe in Java, in JVM, you have lots of performance flags, you", "tokens": [300, 11, 286, 2041, 11, 291, 458, 11, 1310, 294, 10745, 11, 294, 508, 53, 44, 11, 291, 362, 3195, 295, 3389, 23265, 11, 291], "temperature": 0.0, "avg_logprob": -0.17066634328741775, "compression_ratio": 1.722007722007722, "no_speech_prob": 2.8889729946968146e-05}, {"id": 67, "seek": 34452, "start": 349.08, "end": 353.24, "text": " sometimes tune things, and it's better, but, you know, it's not so simple, it's a goal,", "tokens": [2171, 10864, 721, 11, 293, 309, 311, 1101, 11, 457, 11, 291, 458, 11, 309, 311, 406, 370, 2199, 11, 309, 311, 257, 3387, 11], "temperature": 0.0, "avg_logprob": -0.17066634328741775, "compression_ratio": 1.722007722007722, "no_speech_prob": 2.8889729946968146e-05}, {"id": 68, "seek": 34452, "start": 353.24, "end": 359.52, "text": " like, kind of low level, you, I mean, yeah, it's, you need to do more than that, right,", "tokens": [411, 11, 733, 295, 2295, 1496, 11, 291, 11, 286, 914, 11, 1338, 11, 309, 311, 11, 291, 643, 281, 360, 544, 813, 300, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.17066634328741775, "compression_ratio": 1.722007722007722, "no_speech_prob": 2.8889729946968146e-05}, {"id": 69, "seek": 34452, "start": 359.52, "end": 364.32, "text": " another, you know, interesting approach, but very, very good in some way, is it just, okay,", "tokens": [1071, 11, 291, 458, 11, 1880, 3109, 11, 457, 588, 11, 588, 665, 294, 512, 636, 11, 307, 309, 445, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.17066634328741775, "compression_ratio": 1.722007722007722, "no_speech_prob": 2.8889729946968146e-05}, {"id": 70, "seek": 34452, "start": 364.32, "end": 369.56, "text": " I will just put this process into bigger machine, and it's that, and that's totally valid,", "tokens": [286, 486, 445, 829, 341, 1399, 666, 3801, 3479, 11, 293, 309, 311, 300, 11, 293, 300, 311, 3879, 7363, 11], "temperature": 0.0, "avg_logprob": -0.17066634328741775, "compression_ratio": 1.722007722007722, "no_speech_prob": 2.8889729946968146e-05}, {"id": 71, "seek": 36956, "start": 369.56, "end": 374.6, "text": " you know, solution, maybe short term, maybe sometimes it's enough, but, you know, in our", "tokens": [291, 458, 11, 3827, 11, 1310, 2099, 1433, 11, 1310, 2171, 309, 311, 1547, 11, 457, 11, 291, 458, 11, 294, 527], "temperature": 0.0, "avg_logprob": -0.1592486430022676, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.4353805201826617e-05}, {"id": 72, "seek": 36956, "start": 374.6, "end": 380.0, "text": " case, it was not sustainable, because of course, you couldn't grow vertically more and more,", "tokens": [1389, 11, 309, 390, 406, 11235, 11, 570, 295, 1164, 11, 291, 2809, 380, 1852, 28450, 544, 293, 544, 11], "temperature": 0.0, "avg_logprob": -0.1592486430022676, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.4353805201826617e-05}, {"id": 73, "seek": 36956, "start": 380.0, "end": 383.76, "text": " and also, even if you would maybe find the big enough machine that was working for your", "tokens": [293, 611, 11, 754, 498, 291, 576, 1310, 915, 264, 955, 1547, 3479, 300, 390, 1364, 337, 428], "temperature": 0.0, "avg_logprob": -0.1592486430022676, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.4353805201826617e-05}, {"id": 74, "seek": 36956, "start": 383.76, "end": 389.96, "text": " data set, then, you know, obviously, you were overpaying a lot, if the code is naive and", "tokens": [1412, 992, 11, 550, 11, 291, 458, 11, 2745, 11, 291, 645, 670, 22038, 278, 257, 688, 11, 498, 264, 3089, 307, 29052, 293], "temperature": 0.0, "avg_logprob": -0.1592486430022676, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.4353805201826617e-05}, {"id": 75, "seek": 36956, "start": 389.96, "end": 395.8, "text": " maybe wasting a lot of memory, right, then finally, you know, the most fun approach, okay,", "tokens": [1310, 20457, 257, 688, 295, 4675, 11, 558, 11, 550, 2721, 11, 291, 458, 11, 264, 881, 1019, 3109, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.1592486430022676, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.4353805201826617e-05}, {"id": 76, "seek": 39580, "start": 395.8, "end": 400.36, "text": " let's split this one microservice into, you know, like a schedule there, and then, you", "tokens": [718, 311, 7472, 341, 472, 15547, 25006, 666, 11, 291, 458, 11, 411, 257, 7567, 456, 11, 293, 550, 11, 291], "temperature": 0.0, "avg_logprob": -0.16120769182840983, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.5367402486153878e-05}, {"id": 77, "seek": 39580, "start": 400.36, "end": 405.96000000000004, "text": " know, warcares, and then we'll just replicate in my super nice computer, you know, communities", "tokens": [458, 11, 1516, 496, 495, 11, 293, 550, 321, 603, 445, 25356, 294, 452, 1687, 1481, 3820, 11, 291, 458, 11, 4456], "temperature": 0.0, "avg_logprob": -0.16120769182840983, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.5367402486153878e-05}, {"id": 78, "seek": 39580, "start": 405.96000000000004, "end": 410.6, "text": " cluster, and, you know, it will just horizontally scale, so I can use many, many hundreds of", "tokens": [13630, 11, 293, 11, 291, 458, 11, 309, 486, 445, 33796, 4373, 11, 370, 286, 393, 764, 867, 11, 867, 6779, 295], "temperature": 0.0, "avg_logprob": -0.16120769182840983, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.5367402486153878e-05}, {"id": 79, "seek": 39580, "start": 410.6, "end": 417.44, "text": " small machines, so it will work, yes, but, you know, you are putting on small, kind of,", "tokens": [1359, 8379, 11, 370, 309, 486, 589, 11, 2086, 11, 457, 11, 291, 458, 11, 291, 366, 3372, 322, 1359, 11, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.16120769182840983, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.5367402486153878e-05}, {"id": 80, "seek": 39580, "start": 417.44, "end": 422.48, "text": " microservice so much complexity that it will be, like, more expensive, generally, right,", "tokens": [15547, 25006, 370, 709, 14024, 300, 309, 486, 312, 11, 411, 11, 544, 5124, 11, 5101, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.16120769182840983, "compression_ratio": 1.7896825396825398, "no_speech_prob": 2.5367402486153878e-05}, {"id": 81, "seek": 42248, "start": 422.48, "end": 428.68, "text": " so the network costs, like, distributed systems, you know, injects, you know, things that you", "tokens": [370, 264, 3209, 5497, 11, 411, 11, 12631, 3652, 11, 291, 458, 11, 10711, 82, 11, 291, 458, 11, 721, 300, 291], "temperature": 0.0, "avg_logprob": -0.19937293549888155, "compression_ratio": 1.8045977011494252, "no_speech_prob": 9.923165634972975e-06}, {"id": 82, "seek": 42248, "start": 428.68, "end": 434.16, "text": " have to replicate data, finally, so you overpay more and more and more, and you are, kind", "tokens": [362, 281, 25356, 1412, 11, 2721, 11, 370, 291, 670, 22038, 544, 293, 544, 293, 544, 11, 293, 291, 366, 11, 733], "temperature": 0.0, "avg_logprob": -0.19937293549888155, "compression_ratio": 1.8045977011494252, "no_speech_prob": 9.923165634972975e-06}, {"id": 83, "seek": 42248, "start": 434.16, "end": 440.52000000000004, "text": " of, distributing this non-optimized code to different places, that's not always the solution.", "tokens": [295, 11, 41406, 341, 2107, 12, 5747, 332, 1602, 3089, 281, 819, 3190, 11, 300, 311, 406, 1009, 264, 3827, 13], "temperature": 0.0, "avg_logprob": -0.19937293549888155, "compression_ratio": 1.8045977011494252, "no_speech_prob": 9.923165634972975e-06}, {"id": 84, "seek": 42248, "start": 440.52000000000004, "end": 445.16, "text": " Sometimes the code cannot be optimized more, and we can, you know, we should probably horizontally", "tokens": [4803, 264, 3089, 2644, 312, 26941, 544, 11, 293, 321, 393, 11, 291, 458, 11, 321, 820, 1391, 33796], "temperature": 0.0, "avg_logprob": -0.19937293549888155, "compression_ratio": 1.8045977011494252, "no_speech_prob": 9.923165634972975e-06}, {"id": 85, "seek": 42248, "start": 445.16, "end": 450.08000000000004, "text": " scale, but not in the very beginning of the project, right, yet, that was the first suggestion", "tokens": [4373, 11, 457, 406, 294, 264, 588, 2863, 295, 264, 1716, 11, 558, 11, 1939, 11, 300, 390, 264, 700, 16541], "temperature": 0.0, "avg_logprob": -0.19937293549888155, "compression_ratio": 1.8045977011494252, "no_speech_prob": 9.923165634972975e-06}, {"id": 86, "seek": 45008, "start": 450.08, "end": 454.68, "text": " from the community, right, of course, you can just switch from Thanos to something else,", "tokens": [490, 264, 1768, 11, 558, 11, 295, 1164, 11, 291, 393, 445, 3679, 490, 35993, 281, 746, 1646, 11], "temperature": 0.0, "avg_logprob": -0.2064006639563519, "compression_ratio": 1.754863813229572, "no_speech_prob": 1.7791173377190717e-05}, {"id": 87, "seek": 45008, "start": 454.68, "end": 458.47999999999996, "text": " right, that's also solution, and then, if you have this approach, and probably you would", "tokens": [558, 11, 300, 311, 611, 3827, 11, 293, 550, 11, 498, 291, 362, 341, 3109, 11, 293, 1391, 291, 576], "temperature": 0.0, "avg_logprob": -0.2064006639563519, "compression_ratio": 1.754863813229572, "no_speech_prob": 1.7791173377190717e-05}, {"id": 88, "seek": 45008, "start": 458.47999999999996, "end": 463.84, "text": " just jump through project, this is not super efficient, but maybe, you know, some parts", "tokens": [445, 3012, 807, 1716, 11, 341, 307, 406, 1687, 7148, 11, 457, 1310, 11, 291, 458, 11, 512, 3166], "temperature": 0.0, "avg_logprob": -0.2064006639563519, "compression_ratio": 1.754863813229572, "no_speech_prob": 1.7791173377190717e-05}, {"id": 89, "seek": 45008, "start": 463.84, "end": 470.08, "text": " of the project are better or some worse, that's an option, some suggestion, of course, paying", "tokens": [295, 264, 1716, 366, 1101, 420, 512, 5324, 11, 300, 311, 364, 3614, 11, 512, 16541, 11, 295, 1164, 11, 6229], "temperature": 0.0, "avg_logprob": -0.2064006639563519, "compression_ratio": 1.754863813229572, "no_speech_prob": 1.7791173377190717e-05}, {"id": 90, "seek": 45008, "start": 470.08, "end": 476.64, "text": " for vendor, right, like, they will solve the problems for me, for real money, so, but yeah,", "tokens": [337, 24321, 11, 558, 11, 411, 11, 436, 486, 5039, 264, 2740, 337, 385, 11, 337, 957, 1460, 11, 370, 11, 457, 1338, 11], "temperature": 0.0, "avg_logprob": -0.2064006639563519, "compression_ratio": 1.754863813229572, "no_speech_prob": 1.7791173377190717e-05}, {"id": 91, "seek": 47664, "start": 476.64, "end": 481.8, "text": " that's not always a good solution, like, that's just giving up, and also, you know, migration", "tokens": [300, 311, 406, 1009, 257, 665, 3827, 11, 411, 11, 300, 311, 445, 2902, 493, 11, 293, 611, 11, 291, 458, 11, 17011], "temperature": 0.0, "avg_logprob": -0.17540032775313766, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0001135824277298525}, {"id": 92, "seek": 47664, "start": 481.8, "end": 490.2, "text": " of data, huge cost of learning new tools, and so on, and, you know, all of this work", "tokens": [295, 1412, 11, 2603, 2063, 295, 2539, 777, 3873, 11, 293, 370, 322, 11, 293, 11, 291, 458, 11, 439, 295, 341, 589], "temperature": 0.0, "avg_logprob": -0.17540032775313766, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0001135824277298525}, {"id": 93, "seek": 47664, "start": 490.2, "end": 496.64, "text": " we're in the code, we have this, and it's like, you know, it's bumping, and super easy", "tokens": [321, 434, 294, 264, 3089, 11, 321, 362, 341, 11, 293, 309, 311, 411, 11, 291, 458, 11, 309, 311, 9961, 278, 11, 293, 1687, 1858], "temperature": 0.0, "avg_logprob": -0.17540032775313766, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0001135824277298525}, {"id": 94, "seek": 47664, "start": 496.64, "end": 504.96, "text": " ways that you could be avoided, right, and, yeah, so, you know, of course, that was Maloch,", "tokens": [2098, 300, 291, 727, 312, 24890, 11, 558, 11, 293, 11, 1338, 11, 370, 11, 291, 458, 11, 295, 1164, 11, 300, 390, 5746, 8997, 11], "temperature": 0.0, "avg_logprob": -0.17540032775313766, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0001135824277298525}, {"id": 95, "seek": 50496, "start": 504.96, "end": 509.88, "text": " so in C++, I mean, in Bugo, we don't have Maloch and so on, but, you know, memory overhead,", "tokens": [370, 294, 383, 25472, 11, 286, 914, 11, 294, 363, 20746, 11, 321, 500, 380, 362, 5746, 8997, 293, 370, 322, 11, 457, 11, 291, 458, 11, 4675, 19922, 11], "temperature": 0.0, "avg_logprob": -0.22728389501571655, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9389583030715585e-05}, {"id": 96, "seek": 50496, "start": 509.88, "end": 514.24, "text": " memory leaks like that, like, are very common in Golan, like, just imagine how many gorotins", "tokens": [4675, 28885, 411, 300, 11, 411, 11, 366, 588, 2689, 294, 460, 23754, 11, 411, 11, 445, 3811, 577, 867, 24012, 310, 1292], "temperature": 0.0, "avg_logprob": -0.22728389501571655, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9389583030715585e-05}, {"id": 97, "seek": 50496, "start": 514.24, "end": 518.84, "text": " sometimes you put, you created, you forgot to close some kind of abstraction, and the", "tokens": [2171, 291, 829, 11, 291, 2942, 11, 291, 5298, 281, 1998, 512, 733, 295, 37765, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.22728389501571655, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9389583030715585e-05}, {"id": 98, "seek": 50496, "start": 518.84, "end": 523.76, "text": " gorotin is leaking, and so you are leaking memory like this Maloch, right, so, and, you", "tokens": [24012, 310, 259, 307, 32856, 11, 293, 370, 291, 366, 32856, 4675, 411, 341, 5746, 8997, 11, 558, 11, 370, 11, 293, 11, 291], "temperature": 0.0, "avg_logprob": -0.22728389501571655, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9389583030715585e-05}, {"id": 99, "seek": 50496, "start": 523.76, "end": 532.64, "text": " know, what actually, you know, was the solution, was some contributor finally came up, investigated,", "tokens": [458, 11, 437, 767, 11, 291, 458, 11, 390, 264, 3827, 11, 390, 512, 42859, 2721, 1361, 493, 11, 30070, 11], "temperature": 0.0, "avg_logprob": -0.22728389501571655, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9389583030715585e-05}, {"id": 100, "seek": 53264, "start": 532.64, "end": 537.56, "text": " what about this efficiency problem on the code level, algorithm and code level, right,", "tokens": [437, 466, 341, 10493, 1154, 322, 264, 3089, 1496, 11, 9284, 293, 3089, 1496, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.21588081972939627, "compression_ratio": 1.829268292682927, "no_speech_prob": 2.065451735688839e-05}, {"id": 101, "seek": 53264, "start": 537.56, "end": 542.3199999999999, "text": " and we wrote, or like, we wrote small part of the, of the compactor to stream data, right,", "tokens": [293, 321, 4114, 11, 420, 411, 11, 321, 4114, 1359, 644, 295, 264, 11, 295, 264, 14679, 284, 281, 4309, 1412, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.21588081972939627, "compression_ratio": 1.829268292682927, "no_speech_prob": 2.065451735688839e-05}, {"id": 102, "seek": 53264, "start": 542.3199999999999, "end": 550.08, "text": " so instead of building maybe the kind of resulted object that the compactor is doing in memory,", "tokens": [370, 2602, 295, 2390, 1310, 264, 733, 295, 18753, 2657, 300, 264, 14679, 284, 307, 884, 294, 4675, 11], "temperature": 0.0, "avg_logprob": -0.21588081972939627, "compression_ratio": 1.829268292682927, "no_speech_prob": 2.065451735688839e-05}, {"id": 103, "seek": 53264, "start": 550.08, "end": 555.28, "text": " it was as soon as possible streaming that to file system, easy, generally easy, easy,", "tokens": [309, 390, 382, 2321, 382, 1944, 11791, 300, 281, 3991, 1185, 11, 1858, 11, 5101, 1858, 11, 1858, 11], "temperature": 0.0, "avg_logprob": -0.21588081972939627, "compression_ratio": 1.829268292682927, "no_speech_prob": 2.065451735688839e-05}, {"id": 104, "seek": 53264, "start": 555.28, "end": 560.92, "text": " easy change, yet there was lots of discussions, lots of stress, lots of weird ideas, and I", "tokens": [1858, 1319, 11, 1939, 456, 390, 3195, 295, 11088, 11, 3195, 295, 4244, 11, 3195, 295, 3657, 3487, 11, 293, 286], "temperature": 0.0, "avg_logprob": -0.21588081972939627, "compression_ratio": 1.829268292682927, "no_speech_prob": 2.065451735688839e-05}, {"id": 105, "seek": 56092, "start": 560.92, "end": 566.28, "text": " would just find it like, over time, amusing that this, this story was repeating in many,", "tokens": [576, 445, 915, 309, 411, 11, 670, 565, 11, 47809, 300, 341, 11, 341, 1657, 390, 18617, 294, 867, 11], "temperature": 0.0, "avg_logprob": -0.18863968376640802, "compression_ratio": 1.8319327731092436, "no_speech_prob": 1.7342223145533353e-05}, {"id": 106, "seek": 56092, "start": 566.28, "end": 570.92, "text": " many cases, right, so, and you know, that's not only, you know, of course, experience,", "tokens": [867, 3331, 11, 558, 11, 370, 11, 293, 291, 458, 11, 300, 311, 406, 787, 11, 291, 458, 11, 295, 1164, 11, 1752, 11], "temperature": 0.0, "avg_logprob": -0.18863968376640802, "compression_ratio": 1.8319327731092436, "no_speech_prob": 1.7342223145533353e-05}, {"id": 107, "seek": 56092, "start": 570.92, "end": 576.36, "text": " so many, so many kind of nice examples where only small character change, two character", "tokens": [370, 867, 11, 370, 867, 733, 295, 1481, 5110, 689, 787, 1359, 2517, 1319, 11, 732, 2517], "temperature": 0.0, "avg_logprob": -0.18863968376640802, "compression_ratio": 1.8319327731092436, "no_speech_prob": 1.7342223145533353e-05}, {"id": 108, "seek": 56092, "start": 576.36, "end": 582.12, "text": " change there, and, you know, so much kind of like improvement over like large system,", "tokens": [1319, 456, 11, 293, 11, 291, 458, 11, 370, 709, 733, 295, 411, 10444, 670, 411, 2416, 1185, 11], "temperature": 0.0, "avg_logprob": -0.18863968376640802, "compression_ratio": 1.8319327731092436, "no_speech_prob": 1.7342223145533353e-05}, {"id": 109, "seek": 56092, "start": 582.12, "end": 588.4399999999999, "text": " so sometimes, sometimes there are like, very easy ways that we can just pick it up and", "tokens": [370, 2171, 11, 2171, 456, 366, 411, 11, 588, 1858, 2098, 300, 321, 393, 445, 1888, 309, 493, 293], "temperature": 0.0, "avg_logprob": -0.18863968376640802, "compression_ratio": 1.8319327731092436, "no_speech_prob": 1.7342223145533353e-05}, {"id": 110, "seek": 58844, "start": 588.44, "end": 594.12, "text": " just do it, right, but we need to know how, right, so kind of two learnings from the story,", "tokens": [445, 360, 309, 11, 558, 11, 457, 321, 643, 281, 458, 577, 11, 558, 11, 370, 733, 295, 732, 2539, 82, 490, 264, 1657, 11], "temperature": 0.0, "avg_logprob": -0.15766896786897078, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.0920560271188151e-05}, {"id": 111, "seek": 58844, "start": 594.12, "end": 599.9200000000001, "text": " one is that software efficiency on code level and algorithms, so changing code, you know,", "tokens": [472, 307, 300, 4722, 10493, 322, 3089, 1496, 293, 14642, 11, 370, 4473, 3089, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15766896786897078, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.0920560271188151e-05}, {"id": 112, "seek": 58844, "start": 599.9200000000001, "end": 606.44, "text": " matters, and learning how to do it can be, can be useful, and second learning is that", "tokens": [7001, 11, 293, 2539, 577, 281, 360, 309, 393, 312, 11, 393, 312, 4420, 11, 293, 1150, 2539, 307, 300], "temperature": 0.0, "avg_logprob": -0.15766896786897078, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.0920560271188151e-05}, {"id": 113, "seek": 58844, "start": 606.44, "end": 612.6800000000001, "text": " there is common pitfall, I think, generally in the, in this years, because in the past", "tokens": [456, 307, 2689, 10147, 6691, 11, 286, 519, 11, 5101, 294, 264, 11, 294, 341, 924, 11, 570, 294, 264, 1791], "temperature": 0.0, "avg_logprob": -0.15766896786897078, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.0920560271188151e-05}, {"id": 114, "seek": 58844, "start": 612.6800000000001, "end": 617.32, "text": " we have premature optimizations, everybody was playing with the code and trying to over-optimize", "tokens": [321, 362, 34877, 5028, 14455, 11, 2201, 390, 2433, 365, 264, 3089, 293, 1382, 281, 670, 12, 5747, 43890], "temperature": 0.0, "avg_logprob": -0.15766896786897078, "compression_ratio": 1.7413127413127414, "no_speech_prob": 1.0920560271188151e-05}, {"id": 115, "seek": 61732, "start": 617.32, "end": 622.44, "text": " things, I think now we are lazy and we are more like into DevOps, into changing, you", "tokens": [721, 11, 286, 519, 586, 321, 366, 14847, 293, 321, 366, 544, 411, 666, 43051, 11, 666, 4473, 11, 291], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 116, "seek": 61732, "start": 622.44, "end": 627.24, "text": " know, configuration, into horizontal scaling because we have this power, we have cloud,", "tokens": [458, 11, 11694, 11, 666, 12750, 21589, 570, 321, 362, 341, 1347, 11, 321, 362, 4588, 11], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 117, "seek": 61732, "start": 627.24, "end": 631.24, "text": " and this is usually, you know, more chosen solution than actually checking the code,", "tokens": [293, 341, 307, 2673, 11, 291, 458, 11, 544, 8614, 3827, 813, 767, 8568, 264, 3089, 11], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 118, "seek": 61732, "start": 631.24, "end": 636.6400000000001, "text": " right, and I call it closed box thinking, and I think this is a threat a little bit", "tokens": [558, 11, 293, 286, 818, 309, 5395, 2424, 1953, 11, 293, 286, 519, 341, 307, 257, 4734, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 119, "seek": 61732, "start": 636.6400000000001, "end": 641.48, "text": " in our ecosystem, so we should acknowledge that there are different levels, we can sometimes", "tokens": [294, 527, 11311, 11, 370, 321, 820, 10692, 300, 456, 366, 819, 4358, 11, 321, 393, 2171], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 120, "seek": 61732, "start": 641.48, "end": 645.6, "text": " scale, we can sometimes put more bigger machine, we can sometimes throw right to rust, if that", "tokens": [4373, 11, 321, 393, 2171, 829, 544, 3801, 3479, 11, 321, 393, 2171, 3507, 558, 281, 15259, 11, 498, 300], "temperature": 0.0, "avg_logprob": -0.17695169067382813, "compression_ratio": 1.8241379310344827, "no_speech_prob": 3.3796444768086076e-05}, {"id": 121, "seek": 64560, "start": 645.6, "end": 650.36, "text": " makes sense, but you know, that's not the first solution that should come to your mind,", "tokens": [1669, 2020, 11, 457, 291, 458, 11, 300, 311, 406, 264, 700, 3827, 300, 820, 808, 281, 428, 1575, 11], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 122, "seek": 64560, "start": 650.36, "end": 651.36, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 123, "seek": 64560, "start": 651.36, "end": 655.96, "text": " Okay, before we go forward, I will, I have five books to share, and I will start the", "tokens": [1033, 11, 949, 321, 352, 2128, 11, 286, 486, 11, 286, 362, 1732, 3642, 281, 2073, 11, 293, 286, 486, 722, 264], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 124, "seek": 64560, "start": 655.96, "end": 660.88, "text": " link to quiz at the end, and it will be super simple, but pay attention, right, because", "tokens": [2113, 281, 15450, 412, 264, 917, 11, 293, 309, 486, 312, 1687, 2199, 11, 457, 1689, 3202, 11, 558, 11, 570], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 125, "seek": 64560, "start": 660.88, "end": 666.64, "text": " maybe there will be some questions around, and you can answer, send me an email, and", "tokens": [1310, 456, 486, 312, 512, 1651, 926, 11, 293, 291, 393, 1867, 11, 2845, 385, 364, 3796, 11, 293], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 126, "seek": 64560, "start": 666.64, "end": 673.64, "text": " I will just, you know, just choose five people, lucky people, to have my book, so, yeah,", "tokens": [286, 486, 445, 11, 291, 458, 11, 445, 2826, 1732, 561, 11, 6356, 561, 11, 281, 362, 452, 1446, 11, 370, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.19833091047943616, "compression_ratio": 1.75, "no_speech_prob": 6.210403080331162e-05}, {"id": 127, "seek": 67364, "start": 673.64, "end": 683.76, "text": " pay attention, all right, five steps, five steps, yeah, for efficiency, efficiency progress.", "tokens": [1689, 3202, 11, 439, 558, 11, 1732, 4439, 11, 1732, 4439, 11, 1338, 11, 337, 10493, 11, 10493, 4205, 13], "temperature": 0.0, "avg_logprob": -0.24615627786387567, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00010214356007054448}, {"id": 128, "seek": 67364, "start": 683.76, "end": 688.6, "text": " One thing I want to mention, I don't know if you have been in the previous talk, or like", "tokens": [1485, 551, 286, 528, 281, 2152, 11, 286, 500, 380, 458, 498, 291, 362, 668, 294, 264, 3894, 751, 11, 420, 411], "temperature": 0.0, "avg_logprob": -0.24615627786387567, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00010214356007054448}, {"id": 129, "seek": 67364, "start": 688.6, "end": 697.0, "text": " before previous, he kind of explained a lot of optimization ideas, like I think, and I", "tokens": [949, 3894, 11, 415, 733, 295, 8825, 257, 688, 295, 19618, 3487, 11, 411, 286, 519, 11, 293, 286], "temperature": 0.0, "avg_logprob": -0.24615627786387567, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00010214356007054448}, {"id": 130, "seek": 69700, "start": 697.0, "end": 703.44, "text": " might say before, like he mentioned, string optimizations with internings, has just mentioned,", "tokens": [1062, 584, 949, 11, 411, 415, 2835, 11, 6798, 5028, 14455, 365, 2154, 1109, 11, 575, 445, 2835, 11], "temperature": 0.0, "avg_logprob": -0.2658535423925367, "compression_ratio": 1.8221343873517786, "no_speech_prob": 2.5569584977347404e-05}, {"id": 131, "seek": 69700, "start": 703.44, "end": 711.28, "text": " I think, something around, you know, allocations, and many kind of like, I think, padding,", "tokens": [286, 519, 11, 746, 926, 11, 291, 458, 11, 12660, 763, 11, 293, 867, 733, 295, 411, 11, 286, 519, 11, 39562, 11], "temperature": 0.0, "avg_logprob": -0.2658535423925367, "compression_ratio": 1.8221343873517786, "no_speech_prob": 2.5569584977347404e-05}, {"id": 132, "seek": 69700, "start": 711.28, "end": 717.12, "text": " strike padding, and generally, you know, all those kind of ideas, this is fine, but it's", "tokens": [9302, 39562, 11, 293, 5101, 11, 291, 458, 11, 439, 729, 733, 295, 3487, 11, 341, 307, 2489, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.2658535423925367, "compression_ratio": 1.8221343873517786, "no_speech_prob": 2.5569584977347404e-05}, {"id": 133, "seek": 69700, "start": 717.12, "end": 721.92, "text": " optimizing stuff, it's not like looking through dictionary of things I did in the past, it's", "tokens": [40425, 1507, 11, 309, 311, 406, 411, 1237, 807, 25890, 295, 721, 286, 630, 294, 264, 1791, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.2658535423925367, "compression_ratio": 1.8221343873517786, "no_speech_prob": 2.5569584977347404e-05}, {"id": 134, "seek": 69700, "start": 721.92, "end": 726.64, "text": " kind of more fuzzy, more involved, so what I would like you to focus, it's not all particular", "tokens": [733, 295, 544, 34710, 11, 544, 3288, 11, 370, 437, 286, 576, 411, 291, 281, 1879, 11, 309, 311, 406, 439, 1729], "temperature": 0.0, "avg_logprob": -0.2658535423925367, "compression_ratio": 1.8221343873517786, "no_speech_prob": 2.5569584977347404e-05}, {"id": 135, "seek": 72664, "start": 726.64, "end": 732.12, "text": " way of how we optimize an example I would show, because it's super simple and trivial,", "tokens": [636, 295, 577, 321, 19719, 364, 1365, 286, 576, 855, 11, 570, 309, 311, 1687, 2199, 293, 26703, 11], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 136, "seek": 72664, "start": 732.12, "end": 737.04, "text": " but how we get there, right, how we found what to optimize, how we found if we should", "tokens": [457, 577, 321, 483, 456, 11, 558, 11, 577, 321, 1352, 437, 281, 19719, 11, 577, 321, 1352, 498, 321, 820], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 137, "seek": 72664, "start": 737.04, "end": 739.8, "text": " even optimize, okay, so focus on that.", "tokens": [754, 19719, 11, 1392, 11, 370, 1879, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 138, "seek": 72664, "start": 739.8, "end": 744.28, "text": " So first step, first suggestion I would have, and this is from Book, I kind of found, yeah,", "tokens": [407, 700, 1823, 11, 700, 16541, 286, 576, 362, 11, 293, 341, 307, 490, 9476, 11, 286, 733, 295, 1352, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 139, "seek": 72664, "start": 744.28, "end": 750.4399999999999, "text": " I don't know, like I defined this name TFBO, which is essentially a flow for development,", "tokens": [286, 500, 380, 458, 11, 411, 286, 7642, 341, 1315, 40964, 15893, 11, 597, 307, 4476, 257, 3095, 337, 3250, 11], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 140, "seek": 72664, "start": 750.4399999999999, "end": 754.88, "text": " efficiency aware development that worked for me, and generally I see other professionals", "tokens": [10493, 3650, 3250, 300, 2732, 337, 385, 11, 293, 5101, 286, 536, 661, 11954], "temperature": 0.0, "avg_logprob": -0.18931886016345414, "compression_ratio": 1.7031802120141342, "no_speech_prob": 1.275003160117194e-05}, {"id": 141, "seek": 75488, "start": 754.88, "end": 761.28, "text": " doing that a lot as well, so test, fix, benchmark, optimize, so essentially what it is, it's", "tokens": [884, 300, 257, 688, 382, 731, 11, 370, 1500, 11, 3191, 11, 18927, 11, 19719, 11, 370, 4476, 437, 309, 307, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1333501974741618, "compression_ratio": 1.8095238095238095, "no_speech_prob": 9.052686436916701e-06}, {"id": 142, "seek": 75488, "start": 761.28, "end": 766.76, "text": " like a TDD with something else, and TDD you are probably familiar with, test-driven development,", "tokens": [411, 257, 314, 20818, 365, 746, 1646, 11, 293, 314, 20818, 291, 366, 1391, 4963, 365, 11, 1500, 12, 25456, 3250, 11], "temperature": 0.0, "avg_logprob": -0.1333501974741618, "compression_ratio": 1.8095238095238095, "no_speech_prob": 9.052686436916701e-06}, {"id": 143, "seek": 75488, "start": 766.76, "end": 772.16, "text": " you test first, as you can see, and only then you kind of like implement or fix it until", "tokens": [291, 1500, 700, 11, 382, 291, 393, 536, 11, 293, 787, 550, 291, 733, 295, 411, 4445, 420, 3191, 309, 1826], "temperature": 0.0, "avg_logprob": -0.1333501974741618, "compression_ratio": 1.8095238095238095, "no_speech_prob": 9.052686436916701e-06}, {"id": 144, "seek": 75488, "start": 772.16, "end": 777.32, "text": " the test is passing, right, I would like to kind of do the same for optimizations as", "tokens": [264, 1500, 307, 8437, 11, 558, 11, 286, 576, 411, 281, 733, 295, 360, 264, 912, 337, 5028, 14455, 382], "temperature": 0.0, "avg_logprob": -0.1333501974741618, "compression_ratio": 1.8095238095238095, "no_speech_prob": 9.052686436916701e-06}, {"id": 145, "seek": 75488, "start": 777.32, "end": 783.8, "text": " well, so we have benchmark-driven optimizations, because as you can see, we benchmark first,", "tokens": [731, 11, 370, 321, 362, 18927, 12, 25456, 5028, 14455, 11, 570, 382, 291, 393, 536, 11, 321, 18927, 700, 11], "temperature": 0.0, "avg_logprob": -0.1333501974741618, "compression_ratio": 1.8095238095238095, "no_speech_prob": 9.052686436916701e-06}, {"id": 146, "seek": 78380, "start": 783.8, "end": 789.64, "text": " then we optimize, and then we profile, right, and I will tell you later why, but all of", "tokens": [550, 321, 19719, 11, 293, 550, 321, 7964, 11, 558, 11, 293, 286, 486, 980, 291, 1780, 983, 11, 457, 439, 295], "temperature": 0.0, "avg_logprob": -0.16309251137150144, "compression_ratio": 1.662280701754386, "no_speech_prob": 1.4243344594433438e-05}, {"id": 147, "seek": 78380, "start": 789.64, "end": 795.24, "text": " this is a closed loop, right, so after optimizations we have to test as well, okay, so it feels", "tokens": [341, 307, 257, 5395, 6367, 11, 558, 11, 370, 934, 5028, 14455, 321, 362, 281, 1500, 382, 731, 11, 1392, 11, 370, 309, 3417], "temperature": 0.0, "avg_logprob": -0.16309251137150144, "compression_ratio": 1.662280701754386, "no_speech_prob": 1.4243344594433438e-05}, {"id": 148, "seek": 78380, "start": 795.24, "end": 802.0799999999999, "text": " complex, but we'll make one loop, actually maybe two, during the stock on a simple code,", "tokens": [3997, 11, 457, 321, 603, 652, 472, 6367, 11, 767, 1310, 732, 11, 1830, 264, 4127, 322, 257, 2199, 3089, 11], "temperature": 0.0, "avg_logprob": -0.16309251137150144, "compression_ratio": 1.662280701754386, "no_speech_prob": 1.4243344594433438e-05}, {"id": 149, "seek": 78380, "start": 802.0799999999999, "end": 803.3599999999999, "text": " so let's do it.", "tokens": [370, 718, 311, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.16309251137150144, "compression_ratio": 1.662280701754386, "no_speech_prob": 1.4243344594433438e-05}, {"id": 150, "seek": 78380, "start": 803.3599999999999, "end": 810.4399999999999, "text": " So let's introduce a simple function, super simple, super stupid, we are creating millions", "tokens": [407, 718, 311, 5366, 257, 2199, 2445, 11, 1687, 2199, 11, 1687, 6631, 11, 321, 366, 4084, 6803], "temperature": 0.0, "avg_logprob": -0.16309251137150144, "compression_ratio": 1.662280701754386, "no_speech_prob": 1.4243344594433438e-05}, {"id": 151, "seek": 81044, "start": 810.44, "end": 814.44, "text": " of elements, I mean, a slice with millions of elements, and each of those elements are", "tokens": [295, 4959, 11, 286, 914, 11, 257, 13153, 365, 6803, 295, 4959, 11, 293, 1184, 295, 729, 4959, 366], "temperature": 0.0, "avg_logprob": -0.16263971249919293, "compression_ratio": 1.72265625, "no_speech_prob": 5.394730396801606e-05}, {"id": 152, "seek": 81044, "start": 814.44, "end": 821.8000000000001, "text": " just a string, a constant string for them, super simple, it's the first, you know, kind", "tokens": [445, 257, 6798, 11, 257, 5754, 6798, 337, 552, 11, 1687, 2199, 11, 309, 311, 264, 700, 11, 291, 458, 11, 733], "temperature": 0.0, "avg_logprob": -0.16263971249919293, "compression_ratio": 1.72265625, "no_speech_prob": 5.394730396801606e-05}, {"id": 153, "seek": 81044, "start": 821.8000000000001, "end": 827.4000000000001, "text": " of first iteration of this program we want to write, so what we do regarding TFBO, okay,", "tokens": [295, 700, 24784, 295, 341, 1461, 321, 528, 281, 2464, 11, 370, 437, 321, 360, 8595, 40964, 15893, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.16263971249919293, "compression_ratio": 1.72265625, "no_speech_prob": 5.394730396801606e-05}, {"id": 154, "seek": 81044, "start": 827.4000000000001, "end": 832.6800000000001, "text": " so we test, right, I mean, now we have a code, for example, and we want to maybe improve", "tokens": [370, 321, 1500, 11, 558, 11, 286, 914, 11, 586, 321, 362, 257, 3089, 11, 337, 1365, 11, 293, 321, 528, 281, 1310, 3470], "temperature": 0.0, "avg_logprob": -0.16263971249919293, "compression_ratio": 1.72265625, "no_speech_prob": 5.394730396801606e-05}, {"id": 155, "seek": 81044, "start": 832.6800000000001, "end": 837.1600000000001, "text": " it, we test, test-driven development, so let's assume I already had the test, right, but", "tokens": [309, 11, 321, 1500, 11, 1500, 12, 25456, 3250, 11, 370, 718, 311, 6552, 286, 1217, 632, 264, 1500, 11, 558, 11, 457], "temperature": 0.0, "avg_logprob": -0.16263971249919293, "compression_ratio": 1.72265625, "no_speech_prob": 5.394730396801606e-05}, {"id": 156, "seek": 83716, "start": 837.16, "end": 843.16, "text": " the test could look like this, and then, you know, I'm ensuring, okay, it's passing,", "tokens": [264, 1500, 727, 574, 411, 341, 11, 293, 550, 11, 291, 458, 11, 286, 478, 16882, 11, 1392, 11, 309, 311, 8437, 11], "temperature": 0.0, "avg_logprob": -0.16286762782505582, "compression_ratio": 1.7049180327868851, "no_speech_prob": 8.112147042993456e-06}, {"id": 157, "seek": 83716, "start": 843.16, "end": 848.76, "text": " so nothing functionally I have to fix, so what next?", "tokens": [370, 1825, 2445, 379, 286, 362, 281, 3191, 11, 370, 437, 958, 30], "temperature": 0.0, "avg_logprob": -0.16286762782505582, "compression_ratio": 1.7049180327868851, "no_speech_prob": 8.112147042993456e-06}, {"id": 158, "seek": 83716, "start": 848.76, "end": 852.88, "text": " So next is this measurement, it's a benchmark, and again, has this already mentioned how", "tokens": [407, 958, 307, 341, 13160, 11, 309, 311, 257, 18927, 11, 293, 797, 11, 575, 341, 1217, 2835, 577], "temperature": 0.0, "avg_logprob": -0.16286762782505582, "compression_ratio": 1.7049180327868851, "no_speech_prob": 8.112147042993456e-06}, {"id": 159, "seek": 83716, "start": 852.88, "end": 859.8, "text": " to make benchmarks, but I have some additions, extensions to that that you might find helpful,", "tokens": [281, 652, 43751, 11, 457, 286, 362, 512, 35113, 11, 25129, 281, 300, 300, 291, 1062, 915, 4961, 11], "temperature": 0.0, "avg_logprob": -0.16286762782505582, "compression_ratio": 1.7049180327868851, "no_speech_prob": 8.112147042993456e-06}, {"id": 160, "seek": 83716, "start": 859.8, "end": 864.88, "text": " something I want to mention is that, you know, we were talking about micro benchmarks, because", "tokens": [746, 286, 528, 281, 2152, 307, 300, 11, 291, 458, 11, 321, 645, 1417, 466, 4532, 43751, 11, 570], "temperature": 0.0, "avg_logprob": -0.16286762782505582, "compression_ratio": 1.7049180327868851, "no_speech_prob": 8.112147042993456e-06}, {"id": 161, "seek": 86488, "start": 864.88, "end": 870.4, "text": " the same level of testing behavior, like for example, like for this small function, like", "tokens": [264, 912, 1496, 295, 4997, 5223, 11, 411, 337, 1365, 11, 411, 337, 341, 1359, 2445, 11, 411], "temperature": 0.0, "avg_logprob": -0.19458500406016474, "compression_ratio": 1.8828451882845187, "no_speech_prob": 1.3134446817275602e-05}, {"id": 162, "seek": 86488, "start": 870.4, "end": 876.96, "text": " we have this create, you know, unit test is totally enough, right, this is on micro level,", "tokens": [321, 362, 341, 1884, 11, 291, 458, 11, 4985, 1500, 307, 3879, 1547, 11, 558, 11, 341, 307, 322, 4532, 1496, 11], "temperature": 0.0, "avg_logprob": -0.19458500406016474, "compression_ratio": 1.8828451882845187, "no_speech_prob": 1.3134446817275602e-05}, {"id": 163, "seek": 86488, "start": 876.96, "end": 880.0, "text": " we are making just unit test, it's fine, but sometimes if you have a bigger system, you", "tokens": [321, 366, 1455, 445, 4985, 1500, 11, 309, 311, 2489, 11, 457, 2171, 498, 291, 362, 257, 3801, 1185, 11, 291], "temperature": 0.0, "avg_logprob": -0.19458500406016474, "compression_ratio": 1.8828451882845187, "no_speech_prob": 1.3134446817275602e-05}, {"id": 164, "seek": 86488, "start": 880.0, "end": 885.2, "text": " need to do something on macro level, like integration test, end-to-end test, whatever", "tokens": [643, 281, 360, 746, 322, 18887, 1496, 11, 411, 10980, 1500, 11, 917, 12, 1353, 12, 521, 1500, 11, 2035], "temperature": 0.0, "avg_logprob": -0.19458500406016474, "compression_ratio": 1.8828451882845187, "no_speech_prob": 1.3134446817275602e-05}, {"id": 165, "seek": 86488, "start": 885.2, "end": 889.56, "text": " bigger, right, and the same happens in a benchmark, right, this is micro benchmark, this is kind", "tokens": [3801, 11, 558, 11, 293, 264, 912, 2314, 294, 257, 18927, 11, 558, 11, 341, 307, 4532, 18927, 11, 341, 307, 733], "temperature": 0.0, "avg_logprob": -0.19458500406016474, "compression_ratio": 1.8828451882845187, "no_speech_prob": 1.3134446817275602e-05}, {"id": 166, "seek": 88956, "start": 889.56, "end": 897.28, "text": " of unit benchmark, there are also micro benchmarks I covered in my book, and then you need to", "tokens": [295, 4985, 18927, 11, 456, 366, 611, 4532, 43751, 286, 5343, 294, 452, 1446, 11, 293, 550, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.1949409076145717, "compression_ratio": 1.8142292490118577, "no_speech_prob": 1.6588837752351537e-05}, {"id": 167, "seek": 88956, "start": 897.28, "end": 902.4, "text": " have more sophisticated kind of setup with low testing, with maybe some automation, with", "tokens": [362, 544, 16950, 733, 295, 8657, 365, 2295, 4997, 11, 365, 1310, 512, 17769, 11, 365], "temperature": 0.0, "avg_logprob": -0.1949409076145717, "compression_ratio": 1.8142292490118577, "no_speech_prob": 1.6588837752351537e-05}, {"id": 168, "seek": 88956, "start": 902.4, "end": 907.52, "text": " some observability, like, you know, Prometheus, maybe, which measures over time some resources,", "tokens": [512, 9951, 2310, 11, 411, 11, 291, 458, 11, 2114, 649, 42209, 11, 1310, 11, 597, 8000, 670, 565, 512, 3593, 11], "temperature": 0.0, "avg_logprob": -0.1949409076145717, "compression_ratio": 1.8142292490118577, "no_speech_prob": 1.6588837752351537e-05}, {"id": 169, "seek": 88956, "start": 907.52, "end": 912.3199999999999, "text": " but here we can, we have a simple unit create function, we can just make it simple with", "tokens": [457, 510, 321, 393, 11, 321, 362, 257, 2199, 4985, 1884, 2445, 11, 321, 393, 445, 652, 309, 2199, 365], "temperature": 0.0, "avg_logprob": -0.1949409076145717, "compression_ratio": 1.8142292490118577, "no_speech_prob": 1.6588837752351537e-05}, {"id": 170, "seek": 88956, "start": 912.3199999999999, "end": 916.56, "text": " micro benchmarks, and, you know, it has already mentioned, but, you know, there is a special", "tokens": [4532, 43751, 11, 293, 11, 291, 458, 11, 309, 575, 1217, 2835, 11, 457, 11, 291, 458, 11, 456, 307, 257, 2121], "temperature": 0.0, "avg_logprob": -0.1949409076145717, "compression_ratio": 1.8142292490118577, "no_speech_prob": 1.6588837752351537e-05}, {"id": 171, "seek": 91656, "start": 916.56, "end": 922.1199999999999, "text": " signature in a test file you have to put, and then there are optional helpers, for example,", "tokens": [13397, 294, 257, 1500, 3991, 291, 362, 281, 829, 11, 293, 550, 456, 366, 17312, 854, 433, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.15581447056361608, "compression_ratio": 1.751937984496124, "no_speech_prob": 2.2448095478466712e-05}, {"id": 172, "seek": 91656, "start": 922.1199999999999, "end": 926.88, "text": " that I like actually to put almost everywhere, report allocs, which is by default making", "tokens": [300, 286, 411, 767, 281, 829, 1920, 5315, 11, 2275, 12660, 82, 11, 597, 307, 538, 7576, 1455], "temperature": 0.0, "avg_logprob": -0.15581447056361608, "compression_ratio": 1.751937984496124, "no_speech_prob": 2.2448095478466712e-05}, {"id": 173, "seek": 91656, "start": 926.88, "end": 931.56, "text": " sure that this function will measure allocations as well, and the reset timer, which is super", "tokens": [988, 300, 341, 2445, 486, 3481, 12660, 763, 382, 731, 11, 293, 264, 14322, 19247, 11, 597, 307, 1687], "temperature": 0.0, "avg_logprob": -0.15581447056361608, "compression_ratio": 1.751937984496124, "no_speech_prob": 2.2448095478466712e-05}, {"id": 174, "seek": 91656, "start": 931.56, "end": 937.1199999999999, "text": " cool because it resets the measurement, so anything before you allocate, you spend time", "tokens": [1627, 570, 309, 725, 1385, 264, 13160, 11, 370, 1340, 949, 291, 35713, 11, 291, 3496, 565], "temperature": 0.0, "avg_logprob": -0.15581447056361608, "compression_ratio": 1.751937984496124, "no_speech_prob": 2.2448095478466712e-05}, {"id": 175, "seek": 91656, "start": 937.1199999999999, "end": 942.3199999999999, "text": " on, it will be discarded from benchmark result, so benchmark will only focus on what will", "tokens": [322, 11, 309, 486, 312, 45469, 490, 18927, 1874, 11, 370, 18927, 486, 787, 1879, 322, 437, 486], "temperature": 0.0, "avg_logprob": -0.15581447056361608, "compression_ratio": 1.751937984496124, "no_speech_prob": 2.2448095478466712e-05}, {"id": 176, "seek": 94232, "start": 942.32, "end": 946.84, "text": " happen within this loop iteration, right?", "tokens": [1051, 1951, 341, 6367, 24784, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 177, "seek": 94232, "start": 946.84, "end": 950.84, "text": " And then this for loop, you cannot change it, don't try to change it, always copy, this", "tokens": [400, 550, 341, 337, 6367, 11, 291, 2644, 1319, 309, 11, 500, 380, 853, 281, 1319, 309, 11, 1009, 5055, 11, 341], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 178, "seek": 94232, "start": 950.84, "end": 954.24, "text": " is a boilerplate that has to be there, right?", "tokens": [307, 257, 39228, 37008, 300, 575, 281, 312, 456, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 179, "seek": 94232, "start": 954.24, "end": 961.96, "text": " Because it allows Go to make repeatable, check the repeatability of your test by running", "tokens": [1436, 309, 4045, 1037, 281, 652, 7149, 712, 11, 1520, 264, 7149, 2310, 295, 428, 1500, 538, 2614], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 180, "seek": 94232, "start": 961.96, "end": 964.6400000000001, "text": " it, you know, hundreds of times.", "tokens": [309, 11, 291, 458, 11, 6779, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 181, "seek": 94232, "start": 964.6400000000001, "end": 970.12, "text": " Okay, so how we execute it, already, again, has this mentioned, but this is, you know,", "tokens": [1033, 11, 370, 577, 321, 14483, 309, 11, 1217, 11, 797, 11, 575, 341, 2835, 11, 457, 341, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.20846994867864646, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.660361340327654e-05}, {"id": 182, "seek": 97012, "start": 970.12, "end": 976.24, "text": " how I do it to, like, focus to one test, but this is not enough, in my opinion, right?", "tokens": [577, 286, 360, 309, 281, 11, 411, 11, 1879, 281, 472, 1500, 11, 457, 341, 307, 406, 1547, 11, 294, 452, 4800, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18079231403492116, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.791234899428673e-05}, {"id": 183, "seek": 97012, "start": 976.24, "end": 980.16, "text": " By default, it runs only one test, one second.", "tokens": [3146, 7576, 11, 309, 6676, 787, 472, 1500, 11, 472, 1150, 13], "temperature": 0.0, "avg_logprob": -0.18079231403492116, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.791234899428673e-05}, {"id": 184, "seek": 97012, "start": 980.16, "end": 985.92, "text": " I recommend to actually make sure you explicitly state some parameters, right?", "tokens": [286, 2748, 281, 767, 652, 988, 291, 20803, 1785, 512, 9834, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18079231403492116, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.791234899428673e-05}, {"id": 185, "seek": 97012, "start": 985.92, "end": 993.36, "text": " And I have one liner, one liner in bash, for example, that I often use, so what it is essentially,", "tokens": [400, 286, 362, 472, 24468, 11, 472, 24468, 294, 46183, 11, 337, 1365, 11, 300, 286, 2049, 764, 11, 370, 437, 309, 307, 4476, 11], "temperature": 0.0, "avg_logprob": -0.18079231403492116, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.791234899428673e-05}, {"id": 186, "seek": 97012, "start": 993.36, "end": 998.36, "text": " I'm kind of creating some variables so I can reference this result later on in a short-term", "tokens": [286, 478, 733, 295, 4084, 512, 9102, 370, 286, 393, 6408, 341, 1874, 1780, 322, 294, 257, 2099, 12, 7039], "temperature": 0.0, "avg_logprob": -0.18079231403492116, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.791234899428673e-05}, {"id": 187, "seek": 99836, "start": 998.36, "end": 1007.12, "text": " future, V1, for example, so this will create a V1.txt file in my locale, it will run this", "tokens": [2027, 11, 691, 16, 11, 337, 1365, 11, 370, 341, 486, 1884, 257, 691, 16, 13, 83, 734, 3991, 294, 452, 1628, 1220, 11, 309, 486, 1190, 341], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 188, "seek": 99836, "start": 1007.12, "end": 1012.6800000000001, "text": " benchmark, it will actually run it, you know, sometime, I specify, again, which is super", "tokens": [18927, 11, 309, 486, 767, 1190, 309, 11, 291, 458, 11, 15053, 11, 286, 16500, 11, 797, 11, 597, 307, 1687], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 189, "seek": 99836, "start": 1012.6800000000001, "end": 1017.04, "text": " amazing because it was like, okay, so I have this V1 file, what I was doing with it, and", "tokens": [2243, 570, 309, 390, 411, 11, 1392, 11, 370, 286, 362, 341, 691, 16, 3991, 11, 437, 286, 390, 884, 365, 309, 11, 293], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 190, "seek": 99836, "start": 1017.04, "end": 1020.44, "text": " then you check in your bash history, okay, oh, that was one second, and then that was", "tokens": [550, 291, 1520, 294, 428, 46183, 2503, 11, 1392, 11, 1954, 11, 300, 390, 472, 1150, 11, 293, 550, 300, 390], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 191, "seek": 99836, "start": 1020.44, "end": 1023.08, "text": " something else, right, so it's kind of useful.", "tokens": [746, 1646, 11, 558, 11, 370, 309, 311, 733, 295, 4420, 13], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 192, "seek": 99836, "start": 1023.08, "end": 1027.72, "text": " And then this is crucial, this is something I don't know why I didn't learn in the beginning,", "tokens": [400, 550, 341, 307, 11462, 11, 341, 307, 746, 286, 500, 380, 458, 983, 286, 994, 380, 1466, 294, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.18683481888032297, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6171235984074883e-05}, {"id": 193, "seek": 102772, "start": 1027.72, "end": 1030.56, "text": " maybe you learned the count, dash count, right?", "tokens": [1310, 291, 3264, 264, 1207, 11, 8240, 1207, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1699042039759019, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.6899522961466573e-05}, {"id": 194, "seek": 102772, "start": 1030.56, "end": 1036.52, "text": " So what it is, is that it runs the same test couple of times, six times, actually, and", "tokens": [407, 437, 309, 307, 11, 307, 300, 309, 6676, 264, 912, 1500, 1916, 295, 1413, 11, 2309, 1413, 11, 767, 11, 293], "temperature": 0.0, "avg_logprob": -0.1699042039759019, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.6899522961466573e-05}, {"id": 195, "seek": 102772, "start": 1036.52, "end": 1041.16, "text": " so one second, six times, and this is super important because then you can use further", "tokens": [370, 472, 1150, 11, 2309, 1413, 11, 293, 341, 307, 1687, 1021, 570, 550, 291, 393, 764, 3052], "temperature": 0.0, "avg_logprob": -0.1699042039759019, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.6899522961466573e-05}, {"id": 196, "seek": 102772, "start": 1041.16, "end": 1047.6000000000001, "text": " tools you will see to check, you know, how reliable are your results, it will essentially", "tokens": [3873, 291, 486, 536, 281, 1520, 11, 291, 458, 11, 577, 12924, 366, 428, 3542, 11, 309, 486, 4476], "temperature": 0.0, "avg_logprob": -0.1699042039759019, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.6899522961466573e-05}, {"id": 197, "seek": 102772, "start": 1047.6000000000001, "end": 1052.92, "text": " calculate the variance between the, you know, the timings, for example, so if the variance", "tokens": [8873, 264, 21977, 1296, 264, 11, 291, 458, 11, 264, 524, 1109, 11, 337, 1365, 11, 370, 498, 264, 21977], "temperature": 0.0, "avg_logprob": -0.1699042039759019, "compression_ratio": 1.7402597402597402, "no_speech_prob": 1.6899522961466573e-05}, {"id": 198, "seek": 105292, "start": 1052.92, "end": 1057.4, "text": " is too big, then your environment is not stable, right?", "tokens": [307, 886, 955, 11, 550, 428, 2823, 307, 406, 8351, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 199, "seek": 105292, "start": 1057.4, "end": 1063.4, "text": " And then I pin to one CPU, this is super important to, generally pinning, not to one, right?", "tokens": [400, 550, 286, 5447, 281, 472, 13199, 11, 341, 307, 1687, 1021, 281, 11, 5101, 5447, 773, 11, 406, 281, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 200, "seek": 105292, "start": 1063.4, "end": 1067.3600000000001, "text": " Just pick something that works for you, for concurrency, pick something that runs on production", "tokens": [1449, 1888, 746, 300, 1985, 337, 291, 11, 337, 23702, 10457, 11, 1888, 746, 300, 6676, 322, 4265], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 201, "seek": 105292, "start": 1067.3600000000001, "end": 1071.24, "text": " maybe, or similar, but always between tests, don't change that, right?", "tokens": [1310, 11, 420, 2531, 11, 457, 1009, 1296, 6921, 11, 500, 380, 1319, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 202, "seek": 105292, "start": 1071.24, "end": 1075.8400000000001, "text": " So that's super important, and also I recommend to change less than numbers of CPU because", "tokens": [407, 300, 311, 1687, 1021, 11, 293, 611, 286, 2748, 281, 1319, 1570, 813, 3547, 295, 13199, 570], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 203, "seek": 105292, "start": 1075.8400000000001, "end": 1078.24, "text": " your operating system has to run on something, right?", "tokens": [428, 7447, 1185, 575, 281, 1190, 322, 746, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1450306104577106, "compression_ratio": 1.7228464419475655, "no_speech_prob": 1.031805823004106e-05}, {"id": 204, "seek": 107824, "start": 1078.24, "end": 1084.32, "text": " So those things matter, also don't run this on laptop without power connected because", "tokens": [407, 729, 721, 1871, 11, 611, 500, 380, 1190, 341, 322, 10732, 1553, 1347, 4582, 570], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 205, "seek": 107824, "start": 1084.32, "end": 1086.16, "text": " you will be CPU trolled off.", "tokens": [291, 486, 312, 13199, 20680, 292, 766, 13], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 206, "seek": 107824, "start": 1086.16, "end": 1089.44, "text": " There are lots of kind of small things that you think, oh, it doesn't matter, no, it matters", "tokens": [821, 366, 3195, 295, 733, 295, 1359, 721, 300, 291, 519, 11, 1954, 11, 309, 1177, 380, 1871, 11, 572, 11, 309, 7001], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 207, "seek": 107824, "start": 1089.44, "end": 1091.88, "text": " because then you cannot rely on your results, right?", "tokens": [570, 550, 291, 2644, 10687, 322, 428, 3542, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 208, "seek": 107824, "start": 1091.88, "end": 1097.72, "text": " So try to make this serious a little bit and at least, you know, don't put, don't benchmark", "tokens": [407, 853, 281, 652, 341, 3156, 257, 707, 857, 293, 412, 1935, 11, 291, 458, 11, 500, 380, 829, 11, 500, 380, 18927], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 209, "seek": 107824, "start": 1097.72, "end": 1100.8, "text": " on your lap, you know, in the bed, you know, because they will be overheating.", "tokens": [322, 428, 13214, 11, 291, 458, 11, 294, 264, 2901, 11, 291, 458, 11, 570, 436, 486, 312, 29807, 990, 13], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 210, "seek": 107824, "start": 1100.8, "end": 1103.96, "text": " So yeah, small things, but it matters, right?", "tokens": [407, 1338, 11, 1359, 721, 11, 457, 309, 7001, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 211, "seek": 107824, "start": 1103.96, "end": 1107.72, "text": " I was doing that all the time, by the way, yeah.", "tokens": [286, 390, 884, 300, 439, 264, 565, 11, 538, 264, 636, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.20460400516039703, "compression_ratio": 1.820069204152249, "no_speech_prob": 1.7120721167884767e-05}, {"id": 212, "seek": 110772, "start": 1107.72, "end": 1111.3600000000001, "text": " So results, you know, result looks like this.", "tokens": [407, 3542, 11, 291, 458, 11, 1874, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 213, "seek": 110772, "start": 1111.3600000000001, "end": 1113.64, "text": " You can see many of them.", "tokens": [509, 393, 536, 867, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 214, "seek": 110772, "start": 1113.64, "end": 1117.1200000000001, "text": " But this is not how I use it or how we supposed to use it, apparently.", "tokens": [583, 341, 307, 406, 577, 286, 764, 309, 420, 577, 321, 3442, 281, 764, 309, 11, 7970, 13], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 215, "seek": 110772, "start": 1117.1200000000001, "end": 1123.96, "text": " There is amazing tool called BenchStat, and it just brings in more human-readable way,", "tokens": [821, 307, 2243, 2290, 1219, 3964, 339, 4520, 267, 11, 293, 309, 445, 5607, 294, 544, 1952, 12, 2538, 712, 636, 11], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 216, "seek": 110772, "start": 1123.96, "end": 1130.1200000000001, "text": " and you can see it also aggregates and have some averages over those runs and tells you", "tokens": [293, 291, 393, 536, 309, 611, 16743, 1024, 293, 362, 512, 42257, 670, 729, 6676, 293, 5112, 291], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 217, "seek": 110772, "start": 1130.1200000000001, "end": 1131.44, "text": " within this percentage.", "tokens": [1951, 341, 9668, 13], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 218, "seek": 110772, "start": 1131.44, "end": 1137.6000000000001, "text": " For example, the time, latency, there is a variance of 1%, which is tolerable, for example,", "tokens": [1171, 1365, 11, 264, 565, 11, 27043, 11, 456, 307, 257, 21977, 295, 502, 8923, 597, 307, 11125, 712, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.19546138539033778, "compression_ratio": 1.6156716417910448, "no_speech_prob": 8.930765034165233e-06}, {"id": 219, "seek": 113760, "start": 1137.6, "end": 1138.6, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 220, "seek": 113760, "start": 1138.6, "end": 1142.8, "text": " And you can kind of like customize what exactly, how it calculates this variance and so on.", "tokens": [400, 291, 393, 733, 295, 411, 19734, 437, 2293, 11, 577, 309, 4322, 1024, 341, 21977, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 221, "seek": 113760, "start": 1142.8, "end": 1148.0, "text": " So we can trust it, like it's within 1% of, I guess, free, you could trust it, depend", "tokens": [407, 321, 393, 3361, 309, 11, 411, 309, 311, 1951, 502, 4, 295, 11, 286, 2041, 11, 1737, 11, 291, 727, 3361, 309, 11, 5672], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 222, "seek": 113760, "start": 1148.0, "end": 1151.6799999999998, "text": " on what you do, but generally it's not too bad.", "tokens": [322, 437, 291, 360, 11, 457, 5101, 309, 311, 406, 886, 1578, 13], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 223, "seek": 113760, "start": 1151.6799999999998, "end": 1154.0, "text": " Allocations, fortunately, are super stable, right?", "tokens": [1057, 905, 763, 11, 25511, 11, 366, 1687, 8351, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 224, "seek": 113760, "start": 1154.0, "end": 1159.3999999999999, "text": " So yeah, so we benchmark, we measure it, okay, we know our function has these numbers, like,", "tokens": [407, 1338, 11, 370, 321, 18927, 11, 321, 3481, 309, 11, 1392, 11, 321, 458, 527, 2445, 575, 613, 3547, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 225, "seek": 113760, "start": 1159.3999999999999, "end": 1162.12, "text": " I mean, what's next, right?", "tokens": [286, 914, 11, 437, 311, 958, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 226, "seek": 113760, "start": 1162.12, "end": 1165.36, "text": " Everybody was like, yeah, let's make it faster, let's make it faster, but wait, wait, wait,", "tokens": [7646, 390, 411, 11, 1338, 11, 718, 311, 652, 309, 4663, 11, 718, 311, 652, 309, 4663, 11, 457, 1699, 11, 1699, 11, 1699, 11], "temperature": 0.0, "avg_logprob": -0.21537823579749282, "compression_ratio": 1.710344827586207, "no_speech_prob": 3.2615193049423397e-05}, {"id": 227, "seek": 116536, "start": 1165.36, "end": 1171.8799999999999, "text": " why, why should we make it faster, maybe, okay, maybe that's a lot, 100 megabytes of", "tokens": [983, 11, 983, 820, 321, 652, 309, 4663, 11, 1310, 11, 1392, 11, 1310, 300, 311, 257, 688, 11, 2319, 10816, 24538, 295], "temperature": 0.0, "avg_logprob": -0.15970871665261008, "compression_ratio": 1.6652719665271967, "no_speech_prob": 1.3618570847029332e-05}, {"id": 228, "seek": 116536, "start": 1171.8799999999999, "end": 1176.08, "text": " every, you know, create invocation, but maybe that's fine, right?", "tokens": [633, 11, 291, 458, 11, 1884, 1048, 27943, 11, 457, 1310, 300, 311, 2489, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15970871665261008, "compression_ratio": 1.6652719665271967, "no_speech_prob": 1.3618570847029332e-05}, {"id": 229, "seek": 116536, "start": 1176.08, "end": 1180.08, "text": " So this is where I think we are missing a lot of experience, usually.", "tokens": [407, 341, 307, 689, 286, 519, 321, 366, 5361, 257, 688, 295, 1752, 11, 2673, 13], "temperature": 0.0, "avg_logprob": -0.15970871665261008, "compression_ratio": 1.6652719665271967, "no_speech_prob": 1.3618570847029332e-05}, {"id": 230, "seek": 116536, "start": 1180.08, "end": 1187.08, "text": " I mean, you have to set some expectations, right, like, to what point you are optimizing,", "tokens": [286, 914, 11, 291, 362, 281, 992, 512, 9843, 11, 558, 11, 411, 11, 281, 437, 935, 291, 366, 40425, 11], "temperature": 0.0, "avg_logprob": -0.15970871665261008, "compression_ratio": 1.6652719665271967, "no_speech_prob": 1.3618570847029332e-05}, {"id": 231, "seek": 116536, "start": 1187.08, "end": 1191.28, "text": " and usually we don't have any expectations, like, okay, yeah, I mean, even from product", "tokens": [293, 2673, 321, 500, 380, 362, 604, 9843, 11, 411, 11, 1392, 11, 1338, 11, 286, 914, 11, 754, 490, 1674], "temperature": 0.0, "avg_logprob": -0.15970871665261008, "compression_ratio": 1.6652719665271967, "no_speech_prob": 1.3618570847029332e-05}, {"id": 232, "seek": 119128, "start": 1191.28, "end": 1198.6, "text": " management here we have maybe functional requirements, but never really concrete performance requirements.", "tokens": [4592, 510, 321, 362, 1310, 11745, 7728, 11, 457, 1128, 534, 9859, 3389, 7728, 13], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 233, "seek": 119128, "start": 1198.6, "end": 1202.72, "text": " So we don't know what to do, and honestly, if you don't, you just ignore those requirements,", "tokens": [407, 321, 500, 380, 458, 437, 281, 360, 11, 293, 6095, 11, 498, 291, 500, 380, 11, 291, 445, 11200, 729, 7728, 11], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 234, "seek": 119128, "start": 1202.72, "end": 1207.0, "text": " okay, I don't have, I just want to make it faster, then this premature optimization is", "tokens": [1392, 11, 286, 500, 380, 362, 11, 286, 445, 528, 281, 652, 309, 4663, 11, 550, 341, 34877, 19618, 307], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 235, "seek": 119128, "start": 1207.0, "end": 1212.92, "text": " always, right, because it's always premature, because it's a random, a random goal you don't", "tokens": [1009, 11, 558, 11, 570, 309, 311, 1009, 34877, 11, 570, 309, 311, 257, 4974, 11, 257, 4974, 3387, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 236, "seek": 119128, "start": 1212.92, "end": 1217.76, "text": " really understand, right, so maybe, maybe just make it fast, right, that's also like", "tokens": [534, 1223, 11, 558, 11, 370, 1310, 11, 1310, 445, 652, 309, 2370, 11, 558, 11, 300, 311, 611, 411], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 237, "seek": 119128, "start": 1217.76, "end": 1220.28, "text": " very fuzzy, obviously, and that's not very helpful.", "tokens": [588, 34710, 11, 2745, 11, 293, 300, 311, 406, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.21714585924905444, "compression_ratio": 1.8695652173913044, "no_speech_prob": 1.884862649603747e-05}, {"id": 238, "seek": 122028, "start": 1220.28, "end": 1221.76, "text": " So what is helpful?", "tokens": [407, 437, 307, 4961, 30], "temperature": 0.0, "avg_logprob": -0.2168661117553711, "compression_ratio": 1.7534246575342465, "no_speech_prob": 1.318536487815436e-05}, {"id": 239, "seek": 122028, "start": 1221.76, "end": 1228.12, "text": " What I will, and I know it's super hard, I know it's kind of uncomfortable, but I suggest", "tokens": [708, 286, 486, 11, 293, 286, 458, 309, 311, 1687, 1152, 11, 286, 458, 309, 311, 733, 295, 10532, 11, 457, 286, 3402], "temperature": 0.0, "avg_logprob": -0.2168661117553711, "compression_ratio": 1.7534246575342465, "no_speech_prob": 1.318536487815436e-05}, {"id": 240, "seek": 122028, "start": 1228.12, "end": 1233.8799999999999, "text": " doing some kind of efficiency requirements, spec, super simple, as simple as possible,", "tokens": [884, 512, 733, 295, 10493, 7728, 11, 1608, 11, 1687, 2199, 11, 382, 2199, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.2168661117553711, "compression_ratio": 1.7534246575342465, "no_speech_prob": 1.318536487815436e-05}, {"id": 241, "seek": 122028, "start": 1233.8799999999999, "end": 1239.6, "text": " I call it rare, so there are efficiency requirements, and what it means is essentially try to find", "tokens": [286, 818, 309, 5892, 11, 370, 456, 366, 10493, 7728, 11, 293, 437, 309, 1355, 307, 4476, 853, 281, 915], "temperature": 0.0, "avg_logprob": -0.2168661117553711, "compression_ratio": 1.7534246575342465, "no_speech_prob": 1.318536487815436e-05}, {"id": 242, "seek": 122028, "start": 1239.6, "end": 1244.3999999999999, "text": " out some kind of function, right, some kind of, you know, complexity, but not as if it's", "tokens": [484, 512, 733, 295, 2445, 11, 558, 11, 512, 733, 295, 11, 291, 458, 11, 14024, 11, 457, 406, 382, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.2168661117553711, "compression_ratio": 1.7534246575342465, "no_speech_prob": 1.318536487815436e-05}, {"id": 243, "seek": 124440, "start": 1244.4, "end": 1250.0800000000002, "text": " very complex, it's just more concrete estimation of the complexity based on inputs, right,", "tokens": [588, 3997, 11, 309, 311, 445, 544, 9859, 35701, 295, 264, 14024, 2361, 322, 15743, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.1943127257483346, "compression_ratio": 1.7658730158730158, "no_speech_prob": 2.498282992746681e-05}, {"id": 244, "seek": 124440, "start": 1250.0800000000002, "end": 1255.2, "text": " and for simple functions, like for example, our function, we can estimate, you know, what", "tokens": [293, 337, 2199, 6828, 11, 411, 337, 1365, 11, 527, 2445, 11, 321, 393, 12539, 11, 291, 458, 11, 437], "temperature": 0.0, "avg_logprob": -0.1943127257483346, "compression_ratio": 1.7658730158730158, "no_speech_prob": 2.498282992746681e-05}, {"id": 245, "seek": 124440, "start": 1255.2, "end": 1261.52, "text": " in our minds we think should happen, roughly, right, so, you know, for runtime, we know", "tokens": [294, 527, 9634, 321, 519, 820, 1051, 11, 9810, 11, 558, 11, 370, 11, 291, 458, 11, 337, 34474, 11, 321, 458], "temperature": 0.0, "avg_logprob": -0.1943127257483346, "compression_ratio": 1.7658730158730158, "no_speech_prob": 2.498282992746681e-05}, {"id": 246, "seek": 124440, "start": 1261.52, "end": 1266.76, "text": " we, one million time we do something, we don't know how many now seconds, let's pick 30,", "tokens": [321, 11, 472, 2459, 565, 321, 360, 746, 11, 321, 500, 380, 458, 577, 867, 586, 3949, 11, 718, 311, 1888, 2217, 11], "temperature": 0.0, "avg_logprob": -0.1943127257483346, "compression_ratio": 1.7658730158730158, "no_speech_prob": 2.498282992746681e-05}, {"id": 247, "seek": 124440, "start": 1266.76, "end": 1271.4, "text": " this is actually pretty big for one iteration of just append, but just really pick some", "tokens": [341, 307, 767, 1238, 955, 337, 472, 24784, 295, 445, 34116, 11, 457, 445, 534, 1888, 512], "temperature": 0.0, "avg_logprob": -0.1943127257483346, "compression_ratio": 1.7658730158730158, "no_speech_prob": 2.498282992746681e-05}, {"id": 248, "seek": 127140, "start": 1271.4, "end": 1275.8400000000001, "text": " number, sometimes it's good, it's just, you know, you can iterate over this number, but", "tokens": [1230, 11, 2171, 309, 311, 665, 11, 309, 311, 445, 11, 291, 458, 11, 291, 393, 44497, 670, 341, 1230, 11, 457], "temperature": 0.0, "avg_logprob": -0.1569222125810446, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012077447900082916}, {"id": 249, "seek": 127140, "start": 1275.8400000000001, "end": 1280.92, "text": " if you don't know where you go, then, you know, how you can make any decisions, decisions.", "tokens": [498, 291, 500, 380, 458, 689, 291, 352, 11, 550, 11, 291, 458, 11, 577, 291, 393, 652, 604, 5327, 11, 5327, 13], "temperature": 0.0, "avg_logprob": -0.1569222125810446, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012077447900082916}, {"id": 250, "seek": 127140, "start": 1280.92, "end": 1288.64, "text": " In allocations, it's a little bit bitter, a little bit easier, because we expect a slice", "tokens": [682, 12660, 763, 11, 309, 311, 257, 707, 857, 13871, 11, 257, 707, 857, 3571, 11, 570, 321, 2066, 257, 13153], "temperature": 0.0, "avg_logprob": -0.1569222125810446, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012077447900082916}, {"id": 251, "seek": 127140, "start": 1288.64, "end": 1297.24, "text": " of six, of one million elements of strings, and as we learn from MachiTalk, every string", "tokens": [295, 2309, 11, 295, 472, 2459, 4959, 295, 13985, 11, 293, 382, 321, 1466, 490, 12089, 72, 33210, 11, 633, 6798], "temperature": 0.0, "avg_logprob": -0.1569222125810446, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.00012077447900082916}, {"id": 252, "seek": 129724, "start": 1297.24, "end": 1304.04, "text": " has these two parts, one part has 16 bits, which has length and capacity, or maybe capacity", "tokens": [575, 613, 732, 3166, 11, 472, 644, 575, 3165, 9239, 11, 597, 575, 4641, 293, 6042, 11, 420, 1310, 6042], "temperature": 0.0, "avg_logprob": -0.20319891370032445, "compression_ratio": 1.8319327731092436, "no_speech_prob": 4.06626240874175e-05}, {"id": 253, "seek": 129724, "start": 1304.04, "end": 1310.16, "text": " not, but then, yeah, length capacity and pointer, and then there's other parts, which", "tokens": [406, 11, 457, 550, 11, 1338, 11, 4641, 6042, 293, 23918, 11, 293, 550, 456, 311, 661, 3166, 11, 597], "temperature": 0.0, "avg_logprob": -0.20319891370032445, "compression_ratio": 1.8319327731092436, "no_speech_prob": 4.06626240874175e-05}, {"id": 254, "seek": 129724, "start": 1310.16, "end": 1316.0, "text": " lies in the heap, but for this, you know, 16 bytes, we can assume that we'll be 16,", "tokens": [9134, 294, 264, 33591, 11, 457, 337, 341, 11, 291, 458, 11, 3165, 36088, 11, 321, 393, 6552, 300, 321, 603, 312, 3165, 11], "temperature": 0.0, "avg_logprob": -0.20319891370032445, "compression_ratio": 1.8319327731092436, "no_speech_prob": 4.06626240874175e-05}, {"id": 255, "seek": 129724, "start": 1316.0, "end": 1320.44, "text": " right, so it's every element is 16 bytes, so now we just multiply, that's our function,", "tokens": [558, 11, 370, 309, 311, 633, 4478, 307, 3165, 36088, 11, 370, 586, 321, 445, 12972, 11, 300, 311, 527, 2445, 11], "temperature": 0.0, "avg_logprob": -0.20319891370032445, "compression_ratio": 1.8319327731092436, "no_speech_prob": 4.06626240874175e-05}, {"id": 256, "seek": 129724, "start": 1320.44, "end": 1325.36, "text": " that's what we all expect, right, and with this, we can, you know, kind of expect that", "tokens": [300, 311, 437, 321, 439, 2066, 11, 558, 11, 293, 365, 341, 11, 321, 393, 11, 291, 458, 11, 733, 295, 2066, 300], "temperature": 0.0, "avg_logprob": -0.20319891370032445, "compression_ratio": 1.8319327731092436, "no_speech_prob": 4.06626240874175e-05}, {"id": 257, "seek": 132536, "start": 1325.36, "end": 1331.12, "text": " every invocation of create should, you know, kind of allocate 15 megabytes, but what we", "tokens": [633, 1048, 27943, 295, 1884, 820, 11, 291, 458, 11, 733, 295, 35713, 2119, 10816, 24538, 11, 457, 437, 321], "temperature": 0.0, "avg_logprob": -0.14538883772052702, "compression_ratio": 1.76171875, "no_speech_prob": 1.818956297938712e-05}, {"id": 258, "seek": 132536, "start": 1331.12, "end": 1336.6399999999999, "text": " see, we allocate 80 megabytes, right, so already we see that, oh, there might be like easy", "tokens": [536, 11, 321, 35713, 4688, 10816, 24538, 11, 558, 11, 370, 1217, 321, 536, 300, 11, 1954, 11, 456, 1062, 312, 411, 1858], "temperature": 0.0, "avg_logprob": -0.14538883772052702, "compression_ratio": 1.76171875, "no_speech_prob": 1.818956297938712e-05}, {"id": 259, "seek": 132536, "start": 1336.6399999999999, "end": 1341.9599999999998, "text": " ways to do, or something I don't understand about this program, and this is what leads", "tokens": [2098, 281, 360, 11, 420, 746, 286, 500, 380, 1223, 466, 341, 1461, 11, 293, 341, 307, 437, 6689], "temperature": 0.0, "avg_logprob": -0.14538883772052702, "compression_ratio": 1.76171875, "no_speech_prob": 1.818956297938712e-05}, {"id": 260, "seek": 132536, "start": 1341.9599999999998, "end": 1348.6, "text": " us to better, to spotting maybe easy wins, and spotting, you know, if we need to do anything,", "tokens": [505, 281, 1101, 11, 281, 4008, 783, 1310, 1858, 10641, 11, 293, 4008, 783, 11, 291, 458, 11, 498, 321, 643, 281, 360, 1340, 11], "temperature": 0.0, "avg_logprob": -0.14538883772052702, "compression_ratio": 1.76171875, "no_speech_prob": 1.818956297938712e-05}, {"id": 261, "seek": 132536, "start": 1348.6, "end": 1353.8, "text": " right, in terms of time, latency, it's already kind of like, more than we kind of expected,", "tokens": [558, 11, 294, 2115, 295, 565, 11, 27043, 11, 309, 311, 1217, 733, 295, 411, 11, 544, 813, 321, 733, 295, 5176, 11], "temperature": 0.0, "avg_logprob": -0.14538883772052702, "compression_ratio": 1.76171875, "no_speech_prob": 1.818956297938712e-05}, {"id": 262, "seek": 135380, "start": 1353.8, "end": 1359.12, "text": " right, but this is more of a guessing, like I just guessed this 30 seconds, right, okay,", "tokens": [558, 11, 457, 341, 307, 544, 295, 257, 17939, 11, 411, 286, 445, 21852, 341, 2217, 3949, 11, 558, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.14469161260695684, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.0889337318076286e-05}, {"id": 263, "seek": 135380, "start": 1359.12, "end": 1367.8, "text": " so what we do, now we know we are, you know, not fast enough, not allocating, we are over", "tokens": [370, 437, 321, 360, 11, 586, 321, 458, 321, 366, 11, 291, 458, 11, 406, 2370, 1547, 11, 406, 12660, 990, 11, 321, 366, 670], "temperature": 0.0, "avg_logprob": -0.14469161260695684, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.0889337318076286e-05}, {"id": 264, "seek": 135380, "start": 1367.8, "end": 1372.6, "text": " allocating, right, so then we profile, then we check, okay, we have a problem, now let's", "tokens": [12660, 990, 11, 558, 11, 370, 550, 321, 7964, 11, 550, 321, 1520, 11, 1392, 11, 321, 362, 257, 1154, 11, 586, 718, 311], "temperature": 0.0, "avg_logprob": -0.14469161260695684, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.0889337318076286e-05}, {"id": 265, "seek": 135380, "start": 1372.6, "end": 1378.32, "text": " find what's going on, and this is where, on micro level, we can, you know, use profiling", "tokens": [915, 437, 311, 516, 322, 11, 293, 341, 307, 689, 11, 322, 4532, 1496, 11, 321, 393, 11, 291, 458, 11, 764, 1740, 4883], "temperature": 0.0, "avg_logprob": -0.14469161260695684, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.0889337318076286e-05}, {"id": 266, "seek": 137832, "start": 1378.32, "end": 1384.0, "text": " very easily by just adding those two flags, it will gather memory profiles and CPU profiles", "tokens": [588, 3612, 538, 445, 5127, 729, 732, 23265, 11, 309, 486, 5448, 4675, 23693, 293, 13199, 23693], "temperature": 0.0, "avg_logprob": -0.19080856572026791, "compression_ratio": 1.6422018348623852, "no_speech_prob": 2.665031934157014e-05}, {"id": 267, "seek": 137832, "start": 1384.0, "end": 1390.96, "text": " in the file, like v1.mempprof, on macro level, you can, there are other ways of gathering", "tokens": [294, 264, 3991, 11, 411, 371, 16, 13, 17886, 79, 29175, 11, 322, 18887, 1496, 11, 291, 393, 11, 456, 366, 661, 2098, 295, 13519], "temperature": 0.0, "avg_logprob": -0.19080856572026791, "compression_ratio": 1.6422018348623852, "no_speech_prob": 2.665031934157014e-05}, {"id": 268, "seek": 137832, "start": 1390.96, "end": 1394.72, "text": " profiles, but you can use the same format, the same tools, there are even continuous", "tokens": [23693, 11, 457, 291, 393, 764, 264, 912, 7877, 11, 264, 912, 3873, 11, 456, 366, 754, 10957], "temperature": 0.0, "avg_logprob": -0.19080856572026791, "compression_ratio": 1.6422018348623852, "no_speech_prob": 2.665031934157014e-05}, {"id": 269, "seek": 137832, "start": 1394.72, "end": 1402.08, "text": " profiling tools in open source, like parkadev, I really recommend them, and it's super easy", "tokens": [1740, 4883, 3873, 294, 1269, 4009, 11, 411, 3884, 762, 85, 11, 286, 534, 2748, 552, 11, 293, 309, 311, 1687, 1858], "temperature": 0.0, "avg_logprob": -0.19080856572026791, "compression_ratio": 1.6422018348623852, "no_speech_prob": 2.665031934157014e-05}, {"id": 270, "seek": 140208, "start": 1402.08, "end": 1408.3999999999999, "text": " then to gather those profiles over time, so this, what we want to really learn is that", "tokens": [550, 281, 5448, 729, 23693, 670, 565, 11, 370, 341, 11, 437, 321, 528, 281, 534, 1466, 307, 300], "temperature": 0.0, "avg_logprob": -0.15178821899078704, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.1461753426119685e-06}, {"id": 271, "seek": 140208, "start": 1408.3999999999999, "end": 1413.36, "text": " what causes this problem, and this is like a CPU profile, and we could spot, and the", "tokens": [437, 7700, 341, 1154, 11, 293, 341, 307, 411, 257, 13199, 7964, 11, 293, 321, 727, 4008, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.15178821899078704, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.1461753426119685e-06}, {"id": 272, "seek": 140208, "start": 1413.36, "end": 1420.52, "text": " wider means it spends more CPU cycles, the depth doesn't matter, this is just how many", "tokens": [11842, 1355, 309, 25620, 544, 13199, 17796, 11, 264, 7161, 1177, 380, 1871, 11, 341, 307, 445, 577, 867], "temperature": 0.0, "avg_logprob": -0.15178821899078704, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.1461753426119685e-06}, {"id": 273, "seek": 140208, "start": 1420.52, "end": 1426.32, "text": " functions we have, right, so we can see that create, of course, is one of the biggest contributors,", "tokens": [6828, 321, 362, 11, 558, 11, 370, 321, 393, 536, 300, 1884, 11, 295, 1164, 11, 307, 472, 295, 264, 3880, 45627, 11], "temperature": 0.0, "avg_logprob": -0.15178821899078704, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.1461753426119685e-06}, {"id": 274, "seek": 142632, "start": 1426.32, "end": 1433.4399999999998, "text": " but the growth slice, right, like why we spend so many cycles growing slice, ideally, I know", "tokens": [457, 264, 4599, 13153, 11, 558, 11, 411, 983, 321, 3496, 370, 867, 17796, 4194, 13153, 11, 22915, 11, 286, 458], "temperature": 0.0, "avg_logprob": -0.19739659118652345, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.1607088708842639e-05}, {"id": 275, "seek": 142632, "start": 1433.4399999999998, "end": 1439.12, "text": " how many elements I have, kind of why it doesn't grow me once, right, and then we can check,", "tokens": [577, 867, 4959, 286, 362, 11, 733, 295, 983, 309, 1177, 380, 1852, 385, 1564, 11, 558, 11, 293, 550, 321, 393, 1520, 11], "temperature": 0.0, "avg_logprob": -0.19739659118652345, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.1607088708842639e-05}, {"id": 276, "seek": 142632, "start": 1439.12, "end": 1444.8, "text": " and by the way, you can use this go tool, pprof.gttp, locally, I kind of use it a lot", "tokens": [293, 538, 264, 636, 11, 291, 393, 764, 341, 352, 2290, 11, 280, 29175, 13, 70, 6319, 79, 11, 16143, 11, 286, 733, 295, 764, 309, 257, 688], "temperature": 0.0, "avg_logprob": -0.19739659118652345, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.1607088708842639e-05}, {"id": 277, "seek": 142632, "start": 1444.8, "end": 1449.56, "text": " on this file to kind of expose this kind of interactive UI, you can do the same for memory,", "tokens": [322, 341, 3991, 281, 733, 295, 19219, 341, 733, 295, 15141, 15682, 11, 291, 393, 360, 264, 912, 337, 4675, 11], "temperature": 0.0, "avg_logprob": -0.19739659118652345, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.1607088708842639e-05}, {"id": 278, "seek": 142632, "start": 1449.56, "end": 1454.24, "text": " but honestly, this is not useful because Append is a standard library function, and they are", "tokens": [457, 6095, 11, 341, 307, 406, 4420, 570, 3132, 521, 307, 257, 3832, 6405, 2445, 11, 293, 436, 366], "temperature": 0.0, "avg_logprob": -0.19739659118652345, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.1607088708842639e-05}, {"id": 279, "seek": 145424, "start": 1454.24, "end": 1458.84, "text": " not very well exposed, right, so they're hidden, so this is not very helpful, actually CPU", "tokens": [406, 588, 731, 9495, 11, 558, 11, 370, 436, 434, 7633, 11, 370, 341, 307, 406, 588, 4961, 11, 767, 13199], "temperature": 0.0, "avg_logprob": -0.15716329351201788, "compression_ratio": 1.6856060606060606, "no_speech_prob": 8.392197742068674e-06}, {"id": 280, "seek": 145424, "start": 1458.84, "end": 1463.16, "text": " profile was more helpful, because it pointed us to the growth slice, and if we just Google", "tokens": [7964, 390, 544, 4961, 11, 570, 309, 10932, 505, 281, 264, 4599, 13153, 11, 293, 498, 321, 445, 3329], "temperature": 0.0, "avg_logprob": -0.15716329351201788, "compression_ratio": 1.6856060606060606, "no_speech_prob": 8.392197742068674e-06}, {"id": 281, "seek": 145424, "start": 1463.16, "end": 1467.84, "text": " for that, you will notice this comes from Append, and then you can go to documentation", "tokens": [337, 300, 11, 291, 486, 3449, 341, 1487, 490, 3132, 521, 11, 293, 550, 291, 393, 352, 281, 14333], "temperature": 0.0, "avg_logprob": -0.15716329351201788, "compression_ratio": 1.6856060606060606, "no_speech_prob": 8.392197742068674e-06}, {"id": 282, "seek": 145424, "start": 1467.84, "end": 1472.0, "text": " of Append and learn what it actually does, and as you probably are familiar, because", "tokens": [295, 3132, 521, 293, 1466, 437, 309, 767, 775, 11, 293, 382, 291, 1391, 366, 4963, 11, 570], "temperature": 0.0, "avg_logprob": -0.15716329351201788, "compression_ratio": 1.6856060606060606, "no_speech_prob": 8.392197742068674e-06}, {"id": 283, "seek": 145424, "start": 1472.0, "end": 1479.0, "text": " this is like, should be a trivial case, Append resizes the slice, or assizes the underlying", "tokens": [341, 307, 411, 11, 820, 312, 257, 26703, 1389, 11, 3132, 521, 725, 5660, 264, 13153, 11, 420, 1256, 5660, 264, 14217], "temperature": 0.0, "avg_logprob": -0.15716329351201788, "compression_ratio": 1.6856060606060606, "no_speech_prob": 8.392197742068674e-06}, {"id": 284, "seek": 147900, "start": 1479.0, "end": 1485.96, "text": " array, whenever it's full, right, and resizing, it's not super simple, it has to kind of create", "tokens": [10225, 11, 5699, 309, 311, 1577, 11, 558, 11, 293, 725, 3319, 11, 309, 311, 406, 1687, 2199, 11, 309, 575, 281, 733, 295, 1884], "temperature": 0.0, "avg_logprob": -0.16118287025613987, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.801517282729037e-05}, {"id": 285, "seek": 147900, "start": 1485.96, "end": 1493.76, "text": " a new, bigger array, and copy things over, and garbage collection will kill the old one,", "tokens": [257, 777, 11, 3801, 10225, 11, 293, 5055, 721, 670, 11, 293, 14150, 5765, 486, 1961, 264, 1331, 472, 11], "temperature": 0.0, "avg_logprob": -0.16118287025613987, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.801517282729037e-05}, {"id": 286, "seek": 147900, "start": 1493.76, "end": 1498.96, "text": " but not fast enough because of the garbage collection, so we kind of aggregate that as", "tokens": [457, 406, 2370, 1547, 570, 295, 264, 14150, 5765, 11, 370, 321, 733, 295, 26118, 300, 382], "temperature": 0.0, "avg_logprob": -0.16118287025613987, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.801517282729037e-05}, {"id": 287, "seek": 147900, "start": 1498.96, "end": 1508.96, "text": " another allocation, right, so this is what happens, and kind of the fix is to just preallocate", "tokens": [1071, 27599, 11, 558, 11, 370, 341, 307, 437, 2314, 11, 293, 733, 295, 264, 3191, 307, 281, 445, 659, 336, 42869], "temperature": 0.0, "avg_logprob": -0.16118287025613987, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.801517282729037e-05}, {"id": 288, "seek": 150896, "start": 1508.96, "end": 1513.64, "text": " right, so to tell, you know, when you create the slice, okay, how much capacity you want", "tokens": [558, 11, 370, 281, 980, 11, 291, 458, 11, 562, 291, 1884, 264, 13153, 11, 1392, 11, 577, 709, 6042, 291, 528], "temperature": 0.0, "avg_logprob": -0.19479299182734214, "compression_ratio": 1.732, "no_speech_prob": 1.9961447833338752e-05}, {"id": 289, "seek": 150896, "start": 1513.64, "end": 1518.24, "text": " to prepare for that, and thanks for that, so what we do now, okay, we did optimize in", "tokens": [281, 5940, 337, 300, 11, 293, 3231, 337, 300, 11, 370, 437, 321, 360, 586, 11, 1392, 11, 321, 630, 19719, 294], "temperature": 0.0, "avg_logprob": -0.19479299182734214, "compression_ratio": 1.732, "no_speech_prob": 1.9961447833338752e-05}, {"id": 290, "seek": 150896, "start": 1518.24, "end": 1524.64, "text": " our TFBO, now we test, before we're even measuring, because if you are not testing", "tokens": [527, 40964, 15893, 11, 586, 321, 1500, 11, 949, 321, 434, 754, 13389, 11, 570, 498, 291, 366, 406, 4997], "temperature": 0.0, "avg_logprob": -0.19479299182734214, "compression_ratio": 1.732, "no_speech_prob": 1.9961447833338752e-05}, {"id": 291, "seek": 150896, "start": 1524.64, "end": 1530.3600000000001, "text": " if this, you know, this code is correct, then, you know, you might be, you know, yeah, we", "tokens": [498, 341, 11, 291, 458, 11, 341, 3089, 307, 3006, 11, 550, 11, 291, 458, 11, 291, 1062, 312, 11, 291, 458, 11, 1338, 11, 321], "temperature": 0.0, "avg_logprob": -0.19479299182734214, "compression_ratio": 1.732, "no_speech_prob": 1.9961447833338752e-05}, {"id": 292, "seek": 150896, "start": 1530.3600000000001, "end": 1533.92, "text": " would be happy that things are faster, but functionally broken, so always test, don't", "tokens": [576, 312, 2055, 300, 721, 366, 4663, 11, 457, 2445, 379, 5463, 11, 370, 1009, 1500, 11, 500, 380], "temperature": 0.0, "avg_logprob": -0.19479299182734214, "compression_ratio": 1.732, "no_speech_prob": 1.9961447833338752e-05}, {"id": 293, "seek": 153392, "start": 1533.92, "end": 1539.16, "text": " be, you know, lazy, run those unit tests, easy, and then, you know, once they are passing,", "tokens": [312, 11, 291, 458, 11, 14847, 11, 1190, 729, 4985, 6921, 11, 1858, 11, 293, 550, 11, 291, 458, 11, 1564, 436, 366, 8437, 11], "temperature": 0.0, "avg_logprob": -0.18024188753158327, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.279317348235054e-05}, {"id": 294, "seek": 153392, "start": 1539.16, "end": 1544.8400000000001, "text": " you can comfortably measure, again, I just changed V2, just to specify another variable,", "tokens": [291, 393, 25101, 3481, 11, 797, 11, 286, 445, 3105, 691, 17, 11, 445, 281, 16500, 1071, 7006, 11], "temperature": 0.0, "avg_logprob": -0.18024188753158327, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.279317348235054e-05}, {"id": 295, "seek": 153392, "start": 1544.8400000000001, "end": 1551.44, "text": " right, on our file system, and then I can do a bunch that V1.txt and then V2.txt, actually,", "tokens": [558, 11, 322, 527, 3991, 1185, 11, 293, 550, 286, 393, 360, 257, 3840, 300, 691, 16, 13, 83, 734, 293, 550, 691, 17, 13, 83, 734, 11, 767, 11], "temperature": 0.0, "avg_logprob": -0.18024188753158327, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.279317348235054e-05}, {"id": 296, "seek": 153392, "start": 1551.44, "end": 1556.3600000000001, "text": " I can put like 100 of those variables, it will compare all of them, but here we compare", "tokens": [286, 393, 829, 411, 2319, 295, 729, 9102, 11, 309, 486, 6794, 439, 295, 552, 11, 457, 510, 321, 6794], "temperature": 0.0, "avg_logprob": -0.18024188753158327, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.279317348235054e-05}, {"id": 297, "seek": 153392, "start": 1556.3600000000001, "end": 1562.24, "text": " two, and not only we have absolute values of those measurements, but also a diff, right,", "tokens": [732, 11, 293, 406, 787, 321, 362, 8236, 4190, 295, 729, 15383, 11, 457, 611, 257, 7593, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.18024188753158327, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.279317348235054e-05}, {"id": 298, "seek": 156224, "start": 1562.24, "end": 1568.28, "text": " so you can see we improved a lot, and if we check absolute value in regards to our efficiency", "tokens": [370, 291, 393, 536, 321, 9689, 257, 688, 11, 293, 498, 321, 1520, 8236, 2158, 294, 14258, 281, 527, 10493], "temperature": 0.0, "avg_logprob": -0.15507198149158108, "compression_ratio": 1.708955223880597, "no_speech_prob": 3.496155613902374e-06}, {"id": 299, "seek": 156224, "start": 1568.28, "end": 1574.72, "text": " requirements, you see that we met our threshold roughly, but like we estimated it, so it's", "tokens": [7728, 11, 291, 536, 300, 321, 1131, 527, 14678, 9810, 11, 457, 411, 321, 14109, 309, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.15507198149158108, "compression_ratio": 1.708955223880597, "no_speech_prob": 3.496155613902374e-06}, {"id": 300, "seek": 156224, "start": 1574.72, "end": 1580.0, "text": " totally good, you know, 15 megabytes, we have 15 megabytes, and then it's faster than our", "tokens": [3879, 665, 11, 291, 458, 11, 2119, 10816, 24538, 11, 321, 362, 2119, 10816, 24538, 11, 293, 550, 309, 311, 4663, 813, 527], "temperature": 0.0, "avg_logprob": -0.15507198149158108, "compression_ratio": 1.708955223880597, "no_speech_prob": 3.496155613902374e-06}, {"id": 301, "seek": 156224, "start": 1580.0, "end": 1585.68, "text": " goal, so now we are good to go and release it, right, so that's kind of the whole loop,", "tokens": [3387, 11, 370, 586, 321, 366, 665, 281, 352, 293, 4374, 309, 11, 558, 11, 370, 300, 311, 733, 295, 264, 1379, 6367, 11], "temperature": 0.0, "avg_logprob": -0.15507198149158108, "compression_ratio": 1.708955223880597, "no_speech_prob": 3.496155613902374e-06}, {"id": 302, "seek": 156224, "start": 1585.68, "end": 1591.76, "text": " and you kind of do it until you're happy with your results, so yeah, this is it, and learnings,", "tokens": [293, 291, 733, 295, 360, 309, 1826, 291, 434, 2055, 365, 428, 3542, 11, 370, 1338, 11, 341, 307, 309, 11, 293, 2539, 82, 11], "temperature": 0.0, "avg_logprob": -0.15507198149158108, "compression_ratio": 1.708955223880597, "no_speech_prob": 3.496155613902374e-06}, {"id": 303, "seek": 159176, "start": 1591.76, "end": 1597.56, "text": " again, five learnings, follow TFBO, test, fix, benchmark, optimize, use benchmarks, they", "tokens": [797, 11, 1732, 2539, 82, 11, 1524, 40964, 15893, 11, 1500, 11, 3191, 11, 18927, 11, 19719, 11, 764, 43751, 11, 436], "temperature": 0.0, "avg_logprob": -0.23300610819170553, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.662242332007736e-05}, {"id": 304, "seek": 159176, "start": 1597.56, "end": 1603.52, "text": " are built into GoLang, they are super amazing, GoTest slash bench, set the clear goals, goals", "tokens": [366, 3094, 666, 1037, 43, 656, 11, 436, 366, 1687, 2243, 11, 1037, 51, 377, 17330, 10638, 11, 992, 264, 1850, 5493, 11, 5493], "temperature": 0.0, "avg_logprob": -0.23300610819170553, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.662242332007736e-05}, {"id": 305, "seek": 159176, "start": 1603.52, "end": 1609.44, "text": " are super important here, right, and then profile, and you can, I mean, GoLang uses", "tokens": [366, 1687, 1021, 510, 11, 558, 11, 293, 550, 7964, 11, 293, 291, 393, 11, 286, 914, 11, 1037, 43, 656, 4960], "temperature": 0.0, "avg_logprob": -0.23300610819170553, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.662242332007736e-05}, {"id": 306, "seek": 159176, "start": 1609.44, "end": 1615.04, "text": " Pprof, which you can Google as well, it's like amazing kind of protocol, kind of set", "tokens": [430, 29175, 11, 597, 291, 393, 3329, 382, 731, 11, 309, 311, 411, 2243, 733, 295, 10336, 11, 733, 295, 992], "temperature": 0.0, "avg_logprob": -0.23300610819170553, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.662242332007736e-05}, {"id": 307, "seek": 159176, "start": 1615.04, "end": 1620.44, "text": " of tools, integrated with other, you know, clouds and so on, and use it, you know, every", "tokens": [295, 3873, 11, 10919, 365, 661, 11, 291, 458, 11, 12193, 293, 370, 322, 11, 293, 764, 309, 11, 291, 458, 11, 633], "temperature": 0.0, "avg_logprob": -0.23300610819170553, "compression_ratio": 1.6988416988416988, "no_speech_prob": 2.662242332007736e-05}, {"id": 308, "seek": 162044, "start": 1620.44, "end": 1626.3200000000002, "text": " day whenever I have to optimize something, and then finally, the key is to try to understand", "tokens": [786, 5699, 286, 362, 281, 19719, 746, 11, 293, 550, 2721, 11, 264, 2141, 307, 281, 853, 281, 1223], "temperature": 0.0, "avg_logprob": -0.14446963904038915, "compression_ratio": 1.796078431372549, "no_speech_prob": 1.8841565179172903e-05}, {"id": 309, "seek": 162044, "start": 1626.3200000000002, "end": 1631.16, "text": " what happens, what I expected, and you know, what's wrong, reading documentation, reading", "tokens": [437, 2314, 11, 437, 286, 5176, 11, 293, 291, 458, 11, 437, 311, 2085, 11, 3760, 14333, 11, 3760], "temperature": 0.0, "avg_logprob": -0.14446963904038915, "compression_ratio": 1.796078431372549, "no_speech_prob": 1.8841565179172903e-05}, {"id": 310, "seek": 162044, "start": 1631.16, "end": 1636.3200000000002, "text": " code, this is what you have to do sometimes, and a general tip, whenever you want to optimize", "tokens": [3089, 11, 341, 307, 437, 291, 362, 281, 360, 2171, 11, 293, 257, 2674, 4125, 11, 5699, 291, 528, 281, 19719], "temperature": 0.0, "avg_logprob": -0.14446963904038915, "compression_ratio": 1.796078431372549, "no_speech_prob": 1.8841565179172903e-05}, {"id": 311, "seek": 162044, "start": 1636.3200000000002, "end": 1642.72, "text": " something super, super carefully in some, you know, bottleneck part of your code, I", "tokens": [746, 1687, 11, 1687, 7500, 294, 512, 11, 291, 458, 11, 44641, 547, 644, 295, 428, 3089, 11, 286], "temperature": 0.0, "avg_logprob": -0.14446963904038915, "compression_ratio": 1.796078431372549, "no_speech_prob": 1.8841565179172903e-05}, {"id": 312, "seek": 162044, "start": 1642.72, "end": 1647.3600000000001, "text": " mean, avoid standard library functions, because they are really built into generic functionality,", "tokens": [914, 11, 5042, 3832, 6405, 6828, 11, 570, 436, 366, 534, 3094, 666, 19577, 14980, 11], "temperature": 0.0, "avg_logprob": -0.14446963904038915, "compression_ratio": 1.796078431372549, "no_speech_prob": 1.8841565179172903e-05}, {"id": 313, "seek": 164736, "start": 1647.36, "end": 1650.9599999999998, "text": " it will test, I mean, it will do a lot of things with, you know, different edge cases", "tokens": [309, 486, 1500, 11, 286, 914, 11, 309, 486, 360, 257, 688, 295, 721, 365, 11, 291, 458, 11, 819, 4691, 3331], "temperature": 0.0, "avg_logprob": -0.18814409573872884, "compression_ratio": 1.6477732793522266, "no_speech_prob": 3.845905666821636e-05}, {"id": 314, "seek": 164736, "start": 1650.9599999999998, "end": 1656.1999999999998, "text": " that you might not have, so a lot of times, I just implemented my own parsing integer", "tokens": [300, 291, 1062, 406, 362, 11, 370, 257, 688, 295, 1413, 11, 286, 445, 12270, 452, 1065, 21156, 278, 24922], "temperature": 0.0, "avg_logprob": -0.18814409573872884, "compression_ratio": 1.6477732793522266, "no_speech_prob": 3.845905666821636e-05}, {"id": 315, "seek": 164736, "start": 1656.1999999999998, "end": 1660.32, "text": " function, it was much faster, so this is a general tip that always works, but again,", "tokens": [2445, 11, 309, 390, 709, 4663, 11, 370, 341, 307, 257, 2674, 4125, 300, 1009, 1985, 11, 457, 797, 11], "temperature": 0.0, "avg_logprob": -0.18814409573872884, "compression_ratio": 1.6477732793522266, "no_speech_prob": 3.845905666821636e-05}, {"id": 316, "seek": 164736, "start": 1660.32, "end": 1665.04, "text": " do it only when you need it, because you might have a box in this code, right?", "tokens": [360, 309, 787, 562, 291, 643, 309, 11, 570, 291, 1062, 362, 257, 2424, 294, 341, 3089, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18814409573872884, "compression_ratio": 1.6477732793522266, "no_speech_prob": 3.845905666821636e-05}, {"id": 317, "seek": 164736, "start": 1665.04, "end": 1668.6399999999999, "text": " So that's it, thank you, you have a link here, bwplotka.dev.", "tokens": [407, 300, 311, 309, 11, 1309, 291, 11, 291, 362, 257, 2113, 510, 11, 272, 86, 564, 310, 2330, 13, 40343, 13], "temperature": 0.0, "avg_logprob": -0.18814409573872884, "compression_ratio": 1.6477732793522266, "no_speech_prob": 3.845905666821636e-05}, {"id": 318, "seek": 166864, "start": 1668.64, "end": 1678.64, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.7126898765563965, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.000977973686531186}], "language": "en"}